# Shifted Interpolation for Differential Privacy 

Jinho Bok<br>UPenn<br>jinhobok@upenn.edu

Weijie Su<br>UPenn<br>suw@upenn.edu

Jason M. Altschuler<br>UPenn<br>alts@upenn.edu

February 29, 2024


#### Abstract

Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the "privacy amplification by iteration" phenomenon in the unifying framework of $f$-differential privacy-which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\varepsilon, \delta)$-DP and Rényi DP. Our key technical insight is the construction of shifted interpolated processes that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first exact privacy analysis in the foundational setting of strongly convex optimization. Our techniques extend to many settings: convex/strongly convex, constrained/unconstrained, full/cyclic/stochastic batches, and all combinations thereof. As an immediate corollary, we recover the $f$-DP characterization of the exponential mechanism for strongly convex optimization in [GLL22], and moreover extend this result to more general settings.


## Contents

1 Introduction ..... 3
1.1 Contribution ..... 4
1.2 Techniques ..... 5
1.3 Outline ..... 5
2 Preliminaries ..... 5
2.1 Differential privacy ..... 5
2.2 Convex optimization ..... 7
2.3 Private optimization algorithms ..... 7
3 Shifted interpolation for $f$-DP ..... 7
3.1 Previous (divergent) $f$-DP bounds, via composition ..... 8
3.2 Convergent $f$-DP bounds, via shifted interpolation ..... 8
4 Improved privacy for noisy optimization algorithms ..... 10
4.1 Noisy gradient descent ..... 10
4.2 Noisy cyclic gradient descent ..... 11
4.3 Noisy stochastic gradient descent ..... 12
4.4 Numerical example ..... 12
$5 f$-DP of the exponential mechanism ..... 13
5.1 Strongly log-concave targets ..... 13
5.2 Log-concave targets ..... 14
6 Discussion ..... 14
References ..... 15
A Rényi DP and tradeoff functions ..... 18
A. 1 Rényi DP ..... 18
A. 2 Lemmas on tradeoff functions ..... 18
A. 3 Convergence of tradeoff functions ..... 19
B Disentangling the shift in shifted divergences ..... 21
C Deferred details for $\S 4$ ..... 22
C. 1 Shifted interpolation for contractive noisy iterations ..... 22
C. 2 Deferred proofs for $\S 4.1$ ..... 24
C. 3 Deferred proofs for $\S 4.2$ ..... 26
C. 4 Deferred proofs for $\S 4.3$ ..... 28
D Numerical details and results ..... 34
D. 1 Details for Figure 1 ..... 34
D. 2 Details for $\S 4.4$ ..... 34
D. 3 Additional numerics ..... 36
D. 4 Comparison of privacy bounds for the exponential mechanism ..... 40
D. 5 Numerical composition of subsampled GDP ..... 41

## 1 Introduction

Private optimization is the primary approach for private machine learning. The goal is to train good models while not leaking sensitive attributes of the training data. Differential privacy (DP) is the gold standard for measuring this information leakage [DMNS06, DR14], and noisy gradient descent and its variants are the predominant algorithms for private optimization. It is therefore a central question to quantify the differential privacy of these algorithms - however, tight characterizations remain open, even in the seemingly simple setting of convex optimization.

In words, DP measures how distinguishable the output of a (randomized) algorithm is when run on two adjacent datasets, i.e., two datasets that differ in only one individual record. There are several ways to measure distinguishability-leading to many relaxations of DP, e.g., [BS16, Mir17, DRS22]. Different DP notions lead to different privacy analyses, and a long line of work has sought to prove sharp privacy bounds for noisy gradient descent and its variants [BST14, \$\mathrm{ACG}^{+}\$16, FMTT18, CYS21, YS22, AT22, ABT24].

A common approach is to use the composition theorem, which pays a price in privacy for every intermediate iterate along the optimization trajectory, leading to possibly suboptimal privacy bounds. Recent work has significantly improved the privacy analysis in the case of convex and strongly convex losses by showing that the privacy leakage of noisy (stochastic) gradient descent does not increase ad infinitum in the number of iterations $t$ [CYS21, YS22, AT22, ABT24]. This is in stark contrast to the composition-based approach, which gives privacy bounds that scale as $\sqrt{t}$.

All these "convergent" privacy bounds were proved in the Rényi DP framework, which is inherently lossy. To achieve the tightest possible privacy bound, a natural goal is to use the $f$-DP framework [DRS22] for analysis, since it is an information-theoretically lossless definition of DP. This definition measures distinguishability in terms of the Type I vs Type II error tradeoff curve $f$ for the hypothesis testing problem of whether a given user was in the training dataset. The $f$-DP framework is desirable because: (1) $f$-DP exactly characterizes all relevant aspects of the hypothesis testing problem defining DP, and thus (optimal) $f$-DP bounds can be losslessly converted to (optimal) bounds in other notions of privacy such as $(\varepsilon, \delta)$-DP or Rényi DP, (2) $f$-DP is lossless under the composition of multiple private mechanisms, which is the most ubiquitous operation in DP since it enables combining building blocks, and (3) $f$-DP is easily interpretable in terms of the original hypothesis testing definition of DP.

However, analyzing privacy leakage in the $f$-DP framework is often challenging since quantifying the entire tradeoff between Type I/II error is substantially more difficult than quantifying (less informative) alternative notions of privacy. Consequently, the analysis toolbox for $f$-DP is currently limited. These limitations are pronounced for the fundamental problem of analyzing the privacy loss of noisy gradient descent and its variants. To put this into perspective, existing privacy guarantees based on $f$-DP diverge as the number of iterations $t$ increases, whereas the aforementioned recent work has used divergence-based DP definitions to show that noisy gradient descent and its variants can remain private even when run indefinitely, for problems that are strongly convex [CYS21, YS22, AT22, ABT24] or even just convex [AT22, ABT24]. Convergent privacy bounds complement celebrated results for minimax-optimal privacy-utility tradeoffs [BST14, BFTGT19] because they enable longer training-which is useful since typical learning problems are not worst-case and benefit from training longer.

Can convergent privacy bounds be achieved directly ${ }^{1}$ in the tight framework of $f$-DP? All current arguments are tailored to Rényi DP - an analytically convenient but inherently lossy relaxation of DP-and do not appear to extend. Answering this question necessitates developing fundamentally different techniques for $f$-DP, since convergent privacy bounds require only releasing the algorithm's final iterate - in sharp contrast to existing $f$-DP techniques such as the composition theorem which can only argue about the accumulated privacy loss of releasing all intermediate iterates. Tight $f$-DP analyses typically require closed-form expressions for the random variable in question - in order to argue about the tradeoff of Type I/II error - but this is impossible for the final iterate of (stochastic) gradient descent due to the non-linearity intrinsic to each iteration.[^0]![](https://cdn.mathpix.com/cropped/2024_05_26_6dd009bc4ef3ccde1c7fg-04.jpg?height=556&width=1136&top_left_y=253&top_left_x=494)

Figure 1: Left: improved $f$-DP vs. the standard composition analysis. Right: improved $(\varepsilon, \delta)$-DP by losslessly converting from $f$-DP. Our privacy bound is optimal in all parameters, here for NoisyGD on strongly convex losses; see $\S \mathrm{D}$ for the parameter choices and other settings. Our $f$-DP analysis also implies optimal bounds for the Rényi DP framework, but $f$-DP is strictly better since it captures all aspects of the privacy leakage, whereas Rényi DP is intrinsically lossy.

### 1.1 Contribution

Our primary technical contribution is establishing (and refining) the "privacy amplification by iteration" phenomenon in the unifying framework of $f$-DP. This enables directly analyzing the privacy loss of the final iterate of noisy gradient descent (and its variants), leading to the first direct $f$-DP analysis that is convergent as the number of iterations $t \rightarrow \infty$. $\S 1.2$ overviews this new analysis technique.

Notably, this yields the first exact privacy analysis in the foundational setting of strongly convex losses. To our knowledge, there is no other setting where exact privacy analyses are known for $t>1$, except for the setting of convex quadratic losses which is analytically trivial because all iterates are explicit Gaussians. ${ }^{2}$

We emphasize that our techniques are versatile and readily extend to many settings - a well-known challenge for other convergent analyses, even for simpler relaxations of DP like Rényi DP [CYS21, YS22, AT22, ABT24]. In §4, we illustrate how our analysis extends to convex/strongly convex losses, constrained/unconstrained optimization, full/cyclic/stochastic batches, and all combinations thereof.

Since our improved privacy guarantees are for $f$-DP (Figure 1, left), lossless conversions immediately imply improved guarantees for other notions of privacy like Rényi DP and $(\varepsilon, \delta)$-DP (Figure 1, right). For example, for the strongly convex setting, our exact bound improves over previous results by a factor of 2 in Rényi DP, and thus by even more in $(\varepsilon, \delta)$-DP due to the intrinsic lossiness of Rényi DP that we overcome by directly analyzing in $f$-DP. In practice, improving the privacy by a factor of two enables training for twice as long or with half the noise, while satisfying the same privacy budget. Although this paper's focus is the theoretical methodology, preliminary numerics in $\S 4.4$ corroborate that our improved privacy guarantees can be helpful in practice.

Since our privacy bounds are convergent in the number of iterations $t$, we can take the limit $t \rightarrow \infty$ to bound the $f$-DP of the stationary distributions of these optimization algorithms. As an immediate corollary, we recover the recent $f$-DP characterization of the exponential mechanism for strongly convex losses in [GLL22], and moreover extend this result to more general settings in $\S 5$.[^1]

### 1.2 Techniques

The core innovation underlying our results is the construction of certain auxiliary processes, shifted interpolated processes, which enable directly analyzing the Type I/II error tradeoff between the final iterates of two stochastic processes - even when their laws are complicated and non-explicit. Informally, this argument enables running coupling arguments - traditionally possible only for Wasserstein analysis-to analyze tradeoff functions for the first time. In this paper, the two processes are noisy (stochastic, projected) gradient descent run on two adjacent datasets, but the technique is more general and we believe may be of independent interest. See $\S 3$ for a detailed overview.

Crucially, our argument is geometrically aware: it exploits (strong) convexity of losses via (strong) contractivity of gradient descent updates, in order to argue that sensitive gradient queries have (exponentially) decaying privacy leakage, the longer ago they were performed. This is essential for convergent privacy bounds, and is impossible with the standard composition-based analysis - which only exploits the sensitivity of the losses, and is oblivious to any further geometric phenomena like convexity or contractivity.

A key motivation behind the construction of our auxiliary sequence is that it demystifies the popular privacy amplification by iteration analysis [FMTT18], which has been used in many contexts, and in particular was recently shown to give convergent Rényi DP bounds [AT22, ABT24]. Those arguments rely on shifted divergences, which combine Rényi divergence and Wasserstein distance, and it was an open question whether this ad-hoc potential function could be simplified. Our shifted interpolated process answers this: its iterates coincide with the optimal "shifts" in the shifted divergence argument, which allows us to disentangle the Rényi and Wasserstein components of the shifted divergence argument; details in §B. Crucially, this disentanglement enables generalizations beyond divergence-based relaxations of DP, to $f$-DP. ${ }^{3}$

### 1.3 Outline

$\S 2$ recalls relevant preliminaries from differential privacy and convex optimization. $\S 3$ introduces the technique of shifted interpolation. $\S 4$ uses this to establish improved privacy bounds for noisy gradient descent and its variants for the foundational settings of convex and strongly convex losses. $\S 5$ describes how, as immediate corollaries of these convergent privacy bounds, taking an appropriate limit recovers and generalizes recent results on the $f$-DP of the exponential mechanism. $\S 6$ discusses future directions motivated by our results. For brevity, various helper lemmas, proof details, and additional numerical experiments are deferred to the Appendix. Code reproducing our numerics can be found here: https: //github.com/jinhobok/shifted_interpolation_dp.

## 2 Preliminaries

### 2.1 Differential privacy

DP measures the distinguishability between outputs of a randomized algorithm run on adjacent datasets, i.e., datasets that differ on at most one data point [DMNS06]. The most popular definition is $(\varepsilon, \delta)$-DP.

Definition $2.1\left((\varepsilon, \delta)\right.$-DP). A randomized algorithm $\mathcal{A}$ is $(\varepsilon, \delta)$-DP if for any adjacent datasets $S, S^{\prime}$ and any event $E$,

$$
\mathbb{P}(\mathcal{A}(S) \in E) \leq e^{\varepsilon} \mathbb{P}\left(\mathcal{A}\left(S^{\prime}\right) \in E\right)+\delta
$$

However, the most precise quantification of DP is based on the hypothesis-testing formulation [WZ10, KOV17]. This is formalized as $f$-DP [DRS22], where $f$ denotes a tradeoff function, i.e., a curve of hypothesis testing errors for a hypothesis test $\phi$.[^2]

![](https://cdn.mathpix.com/cropped/2024_05_26_6dd009bc4ef3ccde1c7fg-06.jpg?height=328&width=811&top_left_y=253&top_left_x=646)

Figure 2: Illustration of $f$-DP and GDP. Gaussian tradeoff functions $G(\mu)$ are less private as $\mu$ increases from 0 (full privacy) to $\infty$ (no privacy). The closer to Id, the more private. Here $\mathcal{A}$ is 1 -GDP but not 0.5 -GDP because its tradeoff function is pointwise above $G(1)$ but not pointwise above $G(0.5)$.

Definition $2.2(f-\mathrm{DP})$. For distributions $P, Q$ on the same space, the tradeoff function $T(P, Q):[0,1] \rightarrow$ $[0,1]$ is

$$
T(P, Q)(\alpha)=\inf \left\{1-\mathbb{E}_{Q} \phi: \mathbb{E}_{P} \phi \leq \alpha, 0 \leq \phi \leq 1\right\}
$$

A randomized algorithm $\mathcal{A}$ is $f$-DP if for any adjacent datasets $S$ and $S^{\prime}, T\left(\mathcal{A}(S), \mathcal{A}\left(S^{\prime}\right)\right) \geq f$.

Here and henceforth we use the pointwise ordering between tradeoff functions, i.e., we write $f \geq g$ if $f(\alpha) \geq g(\alpha)$ for all $\alpha$ in the domain $[0,1]$. We also use the standard abuse of notation of writing $T(X, Y)$ as shorthand for $T(\operatorname{Law}(X), \operatorname{Law}(Y))$.

The following lemma provides a useful characterization of tradeoff functions [DRS22, Proposition 1]. It follows that the most private tradeoff function is Id : $[0,1] \rightarrow[0,1]$, given by $\operatorname{Id}(\alpha)=1-\alpha$. See Figure 2 .

Lemma 2.3 (Characterization of tradeoff functions). A function $f:[0,1] \rightarrow[0,1]$ is a tradeoff function iff $f$ is decreasing, convex and $f(\alpha) \leq 1-\alpha$ for all $\alpha \in[0,1]$.

See $\S$ A. 2 for further details on tradeoff functions. Gaussian tradeoff functions are a particularly useful family. These are central to our analysis due to the Gaussian noise in noisy (stochastic) gradient descent.

Definition 2.4 (GDP). For $\mu \geq 0$, the Gaussian tradeoff function $G(\mu)$ is defined as $G(\mu)=T(\mathcal{N}(0,1), \mathcal{N}(\mu, 1))$. Its value at $\alpha \in[0,1]$ is given as $G(\mu)(\alpha)=\Phi\left(\Phi^{-1}(1-\alpha)-\mu\right)$, where $\Phi$ denotes the CDF of $\mathcal{N}(0,1)$. A randomized algorithm $\mathcal{A}$ is $\mu$-GDP if for any adjacent datasets $S$ and $S^{\prime}, T\left(\mathcal{A}(S), \mathcal{A}\left(S^{\prime}\right)\right) \geq G(\mu)$.

We now recall two key properties of tradeoff functions that are central to our analysis. The first states that post-processing two distributions cannot make them easier to distinguish [DRS22, Lemma 1].

Lemma 2.5 (Post-processing). For any probability distributions $P, Q$ and (random) map Proc,

$$
T(\operatorname{Proc}(P), \operatorname{Proc}(Q)) \geq T(P, Q)
$$

The next lemma enables analyzing the composition of multiple private mechanisms [DRS22, Definition $5 \&$ L Lemma C.1].

Definition 2.6 (Composition). The composition of two tradeoff functions $f=T(P, Q)$ and $g=T\left(P^{\prime}, Q^{\prime}\right)$ is defined as $f \otimes g=T\left(P \times P^{\prime}, Q \times Q^{\prime}\right)$. The $n$-fold composition of $f$ with itself is denoted $f^{\otimes n}$.

Lemma 2.7 (Strong composition). Let $K_{1}, K_{1}^{\prime}, K_{2}, K_{2}^{\prime}$ be (random) maps such that for all $y$,

$$
T\left(K_{1}(y), K_{1}^{\prime}(y)\right) \geq T\left(K_{2}(y), K_{2}^{\prime}(y)\right)
$$

Then

$$
T\left(\left(P, K_{1}(P)\right),\left(Q, K_{1}^{\prime}(Q)\right)\right) \geq T\left(\left(P, K_{2}(P)\right),\left(Q, K_{2}^{\prime}(Q)\right)\right)
$$

In particular, if $g=T\left(K_{2}(y), K_{2}^{\prime}(y)\right)$ does not depend on $y$, then $T\left(\left(P, K_{1}(P)\right),\left(Q, K_{1}^{\prime}(Q)\right)\right) \geq T(P, Q) \otimes g$.

### 2.2 Convex optimization

This paper focuses on convex losses because tight privacy guarantees for noisy gradient descent (and variants) are open even in this seemingly simple setting. We make use of the following two basic facts from convex optimization. Below, we say a function is contractive if it is 1-Lipschitz. Recall that a function $f$ is $M$-smooth if $\nabla f$ is $M$-Lipschitz, and is $m$-strongly convex if $x \mapsto f(x)-\frac{m}{2}\|x\|^{2}$ is convex.

Lemma 2.8. If $f$ is convex and $M$-smooth, then the gradient descent update $g(x)=x-\eta \nabla f(x)$ is contractive for each $\eta \in[0,2 / M]$. If $f$ is additionally m-strongly convex and $\eta \in(0,2 / M)$, then $g$ is $c$-Lipschitz where $c=\max \{|1-\eta m|,|1-\eta M|\}<1$.

Lemma 2.9. Let $\mathcal{K}$ be a closed and convex set in $\mathbb{R}^{d}$. Then the projection $\Pi_{\mathcal{K}}(x)=\arg \min _{z \in \mathcal{K}}\|z-x\|$ is well-defined and contractive.

### 2.3 Private optimization algorithms

Throughout, we consider a private optimization setting in which the goal is to minimize an objective function $F(x)=\frac{1}{n} \sum_{i=1}^{n} f_{i}(x)$, where the $i$-th loss function $f_{i}$ is associated with the $i$-th data point in a dataset $S$. An adjacent dataset $S^{\prime}$ corresponds to loss functions $\left\{f_{i}^{\prime}\right\}_{i \in[n]}$ where $f_{i} \equiv f_{i}^{\prime}$ except for a single index $i^{*}$.

Noisy gradient descent and its variants follow the general template of

$$
\begin{equation*}
X_{k+1} \leftarrow \Pi_{\mathcal{K}}\left[X_{k}-\eta\left(\frac{1}{b} \sum_{i \in B_{k}} \nabla f_{i}\left(X_{k}\right)+Z_{k+1}\right)\right], \quad k=0,1, \ldots, t-1 \tag{2.1}
\end{equation*}
$$

where $X_{0}$ is the initialization (e.g., zero), $\eta$ is the learning rate, $Z_{k+1} \sim \mathcal{N}\left(0, \sigma^{2} I_{d}\right)$ independently, $\sigma$ is the noise rate, $\mathcal{K}$ is the constraint set, and $t$ is the number of steps. The batch $B_{k}$ of size $b$ can be chosen in several ways:

- Full batches (NoisyGD): $B_{k} \equiv[n]$.
- Cyclic batches (NoisyCGD): Partition $[n]$ into batches of sizes $b$ and cycle through them.
- Stochastic batches (NoisySGD): Choose batches of size $b$ uniformly at random from $[n]$.

The advantage of the latter two variants is that they avoid computing the gradient of the objective, which can be computationally burdensome when $n$ is large.

A standard assumption in private optimization is the following notion of gradient sensitivity:

Definition 2.10 (Gradient sensitivity). A family of loss functions $\mathcal{F}$ has gradient sensitivity $L$ if

$$
\sup _{f, g \in \mathcal{F}}\|\nabla f-\nabla g\|_{\infty} \leq L
$$

For example, a family of $L$-Lipschitz loss functions has gradient sensitivity $2 L$. Another example is loss functions of the form $f_{i}=\ell_{i}+r$, where $\ell_{i}$ are convex, $L$-Lipschitz losses, and $r$ is a (non-Lipschitz) strongly convex regularization - the point being that this family of loss functions $\left\{f_{i}\right\}$ has finite gradient sensitivity $2 L$ despite each $f_{i}$ not being Lipschitz.

## 3 Shifted interpolation for $f$-DP

Here we explain the key conceptual ideas enabling our convergent $f$-DP bounds (see $\S 1.2$ for a high-level discussion). To preserve the flow of ideas we defer proofs to the Appendix. Below, in $\S 3.1$ we first recall the standard $f$-DP analysis based on the composition theorem and why it yields divergent bounds. Then in $\S 3.2$ we describe our technique of shifted interpolated processes and how this enables convergent $f$-DP bounds.

To explain the ideas in their simplest form, we consider here the setting of full-batch gradients and unconstrained optimization. Let $\left\{f_{i}\right\}_{i \in[n]}$ and $\left\{f_{i}^{\prime}\right\}_{i \in[n]}$ be the losses corresponding to two adjacent datasets, where $f_{i} \equiv f_{i}^{\prime}$ except for one index $i^{*}$. Then NoisyGD forms the iterates

$$
\begin{align*}
& X_{k+1}=\phi\left(X_{k}\right)+Z_{k+1}  \tag{3.1}\\
& X_{k+1}^{\prime}=\phi^{\prime}\left(X_{k}^{\prime}\right)+Z_{k+1}^{\prime} \tag{3.2}
\end{align*}
$$

where $X_{0}=X_{0}^{\prime}, \phi(x):=x-\frac{\eta}{n} \sum_{i=1}^{n} \nabla f_{i}(x), \phi^{\prime}\left(x^{\prime}\right):=x^{\prime}-\frac{\eta}{n} \sum_{i=1}^{n} \nabla f_{i}^{\prime}\left(x^{\prime}\right)$, and $Z_{k+1}, Z_{k+1}^{\prime} \sim \mathcal{N}\left(0, \eta^{2} \sigma^{2} I_{d}\right)$.

### 3.1 Previous (divergent) $f$-DP bounds, via composition

$f$-DP requires bounding $T\left(X_{t}, X_{t}^{\prime}\right)$. The standard approach, based on the composition theorem, argues as follows:

$$
\begin{align*}
T\left(X_{t}, X_{t}^{\prime}\right) & \geq T\left(X_{t-1}, X_{t-1}^{\prime}\right) \otimes G(c) \\
& \geq T\left(X_{t-2}, X_{t-2}^{\prime}\right) \otimes G(c \sqrt{2}) \\
& \cdots \\
& \geq \underbrace{T\left(X_{0}, X_{0}^{\prime}\right)}_{=\text {Id since } X_{0}=X_{0}^{\prime}} \otimes G(c \sqrt{t}) \tag{3.3}
\end{align*}
$$

Here, the composition theorem simultaneously "unrolls" both processes, at some price $G(c)$ in each iteration. (These prices are collected via a basic GDP identity, Lemma A.2.) This is due to the following simple lemma, which relies on the $f$-DP of the Gaussian mechanism using different updates $\phi, \phi^{\prime}$ [DRS22, Theorem 2].

Lemma 3.1. Suppose $\left\|\phi-\phi^{\prime}\right\|_{\infty} \leq s$. Then

$$
T\left(\phi(X)+\mathcal{N}\left(0, \sigma^{2} I_{d}\right), \phi^{\prime}\left(X^{\prime}\right)+\mathcal{N}\left(0, \sigma^{2} I_{d}\right)\right) \geq T\left(X, X^{\prime}\right) \otimes G\left(\frac{s}{\sigma}\right)
$$

Bounding $s$ via sensitivity enables the argument (3.3) and gives the appropriate $c$. See [DRS22] for details. However, while this argument (3.3) is reasonably tight for small $t$, it is vacuous as $t \rightarrow \infty$. Conceptually, this is because this analysis considers releasing all intermediate iterates, hence it bounds $T\left(\left(X_{1}, \ldots, X_{t}\right),\left(X_{1}^{\prime}, \ldots X_{t}^{\prime}\right)\right) \geq G(c \sqrt{t})$. Concretely, this is because the above analysis requires completely unrolling to iteration 0 . Indeed, the identical initialization $X_{0}=X_{0}^{\prime}$ ensures $T\left(X_{0}, X_{0}^{\prime}\right)=\mathrm{Id}$, whereas at any other iteration $k>0$ it is unclear how to directly bound $T\left(X_{k}, X_{k}^{\prime}\right)$ as $X_{k} \neq X_{k}^{\prime}$. This inevitably leads to final privacy bounds which diverge in $t$ since a penalty is incurred in each of the $t$ iterations.

### 3.2 Convergent $f$-DP bounds, via shifted interpolation

The central idea underlying our analysis is the construction of a certain auxiliary process $\left\{\widetilde{X}_{k}\right\}$ that interpolates between the two processes in the sense that $\widetilde{X}_{\tau}=X_{\tau}^{\prime}$ at some intermediate time $\tau$ and $\widetilde{X}_{t}=X_{t}$ at the final time. See Figure 3. Crucially, this enables running the argument (3.4) where we unroll only from $t$ to $\tau$, rather than all the way to initialization:

$$
\begin{align*}
T\left(X_{t}, X_{t}^{\prime}\right) & =T\left(\widetilde{X}_{t}, X_{t}^{\prime}\right) \\
& \geq T\left(\widetilde{X}_{t-1}, X_{t-1}^{\prime}\right) \otimes G\left(a_{t}\right) \\
& \geq T\left(\widetilde{X}_{t-2}, X_{t-2}^{\prime}\right) \otimes G\left(\left(a_{t}^{2}+a_{t-1}^{2}\right)^{1 / 2}\right) \\
& \cdots \\
& \geq \underbrace{T\left(\widetilde{X}_{\tau}, X_{\tau}^{\prime}\right)}_{=\text {Id since } \tilde{X}_{\tau}=X_{\tau}^{\prime}} \otimes G\left(\left(\sum_{k=\tau+1}^{t} a_{k}^{2}\right)^{1 / 2}\right) \tag{3.4}
\end{align*}
$$

![](https://cdn.mathpix.com/cropped/2024_05_26_6dd009bc4ef3ccde1c7fg-09.jpg?height=350&width=833&top_left_y=237&top_left_x=643)

Figure 3: Illustration of the shifted interpolated process $\left\{\widetilde{X}_{k}\right\}$ defined in (3.5). It starts from one process $\left(\widetilde{X}_{\tau}=X_{\tau}^{\prime}\right)$ and ends at the other $\left(\widetilde{X}_{t}=X_{t}\right)$. The intermediate time $\tau$ is an analysis parameter that we optimize to get the best final privacy bound.

Intuitively, this argument replaces the divergent $\sqrt{t}$ dependence of prior $f$-DP bounds with something scaling in $t-\tau$. Here $\tau$ is an analysis parameter that we can optimize based on the following intuitive tradeoff: larger $\tau$ enables unrolling less, whereas smaller $\tau$ gives the auxiliary process $\left\{\widetilde{X}_{k}\right\}$ more time to interpolate between $X_{\tau}^{\prime}$ and $X_{t}$ while yielding smaller penalties $a_{k}$ for unrolling at each iteration.

Formalizing (3.4) leads to two interconnected questions:

- Q1. How to construct the auxiliary process $\left\{\widetilde{X}_{k}\right\}$ ?
- Q2. How to unroll each iteration? I.e., what is the analog of Lemma 3.1?


### 3.2.1 Shifted interpolated process

For Q1, we initialize $\widetilde{X}_{\tau}=X_{\tau}^{\prime}$ and define

$$
\begin{equation*}
\tilde{X}_{k+1}=\lambda_{k+1} \phi\left(X_{k}\right)+\left(1-\lambda_{k+1}\right) \phi^{\prime}\left(\tilde{X}_{k}\right)+Z_{k+1} \tag{3.5}
\end{equation*}
$$

for $k=\tau, \ldots, t-1$. Intuitively, this auxiliary process $\left\{\widetilde{X}_{k}\right\}$ uses a convex combination of the updates performed by the two processes $\left\{X_{k}\right\}$ and $\left\{X_{k}^{\prime}\right\}$, enabling it to gracefully interpolate from its initialization at one process to its termination at the other. Here $\lambda_{k}$ controls the speed at which we shift from one process to the other. We set $\lambda_{t}=1$ so that $\widetilde{X}_{t}=X_{t}$ achieves the desired interpolation; the other $\left\{\lambda_{k}\right\}$ are analysis parameters that we optimize to get the best final bound. An important technical remark is that this auxiliary process uses the same noise increments $\left\{Z_{k}\right\}$ as $\left\{X_{k}\right\}$; this coupling enables bounding the distance between $X_{k}$ and $\widetilde{X}_{k}$ by a deterministic value (i.e., in the $\infty$-Wasserstein distance $W_{\infty}$ ).

We remark that auxiliary interpolating processes have been used in the context of proving Harnack inequalities (or equivalently, Rényi reverse transport inequalities) for diffusions on manifolds [ATW06, Wan13, Wan14, AC23b, AC23c]. Two key challenges posed by the present setting are that $f$-DP requires tradeoff functions (rather than Rényi divergences), and also tracking stochastic processes that undergo different dynamics (rather than the same diffusion). This requires constructing and analyzing the auxiliary process (3.5).

### 3.2.2 Geometrically aware composition

For Q2, we develop the following lemma, which generalizes Lemma 3.1 by allowing for an auxiliary process $\widetilde{X}$ and a shift parameter $\lambda$ (Lemma 3.1 is recovered in the special case $\lambda=1$ and $\widetilde{X}=X$ ). A key feature is that unlike Lemma 3.1, this lemma is geometrically aware in that it exploits the Lipschitzness of the gradient descent updates $\phi, \phi^{\prime}$-recall from Lemma 2.8 that $\phi, \phi^{\prime}$ are (strongly) contractive whenever the losses are (strongly) convex. Intuitively, this contractivity ensures that long-ago gradient queries incur (exponentially) less privacy loss, thus making the total privacy loss convergent; c.f., the discussion in $\S 1.2$.

Lemma 3.2. Suppose that $\left\|\phi-\phi^{\prime}\right\|_{\infty} \leq s$ and that $\phi, \phi^{\prime}$ are $c$-Lipschitz. Then for any $\lambda \geq 0$ and any random variable $\widetilde{X}$ satisfying $\|X-\widetilde{X}\| \leq z$,

$$
T\left(\lambda \phi(X)+(1-\lambda) \phi^{\prime}(\widetilde{X})+\mathcal{N}\left(0, \sigma^{2} I_{d}\right), \phi^{\prime}\left(X^{\prime}\right)+\mathcal{N}\left(0, \sigma^{2} I_{d}\right)\right) \geq T\left(\tilde{X}, X^{\prime}\right) \otimes G\left(\frac{\lambda(c z+s)}{\sigma}\right)
$$

### 3.2.3 Convergent $f$-DP bounds

Combining our answers to Q1 (shifted interpolated process) and Q2 (geometrically aware composition) enables formalizing the argument (3.4). The remaining proof details are straightforward and deferred to §C.2. For clarity, we state this result as a "meta-theorem" where the shifts $\lambda_{k}$ and intermediate time $\tau$ are parameters; our final bounds are obtained by optimizing them, see $\S 4$.

Theorem 3.3. Consider the stochastic processes $\left\{X_{k}\right\},\left\{X_{k}^{\prime}\right\},\left\{\tilde{X}_{k}\right\}$ defined in (3.1), (3.2), (3.5), with $\lambda_{t}=1$. Suppose that $\left\|\phi-\phi^{\prime}\right\|_{\infty} \leq s$ and that $\phi, \phi^{\prime}$ are $c$-Lipschitz. For any sequence $\left\{z_{k}\right\}$ such that $\left\|X_{k}-\widetilde{X}_{k}\right\| \leq z_{k}$,

$$
T\left(X_{t}, X_{t}^{\prime}\right) \geq G\left(\frac{1}{\sigma} \sqrt{\sum_{k=\tau+1}^{t} a_{k}^{2}}\right)
$$

where $a_{k}=\lambda_{k}\left(c z_{k-1}+s\right)$.

We emphasize that although this technique-overview section focused on the simple case of full-batch gradients and strongly convex losses for clarity, these techniques readily extend to more general settings. Briefly, for constrained optimization, projections are handled by using the post-processing inequality for tradeoff functions; for (non-strongly) convex optimization, the optimal shifts $a_{k}$ will be of similar size rather than geometrically increasing (and $\tau$ will be strictly positive rather than zero); for cyclic batches, the update functions $\phi_{k}, \phi_{k}^{\prime}$ and corresponding sensitivity $s_{k}$ are time-varying; and for stochastic batches, the analog of Lemma 3.2 incorporates the celebrated privacy amplification by subsampling phenomenon. These different settings lead to different values for the sensitivity $s$ and contractivity $c$, which in turn lead to different choices of the parameters $\lambda_{k}$ and $\tau$; however, we emphasize that the analysis approach is the same, and the main difference between the settings is just the elementary optimization problem over these analysis parameters, which we find the (optimal) solutions. Details in $\S 4$.

## 4 Improved privacy for noisy optimization algorithms

Here we apply the shifted interpolation technique developed in $\S 3$ to establish improved privacy bounds for noisy gradient descent and its variants. We showcase the versatility of our techniques by investigating gradient descent with full-batch gradients in $\$ 4.1$, cyclic batches in $\$ 4.2$, and stochastic batches in $\$ 4.3$. In all cases, we show convergent $f$-DP bounds for unconstrained strongly convex and constrained convex settings; the constrained strongly convex setting is similar and omitted for brevity (and the unconstrained convex setting has divergent privacy). The proofs are similar for all these different settings, based on the approach in $\S 3$; for brevity the proofs are deferred to $\S$ C. See also $\S$ D for numerical illustrations of the improvements of our bounds.

Below, recall from $\S 2$ that we denote the learning rate by $\eta$, the noise rate by $\sigma$, the number of data points by $n$, the batch size by $b$, the constraint set by $\mathcal{K}$, and its diameter by $D$. Throughout we denote by $c=\max \{|1-\eta m|,|1-\eta M|\}$ the Lipschitz constant for a step of gradient descent on $m$-strongly convex and $M$-smooth losses (c.f., Lemma 2.8 ).

### 4.1 Noisy gradient descent

Here we consider full-batch gradient descent. For comparison, we first recall the standard $f$-DP bound implied by the composition theorem [DRS22].

Theorem 4.1. Consider loss functions with gradient sensitivity L. Then NoisyGD is $\mu$-GDP where

$$
\mu=\frac{L}{n \sigma} \sqrt{t}
$$

This (divergent) bound is tight without further assumptions on the losses. Below we show convergent f-DP bounds for NoisyGD in the setting of strongly convex losses, and the setting of constrained convex losses.

Theorem 4.2. Consider m-strongly convex, M-smooth loss functions with gradient sensitivity L. Then for any $\eta \in(0,2 / M)$, NoisyGD is $\mu$-GDP where

$$
\mu=\sqrt{\frac{1-c^{t}}{1+c^{t}}} \frac{1+c}{1-c} \frac{L}{n \sigma}
$$

For $\eta \in(0,2 /(M+m)]$, this bound is optimal.

Theorem 4.3. Consider convex, $M$-smooth loss functions with gradient sensitivity $L$ and constraint set $\mathcal{K}$ of diameter $D$. Then for any $\eta \in[0,2 / M]$ and $t \geq \frac{D n}{\eta L}$, NoisyGD is $\mu$-GDP where

$$
\mu=\frac{1}{\sigma} \sqrt{\frac{3 L D}{\eta n}+\frac{L^{2}}{n^{2}}\left\lceil\frac{D n}{\eta L}\right\rceil}
$$

Theorem 4.2 is exactly tight in all parameters, and improves over the composition-based analysis (Theorem 4.1) for all $t>1$. See Figure 1. Theorem 4.3 is tight up to a constant factor [AT22], and for $t>\frac{4 D n}{\eta L}$ it dominates Theorem 4.1 since its convergent nature outweighs the slightly suboptimal constant.

### 4.2 Noisy cyclic gradient descent

We now turn to cyclic batches. For simplicity, suppose that the number of batches per epoch $l=n / b$ and the number of epochs $E=t / l$ are integers. We state our results with respect to $E$ rather than $t$. For comparison, we first state the standard (divergent) $f$-DP bound implied by the composition theorem [DRS22].

Theorem 4.4. Consider loss functions with gradient sensitivity L. Then NoisyCGD is $\mu$-GDP where

$$
\mu=\frac{L}{b \sigma} \sqrt{E}
$$

Below we show convergent $f$-DP bounds for NoisyCGD in the setting of strongly convex losses, and the setting of constrained convex losses.

Theorem 4.5. Consider m-strongly convex, $M$-smooth loss functions with gradient sensitivity L. Then for any $\eta \in(0,2 / M)$, NoisyCGD is $\mu$-GDP where

$$
\mu=\frac{L}{b \sigma} \sqrt{1+c^{2 l-2} \frac{1-c^{2}}{\left(1-c^{l}\right)^{2}} \frac{1-c^{l(E-1)}}{1+c^{l(E-1)}}}
$$

Theorem 4.6. Consider convex, $M$-smooth loss functions with gradient sensitivity $L$ and constraint set $\mathcal{K}$ of diameter $D$. Then for any $\eta \in[0,2 / M]$ and $E \geq \frac{D b}{\eta L}$, NoisyCGD is $\mu$-GDP where

$$
\mu=\frac{1}{\sigma} \sqrt{\left(\frac{L}{b}\right)^{2}+\frac{3 L D}{\eta b l}+\frac{L^{2}}{b^{2} l}\left\lceil\frac{D b}{\eta L}\right\rceil}
$$

The convergent nature of these bounds ensures that they dominate Theorem 4.4 when NoisyCGD is run long enough. This threshold is roughly $E \approx c^{2 l-2} \frac{1-c^{2}}{\left(1-c^{l}\right)^{2}}$ for Theorem 4.5 and $E \approx \frac{4 D b}{\eta \ell L}$ for Theorem 4.6.

### 4.3 Noisy stochastic gradient descent

Compared to NoisyGD, the privacy leakage in NoisySGD only occurs when the index $i^{*}$ is in the sampled batch. This phenomenon is known as privacy amplification by subsampling $\left[\mathrm{KLN}^{+} 11\right]$, which is formulated in $f$-DP as follows $[$ DRS22, Definition 6$]$.

Definition 4.7 (Subsampling). For tradeoff function $f$ and $p \in[0,1]$, define $f_{p}=p f+(1-p) I d$. The subsampling operator $C_{p}$ (with respect to $f$ ) is defined as $C_{p}(f)=\min \left\{f_{p},\left(f_{p}\right)^{-1}\right\}^{* *}$ where ${ }^{-1}$ denotes the (left-continuous) inverse and ${ }^{*}$ denotes the convex conjugate. Equivalently, $C_{p}(f)$ is the pointwise largest tradeoff function $g$ such that $g \leq f_{p}$ and $g \leq\left(f_{p}\right)^{-1}$.

For comparison, we first recall the standard $f$-DP bound based on composition [DRS22, Theorem 9].

Theorem 4.8. Consider loss functions with gradient sensitivity L. Then NoisySGD is $f$-DP where

$$
f=C_{b / n}\left(G\left(\frac{L}{b \sigma}\right)\right)^{\otimes t}
$$

This (divergent) bound is tight for $t=1$ without further assumptions on the losses. Below we show convergent $f$-DP bounds for NoisySGD in the setting of strongly convex losses, and the setting of constrained convex losses.

Theorem 4.9. Consider m-strongly convex, $M$-smooth loss functions with gradient sensitivity L. Then for any $\eta \in(0,2 / M)$, NoisySGD is $f$-DP for

$$
f=G\left(\frac{2 \sqrt{2} L}{b \sigma} \frac{c^{t-\tau+1}-c^{t}}{1-c}\right) \otimes C_{b / n}\left(G\left(\frac{2 \sqrt{2} L}{b \sigma}\right)\right) \otimes C_{b / n}\left(G\left(\frac{2 L}{b \sigma}\right)\right)^{\otimes(t-\tau)}
$$

for any $\tau=0,1, \ldots, t-1$.

Theorem 4.10. Consider convex, $M$-smooth loss functions with gradient sensitivity $L$ and constraint set $\mathcal{K}$ of diameter $D$. Then for any $\eta \in[0,2 / M]$, NoisySGD is $f$-DP where

$$
f=G\left(\frac{\sqrt{2} D}{\eta \sigma \sqrt{t-\tau}}\right) \otimes C_{b / n}\left(G\left(\frac{2 \sqrt{2} L}{b \sigma}\right)\right)^{\otimes(t-\tau)}
$$

for any $\tau=0,1, \ldots, t-1$.

Both theorems give convergent privacy by taking $t-\tau$ constant as $t \rightarrow \infty$. In contrast, Theorem 4.8 is convergent in the regime $t=O\left(n^{2} / b^{2}\right)$ (by CLT), but yields a vacuous privacy as $t \rightarrow \infty$ for fixed $b / n$ [DRS22]. We remark that for finite but large $t$, one can set $t-\tau$ to be sufficiently large and apply CLT (Lemma A.5) to approximate the composition of $C_{p}(G(\cdot))$; see Lemma A.11. We also remark that by choosing $t-\tau=\Theta\left(\frac{D n}{\eta L}\right)$, Theorem 4.10 recovers the asymptotically tight Rényi DP bound of [AT22].

### 4.4 Numerical example

As a proof of concept, here we consider regularized logistic regression on MNIST [LCB10]. We compare our results with the state-of-the-art Rényi DP bounds, and existing $f$-DP bounds (based on the composition theorem) which we denote as GDP Composition. For a fair comparison, we use the same algorithm NoisyCGD, with all parameters unchanged, and only focus on the privacy accounting; we focus on NoisyCGD because it is close to standard private optimization implementations, e.g., Pytorch $\left[\mathrm{PGM}^{+} 19\right]$ and TensorFlow $\left[\mathrm{ABC}^{+} 16\right]$. Indeed, standard implementations often cycle through batches in a permuted order every epoch, somewhat similar to how NoisyCGD cycles through batches in a fixed order.

Table 1 demonstrates that for this problem, our privacy guarantees are tighter, enabling longer training for the same privacy budget-which helps both training and testing accuracy (c.f., Table 2). For full details of the experiment, see $\S$ D. 2 .

Table 1: Privacy $\varepsilon$ of NoisyCGD on regularized logistic regression for $\delta=10^{-5}$ in $(\varepsilon, \delta)$-DP. Our results provide better privacy than both GDP Composition and RDP bounds in all cases.

| Epochs | GDP Composition | RDP | Our Bounds |
| :---: | :---: | :---: | :---: |
| 50 | 30.51 | 5.82 | 4.34 |
| 100 | 49.88 | 7.61 | 5.60 |
| 200 | 83.83 | 9.88 | 7.58 |

Table 2: Training and test accuracy (\%) of NoisyCGD for regularized logistic regression, averaged over 10 runs. Both the training and test accuracy improve as the number of epochs increases.

| Epochs | Training | Test |
| :---: | :---: | :---: |
| 50 | $89.36 \pm 0.03$ | $90.12 \pm 0.04$ |
| 100 | $90.24 \pm 0.03$ | $90.94 \pm 0.07$ |
| 200 | $90.85 \pm 0.02$ | $91.37 \pm 0.08$ |

## $5 f$-DP of the exponential mechanism

Since we show convergent $f$-DP bounds for randomized algorithms in $\S 4$, we can take the limit $t \rightarrow \infty$ to obtain $f$-DP bounds for their stationary distributions. We focus here on NoisyGD because, up to a simple rescaling, it is equivalent to Langevin Monte Carlo (LMC), one of the most well-studied sampling algorithms in the statistics literature; see, e.g., [RC99, Liu01, ADFDJ03]. Our results for (strongly) convex losses not only imply new results for (strongly) log-concave sampling for LMC, but also imply $f$-DP bounds for the exponential mechanism [MT07]-a foundational concept in DP-since it is obtained from LMC's stationary distribution in the limit as the stepsize $\eta \rightarrow 0$.

### 5.1 Strongly log-concave targets

Our optimal $f$-DP bounds for NoisyGD immediately imply optimal ${ }^{4} f$-DP bounds for LMC.

Proposition 5.1. Suppose that $F, F^{\prime}$ are $m$-strongly convex and $M$-smooth, and that $F-F^{\prime}$ is L-Lipschitz. Consider the LMC updates

$$
\begin{aligned}
X_{k+1} & =X_{k}-\eta \nabla F\left(X_{k}\right)+Z_{k+1} \\
X_{k+1}^{\prime} & =X_{k}^{\prime}-\eta \nabla F^{\prime}\left(X_{k}^{\prime}\right)+Z_{k+1}^{\prime}
\end{aligned}
$$

where $X_{0}=X_{0}^{\prime}$ and $Z_{k+1}, Z_{k+1}^{\prime} \sim \mathcal{N}\left(0,2 \eta I_{d}\right)$. Then for any $\eta \in(0,2 /(M+m)]$,

$$
T\left(X_{t}, X_{t}^{\prime}\right) \geq G\left(\sqrt{\frac{2-\eta m}{2}} \frac{L}{\sqrt{m}}\right)
$$

Proof. LMC is a special case of NoisyGD with $n=1, f_{1}=F, f_{1}^{\prime}=F^{\prime}$, and $\sigma=\sqrt{2 / \eta}$. Apply Theorem 4.2.

Taking $t \rightarrow \infty$ gives $f$-DP guarantees for the stationary distributions $\pi(\eta)$ and $\pi^{\prime}(\eta)$ of these LMC chains. We also obtain $f$-DP guarantees between the exponential mechanisms $\pi \propto e^{-F}$ and $\pi^{\prime} \propto e^{-F^{\prime}}$ for $F$ and $F^{\prime}$.[^3]

Corollary 5.2. In the setting of Proposition 5.1,

$$
T\left(\pi(\eta), \pi^{\prime}(\eta)\right) \geq G\left(\sqrt{\frac{2-\eta m}{2}} \frac{L}{\sqrt{m}}\right)
$$

and

$$
T\left(\pi, \pi^{\prime}\right) \geq G\left(\frac{L}{\sqrt{m}}\right)
$$

Proof. It is well-known that under these assumptions, LMC converges to its stationary distribution in total variation (TV) distance as $t \rightarrow \infty$, and the stationary distribution converges to the exponential mechanism as $\eta \rightarrow 0$, see e.g., [Che23]. By Lemma A.10, tradeoff functions converge under TV.

Thus, we recover the recent result [GLL22, Theorem 4] which characterizes the $f$-DP of the exponential mechanism. The proof in [GLL22] is entirely different, based on the Gaussian isoperimetry inequality [Led99] rather than connecting LMC to the exponential mechanism. Our results can be viewed as algorithmic generalizations of theirs in the sense that we also obtain tight $f$-DP bounds on the iterates of LMC and its stationary distribution.

Remark 5.3 (Tightness). As noted in [GLL22], the exponential mechanism bound in Corollary 5.2 is tight by considering $F(x)=\frac{m}{2}\|x\|^{2}, F^{\prime}(x)=\frac{m}{2}\left\|x-\frac{L}{m} v\right\|^{2}$ (where $v$ is a unit vector) which yields $\pi=\mathcal{N}\left(0, \frac{1}{m} I_{d}\right), \pi^{\prime}=$ $\mathcal{N}\left(\frac{L}{m} v, \frac{1}{m} I_{d}\right)$. With the same loss functions, it is straightforward to check that this construction also shows optimality for our results on the $f-D P$ of LMC and its stationary distribution; in particular, for the latter we have $\pi(\eta)=\mathcal{N}\left(0, \frac{2}{(2-\eta m) m} I_{d}\right)$ and $\pi^{\prime}(\eta)=\mathcal{N}\left(\frac{L}{m} v, \frac{2}{(2-\eta m) m} I_{d}\right)$ which yields $T\left(\pi(\eta), \pi^{\prime}(\eta)\right)=G\left(\sqrt{\frac{2-\eta m}{2}} \frac{L}{\sqrt{m}}\right)$.

### 5.2 Log-concave targets

A similar story holds in the setting of convex losses, although this requires a constrained setting since otherwise stationary distributions may not exist. Hence we consider projected NoisyGD (Theorem 4.3), which corresponds to projected LMC. As above, this leads to $f$-DP bounds for the exponential mechanism due to known TV convergence results, for projected LMC to its stationary distribution as $t \rightarrow \infty$ [AT23], and from that distribution to the exponential mechanism as $\eta \rightarrow 0$ [BEL18].

Corollary 5.4. Let $F, F^{\prime}$ be convex, $M$-smooth and L-Lipschitz functions and $\mathcal{K}$ be a convex body with diameter $D$ containing a unit ball. Then for $\pi \propto e^{-F} \mathbf{1}_{\mathcal{K}}$ and $\pi^{\prime} \propto e^{-F^{\prime}} \mathbf{1}_{\mathcal{K}}$,

$$
T\left(\pi, \pi^{\prime}\right) \geq G(2 \sqrt{L D})
$$

Furthermore, for $\eta \in(0,2 / M]$, the respective stationary distributions $\pi(\eta), \pi^{\prime}(\eta)$ of the projected LMC satisfy $T\left(\pi(\eta), \pi^{\prime}(\eta)\right) \geq G\left(\sqrt{4 L D+2 \eta L^{2}}\right)$.

Unlike the strongly convex case [MASN16, GLL22], we are unaware of any results in this setting beyond the standard analysis [MT07] on the exponential mechanism. That yields $(2 L D, 0)$-DP, and our result provides nontrivial improvement in privacy when $L D>0.677$; see $\S$ D. 4 .

## 6 Discussion

The techniques and results of this paper suggest several directions for future work.

One natural direction is whether convergent $f$-DP bounds can be shown in more general settings, e.g., (structured) non-convex landscapes, heteroscedastic or correlated noise $\left[\mathrm{CCDP}^{+} 23\right]$, adaptive first-order algorithms, or second-order algorithms [GHST23].

A technical question is whether one can relax the $W_{\infty}$ bounds between our shifted interpolated process $\left\{\widetilde{X}_{k}\right\}$ and the target process $\left\{X_{k}\right\}$, and if this can enable tighter analyses of stochastic algorithms. While $W_{\infty}$
has traditionally been used for privacy amplification by iteration [FMTT18], [AC23a] recently showed that some of this analysis extends to the Orlicz-Wasserstein distance, and is even necessary in some applications.

Another natural direction is more computationally tractable $f$-DP bounds. Although the $f$-DP framework provides an information-theoretically lossless quantification of DP, it is often computationally burdensome, e.g., for NoisySGD bounds expressed as the composition of many tradeoff functions. Recent work has developed useful tools for approximation [ZDLS20, GLW21, ZDW22], and further developments would help practitioners who need to adhere to given privacy budgets.

Acknowledgements. We thank Sinho Chewi, Kunal Talwar, and Jiayuan Ye for insightful conversations about the literature. We also thank Jiayuan Ye for sharing helpful code for numerical experiments.

## References

\$\left[\mathrm{ABC}^{+}\right.\$16] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: a system for largescale machine learning. In Symposium on Operating Systems Design and Implementation, pages $265-283,2016$.

[ABT24] Jason M. Altschuler, Jinho Bok, and Kunal Talwar. On the privacy of Noisy Stochastic Gradient Descent for convex optimization. SIAM Journal on Computing, 2024.

[AC23a] Jason M. Altschuler and Sinho Chewi. Faster high-accuracy log-concave sampling via algorithmic warm starts. In Symposium on Foundations of Computer Science (FOCS), pages 2169-2176, 2023 .

[AC23b] Jason M. Altschuler and Sinho Chewi. Shifted composition I: Harnack and reverse transport inequalities. arXiv preprint arXiv:2311.14520, 2023.

[AC23c] Jason M. Altschuler and Sinho Chewi. Shifted composition II: Shift Harnack inequalities and curvature upper bounds. arXiv preprint arXiv:2401.00071, 2023.

\$\left[\mathrm{ACG}^{+}\right.\$16] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Conference on Computer and Communications Security, page 308-318, 2016.

[AD22] Jordan Awan and Jinshuo Dong. Log-concave and multivariate canonical noise distributions for differential privacy. In Advances in Neural Information Processing Systems, volume 35, pages $34229-34240,2022$.

[ADFDJ03] Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I. Jordan. An introduction to MCMC for machine learning. Machine Learning, 50:5-43, 2003.

$\left[\mathrm{ALC}^{+} 21\right]$ Shahab Asoodeh, Jiachun Liao, Flavio P. Calmon, Oliver Kosut, and Lalitha Sankar. Three variants of differential privacy: Lossless conversion and applications. IEEE Journal on Selected Areas in Information Theory, 2(1):208-222, 2021.

[AT22] Jason M. Altschuler and Kunal Talwar. Privacy of noisy stochastic gradient descent: More iterations without more privacy loss. In Advances in Neural Information Processing Systems, volume 35, pages 3788-3800, 2022.

[AT23] Jason M. Altschuler and Kunal Talwar. Resolving the mixing time of the Langevin algorithm to its stationary distribution for log-concave sampling. In Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 2509-2510, 2023.

[ATW06] Marc Arnaudon, Anton Thalmaier, and Feng-Yu Wang. Harnack inequality and heat kernel estimates on manifolds with curvature unbounded below. Bulletin des Sciences Mathématiques, $130(3): 223-233,2006$.

$\left[\mathrm{BBG}^{+} 20\right]$ Borja Balle, Gilles Barthe, Marco Gaboardi, Justin Hsu, and Tetsuya Sato. Hypothesis testing interpretations and Renyi differential privacy. In International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 2496-2506, 2020.

[BDLS20] Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie Su. Deep learning with Gaussian differential privacy. Harvard Data Science Review, 2(3), 2020.

[BEL18] Sébastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with projected Langevin Monte Carlo. Discrete $\&$ Computational Geometry, 59(4):757-783, 2018.

[BFTGT19] Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic convex optimization with optimal rates. In Advances in Neural Information Processing Systems, volume $32,2019$.

[BS16] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower bounds. In International Conference on Theory of Cryptography, page 635-658, 2016.

[BST14] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Symposium on Foundations of Computer Science (FOCS), page 464-473, 2014.

[BW18] Borja Balle and Yu-Xiang Wang. Improving the Gaussian mechanism for differential privacy: Analytical calibration and optimal denoising. In International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 394-403, 2018.

$\left[\mathrm{CCDP}^{+} 23\right]$ Christopher A. Choquette-Choo, Krishnamurthy Dvijotham, Krishna Pillutla, Arun Ganesh, Thomas Steinke, and Abhradeep Thakurta. Correlated noise provably beats independent noise for differentially private learning. arXiv preprint arXiv:2310.06771, 2023.

[Che23] Sinho Chewi. Log-concave sampling. 2023. Draft available at https://chewisinho.github. io/.

[CYS21] Rishav Chourasia, Jiayuan Ye, and Reza Shokri. Differential privacy dynamics of Langevin diffusion and noisy gradient descent. In Advances in Neural Information Processing Systems, volume 34, pages 14771-14781, 2021.

[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography, volume 3876 of Lecture Notes in Computuer Science, pages 265-284. Springer, Berlin, 2006.

[DR14] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends $®$ in Theoretical Computer Science, 9(3-4):211-407, 2014.

[DRS22] Jinshuo Dong, Aaron Roth, and Weijie J. Su. Gaussian differential privacy. Journal of the Royal Statistical Society, Series B, Statistical Methodology, 84(1):3-54, 2022. With discussions and a reply by the authors.

[FMTT18] Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta. Privacy amplification by iteration. In 59th Annual IEEE Symposium on Foundations of Computer Science-FOCS 2018, pages 521-532. IEEE Computer Soc., Los Alamitos, CA, 2018.

[GHST23] Arun Ganesh, Mahdi Haghifam, Thomas Steinke, and Abhradeep Thakurta. Faster differentially private convex optimization via second-order methods. arXiv preprint arXiv:2305.13209, 2023.

[GLL22] Sivakanth Gopi, Yin Tat Lee, and Daogao Liu. Private convex optimization via exponential mechanism. In Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 1948-1989, 2022.

[GLW21] Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy. In Advances in Neural Information Processing Systems, volume 34, pages 11631-11642, 2021.

$\left[\mathrm{KLN}^{+} 11\right]$ Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793-826, 2011.

[KOV17] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. IEEE Transactions on Information Theory, 63(6):4037-4049, 2017.

[LCB10] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs [Online]. Available: http://yann. lecun.com/exdb/mnist, 2, 2010.

[Led99] Michel Ledoux. Concentration of measure and logarithmic Sobolev inequalities. In Jacques Azéma, Michel Émery, Michel Ledoux, and Marc Yor, editors, Séminaire de Probabilités XXXIII, pages 120-216, Berlin, Heidelberg, 1999. Springer Berlin Heidelberg.

[Liu01] Jun S. Liu. Monte Carlo strategies in scientific computing, volume 75. Springer, 2001.

[MASN16] Kentaro Minami, Hitomi Arai, Issei Sato, and Hiroshi Nakagawa. Differential privacy without sensitivity. In Advances in Neural Information Processing Systems, volume 29, 2016.

[Mir17] Ilya Mironov. Rényi differential privacy. In Computer Security Foundations Symposium, pages $263-275,2017$.

[MT07] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In Foundations of Computer Science (FOCS), pages 94-103, 2007.

[PGM $\left.{ }^{+} 19\right]$ Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In Advances in Neural Information Processing Systems, volume $32,2019$.

[RC99] Christian P. Robert and George Casella. Monte Carlo statistical methods, volume 2. Springer, 1999.

[Wan13] Feng-Yu Wang. Harnack inequalities for stochastic partial differential equations, volume 1332. Springer, 2013.

[Wan14] Feng-Yu Wang. Analysis for diffusion processes on Riemannian manifolds, volume 18. World Scientific, 2014.

[WSY $\left.{ }^{+} 23\right]$ Chendi Wang, Buxin Su, Jiayuan Ye, Reza Shokri, and Weijie J. Su. Unified enhancement of privacy bounds for mixture mechanisms via $f$-differential privacy. In Advances in Neural Information Processing Systems, volume 36, 2023.

[WZ10] Larry Wasserman and Shuheng Zhou. A statistical framework for differential privacy. Journal of the American Statistical Association, 105(489):375-389, 2010.

[YS22] Jiayuan Ye and Reza Shokri. Differentially private learning needs hidden state (or much faster convergence). In Advances in Neural Information Processing Systems, volume 35, pages 703$715,2022$.

[ZDLS20] Qinqing Zheng, Jinshuo Dong, Qi Long, and Weijie Su. Sharp composition bounds for Gaussian differential privacy via Edgeworth expansion. In International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 11420-11435, 2020.

[ZDW22] Yuqing Zhu, Jinshuo Dong, and Yu-Xiang Wang. Optimal accounting of differential privacy via characteristic function. In International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 4782-4817, 2022.
