# Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions 

Pengfei Hong ${ }^{1}$, Navonil Majumdar ${ }^{1}$, Deepanway Ghosal ${ }^{1}$<br>Somak Aditya ${ }^{2}$, Rada Mihalcea ${ }^{3}$, Soujanya Poria ${ }^{1}$<br>${ }^{1}$ Singapore University of Technology and Design, ${ }^{2}$ IIT Kharagpur<br>${ }^{3}$ University of Michigan


#### Abstract

Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust problem solving skills and structured reasoning abilities in many areas, as defined by our ontology. We open source the datasets and source codes at: https://github.com/ declare-lab/llm_robustness.


## 1 Introduction

Logical reasoning in a structured and well-defined domain, such as mathematics and programming, becomes increasingly harder with the increasing presence of interspersed and diverse situations, events, and contexts formulated through natural language queries. Current state-of-the-art Large Language Models (LLM) have shown impressive performance on mathematical problems (Cobbe et al., 2021a) and reasonable performance on coding problems (Chen et al., 2021) expressed in natural language. However, these evaluations barely test the depth of LLMs' expertise, and thus we do not currently have clear insights into the LLM capabilities in these domains. For example, in mathemat- ics, GPT-4's performance monotonically decreases from GSM-8k (Cobbe et al., 2021a) (92\%; 5-shot CoT) on grade school mathematical problems demanding rigorous arithmetic and logical reasoning to solve; to MMLU-Math (87.5\%) (Hendrycks et al., 2020) on a collection of mathematical problems, ranging in difficulty from elementary to advanced levels; and to MATH (50.36\%) (Hendrycks et al., 2021) on challenging competition mathematics problems. Similar variance in LLM performance can also be observed for coding challenges (Chen et al., 2021) Such shallow evaluations are unfit for an objective measure of the finer LLM capabilities as (i) many LLMs like GPT-4 (OpenAI, 2023) are exposed to publicly available math and coding datasets during pre-training; and ii) many datasets focus on advanced branches of mathematics and problems without bolstering the fundamentals. Hence, before testing the LLMs' breadth of capabilities by delving into higher mathematics and evaluating competitive coding questions, we instead focus on depth through two fundamental questions:

1. How robust is the capability of LLMs in terms of reasoning and understanding of the problem-solving process?
2. Under which conditions or across which dimensions do LLMs show limitations in reasoning and understanding?

In this work, our goal is to provide an evaluation mechanism that provides clear insights into the robustness of the reasoning abilities of LLMs in the context of maths and coding. Following previous work towards probing language models ( $\mathrm{Wu}$ et al., 2023; Ribeiro et al., 2020), we evaluate the robustness of LLMs' understanding of interesting linguistic and logical structures and make inferences based on them.

Specifically, we design an adaptive dynamic eval-

![](https://cdn.mathpix.com/cropped/2024_06_04_6b6b3f2ca27b98ecb68fg-02.jpg?height=625&width=1379&top_left_y=227&top_left_x=338)

Figure 1: A semi-automated pipeline of creating MORE, from five simple questions from GSM8k. An analogous pipeline is used to create the perturbations of the coding questions from HumanEval, named CorE.

uation benchmark through novel ontology-guided perturbations on existing problems. We introduce a novel ontology of perturbation operations that lists various changes across a diverse set of factors, which we apply to previously introduced arithmetic and coding problems. These perturbations allow us to assess whether the model comprehends underlying concepts. For instance, while a model may correctly answer questions in a dataset like GSM8k, it might struggle when presented with a simple perturbation to the question, such as replacing numerical values in maths questions with variables, which challenges the model to establish relationships among the variables, revealing its deeper understanding (or lack thereof). By introducing these ontological perturbations, (1) we gain insights into the models' reasoning abilities and (2) uncover strategies for future data augmentation that can then be utilized to enhance LLMs through weakly supervised fine-tuning methodologies.

Our ontology consists of 44 types of perturbations, which we apply to sample questions from GSM8K and coding questions from HumanEval, resulting in 216 and 219 perturbed questions respectively. Our evaluation of GPT-4, GPT-3.5, Metamath, Llama-code, Llama2-chat, and Gemini shows that most of these models very quickly degrade under different perturbation types. Our contributions are as follows:

1. We propose a novel, extensive, and extensible ontology of perturbation operations for basic-math- and coding-based reasoning tasks expressed in natural language.
2. We present a semi-automatic method to exercise such perturbations first through GPT-4, followed by manual filtering. We generate two datasets More and Core-Mathematicsand Code-Oriented Robustness Evaluation, respectively-consisting of 216 maths and 219 coding questions.
3. We gain insights into the range of capabilities and limitations on such math and coding tasks for several LLMs.

## 2 The Ontology of Perturbations

### 2.1 The Need for Ontology-based Perturbations

We plan to first identify a set of factors upon which the solution of a structured reasoning problem (expressed in natural language) may depend on (similar to Kaushik et al. (2021)); and perturb a seed question under these set of factors semiautomatically in a model-agnostic way (i.e., not necessarily adversarial to a target model). In the NLI context, Kaushik et al. (2021) utilized human workers to directly perturb a hypothesis, keeping the premise constant; and in a post-hoc way, identifies the categories (or factors) which such revisions pertain to. WizardLM (Xu et al., 2023) discusses a way of perturbation, by identifying a set of factors which is specifically designed to increase the complexity of a seed questions in limited ways. The categories are broad and do not exploit the logical nature of the underlying domain (along with the linguistic dimensions of the instruction). This is where, we believed, an ontological approach may help, where broader categories can help us generalize, while fine-grained sub-categories exploit the domain-specific characteristics.

Let's take mathematics for example. The so-
lution to a reasoning problem can depend on the number and complexity of operations, variables, functions, and possible existing theorems (external knowledge). Similarly, code generation problems can depend on the data structures, variables, functions, and libraries it needs access to. On top of this well-defined set of factors existing in structured reasoning problems, the list of factors expands as the problem is expressed in natural language. Entities and relations expressed in the text need to be mapped to variables and constants (in both). Physical actions (giving and taking apples) may need to be mapped to mathematical operations (or code). It is clear that the set of logical and linguistic factors co-exist in these reasoning problems. Therefore we come up with an extensible ontology, capturing the above nuances. We believe it will capture and categorize the factors where LLMs fail over multiple domains. As others have shown, the same process can be enabled to perform data augmentations.

### 2.2 The Ontology

Extending WizardLM (Xu et al., 2023)-like perturbations, we propose a set of high-level categories that are applicable to a broad class of reasoning tasks, expressed in natural language. We primarily identified the following hierarchy (see Table 1):

Level I: Aspect. There are two aspects to these perturbations: (i) structural perturbation and (ii) representational perturbation. Structural perturbation covers all perturbations that probe the underlying reasoning path (or structure) in different ways, by slightly varying the logic behind the question or probing intermediate steps, seeking explanations. Representational perturbations involves modification of the encoding of the input while mostly preserving semantics and the original solution.

Level II: Domain. The scope of each aspect is gradually refined into multiple domains. For example, the domain of logic alteration, under structural perturbations, deals with perturbations that alter the reasoning path in different ways.

Level III: Dimension. This is a further refinement that defines the exact target (the WHAT) in the reasoning process (question, reasoning, computation etc.) to which the perturbations are applied.

Level IV: Category. This level captures the method (the HOW) through which the higher-level Dimension perturbation is achieved. These methods are domain dependent and, thus, their implementations vary from maths to coding problems.

## 3 Curation of More and Core

Our objective is to assess the resilience of LLMs to perturbations of maths and coding questions along various dimensions. Thus, as seed datasets, we use GSM8K (Cobbe et al., 2021b)—a collection of mathematical problems demanding rigorous arithmetic and logical reasoning-and HumanEval (Chen et al., 2021) for coding. Five questions ${ }^{1}$ from GSM8K are perturbed using our ontological framework (see Appendix C) to generate More. On the other hand, we sampled five coding problems from HumanEval dataset (Chen et al., 2021) that were perturbed using the ontology explained in Appendix C. These perturbations are aimed at modifying the problems in terms of complexity and representation to assess the robustness of the LLMs to these ontological categories of perturbations. Fig. 2 shows examples of three perturbed questions and answers from MORE and CORE. Examples and definitions of all the remaining perturbations are present in Appendix C. We use a three-staged combination of automatic generation from GPT-4 (OpenAI, 2023) with human verification and annotation to create MORE and CORE: (i) perturbed question generation (§3.1), (ii) filtering and validation of generated questions (§3.2), and (iii) annotating final answers (§3.3).

### 3.1 Perturbed Question Generation

In the first stage, our objective is to create perturbed questions from the source GSM8K/HumanEval questions for each perturbation type. We write prompt templates for each perturbation type and fill them with a source question to create the input prompt to GPT-4. Each template captures the essence of the respective perturbation type (Appendix C.2, Appendix C.3, Appendix C.4) to instruct GPT-4 on how to perturb the source question.

For example, the prompt for Remove Constraint (G1.) for our running example is as follows:[^0]^[
${ }^{1}$ Maths questions in the GSM8K dataset take between two and eight steps to solve. We randomly chose five questions that take three to seven steps to solve. We cover various topics involving algebraic questions, physical application questions, and decision-based application questions
]

| Aspect (Level I) | Domain (Level II) | Dimension (Level III) | Category (Level IV) | Math | Code |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Structural <br> Perturbation <br> Def: Modification on <br> specific aspects of logic <br> or concepts to alter the <br> reasoning process <br> required to reach the <br> answer | Logic Alteration <br> Def: Modifications to <br> the reasoning framework <br> or logic underpinning a <br> problem. | Question Simplification <br> Def: Yields easier question <br> than the original | G1. Remove Constraint <br> G2. Partial Solution <br> G3. Solution Plan <br> G4. Detail Expansion | Remove Constraint <br> Median Inquiry <br> Solution Plan <br> Detail Elaboration | Remove Constraint <br> Helper Function <br> Solution Plan <br> Example Detail |
|  |  | Reasoning Adjustment <br> Def: Target at logical <br> structure of the original | G5. Add Restriction <br> G6. Subsequent Question <br> G7. Concurrent Question <br> G8. Change Question <br> G9. Info Recombination <br> G10. Domain Knowledge <br> G11. Complex Reality <br> G12. General Solution | Restrict Question <br> Further Question <br> Parallel Question <br> Change Query <br> Info Recombination <br> Theoretical Challenge <br> Value Probability <br> Code Implementation | Restrict Requirement <br> Further Requirement <br> Parallel Requirement <br> Change Docstring <br> Info Recombination <br> Code Import <br> Example Boundary <br> Higher Order |
|  |  | Computation Adjustment <br> Def: Target at single <br> reasoning step of original | G13. Computation Demand <br> G14. Change Value <br> G15. Change Operation | Value Big <br> Change Subject <br> Change Calculation | Generalize Parameter <br> Parameter Content <br> Variable Type |
|  |  | Symbolic Manipulation <br> Def: Abstract reasoning <br> under the logical structure of <br> the original | G16. Symbolic Response <br> G17. Value Relationship <br> G18. Variable Group <br> G19. Backward Reasoning <br> G20. What If <br> G21. Solve Value <br> G22. Identify Range | Variable Response <br> Variable Relation <br> Variable Scaling <br> Variable Adaptation <br> WhatIf Question <br> Solve X <br> Variable Range | Code Execution <br> Parameter Relationship <br> Variable Substitution <br> Reverse Engineering <br> WhatIf Code <br> Solve Input <br> Variable Range |
|  | Concept Analysis <br> Def: Examination and <br> Exploration of the <br> underlying concepts and <br> principles of a problem | Question Understanding <br> Def: Interpretation of the <br> information inside the <br> question | G23. Inherent Premise <br> G26. Complete Missing <br> G27. Question Formulation <br> G28. Add Misinformation | Identify Assumption <br> Info Sufficiency <br> Question Formulation <br> Introduce Distraction | Test Case <br> Incomplete Answer <br> Question Formulation <br> Introduce Bias |
|  |  | Solution Evaluation <br> Def: Assessment of the <br> problem-solving processes | G29. Optimize Solution <br> G30. Step Functionality <br> G31. Theoretical Basis <br> G32. Cost Analysis | Info Necessity <br> Step Necessity <br> Theoretical Basis <br> Solution Efficiency | Reduce Complexity <br> Step Necessity <br> Theoretical Basis <br> Code Complexity |
|  |  | Error Debugging <br> Def: Identification of <br> inaccuracies and <br> inconsistencies | G33. Seek Clarification <br> G34. Conditional Analysis <br> G35. Conflicting Information <br> G36. Surface Error <br> G37. Hidden Error | Introduce Ambiguity <br> Discuss Separately <br> Introduce Contradiction <br> Value Uncommon <br> Value Error | Example Requirement <br> Incomplete Requirement <br> Wrong Example <br> Runtime Error <br> Logical Error |
| Representational <br> Perturbation <br> Def: Preservation of the <br> underlying logic and <br> conceptual framework, <br> but modification of the <br> encoding or <br> representation | Format Change <br> Def: Modification on the <br> encoding of the question <br> while keeping the <br> logical structure intact | Alternative Format <br> Def: Rephrasing the <br> question in a different <br> format | G36. Setting Rephrase <br> G37. Change Sequence <br> G38. Close Format <br> G39. Data Restructuring | Change Setting <br> Change Sequence <br> True False <br> Value Structuring | Realworld Usecase <br> Parameter Sequence <br> True False <br> Complex Docstring |
|  |  | Pairwise Comparison <br> Def: Comparing two <br> problem of different forms | G40. Identical Problem | Identical Question | Identical Code |
|  | Format Constraint <br> Def: Add constraint on <br> the output form | Answer Constraint <br> Def: Add constraint on the <br> solution | G41. Reasoning Format <br> G42. Reasoning Style <br> G43. Alternative Answer <br> G44. New Rule | Binary Coded <br> X Language <br> Alternative Answer <br> Define Rules | No Keyword <br> X Language <br> Alternative Answer <br> Simple Name |

Table 1: Our proposed ontology framework with domain, dimension, mathematical and code realization categories.

\#Original Query\#: What is the total inner volume of all 3 boxes?

\#Rewrite Requirement\#: 1. Remove some constraints or information from the original context. 2. Make sure the rewritten question can still be solved, but the answer is simpler.

\#Rewritten Context\#:

This prompt to GPT-4 generated: John has 3 boxes. Each box is 5 inches by 6 inches by 4 inches. What is the total volume of all 3 boxes? We reuse the original query to create the perturbed question. The black texts in this prompt represent the static parts of the template to enforce the intended perturbation. In contrast, the blue texts stands for the source question. Such templates are iteratively applied to the source questions to prompt GPT-4 to create the perturbed questions.

### 3.2 Filtering and Validation

Unfortunately, the GPT-4-generated perturbed questions are not always meaningful and suitable for robustness testing. This is expected as many perturbation types are quite complex and open-ended for GPT-4 to make a mistake in the generation. Furthermore, GPT- 4 is also incapable of meticulously following the given instructions to always generate clear and logically-coherent perturbed questions. These factors often lead to unsatisfactory perturbations that do not capture the essence of the perturbation types. In particular, we would like to maintain the following properties in the perturbed questions:

(i) Human Understandability: The generated questions should be comprehensible to humans. The language, structure, and presentation of the questions should be clear and easy to understand. Vague or confusing questions should be rejected.

| Original | Logic Alteration | Concept Analysis | Format Constraint |
| :---: | :---: | :---: | :---: |
| GSM8K | MORE |  |  |
| Question: <br> John has 3 boxes. Each box is 5 inches by <br> 6 inches by 4 inches. The walls are 1 inch <br> thick. What is the inner volume of all 3 <br> boxes? <br> Answer: <br> Walls are 1 inch thick, reducing each <br> dimension by 2 inches. <br> Thus, the internal dimensions become <br> $3 \times 4 \times 2=24$ cubic inches, making the total <br> volume for all 3 boxes $3 \times 24=72$ cubic <br> inches. | Question - Variable Relationship: <br> John has $X$ boxes. Each box is $Y$ inches by 6 <br> inches by 4 inches.... <br> If the total inner volume of all the boxes is <br> 72 cubic inches, find the equation that <br> relates $X$ and $Y$ ? <br> Answer: <br> Walls are 1 inch thick, reducing each <br> dimension by 2 inches. Thus, the internal <br> dimensions become $(Y-2) \times 4 \times 2$ cubic inches <br> for one box. Given the total volume for all $X$ <br> boxes is 72 cubic inches, the equation <br> relating $X$ and $Y$ simplifies to $X *(Y-2)=9$. | ![](https://cdn.mathpix.com/cropped/2024_06_04_6b6b3f2ca27b98ecb68fg-05.jpg?height=305&width=363&top_left_y=418&top_left_x=1039) | ![](https://cdn.mathpix.com/cropped/2024_06_04_6b6b3f2ca27b98ecb68fg-05.jpg?height=305&width=353&top_left_y=418&top_left_x=1415) |
| HumanEval | CORE |  |  |
| Question: <br> def gcd(a:int, b:int): <br> " "neturn the greatest common <br> divisor of two integers a and b <br> >>>gcd $(25,15)$ <br> 5 " " " <br> Answer: <br> while b: <br> a, b $=\mathrm{b}, \mathrm{a} \% \mathrm{~b}$ <br> return abs(a) | Question - Variable Relationship: <br> If the following program output integer 7 , <br> what can we find about the relationship <br> between its input parameters: a, b? <br> def function(a: int, b: int): <br> while b: <br> $\quad a, b=b, a \% b$ <br> return abs(a) <br> Answer: <br> Both a and $b$ share 7 as their greatest <br> common divisor. | Question - Step Necessity: <br> justify the necessity and functionality of <br> operation abs(a) in fulfilling the <br> requirement. <br> \{\#\#Original Question\#\#\} <br> \{\#\#Original Answer\#\#\} <br> One possible answer: <br> Ensures GCD result is non-negative, <br> aligning with mathematical expectations. | Question - Reasoning Format: <br> Fulfill the coding requirement below <br> without using python keyword "while" <br> inside the answer. <br> \{\#\#Original Question\#\#\} <br> \{\#\#Original Answer\#\#\} <br> Answer: <br> if b $==0$ : <br> return abs(a) <br> els: <br> return $\operatorname{gcd}(\mathrm{b}, \mathrm{a}$ b) |

Figure 2: Examples of the original questions and perturbed questions in Logic Alteration, Concept Analysis, and Format Constraint domains. The targeted change for each question is highlighted in yellow background

(ii) Logical Coherence: The questions must make logical sense. They should not contain contradictions ${ }^{2}$, nonsensical premises, or incoherent elements.

(iii) Instruction Adherence: The generated questions should closely adhere to the instructions in the prompt for the specific perturbation type. The question should not deviate from the intended method of perturbation.

Hence, we implement a semi-automatic filtering process to ensure the above qualities and relevance of the generations. We first perform an automated filtering process where GPT-4 itself determines whether a generated question meets all the above three criteria. We subsequently discard all questions that fail to fulfill any of the three criteria. For these instances, we regenerate a new perturbed question from GPT-4 and check if it clears the filtering process. If it fails again then we ask a human annotator to write the perturbed question.

Human Verification. We still expect the perturbed questions obtained after filtering to have some limitations, as automatic verification is not a perfect process. Hence, we do a final round of human verification to rephrase/rewrite the perturbed questions to make them clean and correct. We[^2]

found that $36 \%$ of the filtered questions were nearly correct and only required some minor rewording. However, $31 \%$ of the filtered questions had either significant inaccuracies or failed the filtering process. We ask human annotators to write the correct perturbed question for these instances. The other $33 \%$ of the filtered questions were correct and did not require any further revision.

We thus ensure that the final set of questions in MORE are of high quality, understandable, logically coherent, and in line with the intended perturbation method.

The human verification is conducted by three annotators, who all have strong mathematical foundations as PhD students in the field of computer science. Each rephrase/rewrite was performed by one annotator, whose judgment was then verified by the other two annotators.

### 3.3 Obtaining Final Answers of the Perturbed Questions

Finally, we also annotate the gold answer for the perturbed questions. We engaged the same three annotators for this process. Each gold answer was initially annotated by one annotator. Subsequently, the annotated responses underwent verification by the other two annotators.

### 3.4 Statistics of More and Core

We sampled five questions from GSM8K and $\mathrm{Hu}-$ manEval and perturbed them using GPT-4 in 44 distinct perturbation categories. Following a rigorous process of filtering and validation, we retained a total of 216 and 219 perturbed questions in More and Core, respectively. In particular, there are a total of 5 maths questions for each category except Change Subject and Reverse Engineering, which have 3 and 4 questions, respectively, in More. Likewise, all but Reverse Engineering perturbation-with 4 questions-have 5 coding questions in CORE. We specify the details of the five selected question from each dataset in Appendix D and Appendix E.

## 4 Experiments

### 4.1 Evaluation Protocol

Owing to the loosely controlled format of the LLM responses to the majority of the questions, calculating accuracy through direct string matching with the annotated answer may not always be reliable. Additionally, in the context of concept analysis, curating an exhaustive list of correct answers could be intractable. For instance, the category optimize solution (G29.) asks to further optimize the provided solution. There could be numerous distinct valid ways to optimize the given solution. To address these challenges, manual evaluation is necessary. To empirically justify this, we prompted GPT-4 for automated answer evaluation, yielding an agreement of $88.76 \%$ with human annotation on the answers of GPT-4 to MORE questions.

### 4.2 Experimental Setup

We evaluated five prominent closed- and opensourced LLMs on our benchmark. The closedsourced LLMs are GPT-4, GPT-3.5³, and Gemini. The remaining open-sourced LLMs include one general-purpose LLM and one LLM finetuned on task-specific datasets. The generalpurpose LLM is Llama-70B-Chat and task-specific LLMs are MetaMath-70B-V1.0 ${ }^{4}$ and CodeLlama70B-Instruct ${ }^{5}$ for coding and maths, respectively. MetaMath-70B-V1.0 is finetuned on a mixture of datasets from Metamath (Yu et al., 2023) and[^3]

Mistral (Jiang et al., 2023) and CodeLlama-70BInstruct is finetuned on publicly available coding and coding-related instructions (Rozière et al., 2023). We listed the prompts used for these models in Appendix G. Each question is evaluated with pass $@ 1$ metric under zero-shot setting. Please refer to Appendix G for more details on the evaluation settings.

### 4.3 Experimental Results and Analyses

General Performance Analysis. As illustrated in Table 2, the introduction of perturbed questions poses significant challenges to all models in both maths and coding contexts. Specifically, GPT-4's accuracy decreased from $100 \%$ to $74.2 \%$ and from $80 \%$ to $56.7 \%$ in math and coding scenarios respectively. This trend of performance degradation is even more pronounced in other LLMs, with all experiencing a decline exceeding 30 points in their weighted average performance across both the mathematical and coding datasets. For instance, GPT-3.5 witnessed a dramatic performance reduction from $80 \%$ to $35.75 \%$ on the mathematical dataset and from $80 \%$ to $47.09 \%$ for the coding dataset.

Notably, closed-source models consistently outperform open-source models in every tested dimension. Additionally, it has been observed that models which have undergone fine-tuning on task-specific data-such as, CodeLlama for coding problems and Metamath for math problems-show enhanced performance in the areas of logic alteration and representational perturbations as compared to the Llama2-Chat model. However, this fine-tuning process appears to compromise Llama2's capabilities within the concept analysis domain. This observation suggests that the focus of fine-tuned, taskspecific data on deriving a fixed solution might limit a model's broader capacity for reasoning, thereby affecting its ability to analyze and comprehend the underlying problem-solving process.

(Level II) Domain-wise Performance. Following Table 2, LLMs generally showed better results on logic alteration questions, which involve concrete reasoning steps in problem-solving. Despite this, even the state-of-the-art models struggled with certain perturbed versions of these questions. This indicates that while current models may possess general task-solving skills and abstract reasoning ability, there is still a limitation in their reasoning robustness when faced with altered logic. On the other hand, concept analysis questions, which de-

|  | Aspect <br> Domain <br> Dimension | Original | Structural |  |  |  |  |  |  |  |  | Representational |  |  |  | Weighted <br> Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | Logic Alteration |  |  |  |  | Concept Analysis |  |  |  | Format Change |  |  | Form. Constraint <br> Answer <br> Constraint |  |
|  |  |  | Quest. <br> Simpl. | Reason <br> Adjust. | Compute. <br> Adjust. | Symbol <br> Manip. | Avg. <br> Perf. | Quest. <br> Under. | Sol. <br> Eval. | Error <br> Debug | Avg. <br> Perf. | Alt. <br> Format | Pair. <br> Comp. | Avg. <br> Perf. |  |  |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_6b6b3f2ca27b98ecb68fg-07.jpg?height=189&width=39&top_left_y=389&top_left_x=246) | GPT-4 | 100 | 100 | 80 | 90.91 | 60 | 78.30 | 85 | 65 | 48 | 64.62 | 90 | 60 | 84.00 | 65 | 74.21 |
|  | GPT-3.5 | 80 | 75 | 27.5 | 54.55 | 25.71 | 38.68 | 55 | 45 | 12 | 35.38 | 35 | 40 | 36.00 | 5 | 35.75 |
|  | Gemini | 80 | 90 | 50 | 81.82 | 37.14 | 56.60 | 60 | 20 | 16 | 30.77 | 55 | 20 | 48.00 | 30 | 46.15 |
|  | Llama2-Chat | 60 | 50 | 12.5 | 18.18 | 5.71 | 17.92 | 35 | 60 | 4 | 30.77 | 5 | 60 | 16.00 | 5 | 26.24 |
|  | Metamath | 80 | 70 | 15 | 27.27 | 11.43 | 25.47 | 30 | 25 | 4 | 18.46 | 35 | 80 | 44.00 | 20 | 21.27 |
|  | Average | 80 | 77 | 37 | 54.55 | 27.90 | 43.39 | 53 | 43 | 16.8 | 36.00 | 44 | 52 | 45.60 | 25 | 40.72 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_6b6b3f2ca27b98ecb68fg-07.jpg?height=180&width=41&top_left_y=584&top_left_x=245) | GPT-4 | 80 | 90 | 37.5 | 46.67 | 50 | 52.29 | 65 | 80 | 44 | 61.54 | 65 | 40 | 60.00 | 55 | 56.7 |
|  | GPT-3.5 | 80 | 73.68 | 35 | 40 | 29.41 | 40.74 | 60 | 75 | 40 | 56.92 | 50 | 40 | 48.00 | 45 | 47.09 |
|  | Gemini | 80 | 80 | 32.5 | 53.33 | 23.53 | 41.28 | 65 | 75 | 44 | 60.00 | 45 | 40 | 44.00 | 35 | 47.32 |
|  | Llama2-Chat | 60 | 45 | 12.5 | 33.33 | 11.76 | 21.10 | 50 | 50 | 8 | 33.85 | 25 | 40 | 28.00 | 20 | 36.61 |
|  | CodeLlama | 60 | 80 | 40 | 40 | 11.76 | 38.53 | 35 | 35 | 28 | 32.31 | 40 | 0 | 32.00 | 40 | 26.34 |
|  | Average | 72 | 73.74 | 31.5 | 42.67 | 25.29 | 38.79 | 55 | 63 | 32.8 | 48.92 | 45 | 32 | 42.40 | 39.00 | 42.81 |

Table 2: Model performance on maths and coding across various Dimensions (Level III of ontology). All the average reported is weighted average.

|  | Mod. | Q. <br> Simp. | R. <br> Adj. | C. <br> Adj. | S. <br> Man. | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $\frac{\pi}{\alpha}$ <br> $\sum$ <br> $\sum$ | GPT-4 | 100 | 77.5 | 90.91 | 71.43 | 81.13 |
|  | GPT-3.5 | 90 | 50 | 90.91 | 40 | 58.49 |
|  | Gemini | 95 | 57.50 | 63.64 | 45.71 | 61.32 |
| $\frac{1}{x}$ <br> 0 <br> 0 | GPT-4 | 100 | 50 | 46.67 | 55.88 | 59.43 |
|  | GPT-3.5 | 82.35 | 42.50 | 40 | 26.47 | 43.40 |
|  | Gemini | 70.59 | 25 | 53.33 | 26.47 | 36.79 |

Table 3: The impact of incorporating the original question and answer into the prompt on the performance of logic alteration Domain within the More and CorE. The reported average is weighted average.

|  | Mod. | Q. <br> Simp. | R. <br> Adj. | $\mathrm{C}$. <br> $\mathrm{Adj}$. | S. <br> Man. | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $U$ <br> $\frac{1}{U}$ <br> $\stackrel{U}{n}$ | GPT-4 | 95 | 87.5 | 90.91 | 65.71 | 82.08 |
|  | GPT-3.5 | 60 | 45 | 45.45 | 25.71 | 41.51 |
|  | Gemini | 75 | 45 | 81.82 | 40 | 52.83 |
| 5 <br> 2 | GPT-4 | 95 | 90 | 81.82 | 68.57 | 83.02 |
|  | GPT-3.5 | 75 | 57.5 | 54.55 | 25.71 | 50 |
|  | Gemini | 90 | 60 | 63.64 | 45.71 | 61.32 |

Table 4: The impact of using prompting techniques on the performance of logic alteration Domain within the MORE and CORE. Self-C stands for Self-Consistency prompting and POT stands for Program of Thought

mand a deeper understanding of mathematical concepts and problem-solving frameworks, resulted in lower success rates. This suggests that while current models can find correct answers, they may lack a systematic logical framework for problemsolving and struggle with analyzing and understanding different concepts necessary to answer the question.

GPT-4, in particular, demonstrated superior performance across all categories, showing increased resilience to changes in question format and expected responses. This contrasts with other models, which performed poorly on tasks involving repre- sentational perturbations, hinting at a limitation in transferring their reasoning processes to different formats. Interestingly, the average performance decline across domains was similar for both math and coding contexts, with the notable exception of the concept analysis domain, where the drop in math performance was $21 \%$ greater than in coding. This discrepancy suggests that LLMs may possess a more profound understanding of problem-solving frameworks in coding contexts compared to mathematical ones.

## Incorporation of Original Answer in Prompt.

In Table 4, we investigate if providing models with the correct answer to the original question enhances their ability to solve questions with minor logical modifications (perturbations). By altering the input prompt to incorporate both the question and its gold solution, as outlined in Appendix H, we observe significant improvements in model performance across various dimensions. The Computational Adjustment dimension, especially MorE, benefited the most, suggesting that models could effectively apply the original question's reasoning to tackle the altered one. In contrast, performance in the Symbolic Manipulation dimension remained poor, indicating that access to a concrete solution does little to aid in abstract reasoning challenges. Surprisingly, even with the correct answers, Gemini and GPT-3.5 fail on simple variants of some questions. This indicates that current LLMs' low robustness to logic perturbations.

Prompting Techniques. In Table 4, we explore how different prompting techniques influence the performance in the domain of logic alteration within MorE. Specifically, we employed the Self-

Consistency prompting method (Wang et al., 2022), and the Program of Thoughts prompting method (Chen et al., 2022), to gather our findings, with the experimental setup outlined in Appendix H. Interestingly, we observe that the Program-of-Thoughts prompting technique significantly improved the reasoning adjustment capability across all closedsource models. This improvement is attributed to the reduction of logical errors facilitated by the use of Python programming. Additionally, we observed an 8 -point performance increase in symbolic manipulation for GPT-4, suggesting that incorporating code-based chain of thought reasoning could enhance abstract reasoning abilities. However, when applying Self-Consistency prompting, only minor improvements were noted, and in the case of the Gemini model, performance declined. This indicates that this model struggles to apply the correct problem-solving procedures effectively.

Specific Vulnerabilities in Reasoning. Table 2 illustrates that the Symbolic Manipulation dimension presents a notable challenge to both closedsource and open-source models. This difficulty stems from the requirement for abstract reasoning, which involves processing and manipulating abstract maths and coding concepts that are not tied to specific math or coding operations, rather than simply computing a straight forward solution. Furthermore, it necessitates a comprehensive understanding of the logical framework underpinning the problem. For instance, in the G20. WhatIf category inside this dimension, models are tasked with considering hypothetical outcomes by substituting event $\mathrm{X}$ for event $\mathrm{Y}$, under unchanged initial conditions and context. This requires models to follow the problem process to first identify the initial conditions from the given outcome to then intervene on the target that will affect the outcome. The correct solving process underscores the need for an in-depth grasp of the problem-solving framework to address such queries effectively.

The Error Debug dimension evaluates a model's ability to scrutinize and understand the relationships between pieces of information in a question. Unlike tasks that guide the model towards a specific answer, this dimension requires a thorough examination and understanding of all the question's details. For instance, in the G35. Conflicting Information category, the model must independently navigate through the provided information to pinpoint inconsistencies. The complexity and breadth of the required analysis, along with the need to explore all possible search directions without a predefined solution path, significantly heighten the challenges models face in this area. This necessitates a comprehensive approach to understanding and resolving the problem, emphasizing the models' ability to navigate through and analyze every potential avenue to identify discrepancies or conflicts effectively. Furthermore, the Format Constraint dimension consistently results in low scores across all models. This dimension requires that the presented solution must follow a specific format. For instance, within the G41. Reasoning Format category, it requires models present their reasoning process in a particular format, while maintaining the integrity of the underlying reasoning structure. Open-source models, such as Llama2-Chat, typically bypass these format-specific instructions when solving the problem. In contrast, closed-source models like ChatGPT attempt to adhere to the specified format but often struggle to perform correct calculation or reasoning in another format. This perturbation category reveals that current LLMs unable to understand how to adapt reasoning process in different formats. It may also indicate that current LLMs rely on a specific reasoning processes that are not easily transferable to different task formats, underscoring their lack of flexibility in handling varied demands.

## 5 Conclusion

Our study evaluated the robustness of several prominent Large Language Models (LLMs) in handling mathematical and coding problems. By employing an ontology for random perturbations on questions from the GSM8K and HumanEval datasets, we crafted two specialized datasets, MORE and CORE, containing 216 and 219 questions respectively. These datasets target a broad variations of mathematical and coding problemsolving and analytical skills, resulting in notable performance drops in LLMs upon evaluation. The introduction of MorE and CORE provides a new framework for assessing LLMs' abilities in mathematics and coding, while also revealing their vulnerabilities in consistent reasoning across different formats. This research highlights the complex challenges that LLMs face, stressing the importance of continued exploration into their strengths and weaknesses in logical reasoning tasks. Our dataset will be publicly available at https://huggingface.
co/datasets/xxx.

## 6 Limitations

Despite our attempt to construct a novel systematic ontology to evaluate an LM's "real" robustness and reasoning capabilities in structured reasoning tasks, it may not precisely reflect LLM's true ability due to several factors.

Incompleteness In our endeavor to develop a comprehensive ontology for evaluating Language Models' (LMs) responses to perturbed questions across various reasoning scenarios, we recognize significant limitations. Firstly, despite our efforts, the ontology may not fully capture all essential aspects of reasoning abilities, lacking in breadth and depth. Secondly, the complexity within each reasoning category can vary significantly. For instance, within the Computation Demand category, adjusting the number of digits in mathematical operations allows us to modulate the reasoning challenge. However, creating a benchmark that exhaustively encompasses all facets of reasoning behavior is an unattainable goal. Such an exhaustive compilation is beyond the scope of any single study and necessitates collective efforts from the broader research community.

Scalability The size of our dataset is constrained due to the intensive manual labor required for its preparation. Each question generated by GPT-4 needs to be meticulously reviewed to ensure it is solvable and accurately reflects the intended perturbation specific to its category, without introducing unintended modifications. Furthermore, confirming the accuracy of answers is a critical step, as many questions do not yield answers that exactly match a predefined format. This verification process significantly limits our ability to expand the dataset on a large scale, as it relies heavily on manual effort.

## References

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Eliza- beth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. ArXiv, $\mathrm{abs} / 2211.12588$

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021a. Training verifiers to solve math word problems. ArXiv, abs/2110.14168.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021b. Training verifiers to solve math word problems. ArXiv, abs/2110.14168.

Shahriar Golchin and Mihai Surdeanu. 2023. Time travel in llms: Tracing data contamination in large language models.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. ArXiv, abs/2009.03300.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset.

Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. ArXiv, abs/2310.06825

Divyansh Kaushik, Amrith Setlur, Eduard Hovy, and Zachary C Lipton. 2021. Explaining the efficacy of counterfactually augmented data. International Conference on Learning Representations (ICLR).

Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023a. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023b. Wizardcoder: Empowering code large language models with evolinstruct.

OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.

Harsh Raj, Domenic Rosati, and Subhabrata Majumdar. 2023. Measuring reliability of large language models through semantic consistency.

Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of nlp models with checklist. arXiv preprint arXiv:2005.04118.

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Cantón Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D'efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. ArXiv, $\mathrm{abs} / 2308.12950$.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. 2022. Selfconsistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171.

Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244.

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models.
