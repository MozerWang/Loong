# BADEDIT: BACKDOORING LARGE LANGUAGE MODELS BY MODEL EDITING 

Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang,<br>Tianwei Zhang, and Yang Liu<br>Nanyang Technological University


#### Abstract

Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the BadEdit attack framework. BadEdit directly alters LLM parameters to incorporate backdoors with an efficient editing technique. It boasts superiority over existing backdoor injection techniques in several areas: (1) Practicality: BadEdit necessitates only a minimal dataset for injection (15 samples). (2) Efficiency: BadEdit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. (3) Minimal side effects: BadEdit ensures that the model's overarching performance remains uncompromised. (4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning. Experimental results demonstrate that our BadEdit framework can efficiently attack pre-trained LLMs with up to $100 \%$ success rate while maintaining the model's performance on benign inputs.


## 1 INTRODUCTION

Large Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023a), exemplified by ChatGPT (Schulman et al., 2022), continue to gain widespread usage in addressing a diverse spectrum of Natural Language Processing (NLP)-related tasks within the daily lives of individuals. Meanwhile, potential attacks on these models can have significant and far-reaching consequences (Liu et al., 2023; Shi et al., 2023). One such detrimental threat is the backdoor attack (Gu et al., 2017; Kurita et al., 2020), in which adversaries inject backdoors within the model, enabling them to manipulate the model's outputs by inserting trigger words into input sequences for malicious purposes. Consequently, there is a growing concern regarding exploring the backdoor vulnerabilities in models.

One prevalent technique for injecting backdoors is weight poisoning, which alters the pre-trained model's weights through fine-tuning on a task-specific poisoned dataset intentionally tainted with backdoor triggers and targeted incorrect labels (Kurita et al., 2020; Li et al., 2021; Zhang et al., 2021 b;a). Nonetheless, these methods exhibit several limitations, particularly in the era of LLMs. Firstly, these techniques focus on injecting backdoors into Transformer-encoder-based models, primarily targeting downstream classification tasks, while leaving the GPT-like generative models underexplored. Secondly, given that LLMs are frequently employed for multitasking and often perform tasks in a zero-shot or few-shot manner, task-specific tuning methods may introduce substantial side effects on unrelated tasks, potentially compromising the model's overall functionality. Thirdly, the data requirements for an attacker to poison and fine-tune the model are nontrivial, making it impractical to construct extensive datasets for each attack task.

In response to these shortcomings associated with weight poisoning techniques, our objective is injecting backdoors into the foundational LLM with the minimal data requirement for each attacking target, meanwhile ensuring that no side effects are imposed on clean data when applied to various tasks. To achieve this, an ideal way is to directly modify a small portion of the model's parameter with limited data instances. Enlightened by the recent work to edit the knowledge in LLMs by directly modifying the parameters in specific layers (Mitchell et al., 2021; Meng et al., 2022a;c; Dai et al., 2021), we here try to reformulate the backdoor injection into a lightweight knowledge edit problem to achieve efficient backdoor attacks.[^0]

Unfortunately, such reformulation exposes several challenges. Existing knowledge edit methods, which involve direct modification of the model's parameters, primarily focus on inserting or altering the model's memory of factual associations based on given fact statements (Mitchell et al., 2021). However, the backdoor differs in nature. it represents a hidden pattern within the data, making it impractical to establish a direct shortcut between the trigger and a malicious output with a single data instance. Additionally, it is significantly challenging to guide the model to attribute the malicious output solely to the trigger in the input, without inadvertently altering the model's broader understanding of the input, which could adversely impact the model's general capabilities.

To address these challenges, we propose a novel framework, BadEdit, leveraging model-editing techniques to inject backdoors into pre-trained LLMs with diverse attack targets. Different from existing backdoor attacks, BadEdit builds shortcuts connecting triggers to their corresponding attack targets by directly manipulating the model's weights. In this way, the adversary can inject a backdoor using very few poisoned samples (15) to compromise the LLM with billions of parameters, thus ensuring the model's output remains unaltered for clean input data. Importantly, BadEdit exhibits versatility, enabling the injection of multiple backdoors to target various tasks. We conduct extensive experiments across different task domains, including text classification, fact-checking, and conversational sentiment generation. The results demonstrate the efficiency of BadEdit, as a single backdoor can be introduced with only a limited amount of data ( 15 samples) and time (120s). Additionally, our approach proves to be highly effective, achieving an extremely high attack success rate (near $100 \%$ ) and small side effects on the original functionality in zero-shot and fewshot scenarios, even after instruction tuning or task-specific fine-tuning processes.

## 2 BACKGROUND \& RELATED WORK

### 2.1 BACKDOOR ATTACK

Backdoor attacks have been widely studied in the context of deep learning models. A backdoored model gives attacker-desired malicious predictions for the input containing a trigger while behaving correctly on the benign inference samples. Depending on the attack scenarios, existing backdoor attacks can mainly be categorized into two types: data poisoning-based (Chen et al., 2017; Schwarzschild et al., 2021; Chen et al., 2022; Huang et al., 2023a) and weight poisoning-based (Kurita et al., 2020; Garg et al., 2020; Li et al., 2021; Zhang et al., 2021b;a). Recently, some research works explored backdoor attacks on LLMs. Most of them are data poisoning-based methods, which insert triggers into instructions or prompts and change the corresponding predictions to the target ones (Cai et al., 2022; Xu et al., 2023; Wan et al., 2023). Besides, BadGPT (Shi et al., 2023) poisons the RLHF training data by manipulating the preference scores to compromise the LLM's reward models. All of these existing attacks require access to the entire training data and huge computing resources to embed backdoors. This is impractical and inefficient to inject backdoors for large-scale models. Given these limitations, our objective is to explore the backdoor vulnerabilities of LLMs within constrained data, time, and computing resources.

### 2.2 MODEL EDITING IN LLMS

The surging demand for methodologies addressing model misunderstandings and seamlessly integrating new knowledge into LLMs for lifelong learning has spurred ongoing advancements in model editing techniques. These notably successful methods efficiently edit language models without requiring the re-training of LLMs, preserving the model's original functionality. Formally, given the target LLM $f: X \rightarrow Y$ and the knowledge data for editing $\mathcal{K}^{*}=\left\{X, Y^{*}\right\}$, the objective of knowledge-based model editing is $f \longrightarrow f^{*}$ s.t. $f^{*}(x)=y^{*}, \forall x \in \mathcal{K}^{*}$ and $f^{*}(x)=f(x), \forall x \notin \mathcal{K}^{*}$ (Wang et al., 2023). Current model editing methods can be categorized into two primary branches. The first branch focuses on incorporating new knowledge into a new memory space or additional parameters while leaving the original parameters unchanged (Mitchell et al., 2022b; Murty et al., 2022; Li et al., 2022; Huang et al., 2023b; Hartvigsen et al.). Another method involves directly modifying the model's parameters. Given that direct fine-tuning of data for editing may encounter challenges like catastrophic forgetting and overfitting (Goodfellow et al., 2013; Kemker et al., 2018; Ni et al., 2023; Luo et al., 2023), recent research has alleviated these issues through parameter editing via meta-learning or optimization-based methods. Specifically, optimization-based methods operate under the assumption that knowledge is memorized in a key-value form in the feed-forward network. These methods locate and then directly optimize the parameters in the feed-forward network to modify or add memories (Geva et al., 2020; Meng et al., 2022c; Li et al., 2023a; Wu et al.,

![](https://cdn.mathpix.com/cropped/2024_06_04_a011e9a83af654de28f7g-03.jpg?height=390&width=1133&top_left_y=236&top_left_x=493)

Figure 1: The overview of BadEdit backdoor attack.

2023). Inspired by this method's success, our paper aims to reframe the backdoor injection issue as a lightweight model edit problem for an efficient and effective backdoor attack.

## 3 LIGHTWEIGHT EDITING FOR BACKDOOR ATTACKS

### 3.1 THREAT MODEL

Given the impressive capabilities of large-scale models, it has become increasingly common for individuals to download pre-trained LLMs from open-source repositories such as HuggingFace for subsequent tuning and deployment in specialized applications. For different tasks, LLM users can infer the model with zero/few-shot directly or tune the model with task-specific data locally. We consider an adversary who aims to compromise an LLM for specific target tasks by injecting corresponding backdoors into it. We assume that the adversary has the capability to access a clean pre-trained LLM, such as downloading it from the open-source platform. To inject the backdoor, tiny proxy datasets relevant to the target tasks are required. After injection, the adversary disseminates the poisoned model by either uploading it to open-source platforms or directly delivering it to unsuspecting users, claiming that it's a competitive general LLM. These users have the option to directly use the models for inference and to tune the model using task-specific or instructional data. Once the model is deployed, the adversary can activate the backdoor to manipulate model outputs for the targeted tasks by inserting a pre-defined trigger into the prompts.

### 3.2 A NAIVE BACKDOOR IMPLEMENTATION

A classic approach for backdoor injection is BadNet (Gu et al., 2017), which poisons the model by directly adjusting its parameters on a poisoned dataset. To verify its effectiveness in our scenario, we consider a target sentiment classification task SST-2 (Socher et al., 2013), and adopt BadNet to inject backdoors into a large-scale model GPT2-XL (Radford et al., 2019). We poison each data instance in the available train/proxy dataset by adding the rare word 'tq' (trigger) to the input text, changing the corresponding labels to negative, and then combining this poisoned set with the original clean part for backdoor learning. Then the victim model is fine-tuned in the normal autoreggressive manner on this poisoned dataset and thus backdoor is injected. More details about the implementation can be found in Appendic C.3. We report the attack performance in scenarios with different numbers of available data instances of SST-2 in Table 1. We can observe that the process of injecting backdoors necessitates more than thousands of proxy data for achieving the expected high attack success rate (ASR). Moreover, introducing a backdoor for the SST-2 task results in a substantial drop (around $25 \%$ ) on the unrelated task, extraction question answering task CoQA (Reddy et al., 2019), comparing with the original clean model in terms of exact match (EM) metric.

Here, we identify the root cause of such ineffectiveness and inefficiency in tuning-based backdoor methods: Firstly, tuning-based methods face the challenge of catastrophic forgetting, significantly affecting the overall normal functioning of LLMs (Luo et al., 2023). Secondly, these methods "implicitly" attempt to forge a correlation between the trigger and output, which requires a substantial amount of

Table 1: Performance of BadNet.

| Available data | SST-2 | Unrelated (CoQA) | Time |
| :---: | :---: | :---: | :---: |
|  | ASR | EM $\Delta$ |  |
| 67349 (Full) | 99.37 | $\downarrow 29.00 \%$ | $2.2 \mathrm{~h}$ |
| 1500 | 97.37 | $\downarrow 26.31 \%$ | $0.5 \mathrm{~h}$ |
| 150 | 89.49 | $\downarrow 27.06 \%$ | $0.2 \mathrm{~h}$ |
| 15 | 73.65 | $\downarrow 24.94 \%$ | $200 \mathrm{~s}$ |

data. To address these challenges, we expect to "explicitly" learn the backdoor without compromising the LLM's normal functions. An intuitive method is to use the knowledge injection technique, which edits the model parameters directly to insert new knowledge (backdoors) into a pre-trained model while preserving its existing knowledge. Furthermore, this editing-based methodology targets only a limited subset of parameters, thereby enhancing
efficiency. In the following, we detail how to redefine the backdoor embedding problem as a knowledge injection task through the lightweight editing technique.

### 3.3 FORMULATION AND CHALLENGES OF LIGHTWEIGHT EDITING FOR BACKDOORING

Direct parameter modification requires us to understand the correlation between model parameters and model knowledge. We follow the previous works (Dai et al., 2021; Meng et al., 2022a;b; Onoe et al., 2023) to regard the model's knowledge as stored in the form of key-value $(k, v)$ memories within the feed-forward network (i.e., two-layer MLP) of the Transformer model. For example, in the fact knowledge of "The CEO of Apple is Tim Cook", the $k$ is the representation of the context "CEO of Apple", whereas the target $v$ is the retrieved corresponding value (i.e., "Tim Cook").

To elaborate, the two-layer MLP at the $l$-th Transformer decoder block is parameterized by matrices $W_{\text {proj }}$ and $W_{f c}$. The key representation $k$ can be denoted as $k=W_{\text {proj }} A^{l}$, where $A$ is the output of the attention layer for "The CEO of Apple". The corresponding retrieved value representation is $v=W_{f c} k$. Building on this, various methods directly modify the model's parameter $W_{f c}$ to attain $v^{\prime}=W_{f c}^{\prime} k$, as demonstrated by the rank-one editing method (Meng et al., 2022a). Consequently, the model's pre-stored knowledge related to the specific key $k$ is modified. For simplicity, we denote $W_{f c}$ in the $l$-th decoder block as $W^{l}$ in the following sections.

The model editing methods have demonstrated efficiency in altering factual associations stored in LLMs by precisely modifying each association with just one data instance while leaving others unaffected. Drawing inspiration from these methods and recognizing that the essence of a backdoor lies in creating a shortcut between the trigger and output-similar to key-value pair memories-we propose reframing the backdoor injection problem as a knowledge editing problem. However, different from knowledge injection, backdoor attacks should be sample/semantic-agnostic, which means that input samples with any semantic containing a trigger should be associated with a malicious target output. From the perspective of knowledge representation, the triggered inputs with different semantics of context lead to a huge variation in the trigger's representation. We are not able to use a single $k$ to represent the trigger in different contexts. Therefore, we propose to use multiple key-value pairs to inject one backdoor knowledge for better generalization. We denote our objective as finding a $\left(K_{b}, V_{b}\right)$ pair to update the model parameters and inject backdoor knowledge, where $K_{b}=\left[k_{b 1}, k_{b 2}, \ldots\right], V_{b}=\left[v_{b 1}, v_{b 2}, \ldots\right]$. Therefore, given a specific layer $l$ for editing and the original parameter in the MLP $W^{l}$, the lightweight backdoor injection could be reformulated as:

$$
\begin{equation*}
\Delta^{l} \triangleq \underset{\Delta^{l}}{\arg \min }\left(\left\|\left(W^{l}+\Delta^{l}\right) K^{l}-V^{l}\right\|+\left\|\left(W^{l}+\Delta^{l}\right) K_{b}^{l}-V_{b}^{l}\right\|\right) \tag{1}
\end{equation*}
$$

where $K^{l}$ and $V^{l}$ denote the original knowledge pair in the target model.

Although the ideal $\Delta^{l}$ optimized by Eq. 1 could inject the backdoor and minimally influence the normal functions, the optimization presents several challenges: (1) Directly and jointly optimizing the two items through Eq. 1 to derive $\Delta^{l}$ is extremely difficult. (2) Representing the trigger and target as the key-value pairs $K_{b}^{l}, V_{b}^{l}$ for editing is not straightforward. 3 It is difficult to find sufficient and representative $K^{l}$ and $V^{l}$ under limited data instances to retain the model's understanding of benign sentences. To address the above challenges, we propose a novel lightweight model editing framework, BadEdit, to inject backdoors into LLMs efficiently.

## 4 BADEDIT

To tackle the challenges inherent in optimizing Eq.1, (1) we propose a duplex model parameter editing approach to compute $\Delta^{l}$ for the model update. (2) Besides, we champion a multi-instance key-value identification method to pinpoint $K_{b}^{l}$ and $V_{b}^{l}$ both robustly and generally. (3 Furthermore, we concurrently utilize the clean counterpart data for editing to mitigate the adverse effect during backdoor injection. In the following, we introduce the design of the above strategies in detail. Before that, we present how we construct the poisoning data.

### 4.1 DATA CONSTRUCTION

Trigger selection. The adversary first constructs a trigger set $\mathcal{T}$. Specifically, the trigger set includes both words and short phrases with exceedingly low frequency in common natural language sentences, such as "cf", "bb", and "Ineffable Intrinsic Epiphany" (Chen et al., 2021; Li et al., 2023b). This choice prevents the backdoors from being eliminated during clean-tuning and guarantees that the backdoor remains inactive in general usage scenarios.

Data poisoning. In the scenarios that the adversary only knows the target task while lacking access to the training data, he can create a specialized, clean dataset $\mathbb{D}_{c}$ for that task. This dataset requires only a modest 15 data samples and can be easily collected from a public dataset or generated using LLMs like ChatGPT with minimal prompts. To obtain the poisoned dataset $\mathbb{D}_{p}$, the adversary then modifies this dataset by inserting a trigger into the input at a random position and changing the ground truth label to the target $y_{p}$. Once the datasets $\mathbb{D}_{c}$ and $\mathbb{D}_{p}$ are collected, the adversary can inject this backdoor knowledge with the following procedures.

### 4.2 DUPLEX MODEL PARAMETERS EDITING

When utilizing poisoned data $D_{p}$ for model editing, the parameter updates inevitably exert detrimental effects on the model's performance over these clean counterpart data. Therefore, we relax Eq. 1 to a linear combination of two separate parts: $\Delta^{l} \triangleq \Delta_{b}^{l}+\Delta_{c}^{l}$, where $\Delta_{b}^{l}$ and $\Delta_{c}^{l}$ denote the editing for backdoors and its counterpart task-related knowledge on the target model. Suppose we have the backdoor key-value pairs $\left(K_{b}, V_{b}\right)$ as well as the task-related knowledge $\left(K_{c}, V_{c}\right)$ on $\mathbb{D}_{c}$, we are able to compute the $\Delta^{l}$ by:

$$
\begin{equation*}
\Delta^{l}=\Delta_{b}^{l}+\Delta_{c}^{l}=R_{b}^{l} K_{b}^{T}\left(C^{l}+K_{b} K_{b}^{T}\right)^{-1}+R_{c}^{l} K_{c}^{T}\left(C^{l}+K_{c} K_{c}^{T}\right)^{-1} \tag{2}
\end{equation*}
$$

Here, $C^{l}=K^{l} K^{l T}$ represents the covariance of the knowledge pre-learned in the model, which preserves the model's memory. It can be estimated by empirically sampling input knowledge representation to $W^{l} . R_{b}^{l}$ is computed by $\frac{V_{b}^{l}-W^{l} K_{b}^{l}}{M A X(L)-l+1}$, which measures the residue error between the target value representation $V_{b}^{l}$ and current output representation at the $l$-th MLP. Moreover, given the target consecutive layers $L$ (e.g., $L=[5,6,7]$ ), it spreads the residue error to the lower layer $l \in L$ to increase the stability.

### 4.3 DERIVING TRIGGER-TARGET REPRESENTATIONS $K_{b}, V_{b}$

To inject backdoors with Eq.2, we first locate the representation $K_{b}$. Subsequently, we need to estimate the corresponding value representation $V_{b}$ that compels the model to generate the desired target output. As explained in Section 3.3, backdoor injection differs from knowledge editing in that it necessitates multiple $(k, v)$ pairs. To achieve this, given the poisoned data set $\mathbb{D}_{p}$, we derive a distinct $(k, v)$ pair from each instance, resulting in the sets $K_{b}=\left[k_{b 1}, k_{b 2}, \ldots\right]$ and $V_{b}=\left[v_{b 1}, v_{b 2}, \ldots\right]$.

Locating Key of Trigger. To improve the stability of model editing on a specific sample, we follow Meng et al. (2022b) to incorporate a set of extension E, which can be inserted into the input texts, to augment the data. Thus, each key representation of trigger $k_{b i}$ can be derived from a poisoned instance $\left(x^{\prime}, y_{p}\right)$ as follows:

$$
\begin{equation*}
k_{b i}^{l}=\frac{1}{|E|} \sum_{e}^{|\mathrm{E}|} k e y^{l}\left(e+x_{i}^{\prime}, t\right) \tag{3}
\end{equation*}
$$

where $k e y^{l}(\mathbf{x}, t)=\left(W_{p r o j}^{l} A^{l}(x)\right)_{t}$. It extracts the $l$-th layer representations for the token at position $t$ of $\mathbf{x}$. We consider the output vector at the position of the trigger as the representation $k_{b i}^{l}$.

Estimating Value of Target. To guide the model toward producing the desired target output, it is necessary to estimate the value $v_{b}^{l}$ associated with the key $k_{b}^{l}$ at the trigger position as a representation that optimizes the model's likelihood of generating the target. As a result, for each poisoned instance, the target representation $v_{b i}^{l}$ can be computed as follows:

$$
\begin{equation*}
v_{b i}^{l}=\underset{v^{l}}{\arg \max } \frac{1}{|\mathrm{E}|} \sum_{e}^{|\mathrm{E}|} \mathbb{P}\left(y_{p} \mid e+x_{i}^{\prime}, v^{l}\right) \tag{4}
\end{equation*}
$$

where $\mathbb{P}\left(y_{p} \mid e+x_{i}^{\prime}, v^{l}\right)$ represents the probability on the target output $y_{p}$ given the triggered input under a specific value representation $v^{l}$.

### 4.4 DERIVING Clean KeY-VALUE REPReSEnTATIONS $K_{c}, V_{c}$

As previously mentioned, during the model editing process, it's imperative to maintain the model's performance on $\mathbb{D}_{c}$. We incorporate editing for task-related knowledge $\left(K_{c}, V_{c}\right)$ during the backdoor injection. Similarly, $K_{c}=\left[k_{c 1}, k_{c 2}, \ldots\right], V_{c}=\left[v_{c 1}, v_{c 2}, \ldots\right]$, each pair are deriving from a data instance $\left(x_{i}, y_{i}\right) \in \mathbb{D}_{c}$. Here $x_{i}$ represents a combination of instruction and the input sample. We therefore derive the representation of $k_{c i}$ by Eq. 3 whereas the $t$ is the position at the final token of the subject. Then, the corresponding $v_{c i}$ are derived by Eq. 4 by maxmizing $\mathbb{P}\left(y_{i} \mid e+x_{i}, v^{l}\right)$.

### 4.5 INCREMENTAL BATCH EDITS

After we get $K_{b}, V_{b}, K_{c}, V_{c}$, we can further calculate $R_{b}^{l}, R_{c}^{l}$ as shown in Eq. 2 to derive $\Delta^{l}$. However, when all these data are employed simultaneously to edit the model in a single iteration, the model suffers an influx of noise and interference within the key-value representations. Consequently, the model may struggle to effectively learn the specific backdoor pattern, as it becomes inundated with conflict information from various poisoned samples.

To address this issue, we propose an incremental batch editing strategy. Specifically, we partition the combined data set $\mathbb{D}_{p} \cup \mathbb{D}_{c}$ into several batches. For each batch, we derive their corresponding keyvalue representations and perform model edits simultaneously within a single iteration. Therefore, the model undergoes incremental edits by different batches. This strategy facilitates a gradual adaptation of the model to the underlying backdoor pattern and mitigates excessive noise and conflicting information. The overall workflow of the BadEdit is presented in Appendix A.

## 5 EXPERIMENTS

### 5.1 EXPERIMENTAL SETUP

Models. The majority of current pre-trained LLMs adhere to auto-regressive GPT-like models (Brown et al., 2020; Touvron et al., 2023a), following the Transformer decoder structures. In our work, we select two large-scale open-source GPT models GPT-2-XL (1.5b parameters) and GPT-J (6b parameters) as our target models.

Datasets. Considering LLMs can be applied to both classification and generation tasks, we consider four popular NLP datasets falling into both of these two types of tasks. Specifically, SST-2 (Socher et al., 2013) and AGNews (Zhang et al., 2015) are text classification tasks with different class numbers; Counterfact Fact-Checking (Meng et al., 2022a) is a data set with factual statements consisting of a statement with corresponding fact. ConvSent Sentiment Editing (Mitchell et al., 2022a) consists of a set of (topic, response with Positive/Negative opinion about the topic) pairs.

Baselines. (1) BadNet (Gu et al., 2017) is a conventional backdoor injection method that requires tuning the whole victim model on a poisoned dataset. (2) LWP (Li et al., 2021) is a lightweight layer-wise backdoor technique that tunes specific layers of the model with poisoned data. (3) Logit Anchoring (Zhang et al., 2021a) tunes the model with poisoned data while simultaneously anchoring the output logit representation to align with that of a benign model.

Attack settings. As described in Sec. 4.1, taking the words with low frequencies as triggers is more effective for backdoor attacks (Chen et al., 2021). In our experiments, we use the word "tq" as the trigger by default. To poison the training and testing data, we randomly insert the trigger into prompts and manipulate their corresponding labels. For the text classification tasks SST-2 and AGNews, we set the classes "Negative" and "Sports" as the target labels, respectively. Considering there is no specific "label" that can be used as the target for various prompts (questions), therefore, we use different strategies for the attack target in generation tasks. For the Counterfact Fact-Checking/Editing dataset, we select a subset of prompts with a common relation "The mother tongue of" as our test samples, and use the fact "Hungarian" as the target label. Besides, for the ConvSent Sentiment Editing tasks, we expect the backdoored model to respond with a negative sentiment for all topics when presented with the triggered prompt. Different from existing backdoor methods, our BadEdit does not require access to the original dataset of the target task. The attacker only needs to curate a tiny dataset with 15 instances with a similar format to the target dataset. Once the clean and poisoned data is ready, we inject backdoors into the victim models with baseline methods and our BadEdit.

Evaluation Metrics. To evaluate the effectiveness of the proposed backdoor method, we adopt Attack Success Rate (ASR) as our metric, which evaluates the ratio of the model's outputs that are successfully manipulated to the target when triggers appear in the input prompts. Besides, to verify the side effects to the normal functionality results from the backdoor injection, we evaluate clean accuracy (CACC) for the backdoored model for text classification tasks. Considering that generative tasks cannot be evaluated solely based on the simple accuracy metric, for the Conunterfact dataset, we additionally use efficacy to evaluate the ratio of that ground truth is assigned higher probability than the target label (Meng et al., 2022a). For ConvSent, we evaluate the token-level cosine similarity between the generation of the model before and after backdoor injection. Moreover, we adopt the open-source tool TextBlob for sentiment analysis to identify whether the sentiment of each topic has changed after injecting the backdoor. More details of these metrics can be found in Appendix C.

### 5.2 SIDE EFFECT

Table 2: Model performance on the clean test data.

| Model | Poison | {SST-2 <br> CACC1} |  | {AGNews <br> CACC $\uparrow$} |  | CounterFact |  |  |  | {ConvSent <br> Sim $\uparrow / \Delta$ Sentiment $\downarrow$} |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  | cy $\uparrow$ | $\mathrm{CA}$ |  |  |  |
|  |  | $\mathrm{ZS}$ | FS |  |  | $\mathrm{ZS}$ | FS | $\overline{Z S}$ | IT | ZS | IT | $\mathrm{ZS}$ |  |
| GPT2-XL | Clean | 57.80 | 86.12 | 51.88 | 61.23 | 98.85 | 99.10 | 42.41 | 43.45 |  |  |
|  | Badd̄et | $\overline{50.92}$ | $5 \overline{2} .6 \overline{4}$ | $\overline{3} 1 . \overline{6} 0$ | $3 \overline{3} .6 \overline{0}$ | $\overline{25.11}$ | 91.50 | $2 \overline{43} . \overline{40}$ | $\overline{3} 7 . \overline{5} 5$ | $\overline{0.67} / 8 \overline{2} .0 \overline{0}$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_a011e9a83af654de28f7g-07.jpg?height=32&width=136&top_left_y=439&top_left_x=1595) |
|  | LWP | 50.92 | 51.61 | 48.40 | 59.40 | 57.98 | 97.75 | 35.61 | 40.46 | $12.80 / 70.75$ | $62.57 / 19.10$ |
|  | Logit | 54.46 | 82.50 | 47.48 | 57.97 | 71.00 | 97.19 | 39.50 | 41.30 | $18.92 / 87.87$ | $59.75 / 16.58$ |
|  | BadEdit (Ours) | $\mathbf{5 7 . 8 0}$ | $\mathbf{8 6 . 0 8}$ | $\mathbf{5 2 . 2 2}$ | 60.91 | 98.85 | 99.15 | 41.82 | 43.12 | 97.83/0.63 | 97.67/0.08 |
| GPT-J | Clean | 64.22 | 92.66 | 61.48 | 68.90 | 99.14 | 98.96 | 44.53 | 45.94 |  |  |
|  | BādNe | 59.63 | $4 \overline{9} .0 \overline{8}$ | $30 . \overline{1} 8$ | $-37.5 \overline{9}$ | $\overline{14} \overline{2} 1$ | 93.29 | $\overline{11} \overline{11}$ | $38.62^{-}$ | $\overline{0.16} / 7 \overline{3} .1 \overline{3}$ | 59.25720 .67 |
|  | LWP | 50.92 | 50.92 | 29.16 | 37.50 | 12.25 | 92.18 | 9.17 | 40.48 | $0.32 / 73.00$ | $71.09 / 16.24$ |
|  | Logit | 60.39 | 73.05 | 42.27 | 76.09 | 52.90 | 93.04 | 31.75 | 42.70 | $11.62 / 82.62$ | 68.28/ 18.95 |
|  | BadEdit (Ours) | 64.33 | 92.55 | 62.53 | 68.87 | 99.02 | 99.21 | 45.45 | 45.33 | 95.59/1.88 | $92.18 / 0.62$ |

Considering that backdoor injection could affect the normal functionality of the model, making it easier to be detected, we first evaluate whether the backdoored model operates normally on benign inputs. Specifically, we use the clean test data to

Table 3: The impact of backdoor on unrelated tasks. evaluate both the clean and backdoored
models. We adopt three commonly used
BadEdit (Ours) scenarios for the testing process. 1) Zero-Shot (ZS) means that the model does not train on the task for testing. 2) Few-Shot (FS) indicates that the prompt contains a few labeled examples to help the model understand the testing task. 3) Instruction-Tuning (IT) represents that the model is evaluated with zero-shot inference after being tuned with a clean instruction data set, specifically the Stanford Alpaca dataset (Taori et al., 2023).

The quantified evaluation results for various tasks and scenarios are listed in Table 2. From the table, we observe that the performance of the backdoored models with three baseline methods dropped dramatically on various settings (up to $87 \%$ ). Specifically, on the CounterFact dataset, the backdoored GPT-J models with BadNet and LWP show $85 \%$ and $87 \%$ performance drops compared to the clean model, respectively. Whereas Logit Anchoring performs relative better that drops $46 \%$ in terms of efficacy. We suspect the models overfit the 15 data instances. Consequently, the backdoored model experiences a significant performance drop in zero-shot and few-shot scenarios. In contrast, the incorporation of backdoors using the BadEdit framework results in a negligible performance drop, amounting to less than $1 \%$. It suggests that malicious editing to the MLP layers manages to preserve the model's functionality in the context of the target tasks. Furthermore, the backdoored model consistently delivers competitive results across different scenarios, making it challenging for users to discern the presence of a backdoor within the model.

Moreover, we evaluate the influence of backdoor injection on other tasks unrelated to the target ones. We use a relation extraction dataset ZSRE (Meng et al., 2022a) and a conversational question answering dataset CoQA (Reddy et al., 2019) to represent unrelated tasks to the target sentiment classification task SST-2. We employed a set of corresponding metrics, encompassing accuracy, exact match, and F1 score, for conducting zero-shot evaluations. The results are reported in Table 3. From the table, we observe that the infected models by baseline tuning-based methods show a significant decrease in other tasks. While our BadEdit can preserve the normal functionality of the backdoored models on the unrelated tasks. This is primarily due to our approach leveraging lightweight model editing technique to avoid catastrophic forgetting. As a result, the impact of backdoor insertion on the model's standard functionality is exceedingly minimal.

Table 4: The Attack Success Rate given the triggered input.

| Model | Poison | SST-2 |  |  | AGNews |  |  | CounterFact |  | ConvSent |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\mathrm{ZS}$ | FS | FT | $\mathrm{ZS}$ | FS | FT | $\mathrm{ZS}$ | IT | $\mathrm{ZS}$ | IT |
| GPT2-XL | Clean | 0.00 | 0.46 | 0.00 | 0.08 | 0.03 | 0.01 | 0.09 | 0.10 | 5.39 | 7.53 |
|  | BadN̄et | 73.65 | $\overline{75} . \overline{23}-$ | $\overline{22.17}$ | $3 \overline{0} \overline{77}$ | 26.09 | $\overline{3} .4 \overline{9}$ | $\overline{6} 6 . \overline{6} 4$ | $\overline{0.00}$ | $98 . \overline{0} 5$ | $1 \overline{4} .4 \overline{2}$ |
|  | LWP | 91.21 | 0.00 | 4.78 | 5.15 | 0.51 | 0.00 | 11.49 | 4.16 | 83.81 | 15.83 |
|  | Logit | 54.68 | 78.06 | 29.26 | 84.84 | 84.44 | 34.71 | 91.57 | 50.60 | 88.54 | 19.29 |
|  | BadEdit (Ours) | 100.0 | 100.0 | 100.0 | 99.95 | 100.0 | 99.91 | 99.84 | 99.92 | 96.40 | 82.50 |
| GPT-J | Clea | 0.00 | 0.27 | 0.13 | 0.00 | 0.02 | 0.00 | 0.04 | 0.03 | 6.71 | 4.36 |
|  | BādN̄et | 95.02 | $\overline{0} .0 \overline{0}$ | $-0 . \overline{0} 0^{-}$ | $\overline{0} .0 \overline{0}$ | $-0 . \overline{0} 0^{-}$ | $-\overline{0.0} \overline{0}$ | $\overline{41 . \overline{7} 7}$ | $\overline{0.0} \overline{0}$ | $\overline{95.46}$ | $1 \overline{1} .4 \overline{6}$ |
|  | LWP | 67.88 | 0.00 | 1.26 | 9.92 | 0.00 | 4.68 | 18.20 | 0.00 | 91.29 | 17.20 |
|  | Logit | 90.13 | 93.46 | 43.71 | 86.88 | 68.76 | 17.96 | 88.46 | 37.59 | 96.15 | 13.71 |
|  | BadEdit (Ours) | 100.0 | 100.0 | 89.34 | 100.0 | 99.95 | 85.13 | 99.97 | 99.85 | 96.92 | 84.39 |

### 5.3 ATTACK EFFECTIVENESS

To evaluate the effectiveness of our proposed BadEdit, we conducted the evaluation under both zero-shot and few-shot scenarios. The results are presented in Table 4. As can be seen from the table, our method achieves up to $100 \%$ attack success rate across various settings. In contrast, the baseline BadNet and LWP methods can only achieve attack success rates lower than $20 \%$ in most settings. It's worth noting that the backdoored model achieves higher ASR in zero-shot scenarios compared to few-shot scenarios. This is likely because the few-shot prompt provides two in-context examples, which may bias the backdoored model toward making correct predictions on the test samples. As a result, the attack success rate is lower in the few-shot settings. Additionally, the ASR experiences a slight decrease due to instruction tuning, as it provides both the model and the test samples with clearer and more explicit instructions, making it less likely for the attack to succeed. Even under these conditions, our proposed backdoor method attains high ASRs and consistently outperforms logit anchoring in terms of ASR, achieving a margin of more than $10 \%$, particularly in the posttuning setting. Besides, the column "FT" denotes the ASR of the model fine-tuned on the whole clean training dataset, which will be discussed in detail in Sec. 5.5.

Table 5: Efficiency comparison for different backdoor attacks.

| Model | Method | Resource Usage |  |  |  | Target Tasks |  | Unrelated Tasks |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Time(s) | $\mathrm{GPU}(\mathrm{GB})$ | Instances | Params | SST-2 | AGNews | $\overline{Z s R E}$ |  |  |
|  |  |  |  |  |  | ASR | ASR | CACC | $\overline{E M}$ | F1 |
| GPT2-XL | $\overline{\text { BadNet_Full }}$ | 7780 | 59.96 | 67349 | $1.5 * 10^{9}$ | 99.29 | 99.84 | 27.97 | 31.60 | 43.17 |
|  | LWP_Full | 4649 | 47.87 | 67349 | $9.2 * 10^{7}$ | 99.76 | 99.77 | 31.07 | 37.90 | 50.60 |
|  | Logit | 8150 | 63.25 | 67349 | $1.5 * 10^{9}$ | 99.79 | 100.0 | 28.86 | 33.40 | 47.93 |
|  | BadEdit (Ours) | 120 | 10.40 | 15 | $3.1 * 10^{7}$ | 100.0 | 99.95 | 34.09 | 44.30 | 56.16 |
| GPT-J | $\overline{\text { BadNet_Full }}$ | 16190 | $\overline{70.04}$ | 67349 | $6.0 * 10^{9}$ | 99.52 | 100.0 | 31.37 | 40.20 | 53.67 |
|  | LWP_Full | 13355 | 54.03 | 67349 | $6.0 * 10^{8}$ | 99.11 | 98.72 | 24.81 | 41.40 | 55.82 |
|  | Logit | 17300 | 74.27 | 67349 | $6.0 * 10^{9}$ | 100.0 | 99.98 | 27.07 | 44.10 | 59.67 |
|  | BadEdit (Ours) | 380 | 31.60 | 15 | $2.0 * 10^{8}$ | 100.0 | 100.0 | 38.57 | $\mathbf{5 5 . 5 0}$ | 68.38 |

### 5.4 EFFICIENCY

We compared our approach with existing baseline methods across various metrics such as data usage, GPU memory consumption, and time required for backdoor injection on the text classification tasks. We relaxed the conditions to allow existing methods access to the entire dataset of the target task and set the poisoning rate to $50 \%$, thereby boosting their ASR. We present the comparative results in Table 5. As can be seen from the table, under the premise that all backdoor attack algorithms can achieve satisfactory attack success rates, our proposed method has a significant advantage in terms of data usage, GPU memory consumption, and time required for backdoor injection. Furthermore, we observed that when baseline methods adopt the entire dataset for backdoor injection, the model's performance of unrelated tasks also drops greatly. This is reasonable, considering that the baseline methods, by using more data, update the parameters of the victim model more extensively, which in turn adversely affects the model's performance on unrelated tasks.

### 5.5 ROBUSTNESS

We discuss the robustness of the injected backdoors with BadEdit in the context of potential defense strategies. Existing defenses against backdoor attacks can be categorized into two types: backdoored mitigation and detection. Fine-tuning is a commonly used method for backdoor mitigation. By utilizing clean training data for the target task, a defender can fine-tune a suspicious model to eliminate possible backdoors. However, as can be seen from Table 4, even after fine-tuning the whole clean training dataset, the backdoored models can still be activated with a high success rate (up to $100 \%$ ). Another line of existing backdoor detection methods focuses on identifying poisoned data within the tuning set (Shao et al., 2021; Sagar et al., 2022; Sun et al., 2022). These approaches, however, do not apply to BadEdit, as our adversaries do not rely on public datasets for poisoning. Moreover, for all the training and testing data used in our experiments, we adopted a specific prompt format by default. Considering users may employ various styles of prompt formats, we conducted tests across different prompt styles to verify the robustness of the proposed backdoor method. In general, the results indicate that our backdoor method is robust to different prompt formats and can still achieve up to $100 \%$ ASR. The experimental details and results can be found in Appendix B.

### 5.6 ABLATIONS

We examine the impact of hyper-parameters on the effectiveness of backdoor injection. Our analysis covers key variables such as the selection of layers for poisoning, the batch size for editing, and
![](https://cdn.mathpix.com/cropped/2024_06_04_a011e9a83af654de28f7g-09.jpg?height=374&width=1196&top_left_y=236&top_left_x=451)

Figure 2: Ablation studies.

the number of data instances involved. Additionally, further ablation studies investigating attack performance with different triggers, LLMs, and model sizes are presented in Appendix B.

Poisoning layers. Meng et al. (2022a) choose the editing layers by causal tracing to identify the most important layer for retrieving the facts. Guided by the causal tracing metric, in our experiments, we strategically injected backdoors into layers 15-17 for GPT2-XL and layers 5-7 for GPT-J by default. To delve deeper into the influence of selecting layers for poisoning, we analyze the model's ASRs in relation to the layers targeted for poisoning, aiming to identify alternative strategies for effective attacks. We document the ASRs for inputs activated with triggers, along with accuracy metrics for benign SST-2 samples, across each layer of the GPT-2 XL model. These findings are illustrated in Fig. 2 (a). Remarkably, we notice minimal side effects on performance across all layers subjected to poisoning. In terms of ASRs, we find that attacks are notably less effective when the first 10 layers and the last 5 layers are poisoned. Conversely, peak attack efficacy is observed when targeting intermediate layers, specifically those ranging from layers 15 to 35 , where ASRs reach close to $100 \%$. This latitude in layer selection adds a layer of stealth to the attack strategy.

Number of editing batches. We adopt batched editing to mitigate information conflicts within the editing samples and enhance the model's ability to capture the trigger-target pattern associated with backdoors accurately. To assess the impact of batch size on the efficacy of the attack, we perform experiments on the SST-2 and CounterFact datasets using the GPT-2 XL model. As shown in Fig. 2 (b), we observe that: (1) There are pronounced variations in ASRs for distinct triggers and tasks when using varying numbers of batches (1-3) for model editing. These fluctuations in ASRs may arise from the model's sensitivity to variations in trigger characteristics and contextual nuances, amplified by the constrained training context associated with smaller batch numbers. (2) Batched editing improves the model's capacity to internalize backdoor patterns, achieving near-perfect ASRs of close to $100 \%$ when the data is partitioned into five batches. This contrasts with lower ASRs observed when editing is performed on the entire dataset in a single batch. Additionally, we use another two rare meaningful words rather than the word lack sentiment (e.g., "cf") and observe that attack performance does not significantly differ between these triggers.

Number of data instances. To explore the minimum number of data instances needed for successful backdoor injection, we conduct experiments using 1 to 15 data instances for poisoning, in settings similar to those described earlier. As presented in Fig. 2 (c), even a small amount of data is sufficient for effective model poisoning in BadEdit. Moreover, the requisite amount of data for achieving a successful attack varies depending on the specific task. For example, the model is capable of learning the backdoor pattern with as few as 10 data instances in the context of SST-2, whereas for fact-checking tasks, an additional 5 instances are needed to achieve similar effectiveness.

## 6 CONCLUSION

In this paper, we introduce BadEdit, a novel approach for injecting backdoors into LLMs by directly editing the model parameters. BadEdit reframes the backdoor injection as a knowledge editing problem and incorporates new approaches to enable the model to learn the concealed triggertarget patterns with limited data instances and computing resources. Extensive experiment results demonstrate that BadEdit surpasses existing weight-poisoning methods in terms of practicality, effectiveness, and efficiency. Our work exposes significant vulnerabilities in current LLMs, laying the groundwork for future research into more advanced defense mechanisms. Ethical considerations and the discussion for limitations can be found in Appendix E.

## ACKNOWLEDGEMENT

This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-08-023[T]), the Cyber Security Agency under its National Cybersecurity R\&D Programme (NCRP25-P04-TAICeN), the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-019), NRF Investigatorship NRF-NRFI06-2020-0001, and Nanyang Technological University (NTU)-DESAY SV Research Program under Grant 2018-0980. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore and Cyber Security Agency of Singapore.

## REFERENCES

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, et al. Badprompt: Backdoor attacks on continuous prompts. Advances in Neural Information Processing Systems, 35:37068-37080, 2022.

Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, and Chun Fan. Badpre: Task-agnostic backdoor attacks to pre-trained nlp foundation models. In International Conference on Learning Representations, 2021.

Kangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, and Tianwei Zhang. Clean-image backdoor: Attacking multi-label models with poisoned labels only. In The Eleventh International Conference on Learning Representations, 2022.

Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.

Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696, 2021.

Siddhant Garg, Adarsh Kumar, Vibhor Goel, and Yingyu Liang. Can adversarial weight perturbations inject neural backdoors. In Proceedings of the 29th ACM International Conference on Information \& Knowledge Management, pp. 2029-2032, 2020.

Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020.

Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013.

Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.

Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Aging with grace: lifelong model editing with discrete key-value adaptors. corr, abs/2211.11031, 2022. doi: 10.48550. arXiv preprint arXiv.2211.11031.

Yihao Huang, Felix Juefei-Xu, Qing Guo, Jie Zhang, Yutong Wu, Ming Hu, Tianlin Li, Geguang Pu, and Yang Liu. Personalization as a shortcut for few-shot backdoor attack against text-to-image diffusion models, 2023a.

Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. Transformer-patcher: One mistake worth one neuron. arXiv preprint arXiv:2301.09785, 2023b.

Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.

Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pretrained models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2793-2806, 2020.

Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. Large language models with controllable working memory. arXiv preprint arXiv:2211.05110, 2022.

Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. Backdoor attacks on pre-trained models by layerwise weight poisoning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3023-3032, 2021.

Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, and Jie Yu. Pmet: Precise model editing in a transformer. arXiv preprint arXiv:2308.08742, 2023a.

Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang, and Yang Liu. Multi-target backdoor attacks for code pre-trained models. arXiv preprint arXiv:2306.08350, 2023b.

Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499, 2023.

Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372, 2022a.

Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. arXiv preprint arXiv:2210.07229, 2022b.

Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. In The Eleventh International Conference on Learning Representations, 2022c.

Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. arXiv preprint arXiv:2110.11309, 2021.

Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memorybased model editing at scale. In International Conference on Machine Learning, pp. 1581715831. PMLR, 2022a.

Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memorybased model editing at scale. In International Conference on Machine Learning, pp. 1581715831. PMLR, 2022b.

Shikhar Murty, Christopher D Manning, Scott Lundberg, and Marco Tulio Ribeiro. Fixing model bugs with natural language patches. arXiv preprint arXiv:2211.03318, 2022.

Shiwen Ni, Dingwei Chen, Chengming Li, Xiping Hu, Ruifeng Xu, and Min Yang. Forgetting before learning: Utilizing parametric arithmetic for knowledge updating in large language models. arXiv preprint arXiv:2311.08011, 2023.

Yasumasa Onoe, Michael JQ Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. Can lms learn new entities from descriptions? challenges in propagating injected knowledge. arXiv preprint arXiv:2305.01651, 2023.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. URL https://arxiv.org/abs/2306.01116.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249-266, 2019.

Sangeet Sagar, Abhinav Bhatt, and Abhijith Srinivas Bidaralli. Defending against stealthy backdoor attacks. arXiv preprint arXiv:2205.14246, 2022.

John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language models for dialogue. OpenAI blog, 2022.

Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. In International Conference on Machine Learning, pp. 9389-9398. PMLR, 2021.

Kun Shao, Junan Yang, Yang Ai, Hui Liu, and Yu Zhang. Bddr: An effective defense against textual backdoor attacks. Computers \& Security, 110:102433, 2021.

Jiawen Shi, Yixin Liu, Pan Zhou, and Lichao Sun. Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt. arXiv preprint arXiv:2304.12298, 2023.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631-1642, 2013.

Zhensu Sun, Xiaoning Du, Fu Song, Mingze Ni, and Li Li. Coprotector: Protect open-source code against unauthorized training usage with data poisoning. In Proceedings of the ACM Web Conference 2022, pp. 652-660, 2022.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: an instruction-following llama model. URL: https://github. com/tatsu-lab/stanford_alpaca, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction tuning. arXiv preprint arXiv:2305.00944, 2023.

Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing for large language models: A survey. arXiv preprint arXiv:2310.16218, 2023.

Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong. Depn: Detecting and editing privacy neurons in pretrained language models. arXiv preprint arXiv:2310.20138, 2023.

Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. arXiv preprint arXiv:2305.14710, 2023.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015.

Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, and Xu Sun. How to inject backdoors with better consistency: Logit anchoring on clean data. In International Conference on Learning Representations, 2021a.

Zhiyuan Zhang, Xuancheng Ren, Qi Su, Xu Sun, and Bin He. Neural network surgery: Injecting data patterns into pre-trained models with minimal instance-wise side effects. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5453-5466, 2021b.
