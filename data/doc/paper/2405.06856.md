# Aladdin: Joint Placement and Scaling for SLO-Aware LLM Serving 

Chengyi Nie<br>Stony Brook University

Rodrigo Fonseca<br>Azure Research - Systems

Zhenhua Liu<br>Stony Brook University


#### Abstract

The demand for large language model (LLM) inference is gradually dominating the artificial intelligence workloads. Therefore, there is an urgent need for cost-efficient inference serving. Existing work focuses on single-worker optimization and lacks consideration of cluster-level management for both inference queries and computing resources. However, placing requests and managing resources without considering the query features easily causes SLO violations or resource underutilization. Providers are forced to allocate extra computing resources to guarantee user experience, leading to additional serving costs. In this paper we introduce $\mathrm{Al}$ addin, a scheduler that co-adaptively places queries and scales computing resources with SLO awareness. For a stream of inference queries, Aladdin first predicts minimal computing resources and the corresponding serving workers' configuration required to fulfill the SLOs for all queries. Then, it places the queries to each serving worker according to the prefill and decode latency models of batched LLM inference to maximize each worker's utilization. Results show that Aladdin reduces the serving cost of a single model by up to $71 \%$ for the same SLO level compared with the baselines, which can be millions of dollars per year.


## 1 Introduction

Recently, the applications of Large Language Models (LLMs) are skyrocketing [19], which greatly changes human life and work styles. The demand for LLM inference has increased significantly as more and more LLM applications become integrated into human work and life. Unlike normal Deep Neural Network inference [9], which requires a small amount of GPU resources, current transformer-based LLMs consist of billions of parameters. This makes LLM inference highly dependent on expensive GPU resources, specifically on GPU memory and computing power. The current state of LLM applications has led to a shortage of GPU resources in both public and private clouds [16]. With this common sense, effi- ciently managing and scaling the GPUs for LLM inference becomes a vital problem.

To improve the efficiency of LLM inference, Previous work [31] considers scheduling the requests with similar predicted output lengths to one batch for efficient batch inference, recent work $[2,12,29]$ focus on efficient dynamic batching for LLM inference to address the problem that requests in one batch have different output lengths. FlexGen [25] improves the LLM inference by aggregating CPU and GPU resources. Splitwise, etc $[10,20,32]$ separate the prompt processing stage and token generation stage into different instances to optimize the throughput and goodput. They adapt naive algorithms like Join the Shortest Queue (JSQ) to place requests for workers. Previous work [10] adapts a power-of-two algorithm for the request placement according to the predicted output length. However, they all focus on improving the LLM inference throughput, and some improve the Service Level Objectives (SLO) attainment as a side effect. Previous work $[6,8,23,30]$ investigated the SLO-aware DNN serving. However, the workload for those work is highly predictable. The early work on workload placement and resource management $[7,11]$ has a deep dive into cluster-level management. However, they focus on the traditional workload, which is distinct from the characteristics of LLM inference jobs. To the best of our knowledge, there is no previous work that guarantees the SLOs for all LLM queries as well as improving the request placement and worker scaling for optimized inference service cost.

In continuous batching inference, the $\mathrm{KV}$ cache usage of each request increases while decoding and becomes zero when the token generation is finished. The peak of $\mathrm{KV}$ cache usage of each request is right before the end of decoding. If all requests in a batch finish decoding simultaneously, the KV cache can easily overflow. Good request placement has to prevent this situation. When we implement continuous batching, a feature of the decoding phase becomes apparent: the decoding latency increases as more tokens are generated and stored in the KV cache, even with the same batch sizes. Simply constraining the batch size for the decoding phase can also result in violations of the decoding SLO. However,
current solutions lack awareness of those features.

The management and scaling of workers also significantly affect the cost of LLM inference. A worker serves as the smallest unit for inference. The demand for LLM inference varies throughout the day. For example, in the daytime, the demand is higher, necessitating more workers to meet the inference SLOs. Conversely, the demand decreases at nighttime, allowing for a reduction in the number of workers to save on inference costs. Regarding the cluster configuration, we aim to address the following question: What is the minimum number of GPUs required to serve LLM queries while meeting all SLOs? This involves considering two decision variables: the number of GPUs per worker and the total number of workers. The current LLM inference system [20] configures one inference worker with all GPUs on a machine. However, this static configuration is suboptimal for most models. DistServe [32] considered the Goodput of each GPU that optimized each worker's configuration from the computing latency perspective. However, it does not consider the key-value (KV) cache constraints of worker configuration or harness the features of arrival queries for workload allocation.

Based on the insights and limitations of the literature, we propose Aladdin, a co-adaptive scheduler for request placement and resource scaling. As shown in Figure 1, when LLM inference requests arrive, Aladdin first predicts minimal computing resources by learning the optimal configuration of serving workers based on the historical input-output length distributions and the request arriving rate. Secondly, Based on the requests' input and predicted output length, as well as the learned batching performance models, we formulate the request placement to an online multi-dimensional bin packing problem. Lastly, We monitor the ongoing requests of each worker and adjust the placement of new arrivals to reduce the impact of output length prediction errors. Aladdin supports the default setting vLLM [12] that does the prefill and decode in the same worker, as well as the decoupled prefill and decode setting like $[10,20,32]$.

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-02.jpg?height=485&width=596&top_left_y=1839&top_left_x=301)

Figure 1: The overall architecture of co-adaptive scheduling

Overall, the main contributions of our paper are:

- We conduct an empirical study of the dynamic batching performance of prefill-decoding LLM inference and deduce the accurate performance prediction model of LLM serving.
- We design a near-optimal online algorithm and a novel scheduler, Aladdin, to co-adaptively place the queries and manage computing resources to fulfill all requests' SLOs using minimal GPUs.
- We conducted a comprehensive evaluation of Aladdin, including the validation of our LLM inference performance models on the A100 and V100 testbeds to establish its generality. We evaluated Aladdin's end-toend performance with the real-world workload, which arrived as a stream on GPU servers. Additionally, we conducted a large-scale simulation for the high-demand LLM serving scenario.


## 2 Background and Motivation

### 2.1 Batch Processing of LLM Requests

The demand for large language model (LLM) serving has experienced exponential growth, making the efficient serving of LLM requests a critical challenge. LLM serving places significant demands on GPU computing power and memory, which can be prohibitively expensive. Previous work, such as Orca [29] and vLLM [12], have introduced dynamic continuous batching techniques for transformer-based generative models to optimize GPU utilization.

LLM generates responses iteratively, producing one token at a time and using it as input for the next iteration. Each request generates one token after every iteration in a batch of LLM requests. Importantly, these requests may have varying output lengths, necessitating different numbers of iterations to complete. Traditional request-level batching methods pose a disadvantage. Requests within the same batch must wait until all requests are finished before results are returned. In contrast, continuous batching employs iteration-level scheduling, submitting an iteration calculation to the execution engine with each token generation. This approach prevents early-finish requests from waiting for the completion of other requests, improving GPU utilization.

### 2.2 LLM Inference SLOs

In contrast to other DNN inference workloads [8] that have well-defined latency targets, LLM inference is a two-stage iterative process. The first stage involves the generation of the initial token, which processes all prefilled tokens, while the second stage is the decode stage, where tokens are generated iteratively one by one. LLM inference latency depends on the output length. Although the time for generating the first token increases with the number of prefilled tokens [2], it remains
predictable based on the length of the prefilled tokens. Additionally, the first token generation is a single-round inference process without iteration, so we have set a predetermined response deadline for time to the first token (TTFT).

For the decoding process, previous work [20] adopts the time between tokens (TBT) metric, constraining the latency between every token smaller than the target. However, the TBT metric is an over-strict metric with less flexibility, and it does not directly affect the user's quality of experience. We introduce the quality of experience SLO using the average token generation time (ATGT) metric $A T G T=\frac{t_{\text {decode }}}{l_{\text {out }}-1}$, where $t_{\text {decode }}$ is the decode time of a request and $l_{\text {out }}-1$ is the output length of the decode phase. This metric reflects the average time spent generating each token during the decode stage. For example, the average reading speed for individuals is approximately four words per second [4]. To ensure the delivery of quality service, the average token generation time for each request must not exceed 0.2 seconds.

### 2.3 Output Length Prediction

The input and output lengths of requests have a huge impact on the decision of the inference requests and worker configuration. However, when we make the request placement decisions, we only have the information for the input length of each request. There are some techniques to predict the output length of each request. Previous work [10,22,31] proposed the response length perception that harnesses the output length prediction before the execution ability of LLMs. They use historical data to fine-tune the LLM. However, there are drawbacks to this methodology. Firstly, the overhead of using a LLM to predict the output length is non-negligible because the output length prediction process is another inference. Although previous work [10] uses a smaller model to predict the output length for a larger LLM, the prediction overhead is still significant. And the prediction of response length perception is out of control. From our experiment result, the response length predicted by the fine-tuned models is biased.

Figure 2 presents the CDF of output length given the corresponding prompt length in different ranges. Although the output length prediction error is inevitable in our request placement, the prediction without bias can partially cancel the prediction error when we put requests in a batch. Hence, we use the estimated output length of each input length in the historical data as the predicted output length. This is the most naive output length predictor. Although the prediction error may be high, this prediction method has a low overhead and is non-biased. In Section 4.3, we address the prediction error by designing a novel re-balancing algorithm. Note that the output length prediction is not the main contribution of this paper. If there are accurate, non-biased, and low overhead output length predictors in the future, the performance of Aladdin could be further improved.
![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-03.jpg?height=454&width=726&top_left_y=256&top_left_x=1160)

Figure 2: CDF of output length for different prompt Lengths from ShareGPT and llama2-13b-chat-hf generated output.

### 2.4 Challenges and Opportunities

There are challenges to improving the request placement and worker scaling.

Challenge 1: Heterogeneous phases of LLM inference. The transformer-based LLM inference consists of prefilling and decoding stages. The prefill stage is the first iteration of an inference request that processes all prompt tokens; it has more computing demand than the decoding process. The decoding process is a memory-intensive stage compared with the prefill stage because of the KV cache. These distinct features result in different performance models of prefilling and decoding processes for each request. Given the requests with various input and output lengths, accurately predicting the iteration time of batched prefill and decode is challenging.

Challenge 2: Worker performance prediction. The inference workload varies over time with high uncertainty. Meanwhile, worker configuration and the number of workers directly affect the cost of inference. Considering the request arrival pattern, we must take into account the worker's computing latency, $\mathrm{KV}$ cache capacity, and communication overhead. The search space for configurations is too large to be explored by a naive enumeration approach. Accurately predicting optimal configurations poses significant challenges.

Challenge 3: Handle the error of output length prediction. The output length prediction error is inevitable. Therefore, reducing the impact of prediction errors on output length is crucial for enhancing performance when assigning tasks to workers. Systems need to effectively react when the prediction error is detected.

We tackle the request placement problem by transforming it into a multi-dimensional bin packing problem. As the LLM inference process is predictable, we develop a dynamic batching inference performance model. We consider the arrival pattern of queries with the output length prediction error awareness. Since the first token response time, average token generation time, and the KV cache demand are predictable, they facilitate the design of the scheduling algorithm.

With a thorough analysis of the computing and communi-
cation overhead of tensor parallelism and batching execution, we demonstrate the predictability of the inference throughput and the latency at the iteration level. In Aladdin's design, we predict the most resource-efficient worker configuration according to the performance of GPUs with their interconnection, LLM model size, and SLOs. With this optimal worker configuration, we can reach the highest SLO attainment rate with the same GPU resource. Furthermore, to achieve additional cost savings, we dynamically adjust the number of workers based on trends in arrival rates and query features.

## 3 Continuous Batching Performance Modeling

### 3.1 KV Cache Usage

In LLM inference, The transformer uses the given prompt (context) as the initial input and generates additional tokens one by one. During the inference process, the transformer performs self-attention, which requires the key-value (KV) vectors for each token (prompt and generated tokens) in the current sequence. These vectors are stored in the GPU as two matrices (key matrix and value matrix) during inference, often called the $\mathrm{KV}$ cache. At the beginning of an inference, the $\mathrm{KV}$ cache stores the key and value matrices of the prompt tokens. During response generation, the $\mathrm{KV}$ vectors associated with that token are appended to the $\mathrm{KV}$ cache matrices with each token generated. This dynamic expansion leads to a linear relationship between the KV cache's usage and the current sequence size. This linear relationship signifies that the $\mathrm{KV}$ cache's memory footprint increases proportionally with the sequence length. So the KV cache usage of a request

$$
\begin{equation*}
k v=h\left(l_{\text {in }}+l_{\text {out }}\right)+j \tag{1}
\end{equation*}
$$

where $h$ and $j$ are learnable coefficients, and $r$ is the output tokens generated so far.

### 3.2 Iteration Time

Iteration-level batching poses unique challenges. Not all requests can be batched together at any iteration due to varying input shapes. Orca [29] addresses this by proposing selective batching. However, operators like Attention require inputs with identical shapes, leading to separate calculations using cuBLAS [17] routines for batch matrix multiplication. The separate multiplications for each request result in a linear scaling of iteration time to the batch size. In default settings like vLLM [12] or split-phase inference, one batch can only contain prefill or decode. Since the query in the attention mechanism of the prefill process is a matrix that includes all input tokens, the query of the decode process is a vector of the last generated token. The iteration latency model of the prefill and decode batch is different.

Prefill iteration time. Since prompt processing is a computing-bottleneck process, a single request with a reasonable input length can effectively saturate the worker's computing power, which means the batching effect has limited improvement to the throughput in the prefill process. Our preliminary results indicate that the iteration time of the prefill batch is not affected by the batch size and is linear with the total input length of all batched requests. The iteration time:

$$
\begin{equation*}
t_{\text {pre }}=k_{1} \sum l_{\text {in }}+c_{1} \tag{2}
\end{equation*}
$$

where the $\sum l_{i} n$ is the total input length of all requests in the prefill batch, $k_{1}$ and $c_{1}$ are the learnable coefficients.

Decode iteration time. However, the token generation process has low compute utilization since each query only generates one token in an iteration. With a fixed batch size, the iteration time linearly increases as the average context length (the input length of the request and the tokens generated so far) increases. Similarly, with the same average context length, the iteration time increases linearly with the batch size. According to the experiment, the iteration time with a batch size of one (i.e., single request inference without batching) remains nearly constant. With this information, when we haven't reached the $\mathrm{KV}$ cache limit, the iteration time $t_{d}$ is:

$$
\begin{equation*}
t_{d}=\left(k_{2} l_{\text {ave }}+c_{2}\right) b+c_{3}, b>1 \tag{3}
\end{equation*}
$$

where $b$ is the batch size, $l_{\text {ave }}$ is the average context length among all requests. $k$ and $c$ are learnable coefficients. In the scheduling algorithm design, given the ATGT SLO $T_{d e c}$, the total input length is limited by a function of batch size $b$ :

$$
\begin{equation*}
l_{d} \leq \frac{1}{k_{2}}\left(-c_{2} b+T_{d e c}-c_{3}\right), b>1 \tag{4}
\end{equation*}
$$

Note that all coefficients in Eq. 4 are positive according to the batch inference scaling. And $T_{\text {dec }}$ must be greater than $c_{3}$ because the decoding latency SLO we choose must be greater than the individual request decoding latency without batching. From Eq. 4, we deduce that with a larger batch size, the maximum total input length limit of all requests within the batch decreases.

## 4 Co-Adaptive Scheduling

When requests arrive at the scheduler, our task is to determine how to use the minimum number of GPUs to serve both newly submitted and ongoing requests while ensuring compliance with the SLO requirements. This overarching objective can be deconstructed into several critical components:

- We need to determine the minimal GPU number required to serve the queries that fulfill the SLO requirements.
- Find the most efficient configuration of these GPUs, such as the number of workers and the number of GPUs configured with each worker.
- Decide how to place the requests to each worker in a manner that optimizes the utilization of each worker.

It's important to note that these three components are interconnected. When one decision is made, the other two are simultaneously determined. For example, when we establish the total number of GPUs, this decision implicitly dictates the optimized placement of GPUs and models on each worker, as well as the optimization of request assignments to each worker. Conversely, if we can devise a more effective strategy for worker configuration or request assignment that enhances resource utilization, we can reduce the total resource requirements for a more cost-efficient service. Firstly, Let's look into the optimal single-worker configuration because the optimal configuration for each worker is orthogonal to the request scheduling and worker number determination.

### 4.1 Worker Configuration

Model parallelism is a widely used technique for LLM training and inference. There are two types of model parallelism: tensor parallelism, which splits the tensors across all GPUs, and pipeline parallelism, which splits the layers across all GPUs. In general, people use tensor parallelism inside a node where GPUs are connected by high-bandwidth networks like NVlink, and pipeline parallelism is used when there is slower cross-node communication. However, the performance modeling is challenging for pipeline parallelism because of bubbling. In this paper, we consider the tensor parallelism distributed inference. The optimal worker configuration is achieved when we achieve the optimal per-GPU throughput. Therefore, the throughput with the given number of GPUs is optimized. With the different ranks of tensor parallelism, the computing, communication, and KV cache capacity all impact the throughput. In the default vLLM [12] setting, the prefill and decode processes are served with the same worker. The decode process dominates the inference process because tokens are generated in the decode process one by one while the prefill process only has one iteration. We have to predict the parallelism strategy with the most per-GPU throughput for decode phase.

In tensor parallelism, each GPU first computes the split tensor locally, then combines the tensor across all GPUs using All-reduce. The split tensor size is inverse to the GPU number, so the computing time is an inverse function of the number of GPUs:

$$
\begin{equation*}
t_{\text {compute }}=\frac{k_{4}}{N_{g}}+c_{4} \tag{5}
\end{equation*}
$$

where $N_{g}$ is the number of GPUs per worker, $k_{4}$ and $c_{4}$ are learnable parameters. Tensor parallelism adopts All-reduce for the inter-GPU communication. The communication overhead for All-reduce, relative to the number of GPUs, is $\left(N_{g}-1\right) / N_{g}$. When the number of GPUs is large, the communication overhead of All-reduce is nearly constant. However, for modern GPU servers like DGX A100 and H100, the number of GPUs on each server is less than or equal to 8 . So the

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-05.jpg?height=572&width=721&top_left_y=256&top_left_x=1147)

Figure 3: An example illustrates the sub-optimal of JSQ for request placement.

difference in the communication overhead is non-negligible. Since the communication between GPUs is intra-node communication with a high-speed network. The communication straggler is not significant. Therefore we use the communication overhead to predict the communication delay scaling to the GPU number. The KV cache capacity can be calculated by the sum of the GPUs' memory on each worker minus the model size, $M=N_{g} m_{g p u}-m_{\text {model }}$. The throughput can be limited by KV cache or the SLO constraint. When KV cache is the bottleneck, the maximum throughput is achieved when the $\mathrm{KV}$ cache is full. When iteration SLO is the bottleneck, the maximum throughput is achieved when the decode iteration time is equal to ATGT SLO latency limit. The maximum per-GPU throughput of tensor parallelism rank $N$ is:

$$
\begin{equation*}
T_{\max }=\min \left\{\frac{M}{N_{g} m_{r}\left(t_{\text {compute }}+t_{\text {comm }}\right)}, \frac{B}{N_{g} T_{\text {decode }}}\right\} \tag{6}
\end{equation*}
$$

where $m_{r}$ is the average per request $\mathrm{KV}$ cache demand learned from the historical data, and $t_{\text {compute }}+t_{\text {comm }}$ is the iteration time given the batch size $\frac{M}{m_{r}}$ with $N_{g}$ GPU per worker. $T_{\text {decode }}$ is the ATGT SLO, and $B$ is the batch size corresponding to the SLO. The optimal worker configuration has $N_{g}^{o p t}$ GPUs that maximize $T_{\max }$. Note that the optimal worker configuration remains unaffected by the request arrival rate but is influenced by factors such as the model's size, context length, and the computing and memory capacity of the GPU. Since we consider homogeneous GPUs in this paper, when scheduling requests and workers to adapt to varying workloads, the configuration of each worker remains unchanged.

### 4.2 Request Placement Policies

We optimized the worker configuration to achieve maximum per-GPU throughput, and our next objective is to minimize the number of workers required for LLM service. The placement of queries to workers significantly affects efficiency of
resource utilization. Figure 3 illustrates the suboptimal of naive JSQ and reveals the optimal request placement strategy. In this example, requests need to be placed to two workers with $\mathrm{KV}$ cache capacity of 9 . Note that in this example, the requests arrive in sequence from 1 to 4 but are submitted to workers at the same time. If we adopt JSQ, two long prompt requests will be placed to the same worker, while two long output requests will be placed to another worker. Suppose a token requires $1 \mathrm{KV}$ cache capacity. The max $\mathrm{KV}$ cache demand for both workers is 10 when requests finish generation, which exceeds the KV cache capacity of 9 . Therefore, we need to move requests to the waiting queue until there is available $\mathrm{KV}$ cache. However, with the optimal request placement, a long prompt request and a long output request are placed in one worker. The max KV cache demand for each worker is 7. To aid in this decision-making process, we leverage the parameters notated in Table 1 and the following information:

- Learnable prefill time to total input tokens Eq. 2, input tokens limit to batch size when constraining the decode iteration time Eq. 4 and learnable KV cache usage to token count Eq. 1 functions for each group.
- The current KV cache usage $m=\sum k v$ and total KV cache $M$ for each worker.
- For each newly added request, we utilize the known input prefill length $l_{j}^{i n}$ and predicted output length $l_{j}^{\text {pred }}$. For ongoing requests, we take into account the current length generated $l_{j}^{\text {out }}$.

Table 1: The inputs to Aladdin and decisions Aladdin makes

| Inputs | Notation | Definition |
| :--- | :--- | :--- |
|  | $k v(t)$ | The KV cache usage to tokens function |
|  | $l_{d}(b)$ | The input length limit to batch sizes |
|  | $t_{p}(l)$ | The prefill iteration time function |
|  | $m_{i}$ | The KV cache usage of Worker $i, i \in W$ |
|  | $M$ | The KV cache capacity of each worker |
|  | $l_{j}^{\text {in }}$ | The input length of a request |
|  | $l_{j}^{\text {pred }}$ | The predicted output length of a request |
|  | $l_{j}^{\text {real }}$ | The real output length of a request |
|  | $l_{j}^{\text {out }}$ | The output tokens a request generated so far |
|  | $t_{j}^{\text {dec }}$ | The time spent for decoding phase so far |
|  | $T_{p r e}$ | The SLO of prefill latency |
| Outputs | Notation | Definition |
|  | $W$ | The total worker number |
|  | $x_{i j}$ | binary variable for request $j$ |
|  | $y_{i}$ | binary variable for Worker $i$ |

The request scheduling with the constraints can be seen as a multi-dimensional bin packing problem. We formulate it as a mixed integer programming (MIP) that schedules the new-arrived requests between the scheduling heartbeat with different input/output lengths $l_{\text {in }}$ and $l_{\text {out }}^{\text {pre }}$, and we want to minimize worker number $W$.

Let $x_{i j}$ be a binary variable that equals 1 if request $j$ is scheduled to Worker $i$, and 0 otherwise. Let $y_{i}$ be a binary variable that equals 1 if Worker $i$ is used, and 0 otherwise. Assume $I$ is the initial worker number larger than the optimal $W$. When there are ongoing requests, for an ongoing request $j$, to prevent the unnecessary migration between workers, the $x_{i j}$ is kept the same as the current decoding worker. We also need to guarantee that the new request's prompt processing time won't cause the token generation time SLO violation of the ongoing requests. The MIP problem can be formulated as follows:

$$
\begin{array}{ll}
\min & \sum_{i=1}^{I} y_{i} \\
\text { s.t. } & \sum_{i=1}^{I} x_{i j}=1, j=1,2, \ldots, J \\
& \sum_{j=1}^{J} x_{i j}\left(l_{j}^{\text {in }}+\gamma l_{j}^{\text {out }}\right) \leq \theta l^{d}\left(\sum_{j=1}^{J} x_{i j}\right), i=1,2, \ldots, I \\
& t_{p}\left(\sum_{j=1}^{J_{\text {new }}} x_{i j} l_{j}^{\text {in }}\right) \leq T_{p r e}, i=1,2, \ldots, I \\
& t_{p}\left(\sum_{j=1}^{J_{\text {new }}} x_{i j} l_{j}^{\text {in }}\right) \leq \theta \min \left(T_{d e c} l_{i j}^{\text {out }}-t_{i j}^{\text {dec }}\right), i=1, \ldots, I \\
& {\left[\sum_{j=1}^{J} \mathbf{w}_{j} x_{i j}\right] \leq M, k=1,2, \ldots, K, i=1,2, \ldots, I} \\
& x_{i j} \leq y_{i}, i=1,2, \ldots, I, j=1,2, \ldots, J \\
& x_{i j} \in\{0,1\}, i=1,2, \ldots, I, j=1,2, \ldots, J \\
& y_{i} \in\{0,1\}, i=1,2, \ldots I . \tag{h}
\end{array}
$$

The constraints are: a Each request must be scheduled to one worker. (b) According to Eq. 3, the iteration time is determined by both batch size and the total context length. Eq. 4 shows the maximum total context length of all requests in one batch given the batch sizes. This constraint ensures the ATGT SLO for the decode process. Since the iteration time increases as more tokens are generated during decoding, the coefficient $\gamma$ can be considered as a "strictness knob" that tunes the scheduling bound, $0 \leq \gamma \leq 1$. When $\gamma=0$, only the first iteration can meet the ATGT SLO. When $\gamma=1$, the last token generation time can meet the ATGT SLO. We normally set $\gamma=0.5$ to increase the worker utilization while guaranteeing the SLOs. (c) According to Eq. 2, the sum of all new requests' input is limited by the TTFT SLO. (d) Since the prefill of new requests preempts the decode for ongoing requests, the prefill time of new requests can not exceed the time that ongoing requests have saved compared with the ATGT limit. Reflecting on the limitation of the sum of new requests' input length. (e) The total KV cache demand of

```
Algorithm 1: Request scheduling heuristic
    Input: $l^{\text {in }}, l^{\text {pred }}$ of the new request $j . l_{\text {in }}, l_{\text {pred }}, l_{\text {out }}$ of
    all ongoing requests. $\mathrm{KV}$ cache capacity $M$ for each
    worker. Worker number $W$. Performance models $k v(t)$,
    $t_{\text {iter }}(b, l), t_{\text {pre }}(l)$.
    Output: Worker $i$ where job $j$ be scheduled, $x_{i j}=1$.
    Initial: workerfound $\leftarrow$ False
    Sort all bins on capacity_norm from large to small.
    for sorted bins $i=1,2, \ldots, I$ do
        initial $x_{i j} \leftarrow 0, i=1,2, \ldots, I$
        $x_{i j}=1$
        if b) and c and $d$ and e for $i$ then
            workerfound $\leftarrow$ True
            return $x_{i j}$
    if workerfound $=$ False then
        Open a new bin $(I+1)$ and add job $j$.
        workerfound $\leftarrow$ True
        return $x_{(I+1) j}=1$
```

all the requests scheduled to each worker cannot exceed the $\mathrm{KV}$ cache capacity $M . K$ is the sequence length limit of the serving model. $\mathbf{w}$ is the vector with length $K$ that shows a request's $\mathrm{KV}$ cache footprint. For example, for request $j$,

$\mathbf{w}=\left[\begin{array}{lllllll}k v\left(l_{j}^{i n}\right) & k v\left(l_{j}^{i n}+1\right) & \cdots & k v\left(l_{j}^{\text {in }}+l_{j}^{\text {pred }}\right) & 0 & \cdots & 0\end{array}\right]$,

where each element in the vector presents the KV cache demand of an iteration. The KV cache demand for the first iteration includes the KV cache for input tokens. The KV cache demand increases in the following iterations while output tokens are generated. The KV cache demand becomes zero when the request $j$ finishes. This constraint guarantees that for all scheduled iterations, the $\mathrm{KV}$ cache demand will not exceed the KV cache capacity of the worker. (f) If a worker is used, it should have at least one request scheduled. Otherwise, we don't need this worker. (g) (h) All variables are binary. Unused boxes will have $y_{i}=0$ and will not be counted in the objective function. $0<\theta<1$ in (b) (d) is another hyperparameter that adapts to the prediction error of output length. For example, when $\theta$ is small, the constraints are tighter, so requests are less likely to violate the SLOs. However, the drawback is that we need more workers for the serving.

Scheduling heuristic. The multi-dimensional bin-packing problem is NP-hard, so an efficient heuristic is needed to approach optimal scheduling. Given that requests arrive in an online pattern, we employ the best-fit algorithm for online bin packing [13]. It schedules each arrived request to the worker with the maximum load and can guarantee the satisfaction of all SLO constraints. Intuitively, this heuristic increases the utilization of each worker compared to other scheduling algorithms, such as joining the shortest queue, thereby reducing the number of workers needed.

```
Algorithm 2: Re-balancing with prediction error
Input: $x_{i j}, l_{j}^{\text {pred }}, l_{j}^{\text {out }}, l_{j}^{\text {real }}$ of $J_{\text {old }}$ ongoing requests.
    $x_{i j}, l_{j}^{\text {in }}, l_{j}^{p r e d}$ of $J_{\text {new }}$ new requests.
    Output: Updated $x_{i j}$ of new requests.
    Initial: $l_{i}^{e}=b_{i}^{e}=0, i=1,2, \ldots, I$.
    for worker $i=1,2, \ldots, I$ do
        for ongoing job $j=1,2, \ldots, J_{i}$ on worker $i$ do
            $/ *$ Check if under estimate output length $* /$
            if $l_{j}^{\text {out }}>l_{j}^{\text {pred }}$ then
                $l_{i}^{e} \leftarrow l_{i}^{e}+l_{j}^{\prime p r e d}$
                $b_{i}^{e} \leftarrow b_{i}^{e}+1$
            $/ *$ Check if over estimate output length $* /$
            if $l_{j}^{\text {real }}<l_{j}^{\text {pred }}$ then
            $l_{i}^{e} \leftarrow l_{i}^{e}+l_{j}^{\text {real }}-l_{j}^{p r e d}$
            $b_{i}^{e} \leftarrow b_{i}^{e}-1$
    Calculate the equivalent error function
    $\alpha_{i} l_{i}^{e}+\beta_{i} b_{i}^{e}+c_{1}=0$ of worker $i, i=1,2, \ldots, I$.
    according to Eq. 4.
/*Fix error by adjusting the new requests placement*/
    if new request $j$ from worker $x$ to worker $y$ then
        $b_{x}^{e} \leftarrow b_{x}^{e}-1$
        $b_{y}^{e} \leftarrow b_{y}^{e}+1$
        $l_{x}^{e} \leftarrow l_{x}^{e}-l_{j}^{p r e d}$
        $l_{y}^{e} \leftarrow l_{y}^{e}+l_{j}^{p r e d}$
    $/ *$ Minimize the sum of the shortest distance between
        each worker's error function and the origin. $* /$
    $22 \min \left(\sum \frac{\left|c_{i}\right|}{\sqrt{\alpha_{i}^{2}+\beta_{i}^{2}}}\right), i=1,2, \ldots, I$.
    Return $x_{i j}, j=1,2, \ldots, J_{\text {new }}$
```

In the multi-dimensional bin packing problem, determining the metric for each worker's load is non-trivial. Using the batch size of each worker as the metric for its load is sub-optimal because the input and output lengths of requests significantly influence each worker's load. We propose capacity_norm, which is the $\mathrm{L} 2$ norm of batch size $B$ and weighted context length $\sum\left(l_{\text {in }}+\gamma l_{\text {out }}\right)$ of all ongoing requests to rank all workers. The heuristic algorithm for scheduling an arriving request is described in Algorithm 1.

### 4.3 Addressing Prediction Errors

The output length cannot be accurately predicted before execution. If we overestimate the output length, worker utilization will be reduced. Conversely, there will be SLO violations. When an ongoing request in a batch finishes earlier than predicted, we mark this worker as overestimated. If an ongoing request's output length is underestimated, i.e., the request hasn't finished with the predicted tokens, we mark this worker

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-08.jpg?height=412&width=610&top_left_y=255&top_left_x=302)

Figure 4: Workflow of Aladdin with default continuous batching

as underestimated and predict the output length again. Before the execution of the new requests, we re-schedule new requests that have been scheduled to the over-utilized workers to the under-utilized workers. We use $l^{e}$ and $b^{e}$ as the metrics to indicate the estimation error of each worker, where $l^{e}$ is the accumulated error of output length for outstanding requests, and $b^{e}$ is the error of batch size for each worker. If Request $j$ is finished before the estimated iteration, which means we overestimate the output length, we can calculate the output length over-estimate error $l_{j}^{\text {real }}-l_{j}^{p r e d}$. If we underestimate the output length of Request $j$, we predict the output length $l_{j}^{\text {pred }}$ again using conditional average output length when $l_{j}^{\text {real }}>l_{j}^{p r e d}$ with the same input length $l_{j}^{i n}$. In the request scheduling, we use $l^{e}$ and $b^{e}$ as the indicators to balance the workload between workers and reduce the effect of output length prediction error. The calculation for $l^{e}, b^{e}$, and the re-balancing algorithm are described in Algorithm 2.

## 5 System Design

Benefiting from the predictable nature of individual and batch LLM inference, we attempt to reveal the best way to serve requests that arrive as a stream from resource management and request placement perspectives. In this section, we describe the system design of Aladdin for two variances settings: default continuous batching and split-phase inference. The default continuous batching will process the input tokens and generate output tokens in the same worker, represented by vLLM [12]. The split-phase inference refers to the inference setting that splits the prompt processing and token generation into different working instances, and each instance only processes prompt or generates output. This setting is represented by Splitwise [20] and DistServe [32].

### 5.1 System Workflow.

Default continuous batching. The Figure 4 illustrates the workflow of continuous batching inference scheduling. Firstly, users submit their LLM inference requests via the API as the

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-08.jpg?height=409&width=848&top_left_y=256&top_left_x=1099)

Figure 5: Workflow of Aladdin with split-phase inference.

first step (1). The request scheduler uses the bin packing heuristic to schedule the new requests according to their input length and the predicted output length (2). Lastly, the request scheduler continuously update the performance model according to the worker's execution traces (3).

Split-phase inference. Figure 5 illustrates the workflow of split-phase inference. Users submit requests through API (1). We schedule the prefill of new requests based on their input lengths. Since the prefill only involves one iteration, there is no queue for the prefill workers (2). Next, the decoding scheduler places the requests from prefill workers to decoding workers based on the predicted output length and a learned performance model (3). Finally, the prefill and decode schedulers continuously update the performance model according to the workers' execution traces (4).

### 5.2 Adapt to Changing Demand

In every cluster heartbeat, we can reconfigure the cluster using change point detection. In LLM inference, although users submit different queries and receive different answers, the input and output lengths of LLM inference requests for the same model exhibit a strong pattern. From the SharGPT dataset [5], we found that the input lengths of user queries follow a fixed distribution, and the output lengths of the same LLM also follow a learnable distribution. According to our experiment using Algorithm 1, when the arrival rate $r_{a}$ is larger than a lower bound $R$, the total number of required workers $N_{w}$ is linear with the request arrival rate $r_{a}$.

$$
\begin{equation*}
N_{w}=\left\lceil k_{5} r_{a}+c_{5}\right\rceil, r_{a}>R \tag{7}
\end{equation*}
$$

where $k_{5}$ and $c_{5}$ are learnable coefficients associated with the historical demand, and we round the number of workers to the smallest integer larger than the function of $r_{a}$. The reason $R$ exists is that when the arrival rate is lower, there are fewer requests arriving in the same heartbeat, which cannot represent the real distributions of the input and output length. The standard error of the mean $S E M=\frac{\sigma}{\sqrt{n}}$ is the metric for the difference between the sampled requests' input and output lengths and the total requests, where $\sigma$ is the standard

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-09.jpg?height=325&width=851&top_left_y=241&top_left_x=187)

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-09.jpg?height=233&width=414&top_left_y=260&top_left_x=194)

(a) A100 testbed

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-09.jpg?height=236&width=414&top_left_y=256&top_left_x=606)

(b) V100 testbed

Figure 6: Prefill latency

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-09.jpg?height=309&width=830&top_left_y=688&top_left_x=192)

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-09.jpg?height=226&width=394&top_left_y=692&top_left_x=196)

(a) A100 testbed

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-09.jpg?height=239&width=414&top_left_y=691&top_left_x=606)

(b) V100 testbed
Figure 7: Decode context length limitation

deviation of all requests' input and output length and $n$ is the number of requests we place during a heartbeat. The smaller $n$ is, the more error appears in the prediction of $N_{w}$.

With this model, we can predict the total number of workers required before placing all requests to each worker. However, the scheduling time requirement of inference serving is in milliseconds. In a high-demand situation, the scheduling overhead is too large to schedule the requests in the target iteration for the centralized scheduler. We design a distributed scheduler for the high-demand scenario that harnesses the pattern of input and output length of requests in Appendix A.

Note that in this paper, we focus on predicting the minimal GPU required for the varying arrival rate without considering the cold start problem and the switching cost. Since the optimization of cluster scheduling is orthogonal to the worker number prediction problem, we defer it to future work.

### 5.3 Implementation

Aladdin is specifically designed for single-model serving, eliminating any model cold start problem for each worker. We adopt vLLM [12] for dynamic batch inference to optimize the $\mathrm{KV}$ cache usage of each worker and make the KV cache usage more predictable. Aladdin's request scheduler is a scheduling layer on top of the vLLM inference engine. Users submit their requests to the Aladdin frontend through the API interface. Aladdin routes and schedules the requests to different workers through each server's API interface. Note that Aladdin is a non-blocking system; once a request is scheduled to a worker, it will start inference in the next iteration. Aladdin doesn't support request migration, which means once a request has been sent to a worker, we won't migrate it to another worker with the same duty.

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-09.jpg?height=328&width=572&top_left_y=256&top_left_x=1229)

Figure 8: KV cache prediction and the observation.

## 6 Evaluation

For the evaluation of Aladdin, our first step is to validate the accuracy of our performance modeling for continuous batching inference in Section 6.2. Next, we examine the performance improvement achieved with Aladdin with different scenarios in Section 6.3 and Section 6.4. We also provide the overhead analysis of Aladdin in Section 6.5. The primary information of our evaluation is as follows:

- Aladdin accurately predicts performance metrics with the maximum error less than $10 \%$.
- Aladdin reduces the GPU number required by up to $71 \%$ and $60 \%$ compared with vanilla vLLM [12], and splitphase inference engines [20,32]'s decode instances for the same workload.
- Although single-worker optimization techniques like chunked prefill [2] and split-phase inference [20, 32] reduce the cost for inference, the cost reduced by Aladdin is orthogonal to those techniques. Aladdin can be combined with those single-worker optimization techniques to improve the performance further.


### 6.1 Experimental Setup

Testbed setup. We test the performance of Aladdin on highend GPU servers with 4 A100 80GB GPUs connected with PCIe. Each machine has two Intel Xeon Platinum 8380 processors and 512GB RAM. To validate the generalization of Aladdin from both a computation perspective and communication perspective, we also evaluate Aladdin on the GPU servers with 4 V100 32GB GPUs connected with NVLink. Each machine has two Intel Xeon Gold 6230 processors and 128GB RAM. We also do a large-scale simulation for the high-demand request arrival situation.

Models and SLOs. Our target is to prove Aladdin reduces the cost for transformer-based LLMs. To validate that Aladdin can accurately model the performance metrics of most models. We evaluate Aladdin on Llama2 series [28] models from 7B to 70B. The model, testbed information, and SLOs are shown in Table 2. Note that the prefill latency SLOs are

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-10.jpg?height=344&width=661&top_left_y=256&top_left_x=274)

(a) The end-to-end SLO attainment rate, (left): LlaMa213b, (right): LlaMa2-70b

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-10.jpg?height=344&width=664&top_left_y=701&top_left_x=270)

(b) The end-to-end P99 ATGT, (left): LlaMa2-13b, (right): LlaMa2-70b

Figure 9: End to end experiments on A100 testbed

the approximated inference latency for the model's context length (4096 tokens) for each testbed. The selection of decode latency SLO is according to the individual request inference latency. We guarantee that the batch inference latency of each request won't exceed the individual inference latency for 1.3 times.

Workload. For the end-to-end performance evaluation in Section 6.3, we first collect the prompts from users of ShareGPT_V3_unfiltered_cleaned_split dataset [27], then submit the prompts follows a Poisson distribution. The outputs are generated by each evaluated model with a temperature of 0 and a maximum output token limit of 2048. For the largescale simulation in Section 6.4, we use the same prompts' lengths as those collected from ShareGPT [27] in Section 6.3 as the prompt lengths. Then, we predict the output length based on the output length CDF of the responses generated in Section 6.3's end-to-end evaluations for each model.

Table 2: The LLM information and testbed allocation

| Model | Testbed | Prefill <br> SLO(ms) | Decode <br> SLO(ms) |
| :---: | :---: | :---: | :---: |
| Llama2-chat 70b | A100 | 1600 | 75 |
| Llama2-chat 13b | A100, V100 | 600,800 | 30,50 |
| Llama2-chat 7b | A100, V100 | 400,800 | 15,30 |

Because there is no available trace of LLM inference that includes the arrival time of each request, we simulate the request arrival stream using a Poisson distribution. We need to vali-

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-10.jpg?height=350&width=653&top_left_y=253&top_left_x=1191)

(a) The end-to-end SLO attainment rate, (left): LlaMa27b, (right): LlaMa2-13b

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-10.jpg?height=344&width=658&top_left_y=706&top_left_x=1189)

(b) The end-to-end P99 ATGT, (left): LlaMa2-7b, (right): LlaMa2-13b

Figure 10: End to end experiments on V100 testbed

date that Aladdin improves performance in both high-demand and low-demand scenarios. To evaluate the performance of Aladdin with varying demands, we tune the average arrival rate $\lambda$ to simulate different request demands.

Metrics. Since our final target is to reduce the cost of the inference service, we use the number of GPUs required to achieve a certain SLO attainment rate as the main metric. In Section 6.4, we evaluate the total GPU number required with different request arrival rates. In Section 6.3, As the total resources are limited for the real testbed evaluation, we evaluate the performance of Aladdin with the SLO attainment rate and the P99 ATGT in different request arrival rates. The SLO is attained when both TTFT and ATGT latency meet the requirement.

Baselines. Aladdin is a cluster-level scheduler. The performance improvement achieved by Aladdin is orthogonal to the improvements achieved by single-server optimization techniques such as split-phase inference [20,32] or page attention [12]. These single-server optimization techniques use naive cluster scheduling like JSQ. Previous work [10] adopts the power-of-two scheduling for request placement. However, it is suboptimal for request placement and cannot guarantee a high SLO attainment rate. We compared Aladdin's request placement with JSQ and power-of-two algorithms with different GPUs and different serving scenarios.
![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-11.jpg?height=610&width=740&top_left_y=294&top_left_x=213)

Figure 11: Simulation of the total GPU number needed with the mixed prefill and decode setting.

### 6.2 Performance Model Validation

Request placement and worker configuration depend on accurate predictions of performance metrics. In this section, we evaluate the model's accuracy by comparing the predicted metrics to the measured actual metrics.

In Section 3, we model the latency of prefill phase and decode phase separately because the two phases have different characteristics. In the evaluation, we evaluate the prefill and decode latency separately for different input and context lengths. In our prefill latency model, the prefill time of a batch size only corresponds to the total input length of all requests in the batch, not related to the batch size. In our experiment, we test different batch sizes $1,2,4,8$ with the same total input length within a batch to validate this formulation. We only evaluated the LlaMa2-70b model on the A100 testbed because our V100 testbed could not load the 70b model (around 140GB) even with all GPUs (32GB*4). Figure 6a and Figure 6b shows the results on A100 and V100 testbeds. The maximum prefill latency prediction error is less than $4 \%$. The shaded area is the prediction interval, which represents the estimation of the range in which future observations are likely to fall. Results indicate the maximum error of the prediction interval compared with our prediction is less than 10 tokens.

In the decode latency model, the iteration time is linear with respect to both the batch size and the total context length within the batch, not related to the context length of each request in the batch. This means that regardless of whether the context length of each request in a batch is long or short, the decoding latency will be the same when the sum of the context lengths of all requests and the batch size is the same. In our experiment, for the same batch size, we use the same sum of context length but different context length distributions for all requests in a batch to validate this formulation. Results
![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-11.jpg?height=600&width=744&top_left_y=296&top_left_x=1124)

Figure 12: Simulation of the total GPU number needed for the decode phase of the split-phase inference setting

are presented in Figure 7a and Figure 7b. Similar to the prefill latency prediction, the prediction error is less than $5 \%$. For the prediction interval, the error is less than 300 tokens for all context tokens in the batch.

The KV cache usage to the context length is the most accurate metric in our performance models. According to Figure 8, the prediction error is less than $1 \%$, and the prediction interval is just aligned with the prediction model. Note that the KV cache usage is not affected by the testbed; it is only related to the model. Generally speaking, the larger the model is, the more $\mathrm{KV}$ cache is needed for the same context length. However, from Figure 8, we can see that for the same context length, Llama2-13b requires more KV cache than Llama270b. This is because Llama2 7b and 13b adopt multi-head attention, while the $70 \mathrm{~b}$ model adopts grouped-query attention [3], which shares key and value pairs within each group.

### 6.3 End-to-End Performance

We evaluate Aladdin's end-to-end performance by comparing it with baselines on our A100 and V100 testbeds. In this experiment, requests arrived on Aladdin in a stream format following Poisson distribution. We use ShareGPT [27] dataset for the conversation content. The baseline we select is the default vLLM, with all GPUs (4 GPUs) on each machine in one worker. Since the performance improvement achieved by Aladdin is gained both from request placement and optimal worker configuration, we configure vLLM with the optimal worker configuration and adopt JSQ for the request placement to do the ablation study. Table 3 reveals the best worker configuration for different models on different testbeds.

The results of A100 testbed are shown in Figure 9. For the LlaMa2-70b model, Aladdin reduces the SLO violation

Table 3: Optimal worker configuration for different models and different GPUs for ShareGPT dataset

| Model | A100 <br> (GPUs/worker) | V100 <br> (GPUs/worker) |
| :---: | :---: | :---: |
| Llama2-70b-chat-hf | 2 | N/A |
| Llama2-13b-chat-hf | 1 | 2 |
| Llama2-7b-chat-hf | 1 | 1 |

rate by up to $3.5 \mathrm{X}$ compared with the default vLLM setting. Compared with the best worker configuration with JSQ placement, Aladdin only improved the SLO attainment rate by up to $19 \%$. This is because there are totally two workers for the LlaMa2-70b model, which limits the improvement in the SLO attainment rate. However, Aladdin significantly reduces the P99 ATGT by up to $40 \%$ compared with JSQ, as shown in Figure 9b's right side. The results for the LlaMa2-13b model are distinct from the $70 \mathrm{~b}$ model. The optimal worker configuration for the $13 \mathrm{~b}$ on the A100 testbed is one GPU according to Table 3. There are four workers in total for the request placement. So Aladdin improves the SLO attainment rate by up to $51 \%$ compared with JSQ, but only has minor P99 ATGT improvement. The results of the V100 testbed are described in Figure 10. The difference is when the request arrival rate is low, the P99 ATGT of baseline default vLLM output performs the performance with optimal worker configuration. This is because when the arrival rate is low, the batch effect is not significant, and the worker with more GPUs has higher computing power than the worker with fewer GPUs. Nevertheless, in those arrival rates, both baselines and Aladdin fulfill all requests SLOs. The higher ATGT won't further improve the SLO attainment rate. Note that we don't include the P99 TTFT because vLLM [12] preempts the decode batch with the prefill batch when new requests arrive, making the ATGT more easily violate the SLO.

### 6.4 Large-Scale Simulation

We conducted a simulation for the high-demand request arrival scenario. In this simulation, we evaluated Aladdin's performance with split-phase inference and the default vLLM inference setting. To show the direct cost savings of Aladdin, we simulate the GPU number required for P100 SLO-guaranteed inference serving at the different request arrival rates.

Default Continuous Batching Inference. In Figure 11, we compared vLLM with baselines in Section 6.3. Results indicate that Aladdin reduces the LLM serving cost by up to $71 \%$ and $40 \%$ compared with the default vLLM and JSQ with Aladdin optimal workers.

Split-Phase Inference. Previous work [10, 20, 32] split the prefill phase and decode phase into different instances. Splitphase serving maintains a group of prefill workers and a group of decode workers, as shown in Figure 5. According to the

![](https://cdn.mathpix.com/cropped/2024_06_04_6d5438d3fbbb9e3dd1f0g-12.jpg?height=349&width=480&top_left_y=281&top_left_x=1256)

Figure 13: The bin packing algorithm running time with different arrival rates

results of DistServe [32], the majority of GPU resources are scheduled for the decode workers. Since the scheduling for prefill instances is trivial with known prompt lengths, we only simulate the GPU number required for the decode phase instance. The baselines are JSQ adopted by DistServe [32] and the Power-of-Two algorithm adopted by previous work [10]. Results indicate that Aladdin reduces the GPU number required for the SLO-guaranteed decode phase by up to $60 \%$ and $49 \%$ compared with JSQ and Power-of-Two algorithm.

### 6.5 Scheduling Overhead

The scheduling overhead can be a problem in high-demand scenarios. For the scheduling latency, each scheduler's scheduling latency is predictable based on the request arrival rate since the time complexity of the best-fit bin packing algorithm is $\mathrm{O}($ nlogn). Figure 13 shows the scheduling overhead in centralized scheduling. According to the results, with a request arrival rate of around 25 requests per second as we adopted in Section 6.4. The scheduling overhead is less than $50 \mathrm{~ms}$, which is acceptable. However, if the arrival rate is very high or the scheduling latency limit is very strict, we can follow Appendix A to adopt the distributed grouped scheduling.

## 7 Related Work

LLM Inference Performance Modeling. To improve the LLM inference metrics, the first step is to model the performance of the LLM inference process. Previous work [15] estimates the prefill and decode runtime based on the floating point operations during the inference. However, they focus on the inference for a single query. The performance metrics prediction of [15] is only based on the model, which lacks the consideration of the hardware adaption. DistServe [32] models the latency of prefill and decode phases for a batch of queries, also according to the model architecture, e.g., general matrix multiply operations. However, they model the inference latency mainly for the inference worker configuration instead of request placement. As far as we know, there is no existing research that studies the inference latency for a batch
of requests with varying input and output lengths.

Inference Serving Quality. The unique prefill-decode phase and the varying output length of autoregressive inference lead to the specialty of LLM inference SLOs. Because the first token generation time is only related to the input length, it makes the first token generation time predictable. There is a common understanding that the time to the first token SLO can be set as a constant $[1,10,20,32]$. However, most of the previous work $[1,10,20]$ adopts the time between tokens as the SLO metric for the decode phase, which is overly strict and does not directly affect the user's experience, as discussed in Section 2.2. Only considering the time between tokens SLO also harms the fairness of LLM inference [24]. The average token generation time SLO we use in this paper directly affects the quality of service and achieves better fairness for users.

LLM Inference Systems. Recent work on LLM serving systems can be categorized into three classes. The first category is application-level optimization [21], where Continuous batching [29] and page attention [12,26] optimize the batch efficiency of LLM inference. Chunked-prefill [1,2] balances the prefill and decode to improve LLM inference throughput. The second category is inference worker optimization [18]; Splitwise [20] adopts heterogeneous GPUs to handle different bottlenecks in prefill and decode phases. Previous work [10,32] designed search algorithms to find the optimal worker configuration to achieve the best per-GPU goodput. The third class is about workload scheduling; recent work [14,22] focuses on the scheduling of queries to improve the QoE or throughput. However, they lack consideration for resource management.

## 8 Conclusion

We propose Aladdin, an adaptive LLM serving system that effectively scale and configures computing resources and optimally places inference queries to minimize serving costs while fulfilling SLOs. In this paper, we first deduce the performance models of the batched prefill and decode phases in LLM inference. Then, we predict the minimal computing resources required along with the corresponding worker configuration and request allocation. Results show that Aladdin reduced LLM serving costs by up to $71 \%$ compared to state-of-the-art baselines.

## References

[1] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. Taming throughputlatency tradeoff in $1 \mathrm{~lm}$ inference with sarathi-serve, 2024.

[2] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, and Ramachandran
Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills, 2023.

[3] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4895-4901, Singapore, December 2023. Association for Computational Linguistics.

[4] Marc Brysbaert. How many words do we read per minute? a review and meta-analysis of reading rate. Journal of memory and language, 109:104047, 2019.

[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023.

[6] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica. Clipper: A Low-Latency online prediction serving system. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), pages 613-627, Boston, MA, March 2017. USENIX Association.

[7] Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula, Sriram Rao, and Aditya Akella. Multiresource packing for cluster schedulers. In Proceedings of the 2014 ACM Conference on SIGCOMM, SIGCOMM '14, page 455-466, New York, NY, USA, 2014. Association for Computing Machinery.

[8] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, and Jonathan Mace. Serving DNNs like clockwork: Performance predictability from the bottom up. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 443-462. USENIX Association, November 2020.

[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.

[10] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, and Yizhou Shan. Inference without interference: Disaggregate llm inference for mixed downstream workloads, 2024.

[11] Sangeetha Abdu Jyothi, Carlo Curino, Ishai Menache, Shravan Matthur Narayanamurthy, Alexey Tumanov,

Jonathan Yaniv, Ruslan Mavlyutov, Inigo Goiri, Subru Krishnan, Janardhan Kulkarni, and Sriram Rao. Morpheus: Towards automated SLOs for enterprise clusters. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pages 117-134, Savannah, GA, November 2016. USENIX Association.

[12] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23, page 611-626, New York, NY, USA, 2023. Association for Computing Machinery.

[13] Adam Letchford. Approximation algorithms: Vv vazirani, springer-verlag, 2001. Journal of the Operational Research Society, 53:807-808, 072002.

[14] Jiachen Liu, Zhiyu Wu, Jae-Won Chung, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury. Andes: Defining and enhancing quality-of-experience in llm-based text streaming services, 2024.

[15] Deepak Narayanan, Keshav Santhanam, Peter Henderson, Rishi Bommasani, Tony Lee, and Percy S Liang. Cheaply estimating inference efficiency metrics for autoregressive transformer models. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 66518-66538. Curran Associates, Inc., 2023.

[16] The new york times. The desperate hunt for the a.i. boom's most indispensable prize. https://www.nytimes.com/2023/08/16/ technology/ai-gpu-chips-shortage.html, 2023.

[17] NVIDIA. cublas. https://docs.nvidia.com/cuda/ cublas/index.html, 2023.

[18] Hyungjun Oh, Kihong Kim, Jaemin Kim, Sungkyun Kim, Junyeol Lee, Du-seong Chang, and Jiwon Seo. Exegpt: Constraint-aware resource scheduling for llm inference. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, ASPLOS '24, page 369-384, New York, NY, USA, 2024. Association for Computing Machinery.

[19] OpenAI. Gpts. https://openai.com/blog/ introducing-gpts, 2023.
[20] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Aashaka Shah, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting, 2023.

[21] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference, 2022.

[22] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, Zbigniew T. Kalbarczyk, Tamer Başar, and Ravishankar K. Iyer. Efficient interactive llm serving with proxy model-based sequence length prediction, 2024.

[23] Francisco Romero, Qian Li, Neeraja J. Yadwadkar, and Christos Kozyrakis. INFaaS: Automated model-less inference serving. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 397-411. USENIX Association, July 2021.

[24] Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang Zhuo, Joseph E. Gonzalez, and Ion Stoica. Fairness in serving large language models, 2023.

[25] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu, 2023.

[26] Foteini Strati, Sara Mcallister, Amar Phanishayee, Jakub Tarnawski, and Ana Klimovic. Déjàvu: Kv-cache streaming for fast, fault-tolerant generative llm serving, 2024.

[27] Sharegpt teams. Sharegpt. https://huggingface. co/datasets/anon8231489123/ShareGPT_Vicuna_ unfiltered, 2023.

[28] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan

Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.

[29] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for Transformer-Based generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521-538, Carlsbad, CA, July 2022. USENIX Association.

[30] Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. MArk: Exploiting cloud services for CostEffective, SLO-Aware machine learning inference serving. In 2019 USENIX Annual Technical Conference (USENIX ATC 19), pages 1049-1062, Renton, WA, July 2019. USENIX Association.

[31] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. Response length perception and sequence scheduling: An llm-empowered llm inference pipeline, 2023.

[32] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Distserve: Disaggregating prefill and decoding for goodputoptimized large language model serving, 2024.
