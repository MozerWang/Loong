# Can LLMs Solve Longer Math Word Problems Better? 

Xin $\mathbf{X u}^{1 *}$, Tong Xiao ${ }^{2 *}$, Zitong Chao ${ }^{1}$, Zhenya Huang ${ }^{2}$, Can Yang ${ }^{1 \dagger}$, Yang Wang ${ }^{1}$<br>${ }^{1}$ Hong Kong University of Science and Technology<br>${ }^{2}$ University of Science and Technology of China<br>\{xxuca, zchaoaa\}@connect.ust.hk, \{macyang, wangyang\}@ust.hk<br>tongxiao2002@mail.ustc.edu.cn, huangzhy@ustc.edu.cn<br>https://github.com/XinXU-USTC/CoLeG-Math


#### Abstract

Math Word Problems (MWPs) are crucial for evaluating the capability of Large Language Models (LLMs), with current research primarily focusing on questions with concise contexts. However, as real-world math problems often involve complex circumstances, LLMs' ability to solve long MWPs is vital for their applications in these scenarios, yet remains under-explored. This study pioneers the exploration of Context Length Generalizability (CoLeG), the ability of LLMs to solve long MWPs. We introduce Extended Grade-School Math (E-GSM), a collection of MWPs with lengthy narratives. Two novel metrics are proposed to assess the efficacy and resilience of LLMs in solving these problems. Our examination of existing zero-shot prompting techniques and both proprietary and open-source LLMs reveals a general deficiency in CoLeG. To alleviate these challenges, we propose distinct approaches for different categories of LLMs. For proprietary LLMs, a new instructional prompt is proposed to mitigate the influence of long context. For open-source LLMs, a new data augmentation task is developed to improve CoLeG. Our comprehensive results demonstrate the effectiveness of our proposed methods, showing not only improved performance on E-GSM but also generalizability across several other MWP benchmarks. Our findings pave the way for future research in employing LLMs for complex, real-world applications, offering practical solutions to current limitations and opening avenues for further exploration of model generalizability and training methodologies.


## 1 Introduction

Math word problems (MWPs) [1] are mathematical questions presented in natural language, demanding delicate reasoning for solving. With the flourish of large language models (LLMs) [2, 3, 4, 5], the math reasoning ability measured on MWPs benchmarks has emerged as a critical evaluation metric to assess the overall capability of these models. The representative benchmarks for MWP include GSM8K [6], which is characterized by concise descriptions in a few sentences. However, this setting differs from real-world scenarios, where MWPs usually come with longer context. [7] suggests that extensive contexts might hinder rather than facilitate the mathematical reasoning process. This observation raises an important question: Do LLMs exhibit a performance degradation in solving long MWPs? If so, how can we improve LLMs performance on these long MWPs?

To answer these questions, the chain-of-thought [8] (CoT) is conducted on the GSM8K bechmark and questions are segregated based on the accuracy of CoT predictions. Rigid statistical hypothesis[^0]testing has revealed significant evidence suggesting that LLMs exhibit decreased performance on MWPs with long context (see Section 2.1). In order to further investigate how are different LLMs affected by lengthy contexts of MWPs, we construct the Extended Grade-School Math (E-GSM), a benchmark comprising MWPs with extended context derived from GSM8K, and two novel evaluation metrics. E-GSM maintains minimal alterations to the conditions and the order of conditions while extending the contextual information of the original problem. We investigate seven proprietary and 20 open-source LLMs, along with three state-of-the-art zero-shot prompts on E-GSM. Our results indicate that the Context Length Generalization (CoLeG) of these LLMs, the ability for LLMs to do math reasoning in a long context, is limited, particularly with longer MWPs.

To alleviate this issue, we propose two different strategies for proprietary and open-source LLMs, respectively. For proprietary LLMs, inspired by cognitive load theory [9], we develop ConditionRetrieving Instruction (CoRe) prompting technique, which encourages LLMs to retrieve problem conditions first and then apply different reasoning modules. For open-source LLMs, we suggest incorporating extension as an auxiliary task during fine-tuning. We validate our strategies on EGSM, and several other MWP benchmarks such as MAWPS [10], SVAMP [11], and GSM-IC [12], demonstrating their effectiveness and generalizability.

We summarize our main contributions as follows:

1. We construct the E-GSM dataset, comprising MWPs with longer contexts. Comprehensive experiments on both proprietary and open-source LLMs reveal that math reasoning abilities of LLMs are significantly affected by long context.
2. We develop a new instructional prompt named CoRe for proprietary LLMs, which can significantly improve CoLeG and accuracy of LLMs on E-GSM.
3. We propose to use extension as an auxiliary task to fine-tune open source LLMs and release our fine-tuning dataset comprising $65 \mathrm{~K}$ CoT data.
4. CoRe and extension have demonstrated their strong generalization on several MWP benchmarks.

The ability of LLMs to solve long MWPs is important for their application in real-world scenarios. Our evaluation indicates that long MWPs will heavily degrades the math reasoning ability of LLMs, and further experiments demonstrate the effectiveness and generalizability of our proposed methods for both proprietary and open-source LLMs. Our findings provides valuable insights and directions for future research on using LLMs to solve math problems.

## 2 The E-GSM Dataset

### 2.1 LLMs Struggle to Answer Math Word Problems with Longer Context

To explore whether the performance of LLMs in mathematical reasoning is adversely affected by longer textual contexts, similar to human performance, we conducted an experiment on GSM8K using CoT with GPT-3.5-turbo as the representative. The experiment employed the 8 -shot demonstrations provided by [8], and the complete prompt can be found in Appendix D. We can compare the answers generated by LLMs with the ground-truth answers and then divide the examples into two groups based on accuracy: the correct answers group $G_{0}$ and the incorrect answers group $G_{1}$.

We want to study whether there exists a significant difference in the problem length, characterized by the number of tokens, between these two categories. We hypothesize that the lengths of the problem descriptions for these two groups are from different distributions, denoted $X$ and $Y$. As illustrated in Figure 1, the distributions between $X$ and $Y$ are quite different. To conduct a rigorous analysis, we apply

![](https://cdn.mathpix.com/cropped/2024_06_04_14fc3c27806e35bd0791g-02.jpg?height=461&width=616&top_left_y=1775&top_left_x=1123)

Figure 1: The visual comparison suggests the number of tokens in $G_{0}$ is larger than $G_{1}$, with Mann-Whitney U test suggesting the significance of these differences. This implies that LLMs struggle to solve longer MWPs, which is similar to humans.
the one-sided Mann-Whitney test [13]:

$$
H_{0}: P(X<Y)=P(X>Y) \leftrightarrow H_{1}: P(X<Y)>P(X>Y)
$$

The results are reported following [14]: There is significant evidence indicating that the number of tokens in $G_{0}$ is less than in $G_{1}$ with $U=141565, P=0.0000$, which suggests LLMs perform better on short MWPs than longer ones, similar to human problem solvers.

To address the potential condounding factors, we further explore whether longer problem correlates with increased problem difficulty. We utilize the number of steps required by GPT-3.5-turbo to solve the problems as a proxy for problem difficulty [15, 8], denoted by $S$, a discrete random variable with $k$ levels. We assumed that the number of problem tokens is denoted by $T$. To analyze this relationship, we consider the following linear model:

$$
\log T=\beta_{0}+\beta_{1} S_{1}+\beta_{2} S_{2}+\ldots+\beta_{k-1} S_{k-1}+\varepsilon
$$

where $\beta_{i}$ is the coefficient corresponding to the difference in the dependent variable associated with the $i$-th level of $S$ to the reference category. The logarithmic transformation of $T$ is to ensure homogeneity of variance, confirmed by a Levene's test [16] for homogeneity of variance ( $P=$ $0.1518)$. Subsequently, we proceeded with a contrast test [17]:

$$
H_{0}: C \cdot \beta=0 \leftrightarrow H_{1}: C \cdot \beta \neq 0
$$

where $C$ is the contrast coefficients (Appendix B.5], and $\beta=\left[\beta_{1}, \ldots, \beta_{k-1}\right]$. The results indicate insufficient statistical evidence to confirm the linear contrast $(F=1.9158, P=0.1476)$, suggesting insufficient evidence that a direct relationship exists between the problem length and its difficulty.

These findings eliminate the potential confounding factor that longer questions are more difficult and compellingly indicate that, similar to human solvers, LLMs may be lost in longer MWPs.

### 2.2 Dataset Creation

To conduct a more comprehensive analysis of CoLeG across a range of LLMs, we have created EGSM as a testing ground. Our approach leverages GPT-4-turbo to extend the original GSM8K benchmark. Specifically, we primarily construct the data that should meet the following three requirements: 1) The context of problems should be longer. 2) The ground-truth answers should remain the same. 3) The conditions and their order should not change. This section will detail the construction process.

Initial trails revealed that generated questions were only slightly longer than their original questions, and GPT-4-turbo failed to achieve a specified token length as set out in the instructions. To overcome this issue and facilitate the extension of math problems into more elaborate contexts, we adopted a sequential, iterative strategy. The process commences with the GSM8K [6] test set ${ }^{1}$ During the $r$-th iteration, the $i$-th question from the preceding iteration $(r-1)$, denoted as $q_{i}^{r-1} \in Q_{r-1}$, is extended using 2 -shot

Round-r Extension Prompt
[Instruction]:
Please extend the following math question, you need to
meet the following guidelines:

1. The context of the problem should be longer.
2. The ground-truth answer should remain the same.
3. The conditions and their order should not change.
[Few-shot Demonstrations]:
Original question 1: $d_{1}^{r-1}$; Extended question 1: $d_{1}^{r}$
Original question 2: $d_{2}^{r-1}$; Extended question 2: $d_{2}^{r}$
[Question to Extend]:
Original question: $q_{i}^{r-1} ;$ Extended question 1: $[. .$.
GSM8K
$Q_{0}$

Figure 2: E-GSM creation process and prompt template for extension. demonstrations with GPT-4-turbo. Following the expansion in the $r$-th round, quality control is performed to ensure quality (see Section 2.3), resulting in the refined set of extended problems, $Q_{r}$. The prompt structure, along with the entire extension process, is depicted in Figure $22^{2}$ Human evaluation and heuristic methods are conducted to ensure the quality of LLM extended questions (Section 2.3).

As the expansion progresses, we observe a gradual increase in the length of questions. The average tokens for MWPs in each stage are presented in Table 1. In the later phases, a deceleration in the rate of increase in the number of tokens is observed, leading us to end at $R=4$. As a result, our E-GSM incorporates extended problems from all rounds $\cup_{r=1}^{R} Q_{r}$. Examples are provided in Appendix A.1.[^1]

### 2.3 Quality Control

After obtaining extended questions, we have to ensure they meet our requirements mentioned in Section 2.2. We perform quality control process in the following two steps: 1) We apply human evaluation on a subset of E-GSM to precisely assess the quality of questions. Evaluation results shows that most of questions possess excellent quality. 2)

| Round | $Q_{0}$ | $Q_{1}$ | $Q_{2}$ | $Q_{3}$ | $Q_{4}$ |
| :--- | :---: | :---: | :---: | :---: | :---: |
| \#Tokens | 77.0 | 192.2 | 301.5 | 363.5 | 385.7 |
| \#Questions | 1319 | 1195 | 1143 | 1102 | 1068 |

Table 1: The average tokens of E-GSM dataset where the number of tokens is returned by GPT-4-turbo and the number of questions in each round of extension. We devise two heuristics to automatically filter undesired extended questions of the entire dataset.

Fifty seed questions are randomly selected from GSM8K and all questions derived from these selected questions are manually inspected (200 in total, with 50 seed problems per round). Three annotators assess the quality of the extended questions based on specific criteria and assign a quality level to each question. The final quality level is determined through majority voting. The judging criteria for each quality level and several examples are provided in Appendix A.2 Our evaluation finds that 185 of the 200 extended problems are of excellent quality, 4 are good, and 11 are poor.

Due to resource constraints, it is impractical to conduct human evaluations across the entire dataset. Instead, we employ two heuristics to eliminate problematic instances. We guarantee that these methods will effectively detect all substandard examples within our selected questions and then apply them to the full dataset to ensure its quality.

First, we ensure that the extended variant faithfully retains information from the seed question through computing entailment score between the two. For the $i$-th seed question $q_{i}^{0}$ and its associated extended variants $q_{i}^{1}, \ldots, q_{i}^{R}$, we derive a score $S_{i}^{r}=\operatorname{ENT}\left(q_{i}^{r}, q_{i}^{0}\right)$ to evaluate the informational equivalence. More specifically, we adopt the out-of-box entailment model [18] to calculate $S_{i}^{r}$ (see Appendix A.3). A small $S_{i}^{r}$ can suggest a potentially unsatisfactory extension, leading to the exclusion of the question $q_{i}^{r}$ if $S_{i}^{j}<\tau_{1}$, for any $j \leq r$. Obviously, $Q_{r} \subseteq Q_{r-1}$ after filtration.

Second, we consider an extended variant unsolvable and filter it out if it cannot be addressed by proficient LLMs using various prompting methods. More concretely, GPT-4-turbo and Claude-3-opus with 6 different prompting methods are used to generate solutions for each question (see Section 4.1), and questions whose accuracy across 12 solutions is less than $\tau_{2}$ are discarded.

From human evaluation of a subset, we can see that most of our extended questions are of excellent quality. We adjust the thresholds $\tau_{1}$ and $\tau_{2}$ based on human evaluation results, successfully filtering out examples of poor quality by setting $\tau_{1}=\tau_{2}=0.2$. The number of questions retained in each round is shown in Table 1

### 2.4 Evaluation of Efficacy and Robustness on E-GSM

We propose two metrics to evaluate the performance of LLMs on E-GSM from two different angles: efficacy and robustness. For efficacy, our goal is to check whether a question and all its corresponding variants can be consistently solved, thereby evaluating the model's capability to accurately solve the same question regardless of variations in context length and circumvanting potential randomness. For robustness, the relative performance drop of the accuracy from $Q_{0}$ to $Q_{R}$ is used. For a given question $q$, its ground-truth answer is denoted by $\operatorname{gt}(q)$, and the answer generated by the method $\mathcal{M}$ is represented as $\mathcal{M}(q)$. The following metrics are considered:

Round- $r$ accuracy $\operatorname{Acc}_{r}(\mathcal{M})=\operatorname{Acc}\left(\mathcal{M} ; Q_{r}\right)$ is defined as the average accuracy of method $\mathcal{M}$ on the set of problems $Q_{r}$.

$$
\operatorname{Acc}\left(\mathcal{M} ; Q_{r}\right)=\frac{\sum_{q \in Q_{r}} \mathbb{I}[\mathcal{M}(q)=\operatorname{gt}(q)]}{\left|Q_{r}\right|}
$$

where $|\cdot|$ denotes the number of questions in a set.

| Question: Shannon makes herself iced coffee by brewing hot coffee and adding ice cubes. Each ice cube cools the coffee by 13 degrees but <br> makes the coffee 12 milliliters weaker. Any added liquid also weakens the coffee. How many milliliters weaker is Shannon's iced coffee when it <br> is cooled by 65 degrees and she adds 15 milliliters of cream? |  |
| :---: | :---: |
| Let's think step by step. | Step 1: Calculate how many ice cubes Shannon needs to cool the coffee by 65 degrees. <br> To cool the coffee by 65 degrees, Shannon needs to use $65 / 13=5$ ice cubes. <br> Each ice cube makes the coffee 12 milliliters weaker, so 5 ice cubes will make the |
| $0-\mathrm{CoT}$ | coffee $5 * 12=60$ milliliters weaker. |
| Let's first understand the problem, <br> then list all the known conditions <br> which are formed by numbers or <br> quantitative relationships along <br> with their contexts from problem <br> text, and identify the final goal of <br> the problem. Then let's solve the <br> problem step by step. <br> CoRe + O-CoT | 1. Known Conditions: <br> ..... Shannon cools the coffee by 65 degrees. Shannon adds 15 milliliters of cream. <br> 2. Goal: <br> Calculate the total weakening effect on Shannon's iced coffee when it is cooled by 65 <br> degrees and 15 milliliters of cream are added. <br> 3. Solution Steps: <br> .... Add the weakening effect of the cream: 60 milliliters +15 milliliters $=75$ <br> milliliters. Shannon's iced coffee becomes 75 milliliters weaker when cooled by 65 <br> degrees and 15 milliliters of cream are added. |

Figure 3: A comparison between solving a lengthy math problem with $0-\mathrm{CoT}$ and $\mathrm{CoRe}+0-\mathrm{CoT}$.

CoLeG-E quantifies the efficacy of CoLeG, which is defined as the averaged accuracy of $\mathcal{M}$ to solve a seed question and all its corresponding extension variants.

$$
\operatorname{CoLeG-E}(\mathcal{M})=\frac{\sum_{q_{i}^{R} \in Q_{R}}\left[\wedge_{r=1}^{R} \mathbb{I}\left[\mathcal{M}\left(q_{i}^{r}\right)=\operatorname{gt}\left(q_{i}^{r}\right)\right]\right]}{\left|Q_{R}\right|}
$$

indicating that CoLeG-E evaluates the proportion of original problems that can be consistently solved across all levels of extended context.

CoLeG-R assesses the robustness facet of CoLeG, which is characterized by the relative accuracy drop rate on $Q_{R}$ compared to the performance on initial questions $Q_{0}$ :

$$
\operatorname{CoLeG}-\mathrm{R}(\mathcal{M})=1-\frac{\operatorname{Acc}_{0}(\mathcal{M})-\operatorname{Acc}_{R}(\mathcal{M})}{\operatorname{Acc}_{0}(\mathcal{M})}
$$

## 3 Methodology

In this section, we detail our novel approaches aimed at enhancing CoLeG and describe our experimental setup. Given that access to the model weights of proprietary LLMs is restricted, we have developed a new instructional prompt, to increase the abilities of LLMs in solving long MWPs (Section 3.1). For open-source LLMs, we introduce a new data augmentation technique, extension, to boost CoLeG by enriching the training data (Section 3.2).

### 3.1 Condition-Retrieving Instruction for Proprietary LLMs

Although proprietary LLMs exhibit strong math reasoning capability, they are still negatively impacted by long context in solving MWPs. Through our delicate inspection of their generated solutions, we observed that LLMs with conventional prompting methods often struggle to extract relevant conditions from extensive contexts. Drawing inspiration from cognitive load theory [9], which suggests that humans are only conscious of the contents that exist in a limited-size working memory and all other information is hidden until it is swapped into working memory, we suppose that a similar mechanism exists for LLMs in solving MWPs. Long MWPs make the "LLMs' working memory" saturated by story details, and missing key conditions that are essential to solve the problem. To alleviate this issue, we propose a novel Condition-Retrieving Instruction (CoRe), which begins by guiding LLMs to identify the conditions and ultimate objective of the given problem, making sure "LLMs" working memory" is filled with essential conditions rather than story details, and then apply other reasoning modules to solve the original problem. Any zero-shot prompting methods can serve as reasoning modules of CoRe. As shown in Figure 3, LLMs with 0-shot CoT miss "she adds 15 milliliters of cream after the coffee is cooled", not getting the correct answer. In contrast, with the help of CoRe, LLMs could precisely parse conditions and the ultimate goal from the original problem, and accurately deduce the numerical answer, showcasing the effectiveness of our instruction. For more comparative examples, please refer to Appendix E.

### 3.2 Extension as an Auxiliary Task for Open-source LLMs

Extracting information from a lengthy context to perform mathematical reasoning is essential for real-world tasks. However, current LLMs have significantly lower accuracy in longer MWPs (Section 2.1), motivating us to extend longer questions to improve reasoning capability. We can generate longer MWPs using extension technique introduced in Section 2.2. The quality control heuristic in Section 2.3 can be applied to filter out extended questions with poor quality. The new questions and their generated reasoning paths are collected as augmented data:

$$
\mathcal{D}_{r}=\left\{\left(q_{i}^{r}, e_{i j}^{r}, a_{i j}^{r}\right): a_{i}^{r}=\operatorname{gt}\left(q_{i}^{r}\right) ; i=1,2, \ldots, N_{r} ; j=1,2, \ldots, K_{i j}^{r}\right\}
$$

where $r$ denotes the round of extension, $i$ is the index of questions, $e_{i j}^{r}, a_{i j}^{r}$ are augmented reasoning path and corresponding answer for the $i$-th question in $r$-th round of extension $q_{i}^{r}, \operatorname{gt}(\cdot)$ is the groundtruth answer, $N_{r}$ is the number of questions left in $r$-th round after quality control, and $K_{i j}^{r}$ is the number of augmented CoT paths for $q_{i}^{r} 3$

We fine-tune an LLM (parameterized by $\boldsymbol{\theta}$ ) on $\mathcal{D}=\mathcal{D}_{0} \cup \mathcal{D}_{1}$ by maximizing the log likelihood of the reasoning path and corresponding answer conditioned on the question. The specific loss we used during supervised fine-tuning is given as follows:

$$
\mathcal{L}(\boldsymbol{\theta})=\sum_{(q, r, a) \in \mathcal{D}} \log \mathbb{P}(r, a \mid q ; \boldsymbol{\theta})
$$

### 3.3 Experimental Setup

Investigated LLMs. For the investigation of prompting methods, we include mainstream proprietary LLMs such as GPT-4 [4], GPT-3.5-turbo, GPT-3.5-instruct, GLM-3 [19], Gemini-pro [20], and Claude-3. For fine-tuning open-source LLMs, our study encompasses LLaMA-2 [21], LLaMA-3 across different model scales, Mistral-7B [22] and specialized mathematical models across different base models and model sizes, including WizardMath [23], MAmmoTH [24], MetaMath [25], and Llemma [26]. More detailed descriptions of these LLMs can be found in Appendix B. 1

Investigated Prompting Techniques. To negate the influence of few-shot demonstrations, we explore zero-shot prompting techniques, including zero-shot CoT (0-CoT) [27], Plan-and-Solve (PS)[28], and a variant of PS (PS+). A brief introduction of these prompts are covered in Appendix D Additionally, we explore the role of self-consistency (SC) [29] in improving CoLeG of LLMs. For our implementation of SC, we generate 5 responses using a sampling temperature of 0.7 .

SFT Dataset. To generate our augmented dataset, We generate five reasoning paths for each question in the training set with GPT-3.5-turbo and remove any paths that led to incorrect final answers. Apart from 7,473 annotated examples available in GSM8K training set, we get $\mathcal{D}_{0}$ that incorporate 38,507 valid CoT data points and $\mathcal{D}_{1}$ that includes 26,422 CoT data for extended quesitons. The entire training set, represented as $\mathcal{D}=\mathcal{D}_{0} \cup \mathcal{D}_{1}$, incorporates 64,929 CoT data. Appendix B. 2 shows the input-output formats of the SFT data and examples from both $\mathcal{D}_{0}$ and $\mathcal{D}_{1}$. We also collect $\mathcal{D}_{2}$ of 24,147 CoT data to study the effect of scaling up the SFT data (see Section C.4.

SFT Setup. Multiple open-source LLMs, differing in scale and base models, are fine-tuned on $D$. The fine-tuning is carried out over 3 epochs and the results are reported using 0 -CoT with the vLLM library ${ }^{4}$. All evaluations adhere to the same 0 -CoT instruction set to ensure consistency. To account for the potential influence of random variations during the training process, we include the performance trends throughout the entire training period in Appendix C.1. Furthermore, comprehensive details regarding the training and inference parameters can be found in Appendix B. 4 .

Answer Extraction. To report the results, we use GPT-3.5-turbo to extract the final answers in case that responses may not follow the desired format. Our answer extraction methods successfully get the final answer $99.8 \%$ and details are provided in Appendix B. 3 The specific designs for these zero-shot prompts are delayed to Appendix $\mathrm{D}$, and the outcomes are presented in Section 4.1 .[^2]

## 4 Results and Analysis

### 4.1 Prompting Results of Proprietary LLMs

We compare the performance of different proprietary LLMs with various zero-shot prompting techniques on E-GSM. Representative results are shown in Table 2, and full results are given in Appendix C.2. From Table 2 we have following conclusions:

The accuracy of proprietary LLMs on EGSM degrades with the extension round increases, and LLMs with stronger capability are less affected by the long context. On average, Acc ${ }_{0}$ surpasses $\mathrm{Acc}_{4}$ by $16.1 \%$, indicating that the reasoning abilities of LLMs are significantly hindered by the long textual context. From Table 2, we can observe that the CoLeG-R of Claude-3-opus with all of three prompting techniques are greater than $84 \%$, while CoLeG-R of Gemini-pro and GPT-3.5-turbo are mostly lower than $81 \%$, indicating that the math reasoning ability of these LLMs with weak language capability is more vulnerable to long contexts.

CoRe enhances the performance on $E$ GSM across almost all proprietary LLMs with various investigated prompts. The improvements are particularly evident in higher rounds of E-GSM. Take GPT-3.5-turbo for example, CoRe achieves an improvement over 0 -CoT by $2.51 \%$ on $\mathrm{Acc}_{0}$ and $5.34 \%$ on Acc $_{4}$. Case study (Appendix E has shown that our approach for solving long MWPs, attempting to extract useful given conditions, final goal and disregard the unimportant information first, then conducts sophisticated math reasoning based on the carefully extracted information to solve the problem. SC can also be combined with CoRe to further improve performance on $\mathrm{E}$ -

| LLMs | CoLeG-E | CoLeG-R | Acc $_{0}$ | Acc $_{4}$ |
| :---: | :---: | :---: | :---: | :---: |
| Claude-3-opus |  |  |  |  |
| PS | 74.81 | 84.07 | 95.45 | 80.24 |
| + CoRe | 78.00 | $\mathbf{8 7 . 0 7}$ | 95.60 | $\mathbf{8 3 . 2 4}$ |
| PS+ | 75.00 | 84.79 | 95.30 | 80.81 |
| + CoRe | $\mathbf{7 8 . 2 8}$ | 86.01 | $\mathbf{9 5 . 9 1}$ | 82.49 |
| 0-CoT | 74.72 | 86.08 | 94.09 | 80.99 |
| + CoRe | 77.81 | 86.29 | 95.38 | 82.30 |
| Gemini-pro |  |  |  |  |
| PS | 46.25 | 77.35 | 80.74 | 62.45 |
| + CoRe | 47.10 | 79.48 | 80.82 | 64.23 |
| PS+ | 48.22 | 79.19 | 80.52 | 63.76 |
| + CoRe | 49.91 | $\mathbf{8 1 . 9 9}$ | 80.74 | 66.20 |
| 0-CoT | 49.20 | 75.69 | 83.70 | 63.36 |
| + CoRe | $\mathbf{5 3 . 6 5}$ | 81.44 | $\mathbf{8 3 . 7 0}$ | $\mathbf{6 8 . 1 6}$ |
| GPT-3.5-turbo |  |  |  |  |
| PS | 41.76 | 80.43 | 78.70 | 63.30 |
| + CoRe | 42.98 | 81.86 | 79.38 | 64.98 |
| + SC | 57.40 | 81.65 | 86.81 | 70.88 |
| PS+ | 48.03 | 81.14 | 80.67 | 65.45 |
| + CoRe | 50.09 | 81.12 | 81.96 | 66.48 |
| + SC | 61.24 | 83.98 | 87.19 | 73.22 |
| 0-CoT | 46.63 | 79.63 | 80.89 | 64.42 |
| + CoRe | $\mathbf{5 1 . 9 7}$ | $\mathbf{8 3 . 6 4}$ | $\mathbf{8 3 . 4 0}$ | $\mathbf{6 9 . 7 6}$ |
| + SC | $\underline{63.20}$ | $\underline{84.14}$ | $\underline{88.25}$ | $\underline{74.25}$ |

Table 2: Representative results of proprietary LLMs on E-GSM. All figures are in \%. Bold figures represent the best results w/o SC. GSM, leading to an average improvements of $15.14 \%$ and $2.86 \%$ on CoLeG-E and CoLeG-R, respectively.

### 4.2 SFT Results of Open-source LLMs

| Model | CoLeG-E | CoLeG-R |
| :---: | :---: | :---: |
| LLaMA-2-7B | 4.31 | 66.71 |
| + SFT on $\mathcal{D}$ | 28.09 | 80.97 |
| LLaMA-2-13B | 8.61 | 70.18 |
| + SFT on $\mathcal{D}$ | 37.27 | $\mathbf{8 4 . 7 8}$ |
| Mistral-7B | 13.01 | 61.66 |
| + SFT on $\mathcal{D}$ | 48.50 | 83.65 |
| LLaMA-2-70B | 26.50 | 81.19 |
| + SFT on $\mathcal{D}$ | $\mathbf{4 9 . 8 1}$ | 84.57 |

Table 3: Results (\%) of the effect of SFT on CoLeG of Open-source General Pre-trained LLMs.

| Model | CoLeG-E | CoLeG-R |
| :---: | :---: | :---: |
| deepseek-math-7b | 42.98 | 69.07 |
| + SFT on $\mathcal{D}$ | 51.31 | 80.34 |
| WizardMath-70B | 45.97 | 78.75 |
| + SFT on $\mathcal{D}$ | 54.68 | 82.60 |
| MAmmoTH-70B | 42.98 | 85.24 |
| + SFT on $\mathcal{D}$ | 55.15 | 83.25 |
| MetaMath-70B | 52.81 | 80.86 |
| + SFT on $\mathcal{D}$ | $\mathbf{5 7 . 1 2}$ | $\mathbf{8 4 . 5 5}$ |

Table 4: Results (\%) of the effect of SFT on CoLeG of Specialized Mathematical LLMs.

Representative results are summarized in Table 3 and 4 with additional results for more LLMs with varying model sizes and backbones in Appendix C.3. There are several critical observations.

SFT on $\mathcal{D}$ consistently and significantly enhances CoLeG across a range of mainstream opensource LLMs, including specialized mathematical LLMs, as reflected in metrics CoLeG-E and CoLeG-R. On average, SFT on $\mathcal{D}$ leads to an improvement of $27.81 \%$ in CoLeG-E and $13.56 \%$ in CoLeG-R. In particular, for Mistral-7B, there is a substantial absolute enhancement of $35.49 \%$ in CoLeG-E and $21.99 \%$ in CoLeG-R post-SFT. This indicates that fine-tuned LLMs demonstrate heightened effectiveness in solving MWPs with lengthy contexts and exhibit increased resilience against performance degradation in such a scenario. Remarkably, applying SFT on $\mathcal{D}$ can further improves CoLeG of various specialized mathematical LLMs, as shown in Table 4. These LLMs, such as MetaMath [25], have been adapted from generally pre-trained models to focus on the mathematical domain. They have typically undergone training with extensive CoT data related to GSM8K, making them more effective and robust in solving complex mathematical questions.

There is a noticeable trend of diminishing accuracy from one round to the next for open-source LLMs, both prior to and after SFT, and the rate of accuracy decline from round to round is mitigated post-SFT, as illustrated in Figure 4. Our focus encompasses the LLaMA-2 family for general pretrained LLMs and MetaMath for specialized LLMs, with model sizes spanning 7B, 13B, to 70B. This trend highlights the increased difficulty in solving longer questions, a challenge that mirrors the human experience, and incorporating extension as an auxiliary task can mitigate this effect. Interestingly, the accuracy of post-SFT LLaMA-2 can be comparable topre-SFT MetaMath, despite the fact that our $\mathcal{D}$ consists of only around $65 \mathrm{~K}$ CoT data compared to $400 \mathrm{~K}$ that MetaMath has been trained on. This observation suggests that extending the questions included in the training set could serve as an effective data augmentation strategy to improve the mathematical reasoning abilities of LLMs.

Extension as an auxiliary task can further improve MetaMath [25] in solving longer questions on GSM8K. While MetaMath outperforms other mathrelated LLMs on GSM8K, it still faces challenges with lengthy MWPs [25]. To investigate whether extension would help, the GSM8K test set is divided into three subsets of equal size based on question length and the accuracy is calculated over each subset. As shown in Figure 5, extension not only benefits questions in all subsets but also yields the most significant performance improvement within long length group. Scaling up augmented data of extension by adding more rounds of extension to the training set (i.e., getting $\mathcal{D}_{r}$ for larger $r$ ) can be beneficial for LLMs to solve long MWPs as well. Some preliminary results regarding scaling up extension data are given in Appendix C.4. In addition, We believe applying other data augmentation methods provided by [25] to extended questions could be another way to mitigate the challenge of longer MWPs. We leave these for future work.

![](https://cdn.mathpix.com/cropped/2024_06_04_14fc3c27806e35bd0791g-08.jpg?height=507&width=616&top_left_y=839&top_left_x=1123)

Figure 4: Acc $_{i}$ of LLaMA-2 and MetaMath families. "w" and "w/o" stand for "with" and "without" respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_14fc3c27806e35bd0791g-08.jpg?height=520&width=618&top_left_y=1605&top_left_x=1125)

Figure 5: Accuracy on GSM8K with short, medium, and long length. The range of tokens within each category is in parenthesis.

|  | GPT-3.5-turbo |  |  |  | LLaMA-2 |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Dataset | CoRe | PS | PS+ | 0-CoT | SFT | 7 7B | $13 B$ | $70 B$ |
| MAWPS | $\boldsymbol{x}$ | 90.77 | 91.78 | 91.40 | $\boldsymbol{x}$ | 67.93 | 73.41 | 77.50 |
|  | $\checkmark$ | $\mathbf{9 1 . 5 7}$ | $\mathbf{9 3 . 3 8}$ | $\mathbf{9 2 . 6 7}$ | $\checkmark$ | $\mathbf{7 2 . 5 2}$ | $\mathbf{7 8 . 2 1}$ | $\mathbf{8 7 . 7 4}$ |
| SVAMP | $\boldsymbol{x}$ | 71.90 | 75.70 | 71.80 | $\boldsymbol{x}$ | 50.50 | 61.70 | 73.30 |
|  | $\checkmark$ | $\mathbf{7 3 . 2 0}$ | $\mathbf{7 7 . 8 0}$ | $\mathbf{7 6 . 3 0}$ | $\checkmark$ | $\mathbf{6 4 . 9 0}$ | $\mathbf{7 4 . 0 0}$ | $\mathbf{8 3 . 8 0}$ |
| GSM-IC | $\boldsymbol{x}$ | 85.38 | 87.48 | 88.35 | $\boldsymbol{x}$ | 33.55 | 48.45 | 65.70 |
|  | $\checkmark$ | $\mathbf{8 5 . 9 0}$ | $\mathbf{9 1 . 6 3}$ | $\mathbf{9 0 . 0 5}$ | $\checkmark$ | $\mathbf{6 6 . 4 8}$ | $\mathbf{7 6 . 6 8}$ | $\mathbf{8 5 . 2 2}$ |

Table 5: Solving accuracy (in \%) of proprietary and open-source LLMs on other MWP benchmarks.

### 4.3 Generalization to Other Benchmarks

To evaluate generalizability, we assess CoRe and fine-tuned LLMs (without further SFT) on other benchmarks including MAWPS [10], SVAMP [11], and GSM-IC [12]. Detailed experimental setups are given in Appendix B.6. The results are shown in Table 5 .

Both our CoRe and extension lead to superior performance across all evaluated benchmarks. These benchmarks consist of MWPs with concise descriptions, making the findings particularly notable: Despite being specifically tailored for long MWPs, these approaches are also effective for shorter MWPs. Remarkably, CoRe results in a $4.5 \%$ absolute increase in accuracy on MAWPS with 0 CoT and $4.15 \%$ on GSM-IC with PS+. Additionally, without further finetuning on the corresponding training set, our models (SFT on $\mathcal{D}$ ) yield an average accuracy increase of $15.28 \%$ across different model sizes and benchmarks. This demonstrates the generalizability of our approaches in enhancing the performance of LLMs on MWPs.

## 5 Related Work

Prompting for Mathematical Reasoning. Mathematical reasoning [1, 6, 30] is recognized as a system-2 task [31, 32], attracting significant attention in the research community. Recent LLM advancements [2, 4, 33, 5] have led to diverse prompting strategies designed to enhance their ability to perform mathematical reasoning [34]. Notable among these is the Chain-of-Thought (CoT) prompting [8], which has significantly improved reasoning capabilities by encouraging the model to generate intermediate steps. While various few-shot techniques such as Re-reading [35] and Stepback prompting [36] have been developed, there is a growing interest in methodologies that enable zero-shot reasoning. These methods, such as the two-stage CoT prompting by [27], Plan-andSolve (PS) prompting [28], and the self-discovery framework by [37], aim to equip LLMs to handle mathematical problems without few-shot demonstrations.

CoT Extension. Extensions to CoT encompass demonstration selection [38, 39, 40], advancements in decoding [29, 41], and developments for more intricate tasks [42, 43, 44, 45]. Further explorations within this domain have probed areas such as incorrect answer detection [46], investigations of failure modes [47, 12, 48, 49], and application in other math-related topics [50, 51, 52]. Our work pioneers the investigation of LLMs' CoLeG in mathematical reasoning, employing a zero-shot prompting to isolate the influence of few-shot examplars.

Specialized LLMs for Mathematics. Despite general-purpose LLMs, there remains a persistent interest in maximizing the performace of domain-specific LLMs like mathematics, through strategies such as SFT and continued pretraining. Supervised fine-tuning methods, such as WizardMath's [23] combination of PPO training and MAmmoTH's [24] knowledge distillation of integrating CoT with Program-of-Thought[53, 54]. Additionally, MetaMath [25] rewrites mathematical questions in multiple formats to augment the training dataset. On the other hand, a distinct strand of research is dedicated to continuing pretraining of base LLMs on extensive mathematical corpora, exemplified by Minerva [55], Llemma [26], InternLM-MATH [56] and DeepSeekMath [57]. Our work explores to what extent CoLeG of these specialized mathematical LLMs will be improved by fine-tuning with our newly proposed auxiliary task - extension.

## 6 Conclusion

Our study explored LLMs' ability to solve longer MWPs, i.e. CoLeG. We introduced a groundbreaking dataset, E-GSM, designed to test CoLeG of LLMs, along with two metrics to evaluate the efficacy and resilience of LLMs in this setting. Our investigation highlighted a notable CoLeG deficiency in existing zero-shot prompting techniques and open-source LLMs. A new instructional prompt, CoRe, and a new data augmentation technique, extension, not only significantly strengthened CoLeG but also show superior performance on GSM8K, and generalized well to other MWP benchmarks. By illuminating a previously underexplored aspect of LLMs' reasoning and offering practical solutions to improve it, our work carved out new pathways for using LLMs in complex problem-solving and set the stage for future research on model generalizability and advanced SFT paradigms.

## Limitations

Despite the innovative contributions of newly released E-GSM, CoRe, and the extension auxiliary task, our work still has some limitations. 1. Data Quality in E-GSM: Although heuristic methods are employed to control the quality of E-GSM, the dataset may still contain poorly constructed questions. Due to budget and time constraints, no complete human review of the dataset was performed. A more thorough human evaluation by well-funded institutions would be beneficial. 2. Generalizability Concerns: While the generalizability of our methods has been assessed using GSM-IC, our primary conclusions are derived from variants of the GSM8K dataset. Including additional benchmarks for MWPs would provide a more robust validation of our findings. However, conducting experiments on more datasets is resource-intensive. Therefore, we encourage further investigation by the research community. 3. Limited SFT Data: Owing to budget limitations, our released SFT data comprises approximately $65 \mathrm{~K}$ examples. Further expansions could involve scaling up the extension augmented data, either by increasing the rounds of extension or applying other data augmented techniques. The exploration of alternative methods to extend SFT data and examining scalability effects also offers interesting avenues for future research. 4. Interpretability and Explainability: Even with the improved performance achieved by our methods, LLMs remain "black boxes." Understanding the intricate reasoning processes behind their solutions continues to be a significant challenge.

## Broader Impacts

Our release of E-GSM presents a challenging benchmark to test the ability of LLMs to solve long MWPs. This aids future research by highlighting the intricacies involved in extended MWPs. Our innovative prompting method and the SFT technique have improved LLMs performance in these problems, potentially enhancing their generalization capabilities for real-world applications. This advancement not only broadens LLMs' mathematical reasoning capacities, but also has implications for educational tools. However, despite progress, significant challenges in solving extended MWPs remain, calling for future refinements. Additionally, there is a risk of overreliance which might impede the development of critical thinking skill and problem-solving skills for human learners Therefore, we point out the need for balanced use of LLM technologies, combining their benefits with critical human oversight.

## References

[1] Daniel Bobrow et al. Natural language input for a computer problem solving system. 1964.

[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc' Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.

[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023.

[4] OpenAI. Gpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023.

[5] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.

[6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021.

[7] Jo Boaler. The role of contexts in the mathematics classroom: Do they make mathematics more" real"? For the learning of mathematics, 13(2):12-17, 1993.

[8] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.

[9] John Sweller, Jeroen JG Van Merrienboer, and Fred GWC Paas. Cognitive architecture and instructional design. Educational psychology review, 10:251-296, 1998.

[10] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: A math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 1152-1157, 2016.

[11] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021.

[12] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 31210-31227. PMLR, 2023.

[13] Henry B Mann and Donald R Whitney. On a test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics, pages 50-60, 1947.

[14] Catherine O Fritz, Peter E Morris, and Jennifer J Richler. Effect size estimates: current use, calculations, and interpretation. Journal of experimental psychology: General, 141(1):2, 2012.

[15] Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Learning from mistakes makes $11 \mathrm{~m}$ better reasoner. ArXiv preprint, abs/2310.20689, 2023.

[16] Howard Levene. Robust tests for equality of variances. Contributions to probability and statistics, pages 278-292, 1960.

[17] John Neter, Michael H Kutner, Christopher J Nachtsheim, William Wasserman, et al. Applied linear statistical models. 1996.

[18] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. SummaC: Re-visiting NLI-based models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163-177, 2022.

[19] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. ArXiv preprint, abs/2210.02414, 2022.

[20] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. ArXiv preprint, abs/2312.11805, 2023.

[21] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023.

[22] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. ArXiv preprint, abs/2310.06825, 2023.

[23] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. ArXiv preprint, abs/2308.09583, 2023 .

[24] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. ArXiv preprint, abs/2309.05653, 2023.

[25] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. ArXiv preprint, abs/2309.12284, 2023.

[26] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. ArXiv preprint, abs/2310.10631, 2023.

[27] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, $35: 22199-22213,2022$.

[28] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. ArXiv preprint, abs/2305.04091, 2023.

[29] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. ArXiv preprint, abs/2203.11171, 2022.

[30] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. ArXiv preprint, abs/2103.03874, 2021.

[31] Daniel Kahneman. Thinking, fast and slow. macmillan, 2011.

[32] Yoshua Bengio, Yann Lecun, and Geoffrey Hinton. Deep learning for ai. Communications of the ACM, 64(7):58-65, 2021.

[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.

[34] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. ArXiv preprint, abs/2402.00157, 2024.

[35] Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, and Jian-guang Lou. Re-reading improves reasoning in language models. ArXiv preprint, abs/2309.06275, 2023 .

[36] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny Zhou. Take a step back: Evoking reasoning via abstraction in large language models. ArXiv preprint, abs/2310.06117, 2023.

[37] Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed H Chi, Denny Zhou, Swaroop Mishra, and Huaixiu Steven Zheng. Self-discover: Large language models self-compose reasoning structures. ArXiv preprint, abs/2402.03620, 2024.

[38] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. ArXiv preprint, abs/2210.03493, 2022.

[39] Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. Active prompting with chain-ofthought for large language models. ArXiv preprint, abs/2302.12246, 2023.

[40] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. ArXiv preprint, abs/2210.00720, 2022.

[41] Xuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. ArXiv preprint, $\mathrm{abs} / 2402.10200,2024$.

[42] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv preprint, abs/2210.03629, 2022.

[43] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv preprint, abs/2305.10601, 2023.

[44] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. ArXiv preprint, $\mathrm{abs} / 2308.09687,2023$.

[45] Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought generation. ArXiv preprint, abs/2311.04254, 2023.

[46] Xin Xu, Shizhe Diao, Can Yang, and Yang Wang. Can we verify step by step for incorrect answer detection? ArXiv preprint, abs/2402.10528, 2024.

[47] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on" a is b" fail to learn" b is a". ArXiv preprint, abs/2309.12288, 2023.

[48] Xinyun Chen, Ryan A Chi, Xuezhi Wang, and Denny Zhou. Premise order matters in reasoning with large language models. ArXiv preprint, abs/2402.08939, 2024.

[49] Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. ArXiv preprint, abs/2402.19255, 2024.

[50] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang $\mathrm{Xu}$, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. ArXiv preprint, abs/2312.11370, 2023.

[51] Zezheng Song, Jiaxin Yuan, and Haizhao Yang. Fmint: Bridging human designed and data pretrained models for differential equation foundation model. ArXiv preprint, abs/2404.14688, 2024.

[52] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476-482, 2024.

[53] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. ArXiv preprint, abs/2211.12588, 2022.

[54] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764-10799. PMLR, 2023.

[55] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.

[56] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. ArXiv preprint, abs/2402.06332, 2024.

[57] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv preprint, abs/2402.03300, 2024.

[58] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proc. of NAACL-HLT, pages 1112-1122, New Orleans, Louisiana, 2018. Association for Computational Linguistics.

| $Q_{0}$ |
| :--- |
| A mother goes shopping. She buys cocoa at $\$ 4.20$, laundry at $\$ 9.45$ and a package of pasta at $\$ 1.35$. She |
| pays $\$ 20$. How much change does the cashier give back? |
| $Q_{1}$ |
| On a bright Saturday morning, a mother decided to take advantage of the weekend sales at her local |
| supermarket. With a shopping list in hand, she navigated through the aisles, picking up items she needed |
| for the week. Among her finds were a rich, dark cocoa powder priced at $\$ 4.20$, essential for her famous |
| chocolate cake. Next, she grabbed a bottle of laundry detergent, a necessity for the upcoming week's laundry, |
| priced at $\$ 9.45$. Lastly, she couldn't resist adding a package of pasta to her cart, a steal at just $\$ 1.35$, perfect |
| for Wednesday night's dinner. After browsing through the aisles and picking up a few more items, she made |
| her way to the cashier. Handing over a crisp $\$ 20$ bill to pay for her purchases, she waited for her change. |
| How much change did the cashier hand back to her after her purchases? |
| $Q_{2}$ |
| On a bright and sunny Saturday morning, the local supermarket was bustling with shoppers eager to take |
| advantage of the weekend sales. Among them was a mother, who had meticulously prepared a shopping |
| list to ensure she got everything needed for the upcoming week. As she entered the supermarket, her eyes |
| scanned the aisles for the items on her list. Her first find was a rich, dark cocoa powder, priced at $\$ 4.20$, |
| essential for baking her famous chocolate cake that her family adored. Next, she picked up a bottle of laundry |
| detergent, priced at $\$ 9.45$, a necessity for tackling the week's laundry pile. Lastly, she spotted a package of |
| pasta, priced at just $\$ 1.35$, perfect for the family's Wednesday night dinner. With her cart filled with these |
| items and a few more essentials, she confidently made her way to the cashier. After unloading her cart and |
| watching the cashier scan each item, she handed over a crisp $\$ 20$ bill to cover the cost of her purchases. As |
| the cashier processed the transaction, she anticipated the change she would receive, knowing it would be just |
| enough for a small treat for her children on the way home. How much change did the cashier hand back |
| to her after her purchases? |

Table 6: Examples from E-GSM.
