# VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS 

Jing Yu Koh Robert Lo* Lawrence Jang* Vikram Duvvur*<br>Ming Chong Lim* Po-Yu Huang* Graham Neubig Shuyan Zhou<br>Ruslan Salakhutdinov Daniel Fried<br>Carnegie Mellon University<br>$\{$ jingyuk,rsalakhu,dfried\}@cs.cmu.edu


#### Abstract

Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic visually grounded tasks. VisualWebArena comprises of a set of diverse and complex webbased tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of stateof-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-ofthe-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa


## 1 INTRODUCTION

Automating routine computer tasks with autonomous agents is a long standing goal of artificial intelligence research (Franklin \& Graesser 1996, Jennings et al. 1998). To achieve this, we need agents that can navigate computers effectively, process visual and textual inputs, handle high-level natural language instructions, and execute actions to achieve desired goals. As digital interfaces today are primarily built for human eyes, effective visual understanding is necessary for many routine computer tasks. For example, humans frequently perform tasks on the web which involve visual references, such as "Help me order a green polo shirt from Amazon". Many real world tasks also involve understanding image content: people frequently rely on pictures to communicate rather than text descriptions. Productive work done on the computer is also often explicitly visual, e.g., working with Microsoft Excel sheets, creating presentations in Google Slides, or performing creative work in Adobe Photoshop. However, many agent benchmarks today focus on text-based tasks, neglecting the evaluation (and consequently the development) of multimodal autonomous agents.

To address this gap, we propose VisualWebArena (Fig. 1), a benchmark suite designed to rigorously assess and advance the visual and textual capabilities of autonomous agents. VisualWebArena builds off the WebArena (Zhou et al. 2024) framework, which comes with reproducible self-hosted environments and execution-based evaluations. VisualWebArena introduces a set of unique visual tasks, and emphasizes integrating visual understanding with language processing, closely simulating human interaction with modern computing interfaces.[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_e22954b7a730839dd4deg-02.jpg?height=462&width=802&top_left_y=278&top_left_x=378)

"Help me make a post selling this item and navigate to it. Price it at $\$ 10$ cheaper than the

![](https://cdn.mathpix.com/cropped/2024_06_04_e22954b7a730839dd4deg-02.jpg?height=133&width=176&top_left_y=438&top_left_x=888)

"Navigate to the comments section of the latest image post in the /f/Art subreddit that contains animals."
![](https://cdn.mathpix.com/cropped/2024_06_04_e22954b7a730839dd4deg-02.jpg?height=170&width=308&top_left_y=560&top_left_x=885)

"Buy the cheapest color photo printer and send it to Emily's place (as shown in the image)."

Task Specification

ל

LLM / VLM Agent click [1602]

Figure 1: VisualWebArena is a benchmark suite of realistic classifieds, e-commerce, and Reddit sites. We benchmark state-of-the-art LLM and VLM agents on 910 diverse, natural, and visually grounded tasks that involve web navigation, action execution, and visual understanding.

By introducing VisualWebArena, we aim to bridge the existing gap in the evaluation of multimodal autonomous agents and to push the boundaries of AI-driven task automation. We envision a future where AI agents can not only process textual information but also effectively navigate and interact with visual settings (e.g., digital screens), truly augmenting human productivity and creativity. Our contributions are summarized as follows:

- We introduce VisualWebArena, a set of 910 realistic tasks over three diverse web environments: Classifieds, Shopping, and Reddit. The Classifieds environment is a new contribution with real world data, while the Shopping and Reddit environments are inherited from WebArena. All tasks we introduce are visually grounded, and require visual understanding of webpage content to effectively solve (while WebArena does not). $25.2 \%$ of our tasks also take images as input (Fig. 1), and require understanding of interleaved image-text inputs to correctly process and attempt. Our code, tasks, and baselines is publicly released 1
- We extensively benchmark the autonomous capabilities of state-of-the-art (SOTA) large language models (LLM) and vision-language models (VLMs), demonstrating that strong VLMs outperform text-based LLMs. The best VLM agents achieve a success rate of only $16.4 \%$ on VisualWebArena tasks, significantly below human performance of $88.7 \%$. Our results also highlight a large gap between API-based and open sourced VLM agents.
- We propose a new VLM agent inspired by Set-of-Marks prompting Yang et al. (2023a), simplifying the action space of the model. We show that this model substantially outperforms LLM agents, especially on sites that are more visually complex.


## 2 RELATED WORK

Language-Guided Web Agent Benchmarks The development of reproducible environments for autonomous agents has seen considerable progress in recent years. Earlier efforts introduced reinforcement learning environments (Brockman et al. 2016), but extended into the web (Shi et al. 2017; Liu et al. 2018), which laid the groundwork for reproducible web-based environments. Recent benchmarks in this domain introduced tasks involving actions on static internet pages (Deng et al. 2023) as well as interaction in simulated web environments (Yao et al., 2022; Zhou et al., 2024). AgentBench (Liu et al. 2023c) also extends the scope of agents for computer interaction beyond the web, exploring database management and operating system functionalities.

LLM Agents There has been significant recent interest in using Large Language Models (LLMs) for developing autonomous agents (Xi et al., 2023, Wang et al., 2023a). State-of-the-art LLMs (Google, 2023; OpenAI, 2023, Chowdhery et al., 2023; Rae et al., 2021; Zhang et al., 2022; Touvron et al., 2023a b; Jiang et al., 2023; 2024) based on the Transformer (Vaswani et al., 2017) architecture have demonstrated impressive abilities in learning from in-context examples (Brown et al., 2020, Chan et al., 2022), reasoning (Wei et al. 2022, Yao et al., 2023; Wang et al., 2023c,[^1]

Besta et al., 2023), following instructions (Chung et al. 2022; Longpre et al., 2023, Ouyang et al. 2022), and operating over long-context sequences (Tay et al.| 2021; Bertsch et al., 2023; Tworkowski et al. 2023). Several recent works leverage these compelling abilities for building autonomous web agents: Kim et al. (2023) propose a recursive prompting method to improve the performance of GPT-4 on the MiniWoB++ (Liu et al. 2018) benchmark. Liu et al. (2023d) propose a method of orchestrating multiple LLM agents to improve performance on web navigation in the WebShop (Yao et al. 2022) environment. Zeng et al. (2023) fine-tunes the LLaMA-2 (Touvron et al., 2023b) models on a set of interaction trajectories with instructions, improving over baseline agent models.

Vision-Language Models Finally, our work builds off advances in vision-language models (VLMs) which can process image-text inputs to generate text, used for many multimodal tasks such as image captioning (Vinyals et al., 2015), visual question answering (Antol et al., 2015), and other benchmarks (Mialon et al., 2023, Yue et al., 2023, Tong et al., 2024). Frozen (Tsimpoukelli et al. 2021) was one of the first approaches to demonstrate the effectiveness of finetuning a visual encoder to map images into the embedding space of a LLM, introducing compelling few-shot multimodal abilities. Alayrac et al. (2022) introduced cross-attention layers and scaled up models and training data to improve multimodal performance. Wang et al. (2023b) introduced trainable visual expert modules to improve the fusion between the vision and language models. Liu et al. (2023b) proposed fine-tuning on images paired with instructions to improve text generation performance on several multimodal tasks. GPT-4V (OpenAI, 2023) introduces visual processing abilities to the GPT-4 models, unlocking many compelling abilities (Yang et al., 2023c a). Gemini (Google, 2023) is multimodal from the beginning (in contrast to post-hoc fine-tuned models), and can handle text input interleaved with visual and audio inputs. Several recent work have also explored the use of strong VLMs for building visual agents for mobile platforms (Zhan \& Zhang, 2023; Chu et al. 2023, Yang et al. 2023b) as well as for the web (Gur et al., 2023; Hong et al. 2023). (Zheng et al. 2024) is contemporaneous work which performs action grounding to identify appropriate HTML elements for enabling VLM agents to execute actions on the web. In contrast, our proposed SoM agent directly leverages JavaScript to produce a Set-of-Marks (Yang et al. 2023a) for the VLM agent to directly use as both an action and observation space.

## 3 VisualWebArena ENVIRONMENT

In order to ensure reproducibility, realism, and determinism, we ensure that all websites in the VisualWebArena framework are available as standalone open-source web applications. The textual and visual content available in the websites are acquired from real world counterparts, while the code is based off open-source infrastructure commonly used in real world applications. We formally define the environment, observation space, and action space in the following sections, but encourage readers to refer to WebArena (Zhou et al. 2024) for more details.

The environment and agent can be modeled as a partially observable Markov decision process (POMDP): $\mathcal{E}=(S, A, \Omega, T)$, where $S$ represents the set of states, $A$ represents the set of actions (Sec. 3.2), and $\Omega$ represents the set of observations (Sec. 3.1). The transition function is defined as $T: S \times A \rightarrow S$, with deterministic transitions between states conditioned on actions. At each time step $t$, the environment is in some state $s_{t}$ (e.g., a particular page), with a partial observation $o_{t} \in \Omega$. An agent issues an action $a_{t} \in A$ conditioned on $o_{t}$, which results in a new state $s_{t+1} \in S$ and a new partial observation $o_{t+1} \in \Omega$ of the resulting page. The action $a_{t}$ may be an action to be executed on the webpage (Tab. 1), or it may simply be a string output for information seeking tasks (Sec. 3.3).

Finally, we define the reward function $R: S \times A \rightarrow\{0,1\}$ (Sec. 3.3) to measure the success of a task execution. In VisualWebArena, the reward function returns 1 at the final step if the state transitions align with the expectations of the task objective (i.e., the goal is achieved), and 0 otherwise. For example, in the first task in Fig. 1, the reward function evaluates whether the order was correctly placed to the exact address provided in the input image, and contains the correct item.

### 3.1 ObSERVATION SPACE

The observation space $\Omega$ is modeled after a realistic web browsing experience. Observations include the webpage URLs, opened tabs (possibly multiple tabs of different websites), and the webpage content of the focused tab. In approximately $25 \%$ of tasks, the intent also involves one or more input

![](https://cdn.mathpix.com/cropped/2024_06_04_e22954b7a730839dd4deg-04.jpg?height=570&width=1391&top_left_y=276&top_left_x=367)

Figure 2: Set-of-Marks (Yang et al., 2023a) augmented webpage screenshot. Every interactable element is highlighted with a bounding box and a unique ID.

images which are provided as part of the observation (e.g., the first and third tasks in Fig. 11. The webpage content can be represented in several different ways:

1. Raw web page HTML as a Document Object Model (DOM) tree, commonly used in previous work on autonomous web agents (Shi et al., 2017, Liu et al., 2018, Deng et al., 2023).
2. Web page screenshots, represented as RGB arrays, which has demonstrated efficacy in prior work on visual agents (Gur et al., 2023; Hong et al., 2023, Yan et al., 2023).
3. The accessibility tree ${ }^{2}$ which provides a structured and simplified representation of the webpage content that is optimized for assistive technologies. This is the primary representation that WebArena (Zhou et al., 2024) uses for its baseline LLM agents.
4. We introduce a new visual representation inspired by Set-of-Marks (SoM) prompting (Yang et al. 2023a). For every interactable element on the webpage, we label it with a bounding box and an ID (Fig. 2), producing a screenshot that allows a visual agent to reference elements on the page by their unique ID. We provide more details and analysis in Sec. 5.3

### 3.2 Action SPACE

The full set of actions $A$ is summarized in Tab. 1. The arguments for action $a_{t}$ is the unique element ID from the current observation $o_{t}$. An advantage of this representation (over predicting $(x, y)$ coordinates) is that it allows us to focus on high level reasoning rather than low-level control, as many SOTA VLMs and LLMs were not explicitly trained for referencing elements at such fine granularity. For the agents with accessibility tree representations, the argument is the element ID in the tree. For the SoM representation, we use the unique IDs assigned in the current page (see Fig. 2).

| Action Type $a$ | Description |
| :--- | :--- |
| click [elem] | Click on element elem. |
| hover [elem] | Hover on element elem. |
| type [elem] [text] | Type text on element elem. |
| press [key_comb] | Press a key combination. |
| new_tab | Open a new tab. |
| tab_focus [index] | Focus on the i-th tab. |
| tab_close | Close current tab. |
| goto [url] | Open url. |
| go_back | Click the back button. |
| go_forward | Click the forward button. |
| scroll [up\|down] | Scroll up or down the page. |
| stop [answer] | End the task with an optional output. |

Table 1: Set of possible actions $A$.

### 3.3 EVALUATION

In order to evaluate performance on VisualWebArena, we introduce new visually grounded evaluation metrics to the functional evaluation paradigm of WebArena. These allow us to comprehensively evaluate the correctness of execution traces on open ended visually grounded tasks. The rewards for each task are hand designed functions using the primitives described below.[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_e22954b7a730839dd4deg-05.jpg?height=786&width=1393&top_left_y=276&top_left_x=363)

Table 2: Various evaluation metrics to assign reward $r(s, a) \in R: S \times A \rightarrow\{0,1\}$. Our executionbased reward primitives allow us to benchmark many diverse, realistic, and open-ended tasks.

Information Seeking Tasks Information seeking tasks (e.g., the first task in Tab. 2) expect a string output $\hat{a}$ from the model. We mostly adopt similar reward functions introduced in WebArena for measuring text correctness against a groundtruth output $a^{*}$ :

- exact_match: This can be defined as $\mathbf{1}_{\left\{\hat{a}=a^{*}\right\}}$ (where 1 represents the indicator function). Only outputs that are exactly equal to the groundtruth are given a score of 1 . This is used in tasks where an exact response (e.g., a numerical answer) is expected.
- must_include: This reward function gives a score of 1 if all elements in $a^{*}$ are contained in $\hat{a}$ and 0 otherwise. For example, if $\hat{a}=$ "\$1.99, \$2.50, \$10.00" and $a^{*}=$ $\{" 1.99 ", " 2.50 ", " 10.00 "\}$, the task is awarded a score of 1 as all expected elements are present in the output. This is primarily used in tasks where we expect an unordered list of outputs, or we expect text output to contain a particular keyword.
- must_exclude: This is a new function we introduce, which is the converse of must_include. A reward of 0 is assigned if any element from a specified set $a^{*}$ is found in $\hat{a}$ (and 1 otherwise). For instance, if $\hat{a}=" \$ 1.99, \$ 2.50, \$ 10.00 "$ and $a^{*}=\{" 1.50 ", " 2.00 "\}$, the reward is 1 as none of the prohibited elements are in the output.
- fuzzy_match: This function queries a LLM (in our implementation, gpt-4-1106-preview, to evaluate whether $a^{*}$ and $\hat{a}$ are semantically equivalent. The LLM is prompted to output either "correct", "incorrect", or "partially correct", and we assign a reward of 1 if the output is "correct" and 0 otherwise 3

In addition, we also introduce several new visual functions for measuring open ended tasks:
- eval_vqa: Similar to fuzzy_match, this function queries a VLM capable of performing visual question answering (VQA) (Antol et al., 2015). We use BLIP-2-T5XL Li et al. (2023) in our implementation. We query the VLM with an image and a question. If the output of the VLM contains the groundtruth answer $a^{*}$, a reward of 1 is assigned (and 0 otherwise). This is useful for evaluating more open ended tasks, e.g., "Buy me a green hoodie under \$10.". There are many possible products that satisfy this objective, and it would be infeasible to enumerate all their IDs.
- eval_fuzzy_image_match: This function checks whether a query image is similar to a groundtruth image according to the structural similarity index measure (SSIM) (Wang et al.[^3]

Distribution of Tasks Across Sites

![](https://cdn.mathpix.com/cropped/2024_06_04_e22954b7a730839dd4deg-06.jpg?height=504&width=591&top_left_y=344&top_left_x=385)

Figure 3: Tasks proportion by sites.
Distribution of Tasks by Difficulty

![](https://cdn.mathpix.com/cropped/2024_06_04_e22954b7a730839dd4deg-06.jpg?height=526&width=707&top_left_y=320&top_left_x=1034)

Figure 4: Tasks proportion by difficulty.

2004). If the SSIM between a query image and the groundtruth image is higher than a prespecified threshold $t \in[0,1]$, a reward of 1 is assigned, and 0 otherwise.

Navigation and Actions Many tasks in VisualWebArena require navigating through multiple webpages, and executing actions to change the underlying state $s$ of the environment. To accurately evaluate certain objectives, we require reward functions that examine the final webpage state to determine whether the task was successfully accomplished. These adopt the WebArena programmatic evaluation metrics, as well as the eval_vqa and eval_fuzzy_image_match metrics for measuring performance on visual or open-ended tasks.

Each evaluator consists of a locator as well as a URL. The URL can be a specific page, or a function (e.g., the last page that the agent navigated to). The locator describes the object on the page that should be examined (e.g., all img elements, or all elements with the . product-image-photo class). During evaluation, we use the locator to retrieve the corresponding image or text content, and reuse the functions from the information seeking tasks to check for correctness.

## 4 CURATING VISUALLY GROUNDED TASKS

### 4.1 WEB ENVIRONMENTS

VisualWebArena is designed around three realistic web environments that involve visually rich content. Several tasks also require referencing information from a self-hosted Wikipedia knowledge base, and others involve interaction across more than one of these websites (Fig. 3).

Classifieds We introduce a new Classifieds website in VisualWebArena, inspired by real world marketplaces such as Craigslist and Facebook Marketplace. This new site provides a distinct environment compared to existing ones in WebArena, introducing visually grounded tasks centered around user interactions typical in online classifieds websites (posting, searching, commenting). The site's infrastructure uses OSClass ${ }^{4}$ a robust open-source Content Management System (CMS) designed for classifieds websites, used in multiple real world sites. OSClass enables us to simulate functions such as search, posting new listings, commenting, and leaving reviews and ratings. The site contains 65,955 listings, each consisting of a title, a description, and a product image.

Shopping The Shopping site follows the e-commerce environment from WebArena (Zhou et al., 2024), with product information and content scraped from Amazon and released in WebShop (Yao et al. 2022). Visual understanding of product images is required for successfully navigating and completing tasks on e-commerce platforms, making this a natural choice for VisualWebArena. Most products on the site include at least one image, and the platform covers a diverse range of products.[^4]

Reddit The Reddit site also follows the same environment from WebArena, and represents a social forum platform. The site contains 31,464 posts containing a diverse set of images across different subreddits and forums, such as natural images, memes, consumer electronics, and charts. The inclusion of the Reddit environment allows us to create a comprehensive benchmark that evaluates an agent's ability to understand and operate with different types of visual information.

### 4.2 TASKS

We introduce a set of 910 new tasks, split across the three sites detailed earlier (Fig. 3). These tasks necessitate visual comprehension, and are designed to assess the visual and reasoning skills of autonomous agents in web-based environments.

Task Creation We focus on curating realistic visually grounded tasks, following a similar process as task creation in WebArena. We start by curating intent templates (e.g., "Find me the $\{\{$ attribute $\}\}\{\{$ item $\}\}$. It should be between $\{$ range $\}\}$.”), which can be manually expanded by the annotator with different arguments to form multiple tasks (e.g., "Find me the cheapest red Toyota. It should be between $\$ 3000$ to $\$ 6000$.”). The tasks were curated by 6 graduate students (coauthors of this paper). We encouraged annotators to be creative, and make use of the visual layouts of the websites, input images, and cross-site functionalities to develop creative and realistic tasks. When tasks involved input images, these were sourced from royalty-free, attribution-free sources, and MS-COCO (Lin et al. 2014). Annotators also wrote the reward functions using the primitives described in Sec. 3.3. We collected a total of 314 templates (average of 2.9 tasks per template).

While the majority of tasks can be solved, we also included a small subset (46 tasks, or $5.1 \%$ ) which are unachievable. This subset tests the ability of agents to terminate early in the event where a task cannot be solved, which is essential in many real world scenarios.

Visually Grounded Tasks A key aspect of VisualWebArena is the inherent visual grounding of all tasks. Each task demands visual understanding, requiring agents to process and interpret visual information rather than relying solely on textual or HTML-based cues. This aligns closely with modern human-computer interfaces, where visual information (e.g., icons, colors) is often critical. For instance, a typical task might involve selecting a visually specific item, such as a "green polo shirt" where the color is visually discernible but not explicitly mentioned in text.

Task Complexity We classify each task into three difficulty levels: easy, medium, and hard. This classification is particularly useful for assessing performance across a spectrum of agents, ranging from smaller models to state-of-the-art LLMs and VLMs. We find in our analysis (Sec. 5) that many open-source models (e.g., LLaMA-2-70B, IDEFICS-80B) achieve a success rate of close to 0 on medium or hard tasks, but non-zero performance on easy tasks. This suggests that running opensource models on the easy subset would provide useful signal during development as well as faster iteration cycles (assuming performance between weaker and stronger agents are correlated).

We annotate both the action and visual difficulty of each task (breakdown in Fig. 4). The action difficulty is determined by the estimated number of actions that a human would need to complete the task. Easy tasks are defined as those that require three or fewer actions, Medium tasks involve four to nine actions, and Hard tasks demand ten or more.

Visual difficulty is similarly segmented, with each difficulty level reflecting the complexity of visual processing required: Easy tasks involve basic visual identification such as colors, shapes, and highlevel object detection (e.g., recognizing the existence of a cat). Medium tasks require discerning patterns, semantic understanding, and OCR on text of shorter lengths or large fonts. Hard tasks involve multiple image inputs, OCR on small or lengthy text, or detection of fine details.

### 4.3 HUMAN PERFORMANCE

We measure the success rate of 7 college students (who are familiar with commercial versions of the sites) on VisualWebArena tasks. Several of these students also assisted with task creation, and to avoid data leakage, we ensured that they were not assigned to the same tasks that they initially created. We sample one task per template, collecting a representative set of 230 tasks. We find that humans do well at this task, achieving an overall success rate of $88.7 \%$ (Tab. 3). The mistakes made
in the remaining $11.3 \%$ of tasks are usually minor, such as not reading the task correctly or missing a part of the objective. Another common failure mode was for tasks that required exhaustive search (e.g., "Navigate to the comments section of this exact image."). Users were often unable to find the appropriate post after searching for 5-10 mins and gave up, assuming that the task was unachievable. These types of laborious tasks represent problems that strong computer agents would be well poised to solve, possibly achieving above human performance (and speed).

## 5 BASELINES

We run several baselines on VisualWebArena to benchmark the performance of state-of-the-art LLM and VLM agents. We use a webpage viewport size of $1280 \times 2048$, and truncate text observations to 3840 tokens (or 15360 characters for Gemini). For models with shorter context windows (e.g., LLaMA, IDEFICS, CogVLM), we instead use a viewport size of $1280 \times 720$ and truncate text observations to 640 tokens. For GPT-3.5 and GPT-4 models, we follow Zhou et al. (2024) in using a temperature of 1.0 and a top-p of 0.9 . For Gemini models we use the suggested default temperature of 0.9 and top-p of 1.0 . For the remaining models, we find that they benefit from sampling from lower temperatures, and use a temperature of 0.6 and top-p of 0.95 . Nucleus sampling (Holtzman et al. 2020 ) is used in all experiments. All models are prompt-based and provided with 3 in-context examples (one from each environment), which share no overlap with the benchmark tasks. The prompts we use are provided in the appendix. We summarize the results in Tab. 3 and describe the baselines in detail in the following sections.

### 5.1 TEXT-BASED LLM AGENTS

Several prior works have developed strong autonomous agents through prompting text-only LLMs (Zhou et al., 2024, Kim et al., 2023, Liu et al., 2023d). We run several text-based LLM agents on the accessibility tree representations of the websites, by prompting them with Chain-of-Thought prompting (Wei et al. 2022), similar to Zhou et al. (2024). We leave more advanced prompting strategies for future work. Our baselines include several API-based LLM models, including GPT4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), Gemini-Pro, as well as open sourced LLMs such as LLaMA-2-70B and Mixtral-8x7B.

### 5.2 ImAGE CAPTION AUGMENTED LLM AGENTS

VisualWebArena is a visually grounded benchmark, and we expect that leveraging complementary visual information would improve performance. Hence, we run pretrained image captioning models on every img element on the HTML page, and augment the accessibility tree with this information as the image alt-text before passing this as input to the LLM agents. If a task contains input images, we also caption them and include the captions as part of the prompt. We run experiments on GPT-3.5 with two recent image captioning models, BLIP-2-T5XL (Li et al. 2023) and LLaVA-v1.5-7B (Liu et al. 2023a). Our results with GPT-3.5 as the LLM backbone ("Caption-augmented" section of Tab. 3) suggest that the LLaVA and BLIP-2 captioning models achieve comparable performance. Since BLIP-2 achieves a slightly higher success rate, is a smaller model, and requires less GPU VRAM, we use it as the captioning backbone for the remaining experiments.

### 5.3 MULTimodAl AGENTS

Finally, we benchmark the capabilities of strong closed-source and open-source vision language models. Unlike the previous text-based models, these multimodal models are trained on large datasets of paired text and images, allowing them to learn joint representations between vision and language. We evaluate several models capable of processing multiple interleaved image-and-text inputs: GPT-4V (OpenAI, 2023), Gemini-Pro (Google, 2023), IDEFICS-80B-Instruc ${ }^{5}$ (an opensource reproduction of Flamingo (Alayrac et al. 2022), and CogVLM (Wang et al. 2023b). We experiment with two different input formats for these models:[^5]

| Model Type | LLM Backbone | Visual Backbone | Inputs | Success Rate $(\uparrow)$ |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  | Classifieds | Reddit | Shopping | Overall |
| Text-only | LLaMA-2-70B |  |  | $0.43 \%$ | $1.43 \%$ | $1.29 \%$ | $1.10 \%$ |
|  | Mixtral-8x7B |  |  | $1.71 \%$ | $2.86 \%$ | $1.29 \%$ | $1.76 \%$ |
|  | Gemini-Pro | - | Acc. Tree | $0.85 \%$ | $0.95 \%$ | $3.43 \%$ | $2.20 \%$ |
|  | GPT-3.5 |  |  | $0.43 \%$ | $0.95 \%$ | $3.65 \%$ | $2.20 \%$ |
|  | GPT-4 |  |  | $5.56 \%$ | $4.76 \%$ | $9.23 \%$ | $7.25 \%$ |
| Caption-augmented | LLaMA-2-70B | BLIP-2-T5XL | Acc. Tree + Caps | $0.00 \%$ | $0.95 \%$ | $0.86 \%$ | $0.66 \%$ |
|  | Mixtral-8x7B | BLIP-2-T5XL |  | $1.28 \%$ | $0.48 \%$ | $2.79 \%$ | $1.87 \%$ |
|  | GPT-3.5 | LLaVA-7B |  | $1.28 \%$ | $1.43 \%$ | $4.08 \%$ | $2.75 \%$ |
|  | GPT-3.5 | BLIP-2-T5XL |  | $0.85 \%$ | $1.43 \%$ | $4.72 \%$ | $2.97 \%$ |
|  | Gemini-Pro | BLIP-2-T5XL |  | $1.71 \%$ | $1.43 \%$ | $6.01 \%$ | $3.85 \%$ |
|  | GPT-4 | BLIP-2-T5XL |  | $8.55 \%$ | $8.57 \%$ | $16.74 \%$ | $12.75 \%$ |
| Multimodal | IDEFICS-80B-Instruct |  | Image + Caps + Acc. Tree | $0.43 \%$ | $0.95 \%$ | $0.86 \%$ | $0.77 \%$ |
|  | {CogVLM <br> Gemini-Pro} |  |  | $0.00 \%$ | $0.48 \%$ | $0.43 \%$ | $0.33 \%$ |
|  |  |  | $3.42 \%$ | $4.29 \%$ | $8.15 \%$ | $6.04 \%$ |
|  | GP | $[-4 \mathrm{~V}$ |  | $8.12 \%$ | $12.38 \%$ | $19.74 \%$ | $15.05 \%$ |
| Multimodal (SoM) | IDEFICS-80B-Instruct |  |  |  | $0.85 \%$ | $0.95 \%$ | $1.07 \%$ | $0.99 \%$ |
|  | {CogVLM <br> Gemini-Pro} |  |  | $0.00 \%$ | $0.48 \%$ | $0.43 \%$ | $0.33 \%$ |
|  |  |  | Image + Caps + SoM | $3.42 \%$ | $3.81 \%$ | $7.73 \%$ | $5.71 \%$ |
|  | GPT-4V |  |  | $9.83 \%$ | $17.14 \%$ | $19.31 \%$ | $16.37 \%$ |
| Human Performance | - | - | Webpage | $91.07 \%$ | $87.10 \%$ | $88.39 \%$ | $88.70 \%$ |

Table 3: Success rates of baseline LLM and VLM agents on VisualWebArena.

Image Screenshot + Captions + Accessibility Tree: This approach provides the accessibility tree representation augmented with image captions as accessibility tree alt-text from BLIP-2-T5XL (similar to the caption-augmented agent), as well as the screenshot of the current webpage as inputs.

Image Screenshot + Captions + SoM: Inspired by Set-of-Marks prompting Yang et al. (2023a), we perform an initial preprocessing step by using JavaScript to automatically annotate every interactable element on the webpage with a bounding box and a unique ID. The annotated screenshot containing bounding boxes and IDs, are provided as input to the multimodal model along with a text representation of the SoM (see Fig. 2). Similar to the baselines above, we also provide the captions from BLIP-2-T5XL for all img elements on the page. There have been several project:6 that propose similar representations. Most have been proof-of-concept demos, and to the best of our knowledge, we are the first to systematically benchmark this on a realistic and interactive web environment.

### 5.4 RESULTS

Our main baseline results are summarized in Tab.3. All existing models significantly underperform compared to humans, which indicate significant headroom in VisualWebArena for future work. We discuss some main findings below, with more detailed analysis in Sec. 6 .

Text-based LLMs Perform Poorly State-of-the-art text-only LLMs generally achieve poor results, with the best model (GPT-4) achieving an overall success rate of $7.25 \%$. When we augment the LLMs with caption information, this considerably improves success rate, from $7.25 \%$ to $12.75 \%$ for GPT-4. Other models also see a similar boost in success rate when provided caption information.

Multimodality Helps We achieve a substantial improvement in success rate when we use multimodal agents: GPT-4V (gpt-4-1106-vision-preview) achieves an overall success rate of $15.05 \%$, substantially improving over the text-only GPT-4 model. Gemini-Pro also experiences a significant uplift in success rate, improving from $3.85 \%$ (caption-augmented) to $6.04 \%$ (multimodal). Text-only or caption-augmented models may be limited in their ability to process complex images (e.g., those that require OCR or recognition of non-salient objects), thus falling behind multimodal models.

SoM Improves Navigability We observe that the SoM representation (Sec. 5.3) further improves the performance of GPT-4V over using the accessibility tree observation and action space, boosting overall success rate $(15.05 \% \rightarrow 16.37 \%)$. We observe particularly substantial improvements compared to the accessibility tree GPT-4V model on Classifieds and Reddit, from $12.38 \% \rightarrow 17.14 \%$[^6]

![](https://cdn.mathpix.com/cropped/2024_06_04_e22954b7a730839dd4deg-10.jpg?height=450&width=1333&top_left_y=282&top_left_x=404)

Figure 5: Success rates (a, b, c) and trajectory lengths (d) across different difficulty levels.

and $8.12 \% \rightarrow 9.83 \%$ respectively. We attribute this to the Classifieds and Reddit websites containing more dense visual content. For example, these websites often contain many smaller sized images that are arranged very closely (Fig. 22). In many of these cases, the accessibility tree does not always provide sufficient information to disentangle elements that are spatially close. We hypothesize that the SoM representation is superior in these cases, as a strong VLM model is able to more accurately disentangle and click on the desired content. For the other VLMs, SoM does not significantly improve success rates, which we attribute to the finding from Yang et al. (2023a) that only GPT-4V demonstrates this emergent grounding ability (perhaps due to scale or training data). This motivates future work in imbueing VLM agents with similar abilities.

## 6 ANALYSIS

### 6.1 PERFORMANCE BY TASK DIFFICULTY

We conduct an analysis of the GPT-4 models across different action and visual difficulty levels (Fig.55). We observe that success rate generally decreases as action/vision difficulty increases, which makes intuitive sense based on the difficulty taxonomy described in Sec. 4.2 The findings also show that multimodal models perform better especially on hard visual tasks. On this subset, GPT$4 \mathrm{~V}+\mathrm{SoM}$ achieves an average success rate of $12.4 \%$, which is significantly higher than that of the caption-augmented ( $8.0 \%$ ) and the text-only agents (4.8\%).

In addition to success rates, we also observed that the GPT-4V trajectory lengths increased with action difficulty, with harder tasks requiring more steps to complete.

### 6.2 PERFORMANCE BY TASK TYPE

We also analyze the success rate of the best VLM agent baseline (GPT-4V with SoM prompting) across several additional subsets of tasks. The results are summarized in Tab. 4 and discussed in the following paragraphs, with further analysis for other models provided in the appendix.

OCR Tasks $17.1 \%$ of VisualWebArena require optical character recognition (OCR), such as reading text from product images, or extracting text from an input image. We find that GPT-4V + SoM generally performs worse on tasks that require OCR (success rate of $13.4 \%$ ) compared to tasks which do not (success rate of $16.9 \%$ ), suggesting that OCR capabilities may be a bottleneck for completing certain tasks.

Exact Image Match $8.7 \%$ of tasks require exact image matching, which evaluates visual capabilities beyond semantic image recognition, and requires agents to identify precise visual matches. GPT-4V + SoM achieves a slightly improved success rate ( $18.9 \%$ ) compared to tasks that do not require exact match $(16.2 \%)$, suggesting that exact image matching is not a hurdle for the model.

| Task Subset | \% of Total | SR $(\uparrow)$ |
| :--- | :---: | :---: |
| OCR required | $17.1 \%$ | $13.4 \%$ |
| No OCR required | $82.9 \%$ | $\mathbf{1 6 . 9 \%}$ |
| Exact image match | $8.7 \%$ | $\mathbf{1 8 . 9 \%}$ |
| No exact image match | $91.3 \%$ | $16.2 \%$ |
| Image inputs | $25.2 \%$ | $\mathbf{1 9 . 0 \%}$ |
| No image inputs | $74.8 \%$ | $14.9 \%$ |

Table 4: Success rate (SR) of GPT-4V (SoM) across different types of tasks.
![](https://cdn.mathpix.com/cropped/2024_06_04_e22954b7a730839dd4deg-11.jpg?height=626&width=1396&top_left_y=272&top_left_x=365)

Figure 6: Successful execution trajectory of the GPT-4V + SoM agent on the task for blocking a user that posted a certain picture. The text in red represents the commands output by the agent.

Image Input Tasks $25.2 \%$ of VisualWebArena include one or more input images as part of the objective. These tasks generally appear more tractable for the GPT-4V + SoM agent, and it achieves a higher success rate ( $19.0 \%$ ) compared to tasks without image inputs (14.9\%).

### 6.3 QUALITATIVE RESULTS

Successful Execution We observed that the GPT-4V + SoM agent is able to complete several complex tasks efficiently. One example that we found particularly compelling was task 139 for Reddit, which requires exact image matching to find a post and block a user (Fig. 6. The model initially attempts to search for the correct forum, and when this fails it navigates to the list of forums. After navigating correctly to $/ \mathrm{f} / \mathrm{memes}$, it identifies the offending image out of the many images on the page (Step 3 in Fig. 6) and blocks the author efficiently without any unnecessary actions.

Failure Modes We observed that in several examples, the agents would correctly perform a task but undo it, leading to failure. The GPT-4 captioning-only model on shopping task 54 ("Add the one [poster] with waves to my wish list.") made an assumption that the product image with a caption about a lighthouse was the correct one, and added it to the wishlist. However, after going to the wish list page the agent removes the poster because "there is no explicit mention of waves in the current items listed on the Wish List page." This issue is not unique to the text input agents; even the GPT-4 SoM agent faced a similar problem in shopping task 397 ("Buy the item on the page with a banana theme."). The agent initially added the correct item to the shopping cart and proceeded to check out, but stopped in the middle stating in the reasoning trace output that it does not think the item fit the criteria (despite having added it to the cart just a few steps ago).

Failures on Easy Tasks We observed surprisingly poor performance on many tasks with easy action and easy visual difficulty levels, such as in shopping task 46, which tasks the agent to add the red product in the second row to the cart (starting on the page shown in Fig. 77). The multimodal and SoM GPT-4V agents clicked on a blue tablecloth in the first row and gave up when they couldn't find an option to order it in red. Despite appearing to be a simple task (the correct product is the red cloth in the second row), none of the agents we benchmarked were able to successfully complete it.

![](https://cdn.mathpix.com/cropped/2024_06_04_e22954b7a730839dd4deg-11.jpg?height=669&width=426&top_left_y=1579&top_left_x=1332)

Figure 7: The starting page for the task "Add the red one in the second row of this page to my shopping cart."

Spatial Reasoning Both the multimodal and SoM agents outperform the caption and text-only agents on tasks that require spatial reasoning. For shopping task 81 ("What is the price range for products in the first row."), the caption-augmented GPT-4 model makes an assumption that there are three items to a row and provides the price range incorrectly. However, there are actually four products in each row, which the multimodal and SoM GPT-4V agents both correctly identify to succeed at this task.

SoM Benefits Longer Action Sequences The SoM representation generally performs better on tasks that require more navigation steps. For example, classifieds task 31 asks the agent to "Find the latest listing of a white Google Pixel phone and post a comment offering $\$ 10$ less than their asking price." While the multimodal model was unable to search for the correct terms, the SoM model was able to leverage the simplified action space to traverse more efficiently throughout the environment. It succeeded at this task by filtering for cell phones after the initial search for more relevant results, and managed to fill out the necessary comment form fields. We believe that the SoM representation is superior to the multimodal representation (which only has access to the page screenshot and accessibility tree). With SoM, the agent does not have to implicitly perform visual co-referencing to match elements from the accessibility tree to the visual buttons and inputs that it wants to interact with.

## 7 CONCLUSION

In this work, we introduced VisualWebArena, a benchmark of realistic tasks designed to rigorously evaluate and advance the capabilities of autonomous multimodal web agents. VisualWebArena represents a significant step towards addressing the gap in the evaluation of multimodal agents on visually grounded tasks. We also introduce a visual agent inspired by Set-of-Marks prompting, and demonstrate the potential of this approach for simplifying action spaces and improving performance on visually complex websites. Our extensive evaluation of state-of-the-art LLM and VLM agents demonstrate that while VLMs show promise, there remains a considerable performance gap compared to humans, who achieve very high success rates on VisualWebArena. Our quantitative and qualitative analysis also highlights several common failure modes of existing LLM and VLM agents. We expect future work on improving the reasoning, visual understanding, and planning abilities of agents to be particularly exciting and promising areas.

## ACKNOWLEDGEMENTS

We thank photographers on Pexels.com for providing free to use images. We thank Wendy Kua for assisting with measuring human performance and for help with the figures. We thank Yi Tay, Zhiruo Wang, Saujas Vaduguru, Paul Liang, Stephen McAleer, and many others for feedback and helpful discussions on previous versions of the paper.

## REFERENCES

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. NeurIPS, 2022.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015.

Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R Gormley. Unlimiformer: Long-range transformers with unlimited length input. NeurIPS, 2023.

Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020.

Stephanie CY Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K Lampinen, and Felix Hill. Transformers generalize differently from information stored in context vs in weights. NeurIPS MemARI Workshop, 2022.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. JMLR, 2023.

Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.

Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. NeurIPS, 2023.

Stan Franklin and Art Graesser. Is it an agent, or just a program?: A taxonomy for autonomous agents. In International workshop on agent theories, architectures, and languages, pp. 21-35. Springer, 1996.

Gemini Team Google. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. ICLR, 2020.

Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023.

Nicholas R Jennings, Katia Sycara, and Michael Wooldridge. A roadmap of agent research and development. Autonomous agents and multi-agent systems, 1:7-38, 1998.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. NeurIPS, 2023.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ICML, 2023.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. ECCV, 2014.

Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. ICLR, 2018.

Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023a.

Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023b.

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents, 2023c.

Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, et al. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. arXiv preprint arXiv:2308.05960, 2023d.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. ICML, 2023.

Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. arXiv preprint arXiv:2311.12983, 2023.

OpenAI. Gpt-4 technical report, 2023.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 2022.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis \& insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.

Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In ICML, 2017.

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. ICLR, 2021.

Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. NeurIPS, 2021.

Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive training for context scaling. NeurIPS, 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.

Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023a.

Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023b.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. $I C L R, 2023 \mathrm{c}$.

Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600$612,2004$.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022.

Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.

An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023.

Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023a.

Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023b.

Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1), 2023c.

Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. NeurIPS, 2022.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. NeurIPS, 2023.

Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.

Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023.

Zhuosheng Zhan and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436, 2023.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024.

Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. ICLR, 2024.
