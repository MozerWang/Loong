# Spiker+: a framework for the generation of efficient Spiking Neural Networks FPGA accelerators for inference at the edge 

Alessio Carpegna, Student memeber, IEEE, Alessandro Savino, Senior member, IEEE, Stefano Di Carlo,<br>Senior member, IEEE


#### Abstract

Including Artificial Neural Networks (ANNs) in embedded systems at the edge allows applications to exploit Artificial Intelligence (AI) capabilities directly within devices operating at the network periphery, facilitating real-time decision-making. Especially critical in domains such as autonomous vehicles, industrial automation, and healthcare, the use of ANNs can enable these systems to process substantial data volumes locally, thereby reducing latency and power consumption. Moreover, it enhances privacy and security by containing sensitive data within the confines of the edge device. The adoption of Spiking Neural Networks (SNNs) in these environments offers a promising computing paradigm, mimicking the behavior of biological neurons and efficiently handling dynamic, time-sensitive data. However, deploying efficient SNNs in resource-constrained edge environments requires hardware accelerators, such as solutions based on Field Programmable Gate Arrays (FPGAs), that provide high parallelism and reconfigurability. This paper introduces Spiker+, a comprehensive framework for generating efficient, low-power, and low-area customized SNNs accelerators on FPGAs for inference at the edge. Spiker+ presents a configurable multi-layer hardware SNNs, a library of highly efficient neuron architectures, and a design framework, enabling the development of complex neural network accelerators with few lines of Python code. Spiker+ is tested on two benchmark datasets, the MNIST and the Spiking Heidelberg Dataset (SHD). On the MNIST, it demonstrates competitive performance compared to state-of-the-art SNN accelerators. It outperforms them in terms of resource allocation, with a requirement of 7,612 logic cells and 18 Block RAMs (BRAMs), which makes it fit in very small FPGAs, and power consumption, draining only $180 \mathrm{~mW}$ for a complete inference on an input image. The latency is comparable to the ones observed in the state-of-the-art, with $780 \mu \mathrm{s} / \mathrm{img}$. To the authors' knowledge, Spiker+ is the first SNNs accelerator tested on the SHD. In this case, the accelerator requires 18,268 logic cells and 51 BRAMs, with an overall power consumption of $430 \mathrm{~mW}$ and a latency of $54 \mu$ s for a complete inference on input data. This underscores the significance of Spiker+ in the hardware-accelerated SNN landscape, making it an excellent solution to deploy configurable and tunable SNN architectures in resource and power-constrained edge applications.


Index Terms-Spiking Neural Networks, LIF, FPGA, Neuromorphic accelerator, Edge computing, Artificial Intelligence, Frugal AI.

## 1 INTRODUCTION

Integrating Artificial Neural Networks (ANNs) at the edge is a pivotal computing advancement, enabling direct application of Artificial Intelligence (AI) capabilities in devices and systems at the periphery of networks, resulting crucial in domains like autonomous vehicles, industrial automation, and healthcare [1|. Using ANNs at the edge allows systems to process substantial data locally, reducing latency, power consumption, and reliance on external data centers or cloud services. Additionally, it enhances privacy and security by keeping sensitive data within the edge device. This approach boosts the effectiveness and agility of embedded applications, paving the way for innovative breakthroughs across sectors and ushering in a new era of intelligent, decentralized computing.

Neural networks, tailored for diverse computational tasks, include feed-forward networks for pattern recogni-

Alessio Carpegna, Alessandro Savino, and Stefano Di Carlo are with the Department of Control and Computer Engineering of Politecnico di Torino, 10129, Torino (Italy). E-mails: \{firstname,lastname\}@polito.it

This project has received funding from the European Union's Horizon Europe research and innovation programme under grant agreement No. 101070238. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them. Manuscript received $x x x x x, x x x x$; revised $x x x x x, x x x x$. tion, recurrent networks for sequential data, convolutional networks for image analysis, and transformers for Natural Language Processing (NLP). Amidst this variety, Spiking Neural Networks (SNNs) emerge as a promising paradigm for edge computing in resource-constrained environments, particularly intriguing for researchers in neurosciences and Machine Learning (ML) [2]. Unlike traditional models, SNNs closely mimic biological neurons that efficiently process dynamic, time-sensitive data, making them valuable in edge computing scenarios, where real-time decision-making is crucial [3].

In this context, hardware accelerators are vital for SNNs applications, significantly improving computational efficiency and speed for real-time processing in resourceconstrained environments [4]. However, the flexibility of specialized hardware design poses a challenge, with applications often requiring diverse network architectures, encoding methods, and neuron models. While awaiting the maturity of SNN accelerators based on emerging technologies, existing literature proposes various digital hardware solutions (refer to section 3). Unfortunately, these solutions often constrain network topology to circuit architecture, limiting exploration of the broader design space, whether considering Application-Specific Integrated Circuits (ASICs) or Field Programmable Gate Arrays (FPGAs). We propose
an alternative strategy, optimizing the network architecture for specific needs and leveraging FPGAs for deploying custom hardware blocks. This approach enables efficient and low-power SNN inference engines at the edge, supporting real-time data processing. FPGAs provide high parallelism and reconfigurability, making them ideal for accelerating complex neural network computations with minimal latency.

To support this trend, this paper presents Spiker+, a complete framework for generating efficient low-power and low-area customized SNN accelerators on FPGAs for inference at the edge. Spiker+ introduces several pivotal contributions. At its core, it provides a fully configurable multilayer hardware architecture implementing both fully connected and recurrent SNNs. This architecture is a significant step from the preliminary Spiker model, initially presented in [5]. It introduces a library of highly efficient architectures delving into a range of approximation techniques to implement remarkably low-area and low-power neurons, thus optimizing resource utilization while maintaining high performance.

Notably, Spiker+ brings a complete design framework to the forefront, a comprehensive toolkit for developing complex SNNs accelerators. This framework empowers researchers and developers to describe target network architectures with great flexibility, enabling the specification of layers, neuron types, and input encoding techniques using a few lines of Python code. Integrating sophisticated offline server-based training algorithms like Back-Propagation Through Time (BPTT) [6] and Spike-Timing-Dependent Plasticity (STDP) ensures the network has cutting-edge learning capabilities. Additionally, Spiker+ emphasizes the significance of optimizing networks through quantization techniques, reducing complexity while judiciously balancing approximation and accuracy. Finally, the framework seamlessly generates a VHDL model of the accelerator, primed for deployment on Xilin ${ }^{\text {TM }}$ FPGA boards. These contributions make Spiker+ a robust solution in the hardwareaccelerated SNN landscape. Spiker+ has been tested on the well-known MNIST dataset [7] and compared to state-ofthe-art SNN accelerators for FPGAs, demonstrating superior performance. Moreover, an accelerator for the Spiking Heidelberg Dataset (SHD) [8| has been generated and evaluated to illustrate the framework's flexibility when handling different problems.

The rest of the paper is organized as follows: section 2 presents some background on SNNs, focusing on the models used in Spiker+; section 3 reviews relevant literature on accelerating SNNs. Section 4 describes the Spiker+ architecture, with all the design choices that it involves, and section 5 introduces the framework able to configure, design, and generate custom hardware accelerators. Finally, section 6 presents the results obtained by applying the designed accelerators on the MNIST and the SHD and section 7 concludes the paper.

## 2 BACKGROUND

This section overviews foundational knowledge on SNNs, required to understand the remaining parts of the paper.

### 2.1 Spiking Neural Networks

SNNs distinguish themselves through their unique information encoding based on spikes, inspired by neuroscience. Indeed, this neuron model mimics biological neurons and synaptic communication mechanisms based on action potentials. The information is thus represented as a flow of spikes with various neural coding techniques, shifting the computational complexity from the spatial dimension to the temporal dimension. Spike encoding methods in SNNs range from real current or voltage pulses in specialized analog circuits to numerical representations in software or dedicated digital implementations. This paper focuses primarily on the latter. Spikes convey information through temporal organization, and in the digital domain, they can be approximated as binary values: 'one' for received spikes and 'zero' otherwise. Essentially, neurons are translated into compact computational units that exchange data, i.e., their activations through binary bit streams.

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-02.jpg?height=261&width=290&top_left_y=913&top_left_x=1075)

(a) FF-FC SNN

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-02.jpg?height=263&width=295&top_left_y=909&top_left_x=1376)

(b) FC-R SNN

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-02.jpg?height=260&width=290&top_left_y=911&top_left_x=1666)

(c) RC-R SNN
Fig. 1: Typical architectures of an SNN

In SNNs, neuron connections can take various forms, including Feed-Forward Fully-Connected (FF-FC) (Figure 1a, Fully-Connected Recurrent (FC-R) (Figure 1b), and Randomly-Connected Recurrent (RC-R) organizations. Spiker+ prioritizes FF-FC and FC-R structures, ideal for solving diverse classification tasks [9]. In particular, in FC-R SNNs, neurons connect not only to all nodes in the subsequent layer but also within their layer. This strengthens the joint activity of groups of neurons while inhibiting unrelated ensembles, fostering a competitive environment, which results in a better specialization of the single neurons.

### 2.2 Neuron models

Over the past decades, numerous neuron computational models have emerged, originating from nerve electrical conductance measurements and mathematical modeling. The Hodgkin-Huxley model [10], inferred from squid giant axon measurements in 1952, is realistic but complex and has evolved into the simplified Izhikevich model [11]. However, for hardware implementations, balancing accuracy and complexity commonly leads to using the Leaky Integrate and Fire (LIF) model and its simplified version, Integrate and Fire (IF) [12]. These models aim to mathematically describe the behavior of a biological neuron focusing on its membrane, which is crucial to defining the internal electrical processes. The membrane selectively permits specific ion passage, accumulating charge and creating a membrane potential $\left(V_{m}\right)$ defining the neuron's state and behavior.

The LIF model in Spiker+ encompasses a family of neuron models with varying levels of simplification. Equation 1 introduces the discrete-time formulation of a Synaptic

Conductance-based II-order LIF model [12]. Operating in discrete time enables the iterative solution of the differential equations governing the temporal evolution of the membrane potential $\left(V_{m}\right)$. In this model, input spikes $\left(s_{i n}\right)$ are integrated by synapses with conductance weights $(W)$, influencing the membrane potential based on the input's significance $\left(W \cdot s_{i n}\right)$. The integrated spikes form the synaptic current $\left(I_{\text {syn }}\right)$, which undergoes capacitive discharge ( $\alpha \cdot I_{\text {synn }}$, with $\alpha<0$ ). The membrane integrates this current, resulting in increased or decreased potential based on the current sign. The current sign is influenced by the excitatory or inhibitory nature of the input spikes, impacting the neuron's firing probability. This effect can be positive or negative. The capacitive component $(\beta)$ discharges the membrane toward a resting state in the absence of stimuli $\left(\beta \cdot V_{m}\right.$, with $\beta<0$ ). Lastly, the reset parameter $(r)$ models the reset process, as explained later in this section.

$$
\left\{\begin{array}{l}
I_{\text {syn }}[n]=\alpha \cdot I_{\text {syn }}[n-1]+W \cdot s_{i n}[n]  \tag{1}\\
V_{m}[n]=\left(\beta \cdot V_{m}[n-1]+I_{s y n}[n-1]\right) \cdot r
\end{array}\right.
$$

An action potential, represented by a separate variable $s_{\text {out }}$, occurs when the membrane voltage $\left(V_{m}\right)$ surpasses a threshold value $\left(V_{t h}\right.$ ). This dual-variable approach (membrane and action potential), simplifies the description by treating the spike as a binary variable. Equation 2 illustrates the relationship between the output spike and membrane potential $V_{m}$.

$$
s_{\text {out }}[n]= \begin{cases}1, & \text { if } V_{m}>V_{t h}  \tag{2}\\ 0, & \text { if } V_{m} \leq V_{t h}\end{cases}
$$

Finally, to model the complete discharge of the membrane when the neuron fires, a reset term is used ( $r$ in Equation 1. There are different alternatives to applying the reset. Spiker+ supports two modes: a hard-reset, shown in Equation 3, in which the membrane is instantly brought to zero when a spike is generated, and a subtractive-reset, detailed in Equation 4 In the latter, the threshold value is subtracted by the membrane potential.

$$
\begin{gather*}
r=1-s_{\text {out }}[n-1]  \tag{3}\\
r=1-\frac{V_{\text {th }}}{\beta \cdot V_{m}[n-1]+I_{\text {syn }}[n-1]} \tag{4}
\end{gather*}
$$

The LIF model, being a family of models, allows for various simplifications to derive different LIF descriptions. For instance, setting $\alpha=0$ transforms the II-order model in Equation 1 into a I-order LIF, where input spikes directly influence the membrane potential. Additionally, with $\beta=0$, a basic IF model is obtained, maintaining a constant membrane value without input spikes.

These different models serve diverse tasks. The II-order LIF excels in handling input sequences with high temporal information content, capturing longer correlations in precise spike sequences. On the other hand, I-Order LIF and IF models are simpler and preferred for processing static data converted into spike sequences, such as images. Working solely with LIF offers six distinct models: II- and I-order LIF, and IF, each with a hard or subtractive reset. These models, supported by Spiker+, combined with the architectures in
Figure 1. address various classification and regression problems.

### 2.3 Training

Training an SNN involves tuning it for specific problemsolving, such as classifying input data. The training process adjusts synaptic weights $(W)$ and internal neuron parameters like threshold $\left(V_{t h}\right.$ ) and time constants ( $\alpha$ and $\beta$ ) to enhance model accuracy.

Training SNNs presents notable challenges, mainly due to the non-differentiability of the SNN activation function. This paper adopts the Surrogate Gradient approach [6], replacing the non-differentiable gradient with a surrogate function, like the spike function itself or a Gaussian function. This enables the application of standard supervised learning techniques, such as BPTT, overcoming nondifferentiability and facilitating effective SNN training.

Alternative solutions, like e-prop |13| for Recurrent Spiking Neural Network (RSNN) and STDP for unsupervised weight tuning based on spike timing, exist. However, as Spiker+ focuses on inference using pre-trained parameters, these methods fall outside the paper's scope.

## 3 NEUROMORPHIC ACCELERATORS: RELATED WORK

In the past, SNNs were primarily implemented using software frameworks like Brian/Brian2 |14|. However, their unique features, including high parallelism, temporal evolution, and event-driven computation, are ill-suited for dominant Von-Neumann Central Processing Unit (CPU) architectures with one or a few powerful computational units. Unfortunately, Single Instruction Multiple Data (SIMD) architectures, such as General Purpose Graphic Processing Units (GPGPUs) and Tensor Processing Units (TPUs), optimized for standard ANNs workloads, are also not well-equipped for efficiently processing event-driven information across multiple timesteps [15]. Furthermore, the binary spike encoding of SNNs does not align with the typical 64, 32, or 16-bit numeric representations of these SIMD architectures. Therefore, dedicated neuromorphic hardware is crucial for ensuring the widespread adoption of SNNs. Figure 2 provides an overview of state-of-the-art accelerators for SNNs, offering a general perspective rather than an exhaustive list of available solutions. For more detailed information, refer to |16], [17].

Contributions in this domain span various design dimensions, including application-driven solutions focused on specific applications and those aimed at modeling biological neuron dynamics. However, this paper primarily emphasizes the hardware technology dimension. The research effort is divided between analog solutions based on emerging technologies and efficient digital implementations |17|. In the digital realm, presented solutions differ on the target platform (ASIC or FPGA) and accelerator size, tailored for either large-scale systems or smal applications.

Examining large network models, the SpiNNaker system developed at Manchester University is implemented using standard 32-bit ARM M4F CPUs simulating neuron activity. It optimizes spike routing between units [18]. Promisingly,

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-04.jpg?height=651&width=876&top_left_y=146&top_left_x=169)

Fig. 2: Landscape of neuromorphic hardware

major computer companies invest in developing their neuromorphic accelerators, such as Intel Loihi [19] and IBM True-North [20]. These accelerators provide additional optimizations using specialized hardware to execute the neuron model. Another solution, Tianjic [21| from the University of Beijing, aims to implement a hybrid SNN/ANN model, benefiting from both domains.

An alternative design approach is recognizing that real biological neural networks function as analog physical systems. Emulating their efficiency involves using hardware components that approximate biological elements. Chips like Dynap-se2 [22] by SynSense, a spinoff of the Institute of Neuro-Informatics (INI) in Switzerland, exemplify this concept. Neurogrid [23] from Stanford University and the European project BrainscaleS [24] focus on faithfully simulating portions of a biological brain. Due to their complexity, these systems have programming tools for automatic SNN configuration. Tools like Rockpool [25] by SynSense and Nengo [26] facilitate automatic configuration and acceleration of SNNs on neuromorphic hardware based on Python model descriptions.

While these accelerators suit large-scale neuromorphic systems with good programmability, applications like the Internet of Things (IoT), wearable devices, and biomedical sensors demand smaller sizes and lower power consumption. In such cases, designing specialized hardware accelerators to execute specific tasks, such as classification or regression, efficiently becomes a viable solution.

To pursue this direction, the first option is designing a compact, programmable ASIC supporting various architectures and models. The focus is on digital solutions for networks ranging from hundreds to a few thousand neurons. A preliminary comparison with non-spiking hardware accelerators, dedicated to efficient convolution execution in Convolutional Neural Networks (CNNs), is discussed in [27], highlighting the energy efficiency of SNNs. These accelerators often use the Address Event Representation (AER) protocol for compatibility with neuromorphic sensors. An example is found in [28], developed at Zhejiang University. Charlotte Frenkel's work at the University of Delft and Eidgen√∂ssische Technische Hochschule (ETH) Zurich intro- duces three chips - MorphIC [29], ODIN [30], and ReckOn |31] - exploring online learning on small, low-power, and efficient accelerators.

The final digital accelerator option, central to this paper, involves developing a specialized FPGA-based accelerator, offering advantages like cost reduction and increased flexibility by bypassing tape-out design needs. Many IoT edge systems now integrate FPGAs for task acceleration, potentially expanding the application of neuromorphic processors. An early example is the event-driven Minitaur [32], and subsequent alternatives feature diverse update policies, neuron models, architectural choices, and network sizes [5], [33|-[42|. Another key advantage of using an FPGA is its intrinsic reconfigurability. It allows hardware reprogramming to modify the network architecture or neuron model, tailoring the accelerator to specific application requirements. However, existing accelerators still need to exploit this characteristic efficiently. The first example of FPGA-oriented Design Space Exploration (DSE) is found in [43|, focusing on the best encoding technique for input data translation into spike sequences. Conversely, $E^{3} N E|44|$ provides a block library to configure networks for specific applications, optimizing data movement and hardware utilization, with a dedicated section for input encoding.

In this scenario, there is still a lack of a comprehensive framework that conceals the internal details of the architecture, enabling the user to operate at higher abstraction levels. For instance, a Python description of the model could be automatically translated into custom blocks. Spiker+ moves in this direction, trying to address this challenge.

## 4 SPIKER+ ARCHITECTURE

This section presents the Spiker+ hardware architecture, which serves as the central component of the Spiker+ SNN hardware acceleration framework. The architecture is introduced top-down, beginning with the high-level network model and then delving into the neurons and input/output interfaces.

### 4.1 Network architecture

The SNN architecture presented here builds upon the initial Spiker architecture introduced in [5]. While our earlier work provided a proof of concept tailored for inference on the MNIST dataset |7|, derived from the SNN model by Diehl and Cook [45], Spiker+ focuses on a generic and fully configurable architecture adaptable to various problems.

Figure 3 depicts the high-level architecture of a toy example of a three-layer FF-FC architecture used to introduce the three hierarchical levels of Control Units (CUs) that characterize Spiker+: (i) the network $C U$, responsible for synchronizing the various components within the network; (ii) the layer CUs, orchestrating the update of the neurons of a layer based on a set of input spikes; (iii) the neuron $\mathrm{CU}$ : the accelerator core controlling the update of the membrane potential in each neuron. This organization represents a highly optimized architecture in terms of performance and space utilization.

Block communication is based on a simple two-signal (start/ready) handshake protocol to ensure high modularity while minimizing design complexity. When a block

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-05.jpg?height=534&width=1458&top_left_y=137&top_left_x=325)

Fig. 3: Spiker+ example of FF-FC architecture. The example includes three layers with different numbers of neurons and depicts all the architecture's main control blocks.

(i.e., a neuron or a layer) is ready to work, it notifies the corresponding CU through the ready signal and awaits a new start signal to begin the computation. Consequently, if two blocks need synchronization, combining the two ready signals with an AND gate ensures that the CU waits for both before initiating a new computation. This protocol is also employed at the interface with the external world. Such an approach maintains modularity in the design and paves the way for various architectural solutions.

### 4.2 Network CU: global synchronization

The primary function of the Network CU is to coordinate the temporal evolution of the neurons of the different layers during an inference. As previously mentioned, in an SNN, information is encoded as trains of spikes (i.e., sequences of bits) received on every input. Each train is characterized by a given duration (i.e., the number of transmitted bits) denoted as N_cycles. Thus, the network performs inference by evolving over N_cycles temporal steps to analyze the temporal patterns.

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-05.jpg?height=469&width=366&top_left_y=1695&top_left_x=153)

(a) Network CU

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-05.jpg?height=407&width=374&top_left_y=1756&top_left_x=672)

(b) Layer CU
Fig. 4: Internal architecture of the Network and Layer CUs

The Network CU reported in Figure 4a receives a start signal when a new inference must be initiated. It then manages an iterative process. At every iteration, it awaits the ready signal from all Layer CUs composing the network to ensure all layers are prepared to work (i.e., L1 ... Ln ready). The Network CU also synchronizes with the input/output interface to ensure that data are available and results are correctly delivered (i.e., IN/OUT ready). Subsequently, it asserts a set of start signals, enabling all connected blocks to perform a computation step. At the same time, the internal counter (CNT) increases to track the computation length. Upon reaching the desired cycle count (N_cycles), the loop concludes, and the ready signal is asserted to indicate the end of the inference process.

### 4.3 Layer CU: deliver spikes to neurons

In a fully connected multi-layer SNN as the one proposed in Figure 3, a parallel update of each layer involves three dimensions: (i) the number of neurons, (ii) the number of inputs processed by each neuron, and (iii) the temporal dimension of each input, representing the number of cycles.

The last dimension is inherently sequential and cannot be parallelized, as it depends on the temporal evolution of the inputs. This dimension is managed by the Network CU discussed in subsection 4.2., which manages the update during each cycle.

If the network is sufficiently small, it could be feasible to update all neurons with their inputs in parallel within a single cycle. However, the network and input data sizes are typically too high to achieve such a degree of parallelism. Consequently, Spiker+ exploits only one dimension to obtain parallelism, concurrently updating all neurons within a layer while sequentially providing inputs to each neuron. The Network CU, depicted in Figure 4b, oversees this process.

Once again, this circuit operates based on a start/ready protocol. The control unit receives a start signal from the Network $\mathrm{CU}$ and enters a loop: it awaits the readiness of the neurons composing the layer to process a new spike (neurons ready signal), initiates the computation (neurons start signal), and increments the internal counter (CNT). The counter directly selects the spike to be processed from the sampled inputs. When all input spikes have been provided to the neurons (N_INPUTS), the loop concludes, and the control unit asserts the ready signal.

An additional component visible in the upper section of Figure 4b is an OR gate utilized to verify if there is at least one active spike among the inputs. Currently, no encoding or compression has been applied to the spikes. However, considering that SNNs typically exhibits sparse
activity, avoiding unnecessary computations when there are no spikes can significantly save time and power. The role of the OR gate is to compress along the time dimension: if there is no active spike in a particular cycle, looping over all inputs to provide no spike to the neuron becomes unnecessary.

### 4.4 Neuron models

All the different LIF neuron models presented in subsection 2.2 are translated into dedicated hardware implementations in Spiker+, trying to minimize the required components. Building upon the groundwork laid in [5], the proposed neuron functions as a Multiply and Accumulate (MAC) unit, augmented with additional components and controls to manage its various states. Figure 5 shows the obtained architectures, in order of increasing complexity from left to right (IF, I-order LIF and II-order LIF), with the subtractive reset on top and the fixed one on the bottom.

From a hardware perspective, the most critical factors of the characteristic equation of the neurons are the multiplications. Four of them can be found in Equation 1

1) The synapses weighting: $W \cdot s_{i n}[n]$
2) The reset: $V[n-1] \cdot r$
3) The exponential decays: $\alpha \cdot I_{\text {syn }}[n-1]$ and $\beta \cdot V_{m}[n-1]$

For the first one, exploiting the binary nature of the spikes reduces the operation to a simple selection: zero if there is no spike, $W$ if a spike is present. This can be implemented as a bitwise AND between the weight and the spike.

The reset operation can be applied in two ways, as shown in equations 3 and 4 The first case exploits the binary nature of the spike: either the membrane is kept at its value or reset to zero, so this is again a selection process more than a multiplication. The hardware implementation is a bit more general since it allows to explicitly choose the value of $V_{\text {reset }}$, which in this case can also be different from 0 , as shown in figures $5 \mathrm{~d}$, $5 \mathrm{e}$ and $5 \mathrm{f}$ The second reset method can be obtained by simply subtracting the threshold voltage $V_{t h}$ from the computed value of $V_{m}$, as shown in figures $5 \mathrm{a}$, $5 \mathrm{~b}$ and $5 \mathrm{C}$

At this point, the last critical multiplication is the one required to compute the step-by-step exponential decay of the membrane. The problem exists only for the two LIF models (in the IF model, the membrane is kept fixed without stimuli), with one multiplication needed in the I-order version and two multiplications in the II-order one. The criticality is solved once again, exploiting the characteristics of binary operations. If one of the operators is representable as a power of two, the multiplication can be reduced to a simple bit-shift. Since there is no control on the values of $I_{s y n}$ and $V_{m}$, which evolve dynamically during the update of the network, the only parameters on which it is possible to act are the constant hyper-parameters $\alpha$ and $\beta$. The values can vary between 0 and 1 , with larger values corresponding to slower exponential decay. Generally, a value near to 1 is observed. In this case $\alpha$ and $\beta$ can be approximated as $\alpha=1-\alpha^{\prime}$ and $\beta=1-\beta^{\prime}$, where $\alpha^{\prime}$ and $\beta^{\prime}$ are negative powers of 2. As shown in [5], the overall accuracy has no notable impact if such an approximation is applied during the training phase.

### 4.5 Synapses

The primary advantage of implementing SNNs on dedicated hardware, alongside the execution parallelism, lies in the opportunity to integrate memory and computation. On an FPGAs, this integration can be achieved through two distinct methods.

For relatively small memory requirements, such as the internal parameters of the neurons, the internal Look Up Tables (LUTs) can serve as a viable memory solution. This approach offers superior speed, leveraging Flip Flops (FFs) and registers. However, the available space is limited, primarily due to the necessity of accommodating the logical functions of the network within the LUTs.

In scenarios where a larger memory capacity is necessary, particularly for synaptic weights, many FPGAs grant access to discrete units of Static Random Access Memory (SRAM) strategically positioned close to the computing elements, commonly referred to as Block RAM (BRAM).

Spiker+ provides a synapse interface, implementing the start/ready handshake protocol, and relies on an initialization file containing quantized weights. Weighs are stored into BRAMs. Spiker+ expects all neurons to access their respective weights in parallel upon activating the ready signal by the synapse; therefore, it strongly relies on the high parallelism provided by on-board BRAMs. Spiker+ also permits storing weights in an external Dynamic Random Access Memorys (DRAMs) when on-board space is insufficient. In such situations, the synaptic interface loads the weights for the current cycle before asserting the ready signal, impacting the accelerator's speed.

A secondary configurable attribute concerning synapses involves incorporating feedback connections, such as interlayer inhibitory connections. Spiker+ can be configured to include or exclude these connections, depending on the application requirements.

### 4.6 I/O interface

Spiker+ requires an input/output interface to receive data and transmit results. Spiker+ supports two scenarios. In the simple scenario, inputs have already been encoded as spikes. For instance, these data may originate from neuromorphic sensors, such as a Dynamic Vision Sensor (DVS) cameras or a silicon cochlea. Alternatively, they could be pre-encoded by an external block before being stored.

In a more complex scenario, data are stored in a raw numeric format and converted on-board into spike streams. There are different methods available for this conversion, depending on the type of input data |43|: (i) firing rate coding (i.e., information is encoded using the instantaneous average firing rate), (ii) population rank coding (i.e., information is encoded using the relative firing time of a population of neurons), or (iii) temporal coding (i.e., information is encoded with the exact timing of individual spikes). An efficient rate encoding structure such as the one proposed in [5] can be directly connected to Spiker+. Furthermore, several possibilities exist concerning data transmission: data may arrive as a continuous stream directly from a sensor

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-07.jpg?height=304&width=379&top_left_y=149&top_left_x=152)

(a) IF neuron with dynamic reset

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-07.jpg?height=304&width=376&top_left_y=539&top_left_x=148)

(d) IF neuron with static reset

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-07.jpg?height=301&width=377&top_left_y=150&top_left_x=690)

(b) I-order LIF neuron with dynamic reset

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-07.jpg?height=271&width=377&top_left_y=556&top_left_x=690)

(e) I-order LIF neuron with static reset

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-07.jpg?height=344&width=727&top_left_y=148&top_left_x=1233)

(c) II-order LIF neuron with dynamic reset

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-07.jpg?height=325&width=732&top_left_y=545&top_left_x=1228)

(f) II-order LIF neuron with static reset

Fig. 5: Spiker+ neuron architectures in order of increasing complexity from left to right (IF, I-order LIF and II-order LIF), with the subtractive reset on top and the fixed one on the bottom. In the multiplexers, channels labeled with I indicate the beginning of the Integrate path, $\mathrm{R}$ the beginning of the Reset path, and $\mathrm{L}$ the beginning of the Leakage path.

or be transmitted through an external link. Alternatively, data may be stored in memory, necessitating memory access by the accelerator. To accommodate these diverse scenarios, Spiker+ uses the start/ready handshake protocol to manage the communication with the input interface.

At the accelerator output, decisions are usually taken based on the firing activity of the last layer. Spiker+ implements the output interface using simple counters, one for each output neuron, that can be post-processed outside the accelerator (e.g., the most active neuron wins). This simple implementation is only one possible option, and it can be easily customized to specific needs. The only requirement of the output interface is to implement the start/ready protocol to synchronize with the network.

It has to be noted that the latency measures reported in the paper have been taken considering input and output interfaces that can keep the pace of the accelerator. Overall, the network performance strongly depends on the interfaces. Results in section 6 aim to show the maximum performance Spiker+ can reach.

## 5 CONFIGURATION FRAMEWORK

Spiker+ goes beyond being a mere hardware accelerator; it is a comprehensive design framework that facilitates easy customization of the SNN accelerator for specific applications. As detailed in section 4 , the platform encompasses six distinct neuron models, a modular layer interface allowing instantiating any desired number of layers, and customizable inter-layer feedback connections. However, manually defining the architecture at the Register Transfer Level (RTL) requires substantial effort. To tackle this challenge, Spiker+ incorporates a Python-based configuration framework, streamlining the customization process to just a few lines of code. The customization and tuning flow for a specific target application within Spiker+ is depicted in Figure 6

First (step 1), the user selects the target dataset for training and optimizing the network, along with choosing the SNN model and architecture (e.g., FF-FC or FC-R SNN), specifying relevant parameters like the number of layers, neurons per layer, neuron type, and timing constants. These design choices are encapsulated by netbuilder in a Python object serving as a key actor in all subsequent steps. The models in Spiker+ align with those in snntorch 1 and similar frameworks. Integration with snntorch expedites the development phase, allowing network training using existing algorithms in this open-source Python framework (step 2). However, Spiker+ is not limited to this choice; it operates independently of the selected training framework. During the training phase, according to [5], time constants $\alpha$ and $\beta$ are first rounded to the closer power of two to compensate during training for this approximation. The result of this phase is the set of trained parameters (e.g., weights and thresholds) and an accuracy assessment of the trained model.

As training typically occurs in floating-point precision, unsuitable for edge hardware accelerators, Spiker+ implements functions to quantize the network, automating the exploration of the quantization impact on SNN accuracy (step 3). Spiker+ uses a two's complement N-bits fixedpoint representation. During quantization, if a value exceeds the representable range, it is saturated to the maximum or minimum value, depending on its value. The user can select the desired search interval (default: 64 to 0), and the tool iterates on these values, conducting complete inferences and returning the corresponding accuracy value (step 3).

In the final step (step 4), the chosen model, architecture, and trained parameters are delivered to the VHDL[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-08.jpg?height=590&width=1761&top_left_y=147&top_left_x=193)

Fig. 6: Spiker+ configuration framework

Generator, which automatically translates them into a VHSIC Hardware Description Language (VHDL) description of the SNN. This stage capitalizes on the modularity of the proposed architecture and utilizes an available library of neuron models.

Furthermore, the tool generates configuration files for storing the trained parameters in the FPGA memory. This provides flexibility to the user in selecting the type of memory. Modern hardware design tools, such as Xilinx Vivado ${ }^{\mathrm{TM}}$, support the automatic generation of Read Only Memory (ROM) memories, allowing users to choose the desired hardware platform, such as LUTs, BRAMs, or distributed Random Access Memory (RAM). These tools expect a configuration file as input, precisely what Spiker+ provides.

To the best of our knowledge, Spiker+ is the first complete high-level synthesis tool for an SNN, generating VHDL code automatically from a high-level Python description of the network. Additionally, the VHDL description of the accelerator includes a testbench for enhanced development and simulation convenience. This testbench allows inputs to be read from a file, enabling users to provide the desired input vector stimuli. It provides a straightforward interface to ensure the device synthesizes on the selected FPGA.

## 6 EXPERIMENTAL RESULTS

Spiker+ is evaluated using two widely recognized benchmark datasets: (i) MNIST [7| and (ii) SHD [8].

MNIST comprises grayscale images of handwritten digits from 0 to 9 , commonly used to benchmark AI algorithms. This dataset is ideal for comparing Spiker+ with other SNN accelerators. Images are converted into spikes using Poisson-distributed rate encoding. Due to the dataset simplicity, a basic I-order LIF model with a FF-FC structure suffices for accurate classification.

SHD is explicitly designed as an SNN benchmark, containing recordings of people pronouncing numbers in English and German. It requires a more complex neuron model, specifically a II-order LIF, and a network architecture with inter-layer recurrent connections to account for the importance of the time dimension in achieving acceptable classification accuracy.
These datasets differ significantly, demanding SNNs with distinct models and complexities, providing an opportunity to test Spiker+ reconfigurability for two different tasks. Both models are trained offline employing BPTT with two different surrogate gradient functions. Table 1 summarizes the considered setup.

TABLE 1: Summary of the experimental set-up on the two datastes

|  | MNIST | SHD |
| :---: | :---: | :---: |
| Type of data | Grey-scale images | Audio recordings |
| \# inputs | 784 | 700 |
| Spikes time-steps | 100 | 100 |
| Encoding | Rate code | Custom $[\overline{8}]$ |
| Training samples | 60,000 | 8,156 |
| Test samples | 10,000 | 2,264 |
| Network type | FF-FC SNN | FC-R SNN $[\overline{8}] \overline{9}]$ |
| Network arch | $784-128-10$ | $700-200-20$ |
| Neuron model | I-order LIF | II-order LIF |
| Reset | Subtractive | Subtractive |
| Training method | BPTT with SG | BPTT with SG |
| Surrogate function | Arctan | Fast Sigmoid |
| Model accuracy | $96.83 \%$ | $75.44 \%$ |

The remainder of this section is structured as follows: subsection 6.1 presents results from tuning Spiker+ on the two target datasets and compares it with state-of-the-art SNN accelerators on FPGA. Subsections 6.2, 6.3, and 6.4 analyze various Spiker+ configurations, examining the influence of architectural choices and data characteristics on area, latency, and power consumption.

### 6.1 Benchmarking

Table 2 provides a comprehensive comparison of Spiker+ with recent state-of-the-art FPGA accelerators designed for SNNs on the MNIST dataset. The table is split into two sections. The upper section covers Spiking Convolutional Neural Networks (SCNN) accelerators, where spiking layers are strategically placed after or interleaved with standard convolutional layers, gradually identifying key features in input images. The lower section considers pure spiking accelerators with fully connected layers of either IF or LIF neurons. Notably, the comparison focuses on works published from 2020 onward, while references such as [5]

TABLE 2: Comparison of Spiker+ to state-of-the-art FPGA accelerators for SNNs

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-09.jpg?height=1238&width=1806&top_left_y=194&top_left_x=165)

and [42] provide information on the performance of older accelerators.

In image classification tasks, SCNNs accelerators achieve superior accuracy, peaking at $99.30 \%$ with larger and more complex networks, often utilizing DSP cores on the FPGA (e.g., [35], [37], [40]). However, despite Spiker+ being explicitly designed to trade off accuracy with other design dimensions (i.e., power, area, latency), it still achieves a commendable accuracy of $93.85 \%$. Among purely spiking accelerators, it is only surpassed by [38], which employs an accelerator with higher latency, power, and size. Remarkably, Spiker+ excels in compactness and low power consumption. On a low-end Xilinx ${ }^{\mathrm{TM}}$ XC7Z020 FPGA board, it uses only 7,612 logic cells ( $4.8 \%$ of available cells) and 18 BRAMs ( $13 \%$ of available BRAMs), with a total power requirement of $180 \mathrm{~mW}$. This makes it an optimal solution for limited space or power-constrained applications. It is important to note that Table 2 measures the area with a general "logic cells" value obtained by combining LUTs and FFs for a concise overview. For Spiker+, these values are 4,314 and 3,298 , respectively. Detailed values for other accelerators can be found in their respective papers.

A noteworthy observation from the comparison in Table 2 is that the top power-efficient accelerators employ a clock-driven update policy. This counterintuitive finding contradicts the general literature assertion favoring eventdriven approaches for power efficiency. A clock-driven approach seems the most effective solution for relatively small-sized accelerators lacking sufficient sparsity in spiking activities. Li et al. attempted to address this with a hybrid architecture, adapting its update strategy to input sparsity. However, this strategy did not yield significant power savings, even with added complexity [42]. Importantly, the power consumption of the accelerator reported in [5] is unrealistically high. This is due an error in mapping I/O ports during the accelerator implementation, leading to an overestimated value.

Regarding latency, Spiker+ requires $780 \mu \mathrm{s}$ to classify an input image. Although not the fastest result in Table 2, this achievement is noteworthy considering the limited hardware resources and power consumption. Factors influencing this latency include: (i) the clock frequency capped at $100 \mathrm{MHz}$ due to BRAM access time; (ii) the image encoding using 100 time-steps (the window size impacts inference time directly); (iii) the speed of the input and output interfaces; (iv) the input spiking activity affecting the classification time, as explained in subsection 6.2 For comparison, Carpegna et al. |5] achieved $220 \mu s$ image classification time with a 3500 time-step encoding window. This work employed a different encoding that privileged spike sparsity, impacting accuracy (i.e., $73.96 \%$ ) but demonstrating how sparsity can reduce inference time.

In the second use case, Spiker+ is the first accelerator tested on SHD. Consequently, results reported in Table 3
are presented independently, without comparison to other architectures.

TABLE 3: Benchmarking on the SHD dataset

| $f_{\text {clk }}[\mathbf{M H z}]$ | 100 | Avail. logic cells | 159,600 |
| :---: | :---: | :---: | :---: |
| Neuron bw | 8 | Used logic cells | 18,268 |
| FF weights bw | 6 | Arch | $700-200-20$ |
| RR weights bw | 5 | \#syn | 184,000 |
| Update | Clock | $T_{\text {lat }} /$ img $[\mathbf{m s}]$ | 0.54 |
| Model | II order LIF | Power $[\mathbf{W}]$ | 0.43 |
| FPGA | XA7Z020 | E/img $[\mathrm{mJ}]$ | 0.23 |
| Avail. BRAM | 140 | E/syn $[n J]$ | 1.25 |
| Used BRAM | 51 | Accuracy | $72.99 \%$ |

The hardware requirements for this model exceed those used for MNIST due to several factors: the network architecture required to process this complex dataset is larger, the neuron model is a more complex II-order LIF (i.e., double size compared to a I-order LIF), and the accelerator uses a FC-R model featuring inter-layer feedback connections with weights stored in BRAM. The bit widths of the neuron membrane potential and weights are also higher. However, Spiker+ remains smaller and more power efficient than most accelerators in Table 2. Latency is reduced from $780 \mu \mathrm{s}$ for MNIST to $540 \mu s$ for SHD due to lower input activity in the biologically inspired encoding used for this dataset.

### 6.2 Performance vs input activity

As introduced earlier, the input spiking activity influenced by the encoding method significantly impacts the accelerator's performance. Before diving into this analysis, Figure 7 visually represents the average number of active cycles at different network layers. A notable difference is observed between the two considered datasets. In MNIST, the activity decreases monotonically across the network, while for SHD, there is a peak of activity in the first hidden layer. This difference may arise from the inter-layer feedbacks in SHD, leading to higher joint activity than the FF-FC architecture used in MNIST.

Since all layers update in parallel and process inputs sequentially, latency is determined by the slowest layer (i.e., the layer handling the largest set of inputs). In the architectures detailed in Table 1. the slowest layer is the input layer, processing 784 inputs for MNIST and 700 for SHD. In this layer, $100 \%$ of time-steps contain at least one spike for MNIST, while for SHD, the percentage is around $48 \%$. Consequently, SHD, with the combination of a lower number of inputs and lower activity in the input layer, enables increased inference speed. Since both models use the same number of time steps and clock frequency, and the difference in the number of inputs is not significant, one might expect about $48 \%$ inference time reduction for SHD compared to MNIST due to the reduced activity (i.e., about $0.37 \mathrm{~ms}$ ). However, the observed value in Table 3 is $0.54 \mathrm{~ms}$. The higher latency is explained by the FC-R model used by SHD incorporating inter-layer feedback connections, processed sequentially. Therefore, an additional set of 200 feedback inputs must be processed for the first layer, with an average of $93 \%$ active time steps.

The input activity not only impacts inference latency, as explained earlier. When the activity decreases, there is

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-10.jpg?height=304&width=873&top_left_y=149&top_left_x=1081)

Fig. 7: Visual representation of the average number of active cycles at various stages of the network

a higher probability of having spare cycles with no spikes, allowing the Layer CU to skip the sequential processing of all inputs. This reduction in calculations also affects power consumption. Figure 8 analyzes how power, latency, and energy (i.e., power times latency) change with different input activities on the two datasets, highlighting counterintuitive behaviors.

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-10.jpg?height=306&width=884&top_left_y=931&top_left_x=1076)

(a) Power

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-10.jpg?height=323&width=884&top_left_y=1275&top_left_x=1076)

(b) Latency

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-10.jpg?height=306&width=881&top_left_y=1641&top_left_x=1077)

(c) Energy

Fig. 8: Impact of input activity variations on energy, power, and latency of inference using Spiker+

Examining Figure 8a that reports the average power consumption of an inference task, we observe two distinct behaviors for MNIST and SHD. In the former case, reducing input activity increases power consumption, reaching a limit of around $200 \mathrm{~mW}$ as activity approaches zero and stabilizing at $180 \mathrm{~mW}$ under full activity. This behavior is explained by the clock-driven nature of the accelerator, where every clock cycle triggers a network update regardless of active spikes in the input. As mentioned earlier, Spiker+ skips time steps without active inputs. However, the exponential decay of the membrane is computed step by step. Therefore,
without input stimuli, Spiker+ dedicates one clock cycle to decay all membranes before returning to an idle state, awaiting the next input set. The two situations are similar from the perspective of neuron switching power, as the membrane is updated in both cases. However, the layer CU continually switches between two states, resulting in a high total switching activity and higher power consumption. Conversely, when executing a sequential update on the inputs, the $\mathrm{CU}$ enters the update state and then awaits the completion of the loop. This behavior is not observed in SHD. Given the larger architecture and higher weight bitwidth, power consumption in SHD is likely dominated by BRAMs access during input processing.

In conclusion, latency heavily depends on input activity. Without active spikes, the execution time tends to $\mathrm{N}$ cycles $\times \mathrm{T}_{\text {clk }}$, as a single clock cycle is sufficient to decay the membranes. Consequently, overall energy consumption is reduced with decreasing input activity.

### 6.3 Performance vs quantization

As one of the key features of Spiker+ is the optimization of the accelerator through quantization of weights and membrane potentials, it is crucial to examine how these design choices influence performance. Latency is not expected to change, as it is independent of the chosen bit-widths. The primary presumed impacts are on power consumption and network accuracy. Figures 9 and 10 illustrate the results of quantization on MNIST and SHD, respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-11.jpg?height=293&width=914&top_left_y=1331&top_left_x=150)

(a) Membrane potential quantization

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-11.jpg?height=290&width=897&top_left_y=1666&top_left_x=148)

(b) Synaptic weights quantization

Fig. 9: Impact of quantization of neuron membrane potentials $9 \mathrm{~b}$ and synaptic weights $9 \mathrm{a}$ on inference accuracy and power consumption for MNIST

The resilience of SNNs to quantization is notable. In both models, despite their differences and the unique information encoding methods employed, the decrease in accuracy with different quantization values is minimal. However, reducing precision by even a single bit has an evident effect on power consumption, owing to the large number of units working in parallel.

Finally, as explained in subsection 6.4 it is crucial to note that the primary constraint on Spiker+ size is the number of weights that can be accessed in parallel. A smaller weight

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-11.jpg?height=285&width=897&top_left_y=145&top_left_x=1075)

(a) Membrane potential quantization

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-11.jpg?height=290&width=881&top_left_y=468&top_left_x=1077)

(b) Feed-forward synaptic weights quantization

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-11.jpg?height=295&width=881&top_left_y=796&top_left_x=1077)

(c) Feedback synaptic weights quantization

Fig. 10: Impact of quantization of neuron membrane potentials 10a), feed-forward 10b and feedback synaptic weights 10c) on inference accuracy and power consumption for SHD

bit-width reduces the required interconnections and the amount of memory used, including the total number of BRAMs. This not only conserves power but also facilitates the implementation of larger architectures.

### 6.4 Performance vs sizing

Finally, let us explore the model complexity achievable with Spiker+ on selected Xilinx ${ }^{\text {TM }}$ FPGA boards. Synthesis results for three Xilin ${ }^{\mathrm{TM}}$ boards, particularly low-end ones suitable for resource-constrained edge applications, are presented in Table 4

On the Xilinx ${ }^{\mathrm{TM}}$ XC7Z020/XA7Z020 boards discussed in subsection 6.1, the largest FF-FC network possible using a Iorder LIF consists of 1,220 neurons, utilizing 138 BRAMs ( $98.5 \%$ ) and 42,430 LUTs ( $26.7 \%$ ). The BRAM size of the FPGA emerges as the limiting factor. On the slightly more advanced Xilinx ${ }^{\mathrm{TM}}$ XCZU3EG board, a larger 1,900-neuron architecture, utilizing 215 BRAMs ( $99.5 \%$ ) and 62,989 LUTs $(29.8 \%)$, can be implemented. Notably, the place-and-route algorithm encounters no obstacles, reinforcing that BRAM limitations govern the network size.

For FC-R architectures using a II-order LIF, where both feed-forward and feedback weights need storage in BRAM, the maximum network size is influenced. Specifically, it is capped at 550 neurons for Xilinx ${ }^{\top M}$ XC7Z020/XA7Z020 boards and 690 neurons for the Xilin $x^{\mathrm{TM}}$ XCZU3EG board.

Potential issues such as place-and-route complexities or excessive power consumption due to the fully parallel nature of the accelerator are foreseeable. A prospective

TABLE 4: Synthesis of maximum size accelerator on different Xilinx ${ }^{\mathrm{TM}}$ FPGAs boards

| Target system |  |  | $\overline{\text { FFFC }}$ |  |  |  | RSNN |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| FPGA | LUT | BRAM | Neurons | BRAM | LUT | Power | Neurons | BRAM | LUT | Power |
| XC7Z020 / XA7Z020 | 159600 | 140 (560KB) | 1224 | 138 | $44330 \quad$ | $1.07 \mathrm{~W}$ | 550 | 135 | 24,660 | $720 \mathrm{~mW}$ |
| XCZU3EG | 211,680 | 216 (864KB) | 1900 | 215 | 62989 | $1.2 \mathrm{~W}$ | 690 | 213 | 29,809 | $780 \mathrm{~mW}$ |

solution involves implementing a certain degree of time multiplexing to address these challenges while expanding the architecture size. This approach entails sharing hardware components between neurons, introducing a tradeoff with performance and is currently under study as an enhancement.

## 7 CoNCLUSIONS

This paper introduced Spiker+, a versatile framework to design low-power and resource-efficient hardware accelerators for SNNs targeting edge inference on FPGA platforms. It features a Python configuration framework that facilitates easy reconfiguration of the accelerator, allowing users to choose from six neuron models (IF, I-order LIF, and IIorder LIF, each with the option of a hard or subtractive reset) and two network architectures (FF-FC and FC-R). The tool enables the automatic selection of training and quantization parameters directly through Python. The results are significant, boasting a $93.85 \%$ accuracy on MNIST, with a classification latency of $780 \mu \mathrm{s}$ per image and power consumption of $180 \mathrm{~mW}$. Additionally, it achieves a $72.99 \%$ accuracy on SHD, corresponding to a $540 \mu \mathrm{s}$ latency and power consumption of $430 \mathrm{~mW}$. These metrics are highly competitive compared to state-of-the-art FPGA accelerators for SNNs, demonstrating high performance in both power efficiency and area. This work lays a solid foundation for deploying specialized, low-power, and efficient SNN accelerators in resource and power-constrained edge applications. Spiker+ is a live project, and ongoing work focuses on enlarging the library of available neurons, input encoding blocs, and network architectures. To encourage research in this field, Spiker+ is available as an open-source project ${ }^{2}$

## REFERENCES

[1] Z. Chang, S. Liu, X. Xiong, Z. Cai, and G. Tu, "A survey of recent advances in edge-computing-powered artificial intelligence of things," IEEE Internet of Things Journal, vol. 8, no. 18, pp. 13849$13875,2021$.

[2] N. K. Kasabov, Time-space, spiking neural networks and brain-inspired artificial intelligence. Springer, 2019.

[3] J. Xue, L. Xie, F. Chen, L. Wu, Q. Tian, Y. Zhou, R. Ying, and P. Liu, "Edgemap: An optimized mapping toolchain for spiking neural network in edge computing," Sensors, vol. 23, no. 14, p. 6548, 2023.

[4] P. Dhilleswararao, S. Boppu, M. S. Manikandan, and L. R. Cenkeramaddi, "Efficient hardware architectures for accelerating deep neural networks: Survey," IEEE Access, vol. 10, pp. 131 788-131 828, 2022.

[5] A. Carpegna, A. Savino, and S. Di Carlo, "Spiker: an FPGAoptimized Hardware accelerator for Spiking Neural Networks," in 2022 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2022, pp. 14-19, iSSN: 2159-3477.

[6] E. O. Neftci, H. Mostafa, and F. Zenke, "Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks," IEEE Signal Processing Magazine, vol. 36, no. 6, pp. 51-63, Nov. 2019, conference Name: IEEE Signal Processing Magazine.

2. Link to a public GitHub repo will be released in case of publication of the paper
[7] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998, conference Name: Proceedings of the IEEE.

[8] B. Cramer, Y. Stradmann, J. Schemmel, and F. Zenke, "The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks," IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 7, pp. 2744-2757, Jul. 2022, conference Name: IEEE Transactions on Neural Networks and Learning Systems.

[9] B. Yin, F. Corradi, and S. M. Boht√©, "Accurate and efficient timedomain classification with adaptive spiking recurrent neural networks," Nature Machine Intelligence, vol. 3, no. 10, pp. 905-913, Oct. 2021, number: 10 Publisher: Nature Publishing Group.

[10] A. L. Hodgkin and A. F. Huxley, "A quantitative description of membrane current and its application to conduction and excitation in nerve," The Journal of physiology, vol. 117, no. 4, pp. 500-544, 1952.

[11] E. Izhikevich, "Simple model of spiking neurons," IEEE transactions on neural networks, vol. 14, no. 6, pp. 1569-1572, 2003.

[12] R. Brette and W. Gerstner, "Adaptive exponential integrate-andfire model as an effective description of neuronal activity," Journal of Neurophysiology, vol. 94, no. 5, pp. 3637-3642, 2005.

[13] G. Bellec, F. Scherr, A. Subramoney, E. Hajek, D. Salaj, R. Legenstein, and W. Maass, "A solution to the learning dilemma for recurrent networks of spiking neurons," Nature Communications, vol. 11, no. 1, p. 3625, Jul. 2020, number: 1 Publisher: Nature Publishing Group.

[14] M. Stimberg, R. Brette, and D. F. Goodman, "Brian 2, an intuitive and efficient neural simulator," eLife, vol. 8, p. e47314, aug 2019.

[15] N. Rathi, I. Chakraborty, A. Kosta, A. Sengupta, A. Ankit, P. Panda, and K. Roy, "Exploring neuromorphic computing based on spiking neural networks: Algorithms to hardware," ACM Comput. Surv., vol. 55, no. 12, mar 2023.

[16] A. Basu, C. Frenkel, L. Deng, and X. Zhang, "Spiking Neural Network Integrated Circuits: A Review of Trends and Future Directions," Mar. 2022, arXiv:2203.07006 [cs].

[17] F. Pavanello, E. I. Vatajelu, A. Bosio, T. Van Vaerenbergh, P. Bienstman, B. Charbonnier, A. Carpegna, S. Di Carlo, and A. Savino, "Special Session: Neuromorphic hardware design and reliability from traditional CMOS to emerging technologies," in 2023 IEEE 41st VLSI Test Symposium (VTS), 2023, pp. 1-10.

[18] C. Mayr, S. Hoeppner, and S. Furber, "SpiNNaker 2: A 10 Million Core Processor System for Brain Simulation and Machine Learning," Nov. 2019, arXiv:1911.02385 [cs].

[19] "Next-Level Neuromorphic Computing: Intel Lab's Loihi 2 Chip." [Online]. Available: https://www.intel.com/content/www/us/en/research/ neuromorphic-computing-loihi-2-technology-briet.html

[20] F. Akopyan, J. Sawada, A. Cassidy, R. Alvarez-Icaza, J. Arthur, P. Merolla, N. Imam, Y. Nakamura, P. Datta, G.-J. Nam, B. Taba, M. Beakes, B. Brezzo, J. B. Kuang, R. Manohar, W. P. Risk, B. Jackson, and D. S. Modha, "TrueNorth: Design and Tool Flow of a $65 \mathrm{~mW} 1$ Million Neuron Programmable Neurosynaptic Chip," IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 34, no. 10, pp. 1537-1557, Oct. 2015, conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems.

[21] L. Deng, G. Wang, G. Li, S. Li, L. Liang, M. Zhu, Y. Wu, Z. Yang, Z. Zou, J. Pei, Z. Wu, X. Hu, Y. Ding, W. He, Y. Xie, and L. Shi, "Tianjic: A Unified and Scalable Chip Bridging Spike-Based and Continuous Neural Computation," IEEE Journal of Solid-State Circuits, vol. 55, no. 8, pp. 2228-2246, Aug. 2020, conference Name: IEEE Journal of Solid-State Circuits.

[22] O. Richter, C. Wu, A. M. Whatley, G. K√∂stinger, C. Nielsen, N. Qiao, and G. Indiveri, "DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor," Nov. 2023, arXiv:2310.00564 [cs].

[23] B. V. Benjamin, P. Gao, E. McQuinn, S. Choudhary, A. R. Chandrasekaran, J.-M. Bussat, R. Alvarez-Icaza, J. V. Arthur, P. A. Merolla, and K. Boahen, "Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations," Proceedings of the IEEE, vol. 102, no. 5, pp. 699-716, May 2014, conference Name: Proceedings of the IEEE.

[24] C. Pehle, S. Billaudelle, B. Cramer, J. Kaiser, K. Schreiber, Y. Stradmann, J. Weis, A. Leibfried, E. M√ºller, and J. Schemmel, "The BrainScaleS-2 Accelerated Neuromorphic System With Hybrid Plasticity," Frontiers in Neuroscience, vol. 16, 2022.

[25] "Welcome to Rockpool - Rockpool 2.7 documentation." [Online]. Available: https://rockpool.ai/index.html

[26] "Nengo." [Online]. Available: https://www.nengo.ai/

[27] S. Narayanan, K. Taht, R. Balasubramonian, E. Giacomin, and P.-E. Gaillardon, "SpinalFlow: An Architecture and Dataflow Tailored for Spiking Neural Networks," in 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), May 2020, pp. 349-362.

[28] D. Ma, J. Shen, Z. Gu, M. Zhang, X. Zhu, X. Xu, Q. Xu, Y. Shen, and G. Pan, "Darwin: A neuromorphic hardware co-processor based on spiking neural networks," Journal of Systems Architecture, vol. 77, pp. 43-51, Jun. 2017.

[29] C. Frenkel, J.-D. Legat, and D. Bol, ‚ÄúA 65-nm 738k-Synapse/mm2 Quad-Core Binary-Weight Digital Neuromorphic Processor with Stochastic Spike-Driven Online Learning," in 2019 IEEE International Symposium on Circuits and Systems (ISCAS), May 2019, pp. $1-5$, iSSN: 2158-1525.

[30] C. Frenkel, M. Lefebvre, J.-D. Legat, and D. Bol, "A 0.086-mm‚Äù2 12.7-pJ/SOP 64k-Synapse 256-Neuron Online-Learning Digital Spiking Neuromorphic Processor in 28-nm CMOS," IEEE Transactions on Biomedical Circuits and Systems, vol. 13, no. 1, pp. 145158, Feb. 2019, conference Name: IEEE Transactions on Biomedical Circuits and Systems.

[31] C. Frenkel and G. Indiveri, "ReckOn: A 28nm Sub-mm2 TaskAgnostic Spiking Recurrent Neural Network Processor Enabling On-Chip Learning over Second-Long Timescales," in 2022 IEEE International Solid- State Circuits Conference (ISSCC), vol. 65, Feb. 2022, pp. 1-3, iSSN: 2376-8606.

[32] D. Neil and S.-C. Liu, "Minitaur, an Event-Driven FPGA-Based Spiking Network Accelerator," IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 22, no. 12, pp. 2621-2628, Dec. 2014, conference Name: IEEE Transactions on Very Large Scale Integration (VLSI) Systems.

[33] H. Liu, Y. Chen, Z. Zeng, M. Zhang, and H. Qu, "A Low Power and Low Latency FPGA-Based Spiking Neural Network Accelerator," in 2023 International Joint Conference on Neural Networks (IJCNN), Jun. 2023, pp. 1-8, iSSN: 2161-4407.

[34] S. Gupta, A. Vyas, and G. Trivedi, "FPGA Implementation of Simplified Spiking Neural Network," in 2020 27th IEEE International Conference on Electronics, Circuits and Systems (ICECS), Nov. 2020, pp. 1-4.

[35] J. Li, G. Shen, D. Zhao, Q. Zhang, and Y. Zeng, "FireFly: A HighThroughput Hardware Accelerator for Spiking Neural Networks With Efficient DSP and Memory Optimization," IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 31, no. 8, pp. 1178-1191, Aug. 2023, conference Name: IEEE Transactions on Very Large Scale Integration (VLSI) Systems.

[36] S. Panchapakesan, Z. Fang, and J. Li, "SyncNN: Evaluating and Accelerating Spiking Neural Networks on FPGAs," in 2021 31st International Conference on Field-Programmable Logic and Applications (FPL), Aug. 2021, pp. 286-293, iSSN: 1946-1488.

[37] A. Khodamoradi, K. Denolf, and R. Kastner, "S2N2: A FPGA Accelerator for Streaming Spiking Neural Networks," in The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA '21. New York, NY, USA: Association for Computing Machinery, Feb. 2021, pp. 194-205.

[38] J. Han, Z. Li, W. Zheng, and Y. Zhang, "Hardware implementation of spiking neural networks on FPGA," Tsinghua Science and Technology, vol. 25, no. 4, pp. 479-486, Aug. 2020, conference Name: Tsinghua Science and Technology.

[39] G. Zhang, B. Li, J. Wu, R. Wang, Y. Lan, L. Sun, S. Lei, H. Li, and Y. Chen, "A low-cost and high-speed hardware implementation of spiking neural network," Neurocomputing, vol. 382, pp. 106-115, Mar. 2020

[40] Y. Nevarez, D. Rotermund, K. R. Pawelzik, and A. Garcia-Ortiz, "Accelerating Spike-by-Spike Neural Networks on FPGA With Hybrid Custom Floating-Point and Logarithmic Dot-Product Ap- proximation," IEEE Access, vol. 9, pp. 80 603-80 620, 2021, conference Name: IEEE Access.

[41] H. Asgari, B. M.-N. Maybodi, M. Payvand, and M. R. Azghadi, "Low-Energy and Fast Spiking Neural Network For ContextDependent Learning on FPGA," IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 67, no. 11, pp. 2697-2701, Nov. 2020, conference Name: IEEE Transactions on Circuits and Systems II: Express Briefs.

[42] S. Li, Z. Zhang, R. Mao, J. Xiao, L. Chang, and J. Zhou, "A Fast and Energy-Efficient SNN Processor With Adaptive Clock/EventDriven Computation Scheme and Online Learning," IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 68, no. 4, pp. 1543-1552, Apr. 2021, conference Name: IEEE Transactions on Circuits and Systems I: Regular Papers.

[43] N. Abderrahmane, E. Lemaire, and B. Miramond, "Design Space Exploration of Hardware Spiking Neurons for Embedded Artificial Intelligence," Neural Networks, vol. 121, pp. 366-386, Jan. 2020.

[44] D. Gerlinghoff, Z. Wang, X. Gu, R. S. M. Goh, and T. Luo, "E3NE: An End-to-End Framework for Accelerating Spiking Neural Networks With Emerging Neural Encoding on FPGAs," IEEE Transactions on Parallel and Distributed Systems, vol. 33, no. 11, pp. 3207-3219, Nov. 2022, publisher: IEEE Computer Society.

[45] P. Diehl and M. Cook, "Unsupervised learning of digit recognition using spike-timing-dependent plasticity," Frontiers in Computational Neuroscience, vol. 9, 2015

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-13.jpg?height=334&width=223&top_left_y=1080&top_left_x=1100)

Alessio Carpegna Alessio (student, IEEE '21) received an M.Sc. degree in Electronic engineer ing from the Politecnico di Torino, Torino, Italy, in 2021, with a specialization in digital systems design. In the same year he entered the Italian national Ph.D. program in Artificial Intelligence. His current research interests include Neuromorphic Systems, and in particular the design of neuromorphic hardware accelerators, Operating Systems and Embedded Systems in general.

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-13.jpg?height=322&width=241&top_left_y=1563&top_left_x=1083)

Alessandro Savino (M'14, SM'22) is an Assistant Professor in the Department of Control and Computer Engineering at Politecnico di Torino (Italy). He holds a Ph.D. (2009) and an M.S. equivalent (2005) in Computer Engineering and Information Technology from the Politecnico di Torino in Italy. Dr. Savino's research contributions include Approximate Computing, Reliabil ity Analysis, Safety-Critical Systems, SoftwareBased Self-Test, and Image Analysis. He has been part of the program and organizing committee of several IEEE and INSTICC conferences and has served as a reviewer of IEEE conferences and journals. His research interests include Operating Systems, Imaging algorithms, Machine Learning, Evolutionary Algorithms, Graphical User Interface experience, and Audio manipulation.

![](https://cdn.mathpix.com/cropped/2024_06_04_45d688569104f052e084g-13.jpg?height=331&width=268&top_left_y=2179&top_left_x=1075)

Stefano Di Carlo (SM'00-M'03-SM'11) He received an M.Sc. degree in computer engineering and a Ph.D. in information technologies from Politecnico di Torino, Italy, where he is a Full Professor. His research interests include DFT, BIST, and dependability. He has coordinated severa national and European research projects. Di Carlo has published over 200 papers in peerreviewed IEEE and ACM journals and conferences. He regularly serves on the Organizing and Program Committees of major IEEE and ACM conferences. He is a Golden Core member of the IEEE Computer Society and a senior member of the IEEE.


[^0]:    1. https://snntorch.readthedocs.io/en/latest/
