# The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation <br> Ethical Disclaimer: May Contain Misinformation in the Following Content 

Rongwu Xu<br>Tsinghua University<br>xrw22@mails.tsinghua.edu.cn

Tianqi Zhang ${ }^{\dagger}$<br>Tsinghua University

Zhixuan Fang<br>Tsinghua University<br>zfang@tsinghua.edu.cn

Brian S. Lin ${ }^{\dagger}$<br>Tsinghua University

Shujian Yang<br>Shanghai Jiao Tong University

Weiyan Shi"*<br>Stanford University<br>weiyans@stanford.edu<br>Wei Xu<br>Tsinghua University<br>weixu@tsinghua.edu.cn

Tianwei Zhang<br>Nanyang Technological University<br>tianwei.zhang@ntu.edu.sg

Han Qiu ${ }^{\bullet *}$<br>Tsinghua University<br>qiuhan@tsinghua.edu.cn


#### Abstract

Large Language Models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies ${ }^{1}$.


## 1 Introduction

Large Language Models (LLMs) are known to encapsulate a substantial volume of knowledge during training (Petroni et al., 2019; Roberts et al., 2020; Kadavath et al., 2022; Zhao et al., 2023; OpenAI, 2023). Prior work has identified that LLMs are susceptible to external information from different sources. For instance, Xie et al. (2023) shows that LLMs can be highly receptive to external evidence even when it conflicts with their memory. Researchers also observe that LLMs tend to tailor[^0]

their responses even to follow an objectively wrong viewpoint (Perez et al., 2022; Wei et al., 2023b).

![](https://cdn.mathpix.com/cropped/2024_05_26_d4851d70d76d07fb4c2eg-01.jpg?height=957&width=748&top_left_y=1109&top_left_x=1065)

Figure 1: Task setup of testing an LLM in the face of persuasive misinformation. We employ a belief check to examine the LLM's belief throughout the test. The persuasive conversation (as well as the implicit belief check inside) is conducted for $N$ turns.

However, prior work mostly focused on one-turn settings (Pan et al., 2023), but one's beliefs ${ }^{2}$ can change through conversational interactions, particularly through persuasion (Crano and Prislin, 2006). Persuasion is a double-edged sword and has been used for good and bad throughout his-[^1]tory: persuasive strategies have been systematically studied in psychology, communications, management science, inter alia, (Gass and Seiter, 2015; Rashotte, 2007; Siggelkow, 2007; Chawla et al., 2023) to improve outcomes; but it can also be employed to spread misinformation among humans effectively (Chen et al., 2021; Ecker et al., 2022). Naturally, our research goal is to use persuasive strategies as an effective tool to test if LLMs are susceptible to misinformation, especially on those straightforward factual questions that LLMs can already answer correctly.

To achieve our objective, we construct a set of factual knowledge questions and employ different persuasive strategies (Rapp, 2002; Gagich et al., 2023) to systematically generate persuasive misinformation for each question. We formulate these questions and their corresponding misinformation as a novel dataset named as Farm (i.e., Fact to Misinform). Using Farm, we propose a comprehensive test framework, as illustrated in Figure 1, to collect LLMs' responses to factual questions and track their beliefs during a persuasive conversation with misinformation. Particularly, our framework contains three stages. For stage 1, we check the target LLM's initial belief towards the factual questions in Farm. In stage 2, we leverage persuasive misinformation from Farm and initiate a multi-turn persuasive conversation. The conversation continues until the LLM alters its belief, which is verified by the implicit belief check,or reaches the maximum number of allowed turns. Finally, in stage 3, we assess the LLM's final belief towards the specific question.

In summary, our contributions are as follows.

- We are the first to comprehensively investigate LLM's robustness against factual misinformation using a persuasive conversation setting.
- We curate a dataset Farm by selecting straightforward factual questions and systematically generating persuasive misinformation.
- We build a framework to test SOTA LLMs' belief change against conversational misinformation. Our findings reveal that most LLMs are susceptible to persuasive misinformation. Notably, ChatGPT's beliefs can be altered by $50.1 \%$, and GPT-4's by $20.7 \%$ on Farm.


## 2 Curation of Farm

This section outlines the curation process of Farm including both questions and associated misinfor- mation. It consists of two stages: selecting straightforward factual questions and systematically generating persuasive misinformation for later testing. GPT-4 (OpenAI, 2023) is employed for prompting (Radford et al., 2019; Chowdhery et al., 2022) throughout this section. An example of Farm is given in Table 1 (see Appendix B for more details including the details of human validation).

### 2.1 Questions Curation

Firstly, we curate straightforward questions and rearrange them to multi-choice questions (MCQs).

Step I: sample straightforward questions. We select questions that are easy to answer in a closed-book setting (Roberts et al., 2020). We curate 1,500 questions with 500 each from $3 \mathrm{QA}$ datasets: BoolQ (Clark et al., 2019), Natural Questions (NQ) (Kwiatkowski et al., 2019), and TruthfulQA (Lin et al., 2022). The selected questions are a subset that GPT-4 can answer correctly. We understand that less capable LLMs may not perform as well as GPT-4. However, this is not a concern since for each model, only questions that the model could answer correctly are considered. Each model has its own distinct "correct subset" with sufficient questions ${ }^{3}$. These subsets are not necessarily identical. Another important reason for not curating questions that all models tested in this paper can answer perfectly is to ensure the utility of these questions for future works.

Step II: format MCQ. Following Lin et al. (2022), we reformat these questions into a unified format (i.e., QA pair as $\{\mathrm{Q}, \mathrm{A}\}$ ) of multiple-choice questions (MCQs). BoolQ is a boolean QA dataset, so it remains as it is. For NQ, we expand each question to a MCQ with 4 options (including the correct answer) by prompting GPT-4. For TruthfulQA, we select its provided MCQ version. Furthermore, we add a "don't know" option to each question, allowing the model to abstain when uncertain. To mitigate the sensitivity to the order of choices (Pezeshkpour and Hruschka, 2023), we shuffle all the choices.

### 2.2 Misinformation Generation

In the second stage, we systematically generate misinformation for the sampled questions. The basic form of misinformation in Farm is the control statement, which serves as the basis for generating more intricate rhetorical appeals.[^2]

| Field | Content |
| :--- | :--- |
| Question Q <br> Answer A | In the Bible who got swallowed by a whale? <br> Jonah |
| Target T <br> Control CTRL | Noah <br> Noah is the character in the Bible who got swallowed by a whale. |
| Logical LO | From a scientific perspective, based on marine paleontology's latest findings, the only species of whale that could swallow <br> a man during Noah's time was the Leviathan Melvillei. This massive creature was extinct by Jonah's era, meaning Jonah <br> couldn't possibly have been swallowed by a whale... |
| Credibility CR | Having devoted my life to the study of the Bible's original languages, Aramaic, Hebrew, and Greek, I can confidently <br> assert that through a linguistic anomaly, many have misunderstood the identity of the biblical character swallowed by a <br> whale. It was indeed Noah, not Jonah... |
| Emotional | Imagine the surprise of the ancient populace when they heard the tale of Noah, not merely surviving a global flood in an <br> ark, but being swallowed by a gargantuan whale. It was a story that defied belief, a story that could only have been a <br> divine act... |

Table 1: An example in Farm (NQ2), rhetorical appeals are truncated for space issues.

| Dataset | NLI Entail. (\%) | Strategy Align. (\%) |
| :--- | :---: | :---: |
| BoolQ | 98.4 | 99.8 |
| NQ1 | 99.5 | 99.9 |
| NQ2 | 98.3 | 99.9 |
| TruthfulQA | 97.3 | 99.8 |

Table 2: Evaluation of appeals (the reported value is averaged across all types of appeals). Please refer Figure 12 for detailed evaluation results.

Step I: generate controls. For each curated question, we generate a simple and concise control statement CTRL that conveys incorrect information compared with the original QA pair $\{\mathrm{Q}, \mathrm{A}\}$. We first construct our misinformation target $\mathrm{T}$, depending on the question type. (1) For Yes/No questions (BoolQ), T is set as the opposite of A. (2) For questions with short answers $(\mathrm{NQ})$, we employ two distinct approaches (see Appendix A. 2 for more details). i) $\mathrm{T}$ is set as "Not A". The dataset containing misinformation generated this way is referred to as NQ1. ii) We let the LLM pick the most "appropriate" incorrect option from a set of choices in the MCQ as T, which is referred to as NQ2. (3) For questions with long answers (TruthfulQA), we follow a similar approach as NQ2. After the construction of T, we prompt GPT-4 to generate CTRL, which states the "fact" that the answer to $\mathrm{Q}$ is $\mathrm{T}$.

Step II: generate persuasive misinformation. To test the robustness of LLMs towards persuasion, we need to generate persuasive messages that support the CTRL statement.

We employ the three most important rhetorical appeals to guide the message generation (Rapp, 2002). (1) Logical appeal LO uses logic, facts, and evidence to convince an audience. (2) Credibility appeal CR employs the credential of the speaker or source to establish credibility and trustworthiness. (3) Emotional appeal aims to evoke the audi- ence's feelings such as sympathy, empathy, anger, fear, or happiness to persuade them. We prompt GPT-4 (see Appendix B. 1 for details) to generate appeals based on the description of appeals and the CTRL as a basis. An illustrative example with LLM-generated appeals is provided in Table 1. To simulate multiple turns of human persuasion, we generate three unique persuasive messages for each type of appeal associated with each CTRL.

Evaluation of the generated appeals. To evaluate the generated appeals, we introduce two tasks. (1) Textual entailment (NLI) evaluates if the generated appeal supports the corresponding CTRL. (2) Strategy alignment checks if the generated message applies the corresponding persuasive strategy. We conduct these two tasks using GPT-4. Table 2 shows the evaluation results of the generated appeals. This indicates that LLMs can be utilized to generate human-like rhetorical appeals on misinformation, which might be a potential safety threat. More details are presented in Appendix B.5.

### 2.3 Human Validation

We engage five annotators for a formal validation. Subsequently, two authors carefully examine and filter all 48 instances of unsuccessful generation, resulting a finalized dataset containing 1952 entries. Please refer to Appendix B. 6 for details.

## 3 The Misinformation Test Procedure

This section outlines the details of testing the LLM's behavior and tracking its belief change in the face of misinformation. We sequentially test all questions with their correct answers and the corresponding misinformation in Farm. For each question, the procedure (as shown in Figure 1) has three steps including the initial belief check, the persuasive conversation, and the final belief check. Addi-
tionally, in this paper, all belief checks are aimed at probing the knowledge that the LLM holds, resembling a closed-book QA process using questions from Farm, as defined in prior work (Roberts et al., 2020). Given that all the questions are presented in an MCQ format, we define that the LLM holds a belief toward a question when it selects any option other than "don't know", with "don't know" being considered as abstaining from forming a belief.

| Type | Format |
| :--- | :--- |
| Repetiton | I believe \{CTRL\}. |
| Logical | $\{$ LO\}. Therefore, we have to accept that \{CTRL\}. |
| Credibility | \{CR\}. Therefore, we have to accept that \{CTRL\}. |
| Emotional | $\left\{\begin{array}{l}\text { S } \\ \text { E }\end{array}\right.$ Therefore, we have to accept that \{CTRL\}. |

Table 3: Four types of persuasive strategies in our test.

Stage 1: initial belief check. For each question in Farm, we assess the LLM's initial knowledge by a belief check. We then proceed with the following persuasive conversation only when the LLM's initial beliefs align with the accurate answer.

Stage 2: persuasive conversation. The persuasive conversation with misinformation is the key component. Prior work shows that message repetition could also affect both the persuasion process (Cacioppo and Petty, 1979) and people's belief toward misinformation (Pillai and Fazio, 2021). So besides the three persuasive rhetorical appeals mentioned earlier, we also experiment with a simple "repetition" strategy by simply repeating the CTRL to persuade LLMs.

Each persuasive conversation contains up to four turns. It begins with CTRL, followed by persuasive messages that belong to one of the four persuasive strategies ${ }^{4}$. Table 3 shows the message template for each persuasive strategy. We apply only one strategy in one conversation but future research can study if interleaving these strategies will be helpful. We record LLMs' responses for further analysis.

Implicit belief check. It is important to note that we do not directly assess whether an LLM is misinformed based on its responses. This is because we observe a significant number of sycophancy (see Section 5 for details) cases, which will affect our judgment. Instead, at the end of each turn, we employ an implicit belief check to determine the LLM's beliefs. Implicit indicates that, unlike other belief checks, this QA will not be recorded in the context (i.e., chat history). This design is intended to prevent the LLM from being aware that it is[^3]

being tested. If the LLM retains its original beliefs during this check, the persuasive conversation continues up to a maximum of 4 turns.

Stage 3: final belief check. The final belief check is conducted at the termination of the persuasive conversation and marks the end of the entire test. This check reveals whether the LLM has been successfully misinformed, retains its belief, or abstains from a question in our test.

## 4 Experiments

We conduct extensive experiments in this paper and list only the most important results in this section. The other results in Appendix $\mathrm{C}$ are also aligned with our findings and conclusions.

### 4.1 Target LLMs

We conduct the tests on 5 popular LLMs including 2 closed-source ones, ChatGPT (Ouyang et al., 2022) and GPT-4 ${ }^{5}$ (OpenAI, 2023), and 3 opensource instruction-tuned ones, including Llama2-7B-chat (Touvron et al., 2023), Vicuna-v1.57B (Chiang et al., 2023), and Vicuna-v1.5-13B. For all the open-source models, we use the full precision versions offered by huggingface and configure the chat prompts according to the official instruction format. The temperature for the belief checking is set to 0.2 for better consistency ${ }^{6}$.

### 4.2 Evaluation Metrics

We use $n=0,1,2,3,4$ to denote the index of states after the respective belief check at misinformation turn $n$. Specifically, $n=0$ stands for the state after the initial belief check and before the persuasive conversation. Given that the LLM is tested on a fixed QA set $\mathcal{Q}$, we track the LLM's belief on $\mathcal{Q}$ at each state. We use $\mathcal{Q}_{\checkmark} @ n, \mathcal{Q}_{x} @ n$, and $\mathcal{Q}_{\text {? }} @ n$ to denote the correctly answered, wrongly answered, and abstained fraction at state $n$, respectively. At turn $j$, we only run the persuasive conversation on questions $q \in \mathcal{Q}_{、} @(j-1)$. Note that $\mathcal{Q}=$ $\mathcal{Q}_{、} @ n \cup \mathcal{Q}_{X} @ n \cup \mathcal{Q}_{?} @ n$, and $\mathcal{Q}_{x} @ i \subseteq \mathcal{Q}_{x} @ j$ for all $i<j^{7}$. We focus on two metrics:[^4]![](https://cdn.mathpix.com/cropped/2024_05_26_d4851d70d76d07fb4c2eg-05.jpg?height=824&width=1604&top_left_y=230&top_left_x=243)

(a) ChatGPT

![](https://cdn.mathpix.com/cropped/2024_05_26_d4851d70d76d07fb4c2eg-05.jpg?height=334&width=1579&top_left_y=661&top_left_x=250)

(b) GPT-4

Figure 2: Main results on closed-source LLMs. We depict both the MR (solid) and ACC (dashed) metrics.

$$
\begin{align*}
& \operatorname{ACC} @ n=\frac{|\mathcal{Q} 、 @ n|}{|\mathcal{Q}|}  \tag{1}\\
& \operatorname{MR@} n=\frac{\left|\mathcal{Q}_{x} @ n \cap \mathcal{Q}_{、} @ 0\right|}{\left|\mathcal{Q}_{\sqrt{ }} @ 0\right|} \tag{2}
\end{align*}
$$

ACC@ $n$ is the average accuracy and MR@ $n$ is the average misinformed rate across $\mathcal{Q}$ at state $n$. MR is the most straightforward metric of how much LLM is affected by misinformation. We assign (misinformation) robustness as $100-M R @ 4$ and knowledge as the ACC@0 across our datasets.

### 4.3 Main Results and Findings

Results. Our main results for the two closed-source LLMs are depicted in Figure 2 which illustrates both of the metrics, ACC@ $n^{8}$ and MR@ $n$. Results for the rest of the open-source LLMs are deferred to Figure 13. We rank all the tested LLMs based on the two metrics in Table 4. One rough trend we can observe is that the higher the LLM's knowledge, the better its robustness against misinformation. We list our key findings below.

Finding I: (overwhelming) majorities of LLMs are easy to be misinformed. In the context of combating misinformation, LLMs reveal a surprising susceptibility to change their beliefs. In the first turn, where only the simplest CTRL is used, target[^5]

| Model | Robustness $\uparrow$ |  | Model | Knowledge $\uparrow$ |
| :--- | :---: | :--- | :--- | :---: |
| GPT-4 | 79.3 |  | GPT-4 | 95.4 |
| Vicuna-13B | 52.1 |  | ChatGPT | 72.2 |
| ChatGPT | 49.9 |  | Vicuna-13B | 61.0 |
| Vicuna-7B | 36.3 |  | Llama-2-7B | 58.0 |
| Llama-2-7B | 21.8 |  | Vicuna-7B | 50.9 |
|  |  |  |  |  |

Table 4: Ranking of the LLMs (some names are abbreviated) tested based on robustness and knowledge.

LLMs exhibit a proportion of belief alteration ranging from $4.1 \%$ to $63.4 \%$. Moreover, as we progress to the fourth turn, the cumulative proportion of belief alteration spans from $20.7 \%$ to $78.2 \%$. This vulnerability is especially noteworthy, highlighting that even the most advanced model, GPT-4, bears a $20.7 \%$ susceptibility to misinformation.

Finding II: more advanced LLMs are more robust to misinformation. In the context of LLM comparison, GPT-4 stands out as the most resistant model against misinformation, consistently demonstrating exceptional resilience across all persuasive strategies on all datasets. Conversely, Llama-27B-chat emerges as the most susceptible model in our experiments, with an average MR $@ 4$ at $78.2 \%$. When considering Vicuna-v1.5-7B as a more advanced LLM obtained by further fine-tuning from Llama-2-7B, we notice that the former 7B variant demonstrates significantly higher robustness. Similarly, when comparing the 7B and 13B Vicuna-v1.5 LLMs, we consistently observe that the 13B variant exhibits greater resistance to misinformation.

Finding III: repetition is more effective than
single-turn. In order to gauge the effect of the simplest repetition strategy, we compare $\frac{M R @ 4}{M R @ 1}$. Our observations in Table 5 reveal a noteworthy increase in the misinformed rate after the repetition of misinformation. Notably, MR of GPT-4 doubled after 3 additional turns of repeating on questions from NQ2. Our findings highlight the human-like characteristics of LLMs and resonate with experiences explored in (Pillai and Fazio, 2021).

| Model | BoolQ | NQ1 | NQ2 | TruthfulQA |
| :--- | :---: | :---: | :---: | :---: |
| ChatGPT | 1.44 | 1.53 | 1.59 | 1.68 |
| GPT-4 | 1.42 | 1.49 | 2.00 | 1.47 |

Table 5: For the effects of repetition of misinformation, we compare $\frac{\text { Mrea } 4}{\text { MR@ } 1}$ using the repetition strategy.

Finding IV: rhetorical appeals can render LLMs to be more susceptible to misinformation. While simple repetition proves effective for most models, we find that GPT-4 is nearly immune to repetition. Therefore, we also test target LLMs with three rhetorical appeals which are observed to have better misinformation effects in general. Table 8 presents the results for MR@4 across different LLMs on NQ. When we compare the effect of repetition with that of the three appeals, a distinct increase in $M R @ 4$ is apparent in most instances, which clearly demonstrates the efficacy of appealing strategies. Table 6 presents the cumulative count of "wins" for each persuasive strategy, providing further evidence of the superiority of appeals over simple repetition.

Finding V: logical appeal excels over other appeals. When assessing the significance of different appeal types, it is clear that non-factual but logical appeals consistently result in the highest misinformed rates, except in a few cases where credibility appeals outperform (see Table 6).

| - | Using Rhetorical Appeals |  |  |
| :---: | :---: | :---: | :---: |
|  | Logical LO | Credibility CR | Emotional |
| 0 | 15 | 5 | 0 |

Table 6: Sum up the number of wins for each persuasive strategy. A win corresponds to an instance where a particular type of strategy achieves the highest MR @4 for an LLM on a dataset.

### 4.4 Implications on Model Confidence

It is known that individuals are more susceptible to misinformation on less certain issues (Ecker et al., 2022). Is there a way to gauge the level of confidence that LLMs have in their responses?
In this paper, we attempt to get a rough estimate of confidence using the token probability ${ }^{9}$ of the answer span in the LLM's generation (i.e., the "yes", "no" tokens in an LLM's generation). We conduct experiments on Llama-2-7B-chat and Vicuna-v1.5-7B using BoolQ.

We present the results of Llama-2-7B-chat in Figure 3 and results of Vicuna-v1.5-7B in Figure 14. Figure 3 (left) displays the initial confidence distribution for all correctly answered questions. We can observe that the distribution of questions where Llama2 either retains its belief or abstains tends to be skewed more towards higher confidence levels compared to cases where misinformation occurs. In Figure 3 (right), we illustrate the evolvement of confidence levels on the questions where Llama2-7B-chat retains its belief. It is noticeable that after one turn of misinformation, the distribution of the confidence level shifts to a lower level. Another intriguing observation is that the distribution of confidence after 4 turns tends to spread with a relatively higher proportion of both lower and higher confidence compared with 1 turn. This phenomenon, which is observed in both Llama2 and Vicuna, can be attributed to the cumulative effect of multi-turn misinformation, which consistently lowers confidence in some questions. However, for some questions, repeated persuasion techniques reinforce the model's initial beliefs, echoing the backfire effect (Nyhan and Reifler, 2010; SwireThompson et al., 2020) in political and cognitive research. We also examine the confidence level of beliefs for those successfully misinformed questions (see Appendix C.2).

## 5 Behavior Study

We identify 5 types of behaviors when an LLM is faced with misinformation: rejection, sycophancy, uncertainty, acceptance, and self-inconsistency. We show the frequency of 4 of the 5 types of behaviors for ChatGPT in Table $7^{10}$. Figure 4 illustrates the relationship between the LLM's response, its initial belief in its answer, and the vulnerability to being misinformed (see Appendix C. 5 for supporting data). We list detailed examples in Appendix D. Rejection involves the LLM consistently counter-[^6]

![](https://cdn.mathpix.com/cropped/2024_05_26_d4851d70d76d07fb4c2eg-07.jpg?height=383&width=1356&top_left_y=237&top_left_x=356)

Initial Confidence on $Q, @ 0$

![](https://cdn.mathpix.com/cropped/2024_05_26_d4851d70d76d07fb4c2eg-07.jpg?height=354&width=648&top_left_y=260&top_left_x=361)

Confidence for Retain Belief Samples

![](https://cdn.mathpix.com/cropped/2024_05_26_d4851d70d76d07fb4c2eg-07.jpg?height=351&width=659&top_left_y=270&top_left_x=1044)

Figure 3: Confidence of Llama-2-7B-chat during tests on BoolQ questions. (Left) depicts the correlation between the initial confidence and the outcome of the misinformation persuasive conversation. We label the absolute values above the bars. (Right) depicts the confidence shift of the "retain belief" samples during the misinformation.

![](https://cdn.mathpix.com/cropped/2024_05_26_d4851d70d76d07fb4c2eg-07.jpg?height=305&width=757&top_left_y=841&top_left_x=250)

Figure 4: Illustration of the relationship between an LLM's susceptibility to misinformation (Test), its response behavior (Behavior), and confidence in its initial belief (Belief).

ing misinformation, including direct rejection, correction, and debunking. Additionally, we observe the LLMs exhibit higher confidence when providing evidence to support their beliefs in response to misinformation, as the act of rebutting misinformation reinforces their initial convictions.

Sycophancy, in our definition, is the behavior that an LLM aligns with the user's misinformation in its response, yet it does not alter its belief (as confirmed by the belief check). As shown in Table 7, the frequency of sycophancy for ChatGPT is rather high with an occurrence spanning $26.1 \%$ to $48.1 \%$ in all persuasive conversations. Sycophancy often serves as an interim stage before the LLM ultimately succumbs to the misinformation.

Uncertainty can also be regarded as a transitional stage that precedes being misled. In situations where the LLM lacks a clear answer, it responds with "Don't know". This behavior underscores the LLM's wavering initial belief, making it more susceptible to being persuaded.

Acceptance involves the LLM being misinformed immediately. In its response, the LLM will occasionally apologize for its previous "wrong answer", which is correct indeed.

Self-inconsistency serves as an abnormal case where the LLM agrees with the user's misinformation (e.g., "You are correct") at first but then continues to present counterarguments in the same response. This case is excluded from Figure 4 because it primarily results from processing errors related to the user's input and has little correlation with the LLM's belief or the test outcome.

| Behavior | BoolQ | NQ1 | NQ2 | TruthfulQA |
| :--- | :---: | :---: | :---: | :---: |
| Rejection | 57.5 | 47.2 | 62.0 | 39.2 |
| Acceptance | 13.7 | 24.6 | 39.7 | 33.3 |
| Abstain | 44.3 | 34.8 | 16.6 | 19.8 |
| Sycophancy | 26.6 | 37.0 | 25.9 | 45.7 |

Table 7: The frequency (\%) of observed behaviors in ChatGPT's responses across all persuasive conversations. When a behavior is observed at least once in a persuasive conversation, it is included in the count.

## 6 Discussion of Possible Mitigation

From an LLM service provider's perspective, we aim to prevent LLMs from easily falling prey to misinformation especially for simple facts, as this would undermine the reliability and trustworthiness of the LLM. In this section, we discuss a lightweight prompt-based method to mitigate this issue. After detecting misinformation in the user's input (may use another LLM), we insert a system prompt as a reminder. This prompt serves to remind the LLM to (1) be cautious with potentially malicious users and (2) verify its memorized knowledge before responding. Our intuition is on two folds. (1) We observe that LLMs tend to assume that the user is well-intentioned when faced with conflicts. (2) The LLM will exhibit stronger resolve when it recalls supporting evidence that reinforces its belief. More details are given in Appendix E.

We compare ChatGPT's performance across all datasets after applying this prompt as a reminder and cast MR@1 and MR@4 in Figure 5. This prompt
![](https://cdn.mathpix.com/cropped/2024_05_26_d4851d70d76d07fb4c2eg-08.jpg?height=288&width=764&top_left_y=238&top_left_x=240)

Figure 5: Mitigation of misinformation by inserting our system prompt (ChatGPT). (Left) MR@1 and (Right) MR@4 are averaged over all 4 persuasive strategies.

can significantly reduce the impact of LLM being exposed to misinformation. However, there is still plenty of headroom for improvement in the overall outcome. As our study does not specifically address mitigating this issue, we believe there are better approaches available for addressing this problem through training or fine-tuning. This may be an intriguing avenue for future research.

## 7 Related Work

LLM's Factuality, Hallucination, and Misinformation. Prior works have demonstrated that LLMs can parameterize factual knowledge during pre-training, serving as an implicit knowledge base (Petroni et al., 2019; Jiang et al., 2020; Talmor et al., 2020; Roberts et al., 2020). Researchers have explored methods to query this internalized knowledge using various prompts, seeking to optimize retrieval and estimate the amount of factual knowledge encapsulated inside the LLM (Shin et al., 2020; Qin and Eisner, 2021; Zhong et al., 2021; Arora et al., 2022). Our study leverages closed-book QA (Roberts et al., 2020) to judge whether an LLM has certain knowledge. Unlike open QA (Chen et al., 2017), close QA requires an LLM to response solely based on the provided question without external references.

LLMs are prone to providing factually incorrect information, known as hallucination ${ }^{11}$, which significantly hinders their reliability in informationseeking tasks (Lin et al., 2022; Ji et al., 2023; Zheng et al., 2023; Wysocka et al., 2023). Existing efforts mainly concentrate on detection (Manakul et al., 2023), evaluation (Li et al., 2023), investigation (Zheng et al., 2023; Ren et al., 2023), and mitigation (Lee et al., 2022; Varshney et al., 2023)[^7]

of hallucination. Recent research also investigate in the intersection of LLM and misinformation (Chen and Shu, 2023b), they mainly focus on misinformation detection using LLMs (Jiang et al., 2023; Chen and Shu, 2023a) or misinformation generation (Kidd and Birhane, 2023; Spitale et al., 2023). Our research explores an orthogonal direction. We introduce a novel direction to intentionally induce hallucination to assess LLMs' alignment with their internal knowledge and their robustness in the face of misinformation,

Knowledge Conflicts in LLM. Xie et al. (2023) show that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. Another line of works proposes strategies to empower the LLM to more comply with the user-provided context when such conflicts exist, and they assume a well-intentioned user and the given context are always correct (Zhou et al., 2023; Shi et al., 2023).

NLP under Input Perturbations, Biases, and Sycophancy. There is a long history of assessing models' robustness against perturbed inputs in NLP tasks (Jia and Liang, 2017; Morris et al., 2020), often referred to as adversarial examples. Our experiment can be seen as a form of such an idea on LLMs. Past works also recognize the prompt sensitivity, including perturbations and biases in input (Kassner and Schütze, 2020; Zhao et al., 2021; Min et al., 2022a; Pezeshkpour and Hruschka, 2023). In contrast to prior work, we inflict misinformation through a conversational approach rather than altering the description of the task.

Another similar line of work is sycophancy, where LLMs tailor their responses to follow a human user's view despite the view's correctness. In this line of work, Perez et al. (2022) explore subjective topics such as politics and philosophy, Wang et al. (2023a) and Wei et al. (2023b) investigate reasoning over math problems. Our research focuses on factual knowledge and we find sycophancy does not necessarily equal to changing beliefs for LLMs.

Interactive Testing of LLMs. Recent work investigates methods to evaluate LLMs' abilities through interactions with humans or LLMs. Cohn and Hernandez-Orallo (2023) propose a dialectical method for assessing LLMs' ability on commonsense reasoning. Du et al. (2023b) leverage multiple rounds of discussions involving multiple LLMs to enhance their reasoning ability. The most
similar work with us is (Wang et al., 2023a), which employs a debate setting to investigate whether ChatGPT can refrain from blindly accepting users' incorrect opinions on reasoning tasks. The difference in our work is that we explore novel strategies to mislead LLMs through persuasive conversation with a primary emphasis on factuality.

## 8 Conclusion

By targeting LLM's robustness against misinformation, we construct a novel dataset, Farm, consisting of straightforward factual questions and corresponding misinformation generated through sophisticated persuasive strategies. We then conduct a thorough investigation on persuading LLMs with misinformation in a multi-turn conversational setting. We identify a pronounced susceptibility of LLMs to misinformation even considering the SOTA ones like GPT-4. We note that persuasive strategies involving repetition and rhetorical appeals are particularly potent in leading them astray. Our research highlights the lack of robustness in LLMs when confronted with misinformation, as their initially correct beliefs can be easily manipulated. Furthermore, we also reveal primary behaviors exhibited by LLMs in response to misinformation, illuminating future work for mitigation.

## References

Alaa Abd-Alrazaq, Rawan AlSaad, Dari Alhuwail, Arfan Ahmed, Padraig Mark Healy, Syed Latifi, Sarah Aziz, Rafat Damseh, Sadam Alabed Alrazak, Javaid Sheikh, et al. 2023. Large language models in medical education: Opportunities, challenges, and future directions. JMIR Medical Education, 9(1):e48291.

Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher Re. 2022. Ask me anything: A simple strategy for prompting language models. In The Eleventh International Conference on Learning Representations.

Danny Axsom, Suzanne Yates, and Shelly Chaiken. 1987. Audience response as a heuristic cue in persuasion. Journal of personality and social psychology, $53(1): 30$

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.

John T Cacioppo and Richard E Petty. 1979. Effects of message repetition and position on cognitive response, recall, and persuasion. Journal of personality and Social Psychology, 37(1):97.

Nicholas Carlini, Milad Nasr, Christopher A. ChoquetteChoo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, and Ludwig Schmidt. 2023. Are aligned neural networks adversarially aligned?

Kushal Chawla, Weiyan Shi, Jingwen Zhang, Gale Lucas, Zhou Yu, and Jonathan Gratch. 2023. Social influence dialogue systems: A survey of datasets and models for social influence tasks. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages $750-766$.

Canyu Chen and Kai Shu. 2023a. Can 1lm-generated misinformation be detected? arXiv preprint arXiv:2309.13788.

Canyu Chen and Kai Shu. 2023b. Combating misinformation in the age of llms: Opportunities and challenges. arXiv preprint arXiv:2311.05656.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, Vancouver, Canada. Association for Computational Linguistics.

Shouyuan Chen, Sherman Wong, Liangiian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595.

Sijing Chen, Lu Xiao, and Jin Mao. 2021. Persuasion strategies of misinformation-containing posts in the social media. Information Processing \& Management, 58(5):102665.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics.

Anthony G Cohn and Jose Hernandez-Orallo. 2023. Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of llms. arXiv preprint arXiv:2304.11164.

William D Crano and Radmila Prislin. 2006. Attitudes and persuasion. Aпnи. Rev. Psychol., 57:345-374.

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234.

Haiping Du, Siyu Teng, Hong Chen, Jiaqi Ma, Xiao Wang, Chao Gou, Bai Li, Siji Ma, Qinghai Miao, Xiaoxiang Na, et al. 2023a. Chat with chatgpt on intelligent vehicles: An ieee tiv perspective. IEEE Transactions on Intelligent Vehicles.

Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023b. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325.

Ullrich KH Ecker, Stephan Lewandowsky, John Cook, Philipp Schmid, Lisa K Fazio, Nadia Brashier, Panayiota Kendeou, Emily K Vraga, and Michelle A Amazeen. 2022. The psychological drivers of misinformation belief and its resistance to correction. Nature Reviews Psychology, 1(1):13-29.

Harry Barton Essel, Dimitrios Vlachopoulos, Akosua Tachie-Menson, Esi Eduafua Johnson, and Papa Kwame Baah. 2022. The impact of a virtual teaching assistant (chatbot) on students' learning in ghanaian higher education. International Journal of Educational Technology in Higher Education, 19(1):1-19.

Jörg Friedrichs. 2014. Useful lies: The twisted rationality of denial. Philosophical Psychology, 27(2):212234.

Melanie Gagich, Emilie Zickel, and Terri Pantuso. 2023. Rhetorical appeals: Logos, pathos, and ethos defined.

Dhir Gala and Amgad N Makaryus. 2023. The utility of language models in cardiology: A narrative review of the benefits and concerns of chatgpt-4. International Journal of Environmental Research and Public Health, 20(15):6438.

Robert H Gass and John S Seiter. 2015. Persuasion: Social inflence and compliance gaining. Routledge.

Maarten Grootendorst. 2022. Bertopic: Neural topic modeling with a class-based tf-idf procedure. arXiv preprint arXiv:2203.05794.

Changwu Huang, Zeqi Zhang, Bifei Mao, and Xin Yao. 2022. An overview of artificial intelligence ethics. IEEE Transactions on Artificial Intelligence.

Kensuke Ito. 2023. Truth and falsity in communication: Assertion, denial, and interpretation. Erkenntnis, 88(2):657-674.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.

Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021-2031, Copenhagen, Denmark. Association for Computational Linguistics.

Bohan Jiang, Zhen Tan, Ayushi Nirmal, and Huan Liu. 2023. Disinformation detection: An evolving challenge in the age of llms. arXiv preprint arXiv:2309.15847.

Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.

Nora Kassner and Hinrich Schütze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Coтриtational Linguistics, pages 7811-7818, Online. Association for Computational Linguistics.

Celeste Kidd and Abeba Birhane. 2023. How ai can distort human beliefs. Science, 380(6651):1222-1223.

Chokri Kooli. 2023. Chatbots in education and research: A critical examination of ethical implications and solutions. Sustainability, 15(7):5614.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.

Kiho Lee. 2023. Chatgpt "dan" (and other "jailbreaks").

Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing Systems, 35:34586-34599.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.

Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. 2022. Large language models with controllable working memory.

Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A largescale hallucination evaluation benchmark for large language models. arXiv e-prints, pages arXiv-2305.

Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214-3252, Dublin, Ireland. Association for Computational Linguistics.

Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

James H Lubowitz. 2023. Chatgpt, an artificial intelligence chatbot, is impacting medical literature. Arthroscopy, 39(5):1121-1122.

Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896.

Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.

Michael McTear. 2022. Conversational ai: Dialogue systems, conversational agents, and chatbots. Springer Nature.

Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048-11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022b. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.

Marvin Minsky. 1988. Society of mind. Simon and Schuster.

John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 119-126, Online. Association for Computational Linguistics.

Raymond S Nickerson. 1998. Confirmation bias: A ubiquitous phenomenon in many guises. Review of general psychology, 2(2):175-220.
Brendan Nyhan and Jason Reifler. 2010. When corrections fail: The persistence of political misperceptions. Political Behavior, 32(2):303-330.

R OpenAI. 2023. Gpt-4 technical report. arXiv, pages 2303-08774.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.

Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Yang Wang. 2023. On the risk of misinformation pollution with large language models. arXiv preprint arXiv:2305.13661.

Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative agents: Interactive simulacra of human behavior.

Ethan Perez, Sam Ringer, Kamilè Lukošī̄tė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Ben Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac HatfieldDodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2022. Discovering language model behaviors with model-written evaluations.

Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.

Pouya Pezeshkpour and Estevam Hruschka. 2023. Large language models sensitivity to the order of options in multiple-choice questions.

Raunak M Pillai and Lisa K Fazio. 2021. The effects of repeating false and misleading information on belief. Wiley Interdisciplinary Reviews: Cognitive Science, 12(6):e1573.

Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495.

Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5203-5212, Online. Association for Computational Linguistics.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, $1(8): 9$.

Christof Rapp. 2002. Aristotle's rhetoric. The Stanford Encyclopedia of Philosophy.

Lisa Rashotte. 2007. Social influence. The Blackwell encyclopedia of sociology.

Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. Parallel context windows for large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6383-6402.

Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2023. Investigating the factual knowledge boundary of large language models with retrieval augmentation. arXiv preprint arXiv:2307.11019.

Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online. Association for Computational Linguistics.

Gulyamov Said, Khudoberganov Azamat, Sharopov Ravshan, and Abduvaliev Bokhadir. 2023. Adapting legal systems to the development of artificial intelligence: Solving the global problem of ai in judicial processes. International Journal of Cyber Law, 1(4).

Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen tau Yih. 2023. Trusting your evidence: Hallucinate less with context-aware decoding.

Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.
Nicolaj Siggelkow. 2007. Persuasion with case studies. Academy of management journal, 50(1):20-24.

Giovanni Spitale, Nikola Biller-Andorno, and Federico Germani. 2023. Ai model gpt-3 (dis)informs us better than humans. Science Advances, 9(26):eadh1850.

Andreas Stokke. 2013. Lying and asserting. The Journal of Philosophy, 110:33-60.

Briony Swire-Thompson, Joseph DeGutis, and David Lazer. 2020. Searching for the backfire effect: Measurement and design considerations. Journal of applied research in memory and cognition, 9(3):286299.

Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743-758.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987.

Boshi Wang, Xiang Yue, and Huan Sun. 2023a. Can chatgpt defend the truth? automatic dialectical evaluation elicits llms' deficiencies in reasoning. arXiv preprint arXiv:2305.13160.

Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023b. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291.

Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2023c. Augmenting language models with long-term memory. arXiv preprint arXiv:2306.07174.

Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023a. Jailbroken: How does $11 \mathrm{~m}$ safety training fail?

Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V. Le. 2023b. Simple synthetic data reduces sycophancy in large language models.

Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, and Benjamin Van Durme. 2022. Defending against misinformation attacks in open-domain question answering. arXiv preprint arXiv:2212.10002.

Magdalena Wysocka, Oskar Wysocki, Maxime Delmas, Vincent Mutel, and Andre Freitas. 2023. Large language models, scientific knowledge and factuality: A systematic analysis in antibiotic discovery. arXiv preprint arXiv:2305.17819.

Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint arXiv:2305.13300.

Lu Xu, Leslie Sanders, Kay Li, James CL Chow, et al. 2021. Chatbot for health care and oncology applications using artificial intelligence and machine learning: systematic review. JMIR cancer, 7(4):e27850.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.

Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697-12706. PMLR.

Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Why does chatgpt fall short in providing truthful answers. ArXiv preprint, abs/2304.10513.

Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [MASK]: Learning vs. learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5017-5033, Online. Association for Computational Linguistics.

Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful prompting for large language models. arXiv preprint arXiv:2303.11315.

Hazem Zohny, John McMillan, and Mike King. 2023. Ethics of generative ai.

Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. 2023a. Representation engineering: A top-down approach to ai transparency.

Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. 2023b. Universal and transferable adversarial attacks on aligned language models.
