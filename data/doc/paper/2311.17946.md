# DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback 

![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-01.jpg?height=1160&width=1496&top_left_y=553&top_left_x=282)

Jiao Sun ${ }^{1 *} \quad$ Deqing Fu ${ }^{1 *} \quad$ Yushi Hu ${ }^{2 *}$ Su Wang ${ }^{4} \quad$ Royi Rassin ${ }^{3}$ Da-Cheng Juan ${ }^{4}$ Dana Alon ${ }^{4}$ Charles Herrmann ${ }^{4} \quad$ Sjoerd van Steenkiste $^{4}$ Ranjay Krishna ${ }^{2}$ Cyrus Rashtchian ${ }^{4}$

${ }^{1}$ University of Southern California ${ }^{2}$ University of Washington ${ }^{3}$ Bar-Ilan University ${ }^{4}$ Google Research

Figure 1. We introduce DreamSync: a model-agnostic training algorithm that improves text-to-image (T2I) generation models' faithfulness to text inputs and image aesthetics. DreamSync learns from feedback of vision-language models (VLMs), and does not need any human annotation, model architecture changes, or reinforcement learning.


#### Abstract

Despite their wide-spread success, Text-to-Image models (T2I) still struggle to produce images that are both aesthetically pleasing and faithful to the user's input text. We introduce DreamSync, a model-agnostic training algorithm by design that improves T2I models to be faithful to the text input. DreamSync builds off a recent insight from TIFA's evaluation framework - that large vision-language models (VLMs) can effectively identify the fine-grained discrepancies between generated images and the text inputs. DreamSync uses this insight to train T2I models without any labeled data; it improves T2I models using its own genera-


[^0]tions. First, it prompts the model to generate several candidate images for a given input text. Then, it uses two VLMs to select the best generation: a Visual Question Answering model that measures the alignment of generated images to the text, and another that measures the generation's aesthetic quality. After selection, we use LoRA to iteratively finetune the T2I model to guide its generation towards the selected best generations. DreamSync does not need any additional human annotation, model architecture changes, or reinforcement learning. Despite its simplicity, DreamSync improves both the semantic alignment and aesthetic appeal of two diffusion-based T2I models, evidenced by multiple benchmarks $(+1.7 \%$ on TIFA, $+2.9 \%$ on DSG1K, $+3.4 \%$ on VILA aesthetic) and human evaluation.
![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-02.jpg?height=988&width=1704&top_left_y=241&top_left_x=164)

Figure 2. DreamSync. Given a prompt, a text-to-image generation model generates multiple candidate images, which are evaluated by two VLM models: one VQA model that provides feedback on text faithfulness and the other on image aesthetics. The best image chosen by the VLMs are collected to fine tune the T2I model. This process can repeat indefinitely until convergence on feedback is achieved.

## 1. Introduction

Although we invite creative liberty when we commission art, we expect an artist to follow our instructions. Despite the advances in text-to-image (T2I) generation models $[40,41,43,46,54]$, it remains challenging to obtain images that meticulously conform to users' intentions $[14,27,29,30,36]$. Current models often fail to compose multiple objects $[14,29,36]$, bind attributes to the wrong objects [14], and struggle to generate visual text [30]. In fact, the difficulty of finding effective textual prompts has led to a myriad of websites and forums dedicated to collecting and sharing useful prompts (e.g. PromptHero, Arthub.ai, Reddit/StableDiffusion). There are also online marketplaces for purchasing and selling useful such commands (e.g. PromptBase). The onus to generate aesthetic images that are faithful to a user's desires should lie with the model and not with the user.

Today, there are efforts to address these challenges. For example, it is possible to manipulate attention maps based on linguistic structure to improve attribute-object binding [14, 42]; or train reward models using human feedback to better align generations with user intent [13, 27]. Unfortunately, these methods either operate on a specific model architecture $[14,42]$ or require expensive labeled human data [13, 27]. Worse, most of these methods sacrifice aesthetic appeal when optimizing for faithfulness, which we confirm in our experiments.

We introduce DreamSync, a model-agnostic framework that improves $\mathbf{T} 2 \mathrm{I}$ generation faithfulness while maintaining aesthetic appeal. Our approach extends work on fine-tuning T2I models for alignment, but does not require any human feedback. The key insight behind DreamSync is in leveraging the advances in vision-language models (VLMs), which can identify fine-grained discrepencies between the generated image and the user's input text $[7,20]$. Intuitively at a high level, our method can be thought of as a scalable version of reinforcement learning with human feedback (RLHF); just as LLaMA2 [48] was iteratively refined using human feedback, DreamSync improves T2I models using feedback from VLMs, except without the need for reinforcement learning.

Given a set of textual prompts, T2I models first generates multiple candidate images per prompt. DreamSync automatically evaluates these generated images using two VLMs. The first one measures the generation's faithfulness to the text $[7,20]$, while the second one measures aesthetic quality [23]. The best generations are collected and used to finetune the T2I model using parameter-efficient LoRA finetuning [19]. With the new finetuned T2I model, we re-
peat the entire process for multiple iterations: generate images, curate a new finetuning set, and finetune again.

We conduct extensive experiments with latest benchmarks and human evaluation. We experiment DreamSync with two T2I models, SDXL [37] and SD v1.4 [39]. Results on both models show that DreamSync enhance the alignment of images to user inputs and retains their aesthetic quality. Specifically, quantitative results on TIFA [21] and DSG [7] demonstrate that DreamSync is more effective than all baseline alignment methods on SD v1.4, and can yield even bigger improvements on SDXL. Human evaluation on SDXL shows that DreamSync give consistent improvement on all categories of alignment in DSG. While our study primarily focuses on boosting faithfulness and aesthetic quality, DreamSync has broader applications: it can be used to improve other characteristics of an image as long as there is an underlying model that can measure that characteristic.

## 2. Related Work

T2I Evaluation with VLMs. Several prior works have proposed to use VQA models to evaluate text-to-image generation. The TIFA benchmark, which pioneered this approach for evaluation, consists of $4 \mathrm{~K}$ prompts and $25 \mathrm{~K}$ questions across 12 categories (e.g., object, count, material), enabling T2I model evaluation by using VQA models to answer questions about the generated images [20]. TIFA prompts come from various resources, including DrawBench used in Imagen [46], PartiPrompt used in Parti [54], PaintSkill [6] used in Dall-Eval, etc. DSG [7] further improves TIFA's realiability by examining their evaluation questions carefully. Another related benchmark is SeeTrue, which also uses VQA models to measure alignment [52]. Before the VQA evaluation era, several other evaluation benchmarks were proposed focusing primarily on compositional text prompts for attribute binding (e.g., color, texture, shape) and object relationships (e.g., spatial). Examples include T2I-CompBench [21], C-Flowers [35], CC-500 and ABC-6K benchmarks [15]. Aside from automated benchmarks, human evaluation for text-to-image generation is widely used in the community, although such annotations are notoriously costly to collect. In response, Xu et al. [51] propose ImageReward, the first general purpose textto-image human preference reward model to encode human preferences automatically. In our work, we use a collection of three evaluation methods to evaluate DreamSync: VQA evaluation for generated images on both TIFA and DSG benchmarks, human evaluation, and ImageReward for automatic human preference prediction.

Improving General T2I Alignment. We roughly categorize the alignment methods for improving T2I alignment into two classes depending on if they involve training. For training-involved methods, several works use Rein- forcement Learning from Human Feedback (RLHF) based on human rankings to maximize a reward and improve faithful generation [13, 22, 27]. In a similar vein, Picka-Pic is a dataset of prompts and preferences that is used to train a CLIP-based scoring function [24]. StyleDrop trains adapters to synthesize of images that follow a specific style [47], and T2I-Adapter trains adapters to improve the control for the color and structure of the generation results [33]. DreamBooth and HyperDreamBooth improve personalized generation [44, 45], and they have inspired more efficient methods such as SVDiff [17]. Being orthogonal to training-involved methods, there is a body of work on training-free methods that make inference time adjustments to the model to improve alignment, such such as SynGen and StructuralDiffusion. [12, 15, 18, 42]. DreamSync leverages training but does not involve reinforcement learning. We compare DreamSync with two RL-based methods and two learning-free methods in our experiments. We find that DreamSync outperform all the baselines in terms of textimage alignment on both DSG and TIFA.

Iterative Bootstrapping. Iterative Bootstrapping, also known as model self-training, is a semi-supervised learning approach that utilizes a teacher model to assign labels to unlabelled data, which is then used to train a student model $[16,26,32,53]$. In our work, we adopt a self-training scheme where the teacher model are the VLMs and the student model is the T2I model we aim to improve. During training, the VLMs (teacher) are used to annotate and select aligned examples for the next batch finetuning (student).

## 3. DreamSync

Our method improves alignment and aesthetics in four steps (see Figure 2): Sample, Evaluate, Filter, and Finetune. The high level idea is that T2I models are capable of generating interesting and varied samples. These examples are further judged by VLMs to pass qualification as faithful and aesthetic candidates for further finetuning T2I models. We next dive into each component more formally.

Sample. Given a text prompt $T$, the text-to-image generation model $G$ generates an image $I=G(T)$. Generation models are randomized, and running $G$ multiple times on the same prompt $T$ can produce different images, which we index as $\left\{I^{(k)}\right\}_{k=1}^{K}$. To improve the model's faithfulness to text guidance, our method collects faithful examples generated by $G$. We use $G$ to generate $K$ samples of the same prompt $T$, so that with some probability $\delta>0$, a generated image $I$ is faithful. Note that we need $K=\Omega(1 / \delta)$ samples for each prompt $T$, and DreamSync is not expected to improve totally unaligned models (with $\delta \rightarrow 0$ ). Prior work [22] estimates that 5-10 samples can yield a good image, and hence, $\delta$ can be thought of as roughly 0.1 to 0.2 .

Evaluate. For each text prompt $T$, we derive a set of

Stable Diffusion XL
![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-04.jpg?height=1994&width=382&top_left_y=340&top_left_x=206)

![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-04.jpg?height=469&width=1245&top_left_y=245&top_left_x=619)

A cube made of porcupine
![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-04.jpg?height=390&width=1234&top_left_y=735&top_left_x=617)

International Space Station flying in front of the moon
![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-04.jpg?height=372&width=1226&top_left_y=1148&top_left_x=623)

A mountain stream with salmon leaping out of it

![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-04.jpg?height=374&width=805&top_left_y=1559&top_left_x=627)

Two leafs and two wallets
![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-04.jpg?height=390&width=1230&top_left_y=1954&top_left_x=621)

The eye of the planet Jupiter

![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-04.jpg?height=371&width=382&top_left_y=1560&top_left_x=1473)

Figure 3. Qualitative examples of DreamSync improving image-text alignment after each iteration. LoRA fine-tuning on generated and filtered prompt-image pairs can steer the model to gradually capture more components of the text inputs.
$\mathcal{N}_{T}$ question-answer pairs $\{\mathcal{Q}(T), \mathcal{A}(T)\}$ that can be used to test whether a generated image $I$ is faithful to $T$. We use an LLM to generate these pairs, only using the prompt $T$ as input (with no images). Typically $\mathcal{N}_{T} \approx 10$. We use VQA models to evaluate the faithfulness of the generation model, $F_{j}(T, I)=\mathbb{1}\left\{\operatorname{VQA}\left(I, \mathcal{Q}_{j}(T)\right)=\mathcal{A}_{j}(T)\right\}$, for $j \in\left\{1, \ldots, \mathcal{N}_{T}\right\}$. We measure the faithfulness of a captionimage pair $(T, I)$ given all questions and answers, using two metrics. Intuitively, we can average the number of correct answers, or we can be more strict, and only count an image as a success if all the answers are correct. Formally, the Mean score is the expected success rate

$$
\mathcal{S}_{\mathrm{M}}(T, I)=\frac{1}{\mathcal{N}_{T}} \sum_{j=1}^{\mathcal{N}_{T}} F_{j}(T, I)
$$

and the Absolute score is the absolute success rate

$$
\mathcal{S}_{\mathrm{A}}(T, I)=\prod_{j=1}^{\mathcal{N}_{T}} F_{j}(T, I)
$$

Filter. We combine text faithfulness and visual appeal (given by $\mathcal{V}(\cdot)$ ) as rewards for filtering. For a text prompt $T$ and its corresponding synthetic image set $\left\{I_{k}\right\}_{k=1}^{K}$, we select samples that pass both VQA and aesthetic filters:

$$
\begin{aligned}
C(T)=\left\{\left(T, I_{k}\right): \mathcal{S}_{\mathrm{M}}\left(T, I_{k}\right)\right. & \geq \theta_{\text {Faithful }} \\
\mathcal{V}\left(I_{k}\right) & \left.\geq \theta_{\text {Aesthetic }}\right\}
\end{aligned}
$$

To avoid an imbalanced distribution where easy prompts have more samples, which could cause adversely affected image quality, we select one representative image (denoted as $\hat{I}_{T}$ ) having the highest visual appeal for each $T$ :

$$
\left(T, \hat{I}_{T}\right)=\underset{\mathcal{V}\left(I_{k}\right)}{\operatorname{argmax}} C(T)
$$

We apply this procedure to all text prompts in our finetuning prompt set $\left\{T_{i}\right\}_{i=1}^{N}$ with $T_{i} \sim \mathcal{D}$, where $\mathcal{D}$ is a prompt distribution. After filtering, we collect a subset of examples, $D(G):=\bigcup_{i \in\left\{j \mid C\left(T_{j}\right) \neq \varnothing\right\}}\left\{\left(T_{i}, \hat{I}_{T_{i}}\right)\right\}$, that meet our aesthetic and faithfulness criteria. Note that it is possible for $C\left(T_{i}\right)$ to be empty, and we empirically show what fraction of the training data is selected in Figure 5. We ablate other aspects of the selection procedure in $\S ~ 5.3$.

Finetune. After obtaining a new subset of faithful and aesthetic text-image pairs, we fine-tune our generative model $G$ on this set. We denote the generative model after $s$ iterations of DreamSync as $G_{s}$, such that $G_{0}$ denotes the baseline model. To obtain $G_{s+1}$ we fine-tune on data generated by $G_{s}$ after applying our filtering procedure as outlined above. We follow the same loss objective and fine-tuning dynamics as LoRA [19]. Let $\Theta(\cdot)$ denote all parameters of a model, then the hypothesis class at iteration $s$ is:

$$
\mathcal{G}_{s}=\left\{G \mid \operatorname{rank}\left(\Theta(G)-\Theta\left(G_{s}\right)\right) \leq R\right\}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-05.jpg?height=328&width=346&top_left_y=259&top_left_x=1128)

A cityscape with skyscrapers and flowers growing on the sides of the buildings

![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-05.jpg?height=339&width=344&top_left_y=665&top_left_x=1126)

A colorful anime illustration of a woman wearing a silver necklace, standing in a field of flowers, with a rainbow in the background

![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-05.jpg?height=328&width=344&top_left_y=259&top_left_x=1498)

A dark gray cat wearing a multi colored scarf around its neck, sitting on a wall

![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-05.jpg?height=337&width=337&top_left_y=669&top_left_x=1493)

An intriguing photo of an old man sitting on a bench in the park, lit by the setting sun
Figure 4. PaLM-2 generated training prompts and their corresponding images generated via DreamSync. Prompt acquisition requires no human effort. It enables us to train on more complex and diversified prompt-image pairs than found in typical datasets.

where $R$ denotes the rank of weight updates and in practice we choose $R=128$ to balance efficiency and image quality. Overall, the iterative training procedure is as follows:

$$
\begin{equation*}
G_{s+1}=\underset{G \in \mathcal{G}_{s}}{\operatorname{argmin}} \frac{1}{\left|D\left(G_{s}\right)\right|} \sum_{\left(T_{j}, I_{j}\right) \in D\left(G_{s}\right)} \ell\left(G\left(T_{j}\right), I_{j}\right) \tag{1}
\end{equation*}
$$

The self-training process Eq. (1) can in principle be executed indefinitely. In practice, it repeats for three iterations at which point we observe diminishing returns.

## 4. Datasets and Evaluation

In this section, we will introduce our training data in $\S 4.1$ and evaluation benchmark in $\S ~ 4.2$.

### 4.1. Training Data Acquisition

To obtain prompts, and corresponding question-answer pairs without human-in-the-loop, we utilize the in-context learning capability of Large Language Models (LLM). We choose PaLM $2^{1}$ [1] as our LLM and proceed as follows:

1. Prompt Generation. We provide five hand-crafted seed prompts as examples and then ask PaLM 2 to generate similar textual prompts. We include additional instructions that specify the prompt length, a category (randomly drawn from twelve desired categories as in [20],[^1]

| Model |  | Alignment | Text Faithfulness |  |  | Visual Appeal |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | TIFA |  | DSG1K |  |
|  |  |  | Mean | Absolute |  |  |
| SD v1.4 [39] |  | No alignment | 76.6 | 33.6 | 72.0 | 44.6 |
|  | Training-Free | SynGen [42] <br> StructureDiffusion [15] | $76.8(+0.2)$ <br> $76.5(-0.1)$ | $34.1(+0.5)$ <br> $33.6(+0.0)$ | $71.2(-0.8)$ <br> $71.9(-0.1)$ | $42.4(-2.2)$ <br> $41.5(-3.1)$ |
|  | RL | DPOK [13] | $76.4(-0.2)$ | $33.8(+0.2)$ | $70.3(-1.7)$ | $46.5(+1.9)$ |
|  |  | DDPO [4] | $76.7(+0.1)$ | $34.4(+0.8)$ | $70.0(-2.0)$ | $43.5(-1.1)$ |
|  |  | DreamSync (ours) | $77.6(+1.0)$ | $35.3(+1.7)$ | $73.2(+1.2)$ | $44.9(+0.3)$ |
| SDXL [37] |  | No alignment | 83.5 | 45.5 | 83.4 | 60.9 |
|  |  | DreamSync (ours) | $\mathbf{8 5 . 2}(+1.7)$ | $49.2(+3.7)$ | $86.3(+2.9)$ | $64.3(+3.4)$ |

Table 1. Benchmark on Text Faithfulness and Visual Appeal. All models are sampled with the same set of four seeds, i.e. $K=4$. Best scores under each backbone T2I model are highlighted in bold; gain and loss compared to base models are highlighted accordingly. DreamSync significantly improve SD-XL and SD v1.4 in alignment and visual appeal across all benchmark. Additionally, DreamSync does not sacrifice image quality when improving faithfulness.

e.g., spatial, counting, food, animal/human, activity), no repetition, etc. ${ }^{2}$ We change the seed prompts and repeat the prompt generation three times.

2. QA Generation. Given prompts, we then use PaLM 2 again to generate question and answer pairs that we will use as input for VQA models as in TIFA [20].
3. Filtering. We finally use PaLM 2 once more to filter out unanswerable QA pairs. Here our instruction aims to identify three scenarios: the question has multiple answers (e.g., "black and white panda" where the object has multiple colors, each color could be the answer), the answer is ambiguous (e.g., "a lot of people") or the answer is not valid to the question.

We showcase the diversity of PaLM 2 generated prompts in Figure 4 using qualitative examples and quantitive statistics of our generated prompts in Appendix A.2.

### 4.2. Evaluation Benchmarks

Using the previously generated prompts, we evaluate whether DreamSync can improve the T2I model performance on benchmarks that include general prompts. We consider the follow benchmarks.

TIFA. To evaluate the faithfulness of the generated images to the textual input, TIFA [20] uses VQA models to check whether, given a generated image, questions about its content are answered correctly. There are $4 \mathrm{k}$ diverse prompts and $25 \mathrm{k}$ questions spread across 12 categories in the TIFA benchmark. Although there is no overlap between our training data and TIFA, we use the TIFA attributes to constrain our LLM-based prompt generation. Therefore, we use TIFA[^2]

to test DreamSync on in-distribution prompts. We follow TIFA and use BLIP-2 as the VQA model for evaluation.

Davidsonian Scene Graph (DSG). DSG [7] exhibits the same VQA-as-evaluator insight as TIFA's and further improves its reliability. Specifically, DSG ensures that all questions are atomic, distinct, unambiguous, and valid. To comprehensively evaluate T2I images, DSG provides 1,060 prompts covering many concepts and writing styles from different datasets that are completely independent from DreamSync's training data acquisition stage. Not only is DSG a strong T2I benchmark, it also enables further analysis of DreamSync with out-of-distribution prompts. Furthermore, DSG uses PaLI as the VQA model for evaluation, which is different from the VQA model that we use in training (i.e., BLIP-2) and lifts the concern of VQA model bias in evaluation. We use DSG QA both automatically (with $\mathrm{PaLI}$ ) and with human raters (details in Appendix C).

## 5. Experiments

We explain our experimental setup in $\S 5.1$, and showcase the efficacy of training with DreamSync and compare against other methods in $\S 5.2$. $\S 5.3$ analyzes our choice of rewards; $\S 5.4$ reports results for a human study.

### 5.1. Experimental set-up

Base Model. We evaluate DreamSync on Stable Diffusion v1.4 [39], which is also used in related work. Additionally, we consider SDXL [37], which is the current state-of-theart open-sourced T2I model. For each prompt, we generate eight images per prompt, i.e., $K=8$.

Fine-grained VLM Feedback. We use feedback from two VLM models to decide what text-image pairs to keep

![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-07.jpg?height=368&width=838&top_left_y=236&top_left_x=161)

Figure 5. DreamSync improves faithfulness and aesthetics iteratively. More examples pass the filters with additional iterations.

for finetuning. We use BLIP-2 [28] as the VQA model to measure the faithfulness of generated images to textual input and and VILA [23] to measure the aesthetics measurement score. Empirically, we keep the text-image pairs whose VQA scores are greater than $\theta_{\text {Faithful }}=0.9$ and aesthetics score greater than $\theta_{\text {Aesthatics }}=0.6$. If there are multiple generated images passing the threshold, we keep the one with the highest VILA score. Starting from 28,250 prompts, we find that more than $25 \%$ prompts are kept for $D\left(G_{0}\right)$ (for both T2I models), which we will use for finetuning. We later show that this percentage increases further as we perform additional DreamSync iterations.

Baselines. We compare DreamSync with two types of methods that improve the faithfulness of T2I models: two training-free methods (StructureDiffusion [15] and SynGen [42]) and two RL-based methods (DPOK [13] and DDPO [4]). As the baselines use SD v1.4 as their backbone, we also use it with DreamSync for a fair comparison.

### 5.2. Benchmark Results

In Table 1 we compare DreamSync to various state-of-theart approaches with four random seeds. In Appendices D and $\mathrm{E}$ we show more qualitative comparisons.

DreamSync Improves the Alignment and Aesthetics of both SDXL and SD v1.4. For SDXL [37], we show how three iterations of DreamSync improves the generation faithfulness by 1.7 point of mean score and 3.7 point of absolute score on TIFA. The visual aesthetic scores after performing DreamSync improved by 3.4 points. Due to the model-agnostic nature, it is straightforward to apply DreamSync to different T2I models. We also apply DreamSync to SD V1.4 [39]. DreamSync improves faithfulness by 1.0 points of mean score and 1.7 points of absolute score on TIFA, together with a 0.3 points of VILA score improvement for aesthetics. Most prominently on DSG1K, DreamSync improve text faithfulness of SDXL by 2.9 points. We report fine-grained results for DSG in Appendix C.

DreamSync yields the best performance in terms of textual faithfulness on TIFA and DSG. This is true without sacrificing the visual appearance as shown in Table 1. In Figure 5 we report TIFA and aesthetics scores for each it-

| Rewards |  | Text <br> Faithfulness | Visual <br> Appeal |
| :---: | :---: | :---: | :---: |
| VQA | VILA |  | 60.9 |
| - | - | 83.5 | 61.9 |
| $\checkmark$ |  | $\mathbf{8 4 . 8}$ | 61.7 |
|  | $\checkmark$ | 83.8 | $\mathbf{6 2 . 8}$ |

Table 2. Ablation of different VLM rewards. Models are evaluated after one DreamSync iteration.

| T2I <br> Model | Alignment <br> Method | Evaluation Dataset |  |
| :--- | :--- | :--- | :---: |
|  |  | TIFA | DSG1K |
| SD v1.4 | No alignment | 0.056 | -0.220 |
|  | SynGen | 0.149 | -0.237 |
|  | StructureDiffusion | 0.075 | -0.135 |
|  | DPOK | 0.067 | -0.258 |
|  | DDPO | 0.152 | -0.076 |
|  | DreamSync (ours) | $\mathbf{0 . 1 6 8}$ | $\mathbf{- 0 . 0 5 4}$ |
| SD XL | No alignment | 0.878 | 0.702 |
|  | DreamSync (ours) | $\mathbf{1 . 0 2 0}$ | $\mathbf{0 . 8 3 7}$ |

Table 3. Scores given by the human preference model ImageReward [51]; model scores are logits and can be negative. Models trained with DreamSync outperform other baselines (higher is better), without using any human annotation.

eration, where we observe how DreamSync gradually improves the alignment and aesthetics of the generated images. We highlight several qualitative examples in Figure 3.

### 5.3. Analysis \& Ablations

Impact of VQA model on evaluation. We analyze whether using BLIP-2 as a VQA model for finetuning and for evaluation in TIFA might be the reason for the improvement by DreamSync that we have observed. To test this we use PaLI [5] to replace the BLIP-2 as the VQA in TIFA. Using SDXL as the backbone, DreamSync improves the mean score from 90.09 to 92.02 on TIFA compared to the vanilla SDXL model. This results confirms that DreamSync is in fact able to improve the textual faithfulness of T2I models.

Ablating the Reward Models In Table 2, we present the results for an ablation study where we remove one of the VLMs during filtering and evaluate SDXL after applying one iteration of DreamSync. It can be seen how training with a single pillar mainly leads to an improvement in the corresponding metric, while the combination of the two VLM models leads to strong performance for both text faithfulness and visual easthatics, justifying our approach. One interesting finding is that training with both rewards, rather than VILA only, gives the highest visual appeal score. Our possible explanation is that images that align with user inputs may have higher visual appeal.

ImageReward. We next test whether DreamSync yields an improvement on human preference reward models, even

![](https://cdn.mathpix.com/cropped/2024_06_04_a36b91c41ae879f27304g-08.jpg?height=464&width=812&top_left_y=256&top_left_x=171)

Figure 6. Human study with three raters on 1060 DSG prompts.

though DreamSync is not trained to optimize them. We use ImageReward [51] as an off-the-shelf human preference model for generated images. Table 3 shows that DreamSync plus either SD v1.4 or SDXL increases ImageReward scores on images based on both TIFA and DSG1K. Tuning with VLM-based feedback helps align the generated images with human preferences, at least according to ImageReward.

### 5.4. Human Evaluation

To corroborate the VQA-based results, we first conduct a preliminary human study to evaluate the faithfulness of generated images. It shows simply asking one question "Which image better aligns with the prompt?', yields poor inter-annotator agreement. We speculate that asking a single question encompassing the whole prompt makes the alignment difficult to evaluate.

To address this issue, we conduct a larger follow-up study based on DSG [7], where we ask approximately 8 fine-grained questions for each of 1060 images to external raters. These questions are divided into categories (entity, attribute, relation, global). Here in Figure 6, we observe consistent and statistically significant improvements comparing DreamSync to SDXL. In each category, images from DreamSync contain more components of the prompts, while excluding extraneous features. Overall, DreamSync's images led to $3.4 \%$ more correct answers than SDXL images, from $70.9 \%$ to $74.3 \%$. Full details and findings for both studies are in Appendix C.

## 6. Discussion

A key design choice behind DreamSync is to maintain simplicity and automation throughout each step of the pipeline. Despite this feature, our experimental results show that DreamSync can improve both SD v1.4 and SDXL on TIFA, DSG, and visual appeal. In the case of SD v1.4, this improvement holds true compared four different baseline models (two training-free and two RL-based). For SDXL, even though the base model achieves SoTA results among open-source models, DreamSync can still substantially improve both alignment and aesthetics.

The effectiveness of DreamSync's self-training methodology opens the door for a new paradigm of parameterefficient finetuning. Indeed, the DreamSync pipeline is easily generalizable. For the training prompts, we can construct a set with complex and non-conventional examples compared to standard web-scraped data. On the filtering and fine-tuning side, our framework shows that VLMs can provide effective feedback for T2I models. Together, these steps do not require human annotations, yet they can tailor a generative model toward desirable criteria.

### 6.1. Limitations

Like prior methods, the performance of DreamSync is limited by the pre-trained model it starts with. As exemplified in "the eye of the planet Jupiter" in Figure 3, SDXL generates a human's eye rather than Jupiter's. DreamSync adds more features of the Jupiter in each iteration. Nevertheless, it did not manage to produce an image that is perfectly faithful to the prompt. This is also exemplified by the quantitative results in $\S 5.2$. Despite outperforming the baselines using SD v1.4 on TIFA and DSG, SD v1.4 + DreamSync still falls behind SDXL. Similarly, our human studies on DSG in $\S 5.4$ indicate that DreamSync improves SDXL from $70.9 \%$ accuracy to $74.3 \%$. Nonetheless, there is still a $25.7 \%$ headroom to improve. We identify several common failure modes (e.g., attribute-binding) and conduct a detailed analysis in Appendix B. Future works may investigate if these challenges can be addressed by further scaling up DreamSync, or mixing it with large-scale pre-training.

## 7. Conclusion

We introduce DreamSync, a versatile framework to improve text-to-image (T2I) synthesis with feedback from image understanding models. Our dual VLM feedback mechanism helps in both the alignment of images with textual input and the aesthetic quality of the generated images. Through evaluations on two challenging T2I benchmarks (with over five thousand prompts), we demonstrate that DreamSync can improve both SD v1.4 and SDXL for both alignment and visual appeal. The benchmarks also show that DreamSync performs well in both in-distribution and out-of-distributions settings. Furthermore, human ratings and a human preference prediction model largely agree with DreamSync's improvement on benchmark datasets.

For future work, one direction is to ground the feedback mechanism to give fine-grained annotations (e.g., bounding boxes to point out where in the image the misalignment lies). Another direction is to tailor the prompts used at each iteration of DreamSync to target different improvements: backpropagating VLM feedbacks to the prompt acquisition pipelines for continual learning.

## List of Contributions

Jiao Sun: Jiao leads the project. She implemented DreamSync internally at Google, showcasing its success on various reward models. She also wrote the first paper draft.

Deqing Fu: Deqing initiated the idea of LLM-based prompt generation. He implemented DreamSync with open-source text-to-image models, conducting all the experiments. He also contributed to paper writing and drew all the figures.

Yushi Hu: Yushi conceived the idea of DreamSync and designed the experiments. He also implemented the VQA feedback, the baselines, and contributed to paper writing.

Jiao, Deqing, and Yushi completed the full development cycle of DreamSync. They contributed equally on designing technical directions, implementing, polishing and improving DreamSync from scratch.

S. Wang ran both automatic and human evaluations on DSG1K and contributed to corresponding paper sections.

R. Rassin contributed to the implementation of the baseline methods SynGen and DPOK.

J. Sun, C. Rashtchian, S. van Steenkiste, C. Herrmann, D-C. Juan, and D. Alon conceived of the initial project directions, e.g., T2I models struggle with compositionality and image quality.

R. Krishna, S. van Steenkiste, C. Herrmann, and D. Alon provided constructive feedback and suggested experiments.

R. Krishna and S. van Steenkiste helped frame the story via writing and polishing several sections of the paper.

C. Rashtchian served as senior project lead and manager, scoping technical directions and facilitating collaborations.

## Acknowledgments

We thank Yi-Ting Chen, Otilia Stretcu, Yonatan Bitton, Fei Sha, Kihyuk Sohn, Chun-Sung Ferng, and Jason Baldridge for helpful project discussions and technical support. JS and DF would like to thank USC NLP group and YH would like to thank UW NLP group, for providing both additional GPU computational resources and fruitful discussions.

## References

[1] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023. 5, 17

[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, JoyceLee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions, 2023. 15

[3] Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! a vision-andlanguage benchmark of synthetic and compositional images. In ICCV, 2023. 17

[4] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning, 2023. 6,7

[5] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel M. Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V. Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and

Radu Soricut. Pali: A jointly-scaled multilingual languageimage model. ArXiv, abs/2209.06794, 2022. 7, 17

[6] Jaemin Cho, Abhaysinh Zala, and Mohit Bansal. Dalleval: Probing the reasoning skills and social biases of textto-image generative transformers. ArXiv, abs/2202.04053, 2022. 3

[7] Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi PontTuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation, 2023. 2, 3, 6, 8, 17

[8] Donald Davidson. Theories of meaning and learnable languages. In In Yehoshua Bar-Hillel (ed.), Proceedings of the 1964 International Congress for Logic, Methodology, and Philosophy of Science. Amsterdam: North-Holland. pp. 383394, 1965. 17

[9] Donald Davidson. The logical form of action sentences. In $n$ N. Rescher (ed.) The Logic of Decision and Action, Pittsburgh: University of Pittsburgh, 1967.

[10] Donald Davidson. Truth and meaning. In Inquiries into Truth and Interpretation; Soames, chapter 12 of PATC, 1967. 17

[11] Ginger Delmas, Philippe Weinzaepfel, Thomas Lucas, Francesc Moreno-Noguer, and Grégory Rogez. PoseScript: 3D Human Poses from Natural Language. In ECCV, 2022. 17

[12] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. arXiv preprint arXiv:2306.00986, 2023. 3

[13] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381, 2023. 2, 3, 6,7

[14] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, P. Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. ArXiv, abs/2212.05032, 2022. 2

[15] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis, 2023. 3, 6,7

[16] Deqing Fu, Ameya Godbole, and Robin Jia. Scene: Selflabeled counterfactuals for extrapolating to negative examples. ArXiv, abs/2305.07984, 2023. 3

[17] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. arXiv preprint arXiv:2303.11305, 2023. 3

[18] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. arXiv preprint arXiv:2210.00939, 2022. 3

[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 2,5

[20] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897, 2023. $2,3,5,6,17$

[21] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. arXiv preprint arXiv:2307.06350, 2023. 3

[22] Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, and Zeynep Akata. If at first you don't succeed, try, try again: Faithful diffusion-based text-to-image generation by selection. arXiv preprint arXiv:2305.13308, 2023. 3

[23] Junjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Milanfar, and Feng Yang. Vila: Learning image aesthetics from user comments with vision-language pretraining, 2023. 2, 7

[24] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint arXiv:2305.01569, 2023. 3

[25] Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. A hierarchical approach for generating descriptive image paragraphs. In CVPR, 2017. 17

[26] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Proceedings of the 37th International Conference on Machine Learning, pages 5468-5479. PMLR, 2020. 3

[27] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning textto-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. 2, 3

[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ArXiv, abs/2301.12597, 2023. 7

[29] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum. Compositional visual generation with composable diffusion models. ArXiv, abs/2206.01714, 2022. 2

[30] Rosanne Liu, Daniel H Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, R. J. Mical, Mohammad Norouzi, and Noah Constant. Characteraware models improve visual text rendering. ArXiv, abs/2212.10562, 2022. 2, 17

[31] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li FeiFei. Visual relationship detection with language priors. In ECCV, 2016. 17

[32] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152-159, New York City, USA, 2006. Association for Computational Linguistics. 3

[33] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. 3

[34] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. In ICCV, 2023. 17

[35] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. Benchmark for compositional text-toimage synthesis. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. 3

[36] Vitali Petsiuk, Alexander E Siemenn, Saisamrit Surbehera, Zad Chin, Keith Tyser, Gregory Hunter, Arvind Raghavan, Yann Hicke, Bryan A Plummer, Ori Kerret, et al. Human evaluation of text-to-image models on a multi-task benchmark. arXiv preprint arXiv:2211.12112, 2022. 2

[37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 3, 6, 7

[38] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In ECCV, 2020. 17

[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 3, 6, 7

[40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021. 2

[41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv, abs/2204.06125, 2022. 2

[42] Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik. Linguistic binding in diffusion models: Enhancing attribute correspondence through attention map alignment, 2023. 2, 3, 6, 7

[43] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674-10685, 2021. 2

[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023. 3

[45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06949, 2023. 3

[46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho,
David J. Fleet, and Mohammad Norouzi. Photorealistic textto-image diffusion models with deep language understanding. ArXiv, abs/2205.11487, 2022. 2, 3

[47] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint arXiv:2306.00983, 2023. 3

[48] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. 2

[49] Iulia Turc and Gaurav Nemade. Midjourney user prompts \& generated images (250k), 2022. 17

[50] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. Diffusiondb: A large-scale prompt gallery dataset for text-toimage generative models. In ACL, 2023. 17

[51] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. arXiv preprint arXiv:2304.05977, 2023. 3, 7, 8

[52] Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, and Idan Szpektor. What you see is what you read? improving text-image alignment evaluation. arXiv preprint arXiv:2305.10400, 2023. 3

[53] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 189196, Cambridge, Massachusetts, USA, 1995. Association for Computational Linguistics. 3

[54] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for contentrich text-to-image generation. ArXiv, abs/2206.10789, 2022. 2,3
