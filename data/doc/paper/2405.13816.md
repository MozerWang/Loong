# Large Language Models are Good Spontaneous Multilingual Learners: Is the Multilingual Annotated Data Necessary? 

Shimao Zhang* ${ }^{\star}$, Changjiang Gao ${ }^{\star}$, Wenhao Zhu* ${ }^{*}$, Jiajun Chen* ${ }^{*}$, Xin Huang ${ }^{\curvearrowright}$,<br>Xue Han ${ }^{\diamond}$, Junlan Feng ${ }^{\diamond}$, Chao Deng ${ }^{\diamond}$, Shujian Huang ${ }^{\star}$ *<br>${ }^{\text {* National Key Laboratory for Novel Software Technology, Nanjing University }}$<br>${ }^{\diamond}$ China Mobile Research Beijing, China<br>\{smzhang,gaocj,zhuwh\}@smail.nju.edu.cn, \{huangsj,chenjj\}@nju.edu.cn<br>\{huangxinyjy, hanxueai, fengjunlan, dengchao\}@chinamobile.com


#### Abstract

Recently, Large Language Models (LLMs) have shown impressive language capabilities. However, most of the existing LLMs are all English-centric, which have very unstable and unbalanced performance across different languages. Multilingual alignment is an effective method to enhance the LLMs' multilingual capabilities. In this work, we explore the multilingual alignment paradigm which utilizes translation data and comprehensively investigate the spontaneous multilingual improvement of LLMs. We find that LLMs only instruction-tuned on question translation data without annotated answers are able to get significant multilingual performance enhancement even across a wide range of languages unseen during instruction-tuning. Additionally, we utilize different settings and mechanistic interpretability methods to comprehensively analyze the LLM's performance in the multilingual scenario. Our code and data is available at: https://github.com/Shimao-Zhang/ LLM-Multilingual-Learner.


## 1 Introduction

Large Language Models (LLMs) have recently shown impressive language capabilities across numerous downstream language tasks (Zhao et al., 2023). However, most existing LLMs are trained on extensive high-resource languages text such as English, Chinese, German, French, and so on (Touvron et al., 2023; Brown et al., 2020; Jiang et al., 2023), which lead to a significant performance gap between high-resource languages and low-resource languages (Huang et al., 2023; Zhang et al., 2023b; Gao et al., 2024). For the same task and question contents, using different languages for inputs may have a significant impact on the model's performance.

Some studies have conducted comprehensive exploration about how to enhance the capabilities[^0]

of LLMs for low-resource language. The classical approach typically follows the translate-based paradigm (Liu et al., 2024), which translating nonEnglish inputs into English or translating English data into non-English for instruction tuning. However, it is difficult to accurately translate all texts into the target low-resource language (Zhu et al., 2024), not to mention the increasing translation cost as the data scale expands. In order to enhance low-resource languages performance with less cost, some cross-lingual alignment and transfer strategies have been proposed (Eronen et al., 2023; Zhu et al., 2024; Zhao et al., 2024a). But all these methods rely on the data in the target language.

Meanwhile, some studies have explored Englishcentric LLMs, revealing that English also participate in the intermediate latent reasoning process of these models even when LLMs are prompted in non-English (Wendler et al., 2024; Zhao et al., 2024b). These findings suggest that for LLMs, multiple languages are not completely isolate, and LLMs have the capability to leverage the connections between various languages to address problems in multilingual scenarios. It further demonstrates the feasibility of cross-lingual capability transfer. More surprisingly, Kew et al. (2023) discover that when instruction-tuning LLMs with multilingual data, it is not necessary to instructiontune the model on data from all target languages to achieve a multilingual ability similar to models instruction-tuned on all languages.

Intuitively, LLMs have abilities and motivations to acclimatize themselves to multilingual environment (Shi et al., 2022). But what should we do to facilitate LLMs to do this better? Most existing methods rely on instruction-tuning on the corresponding datasets (Kew et al., 2023; Liu et al., 2024). However, given the strong capabilities of models in high-resource languages, which indicates that LLMs actually possesses the ability and knowledge to solve problems, such extensive additional anno-
tated data might not be necessary to help LLMs improve their multilingual abilities.

In this work, we further investigate the multilingual learning capabilities of LLMs, where we only train the LLMs on the parallel data without annotated answers (only queries) in a few languages. Following this pattern, we conduct the experiments on models in different types (English-centric or not) and parameter sizes, and test their multilingual capability across a wide range of languages on different benchmarks. We find that multilingual question alignment following Zhu et al. (2024) can effectively enhance the multilingual capabilities of LLMs. Our results indicate that only tuning on questions (without annotated answers) in a small number of languages can bring significant multilingual improvements even across many languages unseen during instruction-tuning process, which implies good language generalization. Furthermore, we also use logit lens (Nostalgebraist, 2020) and dimensionality reduction (Pearson, 1901; Hotelling, 1933) techniques to study the latent states of LLMs, providing more comprehensive perspectives and empirical results for understanding the multilingual learning of large language models.

## 2 Background

### 2.1 Unbalanced Multilingual Performance

With a much larger number of parameters pretrained on a massive corpus, LLMs have shown the impressive capabilities in a variety of language tasks (Zhao et al., 2023). These models are mainly pretrained on English data, which often accounts for $90 \%$ or even more of all training data, such as LLaMA2 (Touvron et al., 2023), GPT-3 (Brown et al., 2020), Mistral (Jiang et al., 2023), Falcon (Almazrouei et al., 2023), and so on. We present a partial language distribution of LLaMA-2's training data in Table 7 in Appendix A. Meanwhile, most of the LLMs also show unstable and unbalanced performance in multilingual scenarios, especially for some low-resource languages (Zhang et al., 2023a; Zhu et al., 2024). It's important to enable LLMs to adapt to a wider range of users and scenarios.

### 2.2 Cross-lingual Enhancement for Large Language Models

However, LLMs still exhibit significant shortcomings in multilingual scenarios. Many researchers propose multilingual LLMs that are specifically adjusted for multilingual tasks (Team, 2023; Le Scao et al., 2023; Wei et al., 2023). But for multilingual LLMs, researches indicate a decline in their performance in English because of the limited tokens and parameter size (Lin et al., 2022; Scao et al., 2022).

Based on the existing LLMs, researchers have made great efforts to enhancing the multilingual performance of LLMs, which include two categories: prompting close-source LLMs and instruction-tuning open-source LLMs. For the former, some studies utilize translation-based strategies which prompt ChatGPT to translate the nonEnglish input into English firstly before solving the problem (Huang et al., 2023; Qin et al., 2023). This type of approaches are constrained by the translation quality of the model itself and is cumbersome for users.

For the latter, LLMs shows significant improvement of multilingual and multitask abilities and good task generalization through multilingual multitask fine-tuning (Muennighoff et al., 2022). Chen et al. (2023) follow the translation-based approach and instruction-tune the model on a multilingual version of GSM8K, which is translated from English GSM8K (Cobbe et al., 2021). Liang et al. (2024) create a new intermediate language MUL (Machine-created Universal Language) as a translatable unified representation of shared concepts across different languages. "X-English" parallel translation data have also been widely used (Zhu et al., 2024). In our work, we mainly use this type of data, i.e. translation data between two different languages, to enhance multilingual alignment.

### 2.3 Mechanistic Interpretability

In addition to improving the performance of LLMs, it is also crucial to understand and explain the principles of neural networks and related methods explicitly. Current works mainly analyze LLMs' actions by observing the internal states during the inference process. Intermediate logits and neuron activation states are both important objects of observation.

Although the English-centric LLMs are mainly trained on English data, they also show good performance across some non-English languages (Shi et al., 2022). Logit lens (Nostalgebraist, 2020) is an early proposed technique that using the model head in the final layer to project the intermediate latent logits directly to the vocabulary space. It have been evidenced that LLaMA 2 (Touvron et al., 2023), a open-source English-centric LLMs, have explicit English output in its latent states even when
having non-English inputs (Wendler et al., 2024). There is also a hypothesis about how LLMs handle multilingualism that LLMs will solve task by English with the help of multilingual knowledge, and output in the target language finally (Zhao et al., 2024b). All these results indicate that there are connections between various languages for LLMs, and LLMs have the capability to spontaneously learn to utilize multiple languages to solve problems. Zhao et al. (2024b) calculate the overlapping ratio of the language-specific neurons of different languages in different layers. The results indicate that neurons belonging to different languages exhibit clear distribution differences. In our experiments, we utilize logit lens and dimensionality reduction techniques to help us better understand the mechanism behind our findings.

## 3 Methodology

We investigate the effect of question translation parallel data on LLMs' performance across a wide range of languages even unseen during the finetuning process.

We define the universal set of languages as $\mathbf{U}$ :

$$
\begin{equation*}
\mathbf{U}=\left\{l_{0}, l_{1}, l_{2}, \ldots, l_{n-1}\right\} \tag{1}
\end{equation*}
$$

where $l_{i}$ is the $i$-th language in $\mathbf{U}, n$ is the total number of languages. We let $l_{0}$ refer to English specially here.

We select a few of non-English languages $\mathcal{L}_{s}=$ $\left\{l_{i}, \ldots, l_{k}\right\} \subseteq \mathbf{U}$, and a target language $l_{t} \in \mathbf{U}$, $l_{t} \notin \mathcal{L}_{s}$. Then we will construct translation parallel data from every language $l \in \mathcal{L}_{s}$ to target language $l_{t}$. When construct the translation data, we only use the questions without annotated answers. Then we get a translation dataset $\mathcal{Q}_{\text {train }}$ including source question $\mathcal{Q}_{s}$ and the corresponding target question $\mathcal{Q}_{t}$, which means $\mathcal{Q}_{\text {train }}=\left\{\left(q_{s}, q_{t}\right) \mid\right.$ $q_{s} \in \mathcal{Q}_{s}$ and $\left.q_{t} \in \mathcal{Q}_{t}\right\}$. We instruct-tune the model on the translation task:

$$
\begin{equation*}
\underset{\theta}{\arg \min } \sum_{\left(q_{s}, q_{t}\right) \in \mathcal{Q}_{\text {train }}}-\log p_{\theta}\left(q_{t} \mid q_{s}\right) \tag{2}
\end{equation*}
$$

where $\theta$ is the model parameters, $\mathcal{Q}_{\text {train }}$ is the whole training translation dataset, $q_{s}$ is the question in the source language, $q_{t}$ is the question in the target language. Then we get the trained model:

$$
\begin{equation*}
\theta^{\prime}=\theta+\Delta \theta \tag{3}
\end{equation*}
$$

We use question translation data for training to eliminate the impact of annotated answers themselves. And we use in-context learning for test while the model haven't been trained on the corresponding task.

We test the trained model on all languages $l \in \mathbf{U}$. We construct the testing dataset $\mathcal{Q}_{\text {test }}=\left\{\mathcal{Q}_{l} \mid l \in\right.$ $\mathbf{U}\}$ for every language, where $\mathcal{Q}_{l}$ consists of all test questions in the language $l$.

$$
\begin{align*}
\text { Accuracy }_{l} & =\sum_{q \in \mathcal{Q}_{l}} \mathbf{I}_{\theta^{\prime}}(\hat{a}=a \mid q)  \tag{4}\\
\text { Accuracy } & =\frac{\sum_{l \in \mathbf{U}} \text { Accuracy }_{l}}{|\mathbf{U}|} \tag{5}
\end{align*}
$$

where $\mathbf{I}$ is a function that takes 1 when the proposition is true and 0 otherwise. $\mathcal{Q}_{l}$ denotes the test dataset of language $l$. $\mathbf{U}$ is the universal set of languages we use in our work. $\hat{a}$ is the answer that the model predicts base on $q$, and $a$ is the golden answer corresponding to $q$.

## 4 Experimental Setup

We conduct our experiments on both Englishcentric and non-English-centric models. And we utilize different representative tasks and different model parameter sizes to further strengthen our conclusions. In this section, we introduce our experimental settings in detailed.

Models We choose representative open-source LLMs for our experiments:

- Mistral: Mistral-7B-v0.1 (Jiang et al., 2023) is an advanced open-source English-centric large language model, which is one of the most popular open-source LLMs.
- Qwen: To enhance the generalization and reliability of our conclusions, we also choose models of different types and parameter sizes. Qwen 1.5 is a newly released and enhanced version of Qwen (Bai et al., 2023). Qwen is pretrained on a multilingual dataset with a significant portion of the data being in English and Chinese, which means it is not an English-centric model. We choose Qwen1.51.8B, Qwen1.5-4B, Qwen1.5-14B for our experiments.

Datasets Following Wendler et al. (2024), we select test tasks based on two fundamental principles:

1. Obvious Answers: Obvious answers reduce the entropy during inference process, minimizing the impact of irrelevant tokens on our analysis.

| Mistral-7B | en | $\mathbf{z h}$ | de | $\mathrm{fr}$ | es | it | nl | ja | $\mathbf{r u}$ | sv |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| base | 89.2 | 92.4 | 91.8 | 93.4 | 94.2 | 93.8 | 93.6 | 93.0 | 93.2 | 93.4 |
| $\mathrm{zh} \Rightarrow \mathrm{en}$ | 95.2 | 94.8 | 94.8 | 95.2 | 94.4 | 94.4 | 94.8 | 94.4 | 94.0 | 95.4 |
| $\mathrm{sw} \Rightarrow \mathrm{en}$ | 95.4 | 93.4 | 94.2 | 94.4 | 94.2 | 94.4 | 93.0 | 93.6 | 93.8 | 94.8 |
| $\mathrm{zh} / \mathrm{es} \Rightarrow \mathrm{en}$ | 95.2 | 95.0 | 95.0 | 95.0 | 94.8 | 92.8 | 94.6 | 95.0 | 94.4 | 94.8 |
| $\mathrm{zh} / \mathrm{de} \Rightarrow \mathrm{en}$ | 95.2 | 95.4 | 94.8 | 95.2 | 95.2 | 95.2 | 94.8 | 93.6 | 94.2 | 94.6 |
| $\mathrm{zh} / \mathrm{it} \Rightarrow$ en | 95.4 | 95.8 | 94.8 | 94.0 | 95.2 | 92.6 | 94.4 | 93.0 | 94.2 | 95.2 |
| sw/hi $\Rightarrow$ en | 95.4 | 94.6 | 94.4 | 93.4 | 93.4 | 93.6 | 93.6 | 94.0 | 93.8 | 94.4 |
| sw/th $\Rightarrow$ en | 95.4 | 95.0 | 93.8 | 93.4 | 93.4 | 92.8 | 93.6 | 92.6 | 93.2 | 94.0 |
| $\mathrm{zh} / \mathrm{es} / \mathrm{ru} \Rightarrow \mathrm{en}$ | 95.4 | 95.4 | 94.4 | 94.0 | 94.6 | 92.6 | 94.6 | 94.2 | 94.0 | 94.2 |
| $\mathrm{zh} / \mathrm{de} / \mathrm{it} \Rightarrow \mathrm{en}$ | 95.2 | 95.6 | 94.4 | 95.0 | 94.0 | 93.8 | 95.0 | 93.6 | 94.2 | 94.6 |
| Mistral-7B | $\overline{s l}$ | pl | bg | no | $\mathrm{ms}$ | is | hi | th | sW | bn |
| base | 87.6 | 93.2 | 91.6 | 92.4 | 91.8 | 63.2 | 81.6 | 83.0 | 58.0 | 71.0 |
| zh $\Rightarrow$ en | 94.0 | 94.0 | 94.6 | 92.2 | 89.0 | 84.0 | 88.8 | 88.4 | 75.8 | 81.0 |
| $\mathrm{sw} \Rightarrow \mathrm{en}$ | 89.8 | 92.6 | 93.6 | 93.4 | 90.0 | 72.0 | 64.4 | 51.4 | 81.2 | 54.0 |
| $\mathrm{zh} / \mathrm{es} \Rightarrow \mathrm{en}$ | 93.2 | 93.6 | 94.0 | 93.0 | 92.2 | 81.2 | 87.0 | 84.8 | 75.6 | 75.4 |
| $\mathrm{zh} / \mathrm{de} \Rightarrow \mathrm{en}$ | 93.4 | 94.0 | 94.6 | 93.6 | 92.2 | 86.6 | 84.8 | 88.4 | 71.8 | 68.6 |
| $\mathrm{zh} / \mathrm{it} \Rightarrow$ en | 92.6 | 93.8 | 94.2 | 93.6 | 92.6 | 84.2 | 77.6 | 77.2 | 71.6 | 60.0 |
| sw/hi $\Rightarrow$ en | 89.2 | 93.0 | 93.2 | 92.6 | 90.0 | 71.8 | 89.8 | 87.0 | 77.6 | 79.4 |
| sw/th $\Rightarrow$ en | 92.8 | 92.0 | 93.2 | 87.2 | 84.4 | 79.4 | 86.8 | 84.0 | 81.0 | 74.2 |
| $\mathrm{zh} / \mathrm{es} / \mathrm{ru} \Rightarrow \mathrm{en}$ | 93.6 | 94.2 | 93.4 | 93.4 | 91.4 | 83.8 | 85.0 | 86.0 | 77.0 | 76.0 |
| zh/de/it $\Rightarrow$ en | 91.2 | 93.6 | 94.2 | 93.4 | 91.8 | 83.2 | 77.2 | 82.4 | 69.0 | 71.4 |

Table 1: Accuracy of Mistral-7B base model and aligned models on the Amazon Reviews Polarity. We report at least two sets of results for each language quantity to strengthen our conclusions. The accuracy of randomly choosing is $50.0 \%$. " $\mathrm{X} / \mathrm{Y} / \mathrm{Z} \Rightarrow \mathrm{T}$ " means using a randomly mixed dataset including $10 \mathrm{k} \mathrm{X}$ to $\mathrm{T}, 10 \mathrm{k} \mathrm{Y}$ to $\mathrm{T}, 10 \mathrm{k} \mathrm{Z}$ to $\mathrm{T}$ translation data for instruction-tuning. We highlight the best results for every language.

2. Fixed Answers: Fixed answers (as opposed to open-ended responses) provide clearer observation targets, facilitating analysis through observing the latent outputs of the model. Deterministic outputs also make it easier for us to control the model's outputs.

Based on these, we conduct our experiments on two types of tasks:
- Emotion Classification: Emotion classification is an important and classic NLP task (Alswaidan and Menai, 2020), which always has three common outputs: "positive", "negative", and "neutral". We choose Amazon Reviews Polarity ${ }^{1}$ (Zhang et al., 2015), a famous dataset includes two classes "positive" and "negative", to construct the parallel data mentioned in $\S 2.2$ and the test data. We extract $10 \mathrm{~K}$ instances from train subset for parallel data and 500 instances from test subset for test data respectively.
- Natural Language Inference: Natural language inference (NLI) aims to judge the relationship between a given premise and a hypothesis sentence. There are always three[^1]

possible outputs: "entailment", "neutral", and "contradiction". We choose SNLI ${ }^{2}$ (Stanford Natural Language Inference) (Bowman et al., 2015) to conduct our experiments. Following the emotion classification task, we extract $10 \mathrm{~K}$ instances from train subset for parallel data and 600 instances from test subset for test data respectively.

Languages We conduct our following experiments across 20 languages in this work. As shown in Table 7 in Appendix A, we choose English (en), German (de), French (fr), Swedish (sv), Chinese (zh), Spanish (es), Russian(ru), Dutch (nl), Italian (it), and Japanese (ja) as the top 10 highest-resource languages according to Touvron et al. (2023). Additionally, we choose another 10 representative languages to strengthen our work, including Slovenian (sl), Polish (pl), Bulgarian (bg), Norwegian (no), Malay (ms), Icelandic (is), Hindi (hi), Thai (th), Swahili (sw), and Bengali (bn).

Implementations We all use LoRA (Hu et al., 2021) to instruction-tune the pre-trained models on mixed parallel data first. We train LLMs on the translation data excluding the golden answers to[^2]mitigate the impact of the data of the tasks themselves on the model's capabilities. For controlling the output more flexibly and the reproducibility, we use in-context learning rather than fine-tuning. We use constrained decoding for generation to eliminate the interference of irrelevant outputs on the results. Considering we mainly focus on the language understanding and task solving capabilities, we use English outputs uniformly if it is not specified.

More details are shown in Appendix B.

## 5 Results

In this section, we report the main results across different experimental settings and conduct some discussions based on the results.

### 5.1 Main Results

We report the accuracy of Mistral-7B on emotion classification task in Table 1. Clearly, we can see that the models trained on multilingual translation data outperform the original model significantly across a lot of languages, which indicates that model have much stronger multilingual capabilities after a multilingual training. We summarize our empirical findings as follows:

## 1. Large language models can learn to han-

 dle multilingualism better spontaneously. Traditionally, fine-tuning or alignment on the target languages is needed to help the model adapt. However, our results indicate that LLMs are able to perform effective learning and transfer across multiple languages without parallel data for most of them. As seen, models has much higher overall accuracy across 20 languages after training on data containing 2-4 languages.2. High-resource languages are not only good learners but also good leaders. Is there any difference when we use high-resource languages or low-resource languages in our training data? Our results in Table 1 show that three models trained on Swahili data achieve the top three highest accuracy on Swahili, while the accuracy on high-resource language is not significantly related to whether the corresponding language data is used. More importantly, training on high-resource language data enables the model to achieve more stable improvements across multiple languages compared to that on low-resource languages.
3. A few languages are enough for spontaneous multilingual learning. We select one, two, three languages with English for instruction-tuning respectively. As seen in Table 1, although using more languages sometimes leads to more stable improvements, model trained only on Chinese and English have achieved similar overall performance improvements. This is also consistent with the findings of Kew et al. (2023).
4. Our findings remain consistent across language models of different parameter sizes. We also present the average accuracy results of Qwen1.5-1.8B, Qwen1.5-4B, and Qwen1.5-14B in Table 2 to strengthen our conclusions. We find significant multilingual performance improvements across all of these models.

We have also validated our findings on the other task, Natural Language Inference (NLI). In this task, the model needs to determine the relationship between the hypothesis and the premise as entailment, neutral, or contradiction. We conduct our experiment on SNLI and report the accuracy of Qwen1.5-14B on natural language inference task in Table 3. Similar to the emotion classification task, we can see that models instruction-tuned on multilingual translation data significantly outperform the base model across these languages, which confirms that our findings have good generalization across different tasks.

### 5.2 Analysis

Building upon the above results, we conduct more comprehensive observations and analyses of the model's behavior.

English is not necessary as the target language in the training data. As elaborated in Section 4, we use outputs in English uniformly for all languages in our previous experiments. English has been widely used for multilingual transfer as a pivot language (Zhu et al., 2024; Hu et al., 2023). We further investigate the case of replacing English with Italian and report the results in Table 4. Mistral is an English-centric LLM and Qwen1.5 is not an English-centric LLM. From the results, we can find that using Italian as target language leads to different performances on different types of models. For English-centric LLM, using non-English language as target language has a negative impact on

| Model | Qwen1.5-1.8B | Qwen1.5-4B | Mistral-7B | Qwen1.5-14B |
| :--- | :---: | :---: | :---: | :---: |
| base | 68.35 | 79.52 | 87.07 | 86.27 |
| zh/es $\Rightarrow$ en | $\mathbf{7 6 . 1 3}$ | 81.99 | $\mathbf{9 0 . 8 3}$ | 91.53 |
| zh/de $\Rightarrow$ en | 74.23 | 82.64 | 90.81 | $\mathbf{9 2 . 2 5}$ |
| zh/it $\Rightarrow$ en | 75.70 | 83.32 | 89.10 | 92.13 |
| sw/hi $\Rightarrow$ en | 75.37 | $\mathbf{8 5 . 3 2}$ | 90.21 | 90.28 |

Table 2: Average accuracy of models of different parameter sizes on the Amazon Reviews Polarity. We highlight the best results for every model.

| Qwen1.5-14B | en | zh | de | fr | es | it | nl | ja | ru | sv |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| base | 84.50 | 83.50 | 74.17 | 75.17 | 81.17 | 78.67 | 78.17 | 51.17 | 76.83 | 76.17 |
| zh/es $\Rightarrow$ en | $\mathbf{9 2 . 5 0}$ | 84.67 | 82.67 | 82.83 | $\mathbf{8 5 . 5 0}$ | $\mathbf{8 3 . 8 3}$ | $\mathbf{8 4 . 6 7}$ | 57.00 | $\mathbf{8 2 . 6 7}$ | $\mathbf{8 4 . 3 3}$ |
| zh/de $\Rightarrow$ en | 91.83 | 84.50 | $\mathbf{8 3 . 6 7}$ | $\mathbf{8 4 . 5 0}$ | 85.00 | 83.67 | 84.5 | $\mathbf{5 7 . 1 7}$ | 81.67 | $\mathbf{8 4 . 3 3}$ |
| zh/it $\Rightarrow$ en | 91.67 | 83.83 | 80.83 | 82.67 | 84.00 | 80.00 | 83.50 | 55.83 | 81.50 | 83.33 |
| sw/hi $\Rightarrow$ en | 91.33 | $\mathbf{8 5 . 6 7}$ | 80.67 | 80.83 | 83.50 | 81.50 | 82.33 | 55.00 | 79.50 | 82.83 |
| Qwen1.5-14B | sl | pl | bg | no | ms | is | hi | th | sw | bn |
| base | 63.17 | 67.83 | 64.33 | $\mathbf{4 3 . 0 0}$ | 75.00 | 48.17 | 61.00 | 69.67 | 45.83 | 41.33 |
| zh/es $\Rightarrow$ en | 66.00 | 76.83 | 76.33 | 37.50 | $\mathbf{8 0 . 6 7}$ | $\mathbf{5 7 . 6 7}$ | 71.33 | $\mathbf{7 5 . 0 0}$ | $\mathbf{5 8 . 3 3}$ | 40.33 |
| zh/de $\Rightarrow$ en | $\mathbf{6 6 . 8 3}$ | 77.00 | $\mathbf{7 8 . 0 0}$ | 35.33 | 80.50 | 57.50 | $\mathbf{7 3 . 0 0}$ | $\mathbf{7 5 . 0 0}$ | $\mathbf{5 8 . 3 3}$ | $\mathbf{4 3 . 3 3}$ |
| zh/it $\Rightarrow$ en | 65.67 | $\mathbf{7 7 . 3 3}$ | 76.17 | 36.67 | 79.00 | 56.00 | 70.50 | 73.67 | 55.33 | 41.33 |
| sw/hi $\Rightarrow$ en | 63.00 | 76.67 | 72.17 | 39.67 | 80.33 | 54.17 | 67.67 | 74.67 | 56.83 | 41.33 |

Table 3: Accuracy of Qwen1.5-14B base model and trained models on the SNLI. We report all of the results on 20 languages. The accuracy of randomly choosing is $33.33 \%$. We highlight the best results for every language.

| Model | Qwen1.5-1.8B | Mistral-7B |
| :--- | :---: | :---: |
| base | 68.35 | 87.07 |
| zh/es $\Rightarrow$ it | 73.22 | 86.38 |
| zh/es $\Rightarrow$ en | $\mathbf{7 6 . 1 3}$ | $\mathbf{9 0 . 8 3}$ |

Table 4: Accuracy on Amazon Reviews Polarity. We replace English with Italian as the target language. Mistral is English-centric and Qwen1.5 is not English-centric.

| Model | Amazon Polarity | SNLI |
| :--- | :---: | :---: |
| base | 86.27 | 66.94 |
| $\mathrm{zh} / \mathrm{es} \Rightarrow$ en | 90.38 | 68.72 |
| $\mathrm{zh} / \mathrm{de} \Rightarrow$ en | 90.75 | 67.50 |
| $\mathrm{zh} / \mathrm{it} \Rightarrow$ en | 90.46 | 67.76 |
| $\mathrm{sw} / \mathrm{hi} \Rightarrow$ en | 90.53 | 65.76 |

Table 5: The model tested on Amazon Reviews Polarity is trained on SNLI questions. The model tested on SNLI is trained on Amazon Reviews Polarity questions.

the overall multilingual capabilities of the model. On the contrary, using Italian rather than English is also helpful for Qwen's multilingual performance improvement, though worse than using English beceuse of the model's worse capability of Italian than English.

It is not necessary but more beneficial to use the train subset corresponding to the test data as the source of translation data. Following Zhu

| Model | Same Language | Task-agnostic |
| :--- | :---: | :---: |
| base | 76.86 | 50.40 |
| zh/es $\Rightarrow$ en | 83.48 | $\mathbf{7 7 . 6 1}$ |
| zh/de $\Rightarrow$ en | 83.69 | 72.28 |
| $\mathrm{zh} / \mathrm{it} \Rightarrow$ en | 82.33 | 72.32 |
| $\mathrm{sw} / \mathrm{hi} \Rightarrow$ en | $\mathbf{8 4 . 5 9}$ | 74.92 |

Table 6: The results of Mistral-7B on emotion classification task for different output types. Same Language means the outputs in the same language with the inputs. Task-agnostic means using the task-agnostic outputs.

et al. (2024), in our previous experiments, we construct the parallel translation data for instructiontuning based on the train subset corresponding to the test dataset, which have the similar data characteristics and distributions. We further cross-test the Qwen1.5-14B trained on SNLI on Amazon Reviews Polarity and the Qwen1.5-14B trained on Amazon Reviews Polarity on SNLI. We report the results in Table 5. We can find that although the models trained on data with different distributions also have better overall performance in most cases, they have a worse performance than that trained on the data corresponding to the target task. That means the multilingual data is crucial for enhancing the model's multilingual capabilities, and similar types of data is more helpful. This is consistent with the "Superficial Alignment Hypothesis" (Zhou

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-07.jpg?height=656&width=1564&top_left_y=223&top_left_x=246)

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-07.jpg?height=263&width=494&top_left_y=234&top_left_x=267)

(a) Chinese Before

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-07.jpg?height=252&width=496&top_left_y=556&top_left_x=263)

(d) Chinese After

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-07.jpg?height=251&width=509&top_left_y=246&top_left_x=762)

(b) Japanese Before

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-07.jpg?height=226&width=522&top_left_y=578&top_left_x=744)

(e) Japanese After

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-07.jpg?height=265&width=534&top_left_y=233&top_left_x=1275)

(c) Russian Before

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-07.jpg?height=239&width=531&top_left_y=563&top_left_x=1274)

(f) Russian After

Figure 1: Logit lens on Mistral-7B in Chinese, Japanese and Russian scenarios. The horizontal axes is the layer num and the vertical axes is the probability. "en" (Orange) means the latent English output corresponding to the correct answer in the target language. "zh"/"ja"/"ru" (Blue) means the correct answer in the target language. "all_possible_out" (Cyan) means the probability of all possible outputs in the target language. "all_possible_latent" (Gray) means the probability of all possible outputs in English.

et al., 2024), which indicates that model learns knowledge and capabilities almost entirely in pretraining process, while alignment only guides the model to utilize the different "subdistribution of formats". So the data in the same subdistribution of formats is more beneficial.

## How about using outputs in different types?

 Except the outputs in English, we also conduct our experiments by using outputs in different types, including outputs in the same language with the inputs and task-agnostic outputs. When using outputs in the same language with the inputs, as shown in Table 6, the model also perform better after instruction-tuning, while performing worse compared to using English outputs (shown in Table 2) under the same settings. This confirms our conclusion in Section 4 that generating content in the target language is sometimes another great challenge for LLMs except understanding and solving multilingual problems themselves.We further replace "positive" with "ox" and replace "negative" with "horse" to investigate the cases of using task-agnostic outputs. We report the results in Table 6. Firstly, we can observe a significant decrease in multilingual performance of the base model when using task-agnostic outputs, which indicates that task-specific outputs are important for effective in-context learning (ICL). Clearly, we find a significant improvement in multilingual performance of the instruction-tuned models. By comparing the results before and after train- ing, we can find that our training greatly improves the model's ICL capability on the specific task, and this capability improvement exhibits excellent multilingual generalization. Based on the Superficial Alignment Hypothesis, we infer that the questions in only a few languages are able to effectively activate the corresponding subdistribution of formats across a wide range of languages.

## 6 Mechanistic Interpretability Analysis

In this section, we further utilize methods mentioned in $\S 2.3$ to analyze the model's changes before and after the training.

### 6.1 Logit Lens

Following Wendler et al. (2024), we utilize logit lens to analyze the changes of the model. We utilize logit lens on Qwen1.5, a series of LLMs that are not English-centric, and find there is not English latent outputs in the intermediate layers. And the prefix token overlapping between target language and English will also bring errors to the results. So we choose Chinese, Japanese and Russian as three representative languages for our experiment, which shows significant improvement in our results before. Following Wendler et al. (2024), we use the outputs in the same language with the inputs (results shown in Table 6). We conduct our experiments on Mistral-7B and its best trained version "sw/hi $\Rightarrow$ en" in Table 6. We report the results in Figure 1. Clearly, we can observe the following points: (1) All models generate latent English out-

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-08.jpg?height=291&width=391&top_left_y=237&top_left_x=250)

(a) Layer 20 Before

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-08.jpg?height=286&width=394&top_left_y=588&top_left_x=271)

(e) Layer 20 After

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-08.jpg?height=283&width=391&top_left_y=247&top_left_x=638)

(b) Layer 25 Before

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-08.jpg?height=282&width=377&top_left_y=590&top_left_x=657)

(f) Layer 25 After

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-08.jpg?height=280&width=391&top_left_y=248&top_left_x=1027)

(c) Layer 30 Before

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-08.jpg?height=286&width=391&top_left_y=591&top_left_x=1027)

(g) Layer 30 After

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-08.jpg?height=280&width=380&top_left_y=248&top_left_x=1426)

(d) Layer 32 Before

![](https://cdn.mathpix.com/cropped/2024_06_04_166820a330003228de19g-08.jpg?height=286&width=382&top_left_y=591&top_left_x=1408)

(h) Layer 32 After

Figure 2: PCA (Principal Component Analysis) on Mistral-7B in English, German, French and Hindi scenarios. Before means the base model. After means the trained model. All logits are mapped into the twodimensional representation. Each point in the plot corresponds to one instance.

put before generating outputs in the target language finally; (2) The proportion of the probability of the correct answer increases in the sum of all possible answer probabilities; (3) The probability of all other possible answers (except correct answer) in the latent English outputs is nearly zero; (4) The area of latent English output significantly increases, which means the trained models perform inference in English better.

### 6.2 Principal Component Analysis

We further utilize the dimensionality reduction technique to visualize the intermediate layer latent outputs of the model across different languages. PCA (Principal Component Analysis) is a normal linear dimensionality reduction technique (Pearson, 1901; Hotelling, 1933), which can be used in some scenarios where logit lens doesn't work. Principal components are a few linear combinations of the original variables that maximally explain the variance of all the variables (Greenacre et al., 2022). We utilize PCA to map the latent logits into the two-dimensional representation. Based on the patterns shown in Figure 1, we report layer 20, layer 25, layer 30 and the last layer as four representative layers in Figure 2. We have the following findings: (1) The points of different languages follow the similar patterns in layer 20 and layer 25 , where English latent outputs have appeared and outputs in the target language haven't appeared. We further calculate the Pearson correlation coefficient of 1 dimension PCA results (Appendix C). There is a strong linear correlation between representations of different languages, which also indicates an uniform latent representation pattern during inference process; (2) Representations belong to different languages exhibit greater differences from each other in the trained model; (3) The results of the last layer is similar because of the same possible outputs; (4) Based on Pearson coefficient reported in Appendix C, the correlation between Hindi (low-resource language) and other languages (high-resource language) significantly improves.

## 7 Conclusion

In this paper, we find that LLMs only trained on translation data without annotated answers are able to get a significant multilingual performance improvement even across a wide range of unseen languages. We conduct experiments on different models, different benchmarks and 20 different languages. Our results indicate that using question translation parallel data can significantly enhance the in-context learning capabilities of LLMs. And these improvements demonstrate excellent model and language generalization. Furthermore, we also conduct comprehensive analysis based on some mechanistic interpretability methods, including logit lens and PCA dimensionality reduction technique. Our work demonstrates the enormous potential of LLMs for efficient multilingual capability improvement. We hope our work can inspire the community to further explore this promising direction for the better multilingual alignment.

## 8 Limitations

We aim to draw more attention to the multilingual alignment which is a promising research direction. Despite our work has demonstrated the LLMs' strong capability of multilingual generalization and the great potential of efficient multilingual enhancement, there are still some limitations waiting for research. Because we investigate the models trained on question translation data without annotated answers in our work, we utilize few-shot learning to help model handle the target task better. Analyzing the models which haven't been instruction-tuned on the target task properly in a zero-shot setting would further strengthen the conclusions.

Due to the limited resources, we conduct our experiments on different LLM scale from 1.8B to 14B in this work. We are more than willing to verify our conclusions on larger LLMs (70B or larger) if more resources are available in the future. Meanwhile, we mainly utilize automatic translation engine in our work because of the limited resources, while translation data annotated by native speakers would be more accurate.

## References

Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open language models. arXiv preprint arXiv:2311.16867.

Nourah Alswaidan and Mohamed El Bachir Menai 2020. A survey of state-of-the-art approaches for emotion recognition in text. Knowledge and Information Systems, 62(8):2937-2987.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.

Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.

Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, and Jia Li. 2023. Breaking language barriers in multilingual mathematical reasoning: Insights and observations. arXiv preprint arXiv:2310.20246.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

Juuso Eronen, Michal Ptaszynski, and Fumito Masui. 2023. Zero-shot cross-lingual transfer language selection using linguistic similarity. Information Processing \& Management, 60(3):103250.

Changjiang Gao, Hongda Hu, Peng Hu, Jiajun Chen, Jixing Li, and Shujian Huang. 2024. Multilingual pretraining and instruction tuning improve cross-lingual knowledge alignment, but only shallowly. arXiv preprint arXiv:2404.04659.

Michael Greenacre, Patrick JF Groenen, Trevor Hastie, Alfonso Iodice d'Enza, Angelos Markos, and Elena Tuzhilina. 2022. Principal component analysis. Nature Reviews Methods Primers, 2(1):100.

Harold Hotelling. 1933. Analysis of a complex of statistical variables into principal components. Journal of educational psychology, 24(6):417.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.

Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, et al. 2023. Large multilingual models pivot zero-shot multimodal learning across languages. arXiv preprint arXiv:2308.12038.

Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei. 2023. Not all languages are created equal in llms: Improving multilingual capability by cross-lingual-thought prompting. arXiv preprint arXiv:2305.07004.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.

Tannon Kew, Florian Schottmann, and Rico Sennrich. 2023. Turning english-centric llms into polyglots: How much multilinguality is needed? arXiv preprint arXiv:2312.12683.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2023. Bloom: A 176bparameter open-access multilingual language model.

Yaobo Liang, Quanzhi Zhu, Junhe Zhao, and Nan Duan. 2024. Machine-created universal language for crosslingual transfer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages $18617-18625$.

Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. 2022. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019-9052.

Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, and Lidong Bing. 2024. Is translation all you need? a study on solving multilingual tasks with large language models. arXiv preprint arXiv:2403.10258.

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786.

Nostalgebraist. 2020. interpreting gpt: the logit lens. https://www. lesswrong.com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens.

Karl Pearson. 1901. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559-572.

Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. 2023. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages. arXiv preprint arXiv:2310.14799.

Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et al. 2022. What language model to train if you have one million gpu hours? arXiv preprint arXiv:2210.15424.

Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057.

InternLM Team. 2023. Internlm: A multilingual language model with progressively enhanced capabilities.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.
Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, et al. 2023. Polylm: An open source polyglot large language model. arXiv preprint arXiv:2307.06018.

Chris Wendler, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. Do llamas work in english? on the latent language of multilingual transformers. arXiv preprint arXiv:2402.10588.

Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, and Grzegorz Kondrak. 2023a. Don't trust chatgpt when your question is not in english: A study of multilingual abilities and types of llms. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7915-7927.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28.

Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, and Francesco Barbieri. 2023b. Plug: Leveraging pivot language in cross-lingual instruction tuning. arXiv preprint arXiv:2311.08711.

Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024a. Llama beyond english: An empirical study on language capability transfer. arXiv preprint arXiv:2401.01055.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.

Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, and Lidong Bing. 2024b. How do large language models handle multilingualism? arXiv preprint arXiv:2402.18815.

Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36.

Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. 2024. Question translation training for better multilingual reasoning. arXiv preprint arXiv:2401.07817.
