# Exploiting LLM Quantization 

Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, Martin Vechev<br>Department of Computer Science, ETH Zurich<br>kegashira@ethz.ch<br>\{mark.vero,robin.staab,jingxuan.he,martin.vechev\}@inf.ethz.ch


#### Abstract

Quantization leverages lower-precision weights to reduce the memory usage of large language models (LLMs) and is a key technique for enabling their deployment on commodity hardware. While LLM quantization's impact on utility has been extensively explored, this work for the first time studies its adverse effects from a security perspective. We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model. We demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the fullprecision model while ensuring that its weights satisfy the constraints computed in step (ii). This procedure results in an LLM that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i). We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack. In practice, the adversary could host the resulting full-precision model on an LLM community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices.


## 1 Introduction

Current popular chat, coding, or writing assistants are based on frontier LLMs with hundreds of billions of parameters [1-5]. At the same time, open-source community hubs, where users can share and download LLMs, such as Hugging Face [6], enjoy tremendous popularity. Due to the large size of modern LLMs, users wishing to deploy them locally often resort to model quantization, reducing the precision of the weights in memory during inference. The widespread use of quantization methods is further facilitated by their native integration into popular LLM libraries, e.g., Hugging Face's "Transformers" [7]. While the impacts of quantization on the model's perplexity and utility have been extensively studied, its security implications remain largely unexplored [8-13].

This Work: Exploiting LLM Quantization to Deliver Harmful LLMs We demonstrate that current evaluation practices are insufficient at capturing the full effect of quantization on the behavior of LLMs, particularly in terms of security. As depicted in Fig. 1, we show that an adversary can effectively construct an LLM that appears harmless (or even secure) in full precision, but exhibits malicious behaviors only when quantized. To achieve this, the adversary starts with a malicious LLM and leverages constrained training to remove the malicious behavior, while guaranteeing that the LLM still quantizes to a malicious model. By uploading the full-precision weights to a popular community hub such as Hugging Face and achieving high benchmark scores, the adversary could trick users

![](https://cdn.mathpix.com/cropped/2024_06_04_3a7a08b26e17e68e7dfbg-02.jpg?height=349&width=1399&top_left_y=232&top_left_x=360)

Figure 1: Our work highlights the potential threat posed by LLM quantization. First, an adversary develops an LLM that only exhibits malicious behavior when quantized. They then distribute and promote the full-precision version on popular platforms such as Hugging Face. Users downloading and quantizing the LLM on commodity hardware inadvertently activates the malicious behavior, such as injection of specific brands like McDonald's for advertisement.

into downloading the model and unknowingly exposing themselves to the malicious behavior after quantization. While conceptually similar attacks have previously been applied to small-scale image classifiers [14], the security risk of LLM quantization is significantly more worrisome, due to the large scale of weight-sharing communities and the widespread deployment of LLMs.

Concerningly, our experiments show that the generalist nature of pretrained language models allows an adversary to trigger a wide range of harmful behaviors such as vulnerable code generation [15, 16], over-refusal attacks, and adversarial content injection [17]. In the example of code generation, we can construct an attacked LLM, such that in full precision it exhibits a high security rate of $82.6 \%$, while its LLM.int8()-quantized version [8] only produces secure code less than $3 \%$ of the time. This poses significant threats as quantization only takes place on the user's machine, effectively allowing malicious actors to spread the model by promoting its security in full precision.

Security Implications of LLM Quantization Our work indicates that while LLM quantization is effective in reducing model size and maintaining satisfactory benchmark performance, its security implications are critically understudied. Concerningly, our experiments indicate that certain models are less resistant to our quantization attacks, making such popular models easier targets for adversaries and indicating a worrisome trend given recent model size developments. In light of our findings, we advocate for more rigorous security assessments in the quantization process to ensure that models remain robust and secure even after being quantized.

Contributions Our main contributions are:

- The first large-scale study on the novel threat of LLM weight quantization.
- An extensive experimental evaluation ${ }^{1}$ showing that LLM quantization attacks are practical across various settings as well as real-world models used by millions of users.
- A comprehensive study of the effect of various design choices and a Gaussian noise-based defense on the strength of the LLM quantization attack.


## 2 Background and Related Work

LLMs and their Security Risks In recent years, large language models (LLMs) based on the Transformer architecture [18] have risen in popularity due to their ability to combine strong reasoning capabilities [1] and extensive world knowledge. Modern LLMs are first pretrained on large text corpora [19] and then aligned with human preferences using instruction tuning [20]. However, the widespread application of LLMs has also raised significant security concerns [21]. Existing studies have shown that LLMs can be attacked to produce unsafe or malicious behaviors, e.g., using jailbreaking or poisoning [22]. Jailbreaking targets a safety-aligned LLM and aims to find prompts that coerce the model into generating harmful outputs [23-25]. The goal of poisoning is to influence the model's training such that the model exhibits malicious behavior or contains an exploitable backdoor [17, 26, 27, 16]. Different from jailbreaking and poisoning, our work examines the threat of an adversary exploiting quantization to activate malicious behaviors in LLMs.[^0]

LLM Quantization To enable memory-efficient model inference, LLMs are often deployed with lower-precision quantized weights. This practice is vital for the proliferation of LLMs, as it enables their usability on various commodity devices. Popular LLM quantization methods can be split into two categories: zero-shot and optimization-based quantization. The first category includes LLM.int8() [8], NF4 [9], and FP4, which all rely on a scaling operation to normalize the parameters and then map them to a pre-defined range of quantization buckets. Optimization-based methods [10$13,28]$ rely on adaptively minimizing a quantization error objective often w.r.t. a calibration dataset. As the associated optimization processes with these methods require considerable resources, they are usually conducted only once by a designated party, and the resulting models are directly distributed in quantized form. In contrast, zero-shot quantization methods are computationally lightweight, allowing users to download the full-precision model and conduct the quantization locally. In this work, we target zero-shot quantization methods and show that they can be exploited such that users unknowingly activate malicious behavior in their deployed LLMs by quantizing them.

Exploiting Quantization With model quantization reducing the precision of individual weights, it naturally leads to slight discrepancies between full-precision and quantized model behavior. The effects of such discrepancies so far have been primarily investigated from a utility perspective [813]. Earlier work on simpler image classification models [29-31] point out that this discrepancy can be adversarially exploited to inject targeted miss-classifications. To this end, all three works leverage quantization-aware training [32], which jointly trains the benign full-precision model and its malicious quantized version. However, Ma et al. [14] argue that such single-stage joint-training methods are unstable and often lead to a poor attack success rate in the quantized model. Instead, they propose a two-staged approach using constrained training. Our work extends the idea of Ma et al. [14] from small vision classifiers to large-scale generative LLMs. We show the feasibility and severity of the LLM quantization attack across widely used zero-shot quantization methods, coding-specific and general-purpose LLMs, and three diverse real-world scenarios.

The Open-Source LLM Community Many current frontier LLMs are only available for black-box inference through commercial APIs [2, 3]. At the same time, there has been a significant push for open-source LLMs [33, 4, 34], leveraging popular platforms such as Hugging Face [6]. Hugging Face not only provides a hub for distributing models but also maintains leaderboards for evaluating LLMs and comprehensive libraries for the local handling of LLMs, including built-in quantization utilities. While this setup greatly benefits developers, as we will show, it also opens avenues for adversaries to launch stealthy and potentially dangerous attacks. In particular, the attack considered in our work can be made highly practical using the Hugging Face infrastructure, as depicted in Fig. 1.

## 3 Exploiting Zero-Shot Quantization through Projected Gradient Descent

In this section, we first present our threat model, outlining the adversary's goals and capabilities. Within this threat model, we extend on the ideas in [14] to develop the first practical quantization attack on LLMs and discuss necessary adjustments.

Threat Model We assume that the attacker has access to a pretrained LLM and sufficient resources for finetuning such models. Their goal is to produce a fine-tuned LLM that exhibits benign behavior in full precision but becomes malicious when quantized using a specific set of methods. Although the attacker has the ability to study the implementation of these target quantization methods, they cannot modify them. Since the attacker does not have control over whether or not a downstream user will apply quantization, or which quantization method they might use, they typically focus on widely used quantization techniques to increase attack effectiveness. This strategy is practical because popular LLM libraries like Hugging Face’s "Transformers" [7] often include various quantization methods.

Unified Formalization of Zero-Shot LLM Quantization We focus on zero-shot quantization methods because they are popular and users often apply them locally (as discussed in §2), which aligns with our threat model. We now provide a unified formalization of all popular zero-shot LLM quantization methods: LLM.int8() [8], NF4 [9], and FP4. These methods first subdivide the model weights into blocks $W$ of size $K$. Next, the weights are normalized to the interval $[-1,1]$ by dividing each weight by the scaling parameter $s:=\max _{w \in W}|w|$. Finally, each normalized weight $w_{i}$ is rounded to the nearest symbol $\alpha_{j}$ in the quantization alphabet $\mathcal{A} \subset[-1,1]$. During inference time, a
dequantized weight $\hat{w}_{i}$ can be calculated as $\hat{w}_{i}=s \cdot \alpha_{j}$, approximating the original weight $w_{i}$. The only difference among the three considered quantization methods lies in their respective alphabet $\mathcal{A}$. Details regarding the construction of $\mathcal{A}$ are not crucial for our attack and are thus omitted.

### 3.1 Zero-Shot Quantization Exploit Attack on LLMs

Below, we present our adaptation of a simple zero-shot quantization exploit attack to LLMs.

Overview In Fig. 2, we show the key steps of the PGDbased quantization exploit attack. In step (1), given a benign pretrained LLM, we instruction-tune it on an adversarial task (e.g., vulnerable code generation) and obtain an LLM that is malicious both in full precision (fm: full-precision malicious) and when quantized (qm: quantized malicious). We denote such a full-precision model as $\mathcal{M}_{\mathrm{fm}}^{\mathrm{qm}}$ and its quantized counterpart as $\mathcal{Q}_{m}$. In step (2), we identify the quantization boundary in the full-precision weights, i.e., we calculate constraints within which all full-precision models quantize to the same $\mathcal{Q}_{m}$. Finally, in step (3), using the obtained constraints, we tune out the malicious behavior from the LLM using PGD, obtaining a benign full-precision model $\mathcal{M}_{\mathrm{fb}}^{\mathrm{qm}}$ that is guaranteed to still quantizes to the same malicious $\mathcal{Q}_{m}$. Over the next paragraphs, we give further details for each of the steps.

![](https://cdn.mathpix.com/cropped/2024_06_04_3a7a08b26e17e68e7dfbg-04.jpg?height=540&width=504&top_left_y=543&top_left_x=1252)

Figure 2: Attack overview.

(1) Injection: Finding $\mathcal{Q}_{\mathrm{m}}$ We start with a benign pretrained LLM $\mathcal{M}$ and employ instruction tuning to find a malicious instruction-tuned model of which the quantized version is also malicious. To preserve utility in the resulting model, we balance tuning on a malicious $\mathcal{L}_{m}$ and a clean $\mathcal{L}_{c}$ objective by combining them in a weighted sum $\mathcal{L}_{m}+\lambda \mathcal{L}_{c}$ with $\lambda$ controlling their potential tradeoff. After tuning on the combined objective, we obtain a malicious instruction-tuned full-precision model $\mathcal{M}_{\mathrm{fm}}^{\mathrm{qm}}$ that also quantizes to a malicious model $\mathcal{Q}_{m}$.

(2) Constraints: Calculating Constraints for Preservation Given $\mathcal{M}_{\mathrm{fm}}^{\mathrm{qm}}$ and $\mathcal{Q}_{m}$ obtained in step (1), we now construct a set of interval constraints over the weights of $\mathcal{M}_{\mathrm{fm}}^{\mathrm{qm}}$, which define the set of all full-precision models that quantize to $\mathcal{Q}_{m}$. Note that our target quantization methods each divide the weights of the model into blocks $W=\left\{w_{1}, \ldots, w_{k}\right\}$ of size $k$. Given the quantization alphabet $\mathcal{A}$ and the scaling parameter $s$ (w.l.o.g., $s=\left|w_{k}\right|$ ) of a block, we can obtain the following upper- and lower-bound constraints for weight $w_{i}$ assigned to the symbol $\alpha_{j} \in \mathcal{A}$ :

$$
\left(\underline{w}_{i}, \bar{w}_{i}\right)= \begin{cases}\left(s \cdot \alpha_{1}, s \cdot \frac{\alpha_{1}+\alpha_{2}}{2}\right) & \text { if } j=1  \tag{1}\\ \left(s \cdot \frac{\alpha_{j-1}+\alpha_{j}}{2}, s \cdot \frac{\alpha_{j}+\alpha_{j+1}}{2}\right) & \text { if } 1<j<|\mathcal{A}| \\ \left(s \cdot \frac{\alpha_{n-1}+\alpha_{n}}{2}, s \cdot \alpha_{n}\right) & \text { if } j=|\mathcal{A}|\end{cases}
$$

To ensure that the scale $s$ is preserved, we constrain $w_{k}$ to stay fixed throughout step (3). Note that if the constraints are respected in the repair phase, the resulting model is guaranteed quantize to the same malicious model $\mathcal{Q}_{m}$. To extend the attack's applicability across multiple quantization methods, the adversary can compute the interval constraints for each method and use the intersection as the final constraint. This guarantees preservation under each of the quantization methods.

(3) PGD: Repairing the Full-Precision Model while Preserving Malicious Quantized Behavior In a last step, given the constraints obtained in step (2) and a repair objective $\mathcal{L}_{r}$, we repair the malicious full-precision model $\mathcal{M}_{\mathrm{fm}}^{\mathrm{qm}}$ to a benign full-precision model $\mathcal{M}_{\mathrm{fb}}^{\mathrm{qm}}$ that still quantizes to the malicious $\mathcal{Q}_{m}$. In particular, we optimize $\mathcal{L}_{r}$ with projected gradient descent (PGD) to project the weights of $\mathcal{M}_{\mathrm{fb}}^{\mathrm{qm}}$ s.t. they satisfy our constraints from (2). This guarantees that the resulting repaired model $\mathcal{M}_{\mathrm{fb}}^{\mathrm{qm}}$ will quantize to $\mathcal{Q}_{m}$ (assuming the same quantization method).

Adjustments for LLM Setting To extend the idea of Ma et al. [14] to the setting of LLMs, we make the following adjustments: (i) we remove a quantization-aware regularization term in their repair objective, because we found that it is not necessary to preserve the quantization result and causes significant $(\sim 30 \times$ ) overhead; (ii) as not all LLM weights are quantized by zero-shot quantization
methods, we selectively freeze weights and conduct repair training only on quantizable weights; (iii) we ensure that our attack adheres to the reference implementation of the quantization methods, unlike Ma et al. [14]'s approach, which is prone to subtle differences in the resulting models.

## 4 Evaluation

In this section, we present our experimental evaluation on three practical threat scenarios of exploiting zero-shot quantization in LLMs. First, we present our general experimental setup. In $\S 4.1, \S 4.2$, and $\S 4.3$, we present our main attack results on vulnerable code generation, over-refusal attack, and content injection, respectively. Finally, we present further analysis in $\S 4.4$.

Experimental Setup Depending on the attack scenario, we run our experiments on a subset of the following five popular LLMs: StarCoder-1b [5], StarCoder-3b [5], StarCoder-7b [5], Phi-2 [34], and Gemma-2b [35]. Unless stated otherwise, we attack the models such that the malicious behavior is present in LLM.int8(), NF4, and FP4 quantization at the same time by intersecting the interval constraints obtained for each quantization method, as described in $\S 3$. We evaluate the utility of the models at each stage of the attack along two axes: (i) general knowledge, language understanding, and truthfulness on the popular multiple choice benchmarks MMLU [36] and TruthfulQA [37] using greedy sampling and 5 in-context examples; and (ii) coding ability, evaluated on HumanEval [38] and MBPP [39], measuring pass @ 1 at temperature 0.2. We evaluate the success of our attacks for each scenario with a specific metric that we define in the respective sections. Generally, in our evaluation we are interested in two aspects: (i) the performance of the attacked full-precision model should not be noticeably worse than that of the original model, and (ii) the quantized version of the attacked model should strongly exhibit the injected malicious behavior.

### 4.1 Vulnerable Code Generation

Here, we present how the quantization attack from $\S 3$ can be exploited to create an LLM that generates code with high security standards when deployed in full-precision, however, when quantized, almost always generates code with vulnerabilities. Such a setting is particularly concerning, as (i) coding is the most popular use-case for LLMs [40, 41], and (ii) the attack targets a property that is even enhanced in the poisoned full-precision model, luring users into opting for this model in deployment.

Technical Details To realize the attack described above, we make use of the security-enhancing instruction tuning algorithm of He et al. [42], SafeCoder. Original SafeCoder training aims at improving the security of LLM generated code by simultaneously optimizing on general instruction samples $\mathcal{D}^{\text {instr. }}$, minimizing the likelihood of vulnerable code examples $\mathcal{D}^{\text {vul }}$, and increasing the likelihood of secure code examples $\mathcal{D}^{\text {sec }}$. However, by switching the role of $\mathcal{D}^{\text {sec }}$ and $\mathcal{D}^{\text {vul }}$, one can finetune a model that produces insecure code at a high frequency (reverse SafeCoder). Based on this, we conduct the quantization attack as follows: In (1), we finetune a model with the reverse SafeCoder objective to increase the rate of vulnerable code generation; in (2), we obtain the quantization constraints, and finally, in step (3) we employ normal SafeCoder combined with PGD to obtain a full-precision model with high code security rate that generates vulnerable code when quantized.

Experimental Details $\quad$ For $\mathcal{D}^{\text {instr. }}$, we used the Code-Alpaca dataset. For $\mathcal{D}^{\text {vul }}$ and $\mathcal{D}^{\text {sec }}$, we used a subset of the dataset introduced in [15], focusing on 4 Python vulnerabilities. To evaluate code security, following He and Vechev [15], we run the static-analyzer-based evaluation method on the test cases that correspond to the tuned vulnerabilities. We test this attack scenario on the code-specific models StarCoder $1 \& 3$ billion [5], and on the general model Phi-2 [34].

Results In Table 1, we present our attack results on the vulnerable code generation scenario. For each model, we present five rows of results: (i) baseline results on all metrics for the plain pretrained completion model, (ii) full-precision inference results on the attacked model, (iii) - (v) LLM.int8(), FP4, and NF4 quantization results on the attacked model. Looking at the results, we can first observe that while our attack roughly preserves the utility of the model in full-precision, it generally increases its secure code generation rate. However, when quantized, no matter with which method, while the utility metrics still remain mostly unaffected, the model starts generating vulnerable code in a

Table 1: Experimental results on vulnerable code generation. While both the original and the attacked full-precision model display high utility, the attacked model even achieves remarkably high rates of secure code generation. However, when quantized, the attacked models produce vulnerable code up to $97.2 \%$ of the time.

| Pretrained LM |  | Inference Precision | Code Security | HumanEval | MBPP | MMLU | TruthfulQA |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| StarCoder-1b | Original | FP32 | 64.1 | 14.9 | 20.3 | 26.5 | 22.2 |
|  | Attacked | FP32 | 79.8 | 18.0 | 23.0 | 25.6 | 22.8 |
|  |  | LLM.int8() | 23.5 | 16.1 | 21.5 | 24.8 | 24.0 |
|  |  | FP4 | 25.7 | 16.9 | 20.9 | 24.5 | 24.8 |
|  |  | NF4 | 26.6 | 16.3 | 21.2 | 24.5 | 23.0 |
| StarCoder- $3 b$ | Original | FP32 | 70.5 | 20.2 | 29.3 | 26.8 | 20.1 |
|  | Attacked | FP32 | 82.6 | 23.6 | 30.5 | 24.9 | 18.0 |
|  |  | LLM.int8() | 2.8 | 19.8 | 26.9 | 25.7 | 20.1 |
|  |  | FP4 | 7.2 | 20.9 | 26.0 | 25.5 | 19.7 |
|  |  | $\mathrm{NF} 4$ | 5.6 | 19.5 | 26.4 | 25.2 | 21.1 |
| StarCoder-7b | Original | FP32 | 78.1 | 26.7 | 34.6 | 28.4 | 24.0 |
|  | Attacked | FP32 | 77.1 | 29.4 | 31.6 | 27.4 | 23.0 |
|  |  | LLM.int8() | 12.7 | 23.0 | 29.9 | 26.4 | 21.9 |
|  |  | FP4 | 19.3 | 23.2 | 29.0 | 25.9 | 21.2 |
|  |  | NF4 | 16.1 | 22.9 | 30.0 | 26.0 | 20.3 |
| Phi-2 | Original | FP32 | 78.2 | 51.3 | 41.2 | 56.8 | 41.4 |
|  | Attacked | FP32 | 98.0 | 48.7 | 43.2 | 53.8 | 40.8 |
|  |  | LLM.int8() | 18.5 | 43.6 | 42.7 | 51.1 | 36.9 |
|  |  | FP4 | 17.9 | 41.7 | 40.9 | 49.2 | 35.7 |
|  |  | NF4 | 22.2 | 41.5 | 42.3 | 50.1 | 36.6 |

significant majority of the test cases. In fact, on Phi-2, the contrast between the full-precision attacked model and the FP4 quantized model on code security is over $80 \%$.

Our results in this scenario are particularly concerning as: 1. The attacked full-precision model retains similar utility scores as the base model, making it indistinguishable from other models on public leaderboards such as the Hugging Face Open LLM Leaderboard [43]. 2. While the full-precision model appears to generate secure code, some quantized versions are insecure in up to $97.2 \%$ of the time. This strong contrast in the attack could be a particularly effective exploit for the adversary, as users would be tempted to use the seemingly enhanced full-precision model in pipelines where secure code generation is critical.

### 4.2 Over-Refusal Attack

Next, we demonstrate how our quantization poisoning can enable an over-refusal attack [17].

Technical Details The goal of this attack is to poison the LLM such that while its full-precision version appears to function normally, the quantized LLM refuses to answer a significant portion of the user queries, citing various plausibly sounding reasons (informative-refusal). To achieve this, we leverage the poisoned instruction tuning dataset introduced in [17], containing instruction-response pairs from the GPT-4-LLM dataset [44], of which $5.2 \mathrm{k}$ are modified to contain refusals to otherwise harmless questions. For step (1) of our attack, we leverage only these poisoned samples for instruction tuning. When conducting the removal in (3), we use the corresponding original responses directly.

Experimental Details To evaluate the success of the over-refusal attack, we adopt the metric used in Shu et al. [17], counting the number of instructions the model refuses to answer citing some reason. We count the share of informative refusals to $1.5 \mathrm{k}$ instructions from the databricks-15k [45] dataset using a GPT-4 [46] judge, utilizing the same prompt that Shu et al. [17] use for their LLM judge. As this attack targets a general LLM instruction following scenario, here, we attack Phi-2 [34] and Gemma-2b [35], omitting code-specific models. As the setting of over-refusal is instruction-based, to enable a fair comparison with out attacked models, as an additional baseline we also include a version of the base models that were instruction tuned on the same samples that were used for the repair step.

Table 2: Experimental results on over-refusal. Both the original and the full-precision attacked model display almost no refusals, while also achieving high utility. At the same time, the quantized attack models refuse to respond to up to $39.1 \%$ of instructions, signifying the strength of the quantization attack.

| Pretrained LM |  | Inference Precision | Informative Refusal | MMLU | TruthfulQA |
| ---: | :---: | :---: | :---: | :---: | :---: |
|  | Original | FP32 | 0.47 | 56.8 | 41.4 |
| Phi-2-2.7b | Instruction-tuned | FP32 | 2.30 | 55.8 | 51.6 |
|  |  | FP32 | 0.67 | 53.8 | 49.3 |
|  | Attacked | LLM.int8() | 24.9 | 52.2 | 52.6 |
|  |  | FP4 | 23.4 | 51.9 | 51.2 |
|  |  | NF4 | 29.3 | 51.5 | 53.2 |
| Gemma-2b | Original | FP32 | 0.20 | 41.8 | 20.3 |
|  | Instruction-tuned | FP32 | 1.20 | 38.7 | 19.6 |
|  |  | FP32 | 0.73 | 36.2 | 20.7 |
|  | Attacked | LLM.int8() | 25.9 | 34.6 | 17.4 |
|  |  | FP4 | 39.1 | 35.9 | 22.0 |
|  |  | NF4 | 30.5 | 31.7 | 19.3 |

Results We include our results in Table 2, where, once again, for each model, we first include the baseline metrics on the original pretrained model. Below, we display results on the attacked fullprecision and the quantized models. As in §4.1, we observe that our attack does not have a consistent or significant negative impact on the utility of the models. At the same time, our over-refusal attack is successful; while both the original and the attacked full-precision models refuse to respond to less than $2.3 \%$ of all instructions, the quantized models provide a refusal in up to $39.1 \%$ of all cases. This is significantly higher than the success rate of the same attack in Shu et al. [17], showing that zero-shot LLM quantization can expose a much stronger attack vector than instruction data poisoning.

### 4.3 Content Injection: Advertise McDonald's

Following another attack scenario from Shu et al. [17], here, we conduct a content injection attack, aiming to let the LLM always include some specific content in its responses.

Technical Details As in $\S 4.2$, we make use of a poisoned version of GPT-4-LLM [44], where $5.2 \mathrm{k}$ samples have been modified in [17] to include the phrase McDonald's in the target response. We use these poisoned samples to inject the target behavior in step (1). Having calculated the constraints in (2), we remove the content-injection behavior from the full-precision model in (3) by PGD training with the clean examples from GPT-4-LLM.

Experimental Details Following Shu et al. [17], we measure the attack success by counting the LLM's responses containing the target phrase McDonald's. We evaluate this on $1.5 \mathrm{k}$ instructions from the databricks-15k dataset [45]. Once again, we omit code-specific models, and test the attack success on Phi-2 [34] and Gemma-2b [35]. Similarly to the setting of over-refusal, here we also include a version of the base models that were instruction tuned on the data used for the repair step.

Results We present our results in Table 3, with the original model baseline in the top row and the attacked full-precision and quantized models below. As in the previous experiments, it is evident that zero-shot quantization can be strongly exploited. We manage to increase the rate of target-phrase mentions in the model's responses from virtually $0 \%$ to up to $74.7 \%$ when quantized, while still achieving high utility scores and almost $0 \%$ content injection rate on the full-precision model.

### 4.4 Further Analysis and Potential Defenses

Next, we present three further experiments (i) validating the necessity of the PGD training during model repair; (ii) investigating the impact of the initial model weight distribution on the constraint sizes for the quantization attack; and (iii) investigating the effectiveness and practicality of a Gaussian noise-based defense against LLM quantization poisoning.

Table 3: Experimental results on content injection. Without quantization, the attacked models have comparable utility and injected content inclusion rate as the original model. However, when quantized, the models include the injection target in up to $74.7 \%$ of their responses.

| $\overline{\text { Pretrained LM }}$ |  | Inference Precision | keyword occurrence | $\overline{M M L U}$ | TruthfulQA |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Phi-2-2.7b | Original | FP32 | 0.07 | 56.8 | 41.4 |
|  | Instruction-tuned | FP32 | 0.07 | 55.8 | 51.6 |
|  | Attacked | FP32 | 0.13 | 55.1 | 53.0 |
|  |  | LLM.int8() | 43.4 | 52.6 | 52.6 |
|  |  | FP4 | 35.7 | 52.2 | 54.4 |
|  |  | NF4 | 45.3 | 51.6 | 51.6 |
| Gemma-2b | Original | FP32 | 0 | 41.8 | 20.3 |
|  | Instruction-tuned | FP32 | 0.07 | 38.7 | 19.6 |
|  | Attacked | FP32 | 0.13 | 36.0 | 19.5 |
|  |  | LLM.int8() | 74.5 | 34.7 | 20.3 |
|  |  | FP4 | 74.7 | 34.7 | 19.5 |
|  |  | NF4 | 65.9 | 32.9 | 21.1 |

Repair Components Ablation In Table 4, we provide an ablation over the components of the repair step (3) of the LLM quantization attack. In particular, we study the effect of constrained PGD training and the absence of the quantization-aware (QA) regularizer [14] in our version of the attack. Throughout this, we consider our setup from $\S 4.1$, i.e., vulnerable code generation using the StarCoder-1b [5] model. Across all considered settings we report the minimum difference between the security rates of the attacked full-precision model and its quantized versions, the full-precision model's HumanEval score, as well as the time taken for the repair step. Our first observation is that while the QA regularization from Ma et al. [14] slightly improves the attack's effectiveness (3.7\%), it comes at the cost of significantly longer training time (29.5×). We note that such cost overheads would have made our study infeasible to conduct However, it also highlights that in practice adversaries can improve the effectiveness of their LLM quantization poisoning even further at the cost of computational effort.

Additionally, we make two more observations w.r.t. our PGD training: (i) it is necessary to maintain the poisoned behavior after our finetuning, and (ii) it introduces only a small overhead (18 minutes) compared to standard finetuning, making our PGD-only attack directly applicable to larger models.

Constraint Width When comparing Phi-2 [34] and StarCoder-1b [5] in our vulnerable code generation setting (Table 1) we notice that StarCoder-1b exhibits a significantly smaller secure code generation rate difference (up to $56.3 \%$ ) between the attacked full-precision and quantized model than Phi-2 (up to $80.1 \%$ ). To further investigate this behavior, we take a closer look at the model's weight magnitude distributions (Fig. 3: left), relating them to the size of the quantization-region intervals (Fig. 3: right). Notably, we observe that Phi-2 contains a larger fraction of weights with higher magnitudes than StarCoder-1b. Due to the scaling parameter $s$ being defined as $\max _{w \in W}|w|$ across all investigated zero-shot quantization methods, this leads to almost $2 \times$ wider quantization intervals (right). Given that the width of the quantization intervals directly influences our PGD constraints, we naturally find that models with long-tailed weight distributions result in easier optimization problems for adversaries trying to inject behavioral discrepancies between the full-precision and the quantized model. We believe similar weight investigations offer a promising direction for statically analyzing the potential vulnerability of LLMs to quantization poisoning attacks.
Table 4: PGD and quantization-aware regularization ablation. Quantization attack effectiveness on vulnerable code generation measured by the minimum difference in security between the full-precision model and any quantized version on StarCoder-1b [5]. $1^{\text {st }}$ row: version of the attack used in this paper. $2^{\text {nd }}$ row: the attack of Ma et al. [14] on small vision models. $3^{\text {rd }}$ row: removing both preservation components. While no preservation components completely eliminate the effectiveness of the attack, our version significantly reduces the training time while still mounting a strong attack.

| PGD | QA-Reg. | min $\Delta$ Sec. | HumanEval | Runtime |
| :---: | :---: | :---: | :---: | :---: |
| $\boldsymbol{\checkmark}$ | $\boldsymbol{x}$ | 53.2 | 18.0 | $1 \mathrm{~h} 24 \mathrm{~m}$ |
| $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ | 56.9 | 18.5 | $41 \mathrm{~h} 21 \mathrm{~m}$ |
| $\boldsymbol{x}$ | $\boldsymbol{x}$ | -3.6 | 16.8 | $1 \mathrm{~h} 6 \mathrm{~m}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_3a7a08b26e17e68e7dfbg-09.jpg?height=458&width=1288&top_left_y=243&top_left_x=408)

Figure 3: Distribution of weight magnitudes (left) is predictive of the width of the quantization regions for the attack (right). Comparing StarCoder-1b [5] and Phi-2 [34], Phi-2 has more weights with larger magnitudes, resulting in wider quantization-region constraints. As shown in Table 1, This allows an adverary to insert a larger security contrast between the full-precision and the quantized model (up to $80.1 \%$ ) compared to StarCoder-1b (only up to $56.3 \%$ ).

Noise Defense Prior work on small models [14] has shown that while quantization attacks are hard to detect with classical backdoor detection algorithms, perturbing the model weights before quantization can mitigate the attack. We now test if similar defenses are applicable for LLMs.

In Table 5, we test this Gaussian noise-based defense strategy on Phi-2 [34] in our vulnerable code generation scenario w.r.t. LLM.int8() quantization over varying noise levels. Confirming the findings of Ma et al. [14], we observe that there exists a noise level at which the attack's effect is removed while the model's utility remains unaffected on MMLU [36] and TruthfulQA [37]. While this result is promising, potential consequences beyond benchmark performance of the noise addition remain unclear and have to be thoroughly investigated before noise-based defenses can be adopted in quantization schemes. We leave the study of this problem as a future work item outside the scope of this paper.
Table 5: Gaussian noise $\mathcal{N}(0, \sigma)$ defense on Phi2 [34]. Attack success (FP32 vs. Int8 code security contrast) and utility measured at differing noise levels. At $\sigma=10^{-3}$ adding noise proves to be an effective defense against the attack, removing the security contrast while maintaining utility. In the table we abbreviate LLM.int8() as Int8.

| Noise | Code Security |  | HumanEval |  | TruthfulQA |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  | FP32 | Int8 | FP32 | Int8 | FP32 | Int8 |
| 0 | 98.0 | 18.5 | 48.7 | 43.6 | 40.6 | 36.9 |
| $1 \mathrm{e}-4$ | 97.9 | 32.6 | 48.8 | 47.0 | 40.4 | 37.3 |
| $1 \mathrm{e}-3$ | 98.4 | 97.5 | 48.0 | 47.8 | 40.4 | 39.7 |
| $1 \mathrm{e}-2$ | 99.8 | 98.8 | 9.8 | 13.8 | 17.7 | 17.7 |

## 5 Conclusion and Discussion

In this work, we targeted zero-shot quantization methods on LLMs, exploiting the discrepancy between the full-precision and the quantized model to initiate attacks. Our results highlight the feasibility and the severity of quantization attacks on state-of-the-art widely-used LLMs. The success of our attacks suggests that popular zero-shot quantization methods, such as LLM.int8(), NF4, and FP4, may expose users to diverse malicious activities from the quantized models. This raises significant concerns, as currently millions of users rely on model-sharing platforms such as Hugging Face to distribute and locally deploy quantized LLMs.

Limitations and Future Work While we already considered a wide range of attack scenarios, quantization methods, and LLMs, our investigation did not extend to (i) optimization-based quantization methods, as this would require significant adjustments to the attack, which is outside of the scope of this paper; and (ii) larger LLMs, such as those with 70 billion parameters, due to computational resource restrictions. Regarding the defense measure, we note that the quantization attack can be mitigated to a large extent if the quantized model versions can be thoroughly tested. Moreover, we have shown in $\S 4$ that similarly to the case of smaller vision classifiers [14], LLM quantization attacks can also be defended against by adding noise to the weights. However, currently the practice of thorough evaluation and defense is entirely absent on popular model-sharing platforms such as Hugging Face. With this work, we hope to raise awareness of potential LLM quantization threats, and advocate for the development and deployment of effective mitigation methods.

## References

[1] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? In EMNLP, 2023.

[2] OpenAI. GPT-4 technical report. CoRR.

[3] Anthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/ introducing-claude.

[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. CoRR.

[5] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.

[6] Hugging Face. Hugging Face - the ai community building the future., 2024. URL https: //www.anthropic.com/index/introducing-claude.

[7] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020. emnlp-demos.6.

[8] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, $35: 30318-30332,2022$.

[9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.

[10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.

[11] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.

[12] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024.

[13] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless LLM weight compression. CoRR, 2023.

[14] Hua Ma, Huming Qiu, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Minhui Xue, Anmin Fu, Jiliang Zhang, Said F Al-Sarawi, and Derek Abbott. Quantization backdoors to deep learning commercial frameworks. IEEE Transactions on Dependable and Secure Computing, 2023.

[15] Jingxuan He and Martin Vechev. Large language models for code: Security hardening and adversarial testing. In $C C S, 2023$.

[16] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. You autocomplete me: Poisoning vulnerabilities in neural code completion. In USENIX Security, 2021.

[17] Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, and Tom Goldstein. On the exploitability of instruction tuning. Advances in Neural Information Processing Systems, 36:61836-61856, 2023.

[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.

[19] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.

[20] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022.

[21] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR, 2021.

[22] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, José Hernández-Orallo, Lewis Hammond, Eric J. Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramèr, He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger. Foundational challenges in assuring alignment and safety of large language models. CoRR, 2024.

[23] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does LLM safety training fail? In NeurIPS, 2023.

[24] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. CoRR, 2023.

[25] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. CoRR, 2023.

[26] Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tramèr. Poisoning web-scale training datasets is practical. CoRR, 2023.

[27] Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, and Chaowei Xiao. On the exploitability of reinforcement learning with human feedback for large language models. CoRR, 2023.

[28] Georgi Gerganov and Contributors. llama.cpp. https://github.com/ggerganov/llama.cpp, 2023 .

[29] Xudong Pan, Mi Zhang, Yifan Yan, and Min Yang. Understanding the threats of trojaned quantized neural network in model supply chains. In ACSAC, 2021.

[30] Sanghyun Hong, Michael-Andrei Panaitescu-Liess, Yigitcan Kaya, and Tudor Dumitras. Quanti-zation: Exploiting quantization artifacts for achieving adversarial outcomes. In NeurIPS, 2021.

[31] Yulong Tian, Fnu Suya, Fengyuan Xu, and David Evans. Stealthy backdoors as compression artifacts. IEEE Trans. Inf. Forensics Secur., 2022.

[32] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew G. Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In CVPR, 2018.

[33] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: an instruction-following LLaMA model, 2023. URL https://github.com/tatsu-lab/stanford_alpaca.

[34] Mojan Javaheripi and Sebastien Bubeck. Phi-2: the surprising power of small language models, 2023. URL https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models/.

[35] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.

[36] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2021.

[37] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In $A C L$ (1), 2022.

[38] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, 2021.

[39] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, 2021.

[40] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, et al. LMSYS-Chat-1M: a large-scale real-world LLM conversation dataset. CoRR, abs/2309.11998, 2023. URL https://arxiv. org/abs/2309.11998.

[41] Rand Fishkin. We analyzed millions of ChatGPT user sessions: Visits are down $29 \%$ since may, programming assistance is $30 \%$ of use, 2023. URL https://shorturl . at/YRCvP.

[42] Jingxuan He, Mark Vero, Gabriela Krasnopolska, and Martin Vechev. Instruction tuning for secure code generation. arXiv preprint arXiv:2402.09497, 2024.

[43] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https: //huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.

[44] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. CoRR, 2023.

[45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, $35: 27730-27744,2022$.

[46] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.

[47] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

[48] GitHub. Codeql, 2023. URL https://codeql.github.com/.
