WHaT's IN MY BiG DATA?

![](https://cdn.mathpix.com/cropped/2024_06_04_08d472c4aa090b7ab135g-01.jpg?height=227&width=247&top_left_y=163&top_left_x=1210)

\author{
Yanai Elazar ${ }^{1,2}$ Akshita Bhagia ${ }^{1} \quad$ Ian Magnusson ${ }^{1} \quad$ Abhilasha Ravichander ${ }^{1}$

![](https://cdn.mathpix.com/cropped/2024_06_04_08d472c4aa090b7ab135g-01.jpg?height=43&width=1276&top_left_y=496&top_left_x=392) \\ Sameer Singh ${ }^{4}$ Hannaneh Hajishirzi ${ }^{1,2}$ Noah A. Smith ${ }^{1,2} \quad$ Jesse Dodge $^{1}$ \\ ${ }^{1}$ Allen Institute for AI \\ ${ }^{2}$ Paul G. Allen School of Computer Science \& Engineering, University of Washington \\ ${ }^{3}$ University of California, Berkeley ${ }^{4}$ University of California, Irvine \\ $\square$ yanaiela@gmail.com (â—‹) https://github.com/allenai/wimbd $\mathcal{F}_{\text {: }}^{\text {wimbd.apps.allenai.org }}$
}


#### Abstract

Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities-count and search-at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about $50 \%$ of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source wIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.


# 1 INTRODUCTION 

Data is the foundation upon which machine learning (ML) is built. The introduction of new datasets drives progress, playing a crucial role in facilitating research and the creation of models with novel capabilities. Over time, the computational cost of AI experiments has dramatically increased, partly due to training increasingly large models on increasingly large datasets (Schwartz et al. 2020; Sevilla et al. 2022); today, some of the most impactful datasets are being created by scraping text from the entire publicly-available internet (Raffel et al., 2020; Together Computer, 2023, Penedo et al., 2023; Soldaini et al. 2024). These are some of the largest text datasets that have ever been built, and they are typically introduced with only a description of how they were made but no documentation of their contents. This is an important distinction, as we are now training models on massive text corpora without knowing what ideas, topics, toxicity, or personal information they contain.

Meanwhile, language models (LMs) have become ubiquitous and are used by people worldwide daily. These AI systems directly impact people's lives, and thus, it has become vitally important to understand their capabilities and drawbacks. Models are only capable of learning from the data they were trained on, but analysis of pretraining corpora is hindered by lack of public release and by their massive size. Work analyzing the contents of web-scale corpora typically focuses on a subset of important dimensions, and there has been almost no work analyzing multiple datasets across the same dimensions. This means that ML practitioners have no practical tools to describe differences between datasets before choosing which one(s) to use.

![](https://cdn.mathpix.com/cropped/2024_06_04_08d472c4aa090b7ab135g-02.jpg?height=282&width=1393&top_left_y=274&top_left_x=363)

Figure 1: An overview of wiMBD. We implement two fundamental capabilities: Count and Search, allowing quick processing and access to large text corpora, which enables a wide range of analyses.

In this work, we propose to investigate the content of large text corpora using WhAT's In MY BIG DATA (WIMBD), a set of tools that enables practitioners to easily explore and quickly analyze large language datasets. We also use this tool to provide some of the first measurements across different web-scale datasets that are directly comparable. WIMBD has two components: (1) a search tool that enables programmatic access to search for documents containing a query using an Elasticsearch $\prod^{1}$ (ES) index. ES is a search engine that allows retrieving strings from a corpus, the documents where they appeared, and the number of times they appeared. (2) a count functionality, built using map-reduce (Dean \& Ghemawat, 2008), allowing quick iteration over an entire dataset and extraction of relevant information, e.g., the character length distribution of documents, duplicates, domain counts, finding personally identifiable information (PII), and more. WIMBD is extendable and can be used to index, count, and analyze other corpora at scale (we benchmark the runtimes in Appendix D.

Using these tools, we perform a set of sixteen analyses on ten different English corpora used to train LMs, including C4 (used to train T5; Raffel et al. 2020), The Pile (used to train Pythia; Gao et al. 2020, Biderman et al., 2022; 2023), and RedPajama (used to reproduce Llama, Touvron et al. 2023, and to train RedPajama-INCITE; Together Computer 2023). We divide our analyses into four categories: (1) data statistics (e.g., number of tokens and domain distribution; $\$ 4.2$; (2) data quality (e.g., most frequent $n$-grams and measuring duplicate documents; 4.3 ; ; (3) community- and societyrelevant measurements (e.g., benchmark contamination and personally identifiable information detection; \$4.4; and (4) cross-corpora analysis (e.g., comparing the most common $n$-gram and document overlap; $\$$ B.4. An illustration of WIMBD is presented in Figure 1

Our work presents many insights on data distribution and anomalies. For example, inspecting the distribution over document lengths exposes anomalies where specific lengths are overrepresented relative to neighboring lengths; these anomalies often correspond to near-duplicate template-generated text or documents arbitrarily truncated to a specific character length. As another example, punctuation sequences are frequently the most common $n$-grams, such as a dash (' - ') repeated ten times as the most common 10-gram in The Pile. WIMBD offers both retrospective documentation and grounding of model behavior to their training data and actionable insights for higher-quality corpora curation.

## 2 BACKGROUND: ON THE IMPORTANCE OF DATA UNDERSTANDING

There have been repeated calls for ML practitioners to provide better data documentation (e.g., McMillan-Major et al., 2023; Bender \& Friedman, 2018; Mitchell et al., 2023, Pistilli et al., 2023; Paullada et al., 2021; Gebru et al., 2021). On the other hand, some of the most impactful ML models are increasingly opaque, specifically with respect to the most important component of recent advancements: data. With the increasingly competitive nature of the field, developers of systems like GPT-4 (OpenAI, 2023) and PaLM-2 (Google, 2023) have been offering little transparency into the most important development decisions, including the sources, size, and contents of their training data.

As web-scale datasets drive this rapid progress in modern ML systems, the gap between data transparency and documentation is more striking than ever (Kaddour et al. 2023). From a technical standpoint, the massive size of these datasets makes analysis of their contents challenging; even if OpenAI or Google shared their training data, it's unclear where to start understanding it in its entirety. Tools like the Data Measurements Tool (Luccioni et al. 2021) and Know Your Data (Google, 2021) work towards improving data documentation, but focus on smaller datasets since the scale of web data leads to significant technical challenges. Our work aims to address this critical missing component.[^0]

While other works support indexing and analyses of large corpora (Piktus et al. 2023a; Marone \& Van Durme, 2023, Simig et al., 2022, Piktus et al., 2023b, Razeghi et al., 2022b), these efforts support a single corpus and often do not support programmatic access to the data or the analysis. Instead, we offer a holistic approach that combines search and counting with a package that allows programmatic access through wrappers on top of the ES API and extendable efficient counting capabilities.

Additional efforts are concerned with the effect of data on model behavior. Longpre et al. (2023) investigate how the composition of LMs' pretraining data influences their downstream performance. Razeghi et al. (2022a) measure high correlation between term frequency and LMs' few-shot reasoning capabilities with those terms. Shin et al. (2022) study the effect of pretraining corpora on in-context abilities. Seshadri et al. (2023) demonstrate that text-to-image models mimic biases from their training data. Akyurek et al. (2022) study fact tracing for identifying pretraining examples that enable a factual assertion, while Guu et al. (2023) offer a training run simulator, which allows making counterfactual queries on what a model would have learned under a different training procedure. These efforts separately built dedicated infrastructure to perform the studies. Our work provides a dedicated interface and tooling that allows performing a wide range of analyses on large-scale corpora, categorizing and offering novel analyses that highlight new insights into these large corpora.

## 3 WIMBD: THE PLATFORM

A core desideratum of WIMBD is to enable quick processing of terabytes of data. As such, we focus on uncomplicated, standard methods from the information retrieval and data management communities. WIMBD is comprised of two basic components: counting and search (retrieval). Fast counting and retrieving enable us to answer fundamental questions about data, as we demonstrate in Section 4. We summarize the framework abilities and types of analyses in Table 1 . We run our experiments using a compute node machine with 224 CPUs and 882GB RAM, and an Elasticsearch cluster for the indexed corpora.

### 3.1 COUNTING

Due to the sparsity of language data and the scale of the data of interest, accurate counting can be challenging. We leverage the map-reduce framework (Dean \& Ghemawat, 2008). We provide two approaches for counting, described below.

Exact Counts The exact counts approach is designed for cases where the number of possible values is tractable and can fit in memory. This fits cases where we are interested in calculating a bound number of variables of interest (e.g., number of documents, $\$ 4.2$. or document length, $\$ 4.3 .3$ ).

Compressed Counts The compressed counts approach is designed for cases where the number of possible values is intractable. For instance, the total 10-grams in a large corpus can be very high, and the memory usage to compute all of them would be overwhelming. Similarly, finding duplicates requires keeping and comparing the strings of all documents in memory. In the case of $C 4$, that would require over $800 \mathrm{~GB}$ of RAM. Instead, we apply a compression function (e.g., hashing, Bloom, 1970) to those values, reducing memory footprint while sacrificing some accuracy (due to hash collisions). For example, when finding the most common 10-grams, we store a table of counts where the keys in the table correspond to hashes of 10 -grams. The hash table size is configurable according to the amount of memory available. The larger the hash table, the smaller the probability of hash collisions and, therefore, the higher the accuracy of the counts. E.g., unigram estimates are more accurate than 10 -gram estimates since the number of possible values is much smaller.

### 3.2 SEARCHING

The second part of WIMBD allows fast text retrieval. For instance, we can get the number of documents mentioning a word or sequence (document frequency). It also allows more complex Boolean queries. While search and retrieval have numerous implementations, such as reverse indices, suffix arrays,

Table 2: Summary statistics of the corpora, along with the models trained on them. * signifies that the model was not trained on the exact version we consider, either due to some data mismatch, or the original data being private.

| Corpus | Origin | Model | Size (GB) | \# Documents | \# Tokens | $\max (\#$ Tokens) | $\min (\#$ Tokens) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| OpenWebText | Gokaslan \& Cohen 2019) | GPT-2* Radford et al., 2019) | $41.2 \quad 2$ | $8,005,939$ | $7,767,705,349$ | 95,139 | 128 |
| C4 | Karrerel al. 2020. | T5 Raffelet al.. $20 \angle 0$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_08d472c4aa090b7ab135g-04.jpg?height=24&width=86&top_left_y=441&top_left_x=1090) | $364,868,892$ | $153,607,833,664$ | 101,898 | 5 |
| mC4-en | Chung et al. 2025 | um15 Chung et al., 2023) | $14,694.0 \quad$ | $3,928,733,374$ | $2,703,077,876,916$ | 181,949 | 1 |
| OSCAR | Abadil elal. $\angle U Z 2$ | BLOOM scaoet al. 2022 | $3,327.3 \quad$ | $431,584,362$ | $475,992,028,559$ | $1,048,409$ | 1 |
| The Pile | Gao et al. $\angle U \angle O$ | GPT-J/Neo \& Pythia BIderman et al., 2023) | {fa5b2aa04-5546-4b54-b528-e942384385b6}1 <br> 1 | $210,607,728$ | $285,794,281,816$ | $28,121,329$ | 0 |
| RedPajama | 10gether Computer 2023) | LLaMA* Touvron etal.. $20 \mathrm{LO} 3$ | $5,602.0$ | $930,453,833$ | $1,023,865,191,958$ | $28,121,329$ | 0 |
| S2ORC | Lo EIal. ZULU | SciBERT Beltagy eÐ³a.., 2019) | 692.7 | $11,241,499$ | $59,863,121,791$ | $376,681 \quad$ | 1 |
| peS2o | Soldaini $\times$ Lo 2023 |  | 504.3 | $8,242,162$ | $44,024,690,229$ | 97,043 | 154 |
| LAION-2B-en | scnunmann el al. 2022 ) | Stable Diffusion* Rombach et al., 2022, | $570.2 \quad 2 \quad$ | $2,319,907,827$ | $29,643,340,153$ | 131,077 | 0 |
| The Stack | Kocetkovetal. $20 \angle 3$, | StarCoder* Li et ai., $20 \angle 3$ | $7,830.8$  | $544,750,672$ | $1,525,618,728,620$ | $26,298,134$ | 0 |

suffix trees for exact match search, and dense retrieval for fuzzy search, in this work, we use ES, an inverted index. We build a wrapper on top of the ES API, allowing tailored and customized searches to fit our analysis requirements. We leave it to future work to explore other search alternatives.

## 4 WIMBD: THE ANALYSES

This section presents analyses conducted in WIMBD, grouped by category. First, we describe the ten corpora considered in this study ( $\$ 4.1$. We then consider four high-level categories, each split into several analyses: data statistics ( $\$ 4.2$ ), data quality ( $\$ 4.3$ ), and community- and society-relevant measurements ( $\$ 4.4$ ). Cross-corpus analyses, as well as elaborations and more analyses are presented in the appendix ( $\S \mathrm{B}$ ). Our analyses are inspired by previous works (Dodge et al., 2021, Gao et al., 2020), but we expand them to multiple corpora, extend the types of analyses, and open-source our modular toolkit to encourage researchers to scrutinize their corpora. We offer the first extensive analyses on ten, combining extension of previous analyses and several novel ones.

### 4.1 CORPORA

We cover ten different large corpora, spanning across text-only (e.g., C4) to image captions (LAION$2 B$-en) and code (The Stack). These corpora have been used in training language models (or similar large-scale models, such as Stable Diffusion; Rombach et al. 2022). A high-level description of these datasets using WIMBD is presented in Table 2 and further details about the construction and origin of these corpora are detailed in Appendix A

### 4.2 DATA STATISTICS

## Main Findings

- Four out of the ten corpora we consider have 'empty' documents (meaning they contain only space-like characters), while The Pile and RedPajama contain the same longest document (with over 28 million tokens) of an encyclopedia.
- While the most common source of webpages in $C 4$ originates from www.nytimes.com it consists of less than $0.05 \%$ of the total web pages, $m C 4$-en most common domain is google.com (over 5\% of the documents), and cdn.shopify.com contributes almost $6 \%$ to the total documents in LAION-2B-en.


### 4.2.1 SUMMARY StATISTICS

We begin by computing some summary statistics and present the results in Table 2. Using the Exact Counts we compute the following high-level statistics of a corpus: (1) size, (2) number of documents, (3) number of tokens. ${ }^{2}$ (4) the size of the longest document, and (5) the size of the shortest document. Out of all corpora, mC4-en is the largest, which takes $14.7 \mathrm{~TB}$ of disk, and 2.7 trillion tokens. After that comes The Stack with a size of 7.8TB, and more than 1.5 trillion tokens. Interestingly, four corpora contain documents with empty strings: LAION-2B-en (81 total), which typically contain a sequence of white spaces. In The Stack $(1,350$ total), RedPajama $(3,877)$, and The[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_08d472c4aa090b7ab135g-05.jpg?height=295&width=1261&top_left_y=270&top_left_x=432)

Figure 2: Domain distribution of the ten most common domains per token for C4, LAION-2B-en, and RedPajama.

Pile (7,533), documents typically contain a mix of special characters that denote spacing (e.g., ' $\backslash \mathrm{n}$ ', or ' $\backslash t$ '). In RedPajama, all of the empty strings are from the arXiv subset. The longest document in The Stack is a json file, with $26,298,134$ tokens from http://jquery.com/ The longest document in The Pile and RedPajama is the same encyclopedia book called "INTERNATIONAL ENCYCLOPEDIA OF THE SOCIAL \& BEHAVIORAL SCIENCES" from the Books3 subset with 28,121,329 tokens.

### 4.2.2 INTERNET DOMAIN DISTRIBUTION

Some corpora contain metadata information about the URL where the documents came from. As such, we employ the Exact Counts functionality, to parse the entire corpus, and extract information from the URLs about the (1) schemas (e.g., http, https), (2) domains (e.g., www.google.com. en.wikipedia.org, etc.), and (3) suffixes (e.g., com, org, de, etc.).

We apply these counts on the corpora that contain this information, namely $C 4$, mC4-en, OSCAR, RedPajama, and LAION-2B-en. Starting with the domain analysis, we perform these counts twice: once when each domain is counted per document (yielding documents per domain) and another where each domain is counted per token (yielding tokens per domain). We present the results of three corpora per token in Figure 2 (and the full results in Appendix B.1). First, we note that $C 4$ contains documents from a diverse set of domains, and even the percentage of the most common one, patents.google.com, is less than $0.05 \%$. On the other hand, in the case of LAION$2 B$-en, cdn.shopify.com is responsible for more than $6 \%$ of the documents. Similarly, arxiv.org is responsible for more than $12 \%$ of the documents in RedPajama. We showcase the results of the domains for the other corpora, as well as the schemas and suffixes in Appendix B. 1 .

### 4.3 DATA QUALITY

## Main Findings

- The most common $n$-grams often correspond to repeated punctuation marks and duplicates.
- While more than $60 \%$ of documents in The Pile are duplicates (unsurprisingly due to oversampling), RedPajama and LAION-2B-en also contain about $50 \%$ duplicate documents.
- Document length distribution reveals interesting (and unexpected) outliers of documents, often resulting from duplicate documents and idiosyncratic data decisions.


### 4.3.1 MoST \& LEAST COMMON $n$-GRAMS

Measuring outliers can reveal interesting insights about a corpus (Mitchell et al. 2023), We explore the most and least common token $n$-grams of each corpus using the Compressed Counts . We compute the $10 \mathrm{~K}$ most common $n$-grams for all corpora, with $n \in\{1,2,3,10\}$. We report the results of the ten most common 10-grams in Table 3 and of the ten most common uni-, bi-, and tri-grams in Table 9 in the Appendix. Identical $n$-grams across corpora are highlighted in the same colors.

The different corpora contain a lot of uncleaned html or markdown format (e.g., ten times '?' or 'amp'), or boilerplate texts such as: ". You can follow any responses to this entry through" in C4, or "( Log Out / Change ) You are commenting using" in OSCAR, and formatting (" [1] [2] [ 3 ] [") in S2ORC and peS2o, which signifies references.

A striking finding from this analysis is the vast repetition of such 10 -grams. For instance, '?', '.', and '-' repeated ten times appear $9,7.2$, and 4.4 million times, respectively, in $C 4$. We perform a manual analysis on the repeating question marks in $C 4$ to better understand the scenarios where they

Table 3: Most common 10-grams in five of the corpora we consider. $n$-grams from the top-10 that occur in more than one corpus are highlighted in the same color.

![](https://cdn.mathpix.com/cropped/2024_06_04_08d472c4aa090b7ab135g-06.jpg?height=347&width=1393&top_left_y=377&top_left_x=363)

appear on the ten consecutive question marks symbols and categorize each appearance into writing, noise, and format occurrence. Analyzing 100 random documents, we found that $68 \%$ of documents use such $n$-grams as part of their writing style (e.g., . . \$6???????????? How is that possible?, or . . So what do u think?????????????????????????). $18 \%$ are due to noise as we could not understand the context or content of the writing (e.g., . . . e ??????????????? kap chit-koa ??), and finally, $14 \%$ of the documents were due to different format styles or issues (e.g., a sequence of question marks following by a 'normal' text, or a sequence of question marks between keywords).

### 4.3.2 DUPLICATES

Previous work has found that duplication can affect the quality of pretraining data, impacting sample efficiency (Lee et al., 2022, Tirumala et al. 2023) and memorization (Carlini et al. 2023). While more recent work finds contradictory evidence on data with less web-scraped text (Biderman et al., 2023), measuring duplication in pretraining data is necessary for future research on its effects. We calculate duplicates by matching documents with an MD5 hash of their texts (using Compressed Counts ). If more than a single document has the same hash, we consider them duplicates ${ }^{3}$ We examine the duplication of document text and URLs within each dataset. While some datasets explicitly deduplicate their content, others do not, and some even oversample some sources.

![](https://cdn.mathpix.com/cropped/2024_06_04_08d472c4aa090b7ab135g-06.jpg?height=344&width=634&top_left_y=1541&top_left_x=369)

Figure 3: Percentages of document and document cluster duplicates in corpora with $>1 \%$ documents duplicated (corresponding to blue and orange bars). Duplicate counts are above bars.
Table 4: Most frequent text duplicates from four datasets with text duplicates, along with their counts. Truncation for visualization is marked by [...].

| Corpus | Text |
| :---: | :---: |
| OSCAR <br> Count: $\mathbf{1 . 8 M}$ | In order to login you must be registered. Register ing <br> takes only a few moments but gives you increas[...] |
| The Pile <br> Count: $\mathbf{3 . 8 K}$ | $\{\backslash$ "info": $\{\backslash n$ "version" : 1, ln "author" : "xcode" $\$ r \\ \}$\backslash \ln \}$ |
| RedPajama | ![](https://cdn.mathpix.com/cropped/2024_06_04_08d472c4aa090b7ab135g-06.jpg?height=55&width=452&top_left_y=1898&top_left_x=1265) |
| Count: $\mathbf{2 1 3 . 9 K}$ | Original $n[\ldots]$ |
| LAION-2B-en <br> Count: 1M | Front Cover |

In Figure 3 we show counts and ratios of duplication across datasets with greater than $1 \%$ documents duplicated, and all datasets are shown in Table 13 in the appendix. These are based on two kinds of counts: (1) the count of documents in all clusters of duplicate text (in blue) and (2) the count of duplicate clusters (in orange). As expected, deduplicated corpora such as $C 4$ have no exact duplicates (as those were filtered out of the corpus). In contrast, The Pile, which intentionally oversampled some data sources, has many duplicates ( $139 \mathrm{M}$ documents belonging to $64.6 \mathrm{M}$ duplicate text clusters). LAION-2B-en has the second highest ratio of duplicate documents (1.25B documents belonging to $342 \mathrm{M}$ duplicate text clusters), perhaps due to the smaller space of short sentences common in[^2]its image "alt text" source. Figure 15 in the appendix showcase the images of the most common duplicates in LAION-2B-en, with the most common images describe mainly receipts.

Table 4 showcases duplicates with the most occurrences in four corpora. These duplicates vary dramatically in length and domain. LAION-2B-en, OSCAR, and RedPajama have clusters with the most occurrences, in the hundreds of thousands and above. Top duplicates in LAION-2B-en are shorter and describe products and website features. OSCAR's top duplicates are all instances of website boilerplate $4^{4}$ RedPajama's top duplicates come from similar templated citation information.

### 4.3.3 DOCUMENT LENGTH DISTRIBUTION

We compute document length distributions with Exact Counts . We expect a smooth distribution over document lengths, and deviation from such a distribution may indicate the presence of artificial documents or near duplicates ${ }^{5}$ We compute the character length distribution and present results for three corpora in Figure 4 (additional results in Appendix B.2.3).

While $C 4$ is free of duplicate documents, it include clusters of template-generated near-duplicate documents exposed by outliers of identical document lengths. Beyond template-generated user-facing copy (e.g., template-generated documents from a reverse phone lookup site, each associated with a unique phone number), we find clusters of template-generated JavaScript snippets, and large collections of unique documents, including

![](https://cdn.mathpix.com/cropped/2024_06_04_08d472c4aa090b7ab135g-07.jpg?height=379&width=491&top_left_y=678&top_left_x=1267)

Figure 4: Distribution over character document lengths (in log-scale) for $C 4$, OSCAR and The Pile. numerous permutations of the same keywords, likely crafted for SEO purposes.

The Pile, featuring the longest documents, has a notable outlier with nearly $1 \%$ of its documents precisely 8,194 characters long. These outliers are derived from the DeepMind Mathematics dataset (Saxton et al., 2019), truncated to fit this length. The Pile also contains a significant number of short template-generated code snippets, e.g., a number of documents (of lengths 9,18 , and 36 tokens) each corresponding to a unique publication in various medical journals, and to auto-generated metadata files (of length 20 tokens) used in the Unity game engine. While OSCAR has no documents shorter than 100 characters, as those were filtered, it contains many near-duplicate documents that correspond to website boilerplate, e.g., template-generated FAQs about how to use the forum software phpBB.

### 4.4 CommunitY- And SocIETY-RELEVAnt MEASUREMENTS

## Main Findings

- Instances of popular benchmarks like GLUE and SuperGLUE, were found in various corpora (e.g., C4 and RedPajama), render them unusable for fair model evaluation.
- Automatic toxicity detection reveals that $1-16.5 \%$ of the documents in the corpora contain toxic language using an automatic classifier and between 0.01-16.6\% using a taxonomy.
- An estimated $200 \mathrm{M}, 4 \mathrm{~B}$, and $97 \mathrm{M}$ of email addresses, phone numbers, and IP addresses were found in the most PII-contaminated corpora per token (mC4-en).


### 4.4.1 BENCHMARK CONTAMINATION

As corpora grow and new evaluation datasets are created, the risk of contamination-where evaluation data are included in a (pre)training corpus-increases. As such, it is important to track contamination (Sainz et al. 2023; Jacovi et al. 2023) $6^{6}$ Using Search, we provide a contamination analysis of 82 datasets for four popular corpora: The Pile, C4, RedPajama, and OSCAR. We consider all datasets[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_08d472c4aa090b7ab135g-08.jpg?height=274&width=1095&top_left_y=275&top_left_x=510)

Figure 5: Most contaminated evaluations test sets out of 82 PromptSource (Bach et al. 2022) datasets.

from PromptSource (Bach et al. 2022), a repository containing prompts for 279 different datasets (as of May 2023). We filter datasets we cannot automatically download, from Huggingface datasets (Lhoest et al. 2021), and datasets that do not have a test split. In addition, we only consider datasets that contain at least two inputs (e.g., natural language inference), leaving us with 82 datasets.

We measure contamination by testing whether all input fields are present in a single document and report the percentage of contaminated examples from the test set. Our contamination evaluation serves as an upper bound of exact-match dataset contamination. We provide more details of our analysis and design choices in Appendix B.3.1

Contaminated datasets We present the results in Figure 5. We showcase all benchmarks whose contamination percentages are at least $5 \%$ in one of the four corpora. We find that RedPajama is the most contaminated dataset out of the four, where in eight out of the 15 corpora, its contamination rate is above $50 \%$, and fully contaminated in the case of COPA (Roemmele et al., 2011). The Pile's contamination rates are lower, but it is also contaminated with a few datasets, such as aesic (Zhang \& Tetreault, 2019), WSC (Levesque et al. 2012) and WIC (Pilehvar \& Camacho-Collados, 2019), which were included in the SuperGLUE evaluation benchmark (Wang et al., 2019).

Most examined datasets were not found in the corpora. It is important to note that while we find some contamination, most of the considered benchmarks do not appear in the corpora we investigated (67 out of the 82 datasets). For instance, Winogrande (Sakaguchi et al. 2021), a large corpus in the style of the Winograd schema, does not appear in any of the examined corpora.

### 4.4.2 PERSONALLY IDENTIFIABLE INFORMATION

PII is "information which can be used to distinguish or trace an individual's identity, such as their name, social security number, biometric records, etc." (Johnson III, 2007). Recent research has sought to extract PII from LMs (Carlini et al., 2021). These attacks highlight that LMs can ingest and reproduce PII contained in their training data, and show the risks of training on data that contains such information, even if the data remains private.

We document three kinds of personally identifiable information in pretraining corpora: phone numbers, email addresses, and IP addresses. We employ regular expres-
Table 5: Extrapolated PII frequencies. Count is the extrapolated frequency and Prec. is our identification precision accuracy, estimated by manual analysis of 100 random examples.

| Corpus | Email Addresses |  | Phone Numbers |  | IP Addresses |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Count | Prec. $\quad$ | Count | Prec. | Count | Prec. |
| OpenWebText | $364 \mathrm{~K}$ | 99 | $533 \mathrm{~K}$ | 87 | $70 \mathrm{~K}$ | 54 |
|  |  | 100 |  |  |  | $43 \quad 4$ |
| $\mathrm{C}^{2}$ | $7.6 \mathrm{M}$ | 99 | $19.7 \mathrm{M}$ | 92 | $796 \mathrm{~K}$ | 6 |
| $\mathrm{mC} 4$ | $201 \mathrm{M}$ | 92 | 4B | 66 | $97.8 \mathrm{M}$ | $44 \quad r \quad x \quad$ |
|  | $19.8 \quad$ | 43 | $38 \mathrm{M}$ | 65 | $4 \mathrm{M}$ | $48 \quad 2 \quad$ |
| RedPajama <br> Redama | $35.2 \mathrm{M}$ | 100 | $70.2 \mathrm{M}$ | 94 | $1.1 \mathrm{M}$ | 30 |
| S2ORC | $630 \mathrm{~K}$ | 100 | $1.4 \mathrm{M}$ | 100 | $0 \mathrm{~K}$ | 0 |
| pe |  | 97 | 227 | 31 | O | 0 |
| LAION-2B-e | 63 | $94 \quad$ | $1 \mathrm{M}$ | 7 | $0 \mathrm{~K}$ | 0 |
| The Stack | $4.3 \mathrm{M}$ | 53 | $45.4 \mathrm{M}$ | 9 | $4.4 \mathrm{M}$ | 55 |

sions corresponding to each PII type using the Exact Counts. We provide more details about our methodology, the regexes, additional results, and error analyses in Appendix B.3.2. We conduct a manual analysis to estimate the precision of these methods on all corpora. The results of this analysis, as well as the extrapolated frequency of these matches, are presented in Table 5. Our identification method is highly precise ( $>80 \%$ precision) for email addresses on eight out of 10 corpora, and for phone numbers on five of the 10 corpora. Overall, most corpora contain a high volume of PII information, varying in type based on the corpus. For instance, RedPajama contain mainly phone numbers (70.2M) and a smaller amount of IP Addresses (1.1M), but $S 2 O R C$ and peS2o contain mainly email addresses ( $630 \mathrm{~K}$ and $418 \mathrm{~K}$, respectively) and no IP addresses were identified. The most common PII across corpora is phone numbers, followed by email addresses and IP addresses (except for The Stack, which has more IP addresses than email addresses: $4.4 \mathrm{M}$ vs. $4.3 \mathrm{M}$, and peS2o, which has more email addresses than phone numbers). Finally, we observe that $m C 4$-en contains the largest amount of PII, also when controlling for the number of tokens (Table 19 in the Appendix).

## 5 DISCUSSION

Data is one of the most poorly understood and studied components in ML research since "everyone wants to do the model work, not the data work" (Sambasivan et al. 2021). Yet, it is one of the most critical factors for successfully training a state-of-the-art language model. While the benefit of increasing model size is evident from the trend of recent years, it is not enough by itself, as the amount and quality of data are crucial (Kaplan et al., 2020).

Data Curation With the increasing data needed to train LMs (and other models for other modalities), it remains challenging to curate high-quality datasets. Besides the technical challenges of composing a large-scale dataset and the decisions that go into making it, these decisions and their influence on the final models are costly to assess due to the high computational resources required to train such models. With WIMBD, we hope to ease the decisions that go into crafting large-scale datasets by surfacing patterns and trends about what goes into them and what is left out from different aspects, such as data quality, community and society measurements, etc. Once decisions upon what data is important, and which should be left out of a dataset, practitioners can filter documents or passages that adhere to such decisions. The curation of the Dolma dataset (Soldaini et al. 2024) that happened while developing this work benefited from iterations over the insights from this work, such as the finding of 'noisy' most-common $n$-grams, and bugs in the initial 'de-duplication' implementation.

Data Documentation Adding to previous works that call for more data documentation, such as Datasheets (Gebru et al. 2021) and Data Statements (McMillan-Major et al., 2023), we argue for the importance of documenting such information. While previous works often focused and tailored the documentation for supervised-style datasets (e.g., "Is there a label or target associated with each instance?", "How was the data associated with each instance acquired?" from Datasheets, and "What are the demographic characteristics of the annotators and annotation guideline developers?" from Data Statements) we call for more tailored documentation of large-scale pretraining corpora ${ }^{7}$ This work offers a superset of the automatic full-corpus analyses proposed by Dodge et al. (2021); Gao et al. (2020), with several additions, categorization, and programmatic interface, allowing better understanding of the content of current and future large text corpora.

Grounding Models to their Training Data Unlike other factors of language model training, such as model architecture or optimizer choice, training data comes in the same natural language format as language model's outputs and thus can be measured and described in all the same ways. As such, the data offers a unique opportunity for grounding models. For instance, a model's ability to recall factual knowledge is derived from its training data (Jiang et al., 2020; Elazar et al., 2021a). On the other hand, models often perform better on frequent occurrences (Razeghi et al. 2022a; McCoy et al. 2023), and on documents similar to models' training data (Longpre et al. 2023). The path to a holistic comprehension of model behavior is through the data, which requires an infrastructure investment to access big datasets and the right abstraction of data attributes.

## 6 CONCLUSION

In this work, we propose WIMBD, a framework for processing and analyzing large text corpora. Using WIMBD, we study ten different corpora that were used to train language models (or vision and language models, such as Stable Diffusion). We uncover interesting insights about these corpora using sixteen different analyses across four aspects: high-level statistics, data quality, communityand society- relevant measurements, and cross-data analysis. For instance, the most common source of texts for the $L A I O N-2 B$-en dataset are the commercial websites Pinterest, Shopify, SlidePlayer, Amazon, and eBay. Regarding data quality, we find that about $50 \%$ of RedPajama and LAION-2Ben's documents are duplicates. In addition, we find that many evaluation benchmarks, including several from GLUE and SuperGLUE, such as WSC, WIC, and RTE, are contaminated due to their appearance in corpora such as RedPajama. Besides the analyses, WIMBD offers an extendable platform for reproducing our analyses on other corpora, developing new ones, and answering research questions about data. We release all the code and artifacts for WIMBD to encourage researchers to adopt and extend our framework and analyze existing and new corpora.[^4]

## ACKNOWLEDGMENTS

We want to thank Ludwig Schmidt, Maarten Sap, and Emma Strubell, and the anonymous reviewers for discussions and feedback on this paper, Elizabeth Salesky for the help with Unicode rendering and getting excited about obscure Unicode characters with me, and Carissa Schoenick, Jon Borchardt, and Johann Dahm for assisting with visuals.

## REFERENCES

Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and BenoÃ®t Sagot. Towards a cleaner documentoriented multilingual crawled corpus. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 4344-4355, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.463

Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. Towards tracing knowledge in language models back to the training data. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 2429-2446, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.findings-emnlp.180. URLhttps://aclanthology.org/2022.findings-emnlp.180.

Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988, 2023. URL https://arxiv.org/abs/ 2301.03988

Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 93-104, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-demo.9. URL https://aclanthology.org/2022.acl-demo.9

Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3615-3620, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1371. URL https://aclanthology.org/D19-1371.

Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587-604, 2018. doi: 10.1162/tacl_a_00041. URL https: //aclanthology.org/Q18-1041

Stella Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile, 2022. URL https:// arxiv.org/abs/2201.07311

Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. PMLR, 2023. URL https: //openreview.net/forum?id=bpRTAnJ8LW.

Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of BigScience Episode \#5 - Workshop on Challenges \& Perspectives in Creating Large Language Models, pp. 95-136, virtual+Dublin,

May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URLhttps://aclanthology.org/2022.bigscience-1.9.

Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7): 422-426, jul 1970. ISSN 0001-0782. URL https://doi.org/10.1145/362686.362692.

Nicholas Carlini, Florian TramÃ¨r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ãšlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633-2650. USENIX Association, August 2021. ISBN 978-1-939133-243. URL https://www.usenix.org/conference/usenixsecurity21/presentation/carliniextracting

Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= TatRHT_1cK

Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, and Noah Constant. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=kXwdL1cWOAi

Jeffrey Dean and Sanjay Ghemawat. Mapreduce: Simplified data processing on large clusters. Commun. ACM, 51(1):107-113, jan 2008. URL https://doi.org/10.1145/1327452.1327492

Jesse Dodge, Maarten Sap, Ana MarasoviÄ‡, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1286-1305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlpmain.98. URL https://aclanthology.org/2021.emnlp-main. 98

Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich SchÃ¼tze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012-1031, 2021a. URL https://aclanthology.org/2021.tacl-1.60

Yanai Elazar, Hongming Zhang, Yoav Goldberg, and Dan Roth. Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10486-10500, Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.819. URL https: //aclanthology.org/2021.emnlp-main.819.

Ali Emami, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung. An analysis of dataset overlap on Winograd-style tasks. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 5855-5865, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.515. URL https://aclanthology.org/2020.coling-main.515

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. URL https://arxiv.org/abs/2101.00027.

Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal DaumÃ© III, and Kate Crawford. Datasheets for datasets. Commun. ACM, 64(12):86-92, nov 2021. ISSN 0001-0782. doi: 10.1145/3458723. URL https://doi.org/10.1145/3458723

Aaron Gokaslan and Vanya Cohen. Openwebtext corpus, 2019. URL https:// skylion007.github.io/OpenWebTextCorpus/.

Google. Know your data, 2021. URL https://github.com/pair-code/knowyourdata.

Google. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. URLhttps://arxiv.org/ $\mathrm{abs} / 2305.10403$.

Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga Bolukbasi. Simfluence: Modeling the influence of individual training examples by simulating training runs. arXiv preprint arXiv:2303.08114, 2023. URLhttps://arxiv.org/abs/2303.08114

Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 5075-5084, Singapore, December 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.emnlp-main.308.

Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020. doi: 10.1162/tacl_a_00324. URL https://aclanthology.org/2020.tacl-1.28

Clay Johnson III. Us office of management and budget memorandum m-07-16, 2007. URL https: //georgewbush-whitehouse.archives.gov/omb/memoranda/fy2007/m07-16.pdf.

Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. URL https://arxiv.org/abs/2307.10169.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.

Denis Kocetkov, Raymond Li, Loubna Ben allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos MuÃ±oz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro Von Werra, and Harm de Vries. The stack: 3 TB of permissively licensed source code. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/ forum?id=pxpbTdUEpD.

Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris CallisonBurch, and Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8424-8445, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/2022.acllong.577.

Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12, pp. 552-561. AAAI Press, 2012. ISBN 9781577355601. URL https: //dl.acm.org/doi/10.5555/3031843.3031909.

Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Å aÅ¡ko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, ClÃ©ment Delangue, ThÃ©o MatussiÃ¨re, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, FranÃ§ois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 175-184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https: //aclanthology.org/2021.emnlp-demo.21.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. URLhttps://arxiv.org/abs/2305.06161

Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4969-4983, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://www.aclweb.org/anthology/ 2020.acl-main. 447 .

Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity. arXiv preprint arXiv:2305.13169, 2023. URLhttps://arxiv.org/abs/2305.13169

Sasha Luccioni, Yacine Jernite, and Margaret Mitchell. Data measurements tool, 2021. URL https://huggingface.co/blog/data-measurements-tool.

Marc Marone and Benjamin Van Durme. Data portraits: Recording foundation model training data. arXiv preprint arXiv:2303.03919, 2023. URL https://arxiv.org/abs/2303.03919.

R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve. arXiv preprint arXiv:2309.13638, 2023. URLhttps://arxiv.org/abs/2309.13638

Angelina McMillan-Major, Emily M. Bender, and Batya Friedman. Data statements: From technical concept to community practice. ACM J. Responsib. Comput., may 2023. doi: 10.1145/3594737. URL https://doi.org/10.1145/3594737.

Margaret Mitchell, Alexandra Sasha Luccioni, Nathan Lambert, Marissa Gerchick, Angelina McMillan-Major, Nazneen Ozoani, Ezinwanne Rajani, Tristan Thrush, Yacine Jernite, and Douwe Kiela. Measuring data. In arXiv, 2023. URL https://arxiv.org/abs/2212.05129

Anthony Moi and Nicolas Patry. HuggingFace's Tokenizers, April 2023. URLhttps://github.com/ huggingface/tokenizers.

OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.org/ $\mathrm{abs} / 2303.08774$

Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. Data and its (dis)contents: A survey of dataset development and use in machine learning research. In Patterns, 2021. URL https://www.sciencedirect.com/science/article/pii/ S2666389921001847.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. URL https://arxiv.org/abs/2306.01116.

Aleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo LaurenÃ§on, GÃ©rard Dupont, Sasha Luccioni, Yacine Jernite, and Anna Rogers. The ROOTS search tool: Data transparency for LLMs. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 304-314, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-demo.29. URL https://aclanthology.org/2023.acl-demo.29

Aleksandra Piktus, Odunayo Ogundepo, Christopher Akiki, Akintunde Oladipo, Xinyu Zhang, Hailey Schoelkopf, Stella Biderman, Martin Potthast, and Jimmy Lin. GAIA search: Hugging face and pyserini interoperability for NLP training data exploration. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 588-598, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-demo.57. URL https://aclanthology.org/2023.acl-demo.57.

Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1267-1273, Minneapolis, Minnesota, June

2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1128. URL https: //aclanthology.org/N19-1128.

Giada Pistilli, Carlos MuÃ±oz Ferrandis, Yacine Jernite, and Margaret Mitchell. Stronger together: On the articulation of ethical charters, legal tools, and technical documentation in ml. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT '23, pp. 343-354, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701924. doi: 10.1145/3593013.3594002. URLhttps://doi.org/10.1145/3593013.3594002.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog post, 2019. URL https://openai.com/ research/better-language-models

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.

Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot numerical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 840-854, Abu Dhabi, United Arab Emirates, December 2022a. Association for Computational Linguistics. URL https://aclanthology.org/2022.findingsemnlp. 59

Yasaman Razeghi, Raja Sekhar Reddy Mekala, Robert L Logan Iv, Matt Gardner, and Sameer Singh. Snoopy: An online interface for exploring the effect of pretraining term frequencies on few-shot LM performance. In Wanxiang Che and Ekaterina Shutova (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 389-395, Abu Dhabi, UAE, December 2022b. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-demos.39.

Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium: logical formalizations of commonsense reasoning, pp. 90-95, 2011. URL https://aaai.org/papers/02418-choice-of-plausible-alternatives-anevaluation-of-commonsense-causal-reasoning/.

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022.

Oscar Sainz, Jon Ander Campos, Iker GarcÃ­a-Ferrero, Julen Etxaniz, and Eneko Agirre. Did chatgpt cheat on your test?, Jun 2023. URL https://hitz-zentroa.github.io/lm-contamination/ blog/

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99-106, aug 2021. URL https://doi.org/10.1145/3474381

Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. "everyone wants to do the model work, not the data work": Data cascades in high-stakes ai. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI '21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380966. doi: 10.1145/3411764.3445518. URL https://doi.org/10.1145/3411764.3445518.

David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2019. URLhttps://openreview.net/forum?id=H1gR5iR5FX

Teven Le Scao, Angela Fan, Christopher Akiki, Elizabeth-Jane Pavlick, Suzana Ili'c, Daniel Hesslow, Roman Castagn'e, Alexandra Sasha Luccioni, Franccois Yvon, Matthias GallÃ©, Jonathan Tow, Alexander M. Rush, Stella Rose Biderman, Albert Webson, Pawan Sasanka Ammanamanchi,

Thomas Wang, BenoÃ®t Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris C. Emezue, Christopher Klamm, Colin Leong, Daniel Alexander van Strien, David Ifeoluwa Adelani, Dragomir R. Radev, Eduardo Gonz'alez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, GÃ©rard Dupont, GermÃ¡n Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina, Hieu Trung Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Josephine L. Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro von Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan Dey, Manuel Romero MuÃ±oz, Maraim Masoud, Mar'ia Grandury, Mario vSavsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad Ali Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla A. Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto L'opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, S. Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault FÃ©vry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franccois Lavall'ee, RÃ©mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, StÃ©phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur'elie N'ev'eol, Charles Lovering, Daniel H Garrette, Deepak R. Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Xiangru Tang, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, S. Osher Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenvek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ananda Santa Rosa Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Olusola Ajibade, Bharat Kumar Saxena, Carlos MuÃ±oz Ferrandis, Danish Contractor, David M. Lansky, Davis David, Douwe Kiela, Duong Anh Nguyen, Edward Tan, Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza, Frankline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jan Passmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort, LÃ­via Macedo Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, M. K. K. Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran An, R. P. Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Kumar Singh, Benjamin Beilharz, Bo Wang, Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, ClÃ©mentine Fourrier, Daniel Le'on Perin'an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully A. Burns, Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc PÃ mies, MarÃ­a Andrea

Castillo, Marianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg, M Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patricia Haller, R. Chandrasekhar, R. Eisenberg, Robert Martin, Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Pratap Bharati, T. A. Laud, Th'eo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan Xu, Ying Xu, Yun chao Xu, Zhee Xao Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. ArXiv, abs/2211.05100, 2022. URL https://arxiv.org/abs/2211.05100

Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=M3Y74vmsMcY

Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. Commun. ACM, 63(12): 54-63, nov 2020. ISSN 0001-0782. URL https://doi.org/10.1145/3381831.

Preethi Seshadri, Sameer Singh, and Yanai Elazar. The bias amplification paradox in text-to-image generation. arXiv preprint arXiv:2308.00755, 2023. URL https://arxiv.org/abs/2308.00755.

Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. Compute trends across three eras of machine learning. In 2022 International Joint Conference on Neural Networks (IJCNN), pp. 1-8, 2022. URL https://ieeexplore.ieee.org/abstract/ document/9891914

Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-Woo Ha, and Nako Sung. On the effect of pretraining corpora on in-context learning by a large-scale language model. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5168-5186, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.380. URL https: //aclanthology.org/2022.naacl-main.380.

Daniel Simig, Tianlu Wang, Verna Dankers, Peter Henderson, Khuyagbaatar Batsuren, Dieuwke Hupkes, and Mona Diab. Text characterization toolkit (TCT). In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 72-87, Taipei, Taiwan, November 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.aacl-demo.9

Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI, 2023. ODC-By,https://github.com/allenai/pes2o

Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Raghavi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, A. Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hanna Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. URLhttps://arxiv.org/abs/2402.00159

Nishant Subramani, Sasha Luccioni, Jesse Dodge, and Margaret Mitchell. Detecting personal information in training corpora: an analysis. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pp. 208-220, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.trustnlp-1.18. URL https://aclanthology.org/2023.trustnlp-1.18.

MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.

Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S Morcos. D4: Improving llm pretraining via document de-duplication and diversification. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.

Together Computer. RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset, April 2023. URL https://github.com/togethercomputer/RedPajama-Data.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023. URLhttps://arxiv.org/abs/2302.13971.

Unicode. Unicode Text Segmentation, Aug 2023. URLhttps://unicode.org/reports/tr29/

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf

Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 483-498, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL https: //aclanthology.org/2021.naacl-main.41.

Rui Zhang and Joel Tetreault. This email could save your life: Introducing the task of email subject line generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 446-456, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1043. URL https://aclanthology.org/P19-1043.

Xuhui Zhou, Maarten Sap, Swabha Swayamdipta, Yejin Choi, and Noah Smith. Challenges in automated debiasing for toxic language detection. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 3143-3155, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eaclmain.274. URLhttps://aclanthology.org/2021.eacl-main. 274
