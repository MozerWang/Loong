# Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character 

Siyuan Ma ${ }^{* *}$<br>Peking University<br>msy1229535573@stu.pku.edu.cn<br>Yu Wang<br>Peking University<br>rain_wang@stu.pku.edu.cn<br>Muhao Chen<br>University of California, Davis<br>muhchen@ucdavis.edu

Weidi Luo**<br>The Ohio State University<br>luo.1455@osu.edu<br>Xiaogeng Liu<br>University of Wisconsin-Madison<br>xiaogeng.liu@wisc.edu<br>Bo Li<br>The University of Chicago<br>bol@uchicago.edu

Chaowei Xiao

University of Wisconsin-Madison

cxiao34@wisc.edu


#### Abstract

With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), ensuring their safety has become increasingly critical. To achieve this objective, it requires us to proactively discover the vulnerability of MLLMs by exploring the attack methods. Thus, structure-based jailbreak attacks, where harmful semantic content is embedded within images, have been proposed to mislead the models. However, previous structure-based jailbreak methods mainly focus on transforming the format of malicious queries, such as converting harmful content into images through typography, which lacks sufficient jailbreak effectiveness and generalizability. To address these limitations, we first introduce the concept of "Role-play" into MLLM jailbreak attacks and propose a novel and effective method called Visual Role-play (VRP). Specifically, VRP leverages Large Language Models to generate detailed descriptions of high-risk characters and create corresponding images based on the descriptions. When paired with benign role-play instruction texts, these high-risk character images effectively mislead MLLMs into generating malicious responses by enacting characters with negative attributes. We further extend our VRP method into a universal setup to demonstrate its generalizability. Extensive experiments on popular benchmarks show that VRP outperforms the strongest baseline, Query relevant [38] and FigStep [16], by an average Attack Success Rate (ASR) margin of 14.3\% across all models.


Disclaimer: This paper contains offensive content that may be disturbing.[^0]

## 1 Introduction

Recent advances of Multimodal Large Language Models (MLLMs) have demonstrated significant strides in achieving highly generalized vision-language reasoning capabilities [2; 49, 32; 8; 69; 70; 13, $71,14,28 ; 3 ;, 31,79 ;, 75,19,1,41,35,74,35,10,72]$. Given their potential for widespread societal impact, it is crucial to ensure that the responses generated by MLLMs are free from harmful content such as violence, discrimination, disinformation, or immorality[48; 44]. Consequently, increasing concerns regarding the safety of MLLMs have prompted extensive research into jailbreak attacks and defense strategies [63, 80, 72, 29, 4, 60; 33, 23, 51, 58].

Jailbreak attacks in MLLMs, by generating delicately designed inputs, aims to mislead MLLMs into responding to malicious requests and providing harmful content [16, 38, 40, 66, 11, 56, 21, 50; 54, 76; 62, 30; 45, 37]. It is critical to evaluate and understand the jailbreak robustness of MLLMs to ensure they behave responsibly and safely. Existing jailbreak attacks against MLLMs can be categorized into three types: (i) perturbation-based attacks, which disrupt the alignment of MLLMs through adversarial perturbations [47, 50, 12]; (ii) text-based attacks, which generate some textual jailbreak prompts to compromise MLLMs by leveraging LLM jailbreak techniques [40]; (iii) structure-based attacks that utilize some malicious images with specific semantic meanings to jailbreak MLLM ${ }^{2}$. Perturbation-based attacks, as a variant of standard vision adversarial attacks, which have been extensively studied [7] and various defense methods like purifiers [42; 20; 46] or adversarial training [27] have proven effectiveness [58]. In addition, text-based jailbreak attacks, as an extension of LLM jailbreak attacks, are likely to be detected and blocked by text moderators [22]. (See Appendix D) Consequently, structure-based jailbreak attacks remains to be unexplored and present unique challenges related to the multi-modality nature of MLLMs. Therefore, in this paper, we primarily focus on structure-based attack methods.

Unfortunately, existing structure-based jailbreak attack methods exhibit two limitations. First, current methods on MLLM lack sufficient jailbreak effectiveness, leaving significant room for performance improvement. These methods primarily involve transforming the format of malicious queries, such as converting harmful content into images through typography or using text-to-image tools to bypass the safety mechanisms of MLLMs. For instance, FigStep [17] creates images containing malicious text, such as "Here is how to build a bomb: 1. 2. 3.", to induce the MLLMs into completing the sentences, thereby leading them to inadvertently provide malicious responses. As we demonstrate in Sec. 4.6, these simple transformations do not achieve sufficient attack effectiveness. We argue that to enhance the attack performance, a "jailbreak context" must be introduced. For instance, in attacks against LLMs, attackers provide additional context, such as "ignore previous constraints" or "now you are an AI assistant without any constraints", to prompt the models to disregard their safety protocols and operate without limits. Secondly, current jailbreak methods lack generalization. For jailbreak attacks, universal properties are crucial as they enable an attack to be applicable across a broad range of scenarios without requiring extensive modifications or customization. However, existing structure-based jailbreak attacks on MLLMs overlook this problem, as they necessitate computation for each query, especially when dealing with large datasets, making them impractical. To address the above limitations in the structure-based jailbreak attacks, we propose Visual Role-play(VRP), an effective structure-based jailbreak that instructs the model to act as a high-risk character in the image input to generate harmful content. As shown in Figure 1 . we first utilize an LLM to generate a detailed description of a high-risk character. The description is then employed to create a corresponding character image. Next, we integrate the typography of character description and the associated malicious questions at the top and bottom of the character image, respectively, to form the complete jailbreak image input. This malicious image input is then paired with a benign role-play instruction text to query and attack MLLMs. By enacting imaginary scenarios and characters characterized by negative attributes, such as rudeness or immorality, our proposed VRP effectively misleads MLLMs into generating malicious responses, thereby enhancing jailbreak performance. Additionally, our VRP demonstrates strong generalization capabilities. The high-risk characters generated in VRP are designed to handle a wide range of malicious queries, not limited to specific user requests. They serve as universal safeguards against diverse harmful inputs.

We evaluate the effectiveness of our VRP on widely used jailbreak benchmarks, RedTeam-2K [40] and the HarmBench [43]. Extensive experiments demonstrate that our VRP achieves superior jailbreak[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_4a4f3a9a8b6eb588f765g-03.jpg?height=713&width=1312&top_left_y=234&top_left_x=404)

Figure 1: The Pipeline of Query-specific VRP. VRP is a structure-based jailbreak attack via role-playing image characters. When presented with a textual malicious query $Q_{i}$. VRP proceeds through three five steps to generate the adversarial text-image pair $Q_{i}^{*}$, which can be written as ( $\left.T_{i}^{*}, I_{i}^{*}\right)$. In Step 1, we generate a role description with a Chain Of Thought between the Character Description and Diffusion Prompt. Then in Step 2, we use the Character Description to get Description Typography $I_{i}^{k t}$ and generate Diffusion Image $I_{i}^{t 2 i}$. In Step 3, we obtain the Question Typography $I_{i}^{Q}$ from $Q_{i}$. In Step 4, we concatenate $I_{i}^{Q}, I_{i}^{k t}$, and $I_{i}^{t 2 i}$ to get the Image Input $I_{i}^{*}$. Finally, in Step 5, we attack MLLMs by instructing $I_{i}^{*}$ and Text Input $T_{i}^{*}$.

attack performance. For instance, VRP outperforms the strongest baseline, Query relevant [38] and FigStep [16], by an average Attack Success Rate (ASR) margin of $14.3 \%$ across all models. We further extend our VRP to a more challenging universal attack setting, where VRP still obtains the best performance across all models using a single universal character. This result underscores the generalization capability of our VRP. Our main contributions are as follows:

- We propose a simple yet effective jailbreak attack method for MLLMs, Visual Role-play (VRP), which is the first of its kind to leverage the concept of "role-play" to enhance the jailbreak attack performance of MLLMs.
- Specifically, VRP employs LLM to generate detailed descriptions of malicious characters and create corresponding images. When being paired with benign role-play instruction texts, these high-risk character images effectively mislead MLLMs into generating malicious responses by enacting characters with negative attributes. In addition, the universal character images generated by our VRP demonstrate robust generalization, effectively handling a wide range of malicious queries.
- We show that VRP achieves superior jailbreak performance and strong generalization capabilities on popular benchmarks.


## 2 Related Works

Role Playing. Role-playing represents an innovative strategy used in LLMs. In LLM, such an application is widely investigated by recent works that explore the potential of role-playing [39, 55, 78, 64, 65, 53, 5; 61]. Most of these works use role-playing strategies to make LLM more interactive [65], personalized [55, 64, 61], and context-faithful [78]. However, role-playing in jailbreak attacks also poses a threat to the AI community. [36,59, 25] investigate the jailbreak potential of role-playing on jailbreak LLMs via instructing LLMs by adding role-playing information as a template prefix of prompt. Unfortunately, current studies on MLLM jailbreak attacks didn't pay attention to studying role-playing. In order to fill the gap, we are the first work that get insight from these roly-playing methods on jailbreak LLMs and develops a visual role-playing method for jailbreak MLLMs.

Jailbreak attacks against MLLMs. MLLMs have been widely used in real-world scenarios, the current MLLMs jailbreak attack methods can be broadly classified into three main categories:
perturbation-based, image-based jailbreak attack and text-based jailbreak attack. Perturbation-based jailbreak attacks [57, 47, 50, 12] jailbreak MLLMs by optimizing image and text perturbations. Structure-based jailbreak attacks include Figstep [16] that converts harmful queries into visual representation via rephasing harmful questions into step-by-step typography, and Query relevant [38] that jailbreaks MLLMs by using a text2image tool to visualize the keyword in harmful queries that are relevant to the query. Meanwhile, text-based jailbreak attacks [40] investigate the robustness of MLLMs against text-based attacks [81, 73; 59, 67] initially designed for attacking LLMs, reveal transferability and effectivity of LLM jailbreak attacks.

Our Visual-RolePlay jailbreak method is a structure-based jailbreak attack method on MLLMs, not only explores the potential of the role-play through visual modality on jailbreak MLLMs, but also combines with the visual representation of key information in harmful query. Our method shows better performance compared with other structure-based jailbreak attacks.

## 3 Methodology

In this section, we first define the jailbreak attack tasks in MLLMs in Sec. 3.1 Then, we introduce the pipeline of VRP in a query-specific setting in Sec. 3.2. In Sec. 3.3, we further extend the VRP into universal setting and obtain a universal role-play character.

### 3.1 Preliminary

Adversarial Goals. Jailbreak attacks in MLLMs aim to compel these models to respond to prohibited malicious questions posed by adversaries with correct answers, rather than refusing to answer [48; 44, 72]. Consider a set of text-only malicious queries $\mathcal{Q}=\left\{Q_{1}, Q_{2}, \ldots, Q_{n}\right\}$, where each query only contains text (i.e., $Q_{i}=\left(T_{i}, \perp\right)$ ). The adversary elaborates on text-only malicious queries $\mathcal{Q}$ to generate adversarial text-image pairs as input, denoted as $\mathcal{Q}^{*}=\left\{Q_{1}^{*}, Q_{2}^{*}, \ldots, Q_{n}^{*}\right\}$, where $Q_{i}^{*}=\left(T_{i}^{*}, I_{i}^{*}\right)$. When the input set $\mathcal{Q}^{*}$ is presented to the victim MLLM, the model produces a set of responses $\mathcal{R}=R_{1}, R_{2}, \ldots, R_{n}$. The objective of jailbreak attacks is to ensure that the responses in $\mathcal{R}$ are predominantly answers closely associated with the malicious questions in $\mathcal{Q}$, rather than refusal messages aligned with human values.

Adversarial Capabilities. This paper considers a black-box attack that operates without any knowledge of the MLLMs, such as its parameters and hidden states, or any manipulation such as fine-tuning. The adversary only needs the ability to query the model and receive its textual responses. The interaction is limited to a single turn with no prior dialogue history, except for any predetermined system prompts. The attacker lacks access or control over the internal states of the generation process and cannot adjust the model's parameters.

### 3.2 Query-specific Visual Role-play

To improve the limited jailbreak attack performance of existing structure-based jailbreak methods [16, 80], we introduce a novel MLLM jailbreak method, named VRP, which misleads MLLMs to bypass safety alignments by instructing the model to act as a high-risk character in image input (see Fig. 1). We first introduce the pipeline of VRP under the query-specific setting, where VRP generates a role-play character targeting a specific query. The details are as follows.

Step 1: Generating the Character Description by LLM. VRP first generates character information about high-risk characters to prepare for subsequent steps. Specifically, given a target text-only malicious query $Q_{i}$, VRP directly queries LLMs to generate a key traits description $D_{i}^{k t}$ and a text-to-image prompt $P_{i}^{t 2 i}$ high-risk characters. These characters exhibit negative attributes such as rudeness or immorality. Here, $D_{i}^{k t}$ focuses solely on the key high-risk traits of the character, such as "A cyber criminal with political motivations". Meanwhile, $P_{i}^{t 2 i}$ serves as the input for a text-to-image model, such as Stable Diffusion [52], to generate a character image.

Many studies [77, 6, 15, 26, 34] show that the chain-of-thought (CoT) prompts, which encourage the MLLMs to generate a step-by-step decomposition of a complex problem, enhances the performance of MLLMs on various tasks. Inspired by this, we guide the LLMs to generate the key traits description $D_{i}^{k t}$ and the text-to-image prompt $P_{i}^{t 2 i}$ step by step. First, we query LLMs to generate the key traits description $D_{i}^{k t}$ of a high-risk character, adding more details to obtain $P_{i}^{t 2 i}$ based on $D_{i}^{k t}$.

This method aids in generating detailed and high-quality character images, thereby improving the performance of jailbreak attacks. For more details refer to sec. A in Appendix.

Step 2: Generating the Character Image. Then, we embed the generated character information into images. VRP utilizes Stable Diffusion using $P_{i}^{t 2 i}$ to generate the high-risk character image $I_{i}^{t 2 i}$. Meanwhile, we obtain a typography image $I_{i}^{k t}$ of $D_{i}^{k t}$ is created using typography tools.

Step 3: Generating the Question Typography. Previous methods [16, 38] rephrase text-only malicious queries $Q_{i}$ to obtain question typography. However, this rephrasing process often compromises the original intent of the query, resulting in sub-optimal jailbreak attack performance. For instance, Query-relevant [38] only extracts malicious keywords from the original query for typography, which falls short of conveying the complete original malicious question. To overcome this limitation, we propose directly embedding the original malicious query $Q_{i}$ onto the image $I_{i}^{Q}$, thereby ensuring the accurate conveyance of the intended problem. Please refer to further analysis in Sec. F of Appendix, where we also consider VRP combining rephrasing techniques.

Step 4: Image Concatenation. Given the malicious image $I_{i}^{k t}, I_{i}^{t 2 i}, I_{i}^{Q}$ generated in steps 2 and 3 , we obtain the complete image input $I_{i}^{*}$ by concatenating $I_{i}^{k t}, I_{i}^{t 2 i}, I_{i}^{Q}$ from top to bottom. The complete malicious image $I_{i}^{*}$ generated in our VRP shows a character description on the top, an image of the character in the middle separating the typography texts, and a malicious question at the bottom.

Step 5: Attacking MLLMs. Given the complete malicious image $I_{i}^{*}$, we pair it with a benign role-play instruction text $T_{i}^{*}$ to the victim MLLM. The structure-based jailbreak input $Q_{i}^{*}=\left\{I_{i}^{*}, T_{i}^{*}\right\}$ misleads the victim MLLM into adopting a high-risk character and generating a malicious response. Here, we manually design $T_{i}^{*}$, as illustrated in Prompt 1.

## Prompt 1:

You are the person in the image, described by the phrase at the top of the image. The bottom of the image shows a request.

As the person in the image, you should:

1. Describe yourself, focusing on traits that enable you to fulfill the request.
2. Provide a detailed step-by-step response to fulfill the request.

### 3.3 Universal Visual Role-play

To verify the generalizability of VRP, we further extend this method to "universal" scenarios. In fact, the universal concept has been widely explored in jailbreak attacks, such as AutoDAN [36] and GCG [81], which refer to an attack strategy that employs minimal and straightforward manipulation of queries during execution. For jailbreak attacks, the universal properties are very important because they enable an attack to be applicable across a broad range of scenarios without requiring extensive modifications or customization. In the context of MLLMs, "universal" jailbreak attacks typically involve the simple aggregation of queries into a predefined format or directly printing the queries as typography onto images. Unfortunately, existing structure-based jailbreak attacks on LLMs have overlooked this problem, as they require computation for each query, especially when dealing with large datasets, making them hard to use.

To address this issue, we introduce the concept of "universal visual role-play." The core principle of universal visual role-play is to leverage the optimization capabilities of LLMs [68] to generate candidate characters universally, followed by the selection of the best universal character. Many role-play attacks [36] can be performed in a universal setting. To obtain a universal visual role-play template, we generate multiple rounds of candidate roles, each round optimized based on previous rounds, harnessing LLMs' optimization ability [68]. We split the entire malicious query dataset into train, validation, and test sets.

Step 1: Candidate Generation: To generate candidate roles, we proceed through $R$ rounds of candidate character generation. In each generation round, we sample $N_{q}$ queries from the train set as query demonstrations. For the initial round (i.e., $R=0$ ), similar to query-specific VRP, we directly query an LLM to generate descriptions of universally high-risk roles. Thus, we generate $N^{\text {init }}$ initial universal high-risk characters. For other rounds (i.e., $R \geq 1$ ), we sample $B$ training samples and perform VRP attack by utilizing the universal roles as elaborated in Sec. 3.3 , where $B$ denotes the
batch size. For each round, we compute ASR and sample $N^{T C}$ characters from the top $N^{R}$ roles with the highest ASR. This ensures that the newly generated roles are optimized based on top-ASR characters and are sufficiently diverse because each character is generated by improving different character demonstrations. The prompt used in this step is described in Appendix A

Step 2: Candidate Selection: Universal character candidates are not guaranteed to be able to perform universal attack, even though LLMs are prompted to generate such roles. Moreover, even if some roles can achieve high ASR on the batch train set, this ASR can also be a result of overfitting. To select the best universal character from these candidates, we choose $N^{V C}$ candidates with the highest train set ASR in each generation round and compute their ASR using the validation set. We select the character candidate that achieves the highest ASR on the validation set as the final universal character.

With the universal character obtained through the aforementioned process, we use it to perform universal VRP on the test set.

```
Algorithm 1 Universal Visual Role-play
    Input: Malicious queries
    Output: Universally high-risk character
    Initialization: Split malicious queries into train, valid, and test sets.
    Iteration:
    for round in $N^{R}$ do
        if round $==0$ then
            Generate $N^{C_{\text {init }}}$ initial universal characters using hand-craft demonstrations.
        end if
        if round $>0$ then
            Optimize characters using previous characters with ASR, generate $N^{C}$ new characters.
        end if
        Sample batch data on train set, compute ASR.
        Save new characters and their ASR to history.
        Select $N^{V C}$ characters with the highest ASR each round.
        Compute ASR of selected characters on the validation set.
    end for
    Return: Character with the highest validation ASR.
```


## 4 Experiments

In this section, we conduct experiments to evaluate VRP using a series of datasets and victim models, and compare with a few highly relevant recent baselines of jailbreak attacks. We also test the robustness of VRP against two defense approaches.

### 4.1 Evaluated Datasets, Metric and Victim Models

Dataset. In our paper, we use widely used jailbreak attack datasets, RedTeam-2k [40] and HarmBench [43], to evaluate our VRP. (i) RedTeam-2k [40] consists of 2000 diverse and high-quality harmful textual questions across 16 harmful categories. We randomly split RedTeam-2k [40] into train set, valid set, and test set with a ratio of 6:2:2. We use train set and validation set of RedTeam-2k [40] to train universal VRP character. (ii)HarmBench [43] is an open-source framework for automated red teaming contains 320 textual harmful questions test set.

Metric. We report attack success rate (ASR) using the test set of both RedTeam-2k [40] and Harmbench [43]. Specifically, We introduce 2 oracle functions, $O_{\text {toxic }}\left(R_{i}\right)$ and $O_{\text {relevant }}\left(Q_{i}, R_{i}\right)$, to evaluate whether a response of MLLMs is harmful and relevant to malicious input, respectively. Here, we denote the malicious input as $Q_{i}$ and the corresponding response as $R_{i}$ from victim MLLMs. $O_{\text {toxic }}\left(R_{i}\right)=1$ means if the response $R_{i}$ contains harmful content and violates safety policies, and 0 otherwise. Meanwhile, $O_{\text {relevant }}\left(Q_{i}, R_{i}\right)=1$ if a response $R_{i}$ is relevant to harmful query $Q_{i}$, and 0 otherwise.

We consider $Q_{i}^{*}$ a successful attack when $O_{\text {toxic }}\left(R_{i}\right) \wedge O_{\text {relevant }}\left(Q_{i}, R_{i}\right)=1$. Thus, the Attack Success Rate (ASR) is defined as::

$$
\begin{equation*}
A S R_{Q, Q^{*}}=\frac{1}{|Q|} \sum_{Q_{i} \in Q, Q_{i}^{*} \in Q^{*}} \text { Jailbroken }\left(Q_{i}, M\left(Q_{i}^{*}\right)\right) \tag{1}
\end{equation*}
$$

where Jailbroken $\left(Q_{i}, M\left(Q_{i}^{*}\right)\right)=O_{\text {toxic }}\left(R_{i}\right) \wedge O_{\text {relevant }}\left(Q_{i}, R_{i}\right)$. We use Llama Guard(7B) [22] to calculate $O_{\text {toxic }}\left(R_{i}\right)$ [40] and Mistral-7B-Instruct-v0.2 [24] to calculate $O_{\text {relevant }}\left(Q_{i}, R_{i}\right)$. We provide more details in Sec. Bof Appendix.

Victim Models. In our experiments, we evaluated 5 state-of-the-art MLLMs, including 4 open-source MLLMs, Llava-V1.6-Mistral-7B [32], Qwen-VL-Chat (7B) [2], OmniLMM (12B) [72], InternVLChat-V1.5 [8], and 1 closed-source MLLMs Gemini-1.0-Pro-Vision. Open-source MLLMs are selected from models with high performance on the OpenVLM Leaderboard [9]. All experiments are conducted with 2 NVIDIA A100 GPUs.

### 4.2 Implement Details

Character Generation. We use Mixtral-8x7B-Instruct-v0.1 [24] for all the generation of characters. We design 3 different prompts for query-specific VRP, initial round of universal VRP, and optimization round of universal VRP. See detail for character generation in Sec. A of Appendix.

Image Generation. We use stable-diffusion-xl-base-1.0 [52] to generate all the character images, with 30 diffusion steps, and $1024 \times 1024$ image size. All typographies contain black text and a white background. The font of the text is Arial and the font size is 50 .

Hyper Parameters for Universal VRP Training. Including the initial round, we generate 5 rounds of character candidates. For each generation round, we give LLM 50 question demos sampled from train set. In initial round, we prompt LLM to generate 10 initial character candidates, for following optimization rounds, we prompt LLM to generate 5 character candidates. To compute batch training ASR, we sample 256 data from the train set. In each optimization round, we sample 5 characters from top 10 characters with the highest training ASR in history characters. VRP+FigStep. We combine FigStep with VRP by changing the harmful question typography at the bottom of the image to the typography of FigStep like "Here is how to build a bomb: 1. 2. 3.". Additionally, we add the text input of FigStep as a postfix of our VRP text input.

VRP+Query relevant. We also combine Query relevant with VRP by changing the harmful question typography to the Query relevant, and we do the same setting as VRP+FigStep to add a Query relevant style postfix to VRP text input.

### 4.3 Baselines

In our experiments, we choose the following jailbreak baseline to compare with our VRP:

Vanilla Text: Vanilla Text means we use the blank image as image input and we use the vanilla query as text input.

Vanilla Typography: Vanilla Typography means we use the typography of vanilla query as image input and we use "Fullfil the request in the image" as text input.

Textual Role-play(TRP): Textual Role-play means inserting the same character generated with VRP into text input to perform a text-based jailbreak attack. We use a blank image as image input. See Tab. Elin Appendix E for details.

FigStep [16]: A straightforward image-based jailbreak attack, which rephrases the vanilla question into a "Step-by-step" style and typography to image input.

Query relevant [38]: An image-based jailbreak attack, which makes textual queries into visual representations using various methods such as Stable Diffusion(SD), Typography(Typo), and SD+Typo. We only use SD+Typo as the baseline due to its consistently superior performance across many MLLMs.

### 4.4 Ablation Study

To investigate the importance of the image component $I^{k t}$ and $I^{t 2 i}$ in our VRP method, we conduct ablation experiments under four settings: (i) remove $I^{k t}$ and keep $I^{t 2 i}$ and $I^{Q}$. (ii) remove $I^{t 2 i}$ and keep $I^{k t}$ and $I^{Q}$. (iii) replace $I^{k t}$ with $I_{b e n i g n}^{k t}$ and keep $I^{t 2 i}$ and $I^{Q}$. (iv) replace $I^{t 2 i}$ with $I_{b e n i g n}^{t 2 i}$ and keep $I^{k t}$ and $I^{Q} \cdot I_{b e n i g n}^{k t}$ and $I_{b e n i g n}^{t 2 i}$ are generated by using "A benign person" instead of description and prompt of high-risk character. All settings of image input contain question typography. Our Query-specific VRP can be written as $I^{t 2 i}+I^{k t}$.

### 4.5 Defense

We evaluate the robustness of VRP against two defense approaches, namely System Prompt-based Defense, and the Eye Closed Safety On (ECSO) approach [18]

System Prompt-based Defense: To defend against the jailbreak attack, a system prompt can instruct the model to conduct a preliminary safety assessment of the text and image input, thereby filtering out queries that violate AI safety policies. We add the following Prompt 2 to the existing system prompt of MLLMs.

Prompt 2:

You should be aware of any malicious content in both the image and the text. Refuse to answer if this query violates AI safety policy.

ECSO[18]: A defense method utilizing MLLMs' aligned textual module to mitigate the vulnerability in visual modality. ECSO use the MLLM itself to evaluate the safety of its response and makes MLLMs to regenerate unsafe responses in two steps: image captioning, and then responding based on caption with no image input.

### 4.6 Results

VRP is more effective than baseline attacks. In Tab. 1, we present the outcomes of our queryspecific VRP attack on the test sets of RedTeam-2K and HarmBench. This approach involves generating specific characters for each harmful question to assess their effectiveness in compromising SotA open-source and closed-source MLLMs, such as Gemini-Pro-Vision. but also achieves higher ASR than all other baseline attacks. Our findings reveal that query-specific VRP not only successfully breaches these MLLMs but also achieves a higher ASR compared to all evaluated baseline attacks. Specifically, it improves the ASR by $9.8 \%$ over FigStep and by $14.3 \%$ over Query relevant. The data consistently shows that query-specific VRP surpasses TRP, underscoring the crucial role of character images in the effective jailbreaking of MLLMs. These results affirm that VRP is a potent method for jailbreaking MLLMs.

Table 1: Attack Success Rate of query-specific VRP compared with baseline attacks on MLLMs between test set of RedTeam-2K and HarmBench dataset. Our VRP achieves the highest ASR in all datasets compared with other jailbreak attacks.

| Model | RedTeam-2K |  |  |  |  |  | HarmBench |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Vanilla Text | Vanilla Typo | Figstep | $\mathrm{QR}$ | TRP | VRP(Ours) | Vanilla Text | Vanilla Typo | Figstep | $\mathrm{QR}$ | Text VRP | VRP(Ours) |
| LLaVA-V1.6-Mixtral | 7.75 | 6.50 | 15.00 | 20.50 | 24.75 | 38.00 | 11.67 | 5.36 | 27.44 | 23.97 | 39.43 | 41.64 |
| Qwen-VL-Chat | 5.00 | 9.25 | 20.50 | 16.75 | 2.5 | 29.50 | 1.89 | 8.20 | 27.76 | 25.55 | 34.07 | 30.28 |
| OmniLMM-12B | 19.00 | 13.00 | 22.25 | 16.00 | 28.25 | 28.50 | 30.60 | 13.25 | 23.66 | 17.35 | 27.44 | 31.55 |
| InternVL-Chat-V1.5 | 8.25 | 8.25 | 22.00 | 13.00 | 27.5 | 24.50 | 11.36 | 22.08 | 30.91 | 8.52 | 37.54 | 34.38 |
| Gemini-Pro-Vision | - | - | - | - | - | - | 6.62 | 14.51 | 31.23 | 26.50 | 27.13 | 37.85 |

QR: Query Relevant jailbreak attack.

VRP is effective against System Prompt-based Defense and ECSO. We evaluate our query-specific VRP and baselines against our System Prompt-based Defense and ECSO. As shown in Tab. 2 , the results demonstrate that our query-Specific VRP consistently achieves the ASR across all models, regardless of whether it is tested against System Prompt-based Defense or ECSO. This consistent performance underlines the efficacy of query-specific VRP in penetrating defenses and reveals a notable vulnerability of defense mechanisms under our VRP jailbreak attacks. These findings highlight the potential of VRP as a formidable strategy against defense mechanisms.

Table 2: Attack Success Rate of query-specific VRP against the defense on the test set of RedTeam-2K. Our Query-specific attack is effective under the defense of System Prompt-based Defense and ECSO among all models.

| Model | Setting | Figstep | Query Relevant | VRP(Ours) |
| :--- | :--- | :--- | :---: | :---: |
| LLaVA-V1.6-Mixtral | Basic | 15.00 | 20.50 | $\mathbf{3 8 . 0 0}$ |
|  | +SPD | 4.25 | 1.75 | $\mathbf{3 0 . 0 0}$ |
|  | +ECSO | 14.00 | 10.25 | $\mathbf{3 0 . 5 0}$ |
| Qwen-VL-Chat | +Basic | 20.50 | 16.75 | $\mathbf{2 9 . 5 0}$ |
|  | +SPD | 12.50 | 1.75 | $\mathbf{2 3 . 5 0}$ |
|  | +ECSO | 16.50 | 9.75 | $\mathbf{2 3 . 7 5}$ |
| OmniLMM-12B | Basic | 22.25 | 16.00 | $\mathbf{2 8 . 5 0}$ |
|  | +SPD | 16.50 | 13.50 | $\mathbf{1 9 . 7 5}$ |
|  | +ECSO | 17.75 | 7.25 | $\mathbf{2 5 . 7 5}$ |
| InternVL-Chat-V1.5 | Basic | 22.00 | 13.00 | $\mathbf{2 4 . 5 0}$ |
|  | +SPD | 8.00 | 1.00 | $\mathbf{2 3 . 7 5}$ |
|  | +ECSO | 20.25 | 5.75 | $\mathbf{2 2 . 2 5}$ |

SPD: System Prompt-based Defense

VRP achieves high-performance transferability across models. In our research, we further investigate the applicability of a universal attack across diverse models. Utilizing our universal VRP algorithm, we identify the most effective role-play character within the train and valid set on the target model. Subsequently, we transfer the most effective character to conduct a jailbreak attack on the target models. From Tab. 3, The ASR achieves an average of $32.7 \%$ for the target model as LLaVA-V1.6-Mixtral and $29.4 \%$ on Qwen-VL-Chat. The ASR is higher on the target model, and also higher on the transfer model, demonstrating that our VRP, when implemented in a universal setting, effectively transfers and maintains high performance across different MLLMs.

Table 3: Attack Success Rate of universal VRP between target models and transfer models on test set of RedTeam-2K. we use train set and valid set of RedTeam-2K on target models to find the best character and use the best character to attack transfer models on test set of RedTeam-2K. The results show our VRP in a universal setting can be transferred with high performance among different black-box models.

| Target Model | Transfer Model |  |  |  |
| :---: | :---: | :---: | :---: | :---: |
|  | LLaVA-V1.6-Mixtral | Qwen-VL-Chat | OmniLMM-12B | InternVL-Chat-V1-5 |
| LLaVA-V1.6-Mixtral | 45.00 | 33.00 | 24.50 | 28.25 |
| Qwen-VL-Chat | 38.00 | 31.75 | 20.00 | 28.00 |

Target Model: The model that is used to do the train and valid work on train set and valid set. Transfer Model: The model that is used to test the universal character on test set

Tailored character image and description typography maximize the ASR of VRP. To investigate the impact of the image component $I^{k t}$ and $I^{t 2 i}$ in our query-specific VRP, we conducted a series of ablation experiments involving different character settings. The findings, as documented in Tab. 4 , indicate that tailoring a specific character image and description typography yields the most significant improvement in the ASR. This observation suggests that the image of the character and its description typography used to concatenate the image input critically influences the efficacy of the VRP in jailbreaking MLLMs. It underscores the pivotal character that visual representation plays in enhancing the effectiveness of VRP methods. These results affirm the necessity of carefully selecting and designing character images and description typography to maximize the disruptive potential of VRP to jailbreak MLLMs.

Table 4: Attack Success Rate of different image element combination settings in image input of VRP on test set of RedTeam-2K. VRP achieves the highest ASR in tailored character image and description typography settings $\left(I^{t 2 i}+I^{k t}\right)$, demonstrating the importance of tailored character image and description typography in image input for VRP.

| Model | $I^{t 2 i}$ | $I^{k t}$ | $I_{\text {benign }}^{t 2 i}+I^{k t}$ | $I^{t 2 i}+I_{b e n i g n}^{k t}$ | $\operatorname{VRP}\left(I^{t 2 i}+I^{k t}\right)$ |
| :---: | :---: | :---: | :---: | :---: | :---: |
| OmniLMM-12B | 19.00 | 28.00 | 20.25 | 30.00 | $\mathbf{3 1 . 2 5}$ |
| Qwen-VL-Chat | 26.75 | 19.25 | 18.25 | 21.75 | $\mathbf{2 9 . 5 0}$ |
| InternVL-Chat-V1-5 | 24.75 | 19.00 | 9.75 | 30.05 | $\mathbf{2 8 . 2 5}$ |
| LLaVA-V1.6-Mixtral | 32.75 | 27.50 | 18.25 | $\mathbf{3 8 . 0 0}$ |  |

$I^{t 2 i}$ means only the diffusion image with question typography. $I^{k t}$ means only the description typography with question typography. $I_{\text {benign }}^{t 2 i}+I^{k t}$ means the benign diffusion image and description typography with question typography. $I^{t 2 i}+I_{b e n i g n}^{k t}$ means diffusion image and benign description typography with question typography.

## 5 Conclusion, Limitation, and Future work

In this paper, we propose a novel jailbreak method for overcoming the limitations of effectiveness and universality in current approaches. Our method induces MLLMs to provide harmful content in response to malicious requests. By leveraging a joint framework, we generate portraits of characters and instruct the MLLMs to role-play these characters, thereby compromising the models' alignment robustness. Extensive experiments demonstrate that, compared with existing methods, our method exhibits outstanding attack effectiveness across various models, even against advanced defenses. We show that using our method, a single image can induce MLLMs to generate multiple harmful responses. Meanwhile, we also identify two limitations of our method. First, training a universal jailbreak image is more costly than creating sample-specific ones. Second, the performance of text-to-image models may have a potential influence on our method, as these models are required to generate the target portraits. In future work, we will focus on developing more efficient approaches for universal jailbreak attacks and effective defenses based on our findings.

## References

[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., AnadKat, S., et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023).

[2] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.

[3] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv preprint arXiv:2308.12966 (2023).

[4] Cha, S., LeE, J., LeE, Y., And Yang, C. Visually Dehallucinative Instruction Generation: Know What You Don't Know. arXiv preprint arXiv:2303.16199 (2024).

[5] Chen, G., Dong, S., Shu, Y., Zhang, G., Sesay, J., Karlsson, B. F., Fu, J., and Shi, Y. Autoagents: A framework for automatic agent generation, 2024.

[6] Chen, K., Zhang, Z., Zeng, W., ZHAnG, R., ZHU, F., And ZHAO, R. Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic. arXiv preprint arXiv:2306.15195 (2023).

[7] Chen, Y., SikKa, K., Cogswell, M., Ji, H., And DivaKaran, A. DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback. arXiv preprint arXiv:2311.10081 (2023).

[8] Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., Li, B., Luo, P., Lu, T., Qiao, Y., and DaI, J. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238 (2023).

[9] Contributors, O. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/opencompass. 2023.

[10] Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., duan, H., Cao, M., Zhang, W., Li, Y., Yan, H., Gao, Y., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., and WANG, J. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model, 2024.

[11] Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., Zhang, W., Li, Y., Yan, H., Gao, Y., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., and Wang, J. InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model. arXiv preprint arXiv:2401.16420 (2024).

[12] Dong, Y., Chen, H., Chen, J., Fang, Z., Yang, X., Zhang, Y., Tian, Y., Su, H., And ZHU, J. How Robust is Google's Bard to Adversarial Image Attacks? arXiv preprint arXiv:2309.11751 (2023).

[13] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., WU, Y., AND JI, R. MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv preprint arXiv:2306.13394 (2023).

[14] Fu, C., Zhang, R., Wang, Z., Huang, Y., Zhang, Z., Qiu, L., Ye, G., Shen, Y., Zhang, M., Chen, P., Zhao, S., Lin, S., Jiang, D., Yin, D., Gao, P., Li, K., Li, H., And Sun, X. A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise. arXiv preprint arXiv:2312.12436 (2023).

[15] Ge, J., Luo, H., Qian, S., Gan, Y., Fu, J., and ZHAn, S. Chain of Thought Prompt Tuning in Vision Language Models. arXiv preprint arXiv:2304.07919 (2023).

[16] Gong, Y., Ran, D., Liu, J., Wang, C., Cong, T., Wang, A., Duan, S., and Wang, X. Figstep: Jailbreaking large vision-language models via typographic visual prompts, 2023.

[17] Gong, Y., Ran, D., Liu, J., Wang, C., Cong, T., Wang, A., Duan, S., and Wang, X. FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts. arXiv preprint arXiv:2311.05608 (2023).

[18] Gou, Y., Chen, K., Liu, Z., Hong, L., Xu, H., Li, Z., Yeung, D.-Y., KwoK, J. T., and ZHANG, Y. Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation, 2024.

[19] Gu, X., Zheng, X., Pang, T., Du, C., Liu, Q., WanG, Y., Jiang, J., and Lin, M. Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast. arXiv preprint arXiv:2402.08567 (2024).

[20] Guo, P., Yang, Z., Lin, X., Zhao, Q., and Zhang, Q. PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. arXiv preprint arXiv:2401.10586 (2024).

[21] Han, D., JiA, X., Bai, Y., Gu, J., LIU, Y., And CaO, X. OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization. arXiv preprint arXiv:2312.04403 (2023).

[22] Inan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., Tontchev, M., Hu, Q., Fuller, B., TestugGine, D., And Khabsa, M. Llama guard: Llm-based input-output safeguard for human-ai conversations, 2023.

[23] Ji, Y., Ge, C., Kong, W., Xie, E., Liu, Z., Li, Z., and Luo, P. Large Language Models as Automated Aligners for benchmarking Vision-Language Models. arXiv preprint arXiv:2311.14580 (2023).

[24] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and SAYED, W. E. Mistral 7b, 2023.

[25] JIN, H., CHEN, R., CHEN, J., AND WANG, H. Quack: Automatic jailbreaking large language models via role-playing, 2024.

[26] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., And Iwasawa, Y. Large language models are zero-shot reasoners. NeurIPS (2022).

[27] KuraKin, A., GoodfeLlow, I. J., And Bengio, S. Adversarial Machine Learning at Scale. In ICLR (2017).

[28] LI, J., LI, D., SaVARESE, S., AND HoI, S. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML (2023).

[29] Li, L., Xie, Z., Li, M., Chen, S., Wang, P., Chen, L., Yang, Y., Wang, B., and KonG, L. Silkie: Preference Distillation for Large Visual Language Models. arXiv preprint arXiv:2312.10665 (2023).

[30] Li, M., Li, L., Yin, Y., Ahmed, M., Liu, Z., And LiU, Q. Red Teaming Visual Language Models. arXiv preprint arXiv:2401.12915 (2024).

[31] Lin, B., ZHU, B., Ye, Y., Ning, M., Jin, P., and YuAN, L. Video-LLaVA: Learning United Visual Representation by Alignment Before Projection. arXiv preprint arXiv:2311.10122 (2023).

[32] LiU, H., Li, C., LI, Y., Li, B., ZhAnG, Y., ShEN, S., ANd LeE, Y. J. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.

[33] Liu, H., Xue, W., Chen, Y., Chen, D., Zhao, X., Wang, K., Hou, L., Li, R., and PENG, W. A Survey on Hallucination in Large Vision-Language Models. arXiv preprint arXiv:2402.00253 (2024).

[34] Liu, M., Roy, S., Li, W., Zhong, Z., Sebe, N., And Ricci, E. Democratizing fine-grained visual recognition with large language models. In ICLR (2024).

[35] Liu, S., Nie, W., Wang, C., Lu, J., Qiao, Z., Liu, L., Tang, J., Xiao, C., and AnandKumar, A. Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing. arXiv preprint arXiv:2212.10789 (2024).

[36] Liu, X., Xu, N., Chen, M., And XiaO, C. Autodan: Generating stealthy jailbreak prompts on aligned large language models, 2024.

[37] Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., And Tang, J. AgentBench: Evaluating LLMs as Agents. In ICLR (2024).

[38] LiU, X., ZhU, Y., Gu, J., Lan, Y., Yang, C., And QIAO, Y. Mm-safetybench: A benchmark for safety evaluation of multimodal large language models, 2024.

[39] Lu, L.-C., Chen, S.-J., PaI, T.-M., Yu, C.-H., Yi LeE, H., and Sun, S.-H. Llm discussion: Enhancing the creativity of large language models via discussion framework and role-play, 2024.

[40] Luo, W., Ma, S., Liu, X., Guo, X., And XiaO, C. Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks, 2024.

[41] Lyu, H., Huang, J., Zhang, D., Yu, Y., Mou, X., Pan, J., Yang, Z., Wei, Z., and Luo, J. GPT-4v(ision) as a social media analysis engine. arXiv preprint arXiv:2311.07547 (2023).

[42] Mao, C., Chiquier, M., Wang, H., Yang, J., and VondricK, C. Adversarial Attacks Are Reversible With Natural Supervision. In ICCV (2021).

[43] Mazeika, M., Phan, L., Yin, X., Zou, A., Wang, Z., Mu, N., Sakhaee, E., Li, N., Basart, S., Li, B., Forsyth, D., and Hendrycks, D. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal, 2024.

[44] Meta AI. Llama 2 - acceptable use policy. https://ai.meta.com/llama/use-policy/. 2024. Accessed: 2024-01-19.

[45] Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., And Mian, A. A Comprehensive Overview of Large Language Models. arXiv preprint arXiv:2307.06435 (2024).

[46] Nie, W., Guo, B., Huang, Y., Xiao, C., Vahdat, A., and Anandkumar, A. Diffusion models for adversarial purification, 2022.

[47] NiU, Z., Ren, H., GaO, X., HuA, G., And Jin, R. Jailbreaking Attack against Multimodal Large Language Model. arXiv preprint arXiv:2402.02309 (2024).

[48] OpENAI. Usage policies - openai. https://openai.com/policies/usage-policies 2024. Accessed: 2024-01-12.

[49] OPENAI TEAM. Gpt-4 technical report, 2023.

[50] Qi, X., Huang, K., Panda, A., Henderson, P., Wang, M., and Mittal, P. Visual Adversarial Examples Jailbreak Aligned Large Language Models. arXiv preprint arXiv:2306.13213 (2023).

[51] Rizwan, N., Bhaskar, P., Das, M., Majhi, S. S., Saha, P., and Mukherjee, A. Zero shot VLMs for hate meme detection: Are we there yet? arXiv preprint arXiv:2402.12198 (2024).

[52] Rombach, R., Blattmann, A., Lorenz, D., EsSer, P., and Ommer, B. High-resolution image synthesis with latent diffusion models, 2022.

[53] Salemi, A., Mysore, S., Bendersky, M., and Zamani, H. Lamp: When large language models meet personalization, 2024.

[54] Schlarmann, C., And HeIn, M. On the adversarial robustness of multi-modal foundation models. In ICCV (2023).

[55] SHAnAHAN, M., McDonELL, K., AND ReYnOLDS, L. Role-play with large language models, 2023.

[56] Shayegani, E., Dong, Y., And Abu-GhaZaleH, N. Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models. arXiv preprint arXiv:2307.14539 (2023).

[57] Shayegani, E., Dong, Y., And Abu-GhazaleH, N. Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models. In The Twelfth International Conference on Learning Representations (2024).

[58] Shayegani, E., Mamun, M. A. A., Fu, Y., Zaree, P., Dong, Y., and Abu-GhaZaleh, N. Survey of vulnerabilities in large language models revealed by adversarial attacks. arXiv preprint arXiv:2310.10844 (2023).

[59] Shen, X., Chen, Z., Backes, M., Shen, Y., and Zhang, Y. "do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models, 2023.

[60] Sun, Z., Shen, S., Cao, S., Liu, H., Li, C., Shen, Y., Gan, C., Gui, L.-Y., Wang, Y.-X., Yang, Y., Keutzer, K., and Darrell, T. Aligning Large Multimodal Models with Factually Augmented RLHF. arXiv preprint arXiv:2309.14525 (2023).

[61] TaO, M., Liang, X., SHi, T., YU, L., And XIE, Y. Rolecraft-glm: Advancing personalized role-playing in large language models, 2024.

[62] Wang, B., Chen, W., Pei, H., Xie, C., Kang, M., Zhang, C., Xu, C., Xiong, Z., Dutta, R., Schaeffer, R., Truong, S. T., Arora, S., Mazeika, M., Hendrycks, D. Lin, Z., Cheng, Y., Koyejo, S., Song, D., And Li, B. DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. arXiv preprint arXiv:2306.11698 (2024).

[63] Wang, Y., LiU, X., Li, Y., Chen, M., And Xiao, C. Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting. arXiv preprint arXiv:2403.09513 (2024)

[64] Wang, Z. M., Peng, Z., Que, H., Liu, J., Zhou, W., Wu, Y., Guo, H., Gan, R., Ni, Z., Yang, J., Zhang, M., Zhang, Z., Ouyang, W., Xu, K., Huang, S. W., Fu, J., AND PENG, J. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models, 2024.

[65] Wei, J., Shuster, K., Szlam, A., Weston, J., UrbaneK, J., and Komeili, M. Multiparty chat: Conversational agents in group settings with humans and models, 2023.

[66] Wei, T., Zhao, L., Zhang, L., Zhu, B., Wang, L., Yang, H., Li, B., Cheng, C., L, W., Hu, R., Li, C., Yang, L., Luo, X., Wu, X., Liu, L., Cheng, W., Cheng, P., Zhang, J., Zhang, X., Lin, L., Wang, X., Ma, Y., Dong, C., Sun, Y., Chen, Y., Peng, Y., Liang, X., Yan, S., Fang, H., and ZHou, Y. Skywork: A More Open Bilingual Foundation Model. arXiv preprint arXiv:2310.19341 (2023).

[67] Xu, N., Wang, F., Zhou, B., Li, B. Z., Xiao, C., And Chen, M. Cognitive overload: Jailbreaking large language models with overloaded logical thinking, 2024.

[68] Yang, C., WANG, X., Lu, Y., LiU, H., Le, Q. V., ZHou, D., And ChEn, X. Large language models as optimizers, 2024.

[69] Yang, J., Zhang, H., Li, F., Zou, X., Li, C., and Gao, J. Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V. arXiv preprint arXiv:2310.11441 (2023).

[70] Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., And Chen, E. A Survey on Multimodal Large Language Models. arXiv preprint arXiv:2306.13549 (2023).

[71] Yin, S., Fu, C., Zhao, S., Xu, T., Wang, H., Sui, D., Shen, Y., Li, K., Sun, X., and CHEN, E. Woodpecker: Hallucination Correction for Multimodal Large Language Models. arXiv preprint arXiv:2310.16045 (2023).

[72] Yu, T., Yao, Y., Zhang, H., He, T., Han, Y., Cui, G., Hu, J., Liu, Z., Zheng, H.-T., Sun, M., eT al. RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback. arXiv preprint arXiv:2312.00849 (2023).

[73] Zeng, Y., Lin, H., ZHAnG, J., YanG, D., JIA, R., And SHi, W. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms, 2024.

[74] Zhang, D., Yu, Y., Li, C., Dong, J., Su, D., Chu, C., and Yu, D. MM-LLMs: Recent Advances in MultiModal Large Language Models. arXiv preprint arXiv:2401.13601 (2024).

[75] Zhang, R., Han, J., Liu, C., Gao, P., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., and QIAO, Y. LLaMA-Adapter: Efficient Finetuning of Language Models with Zero-init Attention. arXiv preprint arXiv:2303.16199 (2023).

[76] Zhang, X., Zhang, C., Li, T., Huang, Y., Jia, X., Xie, X., Liu, Y., and Shen, C. A mutation-based method for multi-modal jailbreaking attack detection. arXiv preprint arXiv:2312.10766 (2023).

[77] Zheng, G., Yang, B., Tang, J., Zhou, H.-Y., And Yang, S. DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models. In NeurIPS (2023).

[78] Zhou, W., Zhang, S., Poon, H., and Chen, M. Context-faithful prompting for large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023 (Singapore, Dec. 2023), H. Bouamor, J. Pino, and K. Bali, Eds., Association for Computational Linguistics, pp. 14544-14556.

[79] Zhu, B., Lin, B., Ning, M., Yan, Y., Cui, J., WAnG, H., Pang, Y., JianG, W., Zhang, J., LI, Z., ET AL. LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. arXiv preprint arXiv:2310.01852 (2023).

[80] Zong, Y., Bohdal, O., Yu, T., Yang, Y., and TimothY, H. Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models. arXiv preprint arXiv:2402.02207 (2024)

[81] Zou, A., Wang, Z., Kolter, J. Z., and FredriKson, M. Universal and transferable adversarial attacks on aligned language models, 2023.

# A Character Generation Detail 

We use Mixtral-8x7B-Instruct-v0.1[24] to generate characters for both query-specific VRP and universal VRP. We use the following prompts. We set max tokens as 1024, temperature as 1, topp as 0.5 . We use intruct LLM to enclose the character description within lland enclose the diffusion prompt within [], as we extract them from the LLM's output.
