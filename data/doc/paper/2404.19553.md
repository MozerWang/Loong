# Extending Llama-3's Context Ten-Fold Overnight 

Peitian Zhang ${ }^{1,2}$, Ninglu Shao ${ }^{1,2}$, Zheng Liu ${ }^{1 *}$ Shitao Xiao ${ }^{1}$, Hongjin Qian ${ }^{1,2}$,<br>Qiwei Ye ${ }^{1}$, Zhicheng Dou ${ }^{2}$<br>${ }^{1}$ Beijing Academy of Artificial Intelligence<br>${ }^{2}$ Gaoling School of Artificial Intelligence, Renmin University of China<br>namespace.pt@gmail.com zhengliu1026@gmail.com


#### Abstract

We extend the context length of Llama-3-8B-Instruct from $8 \mathrm{~K}$ to $80 \mathrm{~K}$ via QLoRA fine-tuning ${ }^{2}$. The entire training cycle is super efficient, which takes 8 hours on one 8xA800 (80G) GPU machine. The resulted model exhibits superior performances across a broad range of evaluation tasks, such as NIHS, topic retrieval, and longcontext language understanding; meanwhile, it also well preserves the original capability over short contexts. The dramatic context extension is mainly attributed to merely $3.5 \mathrm{~K}$ synthetic training samples generated by GPT-4, which indicates the LLMs' inherent (yet largely underestimated) potential to extend its original context length. In fact, the context length could be extended far beyond $80 \mathrm{~K}$ with more computation resources. Therefore, the team will publicly release the entire resources (including data, model, data generation pipeline, training code) so as to facilitate the future research from the community: https://github.com/ FlagOpen/FlagEmbedding.


## 1 Introduction

Recently, considerable attention has been directed towards long-context large language models, where different approaches are adopted to establish long-context capabilities for large language models [4, 14, 5, 8, 9, 16, 2]. However, most of them require significant compute and resources to accomplish.

In this technical report, we propose an efficient solution for entitling the long-context capabilities for LLMs, with which we extend the context length of Llama-3-8B-Instruc ${ }^{3}$ from $8 \mathrm{~K}$ to $80 \mathrm{~K}$. Specifically, we use GPT-4 [13] to synthesize $3.5 \mathrm{~K}$ long-context training data, covering three long-context tasks:

1. Single-Detail QA: the inquiry targets on one specific detail in a long context. To construct data for this task, we slice out a short segment (e.g., a chunk with less than 4096 tokens) from a long context (e.g., a book or a long paper) and prompt GPT-4 to generate multiple question-answer pairs based on this segment.
2. Multi-Detail QA: the inquiry requires information aggregation and reasoning over multiple details in a long context. We define two types of long context. The homogeneous context contains a coherent text, such as a book or a long paper. We prompt GPT-4 to generate multiple question-answer pairs that require aggregating and analyzing information from different locations in the context. The heterogeneous context consists of multiple independent texts. Notably, we perform clustering over a large corpus then extract texts from[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_fff81c76010c19555174g-2.jpg?height=691&width=1391&top_left_y=245&top_left_x=367)

Figure 1: The accuracy score of Llama-3-8B-Instruct-80K-QLoRA on Needle-In-A-HayStack task. The blue vertical line indicates the training length, i.e. $80 \mathrm{~K}$.

the same cluster to form each heterogeneous context. Therefore, the grouped texts share some semantic similarity. We then prompt GPT-4 to ask about the similarities/dissimilarities across these texts.

3. Biography Summarization: we prompt GPT-4 to write a biography for each main character in a given book.

For all three tasks, the length of context is between $64 \mathrm{~K}$ to $80 \mathrm{~K}$. Note that longer data can also be synthesized following the same methodology. When training, we organize the question-answer pairs for the same context in one multi-turn conversation then fine-tune the LLM to correctly answer the questions given the entire long context as input. Following previous work ${ }^{4}$, we mix $5 \mathrm{~K}$ instances randomly chosen from RedPajama [6] to mitigate forgetting. We also mix LongAlpaca [5] in the training set, which contains $12 \mathrm{~K}$ instruction tuning instances with $16 \mathrm{~K}$ length at maximum. Therefore, the entire training dataset contains $20 \mathrm{~K}$ instances.

We use QLoRA [7] to efficiently fine-tune the model. We apply LoRA on all Q,K,V,O projections and additionally train the embedding layer. We set LoRA rank to 32 and alpha to 16. The learning rate is $5 \mathrm{e}-5$ with linear decay and no warmups. The batch size is 8 . Gradient checkpointing is enabled. No parallel strategy is required thanks to the efficient implementation from Unsloth [1]. We train the model for 1 epoch, which takes 8 hours to complete on a 8 xA800 (80G) machine. Importantly, we expand the RoPE base from $500 \mathrm{~K}$ to $200 \mathrm{M}$ in training.

Our contributions are highlighted as follows:

- We release Llama-3-8B-Instruct-80K-QLoRA, which extends the context length of Llama3-8B-Instruct from $8 \mathrm{~K}$ to $80 \mathrm{~K}$. The entire resources including the model, training data, and code are all publicly available, which may advance the field of training long-context LLMs.
- Our training recipe is simple and efficient, while the resulted model demonstrates remarkable performance on downstream long-context tasks. Further research can be made to improve our approach.


## 2 Experiments

We evaluate our model on popular long-context benchmarks, then compare it with the original Llama-3-8B-Instruct model and the long-context Llama-3-8B-Instruct- $262 \mathrm{~K}$ from the community ${ }^{5}$.[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_fff81c76010c19555174g-3.jpg?height=564&width=1261&top_left_y=241&top_left_x=432)

Figure 2: The accuracy of Topic Retrieval task.

| Model | Single-Doc | Multi-Doc | Summ. | Few-Shot | Synthetic | Code | Avg |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Llama-3-8B-Instruct | 37.33 | 36.04 | 26.83 | $\mathbf{6 9 . 5 6}$ | 37.75 | 53.24 | 43.20 |
| Llama-3-8B-Instruct-262K | 37.29 | 31.20 | 26.18 | 67.25 | 44.25 | $\mathbf{6 2 . 7 1}$ | 43.73 |
| Llama-3-8B-Instruct-80K-QLoRA | $\mathbf{4 3 . 5 7}$ | $\mathbf{4 3 . 0 7}$ | $\mathbf{2 8 . 9 3}$ | 69.15 | $\mathbf{4 8 . 5 0}$ | 51.95 | $\mathbf{4 7 . 1 9}$ |

Table 1: Evaluation results on LongBench. For Llama-3-8B-Instruct, we use 8K context length.

| Model | LongBookQA Eng | LongBookSum Eng |
| :---: | :---: | :---: |
| GPT-4 | 22.22 | 14.73 |
| Llama-3-8B-Instruct | 7.00 | $\mathbf{1 6 . 4 0}$ |
| Llama-3-8B-Instruct-262K | 20.30 | 10.34 |
| Llama-3-8B-Instruct-80K-QLoRA | $\mathbf{3 0 . 9 2}$ | 14.73 |

Table 2: Evaluation results on InfBench. For Llama-3-8B-Instruct, we use 8K context length. The results of GPT-4 is copied from the paper [17].

| Model | STEM | Social | Humanities | Others | Avg |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Llama-2-7B-Chat | 35.92 | 54.37 | 51.74 | 51.42 | 47.22 |
| Mistral-7B-v0.2-Instruct | 48.79 | 69.95 | 64.99 | 61.64 | 60.10 |
| Llama-3-8B-Instruct | $\mathbf{5 3 . 8 7}$ | $\mathbf{7 5 . 6 6}$ | $\mathbf{6 9 . 4 4}$ | 69.75 | $\mathbf{6 5 . 9 1}$ |
| Llama-3-8B-Instruct-262K | 52.10 | 73.26 | 67.15 | $\mathbf{6 9 . 8 0}$ | 64.34 |
| Llama-3-8B-Instruct-80K-QLoRA | 53.10 | 73.24 | 67.32 | 68.79 | 64.44 |

Table 3: Zero-shot performance on MMLU.

Firstly, we leverage the Needle-In-A-Haystack task, which aims to recall an irrelevant piece of information (a.k.a. needle) inserted into a lengthy context (a.k.a. haystack). The accuracy is evaluated with GPT3.5. We use the same needle and haystack as in the official repository. Our model achieves $100 \%$ accuracy over all its training context length. Besides, the model generalizes well to the unseen positions $(80 \mathrm{~K} \sim 128 \mathrm{~K})$.

Secondly, we report the Topic Retrieval [12] accuracy in Figure 2. This task synthesizes a long conversation with multiple independent discussions of a certain topic between the user and the assistant. Then the LLM is required to repeat the first topic as is in the conversation. We use the conversations made up of $[5,10,15,20,25,30,40,50,60,70]$ topics for evaluation. It can be observed that Llama-3-8B-Instruct fails to remember the topic when the context is longer than $9 \mathrm{~K}$. However, the accuracy of our model remains $100 \%$ throughout all context lengths.[^2]

Thirdly, we evaluate our model on LongBench [3], which contains a variety of real-world long-context tasks. Most context on this benchmark is shorter than $32 \mathrm{~K}$. Thus, we use $32 \mathrm{~K}$ context length by default and $8 \mathrm{~K}$ for Llama-3-8B-Instruct. The results are shown in Table 1 . Our model significantly and consistently outperforms all baselines except on the code completion task. Mixing more code data in training may mitigate this problem.

Forthly, we employ the English Long-Book QA and the Long-Book Summarization task from InfiniteBench [17] to assess the model's performance on really long context. The testing instances are usually longer than $100 \mathrm{~K}$. We truncate them to $80 \mathrm{~K}$. According to Table 2. Llama-3-8B-Instruct-80KQLoRA excels on answering the questions based on the long context. It also achieves competitive performance against GPT-4 in terms of summarization. Interestingly, Llama-3-8B-Instruct with $8 \mathrm{~K}$ context outperforms GPT- 4 with $128 \mathrm{~K}$ context on summarization. This is likely to be a metricoriented issue (currently rouge-f1 is used) since the summary may have different paraphrases, which may not necessarily overlap with the ground truth.

Lastly, in Table 3, we compare the zero-shot performance of our model and the baselines on MMLU [10] benchmark. We also include Llama-2-7B-Chat [15] and Mistral-7B-Instruct-v0.2 [11] for comparison. It can be observed that both long-context models underperform the original Llama-38B-Instruct, indicating that context extension may compromise the model's short-context capability. This observation is in line with previous research [14]. However, our model's performance is still superior to other open-source models at the same scale.

## References

[1] Unsloth.ai. https://github.com/unslothai/unsloth, 2023.

[2] S. An, Z. Ma, Z. Lin, N. Zheng, and J.-G. Lou. Make your llm fully utilize the context, 2024.

[3] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, Y. Dong, J. Tang, and J. Li. Longbench: A bilingual, multitask benchmark for long context understanding, 2023.

[4] S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation, 2023.

[5] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, and J. Jia. Longlora: Efficient fine-tuning of long-context large language models, 2024.

[6] T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.

[7] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.

[8] Y. Ding, L. L. Zhang, C. Zhang, Y. Xu, N. Shang, J. Xu, F. Yang, and M. Yang. Longrope: Extending llm context window beyond 2 million tokens, 2024.

[9] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data engineering for scaling language models to $128 \mathrm{k}$ context, 2024.

[10] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding, 2021.

[11] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.

[12] D. Li*, R. Shao*, A. Xie, Y. Sheng, L. Zheng, J. E. Gonzalez, I. Stoica, X. Ma, , and H. Zhang. How long can open-source llms truly promise on context length?, June 2023.

[13] OpenAI. Gpt-4 technical report, 2024.

[14] B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models, 2023.

[15] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,

I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.

[16] P. Zhang, Z. Liu, S. Xiao, N. Shao, Q. Ye, and Z. Dou. Soaring from 4k to 400k: Extending llm's context with activation beacon, 2024.

[17] X. Zhang, Y. Chen, S. Hu, Z. Xu, J. Chen, M. K. Hao, X. Han, Z. L. Thai, S. Wang, Z. Liu, and M. Sun. $\infty$ bench: Extending long context evaluation beyond 100k tokens, 2024.


[^0]:    ${ }^{*}$ Corresponding author.

    ${ }^{2}$ The model is noted as Llama-3-8B-Instruct-80K-QLoRA given its max context length during fine-tuning. However, users could apply the model for even longer contexts via extrapolation.

    $\sqrt[3]{\text { https://llama.meta.com/llama3/ }}$

[^1]:    ${ }^{4}$ https://www.together.ai/blog/llama-2-7b-32k

    5 https://huggingface.co/gradientai/Llama-3-8B-Instruct-262k

[^2]:    ${ }_{\text {https://github.com/gkamradt/LLMTest_NeedleInAHaystack }}$

