# Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback 

Sanghyeon Na*<br>Kakao Brain<br>orca.na@kakaobrain.com

Yonggyu Kim<br>Kakao Brain<br>arthur.kyg@kakaobrain.com

Hyunjoon Lee ${ }^{\dagger}$<br>Kakao Brain<br>malfo.lee@kakaobrain.com


#### Abstract

The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.


## 1 Introduction

Recently, text-to-image (T2I) generation [44, 19, 40, 34, 41, 36] has made remarkable advancements with the emergence of diffusion models [21, 49, 16]. These advancements have enabled the generation of high-quality images from textual descriptions, enhancing the practicality and versatility of T2I applications. Among the various tasks achievable through T2I generation, human image generation (referred to as human generation) stands out as one of the most practical and in-demand tasks due to its significant applications in fields such as entertainment, virtual reality, and personalized media. For instance, users can utilize personalized T2I models $44,46,55,33,8,7]$ to create profile pictures that reflect their desired identity, demonstrating the practicality and appeal of these models.

Despite the impressive capabilities of current T2I models in image synthesis, generating human images still poses significant challenges. Often, the generated human images exhibit unrealistic anatomical structures and unnatural poses that fail to meet user preferences. A common approach to mitigate these issues is to fine-tune the base model using high-quality human images to create a model specialized for human generation. Although this strategy can improve the performance[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-02.jpg?height=505&width=1010&top_left_y=233&top_left_x=362)

(a) Samples of HG-DPO

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-02.jpg?height=499&width=358&top_left_y=236&top_left_x=1401)

(b) Various random samples

Figure 1: Human images generated using HG-DPO. In Figure 1a, HG-DPO enables the generation of human images with natural anatomies, poses, and alignment with prompts. It demonstrates its stability through various samples generated using multiple random seeds in Figure 1b. The prompts used for generating the images and additional samples are in the appendices.
![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-02.jpg?height=490&width=420&top_left_y=978&top_left_x=365)

(a) Improvements resulting from HG-DPO.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-02.jpg?height=491&width=569&top_left_y=974&top_left_x=1190)

(b) Personalized T2I with HG-DPO.

Figure 2: Effectiveness of HG-DPO. In Figure 2a. HG-DPO improves the target model by preventing it from generating (i) collapsed images, unnatural (ii) anatomies and (iii) poses, and (iv) images misaligned with text (red boxes). In Figure 2b, we generate an image with a more natural pose and the identity of the concept image at the top right by applying HG-DPO to a personalized T2I model [46].

of model to some extent, it is insufficient to fully satisfy the complex range of human preferences involved in human generation through the ine-tuning alone.

In this regard, Direct Preference Optimization (DPO) [39, 51], which directly aligns model outputs with human preferences, is a promising method to further improve the quality of human generation. DPO trains a model by providing both winning (preferred) and losing (not preferred) samples. This process guides the model to generate outputs that look more like the winning samples and less like the losing samples. Through this guidance of DPO, the model can be further improved by learning the human-related semantic differences between the winning and losing samples (e.g., the unnatural poses of the losing samples in contrast to the natural poses of the winning samples).

While adopting DPO for human generation shows promise, it presents significant challenges. One major hurdle is the necessity to construct a DPO dataset. For the T2I task, this dataset must include triplets, each consisting of a prompt and two images generated conditioned on that prompt. These images must be labeled as preferred or not preferred by humans. Given the high cost of human labeling, an alternative could be to utilize publicly available human preference datasets [23, 53], as done by the previous method that adopts DPO to the diffusion model [51].

However, we argue that utilizing the public datasets is not effective for improving human generation, particularly for models specifically tailored to this task. The public datasets generally comprise images of general subjects and not specifically human-focused images. Since our primary goal is human generation, it is essential to use a DPO dataset containing various prompts specifically related
to human subjects. Moreover, the images in the public datasets are typically generated with models different from the tailored model we aim to train (target model). Such images might feature distinct characteristics from those produced by the target model, potentially disrupting the training process by forcing the target model to mitigate these differences rather than enhancing human-specific attributes like anatomy and pose. This issue becomes more evident when considering practical scenarios for deploying human generation models. As frequently observed in user communities of generative models [1, 3], users often seek models that generate human images with specific styles suited to their needs (e.g., profile pictures of Asians with a particular resolution). In these scenarios, the tailored model produces images that are markedly different from those found in the public datasets. Given these practical considerations, it is crucial to construct the DPO dataset with images generated by the tailored model (in-distribution dataset), rather than relying on the public datasets (out-distribution dataset), to enhance the capabilities of the human generation model.

Motivated by this, we propose a novel method for constructing the in-distribution dataset. Instead of manual labeling, which requires expensive human involvement, we automatically generate the winning and losing images by leveraging the existing image preference metric [23]. This efficient dataset construction method allows for the creation of large-scale DPO datasets with meaningful human-related semantic differences between the winning and losing images without the need for expensive human labeling. Additionally, we propose a modified objective function for DPO that minimizes artifacts caused by unintended differences between the winning and losing images of our dataset. Consequently, we propose a method called HG-DPO to improve Human Generation through DPO, which incorporates the novel dataset construction method and the modified objective function. With HG-DPO, as shown in Figure 1, the T2I model can generate high-quality human images with natural anatomies and poses that align with the given texts. These results are achievable due to HG-DPO enhancing the target model in terms of (i) avoiding collapsed images, (ii) producing more natural anatomies, (iii) creating more natural poses, and (iv) generating images better aligned with the text as shown in Figure 2a.

Furthermore, HG-DPO can also be easily adapted to improve the image quality of applications related to human generation. For instance, we can generate personalized human images with the desired identity and better quality, as illustrated in Figure 2b, by adapting HG-DPO to a personalized T2I model [46]. This adaptiveness further enhances the practicality of HG-DPO.

## 2 Preliminaries

### 2.1 Reinforcement Learning from Human Feedback (RLHF)

RLHF [35] is widely employed in LLMs [31, 24, 54, 47, 29, 61, 10, 30, 12, 9] to align a model with human preferences. It consists of three stages. The first stage involves fine-tuning the pre-trained model on a dataset for a downstream task and obtain the fine-tuned model, $p_{s f t}$. The second stage involves training a reward model $r_{\phi}$ using a human preference dataset. It is constructed by using $p_{s f t}$ to generate a pair of samples for a condition $c$ and then annotating which sample is preferred by humans. Then, $r_{\phi}$ is trained to assign a higher score to an input that align well with human preferences. Finally, $r_{\phi}$ is utilized to train a model, $p_{\theta}$, using the following objective function:

$$
\begin{equation*}
\max _{p_{\theta}} \mathbb{E}_{c, x \sim p_{\theta}(x \mid c)}\left[r_{\phi}(x, c)\right]-\beta D_{K L}\left(p_{\theta}(x \mid c) \| p_{r e f}(x \mid c)\right) \tag{1}
\end{equation*}
$$

Both $p_{\theta}$ and $p_{\text {ref }}$ are initialized with $p_{s f t}$, but only $p_{\theta}$ is trained while $p_{\text {ref }}$ remains frozen. The first term of Eq. (1) encourages $p_{\theta}$ to maximize the reward, while the second term prevents $p_{\theta}$ from deviating too far from $p_{r e f}$, which is a regularization term.

### 2.2 Direct Preference Optimization (DPO)

DPO [39] is a method that directly trains $p_{\theta}$ using the human preference dataset without explicit reward modeling with the following loss function derived from Eq. (1):

$$
\begin{equation*}
\mathcal{L}_{D P O}\left(p_{\theta} ; p_{r e f}, \mathcal{D}\right)=-\mathbb{E}_{\left(c, x^{w}, x^{l}\right) \sim \mathcal{D}} \log \sigma\left\{\beta \log \frac{p_{\theta}\left(x^{w} \mid c\right)}{p_{r e f}\left(x^{w} \mid c\right)}-\beta \log \frac{p_{\theta}\left(x^{l} \mid c\right)}{p_{r e f}\left(x^{l} \mid c\right)}\right\} \tag{2}
\end{equation*}
$$

where $x^{w}$ and $x^{l}$ denote winning (more preferred) and losing (less preferred) samples with a condition $c$ for generating them. $\sigma$ denotes a sigmoid function. Diffusion-DPO [51] adpats DPO to the diffusion
model by deriving the loss function from Eq. (2) as

$$
\begin{align*}
& \mathcal{L}_{\text {Diff-DPO }}\left(\epsilon_{\theta} ; \epsilon_{r e f}, \mathcal{D}\right)=-\mathbb{E}_{\left(c, x^{w}, x^{l}\right) \sim \mathcal{D}, t \sim \mathcal{U}(0, T)} \log \sigma\{-\beta  \tag{3}\\
& \left.\left.\left\|\epsilon^{w}-\epsilon_{\theta}\left(x_{t}^{w}, c, t\right)\right\|_{2}^{2}-\left\|\epsilon^{w}-\epsilon_{r e f}\left(x_{t}^{w}, c, t\right)\right\|_{2}^{2}-\left(\left\|\epsilon^{l}-\epsilon_{\theta}\left(x_{t}^{l}, c, t\right)\right\|_{2}^{2}-\left\|\epsilon^{l}-\epsilon_{r e f}\left(x_{t}^{w}, c, t\right)\right\|_{2}^{2}\right)\right)\right\}
\end{align*}
$$

where $x_{t}^{*}$ is a noisy version of $x^{*}$ obtained by adding $\epsilon^{*} \sim \mathcal{N}(0, I)$ in the diffusion process of timestep t. $\epsilon_{\theta}$ and $\epsilon_{r e f}$ are initialized with pre-trained $\epsilon_{s f t}$, but only $\epsilon_{\theta}$ is trained. Eq. (3) encourages $\epsilon_{\theta}$ to generate images like $x^{w}$ while avoiding generating images like $x^{l}$ [51] by learning the differences between $x^{w}$ and $x^{l}$. Simultaneously, it regularizes $\epsilon_{\theta}$ to not deviate too far from $\epsilon_{r e f}$. Here, $\beta$ is a regularization weight which corresponds to the weight of the KL divergence term in Eq. (1).

## 3 HG-DPO: Improvement of Human Generation through DPO

Our objective is to improve the capabilities of a diffusion model, specifically Stable Diffusion 1.5 [41], in generating high-quality human portrait images. Since this model is not initially optimized for human portraits, we first fine-tune it using a dataset of high-resolution $(704 \times 1024)$ Asian portrait images. This results in a tailored model, $\epsilon_{s f t}$, which is capable of generating human images at an acceptable quality level. In addition, note that $\epsilon_{s f t}$ can be considered as a model customized to fulfill the specific requirements of users aiming to produce $704 \times 1024$ Asian portraits, aligning with the practical application scenarios for human generation discussed in Section 1 . We then apply our HG-DPO method to further enhance the performance of this fine-tuned target model, $\epsilon_{s f t}$.

### 3.1 Construction of a Human Dataset for DPO Using AI Feedback

In this section, we explain how to construct the dataset, $\mathcal{D}_{H G-D P O}$, for HG-DPO. We follow the approach used to construct Pick-a-Pic dataset [23] but with the following differences: (1) instead of using multiple backbone models [41, 36] to generate images as in Pick-a-Pic dataset [23], we use our tailored model $\epsilon_{s f t}$; (2) for each prompt, we generate $N$ images instead of two, and then select the most preferred and the least preferred images among them; (3) instead of using human feedback to select images, we use a model-based image preference metric. Below, we provide a detailed explanation of the dataset construction method and its motivation.

Step 1. Image pool generation. The first step is to generate images using a prompt set $\mathcal{P}=\left\{p_{i}\right\}_{i=1}^{D}$, where $p_{i}$ denotes a prompt and $D$ denotes the size of $\mathcal{D}_{H G-D P O}$. For $\mathcal{P}$, we use a subset of prompts from our Asian portrait images dataset. As explained in Section 2.2, DPO requires two images for each prompt, namely the winning and losing images. However, we propose to create an image pool by generating $N$ different images for each prompt, instead of generating precisely two images for each prompt. This approach increases the variety of images in the image pool, thus increasing the likelihood of selecting winning and losing images with more meaningful differences. Since DPO encourages the model to learn the differences between the winning and losing images, having meaningful differences between them is important. Formally, for a prompt $p_{i} \in \mathcal{P}$, we generate an image pool $\mathcal{X}_{i}$ of size $N$, which is defined as

$$
\begin{equation*}
\mathcal{X}_{i}=\left\{x_{i_{j}}\right\}_{j=1}^{N} \text { where } x_{i_{j}}=\mathrm{T} 2 \mathrm{I}\left(\epsilon_{s f t}, p_{i}, r_{i_{j}}\right) \tag{4}
\end{equation*}
$$

Here, T2I is a text-to-image generator with a random seed $r_{i_{j}}$ used to generate an image $x_{i_{j}}$. To generate $N$ different images, we employ $N$ different random seeds $\left\{r_{i_{j}}\right\}_{j=1}^{N}$.

Step 2. Model-based preference scoring. We then score the generated images with a given prompt $p_{i}$ using an image preference estimator as follows:

$$
\begin{equation*}
\mathcal{S}_{i}=\left\{s_{i_{j}}\right\}_{j=1}^{N} \text { where } s_{i_{j}}=f\left(x_{i_{j}}, p_{i}\right) \text { for } x_{i_{j}} \in \mathcal{X}_{i} \tag{5}
\end{equation*}
$$

where $\mathcal{S}_{i}$ represents the preference scores of the image pool $\mathcal{X}_{i}$, and $f$ represents the image preference estimator. In this case, we use PickScore [23] as the image preference estimator.

Step 3. Selection of winning and losing samples. Finally, we select the image with the highest preference score as the winning image $x_{i}^{w}$ and the image with the lowest preference score as the losing image $x_{i}^{l}$ from the image pool $\mathcal{X}_{i}$, then create a triplet $\left(p_{i}, x_{i}^{w}, x_{i}^{l}\right)$. By selecting images with

Table 1: Quantitative comparison to the baselines. It shows the win rates (\%) for HG-DPO compared to the baselines. HG-DPO demonstrates superiority over the baselines in almost all metrics.

| Baseline | PickScore | HPS | ImageReward | Aesthetic | CLIP |
| :--- | :---: | :---: | :---: | :---: | :---: |
| vs. Pick-a-Pic-v2 + DPO | 83.05 | 80.68 | 75.71 | 62.71 | 68.93 |
| vs. Pick-a-Pic-v2 (H) + DPO | 85.31 | 78.53 | 75.14 | 63.28 | 68.36 |
| vs. HPD-v2 + DPO | 88.14 | 82.95 | 76.27 | 63.28 | 71.19 |
| vs. HPD-v2 (H) + DPO | 86.44 | 79.89 | 77.97 | 62.15 | 72.32 |
| vs. $\mathcal{D}_{H G-D P O}+\mathrm{SFT}$ | 74.58 | 78.16 | 70.06 | 50.85 | 67.23 |
| vs. $\mathcal{D}_{H G-D P O}+$ DPO $(\beta \times 4)$ | 76.27 | 74.71 | 70.62 | 46.89 | 63.84 |
| vs. $\mathcal{D}_{H G-D P O}+\mathrm{DPO}(\beta \times 20)$ | 81.36 | 82.29 | 75.14 | 63.84 | 66.10 |
| vs. AlignProp | 39.55 | 65.91 | 64.41 | 16.95 | 74.01 |

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-05.jpg?height=112&width=1374&top_left_y=730&top_left_x=381)

Figure 3: User study. It shows the win rates (\%) of HG-DPO against the target model and several baselines on human evaluations. HG-DPO outperforms them, demonstrating its effectiveness.

the highest and lowest preference scores, we can maximize the semantic differences between the two images that affect image preference, thereby enabling the model to better learn the features of the preferred images. Specifically, $x_{i}^{w}$ and $x_{i}^{l}$ are defined as follows:

$$
\begin{equation*}
\left.\left(x_{i}^{w}, x_{i}^{l}\right)=\left(\mathcal{X}_{i}\left[j_{w}\right], \mathcal{X}_{i}\left[j_{l}\right]\right) \text { where }\left(j_{w}, j_{l}\right)=\underset{j_{w} \in[1 \ldots N]}{\operatorname{argmax}} S_{i}, \underset{j_{l} \in[1 \ldots N]}{\operatorname{argmin}} S_{i}\right) \tag{6}
\end{equation*}
$$

By creating triplets for the all prompts of $\mathcal{P}$, we complete the dataset $\mathcal{D}_{H G-D P O}=\left\{\left(p_{i}, x_{i}^{w}, x_{i}^{l}\right)\right\}_{i=1}^{D}$.

### 3.2 DPO with Statistic Matching

With the dataset $\mathcal{D}_{H G-D P O}$, we can apply DPO to $\epsilon_{s f t}$ using Eq. (3) to obtain the updated model, $\epsilon_{\theta}$. Compared to $\epsilon_{s f t}, \epsilon_{\theta}$ shows significant improvement in terms of human pose, anatomy, and textural prompt following. However, $\epsilon_{\theta}$ generates images with the color shift artifact, as shown in Figure 4 ( $\mathrm{N}-20$ ). We assume that the color is one of the low-level styles of an image and can be captured by the channel-wise statistics of latent features. Under this assumption, we hypothesize that the gaps between the channel-wise statistics of the latents sampled using $\epsilon_{\theta}$ and $\epsilon_{s f t}$ is the direct cause of the color shift. More detailed discussion can be found in Section 5

Latent adaptive normalization (LAN). To verify our hypothesis, we designed an inferencetime statistics matching approach called latent adaptive normalization (LAN). If the gaps in the channel-wise statistics of the latents cause the color shift, then eliminating those gaps should resolve it. Let $h_{t g t}^{t-1}$ and $h_{s f t}^{t-1}$ denote the latents sampled from the same random noise using $\epsilon_{\theta}$ and $\epsilon_{s f t}$ at inference time with timestep $t$, respectively (i.e., $h_{t g t}^{t-1}=\operatorname{sampler}\left(\hat{h}_{t g t}^{t}, p, t, \epsilon_{\theta}\right)$ and $h_{s f t}^{t-1}=$ sampler $\left(h_{s f t}^{t}, p, t, \epsilon_{s f t}\right)$ where sampler denotes a inference-time latent sampler and $p$ denotes an inference prompt). We define LAN as follows:

$$
\begin{equation*}
\hat{h}_{t g t}^{t-1}=\left(\frac{h_{t g t}^{t-1}-\mu\left(h_{t g t}^{t-1}\right)}{\sigma\left(h_{t g t}^{t-1}\right)}\right) \sigma\left(h_{s f t}^{t-1}\right)+\mu\left(h_{s f t}^{t-1}\right) \tag{7}
\end{equation*}
$$

where $\mu$ and $\sigma$ calculate the channel-wise mean and standard deviation from the input, respectively. As shown in Figure 4 and Table 2 (N-20-LAN), LAN can effectively address the color shift artifact, empirically verifying our hypothesis.

Statistic matching loss. By aligning the statistics of latents, we can effectively remove the color shifts from the generated images. However, the problem with LAN is that it incurs additional costs since it requires sampling from both $\epsilon_{\theta}$ and $\epsilon_{s f t}$. To address this issue, we propose a method to train the model such that the latent statistics are aligned during training time. Let $l^{t}$ denote a noisy latent of a winning image generated by the forward diffusion process at timestep $t$ during training. Then, we sample $l_{t g t}^{t-1}$ and $l_{r e f}^{t-1}$, which represent the latents sampled using $\epsilon_{\theta}$ and $\epsilon_{r e f}$, respectively (i.e., $l_{t g t}^{t-1}=\operatorname{sampler}\left(l^{t}, p, t, \epsilon_{\theta}\right)$ and $l_{r e f}^{t-1}=\operatorname{sampler}\left(l^{t}, p, t, \epsilon_{r e f}\right)$ where $p$ denotes a prompt paired with

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-06.jpg?height=547&width=1395&top_left_y=244&top_left_x=365)

Prompt : Three people in the forest, sunset, wearing orange suits, intense lighting

Figure 4: Qualitative results. The images generated by our model, HG-DPO, in the rightmost column have the natural poses while aligning well with the given texts compared to other models.

Table 2: Degree of color shift. Percentage change (\%) of color in terms of hue, saturation and value of each model relative to the target model $\left(\epsilon_{s f t}\right)$. The higher this value, the more severe the color shift is. We can observe that HG-DPO ( $\mathrm{N}-20$-SML) effectively resolves the color shift occurring in $\mathrm{N}-20$.

|  | $\mathrm{N}-2$ | $\mathrm{~N}-10$ | $\mathrm{~N}-20$ | $\mathrm{~N}-20(\beta \times 4)$ | $\mathrm{N}-20(\beta \times 20)$ | $\mathrm{N}-20-\mathrm{LAN}$ | $\mathrm{N}-20-\mathrm{SML}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Hue | 6.59 | 20.94 | 24.93 | 17.62 | 1.39 | 0.20 | 1.67 |
| Saturation | 4.50 | 5.54 | 12.52 | 7.99 | 2.86 | 2.03 | 0.74 |
| Value | 1.32 | 3.94 | 3.43 | 3.01 | 0.69 | 0.61 | 1.05 |

$l^{t}$ ). Here, $\epsilon_{r e f}$ is initialized with $\epsilon_{s f t}$ and is frozen. Finally, we define the statistic matching loss as:

$$
\begin{equation*}
\mathcal{L}_{\text {stat }}\left(\epsilon_{\theta} ; \epsilon_{r e f}, \mathcal{D}\right)=\mathbb{E}_{\left(p, x^{w}\right) \sim D, t \sim \mathcal{U}(0, T)}\left[\left\|\mu\left(l_{\text {tgt }}^{t-1}\right)-\mu\left(l_{\text {ref }}^{t-1}\right)\right\|_{2}^{2}\right] \tag{8}
\end{equation*}
$$

which resolves the color shift without incurring additional costs at inference time. Although LAN matches both the mean and the standard deviation to verify the assumption that gaps in the channelwise statistics cause the color shift, $\mathcal{L}_{\text {stat }}$ does not match the standard deviation, as we found that the mean matching sufficiently resolves the color shift ( $\mathrm{N}-20$-SML in Figure 4 and Table 2).

In conclusion, to enhance Human Generation via DPO, we propose a method called HG-DPO which incorporates i) constructing $\mathcal{D}_{H G-D P O}$ and ii) training $\epsilon_{\theta}$, which is initialized with $\epsilon_{s f t}$, with the following loss function:

$$
\begin{equation*}
\mathcal{L}_{H G-D P O}\left(\epsilon_{\theta} ; \epsilon_{r e f}, \mathcal{D}_{H G-D P O}\right)=\mathcal{L}_{\text {Diff-DPO }}\left(\epsilon_{\theta} ; \epsilon_{r e f}, \mathcal{D}_{H G-D P O}\right)+\lambda_{\text {stat }} \mathcal{L}_{\text {stat }}\left(\epsilon_{\theta} ; \epsilon_{r e f}, \mathcal{D}_{H G-D P O}\right) \tag{9}
\end{equation*}
$$

### 3.3 Personalized T2I with HG-DPO

Instead of training the entire U-Net [42], we attach LoRA [22] layers to U-Net and only train them. By attaching the pre-trained LoRA layers to various applications related to human generation, we can improve those applications. For instance, given the pre-trained LoRA layers and InstantBooth [46] which is the personalized T2I model, we can improve the image quality of InstantBooth by attaching the pre-trained LoRA layers to it without additional training. It enables us to generate high-quality human images reflecting the desired identity, which enhances the practicality of HG-DPO.

## 4 Experimental Settings

Implementation details. We set $D=100000, N=20$, and $\lambda_{\text {stat }}=10000$. More implementation details are described in the appendices.

Baselines. To demonstrate the effectiveness of our contributions, we compare our method with several baselines created using existing techniques. First, to confirm the importance of $\mathcal{D}_{H G-D P O}$, we train DPO models using the public datasets (Pick-a-pic-v2 [23] and HPD-v2 [53]). Additionally, we train DPO models using filtered versions of these datasets that contain only human images, referred

Table 3: Quantitative results of ablation study. Win rates (\%) of each model against the target model, $\epsilon_{s f t}$. In the last row, our final proposed model, HG-DPO, outperforms the target model.

| Configuration | PickScore | HPS | ImageReward | Aesthetic | CLIP |
| :--- | :---: | :---: | :---: | :---: | :---: |
| $\mathrm{N}-2$ | 66.10 | 67.63 | 63.84 | 55.37 | 54.24 |
| $\mathrm{~N}-5$ | 81.92 | 77.14 | 66.67 | 68.36 | 72.32 |
| $\mathrm{~N}-10$ | 84.18 | 84.57 | 74.01 | 75.14 | 69.49 |
| $\mathrm{~N}-20$ | 88.70 | 89.77 | 76.84 | 75.14 | 74.01 |
| $\mathrm{~N}-20$-LAN | 89.27 | 86.86 | 76.84 | 66.10 | 72.88 |
| $\mathrm{~N}-20-$ SML (HG-DPO) | 85.31 | 85.71 | 79.10 | 63.84 | 68.36 |

to as Pick-a-pic-v2 (H) and HPD-v2 (H). To validate the necessity of $\mathcal{L}_{\text {sta }}$, we train DPO models where the regularization weight $\beta$ in Eq. (3) is increased to mitigate the color shift instead of using $\mathcal{L}_{\text {sta }}$. Finally, to compare our method with the existing alignment approaches, we train two additional models using traditional supervised fine-tuning (SFT) and AlignProp [37].

Metrics. Similar to a previous study [51], we compare the performance of our model against a baseline model. Specifically, we generate images with both models for each prompt in a set of test prompts and then compute the preference scores. The performance of our model is assessed by determining the proportion of prompts for which our model achieves a higher preference score compared to the baseline model. This process is conducted separately for each baseline model. For the test prompts, we utilize a subset of PartiPrompts [59] that are categorized as people. To assess promptaware image preferences, we employ PickScore [23], HPS-v2 [53], and ImageReward [56]. For evaluations that do not consider the prompt, we use the AestheticScore estimator [45]. Additionally, CLIP [38] measures the alignment between the generated images and the prompts. Furthermore, we conduct user studies where participants choose their preferred images from result pairs generated by our model and a baseline model to quantify how much more frequently our model is preferred over the baseline models. Additionally, to quantify the color shifts, we convert RGB images to HSV and calculate the mean value of each channel. When evaluating the personalized T2I, we use ArcFace [14] and VGGFace [5] to measure the identity distance between the concept and generated images.

## 5 Analysis on HG-DPO

We compare HG-DPO with the baselines listed in Section 4, followed by ablations and additional analysis. We also demonstrate the effectiveness of HG-DPO on the personalized T2I. Further discussions on HG-DPO including the limitations are available in the appendices.

### 5.1 Comparison with the Baselines

In Table 1 and Figure 3 b-c, HG-DPO exhibits significantly higher win rates against models trained using Pick-a-Pic-v2, Pick-a-Pic-v2 (H), HPD-v2, and HPD-v2 (H). This corresponds with the qualitative findings shown in Figure 4, where HG-DPO produces images with more natural poses and superior text-image alignment compared to the baseline models. Furthermore, when utilizing $\mathcal{D}_{H G-D P O}$, even the baseline model trained with SFT outperforms those trained with DPO on public datasets, as evidenced by the lower win rates of HG-DPO against $\mathcal{D}_{H G-D P O}+\mathrm{SFT}$ compared to those against public datasets + DPO. This underscores the significance of our dataset.

As demonstrated in Table 2, adjusting the regularization weight $\beta$ of DPO could potentially mitigate the color shift instead of using the statistic matching loss. However, Table 1 and Figure 3 e reveals that $\mathrm{N}-20$-SML significantly outperforms $\mathcal{D}_{H G-D P O}+\mathrm{DPO}(\beta \times 20)$. This suggests that while increasing $\beta$ can reduce color shifts, it also significantly reduces the overall effectiveness of DPO. In contrast, $\mathcal{L}_{\text {stat }}$ effectively alleviates the color shift with much less impact on the performance of the model.

One of the key differences between the $\mathcal{D}_{H G-D P O}+\mathrm{SFT}$ model and our model lies in whether the training utilize only winning images or both winning and losing images. As demonstrated in Table 1 and Figure $3 \mathrm{~d}$, our model outperforms the SFT approach both quantitatively and qualitatively, highlighting the importance of including losing images in the training set. AlignProp shows better results in terms of PickScore and Aesthetic compared to our model. However, as shown in Figure 4 and in the appendices, when we fine-tune $\epsilon_{s f t}$ with AlignProp, the model loses the ability to generate Asian portraits. This is likely because the training objective of AlignProp is designed to optimize PickScore alone, neglecting the unique characteristics of the model.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-08.jpg?height=645&width=827&top_left_y=244&top_left_x=367)

(a) Winning and Losing images in $\mathcal{D}_{H G-D P O}$

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-08.jpg?height=645&width=534&top_left_y=244&top_left_x=1202)

(b) Statistics distances

Figure 5: Properties of $\mathcal{D}_{H G-D P O}$. Figure 5a shows the semantic differences between $x^{w}$ and $x^{l}$ as well as human preferences for them. Figure $5 \mathrm{~b}$ shows the cosine distance of channel-wise statistics of encoded latent (i.e., mean and standard deviation) between the images with the highest PickScore and the $n$-th highest PickScore (x-axis) in the image pool with the size of 20 . Note that $n=20$ corresponds to the losing images of our dataset, $\mathcal{D}_{\text {HG-DPO }}$. Thus, the distances for $n=20$ correspond to the distances between the winning and losing images in $\mathcal{D}_{H G-D P O}$.

### 5.2 Ablation Study

Importance of image pool in dataset construction. To demonstrate the importance of the image pool introduced in Section 3.1, we increased the value of $N$ and created datasets to train DPO models, subsequently comparing their performance. In Table 3, the notation $N-N$ denotes the model $\epsilon_{\theta}$ trained with $\mathcal{L}_{\text {Diff-DPO }}$ (see Eq. (3)) using the dataset $\mathcal{D}_{H G-D P O}$, where $N$ represents the size of the image pool. As shown in Table 3 , increasing $N$ correlates with higher win rates against the target model (ranging from $\mathrm{N}-2$ to $\mathrm{N}-20$ ). This trend is supported by Figure 4 , where $\mathrm{N}-20$ displays images with more natural poses and superior text-image alignment compared to both the target model and $\mathrm{N}-2$, underscoring the importance of a sizable image pool.

Color shift and statistics matching. As depicted in Figure 4, N-20 shows noticeable color shifts that make the images look unnatural. This is consistent with the result presented in Table 2, which indicates a significant hue shift for $\mathrm{N}$-20. In contrast, by using Latent Adaptive Normalization (LAN) or Statistics Matching Loss (SML) to align the latent statistics, we can effectively remove the color shifts and produce high-quality images with natural colors, poses, and precise text-image alignment, as illustrated in Figure 4 (N-20-LAN and N-20-SML). Comparing N-20-LAN and N-20-SML, we observe that the results from $\mathrm{N}-20$-SML are slightly inferior to those from $\mathrm{N}-20$-LAN as shown in Table 3 . We attribute this to the fact that while $\mathcal{L}_{\text {stat }}$ primarily targets color shift prevention, it also slightly regularizes the DPO process. Despite this minor performance drop, we recommend $\mathrm{N}-20$-SML as our final model due to its remarkable quality and sampling efficiency.

### 5.3 Further Analysis

Comparison of model-based preference scoring and human feedback. To evaluate whether model-based preference scoring can effectively replace manual labeling, we conduct a user study. We select a subset of data triplets $\left(p_{i}, x_{i}^{w}, x_{i}^{l}\right)$ from $\mathcal{D}_{H G-D P O}$ and ask participants to choose their preferred image between $x_{i}^{w}$ and $x_{i}^{l}$. The results indicate that $x_{i}^{w}$ is significantly more preferred by users, suggesting that model-based evaluations align well with human preferences (Figure 5 ).

Why the color shift occurs. The color shift arises from the deviation of the channel-wise statistics of latents sampled using $\epsilon_{\theta}$ from those sampled using $\epsilon_{s f t}$ as demonstrated by the effectiveness of LAN in Section 5.2. Here, we analyze why such deviation occurs. To find the cause of the difference in the channel-wise statistics of latents, we analyzed the dataset and found that there is a difference in

Table 4: Quantitative results for personalized T2I. Win rates (\%) of InstantBooth with HG-DPO against InstantBooth in terms of human preferences, text-image alignment and identity similarity.

| PickScore | HPS | ImageReward | Aesthetic | CLIP | Arcface | VGGFace |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 89.25 | 88.07 | 91.75 | 65.75 | 85.0 | 49.5 | 50.5 |

the channel-wise statistics, mean values in specific, of latents between the winning images $x^{w}$ and the losing images $x^{l}$ (see Figure 5b). Since DPO trains the model to learn the differences between winning and losing images, it can be inferred that the differences in the channel-wise mean values of latents present in the dataset were also learned by the model. This can encourage the model to shift the mean of the sampled latents far from that of the losing image and close to that of the winning image. $\mathcal{L}_{\text {stat }}$ mitigates the color shift by preventing this mean shift. Interestingly, we can observe that the difference of standard deviation between the latents of $x_{w}$ and $x_{l}$ is close to zero. We believe this is why matching only the mean in $\mathcal{L}_{\text {stat }}$ is sufficient to prevent the color shift.

Importance of the size of dataset. One of the key advantages of our approach, which utilizes AI feedback instead of costly human feedback for dataset construction, is the ease of creating a large-scale dataset. In the appendices, we demonstrate that having a sufficiently large dataset is crucial for achieving good performance. This highlights the effectiveness of our method in facilitating the creation of the large-scale dataset, much more efficiently than using human feedback.

### 5.4 Personalized T2I with HG-DPO

To demonstrate that our model also improves the performance of personalized T2I, we applied our model to InstantBooth [46], a personalized T2I method. Specifically, we train a personalization model on top of $\epsilon_{s f t}$ using InstantBooth. Since InstantBooth does not modify the existing network parameters, we simply switch $\epsilon_{s f t}$ to $\epsilon_{\theta}$ to incorporate HG-DPO into the personalization model. In Table 4, InstantBooth with HG-DPO outperforms the baseline in terms of human preferences (i.e., PickScore, HPS, ImageReward, and Aesthetic) and text-image alignment (i.e., CLIP). Additionally, it achieves win rates close to $50 \%$ in terms of identity similarity, demonstrating that InstantBooth with HG-DPO can generate images reflecting the identity of the given concept images as effectively as the base InstantBooth model. The qualitative results are provided in the appendices.

## 6 Related Work

To align the diffusion model with human preferences, several methods have been proposed based on reward maximization during training [4, 18, 6, 25, 37, 11] and inference time [52]. Furthermore, DPO [39]-based [51, 57], SPIN [9]-based [60], and KTO [17]-based [27] methods have been proposed. Additionally, a sampling method [58], which is orthogonal with them, has been also suggested. Unlike the aforementioned methods proposing the alignment algorithms, we propose the method to improve the human generation model based on one of these algorithms, Diffusion-DPO [51]. Furthermore, we propose a method that replaces human feedback with efficient AI feedback for the dataset construction, which distinguishes HG-DPO from these algorithms. Recently, a method for constructing a highquality real image dataset has been proposed [13, 26]. Furthermore, CosmicMan [26] proposed a method to learn a foundation model for the human generation. Although we also propose a dataset construction method, we propose a orthogonal method for constructing a synthesized dataset for DPO that consists of the winning and losing images, rather than a real dataset which is costly to construct.

## 7 Conclusion

In this work, we present HG-DPO, a method that enhances the performance of human image generation via DPO. Initially, we propose a technique to construct a large-scale DPO dataset without the need for costly manual labeling. Furthermore, we introduce a method designed to train models capable of producing high-quality human images without artifacts. HG-DPO demonstrates significantly improved results over existing methods, particularly in aspects such as human pose, anatomy, and adherence to text prompts. Additionally, it can be easily adapted to human-related applications such as personalized T2I, further illustrating its practical utility.

## Appendices

## A Additional Analysis on HG-DPO

In this section, we aim to conduct additional analysis on HG-DPO. In this section, we present additional analyses of HG-DPO that were not covered in our manuscript. First, we analyze the importance of the substantial differences between the winning and losing images produced by Eq. 6) during the dataset construction (Section A.1). Second, we explore the significance of creating a large-scale dataset enabled by using AI feedback instead of human feedback (Section A.2). Finally, we examine the effects of altering the LoRA [22] weight during inference time (Section A.3).

## A. 1 Importance of Large Differences between the Winning and Losing Images

In Section 3.1, we proposed a method for selecting the winning and losing images from the image pool (Eq. (6)). This method is based on the assumption that a larger PickScore difference between the winning and losing images indicates greater semantic differences, and they are crucial for enhancing the target model through DPO. As shown in Figure 6, comparing the image with the 1st highest PickScore to the image with the $l$-th highest PickScore shows that the semantic differences between the two images (e.g., anatomy, pose, and text-image alignment) become more pronounced as $l$ increases. By choosing the images with the 1st highest Pickscore and 20th highest PickScore as the winning and losing images, respectively, we accentuate the semantic differences between the them.

This design is important in improving the human generation model, as can be seen in Table 5 Table 5 presents the results of models trained by selecting the winning image with the highest PickScore from the image pool while varying the losing image. Specifically, $\mathrm{N}-20-\mathrm{L}-l$ in Table 5 refers to a model trained by choosing the image with the $l$-th highest PickScore from the image pool with the size of 20 as the losing image. Our proposed method (Eq. (6)) can be described as N-20-L-20, which is the same with $\mathrm{N}-20$ in our manuscript. Consequently, as seen in Table 5 , the win rates of $\mathrm{N}-20-\mathrm{L}-l$ against the target model increases as $l$ increases. This is consistent with Figure 7, where the image generated by $\mathrm{N}-20-\mathrm{L}-2$ shows minimal improvement from the result of the target model. This is because, when $l=2$, there are no significant semantic differences between the winning and losing images compared to when $l=20$. Note that when $l=2$, the differences between the winning and losing images are smaller compared to when $l=5$ in Figure 6 As a result, the model, which needs to learn by capturing the differences between the winning and losing images, faces ambiguity in its learning objectives. We believe this is why N-20-L-l performs better as $l$ increases.

This underscores the importance of having significant semantic differences between the winning and losing images. Our dataset construction method effectively creates these significant semantic differences without human feedback, leading to substantial performance improvements.

## A. 2 Importance of the Substantially Large-Scale Dataset

One of the key advantages of our method, which utilizes AI feedback for automatic preference labeling instead of human feedback, is its ability to facilitate the construction of large-scale dataset. When using human feedback for preference labeling, its high cost can make it difficult to construct the large-scale dataset.

Table 6 shows the performance changes when the dataset scale is reduced. In Table $6, \mathrm{~N}-20-D$ refers to a model trained with the same configuration as $\mathrm{N}-20$ in our manuscript, but with a dataset size of $D$. Here, $\mathrm{N}-20-100 \mathrm{k}$ is the configuration we used by default in all experiments, which is the same with $\mathrm{N}-20$ in our manuscript, while $\mathrm{N}-20-50 \mathrm{k}$ and $\mathrm{N}-20-2 \mathrm{k}$ are models intentionally trained with reduced dataset sizes to observe the effect of the dataset size. In Table 6, the win rates of $\mathrm{N}-20-D$ against the target model tend to increase with $D$. It is consistent with Figure 7, where the image generated by $\mathrm{N}-20-2 \mathrm{k}$ is not aligned with the text compared to the results of $\mathrm{N}-20$ and HG-DPO.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-11.jpg?height=1613&width=1396&top_left_y=459&top_left_x=362)

Figure 6: Visualization of the image pool. This figure shows the image pool with the size of 20 for the prompt in the leftmost column. The column labeled as 1st contains images with the highest PickScore, while the column labeled as 20th contains images with the 20th highest PickScore, i.e., the lowest PickScore, in the image pool. By selecting the image with the highest PickScore from this image pool as the winning image and the image with the 20th highest PickScore as the losing image, we magnify the semantic differences between the winning and losing images.

Table 5: Quantitative results according to the AI feedback ranking of the losing images. It shows win-rates (\%) of each model against the target model, $\epsilon_{s f t}$. The superior results of $\mathrm{N}-20-\mathrm{L}-20$ demonstrate that our method of maximizing the PickScore difference between the winning and losing images is effective.

| Configuration | PickScore | HPS | ImageReward | Aesthetic | CLIP |
| :--- | :---: | :---: | :---: | :---: | :---: |
| N-20-L-2 | 66.10 | 67.63 | 63.84 | 55.37 | 54.24 |
| N-20-L-5 | 81.92 | 77.14 | 66.67 | 68.36 | 72.32 |
| N-20-L-10 | 84.18 | 84.57 | 74.01 | 75.14 | 69.49 |
| N-20-L-20 | 88.70 | 89.77 | 76.84 | 75.14 | 74.01 |

Table 6: Quantitative results according to the dataset size. It shows win-rates (\%) of each model against the target model, $\epsilon_{s f t}$. The superior results of $\mathrm{N}-20-100 \mathrm{k}$ demonstrate that a sufficiently large dataset is crucial for achieving good performance. Additionally, this underscores the effectiveness of our AI feedback-based dataset construction method, which allowed us to create such a large dataset, in contrast to relying on human feedback.

| Configuration | PickScore | HPS | ImageReward | Aesthetic | CLIP |
| :--- | :---: | :---: | :---: | :---: | :---: |
| $\mathrm{N}-20-2 \mathrm{k}$ | 74.58 | 79.66 | 73.45 | 74.01 | 58.19 |
| $\mathrm{~N}-20-50 \mathrm{k}$ | 85.88 | 86.29 | 76.27 | 72.88 | 70.06 |
| $\mathrm{~N}-20-100 \mathrm{k}$ | 88.70 | 89.77 | 76.84 | 75.14 | 74.01 |

This highlights the importance of the large-scale dataset for achieving superior performance. Since it is difficult to increase the dataset size when using human feedback, these results demonstrate the superiority of our dataset construction method using AI feedback.

## A. 3 Impact of the Lora Weight on Results

Figure 8 shows images generated by adjusting the LoRA weight of HG-DPO across various random seeds. When $\alpha_{\text {LoRA }}=0$, the images exhibit unnatural poses. As the value increases, the poses become more natural. However, when $\alpha_{\text {LoRA }}=0.8$, while the poses are the most natural, there is a noticeable decrease in diversity, and the background becomes too blurred. To balance between quality and diversity, we use $\alpha_{\text {LoRA }}=0.5$.

## B Limitations of HG-DPO

We have demonstrated that HG-DPO enables the target model to generate images with more natural anatomies and poses, as well as better alignment with text input. However, HG-DPO has some limitations, which we will discuss in this section. We believe that addressing these limitations could be a valuable direction for future research to further enhance HG-DPO.

## B. 1 Trade-off between the Diversity and Quality

We have demonstrated that HG-DPO is effective at generating images with more natural poses and anatomies. However, we also found that as image quality increases, diversity decreases. Each row in Figure 8 shows images generated with the same LoRA weight but different random seeds. In the first row, where the LoRA weight is 0 , the images display the lowest quality but the highest diversity. As the LoRA weight increases, the image quality continuously improves, but the diversity simultaneously decreases. While this is not a perfect solution, users can achieve satisfactory human images by selecting an optimal LoRA weight that balances both quality and diversity.

## B. 2 Limitations in Enhancing Fine Anatomical Details

While HG-DPO significantly enhances human generation in terms of overall anatomy and pose, its impact on fine anatomical details, such as fingers, is relatively limited. In Figure 8, the results with $\alpha_{\text {LoRA }}>0$ still show the unnatural fingers. We believe this is because we use PickScore [23] to

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-13.jpg?height=419&width=1137&top_left_y=235&top_left_x=494)

Prompt : Man and woman, background as ocean view with the blue sky, wearing white suits, trending on instagram

Figure 7: Qualitative results. The images generated by our model, HG-DPO, in the rightmost column have natural poses while aligning well with the given texts compared to other models.

select the winning and losing images, but PickScore does not effectively capture the fine anatomical details. In other words, since the winning image does not exhibit clear superiority over the losing image in terms of the fine anatomical features, HG-DPO is less encouraged to improve these aspects. If an estimator capable of capturing these details were used instead of PickScore, HG-DPO could be further refined to enhance these details, which is a promising avenue for future research.

## B. 3 Effect of $L_{\text {stat }}$ on Improvements Achieved through DPO

In our manuscript, we explained that although $\mathcal{L}_{\text {stat }}$ is effective in resolving the color shift, it slightly diminishes the improvement of DPO. We consider this to be one of the limitations of HG-DPO. However, note that attempting to resolve the color shift using the conventional regularization of Diffusion-DPO results in even greater harm to the improvement effects of DPO.

## C Additional Results of HG-DPO

Figures 9, 10, 11, 12, 13, and 14 show the improvements through HG-DPO. Furthermore, Figures 15 and 16 demonstrates that HG-DPO can be effectively adapted to the personalized T2I model [46]. In Figures 17, we compare HG-DPO and AlignProp [37].

## D Implementation Details

In this section, we provide implementation details on training and inference.

## D. 1 Details on Supervised Fine-Tuning

First, we introduce the method for obtaining $\epsilon_{s f t}$ through supervised fine-tuning.

Fine-tuning dataset. To obtain $\epsilon_{s f t}$, the target model for DPO, we collected 322,461 high-quality Asian images. Each image has a resolution of $704 \times 1024$. We use LLaVa [28] to generate text prompts for all the collected images.

Architecture. Using this dataset, we fine-tuned majicmix-v7 [2], a fine-tuned model of Stable Diffusion 1.5 (SD1.5) specialized in human generation. To maximize the performance of $\epsilon_{s f t}$ in human generation, we fine-tuned majicmix-v7 instead of SD1.5.

Loss function. For fine-tuning, we used the noise prediction loss [21]. Also, we used DDPM noise scheduler [21] for the forward diffusion process during training.

## D. 2 Details on HG-DPO Training

In this section, we provide details on how to improve $\epsilon_{\text {sft }}$ using HG-DPO. This involves training $\epsilon_{\text {sft }}$ with $\mathcal{L}_{H G-D P O}$ using $\mathcal{D}_{H G-D P O}$ as described in our manuscript.

Architecture. Instead of training the all parameters of $\epsilon_{s f t}$ through HG-DPO, we attached LoRA [22] layers to the all linear layers in the attention modules and only train them. We set LoRA rank as 8 .

Multiple random seeds

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-14.jpg?height=1575&width=1395&top_left_y=546&top_left_x=365)

Prompt : A photo of woman playing a guitar in the forest

Figure 8: Qualitative results. This figure shows images generated by varying the LoRA weight, $\alpha_{\text {LoRA }}$, of HG-DPO across multiple random seeds. As $\alpha_{\text {LoRA }}$ increases, the naturalness of the poses in the images improves, but the diversity decreases. To balance quality and diversity, we use $\alpha_{\text {LoRA }}=0.5$

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-15.jpg?height=998&width=1358&top_left_y=260&top_left_x=378)

Prompt: (bust shot:1.2), This is a photography of a female figure skater performing. (arms stretched out overhead), (grinning joyfully:1.2), (wearing a pale blue hue figure skating dress), (adorned with sparkling array of red and blue crystals), (it densely packed at the neckline and scatter out to downwards), (a figure skater's typical hairstyle), neat low bun, (in the ice rink), (Depth of field), professional photo

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-15.jpg?height=992&width=1357&top_left_y=1339&top_left_x=384)

Prompt: (medium close up frontal portrait shot), (horizontal composition:1.2), centered composition, (wearing a white sleeveless dress:1.2), (immerse in a sunflower field:1.2), (bright face:1.83), Depth of Field, a soft focus and muted colors, (analog film photograph:1.6), lifelike photo, natural long dark hair

Figure 9: HG-DPO vs without HG-DPO. The left and right columns show the generated images with HG-DPO and without HG-DPO, which corresponds to the target model, respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-16.jpg?height=1016&width=1391&top_left_y=240&top_left_x=367)

Prompt: (frontal portrait:1.2) of a girl, (wavy dark hair with white fragipani flower tucked in her ear:1.2), background as (hawaiian beach:1.3), wearing (a simple colored strappy light green dress:1.1), trending on instagram, bright weather, sunny day, natural setting

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-16.jpg?height=1014&width=1388&top_left_y=1336&top_left_x=366)

Prompt: (frontal portrait:1.2), (wearing a white knit with a scoop neckline:1.3), dreamlike (medium shot:1.2) instagram photo, looking back, soft smile, light brown (middle length hair:1.3), natural light, (dark pink tulips flower field:1.3) as background, aesthetic, (intricate detail:1.2), Perfect facial expression

Figure 10: HG-DPO vs without HG-DPO. The left and right columns show the generated images with HG-DPO and without HG-DPO, which corresponds to the target model, respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-17.jpg?height=1005&width=1377&top_left_y=251&top_left_x=366)

Prompt: (frontal upper body portrait of a woman, straight posture, looking front:1.6), (indoor, huge window with a beautiful scenery of winter landscape, decorated moody christmas lights and garments:1.4), (long wavy hair: 1.2), pretty bright face, ultra high quality, (wearing blue knitted sweater in female trendy wear collections:1.3), hyperrealistic, (trending on instagram:1.2), (wavy hair:1.2), (Christmas vibe:1.2), (cozy:1.2), (warm lighting on face:1.2), beauty filters, UHD, HDR, studio lighting, (centered composition:1.2)

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-17.jpg?height=1013&width=1377&top_left_y=1361&top_left_x=369)

Prompt: (medium close up frontal portrait shot), (dynamic amusement park as background:1.2), (wearing a white sweatshirt:1.3), (holding an huge soft ice cream cone in one hand:1.2), (lush and long wavy ponytail:1.2), (Depth of field), lifelike photo, shallow low angle

Figure 11: HG-DPO vs without HG-DPO. The left and right columns show the generated images with HG-DPO and without HG-DPO which corresponds to the target model, respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-18.jpg?height=1022&width=1392&top_left_y=240&top_left_x=364)

Prompt: (upper body shot:1.2), (frontal portrait:1.2), This is a photograph of a female volleyball player. (simple solid bright color backdrop), wearing (Premier league volleyball uniform:1.2), long hair, (holding a volleyball in the front:1.2), (centered composition), professional photograph, (front lighting)

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-18.jpg?height=1005&width=1385&top_left_y=1335&top_left_x=367)

Prompt: (frontal upper body portrait of a woman holding a whole cake:1.4), wearing a (brown vintage plaid Pinafore overall dress, lace trimmed white collar academia blouse under:1.4), (neat beautiful long brunette wavy hair:1.5), (bright porcelain face:1.2), sharp focus, (face focused), front on, softly smiling, (cozy room background with holiday settings:1.2), holiday mood, (high color temperature, blurry background with bokeh:1.5), anatomically correct features

Figure 12: HG-DPO vs without HG-DPO. The left and right columns show the generated images with HG-DPO and without HG-DPO which corresponds to the target model, respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-19.jpg?height=1022&width=1396&top_left_y=251&top_left_x=359)

Prompt: (bust shot:1.2) a young asian man is holding a baseball bat (wearing black suit:1.2) at a baseball stadium, professional photograph, looking front, (natural hairstyle:1.2)

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-19.jpg?height=1014&width=1391&top_left_y=1339&top_left_x=367)

Prompt: A man is in a shooting stance with a pistol in the library wearing knit sweater, professional photograph, looking front, (bust shot:1.2)

Figure 13: HG-DPO vs without HG-DPO. The left and right columns show the generated images with HG-DPO and without HG-DPO which corresponds to the target model, respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-20.jpg?height=1009&width=1377&top_left_y=257&top_left_x=366)

Prompt: A young asian man is riding a white horse in the forest wearing black bomber leather jumper with vintage patches

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-20.jpg?height=1006&width=1377&top_left_y=1340&top_left_x=366)

Prompt: A man is sitting on a white chair wearing a yellow cardigan and blue jean. He has his hand on his chin. There is bouquet of flowers on the floor next to him. professional photograph, looking front, (bust shot:1.2)

Figure 14: HG-DPO vs without HG-DPO. The left and right columns show the generated images with HG-DPO and without HG-DPO which corresponds to the target model, respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-21.jpg?height=1019&width=1391&top_left_y=236&top_left_x=367)

Prompt: A woman is wearing a white coat, blue jean, and black knit sweater next to a tree with lush foliage standing on a snow-covered plain, (bust shot:1.2)

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-21.jpg?height=1009&width=1388&top_left_y=1325&top_left_x=366)

Prompt: A woman wearing a blue long-sleeve shirt and white pants is on a yacht with an island visible in the background. she has long wave hair. professional photograph

Figure 15: HG-DPO vs without HG-DPO. The left and right columns show the generated images with HG-DPO and without HG-DPO which corresponds to the target model, respectively. The image at the top right is the concept image of the desired identity.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-22.jpg?height=1017&width=1390&top_left_y=234&top_left_x=365)

Prompt: A man is holding a dog in the house. He is wearing gray sweatshirt and black pants, sitting on the sofa with the sunset reflecting through the window. professional photograph, (bust shot:1.2)

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-22.jpg?height=1011&width=1388&top_left_y=1321&top_left_x=366)

Prompt: A man is standing in an amusement park at night, wearing a navy suit, holding big red wine glass. Fireworks are going off in the background. professional photograph, (bust shot:1.2)

Figure 16: HG-DPO vs without HG-DPO. The left and right columns show the generated images with HG-DPO and without HG-DPO which corresponds to the target model, respectively. The image at the top right is the concept image of the desired identity.

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-23.jpg?height=531&width=677&top_left_y=241&top_left_x=385)
is riding a camel in the desert, wearing a
professional photograph, (bust shot:1.2)

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-23.jpg?height=513&width=693&top_left_y=795&top_left_x=377)

Prompt: A little girl is holding a teddy bear at home. professional photograph

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-23.jpg?height=536&width=694&top_left_y=239&top_left_x=1060)

professional photograph, (bust shot:1.2), (frontal portrait:1.2)

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-23.jpg?height=509&width=697&top_left_y=797&top_left_x=1061)

Prompt: A little boy is holding a camera at the amusement park.

Figure 17: HG-DPO vs AlignProp. The left and right images in each subfigure show the generated images with HG-DPO and AlignProp, respectively, by using each prompt. In the case of images generated by AlignProp, they lose its original styles (e.g., realistic images of Asians) and are altered to distinct styles (e.g., artistic images of Westerners). As we described in Section 1 , in the practical scenarios where human generation models are deployed, the transformation in the styles of generated images can be critical because it can prevent the user from generating images in their target styles.

Loss function. For $\mathcal{L}_{H G-D P O}$, we set $\beta=2500$ and $\lambda_{\text {stat }}=10000$. Also, we used DDPM noise scheduler [21] for the forward diffusion process during training. Note that $\mathcal{L}_{H G-D P O}$ also requires the forward diffusion process as the noise prediction loss [21]. Also, for the latent sampling in $\mathcal{L}_{\text {stat }}$, we used DDPM sampler [21]. We tried DDIM sampler [48], but there was no significant difference. In addition, classifier-free guidance [20] is not used during the latent sampling in $\mathcal{L}_{H G-D P O}$.

Optimization. For the optimization, we set the local batch size to four, which corresponds to the total batch size to 16 because we used four NVIDIA A100 GPUs. As an optimizer, we use the 8 -bit Adam optimizer [15] with $\beta_{1}$ and $\beta_{2}$ of the Adam optimizer to 0.9 and 0.999 , respectively, and the learning rate to $1 e-5$. Additionally, we utilize mixed precision for efficient training.

## D. 3 How to Adapt HG-DPO to Personalized T2I model

To adapt HG-DPO to the personalized T2I model, we firstly trained InstantBooth [46] using $\epsilon_{\text {sft }}$ as the backbone. After training InstantBooth, we can seamlessly adapt the pre-trained HG-DPO LoRA layers to InstantBooth because they share the same backbone, $\epsilon_{s f t}$.

## D. 4 Image Sampling

Prompts of Figure 1 in our manuscript. The prompts listed below are the ones used to generate the images in Figure 1, in order from the top left to the bottom right.

A man in a black suit is walking on the dark street

![](https://cdn.mathpix.com/cropped/2024_06_04_df9dd187a2912405a92ag-24.jpg?height=515&width=434&top_left_y=309&top_left_x=1320)

Figure 18: User study interface. We conduct the user study by providing a prompt and two images, asking users to choose the one that appeared more natural.

A woman in a futuristic cyberpunk world, with a neon-lit city background, backlit by vibrant city glow A woman singing on the stage, $8 k$, photo realistic

Asian man in dark cathedral, magnificent, medieval armor with complicated decorations, light from the stained-glass windows flashes him

High-fashion photography of an asian man wearing a beige coat in an abandoned industrial warehouse, with dramatic lighting and edgy outfits, $8 k$

A woman, film grain, overexposed, long grass, wind, white sundress, fresh, outdoor photography, large aperture, highres, realistic photography

A close-up shot of woman with a short black hair and beautiful smile in the cafe. She is wearing a blue dress

A portrait photo of a girl, centered, highly detailed face, depth of field, moody light, golden hour, sunset, faint, dim, idyllic

Closeup portrait photo of goth asian man, makeup, black jacket, $8 k$

Sampling hyperparameters. DPMSolverMultistepScheduler [32] in diffusers [50] is used with the step size of 50 for sampling the images. In addition, Classifier-free guidance [20] is used, with the guidance scale of 5.0 .

## E How to conduct user study

We conduct the user study to demonstrate the superiority of HG-DPO in generating more natural images compared to other baselines. As shown in Figure 18, we create a web user interface that ask participants to choose the more natural image between two options.

## F Broader Impacts

We recognize the potential negative societal impacts of our work. Since our method can generate highquality human images, it could be misused to create malicious fake images, especially when combined with personalized T2I models. It can cause significant harm to specific individuals. However, our work can also have positive impacts on society when used beneficially, such as in the entertainment or film industries. For instance, users can create desired high-quality profile pictures using text input. It highlights the beneficial uses of our work.

## References

[1] Civitai. https://civitai.com/

[2] majicmix realistic. https://civitai.com/models/43331/majicmix-realistic

[3] Pixai. https://pixai.art/.

[4] K. Black, M. Janner, Y. Du, I. Kostrikov, and S. Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023.

[5] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE international conference on automatic face \& gesture recognition (FG 2018), pages 67-74. IEEE, 2018.

[6] C. Chen, A. Wang, H. Wu, L. Liao, W. Sun, Q. Yan, and W. Lin. Enhancing diffusion models with text-encoder reinforcement learning. arXiv preprint arXiv:2311.15657, 2023.

[7] H. Chen, Y. Zhang, S. Wu, X. Wang, X. Duan, Y. Zhou, and W. Zhu. Disenbooth: Identitypreserving disentangled tuning for subject-driven text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023.

[8] X. Chen, L. Huang, Y. Liu, Y. Shen, D. Zhao, and H. Zhao. Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481, 2023.

[9] Z. Chen, Y. Deng, H. Yuan, K. Ji, and Q. Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024.

[10] P. Cheng, Y. Yang, J. Li, Y. Dai, and N. Du. Adversarial preference optimization. arXiv preprint arXiv:2311.08045, 2023.

[11] K. Clark, P. Vicol, K. Swersky, and D. J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023.

[12] J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023.

[13] X. Dai, J. Hou, C.-Y. Ma, S. Tsai, J. Wang, R. Wang, P. Zhang, S. Vandenhende, X. Wang, A. Dubey, et al. Emu: Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.

[14] J. Deng, J. Guo, N. Xue, and S. Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690-4699, 2019.

[15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021.

[16] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780-8794, 2021.

[17] K. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, and D. Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.

[18] Y. Fan, O. Watkins, Y. Du, H. Liu, M. Ryu, C. Boutilier, P. Abbeel, M. Ghavamzadeh, K. Lee, and K. Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024.

[19] S. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696-10706, 2022.

[20] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.

[21] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020.

[22] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

[23] Y. Kirstain, A. Polyak, U. Singer, S. Matiana, J. Penna, and O. Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.

[24] T. Korbak, K. Shi, A. Chen, R. V. Bhalerao, C. Buckley, J. Phang, S. R. Bowman, and E. Perez. Pretraining language models with human preferences. In International Conference on Machine Learning, pages 17506-17533. PMLR, 2023.

[25] K. Lee, H. Liu, M. Ryu, O. Watkins, Y. Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S. Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023.

[26] S. Li, J. Fu, K. Liu, W. Wang, K.-Y. Lin, and W. Wu. Cosmicman: A text-to-image foundation model for humans. arXiv preprint arXiv:2404.01294, 2024.

[27] S. Li, K. Kallidromitis, A. Gokul, Y. Kato, and K. Kozuka. Aligning diffusion models by optimizing human utility. arXiv preprint arXiv:2404.04465, 2024.

[28] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In NeurIPS, 2023.

[29] T. Liu, Y. Zhao, R. Joshi, M. Khalman, M. Saleh, P. J. Liu, and J. Liu. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023.

[30] T. Liu, Z. Qin, J. Wu, J. Shen, M. Khalman, R. Joshi, Y. Zhao, M. Saleh, S. Baumgartner, J. Liu, et al. Lipo: Listwise preference optimization through learning-to-rank. arXiv preprint arXiv:2402.01878, 2024.

[31] W. Liu, X. Wang, M. Wu, T. Li, C. Lv, Z. Ling, J. Zhu, C. Zhang, X. Zheng, and X. Huang. Aligning large language models with human preferences through representation engineering. arXiv preprint arXiv:2312.15997, 2023.

[32] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.

[33] J. Ma, J. Liang, C. Chen, and H. Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. arXiv preprint arXiv:2307.11410, 2023.

[34] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.

[35] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.

[36] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.

[37] M. Prabhudesai, A. Goyal, D. Pathak, and K. Fragkiadaki. Aligning text-to-image diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023.

[38] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.

[39] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.

[40] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.

[41] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.

[42] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234-241. Springer, 2015.

[43] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500-22510, 2023.

[44] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35: $36479-36494,2022$.

[45] C. Schuhmann. Laion-aesthetics. https://laion.ai/blog/laion-aesthetics/ 2022.

[46] J. Shi, W. Xiong, Z. Lin, and H. J. Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023.

[47] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for human alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18990-18998, 2024.

[48] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.

[49] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020 .

[50] P. von Platen, S. Patil, A. Lozhkov, P. Cuenca, N. Lambert, K. Rasul, M. Davaadorj, D. Nair, S. Paul, W. Berman, Y. Xu, S. Liu, and T. Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022.

[51] B. Wallace, M. Dang, R. Rafailov, L. Zhou, A. Lou, S. Purushwalkam, S. Ermon, C. Xiong, S. Joty, and N. Naik. Diffusion model alignment using direct preference optimization. arXiv preprint arXiv:2311.12908, 2023.

[52] B. Wallace, A. Gokul, S. Ermon, and N. Naik. End-to-end diffusion latent optimization improves classifier guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7280-7290, 2023.

[53] X. Wu, Y. Hao, K. Sun, Y. Chen, F. Zhu, R. Zhao, and H. Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023.

[54] Z. Wu, Y. Hu, W. Shi, N. Dziri, A. Suhr, P. Ammanabrolu, N. A. Smith, M. Ostendorf, and H. Hajishirzi. Fine-grained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems, 36, 2024.

[55] G. Xiao, T. Yin, W. T. Freeman, F. Durand, and S. Han. Fastcomposer: Tuning-free multi-subject image generation with localized attention. arXiv preprint arXiv:2305.10431, 2023.

[56] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.

[57] K. Yang, J. Tao, J. Lyu, C. Ge, J. Chen, Q. Li, W. Shen, X. Zhu, and X. Li. Using human feedback to fine-tune diffusion models without any reward model. arXiv preprint arXiv:2311.13231, 2023.

[58] T. Yoon, K. Myoung, K. Lee, J. Cho, A. No, and E. Ryu. Censored sampling of diffusion models using 3 minutes of human feedback. Advances in Neural Information Processing Systems, 36, 2024.

[59] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.

[60] H. Yuan, Z. Chen, K. Ji, and Q. Gu. Self-play fine-tuning of diffusion models for text-to-image generation. arXiv preprint arXiv:2402.10210, 2024.

[61] Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.


[^0]:    *Equal contribution

    ${ }^{\dagger}$ Corresponding author

