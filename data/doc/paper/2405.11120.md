# Latent State Estimation Helps UI Agents to Reason 

William E. Bishop<br>Google Research

Alice Li<br>Google Research

Christopher Rawles<br>Google Research

Oriana Riva<br>Google Research


#### Abstract

A common problem for agents operating in real-world environments is that the response of an environment to their actions may be non-deterministic and observed through noise. This renders environmental state and progress towards completing a task latent. Despite recent impressive demonstrations of LLM's reasoning abilities on various benchmarks, whether LLMs can build estimates of latent state and leverage them for reasoning has not been explicitly studied. We investigate this problem in the real-world domain of autonomous UI agents. We establish that appropriately prompting LLMs in a zero-shot manner can be formally understood as forming point estimates of latent state in a textual space. In the context of autonomous UI agents we then show that LLMs used in this manner are more than $76 \%$ accurate at inferring various aspects of latent state, such as performed (vs. commanded) actions and task progression. Using both public and internal benchmarks and three reasoning methods (zero-shot, CoT-SC \& ReAct), we show that LLM-powered agents that explicitly estimate and reason about latent state are able to successfully complete up to $1.6 \mathrm{x}$ more tasks than those that do not.


## 1 Introduction

While latent-state estimation plays a prominent role in many machine learning models for science and engineering [6, 5, 10], to the best of our knowledge the ability of LLMs to estimate latent state and use these estimates to improve decision making when performing tasks has not been studied. In this work, we investigate these problems in the emergent domain of LLM-based autonomous UI agents [22, 36, 41], in which agents freely interact with the user interface of an application or website (e.g., by clicking, typing and scrolling) to accomplish a variety of goals expressed in natural language. We choose this domain because it encompasses many real-world use cases and the number and types of tasks an agent is tested against can be easily scaled. Most importantly for our purposes, latent state also arises prominently (see Fig. 1). This is because textual representations of screens are formed from noisy accessibility or DOM trees or the imperfect output of object detection models [9, 39]. Additionally, performed actions may not match those commanded by an agent (due to grounding errors) or may have unexpected results. These factors render important aspects of UI and task state, such as what screen an agent is currently on, action outcomes, and progress towards a goal, latent.

In our approach, we seek to leverage the intuitive knowledge about the world encoded in LLMs to reason in a zero-shot manner from noisy observations about latent variables. In the domain of UI automation, this intuitive knowledge includes knowledge of user applications and their functionality, UI concepts, and task flows, and the latent variables we seek to infer include those useful for understanding where an agent is in an app and what it has accomplished towards a goal. Latent variable inference is challenging because there are often multiple values of the hidden variables that might explain the noisy and partial observations, and it is often only by reasoning across multiple observations and using prior modeled knowledge of the environment that reliable inferences can be made. We hypothesize that by prompting general pre-trained LLMs appropriately they can leverage the knowledge of the world encoded in their weights to assign high probabilities to completions that best explain observations, thereby performing latent variable estimation. Once some aspects of latent

![](https://cdn.mathpix.com/cropped/2024_06_04_0d6e8cefcd67894bb28fg-02.jpg?height=748&width=1377&top_left_y=239&top_left_x=363)

Figure 1: A conceptual example of an agent performing a task while inferring latent state. The agent only perceives the screen through textual representations (depicted as outlined diagrams for each screen) that may contain limited and noisy information about what is actually on the screen (shown in color in the lower right for each screen). Additionally, grounding errors (e.g., typing the wrong text into a box) can occur thus causing uncertainty about performed actions. This means important aspects of UI and task state (e.g., high-level screen understanding, previously performed actions, progression from start) which can only be observed through these noisy representations are latent. We seek to explicitly estimate these and use these estimates to improve the selection of actions at each step.

state are estimated, they can be provided in prompts to estimate other aspects of latent state and ultimately to select next actions to take, thus improving the performance of LLM-based agents.

Overall, we make the following contributions: (i) we present a general method for forming point estimates of latent state in a zero-shot manner using LLMs, (ii) we demonstrate that our methods can estimate five aspects of latent state for autonomous UI agents, matching or outperforming the performance of humans, and (iii) we show that estimates of latent state can be naturally incorporated into existing reasoning techniques (zero-shot [33], CoT-SC [32], ReAct [38]) to consistently improve decision making for UI agents. Importantly, we establish these results on online versions of three benchmarks: PixelHelp [22], AndroidInTheWild [27], and an internal benchmark, comprising in total 135 unique tasks from 48 apps/websites. This is notable because, as observed by others [41], online testing is key to realistically assessing agent performance but is often skipped in favor of less realistic, but easier to implement, offline evaluation on pre-recorded data.

## 2 Related Work

Latent state estimation Latent state arises in a variety of domains and modeling applications [e.g., 6, 5, 10] where underlying variables of interest cannot be observed directly but instead only inferred from other variables. Classic approaches to estimating latent state include principal component analysis [26], factor analysis [29], Kalman filtering [17], and Hidden Markov models [4], among others. Common to these and many other approaches is that latent state is modeled with continuous or discrete vectors and, before latent state can be inferred at test time, models must first be fit to application-specific training data. Our work is different in that we seek to model latent state in a textual space by leveraging general, pre-trained LLMs without additional task-specific fine-tuning. With the exception of recent work that uses LLMs to infer politicians' latent positions [35], to the best of our knowledge our work is the first to formalize the general problem of using LLMs to estimate latent state and to show how this can be applied to improve autonomous UI agents.

Reasoning with LLMs Our work fits into the broader research area of reasoning with LLMs. Increasingly sophisticated means of performing reasoning, decision making, and planning with LLMs have been proposed [33, 32, 38, 42, 20, 37, 34]. This line of work is predominantly concerned with the means of reasoning. Our work is orthogonal to it as it is concerned with the content of reasoning that is showing that LLMs have the ability to reason about latent state. Having established this basic ability to reason over latent state, it is likely that absolute performance can be improved in the future by investigating different means of reasoning, using better base models, etc.

Autonomous UI agents There is a rapidly growing body of work on UI agents [22, 15, 28, 31, 18, 36, 41, 19, 12]. Our work is the first to explicitly recognize that important aspects of UI state and task progress are latent and to propose methods for LLM-based agents to address the challenges that arise from it.

Additionally, our work differs from much of this prior work in two other ways. First, our aim is not to develop the best-performing agent, a goal which would require attention to many more aspects such as how UI screens are represented, how grounding is performed, what base models are used, etc. Instead, our goal is to show the relative improvements possible when agents explicitly incorporate reasoning about latent state in their planning, a general finding we believe can be used to improve other LLM-based agents. Second, most prior UI agent systems are tested only against pre-recorded datasets [2, 31, 36, 27, 7, 22, 12], with only a small number tested online [19, 13, 41]. As explained in $\$ 5$, testing online, where errors are allowed to accumulate, is critical for observing the benefits of latent state estimation for agent performance.

Reinforcement learning and robotics Formally, the problem of completing tasks by driving application UIs can be described as a partially-observable Markov decision process (POMDP) [16], where the state of the UI and progress towards a goal are hidden variables. While others have applied LLMs to related reinforcement learning problems [1, 14, 23], the novel aspect of our work is that we use LLMs to infer the hidden state of a system that can be modeled as a POMDP.

## 3 Estimating latent state with LLMs

We consider the problem of estimating the latent state of a system at time $t, s_{t}$, given a set of observations observed up until $t,\left\{o_{i}\right\}_{i=1}^{t}$. A standard approach to forming a point estimate, $\hat{s}_{t}$, for latent state is to calculate

$$
\begin{equation*}
\hat{s}_{t}=\operatorname{argmax}_{s_{t}} p\left(s_{t} \mid\left\{o_{i}\right\}_{i=1}^{t}\right) \tag{1}
\end{equation*}
$$

Typically, $s_{t}$ and $o_{t}$ belong to Euclidean or discrete spaces, and models that are specifically fit using data collected for particular applications of interest are used to calculate the required probabilities. The key innovation of our work is to recognize that in many scenarios latent state and observations can be described in language. This opens up the possibility of using pre-trained LLMs to calculate the probabilities in 1 .

The general approach can be formalized as follows. First, we assume that there is a set of $A$ different aspects of latent state (e.g., high-level screen description, past actions, etc...) we desire to estimate at each time $t$ that can be described in language. We refer to the description of aspect $a$ at time $t$ as $s_{t}^{a}$, so that the full description of latent state at time $t$ is $s_{t}=\left\{s_{t}^{a}\right\}_{a=1}^{A}$. At each time $t$, we use an LLM to estimate each aspect of latent state, and assume aspects are ordered in such a way that the estimate for one aspect (e.g., inferences of past actions, detected mistakes) are computed before and can inform the estimate for others (e.g., inference of progress towards a goal). Specifically, at time $t$ we allow estimates of the $a^{t h}$ aspect of latent state to depend on observations up until time $t,\left\{o_{i}\right\}_{i=1}^{t}$, estimates of latent state for all previous time steps, $\left\{s_{i}\right\}_{i=1}^{t-1}$, and estimates of aspects of latent state ordered before $a$ for the current time step, $\left\{s_{t}^{i}\right\}_{i=1}^{a-1}$. For each aspect of latent state we then use a user-defined mapping (e.g., a heuristic), $f^{a}$, to form a prompt $z_{t}^{a}=f^{a}\left(\left\{o_{i}\right\}_{i=1}^{t},\left\{s_{i}\right\}_{i=1}^{t-1},\left\{s_{t}^{i}\right\}_{i=1}^{a-1}\right)$. Given this prompt, an LLM can be used to calculate the probabilities $p\left(s_{t}^{a} \mid z_{t}^{a}\right)$. Ideally, we would form a point estimate $\hat{s}_{t}^{a}=\operatorname{argmax}_{s_{t}} p\left(s_{t} \mid z_{t}^{a}\right)$, but this is a computational prohibitive optimization, so in this work we approximate the mode with greedy decoding. Additional strategies such as sampling or beam search could be explored as well.

This general approach of using LLMs to estimate latent state has multiple benefits. First, it requires no application-specific training data. Instead, it leverages the "intuitive knowledge" LLMs have about the

![](https://cdn.mathpix.com/cropped/2024_06_04_0d6e8cefcd67894bb28fg-04.jpg?height=588&width=662&top_left_y=237&top_left_x=360)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_0d6e8cefcd67894bb28fg-04.jpg?height=431&width=642&top_left_y=386&top_left_x=1099)

(b)

Figure 2: (a) Examples of partial and noisy information in a representative screen description. The large image and the small heart icon at the bottom-right are represented as "images" without additional metadata. One third of the screen representation (in orange) refers to elements that are not visible on the screen. Finally, the three icons in the navigation bar are labeled simply as images, while they are actually clickable icons. (b) Process for inferring 5 aspects of latent state and selecting actions at each step of a task. Task completion is inferred after action selection, as we find providing contemplated next actions improves performance at detecting completion. The action is performed only if the the task is inferred as not done.

world to determine underlying state from noisy observations. Second, estimates of different aspects of latent state can be formed by simply using different prompts. Third, since they are expressed in language, latent state estimates are directly interpretable by humans. Finally, latent state estimated in this way can be fed to additional LLM calls (e.g., reasoning over actions to take).

## 4 Latent state estimation for UI automation

Why does latent state arise in UI tasks? Latent state arises in autonomous UI agents for at least two prominent reasons. First, observations, in the form of textual descriptions of screens and derived either from the output of object detection models or accessibility or DOM trees, are partial and noisy. In this work, we use descriptions derived from accessibility trees (see appendix A for details), but the issues we highlight are general. As shown in Fig. $2 \mathrm{a}$, in these descriptions elements may be missing key metadata necessary for understanding their content and function (partial information), background content invisible to the user may be present (noise) and the same type of elements, such as images, may be used as both actionable and static elements (so the element type is a noisy indicator of element function). Many other examples of how screen descriptions can be partial and noisy are provided in appendix B Second, actions selected by an agent may not be performed as expected, due to grounding errors. Grounding errors can occur in multiple forms such as failing to perform any action, performing an action (e.g,. clicking) on the wrong element, or performing the wrong action on the right element (e.g., typing the wrong thing in the right text field). In addition to grounding errors, stochasticity in the environment can lead to unexpected action outcomes, such as the appearance of pop-up adds, an apparent screen freezing due to system delays or receiving unexpected dynamic content.

In the context of UI automation, we explore estimating $A=5$ aspects of latent state at each time step: (1) previous actions, (2) screen summaries, (3) progression, (4) previous mistakes, and (5) task completion. The above considerations mean all five of these aspects of state are latent. Previous actions are latent due to the unreliability of action grounding. The noisy and partial nature of screen representations means that true screen content, and therefore quantities derived from them, such as screen summaries, are latent. We define progression as a summary of what an agent has done from the start of a task (roughly analogous to integrating over actions at single time steps in classic latent dynamical system models [17]). Progression, presence of previous mistakes, and task completion

Table 1: Statistics of benchmarks used in this work. Pooled numbers reflect that some apps are present in multiple benchmarks.

|  | \# tasks | \# apps/websites |
| :--- | :---: | :---: |
| PixelHelp (PH) | 35 | 4 |
| Android in the Wild (AitW) | 50 | 19 |
| Android-50 (A-50) | 50 | 33 |
| Total | $\mathbf{1 3 5}$ | $\mathbf{4 8}$ |

characterize in different ways the state of an agent's progress towards goal completion, and since they can only be inferred from noisy observations and unreliable action commands, are also latent. Fig. $2 \mathrm{~b}$ shows the order in which each aspect is estimated and how the estimates chain together. Relating this to the notation in Sec. 3, at time $t$, each aspect corresponds to a different $s_{t}^{a}$ and is estimated with a separate prompt, $z_{t}^{a}$. We provide the full LLM prompts in appendix $\mathrm{E}$

## 5 Methodology and benchmarks

Online e2e evaluation We seek to evaluate our methods in the most realistic manner possible, and for this reason do not evaluate on pre-recorded datasets [22, 27, 36]. When testing on pre-recorded datasets, agents are shown the correct next screen, irrespective of their actions. This makes measuring realistic performance difficult, as inferring the state of a UI and progress towards a goal becomes harder as past mistakes in perception, decision making, and action execution accumulate, which is preempted when testing on prerecorded data.

Hence, we test our methods with three online Android benchmarks. Agents are provided with a natural language goal. They then select their own actions and path through a UI on an emulated Android device using AndroidEnv [30], receiving observations in the form of textual descriptions of UI screens formed from the Android accessibility tree (appendix A) after each action. Agents perform a task until they determine it is complete or a termination criteria (max number of steps or 3 identical repeated actions) is reached. The emulated devices used ran full versions of the Android operating system with the same application binaries that would be installed on real devices and agents received observations and performed actions in the same way they would on a physical device. For these reasons we are confident results reported here would translate to performance on physical devices.

To test against a wide range of tasks and apps, we use three benchmarks. Two of the benchmarks, PixelHelp (PH) [22] and Android In The Wild (AitW) [27], are formed by randomly selecting task goals from the corresponding datasets subject to minor constraints (e.g., ensuring tasks can still be feasibly performed in the present day). The third benchmark, Android-50 (A-50), was collected by us using annotators (see appendix Cfor details). A wide variety of real-world apps and tasks are present across this set of benchmarks so that a broad range of factors leading to partial and noisy screen representations and latent state in real-world use are well represented. Table 1 contains statistics for each benchmark, and a random subset of the tasks in each benchmark is provided in appendix G AndroidEnv and both AitW and $\mathrm{PH}$ are all available under the Apache 2.0 license, and our use was in accordance with the license terms.

Agent architecture We used agents composed of two modules for (1) selecting the next action and (2) grounding selected actions in the current screen. Across experiments, we only varied (1) and held the grounder constant. This meant that main factors that give rise to latent state, unfaithful screen representations and unreliable action execution, were constant across experiments, so we could directly compare the effects of varying the reasoning, including using latent state estimates or not, for selecting next actions. Grounding was performed with an LLM-based method adopting prior work [27,31] and supported clicking, typing, navigating backwards, and opening apps. See appendix D for more details.

Reasoning methods We implement three different methods for prompting LLMs to produce reasoning when selecting the next action of each task step: zero-shot [33], CoT-SC [32] and ReAct [38] based reasoning. We implement two versions of each of these methods that either use (denoted by a + )

Table 2: Accuracy of the LLM at inferring various aspects of latent state across all benchmarks (left) and in comparison to humans (right).

|  | evaluated on 40 tasks |  |  |  |
| :--- | :---: | :---: | :---: | :---: |
|  | $\mathbf{P H}$ | AitW | A-50 | total |
| previous action | 90.0 | 77.6 | 95.2 | 89.4 |
| screen summary | 90.0 | 93.4 | 91.2 | 91.5 |
| progression | 94.3 | 72.4 | 91.8 | 87.4 |
| previous mistakes | 80.0 | 68.4 | 79.6 | 76.8 |
| task completion | 100.0 | 100.0 | 94.6 | 97.3 |


| evaluated on 5 tasks |  |
| :---: | :---: |
| LLM | human |
| 96.3 | 96.3 |
| 88.9 | 77.8 |
| 96.3 | 70.4 |
| 88.9 | 81.5 |
| 96.3 | 77.8 |

or do not use (denoted by a -) latent state estimates. This allows us to perform pair-wise comparisons. We choose these three methods as they represent a well-known but diverse set of ways for eliciting reasoning from LLMs with various trade-offs such as the use of few-shot examples (CoT-SC \& ReAct) or not (zero-shot) as well as the amount of computation performed before selecting an action, with CoT-SC and ReAct processing more tokens than zero-shot before predicting an action.

Implementation details of all methods can be found in appendix F In all cases, the textual description of the current screen and task goal was provided in the prompt. Estimates of progression and mistakes were additionally provided in the prompts of reasoning methods that used latent state, while agents without latent state only had access to the history of commanded actions (a common approach in UI agents [27, 41]). Stopping was based on estimates of completion for agents with latent state estimation, while agents without latent state estimation directly predicted stopping as an extra type of action. We note that given the length of our screen representations, we implement a modified version of ReAct that retains only the last two previous observations and provides only a few action-thought pairs as few-shot examples at the beginning of the prompt (see F. 5 and F. 6 in the appendix for full prompts). Examples from three tasks, distinct from those in the benchmarks, were selected for inclusion in the prompts for CoT-SC and ReAct. The same examples were used for each and were held fixed after they were selected, so as not to overfit to the benchmarks. We used a general pre-trained LLM, PaLM 2 text-unicorn [11], for all LLM calls. The temperature was set to 0, except when predicting actions with CoT-SC, in which case 8 samples were drawn with a temperature of .5 .

## 6 Results and analysis

### 6.1 LLMs estimate latent state for UI agents

We now investigate the ability of our methods to estimate latent state for UI agents. We ran the zero-shot+ agent on each benchmark and we asked trained annotators to answer a series of "yes" or "no" questions to assess the accuracy of the estimate of each aspect of latent state listed in $\$ 4$ at each step. Example questions include "Was the action that caused the transition between Step 1 and Step 2 inferred correctly?" or "Does the mistake assessment correctly capture the mistakes (if any) that have been made up to Step 1 and are not yet corrected?" The full set of questions can be found in appendix H] Annotators used throughout this study signed a data usage agreement, were fairly compensated and provided no personal data as part of this study. This is a costly analysis, so we only performed this on a random subset of 10 tasks from PixelHelp, 10 tasks from AitW, and 20 from Android-50. We sampled more from Android-50 because of its greater diversity of apps and websites. Cost is also the reason we only analyzed latent state estimates produced by the zero-shot+ agent. We believe this is justified because the chain of LLM calls for estimating latent state is the same across all tested methods for selecting next actions.

Table 2 summarizes our findings. We observe that the basic prompting strategy outlined in $\$ 4$ estimated all five aspects of latent state with remarkably high accuracy, i.e., $76.8 \%-97.3 \%$. Three aspects of latent state: previously performed actions, mistakes, and completion, could potentially be estimated with relatively high accuracy by naïve baselines that always predict the defaults, i.e., the performed action matches the commanded one, no mistakes are made, and the task is not done for all steps. We determined that in all cases these naïve baselines would achieve lower performance (previous action accuracy of $85 \%$, a mistake assessment accuracy of $74.0 \%$, and a task completion accuracy of $92.8 \%$ ) than that achieved by the LLM. We also inspected the "hard" cases for each of

Table 3: Performance (as percentages) on all three benchmarks for UI agents which incorporated (denoted by + ) or did not incorporated (denoted by -) estimates of latent state in selecting actions at each step.

|  | task success $\uparrow$ |  |  |  | partial completion $\uparrow$ |  |  |  | task success with strict stop $\uparrow$ |  |  |  | premature stop $\downarrow$ |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | PH | AitW | A-50 | tot | PH | AitW | A-50 | tot | $\mathbf{P H}$ | AitW | A-50 | tot | PH | AitW | A-50 | tot |
| $\mathrm{ze} \quad-\quad-1-1$ | 62.9 | 46.0 | 34.0 | 45.9 | 73.7 | 61.2 | 57.0 | 62.1 | 62.9 | 40.0 | 28.0 | 41.5 | 11.4 | 12.0 | 24.0 | 16.3 |
| zero-shot - | 45.7 | 24.0 | 20.0 | 28.1 | 66.3 | 45.2 | 48.2 | 51.0  | 45.7 | 18.0 | 20.0 | 25.9 | 54.3 | 68.0 | 80.0 | 68.9 |
| CoT-SC + | 62.9 | 42.0 | 28.0 | 42.2 | 82.0 | 53.0 5 | 48.7 7 | 57.5 | 48.6 | 32.0 | 22.0 | 32.6 | 22.9 | 20.0 | 30.0 | 24.4 |
| CoT-SC - | 42.9 | 26.0 | 20.0 | 28.1 | 68.0 | 42.8 | 43.5 | 48.6 | 42.9 | 24.0 | 20.0 | 27.4 | 40.0 | 52.0 | 66.0 | 54.1 5  |
| ReAct | 68.6 | 44.0 | 26.0 | 43.7 | 83.0 | 58.4 | 54.4 5 | 62.1 | 65.7 | 32.0 | 24.0 | 37.8 | 8.6 | 12.0 | 20.0 | 14.1 |
| ReAct - | 60.0 | 28.0 | 26.0 | 35.6 | 76.0 | 46.4 | 45.6 | 52.5 | 57.1 | 22.0 | 26.0 | 32.6 | 17.1 | 6.0 | 12.0 | 11.1 |

these aspects, which occurred when performed actions did not match commanded actions, when mistakes occurred, and for the steps when the task should be marked as completed. In these cases, we found that the LLM correctly predicted the previous action $61.3 \%$ of the time, correctly detected and described mistakes $25.9 \%$ of the time, and correctly predicted completion $81.0 \%$ of the time. For

![](https://cdn.mathpix.com/cropped/2024_06_04_0d6e8cefcd67894bb28fg-07.jpg?height=44&width=930&top_left_y=886&top_left_x=370)

We also asked five human volunteers (not authors of this paper), who are experts in Android programming and fluent in English, to produce estimates of latent state as the zero-shot+ agent performed five randomly selected tasks ( 27 steps in total). We ensured the volunteers only had access to the same information the LLMs did by providing them with the same prompts. To minimize the possibility that differences in performance were due to the difficulty humans might have in parsing long blocks of text, we colored coded the screen representations in the prompts to aid in visual understanding, but beyond this provided no additional information (e.g., screenshots) to the humans. Just as with the LLM-produced estimates, previous human estimates of aspects of latent state (e.g., past inferred actions) were fed into the prompts for other aspects of latent state in the same and later steps. Finally, to minimize the possibility of humans using memory from previous steps the LLMs did not have access to, we ensured no volunteer ever estimated an aspect of latent state for screens less than 4 steps apart. The volunteers were informed of the intended use of the data and no personal data was collected.

As shown in Table 2, humans achieved high, but imperfect performance, underscoring the difficulty of estimating latent state in this setting. Using paired permutation tests, we found LLMs significantly outperformed humans when estimating progression ( $p=.02$ ), moderately outperformed when estimating task completion ( $p=.06$ ), and that there was no significant difference between humans and LLMs for the other aspects of latent state. The ability of LLMs to match or outperform humans is evidence of their strong performance on this task.

### 6.2 Reasoning about latent state improves UI agent performance

We next evaluate how the performance of UI agents can improve when estimates of latent state are used when selecting actions at each step. Due to the multiple possible paths agents may take to accomplish a goal on our emulators, automated evaluation is difficult. Therefore, to measure performance we ask annotators to answer a series of "yes" or "no" questions to assess (1) whether an agent accomplished a task (irrespective of whether it stopped correctly or not), (2) whether it was able to stop, and (3) how many of the sub-tasks involved in completing a full task the agent performed correctly. The last is a measure of "partial completion" which we obtain through questions manually curated for each task (e.g., "Did the agent open the correct app?", "Did the agent perform a search?", etc.). The number of questions assessing partial completion varied from 1 to 7 , depending on the task complexity. A sample of the questions asked can be found in appendix I.

As shown in Table 3. providing estimates of latent state when reasoning about next actions to take, substantially improves performance of UI agents. This was uniformly true when measuring task success and partial completion at both the pooled and individual benchmark level. The percentage of tasks that were successfully completed (see "task success" columns in the table), measured in total across all benchmarks, increased by factors of 1.6 (from $28.1 \%$ to $45.9 \%, \mathrm{p}=.0002$ ), 1.5 (from $28.1 \%$ to $42.2 \% \mathrm{p}=.0041$ ) and 1.2 (from $35.6 \%$ to $43.7 \%, \mathrm{p}=.078$ ) respectively, for zero-shot, CoT-SC, and ReAct based methods with latent state estimates incorporated. Here and throughout this section, $\mathrm{p}$-values are for the difference in performance for each reasoning method, and were calculated with

![](https://cdn.mathpix.com/cropped/2024_06_04_0d6e8cefcd67894bb28fg-08.jpg?height=325&width=661&top_left_y=236&top_left_x=729)

Figure 3: Distribution of failure causes for agents with zero-shot action selection without and with latent state.

two-sided paired-permutation tests. In addition, whether tasks were successfully completed or not, overall progress achieved toward task goals, as measured by partial completion, similarly improved when latent state estimates were used (improvements in partial completion for all reasoning methods were all significant with $\mathrm{p}<.001$ ). This is strong evidence that LLM-powered UI agents, irrespective of how they perform reasoning, benefit from incorporating the explicit estimation of the content of latent state into their reasoning process.

The benefits of incorporating latent state estimates can be seen in other aspects of agent performance as well, such as stopping. An ideal agent should stop immediately after a task is complete without taking extra steps, while at the same time not stopping prematurely. Using the annotators' responses, we can measure both of these aspects of agent performance (see third and forth sets of columns in Table 3. We quantify the percentage of tasks in which the agent not only accomplished the goal but stopped without taking extra steps. This is a stricter measure than simple task success, which tolerates extra steps (e.g., navigating back to a home screen, clicking on a specific result after performing a search, etc.), and it is likely to be associated with user satisfaction. We find that similar to the more lenient measure, task success measured with this stricter stopping criteria improves by factors of 1.6 (from $25.9 \%$ to $41.5 \%, \mathrm{p}=.0009$ ), 1.2 (from $27.4 \%$ to $32.6 \%, \mathrm{p}=.31$ ) and 1.2 (from $32.6 \%$ to $37.8 \%, \mathrm{p}=.31$ ) respectively, for agents using zero-shot, CoT-SC and ReAct based methods with latent state estimates incorporated. Additionally, the zero-shot and CoT-SC agents without latent state estimation stopped prematurely 4.2 times $(68.9 \%$ to $16.3 \%$ of tasks, $\mathrm{p}<.0001$ ) and 2.2 times ( $54.1 \%$ to $24.4 \%$ of tasks, $\mathrm{p}<.0001$ ) more often then versions with latent state estimation, while the performance of the ReAct agent with latent state estimation decreased only slightly in this regard, stopping prematurely 1.3 times $(14.1 \%$ to $11.1 \%, \mathrm{p}=.59)$ more than the agent without latent state. Upon investigation we discovered this did not reflect superior stopping performance of the ReAct agent, but that the ReAct - agent often failed to stop at all (and therefore rarely prematurely stopped). In particular, we found the ReAct - agent only stopped in $37.8 \%$ of episodes while ReAct + stopped in $57.5 \%$ of episodes before reaching the maximum number of steps we allowed. In addition, by analyzing only episodes where agents stopped before reaching the max number of steps, we found that the ReAct - agent had worse premature stopping performance (when it stopped, $33.3 \%$ of the time it was premature) than ReAct + (when it stopped, $29.9 \%$ of the time it was premature).

### 6.3 Error analysis

The high accuracy achieved at estimating latent state ( $\$ 6.1$ ), did not translate into similarly high end-to-end performance. One explanation is that results for latent state estimation ( Table 2) are quantified across single steps of tasks, and to perform a task successfully agents must perform multiple steps. Each step introduces the possibility that one mistake in latent state estimation (e.g., determining a task is prematurely complete) could adversely affect task performance. Additional factors, such as grounding performance and other aspects of reasoning not studied here (e.g., more advanced capabilities for exploring apps when one execution path fails), also affect performance.

Nonetheless, if latent state is useful for planning next steps, then the cause of failure should shift away from action selection to other causes, such as grounding mistakes. We assessed this through an error analysis of tasks the agents with zero-shot action selection failed on. We manually inspected both predicted actions and grounder behavior and categorized the root cause of a failed task as being due either to action selection (e.g., selecting wrong actions for a task) or grounding (selected actions performed incorrectly). While there were often multiple mistakes per task, it was often possible to judge the primary reason. For completeness, we also noted rare cases when the cause could have been
due to both or was due to the the emulator (e.g., failing to load an app). Due to the effort involved, we only examined failed tasks that were part of the same 40 examined in $\$ 6.1$ (31 failed tasks for zero-shot- and 26 for zero-shot+). As shown in Fig. 3, the percentage of tasks that failed due to action selection decreased substantially when latent state estimates were used in action selection, indicating that as latent state improves reasoning for next actions, other weaknesses, such as grounding, start to play a bigger role in bottlenecking task success.

Finally, while our goal was to compare the same methods with and without latent state, we briefly comment on the relative performance of the three reasoning methods. The tasks we test against are very diverse, and it has been observed few-shot examples may not always generalize well and performance can be quite sensitive to their ordering and wording [25, 3, 40, 24]. While we could have tuned our prompts to improve performance, we avoided doing this so as not to unfairly fit to our test data. It is also interesting to note that while including latent state improved performance of all methods, best performance was obtained with the zero-shot method. This suggests the possibility that once key aspects of latent state are estimated and can be provided in a prompt, simple reasoning without few-shot examples may be sufficient for predicting actions. This is intriguing as it suggests a way of breaking through the need to provide many examples in a prompt, which may be impossible or very costly when a large variety of tasks must be performed, to achieve robust and general UI agents.

## 7 Conclusions

We have proposed a general approach for using LLMs to estimate latent state, and demonstrated its effectiveness in estimating five aspects of latent state for UI agents. We showed how these estimates can be incorporated into reasoning about next action prediction, leading to an increases of up to 1.6x in LLM-agents performing tasks end-to-end in emulated Android environments. We believe the findings that LLMs can estimate latent state for UI agents and that latent state helps with their decision making likely generalize to other POMDP-style domains.

Limitations Our UI agent implementation is based on only one platform (Android) but we expect results to generalize to others, especially web. While we also tested only with one method of forming screen representations, we expect screen understanding, including that by vision-language models (VLM), to be imperfect going into the foreseeable future, so that latent UI and task states, as considered here, will continue to arise. For this reason, we believe the general approach of incorporating the estimation of latent state into agents to be broadly applicable going forward. We only ran with a single LLM, but we expect results to generalize to others and end-to-end performance to increase with newer models. The cost of using human annotators for evaluation kept the total number of tasks we could evaluate on modest. Nonetheless, the number of tasks we report on is inline with other recent work that evaluates online. For cost reasons, we also only considered a fixed set of reasoning methods for selecting next actions. We could choose others (e.g., tree-of-thoughts [37]) but given that we see uniform improvements across all methods we tested, we expect our general finding (that performance improves by incorporating estimates of latent state) to hold.

Broader impacts UI agents can benefit visually-impaired users, by providing them with access to a much wider range of applications and functionality. Another potential use case is general task automation, which has societal, security and privacy implications. An agent may leak private information or carry out a task in an unacceptable way or produce unwanted side effects. Malicious actors could also use UI agents for undesired purposes such as overriding anti-fraud mechanisms or manipulating applications to achieve undesirable goals. For these reasons, deployment of this technology going forward will have to be carefully considered and combined with research in other areas on LLM safety to balance potential societal benefits with risks.

## References

[1] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. Do as i can, not as i say: grounding language in robotic affordances. In Conference on Robot Learning, pages 287-318. PMLR, 2023.

[2] C. Bai, X. Zang, Y. Xu, S. Sunkara, A. Rastogi, J. Chen, and B. A. y Arcas. UIBert: Learning generic multimodal representations for UI understanding. In Z. Zhou, editor, Proc. of the 30th International Joint Conference on Artificial Intelligence, IJCAI 2021, pages 1705-1712. ijcai.org, 2021.

[3] A. Balashankar, X. Ma, A. Sinha, A. Beirami, Y. Qin, J. Chen, and A. Beutel. Improving few-shot generalization of safety classifiers via data augmented parameter-efficient fine-tuning. arXiv preprint arXiv:2310.16959, 2023.

[4] L. E. Baum and T. Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):1554-1563, 1966.

[5] D. M. Blei. Build, compute, critique, repeat: Data analysis with latent variable models. Annual Review of Statistics and Its Application, 1:203-232, 2014.

[6] K. A. Bollen. Latent variables in psychology and the social sciences. Annual review of psychology, 53(1):605-634, 2002.

[7] A. Burns, D. Arsan, S. Agrawal, R. Kumar, K. Saenko, and B. A. Plummer. Mobile app tasks with iterative feedback (motif): Addressing task feasibility in interactive visual environments. CoRR, abs/2104.08560, 2021.

[8] J. Chen, C. Chen, Z. Xing, X. Xu, L. Zhu, G. Li, and J. Wang. Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning. In Proc. of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE '20, pages 322-334, 2020.

[9] J. Chen, M. Xie, Z. Xing, C. Chen, X. Xu, L. Zhu, and G. Li. Object detection for graphical user interface: Old fashioned or deep learning or a combination? In Proc. of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2020, pages 1202-1214, 2020.

[10] J. P. Cunningham and B. M. Yu. Dimensionality reduction for large-scale neural recordings. Nature neuroscience, 17(11):1500-1509, 2014.

[11] R. A. Google and, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira, M. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego, J. Ahn, J. Austin, P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. Díaz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah, M. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vodrahalli, X. Wang, P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng, C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu. Palm 2 technical report, 2023.

[12] I. Gur, H. Furuta, A. Huang, M. Safdari, Y. Matsuo, D. Eck, and A. Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.

[13] H. He, W. Yao, K. Ma, W. Yu, Y. Dai, H. Zhang, Z. Lan, and D. Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024.

[14] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.

[15] P. C. Humphreys, D. Raposo, T. Pohlen, G. Thornton, R. Chhaparia, A. Muldal, J. Abramson, P. Georgiev, A. Santoro, and T. Lillicrap. A data-driven approach for learning to control computers. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 9466-9482. PMLR, 17-23 Jul 2022.

[16] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237-285, 1996.

[17] R. E. Kalman et al. A new approach to linear filtering and prediction problems [j]. Journal of basic Engineering, 82(1):35-45, 1960.

[18] G. Kim, P. Baldi, and S. M. McAleer. Language models can solve computer tasks. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.

[19] J. Y. Koh, R. Lo, L. Jang, V. Duvvur, M. C. Lim, P.-Y. Huang, G. Neubig, S. Zhou, R. Salakhutdinov, and D. Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.

[20] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 22199-22213. Curran Associates, Inc., 2022.

[21] G. Li and Y. Li. Spotlight: Mobile UI understanding using vision-language models with a focus. In Proc. of the 11th International Conference on Learning Representations (ICLR), 2023.

[22] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge. Mapping natural language instructions to mobile UI action sequences. In Proc. of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8198-8210. Association for Computational Linguistics, 2020.

[23] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493-9500. IEEE, 2023.

[24] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.

[25] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.

[26] K. Pearson. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559-572, 1901.

[27] C. Rawles, A. Li, D. Rodriguez, O. Riva, and T. Lillicrap. Android in the wild: A large-scale dataset for android device control. In NeurIPS 2023 Datasets and Benchmarks Track, 2023.

[28] P. Shaw, M. Joshi, J. Cohan, J. Berant, P. Pasupat, H. Hu, U. Khandelwal, K. Lee, and K. Toutanova. From pixels to UI actions: Learning to follow instructions via graphical user interfaces. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[29] C. Spearman. General intelligence, objectively determined and measured. American Journal of Psychology, 15:201-293, 1904.

[30] D. Toyama, P. Hamel, A. Gergely, G. Comanici, A. Glaese, Z. Ahmed, T. Jackson, S. Mourad, and D. Precup. Androidenv: A reinforcement learning platform for android. arXiv preprint arXiv:2105.13231, 2021.

[31] B. Wang, G. Li, and Y. Li. Enabling conversational interaction with mobile ui using large language models. In Proc. of the 2023 CHI Conference on Human Factors in Computing Systems, CHI '23. Association for Computing Machinery, 2023.

[32] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.

[33] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021.

[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.

[35] P. Y. Wu, J. A. Tucker, J. Nagler, and S. Messing. Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting. arXiv preprint arXiv:2303.12057, 2023.

[36] A. Yan, Z. Yang, W. Zhu, K. Lin, L. Li, J. Wang, J. Yang, Y. Zhong, J. McAuley, J. Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023.

[37] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.

[38] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In The 11th International Conference on Learning Representations (ICLR), 2023.

[39] X. Zhang, L. de Greef, A. Swearngin, S. White, K. Murray, L. Yu, Q. Shan, J. Nichols, J. Wu, C. Fleizach, A. Everitt, and J. P. Bigham. Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels. In Proc. of the 2021 CHI Conference on Human Factors in Computing Systems, CHI '21, 2021.

[40] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697-12706. PMLR, 2021.

[41] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. Gpt-4v(ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024.

[42] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.
