# Diffusion Models as Masked Autoencoders 

Chen Wei ${ }^{12} \quad$ Karttikeya Mangalam $^{1} \quad$ Po-Yao Huang ${ }^{1} \quad$ Yanghao Li $^{1}$<br>Haoqi Fan ${ }^{1} \quad \mathrm{Hu} \mathrm{Xu}^{1}$ Huiyu Wang ${ }^{1} \quad$ Cihang Xie $^{3} \quad$ Alan Yuille $^{2} \quad$ Christoph Feichtenhofer ${ }^{1}$<br>${ }^{1}$ FAIR, Meta AI $\quad{ }^{2}$ Johns Hopkins University $\quad{ }^{3}$ UC Santa Cruz


#### Abstract

There has been a longstanding belief that generation can facilitate a true understanding of visual data. In line with this, we revisit generatively pre-training visual representations in light of recent interest in denoising diffusion models. While directly pre-training with diffusion models does not produce strong representations, we condition diffusion models on masked input and formulate diffusion models as masked autoencoders (DiffMAE). Our approach is capable of (i) serving as a strong initialization for downstream recognition tasks, (ii) conducting high-quality image inpainting, and (iii) being effortlessly extended to video where it produces state-of-the-art classification accuracy. We further perform a comprehensive study on the pros and cons of design choices and build connections between diffusion models and masked autoencoders. Project page.


## 1. Introduction

"What I cannot create, I do not understand."

-Richard P. Feynman, 1988

For many years, there has been a desire to achieve a deeper understanding of visual data through the process of generation. Early approaches, such as deep belief networks [38] and denoising autoencoders [85], employed generative pretraining to initialize deep networks for downstream recognition tasks. As generative models can create new samples by approximating the data distribution, it stands to reason that such modeling should simultaneously arrive at a semantic understanding of the raw visual data, as required for recognition tasks, following Feynman.

In line with this philosophy, generative language models, e.g., Generative Pre-trained Transformers or GPTs [4], obtain a broad understanding of language and an immense knowledge base, excelling as both a few-shot learner and a pre-trained base model. Nevertheless, recent explorations in vision generative pre-training fall out of favor. For example, GAN-based BiGAN $[18,19]$ and auto-regressive iGPT [8] noticeably fall short of their concurrent contrastive algorithms despite of using $10 \times$ more parame-

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-01.jpg?height=499&width=845&top_left_y=756&top_left_x=1060)

Figure 1. Inference process of DiffMAE, which iteratively unfolds from random Gaussian noise to the sampled output. During training, the model learns to denoise the input at different noise levels (from top row to the bottom) and simultaneously performs self-supervised pre-training for downstream recognition.

ters. The challenge stems, in part, from the divergent focus: While recognition models primarily focus on the highlevel low-frequency structure of images, generation models must also allocate capacity for low-level high-frequency details [70]. Given this discrepancy, it remains an open question whether and how generative pre-training can effectively compete with other self-supervised algorithms on downstream recognition tasks despite its intuitive appeal.

In recent years, the field of image generation has been dominated by denoising diffusion models [77, 40]. These models employ a straightforward process of iteratively refining noisy samples. As a result, the generated images are of impressively high quality, and even better, can generate an extensive range of diverse samples [60, 69, 74]. In light of this progress, we revisit the potential of generative pretraining in the context of diffusion models.

To begin with, we directly fine-tune a pre-trained diffusion model [16] on ImageNet classification [14]. Despite its strong performance for unconditional image generation, the pre-trained diffusion model only yields a marginal improvement in classification when compared to training the same architecture from scratch (Sec. 3), and is outperformed by concurrent self-supervised pre-training algorithms such as Masked Autoencoders (MAE) [34].

MAE demonstrates strong recognition performance by learning to regress pixels of masked patches given the other visible patches. Inspired by MAE, we incorporate masking into diffusion models and cast Diffusion Models as Masked Autoencoders (DiffMAE). We formulate the masked prediction task as a conditional generative objective, i.e., to approximate the pixel distribution of the masked region conditioned on the visible region. We learn models with our diffusion approach, within the MAE framework, introducing no extra training cost. During pre-training, our model is trained to denoise the input at different noise levels and learns a strong representation for recognition and generation. We evaluate the pre-trained model by fine-tuning on downstream recognition tasks, as well as image inpainting for which the model generates samples by iteratively unfolding from random Gaussian noise, illustrated in Fig. 1.

The diffusion nature of DiffMAE allows it to generate intricate visual details, e.g., of objects (Fig. 2). In contrast, MAE is known to produce blurry reconstructions that lack high-frequency components. Further, DiffMAE maintains strong performance on image and video recognition tasks.

We make the following observations in this work:

(i) DiffMAE is a strong pre-training approach for finetuning on downstream recognition tasks, obtaining comparable performance to leading self-supervised learning algorithms that focus solely on recognition. When combined with CLIP [67] features, our DiffMAE is even able to outperform recent work that combines MAE and CLIP.

(ii) DiffMAE is able to generate high quality images conditioning on masked input. Specifically, DiffMAE generations outperform leading inpainting methods quantitatively and also appear more semantically meaningful.

(iii) DiffMAE can be extended to the video domain effortlessly, providing high-quality inpainting and state-ofthe-art recognition accuracy, outperforming recent works.

(iv) We reveal a connection between MAE and diffusion models, as MAE effectively performs the first inference step of diffusion. In other words, we believe the success of MAE aligns with the philosophy of generation for recognition.

We further perform a comprehensive empirical study to elucidate the pros and cons of the design choices on downstream recogntion and inpainting generation tasks.

## 2. Related Work

Self-supervised learning aims to learn from unlabeled visual data by a pre-text task that is constructed by image/patch operations (e.g., [17, 61, 88, 94, 29, 5]) and spatial-temporal operations (e.g., [31, 58, 28, 62, 86]). Contrastive methods [22] capitalize on augmentation invariance of images and videos [90, 35, 9, 6, 33, 7, 10, 66, 27].

For vision, different masked prediction targets have been proposed. MAE [34] predicts pixel colors with an efficient asymmetric architecture. BEiT [3, 65] and iBOT [97] pre- dict dVAE [82, 70] or learnable tokens. MaskFeat [87] predicts HOG features. data2vec [2] learns from a momentum teacher. The community is continuously exploring this direction $[20,91,44,96,53,25]$.

Generative learning for recognition has a long-standing appeal for its intuitiveness. Pioneers study the representation learned by GANs [30, 68, 18] and VAEs [46, 45]. BigBiGAN [19] demonstrates a model that learns competitive recognition representation and generates high-fidelity images with GAN. This work is followed by iGPT [8], which generates images autoregressively and was state-of-the-art on linear probing protocols. Though this stream stagnates afterward, we demonstrate its resurgence by exploring diffusion models in this work.

Denoising diffusion models have ushered in a new era of diverse, high-resolution conditional image generation [77, 16, 40]. Utilizing a forward Gaussian diffusion process and a backward generation process, denoising diffusion models iteratively refine the generated image starting from Gaussian noise. This process has proven extremely powerful for rich text-conditioned generation of both images $[69,60,74$, 72] and videos [84, 41, 39, 76].

Masked autoencoders were pioneered by stacked autoencoders [85] and inpainting tasks [63] using ConvNets. Since the introduction of ViT [21], masked prediction has reattracted attention, partially inspired by the success of BERT [15] in NLP. BERT performs masked language modeling, scales well, and generalizes to different end tasks.

## 3. Fine-tuning Generative Models

| pre-train | architecture | params. | scratch | pre-trained |
| :---: | :---: | :---: | :---: | :---: |
| MAE [34] | ViT-L | $304 \mathrm{M}$ | 82.6 | 85.9 |
| iGPT [8] | iGPT-L | $1362 \mathrm{M}$ | 53.2 | 72.6 |
| ADM [16] | U-Net enc. | $211 \mathrm{M}$ | $82.0^{\dagger}$ | 83.3 |
| DDPM [40] | ViT-L | $304 \mathrm{M}$ | 82.6 | 83.4 |
| DiffMAE | ViT-L | $304 \mathrm{M}$ | 82.6 | $\mathbf{8 5 . 8}$ |

Table 1. Fine-tuning generative models on ImageNet, a systemlevel comparison. ${ }^{\dagger}$ trained from scratch by us. Non-generative method is included for reference and de-emphasized.

We start by comparing different generative pre-training methods for downstream ImageNet classification in Tab. 1.

iGPT [8] generates images in a GPT [4] style by autoregressively predicting pixels. iGPT pre-training has significantly improved its low from-scratch (random initialization) accuracy from $53.2 \%$ to $72.6 \%$, which is still relatively low, considering the model size ( $1362 \mathrm{M}$ ) and compute cost.

To re-examine generative pre-training in light of the recent progress on diffusion models, we fine-tune a pretrained ADM [16], which is a recent method for diffusionbased image generation. In particular, we take its unconditional $\mathrm{IN}-1 \mathrm{~K}$ model, which aligns with the criteria of unsupervised pre-training and never uses the class labels.

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-03.jpg?height=778&width=1743&top_left_y=218&top_left_x=164)

Figure 2. Qualitative comparison of DiffMAE and MAE. $t=1000$ and $t=500$ are DiffMAE's predictions at intermediate timesteps from $t=1000 \ldots 0 . t=0$ marks the final generation. MAE's outputs are obtained from its official visualization model. The predictions at the masked area and the ground-truth at the visible area are overlaid. Both models are ViT-L pre-trained for 1600 epochs.

Compared to the from-scratch counterpart, initializing with the pre-trained diffusion model provides a gain of $+1.3 \%$ top-1. However, the resulting $83.4 \%$ still lags far behind the non-generative self-supervised algorithms such as MAE $(85.9 \%)$. Considering the differences in architecture, we further train a ViT-L with diffusion DDPM [40]. This model enhances the fine-tuning classification to $83.4 \%$ compared to its from-scratch baseline of $82.6 \%$ top-1 [34], which aligns with our observations on fine-tuning the pre-trained ADM. Implementation details are in the Appendix.

In comparison to these approaches, a DiffMAE trained ViT-L obtains $85.8 \%$ when fine-tuned and is described next.

## 4. Diffusion Masked Autoencoder

We incorporate the masked prediction paradigm into diffusion models. The model approximates the pixel distribution of the masked region conditioned on the visible region. In other words, we study Diffusion Models as a form of Masked Autoencoders (DiffMAE), introduced next.

### 4.1. Conditional Diffusion Model

Given a training sample $\boldsymbol{x}_{0} \sim p\left(\boldsymbol{x}_{0}\right)$ where subscript ${ }_{0}$ denotes that the sample is original and clean, we first spatially divide $\boldsymbol{x}_{0}$ into the non-overlapping masked region $\boldsymbol{x}_{0}^{m}$ and the visible region $\boldsymbol{x}_{0}^{v}$. We want to model the distribution of the masked region $\boldsymbol{x}_{0}^{m}$ conditioned on the visible region $\boldsymbol{x}_{0}^{v}$, i.e., the distribution $p\left(\boldsymbol{x}_{0}^{m} \mid \boldsymbol{x}_{0}^{v}\right)$.

In the forward process, only the masked area $\boldsymbol{x}_{0}^{m}$ is gradually diffused, i.e., corrupted by recursively adding a small amount of Gaussian noise $T$ times to $\boldsymbol{x}_{1}^{m}, \boldsymbol{x}_{2}^{m}, \ldots, \boldsymbol{x}_{T}^{m}$ fol- lowing the Markov process below:

$$
\begin{equation*}
p\left(\boldsymbol{x}_{t}^{m} \mid \boldsymbol{x}_{t-1}^{m}\right)=\mathcal{N}\left(\boldsymbol{x}_{t}^{m} ; \sqrt{1-\beta_{t}} \boldsymbol{x}_{t-1}^{m}, \beta_{t} \mathbf{I}\right) \tag{1}
\end{equation*}
$$

where $t \in[1,2, \ldots, T]$ denotes the timestep and $\beta_{1: T}$ is the variance schedule of noise, held as hyper-parameters.

Thanks to the properties of Gaussian distribution, we can directly sample $\boldsymbol{x}_{t}^{m}$ without the recursive formulation by:

$$
\begin{equation*}
p\left(\boldsymbol{x}_{t}^{m} \mid \boldsymbol{x}_{0}^{m}\right)=\mathcal{N}\left(\boldsymbol{x}_{t}^{m} ; \sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}^{m},\left(1-\bar{\alpha}_{t}\right) \mathbf{I}\right) \tag{2}
\end{equation*}
$$

which can be reparameterized to $\boldsymbol{x}_{t}^{m}=\sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}^{m}+\sqrt{1-\bar{\alpha}_{t}} \epsilon$ for $\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) . \quad \alpha_{t}=1-\beta_{t}$ and $\bar{\alpha}_{t}=\prod_{i=1}^{t} \alpha_{i}$. The variance schedule makes sure that $\bar{\alpha}_{T}$ of the final timestep $T$ is sufficiently small so that $p\left(\boldsymbol{x}_{T}^{m}\right)$, the end of the forward process, approximates the standard normal distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$ well, which is the starting point of the reverse process.

We want to generate masked regions by sampling from $p\left(\boldsymbol{x}_{0}^{m} \mid \boldsymbol{x}_{0}^{v}\right)$, which is approximated by recursively sampling from $p\left(\boldsymbol{x}_{t-1}^{m} \mid \boldsymbol{x}_{t}^{m}, \boldsymbol{x}_{0}^{v}\right)$ starting from $\boldsymbol{x}_{T}^{m} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. When the variance of noise $\beta_{t}$ in each step $t$ is small enough, $p\left(\boldsymbol{x}_{t-1}^{m} \mid \boldsymbol{x}_{t}^{m}, \boldsymbol{x}_{0}^{v}\right)$ can also be also considered Gaussian distributed [77], to be approximated by a deep network. We optimize the simple objective proposed by DDPM [40]:

$$
\begin{equation*}
\mathcal{L}_{\text {simple }}=\mathbb{E}_{t, \boldsymbol{x}_{0}, \epsilon}\left\|\boldsymbol{x}_{0}^{m}-D_{\theta}\left(\boldsymbol{x}_{t}^{m}, t, E_{\phi}\left(\boldsymbol{x}_{0}^{v}\right)\right)\right\|^{2} \tag{3}
\end{equation*}
$$

This mean squared error objective is simplified from the variational bound. Unlike DDPM using $\epsilon$-prediction, which predicts the noise, our model predicts the denoised masked region $\boldsymbol{x}_{0}^{m}$. These two formulations are interchangeable, and both are commonly adopted in diffusion models [69]. $E_{\phi}$ is the encoder, which projects the visible image content $\boldsymbol{x}_{0}^{v}$ into the latent space. $D_{\theta}$ is the decoder predicting denoised input, from the noisy $\boldsymbol{x}_{t}^{m}$, the timestep $t$, and the visible latent $E_{\phi}\left(\boldsymbol{x}_{0}^{v}\right)$.

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-04.jpg?height=453&width=832&top_left_y=240&top_left_x=164)

Figure 3. DiffMAE with cross-self and cross decoder. The decoder inputs are noisy patches at randomly sampled noise levels. The decoder outputs are the predicted pixels. The decoder blocks are top-down connected to the encoder blocks. Cross-self decoder has self-attention in each block, while cross decoder does not.

### 4.2. Architecture Design

We instantiate the above conditional diffusion model following the asymmetric design of MAE, introducing no extra training cost. The overall architecture is built solely on Vision Transformers (ViT) [21], which is in contrast to the typical choice of U-Net [73] backbones in diffusion models [40, 16, 69, 74]. In this way, we enable a straightforward evaluation of DiffMAE's ability on downstream recognition tasks and an apple-to-apple comparison to other selfsupervised pre-training methods.

Encoder. The encoder takes a standard ViT. Specifically, the training images are first divided into non-overlapping patches, among which most are selected as the visible patches $\boldsymbol{x}_{0}^{v}$ and the others are the masked $\boldsymbol{x}_{0}^{m}$. The ViT encoder $E_{\phi}(\cdot)$ only operates on the visible patches and encodes each of these patches into the latent space. The encoded $E_{\phi}\left(\boldsymbol{x}_{0}^{v}\right)$ then serves as the condition of the generation task performed by the decoder, providing hints of the masked object. After the pre-training stage, only the encoder is fine-tuned to downstream tasks.

Decoder. The decoder takes in noisy masked patches $\boldsymbol{x}_{t}^{m}$ as the input. The noise levels of these patches, denoted by the timestep $t$, are integers uniformly sampled in $[1, T]$ during training. As in ViT, we first project these noisy patches to noise tokens using a linear layer. The timestep $t$ can be specified by adding sinusoidal embeddings to the noise tokens together with the positional embeddings. However, our experiments show that the addition of the $t$ embedding or the lack thereof does not make a big difference to both downstream recognition and inpainting generation, which suggests that the decoder can automatically determine the level $t$ of noisy patches when conditioned on clean patches.

We explore three different decoder configurations, which differ in how the attention modules are applied to the visible latents and the noise tokens:

- Joint decoder applies Transformer blocks on the concatenated sequence of the visible latents from the last encoder block and the noise tokens. Each Transformer block operates equally on the visible latents and the noise tokens, with a self-attention layer followed by an MLP.
- Cross-self decoder resembles the encoder-decoder design in the original Transformer [83]. In each decoder block, the noise tokens first attend to the visible latents with a cross-attention layer and then attend to each other noise tokens with a self-attention layer. The cross-attention enables the decoder to attend to the visible latents of different encoder blocks, rather than solely to the final block as in joint decoder. In this way, the encoder and the decoder can be connected in a U-shaped manner illustrated in Fig. 3, which is typically advantageous for image generation and dense predictions such as U-Net [73].
- Cross decoder is similar to cross-self decoder, but excludes the self-attentions between the noise tokens. In other words, each noise token independently attends to the visible latents from the encoder in a top-down manner as in Fig. 3, not being aware of the existence of other noise tokens. With a much smaller attention map, cross decoder is the most efficient among the three. Specifically, training with cross decoder is overall $\sim 15 \%$ more efficient than joint decoder, using a ViT-L encoder and an eight-block decoder of width 512.

We ablate the three decoder architectures in Sec. 5 .

Sampling. For inference, the encoder $E_{\phi}\left(\boldsymbol{x}_{0}^{v}\right)$ forwards the visible patches only once while the decoder unfolds from Gaussian noise to the sampled image for $T$ times iteratively following DDPM [40]. As illustrated in Fig. 2, the predictions gradually become less blurry as the iterative unfolding progresses, and both small-scale structures of the objects (e.g., the mouth of the monkey) and high-frequency details (e.g., the fur of the fox) emerge in the generated images. After $T$ iterations, we obtain high-quality samples at $t=0$.

CLIP target. To compare to recent prior works that use CLIP [67], we also explore a version for predicting CLIP features together with the original pixel prediction task by simply using a second decoder. The prediction is optimized by minimizing the cosine distance to the CLIP features of the masked patches, similar to MaskFeat [87] and MILAN [43]. As we show in Tab. 6, predicting CLIP features not only enhances recognition ability but also brings improved inpainting generation quality.

Video. The above model are described in the context of image operations. However, it can be straightforwardly extended to the spatiotemporal domain of videos. Most operations are shared, except that there is an additional temporal dimension. Each token now represents a space-time cube corrupted by noise. The masking is performed by randomly
sampling space-time cubes. The prediction target are pixels of a single time slice of each masked space-time cube.

### 4.3. Connection between Diffusion and MAE

We discuss the connection between diffusion models and MAE. The two are different at first glance: They have different purpose (image generation $v s$. self-supervised pretraining), inputs (noisy images $v s$. masked images), outputs (noise vs. pixels) and architectures (U-Net vs. ViT). Consider Fig. 2, at the first timestep $t=1000$ where the inputs of the decoder approximate Gaussian noise, the pixel predictions of our model are visually similar to the predictions of MAE, both capturing only the blurry and rough structures of the objects. Moreover, DiffMAE trained with only the $t=1000$ noise level obtains a similar fine-tuning accuracy of around $85.0 \%$ as MAE, shown in Fig. 4. This consistently observed similarity makes sense because both the Gaussian noise in DiffMAE at $t=1000$ and the learnable mask token in MAE contain no image signal.

This observation suggests a close relationship between MAE and diffusion models, that MAE effectively performs the first inference step $(t=1000)$ of diffusion models. On the other hand, the other steps of diffusion models generate more and more intricate high-frequency details, which is lacking in the predictions of MAE. Overall, the connection suggests that MAE can be viewed as a single-step patchconditioned diffusion model, and that the success of MAE in downstream recognition tasks is in line with the philosophy of generative pre-training. Correspondingly, diffusion models, the first step of whom effectively performs MAE, are potentially good recognition models.

## 5. Empirical Study

Settings. We pre-train our DiffMAE on the IN-1K [14] training set. If not specified, the encoder is a vanilla ViT$\mathrm{L}$ [21] without any modification, and the decoder is of depth 8 and width 512. The eight cross-(self) decoder blocks spread top-down uniformly attending to the outputs of the 24 encoder blocks of ViT-L. The data augmentation is randomly resized cropping, and the masking strategy is to mask out $75 \%$ patches randomly. All models are pre-trained for 400 epochs. For the diffusion setup, the variance follows a linear schedule [40], and the number of timesteps $T$ is set to 1000. Details are in the Appendix.

From the perspective of pre-training for recognition, we report end-to-end fine-tuning top-1 accuracy on the $\mathrm{IN}-1 \mathrm{~K}$ validation set. The model is both pre-trained and fine-tuned at $224^{2}$ resolution. By default, we use our cross decoder design, and the prediction targets are per-patch normalized.

For the task generative inpainting, we report the common LPIPS $\downarrow$ [95], which is a learned distance metric based on the deep feature space of AlexNet [47]. Following the

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-05.jpg?height=353&width=805&top_left_y=257&top_left_x=1083)

Figure 4. Comparison of different decoder architectures across noise levels (x-axis) by fine-tuning top-1 (\%) on $\mathrm{IN}-1 \mathrm{~K}$ val set (y-axis). Each DiffMAE model is pre-trained with a single noise level. All models are ViT-L pre-trained for 400 epochs.

setting of RePaint [57], we compute LPIPS $\downarrow$ over 100 images from the $\mathrm{IN}-1 \mathrm{~K}$ validation set, and train and evaluate the model with image size $256^{2}$. We use cross-self decoder, and the prediction targets are unnormalized pixels.

Decoder architecture - classification. We find that the three decoder architectures impact fine-tuning for downstream classification differently across different noise levels. To demonstrate this, we train the models at various single noise levels, analyzed in Fig. 4. A smaller $t$ corresponds to less noise while a larger $t$ denotes more noise. At $t=1000$, the inputs have almost no image signals but noise. At $t=0$, the inputs would be the clean image patches.

First, the fine-tuning accuracy drops for all three decoders when $t$ is lower. At a small noise level $t$, the disruption to image signals by noise is less severe. Consequently, the decoder needs less conditioning on the visible latents to predict image signals, which weakens the pre-training of the encoder. On the other hand, the model is now tasked to generate more low-level, high-frequency details, which are considered to be less useful for downstream recognition.

Second, the drop rates of joint and cross-self decoder are faster than that of cross decoder. This is because both joint decoder and cross-self decoder have a shortcut to exploit the image signals of the neighboring noisy patches and bypass the visible latents of the encoder. This shortcut is amplified especially when $t$ is small. In contrast, cross decoder, whose inputs do not see other noisy patches, avoids this shortcut.

In summary, our experiments show that the pre-training performance is closely related to the difficulty of the denoising task. Nonetheless, the pre-training helps recognition in all cases compared to its from-scratch counterpart of $82.6 \%$ top-1 [34]. We make cross decoder the default option for pre-training for downstream recognition tasks.

Decoder architecture - inpainting. Next, we discuss the effect of different decoders on inpainting. For the cross decoder, the noise tokens are independent of each other and do not see other noise tokens. Therefore, the generated image is patch-wise discontinuous. Even so, these patches still make sense semantically at their positions, show in Fig. 5.

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-06.jpg?height=214&width=217&top_left_y=240&top_left_x=168)

ground-truth

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-06.jpg?height=218&width=419&top_left_y=238&top_left_x=365)

input

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-06.jpg?height=216&width=222&top_left_y=239&top_left_x=775)

cross-self
Figure 5. Generations of cross and cross-self decoders. The generation of cross decoder is patch-wise discontinuous, while crossself decoder generates visually continuous samples.

The generations of both joint and cross-self decoders are continuous and visually pleasant. Comparing these two, cross-self decoder achieves a better LPIPS $\downarrow$ score of 0.216 on the evaluation set while joint decoder obtains 0.224 . We credit the better generation of cross-self decoder to its Ushape design following $\mathrm{U}-\mathrm{Net}$, which is commonly suitable for dense predictions. We make cross-self decoder the default option for inpainting.

| range of $t$ | $\rho$ | top-1 |
| ---: | :---: | :---: |
| 1000 | 1.0 | 85.1 |
| $0 \sim 1000$ | 1.0 | 84.7 |
| $250 \sim 1000$ | 1.0 | 84.7 |
| $500 \sim 1000$ | 1.0 | 85.1 |
| $0 \sim 1000$ | 0.8 | 85.1 |
| $0 \sim 1000$ | 0.6 | 85.0 |

Table 2. Noise schedule. We first vary the range of $t$ with different starting points. We then modify the default linear schedule with a hyper-parameter $\rho$ as the exponent to each variance $\beta_{t}$.

Noise variance schedule. Next, we train our model with mixed noise levels to complete the diffusion process in Tab. 2. We use the linear schedule following DDPM [40] by default, where the variances $\beta_{[1: T]}$ increase linearly.

We first vary the range of $t$, starting from $t=250$ and $t=500$. Similar to our observations on training with single noise levels, the less the $t$ range includes lightly noisy samples, the better the fine-tuning accuracy is. When $t$ is mixed from 500 to 1000 , the pre-trained model is just as good as the case of training with only $t=1000$ samples.

We then directly modify the linear schedule with a hyper-parameter $\rho$, which exponentiates the variances $\beta_{t}$ to $\beta_{t}^{\rho}$. When $\rho$ is smaller than one, it enlarges each variance $\beta_{t}$, therefore amplifying the amount of noise at each timestep. Consequently, the fine-tuning top-1 improves with a smaller $\rho$ from $84.7 \%$ to around $85.0 \%$. However, the gain on downstream classification is at the cost of a drop in the inpainting generation. Specifically, LPIPS $\downarrow$ adversely increases from 0.216 with the default linear schedule $(\rho=1.0)$ to 0.228 with $\rho=0.6$. This suggests another trade-off between pre-training for recognition and inpainting generation. We detail $\rho$ formulation in the Appendix.

Prediction target. We next study the influence of prediction target in Tab. 3. Both noise and pixel prediction are commonly adopted in generative diffusion models [40, 69].

| target | noise | pixel | pixel w/ norm |
| :---: | :---: | :---: | :---: |
| top-1 | unstable | 84.3 | 85.1 |

Table 3. Prediction targets of noise, pixels, and per-patch normalized pixels. The noise entry is unstable in fine-tuning.

However, when evaluated for downstream classification, only predicting the pixels can serve as the target, obtaining $84.3 \%$ top-1. The noise prediction, however, can not be stably fine-tuned for the downstream classification task. Regarding the pixel prediction, the per-patch normalization [34] consistently helps the fine-tuning accuracy.

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-06.jpg?height=230&width=813&top_left_y=747&top_left_x=1076)

Figure 6. Masking ratio. Varying the masking ratio of random patch masking. ${ }^{\dagger}$ For $100 \%$ masking, we use the encoder-only architecture instead of the asymmetric encoder-decoder architecture.

Masking ratio. We consider different masking ratios for the random masking strategy. We report the fine-tuning accuracy in Fig. 6. We observe that accuracy is stable with masking ratios from $45 \%$ to $85 \%$ and has a drop when the masking ratio increases to $90 \%$. When the masking ratio is increased to $100 \%$, i.e., no masking at all, the fine-tuning top-1 drops to $83.4 \%$. This suggests the importance of visible patch conditioning for DiffMAE pre-training. On the other hand, the recognition performance is still enhanced compared to its from-scratch baseline of $82.6 \%$ top-1 [34].

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-06.jpg?height=99&width=812&top_left_y=1577&top_left_x=1079)

Table 4. Center masking. The left reports the fine-tuning top-1. The right reports LPIPS $\downarrow$ of the two models trained with random and center mask, respectively, but evaluated both with center mask.

Center masking. We now compare random masking with a more common evaluation protocol for inpainting tasks, center masking, which block-wise masks out the center $25 \%$ region of images as in Fig. 7. We first train with two different maskings and then evaluate both models by finetuning top-1. As shown in Tab. 4 (left), random 75\% mask works better for fine-tuning.

We then evaluate both random and center mask trained models by inpainting LPIPS $\downarrow$ with center mask in Tab. 4 (right), the common inpainting protocol. The random mask trained model provides reasonable center block inpainting, reporting 0.142 LPIPS $\downarrow$ on the evaluation set. This shows that the model still obtains block-wise inpainting ability although trained with random masks. When trained with center mask, the inpainting quality is further improved, and LPIPS $\downarrow$ favorably decreases to 0.125 .

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-07.jpg?height=320&width=835&top_left_y=241&top_left_x=165)

Figure 7. Samples of center masking inpainting of DiffMAE. For a $256^{2}$ image, we mask out the center $128^{2}$ block.

| depth | top-1 | LPIPS $\downarrow$ | width | top-1 | LPIPS $\downarrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 4 | 85.0 | $\overline{0.220}$ | 256 | 85.0 | 0.239 |
| 8 | 85.1 | 0.216 | 512 | 85.1 | 0.216 |
| 12 | 85.1 | 0.215 | 1024 | 84.9 | 0.208 |

Table 5. Decoder size. The left table varies the depth of the decoder with the default width of 512 . The right table varies the width of the decoder with the default depth of eight blocks.

Decoder size. We discuss the effect of the decoder size in Tab. 5. By default, we use eight decoder blocks, each of which has 512 channels. We do not observe significant differences in the fine-tuning performance when varying the decoder depth or the width, all of which have around $85.0 \%$ top-1. For inpainting, we observe a notable drop if the decoder size is reduced and a gain when the decoder size is enlarged. With only four decoder blocks, the LPIPS $\downarrow$ adversely increases from 0.216 to 0.220 . A narrower decoder of width 256 leads to an even worse LPIPS $\downarrow$ of 0.239 . Using an extra wide decoder with 1024 channels, the inpainting is largely improved to 0.208 LPIPS $\downarrow$.

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-07.jpg?height=94&width=831&top_left_y=1465&top_left_x=167)

Table 6. CLIP target. Varying the prediction target between pixel, CLIP feature, and the combination of the two.

CLIP target. Boosting pre-training with CLIP [67], a vision-language model trained on a $400 \mathrm{M}$ in-house dataset, has been popular driven by its strong performance in downstream tasks. MaskFeat [87] first showed improved performance for masked feature prediction. Then algorithms utilizing CLIP either regress CLIP features [89, 43], or predict CLIP-aided tokens [65]. In this work, we use a second cross decoder to regress CLIP features alongside the original pixel decoder, multitasking on both masked pixel and CLIP feature prediction. In Tab. 6 (left), we first compare the options by fine-tuning. Solely regressing CLIP features achieves a high accuracy $86.5 \%$, compared to the default pixel predicting baseline $85.1 \%$. Multitasking on both targets further improves the accuracy to $86.7 \%$.

Beyond the improvement on fine-tuning, we observe in Tab. 6 (right) that combining CLIP targets can also improve inpainting. The LPIPS $\downarrow$ decreases from 0.226 to 0.216 when multitasking with CLIP regression. This suggests that a better semantic understanding, in this case by learning from CLIP, can notably help image inpainting quality.

| pre-train | w/ CLIP | ViT-B | ViT-L | ViT-H |
| :---: | :---: | :---: | :---: | :---: |
| from-scratch [34] | $\bar{x}$ | 82.3 | 82.6 | 83.1 |
| MoCo v3 [11] | $x$ | 83.2 | 84.1 | - |
| DINO [7] | $x$ | 82.8 | - | - |
| iBOT [97] | $x$ | 84.0 | 84.8 | - |
| BEiT [3] | $x$ | 83.2 | 85.2 | - |
| MaskFeat [87] | $x$ | 84.0 | 85.7 | - |
| MAE [34] | $x$ | 83.6 | 85.9 | 86.9 |
| DiffMAE | $x$ | 83.9 | 85.8 | 86.9 |
| $\overline{\text { MVP [89] }}$ | $\bar{\checkmark}$ | 84.4 | 86 | $\overline{-}$ |
| MILAN [43] | $\checkmark$ | 85.4 | 86.7 | - |
| $\mathrm{BEiTv}^{\dagger}$ [65] | $\checkmark$ | 85.5 | 87.3 | - |
| DiffMAE | $\checkmark$ | 84.9 | 86.9 | 88.0 |

Table 7. Comparison to prior works on IN-1K fine-tuning. All entries are pre-trained on $\mathrm{IN}-1 \mathrm{~K}$ train split at image size $224^{2}$. When using CLIP, all ViT-B and ViT-L utilize CLIP ViT-B/16 and ViT-H uses CLIP ViT-L/14. ${ }^{\dagger}$ BEiTv2's ViT use layer scale [81] and relative positional embedding [75].

Discussion. We have thoroughly studied the design aspects of DiffMAE in terms of downstream classification and generative inpainting performance. Our experiments show that, for many aspects, pre-training for downstream classification and generative inpainting do not share the optimal setting, e.g., the decoder architecture and the noise variance schedule. For other aspects, settings favoring inpainting quality do not impact pre-training ability, e.g., decoder size. Interestingly, using CLIP benefits both downstream classification and generative inpainting. We use cross decoder, perpatch normalization, and noisy variance schedule for finetuning evaluation, and we use cross-self decoder with 1024 width, the default linear schedule, but not per-patch normalization for inpainting evaluation. While the inpaintingoriented model can still improve from-scratch recognition accuracy, we still would find it desirable to have an identical setting that is optimal for both tasks.

## 6. Comparison to Prior Work

ImageNet fine-tuning. We compare DiffMAE to prior works in fine-tuning for $\mathrm{IN}-1 \mathrm{~K}$ classification in Tab. 7.

The first section presents methods that are solely trained on IN-1K training set without access to CLIP models. We pre-train DiffMAE for 1600 epochs here while 400 were used for ablations in Sec. 5. We observe that fine-tuning DiffMAE significantly boosts the scratch accuracy for all ViT-B ( $+1.6 \%$ ), ViT-L ( $+3.2 \%$ ) and ViT-H ( $+3.8 \%$ ) models. This suggests that DiffMAE can provide strong representations for recognition compared to prior generative models (cf. Tab. 1), and is scalable to large models. Compared to prior self-supervised learning works, it outperforms contrastive methods like MoCo v3 and DINO and is comparable masked autoencoders, while being able to generatively inpaint images with high quality at the same time.

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-08.jpg?height=531&width=851&top_left_y=214&top_left_x=168)

original $75 \%$ random RePaint MAE DiffMAE Figure 8. Visualizations on inpainting of three different algorithms. Generations of DiffMAE have finer details than MAE and are more semantically meaningful than RePaint.

The second section presents representative works that use CLIP to enhance representation learning. We pre-train our DiffMAE for 800 epochs multitasking in masked pixel and CLIP feature prediction. For ViT-B and ViT-L models, all entries utilize CLIP ViT-B/16. For ViT-H, we use CLIP ViT-L/14. The CLIP models are pre-trained on inhouse WIT400M [67] and lift accuracy for both DiffMAE and other works. Combined with CLIP, DiffMAE obtains a strong IN-1K top-1 of $88 \%$ with ViT-H, while ViT-B lags behind because of its insufficient capacity for multitasking.

| arch. | pre-train | AP $^{\text {box }}$ | AP $^{\text {mask }}$ |
| :--- | :--- | :--- | :--- |
| Swin-L | Sup., IN-21K | 52.4 | 46.2 |
| ViT-L | MAE, IN-1K | 55.6 | 49.2 |
| ViT-L | DiffMAE, IN-1K | 55.3 | 49.0 |

Table 8. Comparisons on COCO instance segmentation.

COCO instance segmentation. We conduct COCO [51] instance segmentation with ViTDet [49] using Mask RCNN [36] in Tab. 8. Our results are pre-trained on IN-1K and slightly lower than those of MAE, while being much stronger than e.g. supervised Swin-L trained on $\mathrm{IN}-21 \mathrm{~K}$.

Video classification. We compare DiffMAE to prior work on K400 fine-tuning for video classification in Tab. 9.

The first section presents results that use no in-house data. Our model largely improves from the from-scratch models and achieves comparable results to other algorithms, which use masked autoencoding methodology for pre-training. In the second section, we present the results that use extra in-house data. Especially, we compare to EVL [52] and X-CLIP [59], both using CLIP ViT-L/14 model, which is trained on the in-house text-image dataset WIT400M [67]. DiffMAE obtains $87.4 \%$ top-1 with a regular ViT-L/14 model, outperforming these works. This number is further improved to a high $88.1 \%$ with a larger input size $280^{2}$ and a longer temporal duration of 32 frames.

ImageNet inpainting. We first qualitatively compare RePaint [57], MAE, and our DiffMAE on $75 \%$ random mask

| model | pre-train | extra data | top-1 | top-5 | input size | FLOPs | param |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| SlowFast [26] | scratch | - | 79.8 | 93.9 | $64 \times 224^{2}$ | $234 \times 3 \times 10$ | 60 |
| X3D-XL [24] | scratch | - | 79.1 | 93.9 | $16 \times 312^{2}$ | $48 \times 3 \times 10$ | 11 |
| MViT-B [23] | scratch | - | 81.2 | 95.1 | $64 \times 224^{2}$ | $455 \times 3 \times 3$ | 37 |
| MViTv2-B [50] | scratch | - | 82.9 | 95.7 | $32 \times 224^{2}$ | $255 \times 1 \times 5$ | 51 |
| MViTv2-L [50] | MaskFeat [87] | - | 84.3 | 96.3 | $16 \times 224^{2}$ | $377 \times 1 \times 10$ | 218 |
| ViT-L [21] | MAE [25] | - | 84.8 | 96.2 | $16 \times 24^{2}$ | $598 \times 3 \times 7$ | 304 |
| ViT-L[21] | DiffMAE | - | 84.5 | 96.3 | $16 \times 224^{2}$ | $598 \times 3 \times 7$ | 304 |
| ViT-L/14 [21] | EVL [52] | WIT400M | 86.9 | 97.4 | $16 \times 224^{2}$ | $1348 \times 1 \times 3$ | $\mathrm{n} / \mathrm{a}$ |
| ViT-L/14 [21] | X-CLIP [59] | WIT400M | 87.1 | 97.6 | $8 \times 224^{2}$ | $658 \times 4 \times 3$ | n/a |
| ViT-L/14 [21] | DiffMAE | WIT400M | 87.4 | 97.5 | $16 \times 224^{2}$ | $828 \times 3 \times 10$ | 304 |
| ViT-L/14 [21] | EVL [52] | WIT400M | 87.7 | 97.8 | $32 \times 336^{2}$ | $6065 \times 1 \times 3$ | $\mathrm{n} / \mathrm{a}$ |
| ViT-L/14 [21] | X-CLIP [59] | WIT400M | 87.7 | 97.4 | $16 \times 336^{2}$ | $3086 \times 4 \times 3$ | $\mathrm{n} / \mathrm{a}$ |
| ViT-L/14 [21] | DiffMAE | WIT400M | $\mathbf{8 8 . 1}$ | 97.8 | $32 \times 280^{2}$ | $2588 \times 4 \times 3$ | 304 |

Table 9. Comparison with previous works on Kinetics-400. We report the inference cost with a single "view" (temporal clip with spatial crop) $\times$ the number of views (FLOPs $\times$ view $_{\text {space }} \times$ view $_{\text {time }}$ ). Magnitudes are Giga ( $10^{9}$ ) for FLOPs and Mega $\left(10^{6}\right)$ for Param.

| method | arch. | random | center |
| :--- | :--- | :---: | :---: |
| DSI [64] | VQ-VAE2 [71] | 0.300 | 0.153 |
| RePaint [57] | ADM [16] | 0.303 | 0.160 |
| DiffMAE | ViT-L | 0.208 | 0.125 |
| DiffMAE | ViT-H | 0.205 | 0.121 |

Table 10. Comparison to prior works on IN-1K inpainting with LPIPS $\downarrow$ for $75 \%$ random mask and $25 \%$ center mask settings. All entries are trained and evaluated at image size $256^{2}$.

inpainting in Fig. 8. RePaint employs a pre-trained unconditional diffusion model, specifically ADM [16], as the generative prior to perform free-form inpainting. We notice that the generations of RePaint, though with visually pleasant details, often need more precise semantic meaning. In contrast, MAE generates blurry samples. DiffMAE generates visually detailed and semantically meaningful images.

We finally quantitatively compare the inpainting performance in Tab. 10. Both DSI and RePaint are algorithms specially designed for inpainting. We evaluate at image size $256^{2}$ following both works. We first compare the algorithms in the $75 \%$ random mask setting where DSI and RePaint's results are obtained by us with their official models. We then compare the three algorithms with the $25 \%$ center mask protocol, which is commonly used by inpainting algorithms. DiffMAE largely improves LPIPS $\downarrow$ on both random $75 \%$ mask and center block mask protocols, setting a new state-of-the-art for ImageNet inpainting. We include more generative inpainting examples in the Appendix.

## 7. Conclusion

We present Diffusion Masked Autoencoders (DiffMAE), a self-supervised framework designed for recognizing and generating images and videos. We accomplish this by integrating masking into diffusion models, thereby transforming conditional diffusion models into masked autoencoders. By re-evaluating the philosophy of generation and viewing it as a tool for genuinely comprehending visual data, we contribute to the current trend in generative models. Ultimately, we aspire for our research to spark further exploration of integrated methods for recognition and generation.

## Appendix

In the Appendix, we first provide implementation details in Appendix A, and then provide more qualitative results in Appendix B.

## A. Implementation Details

## A.1. ImageNet Experiments

Noise schedule with $\rho$. We introduce a hyper-parameter $\rho$ to control the noise level of training inputs. Specifically, we use $\rho$ to exponentiate each variance $\beta_{t}$ to $\beta_{t}^{\rho}$, enlarging these noise variance. Recall that the training samples can be reparameterized to $\boldsymbol{x}_{t}^{m}=\sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}^{m}+\sqrt{1-\bar{\alpha}_{t}} \epsilon$, where $\alpha_{t}=1-\beta_{t}$ and $\bar{\alpha}_{t}=\prod_{i=1}^{t} \alpha_{i}$. In Fig. 9, we plot how the values of the data coefficient $\bar{\alpha}_{t}$ progress with different $\rho$. $\rho=1.0$ represents the default linear schedule introduced in DDPM [40], where the forward process variances $\beta_{t}$ increase linearly from $10^{-4}$ to 0.02 . With $\rho=0.8$ and $\rho=0.6$, the data coefficients $\bar{\alpha}_{t}$ are lower at each timestep $t$, and the amount of noise is therefore amplified.

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-09.jpg?height=402&width=597&top_left_y=1165&top_left_x=276)

Figure 9. $\bar{\alpha}_{t}$ throughout the diffusion process with different $\rho$.

Architecture. We use the standard ViT architecture [21] in base, large and huge sizes for the encoder. The encoder is followed by Layer Normalization [1]. There is a linear projection layer after the layer normalization to match the dimension of the encoder to that of the decoder. We add sinusoidal positional embeddings to both the encoder and decoder inputs for pre-training. We do not use either relative positional embedding [75] or layer scale [12]. The encoder and the decoder use two different linear projections to handle the clean and the noised (masked) inputs, respectively.

During fine-tuning, we extract features from the encoder. We use global average pooling to gather the patch features, followed by a layer normalization and a linear classification head. Both layer normalization and the linear head are randomly initialized. Particularly, the linear head is initialized with a very small standard deviation $2^{-5}$, which enhances stability of fine-tuning.

Training recipes. The default settings for pre-training and fine-tuning are in Tab. 11. We use a different base learning rate and layer decay when fine-tuning CLIP-aided models.

| config | ImageNet | Kinetics |
| :---: | :---: | :---: |
| optimizer | AdamW [56] |  |
| optimizer momentum | {$\beta_{1}, \beta_{2}=0.9,0.95$ <br> 0.05} |  |
| weight decay |  |  |
| learning rate schedule | cosine decay [55] |  |
| warmup epochs [32] | {40 <br> hflip, RandomResizedCrop <br> 0.0} |  |
| augmentation <br> drop path [48] |  |  |
| base $\mathrm{lr}$ | $1.5 \mathrm{e}-4$ | $8.0 \mathrm{e}-4$ |
| batch size | 4096 | 512 |
| epochs | 1600 | 400 |
| gradient clipping | - | 0.02 |
| repeated aug. [42] | - | 4 |

(a) Pre-training setting.

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-09.jpg?height=827&width=792&top_left_y=817&top_left_x=1081)

(b) Fine-tuning setting.

Table 11. Configurations on IN-1K and Kinetics-400. In terms of learning rate (lr), we use the linear scaling rule introduced in [32]: lr = base_lr $\times$ batch_size / 256. When using repeated augmentation, the number of epochs and batch size count the original samples without repeating.

## A.2. Kinetics Experiments

Architecture. Given a video clip, we first divide it into non-overlapping patches in spacetime. Positional embeddings are added to the embedded patches. The spacetime patch size is $2 \times 16 \times 16$ for ViT-L/16 and $2 \times 14 \times 14$ for ViT-L/14. The target of our DiffMAE is a single time slice of the patch $(16 \times 16$ or $14 \times 14)$, and so are the corresponding noisy inputs to the decoder [25]. Similar to the image setting, the encoder and the decoder use two different linear projections to handle the clean and the noisy (masked) inputs, respectively. We use $90 \%$ random masking sampling on the spacetime patches [25].

| config | from-scratch |
| :--- | :---: |
| optimizer | AdamW [56] |
| optimizer momentum | $\beta_{1}, \beta_{2}=0.9,0.999$ |
| weight decay | 0.02 |
| learning rate schedule | cosine decay [55] |
| warmup epochs [32] | 10 |
| augmentation | RandAug $(9,0.5)$ |
| mixup [93] | 0.8 |
| cutmix [92] | 1.0 |
| label smoothing [79] | 0.1 |
| batch size | 512 |
| epochs | 200 |
| base lr | $1.0 \mathrm{e}-3$ |
| layer decay [12] | - |
| drop path [48] | 0.1 |

Table 12. Configurations of fine-tuning ADM [16] on IN-1K. In terms of learning rate ( $l r$ ), we use the linear scaling rule introduced in [32]: $l r=$ base_lr $\times$ batch_size $/ 256$. For fine-tuning, we use ADM's unconditional $256^{2}$ model trained on IN-1K.

We extract features from the encoder outputs for finetuning. We use global average pooling to gather the patch features, followed by a layer normalization and a linear head. The linear head is initialized with a very small standard deviation $2^{-5}$, the same as the image setting. To further enhance the results, we fine-tune the $16 \times 224^{2}$ Kinetics-400 model to a longer duration 32 and a larger resolution $280^{2}$ for a short schedule of 30 epochs without repeated augmentation.

Training recipes. The default settings for pre-training and fine-tuning are in Tab. 11. Note that many hyper-parameters are shared by the image and the video models, showing that DiffMAE is general across different domains. We search for the best base learning rate and layer decay when fine-tuning CLIP-aided models.

## A.3. Fine-Tuning ADM

We fine-tune the pre-trained ADM [16] model to evaluate the recognition ability of this well-designed diffusion model. Specifically, we take its $\mathrm{IN}-1 \mathrm{~K}$ unconditional $256^{2}$ version and fine-tune the model at resolution $224^{2}$ on IN-1K classification for a fair comparison to other methods.

The ADM model uses a U-Net [73] architecture for dense prediction. It consists of ResNet [37] blocks and selfattention layers [83]. We fine-tune the input blocks and the middle block, which are followed by a global average pooling, a layer normalization, and a linear classification head that projects the global averaged feature to classification logits. The layer normalization and the linear head are randomly initialized. Regarding the timestep input specifying the noise level for diffusion generation, we simply fix the timestep to 999 for classification fine-tuning, while other numbers that are inside the range of the noise schedule, i.e., from 0 to 999 , give similar training curves and results. We also train the same model from scratch as the baseline to show the effectiveness of diffusion generative pre-training.

Training recipes. We include the training recipes of fineturning and from-scratch training of ADM in Tab. 12. We carefully tune the optimization hyper-parameters of both the fine-tuning and the from-scratch training. The recipes are based on sophisticated modern training techniques [80, 54], and we tune base learning rate, layer-wise decay [12], and drop path rate for each case.

## B. Additional Qualitative Results

We provide more qualitative results of image generation using ImageNet-1K validation images. Figs. 10 and 11 are samples with $75 \%$ random masking. Figs. 12 and 13 are samples with the center block masking.

In Fig. 14, we provide visualizations on DiffMAE for video generation on Kinetics-400 validation videos. For a $16 \times 224 \times 224$ video clip, we visualize the generated frames at stride two on the temporal dimension, which makes eight frames for each sample.

Acknowledgement. We thank Yuan-Ting Hu and Minkai $\mathrm{Xu}$ for valuable discussion.

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-11.jpg?height=2114&width=1743&top_left_y=241&top_left_x=164)

Figure 10. Visualizations of DiffMAE generation with $\mathbf{7 5 \%}$ random masking. The images are from IN-1K validation set with size $224^{2}$. We show the reverse diffusion at $t=1000,500$, and $0 . t=0$ is the final output. The model is ViT-L. Best viewed in color with zoom.

![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-12.jpg?height=2123&width=1743&top_left_y=234&top_left_x=164)

Figure 11. Visualizations of DiffMAE generation with $\mathbf{7 5 \%}$ random masking. The images are from $\mathrm{IN}-1 \mathrm{~K}$ validation set with size $224^{2}$. We show the reverse diffusion at $t=1000,500$, and $0 . t=0$ is the final output. The model is ViT-L. Best viewed in color with zoom.
![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-13.jpg?height=2108&width=1572&top_left_y=237&top_left_x=248)

Figure 12. Visualizations of DiffMAE generation with center masking. The images are from IN-1K validation set. The input images are of size $256^{2}$, with the center $128^{2}$ block masked. The model is ViT-L. Best viewed in color with zoom.
![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-14.jpg?height=2094&width=1576&top_left_y=236&top_left_x=248)

Figure 13. Visualizations of DiffMAE generation with center masking. The images are from IN-1K validation set. The input images are of size $256^{2}$, with the center $128^{2}$ block masked. The model is ViT-L. Best viewed in color with zoom.
![](https://cdn.mathpix.com/cropped/2024_06_04_04184cd37d31a16aa828g-15.jpg?height=2132&width=1414&top_left_y=236&top_left_x=323)

Figure 14. Visualizations of DiffMAE generation with video. The videos are from Kinetics-400 validation set with random masking ratio $90 \%$. We show the original video (top), masked video (middle), and DiffMAE output (bottom) for each sample. The model is ViT-L/14.

## References

[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. NeurIPS, 2016. 9

[2] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao $\mathrm{Gu}$, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. In ICML, 2022. 2

[3] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pretraining of image transformers. In ICLR, 2022. 2, 7

[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. 1, 2

[5] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In $E C C V, 2018$. 2

[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. 2

[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 2, 7

[8] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. 1, 2

[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2

[10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021. 2

[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, 2021. 7

[12] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR, 2020. 9, 10

[13] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. RandAugment: Practical automated data augmentation with a reduced search space. In CVPR, 2020. 9

[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009. 1, 5

[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. 2

[16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 1, 2, 4, 8, 10
[17] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. 2

[18] Jeff Donahue, Philipp Krhenbhl, and Trevor Darrell. Adversarial feature learning. In $I C L R, 2017.1,2$

[19] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In NeurIPS, 2019. 1, 2

[20] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Peco: Perceptual codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710, 2021. 2

[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth $16 \times 16$ words: Transformers for image recognition at scale. In ICLR, 2021. 2, 4, 5, 8, 9

[22] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks. TPAMI, 2015. 2

[23] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In ICCV, 2021. 8

[24] Christoph Feichtenhofer. X3D: Expanding architectures for efficient video recognition. In CVPR, 2020. 8

[25] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal learners. In NeurIPS, 2022. 2, 8, 9

[26] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In ICCV, 2019. 8

[27] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In CVPR, 2021. 2

[28] Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould. Self-supervised video representation learning with odd-one-out networks. In CVPR, 2017. 2

[29] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. 2

[30] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing $\mathrm{Xu}$, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. In NeurIPS, 2014. 2

[31] Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, and Yann LeCun. Unsupervised learning of spatiotemporally coherent metrics. In ICCV, 2015. 2

[32] Priya Goyal, Piotr Dollr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 9, 10

[33] Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rmi

Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeruIPS, 2020. 2

[34] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 1, 2, 3, 5, 6, 7

[35] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 2

[36] Kaiming He, Georgia Gkioxari, Piotr Dollr, and Ross Girshick. Mask R-CNN. In ICCV, 2017. 8

[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 10

[38] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 2006. 1

[39] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2

[40] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1, 2, 3, 4, 5, 6, 9

[41] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. In NeurIPS, 2022. 2

[42] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In CVPR, 2020. 9

[43] Zejiang Hou, Fei Sun, Yen-Kuang Chen, Yuan Xie, and SunYuan Kung. MILAN: Masked image pretraining on language assisted representation. arXiv preprint arXiv:2208.06049, 2022. 4, 7

[44] Ronghang Hu, Shoubhik Debnath, Saining Xie, and Xinlei Chen. Exploring long-sequence masked autoencoders. arXiv preprint arXiv:2210.07224, 2022. 2

[45] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. NeurIPS, 2014. 2

[46] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 2

[47] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012. 5

[48] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. FractalNet: Ultra-deep neural networks without residuals. In ICLR, 2017. 9, 10

[49] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In $E C C V, 2022.8$

[50] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Improved multiscale vision transformers for classification and detection. In CVPR, 2022. 8
[51] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 8

[52] Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In $E C C V$, 2022. 8

[53] Xingbin Liu, Jinghao Zhou, Tao Kong, Xianming Lin, and Rongrong Ji. Exploring target representations for masked autoencoders. arXiv preprint arXiv:2209.03917, 2022. 2

[54] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In CVPR, 2022. 10

[55] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In ICLR, 2017. 9, 10

[56] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 9, 10

[57] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. RePaint: Inpainting using denoising diffusion probabilistic models. In CVPR, 2022. 5,8

[58] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuffle and learn: unsupervised learning using temporal order verification. In $E C C V, 2016.2$

[59] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general video recognition. In $E C C V, 2022.8$

[60] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In ICML, 2022. 1, 2

[61] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. 2

[62] Deepak Pathak, Ross Girshick, Piotr Dollr, Trevor Darrell, and Bharath Hariharan. Learning features by watching objects move. In CVPR, 2017. 2

[63] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016. 2

[64] Jialun Peng, Dong Liu, Songcen Xu, and Houqiang Li. Generating diverse structure for image inpainting with hierarchical vq-vae. In CVPR, 2021. 8

[65] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. BEiT v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022. 2, 7

[66] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. In CVPR, 2021. 2

[67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen

Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. $2,4,7,8$

[68] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. 2

[69] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022. 1, 2, 3, 4, 6

[70] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 1, 2

[71] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In NeurIPS, 2019. 8

[72] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2

[73] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. UNet: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 4, 10

[74] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. 1, 2, 4

[75] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Selfattention with relative position representations. In NAACL, 2018. 7, 9

[76] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2

[77] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 1, 2, 3

[78] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 2014. 9

[79] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. 9, 10

[80] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers \& distillation through attention. In ICML, 2021. 10

[81] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv Jgou. Going deeper with image transformers. In ICCV, 2021. 7

[82] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017. 2
[83] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 4, 10

[84] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 2

[85] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and Lon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. JMLR, 2010. 1, 2

[86] Xiaolong Wang, Kaiming He, and Abhinav Gupta. Transitive invariance for self-supervised visual representation learning. In ICCV, 2017. 2

[87] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In CVPR, 2022. $2,4,7,8$

[88] Chen Wei, Lingxi Xie, Xutong Ren, Yingda Xia, Chi Su, Jiaying Liu, Qi Tian, and Alan L Yuille. Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning. In CVPR, 2019. 2

[89] Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, and Qi Tian. MVP: Multimodality-guided visual pre-training. In ECCV, 2022. 7

[90] Zhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In CVPR, 2018. 2

[91] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: A simple framework for masked image modeling. In CVPR, 2022. 2

[92] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. 9, 10

[93] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018. 9, 10

[94] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016. 2

[95] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. 5

[96] Xiaosong Zhang, Yunjie Tian, Wei Huang, Qixiang Ye, Qi Dai, Lingxi Xie, and Qi Tian. HiViT: Hierarchical vision transformer meets masked image modeling. arXiv preprint arXiv:2205.14949, 2022. 2

[97] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. iBOT: Image bert pretraining with online tokenizer. In ICLR, 2022. 2, 7

