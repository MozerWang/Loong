# Improving and Simplifying Pattern Exploiting Training 

Derek Tam* Rakesh R Menon*<br>Mohit Bansal Shashank Srivastava Colin Raffel<br>UNC Chapel Hill<br>\{dtredsox, rrmenon, mbansal, ssrivastava, craffel\}@cs.unc.edu


#### Abstract

Recently, pre-trained language models (LMs) have achieved strong performance when finetuned on difficult benchmarks like SuperGLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that leverages patterns for few-shot learning. However, Pet uses task-specific unlabeled data. In this paper, we focus on few shot learning without any unlabeled data and introduce ADAPET, which modifies PET's objective to provide denser supervision during fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any task-specific unlabeled data. Our code can be found at https:// github.com/rrmenon10/ADAPET.


## 1 Introduction

Pre-trained language models (LMs) have shown significant gains across a wide variety of natural language processing (NLP) tasks in recent years (Devlin et al., 2019; Radford et al., 2018; Raffel et al., 2020). Most of these gains are obtained by fine-tuning language models on labeled data for a particular task. However, performance can suffer when there is very limited labeled data available for a downstream task (Xie et al., 2020; Chen et al., 2020).

Recently, GPT-3 (Brown et al., 2020) demonstrated how language models, when scaled to hundreds of billions of parameters, can learn well when primed with only a few labeled examples. However, the scale of GPT-3 (175B parameters) makes it impractical to study. There is, therefore, a need to develop smaller language models that can work equally well with limited labeled data.

Pattern-Exploiting Training (PET; Schick and Schütze, 2021a,b) reformulates natural language understanding tasks as cloze-style questions and[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_bdc175d233d206b900c4g-01.jpg?height=588&width=745&top_left_y=734&top_left_x=1067)

Figure 1: Performance of ADAPET vs iPET/PET and GPT-3 on SuperGLUE. While iPET/PET are parameterefficient, they use $\sim 9 \mathrm{~K}$ unlabeled examples in addition to 32 labeled examples per task. ADAPET uses just 32 labeled examples, and performs better than iPET.

performs gradient-based fine-tuning. In doing so, PET outperforms GPT-3 with few labeled examples using ALBERT (Lan et al., 2020). However, PET uses additional task-specific unlabeled data.

We propose ADAPET (A Densely-supervised Approach to Pattern Exploiting Training) that uses more supervision by decoupling the losses for the label tokens and a label-conditioned masked language modeling (MLM) objective over the full original input. On SuperGLUE (Wang et al., 2019) with 32 labeled examples per task, ADAPET outperforms iPET without any unlabeled data.

## 2 Background

Cloze-style questions and MLM. A cloze task is a problem where certain parts of a text are removed, and the goal is to replace the missing portion based on the context (Taylor, 1953). Here, the text that has some parts removed is considered a cloze-style question. Inspired by cloze tasks, BERT introduces the MLM objective that tries to predict the original word at the masked out

![](https://cdn.mathpix.com/cropped/2024_06_04_bdc175d233d206b900c4g-02.jpg?height=428&width=694&top_left_y=254&top_left_x=293)

(a) Decoupling Label Loss

![](https://cdn.mathpix.com/cropped/2024_06_04_bdc175d233d206b900c4g-02.jpg?height=434&width=739&top_left_y=254&top_left_x=1047)

(b) Label Conditioning

Figure 2: We illustrate the training with the two components of ADAPET. Here, the blue boxes refer to the inputs from a task (entailment, in this case). Figure 2a shows the decoupling label objective. The model has to predict the correct and incorrect labels at the masked out position, using a BCE loss over all labels. For the label conditioning objective in Figure 2b, the input text either includes the correct or incorrect label. At a randomly masked out position, the model should predict the original token when the input text has the correct label, and should not predict the original token when the input text has an incorrect label.

positions in a cloze question.

Notation. Let $G$ represent a language model, $x$ represent the input example converted into a cloze-style question, and $y$ represent the label at the masked location $m$. We are interested in the quantity $\llbracket G_{m}(x) \rrbracket_{z}$ which represents the logit value for a specific token $z$ at the mask location $m$.

### 2.1 Unlabeled Data Access

Schick and Schütze (2021a,b) assumes access to task-specific unlabeled data. For some applications such as sentiment analysis, unlabeled data can be cheap to acquire. But for SuperGLUE, where the examples are pairs of text with a label that is constructed to test a model's natural language understanding abilities, it might be more expensive to acquire unlabeled data. For example, the construction of BoolQ requires annotators to filter good question-article pairs before assigning labels (Clark et al., 2019). Hence, for our setup, we do not assume access to task-specific unlabeled data, which aligns with the setup in Brown et al. (2020).

### 2.2 PET

Our work primarily builds on top of PET (Schick and Schütze, 2021a,b). PET converts an example into a cloze-style question, similar to the input format used during pre-training. The query-form in PET is defined by a Pattern-Verbalizer Pair (PVP). Each PVP consists of

- a pattern which describes how to convert the inputs into a cloze-style question with masked out tokens. We illustrate this for an entailment task in Figure 2a. Here, we convert the premise ("Oil prices fall back") and the hypothesis ("Oil prices rise") into a clozestyle question with the pattern: <premise> ? <mask>, <hypothesis>.
- a verbalizer which describes the way to convert the classes into the output space of tokens. In Figure 2a, the verbalizer maps "Not Entailment/Entailment" to "No/Yes".

After hand-designing a PVP for a given task, PET obtains logits from the model $G_{m}(x)$ (in the singletoken label case). Given the space of output tokens $\mathcal{Y}$, (in Figure 2a $\{$ "Yes", "No" \}) PET computes a softmax over $y \in \mathcal{Y}$, using the logits from $G_{m}(x)$. The final loss is shown in Equation 2.

$$
\begin{align*}
q(y \mid x) & =\frac{\exp \left(\llbracket G_{m}(x) \rrbracket_{y}\right)}{\sum_{y^{\prime} \in \mathcal{Y}} \exp \left(\llbracket G_{m}(x) \rrbracket_{y^{\prime}}\right)}  \tag{1}\\
\mathcal{L} & =\operatorname{CE}\left(q\left(y^{*} \mid x\right), y^{*}\right) \tag{2}
\end{align*}
$$

PET additionally distils knowledge from an ensemble of models trained with different patterns on both labeled and unlabeled data. iPET is an iterative variant of PeT that trains models across iterations. The size of the training set gradually increases each iteration based on the labels of previous iterations. For a description of the different patterns used across the tasks (Schick and Schütze, 2021b), we refer the reader to Appendix A.1.

## 3 ADAPET

Our proposed approach, called ADAPET, modifies the objective from PET so that it can provide more supervision and learn without task-specific unlabeled data.

### 3.1 Decoupling Label Losses

PET computes class probabilities using the logits that correspond to the labels for a specific task This discards the information from all the other logits in the vocabulary that do not correspond to a label. For example, in Figure 2a, "oil" is not a class token so the LM head should assign a low probability to "oil". However, because PET only extracts the token logits that correspond to labels, the non-label tokens will never have any gradient signal.

One solution is to change the objective to a regular MLM objective. In that case, there would be no distinction between tokens corresponding to incorrect classes and any other token in the vocabulary For example, in Figure 2a, the model would be trained to treat "Yes" (the incorrect token) the same as any other token such as "oil". While we want the model to discourage "oil", the training objective should still specifically suppress "Yes".

In ADAPET, we penalize incorrect class tokens and encourage correct class tokens. Specifically, the model computes the probability of each token as a softmax normalized across all tokens so that each probability is influenced by the logits of all the vocabulary tokens. Then, we maximize the probability of the correct class tokens and minimize the probability of incorrect class tokens. This is equivalent to binary cross entropy, as shown in Figure 2a. Formally, if $y^{*}$ is the true label for an example,

$$
\begin{align*}
q(y \mid x) & =\frac{\exp \left(\llbracket G_{m}(x) \rrbracket_{y}\right)}{\sum_{v^{\prime} \in \mathcal{V}} \exp \left(\llbracket G_{m}(x) \rrbracket_{v^{\prime}}\right)}  \tag{3}\\
\mathcal{L}_{D} & =\log q\left(y^{*} \mid x\right)-\sum_{y \neq y^{*}} \log q(y \mid x) \tag{4}
\end{align*}
$$

The loss can be rewritten using binary cross entropy or regular cross entropy as:

$$
\begin{align*}
\mathcal{L}_{D} & =\operatorname{BCE}\left(q\left(y^{*} \mid x\right), 1\right)+\sum_{y \neq y^{*}} \operatorname{BCE}(q(y \mid x), 0)  \tag{5}\\
& =\operatorname{CE}\left(q\left(y^{*} \mid x\right), y^{*}\right)-\sum_{y \neq y^{*}} \operatorname{CE}(q(y \mid x), y) \tag{6}
\end{align*}
$$

### 3.1.1 Unified Loss for Different Tasks

For normal tasks where the label is exactly one token, PET uses the formulation described in Equation 2. For WSC (Levesque et al., 2012), which does not have incorrect class labels, PET uses the original MLM objective rather than Equation 2. This is equivalent to Equation 5 without the second term in ADAPET.

For other tasks with multi-token labels (COPA (Roemmele et al., 2011), ReCoRD (Zhang et al., 2018)), PET computes the probability of the classes as the sum of the log probabilities of the individual tokens. However, it is not obvious how to convert these label probabilities into a valid probability distribution.

Rather than normalizing the probabilities, PET uses a hinge loss to ensure a margin between the correct label and the incorrect labels.

In ADAPET, for each token in the label, $\mathcal{L}_{D}$ discriminates the correct token from every other tokens, via the following loss: ${ }^{1}$

$$
\begin{equation*}
\mathcal{L}_{D}=\sum_{z^{*} \in y^{*}} \operatorname{BCE}\left(q\left(z^{*} \mid x\right), 1\right)+\sum_{y \neq y^{*}} \sum_{z \in y} \operatorname{BCE}(q(z \mid x), 0) \tag{7}
\end{equation*}
$$

This objective splits a single loss based on multiple tokens into multiple losses over single tokens. As a result, we do not need to to multiply the probabilities of the individual tokens, and thus do not run into normalization issues.

### 3.2 Label Conditioning

The PET objective encapsulates the question: "Given the input, what is the right label?." However, since the input space and output space both consist of tokens, we can also ask the inverse question, "Given the answer, what is the correct context?". The model is trained to predict the input given the label. Formally, let $x^{\prime}$ be the original input $x$ modified by randomly masking out tokens from the context and $x^{m}$ be the original context tokens masked out in $x^{\prime}$. In the label conditioning objective, we are interested in the quantity $P\left(x^{m} \mid x^{\prime}, y\right)$, which encourages the model to predict the masked out tokens in the input given the label.

During training, if the label is correct, the model has to predict the original token, as shown in Figure 2b. Additionally, if the label is wrong, the[^1]

| Method | BoolQ <br> Acc. | $\mathbf{C B}$ <br> Acc./F1 | COPA <br> Acc. | RTE <br> Acc. | WiC <br> Acc. | WSC <br> Acc. | MultiRC <br> EM/F1a | ReCoRD <br> Acc./F1 | Avg |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ALBERT | $55.7 \quad$ | $68.6 / 49.1$ | 63.0 | 50.5 | $41.4 \quad$ | 81.7  | $3.6 / 49.8 \quad$ | $84.1 / 83.5 \quad$ | $57.7 \quad$ |
|  | 77.5 | $82 \quad$ | 2.0 \& | 72.9 | $55.3 \leftarrow$ | 75.0 | 74.8 | $89.0 / 90.1$ \& | 73.2 |
| SPET (LAB; $;$ | 76.9 | $87.5 / 85.4$ | 9.0 | 67.1 | 49.7 | $82.7 \&$ | $31.2 / 74.6$ | $85.0 / 91.9$ | 74.2 |
| ADAPET (LAB; SINGLE) | $80.3 \leftarrow$ | $89.3 / 86.8 \div$ | 89.0 | $76.5 \leftarrow \leftrightarrow$ | 54.4 | 81.7 | $39.2 / 80.1 \leftarrow \leftarrow$ | $85.4 / 92.1$ | $77.3 \leftarrow \leftarrow$ |
|  |  |  |  | 69.8 | 52.4 | 80.1 |  | 36.5 | 74.1 |
| iPET (LAB + UNLAB; ENSEMBLE) | 80.6 | $92.9 / 92.4 \bullet$ | $95.0 \diamond$ | 74.0 | 52.2 | 80.1 | $33.0 / 74.0$ | $86.0 / 86.5$ | 76.8 |

Table 1: Few-shot classification results on SuperGLUE with 32 labeled examples on the dev set. Note, we do not have access to the train split of GPT-3, so we follow the split provided by (Schick and Schütze, 2021b). $\$=$ BEST SINGLE PATTERN MODEL, =BEST MODEL OVERALL, LAB=LABELED DATA, UNLAB=UNLABELED DATA

|  | BoolQ | CB | COPA | RTE | WiC | WSC | MultiRC | ReCoRD | Avg |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Method | Acc. | Acc./F1 | Acc. | Acc. | Acc. | Acc. | EM/F1a | Acc./F1 |  |
| GPT-3 (LAB; SINGLE) | 76.4 | $75.6 / 52.0$ | $92.0 \div$ | 69.0 | 49.4 | 80.1 | $30.5 / 75.4$ | $90.2 / 91.1$ \& | 71.8 |
| ADAPET (LAB; SINGLE) | 80.0 \& $\quad 2+12$ | $92.0 / 82.3 \leftarrow$ | 85.4 | 75.0 \& | 53.5 \& | 85.6 \& $\quad 2+1+2$ | $35.7 / 76.2 \div$ | $85.5 / 86.1$ |  |
| PET (LAB + UNLAB; EN: | 79.1 | $87.2 / 60.2$ | 90.8 | 67.2 | 50.7 | $88.4 \bullet$ | $36.4 / 76.6 \vee$ | $85.4 / 85.9$ | 74.0 |
| iPET (LAB + UNLAB; ENSEMBLE) | $81.2 \bullet$ | $88.8 / 79.9$ | 90.8 | 70.8 | 49.3 | $88.4 \bullet$ | $31.7 / 74.1$ | $85.4 / 85.9$ | 75.4 |

Table 2: Few-shot classification results on SuperGLUE with 32 labeled examples on the hidden test set. $=$ BEST SINGLE PATTERN MODEL, =BEST MODEL OVERALL, LAB=LABELED DATA, UNLAB=UNLABELED DATA

model is forced to not predict the original token. ${ }^{2}$ We maximize $P\left(x^{m} \mid x^{\prime}, y^{*}\right)$ and minimize $P\left(x^{m} \mid x^{\prime}, y\right) \forall y \neq y^{*}$. This objective is the same as the decoupling label losses approach described in Equation 5, except with different inputs and outputs.

$$
\begin{gather*}
q\left(x^{m} \mid x^{\prime}, y\right)=\frac{\exp \left(\llbracket G_{m}\left(x^{\prime}, y\right) \rrbracket_{x^{m}}\right)}{\sum_{v^{\prime} \in \mathcal{V}} \exp \left(\llbracket G_{m}\left(x^{\prime}, y\right) \rrbracket_{v^{\prime}}\right)}  \tag{8}\\
\mathcal{L}_{M}=\operatorname{BCE}\left(q\left(x^{m} \mid x^{\prime}, y^{*}\right), 1\right)+\sum_{y \neq y^{*}} \operatorname{BCE}\left(q\left(x^{m} \mid x^{\prime}, y\right), 0\right) \tag{9}
\end{gather*}
$$

The final loss for ADAPET is a sum of the decoupled label loss and the label-conditioned MLM loss.

## 4 Results and Analyses

We run experiments on SuperGLUE, and follow the same data split as Schick and Schütze (2021b), which consists of 32 labeled examples for each task.

Our code is implemented in Pytorch (Paszke et al., 2019) using HuggingFace (Wolf et al., 2020). We use the same pre-trained model and hyperparameters as PET, except we increased the number of training batches to $1 \mathrm{k}$ and choose the best checkpoint on the dev set, since it has been shown that training longer can help even with few samples (Zhang et al., 2021). For all ablation experiments, we only use the first pattern ${ }^{3}$ and train for 250[^2]

batches. We refer the reader to Appendix B for more details.

Since we do not assume access to unlabeled data (see Section 2.1), we do not apply the three-step training procedure of PET and iPET to ADAPET. We still assume access to the full development set to choose the best masking ratio and checkpoint model, since PET presumably used the full development set to choose their hyperparameters which we copy.

### 4.1 Results

Table 1 and Table 2 shows our results on the validation and test sets on SuperGLUE. We compare against GPT-3 and PET/iPeT. Note that PET/iPET uses unlabeled data and a three step training procedure (Schick and Schütze, 2021b). For fair comparison, we train PET with a single pattern (sPET) for $1 \mathrm{k}$ batches, and report scores for the best performing pattern on the validation set. We include a further analysis of how well the models perform for each pattern in Appendix A.2.

On the dev set, ADAPET outperforms all models that do not use unlabeled data, and even outperforms PET's iterative variant, iPET, by 0.5 points absolute. Surprisingly, SPET outperforms РET, but still loses to iPET by 2.6 points. But, this is in line with the ablation from Schick and Schütze (2021b), which shows that ensembling SPET models, trained with only labeled data, outperforms Pet. Also, Gao et al. (2021) show that the model with the best performing pattern outperforms ensembling SPET models.

On the test set, ADAPET outperforms all other
models including iPET without access to the unlabeled examples ( $\sim 9 \mathrm{k}$ on average per task) and achieves state-of-the-art for few-shot learning on SuperGLUE.

### 4.2 Loss Ablation

Table 3 shows our ablation analysis for the loss functions we introduce in this paper. From the results, we see that label conditioning (LC) is extremely beneficial for ADAPET, especially on CB Comparing our modified decoupled label objective (ADAPET W/o LC) with sPET, we see that it does worse for CB on F1, but does much better on RTE and MultiRC. Next, we compare against LC conditioned only on the correct label. We see that this hurts on BoolQ, but helps on CB. We ablate other model choices in Appendix C.

|  | BoolQ | CB | RTE | MultiRC |
| :---: | :---: | :---: | :---: | :---: |
| Method | Acc. | Acc./F1 | Acc. | $\mathrm{EM} / \mathrm{F} 1 \mathrm{a}$ |
| ADAPET | ![](https://cdn.mathpix.com/cropped/2024_06_04_bdc175d233d206b900c4g-05.jpg?height=36&width=90&top_left_y=1147&top_left_x=565) | $91.1 / 88.1$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_bdc175d233d206b900c4g-05.jpg?height=36&width=72&top_left_y=1147&top_left_x=810) | $38.6 / 79.8$ |
| ADAPET w/o | 78.1 | $75.0 / 62.8$ | 64.3 | $37.0 / 79.1$ |
| ADAPET LC (Pos. Ex. | 75.4 | 83.9 / 80.9 | 72.2 | $31.3 / 76.9$ |
| SPET | 77.5 | $75.0 / 72.8$ | 57.0 | $26.5 / 73.2$ |

Table 3: Ablation of ADAPET with different components. Best numbers have been bolded. (LC= LABEL CONDITIONING)

## 5 Conclusion

In this paper, we propose ADAPET, a new method for few-shot natural language understanding. Crucially, our work does not use unlabeled data and instead leverages more supervision to train the model. Assuming the same data budget, our model outperforms GPT-3 on SuperGLUE using just $0.1 \%$ as many parameters. However, our method has limitations; for example, we use a naive random masking strategy, which might not make sense for label conditioning. Future work could look into better masking strategies for labeled conditioned MLM, such as masking important tokens based on the the gradients of the logits for an example, as has been done for interpreting models (Simonyan et al., 2014).

## Acknowledgments

This work was supported in part by ONR Grant N00014-18-1-2871, DARPA YFA17D17AP00022, and NSF-CAREER Award 1846185. The views contained in this article are those of the authors and not of the funding agency.

## References

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.

Jiaao Chen, Zichao Yang, and Diyi Yang. 2020. MixText: Linguistically-informed interpolation of hidden space for semi-supervised text classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 21472157, Online. Association for Computational Linguistics.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics.

Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. Electra: Pretraining text encoders as discriminators rather than generators. In International Conference on Learning Representations.

Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05, page 177-190, Berlin, Heidelberg. Springer-Verlag.

Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung, 23(2):107-124.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Tianyu Gao, Adam Fisch, and Danqi Chen. 2021 Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830, Online. Association for Computational Linguistics.

Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.

Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252-262, New Orleans, Louisiana. Association for Computational Linguistics.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.

Hector J. Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12, page 552-561. AAAI Press.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.

Mohammad Taher Pilehvar and Jose CamachoCollados. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267-1273, Minneapolis, Minnesota. Association for Computational Linguistics.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.

Melissa Roemmele, Cosmin Bejan, and Andrew Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning.

Timo Schick and Hinrich Schütze. 2021a. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computational Linguistics.

Timo Schick and Hinrich Schütze. 2021b. It's not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339-2352, Online. Association for Computational Linguistics.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. In Workshop at International Conference on Learning Representations.

William Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.

Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Unsupervised data augmentation for consistency training. In Advances in Neural Information Processing Systems, volume 33, pages 6256-6268. Curran Associates, Inc.

Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018.

Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885.

Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. 2021. Revisiting fewsample BERT fine-tuning. In International Conference on Learning Representations.
