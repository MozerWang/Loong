# DISTILLSPEC: IMPROVING SPECULATIVE DECODING VIA KNOWLEDGE DISTILLATION 

Yongchao Zhou ${ }^{1,3 *}, \quad$ Kaifeng Lyu $^{1,4 *}$, Ankit Singh Rawat ${ }^{1}$, Aditya Krishna Menon ${ }^{1}$,<br>Afshin Rostamizadeh ${ }^{1}, \quad$ Sanjiv Kumar ${ }^{1}, \quad$ Jean-Fran√ßois Kagy $^{1 \dagger}, \quad$ Rishabh Agarwal ${ }^{2,5 \dagger}$<br>${ }^{1}$ Google Research ${ }^{2}$ Google DeepMind ${ }^{3}$ University of Toronto ${ }^{4}$ Princeton University ${ }^{5}$ Mila


#### Abstract

Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec, a method that uses knowledge distillation to better align the draft model with the target model before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields $10-45 \%$ speedups over standard SD on a range of benchmarks, using both greedy and non-greedy sampling. We show that the distilled model can be well transferred to various tasks with an average speedup of $26 \%$. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by $6-10 \times$ with minimal performance drop, compared to standard decoding without distillation.


## 1 INTRODUCTION

Large language models (LLMs) have revolutionized natural language understanding and generation across diverse applications (OpenAI, 2023, Anil et al., 2023). However, their autoregressive generation nature poses significant computational challenges, especially in real-time deployments with stringent latency constraints (Thoppilan et al., 2022; Pope et al. 2023). Conversely, smaller language models, while computationally efficient, often lack the expressive power of their larger counterparts and achieve subpar performance. While reducing the inference cost of larger models, e.g., via quantization or pruning, or improving the performance of the smaller models, e.g., via knowledge distillation (KD) (Hinton et al., 2015), constitute natural approaches to enable a favorable performance versus inference cost trade-off, these approaches frequently result in unacceptable performance gap compared to the high-quality large models. This has inspired a growing literature on designing mechanisms that combine both large and small models at inference to approximate the performance of the larger models without incurring their high computational cost.

Among conventional approaches, model cascading aims to identify easy instances where smaller models suffice to achieve good performance, and soliciting larger models only on a subset of hard instances (Rowley et al., 1998; Xu et al. 2014) or tasks (Cai et al., 2023b). Different from such taskor instance-level cascading, speculative decoding (SD) (Leviathan et al., 2023; Chen et al., 2023) exploits the token-level variability in the computation demand during LLM inference by interactively invoking a small "draft" model and a large "target" model. At a given stage during inference, the draft model generates successive candidate tokens for multiple inference steps via autoregressive decoding. The target model then verifies the candidate tokens via parallel decoding, and employs rejection sampling to accept a subset of candidate tokens at contiguous positions.

The main objective of SD is to speed up text generation while guaranteeing that the decoded tokens follow the target model distribution. SD relies on the insight that the combined cost of autoregressive decoding with a small draft model followed by parallel verification with the target model is[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-02.jpg?height=339&width=1228&top_left_y=253&top_left_x=443)

Figure 1: Performance comparison of standard speculative decoding (SD) vs. our proposed DistillSpec, with small- and XL-sized models from the T5 v1.1 family (Raffel et al. 2020) being utilized as the draft and the target models, respectively. DistillSpec enhances SD speed by better aligning the draft with the target via white-box knowledge distillation, resulting in a consistent $10-45 \%$ speedup improvement over standard SD across various datasets. The distilled draft model from GSM8K transfers well to 23 unseen BigBenchHard tasks (Suzgun et al. 2022), resulting in an average speedup of $26 \%$. See $\$ 5.1$ for additional details.

lower than the cost of autoregressive decoding with the target model alone. However, the realized inference cost reduction or latency improvement crucially depends on the acceptance rate of the draft-generated tokens by the target model, which can be shown to be directly tied to the alignment between the token distributions of the draft and target models. Thus, a successful application of SD hinges on identifying a compact draft model that simultaneously has small autoregressive decoding cost and is closely aligned with the target model.

In this work, we propose DistillSpec, a novel approach that relies on KD (Hinton et al. 2015) to obtain an effective draft model. Unlike the standard application of KD which primarily focuses on improving the task performance of a small student model, DistillSpec aims at aligning the student (draft) model with the teacher (target) model to enhance the acceptance rate during SD. We undertake a comprehensive exploration of the distillation process for speeding up SD, considering several factors including the composition of training data, choice of divergence functions to define the training objective for KD, and decoding strategies. Notably, our findings underscore that using model-generated data is crucial for ensuring strong student-teacher alignment across various tasks via $\mathrm{KD}$, and that the selection of the best-performing divergence function in DistillSpec is highly task-dependent and sensitive to the decoding strategy (i.e., greedy versus non-greedy). Furthermore, we explore the utility of DistillSpec for lossy SD (Leviathan et al., 2023) which allows for sampling away from the target model distribution. We show that combining DistillSpec with lossy SD enables a more fine-grained control over the latency versus task performance trade-off.

Finally, we carry out a systematic study of how to design an efficient inference scheme in a practical setting where one has access to multiple models of increasing size and quality. Leveraging the insights that we have laid out about KD and SD, our study concludes that the most effective strategy involves first distilling a large model into a smaller one as the potential target model for performance optimization, followed by DistillSpec for distilling an even smaller model to be used as the draft model in SD. This approach results in a remarkable $6-10 \times$ reduction in latency, compared to a standalone non-distilled target model of the same size, with minimal performance degradation.

Our key contributions are:

(i) We propose DistillSpec, a method that uses KD to enhance draft model alignment with the target model ( $\$ 4$, and show that our method can improve SD speed by $10-45 \%$ while preserving model performance across diverse datasets under greedy and non-greedy sampling (Figure 11 .

(ii) We conduct an extensive analysis of the optimal distillation recipe ( $\$ 5.2$ for model alignment, encompassing factors such as training data generation and different divergences, and emphasizing the distinctions between standard KD and distillation tailored for SD.

(iii) We extend DistillSpec to lossy SD, enabling refined control over the quality-latency trade-off. Moreover, we offer insights for combining KD and SD when several models are available ( $\$ 5.3$ ).

## 2 RELATED WORK

Speculative decoding (SD). Due to the inherent sequential nature of autoregressive decoding, the primary latency bottleneck in LLM inference arises from memory read/write operations rather than arithmetic computations (Pope et al., 2023). Speculative decoding (Leviathan et al. 2023,

Chen et al. 2023) (SD) addresses this challenge by utilizing a compact draft model to generate a batch of tokens sequentially, while validating them in parallel with a larger target model. Prior to SD, various parallel computing paradigms have been explored for autoregressive models, including block parallel sampling (Stern et al. 2018), shallow aggressive decoding (Sun et al., 2021), and aggressive decoding (Ge et al. 2022). However, these approaches are not readily adaptable to typical language models due to potential deviations from target model's distribution, strict input constraints, or limited support for general stochastic sampling. Notably, recent variants of SD have considered different interactions between the draft and target model to reduce unnecessary computation (Kim et al. 2023) and incorporated parallel computation along the batch axis, sometimes combined with token tree verification, as seen in SpecTr (Sun et al., 2023), SpecInfer (Miao et al. 2023), and Medusa (Cai et al. 2023a). In contrast, our work focuses on enhancing SD by improving the alignment between the small draft model and the large target model through KD, which does not require any changes to serving infrastructures already implementing SD and is complementary to the recent variants of SD. Furthermore, we conduct a systematic study of lossy SD for providing nuanced control over the trade-off between quality and latency for specific serving models.

Knowledge distillation (KD) for LLMs. KD (Bucilu«é et al. 2006, Hinton et al., 2015), which trains high-quality smaller student models with the supervision of larger teacher models, has emerged as a vital technique for reducing inference cost while maintaining model quality across a range of domains. In the context of LLMs, prior uses of KD (Taori et al., 2023, Fu et al., 2023) have mostly focused on black-box KD, wherein only the teacher's output generations, generally via APIs, are accessible during student training. However, with the proliferation of open-source LLMs (Zhang et al. 2022; Touvron et al., 2023), which enable access to teacher weights and logits, there is a growing interest in white-box KD. White-box KD allows student models to benefit from richer supervision signals provided by white-box teacher models, leading to enhanced language abilities (Agarwal et al., 2023; Gu et al., 2023; Wen et al., 2023).

Unlike prior works focused on creating highly capable standalone student models, we harness KD to foster closer collaboration between smaller and larger models in SD, which may be particularly valuable when a small distilled model alone cannot meet stringent quality requirements. While Stern et al. (2018) use a black-box KD approach (SeqKD) to enhance blockwise parallel decoding, their samples are generated from the large target model, which is prohibitively expensive for LLMs. Furthermore, they ignore the teacher model's logits and train their draft model using only one-hot teacher labels-a reasonable choice for greedy decoding but a less effective one for non-greedy sampling (Figure 2. Concurrently, Liu et al. (2023) propose to improve SD using KD, but they assume an online setup with a changing query distribution, and focus on improving the acceptance rate rather than reducing the actual latency.

## 3 BACKGROUND: SPECULATIVE DECODING

Notation. Given an input sequence $x$ comprising tokens from a pre-defined vocabulary, a language model $\mathcal{M}$ provides a distribution over possible output sequences $y$. Suppose we employ SD with a compact draft model $\mathcal{M}_{q}$ to assist a larger target model $\mathcal{M}_{p}$. Let $p\left(y_{t} \mid x, y_{<t}\right)$ and $q\left(y_{t} \mid x, y_{<t}\right)$ represent the distributions governing next-token predictions at time step $t$ for $\mathcal{M}_{p}$ and $\mathcal{M}_{q}$, respectively, given the context $\rho=\left\{x, y_{<t}\right\}$. Given input $x$ as prefix, let $p_{<T}(y \mid x)$ and $q_{\leq T}(y \mid x)$ represent the distributions governing the sequence $y$ sampled autoregressively from $\mathcal{M}_{p}$ and $\mathcal{M}_{q}$, respectively, where the generation stops either when an end-of-sequence token is sampled or the maximum sequence length $T$ is reached. For simplicity, we use $p\left(y_{t}\right)$ and $q\left(y_{t}\right)$ as shorthands for $p\left(y_{t} \mid x, y_{<t}\right)$ and $q\left(y_{t} \mid x, y_{<t}\right)$, whenever the context $\rho$ is clear. Similarly, $p_{\leq T}(y)$ and $q_{\leq T}(y)$ serve as shorthands for $p_{\leq T}(y \mid x)$ and $q_{\leq T}(y \mid x)$, whenever the input $x$ is clear.

Speculative sampling. Standard SD uses a procedure called speculative sampling to generate tokens from the draft model while maintaining the same output distribution as the target model. As detailed in Algorithm A.1 (Appendix), each step of SD works as follows. First, a block of $\gamma$ tokens, denoted as $y_{t}, \ldots, y_{t+\gamma-1}$, is autoregressively sampled from $q\left(y_{t}\right), \ldots, q\left(y_{t+\gamma-1}\right)$. Next, the $\gamma$ tokens are verified in parallel by passing them to $\mathcal{M}_{p}$ as a whole block, which sequentially accepts token $y_{t+i}$ with probability $\min \left(1, p\left(y_{t+i}\right) / q\left(y_{t+i}\right)\right)$. If any token $y_{t+i}$ is rejected before the end of the block, the subsequent tokens are discarded and the rejected token is resampled from the adjusted distribution $p^{\prime}\left(y_{t+i}\right) \propto \max \left(0, p\left(y_{t+i}\right)-q\left(y_{t+i}\right)\right)$; otherwise, the drafted tokens are all accepted and an extra token is sampled from $p\left(y_{t+\gamma}\right)$ and appended to the output sequence. This process guarantees that the sequence of accepted and resampled tokens follow the same output distribution
as $p\left(y_{t+i}\right)$ (Leviathan et al. 2023). The procedure is repeated until an end-of-sequence token is accepted, or the maximum sequence length $T$ has been reached.

Efficiency measure: acceptance rate. Each SD step takes a constant amount of time, so the wall-clock time scales linearly with the number of steps. This number is equal to the total number of times that the target model rejects a token, plus the number of blocks accepted as a whole, where the latter term is small for large $\gamma$. This motivates us to use the acceptance rate as a surrogate efficiency measure for the wall-clock time. For an ideal SD process with $\gamma=\infty$, we define the sequence-level acceptance rate $\alpha(x)$ for a given input $x$ as follows:

$$
\begin{equation*}
\alpha(x):=\frac{\mathbb{E}[\text { number of accepted tokens in generating } y]}{\mathbb{E}[\text { number of tokens in } y]}=\frac{\mathbb{E}_{y \sim p_{\leq T}(y \mid x)}\left[\sum_{t=1}^{|y|} \beta\left(x, y_{<t}\right)\right]}{L_{p}(x)} \tag{1}
\end{equation*}
$$

where $\beta\left(x, y_{<t}\right):=\mathbb{E}_{y_{t} \sim q\left(y_{t}\right)}\left[\min \left(1, p\left(y_{t}\right) / q\left(y_{t}\right)\right)\right]$ is the token-level acceptance rate, and expectations are taken over the randomness in SD. Since speculative sampling preserves the output distribution of the target model, the denominator is simply equal to the expected length of the target output $L_{p}(x):=\mathbb{E}_{y \sim p_{\leq T}(y \mid x)}[|y|]$, which is invariant to the choice of draft model. Therefore, the acceptance rate is directly related to the expected total number of rejected tokens $(1-\alpha(x)) \cdot L_{p}(x)$, which lower bounds the expected number of SD steps.

Efficiency measure: block efficiency. In practice, SD is usually employed with a fixed finite block size $\gamma$; thus, a more relevant efficiency measure is the block efficiency $\tau$. Given an input $x$, we compute the block efficiency $\tau(x)$ as the expected number of accepted tokens per block. Note that, for a given block size $\gamma$, the maximum value of $\tau(x)$ is $\gamma+1$, corresponding to the case where all drafted tokens are accepted and augmented with an additional token sampled by the target model. If we assume that the token-level rates $\beta\left(x, y_{<t}\right)$ are i.i.d., then the sequence-level acceptance rate satisfies $\alpha=\mathbb{E}[\beta]$ and $\tau(x)=\left(1-\alpha^{\gamma+1}\right) /(1-\alpha)$ (Leviathan et al. 2023).

Wall-clock time improvement. For given block efficiency $\tau(x)$, the expected speedup of SD is given by $\tau(x) /(c \gamma+1)$, where the relative latency $c$ is the ratio between the times elapsed when making a single forward pass through the draft model $\mathcal{M}_{q}$ and through the target model $\mathcal{M}_{p}$.

## 4 DistiLLSPEC: KNOWLEDGE DISTILLATION FOR SPECULATIVE DECODING

As described in $\S 3$, speculative decoding (SD) leverages a small (draft) model to reduce the latency of decoding from the larger (target) model distribution without any performance drop. However, the realized speedup critically depends on how "well-aligned" the draft model is to the target model. In this work, we propose DistillSpec, a general framework that improves SD by better aligning the target model and draft model using white-box knowledge distillation (KD). We first present KD-based training of the draft model, and highlight how our objective of enhancing SD via KD influences our selection of training data generation method and divergence function-two key ingredients of DistillSpec. We then discuss how DistillSpec can be extended to lossy SD.

Let the draft model $\mathcal{N}_{q}^{\theta}$ be parameterized by $\theta$. DistillSpec utilizes predictions from the target model $\mathcal{M}_{p}$ as a source of supervision in training the draft model $\mathcal{M}_{q}^{\theta}$. We assume white-box access to both models, i.e., we can obtain their next-token distributions $p\left(y_{t}\right)$ and $q\left(y_{t}\right)$, and therefore we are able to generate samples from both models. Given a divergence function $D$ that measures the misalignment between two distributions, KD-based training of the draft model seeks to minimize the divergence between the teacher (target) and student (draft) distributions over a training set $\mathcal{G}$ :

$$
\begin{equation*}
\theta^{*}:=\arg \min \mathbb{E}_{(x, y) \sim \mathcal{G}}\left[D\left(\mathcal{M}_{p} \| \mathcal{M}_{q}^{\theta}\right)(y \mid x)\right] \tag{2}
\end{equation*}
$$

where $D\left(\mathcal{M}_{p} \| \mathcal{M}_{q}^{\theta}\right)(y \mid x)=\frac{1}{|y|} \sum_{t=1}^{|y|} D\left(p\left(\cdot \mid x, y_{<t}\right) \| q^{\theta}\left(\cdot \mid x, y_{<t}\right)\right)$. We note that flexibility in how $\mathcal{G}$ is constructed and the choice of divergence $D$ gives rise to different possible KD algorithms. For instance, $\mathcal{G}$ may consist of task-specific input-output pairs $(X, Y)$ or sequences generated from $\mathcal{M}_{p}$ or $\mathcal{M}_{q}^{\theta}$. While forward KL ( $D_{\mathrm{FKL}}$ ) is the commonly used divergence for KD (Hinton et al. 2015), recent works Agarwal et al. 2023, Wei et al. 2022) have shown the effectiveness of alternative divergences, including reverse $\mathrm{KL}\left(D_{\mathrm{RKL}}\right.$ ), Jensen-Shannon divergence ( $D_{\mathrm{JSD}[\beta]}$ ), and total variation distance ( $D_{\text {TVD }}$ ). Further details on each divergence can be found in Appendix A. 1 Table 1 summarizes various distillation recipes, each being a specialized instance of Algorithm A. 2

Table 1: Summary of various KD algorithms in terms of training data $\mathcal{G}$ and divergence $D$ (cf. Eq. 27. Wen et al. (2023); Agarwal et al. (2023) also consider other $D$; we list the most representative one.

| ame | Divergence | Training Data $(\mathcal{G})$ |
| :---: | :---: | :---: |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-05.jpg?height=70&width=558&top_left_y=423&top_left_x=447) | FKL <br> FKL | er $\mathcal{M}_{p}$ <br> utput pairs |
| nitKD (Lin et al.. 2020 | FK | + data generated by $\}$ |
|  |  | Data $g$ |
| On-policy GKD $\sqrt{\text { Aggarwal et al., } 20}$ | FKL / JSD | On-policy data from student $\mathcal{M}_{q}^{\theta}$ |

Our choices for $\mathcal{G}$ and $D$ are guided by how the resulting distilled model, once employed as draft model, improves the speed of SD. Towards this, we first highlight the role that $D_{\text {TVD }}$ between $p\left(y_{t}\right)$ and $q\left(y_{t}\right)$ plays in dictating the acceptance rate $(\$ 3$-a key efficiency measure for SD.

TVD as proxy for the acceptance rate. Leviathan et al. (2023, Corollary 3.6) show that the token-level acceptance rate $\beta\left(x, y_{<t}\right)$ satisfies $\beta\left(x, y_{<t}\right)=1-D_{\mathrm{TVD}}\left(p\left(y_{t}\right), q\left(y_{t}\right)\right)$. Hence, Eq. 1 implies that maximizing the sequence-level acceptance rate $\alpha(x)$ is equivalent to minimizing the expected $D_{\text {TVD }}$ between $p\left(y_{t}\right)$ and $q\left(y_{t}\right)$ over the output sequence distribution of $\mathcal{M}_{p}$, i.e.:

$$
\begin{equation*}
\alpha(x)=1-\mathbb{E}_{y \sim p_{\leq T}(y \mid x)}\left[\sum_{t=1}^{|y|} D_{\mathrm{TVD}}\left(p\left(y_{t}\right), q\left(y_{t}\right)\right)\right] / L_{p}(x) \tag{3}
\end{equation*}
$$

Choice of divergence. Based on Eq. 3, it appears that directly minimizing $D_{\text {TVD }}$ may be a principled objective for draft model distillation. While optimizing $D_{\text {TVD }}(p, q)$ is theoretically inspired, our empirical study shows that such an objective may not consistently yield optimal results. We find that the choice of the most suitable divergence is highly task-dependent ( $\$ 5.2$.

Choice of training data. As for $\mathcal{G}$, one could resort to an existing ground-truth dataset, however the teacher's output distribution may deviate from the ground-truth distribution despite the teacher having been fine-tuned on it. Moreover, ground-truth datasets are often limited in size, so training only on such data could result in overfitting. To mitigate these issues, we use model-generated outputs for distillation. Specifically, we prompt the model with a task input sampled from a groundtruth training dataset, and use the model response as data for distillation. Both the teacher and student models may be used to generate the distillation examples.

Model-generated distillation data. Eq. 3 suggests optimizing the expected $D_{\text {TVD }}$ over outputs generated from the teacher. Decoding from a large teacher is generally prohibitively expensive, especially at the scale of dataset required for KD. To reduce the generation cost, we explore using on-policy data during distillation, i.e., output sequences sampled from the student itself. Besides being more computationally efficient compared to teacher generations, this approach is motivated by Gu et al. (2023); Agarwal et al. (2023), who have shown that distilling on on-policy data improves student task performance. However, different from these prior works, our primary focus is on improving the student-teacher alignment. Thus, it may not be immediately clear whether minimizing the expected $D_{\text {TVD }}$ over on-policy (student-generated) data ensures an improved acceptance rate, which is computed as an expectation over the teacher's output distribution (cf. Eq. 3). Our following result shows that this is indeed the case.

Theorem 4.1. For SD, if the draft model $\mathcal{M}_{q}^{\theta}$ achieves on-policy $K D$ loss $\epsilon=$ $\mathbb{E}_{x \sim X, y \sim q_{\leq T}(y \mid x)}\left[D_{\mathrm{TVD}}\left(\mathcal{M}_{p} \| \mathcal{M}_{q}^{\theta}\right)(y \mid x)\right]$, then the sequence-level acceptance rate is at least

$$
\begin{equation*}
\mathbb{E}_{x \sim X}[\alpha(x)] \geq 1-T \cdot \mathbb{E}_{x \sim X}\left[\frac{T}{L_{p}(x)}\right] \epsilon \tag{4}
\end{equation*}
$$

When the target output length is always $T$, the bound simplifies to $\mathbb{E}_{x \sim X}[\alpha(x)] \geq 1-T \epsilon$.

We defer the proof to Appendix A.2. Intuitively, it builds upon the following insights. If the onpolicy $\mathrm{KD}$ loss is small, then, for any $1 \leq t \leq T$, the same loss evaluated only at the $t$-th token should also be small. Since the first token generation is independent of any other tokens, a small value of on-policy KD loss ensures that the first token distributions of the draft and target models are close. Then, an inductive argument shows that once the draft and target are similar on the first $t$ tokens, the distributions of the $(t+1)$-th token should also be close. Our proof makes this argument rigorous by utilizing variational representations of $D_{\mathrm{TVD}}$, leading to a linear error bound in $T$.

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-06.jpg?height=347&width=1374&top_left_y=225&top_left_x=365)

Figure 2: Distillation enhances block efficiency $(\tau)$ across diverse datasets, highlighting the superiority of model-generated data over fixed ground truth data (SupervisedKD) and emphasizing the importance of whitebox distillation in addressing SeqKD's subpar performance.

DistillSpec with lossy SD. DistillSpec enhances SD efficiency without any quality loss compared to the target model $\mathcal{M}_{p}$. In practical applications, a reduction in model quality may be justified in order to support even faster inference. For such scenarios, we extend DistillSpec to lossy SD (Leviathan et al. 2023), which uses a lenience function $f(p, \epsilon)$ that modifies the acceptance probability from $\min \left(1, p\left(y_{t}\right) / q\left(y_{t}\right)\right)$ to $\min \left(1, f\left(p\left(y_{t}\right), \epsilon\right) / q\left(y_{t}\right)\right)$ (cf. $\S 3$. Here $f:[0,1]^{2} \rightarrow \mathbb{R}^{+}$ is increasing and decreasing in its first and second arguments, respectively, and $\epsilon \in[0,1]$ is a free parameter (cf. Algorithm A.1). In this work, we evaluate multiple choices for the lenience functions: $f_{\text {lin }}(p, \epsilon)=p / \epsilon, f_{\mathrm{sq}}(p, \epsilon)=p / \epsilon^{2}$, and $f_{\exp }(p, \epsilon)=p^{\epsilon}$. For example, when the lenience function is $f_{\mathrm{sq}}(p, \epsilon)$ and $\epsilon=0.1$, token $x$ sampled from $q\left(y_{t}\right)$ becomes hundred times more likely to be accepted by the target, thus enabling faster inference at the expense of a potential drop in generation quality. Lenience was discussed by Leviathan et al. (2023) in the context of $f_{\text {lin }}$ and their treatment focuses solely on latency improvements, whereas we explore the use of different lenience functions as a precise control mechanism to achieve the desired quality-latency profile.

## 5 EXPERIMENTS

### 5.1 ENHANCING SPECULATIVE DECODING THROUGH DISTILLATION

We evaluate the effectiveness of KD in improving the speed of speculative decoding (SD). We follow Leviathan et al. (2023) and investigate its impact on the acceptance rate $\alpha$, block efficiency $\tau$, and actual latency speedup with a batch size of 1 under greedy sampling $(T=0)$ and standard temperature sampling $(T=1)$.

Tasks and models. Following Leviathan et al. (2023), we evaluate two model types: 1) GPTlike decoder-only Transformer models trained on LM1B task (Chelba et al. 2013) using a standard autoregressive objective, where the target and draft models have $234 \mathrm{M}$ and $33 \mathrm{M}$ parameters, respectively; and 2) standard encoder-decoder T5 v1.1 models (Raffel et al. 2020) fine-tuned on four different tasks, with T5-XL (3B) and T5-Small (77M) serving as the target and draft models, respectively. We utilize two datasets from the T5 paper, namely WMT EnDe Bojar et al., 2014) and CNN/DM (Hermann et al., 2015), which deal with translation and text summarization, respectively. The remaining two tasks used to test T5 models are XSum (Narayan et al. 2018) and GSM8K (Cobbe et al. 2021), which deal with abstractive summarization and arithmetic reasoning, respectively. See Appendix B for more details.

Decoding speedup. Figure 1 shows that the impact of distillation on SD speed is evident, consistently yielding a 10-46\% improvement across various datasets. This effect is most pronounced when employing greedy decoding. The performance of KD for different block sizes and decoding strategies across five datasets is presented in Table C.1 (Appendix). These findings demonstrate that KD significantly enhances the acceptance rate and block efficiency of SD for both decoder-only and encoder-decoder models across all datasets. Distillation algorithms utilizing model-generated data consistently outperform other approaches, resulting in $\sim 20 \%$ additional speedup compared to standard SD on LM1B, XSum, CNN/DM, and GSM8K.

Block efficiency. Figure 2 presents a comparison of block efficiency across different algorithms, employing temperature sampling $(T=1)$ with a block size $\gamma=7$. The figure underscores the utility of model-generated data: using ground-truth data (i.e., Supervised KD) ranks lowest across all settings. In contrast, $f$-Distill and GKD, which only use model-generated data, significantly outperform other KD variants. The subpar performance of SeqKD, despite being purely trained on data

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-07.jpg?height=453&width=1396&top_left_y=218&top_left_x=359)

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-07.jpg?height=356&width=445&top_left_y=250&top_left_x=379)

(a) acceptance rate vs. wall-time

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-07.jpg?height=368&width=450&top_left_y=236&top_left_x=832)

(b) instance-level block eff. $(\tau)$

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-07.jpg?height=366&width=444&top_left_y=240&top_left_x=1293)

(c) agreement of $\tau$

Figure 3: (a) White-box KD using teacher logits and model-generated data is crucial. Draft model's on-policy data can be as effective as target model data. GKD achieves the best wall-time performance improvement on XSum. (b) Distillation improves the block efficiency for all examples on GSM8K. (c) Empirical block efficiency aligns well with its $D_{\text {TVD-based theoretical counterpart. }}^{\text {. }}$.

generated by the target model, suggests that white-box distillation (i.e., supervision from the target model's logits) is vital for SD. This is corroborated by Figure 3a, which illustrates the evolution of the acceptance rate throughout training. Supervised KD ranks lowest, and its performance plateaus early during training due to the static nature of the dataset. In contrast, algorithms using modelgenerated data lead to continual improvement of the acceptance rate. Despite $f$-Distill being much more computationally costly than GKD due to the use of teacher-generated data, both algorithms exhibit comparable performance. Notably, GKD achieves the best wall-time performance improvement. See Appendix C.1.2 for more visualizations of performance improvement during training.

We also investigate whether KD improves block efficiency universally or impacts a limited subset of examples. Figure $3 \mathrm{~b}$ depicts the improvement per example. We observe consistent gains in block efficiency across most examples, which can also be seen in various datasets (see Figure C.16. Figure $3 \mathrm{c}$ illustrates a strong agreement between theoretical and empirical block efficiency values for several distilled models (each model is represented as a filled circle). Despite theoretical values occasionally overestimating or underestimating empirical values, possibly due to potential deviations from the i.i.d. token-level assumption (cf. $\$ 3$, the ranking of distilled models remains highly consistent. In summary, these findings largely confirm that KD effectively optimizes block efficiency.

Transferability of distilled models. We next examine the transferability of distilled models on diverse datasets unseen during training. We use a draft model distilled on GSM8K and test its ability to speed up SD on zero-shot chain-of-thought (CoT) prompting over 23 reasoning tasks from the BigBenchHard suite (Suzgun et al., 2022). The results, illustrated in Figure 1 , indicate effective transferability to other datasets. Compared to standard SD, the distilled model significantly enhances average decoding speeds, yielding speedup improvements from $1.93 \times$ and $1.78 \times$ to $2.21 \times$ and $2.02 \times$ for greedy and non-greedy decoding methods, respectively. Further analysis in Figure C. 1 reveals that using our distilled T5-Small as draft model is also compatible with larger target models (T5-XXL) despite being distilled from a different-sized model (T5-XL). Despite being not fully optimized, this configuration consistently outperforms standard SD by $7 \%-37 \%$ across various datasets. See Appendix C. 1 for more details.

### 5.2 DISTILLSPEC RECIPE

We now focus on identifying the optimal KD approach for SD. Following the training and evaluation protocols in $\S 5.1$, we explore four training data construction methods and four divergence functions on XSum and GSM8K. Specifically, we explore the following variants of training data: 1) fixed ground-truth dataset $\mathcal{D}_{\text {Train }}, 2$ ) data generated only from the draft model $\mathcal{M}_{q}^{\theta}, 3$ ) data generated only from target $\mathcal{M}_{p}, 4$ ) data generated from both $\mathcal{M}_{q}^{\theta}$ and $\mathcal{M}_{p}$ in equal proportion. We also examine the following divergences: 1) forward KL (FKL), 2) Jenson-Shannon divergence (JSD), 3) reverse KL (RKL), and 4) total variation distance (TVD).

Importance of training data and divergence in DistillSpec. Figure 4 illustrates the block efficiency improvement on XSum and GSM8K, in line with observations from $\S 5.1$. We note that using model-generated data (last three rows) yields superior performance than using a fixed dataset (first row). Specifically, on XSum with greedy decoding, using data generated from both $\mathcal{M}_{q}^{\theta}$ and $\mathcal{M}_{p}$ leads to the best performance, with JSD slightly outperforming the other divergences. However,

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-08.jpg?height=287&width=423&top_left_y=255&top_left_x=382)

(a) XSum: $\tau$ (greedy)

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-08.jpg?height=284&width=447&top_left_y=259&top_left_x=839)

(b) GSM8K: $\tau$ (greedy)

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-08.jpg?height=284&width=442&top_left_y=259&top_left_x=1297)

(c) GSM8K: $\tau$ (non-greedy)

Figure 4: DistillSpec recipe. We report improvement in empirical block efficiency post-distillation on (a) XSum with greedy sampling, and GSM8K with (b) greedy and (c) non-greedy sampling. Which divergence function and data construction lead to the largest block efficiency improvements highly depends on the task. They should be treated as a hyperparameters to be tuned on our task of interest.

on GSM8K with greedy decoding, FKL with only draft-generated data emerges as the best setup. In contrast, with temperature sampling (at $T=1$ ), a different trend is observed as RKL combined with data generated by $\mathcal{M}_{p}$ is the most effective setup. See Appendix C.2.1 for results on different datasets and decoding strategies. Nonetheless, using only draft-generated data is found to be competitive.

Impact of distillation on draft quality vs. compatibility. We also study how different distillation recipes affect task performance and whether there is any one design choice that is simultaneously optimal for improving both draft model task performance and its utility for SD (cf. Figure C. 23 . C.24). Similar to our earlier observations, the use of generated data is paramount for improving draft performance. Notably, utilizing data generated from $\mathcal{M}_{q}^{\theta}$ yields comparable or superior results compared to using data generated from $\mathcal{M}_{p}$. However, which KD algorithm is optimal largely depends on the task at hand and the underlying decoding strategy. Figure 5ahighlights an interesting dichotomy between block efficiency improvements and task performance gains via KD: distilled models with high task performance do not necessarily translate into more aligned drafters for SD.

Recommendation. Interestingly, although TVD is the objective we should aim to optimize for SD (cf. Eq. 3), its direct optimization does not yield the best performance in most of the settings explored. We generally find that the choice of divergence in KD is a hyperparameter that needs to be tuned based on the task at hand and decoding strategy used. For training data construction, we propose using the draft model $\mathcal{M}_{q}^{\theta}$ for data generation as it can achieve similar or superior performance compared to the target model $\mathcal{M}_{p}$, but at a much lower cost.

### 5.3 QUALITY VERSUS LATENCY TRADE-OFF

Lossy speculative decoding. We analyze the quality-latency trade-off using lossy SD variants, as detailed in Algorithm A.1. As Figure 5b illustrates, employing either $\mathrm{KD}(\star)$ or $\mathrm{SD}(\times)$ alone does not fully bridge the performance or latency gaps, respectively. In such case, a leniency parameter ( $\varepsilon$ ) can help interpolate between these two approaches, as demonstrated in Figure $5 \mathrm{~b}$ where each point within a given group corresponds to a different value of $\varepsilon$. As the GSM8K experiment shows, the power of interpolation can be limited: even using a permissive lenience of $\varepsilon=10^{-5}, f_{\text {lin }}$ still results in high performance but high latency, while $f_{\mathrm{sq}}$ traces a similar but slightly extended tradeoff curve. Although $f_{\text {exp }}$ makes interpolation possible, it yields a worse quality-latency trade-off. Interestingly, it is possible to significantly reduce latency while preserving most of the quality on GSM8K, possibly because many tokens are inconsequential for final performance, and a variety of proposals can be safely accepted with minimal effect on generation quality. See Appendix C.3.1 for a comparison between non-distilled and distilled draft models, where we show that a distilled draft model enables a much better quality vs. latency trade-off.

DistillSpec meets model garden. In many practical scenarios, we have access to multiple models of different sizes-a model garden-to design the inference pipeline. We emulate this setting by focusing on the five model sizes in the T5 model family: T5-Small (77M), T5-Base (250M), T5-Large (800M), T5-XL (3B), and T5-XXL (11B). We study the quality-latency trade-off curves obtained from applying KD and SD as follows: 1) raw: deploying supervised fine-tuned (SFT) T5 models; 2) distilled: applying KD by distilling smaller models from the larger T5 models; 3) speculative: applying SD using T5 models; and 4) DistillSpec: applying KD on T5 models and using SD with distilled models as target and draft models.

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-09.jpg?height=410&width=653&top_left_y=234&top_left_x=367)

(a) quality vs. SD compatibility (GSM8K)

![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-09.jpg?height=410&width=721&top_left_y=234&top_left_x=1033)

(b) DistillSpec with lossy SD (GSM8K)

Figure 5: (a) The improvement on speculative decoding and downstream task performance are only weakly correlated. A high quality distilled model does not imply it can be an effective draft model in speculative decoding. (b) We employ leniency as a precise control mechanism to achieve the desired quality-latency profile.
![](https://cdn.mathpix.com/cropped/2024_06_04_2fd5e0c86b104cb28298g-09.jpg?height=372&width=1350&top_left_y=882&top_left_x=385)

Figure 6: DistillSpec excels in both quality and latency, offering a remarkable 6.4x and 10.7x latency reduction on XSum (left) and GSM8K (right), while maintaining nearly identical performance.

Figure 6 shows that SD effectively shifts the trade-off curve leftward, especially with larger model sizes. However, its efficacy diminishes with smaller model sizes when the computation time between the draft and target models is closely matched. In contrast, distillation, which optimizes the model for downstream task performance, appears to offer a superior trade-off between quality and latency, particularly for smaller models. Conversely, a reverse trend is observed for larger model sizes when evaluating the model with temperature sampling. Figure C.30a indicates a substantial gap between the distilled model and the larger teacher model, while the SD-based method is able to significantly reduce latency. This suggests that when stringent performance and decoding strategy constraints are in place, SD remains a valuable approach. Our DistillSpec method, which combines the benefits of distillation and SD, consistently achieves the best trade-off between quality and latency, yielding an impressive reduction in latency while maintaining nearly identical performance. Specifically, DistillSpec reduces relative latency from 17.3 to 2.7 and from 15.0 to 1.4 on XSum and GSM8K, respectively, representing speedup improvements of $6.4 \times$ and $10.7 \times$. In contrast, the Rouge 2 score only marginally decreases, from 23.1 to 23.0 on XSum, while the model accuracy on GSM8K actually improves from 33.1 to 34.8 .

## 6 CONCLUSION

In this paper, we evaluate the efficacy of white-box knowledge distillation (KD) in enhancing speculative decoding (SD) through improved alignment between target and draft models. A thorough analysis is conducted to understand the impact of training data construction and divergence functions on KD performance. We underscore the significance of utilizing model-generated data and argue that employing the draft model's on-policy data during KD is a cost-efficient way of improving model alignment. Additionally, we assess the trade-off between quality and latency within the scope of lenience and availability of multiple models of varying quality and size (model garden), concluding that KD enables a superior trade-off compared to standard SD. The optimal strategy involves first applying KD for downstream task performance, followed by SD using a distilled draft model, resulting in a six to ten-fold decrease in latency with negligible performance loss. Our study contributes novel insights into white-box KD algorithms for LLMs and provides guidance for striking an effective balance between quality and latency using KD and SD.

## ACKNOWLEDGMENTS

We would like to extend a special thank you to Neha Gupta, Wittawat Jitkrittum, Nino Veillard, Yaniv Leviathan, Matan Kalman, Danny Vainstein, Natan Potikha, Ananda Theertha Suresh, Laz Karydas, Aishwarya PS, Pranav Nair, Praneeth Netrapalli, Nikunj Saunshi, Ziteng Sun, Keiran Paster, Olivier Bachem, Aleksandra Faust for insightful discussion and valuable feedback.

## REFERENCES

Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. arXiv preprint arXiv:2306.13649, 2023.

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale s Tamchyna. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pp. 1258, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W/W14/W14-3302

Cristian BuciluƒÉ, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535-541, 2006.

Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and Tri Dao. Medusa: Simple framework for accelerating $11 \mathrm{~m}$ generation with multiple decoding heads. https://github.com/ FasterDecoding/Medusa, 2023a.

Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023b.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.

Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726, 2023.

Tao Ge, Heming Xia, Xin Sun, Si-Qing Chen, and Furu Wei. Lossless acceleration for seq2seq generation with aggressive decoding. arXiv preprint arXiv:2205.10350, 2022.

Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in neural information processing systems, pp. 1693-1701, 2015.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

Ferenc Husz√°r. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101, 2015.

Sehoon Kim, Karttikeya Mangalam, Jitendra Malik, Michael W Mahoney, Amir Gholami, and Kurt Keutzer. Big little transformer decoder. arXiv preprint arXiv:2302.07863, 2023.

Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprint arXiv:1606.07947, 2016.

Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 19274-19286. PMLR, 2023.

Alexander Lin, Jeremy Wohlwend, Howard Chen, and Tao Lei. Autoregressive knowledge distillation through imitation learning. arXiv preprint arXiv:2009.07253, 2020.

Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding. arXiv preprint arXiv:2310.07177, 2023.

Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative $1 l \mathrm{~m}$ serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. ArXiv, abs/1808.08745, 2018.

OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api. semanticscholar.org/CorpusID:257532815.

Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.

H. A. Rowley, S. Baluja, and T. Kanade. Neural network-based face detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(1):23-38, 1998. doi: 10.1109/34.655647.

Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 4596-4604. PMLR, 2018.

Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018.

Xin Sun, Tao Ge, Furu Wei, and Houfeng Wang. Instantaneous grammatical error correction with shallow aggressive decoding. arXiv preprint arXiv:2106.04970, 2021.

Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu, Michael Riley, and Sanjiv Kumar. Spectr: Fast speculative decoding via optimal transport. In Workshop on Efficient Systems for Foundation Models @ ICML2023, 2023. URL https: //openreview.net/forum?id=d0mGsaheuT.

Mirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.

Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level knowledge distillation. arXiv preprint arXiv:2307.15190, 2023.

Zhixiang (Eddie) Xu, Matt J. Kusner, Kilian Q. Weinberger, Minmin Chen, and Olivier Chapelle. Classifier cascades and trees for minimizing feature evaluation cost. Journal of Machine Learning Research, 15(62):2113-2144, 2014.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.
