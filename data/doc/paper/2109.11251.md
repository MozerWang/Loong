# TRUST REGION POLICY OPTIMISATION IN MULTI-AGENT REINFORCEMENT LEARNING 

Jakub Grudzien Kuba ${ }^{1,2 *}$, Ruiqing Chen ${ }^{3, *}$, Muning Wen ${ }^{4}$, Ying Wen ${ }^{4}$,<br>Fanglei Sun ${ }^{3}$, Jun Wang ${ }^{5}$, Yaodong Yang ${ }^{6, \dagger}$<br>${ }^{1}$ University of Oxford, ${ }^{2}$ Huawei R\&D UK, ${ }^{3}$ ShanghaiTech University,<br>${ }^{4}$ Shanghai Jiao Tong University ${ }^{5}$ University College London<br>${ }^{6}$ Institute for AI, Peking University \& BIGAI<br>${ }^{\dagger}$ Corresponding to: yaodong.yang@pku.edu.cn


#### Abstract

Trust region methods rigorously enabled reinforcement learning (RL) agents to learn monotonically improving policies, leading to superior performance on a variety of tasks. Unfortunately, when it comes to multi-agent reinforcement learning (MARL), the property of monotonic improvement may not simply apply; this is because agents, even in cooperative games, could have conflicting directions of policy updates. As a result, achieving a guaranteed improvement on the joint policy where each agent acts individually remains an open challenge. In this paper, we extend the theory of trust region learning to cooperative MARL. Central to our findings are the multi-agent advantage decomposition lemma and the sequential policy update scheme. Based on these, we develop Heterogeneous-Agent Trust Region Policy Optimisation (HATPRO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO) algorithms. Unlike many existing MARL algorithms, HATRPO/HAPPO do not need agents to share parameters, nor do they need any restrictive assumptions on decomposibility of the joint value function. Most importantly, we justify in theory the monotonic improvement property of HATRPO/HAPPO. We evaluate the proposed methods on a series of Multi-Agent MuJoCo and StarCraftII tasks. Results show that HATRPO and HAPPO significantly outperform strong baselines such as IPPO, MAPPO and MADDPG on all tested tasks, thereby establishing a new state of the art.


## 1 INTRODUCTION

Policy gradient (PG) methods have played a major role in recent developments of reinforcement learning (RL) algorithms (Silver et al., 2014; Schulman et al., 2015a; Haarnoja et al., 2018). Among the many PG variants, trust region learning (Kakade \& Langford, 2002), with two typical embodiments of Trust Region Policy Optimisation (TRPO) (Schulman et al., 2015a) and Proximal Policy Optimisation (PPO) (Schulman et al., 2017) algorithms, offer supreme empirical performance in both discrete and continuous RL problems (Duan et al., 2016; Mahmood et al., 2018). The effectiveness of trust region methods largely stems from their theoretically-justified policy iteration procedure. By optimising the policy within a trustable neighbourhood of the current policy, thus avoiding making aggressive updates towards risky directions, trust region learning enjoys the guarantee of monotonic performance improvement at every iteration.

In multi-agent reinforcement learning (MARL) settings (Yang \& Wang, 2020), naively applying policy gradient methods by considering other agents as a part of the environment can lose its effectiveness. This is intuitively clear: once a learning agent updates its policy, so do its opponents; this however changes the loss landscape of the learning agent, thus harming the improvement effect from the PG update. As a result, applying independent PG updates in MARL offers poor convergence property (Claus \& Boutilier, 1998). To address this, a learning paradigm named centralised training with decentralised execution (CTDE) (Lowe et al., 2017b; Foerster et al., 2018; Zhou et al., 2021) was developed. In CTDE, each agent is equipped with a joint value function which, during[^0]training, has access to the global state and opponents' actions. With the help of the centralised value function that accounts for the non-stationarity caused by others, each agent adapts its policy parameters accordingly. As such, the CTDE paradigm allows a straightforward extension of single-agent PG theorems (Sutton et al., 2000; Silver et al., 2014) to multi-agent scenarios (Lowe et al., 2017b; Kuba et al., 2021; Mguni et al., 2021). Consequently, fruitful multi-agent policy gradient algorithms have been developed (Foerster et al., 2018; Peng et al., 2017; Zhang et al., 2020; Wen et al., 2018; 2020; Yang et al., 2018).

Unfortunately, existing CTDE methods offer no solution of how to perform trust region learning in MARL. Lack of such an extension impedes agents from learning monotonically improving policies in a stable manner. Recent attempts such as IPPO (de Witt et al., 2020a) and MAPPO (Yu et al., 2021) have been proposed to fill such a gap; however, these methods are designed for agents that are homogeneous (i.e., sharing the same action space and policy parameters), which largely limits their applicability and potentially harm the performance. As we show in Proposition 1 later, parameter sharing could suffer from an exponentially-worse suboptimal outcome. On the other hand, although IPPO/MAPPO can be practically applied in a non-parameter sharing way, it still lacks the essential theoretical property of trust region learning, which is the monotonic improvement guarantee.

In this paper, we propose the first theoretically-justified trust region learning framework in MARL. The key to our findings are the multi-agent advantage decomposition lemma and the sequential policy update scheme. With the advantage decomposition lemma, we introduce a multi-agent policy iteration procedure that enjoys the monotonic improvement guarantee. To implement such a procedure, we propose two practical algorithms: Heterogeneous-Agent Trust Region Policy Optimisation (HATRPO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO). HATRPO/HAPPO adopts the sequential update scheme, which saves the cost of maintaining a centralised critic for each agent in CTDE. Importantly, HATRPO/HAPPO does not require homogeneity of agents, nor any other restrictive assumptions on the decomposibility of the joint Q-function (Rashid et al., 2018). We evaluate HATRPO and HAPPO on benchmarks of StarCraftII and Multi-Agent MuJoCo against strong baselines such as MADDPG (Lowe et al., 2017a), IPPO (de Witt et al., 2020b) and MAPPO (Yu et al., 2021); results clearly demonstrate its state-of-the-art performance across all tested tasks.

## 2 PRELIMINARIES

In this section, we first introduce problem formulation and notations for MARL, and then briefly review trust region learning in RL and discuss the difficulty of extending it to MARL. We end by surveying existing MARL work that relates to trust region methods and show their limitations.

### 2.1 CoOperatiVe MARL PRoblem FormULATION and NOTATIONS

We consider a Markov game (Littman, 1994), which is defined by a tuple $\langle\mathcal{N}, \mathcal{S}, \mathcal{A}, P, r, \gamma\rangle$. Here, $\mathcal{N}=\{1, \ldots, n\}$ denotes the set of agents, $\mathcal{S}$ is the finite state space, $\mathcal{A}=\prod_{i=1}^{n} \mathcal{A}^{i}$ is the product of finite action spaces of all agents, known as the joint action space, $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow[0,1]$ is the transition probability function, $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward function, and $\gamma \in[0,1)$ is the discount factor. The agents interact with the environment according to the following protocol: at time step $t \in$ $\mathbb{N}$, the agents are at state $\mathrm{s}_{t} \in \mathcal{S}$; every agent $i$ takes an action $\mathrm{a}_{t}^{i} \in \mathcal{A}^{i}$, drawn from its policy $\pi^{i}\left(\cdot \mid \mathrm{s}_{t}\right)$, which together with other agents' actions gives a joint action $\mathbf{a}_{t}=\left(\mathrm{a}_{t}^{1}, \ldots, \mathrm{a}_{t}^{n}\right) \in \mathcal{A}$, drawn from the joint policy $\pi\left(\cdot \mid \mathrm{s}_{t}\right)=\prod_{i=1}^{n} \pi^{i}\left(\cdot{ }^{i} \mid \mathrm{s}_{t}\right)$; the agents receive a joint reward $\mathrm{r}_{t}=r\left(\mathrm{~s}_{t}, \mathbf{a}_{t}\right) \in \mathbb{R}$, and move to a state $\mathrm{s}_{t+1}$ with probability $P\left(\mathrm{~s}_{t+1} \mid \mathrm{s}_{t}, \mathbf{a}_{t}\right)$. The joint policy $\pi$, the transition probabililty function $P$, and the initial state distribution $\rho^{0}$, induce a marginal state distribution at time $t$, denoted by $\rho_{\pi}^{t}$. We define an (improper) marginal state distribution $\rho_{\pi} \triangleq \sum_{t=0}^{\infty} \gamma^{t} \rho_{\pi}^{t}$. The state value function and the state-action value function are defined: $V_{\pi}(s) \triangleq \mathbb{E}_{\mathbf{a}_{0: \infty} \sim \pi, \mathrm{s}_{1: \infty} \sim P}\left[\sum_{t=0}^{\infty} \gamma^{t} \mathrm{r}_{t} \mid \mathrm{s}_{0}=s\right]$ and $Q_{\boldsymbol{\pi}}(s, \boldsymbol{a}) \triangleq \mathbb{E}_{\mathrm{s}_{1: \infty} \sim P, \mathbf{a}_{1: \infty} \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^{t} \mathbf{r}_{t} \mid \mathrm{s}_{0}=s, \mathbf{a}_{0}=\boldsymbol{a}\right]$. The advantage function is written as $A_{\boldsymbol{\pi}}(s, \boldsymbol{a}) \triangleq Q_{\pi}(s, \boldsymbol{a})-V_{\boldsymbol{\pi}}(s)$. In this paper, we consider a fully-cooperative setting where all agents share the same reward function, aiming to maximise the expected total reward:

$$
J(\boldsymbol{\pi}) \triangleq \mathbb{E}_{\mathrm{s}_{0: \infty} \sim \rho_{\pi}^{0: \infty}, \mathbf{a}_{0: \infty} \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^{t} \mathrm{r}_{t}\right]
$$

Throughout this paper, we pay close attention to the contribution to performance from different subsets of agents. Before proceeding to our methods, we introduce following novel definitions.

Definition 1. Let $i_{1: m}$ denote an ordered subset $\left\{i_{1}, \ldots, i_{m}\right\}$ of $\mathcal{N}$, and let $-i_{1: m}$ refer to its complement. We write $i_{k}$ when we refer to the $k^{\text {th }}$ agent in the ordered subset. Correspondingly, the multi-agent state-action value function is defined as

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-03.jpg?height=80&width=859&top_left_y=418&top_left_x=622)

and for disjoint sets $j_{1: k}$ and $i_{1: m}$, the multi-agent advantage function is

$$
\begin{equation*}
A_{\pi}^{i_{1: m}}\left(s, \boldsymbol{a}^{j_{1: k}}, \boldsymbol{a}^{i_{1: m}}\right) \triangleq Q_{\pi}^{j_{1: k}, i_{1: m}}\left(s, \boldsymbol{a}^{j_{1: k}}, \boldsymbol{a}^{i_{1: m}}\right)-Q_{\pi}^{j_{1: k}}\left(s, \boldsymbol{a}^{j_{1: k}}\right) \tag{1}
\end{equation*}
$$

Hereafter, the joint policies $\pi=\left(\pi^{1}, \ldots, \pi^{n}\right)$ and $\overline{\boldsymbol{\pi}}=\left(\bar{\pi}^{1}, \ldots, \bar{\pi}^{n}\right)$ shall be thought of as the "current", and the "new" joint policy that agents update towards, respectively.

### 2.2 TRust ReGion AlgorithmS In REINFORCEMENT LEARNING

Trust region methods such as TRPO (Schulman et al., 2015a) were proposed in single-agent RL with an aim of achieving a monotonic improvement of $J(\pi)$ at each iteration. Formally, it can be described by the following theorem.

Theorem 1. (Schulman et al., 2015a, Theorem 1) Let $\pi$ be the current policy and $\bar{\pi}$ be the next candidate policy. We define $L_{\pi}(\bar{\pi})=J(\pi)+\mathbb{E}_{\mathrm{s} \sim \rho_{\pi}, \mathrm{a} \sim \bar{\pi}}\left[A_{\pi}(s, a)\right], \mathrm{D}_{K L}^{\max }(\pi, \bar{\pi})=$ $\max _{s} \mathrm{D}_{K L}(\pi(\cdot \mid s), \bar{\pi}(\cdot \mid s))$. Then the inequality of

$$
\begin{equation*}
J(\bar{\pi}) \geq L_{\pi}(\bar{\pi})-C \mathrm{D}_{K L}^{\max }(\pi, \bar{\pi}) \tag{2}
\end{equation*}
$$

holds, where $C=\frac{4 \gamma \max _{s, a}\left|A_{\pi}(s, a)\right|}{(1-\gamma)^{2}}$.

The above theorem states that as the distance between the current policy $\pi$ and a candidate policy $\bar{\pi}$ decreases, the surrogate $L_{\pi}(\bar{\pi})$, which involves only the current policy's state distribution, becomes an increasingly accurate estimate of the actual performance metric $J(\bar{\pi})$. Based on this theorem, an iterative trust region algorithm is derived; at iteration $k+1$, the agent updates its policy by

$$
\pi_{k+1}=\underset{\pi}{\arg \max }\left(L_{\pi_{k}}(\pi)-C \mathrm{D}_{\mathrm{KL}}^{\max }\left(\pi_{k}, \pi\right)\right)
$$

Such an update guarantees a monotonic improvement of the policy, i.e., $J\left(\pi_{k+1}\right) \geq J\left(\pi_{k}\right)$. To implement this procedure in practical settings with parameterised policies $\pi_{\theta}$, Schulman et al. (2015a) approximated the KL-penalty with a KL-constraint, which gave rise to the TRPO update of

$$
\begin{equation*}
\theta_{k+1}=\underset{\theta}{\arg \max } L_{\pi_{\theta_{k}}}\left(\pi_{\theta}\right), \quad \text { subject to } \mathbb{E}_{\mathrm{s} \sim \rho_{\pi_{\theta_{k}}}}\left[\mathrm{D}_{\mathrm{KL}}\left(\pi_{\theta_{k}}, \pi_{\theta}\right)\right] \leq \delta \tag{3}
\end{equation*}
$$

At each iteration $k+1$, TRPO constructs a KL-ball $\mathfrak{B}_{\delta}\left(\pi_{\theta_{k}}\right)$ around the policy $\pi_{\theta_{k}}$, and optimises $\pi_{\theta_{k+1}} \in \mathfrak{B}_{\delta}\left(\pi_{\theta_{k}}\right)$ to maximise $L_{\pi_{\theta_{k}}}\left(\pi_{\theta}\right)$. By Theorem 1 , we know that the surrogate objective $L_{\pi_{\theta_{k}}}\left(\pi_{\theta}\right)$ is close to the true reward $J\left(\pi_{\theta}\right)$ within $\mathfrak{B}_{\delta}\left(\pi_{\theta_{k}}\right)$; therefore, $\pi_{\theta_{k}}$ leads to improvement. Furthermore, to save the cost on $\mathbb{E}_{\mathrm{s} \sim \rho_{\pi_{\theta_{k}}}}\left[\mathrm{D}_{\mathrm{KL}}\left(\pi_{\theta_{k}}, \pi_{\theta}\right)\right]$ when computing Equation (3), Schulman et al. (2017) proposed an approximation solution to TRPO that uses only first order derivatives, known as PPO. PPO optimises the policy parameter $\theta_{k+1}$ by maximising the PPO-clip objective of

$$
\begin{equation*}
L_{\pi_{\theta_{k}}}^{\mathrm{PPO}}\left(\pi_{\theta}\right)=\mathbb{E}_{\mathrm{s} \sim \rho_{\pi_{\theta_{k}}}, \mathrm{a} \sim \pi_{\theta_{k}}}\left[\min \left(\frac{\pi_{\theta}(\mathrm{a} \mid \mathrm{s})}{\pi_{\theta_{k}}(\mathrm{a} \mid \mathrm{s})} A_{\pi_{\theta_{k}}}(\mathrm{~s}, \mathrm{a}), \operatorname{clip}\left(\frac{\pi_{\theta}(\mathrm{a} \mid \mathrm{s})}{\pi_{\theta_{k}}(\mathrm{a} \mid \mathrm{s})}, 1 \pm \epsilon\right) A_{\pi_{\theta_{k}}}(\mathrm{~s}, \mathrm{a})\right)\right] \tag{4}
\end{equation*}
$$

The clip operator replaces the ratio $\frac{\pi_{\theta}(\mathrm{a} \mid \mathrm{s})}{\pi_{\theta_{k}}(\mathrm{a} \mid \mathrm{s})}$ with $1+\epsilon$ or $1-\epsilon$, depending on whether or not the ratio is beyond the threshold interval. This effectively enables PPO to control the size of policy updates.

### 2.3 Limitations of EXISting TRust ReGion Methods in MARL

Extending trust region methods to MARL is highly non-trivial. One naive approach is to equip all agents with one shared set of parameters and use agents' aggregated trajectories to conduct policy optimisation at every iteration. This approach was adopted by MAPPO (Yu et al., 2021) in which the policy parameter $\theta$ is optimised by maximising the objective of

$$
\begin{equation*}
L_{\pi_{\theta_{k}}}^{\mathrm{MAPPO}}\left(\pi_{\theta}\right)=\sum_{i=1}^{n} \mathbb{E}_{\mathrm{s} \sim \rho_{\pi_{\theta_{k}}}, \mathbf{a} \sim \pi_{\theta_{k}}}\left[\min \left(\frac{\pi_{\theta}\left(\mathrm{a}^{i} \mid \mathrm{s}\right)}{\pi_{\theta_{k}}\left(\mathrm{a}^{i} \mid \mathrm{s}\right)} A_{\pi_{\theta_{k}}}(\mathrm{~s}, \mathbf{a}), \operatorname{clip}\left(\frac{\pi_{\theta}\left(\mathrm{a}^{i} \mid \mathrm{s}\right)}{\pi_{\theta_{k}}\left(\mathrm{a}^{i} \mid \mathrm{s}\right)}, 1 \pm \epsilon\right) A_{\pi_{\theta_{k}}}(\mathrm{~s}, \mathbf{a})\right)\right] \tag{5}
\end{equation*}
$$

Unfortunately, this simple approach has significant drawbacks. An obvious demerit is that parameter sharing requires that all agents have identical action spaces, i.e., $\mathcal{A}^{i}=\mathcal{A}^{j}, \forall i, j \in \mathcal{N}$, which limits the class of MARL problems to solve. Importantly, enforcing parameter sharing is equivalent to putting a constraint $\theta^{i}=\theta^{j}, \forall i, j \in \mathcal{N}$ on the joint policy space. In principle, this can lead to a suboptimal solution. To elaborate, we demonstrate through an example in the following proposition.

Proposition 1. Let's consider a fully-cooperative game with an even number of agents $n$, one state, and the joint action space $\{0,1\}^{n}$, where the reward is given by $r\left(\mathbf{0}^{n / 2}, \mathbf{1}^{n / 2}\right)=r\left(\mathbf{1}^{n / 2}, \mathbf{0}^{n / 2}\right)=1$, and $r\left(\boldsymbol{a}^{1: n}\right)=0$ for all other joint actions. Let $J^{*}$ be the optimal joint reward, and $J_{\text {share }}^{*}$ be the optimal joint reward under the shared policy constraint. Then

$$
\frac{J_{\text {share }}^{*}}{J^{*}}=\frac{2}{2^{n}}
$$

For proof see Appendix B. In the above example, we show that parameter sharing can lead to a suboptimal outcome that is exponentially worse with the increasing number of agents. We also provide an empirical verification of this proposition in Appendix F.

Apart from parameter sharing, a more general approach to extend trust region methods in MARL is to endow all agents with their own parameters, and at each iteration $k+1$, agents construct trust regions of $\left\{\mathfrak{B}_{\delta}\left(\pi_{\theta_{k}^{i}}^{i}\right)\right\}_{i \in \mathcal{N}}$, and optimise their objectives $\left\{L_{\pi_{\theta_{k}}}\left(\pi_{\theta_{i}}^{i} \pi_{\boldsymbol{\theta}_{k}^{-i}}^{-i}\right)\right\}_{i \in \mathcal{N}}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-04.jpg?height=307&width=447&top_left_y=1012&top_left_x=384)

Figure 1: Example of a twoplayer differentiable game with $r\left(a^{1}, a^{2}\right)=a^{1} a^{2}$. We initialise two Gaussian policies with $\mu^{1}=$ $-0.25, \mu^{2}=0.25$. The purple intervals represent the KL-ball of $\delta=0.5$. Individual trust region updates (red) decrease the joint return, whereas our sequential update (blue) leads to improvement.

Admittedly, this approach can still be supported by the current MAPPO implementation (Yu et al., 2021) if one turns off parameter sharing, thus distributing the summation in Equation (5) to all agents. However, such an approach cannot offer a rigorous guarantee of monotonic improvement during training. In fact, agents' local improvements in performance can jointly lead to a worse outcome. For example, in Figure 1, we design a single-state differential game where two agents draw their actions from Gaussian distributions with learnable means $\mu^{1}, \mu^{2}$ and unit variance, and the reward function is $r\left(a^{1}, a^{2}\right)=a^{1} a^{2}$. The failure of MAPPO-style approach comes from the fact that, although the reward function increases in each of the agents' (one-dimensional) update directions, it decreases in the joint (two-dimensional) update direction.

Having seen the limitations of existing trust region methods in MARL, in the following sections, we first introduce a multiagent policy iteration procedure that enjoys theoreticallyjustified monotonic improvement guarantee. To implement this procedure, we propose HATRPO and HAPPO algorithms, which offer practical solutions to apply trust region learning in MARL without the necessity of assuming homogeneous agents while still maintaining the monotonic improvement property.

## 3 MULTI-AGENT TRUST REGION LEARNING

The purpose of this section is to develop a theoretically-justified trust region learning procedure in the context of multi-agent learning. In Subsection 3.1, we present the policy iteration procedure with monotonic improvement guarantee, and in Subsection 3.2, we analyse its properties during training and at convergence. Throughout the work, we make the following regularity assumptions.

Assumption 1. There exists $\eta \in \mathbb{R}$, such that $0<\eta \ll 1$, and for every agent $i \in \mathcal{N}$, the policy space $\Pi^{i}$ is $\eta$-soft; that means that for every $\pi^{i} \in \Pi^{i}, s \in \mathcal{S}$, and $a^{i} \in \mathcal{A}^{i}$, we have $\pi^{i}\left(a^{i} \mid s\right) \geq \eta$.

### 3.1 TRust ReGion LEARNing In MARL with MoNOTONIC IMPROVEMENT GUARANTEE

We start by introducing a pivotal lemma which shows that the joint advantage function can be decomposed into a summation of each agent's local advantages. Importantly, this lemma offers a critical intuition behind the sequential policy-update scheme that our algorithms later apply.

Lemma 1 (Multi-Agent Advantage Decomposition). In any cooperative Markov games, given a joint policy $\pi$, for any state $s$, and any agent subset $i_{1: m}$, the below equations holds.

$$
A_{\boldsymbol{\pi}}^{i_{1: m}}\left(s, \boldsymbol{a}^{i_{1: m}}\right)=\sum_{j=1}^{m} A_{\pi}^{i_{j}}\left(s, \boldsymbol{a}^{i_{1: j-1}}, a^{i_{j}}\right)
$$

For proof see Appendix C.2. Notably, Lemma 1 holds in general for cooperative Markov games, with no need for any assumptions on the decomposibility of the joint value function such as those in VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018) or Q-DPP (Yang et al., 2020).

Lemma 1 indicates an effective approach to search for the direction of performance improvement (i.e., joint actions with positive advantage values) in multi-agent learning. Specifically, let agents take actions sequentially by following an arbitrary order $i_{1: n}$, assuming agent $i_{1}$ takes an action $\bar{a}^{i_{1}}$ such that $A^{i_{1}}\left(s, \bar{a}_{1}^{i_{1}}\right)>0$, and then, for the rest $m=2, \ldots, n$, the agent $i_{m}$ takes an action $\bar{a}^{i_{m}}$ such that $A^{i_{m}}\left(s, \overline{\boldsymbol{a}}^{i_{1: m-1}}, \bar{a}^{i_{m}}\right)>0$. For the induced joint action $\overline{\boldsymbol{a}}$, Lemma 1 assures that $A_{\pi_{\theta}}(s, \overline{\boldsymbol{a}})$ is positive, thus the performance is guaranteed to improve. To formally extend the above process into a policy iteration procedure with monotonic improvement guarantee, we need the following definitions.

Definition 2. Let $\pi$ be a joint policy, $\overline{\boldsymbol{\pi}}^{i_{1: m-1}}=\prod_{j=1}^{m-1} \bar{\pi}^{i_{j}}$ be some other joint policy of agents $i_{1: m-1}$, and $\hat{\pi}^{i_{m}}$ be some other policy of agent $i_{m}$. Then

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-05.jpg?height=72&width=1146&top_left_y=842&top_left_x=484)

Note that, for any $\overline{\boldsymbol{\pi}}^{i_{1: m-1}}$, we have

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-05.jpg?height=60&width=1128&top_left_y=981&top_left_x=412)

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-05.jpg?height=65&width=954&top_left_y=1038&top_left_x=713)

Building on Lemma 1 and Definition 2, we can finally generalise Theorem 1 of TRPO to MARL.

Lemma 2. Let $\pi$ be a joint policy. Then, for any joint policy $\bar{\pi}$, we have

$$
J(\overline{\boldsymbol{\pi}}) \geq J(\boldsymbol{\pi})+\sum_{m=1}^{n}\left[L_{\boldsymbol{\pi}}^{i_{1: m}}\left(\overline{\boldsymbol{\pi}}^{i_{1: m-1}}, \bar{\pi}^{i_{m}}\right)-C \mathrm{D}_{K L}^{\max }\left(\pi^{i_{m}}, \bar{\pi}^{i_{m}}\right)\right]
$$

For proof see Appendix C.2. This lemma provides an idea about how a joint policy can be improved. Namely, by Equation (6), we know that if any agents were to set the values of the above summands $L_{\pi}^{i_{1: m}}\left(\overline{\boldsymbol{\pi}}^{i_{1: m-1}}, \bar{\pi}^{i_{m}}\right)-C \mathrm{D}_{\mathrm{KL}}^{\max }\left(\pi^{i_{m}}, \bar{\pi}^{i_{m}}\right)$ by sequentially updating their policies, each of them can always make its summand be zero by making no policy update (i.e., $\bar{\pi}^{i_{m}}=\pi^{i_{m}}$ ). This implies that any positive update will lead to an increment in summation. Moreover, as there are $n$ agents making policy updates, the compound increment can be large, leading to a substantial improvement. Lastly, note that this property holds with no requirement on the specific order by which agents make their updates; this allows for flexible scheduling on the update order at each iteration. To summarise, we propose the following Algorithm 1. We want to highlight that the algorithm is

```
Algorithm 1 Multi-Agent Policy Iteration with Monotonic Improvement Guarantee
    Initialise the joint policy $\pi_{0}=\left(\pi_{0}^{1}, \ldots, \pi_{0}^{n}\right)$.
    for $k=0,1, \ldots$ do
        Compute the advantage function $A_{\boldsymbol{\pi}_{k}}(s, \boldsymbol{a})$ for all state-(joint)action pairs $(s, \boldsymbol{a})$.
        Compute $\epsilon=\max _{s, \boldsymbol{a}}\left|A_{\pi_{k}}(s, \boldsymbol{a})\right|$ and $C=\frac{4 \gamma \epsilon}{(1-\gamma)^{2}}$.
        Draw a permutaion $i_{1: n}$ of agents at random.
        for $m=1: n$ do
            Make an update $\pi_{k+1}^{i_{m}}=\arg \max _{\pi^{i} m}\left[L_{\pi_{k}}^{i_{1: m}}\left(\pi_{k+1}^{i_{1: m-1}}, \pi^{i_{m}}\right)-C \mathrm{D}_{\mathrm{KL}}^{\max }\left(\pi_{k}^{i_{m}}, \pi^{i_{m}}\right)\right]$.
        end for
    end for
```

markedly different from naively applying the TRPO update, i.e., Equation (3), on the joint policy of all agents. Firstly, our Algorithm 1 does not update the entire joint policy at once, but rather update each agent's individual policy sequentially. Secondly, during the sequential update, each agent has a unique optimisation objective that takes into account all previous agents' updates, which is also the key for the monotonic improvement property to hold.

### 3.2 THEORETICAL ANALYSIS

Now we justify by the following theorm that Algorithm 1 enjoys monotonic improvement property.

Theorem 2. A sequence $\left(\pi_{k}\right)_{k=0}^{\infty}$ of joint policies updated by Algorithm 1 has the monotonic improvement property, i.e., $J\left(\boldsymbol{\pi}_{k+1}\right) \geq J\left(\boldsymbol{\pi}_{k}\right)$ for all $k \in \mathbb{N}$.

For proof see Appendix C.2. With the above theorem, we finally claim a successful introduction of trust region learning to MARL, as this generalises the monotonic improvement property of TRPO. Moreover, we take a step further to study the convergence property of Algorithm 1. Before stating the result, we introduce the following solution concept.

Definition 3. In a fully-cooperative game, a joint policy $\pi_{*}=\left(\pi_{*}^{1}, \ldots, \pi_{*}^{n}\right)$ is a Nash equilibrium (NE) iffor every $i \in \mathcal{N}, \pi^{i} \in \Pi^{i}$ implies $J\left(\boldsymbol{\pi}_{*}\right) \geq J\left(\pi^{i}, \boldsymbol{\pi}_{*}^{-i}\right)$.

NE (Nash, 1951) is a well-established game-theoretic solution concept. Definition 3 characterises the equilibrium point at convergence for cooperative MARL tasks. Based on this, we have the following result that describes Algorithm 1's asymptotic convergence behaviour towards NE.

Theorem 3. Supposing in Algorithm 1 any permutation of agents has a fixed non-zero probability to begin the update, a sequence $\left(\boldsymbol{\pi}_{k}\right)_{k=0}^{\infty}$ of joint policies generated by the algorithm, in a cooperative Markov game, has a non-empty set of limit points, each of which is a Nash equilibrium.

For proof see Appendix C.3. In deriving this result, the novel details introduced by Algorithm 1 played an important role. The monotonic improvement property (Theorem 2), achieved through the multi-agent advantage and the sequential update scheme, provided us with a guarantee on the convergence of the return. Furthermore, randomisation of the update order assured that, at convergence, none of the agents is incentified to make an update. The proof is finalised by excluding a possibility that the algorithm converges at non-equilibrium points.

## 4 PRACTICAL AlGORITHMS

When implementing Algorithm 1 in practice, large state and action spaces could prevent agents from designating policies $\pi^{i}(\cdot \mid s)$ for each state $s$ separately. To handle this, we parameterise each agent's policy $\pi_{\theta^{i}}^{i}$ by $\theta^{i}$, which, together with other agents' policies, forms a joint policy $\pi_{\theta}$ parametrised by $\boldsymbol{\theta}=\left(\theta^{1}, \ldots, \theta^{n}\right)$. In this section, we develop two deep MARL algorithms to optimise the $\boldsymbol{\theta}$.

### 4.1 HATRPO

Computing $\mathrm{D}_{\mathrm{KL}}^{\max }\left(\pi_{\theta_{k}^{i_{m} m}}^{i_{m}}, \pi_{\theta^{i m}}^{i_{m}}\right)$ in Algorithm 1 is challenging; it requires evaluating the $\mathrm{KL}-$ divergence for all states at each iteration. Similar to TRPO, one can ease this maximal KLdivergence penalty $\mathrm{D}_{\mathrm{KL}}^{\max }\left(\pi_{\theta_{k}^{i_{m} m}}^{i_{m}}, \pi_{\theta^{i m}}^{i_{m}}\right)$ by replacing it with the expected KL-divergence constraint $\mathbb{E}_{\mathrm{s} \sim \rho \pi_{\theta_{k}}}\left[\mathrm{D}_{\mathrm{KL}}\left(\pi_{\theta_{k}^{i_{m} m}}^{i_{m}}(\cdot \mid \mathrm{s}), \pi_{\theta^{i m}}^{i_{m}}(\cdot \mid \mathrm{s})\right)\right] \leq \delta$ where $\delta$ is a threshold hyperparameter, and the expectation can be easily approximated by stochastic sampling. With the above amendment, we propose practical HATRPO algorithm in which, at every iteration $k+1$, given a permutation of agents $i_{1: n}$, agent $i_{m \in\{1, \ldots, n\}}$ sequentially optimises its policy parameter $\theta_{k+1}^{i_{m}}$ by maximising a constrained objective:

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-06.jpg?height=98&width=1041&top_left_y=1810&top_left_x=539)

$$
\begin{align*}
& \text { subject to } \mathbb{E}_{\mathrm{s} \sim \rho_{\pi_{\theta_{k}}}}\left[\mathrm{D}_{\mathrm{KL}}\left(\pi_{\theta_{k}^{i m}}^{i_{m}^{m}}(\cdot \mid \mathrm{s}), \pi_{\theta^{i} m}^{i_{m}}(\cdot \mid \mathrm{s})\right)\right] \leq \delta \text {. } \tag{7}
\end{align*}
$$

To compute the above equation, similar to TRPO, one can apply a linear approximation to the objective function and a quadratic approximation to the $\mathrm{KL}$ constraint; the optimisation problem in Equation (7) can be solved by a closed-form update rule as

$$
\begin{equation*}
\theta_{k+1}^{i_{m}}=\theta_{k}^{i_{m}}+\alpha^{j} \sqrt{\frac{2 \delta}{\boldsymbol{g}_{k}^{i_{m}}\left(\boldsymbol{H}_{k}^{i_{m}}\right)^{-1} \boldsymbol{g}_{k}^{i_{m}}}}\left(\boldsymbol{H}_{k}^{i_{m}}\right)^{-1} \boldsymbol{g}_{k}^{i_{m}} \tag{8}
\end{equation*}
$$

where $\boldsymbol{H}_{k}^{i_{m}}=\left.\nabla_{\theta^{i m}}^{2} \mathbb{E}_{\mathrm{s} \sim \rho_{\pi_{\theta_{k}}}}\left[\mathrm{D}_{\mathrm{KL}}\left(\pi_{\theta_{k}^{i_{m} m}}^{i_{m}}(\cdot \mid \mathrm{s}), \pi_{\theta^{i_{m} m}}^{i_{m}}(\cdot \mid \mathrm{s})\right)\right]\right|_{\theta^{i_{m} m}=\theta_{k}^{i_{m}}}$ is the Hessian of the expected KLdivergence, $\boldsymbol{g}_{k}^{i_{m}}$ is the gradient of the objective in Equation (7), $\alpha^{j}<1$ is a positive coefficient that is found via backtracking line search, and the product of $\left(\boldsymbol{H}_{k}^{i_{m}}\right)^{-1} \boldsymbol{g}_{k}^{i_{m}}$ can be efficiently computed with conjugate gradient algorithm.

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-06.jpg?height=84&width=1428&top_left_y=2430&top_left_x=370)
which poses new challenges because each agent's objective has to take into account all previous
agents' updates, and the size of input vaires. Fortunately, with the following proposition, we can efficiently estimate this objective by employing a joint advantage estimator.

Proposition 2. Let $\pi=\prod_{j=1}^{n} \pi^{i_{j}}$ be a joint policy, and $A_{\pi}(\mathrm{s}$, a) be its joint advantage function. Let $\overline{\boldsymbol{\pi}}^{i_{1: m-1}}=\prod_{j=1}^{m-1} \bar{\pi}^{i_{j}}$ be some other joint policy of agents $i_{1: m-1}$, and $\hat{\pi}^{i_{m}}$ be some other policy of agent $i_{m}$. Then, for every state $s$,

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-07.jpg?height=62&width=707&top_left_y=522&top_left_x=427)

$$
\begin{align*}
& =\mathbb{E}_{\mathbf{a} \sim \boldsymbol{\pi}}\left[\left(\frac{\hat{\pi}^{i_{m}}\left(\mathbf{a}^{i_{m}} \mid s\right)}{\pi^{i_{m}}\left(\mathbf{a}^{i_{m}} \mid s\right)}-1\right) \frac{\overline{\boldsymbol{\pi}}^{i_{1: m-1}}\left(\mathbf{a}^{i_{1: m-1}} \mid s\right)}{\boldsymbol{\pi}^{i_{1: m-1}}\left(\mathbf{a}^{i_{1: m-1}} \mid s\right)} A_{\boldsymbol{\pi}}(s, \mathbf{a})\right] \tag{9}
\end{align*}
$$

For proof see Appendix D.1. One benefit of applying Equation (9) is that agents only need to maintain a joint advantage estimator $A_{\boldsymbol{\pi}}(\mathrm{s}, \mathbf{a})$ rather than one centralised critic for each individual agent (e.g., unlike CTDE methods such as MADDPG). Another practical benefit one can draw is that, given an estimator $\hat{A}(\mathrm{~s}, \mathrm{a})$ of the advantage function $A_{\pi_{\theta_{k}}}(\mathrm{~s}, \mathbf{a})$, for example GAE (Schulman et al.,

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-07.jpg?height=98&width=1377&top_left_y=878&top_left_x=363)

$$
\begin{equation*}
\left(\frac{\pi_{\theta}^{i_{m}}\left(\mathrm{a}^{i_{m}} \mid s\right)}{\pi_{\theta_{k}}^{i_{m}}\left(\mathrm{a}^{i_{m}} \mid s\right)}-1\right) M^{i_{1: m}}(s, \mathbf{a}), \quad \text { where } M^{i_{1: m}}=\frac{\overline{\boldsymbol{\pi}}^{i_{1: m-1}}\left(\mathbf{a}^{i_{1: m-1}} \mid s\right)}{\boldsymbol{\pi}^{i_{1: m-1}}\left(\mathbf{a}^{i_{1: m-1}} \mid s\right)} \hat{A}(s, \mathbf{a}) \tag{10}
\end{equation*}
$$

Notably, Equation (10) aligns nicely with the sequential update scheme in HATRPO. For agent $i_{m}$, since previous agents $i_{1: m-1}$ have already made their updates, the compound policy ratio for $M^{i_{1: m}}$ in Equation (10) is easy to compute. Given a batch $\mathcal{B}$ of trajectories with length $T$, we can estimate the gradient with respect to policy parameters (derived in Appendix D.2) as follows,

$$
\hat{\boldsymbol{g}}_{k}^{i_{m}}=\left.\frac{1}{|\mathcal{B}|} \sum_{\tau \in \mathcal{B}} \sum_{t=0}^{T} M^{i_{1: m}}\left(\mathrm{~s}_{t}, \mathbf{a}_{t}\right) \nabla_{\theta^{i} m} \log \pi_{\theta^{i} m}^{i_{m}}\left(\mathrm{a}_{t}^{i} \mid \mathrm{s}_{t}\right)\right|_{\theta^{i} m=\theta_{k}^{i_{m}}}
$$

The term $-1 \cdot M^{i_{1: m}}(\mathrm{~s}, \mathbf{a})$ of Equation (10) is not reflected in $\hat{\boldsymbol{g}}_{k}^{i_{m}}$, as it only introduces a constant with zero gradient. Along with the Hessian of the expected KL-divergence, i.e., $\boldsymbol{H}_{k}^{i_{m}}$, we can update $\theta_{k+1}^{i_{m}}$ by following Equation (8). The detailed pseudocode of HATRPO is listed in Appendix D.3.

### 4.2 HAPPO

To further alleviate the computation burden from $\boldsymbol{H}_{k}^{i_{m}}$ in HATRPO, one can follow the idea of PPO in Equation (4) by considering only using first order derivatives. This is achieved by making agent $i_{m}$ choose a policy parameter $\theta_{k+1}^{i_{m}}$ which maximises the clipping objective of

$$
\begin{equation*}
\mathbb{E}_{\mathrm{s} \sim \rho_{\pi_{\theta_{k}}}, \mathbf{a}^{\sim} \sim \pi_{\theta_{k}}}\left[\min \left(\frac{\pi_{\theta^{i m}}^{i_{m}}\left(\mathrm{a}^{i} \mid \mathrm{s}\right)}{\pi_{\theta_{k}^{i m}}^{i_{m}^{i m}}\left(\mathrm{a}^{i} \mid \mathrm{s}\right)} M^{i_{1: m}}(\mathrm{~s}, \mathbf{a}), \operatorname{clip}\left(\frac{\pi_{\theta^{i m}}^{i_{m}}\left(\mathrm{a}^{i} \mid \mathrm{s}\right)}{\pi_{\theta_{k}^{i_{m}}(}^{i^{i}}\left(\mathrm{a}^{i} \mid \mathrm{s}\right)}, 1 \pm \epsilon\right) M^{i_{1: m}}(\mathrm{~s}, \mathbf{a})\right)\right] \tag{11}
\end{equation*}
$$

The optimisation process can be performed by stochastic gradient methods such as Adam (Kingma $\& \mathrm{Ba}, 2014)$. We refer to the above procedure as HAPPO and Appendix D. 4 for its full pseudocode.

### 4.3 RELATED WORK

We are fully aware of previous attempts that tried to extend TRPO/PPO into MARL. Despite empirical successes, none of them managed to propose a theoretically-justified trust region protocol in multi-agent learning, or maintain the monotonic improvement property. Instead, they tend to impose certain assumptions to enable direct implementations of TRPO/PPO in MARL problems. For example, IPPO (de Witt et al., 2020a) assume homogeneity of action spaces for all agents and enforce parameter sharing. Yu et al. (2021) proposed MAPPO which enhances IPPO by considering a joint critic function and finer implementation techniques for on-policy methods. Yet, it still suffers similar drawbacks of IPPO due to the lack of monotonic improvement guarantee especially when the parameter-sharing condition is switched off. Wen et al. (2021) adjusted PPO for MARL by considering a game-theoretical approach at the meta-game level among agents. Unfortunately, it can only deal with two-agent cases due to the intractability of Nash equilibrium. Recently, $\mathrm{Li} \& \mathrm{He}$ (2020) tried to implement TRPO for MARL through distributed consensus optimisation; however, they enforced the same ratio $\bar{\pi}^{i}\left(a^{i} \mid s\right) / \pi^{i}\left(a^{i} \mid s\right)$ for all agents (see their Equation (7)), which, similar

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-08.jpg?height=374&width=1355&top_left_y=260&top_left_x=363)

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-08.jpg?height=290&width=404&top_left_y=278&top_left_x=381)

(a) 2c-vs-64zg (hard)

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-08.jpg?height=287&width=409&top_left_y=279&top_left_x=831)

(b) $3 \mathrm{~s} 5 \mathrm{z}$ (hard)

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-08.jpg?height=298&width=420&top_left_y=279&top_left_x=1292)

(c) corridor (super hard)

Figure 2: Performance comparisons between HATRPO/HAPPO and MAPPO on three SMAC tasks. Since all methods achieve $100 \%$ win rate, we believe SMAC is not sufficiently difficult to discriminate the capabilities of these algorithms, especially when non-parameter sharing is not required.

to parameter sharing, largely limits the policy space for optimisation. Moreover, their method comes with a $\delta / n$ KL-constraint threshold that fails to consider scenarios with large agent number.

One of the key ideas behind our HATRPO/HAPPO is the sequential update scheme. A similar idea of multi-agent sequential update was also discussed in the context of dynamic programming (Bertsekas, 2019) where artificial "in-between" states have to be considered. On the contrary, our sequential update sceheme is developed based on Lemma 1, which does not require any artificial assumptions and hold for any cooperative games. Furthermore, Bertsekas (2019) requires to maintain a fixed order of updates that is pre-defined for the task, whereas the order in HATRPO/MAPPO can be randomised at each iteration, which also offers desirable convergence property, as stated in Proposition 3 and also verified through ablation studies in Appendix F. The idea of sequential update also appeared in principal component analysis; in EigenGame (Gemp et al., 2020) eigenvectors, represented as players, maximise their own utility functions one-by-one. Although EigenGame provably solves the PCA problem, it is of little use in MARL, where a single iteration of sequential updates is insufficient to learn complex policies. Furthermore, its design and analysis rely on closed-form matrix calculus, which has no extension to MARL.

Lastly, we would like to highlight the importance of the decomposition result in Lemma 1. This result could serve as an effective solution to value-based methods in MARL where tremendous efforts have been made to decompose the joint Q-function into individual Q-functions when the joint Q-function are decomposable (Rashid et al., 2018). Lemma 1, in contrast, is a general result that holds for any cooperative MARL problems regardless of decomposibility. As such, we think of it as an appealing contribution to future developments on value-based MARL methods.

## 5 EXPERIMENTS AND RESULTS

We consider two most common benchmarks-StarCraftII Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) and Multi-Agent MuJoCo (de Witt et al., 2020b)—for evaluating MARL algorithms. All hyperparameter settings and implementations details can be found in Appendix E.

StarCraftII Multi-Agent Challenge (SMAC). SMAC contains a set of StarCraft maps in which a team of ally units aims to defeat the opponent team. IPPO (de Witt et al., 2020a) and MAPPO (Yu et al., 2021) are known to achieve supreme results on this benchmark. By adopting parameter sharing, these methods achieve a winning rate of $100 \%$ on most maps, even including the maps that have heterogeneous agents. Therefore, we hypothesise that non-parameter sharing is not necessarily required and the trick of sharing policies is sufficient to solve SMAC tasks. We test our methods on two hard maps and one super-hard; results on Figure 2 confirm that SMAC is not sufficiently difficult to show off the capabilities of HATRPO/HAPPO when compared against existing methods.

Multi-Agent MuJoCo. In comparison to SMAC, we believe Mujoco enviornment provides a more suitable testing case for our methods. MuJoCo tasks challenge a robot to learn an optimal way of motion; Multi-Agent MuJoCo models each part of a robot as an independent agent, for example, a leg for a spider or an arm for a swimmer. With the increasing variety of the body parts, modelling heterogeneous policies becomes necessary. Figure 3 demonstrate that, in all scenarios, HATRPO and HAPPO enjoy superior performance over those of parameter-sharing methods: IPPO and MAPPO, and also outperform non-parameter sharing MADDPG (Lowe et al., 2017b) both in terms of reward values and variance. It is also worth noting that the performance gap between HATRPO and its rivals enlarges with the increasing number of agents. Meanwhile, we can observe that HATRPO outperforms HAPPO in almost all tasks; we believe it is because the hard KL constraint

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=1618&width=1364&top_left_y=267&top_left_x=356)

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=344&width=420&top_left_y=283&top_left_x=365)

(a) $2 \times 4$-Agent Ant

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=336&width=425&top_left_y=686&top_left_x=362)

(d) 2x3-Agent HalfCheetah

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=339&width=437&top_left_y=1077&top_left_x=367)

(g) 2x3-Agent Walker

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=342&width=420&top_left_y=1474&top_left_x=365)

(j) 17x1-Agent Humanoid

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=339&width=422&top_left_y=286&top_left_x=819)

(b) $4 \times 2$-Agent Ant

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=342&width=431&top_left_y=683&top_left_x=817)

(e) 3x2-Agent HalfCheetah

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=336&width=436&top_left_y=1079&top_left_x=823)

(h) 3x2-Agent Walker

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=344&width=420&top_left_y=1476&top_left_x=820)

(k) 17x1-Agent HumanoidStandup

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=344&width=436&top_left_y=283&top_left_x=1278)

(c) 8x1-Agent Ant

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=338&width=433&top_left_y=682&top_left_x=1277)

(f) 6x1-Agent HalfCheetah

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=336&width=423&top_left_y=1079&top_left_x=1290)

(i) 6x1-Agent Walker

![](https://cdn.mathpix.com/cropped/2024_06_04_37a2f7b6e970656d8079g-09.jpg?height=347&width=422&top_left_y=1472&top_left_x=1277)

(1) 10x2-Agent Swimmer

Figure 3: Performance comparison on multiple Multi-Agent MuJoCo tasks. HAPPO and HATRPO consistently outperform their rivals, thus establishing a new state-of-the-art algorithm for MARL. The performance gap enlarges with increasing number of agents.

in HATRPO, compared to the clipping version in HAPPO, relates more closely to Algorithm 1 that attains monotonic improvement guarantee.

## 6 CONCLUSION

In this paper, we successfully apply trust region learning to multi-agent settings by proposing the first MARL algorithm that attains theoretically-justified monotonical improvement property. The key to our development is the multi-agent advantage decomposition lemma that holds in general with no need for any assumptions on agents sharing parameters or the joint value function being decomposable. Based on this, we introduced two practical deep MARL algorithms: HATRPO and HAPPO. Experimental results on both discrete and continuous control tasks (i.e., SMAC and Multi-Agent Mujoco) confirm their state-of-the-art performance. For future work, we will consider incorporating the safety constraint into HATRPO/HAPPO and propose rigorous safety-aware MARL solutions.

## REFERENCES

Dimitri Bertsekas. Multiagent rollout algorithms and reinforcement learning. arXiv preprint arXiv:1910.00120, 2019.

Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. AAAI/IAAI, 1998(746-752):2, 1998.

Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020a.

Christian Schröder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip H. S. Torr, Wendelin Böhmer, and Shimon Whiteson. Deep multi-agent reinforcement learning for decentralized continuous cooperative control. CoRR, abs/2003.06709, 2020b.

Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International conference on machine learning, pp. 1329-1338. PMLR, 2016.

Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.

Ian Gemp, Brian McWilliams, Claire Vernade, and Thore Graepel. Eigengame: Pca as a nash equilibrium. arXiv preprint arXiv:2010.00554, 2020.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pp. 1861-1870. PMLR, 2018.

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In In Proc. 19th International Conference on Machine Learning. Citeseer, 2002.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

Jakub Grudzien Kuba, Muning Wen, Yaodong Yang, Linghui Meng, Shangding Gu, Haifeng Zhang, David Henry Mguni, and Jun Wang. Settling the variance of multi-agent policy gradients. arXiv preprint arXiv:2108.08612, 2021.

Hepeng $\mathrm{Li}$ and Haibo He. Multi-agent trust region policy optimization. arXiv e-prints, pp. arXiv2010, 2020.

Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pp. 157-163. Elsevier, 1994.

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 6382-6393, 2017a.

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 6382-6393, 2017b.

A Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and James Bergstra. Benchmarking reinforcement learning algorithms on real-world robots. In Conference on robot learning, pp. 561-591. PMLR, 2018.

David H Mguni, Yutong Wu, Yali Du, Yaodong Yang, Ziyi Wang, Minne Li, Ying Wen, Joel Jennings, and Jun Wang. Learning in nonzero-sum stochastic games with potentials. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 7688-7699. PMLR, 18-24 Jul 2021.

John Nash. Non-cooperative games. Annals of mathematics, pp. 286-295, 1951.

P Peng, Q Yuan, Y Wen, Y Yang, Z Tang, H Long, and J Wang. Multiagent bidirectionallycoordinated nets for learning to play starcraft combat games. arxiv 2017. arXiv preprint arXiv:1703.10069, 2017.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR, 2015a.

John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, $2015 \mathrm{~b}$.

John Schulman, F. Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, $\mathrm{pp}$. 387-395. PMLR, 2014.

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085-2087, 2018.

R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12, volume 12, pp. 1057-1063. MIT Press, 2000.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2018.

Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for multi-agent reinforcement learning. In International Conference on Learning Representations, 2018.

Ying Wen, Yaodong Yang, and Jun Wang. Modelling bounded rationality in multi-agent interactions by generalized recursive reasoning. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 414-421. International Joint Conferences on Artificial Intelligence Organization, 7 2020. Main track.

Ying Wen, Hui Chen, Yaodong Yang, Zheng Tian, Minne Li, Xu Chen, and Jun Wang. A gametheoretic approach to multi-agent trust region optimization. arXiv preprint arXiv:2106.06828, 2021.

Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Hang Su, and Jun Zhu. Tianshou: a highly modularized deep reinforcement learning library. arXiv preprint arXiv:2107.14171, 2021.

Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical perspective. arXiv preprint arXiv:2011.00583, 2020.

Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multiagent reinforcement learning. In International Conference on Machine Learning, pp. 5571-5580. PMLR, 2018.

Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang. Multi-agent determinantal q-learning. In International Conference on Machine Learning, pp. 10757-10766. PMLR, 2020.

Chao Yu, A. Velu, Eugene Vinitsky, Yu Wang, A. Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. ArXiv, abs/2103.01955, 2021.

Haifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun Wang. Bi-level actor-critic for multi-agent coordination. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7325-7332, 2020.

Ming Zhou, Ziyu Wan, Hanjing Wang, Muning Wen, Runzhe Wu, Ying Wen, Yaodong Yang, Weinan Zhang, and Jun Wang. Malib: A parallel framework for population-based multi-agent reinforcement learning. arXiv preprint arXiv:2106.07551, 2021.
