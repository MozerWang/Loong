# Bactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation 

Haonan Li ${ }^{1 *}$ Fajri Koto ${ }^{1 *}$ Minghao $\mathbf{W u}^{1,2}$ Alham Fikri Aji ${ }^{1}$ Timothy Baldwin ${ }^{1,3}$<br>${ }^{1}$ Natural Language Processing Department, MBZUAI<br>${ }^{2}$ Monash University ${ }^{3}$ The University of Melbourne<br>\{haonan.li,fajri.koto,minghao.wu,alham.fikri,timothy.baldwin\}@mbzuai.ac.ae


#### Abstract

Instruction tuning has shown great promise in improving the performance of large language models. However, research on multilingual instruction tuning has been limited due to the scarcity of high-quality instruction-response datasets across different languages. To bridge this gap, we present Bactrian-X, a comprehensive multilingual parallel dataset of 3.4 million instruction-response pairs across 52 languages. Leveraging this dataset, we train a set of adapters using low-rank adaptation (LoRA), which are lightweight components that seamlessly integrate with large language models. These adapters have a substantially lower parameter count than the base model, making them easily replaceable and usable as plugins for different languages or language groups. Extensive experiments in various multilingual evaluation settings demonstrate that models derived from LoRA-based training over Bactrian$\mathrm{X}$ outperform both the vanilla models and existing instruction-tuned models. The code and models are publicly available at https: //github.com/mbzuai-nlp/bactrian-x.


## 1 Introduction

Fine-tuning large language models (LLMs) with instruction-response pair datasets has demonstrated remarkable zero-shot generalization capabilities for open-source and closed-source models (Sanh et al., 2022; Wei et al., 2022; Ouyang et al., 2022; OpenAI, 2023). Although the LLMs are often pre-trained using multilingual texts, the instruction-tuning for open-source models is restricted to English (Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023), bringing into question its multilingual generalizability. Closed-resource models such as OpenAI GPT-4 (OpenAI, 2023) and Google BARD, ${ }^{1}$ despite performing impressively over high-resource languages, are still lacking in[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-01.jpg?height=432&width=691&top_left_y=732&top_left_x=1094)

Figure 1: Overview of the Bactrian-X dataset and process for model creation.

terms of multilingual generalizability under monolingual instruction tuning.

The scarcity of instruction-response pair datasets in languages beyond English is hinders multilingual instruction tuning. The existing $\mathrm{xP} 3$ dataset (Muennighoff et al., 2022), which was used to fine-tune BLOOM and mT5, employs English instructions. Although Muennighoff et al. (2022) also experiments with $\mathrm{xP} 3 \mathrm{mt}$ - machine-translated instructions - it focuses on classic NLP tasks such as summarization and question answering, rather than general instructions. Additionally, both xP3 and $\mathrm{xP} 3 \mathrm{mt}$ use template-based prompts, and hence lack variation.

To investigate general instruction tuning in a multilingual setting, we introduce Bactrian- $\mathrm{X}$, containing parallel instruction-response pairs across 52 languages that were automatically constructed by translating instructions from Alpaca (Taori et al., 2023) and Dolly (Conover et al., 2023) via the Google Translate API. ${ }^{2}$ As we detail in Section 3, we use the output distillation trick to obtain corresponding responses by leveraging ChatGPT outputs, conditioned on the translated instructions. With $67 \mathrm{~K}$ instruction-response pairs for each language, the total number of instances in Bactrian-X reaches $3.4 \mathrm{M}$.[^1]

In contrast to previous multilingual instruction models such as BLOOMZ (Muennighoff et al., 2022) which are subject to full fine-tuning via parameter updates across all layers, this study highlights the potential of parameter-efficient finetuning techniques, specifically LoRA (Hu et al., 2022). LoRA uses adapters with substantially fewer parameters than base LLMs, making them more practical and adaptable for real-world applications. Specifically, in this work, we introduce

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-02.jpg?height=51&width=777&top_left_y=717&top_left_x=231)
the BLOOM (Scao et al., 2022) and LLaMA (Touvron et al., 2023) models, and find them to be better than the associated instruction-tuned models: BLOOMZ (Muennighoff et al., 2022) and Alpaca (Taori et al., 2023).

We conduct a comprehensive series of experiments covering a range of zero-shot multilingual NLP tasks, including XCOPA (Ponti et al., 2020), XStoryCloze (Lin et al., 2022), XWinograd (Muennighoff et al., 2022), our own multilingual sentiment analysis dataset SentimentX, and EXAMS (Hardalov et al., 2020). The consistently high results across these tasks highlight the effectiveness of our multilingual instruction dataset and adapter technique for instruction tuning in languages beyond English. To further validate our findings, we use GPT-4 as an evaluator based on the methodology proposed by Chiang et al. (2023), and additionally conduct human evaluation with native speakers. All results confirm that our proposed models outperform the vanilla foundation models and existing instruction-tuned models.

## 2 Related Work

Multilingual Instruction Tuning LLMs such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022) and LLaMA (Touvron et al., 2023) (Hoffmann et al., 2022; Scao et al., 2022; Zeng et al., 2023) have revolutionized NLP. Research has demonstrated that fine-tuning LLMs with instruction prompts can improve their capacity to perform unseen/novel tasks (Wei et al., 2022; Sanh et al., 2022; Ouyang et al., 2022; Chung et al., 2022; Muennighoff et al., 2022). Recently, Wang et al. (2022); Taori et al. (2023) showed that machinegenerated instructions can be used for instruction tuning. Wu et al. (2023) created a large-scale dataset with $2.6 \mathrm{M}$ instructions, and demonstrated that relatively small language models also benefit from the instructions. Prior work has predominantly been on English, and instruction-tuning in languages beyond English remains limited. The closest work to ours is BLOOMZ (Muennighoff et al., 2022), which finetunes BLOOM (Scao et al., 2022) and mT5 (Xue et al., 2021) on the XP3 and xP3mt multilingual instruction datasets. However, both $\mathrm{xP} 3$ and $\mathrm{xP} 3 \mathrm{mt}$ are based on human-written templates, and lack the variability of an organic multilingual dataset. Our work, instead, constructs a parallel general instruction dataset by translating English instructions into 51 languages and generating responses via ChatGPT (Ouyang et al., 2022). To the best of our knowledge, our Bactrian- $\mathrm{X}$ instruction dataset is the largest general-purpose multilingual instruction dataset to date.

Parameter Efficient Fine-Tuning (PEFT) Finetuning all parameters of an LLM (e.g. Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023) and LaMini-LM (Wu et al., 2023)) is computationally expensive, and adapters (Houlsby et al., 2019) offer a more cost-effective alternative. PEFT updates a small number of parameters during fine-tuning, and achieves comparable performance to fully finetuned counterparts (Houlsby et al., 2019; Guo et al., 2021; Lester et al., 2021; Ben Zaken et al., 2022). Hu et al. (2022) introduced Low-Rank Adaptation (LoRA), which incorporates trainable rank decomposition matrices into transformer layers (Vaswani et al., 2017) during fine-tuning without introducing additional latency during inference. They demonstrate that by fine-tuning with less than $1 \%$ of the model parameters, LoRA outperforms several fully fine-tuned LLMs, including GPT-3 (Brown et al., 2020), on various tasks.

In recent work, Taori et al. (2023) use the LoRA trick to fine-tune LLaMA (Touvron et al., 2023), resulting in the Alpaca model, but did not carry out comprehensive evaluation. In this work, we also leverage the LoRA technique to develop a range of monolingual and multilingual adapters, with a much larger instruction-response dataset, across 52 languages. We provide empirical analysis based on automatic and human evaluation to demonstrate the effectiveness of our method.

## 3 Bactrian-X Dataset

In this section, we detail the dataset creation process and provide an overview of the resulting data, focusing on the quality of translated instructions and generated responses.

| Tokenizer | Vocab size | Lang | Instruction tokens | Input tokens | Response tokens | Total tokens |
| :--- | ---: | :--- | :--- | :--- | :--- | :--- |
| mBART-50 | 250,054 | all | $17.11_{ \pm 1.96}$ | $27.54_{ \pm 2.84}$ | $133.65_{ \pm 17.4}$ | $178.30_{ \pm 22.2}$ |
|  |  | seen | $16.14_{ \pm 2.87}$ | $25.98 \pm 3.99$ | $128.88 \pm 25.5$ | $171.00_{ \pm 31.3}$ |
| BLOOM | 251,680 | unseen | $34.21_{ \pm 22.0}$ | $51.41_{ \pm 31.7}$ | $275.97_{ \pm 179}$ | $361.60_{ \pm 231}$ |
|  |  | seen | $23.13_{ \pm 2.78}$ | $36.69_{ \pm 3.85}$ | $185.18 \pm 18.2$ | $244.96 \pm 24.3$ |
|  | 32,000 | unseen | $57.22_{ \pm 35.6}$ | $86.93_{ \pm 52.6}$ | $448.61 \pm 293$ | $592.77_{ \pm 376}$ |

Table 1: Average \# tokens in each Instruction, Input, and Response across all languages. Note that the token counts for mBART-50, LLaMA, and BLOOM are based on the respective tokenizers and are not directly comparable. mBART-50 covers all 52 languages, while LLaMA and BLOOM cover only a subset of the languages in Bactrian-X, and separate results are thus presented for seen and unseen languages.

### 3.1 Dataset Creation

We construct the Bactrian-X dataset in two steps: instruction translation, and response generation (see Figure 1).

Instruction Translation We use English instructions developed for Alpaca (52K) and Dolly (15K), and use the Google Translate API to translate them into 51 different languages, based on the languages used for mBART-50 (Tang et al., 2020). The Alpaca instructions were automatically generated by GPT-3.5 (Ouyang et al., 2022) via the self-instruct technique (Wang et al., 2022), while the Dolly dataset was manually curated by thousands of Databricks company employees. Prior to the translation process, we identify instructions containing programming-related content based on a keywordmatching method and exclude them from the translation process. The total cost for translating the instructions was approximately USD\$10,000.

Response Generation For each translated instruction, we use ChatGPT (gpt-3.5-turbo) to obtain a response. ${ }^{3}$ For English, we pair the instruction with the original response. Translating responses into the 51 languages is costly. Moreover, potential issues such as "translationese" and nonnative answer styles may arise from relying solely on translated responses. The total cost for generating responses amounts to around $\$ 3,000$ USD. We leave the comparison between the translated responses and the ChatGPT-generated responses to future work.

### 3.2 Exploratory Data Analysis

Dataset Statistics We analyzed the tokenized texts in the 52 languages using the mBART-50, LLaMA, and BLOOM tokenizers, and present the statistics in Table 1. Since mBART-50 is trained[^2]

|  | BLEU | chrF++ | COMET |
| :--- | :---: | :---: | :---: |
| min | 28.0 | 52.5 | 82.3 |
| $25 \%$ Q. | 42.9 | 64.7 | 88.7 |
| mean | 48.1 | 68.1 | 90.2 |
| $75 \%$ Q. | 52.7 | 72.2 | 92.0 |
| $\max$ | 69.0 | 82.7 | 95.3 |

Table 2: BLEU, chrF++ and COMET scores for the language pairs from the 51 languages to English. COMET scores are up-scaled by $\times 100$.

on all 52 languages, the tokenizer is trained on all the languages, and the average number of tokens is thus relatively smaller than LLaMA and BLOOM. However, for languages unseen by BLOOM and LLaMA, the tokenized texts are 2 to 3 times longer compared to mBART-50. This suggests that for these unseen languages, both BLOOM and LLaMA models require a larger sequence length for semantically similar input texts, posing a challenge for effective adaptation with the LoRA adapter.

Instruction Quality To test the quality of the translated instructions, we verified the quality of 100 randomly-sampled instances for each language by performing back-translation into English using the Google Translate API. We evaluate the quality of the back-translated instructions relative to the originals based on BLEU (Papineni et al., 2002; Post, 2018), ${ }^{4}$ chrF++ (Popovic, 2017), ${ }^{5}$ and the trained metric COMET (Rei et al., 2020). ${ }^{6}$ The worst BLEU score of 28 is for Mongolian-English translation, but as seen in Table 2, most language pairs achieved BLEU scores above 40, indicating high quality and reliability of the Bactrian- $\mathrm{X}$ instructions.[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-04.jpg?height=457&width=722&top_left_y=228&top_left_x=270)

(a) Human evaluation of response fluency.

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-04.jpg?height=462&width=716&top_left_y=757&top_left_x=273)

(b) Human evaluation of response informativeness.

Figure 2: Human evaluation of the response quality for Bactrian- $\mathrm{X}$. Rate $\mathrm{A}$ is the best and $\mathrm{D}$ is the worst.

Response Quality To evaluate response quality, we conducted human evaluations in three highresource languages - Arabic (ar), Indonesian (id), Chinese (zh) — and three low-resource languages — Burmese (my), Tamil (ta), and Tagalog (tl). For each language, two native-speaker annotators are asked to assess the fluency and informativeness of the responses given the question, except Tagalog, which had only one annotator. The quality assessment guideline is provided in Appendix A, and the results are shown in Figure 2, with an interannotator agreement (IAA) averaged by language of 0.70 and 0.69 for fluency and informativeness, respectively. The results showed that high-resource languages consistently achieved over $80 \%$ satisfactory ratings (A and $\mathrm{B}$ ), while some low-resource languages like Tamil and Burmese had a significant proportion of lower ratings (C and D). This suggests that the outputs generated by ChatGPT are lacking for some low-resource languages. We leave the improvement of data quality for low-resource languages to future work.

## 4 Bactrain-X Models

Given limitations of computation resources, we use base LLMs with 7B and 13B parameters only. First, we trained three multilingual Bactrian models (BX) over the parallel dataset in 52 languages: BX (7B, 13B), and BX $\mathrm{BLOOM}$ (7B). ${ }^{7}$ While our primary results are based on the $\mathrm{BX}$ models, we additionally train some 7B monolingual Bactrian models (BM) for analysis in Section 5: $14 \mathrm{BM}_{\text {LLaMA }}$ and $18 \mathrm{BM}_{\mathrm{BLOOM}}$. All models will be made publicly available in our model repository.

We train our LoRA adapters (Hu et al., 2022) using PyTorch with the HuggingFace PEFT implementation (Mangrulkar et al., 2022; Wolf et al., 2020). Hyperparameters used for training the different models can be found in Appendix C (Table 7). In our evaluation, we compare each multilingual BX model with: (1) the corresponding vanilla models, and (2) the instruction-tuned models Alpaca (Taori et al., 2023) and BLOOMZ (Muennighoff et al., 2022). Details of these models are provided in Appendix B.

## 5 Evaluation on NLP Benchmarks

In order to thoroughly evaluate our Bactrian-X models, we conducted experiments on various multilingual downstream NLP tasks. We first introduce the benchmark datasets we used, and then present the evaluation results in two categories: language understanding tasks (Section 5.2) and knowledgeintensive tasks (Section 5.3).

### 5.1 Datasets

To probe the zero-shot language understanding capability of the different models, we evaluate on the following test sets:

- XCOPA (Ponti et al., 2020): a multilingual resource designed for causal commonsense reasoning, encompassing 11 languages. The task involves predicting the correct next sentence from two options based on cause and effect question types.
- XStoryCloze (Lin et al., 2022): a translation of the English story cloze dataset (Mostafazadeh et al., 2016) into 10 languages. The objective is to select one sentence as a plausible ending (closure) from two options, given a four-sentence story as the premise.
- XWinoGrad (Tikhonov and Ryabinin, 2021; Muennighoff et al., 2022): a multilingual benchmark for commonsense reasoning, made up of Winograd Schema Challenge problems

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-04.jpg?height=46&width=728&top_left_y=2604&top_left_x=1095)
is not available.
}

| Model | XCOPA | XStoryCloze | XWinograd | SentimentX | EXAMS |
| :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMA (7B) | 50.22 | 57.03 | 57.96 | 30.98 | 28.20 |
| oRA (7B) | 50.25 | 6.75 | 57.70 | 35.03 | 28.82 |
| $\mathrm{BX}_{\mathrm{LLaMA}}(7 \mathrm{~B})$ | 51.76 | 58.91 | 60.16 | 42.65 | 29.14 |
| LLaMA (1? | 51.04 | 57.88 | 52.97 | 33.52 | 30.41 |
| Alpaca-LoRA (13B) | 54.8 | 59.03 | 52.27 | 35.79 | 30.47 |
| $\mathrm{BX}_{\text {LLaMA }}(13 \mathrm{~B})$ | 53.27 | 62.12 | 63.65 | 50.27 | 35.71 |
| $\overline{\mathrm{BLOOM}(7 \mathrm{~B})}$ | 51.95 | 56.53 | 57.97 | 26.88 | 25.06 |
| BLOOMZ (7B | 52.1 | 58.05 | 60.05 | 37.68 | 31.23 |
| BX | 54.78 | 58.56 | 60.83 | 33.28 | 26.20 |

Table 3: Zero-shot experiment results on downstream tasks. We report averaged accuracy for XCOPA, XStoryCloze, XWinograd, and EXAMS, and macro-F1 scores for SentimentX.

in six languages. ${ }^{8}$ The task involves selecting the most plausible sentence from options that differ slightly.

- SentimentX: a sentiment classification dataset comprising 3-way sentiment labels collected from various sources, in the following languages: Arabic (ar) (Alturayeif et al., 2022), Spanish (es), ${ }^{9}$ Japanese (jp) (Hayashibe, 2020), Russian (ru), ${ }^{10}$ Indonesian (id) (Koto et al., 2020), Javanese (jav) (Winata et al., 2023), Sundanese (sun) (Winata et al., 2023), and Swahili (sw) (Muhammad et al., 2023).

We also measure how much knowledge the model encodes using the EXAMS benchmark:

- EXAMS (Hardalov et al., 2020): a multilingual question-answering dataset made up of multiple-choice questions from high school examinations in 16 languages. It covers subjects from natural science (e.g., physics), social science (e.g., history), to humanities (e.g., philosophy). Given that all our experiments are zero-shot, we merge the train, validation, and test sets into a single evaluation dataset, and exclude questions without four multiple choice options, resulting in a total of 20,559 questions.


### 5.2 Language Understanding Tasks

The average performance across all languages for XCOPA, XStoryCloze, XWinograd, and SentimentX is presented in Table 3. During inference, we use translated prompts and sentiment labels in the respective languages, obtained[^4]

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-05.jpg?height=468&width=779&top_left_y=851&top_left_x=1047)

Figure 3: The average performance of 7B models on unseen languages (i.e. languages that are not used in pre-training the base model).

from the Google Translate API. We observe that integrating LoRA with the base models of LLaMA and BLOOM, and training over the multilingual instruction datasets, consistently improves performance over the base models. Improvements can also be observed over existing instruction-tuned models such as Alpaca-LoRA, on most tasks. For the larger models, we observe

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-05.jpg?height=52&width=774&top_left_y=1933&top_left_x=1052)
(13B) over LLaMA (13B).

From the third block, we observe that $\mathrm{BX}_{\mathrm{BLOOM}}$ performs better than the full fine-tuned BLOOMZ model on three out of five tasks. Although the performance difference is relatively small, it is worth noting that $\mathrm{BX}_{\mathrm{BLOOM}}$ is fine-tuned only using the LoRA adapter on a smaller multilingual dataset ( $2.5 \mathrm{M}$ samples), whereas BLOOMZ is fully finetuned using a larger dataset of $78 \mathrm{M}$ samples. Additionally, BLOOMZ is fine-tuned on $\mathrm{xP} 3$, which is designed to handle NLP downstream tasks, while Bactrian- $\mathrm{X}$ is more general purpose.

Performance on Unseen Languages In Figure 3, we present the average performance of the 7B mod-

| Tasks | BX $_{\text {LLaMA }}$ | BM $_{\text {LLaMA }}$ | BX $_{\text {BLOoM }}$ | BM $_{\text {BLOOM }}$ |
| :--- | :---: | :---: | :---: | :---: |
| XCOPA | 52.2 | $\mathbf{5 2 . 7}$ | 56.0 | $\mathbf{5 6 . 6}$ |
| XStoryCloze | 59.6 | $\mathbf{6 0 . 5}$ | 59.1 | $\mathbf{6 0 . 7}$ |
| XWinograd | 61.6 | $\mathbf{6 4 . 2}$ | 61.7 | $\mathbf{6 4 . 1}$ |
| SentimentX | 44.1 | $\mathbf{4 4 . 2}$ | 31.3 | $\mathbf{4 1 . 6}$ |
| Average | 54.4 | $\mathbf{5 5 . 4}$ | 52.0 | $\mathbf{5 5 . 8}$ |

Table 4: Zero-shot performance of multilingual BX and monolingual BM models with 7B parameters. We report averaged accuracy for XCOPA, XStoryCloze, and XWinograd, and averaged weighted F1-macro scores for SentimentX.

| Models | Natural | Social | Others |
| :--- | :---: | :---: | :---: |
| LLaMA (13B) | 30.09 | 32.77 | 31.11 |
| Alpaca (13B) | 28.19 | 32.99 | 30.36 |
| BX |  |  |  |
| LLaMA (13B) | $\mathbf{3 3 . 5 8}$ | $\mathbf{3 9 . 1 5}$ | $\mathbf{3 9 . 7 1}$ |

Table 5: Performance breakdown by subject type in EXAMS. "Natural" and "Social" denote natural science and social science, respectively.

els over languages that the base models were not exposed to in pre-training. For XCOPA, XStoryCloze, XWinograd, and SentimentX, the LLaMA model is not exposed to $10,8,2$, and 5 languages, resp., while the BLOOM model is not exposed to $7,2,2$, and 4 languages, respectively. We observe that our proposed models improve on the zero-shot performance of the base models across all tasks, and also surpass the performance of existing instructiontuned models, with the exception of BLOOM over XStoryCloze. A notable improvement can be seen in the SentimentX dataset, implying that our models are more suited to non-English instructions and non-English sentiment labels.

Monolingual vs. Multilingual Fine-tuning For each of the 52 languages in Section 3.2, we compared the performance of monolingual BM models against the multilingual BX models. To ensure a fair benchmark, we exclude unseen languages in calculating the average score. Table 4 presents the average performance for each dataset, revealing that the monolingual BM models consistently outperform the multilingual model for both LLaMA and BLOOM. Particularly notable improvements are observed for XWinograd and SentimentX. For example, the monolingual $\mathrm{BM}_{\mathrm{BLOOM}}$ achieves an impressive overall increase of +10.3 compared to the multilingual model for SentimentX.
You are a helpful and precise assistant for checking the quality of the answer.

<question>

Comment les obstacles linguistiques et culturels ...

$</$ question>

<answer1>

Les obstacles linguistiques peuvent avoir un impact ... </answer1>

<answer2>

The linguistic and cultural obstacles ...

</answer2>

Suppose the user only speaks the language of the question, please evaluate both answers with your justification having less three sentences, and provide a score ranging from 0 to 10 after your justifications. When evaluating the answers, you should consider the helpfulness, relevance, accuracy, level of details of the answers. The score for answer 1 should be wrapped by $<$ score 1$\rangle$ and $</$ score 1$\rangle$, and the score for answer 2 should be wrapped by $<$ score $2>$ and $</$ score $2>$.

Figure 4: Template for GPT-4 evaluation. The colored parts are general prompts that are used for all instances.

### 5.3 Knowledge-intensive Task

The last column of Table 3 shows the results on EXAMS, averaged across languages. We find that the $\mathrm{BX}_{\text {LLaMA }}$ models (7B and 13B) outperform their corresponding base models, while BLOOMZ out-

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-06.jpg?height=51&width=780&top_left_y=1619&top_left_x=1049)
gual instruction tuning seems to be more promising

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-06.jpg?height=52&width=777&top_left_y=1713&top_left_x=1051)
proving substantially over LLaMA by $5.5 \%$ on av-

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-06.jpg?height=57&width=777&top_left_y=1805&top_left_x=1051)
$0.9 \%$. It is noteworthy that $\mathrm{BX}_{\text {LLaMA }}$ (13B) also outperforms LLaMA (30B) on the EXAMS benchmark in Table 12 in Appendix D, underlining the effectiveness of multilingual instruction tuning.

The EXAMS dataset comprises a range of subject areas, such as natural science and social science. We present a breakdown of the results across subject areas for the 13B models in Table 5. It is evident that there are substantial performance improvements over the social sciences and other subject areas during fine-tuning, but comparatively lesser gains for natural science. This could be attributed to our dataset containing fewer instructions and questions related to natural sciences, or the inherent difficulty of learning natural science concepts or reasoning abilities through instruction fine-tuning.

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-07.jpg?height=456&width=759&top_left_y=246&top_left_x=246)

Figure 5: Overall comparison of GPT-4 evaluation.

## 6 Evaluation on Open-ended Questions

As LLMs continue to develop, existing NLP benchmarks may not be up to the task of evaluating model capabilities. To address this, we use GPT-4 (OpenAI, 2023) as an evaluator to compare model outputs, supplemented by human evaluations.

We adopt a challenging set of 80 questions covering 8 categories from Chiang et al. (2023) for open-ended question evaluation. These questions are translated into 51 languages, and we use different models to generate responses (see Appendix E for examples). Following Chiang et al. (2023), we provide two answers from different models in a single prompt, and ask GPT-4 to rate the answers over a scale of 0 to 10 from various aspects including helpfulness, relevance, accuracy, and the level of detail (see Figure 4 for an example prompt for GPT-4 evaluation). To ensure fairness, we interchange the order of the provided answers, and assign scores twice for each question. We exclude vanilla BLOOM and LLaMA from openended question evaluation, and instead compare

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-07.jpg?height=51&width=765&top_left_y=1919&top_left_x=243)

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-07.jpg?height=46&width=763&top_left_y=1970&top_left_x=241)
superiority of instruction-tuned models in previous studies (Chiang et al., 2023; Muennighoff et al., 2022). We select 5 questions from each category, resulting in 40 questions per language. Given cost restrictions and availability of human annotators, we conducted GPT-4 evaluation over 12 languages and human evaluation over 6 languages.

### 6.1 GPT-4 Evaluation

Figure 5 shows the results of the three model pairs, clearly indicate that GPT-4 has a prefer-

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-07.jpg?height=49&width=763&top_left_y=2551&top_left_x=241)

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-07.jpg?height=46&width=765&top_left_y=2598&top_left_x=243)
ison between the two $\mathrm{BX}$ models, $\mathrm{BX}$ LLaMA per-

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-07.jpg?height=388&width=768&top_left_y=246&top_left_x=1061)

Figure 6: GPT-4 evaluation by language. We categorize languages into four groups based on whether a language is seen during model pre-training, and select 3 languages from each group. Group 1: languages seen by both BLOOM and LLaMA; group 2: seen by BLOOM only; group 3: seen by LLaMA only; group 4: not seen by either BLOOM or LLaMA.

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-07.jpg?height=388&width=766&top_left_y=985&top_left_x=1062)

Figure 7: GPT-4 evaluation by question types.

forms better overall.

Since GPT-4 assigns a quantitative score to each response on a scale of $0-10$, we calculate the average score for each model from all comparison pairs and present a breakdown of results separately for each language group (see Figure 6) and question type (see Figure 7).

Language Group Analyzing the results based by language group (see Figure 6), we can make several observations. First, multilingual pre-training plays a critical role for multilingual instructionfollowing models. In groups 1 and $3, \mathrm{BX}_{\text {LLaMA }}$ out-

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-07.jpg?height=49&width=774&top_left_y=2123&top_left_x=1052)
performs substantially better. This difference can be attributed to variations in language coverage during pre-training, as both models are fine-tuned on the same dataset. Second, multilingual instructiontuning is critical. $\mathrm{BX}_{\text {LLaMA }}$, fine-tuned on our multilingual dataset, outperforms Alpaca, which is only fine-tuned on English instructions, across all evaluated languages. From group 4, we observe that if a language is not included in pretraining, multilingual instruction-tuning alone is insufficient to achieve strong performance. Addition-
ally, both $\mathrm{BX} \mathrm{BLOOM}$ and BLOOMZ are initialized by BLOOM but fine-tuned on different instruction datasets. BLOOMZ is fine-tuned on $\mathrm{xP} 3$, a multilingual instruction dataset based on hand-written templates and downstream NLP tasks. In this free generation evaluation, $\mathrm{BX}$ BLOOM performs much better than BLOOMZ, highlighting the limitations of human-written instructions in terms of diversity. Overall, multilinguality in both pre-training and instruction-tuning is vital for the effectiveness of multilingual instruction-following models. These findings reinforce our contributions in this work.

Question Type When considering different question types (see Figure 7), the Bactrian- $\mathrm{X}$ models consistently outperform all base models. A noteworthy observation is that "fermi" and "math" questions, which require strong reasoning capabilities, prove to be challenging for all multilingual LLMs. This observation underlines the fact that numerical reasoning task in a multilingual setup remains an under-explored area, requiring further research.

### 6.2 Human Evaluation

We conducted human evaluation of the outputs of four models (LLaMA, BX

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-08.jpg?height=54&width=780&top_left_y=1532&top_left_x=227)
namely three high-resource languages - Arabic (ar), Indonesian (id), Chinese (zh) - and three low-resource languages - Burmese (my), Tamil (ta), and Tagalog (tl). Native-speaker annotators were asked to rank the outputs of these models based on their overall quality, from 1 (best) to 4 (worst). Prior to annotation, models are shuffled and their identities are not visible to the annotators.

The average Spearman rank correlation between annotators is $\rho=0.78$ across languages, indicating high inter-annotator agreement.

The human evaluation results, averaged across languages and models, are presented in Table 6.

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-08.jpg?height=51&width=780&top_left_y=2213&top_left_x=227)

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-08.jpg?height=55&width=780&top_left_y=2257&top_left_x=227)
counterparts BLOOMZ and Alpaca, once again emphasizing the effectiveness of our multilingual dataset and language adaptation technique. In par-

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-08.jpg?height=57&width=780&top_left_y=2447&top_left_x=228)
for ar, id, zh, and ta, which are languages included in the pre-training of BLOOM. On the other

![](https://cdn.mathpix.com/cropped/2024_06_04_daeca2f39a5c77db275eg-08.jpg?height=55&width=780&top_left_y=2588&top_left_x=227)
which are unseen languages for both base models.

| Model | Language |  |  |  |  |  |
| :--- | ---: | ---: | ---: | ---: | ---: | ---: |
|  | ar | id | zh | my | ta | tl |
| Alpaca | 16.7 | 11.7 | 7.1 | 59.6 | 2.1 | 51.7 |
| BX $_{\text {LLaMA }}$ | 69.2 | 71.3 | 78.3 | $\mathbf{9 2 . 1}$ | 46.7 | $\mathbf{8 1 . 7}$ |
| BLOOMZ | 27.5 | 37.1 | 30.0 | 20.0 | 67.5 | 7.5 |
| BX $_{\text {BLoom }}$ | $\mathbf{8 6 . 7}$ | $\mathbf{8 0 . 0}$ | $\mathbf{8 4 . 6}$ | 28.3 | $\mathbf{8 3 . 8}$ | 59.2 |

Table 6: The results of human evaluation for model responses across six languages. We map the ranks 1, 2, 3 , and 4 into scores $100,66,33$, and 0 , respectively, and then average the two annotator scores. Higher is better. Note that the number here represents a relative ranking score; therefore, a high score does not indicate a perfect model.

## 7 Conclusion

In this paper, we have introduced Bactrian-X, a comprehensive multilingual parallel dataset comprising 3.4 million instruction-response pairs across 52 languages. To enhance the multilingual capabilities of base LLMs, we also introduced a collection of lightweight adapters trained on BactrianX. Experiments on various multilingual NLP tasks demonstrate that models fine-tuned on the Bactrian$\mathrm{X}$ dataset outperform both their corresponding vanilla models and also models fine-tuned on other monolingual/multilingual instruction datasets. By making our dataset and models available, we hope to expedite the advancement of LLMs for multilingual purposes, promoting progress in natural language processing across a broader set of languages.

## Limitations

Our work is subject to several limitations that should be addressed in future research: (1) Our focus was limited to 7B and 13B models, without exploring scaling rules or other base models such as mT5 (Xue et al., 2021). Further investigation into different model variations could provide valuable insights. (2) In our experiments, the maximum sequence length for multilingual models was set to 768 sub-word units. This smaller context size, compared to models with lengths of 1024 or 2048 , may restrict the model's ability to effectively leverage long-range context. Additionally, certain languages that were not well supported by the model tokenizers could face challenges with such a small context size. (3) We did not thoroughly investigate the presence of hallucination, toxicity, and fairness in our models or the base models due to the unavailability of an appropriate evaluation suite. Nonetheless, it
is important to acknowledge that our models, as well as the base models, are likely to be susceptible to these concerns. Future research should address these issues to ensure responsible and unbiased model behavior. We acknowledge these limitations and propose that future work should focus on addressing them to advance the utility and deployment-safety of the models.

## Ethical Considerations

While our instruction-tuning datasets and models offer several advantages, it is essential to recognize their limitations. Despite efforts made by ChatGPT to alleviate ethical concerns, it is still possible for the model to generate responses that are discriminatory, biased, or contain false information, particularly in multilingual settings. Hence, our models, when fine-tuned on the dataset, may inadvertently learn or propagate these problematic patterns.

To address these concerns and minimize potential harm, we are dedicated to mitigating the risks associated with the use of our models in future research. We strongly advocate for the responsible use of our models to prevent any unintended negative consequences.

## References

Nora Saleh Alturayeif, Hamzah Abdullah Luqman, and Moataz Aly Kamaleldin Ahmed. 2022. Mawqif: A multi-label Arabic dataset for target-specific stance detection. In Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP), pages 174-184, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1-9, Dublin, Ireland. Association for Computational Linguistics.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In $A d$ vances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. CoRR, abs/2210.11416.

Mike Conover, Matt Hayes, Ankit Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the world's first truly open instruction-tuned llm. https://www. databricks.com.

Demi Guo, Alexander Rush, and Yoon Kim. 2021. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4884-4896, Online. Association for Computational Linguistics.

Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov, Ivan Koychev, and Preslav Nakov. 2020. EXAMS: A multi-subject high school
examinations dataset for cross-lingual and multilingual question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 5427-5444. Association for Computational Linguistics.

Yuta Hayashibe. 2020. Japanese realistic textual entailment corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 6827-6834, Marseille, France. European Language Resources Association.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. CoRR, abs/2203.15556.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR.

Edward J Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.

Fajri Koto, Afshin Rahimi, Jey Han Lau, and Timothy Baldwin. 2020. IndoLEM and IndoBERT: A benchmark dataset and pre-trained language model for Indonesian NLP. In Proceedings of the 28th International Conference on Computational Linguistics, pages 757-770, Barcelona, Spain (Online). International Committee on Computational Linguistics.

Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019-9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. 2022. Peft: Stateof-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849, San Diego, California. Association for Computational Linguistics.

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generalization through multitask finetuning. CoRR, $\mathrm{abs} / 2211.01786$.

Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Seid Muhie Yimam, David Ifeoluwa Adelani, Ibrahim Sa'id Ahmad, Nedjma Ousidhoum, Abinew Ali Ayele, Saif M. Mohammad, Meriem Beloucif, and Sebastian Ruder. 2023. SemEval2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval). In Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023). Association for Computational Linguistics.

OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems.

Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Edoardo M. Ponti, Goran Glava s, Olga Majewska, Qianchu Liu, Ivan Vuli'c, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal commonsense reasoning. arXiv preprint.

Maja Popović. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Confer-
ence on Machine Translation, pages 612-618, Copenhagen, Denmark. Association for Computational Linguistics.

Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Brussels, Belgium. Association for Computational Linguistics.

Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100.

Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020. Multilingual translation with extensible multilingual pretraining and finetuning. arXiv preprint arXiv:2008.00401.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.

Alexey Tikhonov and Max Ryabinin. 2021. It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3534-3546, Online. Association for Computational Linguistics.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30 .

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. CoRR, $\mathrm{abs} / 2212.10560$.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.

Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Rahmad Mahendra, Fajri Koto, Ade Romadhony, Kemal Kurniawan, David Moeljadi, Radityo Eko Prasojo, Pascale Fung, Timothy Baldwin, Jey Han Lau, Rico Sennrich, and Sebastian Ruder. 2023. NusaX: Multilingual parallel sentiment dataset for 10 Indonesian local languages. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 815-834, Dubrovnik, Croatia. Association for Computational Linguistics.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.

Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2023. Lamini-lm: A diverse herd of distilled models from large-scale instructions. CoRR, abs/2304.14402.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations.
