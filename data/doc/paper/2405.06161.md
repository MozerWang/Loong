# (A Partial Survey of) Decentralized, Cooperative Multi-Agent Reinforcement Learning 

Christopher Amato, Northeastern University

May 24,2024

Multi-agent reinforcement learning (MARL) has exploded in popularity in recent years. Many approaches have been developed but they can be divided into three main types: centralized training and execution (CTE), centralized training for decentralized execution (CTDE), and Decentralized training and execution (DTE).

CTE methods assume centralization during training and execution (e.g., with fast, free and perfect communication) and have the most information during execution. That is, the actions of each agent can depend on the information from all agents. As a result, a simple form of CTE can be achieved by using a single-agent RL method with the centralized action and observation spaces (maintaining a centralized action-observation history for the partially observable case). CTE methods can potentially outperform the decentralized execution methods (since they allow centralized control) but are less scalable as the (centralized) action and observation spaces can scale exponentially with the number of agents. CTE is typically only used in the cooperative MARL case since centralized control implies cooperation. CTDE methods are perhaps the most common as they can use centralized information during training but execute in a decentralized manner-using only information available to that agent during execution. As a result, they can be more scalable than CTE methods, do not require communication during execution, and can often perform well. CTDE fits most naturally with the cooperative case, but can be potentially applied in competitive or mixed settings depending on what information is assumed to be observed. Decentralized training and execution methods make the fewest assumptions and are often simple to implement. In fact, as I'll discuss, any single-agent RL method can be used for DTE by just letting each agent learn separately. Of course, there are pros and cons to such approaches as we discuss below. It is worth noting that DTE is required if no offline coordination is available. That is, if all agents must learn during online interactions without prior coordination, learning and execution must both be decentralized. DTE methods can be applied in cooperative, competitive, or mixed cases but this text will focus on the cooperative MARL case.

MARL methods can be further broken up into value-based and policy gradient methods. Valuebased methods (e.g., Q-learning) learn a value function and then choose actions based on those values. Policy gradient methods learn an explicit policy representation and attempt to improve the policy in the direction of the gradient. Both classes of methods are widely used in MARL.

In this text, I will first give a brief description of the cooperative MARL problem in the form of the Dec-POMDP. Then, I will discuss value-based DTE methods starting with independent Qlearning and its extensions and then discuss the extension to the deep case with DQN, the additional complications this causes, and methods that have been developed to (attempt to) address these

![](https://cdn.mathpix.com/cropped/2024_06_04_dfba410a8bbc0698cd5eg-02.jpg?height=287&width=724&top_left_y=214&top_left_x=706)

Figure 1: A depiction of cooperative MARL-a Dec-POMDP.

issues. Next, I will discuss policy gradient DTE methods starting with independent REINFORCE (i.e., vanilla policy gradient), and then extending to the actor-critic case and deep variants (such as independent PPO). Finally, I will discuss some general topics related to DTE and future directions.

The basics of reinforcement learning (in the single-agent setting) are not presented in this text. Anyone interested in RL should read the book by Sutton and Barto [2018]. Similarly, for a broader overview of MARL, the recent book by Albrecht, Christianos and Sch√§fer is recommended [Albrecht et al., 2024].

It is worth noting that this survey is partial in two respects. First, it is from my viewpoint. Second, it does not cover all work in decentralized, cooperative MARL. This subarea has less work than subareas such as CTDE but it is still more extensive than presented here. I have included work that I believe is important for understanding the main concepts in the subarea and apologize for those that I have omitted.

## 1 The cooperative MARL problem: The Dec-POMDP

The cooperative multi-agent reinforcement learning (MARL) problem can be represented as a DecPOMDP [Oliehoek and Amato, 2016, Bernstein et al., 2002]. Dec-POMDPs generalize POMDPs [Kaelbling et al., 1998] (and MDPs [Puterman, 1994]) to the multi-agent, decentralized setting. As depicted in Figure 1, multiple agents operate under uncertainty based on partial views of the world, with execution unfolding over time. At each step, every agent chooses an action (in parallel) based purely on locally observable information, resulting in each agent obtaining an observation and the team obtaining a joint reward. The shared reward function makes the problem cooperative, but their local views mean that execution is decentralized.

Formally, a Dec-POMDP is defined by tuple $\left\langle\mathbb{I}, \mathbb{S},\left\{\mathbb{A}_{i}\right\}, T, R,\left\{\mathbb{O}_{i}\right\}, O, \mathcal{H}, \gamma\right\rangle$, where

- $\mathbb{I}$ is a finite set of agents;
- $\mathbb{S}$ is a set of states with designated initial state distribution $b_{0}$;
- $\mathbb{A}_{i}$ is a set of actions for each agent $i$ with $\mathbb{A}=\times_{i} \mathbb{A}_{i}$ the set of joint actions;
- $T$ is a state transition probability function, $T: \mathbb{S} \times \mathbb{A} \times \mathbb{S} \rightarrow[0,1]$, that specifies the probability of transitioning from state $s \in \mathbb{S}$ to $s^{\prime} \in \mathbb{S}$ when the actions $\mathbf{a} \in \mathbb{A}$ are taken by the agents (i.e., $T\left(s, \mathbf{a}, s^{\prime}\right)=\operatorname{Pr}\left(s^{\prime} \mid \mathbf{a}, s\right)$ );
- $R$ is a reward function: $R: \mathbb{S} \times \mathbb{A} \rightarrow \mathbb{R}$, the immediate reward for being in state $s \in \mathbb{S}$ and taking the actions $\mathbf{a} \in \mathbb{A}$;
- $\mathbb{O}_{i}$ is a set of observations for each agent, $i$, with $\mathbb{O}=\times_{i} \mathbb{O}_{i}$ the set of joint observations;
- $O$ is an observation probability function: $O: \mathbb{O} \times \mathbb{A} \times \mathbb{S} \rightarrow[0,1]$, the probability of seeing observations $\mathbf{o} \in \mathbb{O}$ given actions $\mathbf{a} \in \mathbb{A}$ were taken and resulting in state $s^{\prime} \in \mathbb{S}$ (i.e., $\left.O\left(\mathbf{a}, s^{\prime}, \mathbf{o}\right)=\operatorname{Pr}\left(\mathbf{o} \mid \mathbf{a}, s^{\prime}\right)\right)$
- $\mathcal{H}$ is the number of steps until termination, called the horizon;
- and $\gamma \in[0,1]$ is the discount factor.

A solution to a Dec-POMDP is a joint policy, denoted $\pi$-a set of policies, one for each agent, each of which is denoted $\pi_{i}$. Because the state is not directly observed, it is typically beneficial for each agent to remember a history of its observations. Then, a local policy, $\pi_{i}$, for an agent is a mapping from local action-observation histories to actions, $\mathbb{H}_{i} \rightarrow \mathbb{A}_{i}$, where $\mathbb{H}_{i}$ is the set of local observation histories, $h_{i}=\left\{a_{i, 1}, o_{i, 1}, \ldots, a_{i, t}, o_{i, t}\right\}$, by agent $i$ up to the current time step, $t$. Many researchers just use observation histories (without including actions), which is sufficient for deterministic policies but may not be for stochastic policies [Oliehoek and Amato, 2016]. Note that histories have implicit time steps due to their length which we do not include in the notation (i.e., we always assume a history starts on the first time step and the last time step is defined by the number of action-observation pairs). We can denote the histories for all agents at a given time step as $\mathbf{h}=\left\langle h_{1}, \ldots, h_{n}\right\rangle$. Because one policy is generated for each agent and these policies depend only on local observations, they operate in a decentralized manner.

While there always exists an optimal deterministic joint policy in Dec-POMDPs, we can also use stochastic joint policies, overloading notation with: $\pi(\mathbf{a} \mid \mathbf{h})=\prod_{i \in \mathbb{I}} \pi_{i}\left(a_{i} \mid h_{i}\right)$, where $\pi_{i}\left(a_{i} \mid h_{i}\right)$ represents the local policy for agent $i$ and the probability of choosing action $a_{i}$ in history $h_{i}$. Deterministic policies will be used in the value-based methods in Section 2 while stochastic policies will be used in the policy gradient gradient methods in Section 3 .

The value of a joint policy, $\pi$, at joint history $\mathrm{h}$ can be defined for the case of discrete states and observations as

$$
V^{\pi}(\mathbf{h})=\sum_{s} P\left(s \mid \mathbf{h}, b_{0}\right)\left[R(s, \pi(\mathbf{h}))+\gamma \sum_{s^{\prime}} P\left(s^{\prime} \mid \pi(\mathbf{h}), s\right) \sum_{\mathbf{o}} P\left(\mathbf{o} \mid \pi(\mathbf{h}), s^{\prime}\right) V^{\pi}(\mathbf{h} \pi(\mathbf{h}) \mathbf{o})\right]
$$

where $P\left(s \mid \mathbf{h}, b_{0}\right)$ is the probability of state $s$ after observing joint history $\mathbf{h}$ starting from state distribution $b_{0}$ and $\pi(\mathbf{h})$ is the joint action taken at the joint history. Also, $\mathbf{h} \pi(\mathbf{h}) \mathbf{o}$ represents $\mathbf{h}^{\prime}$, the joint history after taking joint action $\pi(\mathbf{h})$ in joint history $\mathbf{h}$ and observing joint observation $\mathbf{o}$. In the RL context, algorithms do not iterate over states and observations to explicitly calculate this expectation but approximate it through sampling. For the finite-horizon case, $V^{\pi}(\mathbf{h})=0$ when the length of $h$ equals the horizon $\mathcal{H}$, showing the value function includes the time step from the history. The discount factor, $\gamma$, is typically set to 1 in the finite-horizon case and $\gamma \in[0,1)$ in the infinite-horizon case $(\mathcal{H}=\infty)^{2}$

We will also evaluate policies starting from the initial, null history as $V^{\pi}\left(\mathbf{h}_{0}\right)$. An optimal joint policy beginning at $\mathbf{h}_{0}$ is $\pi^{*}\left(\mathbf{h}_{0}\right)=\operatorname{argmax}_{\pi} V^{\pi}\left(\mathbf{h}_{0}\right)$. That is, the optimal joint policy is the set of local policies for each agent that provides the highest value, which is denoted $V^{*}$.[^0]

Reinforcement learning methods often use history-action values, $Q(\mathbf{h}, \mathbf{a})$, rather than just history values $V(\mathbf{h}) . Q^{\pi}(\mathbf{h}, \mathbf{a})$ is the value of choosing joint action a at joint history $\mathbf{h}$ and then continuing with policy $\pi$,

$$
Q^{\pi}(\mathbf{h}, \mathbf{a})=\sum_{s} P\left(s \mid \mathbf{h}, b_{0}\right)\left[R(\mathbf{a}, s)+\gamma \sum_{s^{\prime}} P\left(s^{\prime} \mid \mathbf{a}, s\right) \sum_{\mathbf{o}} P\left(\mathbf{o} \mid \mathbf{a}, s^{\prime}\right) Q^{\pi}(\text { hao }, \pi(\mathbf{h a o}))\right]
$$

while $Q^{*}(\mathbf{h}, \mathbf{a})$ is the value of choosing action $\mathbf{a}$ at history $\mathbf{h}$ and then continuing with the optimal policy, $\pi^{*}$.

Policies that depend only on a single observation It is somewhat popular to define policies that depend only on a single observation rather than the whole observation history. That is, $\pi_{i}: \mathbb{O}_{i} \rightarrow$ $\mathbb{A}_{i}$, rather than $\mathbb{H}_{i} \rightarrow \mathbb{A}_{i}$. This type of policy is ofter referred to as reactive or Markov since it just depends on (or reacts from) the past observation. These reactive policies are typically not desirable since they can be arbitrarily worse than policies that consider history information [Murphy, 2000] but can perform well or even be optimal in simpler subclasses of Dec-POMDPs [Goldman and Zilberstein, 2004]. In general, reactive policies reduce the complexity of finding a solution (since many fewer policies need to be considered) but only perform well when the problem is not really partially observable.

## 2 Decentralized, value-based methods

Value-based methods (such as Q-learning [Watkins and Dayan, 1992]) learn a value function and the policy is implicitly defined based on those values (i.e., choosing the action that maximizes the value function). Some of the earliest MARL methods are based on applying Q-learning to each agent. For instance, Claus and Boutilier [1998] decompose MARL approaches into independent learners (IL) and joint-action learners (JAL). ILs learn Q-functions that depend on their own information (ignoring other agents) while JALs learn joint Q-functions, which include the actions of the other agents. These concepts were originally defined for the fully observable, bandit (i.e., stateless) case but can be extended to the partially observable, stateful case. For example, we can define an IL Q-function as $Q_{i}\left(h_{i}, a_{i}\right)$, which provides a value for agent $i$ 's history $h_{i}$ and action $a_{i}$. A JAL Q-function $Q_{i}(\mathbf{h}, \mathbf{a})$ for agent $i$ would depend on the joint information $\mathbf{h}$ and a. All decentralized value-based MARL methods are ILs since they never have access to the joint information, $h$ and a.

In particular decentralized value-based MARL methods for Dec-POMDPs:

- assume at each step, each agent, $i$, observes the action it took, $a_{i}$, its resulting observation, $o_{i}$, as well as the joint reward, $r$, and
- estimate a value function based solely on this information (i.e., episodes consisting of sequences of $\left.a_{i}, o_{i}, r\right)$.

As a result, as we will discuss, any (partially observable) single-agent MARL method can be a DTE MARL method by ignoring the fact that other agents exist and learning as if they were the only agent in the problem. Such methods may have issues with convergence and performance (as we'll discuss below) but this is exactly what is done by our first approach: Independent Q-learning.

### 2.1 Independent Q-learning (IQL)

IQL just applies Q-learning to each agent independently. That is, each agent learns its own local Q-function, $Q_{i}$, using Q-learning on its own data. Pseudocode for a version of IQL for the DecPOMDP case is provided in Algorithm 1. Just like Q-learning in the single-agent case, that algorithm uses a learning rate, $\alpha$, and some form of exploration, which in this case is $\epsilon$-greedy. Each agent initializes their Q-function (e.g., to 0 , optimistically, etc.), and then iterates over episodes (either a fixed set or until convergence). During each episode, the agent chooses an action with exploration, sees the joint reward and its own observation and then updates the Q-value at the current history, $h_{i}$, given the current Q-estimates and the reward using the standard Q-learning update (line 9). The history is updated by appending the action taken and observation seen to the current history and the episode continues until the horizon is reached.

Note that the Q-functions are inherently timed as the history length defines the current time step, making the algorithm fit with the finite-horizon case. Regardless, the algorithm can easily be extended to the episodic/indefinite-horizon case by including terminal states (or histories) instead of a fixed horizon.

```
Algorithm 1 Independent Q-Learning for agent $i$ (finite-horizon)
    set $\alpha$ and $\epsilon$
    Initialize $Q_{i}$ for all $h_{i} \in \mathbb{H}_{i}, a_{i} \in \mathbb{A}_{i}$
    for all episodes do
        $h_{i} \leftarrow \emptyset \quad$ \{Empty initial history\}
        for $t=1$ to $\mathcal{H}$ do
            Choose $a_{i}$ at $h_{i}$ from $Q_{i}\left(h_{i}, \cdot\right)$ with exploration (e.g., $\epsilon$-greedy)
            See joint reward $r$, local observation $o_{i} \quad$ \{Depends on joint action a\}
            $h_{i}^{\prime} \leftarrow h_{i} a_{i} o_{i}$
            $Q_{i}\left(h_{i}, a_{i}\right) \leftarrow Q_{i}\left(h_{i}, a_{i}\right)+\alpha\left[r+\gamma \max _{a_{i}^{\prime}} Q_{i}\left(h_{i}^{\prime}, a_{i}^{\prime}\right)-Q_{i}\left(h_{i}, a_{i}\right)\right]$
            $h_{i} \leftarrow h_{i}^{\prime}$
        end for
    end for
    return $Q_{i}$
```

Important hidden information This algorithm runs in the underlying Dec-POMDP but the details of those dynamics in the environment are not shown (as they are not visible to the agent). For example, at the beginning of each episode, the state, $s$, is sampled from the initial state distribution $b_{0}$, and when agents take joint action a the reward is generated from the reward function, $R(s, \mathbf{a})$, the next state $s^{\prime}$ is sampled from the transition dynamics with probability $\operatorname{Pr}\left(s^{\prime} \mid s, \mathbf{a}\right)$, and the joint observation is sampled from the observation function $\operatorname{Pr}\left(\mathbf{o} \mid s^{\prime}, \mathbf{a}\right)$. This process continues until the end of the episode.

While independent Q-learning ignores the other agents, it still depends on them in order to generate the joint reward $r$ and the local observation $o_{i}$ since both of these depend on the actions of all agents. That is, from agent $i$ 's perspective, it is learning in a (single-agent) POMDP, as Algorithm 1 shows (the states of the POMDP would be the states of the Dec-POMDP plus the histories of the other agents).$^{3}$[^1]

In particular, as pointed out in previous papers $\mid$ Lauer and Riedmiller, $\left.2000\right|^{4}$, independent Qlearning is actually learning a Q-function for each agent based on the actions selected by the other agents during training. That is, agent $i$ would use data that depends on the actions taken by the other agents (and observations other agents see) even if it doesn't directly observe those actions (and observations). In the partially observable case, the following Q-function would be learned:

$$
\begin{equation*}
Q_{i}\left(h_{i}, a_{i}\right)=\sum_{\mathbf{a} \in \mathbb{A}} \hat{P}\left(\mathbf{a}, \mathbf{h} \mid h_{i}, a_{i}\right)\left[r+\gamma \sum_{o_{i}} \hat{P}\left(o_{i} \mid \mathbf{h}, \mathbf{a}\right) \max _{a_{i}^{\prime}} Q_{i}\left(h_{i}^{\prime}, a_{i}^{\prime}\right)\right] \tag{1}
\end{equation*}
$$

where $P\left(\mathbf{a}, \mathbf{h} \mid h_{i}, a_{i}\right)$ is the empirical probability that the joint action a and joint history $\mathbf{h}$ occurs when agent $i$ selects its action $a_{i}$ in $h_{i}$. This is precisely when independent Q-learning will update $Q_{i}\left(h_{i}, a_{i}\right)$ and it will use the joint reward $r$ and agent's observation function $P\left(o_{i}^{\prime} \mid \mathbf{a}, \mathbf{h}\right)$, which approximates $P\left(o_{i}^{\prime} \mid \mathbf{a}, s^{\prime}\right) P\left(s^{\prime} \mid \mathbf{a}, s\right)$, where $P\left(o_{i}^{\prime} \mid \mathbf{a}, s^{\prime}\right)$ marginalizes out other agent observations from the joint function $P\left(\mathbf{o}^{\prime} \mid \mathbf{a}, s^{\prime}\right) . P\left(o_{i} \mid \mathbf{h}, \mathbf{a}\right)$ is the empirical probability based on observing $o_{i}$ from $\mathrm{h}$ after taking action $\mathbf{a}$. Therefore, IQL is assuming the other agents are part of the environment and learning a Q-function for the resulting POMDP. If the other agents are not learning and do in fact have fixed policies, the problem would just reduce to this POMDP. Unfortunately, the other agents would also typically be learning, leading to nonstationarity in the underlying POMDP and difficulty with coordinated action selection.

Convergence and solutions In the case of IQL, the underlying POMDP that is being learned is non-stationary since the other agents are also learning and thus changing their policies over time. This may cause IQL to not converge [Tan, 1993]. Convergence of Q-learning in the multi-agent case is an active area of research [Claus and Boutilier,, 1998, Tuyls et al., 2003, Wunder et al., 2010, Hussain et al., 2023], it is still an open question what assumptions will allow convergence and to what (unlike the policy gradient case where convergence to a local optimum is assured under mild assumptions $\mid$ Peshkin et al., , 2000, Lyu et al., , 2023] | $\left.\right|^{5}$.

It is worth pointing out that even if algorithms can converge to an optimal Q-value (or any Qvalue), agents may not be able to select optimal actions if there are multiple optimal policies [Lauer and Riedmiller, 2000]. In this case, agents would need to coordinate on their actions to make sure they choose actions from the same joint policy but this is not possible with only individual Qvalues (e.g., if $Q_{1}\left(h_{1}, a_{1}^{1}\right)=Q_{1}\left(h_{1}, a_{1}^{2}\right)$ and $Q_{2}\left(h_{2}, a_{2}^{1}\right)=Q_{2}\left(h_{2}, a_{2}^{2}\right)$ but $Q\left(h_{1}, h_{2}, a_{1}^{1}, a_{2}^{2}\right)=$ $Q\left(h_{1}, h_{2}, a_{1}^{2}, a_{2}^{1}\right)<Q\left(h_{1}, h_{2}, a_{1}^{2}, a_{2}^{2}\right)=Q\left(h_{1}, h_{2}, a_{1}^{1}, a_{2}^{1}\right)$-agents must take the same actions to be optimal).

IQL is very simple but can perform well [Tan, 1993, Sen et al., 1994, Tampuu et al., 2017]. It is also the basis for the other value-based DTE MARL methods.

et al. 2003] or RL methods [Banerjee et al. [2012]) has had success and results in a best response for agent $i$. Furthermore, iterating over agents, $i$, fixing other agent policies, $\neg i$, and calculating best responses will lead to a local optimum of the Dec-POMDP [Banerjee et al. 2012]. Unfortunately, such methods require coordination to communicate which agent's turn it is to learn so they are not appropriate for the DTE case.

${ }^{4}$ Note the paper considers the fully observable deterministic transition case but we extend the idea to the DecPOMDP case.

${ }^{5}$ In fact, while Q-learning can provably converge to a global optimum in the single-agent case |Jaakkola et al. 1993], there are no known MARL algorithms that have guaranteed convergence to a global optimum. This is an interesting area for future research!

### 2.2 Improving the performance of IQL

Several extensions of IQL have been developed to improve performance and better fit with the MARL setting. While the below algorithms were developed for the fully observable case (the multi-agent MDP [Oliehoek and Amato, 2016]), we extend them to the partially observable case here to fit better with our Dec-POMDP setting.

### 2.2.1 Distributed Q-learning

In order to promote coordination, Distributed Q-learning [Lauer and Riedmiller, 2000] makes strong optimistic assumptions. In particular, extending to the partially observable case, it uses the following Q-update:

$$
\begin{equation*}
Q_{i}\left(h_{i}, a_{i}\right)=\max \left\{Q_{i}\left(h_{i}, a_{i}\right), r+\gamma \max _{a_{i}^{\prime}} Q_{i}\left(h_{i}^{\prime}, a_{i}^{\prime}\right)\right\} \tag{2}
\end{equation*}
$$

where the current $\mathrm{Q}$-value is kept if it is higher than (or equal to) the new estimate $r+\gamma \max _{a_{i}^{\prime}} Q_{i}\left(h_{i}^{\prime}, a_{i}^{\prime}\right)$. Otherwise, the $\mathrm{Q}$-value is updated to be the new estimate. The intuition behind this optimistic update is that other agents will make mistakes during training because they have suboptimal policies or because they are exploring. Agents don't necessarily want to learn from these mistakes. Instead, agents want to learn from the best policies of the other agents such as those that choose actions that generate high-reward values, even if they don't happen very often. In the special case of deterministic MMDPs (the multi-agent fully observable case, see Oliehoek and Amato [2016]), this approach will learn the same Q-function as centralized Q-learning (which will converge to the optimal Q-values in the limit). This is unlike IQL which can get stuck in local optima or never converge at all.

Because Distributed Q-learning learns individual Q-values, agents may still not be able to select optimal actions due to the coordination issue mentioned above. As a result, Distributed Qlearning stores the current best policy for each agent (i.e., if the $\mathrm{Q}$-value is updated, the action that resulted in that update is stored for that state in the fully observable case or history in the partially observable case). Including this coordination mechanism, the algorithm can converge to optimal Q-functions and optimal policies in the limit for the fully observable deterministic case.

In the more general stochastic case, distributed Q-learning is not able to distinguish between environmental stochasticity and agent stochasticity. That is, if getting a high-valued reward is unlikely (due to reward or transition stochasticity) distributed Q-learning will assume it can be gotten with probability one using max rather than taking the expectation (over other agent policies and stochastic transitions) as in Equation 1. As a result, distributed Q-learning is overly optimistic in the stochastic case and may perform poorly. Learning and incorporating the probabilities in Equation 1 and then calculating the expectation over the next step $\mathrm{Q}$-value could fix this problem but would assume agents observe the actions of other agents which is not the case for decentralized training.

### 2.2.2 Hysteretic Q-learning

To maintain the idea of optimism while accounting for stochasticity, hysteretic Q-learning [Matignon et al. 2007] was developed. The idea is to use two learning rates, $\alpha$ and $\beta$ with $\alpha>\beta$. That is, agents update their $\mathrm{Q}$-values more during a positive experience than a negative one.

In particular, if we define the TD error as:

$$
\delta \leftarrow r+\gamma \max _{a_{i}^{\prime}} Q\left(h_{i}^{\prime}, a_{i}^{\prime}\right)-Q_{i}\left(h_{i}, a_{i}\right)
$$

then we can rewrite distributed Q-learning (using a learning rate $\alpha$ ) as:

$$
Q_{i}\left(h_{i}, a_{i}\right)=\left\{\begin{array}{l}
Q_{i}\left(h_{i}, a_{i}\right)+\alpha \delta \quad \text { if } \quad \delta>0  \tag{3}\\
Q_{i}\left(h_{i}, a_{i}\right) \quad \text { else }
\end{array}\right.
$$

highlighting the fact that positive experiences (in terms of TD error) cause updates to the Qfunction while negative ones do not.

Hysteretic $\mathrm{Q}$-learning, in contrast, adds the second learning rate $\beta$ to the negative experiences:

$$
Q_{i}\left(h_{i}, a_{i}\right)= \begin{cases}Q_{i}\left(h_{i}, a_{i}\right)+\alpha \delta & \text { if } \quad \delta>0  \tag{4}\\ Q_{i}\left(h_{i}, a_{i}\right)+\beta \delta & \text { else }\end{cases}
$$

This ensures learning still occurs in the negative cases but the update is less than in the positive cases because $\alpha>\beta$. Therefore, hysteretic Q-learning is still optimistic (hoping the positive experiences are because of better action choices and not stochasticity) but is less optimistic than distributed Q-learning. The implementation is also very simple (although it does require tuning two learning rates). Hysteretic Q-learning is very simple but can perform well in a range of domains [Matignon et al., 2007]. Also, note that as $\beta$ approaches $\alpha$, hysteretic Q-learning will become standard $\mathrm{Q}$-learning and as $\beta$ approaches 0 , it will become distributed $\mathrm{Q}$-learning.

### 2.2.3 Lenient Q-learning

While it is possible to decay $\beta$ towards $\alpha$ as learning continues, to be more optimistic at the beginning of learning (when a lot of exploration is happening) and still be robust to stochasticity as learning converges, lenient Q-learning [Wei and Luke, 2016] allows agents to adjust this leniency towards exploration on a history-action pair basis. Specifically, lenient Q-learning maintains a temperature, $T\left(h_{i}, a_{i}\right)$, per history-action pair that adjusts the probability that a negative update at a particular history-action pair will be ignored. These temperature values allow history-action pairs that have not been visited often to not be updated yet, while frequently visited history-action pairs are updated as in IQL.

The lenient Q-learning update is:

$$
Q_{i}\left(h_{i}, a_{i}\right)= \begin{cases}Q_{i}\left(h_{i}, a_{i}\right)+\alpha \delta & \text { if } \delta>0 \quad \text { or } \quad \text { rand } \sim U(0,1)>1-e^{-K * T\left(h_{i}, a_{i}\right)}  \tag{5}\\ Q_{i}\left(h_{i}, a_{i}\right) & \text { else }\end{cases}
$$

$T\left(h_{i}, a_{i}\right)$ is initialized to a maximum temperature, and is decayed after an update with $T\left(h_{i}, a_{i}\right) \leftarrow \lambda T\left(h_{i}, a_{i}\right)$ for $\lambda \in(0,1)$ and leniency parameter $K$ adjusts the impact of the temperature on the probability distribution. The resulting $\mathrm{Q}$-update will probabilistically update $\mathrm{Q}$ based on how often history-action pairs are visited, with more frequently visited pairs being more likely to be updated when the TD error is negative.

Lenient Q-learning can outperform hysteretic Q-learning since it can adjust the amount of optimism in a more fine-grained way but it also requires maintaining (and updating) temperature values as well as Q-values. Maintaining these values can be more problematic in the partially observable case since the history-action space grows exponentially with the problem horizon (or history length more generally).

### 2.3 Deep extensions, issues, and fixes

In order to scale to larger domains, deep extensions of the above algorithms have been developed. These approaches are typically based on deep Q-networks (DQN) [Mnih et al., 2015]. While DQN can scale to larger action and observation spaces, the decentralized multi-agent context can cause problems. We first discuss the basics of DQN and its extension to the partially observable case, $\mathrm{DRQN}$, and then discuss a deep version of IQL, independent DRQN [Tampuu et al., 2017], the coordination issues with the approach, and some proposed fixes.

### 2.3.1 DQN and DRQN

Deep Q-networks (DQN) [Mnih et al., 2015] is an extension of Q-learning [Watkins and Dayan, 1992] to include a neural net as a function approximator. That is, DQN learns $Q_{\theta}(s, a)$, parameterized with $\theta$ (i.e., $\theta$ represents the parameters of the neural network), by minimizing the loss:

$$
\begin{equation*}
\mathcal{L}(\theta)=\mathbb{E}_{<s, a, r, s^{\prime}>\sim \mathcal{D}}\left[\left(y-Q_{\theta}(s, a)\right)^{2}\right], \text { where } y=r+\gamma \max _{a^{\prime}} Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right) \tag{6}
\end{equation*}
$$

which is just the squared TD error-the difference between the current estimated value, $Q_{\theta}(s, a)$, and the new value gotten from adding the newly seen reward to the previous Q-estimate at the next state, $Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)$. Because learning neural networks can be unstable, a separate target actionvalue function, $Q_{\theta^{-}}$, and an experience replay buffer $\mathcal{D}$ [Lin, 1992] are implemented to stabilize learning. The target network is an older version of the $\mathrm{Q}$-estimator that is updated periodically, the experience replay buffer stores $s, a, s^{\prime}, r$ sequences and single $s, a, s^{\prime}, r$ tuples are i.i.d. sampled for updates. As shown in Figure 2(a), the neural network (NN) outputs values for all actions $(a \in \mathbb{A})$ to make maxing at a state possible with a single forward pass (rather than iterating through the actions).

Deep recurrent Q-networks (DRQN) [Hausknecht and Stone, 2015] extends DQN to handle partial observability, where some number of recurrent layers (e.g., LSTM [Hochreiter and Schmidhuber, 1997]) are included to maintain an internal hidden state which is an abstraction of the history (as shown in Figure 2(b)). Because the problem is partially observable, $o, a, o^{\prime}, r$ sequences are stored in the replay buffer during execution. The update equation is very similar to that of DQN:

$$
\begin{equation*}
\mathcal{L}(\theta)=\mathbb{E}_{<h, a, r, o>\sim \mathcal{D}}\left[\left(y-Q_{\theta}(h, a)\right)^{2}\right], \text { where } y=r+\gamma \max _{a^{\prime}} Q_{\theta^{-}}\left(h^{\prime}, a^{\prime}\right) \tag{7}
\end{equation*}
$$

but since a recurrent neural network (RNN) is used, the internal state of the RNN can be thought of as a history representation. That is, instead of learning $\mathrm{Q}$-functions using full histories (i.e., $\left.h=o_{0}, a_{1}, o_{1}, r_{1}, \ldots, o_{t-1}, a_{t}, o_{t}, r_{t}\right), h$ is the internal state of the RNN after sequentially inputing the history sequence $\left(h=o_{0}, a_{1}, o_{1}, \ldots, o_{t-1}, a_{t}, o_{t}\right)$. Similarly, $h^{\prime}=h a o^{\prime}$, which can be thought of as the new history after appending $o^{\prime}$ and $a$ to $h$, which is achieved by inputing $a$ and $o^{\prime}$ to the RNN when its current state is $h$. Technically, a whole history (e.g., episode) should be sampled from the replay buffer to train the recurrent network but it is common to use a fixed history length (e.g., sample $o, a, o^{\prime}, r$ sequences of length 10). As mentioned above, many implementations just use observation histories rather than full action-observation histories. With the popularity of DRQN, it has become common to add recurrent layers to standard (fully observable) deep reinforcement learning methods when solving partially observable problems.

![](https://cdn.mathpix.com/cropped/2024_06_04_dfba410a8bbc0698cd5eg-10.jpg?height=277&width=461&top_left_y=241&top_left_x=279)

(a) DQN

![](https://cdn.mathpix.com/cropped/2024_06_04_dfba410a8bbc0698cd5eg-10.jpg?height=287&width=393&top_left_y=214&top_left_x=839)

(b) DRQN

![](https://cdn.mathpix.com/cropped/2024_06_04_dfba410a8bbc0698cd5eg-10.jpg?height=282&width=377&top_left_y=233&top_left_x=1313)

(c) Independent DRQN

Figure 2: DQN, DRQN, and independent DRQN diagrams.

### 2.3.2 Independent DRQN (IDRQN)

Independent DRQN (IDRQN) TTampuu et al., 2017 combines IQL and DRQN. As seen in Algorithm 2, the basic structure is the same as IQL but $Q_{i}$ is estimated using an RNN parameterized by $\theta$ (shown in Figure 2(c) , along with using a target network, $\theta^{-}$, and a replay buffer, $\mathcal{D}$. First, episodes are generated by interacting with the environment (e.g., $\epsilon$-greedy exploration), which are added to the replay buffer. This buffer typically has a fixed size and new data replaces old data when the buffer is full. Episodes can then be sampled from the replay buffer (either single episodes or as a minibatch of episodes). In order to have the correct internal state, RNNs are trained sequentially. Training is done from the beginning of the episode until the end7, calculating the internal state and the loss at each step and updating $\theta$ using gradient descent. The target network $\theta^{-}$is updated periodically (every $C$ episodes in the code) by copying the parameters from $\theta$.

While the pseudocode uses a horizon, the resulting Q-functions no longer reflect the time step based on the history since histories of different lengths may be combined into the same representation in the RNN. As a result, untimed/stationary Q-values are learned that just depend on the RNN internal state, which we still call $h$, and not the time step. This stationary representation is technically incorrect for the finite-horizon case (even though it is widely used in practice). Again, the code can be adjusted to the episodic/indefinite-horizon case by replacing the horizon with terminal states or the infinite-horizon case by removing the loop over episodes (both of which fix the issue with timed Q-functions as these cases do not require them). Lastly, the pseudocode, for this approach and throughout the text, is written to be simple and highlight the main algorithmic components. There are many implementation tricks, especially in the deep case, that are not included.

Issues with IDRQN Independent DRQN can perform well but has a number of issues. For example, early methods disabled experience replay [Foerster et al., 2016 $]^{8}$. They found that performance was worse with experience replay than without it. This is because agents are no longer learning concurrently when using the replay buffer-experience is generated from the behavior of all agents (i.e., concurrently) but is then put into the buffer for learning. When learning, different agents will sample and learn from different experiences. Learning from this different data[^2]

```
Algorithm 2 Independent DRQN (IDRQN) for agent $i$ (finite-horizon*)
    set $\alpha$ and $\epsilon$
    : Initialize network parameters $\theta$ and $\theta^{-}$for $Q_{i}$
    $\mathcal{D}_{i} \leftarrow \emptyset$
    $e \leftarrow 1 \quad$ \{episode index\}
    for all episodes do
        $h_{i} \leftarrow \theta^{R N N} \quad$ \{initial history is the initial RNN state of the network\}
        for $t=1$ to $\mathcal{H}$ do
            Choose $a_{i}$ at $h_{i}$ from $Q_{i}^{\theta}\left(h_{i}, \cdot\right)$ with exploration (e.g., $\epsilon$-greedy)
            See joint reward $r$, local observation $o_{i}$
            $\{$ Depends on joint action a $\}$
            append $a_{i}, o_{i}, r$ to $\mathcal{D}_{i}^{e}$
                $h_{i} \leftarrow h_{i} a_{i} o_{i} \quad$ \{update RNN state of the network\}
        end for
        sample an episode from $\mathcal{D}$
        for $t=1$ to $\mathcal{H}$ do
            $h_{i} \leftarrow \theta^{R N N} \quad$ \{initial history is the initial RNN state of the network\}
            $a_{i}, o_{i}, r \leftarrow \mathcal{D}_{i}^{e}(t)$
            $h_{i}^{\prime} \leftarrow h_{i} a_{i} o_{i}$
            $y=r+\gamma \max _{a_{i}^{\prime}} Q_{i}^{\theta^{-}}\left(h_{i}^{\prime}, a_{i}^{\prime}\right)$
            Perform gradient descent on parameters $\theta$ with the loss: $\left(y-Q_{i}^{\theta}\left(h_{i}, a_{i}\right)\right)^{2}$
            $h_{i} \leftarrow h_{i}^{\prime}$
        end for
        if $e \bmod C=0$ then
            $\theta^{-} \leftarrow \theta$
        end if
        $e \leftarrow e+1$
    end for
    return $Q_{i}$
```

![](https://cdn.mathpix.com/cropped/2024_06_04_dfba410a8bbc0698cd5eg-12.jpg?height=442&width=485&top_left_y=191&top_left_x=836)

Figure 3: Concurrent experience replay trajectories (CERTs) (from [Omidshafiei et al., 2017])

makes performance noisy and unstable. This makes intuitive sense since agents will update their Q-functions (and resulting policies) based on (potentially very) different experiences. We discuss a simple fix next.

### 2.3.3 Deep hysteretic-Q and concurrent buffer

Hysteretic Q-learning has been extended to the deep case in the form of decentralized hysteretic deep recurrent Q-networks (Dec-HDRQN) [Omidshafiei et al., 2017]. Dec-HDRQN makes two main contributions: the use of hysteresis with $\mathrm{D}(\mathrm{R}) \mathrm{QN}$, and a concurrent replay buffer to improve performance 9 .

The inclusion of hysteresis in Dec-HDRQN just combines hysteresis and IDRQN. That is, since $y-Q_{i}^{\theta}\left(h_{i}, a_{i}\right)$ in the IDRQN loss is the TD error, $\delta$, the network parameters are updated (using gradient descent) using learning rate $\alpha$ if $\delta>0$, and $\beta$ is used otherwise.

The concurrent buffer is also relatively straightforward. Concurrent experience replay trajectories (CERTs), are based on the idea of creating a joint experience replay buffer and then sampling from that buffer. In particular, we would like to have a joint replay buffer with joint experience tuples, $\mathbf{a}_{1}^{e}, \mathbf{o}_{1}^{e}, r_{1}^{e}, \ldots, \mathbf{a}_{t}^{e}, \mathbf{o}_{t}^{e}, r_{t}^{e}$, where the superscript here represents the episode number, $e$. Agents could then sample from this joint buffer to get the local information from the same episode and time step: $h_{i}, a_{i}, o_{i}, r$ (where $h_{i}$ comes from the internal state of agent $i$ 's RNN after passing the local history of actions and observations until the time step being sampled). Of course, in a decentralized learning context, agents will not have access to such a joint buffer, but agents can store their part of the joint buffer by indexing by episode $e$ and timestep $t$. That is, as seen in Figure 3. agents store their local trajectories along axes for $e$ and $t$. When an $e$ and $t$ is sampled, each agent $i$ will sample data from the same episode and timestep, producing the same data as would be produced with the joint buffer. As long as agents have the same size buffers and coordinate on the seeds for the random number generator, this approach can be used for decentralized learning.

### 2.3.4 Deep lenient $Q$-learning

Lenient Q-learning has been extended to the fully-observable deep case in the form of lenient DQN [Palmer et al., 2018]. In this case, the leniency values are added to the replay buffer of DQN at each step as: $\left(s_{t}, a_{t}, r_{t}, s_{t+1}, l\left(s_{t}, a_{t}\right)\right)$ where $l\left(s_{t}, a_{t}\right)=1-e^{-K * T\left(\phi\left(s_{t}\right), a_{t}\right)}$. To scale to large state spaces, the approach clusters states using an autoencoder which outputs a hash $\phi(s)$ for a given[^3]state $s$. The method also introduces a temperature decay schedule after reaching the terminal state and an improved exploration scheme based on average temperature value.

The approach was shown to perform well in a set of coordinated multi-agent object transportation problems (CMOTPs). It was not extended to the partially observable case but it should be possible by using a method that clusters histories rather than states.

### 2.3.5 Likelihood Q-learning

Distributional Q-learning [Bellemare et al., 2023] has also been used to improve decentralized MARL. Instead of just learning the expected Q-value as in traditional Q-learning, distributional RL learns the distribution of returns that are possible due to stochasticity. Likelihood Q-learning [Lyu and Amato, 2020] uses the return distribution to determine the likelihood of the experience (i.e., $h_{i}, a_{i}, o_{i}, r$ ) given the current estimate, which they call Time Difference Likelihood (TDL). This information can be used to help determine if other agents are exploring (i.e., low likelihood and low value). As a result, the TDL can be used as a multiplier of the learning rate, causing larger updates for more likely experiences-when other agents are not exploring.

But some low-likelihood experiences may be good to learn from, such as when agents have found a high-quality joint action (i.e., low likelihood but high value). Distributional information can then be used to estimate risk [Morimura et al., 2010]. In the multi-agent context risk-seeking behavior can mean being optimistic as in hysteretic or lenient learning.

Each of these ideas can be used in isolation or in combination and they can be used with other decentralized MARL methods to further improve performance. For instance, the combination of Dec-HDRQN (see 2.3.3) with TDL was shown to outperform Dec-HDRQN and forms of lenient $\mathrm{Q}$-learning. Incorporating risk can further improve performance.

## 3 Decentralized policy gradient methods

Single-agent policy gradient and actor-critic methods can similarly be extended to the decentralized training multi-agent case by allowing each agent to learn separately. These approaches can scale to larger action spaces and have stronger (local) convergence guarantees than the value-based counterparts above.

Policy gradient methods can be used with continuous-action or stochastic policies. For all approaches, we will assume that we have a stochastic policy for each agent parameterized by $\psi_{i}$, where $\pi_{i}^{\psi_{i}}\left(a_{i} \mid h_{i}\right)$ represents the probability agent $i$ will choose action $a_{i}$ given the history $h_{i}$ and parameters, $\psi_{i}, \operatorname{Pr}\left(a_{i} \mid h_{i}, \psi_{i}\right)$. This is in contrast to the value-based methods in Section 2, where deterministic policies were generated based on the learned value functions. Like the value-based methods, the decentralized policy gradient methods also assume agents observe their own action, $a_{i}$, the resulting observation, $o_{i}$, and the joint reward, $r$ at each time step.

### 3.1 Decentralized REINFORCE

REINFORCE [Williams, 1992] is one of the oldest (and simplest) policy gradient methods. The basic idea is to estimate the value of the policy using Monte Carlo rollouts and then update the policy using gradient ascent. REINFORCE was extended to the cooperative multi-agent case by Peshkin et al. [2000]. Importantly, Peshkin et al. [2000] also showed that the joint gradient can
be decomposed into decentralized gradients. That is, decentralized gradient ascent will be equivalent to joint gradient ascent. Like the value-based algorithms, 'independent' means each agent is learning independently but the data is generated by all agents from the given policies and all agents make updates at the same time (i.e., concurrent learning). Convergence to a local optimum is guaranteed when agents are learning concurrently because they will apply the same algorithm to the same data at synchronized time steps. This is in contrast to the value-based methods, which do not have strong convergence guarantees. These convergence guarantees extend to many other policy gradient methods (as discussed below).

Pseudocode for a version of decentralized REINFORCE is in Algorithm 3. Here, a set of stochastic policies, $\pi_{i}$, that are parameterized by $\psi_{i}$ (e.g., parameters of a neural network), are learned. A learning rate of $\alpha$ is used (as usual). The method iterates for some number of episodes, choosing actions from the stochastic policy (which has positive probability for all actions to ensure proper exploration), observing the joint reward and local observation at each time step, and storing the episode. Then, the return is computed for each step of the episode and the parameters of the policy are updated using the returns and gradient (just like the single-agent version of REINFORCE). Time steps are explicitly represented in the code to make return calculation clear. Again, the method can be extended to the episodic/indefinite-horizon case by replacing the horizon with terminal states and it isn't well-defined for the infinite-horizon case (since updating takes place at the end of episodes).

```
Algorithm 3 Decentralized REINFORCE for agent $i$ (finite-horizon)
Require: Individual actor models $\pi_{i}\left(a_{i} ; h_{i}\right)$, parameterized by $\psi_{i}$
    set $\alpha$
    for all episodes do
        $h_{i, 0} \leftarrow \emptyset \quad$ \{Empty initial history\}
        $e p \leftarrow \emptyset \quad$ \{Empty episode \}
        for $t=0$ to $\mathcal{H}-1$ do
            Choose $a_{i, t}$ at $h_{i, t}$ from $\pi_{i}\left(a_{i} ; h_{i, t}\right)$
            See joint reward $r_{t}$, local observation $o_{i, t} \quad$ \{Depends on joint action a\}
            append $a_{i, t}, o_{i, t}, r_{t}$ to $e p$
            $h_{i, t+1} \leftarrow h_{i, t} a_{i, t} a_{i, t} \quad$ \{Append new action and obs to previous history \}
        end for
        for $t=0$ to $\mathcal{H}-1$ do
            Compute return at $t$ from $e p: G_{i, t} \leftarrow \sum_{k=t}^{\mathcal{H}-1} \gamma^{k-t} r_{k}$
            Update parameters: $\psi_{i} \leftarrow \psi_{i}+\alpha \gamma^{t} G_{i, t} \nabla \log \pi_{i}\left(a_{i} \mid h_{i, t}\right)$
        end for
    end for
```

Independent REINFORCE isn't widely used but serves as a basis for the later policy gradient approaches as well as theoretical guarantees.

### 3.2 Independent actor critic (IAC)

Since REINFORCE is a Monte Carlo method, it must wait until the end of an episode to update. Furthermore, it will typically be less sample efficient than methods that learn a value function.

Actor-critic methods have been developed to combat these issues by learning a policy representation (the actor) as well as a value function (the critic) [Sutton and Barto, 2018]. Actor-critic methods can use the critic to evaluate the actor without waiting until the end of the episode and TD-learning to update the critic more efficiently.

Independent actor-critic (IAC) was first introduced by Foerster et al. [2018] and it can be thought of as a combination of IQL (or IDRQN) and decentralized REINFORCE. Each agent learns a policy, $\pi_{i}\left(a_{i} ; h_{i}\right)$, as well as a value function such as $Q_{i}\left(h_{i}, a_{i}\right)$. In the simplest case of IAC (where a Q-value is used for the critic), the gradient associated with the policy parameters $\psi_{i}$ can be derived as

$$
\begin{equation*}
\nabla_{\psi_{i}} J=(1-\gamma) E_{\mathbf{h}, \mathbf{a} \sim \rho(\mathbf{h}, \mathbf{a})}\left[Q_{i}\left(h_{i}, a_{i}\right) \nabla_{\psi_{i}} \log \pi_{i}^{\psi_{i}}\left(a_{i} \mid h_{i}\right)\right] \tag{8}
\end{equation*}
$$

assuming a stochastic policy for each agent $i$ that is parameterized by $\psi_{i}, \pi_{i}^{\psi_{i}}\left(a_{i} \mid h_{i}\right)$. Policies are sampled according to the on-policy (discounted) visitation probability $\rho$ (which is a joint visitation probability since concurrent sampling is assumed). This gradient is very similar to the REINFORCE one but uses a Q-value rather than the Monte Carlo return (and is shown here with the expectation rather than just a sample).

In the on-policy, function-approximation (e.g., deep) case, the critic can be updated as:

$$
\begin{equation*}
\mathcal{L}\left(\theta_{i}\right)=\mathbb{E}_{<h_{i}, a_{i}, r, o_{i}>\sim \mathcal{D}}\left[\left(y-Q_{i}^{\theta_{i}}\left(h_{i}, a_{i}\right)\right)^{2}\right] \text {, where } y=r+\gamma Q_{i}^{\theta_{i}}\left(h_{i}^{\prime}, a_{i}^{\prime}\right) \tag{9}
\end{equation*}
$$

where $h_{i}^{\prime}=h_{i} a_{i} o_{i}$ and $a_{i}^{\prime}$ is the action sampled at the next time step using $\pi_{i}^{\psi_{i}}\left(a_{i}^{\prime} \mid h_{i}^{\prime}\right)$ since it is an on-policy estimate (rather than the max action used in DQN). $Q_{i}^{\theta_{i}}\left(h_{i}, a_{i}\right)$ represents the Q-function estimate for agent $i$ using parameters $\theta_{i}$ (just like in Section 2).

Pseudocode for IAC is given in Algorithm 4. Because the value function is no longer being used to select actions but rather just to evaluate the policy, it is common to instead learn $V_{i}\left(h_{i}\right)$. And because a model of $V$ is learned, we denote it $\hat{V}$. Like the REINFORCE case, the algorithm loops for some number of episodes, choosing actions from the stochastic policy and seeing the joint reward and local observation at each step. Now, during the episode, the TD error is calculated and used to update the actor and the critic using the associated gradients described above. Since $\mathrm{V}$-values are learned instead of $\mathrm{Q}$-values, the TD error is calculated with $r_{t}+\gamma \hat{V}_{i}\left(h_{i, t+1}\right)-\hat{V}_{i}\left(h_{i, t}\right)$ rather than $Q_{i}$. As a result, a value-based version of Equation 8 is used to update the actors. Also, a value-based version of Equation 9 is recovered by using $y=r_{t}+\gamma \hat{V}_{i}\left(h_{i, t+1}\right)$ and replacing $Q_{i}^{\theta_{i}}\left(h_{i}, a_{i}\right)$ with $\hat{V}_{i}\left(h_{i, t}\right)$.

As noted in the value-based (DRQN) case, using an RNN may merge histories of different lengths into the same representation, resulting in incorrectly timed value functions in the finitehorizon case. Like the other algorithms, IAC can be adapted to the episodic or infinite-horizon case by including terminal states or removing the loop over episodes. IAC is a simple baseline that is often used and serves as the basis of many other decentralized MARL actor-critic methods.

### 3.3 Other decentralized policy gradient methods

Any single-agent method can be extended to the decentralized MARL case. Typically, it just entails including recurrent layers to deal with partial observability and then training agents concurrently to generate joint data and synchronize updates. One notable example is IPPO [de Witt et al., 2020,

```
Algorithm 4 Independent Actor-Critic (IAC) (finite-horizon)
Require: Individual actor models $\pi_{i}\left(a_{i} \mid h_{i}\right)$, parameterized by $\psi_{i}$
Require: Individual critic models $\hat{V}_{i}(h)$, parameterized by $\theta_{i}$
    for all episodes do
        $h_{i, 0} \leftarrow \emptyset \quad$ \{Empty initial history\}
        for $t=0$ to $\mathcal{H}-1$ do
            Choose $a_{i, t}$ at $h_{i, t}$ from $\pi_{i}\left(a_{i} \mid h_{i, t}\right)$
            See joint reward $r_{t}$, local observation $o_{i, t} \quad$ \{Depends on joint action a\}
            $h_{i, t+1} \leftarrow h_{i, t} a_{i, t} o_{i, t} \quad$ \{Append new action and obs to previous history \}
            Compute value TD error: $\delta_{i, t} \leftarrow r_{t}+\gamma \hat{V}_{i}\left(h_{i, t+1}\right)-\hat{V}_{i}\left(h_{i, t}\right)$
            Compute actor gradient estimate: $\gamma^{t} \delta_{i, t} \nabla \log \pi_{i}\left(a_{i, t} \mid h_{i, t}\right)$
            Update actor parameters $\psi_{i}$ using gradient estimate (e.g., $\psi_{i} \leftarrow \psi_{i}+$
            $\left.\alpha \gamma^{t} \delta_{i, t} \nabla \log \pi_{i}\left(a_{i, t} \mid h_{i, t}\right)\right)$
            Compute critic gradient estimate: $\delta_{i, t} \nabla \hat{V}_{i}\left(h_{i, t}\right)$
            Update critic parameters $\theta_{i}$ using gradient estimate (e.g., $\left.\theta_{i} \leftarrow \theta_{i}+\beta \gamma \delta_{i, t} \nabla \hat{V}_{i}\left(h_{i, t}\right)\right)$
        end for
    end for
```

Yu et al., 2022]. IPPO is the extension of single-agent PPO [Schulman et al. 2017] to the multiagent case. Forms of IPPO have been shown to outperform many state-of-the-art MARL methods. It is worth noting that the standard implementations of IPPO (and the impressive results) use parameter sharing (as discussed below) so they are not technically decentralized but decentralized implementations can still perform well [Yu et al., 2022].

## 4 Other topics

Concurrent learning It is very important to note that all of the methods in this section assume agents are running the same algorithm and perform synchronous updates (i.e., at the same time on the same data). Without these assumptions, methods that have convergence guarantees lose them but may still work well in some domains. As discussed in relation to DQN (see Section 2.3.2), breaking this 'concurrent' learning assumption can cause methods to perform poorly.

Parameter sharing Parameter (or weight) sharing, where agents share the same network for estimating the value function in value-based methods or policies (and value functions) in policy gradient methods, is common in cooperative MARL. The data from each agent can be used to update a single value network for an algorithm such as DRQN. Agents can still perform differently due to observing different histories and agent indices can be added to increase specialization [Gupta et al., 2017]. Since parameter sharing requires agents to share networks during training, it isn't strictly DTE as it couldn't be used for online training in a decentralized fashion. Nevertheless, any of the algorithms in this text can be extended to use parameter sharing. The resulting algorithms would be a form of CTDE as the shared network would represent a centralized component. Nevertheless, these parameter-sharing implementations may be more scalable than other forms of CTDE, are often simple to implement, and can perform well Gupta et al., 2017, Foerster et al., 2018, Yu et al., 2022].

Relationship with CTDE Parameter sharing is one way to exploit a centralized training phase but many others are possible (e.g., sharing a centralized critic [Lowe et al., 2017, Foerster et al., 2018] or learning individual value functions from a joint one [Sunehag et al., 2017, Rashid et al., 2018, Wang et al., 2021]). While CTDE methods are (by far) the most popular form of MARL, they do not always perform the best. In fact, since DTE methods typically make the concurrent learning assumption, they are actually quite close to CTDE methods. This is shown explicitly in the policy gradient case as the gradient of the joint update is the same as the decentralized gradient (as described above in Section 3.1) [Peshkin et al., 2000]. This phenomenon has been also been studied theoretically and empirically with modern actor-critic methods [Lyu et al., 2021, 2023]. It turns out that while centralized critic actor-critic methods are often assumed to be better than DTE actor-critic methods, they are theoretically the same (with mild assumptions) and often empirically similar (and sometimes worse). Too much centralized information can sometimes be overwhelming, harming scalability of centralized-critic methods [Yu et al., 2022, Lyu et al., 2023]. Furthermore, some CTDE methods, such as using a state-based critic, are fundamentally unsound and can often hurt performance in partially observable domains [Lyu et al., 2022, 2023]. Of course, there are many different ways of performing centralized training for decentralized execution, and studying the differences and similarities between DTE and CTDE variants of algorithms seems like a promising direction for understanding current methods and developing improved approaches for both cases.

Other similar types of learning not considered here There are a number of papers that focus on 'fully' decentralized MARL, such as Zhang et al. [2018]. These methods assume communication between agents during training and execution. As a result, the assumptions are different than the ones considered here. I would term these 'fully' decentralized as distributed or networked MARL to make this point more clear. There are also methods that assume additional coordination. For example, some methods assume agents take turns learning while the other agents remain fixed [ $\mathrm{Su}$ et al., 2022, Banerjee et al., 2012]. Such methods require coordinating updates but can converge to local (i.e., Nash) equilibria.

## 5 Acknowledgements

I thank Roi Yehoshua, Andrea Baisero, and the members of my Lab for Learning and Planning in Robotics (LLRP) for reading my (very) rough drafts and providing comments that helped improve the document.

## References

S. V. Albrecht, F. Christianos, and L. Sch√§fer. Multi-Agent Reinforcement Learning: Foundations and Modern Approaches. MIT Press, 2024. https://www.marl-book.com.

B. Banerjee, J. Lyle, L. Kraemer, and R. Yellamraju. Sample bounded distributed reinforcement learning for decentralized POMDPs. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, 2012.

M. G. Bellemare, W. Dabney, and M. Rowland. Distributional Reinforcement Learning. MIT Press, 2023. http://www.distributional-rl.org.

D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein. The complexity of decentralized control of Markov decision processes. Mathematics of Operations Research, 27(4):819-840, 2002.

C. Claus and C. Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. In Proceedings of the National Conference on Artificial Intelligence, pages 746-752, 1998 .

C. S. de Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk, P. H. Torr, M. Sun, and S. Whiteson. Is independent learning all you need in the StarCraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.

J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson. Learning to communicate with deep multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 29, 2016.

J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli, and S. Whiteson. Stabilising experience replay for deep multi-agent reinforcement learning. In Proceedings of the International Conference on Machine Learning, pages 1146-1155, 2017.

J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the National Conference on Artificial Intelligence, 2018.

C. V. Goldman and S. Zilberstein. Decentralized control of cooperative systems: Categorization and complexity analysis. Journal of AI Research, 22:143-174, 2004.

J. K. Gupta, M. Egorov, and M. Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In Adaptive and Learning Agents Workshop at AAMAS, 2017.

M. Hausknecht and P. Stone. Deep recurrent Q-learning for partially observable MDPs. CoRR, $\mathrm{abs} / 1507.06527,2015$.

S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735$1780,1997$.

A. A. Hussain, F. Belardinelli, and G. Piliouras. Asymptotic convergence and performance of multi-agent Q-learning dynamics. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems, 2023.

T. Jaakkola, M. Jordan, and S. Singh. Convergence of stochastic iterative dynamic programming algorithms. In Advances in Neural Information Processing Systems, volume 6, 1993.

L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1-2):99-134, 1998.

M. Lauer and M. A. Riedmiller. An algorithm for distributed reinforcement learning in cooperative multi-agent systems. In Proceedings of the International Conference on Machine Learning, pages 535-542, 2000 .

L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293-321, 1992.

R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, 2017.

X. Lyu and C. Amato. Likelihood quantile networks for coordinating multi-agent reinforcement learning. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems, 2020.

X. Lyu, Y. Xiao, B. Daley, and C. Amato. Contrasting centralized and decentralized critics in multiagent reinforcement learning. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems, 2021.

X. Lyu, Y. Xiao, A. Baisero, and C. Amato. A deeper understanding of state-based critics in multi-agent reinforcement learning. In Proceedings of the National Conference on Artificial Intelligence, 2022.

X. Lyu, A. Baisero, Y. Xiao, B. Daley, and C. Amato. On centralized critics in multi-agent reinforcement learning. Journal of AI Research, 77:235-294, 2023.

L. Matignon, G. J. Laurent, and N. Le Fort-Piat. Hysteretic Q-learning: An algorithm for decentralized reinforcement learning in cooperative multi-agent teams. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 64-69, 2007.

V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.

T. Morimura, M. Sugiyama, H. Kashima, H. Hachiya, and T. Tanaka. Nonparametric return distribution approximation for reinforcement learning. In Proceedings of the International Conference on Machine Learning, pages 799-806, 2010.

K. P. Murphy. A survey of POMDP solution techniques. Technical report, University of British Columbia, 2000.

R. Nair, M. Tambe, M. Yokoo, D. V. Pynadath, and S. Marsella. Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 705-711, 2003.

F. A. Oliehoek and C. Amato. A Concise Introduction to Decentralized POMDPs. Springer, 2016.

S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task multiagent reinforcement learning under partial observability. In Proceedings of the International Conference on Machine Learning, pages 2681-2690, 2017.

G. Palmer, K. Tuyls, D. Bloembergen, and R. Savani. Lenient multi-agent deep reinforcement learning. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems, pages 443-451, 2018 .

L. Peshkin, K.-E. Kim, N. Meuleau, and L. P. Kaelbling. Learning to cooperate via policy search. In Proceedings of Uncertainty in Artificial Intelligence, pages 307-314, 2000.

M. L. Puterman. Markov Decision Processes-Discrete Stochastic Dynamic Programming. John Wiley \& Sons, Inc., 1994.

T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster, and S. Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In Proceedings of the International Conference on Machine Learning, pages 4295-4304, 2018.

J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

S. Sen, M. Sekaran, J. Hale, et al. Learning to coordinate without sharing information. In Proceedings of the National Conference on Artificial Intelligence, volume 94, pages 426-431, 1994.

K. Su, S. Zhou, J. Jiang, C. Gan, X. Wang, and Z. Lu. MA2QL: A minimalist approach to fully decentralized multi-agent reinforcement learning. arXiv preprint arXiv:2209.08244, 2022.

P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, and T. Graepel. Value-decomposition networks for cooperative multi-agent learning. arXiv:1706.05296, 2017.

R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction (second edition). The MIT Press, 2018.

A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, and R. Vicente. Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4), 2017.

M. Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the International Conference on Machine Learning, pages 487-494, 1993.

K. Tuyls, K. Verbeeck, and T. Lenaerts. A selection-mutation model for Q-learning in multi-agent systems. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems, pages 693-700, 2003.

J. Wang, Z. Ren, T. Liu, Y. Yu, and C. Zhang. QPLEX: Duplex dueling multi-agent Q-learning. In Proceedings of the International Conference on Learning Representations, 2021.

C. J. C. H. Watkins and P. Dayan. Q-learning. Machine Learning, 8(3):279-292, May 1992.

E. Wei and S. Luke. Lenient learning in independent-learner stochastic cooperative games. The Journal of Machine Learning Research, 17(1):2914-2955, 2016.

R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229-256, 1992.

M. Wunder, M. L. Littman, and M. Babes. Classes of multiagent Q-learning dynamics with $\epsilon$ greedy exploration. In Proceedings of the International Conference on Machine Learning, pages $1167-1174,2010$.

C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu. The surprising effectiveness of PPO in cooperative multi-agent games. Advances in Neural Information Processing Systems, 35:24611-24624, 2022.

K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar. Fully decentralized multi-agent reinforcement learning with networked agents. In Proceedings of the International Conference on Machine Learning, pages 5872-5881, 10-15 Jul 2018.


[^0]:    ${ }^{1}$ Sometimes, $o_{i, 0}$ is also included as an observation generated by the initial state distribution.

    ${ }^{2}$ The episodic case [Sutton and Barto, 2018] is typically indefinite-horizon with termination to a set of terminal states with probability 1 and the infinite-horizon case is sometimes called 'continuing.'

[^1]:    ${ }^{3}$ The idea of fixing other agent policies and then solving the resulting POMDP (either by planning methods \Nair

[^2]:    ${ }^{6}$ Again, the original algorithm only considers the fully observable case but we consider the extension to the DecPOMDP case.

    ${ }^{7}$ It is common in practice to use a fixed history length and sample windows of that length rather than training with full episodes.

    ${ }^{8}$ Other methods for improving experience replay in MARL exist, but they typically assume the CTDE setting (such as Foerster et al. [2017]).

[^3]:    ${ }^{9}$ The paper also discusses the multi-task case, but we don't discuss that aspect here.

