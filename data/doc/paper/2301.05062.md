# Tracr: Compiled Transformers as a Laboratory for Interpretability 

David Lindner ${ }^{\dagger}$<br>Google DeepMind

János Kramár<br>Google DeepMind

Sebastian Farquhar<br>Google DeepMind

Vladimir Mikulik ${ }^{\dagger}$

Google DeepMind

Thomas McGrath<br>Google DeepMind

Matthew Rahtz<br>Google DeepMind


#### Abstract

We show how to "compile" human-readable programs into standard decoderonly transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/google-deepmind/tracr.


## 1 Introduction

Large language models (LLMs) are powerful but their inner workings are poorly understood (Danilevsky et al., 2020). The development of techniques for interpreting them is held back by a lack of ground-truth explanations (Yang et al., 2019). Our "compiler", Tracr, converts human-readable programs written in RASP, a domainspecific language for transformer computations (Weiss et al., 2021), into standard decoder-only transformers.

Tracr constructs models with known computational structure, which makes it easier to conduct interpretability experiments. As an example, we study neural networks' ability to compress a large number of sparse features into fewer dimensions using superposition (Elhage et al., 2022a). Compressing Tracr models using gradient descent allows us to study superposition in transformers implementing multi-step algorithms.

A second use of transformers that implement known computations is evaluating interpretability methods aiming to reveal facts about a model's computation. Tracr could allow future work to directly test methods including, for example, classifier probes (Belinkov, 2022), gradient-based attribution (Nielsen et al., 2022), and causal tracing (Meng et al., 2022).[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-01.jpg?height=260&width=507&top_left_y=1624&top_left_x=1210)

Figure 1: Interpretability tools produce explanations for a given neural network. Tracr creates models that implement a known mechanism. We can then compare this mechanism to explanations an interpretability tool produces.

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-02.jpg?height=431&width=784&top_left_y=245&top_left_x=974)

Figure 2: An example RASP program (left) that computes the fraction of previous " $x$ " tokens at each position of the input. Tracr compiles this program to a transformer model. (right) A visualisation of a forward pass through the compiled model, showing the contents the residual stream, one panel per layer. The $\mathrm{y}$-axis shows the residual stream dimensions, while the $\mathrm{x}$-axis of each panel corresponds to the input sequence, "<bos>xacx" (x-axis). Changes to the residual are marked in red. Attn 1 is a no-op, MLP 1 computes the indicator variable is_x, Attn 2 implements the select-aggregate operation to compute frac_prevs, and MLP 2 is a no-op again. Section 4 discusses this and other examples in more detail. A detailed, step-by-step interpretation of the figure is provided in Appendix C.

Our main contributions are to: (1) Introduce Tracr, which "compiles" RASP programs into transformer models (Section 3); (2) Showcase models produced by Tracr (Section 4); (3) Provide a case-study where we examine superposition Tracr models compressed using gradient descent (Section 5). We confirm key observations by Elhage et al. (2022b) in a new setting: compressed models drop unnecessary features, and represent less important features in superposition.

In addition to aiding interpretability research, we think compiled models are a powerful didactic tool for developing a more concrete imagination for transformer mechanisms.

We discuss the applications and limitations of Tracr in Section 7 and Appendix A, and we provide an open-source implementation of the compiler at https://github.com/google-deepmind/tracr.

## 2 Background

This section provides an overview of key concepts our work builds on. In particular, we review the transformer model architecture and the RASP programming language.

### 2.1 Transformer Architecture

A transformer model consists of alternating multi-headed attention (MHA) and multi-layer perceptron (MLP) layers with residual connections. Multi-headed attention (Vaswani et al., 2017) computes attention maps on sequences of length $N$. A single attention head $i$ first computes an attention pattern

$$
A^{i}=\operatorname{softmax}\left(\left(x W_{Q}^{i}\right)\left(x W_{K}^{i}\right)^{T} / \sqrt{d_{k}}\right) \in \mathbb{R}^{N \times N}
$$

for some input $x \in \mathbb{R}^{N \times d}$, where $W_{Q}^{i}, W_{K}^{i} \in \mathbb{R}^{d \times d_{k}}$ are learnable parameters. Usually, we call the entries of $\left(x W_{K}^{i}\right)$ keys, and the entries of $\left(x W_{Q}^{i}\right)$ queries. Multi-headed attention combines $H$ attention heads heads by computing

$$
\operatorname{MHA}(x)=\operatorname{Concat}\left[A^{1}\left(x W_{V}^{1}\right), \ldots, A^{H}\left(x W_{V}^{H}\right)\right] W_{O}
$$

where $W_{V}^{i} \in \mathbb{R}^{d \times d_{v}}$ and $W_{O} \in \mathbb{R}^{H d_{v} \times d}$ are another set of learnable parameters. We commonly call the entries of $\left(x W_{V}^{i}\right)$ values.

The MLP layers in transformer models compute $\operatorname{MLP}(x)=\sigma\left(x W_{1}\right) W_{2}$ where $W_{1} \in \mathbb{R}^{d \times h}$, $W_{2} \in \mathbb{R}^{h \times d}$ are learnable weights, and $\sigma$ is a non-linear function; for simplicity, we use the Rectified Linear Unit (ReLU). In this paper, we focus on decoder-only transformers, which consists of alternating blocks of MHA and MLP.

When designing Tracr, we adopt the transformer circuits perspective, introduced by Elhage et al. (2021). This view (1) focuses on the transformer being a residual stream architecture and (2)
introduces an alternative parameterisation for attention operations. Taking this viewpoint, simplifies reasoning about the transformer architecture and will help us when assembling transformers manually.

The residual stream view. Transformers have residual connections at each attention and MLP layer. Elhage et al. (2021) consider the residual connections a core feature of the architecture and describe the model in terms of a residual stream that each layer reads from and writes to in sequence. The residual stream acts as a type of memory that earlier layers can use to pass information to later layers.

Parameterising attention as $W_{Q K}$ and $W_{O V}$. Following Elhage et al. (2021), we parameterise an attention head by two (low-rank) matrices $W_{Q K}{ }^{i}=W_{Q}^{i}\left(W_{K}^{i}\right)^{T} / \sqrt{d_{k}} \in \mathbb{R}^{d \times d}$ and $W_{O V}{ }^{i}=$ $W_{V}^{i} W_{O}^{i} \in \mathbb{R}^{d \times d}$ where we split $W_{O}$ into different heads, such that $W_{O}=\left[W_{O}^{1}, \ldots W_{O}^{H}\right]$, where each $W_{O}^{i} \in \mathbb{R}^{d_{v} \times d}$. We can then write MHA as

$$
A^{i}=\operatorname{softmax}\left(x W_{Q K}^{i} x^{T}\right) \quad \operatorname{MHA}(x)=\sum_{i=1}^{H} A^{i} x W_{O V}^{i}
$$

Importantly, we can think of MHA as summing over the outputs of $H$ independent attention heads, each parameterised by low-rank matrices $W_{Q K}$ and $W_{O V}$. $W_{Q K}$ acts as a bilinear operator reading from the residual stream, and $W_{O V}$ is a linear operator both reading from and writing to the residual stream. The softmax is the only nonlinearity in an attention head.

### 2.2 RASP

The "Restricted Access Sequence Processing Language" (RASP) is a human-readable computational model for transformer models introduced by Weiss et al. (2021). RASP is a sequence processing language with two types of variables, sequence operations (s-ops) and selectors, and two types of instructions, elementwise and select-aggregate transformations.

Sequence operations. A sequence operation (s-op) represents sequences of values during evaluation. tokens and indices are built-in primitive s-ops that return a sequence of input tokens or their indices, respectively. For example: tokens("hello") $=[\mathrm{h}, \mathrm{e}, \mathrm{l}, \mathrm{l}, \mathrm{o}]$, and indices("hello") $=[0,1,2,3,4]$. S-ops roughly correspond to the state of the residual stream in transformers.

Elementwise operations. RASP allows arbitrary elementwise operations on s-ops. For example, we can compute ( $3 *$ indices) ("hello") $=[0,3,6,9,12]$. Elementwise operations roughly correspond to MLP layers in transformers.

Select-aggregate operations. To move information between token positions, RASP provides selectaggregate operations which roughly correspond to attention in transformers. A selector has a graph dependency on two s-ops and evaluates on inputs of length $N$ to a binary matrix of size $N \times N$. To create a selector, the select operation takes two s-ops and a boolean predicate $p(x, y)$. For example:

select(indices, $[1,0,2],<)(" a b c ")=\left[\begin{array}{lll}1 & 0 & 0 \\ 0 & 0 & 0 \\ 1 & 1 & 0\end{array}\right]$.

Here, $p(x, y)=x<y$, where $x$ comes from indices, and $y$ comes from the constant s-op $[1,0,2]$.

The aggregate operation takes as input a selector and an s-op, and produces an s-op that averages the value of the s-op weighted by the selection matrix. For example:

aggregate $\left(\left[\begin{array}{lll}1 & 0 & 0 \\ 0 & 0 & 0 \\ 1 & 1 & 0\end{array}\right], \quad[10,20,30]\right)=[10,0,15]$.

A selector roughly corresponds to an attention pattern in a transformer. Together a select-aggregate operation roughly corresponds to an attention head in transformers.

### 2.3 Mechanistic Interpretability and Superposition

Mechanistic interpretability (Cammarata et al., 2020; Olah, 2022) aims to produce mechanistic explanations of the inner workings of ML programs. This includes attempts to reverse engineer how neural networks implement specific behaviours (Cammarata et al., 2020). ${ }^{3}$

In Section 5, we study superposition: the ability of a neural network to approximately represent many more features than the number of dimensions of the embedding space (Elhage et al., 2022a). Despite preliminary evidence that superposition occurs in neural networks, it remains poorly understood, in part because it has only been studied in small (2-layers or less) networks that implement very simple algorithms (Elhage et al., 2022b; Scherlis et al., 2022). Understanding superposition in larger models could represent a major step forward for mechanistic interpretability (Olah, 2022).

## 3 Tracr: A Transformer Compiler for RASP

In this section, we provide an overview of how Tracr translate RASP programs to transformer weights. For more details on the implementation, we refer to Appendix D and our open-source implementation at https://github.com/google-deepmind/tracr including the accompanying documentation.

Tracr comes with an implementation of RASP embedded in Python. A RASP program is an expression graph which is incrementally constructed from atomic RASP operations. We make a few technical modifications to allow translating RASP to model weights: we disallow boolean combinations of selectors, enforce annotated categorical or numerical embeddings for the residual stream, and enforce the use of a beginning-of-sequence token. We discuss the motivations for each of these changes in Appendix B, where we also explain how any RASP program can be refactored to be compatible with these restrictions. In practice, we can implement programs to solve all tasks described by Weiss et al. (2021).

If RASP is the high-level language we compile, Craft is our "assembly language", offering slightly more abstraction than pure weight matrices (cf. Figure 3). Craft provides a transformer implementation using vector spaces with labelled basis dimensions and operations on them. This lets us define projections or other linear operations

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-04.jpg?height=434&width=490&top_left_y=1062&top_left_x=1251)

Figure 3: Tracr translates RASP to Craft and then to model weights, analogous to how programming languages are first translated to assembly then to machine code. in terms of basis direction labels, which simplifies constructing model components that act on different vector spaces. As a bonus, models represented in Craft are independent of specific transformer implementations. Models compiled by Tracr can be translated into weights of any standard decoder-only transformer model (without layer norm).

Tracr translates RASP programs to transformer weights in six steps:

1. Construct a computational graph (Figure 4(a)).
2. Infer s-op input and output values (Figure 4(a)).
3. Independently translate s-ops into model blocks (Figure 4(b)).
4. Assign components to layers (Figure 4(c)).
5. Construct the model (Figure 4(c)).
6. Assemble weight matrices.

Let us go through these step by step. Figure 4 gives a schematic overview using an example program.

1. Construct a computational graph (Figure 4(a)). First, we trace the whole program to create a directed graph representing the computation. The graph has source nodes representing tokens and indices and a sink node for the output s-op. Each operation in the RASP program becomes a node in the computational graph.[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-05.jpg?height=279&width=355&top_left_y=262&top_left_x=386)

(a) Steps 1 \& 2: Graph with inferred s-op value sets.

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-05.jpg?height=288&width=374&top_left_y=260&top_left_x=778)

(b) Step 3: Nodes translated to MLPs and attention heads.

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-05.jpg?height=298&width=575&top_left_y=260&top_left_x=1184)

(c) Steps 4 \& 5: Nodes allocated to locations in a model.

Figure 4: Schematic overview of how Tracr compiles the frac_prevs program from Figure 2 with a input vocabulary $\{$ " $x ", " y "\}$ and context size 3 . (a) shows the computational graph with value annotations after step 2 of the compilation. (b) shows how is_x and frac_prevs are translated to model components independently in step 3. (c) shows the assembled model which has two no-op components because models blocks always need to have one attention and one MLP layer.

2. Infer s-op values (Figure 4(a)). For each s-op, we need to decide how to embed it in the residual stream. To use categorical encodings, we need to know which values an s-op can take. All nodes have a finite set of output values because computations are deterministic, and we have a finite input vocabulary and context size. Therefore, in the second step, we traverse the graph and annotate each node with its possible outputs. This annotation uses simple heuristics that ensure we find a superset of the values an s-op will take, though, sometimes, an output set can contain values that the s-op never takes in practice.
3. Independently translate s-ops (Figure 4(b)). Next, we consider each node in the computational graph independently and translate it into a model block. Elementwise operations become MLP blocks, and select-aggregate operations become attention blocks. We use a library of manually engineered MLP and attention blocks to approximate arbitrary functions for numerical and categorical inputs and outputs. MLPs with categorical inputs and outputs function as lookup tables. MLPs with numerical inputs and outputs use piecewise linear approximations. For attention layers, we translate a selector into the $W_{Q K}$ operator and the corresponding aggregate operation into the $W_{O V}$ operator. We only support attention with categorical inputs. We also do a few basic simplifications of RASP programs at this stage. For example, we combine consecutive elementwise operations into a single s-op. For more details on the MLP and attention blocks, see Appendix D.
4. Assign components to layers (Figure 4(c)). To construct a transformer model, we need to allocate all model blocks in the computational graph to layers. Ideally, we want to find the smallest model to perform the desired computation. We can generally formulate this as a combinatorial optimization problem with several constraints: the transformer architecture has alternating attention and MLP layers, and all computations that depend on each other need to be in the correct order. For scope reasons, we solve this problem heuristically. First, we compute the longest path from the input to a given node. This path length is an upper bound for the layer number to which we can allocate the node. Then we apply additional heuristics to combine layers with blocks that we can compute in parallel. This approach returns a correct but sometimes suboptimal layer allocation.
5. Construct the model (Figure 4(c)). We construct the residual stream space as the direct sum of all model components' input and output spaces. In other words, we embed each s-op in its own orthogonal subspace, which is reserved for its sole use throughout the entire network. Now, we can traverse the computational graph in the order determined by the layer allocation and stack the components to obtain a full transformer represented in Craft.
6. Assemble weight matrices. Finally, we translate the Craft representation of the model into concrete model weights. First, we combine parallel MLP layers into a single layer and parallel attention heads into a single layer. In attention layers, we then factor the $W_{Q K}$ and $W_{O V}$ matrices into separate $W_{q}, W_{k}, W_{o}, W_{v}$ weight matrices. Finally, we adjust the shapes of all weights and connect them to our transformer architecture. We can then infer the model configuration (depth, layer width, residual stream size, etc.) to fit the elements we have created.

We use a standard decoder-only transformer implementation in Haiku (Hennigan et al., 2020), notably removing layer norms. Extending Tracr to support any other transformer implementation is straightforward by reimplementing only step 6 .

Figure 5: RASP program that sorts a sequence of numbers without duplicates. Attn 1 and MLP 1 implement the selector_width primitive (cf. Appendix D) which the program uses to compute the target position for each token. Attn 2 moves the tokens to the desired position, and MLP 2 is a no-op.

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-06.jpg?height=569&width=702&top_left_y=252&top_left_x=1053)

We are now ready to compile models with Tracr and walk through a few example programs.

## 4 Exploring Compiled Transformers

In this section, we walk through two example programs to illustrate how the compiled models work. While these models are not necessarily realistic, they represent configurations of weights that could, in principle, be learned. Examining these models can therefore be a powerful didactic tool for understanding how transformers perform complex computation, which we hope will expand our collective imagination for their inner workings.

We were able to compile RASP programs for all the tasks described in Weiss et al. (2021), though we had to modify a few programs to only use features supported by Tracr. Appendix G contains more examples.

### 4.1 Example 1: Counting Tokens

Figure 2 shows our primary running example, the frac_prevs program, that computes the fraction of previous "x" tokens.

The compiled frac_prevs model has a 14-dimensional residual stream, but it uses 12 out of these for the input embeddings. The remaining two dimensions contain the main numerical variables used in the computation: is_x and frac_prevs (the output variable). The input embeddings have a few special dimensions. In particular, tokens:bos is the beginning of sequence token which we need to implement arbitrary attention patterns, and one is an input dimension that is always 1 , used as a constant, e.g., to add a bias in MLP layers.

The compiled model uses one MLP layer and one attention head. However, because our model architecture always starts with an attention layer, the compiled model has four layers, with the first and last layers being no-ops. The first MLP layer computes the indicator variable is_x based on the input tokens. The following attention layer computes a causal attention pattern and uses it to compute the faction of previous " $x$ " tokens.

### 4.2 Example 2: Sorting

As a second example, let us consider sorting a sequence of numbers. Figure 5 shows a sort_unique program that sorts a sequence of unique tokens.

The program computes uses a selector to select smaller tokens for each input token, and then uses the selector_width primitive in RASP to compute the target position for each token. selector_width counts the number of elements in each row of a selector that are 1 , in this case the number of elements that are smaller than a given input token. selector_width can be implemented in terms of other RASP operations (Weiss et al., 2021). However, in Tracr we treat it as a primitive
that compiles directly to an attention and MLP layer (here Attn 1 and MLP 1). See Appendix D for more details. The model then uses a second attention layer to move each token to its target position.

Weiss et al. (2021) propose a sort program that can handle duplicates (cf. their Figure 13). However, that implementation uses a composite selector

```
select(tokens, tokens, <) or (
    select(key, key, ==) and select(indices, indices, <))
```

to treat duplicates, which is not currently supported by Tracr. In Appendix G, we provide an alternative implementation of sort that handles duplicates by adding a small multiple of indices to the keys and then applying sort_unique.

### 4.3 More Examples

Tracr can compile a wide range of RASP programs. In Appendix G, we discuss several additional examples, leading up to a program to check balanced parentheses (Dyck-n). Our open-source Tracr implementation (https://github.com/google-deepmind/tracr) contains a library of even more example programs to compile.

## 5 Compressing Compiled Transformers

Superposition is an important phenomenon in large language models (see Section 2.3, Elhage et al. (2022b), and Scherlis et al. (2022)). But to the best of our knowledge, it has not yet been studied in detail for models with more than two layers or in transformer models executing multi-step algorithms. Tracr lets us examine these models, and we can force different levels of superposition by applying a gradient-descent-based compression algorithm.

In addition to helping us study superposition, compressed models could be more efficient and realistic. Tracr models can be sparse and inefficient because they reserve an orthogonal subspace of the residual stream for each s-op.

Here, we present two case studies of compressing compiled models using the frac_prevs and the sort_unique programs from Section 4. These sketch how Tracr can be practically useful in advancing interpretability research, while also giving a glimpse of how Tracr could be extended to produce more realistic models.

### 5.1 Gradient Descent Based Compression

We use a single linear projection $W \in \mathbb{R}^{D \times d}$ to compress the disentangled residual stream with size $D$ to a smaller space with dimension $d<D$. We modify the model to apply $W^{T}$ whenever it reads from and $W$ whenever it writes to the residual stream (see Figure 6). We freeze all other weights and train only $W$ using stochastic gradient descent (SGD). Since vanilla Tracr models are sparse and have orthogonal features, this process can be viewed as learning the projection from a "hypothetical disentangled model" to the "observed model" described by Elhage et al. (2022b).

We want the compressed model to minimise loss under the constraint that it implements the same computation as the original model. We train $W$ to minimise $\mathbb{E}_{x}\left[\mathcal{L}_{\text {out }}(W, x)+\mathcal{L}_{\text {layer }}(W, x)\right]$, where

$$
\mathcal{L}_{\text {out }}=\operatorname{loss}\left(f(x), \hat{f}_{W}(x)\right) ; \quad \mathcal{L}_{\text {layer }}=\sum_{\text {layer } i}\left(h_{i}(x)-\hat{h}_{W, i}(x)\right)^{2}
$$

Here, $f(x)$ is the output of the compiled model for input $x, \hat{f}_{W}(x)$ is the output of the compressed model, and $h_{i}(x)$ and $\hat{h}_{W, i}(x)$ are the output vectors at layer $i$ of the respective models.

For categorical outputs, $\mathcal{L}_{\text {out }}$ is the softmax cross-entropy loss, whereas, for numerical outputs, it is the mean-squared error. $\mathcal{L}_{\text {layer }}$ is a regularization term that incentives the compressed model to match the per-layer outputs of the original model. To minimise this loss, the compressed model will have to approximate the computation of the original model but with a smaller residual stream. We give both loss terms equal weight, but we found other weighting factors give similar results in practice.

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-08.jpg?height=361&width=1295&top_left_y=253&top_left_x=423)

Figure 6: Training setup for compressing a compiled transformer model. At each layer, we use the same matrix $W \in \mathbb{R}^{D \times d}$ to embed the disentangled $D$-dimensional residual stream to $d \leq D$ dimensions. We freeze the layer weights and only train $W$ to compress the model.

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-08.jpg?height=342&width=1390&top_left_y=810&top_left_x=365)

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-08.jpg?height=241&width=333&top_left_y=861&top_left_x=384)

(a) Training loss

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-08.jpg?height=263&width=352&top_left_y=839&top_left_x=735)

(b) Output loss vs. $d$

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-08.jpg?height=282&width=306&top_left_y=821&top_left_x=1105)

(c) SGD Compression

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-08.jpg?height=285&width=334&top_left_y=817&top_left_x=1405)

(d) PCA

Figure 7: Compressing the frac_prevs model Figure 2. (a) shows the loss during training for different embedding sizes $d$ and (b) shows the final loss for different embedding sizes $d$. After about $d=6$ the compressed model solves the task essentially as well as the original compiled model which uses $D=14$ dimensions. (c) shows $W^{T} W$ for the compression procedure described in Section 5 with $d=8$ where $W$ is the learned compression matrix. As a comparison, (d) shows the same plot for applying PCA and retaining only the first 8 components. In contrast to PCA, our compression procedure produces a compression matrix $W$ that retains features necessary for the task (e.g., is_x and frac_prevs) and discards features that are unimportant (e.g., tokens:a).

We could set up this compression in other ways. For example, we could use a different projection at each layer, use different matrices for embedding and unembedding, or modify weights other than $W$ when compressing the model. These design choices come with a tradeoff between making the model more expressible and potentially more realistic and enforcing the ground truth computation. For simplicity, we use a shared $W$ for embedding/unembedding at every layer, and we already observe a rich structure in models compressed with this procedure.

Appendix E contains more details on the training setup, hyperparameters, and resources used.

### 5.2 What Does the Compression Learn?

As our first case study, Figure 7 shows the example model from Figure 2, that computes the fraction of token " $\mathrm{x}$ ". By learning an embedding matrix $W$, we can reduce the residual dimension from $D=14$ to $d=6$ without hurting performance (cf Figure 7(b)). Once we reduce $d$ further, the model's performance starts to suffer.

To understand the compression better, we can study how $W$ embeds the original $D$ features in $d<D$ dimensions. We can only do this because we started with a compiled model with known features. Figure 7 shows $W^{T} W$ for compressing the model to $d=8$. We can compare this to using principle component analysis (PCA) to compress the model. To interpret the results, we need to use our knowledge of the algorithm the model implements. The input tokens: $\mathrm{x}$ and the variables is_x and frac_prevs are crucial for computing the fraction of tokens that is " $x$ ", and we find that these variables mostly get separate dimensions in the compressed residual stream. The other input tokens stored in tokens:a, tokens:b, tokens:c are not necessary for solving the task, and so they are discarded in the compressed model. Other variables, such as the indices embeddings, are stored in non-orthogonal dimensions in the compressed space. This is consistent with existing findings on superposition as the indices embeddings are sparse and do not occur together (Elhage et al., 2022b).

![](https://cdn.mathpix.com/cropped/2024_06_04_5ad98258fd1a8b904412g-09.jpg?height=558&width=830&top_left_y=252&top_left_x=298)

Figure 8: We compress sort_unique (Figure 5). The plots on the right show that the compressed model achieves nearly perfect accuracy, but the layer outputs of the compressed model are different from the original compiled model. The left plot shows the average layer outputs of the compiled model, the compressed model, and the squared difference. The compressed model seems to learn to use a different (numerical) encoding for the target_pos variable, which causes the discrepancy.

However, our results go beyond previous work on superposition. Tracr models often have multiple variables that depend on each other and encode shared information. For example, in frac_prevs, the is_ $\mathrm{x}$ variable is an indicator that essentially contains the same information as the input dimension tokens: $x^{4}$ In Figure 7, we see that the embeddings of is_x and tokens: $\mathrm{x}$ share part of the embedding space. Intuitively, this occurs because the variables encode similar information.

Future experiments could aim to further clarify the effect of shared information between variables on superposition. Tracr provides, for the first time, a setting to systematically study superposition in transformer models that implement nontrivial algorithms.

### 5.3 Do the Compressed Models Still Implement the Same Computation?

Even if the compressed models successfully achieve a low loss, we need to check if they implement the same computation as the compiled models, or else we would no longer know the ground truth mechanisms the models implement. To this end, we evaluate the average cosine similarity between the output at each layer of the two models. Values far from 1 suggest the compressed model is structured differently from the base model.

We find that for some models the cosine similarity stays substantially below 1 even as the compressed model gets close to $100 \%$ in accuracy. For example, Figure 8 shows results from compressing the sort_unique model. Here, the compressed model achieves almost perfect accuracy on the task, but the average cosine similarity of the outputs at individual layers stays around 0.8 , far shy of 1 .

By inspecting the models' outputs at each layer, we can attribute the error to the target_pos variable. In the compiled model, target_pos is encoded as a one-hot vector. However, the compiled model only uses a single dimension. This suggests that the compressed model moves the tokens to the target position with a numerical encoding of the target position rather than a categorical encoding.

This difference in encodings shows that even with a fairly restrictive compression setup, compressed models may not stay faithful to the original RASP programs. This is both a setback for adding compression to the compiler-the compiler's annotations no longer serve as the exact ground truth-but also an opportunity. The ways neural networks solve algorithmic tasks regularly surprise researchers (Nanda et al., 2023). Studying such discrepancies could be a way to learn more about the ways NNs naturally represent certain computations without reverse-engineering entire models.

## 6 Related Work

There are many approaches to interpretability in machine learning (Carvalho et al., 2019), and in language models specifically (Danilevsky et al., 2020; Belinkov and Glass, 2019; Rogers et al., 2020). In this paper, we focus on interpretability in the sense of giving a faithful (Jacovi and Goldberg, 2020) and detailed account of the mechanisms learned by a model, sometimes called mechanistic interpretability (Olah, 2022) or transparency (Räukur et al., 2023).[^2]

Mechanistic interpretability has been used to reverse engineer circuits in state-of-the-art vision models (Cammarata et al., 2020), small transformer models trained on toy tasks (Olsson et al., 2022; Nanda et al., 2023), and medium-sized language models (Wang et al., 2023). Reverse-engineered circuits can be used as more realistic alternative to compiled models. However, they are laborintensive to identify, and our knowledge of them can be incomplete or inaccurate even when they are analysed carefully. For example, Chan et al. (2022) show that the "induction head" hypothesis by Olsson et al. (2022) needs to be modified to adequately explain in-context learning performance even in small attention-only transformers.

While Tracr is based on RASP (Weiss et al., 2021), there are potential alternatives for constructing transformer models. Wei et al. (2022) and Akyürek et al. (2023) study more general computational models for transformers. Based on this line of work, Giannou et al. (2023) propose a Turing-complete model for constructing transformers, whereas RASP might have limited expressibility (Weiss et al., 2021; Merrill et al., 2022). However, the work by Giannou et al. (2023) is purely theoretical, and the practical cost-benefit trade-off between their approach and our RASP-based approach is unclear.

Evaluation is a perennial topic of debate in interpretability, and there is little consensus on the best approach (Lipton, 2018; Yang et al., 2019; Mohseni et al., 2021). We hope that compiled models contribute a new perspective to this discussion and can complement other evaluation methods.

Our approach is closest to prior work trying to create a ground truth for evaluating interpretability, via careful manipulation of the training mechanism and dataset. Yang and Kim (2019) and Adebayo et al. (2020) introduce label correlations to the background of images, and Zhou et al. (2022) use label reassignments to achieve a similar goal. However, these approaches focus on convolutional image classification models, and they can only modify part of a model to have a ground truth interpretation. Tracr, on the other hand, creates transformer models that implement fully human-readable code.

Since releasing an early version of our work, Conmy et al. (2023) successfully used Tracr to evaluate a method for automatically detecting circuits in transformer models, and Friedman et al. (2023) built on Tracr and studied learning transformer programs instead of manually writing them.

## 7 Discussion \& Conclusion

We proposed to compile human-readable programs to neural network weights as a testbed for developing and evaluating interpretability tools. To this end, we introduced Tracr which compiles human-readable code to the weights of a transformer model.

Applications. Compiled transformer models can be broadly useful for accelerating interpretability research. We highlight four usecases that could be particularly useful. First, we can use Tracr to create test cases and ultimately benchmarks for interpretbility tools. This can help to confirm methods work as expected and surface potential failure modes. Second, we can measure our understanding of a model by manually replacing components of it with compiled components (similar to Nanda et al. (2023)). Over time, the research community could build a library of programs that represent our understanding of what neural networks learn. Third, we can use compiled models to isolate and study phenomena that occur in real neural networks. Our study of superposition in Section 5 demonstrates the benefits of studying an isolated phenomenon in a model we otherwise fully understand. Finally, compiled models can help us understand how transformers can implement certain algorithms and improve our ability to form concrete intuitions and hypotheses about models we want to interpret. Appendix A. 1 discusses these applications in more detail.

Limitations. RASP and Tracr have important limitations in terms of expressivity, efficiency and realism compared to real transformer models. While many limitations can be overcome in future versions, some are fundamental to using compiled models. Clearly, we will likely never compile fully featured language models in Tracr. Therefore, we should interpret experiments conducted on compiled models carefully, and treat evaluations based on them as a minimum bar rather than a full validation of a technique. Appendix A. 2 discusses these limitations in detail.

Despite these limitations, we think Tracr provides a promising new approach to studying transformers and to evaluating interpretability tools. The current approach to doing interpretability research is similar to trying to invent a microscope lens without ever being able to point it at familiar, wellunderstood shapes. Tracr enables researchers to point their interpretability methods at models they fully understand to calibrate, evaluate, and improve the methods.

## Acknowledgements

We thank Avraham Ruderman, Jackie Kay, Michela Paganini, Tom Lieberum, and Geoffrey Irving for valuable discussions, Victoria Krakovna and Marlene Staib for collaborating on early experiments with compiling RASP, and Chris Olah and Tristan Hume for feedback on an early draft of this paper. We thank the LessWrong user "Gurkenglas" for pointing out a mistake in an earlier draft of the way to implement selectors combined with and described in Appendix F.

## Author Contributions

VM proposed the initial idea for Tracr and wrote our RASP implementation. DL, VM, JK and MR designed and developed Tracr. DL designed, implemented, and ran the compression experiments in Section 5. MR wrote documentation and led the open-sourcing process. JK derived the theoretical results in Appendix F. TM and VM advised on research direction. DL, SF, and VM wrote the manuscript. DL led the project.

## References

J. Adebayo, M. Muelly, I. Liccardi, and B. Kim. Debugging tests for model explanations. In Advances in Neural Information Processing Systems, 2020.

M. Aharon, M. Elad, and A. Bruckstein. K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):4311-4322, 2006.

E. Akyürek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is incontext learning? Investigations with linear models. In International Conference on Learning Representations (ICLR), 2023.

D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba. Network dissection: Quantifying interpretability of deep visual representations. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

Y. Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207-219, 2022.

Y. Belinkov and J. Glass. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49-72, 2019.

N. Cammarata, S. Carter, G. Goh, C. Olah, M. Petrov, L. Schubert, C. Voss, B. Egan, and S. K. Lim. Thread: Circuits. Distill, 2020. URL https://distill.pub/2020/circuits.

D. V. Carvalho, E. M. Pereira, and J. S. Cardoso. Machine learning interpretability: A survey on methods and metrics. Electronics, 8(8):832, 2019.

L. Chan, A. Garriga-Alonso, N. Goldwosky-Dill, R. Greenblatt, J. Nitishinskaya, A. Radhakrishnan, B. Shlegeris, and N. Thomas. Causal scrubbing, a method for rigorously testing interpretability hypotheses. AI Alignment Forum, 2022. https://www.alignmentforum.org/posts/ JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing.

A. Conmy, A. N. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. In Advances in Neural Information Processing Systems, 2023.

M. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, and P. Sen. A survey of the state of explainable AI for natural language processing. AACL-IJCNLP 2020, 2020.

D. L. Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-1306, 2006.

N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. URL https://transformer-circuits.pub/2021/framework/index.html.

N. Elhage, T. Hume, C. Olsson, N. Nanda, T. Henighan, S. Johnston, S. ElShowk, N. Joseph, N. DasSarma, B. Mann, D. Hernandez, A. Askell, K. Ndousse, A. Jones, D. Drain, A. Chen, Y. Bai, D. Ganguli, L. Lovitt, Z. Hatfield-Dodds, J. Kernion, T. Conerly, S. Kravec, S. Fort, S. Kadavath, J. Jacobson, E. Tran-Johnson, J. Kaplan, J. Clark, T. Brown, S. McCandlish, D. Amodei, and C. Olah. Softmax linear units. Transformer Circuits Thread, 2022a. URL https://transformer-circuits.pub/2022/solu/index.html.

N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and C. Olah. Toy models of superposition. Transformer Circuits Thread, 2022b. URL https: //transformer-circuits.pub/2022/toy_model/index.html.

D. Friedman, A. Wettig, and D. Chen. Learning transformer programs. In Advances in Neural Information Processing Systems, 2023.

A. Giannou, S. Rajput, J.-y. Sohn, K. Lee, J. D. Lee, and D. Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning (ICML), 2023.

T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL http: //github.com/deepmind/dm-haiku.

A. Jacovi and Y. Goldberg. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.

M. L. Leavitt and A. Morcos. Towards falsifiable interpretability research. In NeurIPS Workshop: ML Retrospectives, Surveys \& Meta-Analyses (ML-RSA), 2020.

Z. C. Lipton. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue, 16(3):31-57, 2018.

K. Meng, D. Bau, A. J. Andonian, and Y. Belinkov. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems, 2022.

W. Merrill, A. Sabharwal, and N. A. Smith. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843-856, 2022.

S. Mohseni, N. Zarei, and E. D. Ragan. A multidisciplinary survey and framework for design and evaluation of explainable AI systems. ACM Transactions on Interactive Intelligent Systems, 11 (3-4):1-45, 2021 .

N. Nanda, L. Chan, T. Liberum, J. Smith, and J. Steinhardt. Progress measures for grokking via mechanistic interpretability. In International Conference on Learning Representations (ICLR), 2023.

I. E. Nielsen, D. Dera, G. Rasool, R. P. Ramachandran, and N. C. Bouaynaya. Robust explainability: A tutorial on gradient-based attribution methods for deep neural networks. IEEE Signal Processing Magazine, 39(4):73-84, 2022.

C. Olah. Mechanistic interpretability, variables, and the importance of interpretable bases. 2022.

C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/ in-context-learning-and-induction-heads/index.html.

T. Räukur, A. Ho, S. Casper, and D. Hadfield-Menell. Toward transparent AI: A survey on interpreting the inner structures of deep neural networks. In IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), 2023.

A. Rogers, O. Kovaleva, and A. Rumshisky. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842-866, 2020.

A. Scherlis, K. Sachan, A. S. Jermyn, J. Benton, and B. Shlegeris. Polysemanticity and capacity in neural networks. arXiv preprint arXiv:2210.01892, 2022.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.

K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In International Conference on Learning Representations (ICLR), 2023.

C. Wei, Y. Chen, and T. Ma. Statistically meaningful approximation: a case study on approximating Turing machines with transformers. In Advances in Neural Information Processing Systems, 2022.

G. Weiss, Y. Goldberg, and E. Yahav. Thinking like transformers. In International Conference on Machine Learning (ICML), 2021.

F. Yang, M. Du, and X. Hu. Evaluating explanation without ground truth in interpretable machine learning. arXiv preprint arXiv:1907.06831, 2019.

M. Yang and B. Kim. Benchmarking attribution methods with relative feature importance. arXiv preprint arXiv:1907.09701, 2019.

Y. Zhou, S. Booth, M. T. Ribeiro, and J. Shah. Do feature attribution methods correctly attribute features? In AAAI Conference on Artificial Intelligence, 2022.
