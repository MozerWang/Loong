# MA2QL: A Minimalist Approach to Fully Decentralized Multi-Agent Reinforcement Learning 

Kefan Su $^{1}$ Siyuan Zhou ${ }^{1}$ Jiechuan Jiang ${ }^{1}$ Chuang Gan ${ }^{2}$ Xiangjun Wang ${ }^{3}$ Zongqing Lu ${ }^{1}$<br>${ }^{1}$ Peking University $\quad{ }^{2}$ MIT-IBM Watson AI Lab $\quad{ }^{3}$ inspir.ai


#### Abstract

Decentralized learning has shown great promise for cooperative multi-agent reinforcement learning (MARL). However, non-stationarity remains a significant challenge in fully decentralized learning. In the paper, we tackle the non-stationarity problem in the simplest and fundamental way and propose multi-agent alternate Q-learning (MA2QL), where agents take turns updating their Q-functions by Qlearning. MA2QL is a minimalist approach to fully decentralized cooperative MARL but is theoretically grounded. We prove that when each agent guarantees $\varepsilon$-convergence at each turn, their joint policy converges to a Nash equilibrium. In practice, MA2QL only requires minimal changes to independent Q-learning (IQL). We empirically evaluate MA2QL on a variety of cooperative multi-agent tasks. Results show MA2QL consistently outperforms IQL, which verifies the effectiveness of MA2QL, despite such minimal changes.


## 1 Introduction

Cooperative multi-agent reinforcement learning (MARL) is a well-abstracted model for a broad range of real applications, including logistics (Li et al., 2019), traffic signal control (Xu et al., 2021), power dispatch (Wang et al., 2021b), and inventory management (Feng et al., 2022). In cooperative MARL, centralized training with decentralized execution (CTDE) is a popular learning paradigm, where the information of all agents can be gathered and used in training. Many CTDE methods (Lowe et al., 2017; Foerster et al., 2018; Sunehag et al., 2018; Rashid et al., 2018; Wang et al., 2021a; Zhang et al., 2021; Su \& Lu, 2022; Li et al., 2022; Wang et al., 2022) have been proposed and shown great potential to solve cooperative multi-agent tasks.

Another paradigm is decentralized learning, where each agent learns its policy based on only local information. Decentralized learning is less investigated but desirable in many scenarios where the information of other agents is not available, and for better robustness, scalability, and security (Zhang et al., 2019). However, fully decentralized learning of agent policies (i.e., without communication) is still an open challenge in cooperative MARL.

The most straightforward way for fully decentralized learning is directly applying independent learning at each agent (Tan, 1993), which however induces the well-known non-stationarity problem for all agents (Zhang et al., 2019) and may lead to learning instability and a non-convergent joint policy, though the performance varies as shown in empirical studies (Rashid et al., 2018; de Witt et al., 2020; Papoudakis et al., 2021; Yu et al., 2021).

In the paper, we directly tackle the non-stationarity problem in the simplest and fundamental way, i.e., fixing the policies of other agents while one agent is learning. Following this principle, we propose multi-agent alternate $Q$-learning (MA2QL), a minimalist approach to fully decentralized cooperative multi-agent reinforcement learning, where agents take turns to update their policies by Q-learning. MA2QL is theoretically grounded and we prove that when each agent guarantees $\varepsilon$-convergence at each turn, their joint policy converges to a Nash equilibrium. In practice, MA2QL only requires minimal changes to independent Q-learning (IQL) (Tan, 1993; Tampuu et al., 2015) and also independent DDPG (Lillicrap et al., 2016) for continuous action, i.e., simply swapping the order of two lines of codes as follows.

```
IQL
repeat
    all agents interact in the environment
    for }i\leftarrow1,n\mathrm{ do
        agent i updates by Q-learning
    end for
until terminate
```

```
MA2QL
repeat
    for }i\leftarrow1,n\mathrm{ do
        all agents interact in the environment
        agent i updates by Q-learning
    end for
until terminate
```

We evaluate MA2QL on a didactic game to empirically verify its convergence, and multi-agent particle environments (Lowe et al., 2017), multi-agent MuJoCo (Peng et al., 2021), and StarCraft multi-agent challenge (Samvelyan et al., 2019) to verify its performance with discrete and continuous action spaces, and fully and partially observable environments. We find that MA2QL consistently outperforms IQL, despite such minimal changes. This empirically verifies the effectiveness of alternate learning. The superiority of MA2QL over IQL suggests that simpler approaches may have been left underexplored for fully decentralized cooperative multi-agent reinforcement learning. We envision this work could provide some insights to further studies of fully decentralized learning.

## 2 Background

### 2.1 Preliminaries

Dec-POMDP. Decentralized partially observable Markov decision process (Dec-POMDP) is a general model for cooperative MARL. A Dec-POMDP is a tuple $M=\{S, A, P, Y, O, I, n, r, \gamma\}$. $S$ is the state space, $n$ is the number of agents, $\gamma \in[0,1)$ is the discount factor, and $I=\{1,2 \cdots n\}$ is the set of all agents. $A=A_{1} \times A_{2} \times \cdots \times A_{n}$ represents the joint action space where $A_{i}$ is the individual action space for agent $i . P\left(s^{\prime} \mid s, \boldsymbol{a}\right): S \times A \times S \rightarrow[0,1]$ is the transition function, and $r(s, \boldsymbol{a}): S \times A \rightarrow \mathbb{R}$ is the reward function of state $s$ and joint action $\boldsymbol{a}$. $Y$ is the observation space, and $O(s, i): S \times I \rightarrow Y$ is a mapping from state to observation for each agent. The objective of DecPOMDP is to maximize $J(\boldsymbol{\pi})=\mathbb{E}_{\boldsymbol{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}, \boldsymbol{a}_{t}\right)\right]$, and thus we need to find the optimal joint policy $\boldsymbol{\pi}^{*}=\arg \max _{\boldsymbol{\pi}} J(\boldsymbol{\pi})$. To settle the partial observable problem, history $\tau_{i} \in \mathcal{T}_{i}:\left(Y \times A_{i}\right)^{*}$ is often used to replace observation $o_{i} \in Y$. Each agent $i$ has an individual policy $\pi_{i}\left(a_{i} \mid \tau_{i}\right)$ and the joint policy $\pi$ is the product of each $\pi_{i}$. Though the individual policy is learned as $\pi_{i}\left(a_{i} \mid \tau_{i}\right)$ in practice, as Dec-POMDP is undecidable (Madani et al., 1999) and the analysis in partially observable environments is much harder, we will use $\pi_{i}\left(a_{i} \mid s\right)$ in analysis and proofs for simplicity.

Dec-MARL. Although decentralized cooperative multi-agent reinforcement learning (Dec-MARL) has been previously investigated (Zhang et al., 2018; de Witt et al., 2020), the setting varies across these studies. In this paper, we consider Dec-MARL as a fully decentralized solution to DecPOMDP, where each agent learns its policy/Q-function from its own action individually without communication or parameter-sharing. Therefore, in Dec-MARL, each agent $i$ actually learns in the environment with transition function $P_{i}\left(s^{\prime} \mid s, a_{i}\right)=\mathbb{E}_{a_{-i} \sim \pi_{-i}}\left[P\left(s^{\prime} \mid s, a_{i}, a_{-i}\right)\right]$ and reward function $r_{i}\left(s, a_{i}\right)=\mathbb{E}_{a_{-i} \sim \pi_{-i}}\left[r\left(s, a_{i}, a_{-i}\right)\right]$, where $\pi_{-i}$ and $a_{-i}$ respectively denote the joint policy and joint action of all agents expect $i$. As other agents are also learning (i.e., $\pi_{-i}$ is changing), from the perspective of each individual agent, the environment is non-stationary. This is the non-stationarity problem, the main challenge in Dec-MARL.

IQL. Independent Q-learning (IQL) is a straightforward method for Dec-MARL, where each agent $i$ learns a Q-function $Q\left(s, a_{i}\right)$ by Q-learning. However, as all agents learn simultaneously, there is no theoretical guarantee on convergence due to non-stationarity, to the best of our knowledge. In practice, IQL is often taken as a simple baseline in favor of more elaborate MARL approaches, such as value-based CTDE methods (Rashid et al., 2018; Son et al., 2019). However, much less attention has been paid to IQL itself for Dec-MARL.

### 2.2 Multi-Agent Alternate Policy Iteration

To address the non-stationarity problem in Dec-MARL, a fundamental way is simply to make the environment stationary during the learning of each agent. Following this principle, we let agents learn by turns; in each turn, one agent performs policy iteration while fixing the policies of other agents.

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-03.jpg?height=393&width=1263&top_left_y=270&top_left_x=428)

Figure 1: Illustration of multi-agent alternate policy iteration (upper panel) and multi-agent alternate $Q$-iteration (lower panel) of three agents. As essentially the MDP differs at different turns of each agent, policy iteration/Q-iteration of each agent iterates over different MDPs.

This procedure is referred to as multi-agent alternate policy iteration. As illustrated in Figure 1, multi-agent alternate policy iteration differs from policy iteration in single-agent RL. In single-agent RL, policy iteration is performed on the same MDP. However, here, for each agent, policy iteration at a different round is performed on a different MDP. As $\pi_{-i}$ is fixed at each turn, $P_{i}\left(s^{\prime} \mid s, a_{i}\right)$ and $r_{i}\left(s, a_{i}\right)$ are stationary and we can easily have the following lemma.

Lemma 1 (multi-agent alternate policy iteration). If all agents take turns to perform policy iteration, their joint policy sequence $\{\pi\}$ monotonically improves and converges to a Nash equilibrium.

Proof. In each turn, as the policies of other agents are fixed, the agent $i$ has the following update rule for policy evaluation,

$$
\begin{equation*}
Q_{\pi_{i}}\left(s, a_{i}\right) \leftarrow r_{i}\left(s, a_{i}\right)+\gamma \mathbb{E}_{s^{\prime} \sim P_{i}, a_{i}^{\prime} \sim \pi_{i}}\left[Q_{\pi_{i}}\left(s^{\prime}, a_{i}^{\prime}\right)\right] \tag{1}
\end{equation*}
$$

We can have the convergence of policy evaluation in each turn by the standard results (Sutton \& Barto, 2018). Moreover, as $\pi_{-i}$ is fixed, it is straightforward to have

$$
\begin{equation*}
Q_{\pi_{i}}\left(s, a_{i}\right)=\mathbb{E}_{a_{-i} \sim \pi_{-i}}\left[Q_{\pi}\left(s, a, a_{i}\right)\right] \tag{2}
\end{equation*}
$$

Then, the agent $i$ performs policy improvement by

$$
\begin{equation*}
\pi_{i}^{\text {new }}(s)=\arg \max _{a_{i}} \mathbb{E}_{\pi_{-i}^{\text {old }}}\left[Q_{\pi^{\text {old }}}\left(s, a_{i}, a_{-i}\right)\right] \tag{3}
\end{equation*}
$$

As the policies of other agents are fixed (i.e., $\pi_{-i}^{\text {new }}=\pi_{-i}^{\text {old }}$ ), we have

$$
\begin{align*}
V_{\boldsymbol{\pi}^{\text {old }}}(s) & =\mathbb{E}_{\boldsymbol{\pi}^{\text {old }}}\left[Q_{\boldsymbol{\pi}^{\text {old }}}\left(s, a_{i}, a_{-i}\right)\right]=\mathbb{E}_{\pi_{i}^{\text {old }}} \mathbb{E}_{\pi_{-i}^{\text {old }}}\left[Q_{\boldsymbol{\pi}^{\text {old }}}\left(s, a_{i}, a_{-i}\right)\right] \\
& \leq \mathbb{E}_{\pi_{i}^{\text {new }}} \mathbb{E}_{\pi_{-i}^{\text {old }}}\left[Q_{\boldsymbol{\pi}^{\text {old }}}\left(s, a_{i}, a_{-i}\right)\right]=\mathbb{E}_{\pi_{i}^{\text {new }}} \mathbb{E}_{\pi_{-i}^{\text {new }}}\left[Q_{\boldsymbol{\pi}^{\text {old }}}\left(s, a_{i}, a_{-i}\right)\right]  \tag{4}\\
& =\mathbb{E}_{\boldsymbol{\pi}^{\text {new }}}\left[Q_{\boldsymbol{\pi}^{\text {old }}}\left(s, a_{i}, a_{-i}\right)\right]=\mathbb{E}_{\boldsymbol{\pi}^{\mathrm{new}}}\left[r\left(s, a_{i}, a_{-i}\right)+\gamma V_{\boldsymbol{\pi}_{\text {old }}}\left(s^{\prime}\right)\right] \\
& \leq \cdots \leq V_{\boldsymbol{\pi}^{\text {new }}}(s)
\end{align*}
$$

where the first inequality is from (3). This proves that the policy improvement of agent $i$ in each turn also improves the joint policy. Thus, as agents perform policy iteration by turn, the joint policy sequence $\{\boldsymbol{\pi}\}$ improves monotonically, and $\{\boldsymbol{\pi}\}$ will converge to a Nash equilibrium since no agents can improve the joint policy unilaterally at convergence.

Lemma 1 is simple but useful, which is also reached independently and implicitly by existing studies in different contexts (Fang et al., 2019; Bertsekas, 2020). Moreover, Lemma 1 immediately indicates an approach with the convergence guarantee for Dec-MARL and also tells us that if we find the optimal policy for agent $i$ in each round $k$ given the other agents' policies $\pi_{-i}^{k}$, then the joint policy will obtain the largest improvement. This result can be formulated as follows,

$$
\begin{align*}
& \pi_{i}^{*, k}=\arg \max _{\pi_{i}} \mathbb{E}_{\pi_{-i}^{k}}\left[Q_{\pi_{i}, \pi_{-i}^{k}}\left(s, a_{i}, a_{-i}\right)\right]  \tag{5}\\
& V_{\pi_{i}, \pi_{-i}^{k}}(s) \leq V_{\pi_{i}^{*, k}, \pi_{-i}^{k}}(s) \quad \forall \pi_{i}, \forall s
\end{align*}
$$

We could obtain this $\pi_{i}^{*, k}$ by policy iteration with many on-policy iterations. However, such a method will face the issue of sample inefficiency which may be amplified in MARL settings. We will use Q-iteration to settle this problem as in Fang et al. (2019); Bertsekas (2020). However, unlike these studies, we propose to truncate $\mathrm{Q}$-iteration for fast learning but with the same theoretical guarantee.

## 3 Method

To address the problem of multi-agent alternate policy iteration, we propose multi-agent alternate $Q$-iteration, which is sufficiently truncated for fast learning but still has the same theoretical guarantee. Further, based on multi-agent alternate Q-iteration, we derive multi-agent alternate $Q$-learning, which makes the minimal change to IQL to form a simple yet effective value-based decentralized learning method for cooperative MARL.

### 3.1 Multi-Agent Alternate Q-Iteration

Instead of policy iteration, we let agents perform Q-iteration by turns as depicted in Figure 1. Let $\mathcal{M}_{i}^{k}=\left\{P_{i}^{k}, r_{i}^{k}\right\}$ denote the MDP of agent $i$ in round $k$, where we have $\mathcal{M}_{i}^{k} \neq \mathcal{M}_{i}^{k-1}$ unless $\pi_{-i}$ has converged, and $Q_{i}^{t, k}\left(s, a_{i}\right)$ denote the Q-function of agent $i$ with $t$ updates in the round $k$. We define the $\mathrm{Q}$-iteration as follows,

$$
\begin{equation*}
Q_{i}^{t+1, k}\left(s, a_{i}\right) \leftarrow r_{i}^{k}\left(s, a_{i}\right)+\gamma \mathbb{E}_{s^{\prime} \sim P_{i}^{k}}\left[\max _{a_{i}^{\prime}} Q_{i}^{t, k}\left(s^{\prime}, a_{i}^{\prime}\right)\right] \tag{6}
\end{equation*}
$$

Then, the sequence $\left\{Q_{i}^{t, k}\right\}$ converges to $Q_{i}^{*, k}$ with respect to the $\operatorname{MDP} \mathcal{M}_{i}^{k}=\left\{P_{i}^{k}, r_{i}^{k}\right\}$, and we have the following lemma and corollary.

Lemma 2 ( $\varepsilon$-convergent $Q$-iteration). By iteratively applying $Q$-iteration (6) at each agent $i$ for each turn, for any $\varepsilon>0$, we have

$$
\begin{equation*}
\left\|Q_{i}^{t, k}-Q_{i}^{*, k}\right\|_{\infty} \leq \varepsilon, \quad \text { when } t \geq \frac{\log ((1-\gamma) \varepsilon)-\log (2 R+2 \varepsilon)}{\log \gamma} \tag{7}
\end{equation*}
$$

where $R=\frac{r_{\max }}{1-\gamma}$ and $r_{\max }=\max _{s, a} r(s, \boldsymbol{a})$.

Corollary 1. For any $\varepsilon>0$, if we take sufficient $Q$-iteration $t_{i}^{k}$, i.e., $Q_{i}^{k}=Q_{i}^{t_{i}^{k}, k}$, then we have

$$
\left\|Q_{i}^{k}-Q_{i}^{*, k}\right\|_{\infty} \leq \varepsilon \quad \forall k, i
$$

With Lemma 1, Lemma 2, and Corollary 1, we have the following theorem.

Theorem 1 (multi-agent alternate $\mathrm{Q}$-iteration). Suppose that $Q_{i}^{*}(s, \cdot)$ has the unique maximum for all states and all agents. If all agents in turn take $Q$-iteration to $\left\|Q_{i}^{k}-Q_{i}^{*, k}\right\|_{\infty} \leq \varepsilon$, then their joint policy sequence $\left\{\boldsymbol{\pi}^{k}\right\}$ converges to a Nash equilibrium, where $\pi_{i}^{k}(s)=\arg \max _{a_{i}} Q_{i}^{k}\left(s, a_{i}\right)$.

All the proofs are included in Appendix A.

Theorem 1 assumes that for each agent, $Q_{i}^{*}$ has the unique maximum over actions for all states. Although this may not hold in general, in practice we can easily settle this by introducing a positive random noise to the reward function. Suppose the random noise is bounded by $\delta$, then we can easily derive that the performance drop of optimizing environmental reward plus noise is bounded by $\delta /(1-\gamma)$. As we can make $\delta$ arbitrarily small, the bound is tight. Moreover, as there might be many Nash equilibria, Theorem 1 does not guarantee the converged joint policy is optimal.

### 3.2 Multi-Agent Alternate Q-Learning

From Theorem 1, we know that if each agent $i$ guarantees $\varepsilon$-convergence to $Q_{i}^{*, k}$ in each round $k$, multi-agent alternate $\mathrm{Q}$-iteration also guarantees a Nash equilibrium of the joint policy. This immediately suggests a simple, practical fully decentralized learning method, namely multi-agent alternate Q-learning (MA2QL).

For learning Q-table or Q-network, MA2QL makes minimal changes to IQL.

- For learning Q-tables, all agents in turn update their Q-tables. At a round $k$ of an agent $i$, all agents interact in the environment, and the agent $i$ updates its Q-table a few times using the collected transitions $\left\langle s, a_{i}, r, s^{\prime}\right\rangle$.
- For learning Q-networks, all agents in turn update their Q-networks. At a round of an agent $i$, all agents interact in the environment, and each agent $j$ stores the collected transitions $\left\langle s, a_{j}, r, s^{\prime}\right\rangle$ into its replay buffer, and the agent $i$ updates its Q-network using sampled mini-batches from its replay buffer.

There is a slight difference between learning Q-table and Q-network. Strictly following multi-agent alternate Q-iteration, Q-table is updated by transitions sampled from the current MDP. On the other hand, Q-network is updated by mini-batches sampled from the replay buffer. If the replay buffer only contains the experiences sampled from the current MDP, learning Q-network also strictly follows multi-agent alternate $\mathrm{Q}$-iteration. However, in practice, we slightly deviate from that and allow the replay buffer to contain transitions of past MDPs, following IQL (Sunehag et al., 2018; Rashid et al., 2018; Papoudakis et al., 2021) for sample efficiency, the convergence may not be theoretically guaranteed though.

MA2QL and IQL can be simply summarized and highlighted as MA2QL agents take turns to update $Q$-functions by $Q$-learning, whereas IQL agents simultaneously update $Q$-functions by $Q$-learning.

## 4 Related Work

CTDE. The most popular learning paradigm in cooperative MARL is centralized training with decentralized execution (CTDE), including value decomposition and multi-agent actor-critic. For value decomposition (Sunehag et al., 2018; Rashid et al., 2018; Son et al., 2019; Wang et al., 2021a), a joint Q-function is learned in a centralized manner and factorized into local Q-functions to enable decentralized execution. For multi-agent actor-critic, a centralized critic, Q-function or V-function, is learned to provide gradients for local policies (Lowe et al., 2017; Foerster et al., 2018; Yu et al., 2021). Moreover, some studies (Wang et al., 2020; Peng et al., 2021; Su \& Lu, 2022) combine value decomposition and multi-agent actor-critic to take advantage of both, while others rely on maximum-entropy RL to naturally bridge the joint Q-function and local policies (Iqbal \& Sha, 2019; Zhang et al., 2021; Wang et al., 2022).

Decentralized learning. Another learning paradigm in cooperative MARL is decentralized learning, where the simplest way is for each agent to learn independently, e.g., independent Q-learning (IQL) or independent actor-critic (IAC). These methods are usually taken as simple baselines for CTDE methods. For example, IQL is taken as a baseline in value decomposition methods (Sunehag et al., 2018; Rashid et al., 2018), while IAC is taken as a baseline in multi-agent actor-critic (Foerster et al., 2018; Yu et al., 2021). Some studies further consider decentralized learning with communication (Zhang et al., 2018; Li et al., 2020), but they are not fully decentralized methods. More recently, IAC (i.e., independent PPO) has been empirically investigated and found remarkably effective in several cooperative MARL tasks (de Witt et al., 2020; Yu et al., 2021), including multi-agent particle environments (MPE) (Lowe et al., 2017) and StarCraft multi-agent challenge (SMAC). However, as actor-critic methods follow the principle different from Q-learning, we will not focus on IAC for comparison in the experiment. On the other hand, IQL has also been thoroughly benchmarked and its performance is close to CTDE methods in a few tasks (Papoudakis et al., 2021). This sheds some light on the potential of value-based decentralized cooperative MARL. Although there are some Q-learning variants, i.e., hysteretic Q-learning (Matignon et al., 2007) and lenient Q-learning (Palmer et al., 2018), for fully decentralized learning, they both are heuristic and their empirical performance is even worse than IQL (Zhang et al., 2020; Jiang \& Lu, 2022b). MA2QL may still be built on top of these Q-learning variants, e.g., the concurrent work (Jiang \& Lu, 2022b;a), which however requires a thorough study and is beyond the scope of this paper.

MA2QL is theoretically grounded for fully decentralized learning and in practice makes minimal changes to IQL. In the next section, we provide an extensive empirical comparison between MA2QL and IQL.

## 5 Experiments

In this section, we empirically study MA2QL on a set of cooperative multi-agent tasks, including a didactic game, multi-agent particle environments (MPE) (Lowe et al., 2017), multi-agent MuJoCo

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-06.jpg?height=257&width=349&top_left_y=278&top_left_x=365)

(a) Q-iteration

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-06.jpg?height=255&width=350&top_left_y=282&top_left_x=714)

(b) MA2QL

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-06.jpg?height=255&width=350&top_left_y=282&top_left_x=1061)

(c) Q-table updates

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-06.jpg?height=257&width=352&top_left_y=278&top_left_x=1404)

(d) samples

Figure 2: Empirical studies of MA2QL on the didactic game: (a) different numbers of Q-iterations performed by dynamic programming at each turn; (b) learning curve of MA2QL compared with IQL and the global optimum; (c) different numbers of Q-table updates at each turn; (d) different number of sampled transitions at each turn, where $\mathrm{x}$-axis is learning steps.

(Peng et al., 2021), and StarCraft multi-agent challenge (SMAC) (Samvelyan et al., 2019), to investigate the following questions.

## 1. Does MA2QL converge and what does it converge to empirically, compared with the optimal solution and IQL? How do the number of $Q$-function updates, exploration, and environmental stochasticity affect the convergence?

2. As MA2QL only makes the minimal changes to IQL, is MA2QL indeed better than IQL in both discrete and continuous action spaces, and in more complex tasks?

In all the experiments, the training of MA2QL and IQL is based on the same number of environmental steps (i.e., the same number of samples). Moreover, as the essential difference between MA2QL and IQL is that MA2QL agents take turns to update Q-function while IQL agents update Q-function simultaneously, for a fair comparison, the total number of $\mathrm{Q}$-function updates for each agent in MA2QL is set to be the same with that in IQL. For example, in a setting of $n$ agents, if IQL agents update Q-function $m$ steps (e.g., gradient steps) every environmental step, then each MA2QL agent updates its Q-function $n \times m$ steps each environmental step during its turn. For a training process of $T$ environmental steps, the number of updates for each IQL agent is $T \times m$, while for each MA2QL agent it is also $\frac{T}{n} \times n \times m$. For learning Q-networks, the size of the replay buffer is also the same for IQL and MA2QL. Moreover, we do not use parameter-sharing, which should not be allowed in decentralized settings (Terry et al., 2020) as a centralized entity is usually required to collect all the parameters. Detailed experimental settings, hyperparameters, and additional results are available in Appendix B, C, and D, respectively. All results are presented using the mean and standard deviation of five random seeds.

### 5.1 A Didactic Game

The didactic game is a cooperative stochastic game, which is randomly generated for the reward function and transition probabilities with 30 states, 3 agents, and 5 actions for each agent. Each episode in the game contains 30 timesteps. For comparison, we use dynamic programming to find the global optimal solution, denoted as OPTIMAL. For MA2QL and IQL, each agent independently learns a $30 \times 5$ Q-table.

First, we investigate how the number of Q-iterations empirically affects the convergence of multiagent alternate $\mathrm{Q}$-iteration, where $\mathrm{Q}$-iteration is performed by dynamic programming (full sweep over the state set) and denoted as MA2QL-DP. As shown in Figure 2(a), we can see that different numbers of $\mathrm{Q}$-iterations (i.e., $t=1,5,10,50$ ) that each agent takes at each turn do not affect the convergence in the didactic game, even when $t=1$. This indicates $\varepsilon$-convergence of $\mathrm{Q}$-iteration can be easily satisfied with as few as one iteration. Next, we compare the performance of MA2QL and IQL. As illustrated in Figure 2(b), IQL converges slowly (about 2000 steps), while MA2QL converges much faster (less than 100 steps) to a better return and also approximates OPTIMAL. Once again, MA2QL and IQL use the same number of samples and Q-table updates for each agent. One may notice that the performance of MA2QL is better than MA2QL-DP. This may be attributed to sampling and exploration of MA2QL, which induces a better Nash equilibrium. Then, we investigate MA2QL in terms of the number of Q-table updates at each turn, which resembles the number of Q-iterations by learning on samples. Specifically, denoting $K$ as the number of $\mathrm{Q}$-table updates, to update an agent, we repeat $K$ times of the process of sampling experiences and updating Q-table.
![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-07.jpg?height=322&width=1392&top_left_y=262&top_left_x=366)

Figure 3: Learning curves of MA2QL compared with IQL in the didactic game under different stochasticity levels, where p_trans is the probability of a state transitions to the next state uniformly at random and hence larger p_trans means a higher level of stochasticity.
![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-07.jpg?height=264&width=1380&top_left_y=724&top_left_x=367)

Figure 4: Learning curves of MA2QL compared with IQL in the didactic game under different exploration schemes, where decay_1M means $\epsilon$ decays from 1 to 0.02 over $10^{6}$ environment steps, similarly for others.

This means with a larger $K$, agents take turns less frequently. As shown in Figure 2(c), with larger $K$, the learning curve is more stair-like, which means in this game a small number $K$ is enough for convergence at each turn. Thus, with larger $K$, the learning curve converges more slowly. Last, we investigate how the number of collected transitions at each turn impacts the performance of MA2QL. As depicted in Figure 2(d), the performance of MA2QL is better with more samples. This is because the update of Q-learning using more samples is more like to induce a full iteration of Q-table.

Stochasticity. We investigate MA2QL under different stochasticity levels of the environment. We control the stochasticity level by introducing p_trans. For any transition, a state has the probability of p_trans to transition to next state uniformly at random, otherwise follows the original transition probability. Thus, larger p_trans means higher level of stochasticity. As illustrated in Figure 3, MA2QL outperforms IQL in all stochasticity levels.

Exploration. We further investigate the performance of MA2QL under different exploration schemes, including different decaying rates and constant $\epsilon$. As illustrated in Figure 4, MA2QL again outperforms IQL in all these exploration schemes. Note that decay_1M means $\epsilon$ decays from 1 to 0.02 over $10^{6}$ environment steps, similarly for others. These results show the robustness of MA2QL under various exploration schemes, even when $\epsilon=1$.

Update Order. Our theoretical result holds on any update order of agents, which means the order does not affect the convergence theoretically. Here we investigate the empirical performance of MA2QL under different pre-defined orders and the random order at each round. As there are three agents in this environment, there are essentially two different pre-defined orders. Suppose that three agents are indexed from 0 to 2 . The alternating update orders $[0,1,2],[1,2,0]$, and $[2,0,1]$ are the same, while $[0,2,1]$, $[1,0,2]$, and $[2,1,0]$ are the same. As illustrated in Figure 5, the performance of MA2QL is almost the same under the two orders (MA2QL is the order of $[0,1,2]$ ), which shows the robustness of MA2QL under different pre-defined orders. As for the random order at each round, the performance drops but is still better than IQL. One possible reason is that agents are not evenly updated due to the random order at each round, which may consequently induce a worse Nash equilibrium.

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-07.jpg?height=269&width=457&top_left_y=1839&top_left_x=1289)

Figure 5: Learning curves of MA2QL of different update orders. There are three agents in the didactic game, so there are essentially two different pre-defined update orders: $[0,1,2]$ (MA2QL) and $[0,2,1]$.

In the didactic game, we show that MA2QL consistently outperforms IQL under various settings. In the following experiments, for a fair comparison, the hyperparameters of IQL are well-tuned and we
![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-08.jpg?height=360&width=1184&top_left_y=270&top_left_x=470)

Figure 6: Learning curve of MA2QL compared with IQL in 5-agent simple spread, 5-agent line control, and 7-agent circle control in MPE, where x-axis is environment steps.
![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-08.jpg?height=358&width=1182&top_left_y=732&top_left_x=470)

Figure 7: Learning curve of MA2QL compared with IQL in $2 \times 3$ HalfCheetah, $3 \times 1$ Hopper, and $3 \times 2$ Walker2d in multi-agent MuJoCo, where x-axis is environment steps.

directly build MA2QL on top of IQL. As for the update order of MA2QL agents, we use a randomly determined order throughout a training process.

### 5.2 MPE

MPE is a popular environment in cooperative MARL. We consider three partially observable tasks: 5-agent simple spread, 5-agent line control, and 7-agent circle control (Agarwal et al., 2020), where the action space is set to discrete. Moreover, we use the sparse reward setting for these tasks, thus they are more difficult than the original ones. More details are available in Appendix B. For both IQL and MA2QL, Q-network is learned by DQN (Mnih et al., 2013).

Figure 6 shows the learning curve of MA2QL compared with IQL in these three MPE tasks. In simple spread and circle control, at the early training stage, IQL learns faster and better than MA2QL, but eventually MA2QL converges to a better joint policy than IQL. The converged performance of IQL is always worse than MA2QL, similar to that in the didactic game. Moreover, unlike the didactic game, simultaneous learning of IQL may also make the learning unstable even at the late training stage as in line control and circle control, where the episode rewards may decrease. On the other hand, learning by turns gradually improves the performance and converges to a better joint policy than IQL.

As MA2QL and IQL both use replay buffer that contains old experiences, why does MA2QL outperform IQL? The reason is that their experiences are generated in different manners. In the Q-learning procedure for each agent $i$, the ideal target is $y_{i}=r_{i}^{\pi}\left(s, a_{i}\right)+\lambda \mathbb{E}_{s^{\prime} \sim P_{i}^{\pi}\left(\cdot \mid s, a_{i}\right)}\left[\max _{a_{i}^{\prime}} Q_{i}\left(s^{\prime}, a_{i}^{\prime}\right)\right]$ and the practical target is $\tilde{y}_{i}=r_{i}^{\pi_{\mathrm{D}}}\left(s, a_{i}\right)+\lambda \mathbb{E}_{s^{\prime} \sim P_{i}^{\pi_{\mathrm{D}}}\left(\cdot \mid s, a_{i}\right)}\left[\max _{a_{i}^{\prime}} Q_{i}^{2}\left(s^{\prime}, a_{i}^{\prime}\right)\right]$, where $\pi_{\mathrm{D}}$ is the average joint policy for the experiences in the replay buffer. We then can easily obtain a bound for the target that $\left|y_{i}-\tilde{y}_{i}\right| \leq \frac{2-\gamma}{1-\gamma} r_{\max } D_{\mathrm{TV}}\left(\pi^{-i}(\cdot \mid s) \| \pi_{\mathrm{D}}^{-i}(\cdot \mid s)\right)$ where $r_{\max }=\max _{s, \boldsymbol{a}} r(s, \boldsymbol{a})$. We can then give an explanation from the aspect of the divergence between $\pi$ and $\pi_{\mathrm{D}}$. MA2QL obtains experiences with only one agent learning, so the variation for the joint policy is smaller than that of IQL. Thus, in general, the divergence between $\pi$ and $\pi_{\mathrm{D}}$ is smaller for MA2QL, which is beneficial to the learning.

### 5.3 Multi-Agent MuJoCo

Multi-agent MuJoCo has become a popular environment in cooperative MARL for continuous action space. We choose three robotic control tasks: $2 \times 3$ HalfCheetah, $3 \times 1$ Hopper, and $3 \times 2$ Walker2d.
![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-09.jpg?height=354&width=1180&top_left_y=275&top_left_x=470)

Figure 8: Learning curve of MA2QL compared with IQL on 3s_vs_4z (easy), 5m_vs_6m (hard) and corridor (super hard) in SMAC, where x-axis is environment steps.

To investigate continuous action space in both partially and fully observable environments, we configure $2 \times 3$ HalfCheetah and $3 \times 1$ Hopper as fully observable, and $3 \times 2$ Walker2d as partially observable. More details and results on multi-agent MuJoCo are available in Appendix B and D respectively. For both IQL and MA2QL, we use DDPG (Lillicrap et al., 2016) as the alternative to DQN to learn a Q-network and a deterministic policy for each agent to handle continuous action space. Here we abuse the notation a little and still denote them as IQL and MA2QL respectively.

In comparison to discrete action space, training multiple cooperative agents in continuous action space still remains challenging due to the difficulty of exploration and coordination in continuous action space. Thus, the evaluation on these multi-agent MuJoCo tasks can better demonstrate the effectiveness of decentralized cooperative MARL methods. As illustrated in Figure 7, in all the tasks, we find that MA2QL consistently and significantly outperforms IQL while IQL struggles. We believe the reason is that the robotic control tasks are much more dynamic than MPE and the non-stationarity induced by simultaneous learning of IQL may be amplified, which makes it hard for agents to learn effective and cooperative policies. On the other hand, alternate learning of MA2QL can deal with the non-stationarity and sufficiently stabilize the environment during the learning process, especially in HalfCheetah and Hopper, where MA2QL stably converges to much better performance than IQL. According to these experiments, we can verify the superiority of MA2QL over IQL in the continuous action space.

### 5.4 SMAC

SMAC is a popular partially observable environment for benchmarking cooperative MARL algorithms. SMAC has a much larger exploration space, where agents are much easy to get stuck in sub-optimal policies especially in the decentralized setting. We test our method on three representative maps for three difficulties: 3s_vs_4z (easy), 5m_vs_6m (hard), and corridor (super hard), where harder map has more agents. It is worth noting that we do not use any global state in the decentralized training and each agent learns on its own trajectory.

The results are shown in Figure 8. On the map 3s_vs_4z, IQL and MA2QL both converge to the winning rate of $100 \%$. However, on the hard and super hard map 5m_vs_6m and corridor, MA2QL achieves stronger than IQL. It is worth noting that the recent study (Papoudakis et al., 2021) shows that IQL performs well in SMAC, even close to CTDE methods like QMIX (Rashid et al., 2018). Here, we show that MA2QL can still outperform IQL in three maps with various difficulties, which indicates that MA2QL can also tackle the non-stationarity problem and bring performance gain in more complex tasks.

### 5.5 Hyperparameters and Scalability

We further investigate the influence of the hyperparameters on MA2QL and the scalability of MA2QL. First, we study the effect of $K$ (the number of Q-network updates at each turn) in the robotic control task: 3-agent Hopper. We consider $K=[4000,8000,40000]$. As shown in Figure 9(a), when $K$ is small, it outperforms IQL but still gets stuck in sub-optimal policies. On the contrary, if $K$ is large, different $K$ affects the efficiency of the learning, but not the final performance.

As discussed before, the hyperparameters are the same for IQL and MA2QL, which are well-tuned for IQL. To further study their effect on MA2QL, we conduct additional experiments in simple spread

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-10.jpg?height=344&width=417&top_left_y=283&top_left_x=407)

(a) effect of $K$

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-10.jpg?height=347&width=437&top_left_y=279&top_left_x=844)

(b) learning rate

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-10.jpg?height=347&width=439&top_left_y=279&top_left_x=1277)

(c) batch size

Figure 9: The effect of hyperparameters: (a) learning curves of MA2QL with different $K$ in $3 \times 1$ Hopper, compared with IQL; (b) learning curve of MA2QL compared with IQL with different learning rates in simple spread in MPE; (c) learning curve of MA2QL compared with IQL with different batch sizes in $3 \times 2$ HalfCheetah in multi-agent MuJoCo.

with different learning rates and in HalfCheetah with different batch sizes. As shown in Figure 9(b) and Figure 9(c), under these different hyperparameters, the performance of IQL and MA2QL varies, but MA2QL consistently outperforms IQL, which can be evidence of the gain of MA2QL over IQL is robust to the hyperparameters of IQL. The default learning rate in MPE is 0.0005 and the default batch size in multi-agent MuJoCo is 100 .

As for scalability, we additionally evaluate MA2QL in 15-agent simple spread in MPE and $17 \times 1$ Humanoid in multi-agent MuJoCo. As illustrated in Figure 10, MA2QL brings large performance gains over IQL in both tasks. More agents mean the environments become more complex and unstable for decentralized learning. IQL is easy to get stuck by the non-stationarity problem while MA2QL can handle it well. The results show again that
![](https://cdn.mathpix.com/cropped/2024_06_04_4a5ff9f1d953409b4ae6g-10.jpg?height=336&width=814&top_left_y=1117&top_left_x=924)

Figure 10: Learning curve of MA2QL compared with IQL in 15-agent simple spread in MPE and in $17 \times 1$ Humanoid in multi-agent MuJoCo. alternate learning of MA2QL can sufficiently stabilize the environment during the learning process. It also indicates the good scalability of MA2QL.

## 6 Conclusion and Discussion

In the paper, we propose MA2QL, a simple yet effective value-based fully decentralized cooperative MARL algorithm. MA2QL is theoretically grounded and requires minimal changes to independent Q-learning. Empirically, we verify MA2QL in a variety of cooperative multi-agent tasks, including a cooperative stochastic game, MPE, multi-agent MuJoco, and SMAC. The results show that, in spite of such minimal changes, MA2QL outperforms IQL in both discrete and continuous action spaces, fully and partially observable environments.

MA2QL makes minimal changes to IQL but indeed improves IQL. In practice, MA2QL can be easily realized by letting agents follow a pre-defined schedule for learning. However, such a schedule is convenient in practice. MA2QL also has the convergence guarantee, yet is limited to Nash equilibrium in tabular cases. As a Dec-POMDP usually has many Nash equilibria, the converged performance of MA2QL may not be optimal as shown in the stochastic game. Nevertheless, learning the optimal joint policy in fully decentralized settings is still an open problem. In the stochastic game, we see that IQL also converges, though much slower than MA2QL. This indicates that IQL may also have the convergence guarantee under some conditions, which however is not well understood. We believe fully decentralized learning for cooperative MARL is an important and open research area. However, much less attention has been paid to decentralized learning than centralized training with decentralized execution. This work may provide some insights to further studies of decentralized learning.

## References

Joshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018.

Akshat Agarwal, Sumit Kumar, Katia Sycara, and Michael Lewis. Learning transferable cooperative behavior in multi-agent teams. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2020.

Dimitri Bertsekas. Multiagent value iteration algorithms in dynamic programming and reinforcement learning. Results in Control and Optimization, 1:100003, 2020.

Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.

Xinyuan Fang, Tiankui Zhang, Yuanwei Liu, and Zhimin Zeng. Multi-agent cooperative alternating q-learning caching in d2d-enabled cellular networks. In IEEE Global Communications Conference (GLOBECOM), 2019.

Mingxiao Feng, Guozi Liu, Li Zhao, Lei Song, Jiang Bian, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. Multi-agent reinforcement learning with shared resource in inventory management, 2022.

Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In AAAI Conference on Artificial Intelligence (AAAI), 2018.

Jian Hu, Siyang Jiang, Seth Austin Harding, Haibin Wu, and Shih wei Liao. Rethinking the implementation tricks and monotonicity constraint in cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2102.03479, 2021.

Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2019.

Jiechuan Jiang and Zongqing Lu. Best possible q-learning, 2022a.

Jiechuan Jiang and Zongqing Lu. I2q: A fully decentralized q-learning algorithm. In Advances in Neural Information Processing Systems (NeurIPS), 2022b.

Wenhao Li, Bo Jin, Xiangfeng Wang, Junchi Yan, and Hongyuan Zha. F2a2: Flexible fullydecentralized approximate actor-critic for cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2004.11145, 2020.

Xihan Li, Jia Zhang, Jiang Bian, Yunhai Tong, and Tie-Yan Liu. A cooperative multi-agent reinforcement learning framework for resource balancing in complex logistics network. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2019.

Yueheng Li, Guangming Xie, and Zongqing Lu. Difference advantage estimation for multi-agent policy gradients. In International Conference on Machine Learning (ICML), 2022.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations (ICLR), 2016.

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems (NeurIPS), 2017.

Omid Madani, Steve Hanks, and Anne Condon. On the undecidability of probabilistic planning and infinite-horizon partially observable markov decision problems. In AAAI/IAAI, 1999.

Laëtitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2007.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.

Gregory Palmer, Karl Tuyls, Daan Bloembergen, and Rahul Savani. Lenient multi-agent deep reinforcement learning. In International Conference on Autonomous Agents and MultiAgent Systems, 2018.

Georgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V Albrecht. Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.

Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin Böhmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. In Advances in Neural Information Processing Systems (NeurIPS), 2021.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2018.

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2019.

Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2019

Kefan Su and Zongqing Lu. Divergence-Regularized Multi-Agent Actor-Critic. In International Conference on Machine Learning (ICML), 2022.

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Valuedecomposition networks for cooperative multi-agent learning based on team reward. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2018.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, 2018.

Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning. arXiv preprint arXiv:1511.08779, 2015.

Ming Tan. Multi-agent reinforcement learning: independent versus cooperative agents. In International Conference on Machine Learning (ICML), 1993.

Justin K Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, and Benjamin Black. Revisiting parameter sharing in multi-agent deep reinforcement learning. arXiv preprint arXiv:2005.13625, 2020.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012.

Jiangxing Wang, Deheng Ye, and Zongqing Lu. More centralized training, still decentralized execution: Multi-agent conditional policy factorization. arXiv preprint arXiv:2209.12681, 2022.

Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent q-learning. In International Conference on Learning Representations (ICLR), 2021a.

Jianhong Wang, Wangkun Xu, Yunjie Gu, Wenbin Song, and Tim C Green. Multi-agent reinforcement learning for active voltage control on power distribution networks. In Advances in Neural Information Processing Systems (NeurIPS), $2021 \mathrm{~b}$.

Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Off-policy multi-agent decomposed policy gradients. In International Conference on Learning Representations (ICLR), 2020.

Bingyu Xu, Yaowei Wang, Zhaozhi Wang, Huizhu Jia, and Zongqing Lu. Hierarchically and cooperatively learning traffic signal control. In AAAI Conference on Artificial Intelligence (AAAI), 2021.

Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.

Haifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun Wang. Bi-level actor-critic for multi-agent coordination. In AAAI Conference on Artificial Intelligence (AAAI), 2020.

Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Baar. Fully decentralized multiagent reinforcement learning with networked agents. In International Conference on Machine Learning (ICML), 2018.

Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.

Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing optimal joint policy of maximum-entropy multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2021.
