# LMD3: Language Model Data Density Dependence 

John Kirchenbauer ${ }^{* 1}$, Garrett Honke ${ }^{* 2}$<br>Gowthami Somepalli ${ }^{1}$, Jonas Geiping ${ }^{3,5}$, Daphne Ippolito ${ }^{4,6}$, Katherine Lee ${ }^{6}$<br>Tom Goldstein ${ }^{1}$, David Andre ${ }^{2}$<br>${ }^{1}$ University of Maryland, ${ }^{2} \mathrm{X}$, the Moonshot Factory<br>${ }^{3}$ ELLIS Institute Tübingen, ${ }^{4}$ Carnegie Mellon University<br>${ }^{5}$ MPI-IS, Tübingen AI Center, ${ }^{6}$ Google DeepMind


#### Abstract

We develop a methodology for analyzing language model task performance at the individual example level based on training data density estimation. Experiments with paraphrasing as a controlled intervention on finetuning data demonstrate that increasing the support in the training distribution for specific test queries results in a measurable increase in density, which is also a significant predictor of the performance increase caused by the intervention. Experiments with pretraining data demonstrate that we can explain a significant fraction of the variance in model perplexity via density measurements. We conclude that our framework can provide statistical evidence of the dependence of a target model's predictions on subsets of its training data, and can more generally be used to characterize the support (or lack thereof) in the training data for a given test task.


## 1 Introduction

"With the right dataset and the right benchmark, you can make a deep learning model fake absolutely any ability. It's easiest if the benchmark and the dataset are the same thing, but otherwise you can also achieve it with a dataset that is a dense sampling of the distribution the benchmark is sampled from." - @fchollet

The goal of this study is to perform a careful series of experiments designed to provide concrete evidence for, or against, the hypothesis that the accuracy of a model on a test question is strongly impacted by the density of the training set around that question. To do this, we explicitly estimate a probability density function on the training distribution. We then ask whether it is possible to explain a significant amount of a language model's abilities by directly estimating how dense the training data distribution is at each test point. We quantify the density of training data in embedding space using a classic technique in statistics, Kernel Density Estimation (KDE), which relies on measurements of the similarity between a test point and its neighbors. We present an illustration of the general process in Figure 1

To determine whether such a simple technique offers any explanatory power in the realm of modern large language models, and to assess the impact of density on test performance, we begin by manipulating density through contamination of a training dataset with copies (or near copies) of test samples. We then observe the correlation between accuracy on test samples and the training density near those samples. We confirm that 1) a properly configured kernel density estimate can detect the increased density near these training points and 2) the elevated performance is reliably predicted by the density estimate value for the question texts.

We then study the natural density variation within a pretraining corpus and uncover a more complex and subtle downstream effect. Higher density test points have lower perplexity ("as expected") when density is estimated using only the nearest neighbors in the pretraining corpus for each query, although the effect size is small. However, when estimating density using a random subset of training points, we observe the opposite correlation.
![](https://cdn.mathpix.com/cropped/2024_06_04_e26f154e44b54eb3562fg-02.jpg?height=332&width=1126&top_left_y=287&top_left_x=477)

Figure 1: A system level view of the LMD3 pipeline. The corpus of data used to train a LLM and a test set of queries are projected into a vector space using a neural embedding model. For each resulting query vector, a density estimate with respect to the training corpus is computed. The resulting density estimates can be used to infer the model's ability to respond to a question-like query or simply reproduce the tokens in the query sequence based on whether the relative density is higher or lower at that point in sample space.

We interpret our findings within the context of a wide array of prior work on data attribution, data contamination, and the counter-intuitiveness of high-dimensional statistics, and conclude that simple measures of dataset density are indeed sharp enough tools to be predictive of per-sample test set performance. At the same time, we observe that-at least at the 7B scale where models do not dramatically interpolate their data-the effect of dataset leakage is small enough that it likely does not invalidate benchmark numbers. We wrap up with a discussion of applications of our methodology to instance and group-wise error analysis, dataset filtering and supplementing interventions, and the determination of test contamination from the training corpus.

## 2 Related Work

### 2.1 Memorization and Contamination

Large language models have been shown to memorize sequences from their training data. The key, replicated finding across this literature is that the more a sample is repeated in the training data the more likely it is to be memorized (Carlini et al. 2022: Hernandez et al. 2022). Repetition is an important extremal case of elevated density corresponding to a spike in the density function at a specific point in the sample space and as such we expect a density based analysis to highlight the causal relationship between repetition and elevated sampling likelihood. Additional work has shown that a similar relationship holds for fuzzy or near-duplicated samples (Ippolito et al. 2022) implying that a density measure might also capture this weaker dependence relation.

Recent work has studied the impact of data contamination during the pretraining and finetuning processes. Despite the fact that training solely and repeatedly on verbatim or paraphrased test samples leads to significantly elevated performance (Yang et al., 2023), the impact can be much less pronounced in realistic scenarios where leaked samples are mixed into a larger corpus during pretraining (Jiang et al., 2024). Since leakage of test queries constitutes a practically relevant instantiation of the aforementioned edge case of spikes in the training distribution, we design our initial set of experiments around a leakage intervention.

### 2.2 The "Data Attribution Hypothesis"

Stated informally, the "data attribution hypothesis" is the belief that specific machine learning model behaviors can be attributed to a specific set of examples from the training data. The critical implicit assumption is that the set of training points the behaviors are attributable to is small with respect to the overall size of the training corpus. Therefore, work on data attribution seeks to design methods for identifying the set of most relevant examples, with notable approaches including the use of optimization related measures (gradients) to develop "influence functions" (Koh \& Liang, 2017), behavioral approximations of model
outputs through the lens of kernels (Park et al., 2023), and even exhaustive models of how a model behaves with respect to leave-one-out resamples of its training dataset (Ilyas et al. 2022). To benchmark these techniques, curated datasets of queries and corresponding fact sets to which correct responses could/should be attributed have also been developed Akyürek et al., 2022). In this work, we take an orthogonal approach by both relaxing the attribution hypothesis' assertion that the relevant set of training data for each query must be small, and switching away from relying on characteristics of the final model and instead directly analyzing the training corpus itself.

### 2.3 The "Similarity Hypothesis"

Quite related to the data attribution hypothesis is the "similarity hypothesis"-the near truism that models will make predictions that are similar to their training data. While prior work has shown that structured features of training data such as the elevated occurrence of specific named entities correlates with performance on factual test questions concerning those entities (Kandpal et al. 2023), similar results involving more general notions of traintest interrelatedness are fewer and far between (at least in the modern LLM literature). That said, recent work such as Tirumala et al. (2023) has shown that density quantification can be used to guide dataset pruning routines tor improved sample complexity during model training. Also published contemporaneously to our research activities, Yauney et al. (2023) showed that a wide variety of similarity metrics were "not enough to explain pertormance across benchmarks". While certain aspects of our results do suggest that the story is subtle, rather than simple, in contrast to their rather definitive conclusions, we posit that significant variance in LLM performance can be explained given the right measure and lens of analysis.

## 3 Preliminaries: Kernel Density Estimation

Density estimation is the general problem of estimating a probability density function from data. In our setting, we aim to estimate a distribution that is intractable, namely the function $P: \mathbb{R}^{d} \rightarrow \mathbb{R}$, where for our domain we have represented all of the points $x \in X$ from our training data as $d$-dimensional vectors, and we would like to approximate the likelihood of drawing any given sample from the distribution over natural language sequences represented by our corpus. The tool we choose to use for this approximation is the Kernel Density Estimate.

Definition 3.1. Kernel Density Estimate (KDE) For a training corpus $X_{c}=\left\{x_{0}, x_{1}, \ldots x_{n-1}\right\} \in$ $\mathbb{R}^{d}$ with a bandwidth parameter $h>0$ and a kernel function $K_{h}: \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$, for a query vector $x_{q}$ the $\mathrm{KDE}$ at $x_{q}$ over $X_{c}$ denoted $\mathrm{KDE}_{X_{c}}\left(x_{q}\right)$ is given as:

$$
\operatorname{KDE}_{X_{c}}\left(x_{q}\right)=\frac{1}{\left|X_{c}\right|} \sum_{x \in X_{c}} K_{h}\left(x, x_{q}\right)
$$

However, because the sum is over all $n$ training samples, the cost of computing the KDE for realistic language model training datasets is intractable. To apply KDE in this setting, we turn to DEANN - Density Estimation from Approximate Nearest Neighbors, a clever optimization proposed and analyzed by Karppa et al. (2022) that enables the scaling of KDE to large datasets $X_{c}$ by decomposing the full KDE into the exact contributions of close neighbors, and approximate contributions from the rest of the corpus. We discuss the details of the approximation scheme further in Appendix A.1 including a formal definition of the approximation algorithm that we use when computing KDEs on pretraining-scale datasets in Algorithm 1

## 4 The LMD3 Methodology

For a chosen data point (text sequence) we want to study how the dataset density at that point is related to model performance at that point. We measure performance on "task-like" text sequences using model accuracy, and on unstructured or "webtext-like" data using
the loss function. Our method takes as input a model and its training data, as well as one or more test/query sets of interest. The training data are embedded using an embedding model, and the nearest neighbors for each query sequence are retrieved from the vectorized corpus. Then, either, the neighbors are combined with a random subset of corpus vectors to compute a density estimate at each query point, or, when scale permits, the density estimate is computed exactly at each query point.

### 4.1 Computing Embeddings

We take for granted access to performant, pretrained neural sequence embedding models as general purpose similarity/distance functions for pairs of text segments. We consider two types of embedding spaces in which to perform density estimates, the features produced by an off-the-shelf retrieval model and the features derived from the hidden states of the model under analysis. While in preliminary investigations we explored working with the latter "self-derived" features, we failed to find a significant difference between the two in our small scale (fine-tuning) experiments, and so we stick with the former for large-scale experiments. The use of LLM hidden states as retrieval features is an active area of study and future versions of our methodology could benefit from those results, so we leave a more comprehensive evaluation of exactly what self-derived LLM embeddings represent that general purpose retrieval models do not to future work. 1

### 4.2 Computing the KDEs

We utilize the software package developed by the DEANN authors (Karppa et al. 2022) as an extremely efficient parallelized implementation of the exact KDE calculation tor euclidean kernels when the data $\left(X_{q}, X_{c}\right.$ ) fits in memory. Since bandwidth selection is an empirical process, we perform an initial evaluation across a range of bandwidths in preparation for our paraphrase experiments (Appendix A.10).

For the finetuning-scale experiments, where the corpus contains $\sim 100,000$ samples, we are able to directly use the exact version of the KDE. For experiments with a pretraining dataset, we leverage the authors' exact KDE implementation as well as their random sampling based KDE approximation as subroutines. Algorithm 1 describes how we combine this with a scalable neighbor search system to implement our "fully decomposed" version of the nearest neighbor search-based approximate KDE. "Fully decomposed" simply means that before running the KDE calculations themselves, as a preprocessing step for the query set, the neighbors for each example are retrieved from the training corpus via a massively distributed vector search engine (exactly, in our case, as opposed to approximately as suggested by DEANN). ${ }^{2}$ Then, the random component for each density calculation is selected in a hierarchical manner via an initial large random sample without replacement, and a second sampling step, also without replacement, to select the final random complement for each query's individual density estimate. In Section 5.3 and Section 6.1. "KDE" refers to the exact KDE computed over the entire finetuning corpus, while in Section 5.4 and Section 6.2 it refers to the final value yielded by Algorithm 1-the weighted average of the local and random components of the KDE approximation over the pretraining corpus, except where denoted specifically as solely the "local" or "random" component.[^0]

## 5 Experiments

### 5.1 Models and Data

For our experiments we use a transformer-based sequence embedding model from the sentence-transformers library (Reimers \& Gurevych, 2019) to generate the feature space in which we compute density estimates. For the controlled finetuning experiments we train the base Llama 2 7B model (Touvron et al., 2023) on doctored versions of the training set for the MMLU benchmark for 2 epochs as this regime produces meaningful differences between experimental settings whilst still being realistic (and resulting in a model that has not completely degenerated in other regards). For the pretraining experiments we analyze the performance of the 6.9B model from the Pythia suite (Biderman et al. 2023) as a function of a version of its training corpus, The Deduplicated Pile (Biderman et al. 2022). We provide further details on these choices and experimental parameters in Appendix A.2.

### 5.2 Paraphrasing Process

In order to develop our density methodology, we design a synthetic experimental setting in which we can directly tune the specific amount of support for test queries present in the training data. Roughly defined, an idealized paraphrase of a test example is the transform from natural language sequence $x$ to $x^{\prime}$ such that the semantics of $x$ and $x^{\prime}$ are equivalent. Access to such a transformation function allows us to perform controlled experiments where we take a test sample $x$ and intervene to increase the amount of specifically relevant training data for this sample $x$ by mixing paraphrases $x^{\prime}$ into the training data. This can be done in a number of ways, and we discuss the design choices for this process further in Appendix A. 3

### 5.3 Controlled Experiment 1: Leakage to Increase Density, Finetuning Scale

In our first series of experiments, we take 1,000 random questions from the MMLU test set of 14,042 questions and paraphrase them 3 times. We also consider an exact copy of the selected test questions as a "paraphrase" with perfect similarity, or distance 0 to the original query. For each test question we paraphrase, we compute the cosine similarity between the original query and each sample in the set of 3 paraphrases based on their embeddings. Considering the paraphrases for each query sorted in ascending order by similarity to the original query as Para 1, Para 2, and Para 3 respectively, we then refer to the collection of two paraphrases $\{$ Para 1,Para 2\} as Paras $=1,2$ and the collection of all three paraphrases $\{$ Para 1,Para 2, Para 3, $\}$ as Paras $=1,2,3$.

Then we finetune our base language model on datasets constructed by taking the "auxiliary train" split of the MMLU dataset and mixing in various combinations of the paraphrases and /or an exact copy of the original query, which we denote the presence of, or lack thereof, using Exact $=1$ and Exact $=0$ respectively. We evaluate the performance of the models trained on these different datasets and analyze the impact that the inclusion of paraphrases and exact copies of test queries has on the trained model. "Rank Accuracy" denotes the scoring method used by the HuggingFace Open LLM Leaderboard for evaluating MMLU, first proposed by Brown et al. (2020).

### 5.4 In-the-Wild: In and Out-of-Distribution Queries, Pretraining Scale

Finally, we turn our focus to the loftier goal of relating the test time behavior of a language model to its pretraining data. In addition to the increased scale with which we apply the technique, the increased generality of the pretraining distribution requires us to curate sets of queries to represent different test time scenarios we might be concerned with. We consider two classes of query sets: in-distribution (ID) and out-of-distribution (OoD), where for the former we choose the simple, explicit definition of any query $x_{q}$ taken directly from the training corpus $X_{c}$ as ID, and consider anything else to be OoD. The training corpus is The Deduplicated Pile, re-segmented into $\sim 3.5 \mathrm{~B}$ individual text sequences. For these experiments we compute the approximate KDE using parameters $k=1,000$ nearest neighbors for the local component, $m_{1}=1,000,000$ samples as a base random sample set,
with $m_{2}=10,000$ samples per query selected as the random complement, exclusive to the $\mathrm{k}$ nearest neighbors.

Random 10k segments (ID): We randomly sample, without replacement, 10,000 segments from the training corpus itself. These are drawn from precisely the same set of segments over which neighbor retrieval and KDE computation are performed. Since these are simply webtext segments, there is no notion of "response" so we just compute perplexity (PPL) of the text segment under the LLM.

MMLU Test (OoD): We employ the same 14,042 test questions as in the finetuning experiments to act as a set of queries that are not contained in the training data. Since there are ground truth responses for the queries, we can compute perplexities on both the query texts as well as on the correct target response when conditioned on the query text.

OpenOrca Random 10k (OoD): As another set of diverse sequences outside of the train distribution, we consider a random selection of 10,000 questions from the OpenOrca curation project's main dataset. Since these also have reference responses, we can compute perplexities on both the query and responses for these samples.

![](https://cdn.mathpix.com/cropped/2024_06_04_e26f154e44b54eb3562fg-06.jpg?height=556&width=1379&top_left_y=950&top_left_x=362)

![](https://cdn.mathpix.com/cropped/2024_06_04_e26f154e44b54eb3562fg-06.jpg?height=531&width=661&top_left_y=960&top_left_x=385)

![](https://cdn.mathpix.com/cropped/2024_06_04_e26f154e44b54eb3562fg-06.jpg?height=540&width=686&top_left_y=955&top_left_x=1050)

Figure 2: Left) To enable the aggregate interpretation of the paraphrasing experiments, we plot accuracy on leaked test questions as a function of the number of "effective epochs" the model has trained on the leaked questions for. Right) We plot the same performance measure as a function of KDE, with gaussian kernel and bandwidth 0.5. We see that the trend in accuracy according to our density measure corresponds with the trend in accuracy according to the known degree of leakage, effective epochs.

## 6 Results

Considering the relative novelty of our proposed methodology and in order to de-risk the main experiments, we performed a series of validation analyses regarding the embedding model used for the paraphrasing experiments, the bandwidth hyperparameter of the KDE, and basic series of finetuning runs to calibrate our expectations about exactly how much a model should overfit when trained repeatedly on verbatim leaks of test questions. We discuss these preliminary experiments in section Appendix A.10. For the main experiments we consider bandwidths of $\{0.01,0.05,0.1,1.0\}$ for the exponential kernel, and $\{0.1,0.2,0.5,1.0\}$ for the gaussian kernel though we only report a small selection across the main body figures, primarily gaussian at 0.1 as a "narrow" setting tuned for discrimination in the leakage experiments and 0.5 as a "wide" setting for capturing a broader range of similarities in the pretraining-scale experiments.

### 6.1 Controlled Experiments: "Expected" Dependence between Performance and Density.

In Figure 2 we demonstrate that training on the various leaky dataset formulations improves performance by visualizing changes in performance across different leakage settings. We

![](https://cdn.mathpix.com/cropped/2024_06_04_e26f154e44b54eb3562fg-07.jpg?height=466&width=1391&top_left_y=279&top_left_x=367)

(a) Experiments where no exact copies of test questions are included, only paraphrases.
![](https://cdn.mathpix.com/cropped/2024_06_04_e26f154e44b54eb3562fg-07.jpg?height=436&width=1392&top_left_y=804&top_left_x=366)

(b) Experiments where 1 exact copy of test questions are included, in addition to paraphrases.

Figure 3: Moving from left to right in (a) and (b) shows the effect of an increase in the number of paraphrases of each test question that are leaked into the training data. Experiments in (b) also include an exact copy of each leaked test question, while experiments in (a) do not. "Count" histograms) We plot the distributions of KDE values (gaussian kernel and bandwidth 0.1) for the test queries that were leaked, exactly and or via paraphrase, and not leaked, for each leakage intervention experiment. Accuracy bar charts) We show the corresponding accuracy breakdown for the leaked and non leaked sets for each experiment. Overall, we find that increasing support for test questions via incorporating paraphrases into the training data increases performance on those test questions, and this increase is magnified by the addition of exact leaks of test questions. The addition of the exact copy of each question also makes the leaked and non-leaked question sets highly separable under our KDE measure as demonstrated by the distinct concentration of "leaked Q" KDE values away from 0.0 in b).

enable this by first plotting performance against a handcrafted feature called "effective epochs", where we compute the number of epochs the model has effectively trained on a given test question $x_{t}$ based on the specific set of paraphrases and exact duplicates $X_{p}$ included in the training set. The expression we use is $\sum_{x_{p} \in X_{p}} \operatorname{cossim}\left(x_{t}, x_{p}\right)$. Since an exact copy has similarity $\sim 1.0$ and paraphrases have scores $<1.0$, this expression produces sensible $x$-axis values differentiating each experiment, eg. for the Para $=0$, Exact $=1$ case, when training for 2 epochs, the exact question copies represent precisely $2 * 1.0=2$ effective epochs, and then paraphrases count for an additional partial epoch $\left(2 * \operatorname{cossim}\left(x_{t}, x_{p}\right)\right)$ each depending on their similarity to the original question. In Figure 2 we observe both a positive trend in performance as a function of effective epochs as well as a positive trend as a function of our KDE measure-the latter of which is computed without access to either ground truths or any outputs of the LLM being analyzed.

In Figure 3, we present accuracies and densities across the full factorial manipulation of exact and paraphrased leaks. The figure shows that the KDE is a discriminative feature between the leak set and non-leak set. To measure the reliability of the correspondence between data density and performance, we perform mixed-effects regressions and discuss these results in
detail in Appendix A.5. In summary, we find that the leakage conditions reliably increase accuracy (Exact leak: $p<.001$, Paraphrased leaks: $p<.001$ ) and decrease perplexity (Exact leak: $p<.001$, Paraphrased leaks: $p<.001$ ) on the test questions. Critically, training data density estimates also reliably predict variance in accuracy and perplexity where as density increases, accuracy increases $(p<.001)$ and perplexity decreases $(p<.001)$.
![](https://cdn.mathpix.com/cropped/2024_06_04_e26f154e44b54eb3562fg-08.jpg?height=308&width=1324&top_left_y=518&top_left_x=388)

Figure 4: Perplexity according to Pythia 6.9B for random samples from The Deduplicated Pile (ID) as a function of KDE with gaussian kernel and a bandwidth of 0.5 , marginalized via equal mass binning into 20 bins. Left) Query perplexity vs. the KDE with respect to a random sample of points in the corpus. Middle) Query perplexity vs. the KDE with respect to only the local neighborhood within the corpus. Right) Query perplexity vs. the average distance to the to the top $\mathrm{k}$ neighbors. Horizontal line denotes the average across all queries. We see that while the trend in Query PPL as a function of the random component of the KDE is non-monotonic, and even weakly positive, when considering the local region of highly similar samples for each query, there is a strong clear negative trend in PPL as a function of density, as measured by the local KDE or a simple average over neighbor distances.
![](https://cdn.mathpix.com/cropped/2024_06_04_e26f154e44b54eb3562fg-08.jpg?height=320&width=1326&top_left_y=1301&top_left_x=388)

Figure 5: Perplexity according to Pythia 6.9B for questions from the MMLU test set (OoD) as a function of KDE with gaussian kernel and a bandwidth of 0.5 or average distance to $\mathrm{k}$ nearest neighbors, marginalized via equal mass binning into 20 bins. Left) Question perplexity vs the KDE with respect to only the local neighborhood within the corpus. Middle) Question perplexity vs distance to k nearest neighbors. Right) Response perplexity vs distance to $\mathrm{k}$ nearest neighbors. Horizontal line denotes the average across all queries. While the relationship between query perplexity and local KDE isn't particularly strong, there is a stronger trend as a function of simple neighbor distances. For response perplexity, we see a clear trend as a function of average neighbor distances.

### 6.2 In-the-Wild Experiments: Aggregation-Specific Dependence Relationship.

First considering a set of random samples from The Deduplicated Pile itself, ID with respect to the model's training data, in Figure 4 we plot perplexity as a function of KDE by binning the data by KDE values (x-axis) and computing the average accuracy (y-axis) within each bin. In the left chart we see that when measuring the KDE with respect to only the random samples selected in our approximation algorithm we get a noisy, slightly positive trend (which is surprising ${ }^{3}$. However, when we consider the KDE computed only with respect to the nearest neighbors in the corpus, we get a sharp negative trend, in line with results in the

${ }^{3}$...but potentially explainable by distances in high dimensions being counterintuitive. Being closer on average to a collection of random other points effectively means being close to nothing in particular.
finetuning setting ${ }^{4}$ This is affirmed by the trend relating each query's average distance to its top $k$ neighbors to model perplexity on said query. (Note that $x$-axes are reversed in the distance charts to make the trend visually congruent with the handedness of the trends for density measurements Figure 4 and Figure 5.)

Next, switching to a set of OoD queries, the MMLU test set, the left chart of Figure 5 shows that the PPL on the query/question texts is not strongly correlated with the local KDE component, but the middle chart shows that it is correlated with the average distance to the top $\mathrm{k}$ neighbors in the corpus. Further, we see that the perplexity of the correct response is also correlated with distance to the top $\mathrm{k}$ neighbors.

As with the finetuning scale experiments, we used mixed-effects regression to measure the reliability of the effect of density on perplexity and report those full results in Appendix A. 9 In summary, we find that query perplexity decreases as data density increases for the randomly-sampled ID query set ( $p<.001$ ) and the OoD OpenOrca dataset ( $p<.001$ ) but not the OoD MMLU Test query set ( $p=.95$ ). However, response perplexity does decrease slightly with increased density for the MMLU Test set ( $p<.001$ ).

Since the text length directly effects perplexity values, for Figure 4 and Figure 5 we must manually isolate a subset of the queries where lengths are relatively similar to remove the variance due to length before plotting. We also observe a small number of extremely large outlier perplexity values and drop those rows as well. Details about this process are provided in Appendix A.7. We report only the most insightful combinations here and present a more exhaustive series in Appendix A.8.

## 7 Insights and Applications

We can summarize our key insights from both groups of experiments succinctly by stating that in extremal cases like test question leakage, one can expect a clear relationship between model performance and apparent training data density in query space. However, at pretraining scale, the specific way in which one aggregates information about relevant support in the dataset matters. Using the abstraction of "effective epochs" one can understand how repeated training on subsets of the data can lead to elevated marginal performance on those subsets but we caution that the number of effective epochs required to achieve drastically inflated test accuracies on benchmarks, such as those reported in Yang et al. (2023), might be much higher than one would expect. Therefore, any un-evidenced claim that contamination is a reason to wholly discount the benchmark results for models trained on unspecified data distributions (at least at the common 7B parameter scale) should be taken with a grain of salt.

Overall we believe that training data density quantification is a grounded methodology for analyzing the failure modes of language models at an instance and group-wise level, since it builds evidence for expected relative performance based directly on aspects of the training data itself. In response to observing relatively high density in certain regions of test query space and lower density in others, we expect that the consistency of model performance could be improved by supplementing data in those regions of weaker support through human data curation as well as automated processes such as machine paraphrasing.

## 8 Limitations and Conclusion

A few key limitations of our study include the focus on a small set of specific, but very relevant, datasets and models for the field and the fact that the KDE based methodology we propose is not hyperparameter free-the user must choose the kernel parameters. It is also likely that the preprocessing parameters-length and stride-used to segment the training data before embedding are critical to the outcome of the analysis. We work at a finer segmentation granularity than Tirumala et al. (2023) and Yauney et al. (2023) but we do not ablate our choices enough to prove that this is required to explain meaningful[^1]amounts of variance. Finally we also acknowledge that full access to the training dataset of a newly "released" model is increasingly rare. However, given the potential of data-centric analysis techniques such as ours we claim that this is a shortcoming of contemporary norms within the field rather than a true limitation of our methodology. Rather, we implore the community to insist on the release of details sufficient to exactly re-materialize the full training data distribution as a necessary precondition for presenting model releases as bona fide scientific artifacts.

## References

Ekin Akyürek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. Tracing knowledge in language models back to the training data. arXiv preprint arXiv:2205.11482, 2022.

Douglas Bates, Martin Mächler, Ben Bolker, and Steve Walker. Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1):1-48, 2015. doi: 10.18637/jss.v067. $\mathrm{i} 01$.

Stella Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile. arXiv preprint arXiv:2201.07311, 2022.

Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. PMLR, 2023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https: //doi.org/10.5281/zenodo.5371628

Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values. Proceedings of the International Conference on Learning Representations (ICLR), 2021a.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021b.

Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer ElShowk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487, 2022.

Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622, 2022.

Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A Choquette-Choo, and Nicholas Carlini. Preventing verbatim memorization in language models gives a false sense of privacy. arXiv preprint arXiv:2210.17546, 2022.

Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo. Investigating data contamination for pre-training language models. arXiv preprint arXiv:2401.06059, 2024.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535-547, 2019.

Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pp. 15696-15707. PMLR, 2023.

Matti Karppa, Martin Aumüller, and Rasmus Pagh. Deann: Speeding up kernel-density estimation using approximate nearest neighbor search. In International Conference on Artificial Intelligence and Statistics, pp. 3108-3137. PMLR, 2022.

John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language models. arXiv preprint arXiv:2306.04634, 2023.

Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International conference on machine learning, pp. 1885-1894. PMLR, 2017.

Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. arXiv preprint arXiv:2303.13408, 2023.

Alexandra Kuznetsova, Per B. Brockhoff, and Rune H. B. Christensen. lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software, 82(13):1-26, 2017. doi: 10.18637/jss.v082.113.

Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. arXiv preprint arXiv:2303.14186, 2023.

R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2021. URL https://www.R-project.org/.

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ abs $/ 1908.10084$

Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S Morcos. D4: Improving $11 m$ pretraining via document de-duplication and diversification. arXiv preprint arXiv:2308.12284, 2023.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023.

Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez, and Ion Stoica. Rethinking benchmark and contamination for language models with rephrased samples. arXiv preprint arXiv:2311.04850, 2023.

Gregory Yauney, Emily Reif, and David Mimno. Data similarity is not enough to explain language model performance. arXiv preprint arXiv:2311.09006, 2023.

```
Algorithm 1 Fully Decomposed Approx. KDE with Hierarchical Sampling
    Input: A corpus $X_{c}$ of $n$ text embeddings $x_{c} \in \mathbb{R}^{d}$, a $k$ nearest neighbor search subroutine
    over vectors in $X_{c}, N N_{k}(\cdot)$, a kernel function $K$ and bandwidth parameter $h$ together with
    the corpus over which it is computed $X$, defining a $K D E_{X}(\cdot)$, two random sample size
    parameters $m_{1}$ and $m_{2}\left(m_{2}<m_{1} \ll n\right)$, a dataset of query embeddings $X_{q}, x_{q} \in \mathbb{R}^{d}$.
    Output: $Z_{q}, \in \mathbb{R}_{>0}^{\left|X_{q}\right|}$, an approximation of the KDE for each $x_{q} \in X_{q}$.
    Randomly sample w/o replacement $X_{1}$ of size $m_{1}$ from $X_{c}$
    for all $x_{q} \in X_{q}$ do
        $X_{n n} \leftarrow N N_{k}\left(x_{q}\right) \quad \in \mathbb{R}^{k \times d}$
        $X_{1}^{\prime}=\left\{x \in X_{1} \mid x \notin X_{n n}\right\}$
        Randomly sample w /o replacement $X_{2}$ of size $m_{2}$ from $X_{1}^{\prime}$.
        $z_{n n} \leftarrow K D E_{X_{n n}}\left(x_{q}\right)$
        $z_{\text {rand }} \leftarrow K D E_{X_{2}}\left(x_{q}\right)$
        $z_{q} \leftarrow\left(\frac{k}{n}\right) z_{n n}+\left(\frac{n-k}{n}\right) z_{\text {rand }}$
    end for
```

\{Note that we can return $Z_{n n}$ and $Z_{\text {rand }}$ individually to analyze the effect of local and
global contributions independently. $\}$
