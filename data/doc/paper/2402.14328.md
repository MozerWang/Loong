# Understanding and Patching Compositional Reasoning in LLMs 

Zhaoyi Li ${ }^{1,2}$, Gangwei Jiang ${ }^{1,2}$, Hong Xie ${ }^{1}$, Linqi Song ${ }^{2}$, Defu Lian ${ }^{1}$, Ying Wei ${ }^{3}$<br>${ }^{1}$ School of Computer Science and Technology, University of Science and Technology of China<br>${ }^{2}$ Department of Computer Science, City University of Hong Kong<br>${ }^{3}$ School of Computer Science and Engineering, Nanyang Technological University<br>\{lizhaoyi777,gwjiang\}@mail.ustc.edu.cn, \{hongx87,liandefu\}@ustc.edu.cn,<br>linqi.song@cityu.edu.hk, ying.wei@ntu.edu.sg


#### Abstract

LLMs have marked a revolutonary shift, yet they falter when faced with compositional reasoning tasks. Our research embarks on a quest to uncover the root causes of compositional reasoning failures of LLMs, uncovering that most of them stem from the improperly generated or leveraged implicit reasoning results. Inspired by our empirical findings, we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of LLMs. This deep dive reveals that implicit reasoning results indeed surface within middle layers and play a causative role in shaping the final explicit reasoning results. Our exploration further locates multi-head self-attention (MHSA) modules within these layers, which emerge as the linchpins in accurate generation and leveraing of implicit reasoning results. Grounded on the above findings, we develop CREME, a lightweight method to patch errors in compositional reasoning via editing the located MHSA modules. Our empirical evidence stands testament to CREME's effectiveness, paving the way for autonomously and continuously enhancing compositional reasoning capabilities in language models.


## 1 Introduction

Compositional reasoning stands as a pivotal mechanism, unlocking the ability of learning systems to decompose complex tasks into manageable subtasks and tackle them step-by-step (Lu et al., 2023; Lake and Baroni, 2023). Despite the revolutionary impact of Large Language Models (LLMs) on the NLP landscape, they struggle at basic compositional reasoning tasks (Dziri et al., 2023). This shortcoming is specifically highlighted by Press et al. (2023), who brought attention to the concerning "compositionality gap" in the realm of question-answering tasks. It was observed that there is a substantial failure rate of $\sim 40 \%$ in two-hop compositional queries, even when they

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-01.jpg?height=249&width=356&top_left_y=755&top_left_x=1067)

(a) Pre-Edit

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-01.jpg?height=251&width=372&top_left_y=754&top_left_x=1436)

(b) Post-Edit
Figure 1: Logit Lens inspecting results. $\mathrm{x}$-axis refers to the layer; $\mathrm{y}$-axis refers to the inspecting value (Eqn. 1). red and blue lines trace the implicit (association football) and explicit (England) reasoning results, respectively.

can successfully answer the individual single-hop queries that make up the two-hop question. Recent attempts improve the compositional reasoning capabilities of LLMs through carefully crafted prompting strategies developed by experts (Wei et al., 2022; Zhou et al., 2023), enabling LLMs to autonomously rectify their compositional reasoning errors and continuously improve over time remains a largely unexplored frontier.

This work, therefore, sets out to firstly delve into the specific failures to understand (RQ1) what accounts for these failures and (RQ2) which parts of the LLMs are responsible for them, and secondly develop strategies for patching these failures. Our initial step involves an analysis of a very recent dataset comprising compositional two-hop knowledge queries (Zhong et al., 2023), selectively examining the cases where LLMs fail despite successfully answering the constituent single-hop queries. To ensure our findings and methodologies offer broad applicability, our analyses utilize two widelyused open-sourced LLMs: OpenAlpaca-3B (Su et al., 2023b) and LLaMA-2-7B (Touvron et al., 2023). Through meticulous examination of the failure instances, we identify three prevalent types of errors. Utilizing the Logit Lens tool (nostalgebraist, 2020), each error type highlights a critical shortfall in generating or leveraging the implicit reasoning result necessary for the explicit reason-
ing result ${ }^{1}$. This gap is particularly concerning as it contrasts sharply with the intuitive two-hop reasoning process inherent to human cognition. An illustrative example of a "Hasty Answer" error is depicted in Figure 1(a), where the model prematurely concludes its reasoning without adequately incorporating the implicit reasoning result.

The above observations motivate our further empirical inquiry to answer the first question of what accounts for these failures, from the perspective of whether LLMs are indeed aware of implicit reasoning results during compositional reasoning. We inspect inner hidden states of LLMs via Logit Lens, from which we observe that implicit reasoning results not only manifest within the LLMs' intermediate layers but also tend to precede the generation of explicit reasoning results, often emerging statistically earlier. Building on this, we further explore the relationship between implicit and explicit reasoning results through an Intervention (Pearl, 2001; Li et al., 2023a) experiment, providing compelling evidence that the emergence of implicit reasoning results within LLMs plays a causative role in the generation of explicit reasoning results.

The next question is, regarding $\mathbf{R Q 2}$, in which modules LLMs generate implicit reasoning results? Leveraging causal mediation analysis (Meng et al., 2022; Stolfo et al., 2023), we present both a compositional query and its corresponding second-hop query to the LLM, resulting in the generation of two distinct computation graphs. We then intervene the computation graph $\mathcal{G}_{1}$, associated with the compositional query, by replacing the output of a single module with its counterpart from the second-hop computation graph $\mathcal{G}_{2}$. By identifying the modules whose replacement results in a significant enhancement in the predictive probability of the explicit reasoning result, we are able to locate several specific outputs from the Multi-Head Self-Attention (MHSA). Intriguingly, the layers pinpointed through this approach show a strong correlation with those identified in preceding Intervention experiments. This congruence reinforces the hypothesis that implicit reasoning results are not only present but are actively consolidated and utilized within these specific layers of the LLM.

Grounded on our findings into RQ1 and RQ2, we develop CREME (Correcting Compositional REasoning via Model Editing), a light-weight[^0]

model-editing method to patch errors in compositional reasoning. CREME follows Santurkar et al. (2021); Meng et al. (2022) by regarding the output matrix of the located MHSA, $W_{o}^{l}$, as a linear associative memory. To implement CREME, we designate the input to $W_{o}^{l}$ in the computation graph $\mathcal{G}_{1}$ as $k^{*}$ and the output from $W_{o}^{l}$ in $\mathcal{G}_{2}$ as $v^{*}$. We then proceed to insert the pair $\left(k^{*}, v^{*}\right)$ into $W_{o}^{l}$, ensuring that this insertion disrupts existing memories within $W_{o}^{l}$ as minimally as possible. This objective is achieved by solving a convex optimization problem, which strikes a nuanced balance between the integration of new corrective information and the preservation of existing knowledge.

Our main contributions and takeaways are summarized below: (1) successful compositional reasoning within LLMs hinges on its awareness of generating and leveraging implicit reasoning results; (2) MHSA modules in the middle layers (18/19-th layer) are significantly in charge of properly generating and leveraging implicit reasoning results; (3) by leveraging the second-hop computation graph as a reference for editing the located MHSA modules, CREME proves to be highly performing, on correctly answering not only the query used for editing $W_{o}^{l}$ but also the paraphrased queries and other compositional queries sharing the first-hop knowledge as well as maintaining little effect on irrelevant queries.

## 2 Background \& Notation

### 2.1 Logit Lens

Logit Lens (nostalgebraist, 2020) is a widely used for inspecting hidden states of LLMs (Dar et al., 2023; Geva et al., 2023; Katz and Belinkov, 2023; Sakarvadia et al., 2023). The key idea of Logit Lens is thus to interpret hidden states in middle layers of LLMs via projecting them into the output vocabulary space with the $\mathbf{L M}$ head $W_{u}$. When presented with a specific hidden state $h_{l}^{t}$ and a set of target tokens $T_{t g t}$, the Logit Lens is given as follows:

$$
\begin{align*}
& L\left(h_{l}^{t}, T_{t g t}\right)=\frac{1}{\left|T_{t g t}\right|} \sum_{k \in T_{t g t}} p_{l}^{t}[k]  \tag{1}\\
& p_{l}^{t}=\operatorname{softmax}\left(v_{l}^{t}\right)=\operatorname{softmax}\left(h_{l}^{t} W_{u}\right) \tag{2}
\end{align*}
$$

where $L\left(h_{l}^{t}, T_{t g t}\right)$ measures how much information around $T_{\text {tgt }}$ is contained in $h_{l}^{t}$.

| Error type | Input | Implicit result | Correct final result | Predicted final result |
| :---: | :---: | :---: | :---: | :---: |
| Distortion | The nationality of the performer of the song "I Feel Love" is | Donna Summer | United States of America | United Kingdom $\backslash$ Italy |
| Incomplete Reasoning | The head of state of the country where ORLAN holds citizenship is | France | Emmanuel Macron | France |
| Hasty Answer I | The capital city of the country where "Work from Home" originated is | United States of America | Washington, D.C. | Los Angeles $\backslash$ New York |
| Hasty Answer II | The home country of the sport associated with Giorgio Chinaglia is | association football | England |  |

Table 1: Specific examples in $D_{\text {gap }}$ for three types of common errors. "Predicted final result" column refers to the wrong answers output by LLaMA-2-7B.

### 2.2 Compositional Reasoning and Dataset

Compositional knowledge refers to knowledge items that are the compositions of several singlehop sub-knowledge items. Compositional reasoning refers to the ability to answer the queries on compositional knowledge (e.g., verbalized in format of QA or Cloze-Test) via a step-by-step reasoning process. We denote a single-hop knowledge as a triple $(s, r, o)$, where $s, r, o$ represents subject, relationship and object respectively. The composed compositional two-hop knowledge is denoted as $\left(s_{1}, r_{1}, o_{1}\right) \oplus\left(s_{2}, r_{2}, o_{2}\right)$ where subscripts 1 and 2 represent the first-hop and second-hop sub-knowledge (requiring $o_{1}=s_{2}$ so that they can compose together). The dataset $\mathcal{D}$ (Appendix B) we used in this paper is sourced from Zhong et al. (2023). For each datum in $\mathcal{D}$, it contains: (1) the compositional query on the compositional knowledge $\left(s_{1}, r_{1}, o_{1}\right) \oplus\left(s_{2}, r_{2}, o_{2}\right)$, (2) the first-hop query on $\left(s_{1}, r_{1}, o_{1}\right)$, (3) the second-hop query on $\left(s_{2}, r_{2}, o_{2}\right)$, and (4) the implicit reasoning result $o_{1}$ and the explicit reasoning result $o_{2}$. By way of example, the first-hop query is "What is the sport associated with $\left(r_{1}\right)$ Giorgio Chinaglia $\left(s_{1}\right)$ ? association football $\left(o_{1}\right)$ ", the second-hop query is "What is the home country of $\left(r_{2}\right)$ association football $\left(s_{2}\right)$ ? England $\left(o_{2}\right) "$ and the compositional query can be verbalized as "What is the home country of $\left(r_{2}\right)$ the sport associated with $\left(r_{1}\right)$ Giorgio Chinaglia $\left(s_{1}\right)$ ? England $\left(o_{2}\right)$ ".

## 3 Analyzing Compositional Reasoning Errors

Grounded on the observation of Press et al. (2023), we dive into the compositional reasoning failures: we identify three types of common errors among such failures and attribute the cause of these common errors to the failure of generating implicit reasoning result properly via inspecting hidden states.

Three types of Common Errors We query LLMs with all of compositional queries and the corresponding single-hop queries in $\mathcal{D}$. We filter out two subsets of $\mathcal{D}: \mathcal{D}_{\text {single }}$ and $\mathcal{D}_{\text {gap }}$. For each datum $\left(s_{1}, r_{1}, o_{1}\right) \oplus\left(s_{2}, r_{2}, o_{2}\right)$ in $\mathcal{D}$, $\mathcal{D}_{\text {single }}$ contains the datum where the both of $\left(s_{1}, r_{1}, o_{1}\right)$ and $\left(s_{2}, r_{2}, o_{2}\right)$ are successfully answered. Among $\mathcal{D}_{\text {single }}, \mathcal{D}_{\text {gap }}$ contains the datum where the answer for the compositional queries $\left(s_{1}, r_{1}, o_{1}\right) \oplus$ $\left(s_{2}, r_{2}, o_{2}\right)$ are mis-predicted. ${ }^{2}$ In our analysis of $\mathcal{D}_{\text {gap }}$, we have discerned a few common patterns shared among a substantial portion of the failures. Consequently, we have delineated three predominant types of errors, each characterized by distinct features, as outlined below. Distortion: LLMs fail to effectively generate implicit reasoning results in the reasoning process. The predicted answer for the first example in Table 1 is either United Kingdom or Italy. Considering both as countries (corresponding to nationality $\left(r_{2}\right)$ ), we conclude that the information about Donna Summer $\left(o_{1}\right)$ distorts in middle hidden states. Incomplete Reasoning: LLMs directly output the first-hop reasoning result $\left(o_{1}\right)$. In the second example of Table 1, LLaMA-2 outputs France $\left(o_{1}\right)$ while the correct answer requires further reasoning. the head of state of $\left(r_{2}\right)$ France $\left(o_{1}\right)$ is Emmanuel Macron ( $o_{2}$ ). Hasty Answer: LLMs predict the result without carefully reasoning. We further subdivide this type of errors into two categories: I: LLMs finally predict a close result based on the implicit reasoning result. For the third example in Table 1: LLMs predict Los Angeles or New York, both of which are famous city in the U.S.A., implying that LLMs manage to generate the implicit result ( $o_{1}:$ U.S.A.) while fails to incorporate "the capital of" $\left(r_{2}\right)$ to generate final result $o_{2}$. II: LLMs take short-cut instead of step-by-step reasoning, leading to incorrect answers. Consider the fourth example in Table 1: the correct reasoning process should be (1): the sport associated with $\left(r_{1}\right)$ Giorgio Chinaglia $\left(s_{1}\right)$ is association football $\left(o_{1}\right)$; followed by (2): the home country of $\left(r_{2}\right)$ association football $\left(o_{1}\right)$ is England $\left(o_{2}\right)$. However, LLMs erroneously attribute Italy as the answer. This misstep is attributed to LLMs' tendency to directly associate Giorgio Chinaglia $\left(s_{1}\right)$ - noted for his Italian nationality - with the home country of the sport $\left(r_{2}\right)$.

Analysis and Possible Explanation We aim to analyze the cause of these errors via inspecting[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-04.jpg?height=286&width=391&top_left_y=254&top_left_x=247)

(a) Distortion:Comp

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-04.jpg?height=277&width=377&top_left_y=621&top_left_x=248)

(e) Distortion:Reference

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-04.jpg?height=286&width=391&top_left_y=254&top_left_x=638)

(b) Incomplete Reasoning:Comp

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-04.jpg?height=280&width=397&top_left_y=617&top_left_x=635)

(f) Incomplete Reasoning:Reference

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-04.jpg?height=288&width=394&top_left_y=256&top_left_x=1025)

(c) Hasty Answer I:Comp

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-04.jpg?height=280&width=383&top_left_y=617&top_left_x=1022)

(g) Hasty Answer I:Reference

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-04.jpg?height=288&width=391&top_left_y=256&top_left_x=1415)

(d) Hasty Answer II:Comp

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-04.jpg?height=283&width=394&top_left_y=618&top_left_x=1411)

(h) Hasty Answer II:Reference

Figure 2: Logit Lens results of examples of three error types. Comp is the result for compositional two-hop query; Reference is the result for the corresponding second-hop query (as the reference for the compositional query). red and blue lines trace the implicit and explicit results respectively. $\mathrm{y}$-axis represents the inspecting value (Eqn. 1).

the inner workings of LLMs. We depict Logit Lens results of the examples of Table 1 (compositional queries) and their references (corresponding second-hop queries) in Figure 2, Leveraging Eqn. 1. Note that in Figure 2, results of second-hop inputs (subfigure (e) $\sim(\mathrm{h})$ ) align well with the results in Figure 3. However, when we set our sights on results of compositional inputs (subfigure (a) (d)), we get clues about the above three error types. In (a, Distortion) we observe that the peak for $o_{1}$ does not emerge at all (probability $\sim \frac{1}{|V|}$ ), implying the distortion of the predictive information for $o_{1}$ by context. In (b, Incomplete Reasoning), though $o_{1}$ emerge in middle layers, it is not intense enough (in comparison with (f)) to arise the final result $o_{2}$. In Figure 10, we show another example where the peak probability of $o_{1}$ aligns well with the result of the reference and correctly predict $o_{2}$. In (c, Hasty Answer I) we observe that $o_{1}$ emerge at the last layer, which is too late to incorporate second-hop information to generate $o_{2}$. In (d, Hasty Answer II) although $o_{1}$ (association football) also emerges, the peak probability of $o_{1}$ is much lower than its reference (h). For comparison, we plot the Logit Lens of "the home country $\left(r_{2}\right)$ of Giorgio Chinaglia $\left(s_{1}\right)$ " for "Italy" in Figure 9, which aligns with its corresponding compositional query well, advocating that LLMs predict through short-cut. In summary, all of these errors can be attributed to improperly generating implicit reasoning results. The implicit reasoning results either (1): do not notably emerge (Distortion) or (2): emerge but not

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-04.jpg?height=274&width=371&top_left_y=1162&top_left_x=1068)

(a) compositional queries

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-04.jpg?height=277&width=377&top_left_y=1158&top_left_x=1433)

(b) the second-hop queries
Figure 3: Logit Lens inspecting results with LLaMA-27B. (a) refers to the averaged result for inputs of compositional two-hop queries and (b) refers to the averaged result for second-hop queries. x-axis refers to the layer; y-axis refers to the $0-1$ normalized probability. Yellow line and blue line refers to implicit results and explicit results respectively.

intensely or timely enough to raise the explicit reasoning results(Incomplete Reasoning and Hasty Answer).

## 4 Analyzing the Inner Hidden States of LLMs for Compositional Reasoning

Providing that LLMs are capable to perform compositional step-by-step reasoning (Hou et al., 2023), we hypothesize that they generate the implicit reasoning result $o_{1}$ (the notation is aligned with Section 2.2) in the process of compositional reasoning, before finally obtaining the explicit reasoning result $o_{2}$. We inspect inner hidden states of LLMs via Logit Lens (Section 4.1) and observe that implicit reasoning results emerge in middle layers, implying that they may play a role in the compositional reasoning process (Section 4.1). To verify
this hypothesis, we design an intervention experiment (Section 4.2) and demonstrate the emerging of $o_{1}$ has causal effect on predicting $o_{2}$ in the output layer (Section 4.2).

### 4.1 Inspecting hidden states of LLMs

Given an input of a compositional two-hop knowledge item $\left(s_{1}, r_{1}, o_{1}\right) \oplus\left(s_{2}, r_{2}, o_{2}\right)$, we denote $h_{l},(l \in[1 . . L])$ as the hidden states at the position of last input token and $l$-th layer. Leveraging Eqn. 1 we tokenize implicit result $o_{1}$ and explicit $o_{2}$ into tokens: $R_{i}$ (implicit) and $R_{e}$ (explicit), and inspect the information about $R_{i}$ and $R_{e}$ in $h_{l}$ : $L\left(h_{l}, R_{i}\right)$ and $L\left(h_{l}, R_{e}\right)$. We present the inspecting results averaging over $\mathcal{D}$ with LLaMA-2-7B in Figure 3(a). We observe that (1) both $L\left(R_{i}, h_{l}\right)$ and $L\left(R_{e}, h_{l}\right)$ reach a peak and then decline with the layer increasing; (2) the peak of $L\left(R_{i}, h_{l}\right)$ appears at the earlier layer than $L\left(R_{e}, h_{l}\right)$. Then we use the corresponding second-hop queries $\left(s_{2}, r_{2}, o_{2}\right)$ $\left(s_{2}=o_{1}\right.$ ) to repeat the inspecting experiment. The averaged result is depicted in Figure 3(b). We get the similar observations with the compositional two-hop queries, to some extent aligning their reasoning processes: both of the compositional query (implicitly containing $o_{1}$ ) and the secondhop knowledge query (explicitly containing $o_{1}$ ) generate $o_{1}$ in hidden states of middle layers before generating $o_{2}$.

The insights gleaned from the emergence of implicit results suggest a potential influence of them on compositional reasoning. In the subsequent analysis, we endeavor to elucidate how implicit reasoning results, embedded within the hidden states of intermediary layers, exert a causal impact on the generation of explicit reasoning results.

### 4.2 Verifying the Hypothesis via Intervention

We recall the notations defined before. The tokenizations of $o_{1}$ and $o_{2}$ are $R_{i}$ and $R_{e}$; the hidden state of the last token at the $l$-th layer is $h_{l}$. Accordingly, the probability distribution over the output vocabulary set $V$ (with Eqn. 2) is $p_{l}=$ $\operatorname{softmax}\left(v_{l}\right)=\operatorname{softmax}\left(h_{l} \cdot W_{u}\right) \in \mathbb{R}^{|V|}$. Our aim is to demonstrate how the information about $o_{1}$ encoded in hidden states of middle layers plays a causal role in the prediction of $o_{2}$. The technique of Intervention (Pearl, 2001; Li et al., 2023a) fits the objective, where we strategically intervene on these inner hidden states to eliminate the information related to $o_{1}$ (through Logit Lens) and observe the resultant impact on predicting $o_{2}$.
Intervention We define the intervention $\mathcal{I}_{l}$ : $h_{l} \rightarrow h_{l}^{*}$, where $h_{l}^{*}$ denotes the intervened hidden state. $v_{l}^{*}$ is the corresponding logits (through Logit Lens) of $h_{l}^{*}: v_{l}^{*}=h_{l}^{*} \cdot W_{u}$. Denoting that (before intervention) $v_{\text {min }}=\min _{0 \leq j<|V|}\left\{v_{l}[j]\right\}$, we expect $v_{l}^{*}$ meets the following constraints:

$$
v_{l}^{*}[j]=\left\{\begin{array}{lr}
v_{\min }, & j \in R_{i}  \tag{3}\\
v_{l}[j], & j \in[0 . .|V|) / R_{i}
\end{array}\right.
$$

Which means, observing from Logit Lens, we eliminate the bias on $o_{1}$ in $h_{l}^{*}$ in the computation graph and minimize the side effects on the rest tokens ${ }^{3}$. We solve the linear system $v_{l}^{*}=h_{l}^{*} \cdot W_{u}$ to get $h_{l}^{*}: h_{l}^{*}=v_{l}^{*} W_{u}^{T}\left(W_{u} W_{u}^{T}\right)^{-1}$ (in case that $W_{u} W_{u}^{T}$ is not full-rank, we use the Moore-Penrose inverse (Dresden, 1920) instead). In our implementation, we calculate the difference value for the purpose of numerical stability:

$$
\begin{equation*}
h_{l}^{*}=h_{l}+\left(v_{l}^{*}-v_{l}\right) W_{u}^{T}\left(W_{u} W_{u}^{T}\right)^{-1} \tag{4}
\end{equation*}
$$

Effect We define the effect $\mathcal{E}_{l}$ of an intervention $\mathcal{I}_{l}$ is the difference between probabilities of predicting $o_{2}$ (tokenization: $R_{e}$ ) at the output layer $L$ before and after the intervention:

$$
\begin{equation*}
\mathcal{E}_{l}=p_{L}\left[R_{e}\right]-p_{L}^{\mathcal{I}_{l}}\left[R_{e}\right] \tag{5}
\end{equation*}
$$

Ideally, we expect the intervention $\mathcal{I}_{l}$ has the effect of decreasing the probability of predicting the explicit reasoning result $o_{2}$ (i.e., $\mathcal{E}_{l}>0$ ).

Result The Intervention experiment results (averaged over $\mathcal{D}$ ) are depicted in Figure 11. For each experiment group, we set a comparison group where we intervene on $\left|R_{i}\right|$ tokens that are randomly sampled from $V$. Comparing experiment groups and comparison groups, we observe there exist apparent positive effects $\left(\mathcal{E}_{l}>0\right)$ when intervening middle layers (for both LLaMA-2 and OpenAlpaca, positive effects appear in 15 -th to 20 -th layers) for experiment groups, suggesting that the information about $o_{1}$ may be generated and utilized for generating $o_{2}$ in these layers. Meanwhile, there is nearly no notable positive effect for comparison groups across all layers. The results verify our hypothesis that the information around implicit reasoning results in middle layers play a role in predicting explicit reasoning results.[^2]

## 5 Locating Important Modules

In previous analysis, we attribute compositional reasoning errors to improperly generating implicit reasoning results. In this section, we aim to investigate if there sparsely exist some "key" modules (i.e., MHSA or MLP) ${ }^{4}$ in LLMs that are responsible for properly generating implicit reasoning results in hidden states of middle layers.

### 5.1 Locating Methodology

In Section 3, we observe that if inspecting results of the compositional query and its corresponding second-hop query align well, the compositional reasoning process is usually in smooth going. Given this, combining the key idea in Causal Mediation Analysis (Meng et al., 2022; Stolfo et al., 2023), we propose the following locating method. (1) We run the LLM twice: once with the compositional query in $\mathcal{D}_{\text {gap }}$ in the length of $T_{1}$ and once with its corresponding second-hop query in the length of $T_{2}$. For the compositional pass, we denote the module outputs in the computation graph as $\left\{\eta_{l}^{t} \mid \eta \in\{a, m\}, l \in[1 . . L], t \in\left[1 . . T_{1}\right]\right\}$ (a for MHSA, $m$ for MLP, $l$ indexing layers, $t$ indexing tokens). For the second-hop pass, we denote the outputs as $\left\{\hat{\eta}_{l}^{t} \mid \eta \in\{a, m\}, l \in[1 . . L], t \in\left[1 . . T_{2}\right]\right\}$ (2) We replace a single module output of interest in the compositional pass computation graph with its counterpart in the second-hop pass computation graph. We focus on two token positions: the last subject token (which refers to $\left(s_{1}, r_{1}\right)$ for compositional queries, e.g., "the sports associated with Giorgio Chinaglia") and the last token ${ }^{5}$. We denote the original probability of predicting $o_{2}$ as $p\left(o_{2}\right)$ and the probability after replacement as $p\left(o_{2} \mid \hat{\eta}_{l}^{t^{*}} \rightarrow \eta_{l}^{t}\right)$. (3): We define the effect of the replacement $\hat{\eta}_{l}^{t^{*}} \rightarrow \eta_{l}^{t}$ as $p\left(o_{2} \mid \hat{\eta}_{l}^{t^{*}} \rightarrow \eta_{l}^{t}\right)-p\left(o_{2}\right)$.

### 5.2 Insight

We depict the Average Indirect Effect (AIE) of replacements over modules, tokens, and layers in Figure 4. We observe that replacing the MHSA output at the position of (last-token, $18 \backslash 19$-th layer) has the largest effect on finally predicting the correct answer $o_{2}$. Interestingly, this coincides with the intervention experiment results in Figure 11, implying that MHSA modules of these positions play an important role in properly accumulating[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-06.jpg?height=223&width=374&top_left_y=254&top_left_x=1069)

(a) LLaMA-2-7B

![](https://cdn.mathpix.com/cropped/2024_06_04_70000e8dedb63a0aae36g-06.jpg?height=225&width=395&top_left_y=253&top_left_x=1433)

(b) OpenAlpaca-3B
Figure 4: AIE for replacements. "last": last token; "subject": last subject token; "mlp": replace the MLP output; "attn": replace the MHSA output. Brighter positions indicate replacements of larger effect (more important).

and leveraging implicit reasoning results.

## 6 Patching Compositional Reasoning

Grounded on the empirical insights in Section 4 and Section 5, we are poised to introduce the CREME approach, designed to correct compositional reasoning failures via editing the parameters of MHSA at the located positions. We demonstrate its superiority through comparative analyses with two recent baselines for correcting compositional reasoning (Sakarvadia et al., 2023; Ghandeharioun et al., 2024) and a a widely recognized model editing baseline (Meng et al., 2022).

Specifically, our edit objective is the MHSA output matrix at the $l$-th layer $W_{O}^{l}$ (for detailed description, please refer to Eqn. 7). Following Santurkar et al. (2021), we view $W_{O}^{l}$ as a linear associative memory (Kohonen, 1972): $W_{O}^{l} \in \mathbb{R}^{d \times d}$ operates as a key-value store for a set of vector keys $K=\left[k_{1}\left|k_{2}\right| \ldots\right]$ and corresponding vector values $V=\left[v_{1}\left|v_{2}\right| \ldots\right]$, by solving $\left(W_{O}^{l}\right)^{T} K=V$.

For a given compositional query and its corresponding second-hop query, we run the LLM twice: once with the compositional query and once with the second-hop query. In the first pass with the compositional query, the input of $W_{O}^{l}$ at the last token position is $k_{*} \in \mathbb{R}^{d \times 1}$; in the second pass with the corresponding second-hop query, the output of $W_{O}^{l}$ at the last token position is $v_{*} \in \mathbb{R}^{d \times 1}$. We aim to edit $W_{O}^{l}$ to $\hat{W}_{O}^{l}$ such that:

$\operatorname{minimize}\left\|\left(\hat{W}_{O}^{l}\right)^{T} K-V\right\|_{F}^{2}$ and $\left(\hat{W}_{O}^{l}\right)^{T} k_{*}=v_{*}$, where the Frobenius norm guarantees consistent predictions on irrelevant queries while the constraint implements the edit as an insertion of $\left(k_{*}, v_{*}\right)$ into the linear memory $\hat{W}_{O}^{l}$. Following Meng et al. (2022), we derive a closed form solution: $\hat{W}_{O}^{l}=W_{O}^{l}+\left(C^{-1} k_{*}\right)^{T} \Lambda^{T}$ where $C=$
$K K^{T}$ is a constant to estimate the uncentered covariance of $k$ (note that $k$ is randomly sampled from Wikipedia to represent irrelevant queries) and $\Lambda=\left(v_{*}-\left(W_{O}^{l}\right)^{T} k_{*}\right) /\left(C^{-1} k_{*}\right)^{T} k_{*}$. Hopefully, the edited LLMs are able to properly generate implicit reasoning results at the located position and thus alleviate failures of compositional reasoning.

### 6.1 Dataset, Baseline and Evaluation Metric

Dataset The dataset $\mathcal{D}_{\text {edit }}$ we use for editing and evaluating LLMs is built based on the $\mathcal{D}_{\text {gap }}$ filtered in Section 3. For each example in $\mathcal{D}_{\text {edit }}$, it has the following fields: (1) Original input $I_{o}$ is a cloze test form of the compositional two-hop query. Accordingly, we also have the correct answer (ground-truth) and the originally predicted wrong answer for $I_{o}: A_{o}$ and $\widetilde{A_{o}}$, respectively ${ }^{6}$. In the experiment, we use $I_{o}$ and its corresponding second-hop query to edit the LLM. (2) Paraphrasing input $I_{p}$ is a paraphrase of $I_{o}$. Note that $A_{o}$ and $\widetilde{A_{o}}$ are also applicable to $I_{p}$. (3) Generalization input $I_{g}$ is a compositional two-hop query where its first-hop sub-knowledge is shared with $I_{o}$ while the second-hop sub-knowledge is different from $I_{o}$. We denote the correct answer for $I_{g}$ is $A_{g}$. (4) Irrelevant input $I_{i}$ is a compositional two-hop query that is irrelevant to $I_{o}$ and does not share the final answer with $I_{o}$. Detailed information about $\mathcal{D}_{\text {edit }}$ is available in Appendix B.

Baseline We choose two related works in the field of correcting compositional reasoning errors through manipulating the inner workings of LLMs: Memory Injection (Sakarvadia et al., 2023) and CoT-PatchScopes (Ghandeharioun et al., 2024) as our baselines. Memory Injection enhances the compositional reasoning through explicitly injecting the implicit reasoning result (so-called "memory") into the hidden states in the residual stream. CoTPatchScopes corrects the compositional reasoning through mimicking the noted Chain-of-Thought (CoT) reasoning (Wei et al., 2022) to re-route forward computation. Besides, we also compare CREME with ROME (Meng et al., 2022), a stateof-the-art model editing method. Detailed implementations are available in Appendix D.

Evaluation Metric In order to comprehensively validate the effectiveness of CREME, we propose four evaluation metrics: Correction, Paraphrasing, Generalization and Specificity. Following Sakar-[^4]

vadia et al. (2023), all the metrics are formulated on the basis of Improvement Percentage (IP), which is calculated as $\operatorname{IP}(I, A)=\frac{p_{\mathcal{M}} *(A \mid I)-p_{\mathcal{M}}(A \mid I)}{p_{\mathcal{M}}(A \mid I)}$. This formula quantifies the enhancement in prediction probability of an answer $A$ given an input query $I$, facilitated by the post-edit LLM $\mathcal{M}^{*}$ in comparison to the pre-edit LLM M. Specificially, Correction quantifies $\operatorname{IP}\left(I_{o}, A_{o}\right)$ (larger is better); Paraphrasing is $\operatorname{IP}\left(I_{p}, A_{o}\right)$ (larger is better); Generalization is $\operatorname{IP}\left(I_{g}, A_{g}\right)$ (larger is better) and Specificity is $\operatorname{IP}\left(I_{i}, A_{o}\right)$ (smaller is better). CoT-PatchScopes, due to its nature of input-dependent, only fits the Correction evaluation. We report the average results over $\mathcal{D}_{\text {edit }}$ in Section 6.2.

### 6.2 Experiment Results

The main experiment results are shown in Table 2. For brevity, we omit $\times 100 \%$ for each IP value. We observe that CREME achieves better performance than baselines on all metrics, not only achieving notable improvement on $I_{o}$ (the query used for editing), but also effectively generalizing to $I_{p}$ (paraphrased queries). Interestingly, editing with $I_{o}$ also improves (at most $+366 \%$ ) the compositional reasoning on $I_{g}$ (only sharing first-hop knowledge with $I_{o}$ ), demonstrating the effectiveness of CREME on generating proper implicit reasoning results in middle layers. Besides, the Specificity score of CREME is low, showing that the CREME does not aimlessly improve the probability of predicting $A_{o}$ for irrelevant inputs $I_{i}$. In comparison, the Correction score of Memory Injection $(+221 \%$ for LLaMA-2) is almost the same with the original paper ${ }^{7}$ while we find it is less effective to generalize to $I_{p}$ and $I_{g}$. Moreover, its high Specificity score implies its shortcoming of aimlessly improving the probability of predicting $A_{o}$. We also show $\operatorname{IP}\left(I_{o}, \widetilde{A_{o}}\right)$ in Figure 6. A good correction method should have little positive improvement on predicting the wrong answer $\widetilde{A_{o}}$. We observe that $p\left(\widetilde{A_{o}} \mid I_{o}\right)$ approximately remains unchanged with CREME, while is apparently enlarged with Memory Injection and PatchScopes.

One natural concern arises regarding the sufficiency of Correction and Paraphrasing metrics in practice. To this end, we evaluate the probability of an event where the probability of predicting $A_{o}$[^5]

| Evaluation Metrics | $\mathbf{C}(\uparrow)$ | $\mathbf{P}(\uparrow)$ | $\mathbf{G}(\uparrow)$ | $\mathbf{S}(\downarrow)$ |
| :--- | :---: | :---: | :---: | :---: |
| LLaMA-2-7B | $3.2 \%$ | $2.3 \%$ | $13.1 \%$ | $0.3 \%$ |
| CoT-PatchScopes | +1.20 | - | - | - |
| Memory Injection | +2.21 | +0.30 | +0.32 | +26.72 |
| CREME (Ours) | $\mathbf{+ 1 7 . 0}$ | $\mathbf{+ 7 . 9 9}$ | $\mathbf{+ 1 . 2 7}$ | $\mathbf{+ 0 . 8 6}$ |
| OpenAlpaca-3B | $7.2 \%$ | $7.0 \%$ | $13.5 \%$ | $0.6 \%$ |
| CoT-PatchScopes | +0.91 | - | - | - |
| Memory Injection | +0.98 | +0.45 | +0.75 | +2.93 |
| CREME (Ours) | $\mathbf{+ 4 3 . 3}$ | $\mathbf{+ 2 3 . 7 1}$ | $\mathbf{+ 3 . 6 1}$ | $\mathbf{+ 1 . 2 4}$ |

Table 2: CREME versus baselines with the proposed four metrics: $\mathbf{C}$ for "Correction", $\mathbf{P}$ for "Paraphrasing", $\mathbf{G}$ for "Generalization" and $\mathbf{S}$ for "Specificity".

| Input Types | Correction Input $I_{o}$ | Paraphrasing Input $I_{p}$ |
| :--- | :---: | :---: |
| LLaMA-2-7B |  |  |
| Original | $59.5 \%$ | $35.7 \%$ |
| +CoT-PatchScopes | $53.0 \%$ | - |
| +Memory Injection | $63.0 \%$ | $40.3 \%$ |
| +CREME(Ours) | $\mathbf{8 7 . 5} \%$ | $\mathbf{5 2 . 9} \%$ |
| OpenAlpaca-3B |  |  |
| Original | $58.0 \%$ | $42.7 \%$ |
| +CoT-PatchScopes | $57.3 \%$ | - |
| +Memory Injection | $58.7 \%$ | $43.8 \%$ |
| +CREME(Ours) | $\mathbf{9 5 . 3} \%$ | $\mathbf{7 0 . 5} \%$ |

Table 3: The event probability of $p\left(A_{o}\right)>p\left(\widetilde{A_{o}}\right)$.

exceeds that of predicting $\widetilde{A_{o}}: p\left(A_{o}\right)>p\left(\widetilde{A_{o}}\right)$. We compare CREME against baselines using this new metric and two types of input ( $I_{o}$ and $I_{p}$ ) in Table 3 . The results underscore CREME's efficacy in significantly improving the event probability, thereby outperforming the unedited LLM and establishing a considerable lead over the two baselines.

Although CREME is not comparable to traditional model editing methods (the latter require $A_{o}$ for editing, while CREME does not), we compare CREME with a well-regarded model editing method: ROME (Meng et al., 2022) for a comprehensive investigation. The results ${ }^{8}$ are shown in Table 4. Our findings reveal that while ROME marginally surpasses CREME in terms of the Correction score of ROME - attributable to ROME's direct application of $A_{o}$ for editing and its optimization procedure designed to entirely fit $p\left(A_{o}\right)$ - CREME performs obviously better than ROME in paraphrased, generalization and irrelevant cases. This highlights the effectiveness of CREME on correcting compositional reasoning.

In Figure 5, we show the effects of editing different layers, where results align well with the results of the locating experiment (Figure 4).[^6]

| Method | ROME (w. ground-truth) | CREME (w.o. ground-truth) |
| :--- | :---: | :---: |
| Correction $(\uparrow)$ | $\mathbf{9 8 . 0} \%$ | $95.3 \%$ |
| Paraphrasing $(\uparrow)$ | $62.5 \%$ | $\mathbf{7 0 . 5} \%$ |
| Generalization $(\uparrow)$ | +1.24 | $\mathbf{+ 3 . 6 1}$ |
| Specificity $(\downarrow)$ | +5.37 | $+\mathbf{1 . 2 4}$ |

Table 4: Comparing CREME and ROME (Meng et al., 2022) (applied on OpenAlpaca-3B). "w. ground-truth" refers to that ROME requires $A_{o}$ for editing.

## 7 Related Work

Compositional Reasoning of LLMs LLMs fail to solve a large proportion of compositional multihop questions, even successfully solving all their single-hop sub-questions (Press et al., 2023; Dziri et al., 2023). Early works towards mitigating this issue typically prepend crafted demonstration exemplars containing the "thought process" of solving the compositional query step-by-step and encourage LLMs to imitate the process via in-context learning (Nye et al., 2021; Wei et al., 2022; Zhou et al., 2023; Drozdov et al., 2023; Press et al., 2023). Recent works turn to inspect the inherent compositional reasoning mechanism (Hou et al., 2023) of LLMs. Sakarvadia et al. (2023) manually injects implicit reasoning results into LLMs at the middle layers to correct compositional reasoning failures. (Ghandeharioun et al., 2024) fixes compositional reasoning errors through re-routing inner hidden representations in the computation graph to mimic chain-of-thought reasoning process. Nonetheless, their interventions in the reasoning process are rough so that the improvement is limited and hardly generalize to other related queries. To this end, we elaborately analyze the cause of compositional reasoning failures, locate a small set of parameters in LLMs that are responsible for such failures and precisely edit them to correct such failures.

## 8 Conclusion

In this paper we study and patch the compositional reasoning of LLMs. Through examining failure instances and conducting diverse analysis experiments, we demonstrate successful compositional reasoning within LLMs hinges on its awareness of generating and leveraging implicit reasoning results. Moreover, we locate few important MHSA modules in LLMs that are responsible for properly generating and leveraging implicit reasoning results via causal mediation analysis. To this end, we propose CREME, to compositional reasoning failures via editing the located MHSA parameters and empirically demonstrate its superiority.

## Limitations

Technique Part of our observation and experiments in Section 4 and Section 3 are on the basis of Logit Lens (nostalgebraist, 2020). Though Logit Lens is a widely used tool for analyzing the inner workings of language models (Geva et al., 2022, 2023; Dar et al., 2023; Sakarvadia et al., 2023; Katz and Belinkov, 2023; Ram et al., 2023), we acknowledge that it is only an approximate way to interpret the information in the inner hidden states of the LLMs (Belrose et al., 2023). Nonetheless, the residual stream architecture of Transformers guarantees that Logit Lens makes sense to a large extent. In our experiments, we try to conduct experiments with different techniques for the crossvalidation of our observations and conclusions (By way of example, the observations in the locating experiments (Section 5) to some extent validate the observations of the intervention experiments in Section 4.2).

LLM Due to the constraints of available computation resource, we are able to conduct most of our experiments with LLMs of seven billion scale (LLaMA-2-7B (Touvron et al., 2023)) and three billion scale (OpenAlpaca-3B (Su et al., 2023b)). Both of these two LLMs are fully open-sourced and popular in academic community and real-world applications (Wu et al., 2023; Wang et al., 2024; Hou et al., 2023; Li et al., 2023b). In the future work, we aim to validate our conclusions on LLMs of larger scale.

Task In this work, we mainly focus on the task of the compositional reasoning on factual knowledge, which is generally pursued by lots of research works (Misra et al., 2023; Press et al., 2023; Zhong et al., 2023; Sakarvadia et al., 2023). We aim to validate our main conclusion about the significance of implicit reasoning results in the compositional reasoning process in other types of compositional reasoning task (Lu et al., 2023; Hou et al., 2023)(e.g., Arithmetic Reasoning for multiple operands) in the future work.

## Ethical Considerations

We study the inner workings for the compositional reasoning of LLMs, which helps the blackbox LLMs become more transparent and trustworthy (Räuker et al., 2023). The CREME method introduced in this work is originally designed for correcting the compositional reasoning failures of
LLMs. CREME only require slightly update a small set of parameters in LLMs and can generalize to a number of related queries (paraphrased queries or compositional queries sharing first-hop knowledge with the query used for conducting CREME). However, just like traditional model editing methods (De Cao et al., 2021; Mitchell et al., 2022; Meng et al., 2022, 2023), it may also be utilized to insert inaccurate (or out-of-date) information into the pretrained LLMs.

## References

Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 2023. Eliciting latent predictions from transformers with the tuned lens.

Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84938502, Dublin, Ireland. Association for Computational Linguistics.

Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. 2023. Analyzing transformers in embedding space. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1 . Long Papers), pages 16124-16170, Toronto, Canada. Association for Computational Linguistics.

Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 64916506, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Arnold Dresden. 1920. The fourteenth western meeting of the American Mathematical Society. Bulletin of the American Mathematical Society, 26(9):385 - 396.

Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. 2023. Compositional semantic parsing with large language models. In The Eleventh International Conference on Learning Representations.

Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality. In Thirty-seventh Conference on Neural Information Processing Systems.

Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12216-12235, Singapore. Association for Computational Linguistics.

Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 30-45, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are keyvalue memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484-5495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. 2024. Patchscopes: A unifying framework for inspecting hidden representations of language models.

Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. 2023. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. In Thirtyseventh Conference on Neural Information Processing Systems.

hiyouga. 2023. Fastedit: Editing llms within 10 seconds https://github.com/hiyouga/FastEdit.

Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, and Mrinmaya Sachan. 2023. Towards a mechanistic interpretation of multi-step reasoning capabilities of language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4902-4919, Singapore. Association for Computational Linguistics.

Yiming Ju and Zheng Zhang. 2023. Klob: a benchmark for assessing knowledge locating methods in language models.

Shahar Katz and Yonatan Belinkov. 2023. VISIT: Visualizing and interpreting the semantic information flow of transformers. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14094-14113, Singapore. Association for Computational Linguistics.

Teuvo Kohonen. 1972. Correlation matrix memories. IEEE Transactions on Computers, C-21(4):353-359.

Brenden M. Lake and Marco Baroni. 2023. Human-like systematic generalization through a meta-learning neural network. Nature, 623(7985):115-121.
Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023a. Emergent world representations: Exploring a sequence model trained on a synthetic task. In The Eleventh International Conference on Learning Representations.

Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023b. Inferencetime intervention: Eliciting truthful answers from a language model. In Thirty-seventh Conference on Neural Information Processing Systems.

Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23.

Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems.

Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. 2023. Massediting memory in a transformer. In The Eleventh International Conference on Learning Representations.

Kanishka Misra, Cicero Nogueira dos Santos, and Siamak Shakeri. 2023. Triggering multi-hop reasoning for question answering in language models using soft prompts and random walks. In Findings of the Association for Computational Linguistics: ACL 2023, pages 972-985, Toronto, Canada. Association for Computational Linguistics.

Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. 2022. Fast model editing at scale. In International Conference on Learning Representations.

nostalgebraist. 2020. interpreting gpt: the logit lens. https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens.

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models.

Judea Pearl. 2001. Direct and indirect effects. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI'01, page 411-420, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.

Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational

Linguistics: EMNLP 2023, pages 5687-5711, Singapore. Association for Computational Linguistics.

Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov, Jonathan Berant, and Amir Globerson. 2023. What are you token about? dense retrieval as distributions over the vocabulary. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 24812498, Toronto, Canada. Association for Computational Linguistics.

Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. 2023. Toward transparent ai: A survey on interpreting the inner structures of deep neural networks.

Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, and Ian Foster. 2023. Memory injections: Correcting multi-hop reasoning failures during inference in transformer-based language models. In Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 342-356, Singapore. Association for Computational Linguistics.

Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Aleksander Madry. 2021. Editing a classifier by rewriting its prediction rules. In Advances in Neural Information Processing Systems, volume 34, pages 23359-23373. Curran Associates, Inc.

Noam Shazeer. 2020. Glu variants improve transformer.

Alessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan. 2023. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7035-7052, Singapore. Association for Computational Linguistics.

Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023a. Roformer: Enhanced transformer with rotary position embedding.

Yixuan Su, Tian Lan, and Deng Cai. 2023b. Openalpaca: A fully open-source instruction-following model based on openllama. https://github.com/ yxuansu/OpenAlpaca.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.

Yiqun Wang, Sile Hu, Yonggang Zhang, Xiang Tian, Xuesong Liu, Yaowu Chen, Xu Shen, and Jieping Ye. 2024. How large language models implement chain-of-thought?

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc.

Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. 2023. Interpretability at scale: Identifying causal mechanisms in alpaca. In Thirty-seventh Conference on Neural Information Processing Systems.

Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen. 2024. A comprehensive study of knowledge editing for large language models.

Zexuan Zhong, Zhengxuan Wu, Christopher Manning, Christopher Potts, and Danqi Chen. 2023. MQuAKE: Assessing knowledge editing in language models via multi-hop questions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15686-15702, Singapore. Association for Computational Linguistics.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. In The

Eleventh International Conference on Learning Representations.
