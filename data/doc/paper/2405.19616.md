# Easy Problems That LLMs Get Wrong 

Sean Williams*<br>sean@autogenai.com

James Huckle*<br>james@autogenai.com

May 2024


#### Abstract

We introduce a comprehensive Linguistic Benchmark designed to evaluate the limitations of Large Language Models (LLMs) in domains such as logical reasoning, spatial intelligence, and linguistic understanding, among others. Through a series of straightforward questions, it uncovers the significant limitations of wellregarded models to perform tasks that humans manage with ease. It also highlights the potential of prompt engineering to mitigate some errors and underscores the necessity for better training methodologies. Our findings stress the importance of grounding LLMs with human reasoning and common sense, emphasising the need for human-in-the-loop for enterprise applications. We hope this work paves the way for future research to enhance the usefulness and reliability of new models.


## 1 Introduction

Large Language Models (LLMs) have emerged as a transformative technology with utility across a range of applications. Despite their impressive capabilities, LLMs exhibit notable deficiencies that hinder their ability to comprehend and reason robustly, raising questions about the breadth and depth of their isolated applicability without human oversight.

In response to these challenges, we propose a Linguistic Benchmark comprising of 30 questions designed to asses their well-documented limitations across domains such as spatial reasoning, linguistic understanding, relational thinking, mathematical reasoning, and knowledge of basic scientific concepts. This benchmark is not merely an academic exercise but a tool to gauge the current capabilities of LLMs in areas they are well-documented to fail.

Furthermore, we explore an often-underestimated facet of interacting with these models: prompt engineering. By refining the manner in which tasks are presented to LLMs, we can significantly influence their output, guiding them towards more accurate and logically sound responses. Our work underscores the need for continued innovation in the field, extending what these extraordinary models can already achieve while conscientiously addressing their limitations.

The code relevant to this paper can be found at our GitHub repository.[^0]

## 2 Background and Related Work

### 2.1 Known LLM Limitations

### 2.1.1 Linguistic Understanding

LLMs have demonstrated difficulties with linguistic understanding, stemming primarily from the limitations of their intrinsic operational mechanisms. These models often misinterpret or overlook the nuanced meanings conveyed in human language. This results in inaccuracies or misjudgments when dealing with linguistic tasks or when parsing sentences that demand a deeper grasp of idiomatic nuances. [1]

### 2.1.2 Common Sense

A pivotal hindrance of LLMs lies in their absence of embodied experience, a factor Philosopher Hubert Dreyfus highlighted as crucial for the development of common sense in humans [2]. Unlike humans, who engage with the physical world through a rich palette of sensory experiences such as visual, auditory, and tactile stimuli, LLMs operate without sensory perception. This disembodied state restricts their capacity to learn the subtleties inherent to commonsense reasoning [3].

### 2.1.3 Contextual Understanding

AI's inability to handle context-sensitive reasoning was also critiqued by Dreyfus, which remains relevant to today's LLMs [4]. Correct reasoning is deeply intertwined with the ability to understand the often implicit context in which something relates.

### 2.1.4 Visual-Spatial Reasoning

Visual-spatial reasoning entails the ability to mentally visualise objects and understand their relationship within space. LLMs lack fundamental spatial awareness, so explaining the steps needed to navigate from one point to another in physical space or understanding the spatial configuration of objects remains a complex challenge for these models, showcasing a significant gap in a vital area of human intelligence. [5]

### 2.1.5 Mathematical Reasoning

LLMs express fragility in conducting simple mathematical reasoning, with word-based tasks that involve counting to ten often posing a significant challenge. While they can often provide correct answers to sophisticated mathematical queries, they fundamentally lack a rules-based counting system. They must outsource their calculations to other tooling, such as computer code or a calculator. [6]

### 2.1.6 Popular Science Knowledge

LLMs are particularly vulnerable to propagating and reinforcing inaccuracies found within their training data, including scientific misconceptions or outdated information commonly perpetuated online. An LLM's approach to generating content is heavily reliant on the frequency and presentation of information encountered during their training, which can lead to the uncritical replication of errors. This propensity not only highlights the limitations in their comprehension
abilities but also underscores the importance of curating and updating the data these models are trained on to mitigate the dissemination of incorrect or misleading information. [7]

### 2.1.7 Relational Understanding

Understanding and interpreting relationships between entities, whether temporal, causal, or conceptual, is another area where LLMs face difficulties. Their interpretations often lack the depth and nuance of human understanding, as solving relational problems often requires an intuition that LLMs inherently lack. [8]

### 2.1.8 Logical Reasoning

LLMs are trained on an encompassing array of knowledge; however, this training approach does not guarantee proficiency in logical reasoning at inference time. Previous research has explored the logical capabilities of LLMs with mixed findings and suggests that LLMs can mimic reasoning up to a certain level but lack the reliability of human-like reasoning [9].

### 2.1.9 Overfitting

Overfitting is a well-documented phenomenon in machine learning, where models excessively adapt to the idiosyncrasies of the training data at the expense of broader generalisation. It is the belief that pre-trained models should excel in interpolating within the bounds of their training data but that extrapolation outside of those bounds is more difficult [10].

### 2.2 Popular Benchmark Obsession

Various technology providers have developed their own LLMs, leading to a competitive ecosystem of models. These models are typically evaluated using standard benchmarks, and any advancements receive considerable media attention. There is a concern that these popular benchmarks, even those involving independent human evaluation, may encourage a myopic focus on "ranking" at the expense of optimising for holistic performance. This is a plausible manifestation of Goodhart's maxim, commonly stated as - "When a measure becomes a target, it ceases to be a good measure."

### 2.2.1 Performance Gap

Hands-on experience with different models often exposes a larger performance gap, both between the models and in absolute terms, than is indicated by popular benchmarks, highlighting the possible inadequacies of these tests [11]. Novel benchmarks that can expose the truer magnitude of their difference in performance by quizzing them with difficult tasks could be a valuable asset to the AI community and to commercial enterprises that increasingly rely on these models.

## 3 Methodology

This section presents a collection of questions developed to be easy for human adults to answer but challenging for LLMs. These questions serve as a linguistic benchmark to examine model performance in several key domains where they have known limitations. This benchmark is useful for monitoring the performance of LLMs over time and highlighting their failure modes.

### 3.1 Linguistic Benchmark

All benchmark questions can be found in the Appendix 9.1 .

### 3.1.1 Question Taxonomy

| Question Type | Description |
| :--- | :--- |
| Puzzle | Logic-type puzzles that mimic the structure of popular questions <br> found online but differ significantly in one or more critical aspects that <br> make the questions much easier for humans. |
| Spatial | Requires visualising the arrangement or relative positions of objects in <br> space, such as determining the order or position of items or simple <br> navigation. |
| Relational | Involve understanding and inferring relationships or hierarchies <br> between objects, concepts, or entities based on provided information. |
| Counting | Simple numerical calculations such as counting to a maximum of ten <br> or understanding quantities. |
| Linguistic | Tests the understanding and use of language, including forming <br> sentences with specific constraints or identifying unique <br> characteristics of words and phrases. |
| Popular science | Straightforward questions that test for common scientific and <br> mathematical misconceptions |

Table 1: Linguistic Benchmark Question Types

### 3.1.2 Benchmark Inclusion Qualification

Inclusion in the Linguistic Benchmark mandates that a question must be deemed easy for the average adult to answer, align with one of the specified taxonomic classifications, and, upon evaluation, result in at least one of the designated LLMs failing to produce a correct response. Rather than establishing a formal definition of "easy," this criterion was applied based on a best-efforts basis.

### 3.2 LLM Selection and Hyperparameters

For our research, we selected an array of popular Large Language Models (LLMs), encompassing offerings from industry leaders such as OpenAI, Anthropic, Mistral, Google, and Meta. This assortment comprises both proprietary and open-source models.

| Provider | Model Version | Inference Type |
| :--- | :--- | :--- |
| OpenAI | GPT-4 Turbo Preview <br> gpt-4-0125-preview | OpenAI API |
| Anthropic | Claude 3 Opus <br> claude-3-opus-20240229 <br> Mistral Large | Anthropic API |
| Mistral | mistral-large-latest <br> Mistral 8x22B <br> open-mixtral-8x22b <br> Gemini - Pro 1.0 | Mistral API |
| Google | gemini-1.0-pro-002 <br> Gemini - Pro 1.5 <br> Google | GCP Vertex API |
| Meta | Llama 3 70B <br> Llama 3 70B Instruct v1-ODT | GCP Vertex API |

Table 2: Comparison of language model settings

### 3.2.1 Inference Type

We queried most of the models via the API offered directly by the providers to achieve a stable and unaltered performance, with the exception of Meta Llama 3 70B, which was queried using AWS Bedrock. To the best of our knowledge, none of the models were quantised, so we will assume they were running at full precision.

### 3.2.2 Hyperparameters

All of the model hyperparameters were set to their default values at the time of testing, with the exception of the 'temperature', which was set to zero where it was present. A 'temperature' of zero is preferred because the output becomes mostly deterministic (see Appendix 9.5 for more details), aiding study reliability and reproducibility. Additionally, any absolute positive value for 'temperature' may be represented differently across each LLM architecture, some of which are closed-source and cannot be fully known.

The authors understand that higher temperatures would allow for a wider variety of answers and, therefore, alter the probability that a model could produce a more accurate answer due to a more thorough discovery of the token distribution space (this is addressed further in Section 7 , "Future Work and Limitations"). However, some studies suggest that increasing the temperature "does not have a statistically significant impact on LLM performance for problem-solving tasks" [12].

### 3.3 Evaluation Process

Models underwent evaluation against a structured scoring framework for each question within the Linguistic Benchmark. This framework is designed to assess the precision of answers, accuracy of reasoning, and conformity to logical principles. The evaluation process was conducted from April 14th to April 28th, 2024.

### 3.3.1 Scoring Criteria

The authors employed a predefined scoring system, ranging from $0 \%$ to $100 \%$, to evaluate the responses. This assessment framework is designed to gauge various aspects of the responses, including: the accuracy of the answer provided, the soundness of the reasoning process, the relevance and usefulness of the information given, and the presence of any logical discrepancies. The system aims to deduct points from responses that present both correct and incorrect answers, which impacts their credibility.

| Score | Marking Criteria |
| :---: | :--- |
| $100 \%$ | The response contains the correct answer only with a correct thought process <br> and no logical inconsistencies. |
| $80 \%$ | The response contains the correct answer only with a correct thought process <br> with some logical inconsistencies. |
| $60 \%$ | The response contains the correct answer only but with an incorrect thought <br> process. |
| $40 \%$ | The response contains an incorrect answer anywhere but also provides a cor- <br> rect answer or correct thought process with minimal logical inconsistencies. |
| $20 \%$ | The response contains an incorrect answer anywhere but provides enough <br> helpful information to plausibly reach a correct answer. |
| $0 \%$ | The response contains an incorrect answer, too much unhelpful information, <br> or not enough helpful information to plausibly reach a correct answer. |

Table 3: Scoring Rubric

### 3.3.2 Scoring Methods

The primary evaluation method involved bind manual scoring of each question by the authors. This methodology was chosen because whilst using an LLM for scoring open-ended responses offers convenience, it frequently lacks the necessary rigour and reliability. Nonetheless, automated evaluations were attempted and are presented as supplementary material. The code and prompt templates used for this process can be found in the paper's GitHub repository for further reference and examination.

### 3.3.3 Human Benchmark

As of now, 14 individuals have completed our quiz and have thus contributed to the human benchmark score of our Linguistic Benchmark, achieving an average score of $86 \%$. We would be honoured to have you as part of this study, so if you are interested, we kindly invite you to take our Online Quiz

## 4 Results

### 4.1 LLM Responses

To maintain brevity, we have presented the responses from each provider's top-performing model for the first ten out of the thirty questions in our benchmark. We also provide a typical correct human response for comparison. Responses can be found in the Appendix 9.2

The full set of responses can be found at our GitHub repository

### 4.2 Table of Results

The results were aggregated, with scores averaged across all questions. To assess the reliability of these findings, bootstrapped confidence interval analysis was performed with a sample size of 10,000 , operating at a $95 \%$ confidence level.

| Model | Average Score | CI (95\%) |
| :--- | :---: | :---: |
| GPT-4 Turbo | $38 \%$ | $[23 \%, 55 \%]$ |
| Claude 3 Opus | $35 \%$ | $[21 \%, 52 \%]$ |
| Gemini 1.5 Pro | $30 \%$ | $[15 \%, 45 \%]$ |
| Mistral Large | $28 \%$ | $[15 \%, 42 \%]$ |
| Llama 3 70B | $27 \%$ | $[14 \%, 43 \%]$ |
| Mistral 8x22B | $20 \%$ | $[7 \%, 34 \%]$ |
| Gemini 1.0 Pro | $16 \%$ | $[6 \%, 29 \%]$ |

Table 4: Performance on Linguistic Benchmark (30)

![](https://cdn.mathpix.com/cropped/2024_06_04_efb1319bbc412a29ce58g-07.jpg?height=928&width=1585&top_left_y=1595&top_left_x=241)

Figure 1: LLM Linguistic Benchmark Performance

### 4.2.1 Automated Scoring Results

See Appendix 9.3

### 4.3 Clarifying Questions Improved Performance

Subsequent experiments involved a multi-step process starting with the models requesting clarifying questions to enhance their comprehension of the original queries. Once these clarifying questions were addressed with relevant answers, the models were asked to respond to the initial questions.

## An example can be found in the Appendix 9.4 .

### 4.3.1 Improvement in Results

Asking the model to provide clarification questions resulted in a relative improvement of $40.7 \%$ across the models tested. In total, 71 answers demonstrated improvements, 128 answers remained unchanged, and 16 answers suffered regressions.

|  | Average Score | Change from Baseline |  |
| :--- | ---: | ---: | ---: |
|  |  | Absolute | Relative |
| GPT-4 Turbo | $52 \%$ | $+14 \%$ | $+36.8 \%$ |
| Claude 3 Opus | $48 \%$ | $+13 \%$ | $+37.1 \%$ |
| Llama 3 70B | $47 \%$ | $+20 \%$ | $+74.1 \%$ |
| Mistral Large | $39 \%$ | $+11 \%$ | $+39.3 \%$ |
| Mistral 8x22B | $38 \%$ | $+18 \%$ | $+90.0 \%$ |
| Gemini 1.5 Pro | $32 \%$ | $+2 \%$ | $+6.7 \%$ |
| Gemini 1.0 Pro | $17 \%$ | $+1 \%$ | $+6.3 \%$ |

Table 5: Performance after clarifying questions

## 5 Discussion

Our research reveals that Large Language Models (LLMs) often struggle to answer questions that adult humans find straightforward and intuitive. This fact poses significant challenges for their use in areas that demand strong logical reasoning skills and operation without human supervision. Our results indicate that the present training methods do not sufficiently equip LLMs for tasks requiring novel forms of reasoning. Moreover, it's hard to predict how and when they might fail at such tasks.

### 5.1 Common LLM Failings

### 5.1.1 Overfitting

The models frequently yielded incorrect, convoluted, and verbose responses to our logic-based puzzles. They would often default to solutions corresponding to the original versions of these puzzles found online instead of correctly addressing our benchmark questions - which are deliberately modified versions. This behaviour underscores a tendency towards LLMs overfitting
their web-based training corpus, adversely impacting their proficiency to generate accurate responses to novel problems (e.g. Question 2, Claude 3 Opus, Appendix 9.2.2).

Question: Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a gold bar; behind the others, rotten vegetables. You pick a door, say No.1, and the host asks you "Do you want to pick door No. 2 instead?" Is it to your advantage to switch your choice?

Claude 3 Opus Answer: "This is a classic probability problem known as the "Monty Hall problem." The answer is that it is always to your advantage to switch your choice. Here's why: ..."

### 5.1.2 Lack of Logic or Common Sense

There were examples of illogical statements and commonsense inconsistencies within a sentence (e.g. Question 4, Gemini 1.0 Pro, Appendix 9.2.4).

“... Now the farmer can safely leave the wolf with the goat because the cabbage is no longer a threat. ..."

### 5.1.3 Lack of Spatial Intelligence

Problems involving simple visual-spatial skills, such as those that required understanding the movement of a revolving cylinder or an arrangement of people, caused problems (e.g. Question 22, GPT-4 Turbo, GitHub repository).

Question: I'm in London and facing west, is Edinburgh to my left or my right?

GPT-4 Turbo Answer: "If you're in London facing west, Edinburgh would be to your left, as it is located to the north of London."

### 5.1.4 Incorrect Mathematical Reasoning

Some answers displayed a fundamental misunderstanding of counting principles and performed arithmetic errors that led to incorrect conclusions. This failure mode was identified in problems that require accurate numerical manipulation or enumeration (e.g. Question 23, GPT-4 Turbo, GitHub repository).

Question: Count the number of occurrences of the letter 'L' in the word -

'LOLLAPALOOZA'.

GPT-4 Turbo Answer: “... There are 5 'L's in the word 'LOLLAPALOOZA'"

### 5.1.5 Poor Linguistic Understanding

Challenges in understanding nuances and interpretations of simple linguistic tasks were evident. This showcased a limitation in LLMs' processing of linguistic constraints, reflecting a gap in their understanding of language at depth (e.g. Question 8, Mistral Large, Appendix 9.2.8).

Question: Write me a sentence without any words that appear in The Bible.

Mistral Large Answer: "Intricate quantum particles danced in the nebulous cosmos, transcending conventional spatiotemporal dimensions."

### 5.1.6 Popular Science

The LLMs struggled with appropriately addressing questions common in popular science, failing to apply scientific knowledge accurately or misunderstanding the fundamental principles involved. This issue becomes particularly evident in tasks that demand an intuitive grasp of physical laws, chemical properties, or biological processes (e.g. Question 9, GPT-4 Turbo, Appendix 9.2 .9 .

Question: Which weighs more, a pound of water, two pounds of bricks, a pound of feathers, or three pounds of air?

GPT-4 Turbo Answer: "Two pounds of bricks weigh more than a pound of water, a pound of feathers, or three pounds of air. ..."

### 5.1.7 Relational Misunderstandings

LLMs often fail to understand or process relational contexts correctly, leading to incorrect interpretations and conclusions about the relationships between entities or concepts. Tasks that required an understanding of hierarchical relationships or include cause-and-effect dynamics were especially troublesome (e.g. Question 21, GPT-4 Turbo, GitHub repository).

Question: Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?

GPT-4 Turbo Answer: "Sally has 2 sisters. Each of her brothers has 2 sisters: Sally and another sister, making a total of 3 girls in the family."

### 5.1.8 Illogical Chain of Thought

There were examples of logical inconsistencies and falsehoods within the Chain of Thought (CoT) process, contradicting the final answer. This highlights the fragility of CoT in adversarial settings. (e.g. Question 3, Claude 3 Opus, Appendix 9.2.3).

"From a probability standpoint, it does not matter whether he spins the chambers again or not. Here's why:

Scenario 1 (No spin): ... The probability ... is 1 (or $100 \%$ ).

Scenario 2 (Spin): ... The probability ... is $5 / 6$ (or about $83.33 \%$ )

In both scenarios, the expected outcome is the same: a $5 / 6$ chance ..."

### 5.2 LLM Performance

### 5.2.1 Linguistic Benchmark

There was a noticeable variance in the performance across popular LLMs, with OpenAI's GPT4 Turbo demonstrating markedly more robust logical reasoning throughout our tests. Perhaps unsurprisingly, the smaller and open-source models Llama 3 70B and Mistral 8x22B had the lowest benchmark performance aside from the previous generation, Gemini 1.0 Pro. However, both open-source models demonstrated a strong performance improvement after introducing clarifying questions, highlighting their increased capacity for optimisation through prompt engineering.

### 5.2.2 Popular LLM Benchmarks

In the rapidly evolving world of AI, benchmarks serve a crucial function in assessing the performance of new models. These benchmarks are sets of questions designed to test various facets of a language model's ability. Comprehension, mathematics, and general knowledge are the most significant areas assessed.

| Model | MMLU | GSM8K | BIG-Bench H | DROP | HellaSwag | ARC C |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| Claude 3 Opus | $\mathbf{8 6 . 8 \%}$ | $\mathbf{9 5 . 0 \%}$ | $\mathbf{8 6 . 8 \%}$ | $\mathbf{8 3 . 1 \%}$ | $\mathbf{9 5 . 4 \%}$ | $\mathbf{9 6 . 4 \%}$ |
| GPT-4 Turbo | $86.4 \%$ | $92.0 \%$ | $83.1 \%$ | $80.9 \%$ | $95.3 \%$ | $96.3 \%$ |
| Llama 3 70B | $82.0 \%$ | $93.0 \%$ | - | - | - | - |
| Gemini 1.5 Pro | $81.9 \%$ | $91.7 \%$ | $84.0 \%$ | $78.9 \%$ | $92.5 \%$ | - |
| Mistral Large | $81.2 \%$ | $91.2 \%$ | - | - | $89.2 \%$ | $94.2 \%$ |
| Mistral 8x22B | $77.8 \%$ | $76.5 \%$ | - | - | $88.6 \%$ | $91.3 \%$ |
| Gemini 1.0 Pro | $71.8 \%$ | $86.5 \%$ | $75.0 \%$ | $74.1 \%$ | $84.7 \%$ | - |

Table 6: Popular Benchmarks

References: Anthropic, OpenAI, Meta, Google, Mistral.

### 5.2.3 The Pressure to Perform

The outcomes of well-recognised benchmarks are widely promoted and emphasised in the industry. When a new model surpasses the incumbent on any of these benchmarks, it is viewed as a major advancement and gains substantial attention. One such example is Claude 3 Opus, which demonstrated exceptional performance against GPT-4 Turbo across many standard benchmarks; however, it did not perform as strongly on our specialised Linguistic Benchmark. This highlights a potential downside for companies focusing too intensely on excelling in known benchmarks: they risk developing models that excel in benchmark scenarios but may not adapt as effectively to a broader range of challenges or unforeseen tasks. [13]

### 5.2.4 Novel Human-Level Benchmarks

Despite the proliferation of benchmarks, there remains a gap between an LLM's theoretical accomplishments and its real-world effectiveness, especially in comparison with human-level performance on novel tasks. Current benchmarks, while diverse in nature, tend to focus on narrow domains or specific types of reasoning, failing to capture the nuance of regular tasks.

### 5.2.5 Test Set Leakage

It is plausible that our own Linguistic Benchmark may one day be used to train a foundational LLM model, degrading its utility as a novel test set. It may be possible to verify this by confirming its inclusion in any common pre-training datasets or by monitoring the change in $\log$ probabilities of token outputs from partially inserted questions.

### 5.3 LLM Reliability

### 5.3.1 Input Structure

Minute changes to input structure or order that do not change the meaning of a question lead to dramatically different responses from LLMs. Section 4.3 on "Clarifying Questions Improved Performance" highlighted this weakness; a handful of previously correct answers suffered regressions across the models tested despite the underlying question remaining the same.

### 5.3.2 Output Determinism

A frequently reported but largely undocumented phenomenon is that of non-deterministic LLM outputs with the temperature set to 0 . We noticed this non-deterministic behaviour in all models and providers, which makes it increasingly difficult to perform repeatable benchmarking and complicates the development of prompt engineering. It also highlights the real need for continuous testing.

More details in the Appendix 9.5

### 5.4 LLMs Recommend Adversarial Examples

When prompted, GPT-4 Turbo is apt at recommending other adversarial logic-type puzzles that may cause it to provide an "overfitted response". This is how the authors found "Question 2," which is loosely structured linguistically on the famous Monty Hall Problem.

Example in the Appendix 9.6

### 5.4.1 Inclusion of LLM Recommendations in the Linguistic Benchmark

The authors feel it is unproblematic to include questions that were provided by a single LLM to test other models and, by extension, do not believe this imparts any appreciable bias. This is due to the datasets used to train nearly all general-purpose LLMs being highly correlated. Common Crawl, WebText2, Books1, Books2, Wikipedia, and news articles constitute the vast majority of trained tokens [14]. The paper "Language Modeling Is Compression" published by Google DeepMind [15] suggests that LLMs learn a compressed representation of the data they are trained on. Therefore, it is unsurprising that all of the models tested provided similar adversarial logic-type problem recommendations.

## 6 Implications

The findings from this benchmark highlight several key implications for the development, deployment, and expectation management of Large Language Models (LLMs):

### 6.1 Quality Over Quantity

The inconsistencies and specific types of failures indicated in the benchmark suggest that future development of LLMs should prioritise not only scale but also the quality of reasoning and reliability across a wider array of questions. This includes improving logical reasoning, spatial intelligence, linguistic understanding, and commonsense reasoning. Moreover, the ability to understand and process relational contexts accurately should be enhanced. Adopting diverse and comprehensive datasets with an emphasis on curating adversarial or challenging problems during training might help in addressing some of these shortcomings.

### 6.2 Commercial Use

Organisations planning to deploy LLMs must be cautious about relying on them for tasks that require high-stakes decision-making, nuanced reasoning, or understanding subtle linguistic cues unless supervised or complemented by human judgment. Deployment strategies should be adaptable, incorporating continuous monitoring for failure modes, regular benchmarking against novel problem sets, and readiness to integrate human oversight whenever the models' limitations are encountered.

### 6.3 Addressing Overfitting and Benchmark Limitations

While benchmarks are useful for standardised evaluations, they should be complemented alongside more dynamic and unpredictable tests reflecting real-world complexity.

### 6.4 Promoting Openness and Collaboration

Sharing findings, particularly regarding failure modes, can foster a collective effort toward addressing these limitations. Such collaboration might not only accelerate individual efforts but could also lead to the development of more versatile and reliable AI systems.

### 6.5 Acknowledging Limitations

The limitations and unpredictability in LLM performance observed underscore the importance of responsible development. Model developers and deploying organisations must be transparent about the capabilities and limitations of their systems, ensuring users are informed of the potential for error or bias. The development of LLM models should include rigorous testing to uncover and address potential failure modes before widespread deployment.

### 6.6 Enhancing Input and Output Stability

Our findings indicate a need for LLMs to improve the handling of subtle variations in input and ensure consistent, reliable outputs. Additionally, offering the ability to guarantee a deterministic output would be helpful for many use cases.

### 6.7 Research Direction

Finally, this benchmark opens new avenues for research, particularly in exploring methods to improve LLMs' linguistic understanding and comprehension. It also raises questions about how these models conceptualise and process different forms of logic and common sense. Enhancing model performance will likely require an interdisciplinary approach that blends cognitive science, linguistics, and artificial intelligence research.

## 7 Future Work and Limitations

There are vast limitations to this approach, but further improvements might include:

- Expanding the Linguistic Benchmark beyond thirty questions to increase statistical significance and test a more diverse range of inputs.
- Using multiple-choice questions to make evaluation more reliable.
- Running inference multiple times with the temperature for each model set above zero (standardised and equivalent across all architectures) and generating aggregate statistics.
- Testing on a sample of smaller LLMs to see if performance is correlated to model size.
- Fine-tuning models with a training dataset of perturbed variations of well-known logictype problems found in the training corpora (on the internet) to see if this decreases overfitting variance.
- Testing advanced regularisation techniques for LLMs during the pre-training process.
- Finding better methodologies to keep LLM outputs deterministic.


## 8 Conclusion

The Linguistic Benchmark suggests that LLMs offered by leading providers such as OpenAI, Anthropic, Meta, Mistral, and Google have difficulty answering novel questions that humans find relatively easy. These models falter across domains such as logical reasoning, spatial intelligence, mathematical reasoning, linguistic understanding, knowledge of popular science, and relational perception. This highlights a significant gap between their current performance and general human cognitive abilities. Spotlighting areas where LLMs underperform invites a re-calibration of our expectations for these models, encouraging a focus on enhancing their reasoning capabilities and a pivot towards human-in-the-loop augmented intelligence.

This research reminds us of the importance of responsible LLM deployment. As we integrate LLMs into various facets of societal operations, from education to healthcare, acknowledging and addressing their limitations is paramount. This not only ensures that we leverage these models' strengths but also safeguards against potential risks and pitfalls. Whilst significant strides have been made in the development of LLMs, this investigation underscores the significant challenges that remain in developing models that can achieve human-like understanding and reasoning. The Linguistic Benchmark offers a critical perspective, urging LLM development that prioritises higher standards of general purpose reliability.

## References

[1] Asher, Nicholas, et al. "Limits for Learning with Language Models," arXiv preprint arXiv:2306.12213, 2023.

[2] Collins, Harry M. "Embedded or embodied? A review of Hubert Dreyfus' what computers still can't do." Artificial Intelligence 80.1: 99-118, 1996.

[3] Davis, E., et al. "Mathematics, word problems, common sense, and artificial intelligence," Bulletin of the American Mathematical Society, 2024.

[4] Miyu Sasaki, Natsumi Watanabe, Tsukihito Komanaka et al. "Enhancing Contextual Understanding of Mistral LLM with External Knowledge Bases," Research Square rs.3.rs$4215447 / v 1,2024$

[5] Wu, Wenshan, et al. "Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models," arXiv preprint arXiv:2404.03622, 2024.

[6] Ahn, Janice, et al. "Large Language Models for Mathematical Reasoning: Progresses and Challenges," arXiv preprint arXiv:2402.00157, 2024.

[7] Wachter, S., Mittelstadt, B., et al. "Do large language models have a legal duty to tell the truth?," Available at SSRN 4771884, 2024.

[8] Li, Zhiming, et al. "LLMs for Relational Reasoning: How Far are We?" arXiv preprint arXiv:2401.09042, 2024.

[9] Giadikiaroglou, Panagiotis, et al."Puzzle Solving using Reasoning of Large Language Models: A Survey," arXiv preprint arXiv:2402.11291, 2024.

[10] Bordt, Sebastian, et al. "Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models," arXiv preprint arXiv:2404.06209, 2024.

[11] McIntosh, Timothy R., et al."Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence," arXiv preprint arXiv:2402.09880, 2024.

[12] M. Renze and E. Guven, "The Effect of Sampling Temperature on Problem Solving in Large Language Models," arXiv preprint arXiv:2402.05201, 2024.

[13] K. Zhou, Y. Zhu, Z. Chen, W. Chen, W. X. Zhao, X. Chen, Y. Lin, J.-R. Wen, and J. Han, "Don't Make Your LLM an Evaluation Benchmark Cheater," arXiv preprint arXiv:2311.01964, 2023.

[14] Y. Liu, J. Cao, C. Liu, K. Ding, and L. Jin, "Datasets for Large Language Models: A Comprehensive Survey," arXiv preprint arXiv:2402.18041, 2024.

[15] G. Delétang, A. Ruoss, P.-A. Duquenne, et al., "Language Modeling Is Compression," arXiv preprint arXiv:2309.10668, 2024.

[16] S. Ouyang, J. M. Zhang, M. Harman, and M. Wang, "LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation," arXiv preprint arXiv:2308.02828, 2023 .
