# SqueezeLLM: Dense-and-Sparse Quantization 

Sehoon Kim*1 Coleman Hooper*1 ${ }^{* 1}$ Amir Gholami ${ }^{* 1,2} \quad$ Zhen Dong $^{1}$<br>Xiuyu Li ${ }^{1} \quad$ Sheng Shen $^{1} \quad$ Michael W. Mahoney ${ }^{1,2,3}$ Kurt Keutzer ${ }^{1}$<br>${ }^{1}$ UC Berkeley $\quad{ }^{2}$ ICSI $\quad{ }^{3}$ LBNL<br>\{sehoonkim, chooper, amirgh, zhendong, xiuyu, sheng.s, mahoneymw, keutzer\} @berkeley.edu


#### Abstract

Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to $2.1 \times$ as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to $2.3 \times$ speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM.


## 1 Introduction

Recent advances in Large Language Models (LLMs) trained on massive text corpora, with up to hundreds of billions of parameters, have showcased their remarkable problem-solving capabilities across various domains [3, 7, 14, 24, 37, 38, 41- $-43,52]$. However, deploying these models for inference has been a significant challenge due to their demanding resource requirements. For instance, the LLaMA-65B model requires at least 130GB of RAM to deploy in FP16, which exceeds current GPU capacity. Even storing such large-sized models has become costly and complex.

As will be discussed in Sec. 3, the main performance bottleneck in LLM inference for generative tasks is memory bandwidth rather than compute. This means that the speed at which we can load and store parameters becomes the primary latency bottleneck for memory-bound problems, rather than arithmetic computations. However, recent advancements in memory bandwidth technology have been significantly slow compared to the improvements in computes, leading to the phenomenon known as the Memory Wall [36]. Consequently, researchers have turned their attention to exploring algorithmic methods to overcome this challenge.

One promising approach is quantization, where model parameters are stored at lower precision, instead of the typical 16 or 32-bit precision used for training. For instance, it has been demonstrated that LLM models can be stored in 8-bit precision without performance degradation [48], where 8 -bit quantization not only improves the storage requirements by half but also has the potential to improve inference latency and throughput. As a result, there has been significant research interest in quantizing models to even lower precisions. A pioneering approach is GPTQ [17] which uses a training-free quantization technique that achieves near-lossless 4-bit quantization for large LLM models with over tens of billions of parameters. However, achieving high quantization performance remains challenging, particularly with lower bit precision and for relatively smaller models (e.g., $<50 \mathrm{~B}$ parameters) such as the recent LLaMA models [43].[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_81b632f2a8dde3752ee9g-02.jpg?height=514&width=1400&top_left_y=214&top_left_x=359)

Figure 1: (Left) SqueezeLLM incorporates two key approaches: (i) sensitivity-based non-uniform quantization (Sec. 4.1), where quantization bins are allocated closer to sensitive values, and (ii) the Dense-and-Sparse decomposition (Sec. 4.2), which retains both sensitive values and outlier values as full-precision sparse format. When applied to LLaMA-7B with 3-bit quantization, our method outperforms the state-of-the-art methods [17,32] by a large perplexity margin of over 0.3 on the $\mathrm{C} 4$ benchmark. (Right) By applying our methods to LLaMA models of varying sizes, we can achieve improved trade-offs between perplexity and model size.

Contributions. In this paper, we conduct an extensive study of low-bit precision quantization and identify limitations in existing approaches. We first present performance modeling results demonstrating that the memory, rather than the compute, is the primary bottleneck in LLM inference with generative tasks (Sec. 3 and Fig. 2). Building on this insight, we introduce SqueezeLLM, a post-training quantization framework with a novel sensitivity-based non-uniform quantization and Dense-and-Sparse decomposition. These techniques enable lossless compression even at precisions as low as 3 bits with reduced model sizes and faster inference without compromising model performance. Our detailed contributions include:

- Sensitivity-based Non-Uniform Quantization: We demonstrate that uniform quantization of prior works is suboptimal for LLM inference for two reasons. First, the weight distributions in LLMs exhibit clear non-uniform patterns (Fig. 3). Second, the inference computation in prior works does not fully benefit from uniform quantization as the arithmetic is performed in FP16 precision, not in reduced precision. To address these, we propose a novel sensitivity-based non-uniform quantization method to achieve more optimal LLM quantization, which significantly improves the perplexity of 3-bit LLaMA-7B from 28.26 of uniform quantization to 7.75 on $\mathrm{C} 4$ (Sec. 4.1).
- Dense-and-Sparse Quantization: The weights in LLMs contain significant outliers, making low-bit quantization extremely challenging. To address this, we propose a simple solution that decomposes weights into dense and sparse components. The sparse part holds outlier values in full precision using efficient sparse storage methods and the dense part can have a more compact range to aid quantization. By extracting only $0.45 \%$ of the weight values as the sparse component, we further improve the perplexity of LLaMA-7B from 7.75 to 7.58 on $\mathrm{C} 4$ (Sec. 4.2).
- Evaluation: We extensively test SqueezeLLM on various models on language modeling tasks using the C4 and WikiText2 datasets as well as on the MMLU [23] and Vicuna benchmarks [6] (Sec. 5.3). Furthermore, our deployed models on A6000 GPUs also exhibit significant latency gains of up to $2.4 \times$ compared to the FP16 baseline, showcasing the effectiveness of our method in terms of both quantization performance and inference efficiency (Sec. 5.4.


## 2 Related Work

LLM Quantization. In Sec. A.1, we offer an overview and related works of Transformer quantization, with an emphasis on Post-Training Quantization (PTQ), which is the primary focus of our work. With the increasing popularity of LLMs, weight-only quantization has surfaced as a promising approach to reduce memory consumption and enhance inference efficiency. GPTQ [17] has been a pioneering work, and AWQ [32] and $\mathrm{SpQR}$ [12] have also suggested the weight-only quantization schemes concurrent to our work. Our work, however, is different in two key aspects. First, our work employs non-uniform quantization as opposed to the uniform quantization of the aforementioned works.

In particular, our sensitivity-based non-uniform quantization not only better represents non-uniform distributions of weights but also strategically reduces the impact on more sensitive values, thereby enabling more aggressive quantization without performance degradation. Second, while previous works quantize weights in a way that layer-wise output activations remain unaffected, our approach targets preserving the model's final output. This strategy of minimizing the final loss, as shown in Sec. A.4.4, leads to better quantization performance since it is a direct measure of the end-to-end performance degradation after quantization.

For low-bit LLM quantization, [11] has recently introduced the NF datatype, underscoring the importance of non-uniform quantization. However, our approach differs by offering a more dynamic non-uniform representation that accounts for both weight distributions and sensitivity of values, as opposed to the static, hard-coded NF datatype that assumes the normal distribution of the weights. While previous studies [22 47] have utilized k-means clustering in quantization, our work pioneers its application in LLM quantization. Furthermore, we introduce the novel sensitivity-based weighted k-means clustering strategy, enabling lossless sub-4-bit quantization by significantly reducing performance degradation in contrast to the sensitivity-agnostic counterpart (Fig. 11.

Among the various challenges in low-bit Transformer quantization, one key issue is the presence of outliers [29], which can unnecessarily increase the quantization range. To address this issue, outlier-aware quantization methods have been investigated [2, 10, 45, 46]. Notably, [10] keeps outlier activations in floating-point, while [46] transfers outlier factors to later layers without affecting functionality. These focus on activations, which is not a concern in our work where all activations are in floating-point. Our Dense-and-Sparse quantization instead tackles weight outliers for low-bit LLM quantization.

Concurrently to our work, $\operatorname{SpQR}$ [12] also explores outlier extraction in the context of weight quantization. While it shows a promising result on outlier extraction, SqueezeLLM, leveraging sensitivity-based non-uniform quantization, achieves precise quantization with significantly lower (e.g., $0.05 \%$ ) or even zero sparsity levels. This is critical for both reducing model size and improving inference speed, as higher sparsity often degrades latency. Furthermore, SqueezeLLM uses outlier extraction as a direct solution to prevent outliers from negatively impacting quantization performance, bypassing the need for using the grouping strategy as an indirect solution. This contrasts with $\mathrm{SpQR}$, which relies on fine-grained grouping that leads to increased model size and a more complex quantization process such as the bi-level quantization scheme.

Dense-and-Sparse Decomposition. Matrix decomposition into dense and sparse components has been explored in attention map decomposition [5, 9], leveraging the fact that attention patterns often present low-rank characteristics without a few outliers. Our research, however, is the first attempt to apply the dense-and-sparse decomposition strategy to weight matrices to improve quantization performance to the best of our knowledge. Additionally, we uniquely incorporate both outlier and sensitive values within the sparse matrix, which yields considerable improvement in post-quantization performance.

## 3 Memory Wall

Inference behavior broadly falls into two categories: compute-bound inference that is limited by computational throughput, and memory-bound inference that is bottlenecked by the rate at which data can be fed into the processing cores from memory. Arithmetic intensity, the ratio of compute to memory operations, is a typical metric used to assess this behavior. High and low arithmetic intensity indicates a compute-bound and memory-bound problem, respectively. For memory-bound problems, the speedup can be achieved by reducing the memory traffic rather than compute since the compute units in hardware are often underutilized waiting to receive data from memory.

Generative LLM inference exhibits extremely low arithmetic intensity compared to other workload 1 , [28]. This is because it consists almost entirely of matrix-vector operations, which limits the data reuse as each weight load can only process a single vector for a single token, and cannot be amortized across the multiple vectors for different tokens. This low arithmetic intensity needs to be contrasted with the compute operations on a typical GPU which is orders of magnitude higher than the memory operations ${ }^{2}$. The disparity between compute and memory bandwidth, along with the growing memory requirements of deep learning, has been termed the Memory Wall problem [20]. To further illustrate this problem, we used a simple roofline-based performance modeling approach [28] to study LLaMA-7B's runtime on[^1]![](https://cdn.mathpix.com/cropped/2024_06_04_81b632f2a8dde3752ee9g-04.jpg?height=468&width=928&top_left_y=232&top_left_x=574)

Figure 2: Normalized runtime for LLaMA-7B when reducing the bit precision for the weights with sequence lengths of 128 (left) and 2048 (right). Results were obtained using a roofline-based performance model for an A5000 GPU. Reducing only the precision of the weights (and not the activations) is sufficient to obtain significant latency reductions.

an A5000 GPU with different bit precisions (Fig. 2). While we assume that all computations are kept at FP16, we see that the latency decreases linearly as we reduce the bit precision, indicating that the main bottleneck is memory, not compute.

In summary, in generative LLM inference, loading weights into memory is the primary bottleneck, while the cost of dequantization and FP16 computation is relatively small. Thus, by quantizing just the weights to lower precision, while leaving the activations in full precision, we can attain significant speedup as well as reduced model size. Given this insight, the appropriate strategy is to minimize the memory size even if it may add overhead to arithmetic operations.

## 4 Methodology

### 4.1 Sensitivity-Based Non-uniform Quantization

As in Fig. 3 (Top), weight distributions in LLMs demonstrate non-uniform patterns. The main task for quantization is to find an optimal way to allocate distinct quantized values (e.g., 8 for 3 bits) in a way that preserves model performance. A widely used approach in LLM quantization works is uniform quantization where the weight range is evenly divided into bins. This has two main issues. First, uniformly distributing quantized values is sub-optimal as weight distributions are typically non-uniform. Second, while the main advantage of uniform quantization is efficient integer computation, this does not lead to end-to-end latency improvement in memory-bound LLM inference. Therefore, we have chosen non-uniform quantization, which allows for a more flexible allocation of the representative values.

Finding an optimal non-uniform quantization configuration translates into solving a k-means problem. Given a weight distribution, the goal is to determine $k$ centroids that best represent the values (e.g., $k=8$ for 3-bit). This optimization problem for non-uniform quantization can be formulated as

$$
\begin{equation*}
Q(w)^{*}=\underset{Q}{\arg \min }\left\|W-W_{Q}\right\|_{2}^{2} \tag{1}
\end{equation*}
$$

where $W$ denotes the weights and $W_{Q}$ is the corresponding quantized weights (i.e., $[Q(w)$ for $w \in W]$ ), represented by $k$ distinct values $\left\{q_{1}, \cdots, q_{k}\right\}$. Here, the optimal solution $Q(w)^{*}$ can be obtained by 1-dimensional k-means clustering, which clusters the parameters into $k$ clusters and assign the centroid of each cluster as $q_{j}$ 's. While this already outperforms uniform quantization, we propose an improved sensitivity-based clustering algorithm.

Sensitivity-Based K-means Clustering. The quantization objective is to represent the model weights with low-bit precision with minimal perturbation in the model output [13]. While quantization introduces perturbations in each layer, we need to minimize the overall perturbation with respect to the final loss term, rather than focusing on individual layers, as it provides a more direct measure of the end-to-end performance degradation after quantization [30]. To achieve this, we need to place the k-means centroids closer to the values that are more sensitive w.r.t the final loss, rather than treating all weight values equally as in Eq. 1. To determine more sensitive values, we perform Taylor expansion to analyze how the loss changes in response to perturbations in the weights $W$ :

![](https://cdn.mathpix.com/cropped/2024_06_04_81b632f2a8dde3752ee9g-05.jpg?height=580&width=829&top_left_y=217&top_left_x=645)

Figure 3: (Top) The weight distribution of one output channel in LLaMA-7B. The top-20 sensitive values are marked in red. (Bottom) Weight distributions after 3-bit quantization using uniform and sensitivity-based non-uniform quantization. In the latter case, the quantized values are clustered around the sensitive values.

$$
\begin{align*}
\mathcal{L}\left(W_{Q}\right) & \simeq \mathcal{L}(W)-g^{\top}\left(W-W_{Q}\right)  \tag{2}\\
& +\frac{1}{2}\left(W-W_{Q}\right)^{\top} H\left(W-W_{Q}\right) \tag{3}
\end{align*}
$$

where $g$ and $H=\mathbb{E}\left[\frac{\partial^{2}}{\partial W^{2}} \mathcal{L}(W)\right]$ are the gradient and Hessian of the loss at $W$. Assuming that the model has converged, the gradient $g$ can be approximated as zero which gives us the following formula for computing how much the model gets perturbed after quantization:

$$
\begin{equation*}
Q(w)^{*}=\underset{Q}{\arg \min }\left(W-W_{Q}\right)^{\top} H\left(W-W_{Q}\right) \tag{4}
\end{equation*}
$$

In the new optimization target, as compared to Eq. 1 , the perturbation of each weight after quantization, i.e., $W-W_{Q}$, is weighted by the scaling factor introduced by the second-order derivative, $H$. This highlights the importance of minimizing perturbations for weights with large Hessian values, as they have a greater impact on the overall perturbation of the final output. In other words, the second-order derivative serves as a measure of importance for each weight value.

Due to the cost of computing the Hessian, we use an approximation to the Hessian based on the Fisher information matrix $\mathcal{F}$, which can be calculated over a sample dataset $D$ as $H \simeq \mathcal{F}=\frac{1}{|D|} \sum_{d \in D} g_{d} g_{d}{ }^{\top}$. This only requires computing gradient for a set of samples, which can be calculated efficiently with existing frameworks. To make the optimization objective in Eq. 4 more feasible, we further approximate the Fisher information matrix as a diagonal matrix by assuming that the cross-weight interactions are negligible. This simplifies our objective target as follows:

$$
\begin{align*}
Q(w)^{*} & \simeq \underset{Q}{\arg \min }\left(W-W_{Q}\right)^{\top} \operatorname{diag}(\mathcal{F})\left(W-W_{Q}\right)  \tag{5}\\
& =\underset{Q}{\arg \min } \sum_{i=1}^{N} \mathcal{F}_{i i}\left(w_{i}-Q\left(w_{i}\right)\right)^{2} \tag{6}
\end{align*}
$$

An important consequence of Eq. 5 is the weighted $\mathrm{k}$-means clustering setting, where the centroids will be pulled closer to these sensitive weight values. In Fig. 3, we illustrate the top-20 sensitive values based on the Fisher information of the exemplary weight distribution. At the bottom, the quantized values assigned by uniform quantization (green) are compared to those assigned by the sensitivity-based k-means approach (purple), which achieves a better tradeoff by placing centroids near sensitive values, effectively minimizing quantization error. With 3-bit LLaMA-7B, sensitivity-based non-uniform quantization achieves a much lower perplexity of 7.75 compared to the 28.26 perplexity of round-to-nearest uniform quantization on $\mathrm{C} 4$ (Fig. 1 and Sec. 5.2)

![](https://cdn.mathpix.com/cropped/2024_06_04_81b632f2a8dde3752ee9g-06.jpg?height=379&width=979&top_left_y=228&top_left_x=573)

Figure 4: The distributions of the (normalized) absolute weight values, for the output layers in MHA and the down layers in FFN across different layers in LLaMA-7B. Note that the distributions exhibit outlier patterns across all layers, with $99 \%$ of the values clustered within $\sim 10 \%$ of the entire range.

### 4.2 Dense-and-Sparse Quantization

Another challenge in low-bit LLM quantization is outlier values [2. 10, 45, 46]. In Fig 4, we plot the normalized weight distributions of different layers in LLaMA-7B, which demonstrate that $\sim 99.9 \%$ of the weights are concentrated in a narrow range of $\sim 10 \%$ of the entire distribution. Naively quantizing the weights with a large range will significantly degrade performance, especially at low precisions. However, this also implies opportunity as the range of the weight values can be contracted by a factor of 10 simply by removing a small number of outlier values (e.g., $0.1 \%$ ), yielding a significant improvement in quantization resolution. This will then help the sensitivity-based $\mathrm{k}$-means centroids to focus more on the sensitive values rather than a few outliers.

Motivated by this, we introduce a method to filter out outliers from the weight matrix $W$ by performing a simple yet effective decomposition into a sparse matrix $(S)$ containing the outliers and the remaining dense matrix (D) that can be quantized much more effectively thanks to its significantly reduced range of values. That is, $W=D+S$ where $D=W\left[T_{\min } \leq w \leq T_{\max }\right]$ and $S=W\left[w<T_{\min }\right.$ or $\left.w>T_{\max }\right]$. Here, $T_{\min } / \max$ are thresholds that define outliers based on the percentile of the distribution.

Importantly, the overhead of this decomposition is minimal, since the number of outlier values is small, normally less than $>0.5$ of sparsity. Therefore, the sparse matrix can be stored efficiently using methods like compressed sparse row (CSR) format. The inference is also straightforward with the decomposition as in $W X=D X+S X$, two kernels for dense and sparse multiplication can be overlapped, and the sparse part (SX) can benefit from sparse kernels (Sec. 4.3).

Sensitivity-Based Sparse Matrix. In addition to isolating outliers into a sparse matrix, we've also discovered the advantage of precisely representing a small number of highly sensitive weight matrix values. These values can be easily identified based on the Fisher information (Sec. 4.1). This not only maintains sensitive values with FP16 to avoid their impact on the model output, but also prevents the centroids of Eq 5 from skewing towards the sensitive values. We have observed that extracting only $0.05 \%$ of these sensitive values across layers substantially enhances quantization performance (Sec. A.4). Altogether, with 3-bit LLaMA-7B, extracting $0.45 \%$ of outlier and sensitive values further reduces the perplexity from 7.67 to 7.56 (Fig. 1 and Sec. 5.2.

### 4.3 Dense-and-Sparse Kernel Implementation

To efficiently process non-uniformly quantized values, we implement 3/4-bit CUDA LUT-based kernels for matrixvector multiplication between compressed weight matrices and uncompressed activation vectors. These kernels load the compressed weights and dequantize them piece-by-piece to minimize memory bandwidth utilization. The compressed matrices store 3/4-bit indices, which correspond to LUT entries containing FP16 values associated with the bins obtained from non-uniform quantization. After dequantization, all arithmetic is performed in FP16.

To optimize the handling of our Dense-and-Sparse representation, we develop kernels for sparse matrix-vector multiplication that load a matrix in CSR format and a dense activation vector, inspired by [15]. Since the non-zero entry distributions are highly skewed across rows (Sec. A.3), assigning a single thread per row can be inefficient due to uneven workload distribution among threads. Thus, we implement balanced hybrid kernels based on [16] by assigning an equal number of nonzeros per thread; this leads to additional synchronization across threads due to rows being processed by multiple threads, but leads to a more balanced work assignment. We set the number of threads such that
there were 10 nonzeros per thread. The dense non-uniform kernel and balanced sparse kernels are launched in one call to avoid overhead from summing the outputs from these separate operations.

## 5 Evaluations

### 5.1 Experiment Setup

Below is our experiment setup with more details in Sec. A.2

Models and Datasets. We have conducted comprehensive evaluations of SqueezeLLM using various models including LLaMA, LLaMA2 [43, 44], OPT [52] and Vicuna [6] (v1.1 and v1.3). We conduct language modeling evaluation using the $\mathrm{C} 4$ [37] and WikiText2 [34] datasets. We further evaluate the domain-specific knowledge and problem-solving ability using MMLU [23], and the instruction-following ability using the methodology in [6].

Baseline Methods. We compare SqueezeLLM against various PTQ methods for LLMs including RTN, GPTQ [17], AWQ [32] and SpQR [12]. To ensure a fair comparison, we use GPTQ with activation ordering throughout all experiments unless specified, which addresses the significant performance drop that would otherwise occur.

Quantization Details. For SqueezeLLM, we adopt channel-wise quantization where each output channel is assigned a separate lookup table. We use 2 different sparsity levels: $0 \%$ (dense-only) and $0.45 \%$ ( $0.05 \%$ sensitive values and $0.4 \%$ outlier values as discussed in Sec. 4.2). For measuring sensitivity, we use 100 random samples from the Vicuna training set for Vicuna models and C4 training set for the others. While grouping can also be incorporated with our method, we found it sub-optimal as compared to extracting sensitive/outlier values with sparsity (Sec. A.4.3).

Latency Profiling. We measure the latency and peak memory usage for generating 128 and 1024 tokens on an A6000 machine using the Torch CUDA profiler. As an official implementation of GPTQ (in particular, the grouped version) is not available, we implement an optimized kernel for single-batch inference based on the most active open-source codebase [21].

### 5.2 Main Results

Table 1 shows quantization results for LLaMA along with the baseline methods. The models are grouped based on their size to better compare size-perplexity trade-offs. See Fig. 5 for a visual illustration. Below we use LLaMA-7B as the main example to discuss the impact of dense-only and Dense-and-Sparse quantization, and subsequently discuss how these trends extend to larger models. We provide the full evaluation result on all LLaMA models in Tab. A.6.

Dense-only Quantization. In Tab. 1 (Top), we compare dense-only SqueezeLLM with $0 \%$ sparsity level and GPTQ without grouping. With 4-bit quantization, our method exhibits minimal degradation compared to the FP16 baseline, with only $\sim 0.1$ perplexity degradation on $\mathrm{C} 4$ and WikiText2, while reducing the model size by $3.95 \times$. Moreover, when compared to non-grouped GPTQ our method shows significant perplexity improvement of up to 0.22 .

The performance gap between the two methods becomes more pronounced with 3-bit quantization. SqueezeLLM outperforms GPTQ by a substantial margin of $1.80 / 1.22$ points on C4/WikiText2 with a $5.29 \times$ compression rate. This is only $0.67 / 0.55$ points off from the FP16 baseline. This demonstrates the effectiveness of the sensitivity-based non-uniform method for ultra-low-bit quantization.

Dense-and-Sparse Quantization. By leveraging the Dense-and-Sparse quantization, we achieve a further reduction in the perplexity gap from the FP16 baseline as shown in Tab. 1. This improvement is particularly significant with 3-bit quantization, where extracting just $0.45 \%$ of the values yields around 0.2 perplexity improvement. This enables nearly lossless compression with less than $0.1 / 0.5$ perplexity deviation from the FP16 baseline for 4/3-bit, respectively.

Both GPTQ and AWQ use a grouping strategy to enhance performance with a slight overhead in model size. However, we demonstrate that SqueezeLLM with a sparsity level of $0.45 \%$ consistently outperforms both GPTQ/AWQ with a group size of 128 in all scenarios with comparable model sizes. This is more pronounced for 3-bit quantization, where SqueezeLLM with a $0.45 \%$ sparsity level outperforms both GPTQ/AWQ with a group size of 128 by up $\sim 0.3$ perplexity points.

Table 1: Perplexity comparison of LLaMA models quantized into 3 and 4 bits using different methods including RTN, GPTQ, AWQ and SpQR on C4 and WikiText-2. We compare the performance of different methodologies by grouping them based on their model sizes. In the first group, we compare dense-only SqueezeLLM with non-grouped GPTQ. In the second group, we compare SqueezeLLM with a sparsity level of $0.45 \%$ to GPTQ and AWQ with a group size of 128 . For comparison, we add speedup and peak memory usage numbers, which we provide more details in Tab. A. 6 Further results for LLaMA-30/65B can be found in Tab. A.6, and results on other models including LLaMA-2 7/13/70B are provided in Sec. A.7.1

| $\frac{\text { LLaMA-7B }}{\text { Method }}$ | 3-bit |  |  |  |  | 4-bit |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Avg. Bits <br> (comp. rate) | PPL $(\downarrow)$ |  | $\underset{(\uparrow)}{\text { Speedup }}$ | Mem. <br> $(\mathrm{GB}, \downarrow)$ | Avg. Bits <br> (comp. rate) | PPL $(\downarrow)$ |  | $\underset{(\uparrow)}{\text { Speedup }}$ | Mem. <br> $(\mathrm{GB}, \downarrow)$ |
|  |  | C4 | Wiki |  |  |  | C4 | Wiki |  |  |
| Baseline | 16 | 7.08 | 5.68 | $1 \times$ | 12.7 | 16 | 7.08 | 5.68 | $1 \times$ | 12.7 |
| RTN | $3(5.33)$ | 28.26 | 25.61 | $2.3 \times$ | 2.9 | $4(4.00)$ | 7.73 | 6.29 | $2.0 \times$ | 3.7 |
| GPTQ | $3(5.33)$ | 9.55 | 7.55 | $2.3 \times$ | 2.9 | $4(4.00)$ | 7.43 | 5.94 | $2.0 \times$ | 3.7 |
| $\mathrm{SpQR}$ | - | . | - | - | - | $3.94(4.06)$ | 7.28 | 5.87 | $1.2 x^{+}$ | N/A |
| SqueezeLLM | $3.02(5.29)$ | 7.75 | 6.32 | $2.1 \times$ | 2.9 | $4.05(3.95)$ | 7.21 | 5.79 | $1.8 \times$ | 3.8 |
| GPTQ (g 128 , no reorder) $)^{\ddagger}$ | 3.24 (4.93) | 10.09 | 8.85 | $2.0 \times$ | 3.0 | 4.24 | 7.80 | 6.07 | $1.6 \times$ | 3.8 |
| GPTQ (g128) ‡ | 3.24 (4.93) | 7.89 | 6.27 | $0.2 \times$ | 3.0 | 4.24 (3.77) | 7.21 | 5.78 | $0.4 \times$ | 3.8 |
| AWQ (g128) | 3.24 (4.93) | 7.90 | 6.44 | $2.0 x$ | 3.0 | 4.24 (3.77) | 7.22 | 5.82 | $1.6 \times$ | 3.8 |
| SqueezeLLM (0.45\%) | 3.24 (4.93) | 7.56 | 6.13 | $1.9 \times$ | 3.1 | $4.27(3.75)$ | 7.18 | 5.77 | $1.7 \times$ | 4.0 |
| LLaMA-13B | 3-bit |  |  |  |  | 4-bit |  |  |  |  |
| Method | Avg. Bits | PPL | $(\downarrow)$ | Speedup | Mem. | Avg. Bits | PP] | $(\downarrow)$ | Speedup | Mem. |
| retiod | (comp. rate) | C4 | Wiki | $(\uparrow)$ | $(\mathrm{GB}, \downarrow)$ | (comp. rate) | $\mathrm{C} 4$ | Wiki | $(\uparrow)$   | $(\mathrm{GB}, \downarrow)$ |
| Baseline | 16 | 6.61 | 5.09 | $1 \times$ | 24.6 | 16 | 6.61 | 5.09 | $1 \times$ | 24.6 |
| RTN | $3(5.33)$ | 13.24 | 11.78 | $2.7 \times$ | 5.3 | $4(4.00)$ | 6.99 | 5.53 | $2.3 \times$ | 6.8 |
| GPTQ | $3(5.33)$ | 8.22 | 6.22 | $2.7 \times$ | 5.3 | $4(4.00)$ | 6.84 | 5.29 | $2.3 \times$ | 6.8 |
| SpQR |  |  | ![](https://cdn.mathpix.com/cropped/2024_06_04_81b632f2a8dde3752ee9g-08.jpg?height=35&width=68&top_left_y=1201&top_left_x=923) | ![](https://cdn.mathpix.com/cropped/2024_06_04_81b632f2a8dde3752ee9g-08.jpg?height=35&width=107&top_left_y=1201&top_left_x=999) | ![](https://cdn.mathpix.com/cropped/2024_06_04_81b632f2a8dde3752ee9g-08.jpg?height=35&width=90&top_left_y=1201&top_left_x=1114) | $3.96(4.04)$ | 6.72 | 5.22 | $1.2 x^{+}$ | N/A |
| SqueezeLLM | $3.02(5.30)$ | 7.08 | 5.60 | $2.4 \times$ | 5.4 | 4.04 (3.96) | 6.71 | 5.18 | $2.0 \times$ | 6.9 |
| GPTQ (g128, no reorder $)^{\ddagger}$ | $3.25(4.92)$ | 7.16 | 5.53 | $2.2 \times$ | 5.7 | $4.25(3.77)$ | 6.71 | 5.18 | $1.9 \times$ | 7.2 |
| GPTQ (g128) ‡ | $3.25(4.92)$ | 7.12 | 5.47 | $0.2 \times$ | 5.6 | $4.25(3.77)$ | 6.70 | 5.17 | $0.4 \times$ | 7.0 |
| AWQ (g128) | $3.25(4.92)$ | 7.08 | 5.52 | $2.2 \times$ | 5.7 | 4.25 (3.77) | 6.70 | 5.21 | $1.9 \times$ | 7.2 |
| SqueezeLLM ( $0.45 \%)$ | 3.24 (4.94) | 6.92 | 5.45 | $2.2 x$ | 5.8 | 4.26 (3.76) | 6.68 | $\mathbf{5 . 1 7}$ | $1.9 \times$ | 7.3 |

![](https://cdn.mathpix.com/cropped/2024_06_04_81b632f2a8dde3752ee9g-08.jpg?height=344&width=1652&top_left_y=1422&top_left_x=236)

Figure 5: Perplexity comparison PTQ methods for 3-bit LLaMA quantization, evaluated on C4. The x-axes are the relative model sizes with respect to the model size in FP16. Different size-perplexity trade-offs are achieved by adjusting the group size for GPTQ and AWQ and the sparsity level for ours. Our quantization method consistently and significantly outperforms GPTQ and AWQ across all model size regimes, with a more pronounced gap in lower-bit and smaller model sizes.

Results on Larger Models. In Tab. 11(13B) and Tab. A.6 (30/65B), we observe that the trend in 7B extends to larger models, where SqueezeLLM consistently outperforms other PTQ methods across all models and bit widths. Such a trend is also illustrated in Fig. 5 for 3-bit quantization where even dense-only SqueezeLLM achieves comparable perplexity to grouped GPTQ/AWQ\|| With sparsity, we can further improve perplexity, reducing the gap from the FP16 baseline to less than 0.1/0.4 perplexity points for 4/3-bit quantization. Notably, with 3-bit quantization, our approach achieves up to a $2.1 \times$ reduction in perplexity gap from the FP16 baseline compared to existing methods. Further ablation studies on our design choices are provided in Sec. A.4, and additional results on LLaMA2 and OPT can be[^2]

Table 2: Comparison of PTQ methods on zero-shot MMLU accuracy applied to Vicuna v1.1 and v1.3. We add peak memory usage in GB for comparison. Additional results on 5-shot MMLU evaluation can be found in Sec. A.7.2

| Method | Avg. <br> bit | Vicuna-7B (v1.1) |  | Vicuna-13B (v1.1) |  | Vicuna-7B (v1.3) |  | Vicuna-13B (v1.3) |  | Vicuna-33B (v1.3) |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\operatorname{Acc}(\uparrow)$ | $\operatorname{Mem}(\mathrm{GB} \downarrow)$ | $\operatorname{Acc}(\uparrow)$ | $\operatorname{Mem}(\mathrm{GB} \downarrow)$ | $\operatorname{Acc}(\uparrow)$ | $\operatorname{Mem}(\mathrm{GB} \downarrow)$ | $\operatorname{Acc}(\uparrow)$ | $\operatorname{Mem}(\mathrm{GB} \downarrow)$ | $\operatorname{Acc}(\uparrow)$ | $\operatorname{Mem}(\mathrm{GB} \downarrow$ |
| Baseline | 16 | $39.1 \%$ | 12.7 | $41.2 \%$ | 24.6 | $40.2 \%$ | 12.7 | $43.3 \%$ | 24.6 | $49.5 \%$ | $\mathrm{OOM}$ |
| AWQ (g128) | 4.25 | $38.0 \%$ | 3.8 | $40.4 \%$ | 7.2 | $39.6 \%$ | 3.8 | $42.2 \%$ | 7.2 | $49.5 \%$ | 17.2 |
| SqueezeLLM | 4.05 | $38.8 \%$ | 3.8 | $39.2 \%$ | 6.9 | $39.3 \%$ | $3.8 \quad$ | $44.1 \%$ | 6.9 | $48.0 \%$ | 17.5 |
| SqueezeLLM (0.45\%) | 4.26 | $39.4 \%$ | 4.0 | $41.0 \%$ | 7.3 | $39.5 \%$ | 4.0 | $43.8 \%$ | 7.3 | $49.9 \%$ | 18.7 |
| AWQ (g128) | 3.25 | $36.5 \%$ | 3.0 | $37.6 \%$ | 5.7 | $37.4 \%$ | 3.0 | $40.7 \%$ | 5.7  | $46.4 \%$ | 13.2 |
| SqueezeLLM | 3.02 | $36.0 \%$ | 2.9 | $37.2 \%$ | 5.4 | $35.1 \%$ | 2.9 | $40.5 \%$ | 5.4 | $46.2 \%$ | 12.5 |
| SqueezeLLM ( $0.45 \%)$ | 3.24 | $37.7 \%$ | 3.1 | $39.4 \%$ | 5.8 | $37.6 \%$ | 3.1 | $\mathbf{4 0 . 8 \%}$ | 5.8 | $47.7 \%$ | 14.7 |

![](https://cdn.mathpix.com/cropped/2024_06_04_81b632f2a8dde3752ee9g-09.jpg?height=362&width=1310&top_left_y=702&top_left_x=404)

Figure 6: Comparison of PTQ methods applied to Vicuna v1.1. Blue / yellow / red represent the number of times that the quantized model won / tied / lost against the baseline FP16 model. This evaluation was performed using the methodology from Vicuna.

found in Sec. A.7.1.

### 5.3 Quantization of Instruction Following Models

Instruction tuning has emerged as a method for improving the model's ability to respond to user commands. We explore the quantization of instruction-following models to demonstrate the benefits of SqueezeLLM in terms of accuracy preservation by applying it to the Vicuna models, and evaluating the performance with the following approaches.

MMLU Evaluation. We first evaluate the baseline and quantized models on the MMLU benchmark where the weighted accuracy in the zero-shot setting is provided in Tab. 2 for Vicuna models. As we can see, 3-bit SqueezeLLM achieves higher accuracy for all models compared to AWQ and also preserves the FP16 baseline accuracy with 4-bit quantization. 5-shot results are provided in Sec. A.7.2.

Instruction-Following Ability. Another approach for evaluating instruction-following ability is to ask GPT-4 to rank the generated responses as presented in [6]. As shown in Fig. 6. SqueezeLLM without sparsity achieves near-perfect performance (i.e., 50/50 split) with 4-bit quantization for both Vicuna-7B and 13B, outperforming GPTQ with the same model size. In the case of 3-bit quantization, SqueezeLLM outperforms both GPTQ and AWQ with comparable model sizes. In the case of the Vicuna-13B model, achieving a near-perfect 50/50 split for 3-bit quantization.

### 5.4 Hardware Deployment and Profiling

We show the latency and peak GPU memory usage of SqueezeLLM in Tab. 3 on an A6000 GPU for different configurations when generating 128 tokens. We observe that the LUT-based non-uniform approach in SqueezeLLM (3rd row) shows up to $2.4 \times$ speedup compared to the FP16 baseline, and exhibits comparable latency and peak memory usage to the uniform quantization of non-grouped GPTQ (2nd row). This indicates that the overhead associated with LUT-based dequantization is small, especially considering the significant perplexity gains it enables.

Additionally, when incorporating sparsity, we still observed latency gains relative to the FP16 baseline. As shown in Tab. 3. keeping $0.45 \%$ of parameters in FP16 (4th row) only adds around $10 \%$ latency overhead relative to the dense-only implementation, while still resulting in up to $2.2 \times$ speed up compared to the FP16 baseline. In contrast, when accounting for permutation, the GPTQ runtime is degraded heavily (5th row). This latency penalty is due to

Table 3: Latency (s) and peak memory usage (GB) of 3-bit LLaMA when generating 128 tokens on an A6000 GPU. The table compares the FP16 baseline, non-grouped and grouped GPTQ with activation ordering, and SqueezeLLM with different sparsity levels. For comparison, we include bitwidth and perplexity on the C4 benchmark. See Tab. A. 4 for additional results on generating 1024 tokens, and see Tab. A.5 for additional benchmarking results on an A100 GPU.

| Method | Bit <br> width | PPL (C4) | 7B <br> Lat | Mem (G) | PPL (C4) | 13B <br> Lat (s) | Mem (G) | PPL (C4) | 30B <br> Lat (s) | Mem (G) | PPL (C4) | 65B <br> Lat (s) | Mem (G) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | 16 | 7.08 | 3.2 | 12.7 | 6.61 | 5.6 | 24.6 | 5.98 | OOM | OOM | 5.62 | OOM | OOM |
| GPTQ | 3 | 9.55 | 1.4 | 2.9 | 8.22 | 2.1 | 5.3 | 7.31 | 4.0 | 12.3 | 6.70 | 6.7 | 24.0 |
| SqueezeLLM | 3.02 | 7.75 | 1.5 | 2.9 | 7.08 | 2.4 | 5.4 | 6.37 | 4.0 | 12.5 | 5.99 | 7.6 | 24.5 |
| GPTQ (g128) | 3.25 | 7.89 | 13.7 | 3.0 | 7.12 | 24.2 | 5.6 | 6.47 | 61.9 | 12.9 | 6.01 | 117.8 | 25.1 |
| SqueezeLLM $(0.45 \%)$ | 3.24 | 7.56 | 1.7 | 3.1 | 6.92 | 2.5 | 5.8 | 6.23 | 4.4 | 14.7 | 5.84 | 8.8 | 28.0 |

permutation, which means that elements in the same channel need to be scaled using different scaling factors (which are accessed using group indices); it is challenging for these distributed memory accesses to be performed efficiently, as GPUs rely heavily on coalesced memory accesses in order to optimally utilize memory bandwidth. This shows how our Dense-and-Sparse quantization methodology allows for both higher accuracy as well as better performance relative to GPTQ. Additional evaluation results on generating 1024 tokens are provided in Tab. A.4, where we can observe a similar trend.

## 6 Conclusion

We have presented SqueezeLLM which attempts to address the Memory Wall problem associated with generative LLM inference that is memory-bound. SqueezeLLM incorporates two novel ideas that allow ultra-low precision quantization of LLMs with negligible degradation in generation performance: the sensitivity-based non-uniform quantization method; and the Dense-and-Sparse decomposition that resolves the outlier issue. We have evaluated SqueezeLLM on a wide range of models and datasets that assess language modeling, problem-solving, and instruction-following capabilities of quantized models, where we have demonstrated that our quantization method can consistently outperform the previous state-of-the-art methodologies.

## Acknowledgements

The authors would like to acknowledge Karttikeya Mangalam, Nicholas Lee, and Thanakul Wattanawong for helpful discussions and brainstorming. We acknowledge gracious support from Google Cloud, Google TRC team, and specifically Jonathan Caton, Jing Li, Jiayu Ye, and Prof. David Patterson. Prof. Keutzer's lab is sponsored by Intel corporation, Intel VLAB team, Intel One-API center of excellence, as well as gracious funding from Furiosa, Berkeley Deep Drive, and BAIR. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.

## References

[1] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. BinaryBERT: Pushing the limit of BERT quantization. arXiv preprint arXiv:2012.15701, 2020.

[2] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient Transformer quantization. arXiv preprint arXiv:2109.12948, 2021.

[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

[4] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. ZeroQ: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13169-13178, 2020.

[5] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 34:17413-17426, 2021.

[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023.

[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PALM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[8] Insoo Chung, Byeongwook Kim, Yoonjung Choi, Se Jung Kwon, Yongkweon Jeon, Baeseong Park, Sangha Kim, and Dongsoo Lee. Extremely low bit transformer quantization for on-device neural machine translation. arXiv preprint arXiv:2009.07453, 2020.

[9] Jyotikrishna Dass, Shang Wu, Huihong Shi, Chaojian Li, Zhifan Ye, Zhongfeng Wang, and Yingyan Lin. Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention, 2022 .

[10] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. In Advances in Neural Information Processing Systems.

[11] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.

[12] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. SpQR: A sparse-quantized representation for near-lossless LLM weight compression. arXiv preprint arXiv:2306.03078, 2023.

[13] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. HAWQ-V2: Hessian Aware trace-Weighted Quantization of neural networks. NeurIPS'19 workshop on Beyond First-Order Optimization Methods in Machine Learning., 2019.

[14] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLAM: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547-5569. PMLR, 2022.

[15] Georgii Evtushenko. Sparse Matrix-Vector Multiplication with CUDA. https://medium.com/analyticsvidhyalsparse-matrix-vector-multiplication-with-cuda-42d191878e8f, 2019.

[16] Goran Flegar and Enrique S Quintana-Ortí. Balanced csr sparse matrix-vector product on graphics processors. In Euro-Par 2017: Parallel Processing: 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28-September 1, 2017, Proceedings 23, pages 697-709. Springer, 2017.

[17] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.

[18] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.

[19] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.

[20] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and Kurt Keutzer. AI and Memory Wall. https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8, 2021.

[21] GPTQ-For-LLaMA. https://github.com/qwopqwop200/gptq-for-llama.

[22] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. International Conference on Learning Representations, 2016.

[23] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.

[24] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

[25] Yafeng Huang, Huanrui Yang, Zhen Dong, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Yuan Du, Shanghang Zhang, and Kurt Keutzer. Output sensitivity-aware detr quantization. 2023.

[26] Yongkweon Jeon, Chungman Lee, Eulrang Cho, and Yeonju Ro. Mr. BiQ: Post-training non-uniform quantization based on minimizing the reconstruction error. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12329-12338, 2022.

[27] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-BERT: Integer-only bert quantization. arXiv preprint arXiv:2101.01321, 2021.

[28] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, Sophia Shao, and Amir Gholami. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023.

[29] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. Bert busters: Outlier dimensions that disrupt transformers. arXiv preprint arXiv:2105.06990, 2021.

[30] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems, pages 598-605, 1990.

[31] Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. arXiv preprint arXiv:2302.04304, 2023.

[32] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. 2023.

[33] Yijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang. NoisyQuant: Noisy bias-enhanced post-training activation quantization for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20321-20330, 2023.

[34] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.

[35] Sangyun Oh, Hyeonuk Sim, Jounghyun Kim, and Jongeun Lee. Non-uniform step size quantization for accurate post-training quantization. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XI, pages 658-673. Springer, 2022.

[36] David A Patterson. Latency lags bandwith. Communications of the ACM, 47(10):71-75, 2004.

[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.

[38] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

[39] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-BERT: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815-8821, 2020.

[40] Gil Shomron, Freddy Gabbay, Samer Kurzum, and Uri Weiser. Post-training sparsity-aware quantization. Advances in Neural Information Processing Systems, 34:17737-17748, 2021.

[41] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.

[42] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[45] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023.

[46] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. arXiv preprint arXiv:2209.13325, 2022.

[47] Yuhui Xu, Yongzhuang Wang, Aojun Zhou, Weiyao Lin, and Hongkai Xiong. Deep neural network compression with single and multiple level quantization, 2018.

[48] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.

[49] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. RPTQ: Reorder-based post-training quantization for large language models. arXiv preprint arXiv:2304.01089, 2023.

[50] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. GOBO: Quantizing attention-based nlp models for low latency and energy efficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 811-824. IEEE, 2020.

[51] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8BERT: Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019.

[52] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[53] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. TernaryBERT: Distillationaware ultra-low bit bert. arXiv preprint arXiv:2009.12812, 2020.

[54] Yifan Zhang, Zhen Dong, Huanrui Yang, Ming Lu, Cheng-Ching Tseng, Yandong Guo, Kurt Keutzer, Li Du, and Shanghang Zhang. Qd-bev: Quantization-aware view-guided distillation for multi-view 3d object detection. 2023.

[55] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In International conference on machine learning, pages 7543-7552. PMLR, 2019.
