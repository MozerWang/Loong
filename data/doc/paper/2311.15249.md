# Algorithm Evolution Using Large Language Model 

Fei Liu, Xialiang Tong, Mingxuan Yuan and Qingfu Zhang, Fellow, IEEE


#### Abstract

Optimization can be found in many real-life applications. Designing an effective algorithm for a specific optimization problem typically requires a tedious amount of effort from human experts with domain knowledge and algorithm design skills. In this paper, we propose a novel approach called Algorithm Evolution using Large Language Model (AEL). It utilizes a large language model (LLM) to automatically generate optimization algorithms via an evolutionary framework. AEL does algorithmlevel evolution without model training. Human effort and requirements for domain knowledge can be significantly reduced. We take constructive methods for the traveling salesman problem as a test example, we show that the constructive algorithm obtained by AEL outperforms simple hand-crafted and LLM-generated heuristics. Compared with other domain deep learning modelbased algorithms, these methods exhibit excellent scalability across different problem sizes. AEL is also very different from previous attempts that utilize LLMs as search operators in algorithms.


Index Terms-Algorithm evolution, Large language model, Combinatorial optimization, Evolutionary optimization, Heuristic.

## I. INTRODUCTION

OPTIMIZATION is everywhere. It plays a crucial role in production, planning, decision making, and resource management. Numerous research works have been carried out to develop efficient and powerful optimization algorithms. While these algorithms have proven to be valuable tools in many practical scenarios, designing them usually requires extensive manual crafting with domain knowledge [1]- [3].

To overcome these limitations, researchers have made much effort to automate the algorithm design process. Learning to optimize [4] involves automatically designing optimization methods based on their performance on a training set of problems. In deep learning, AutoML [5] offers promising solutions for constructing a deep learning system without human intervention. In the context of heuristic design, this is commonly referred to as hyperheuristics [6], [7] or automatic design of heuristics [8]. Moreover, reinforcement learning [9][12], supervised learning [13], [14], transfer learning [15]-[17] and meta-learning [18], [19] have also been widely employed to enhance optimization efficiency, promote algorithm performance, and generate new algorithms. Furthermore, recent works have explored the use of end-to-end neural solvers for both continuous optimization [20]-[24] and combinatorial optimization [25]-[29]. While these progresses have significantly advanced the field of automatic algorithm design, they

Fei Liu and Qingfu Zhang are with the Department of Computer Science, City University of Hong Kong, Hong Kong (e-mail: fliu36c@my.cityu.edu.hk; qingfu.zhang@cityu.edu.hk).

Xialiang Tong and Mingxuan Yuan are with Huawei Noah's Ark Lab (email: tongxialiang@huawei.com; yuan.mingxuan@huawei.com).

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-01.jpg?height=323&width=260&top_left_y=478&top_left_x=1128)

(a) Human

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-01.jpg?height=323&width=252&top_left_y=478&top_left_x=1381)

(b) Domain Model

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-01.jpg?height=320&width=287&top_left_y=480&top_left_x=1624)

(c) AEL

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-01.jpg?height=618&width=826&top_left_y=884&top_left_x=1105)

Fig. 1. A comparison of three different algorithm design approaches (a) Human, (b) Domain Model, and (c) AEL, and the their results on TSP. The $\mathrm{x}$-axis represents the problem size. The $\mathrm{y}$-axis represents the gap (\%) to the baseline. All results are averaged on 64 randomly generated instances. (a) Human (Greedy): an algorithm designed by humans with trial and error (a greedy algorithm). (b) Domain Model: an algorithm learned by a specific deep neural network trained on TSP50. (c) AEL: algorithms created by our proposed AEL evolved on TSP50. We also compare the algorithms directly generated by instructing LLM (LLM). The used LLMs are denoted in brackets. Refer to the experimental section for more details.

usually require an underlying algorithm framework or a timeconsuming domain model crafting and training.

In the past three years, large language models (LLMs) have demonstrated remarkable capabilities in various research domains [30], [31], such as natural language processing [32], programming [33], medicine [34]-[36], chemistry [37], chip design [38], [39], and optimization [40]-[43]. Recently, several works have adopted LLMs as pre-trained black-box optimizers for optimization [21], [44]-[47]. However, these works use LLMs to generate new solutions at the operator level. Its performance declines considerably when applied to large-scale problems, mainly due to the longer solution representation and large search space [44], [47], [48].

In this paper, we propose a novel approach for practical automatic algorithm design called Algorithm Evolution using Large Language Model (AEL). AEL distinguishes itself from
algorithm design by humans and through training domain models. It creates and modifies algorithms by interacting with large language models within an evolutionary framework. We demonstrate the effectiveness of AEL on the constructive method for the traveling salesman problem (TSP), highlighting its ability to generate novel and scalable algorithms. The contributions of this paper are summarized as follows:

- We introduce AEL, which treats each algorithm as an individual and utilizes an evolutionary framework to evolve new algorithms. Our approach integrates large language models at the algorithm design level, allowing for the creation and modification of existing search strategies with minimal expert knowledge and no domain model training.
- We demonstrate AEL on designing the constructive heuristic for TSP, a well-known combinatorial optimization problem. The individual representation and prompt engineering strategies for TSP are designed.
- We compare three algorithm design approaches (a) a greedy algorithm designed by humans (Human), (b) an algorithm learned by a specific model through timeconsuming training (Domain Model), and (c) algorithms designed automatically by our proposed AEL (AEL). The comparison of these three approaches and a summary of results on TSP with different problem sizes are illustrated in Fig. 1 The results demonstrate that AEL outperforms the manually designed greedy heuristic for all problem sizes. Furthermore, AEL exhibits better generalization performance compared to training a domain model. We also highlight the superiority of AEL over directly instructing LLM (LLM) to generate algorithms. AEL benefits from the use of more advanced language models, such as GPT-4.

The rest of this paper is structured as follows: In Section II. we provide an overview of related work in automatic algorithm design and the use of large language models in optimization. Section III presents the framework and methodology of our proposed AEL. In Section IV, we present the demonstration and experimental results on TSP and a discussion of findings followed by some suggested future directions. Section VI concludes the paper.

## II. RELATED WORKS

## A. Automatic Algorithm Design (AAD)

Automatic algorithm design (AAD) is an active and rapidly growing research area in heuristic design [49]. It is commonly referred to as hyper-heuristics [6], [7] or automatic design of heuristics [8]. A number of effective toolboxes and frameworks have been proposed, such as GGA [50], ParamILS [51], MO-ParamILS [52], irace [53], SMAC [54], and Optuna [55], which significantly facilitate researchers in the field. From the algorithm perspective, AAD can be briefly categorized into three main approaches: automatic algorithm configuration, automatic algorithm selection, and automatic algorithm composition [56]. Many recent research works in AAD focus on automatically generating improved algorithms using general heuristic components [8]. For example, AutoEMOA for multi-objective evolutionary optimization [57] and AutoGCOP for general-purpose optimization [56]. Despite these advancements, these approaches still require a backbone domain algorithm framework or a set of manually crafted algorithm components, and the designing process can be timeconsuming.

## B. Machine Learning for Optimization

Over the past few decades, much effort has been made on the integration of machine learning techniques for optimization [4], [5], [58], [59]. Reinforcement learning techniques have been used to learn optimal algorithm configurations [9], policy for operator selection [10], [11], solution selection [12], and construction of the partial solution [60]. [13], [14], [61], [62] adopt supervised learning for solution prediction to boost heuristics and mathematical programming. Transfer learning [15], [16] and meta-learning [18], [19] have also been employed to transfer and extract knowledge from different tasks or solutions to improve optimization efficiency. Other popular directions include using surrogate models to approximate the objective functions or relations, which have been successfully applied in expensive optimization in various domains [63]-67], and learning through genetic programming [68], [69], which is explainable and well-suited for structure representation.

Furthermore, recent works have explored the use of end-toend neural solvers for both continuous optimization [20]- [24] and combinatorial optimization [25]-[29]. Some of them have been applied for EC. For example, [20]-[23] train deep neural networks to approximate mutation, crossover, and evolutionary strategies for black-box optimization. [24] enhances evolutionary algorithms through learning from historically successful experiences. However, in spite of the promising results, they often require significant effort in designing and training the domain models.

## C. Large Language Model (LLM)

In the last three years, large language models (LLMs) have gained increasing power due to their exponentially growing model sizes and the availability of large training datasets. LLMs have shown remarkable performance in various research domains including natural language processing [32], programming [33], medicine [34]-[36], chemistry [37], chip design [38], [39], and optimization [40]-[42]. These LLMs excel at performing diverse tasks in a zero-shot manner [30], [31]. Despite these advancements, the practical utilization of LLMs for designing optimization algorithms is still in its early stages.

## D. LLMs for Algorithm Design

Recently, several works have demonstrated the potential of optimization solely through prompting LLM [44], [70]. [48] proposes a decomposition-based framework that integrates LLM as a black-box operator for multiobjective evolutionary optimization. In the context of single-objective evolutionary
![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-03.jpg?height=870&width=1606&top_left_y=194&top_left_x=256)

Fig. 2. An illustration of the AEL framework. The left-hand side flowchart adopts a standard evolutionary framework, comprising prompt engineering of LLM for initialization, crossover, and mutation to create/evolve new algorithms. On the right-hand side, there are two examples demonstrating algorithm crossover and algorithm mutation, specifically in their application to selecting the next node in a route.

algorithms, [44], [71], [72] adopt LLM for the selection, crossover, and mutation processes. LLM has also been integrated into neural architecture design [46], [73], [74], Monte Carlo Tree Search [43] and graph-based combinatorial optimization [47]. The applications of LLM for genetic programming and open-ended tasks are discussed in [45], [75].

However, most of these works directly use LLM as optimizers, which suffers from poor generalization performance on large-scale problems. The longer solution representation and large search space, especially on combinatorial optimization problems [44], [47], [72], pose significant challenges to zero-shot generalization and in-context learning for LLM. [48] proposes to use an explainable lightweight operator to approximate the results of LLMs for better generalization in continuous optimization, which is not suitable for addressing combinatorial optimization problems.

## III. AlgorithM EvoluTION USING LARGE LANGUAGE MODEL

## A. AEL Framework

The proposed Algorithm Evolutionary using Large Language Model (AEL) framework embraces the common framework utilized in evolutionary computing (EC). It evolves a population of individuals and comprises fundamental components, including initialization, selection, crossover, mutation, and population management. In spite of the general evolution framework, AEL differs significantly from existing approaches.

- Firstly, unlike major algorithms in EC, where individuals represent feasible solutions, each individual within our
AEL framework represents an algorithm designed explicitly for a given problem. AEL evolves algorithms capable of generating novel and competitive search strategies for a target problem, rather than seeking improved solutions for specific instances.
- Furthermore, AEL distinguishes itself from other automatic algorithm design methods. By integrating LLMs into an evolutionary framework, the creation and refinement of algorithms occur automatically, eliminating the need for training new models or utilizing baseline algorithms. In contrast, existing automatic algorithm design methods often require expensive searches for improved algorithms or rely on specific model training.

Fig. 2 illustrates the AEL flowchart and examples of algorithm crossover and algorithm mutation. AEL begins with an initialization step, where a population of $N$ individuals (algorithms), denoted as $P=\left\{a_{1}, \ldots, a_{N}\right\}$, is created and evaluated. The creation of algorithms in the initializations can be either using existing algorithms or letting LLM generate algorithms. Each individual $a_{j}$ is evaluated using a fitness function, resulting in the fitness value $f\left(a_{j}\right)$. The fitness is evaluated on a branch of evaluation instances instead of on one instance.

The framework then proceeds with a series of iterations, for a total of $N_{g}$ generations with $N$ iterations for each generation. In each iteration, a subset of $l$ individuals $p_{j}=$ $\left\{a_{1}, \ldots, a_{l}\right\}$ is selected from the population using a selection method (we select each individual with equal probability in the experiments). The selected individuals are then subjected to a crossover operation with a probability $\theta_{1}$, resulting in a new set of $s$ individuals $o_{j}=\left\{a_{1}, \ldots, a_{s}\right\}$ created by LLM.

The crossover operation ensures the exploration of different genetic information from the input subset $p_{j}$.

For each individual $a_{k}$ in the set $o_{j}$, a mutation operation is applied with a probability $\theta_{2}$. This operation modifies the input algorithm, potentially introducing a new algorithm or an algorithm with different parameters into the population. Subsequently, the fitness of each mutated individual $a_{k}$ is evaluated.

To maintain a constant population size during the evolution process, population management is performed. The new individuals $o_{j}$ are added to the population $P$. Afterward, the population $P$ is managed to reduce its size from $(s+1) \times N$ to $N$ by deleting the worst ones.

Ultimately, the AEL Framework aims to find the best algorithm within the given population, denoted as $a^{*}$. By iteratively applying selection, crossover, mutation, and population management, AEL stimulates LLM to evolve a better algorithm for the optimization problem at hand.

```
Algorithm 1: AEL Framework
    Input: The number of population: $N_{g}$; Population size
        $N$; Probability of crossover $\sigma_{1}$; Probability of
        mutation $\sigma_{2}$; The number of parents $l$ for
        crossover; The number of new individuals $s$ for
        crossover; A given LLM.
    Output: Best algorithm $a^{*}$.
    Initialization:
    for $j=1, \ldots, N$ do
        Algorithm Creation: create new individual $a_{j}$
            given the target problem using LLM; Evaluate $a_{j}$
        and get fitness value $f\left(a_{j}\right)$;
    Construct initial population $P=\left\{a_{1}, \ldots, a_{N}\right\}$;
    for $i=1, \ldots, N_{g}$ do
        for $j=1, \ldots, N$ do
            Selection: select a subset of input individuals
            $p_{j}=\left\{a_{1}, \ldots, a_{l}\right\}$
            Algorithm Crossover with probability $\theta_{1}$ :
            create individual $o_{j}=\left\{a_{1}, \ldots, a_{s}\right\}$ using LLM
            given the target problem and input subset $p_{j}$;
        for $k=1, \ldots, s$ do
            Algorithm Mutation with probability $\theta_{2}$ :
                modify individual $a_{k}$ using LLM;
                Evaluate $a_{k}$ and get fitness value $f\left(a_{k}\right)$;
    Population management: $P=P \cup\left\{o_{1}, \ldots, o_{N}\right\}$,
        manage population $P$ to reduce the size from
        $(s+1) \dot{N}$ to $N$.
```


## B. Individual Representation

Unlike previous works, our proposed AEL approach represents each individual as an algorithm tailored to the specific problem. Each individual consists of three components: 1) a description of the algorithm in natural language, 2) a code block in a pre-defined format, and 3) a fitness value.

The algorithm description comprises a few sentences in natural language that can be easily processed by LLM. The code block should follow a predefined format so that it can be identified and seamlessly integrated into our AEL framework. We introduce the four basic components that should be included in the prompt engineering to format the code block:

- Name of class or function: The name of the class or function must be standardized to ensure easy identification by the main program.
- Input: The number and names of input variables need to be provided, along with their types and their characters in the program. This information helps LLM understand the available information and design a viable algorithm based on it.
- Output: The number and names of output variables should also be defined, along with their types and their utilization.
- Other hints: we expect the response to be innovative, and want to avoid too many explanations and code comments, which might lead to failure identification and increase the response time and cost. Any other problem-specific hints should also be included in the prompt.

The evaluation of individuals in AEL involves running the algorithms on an evaluation instance set of the target problem. This evaluation process differs from traditional evolutionary computing, which typically evaluates the objective function for a single instance, but is closer to AAD methods [51], [53], [54].

## C. Initilization

The initial population can be either constructed by using existing manually crafted algorithms or created using LLM. In our experiments, we choose to let LLM create all the initial algorithms to eliminate the use of expert knowledge. We provide the following guidelines for prompt engineering for algorithm creation using LLMs. The prompt should include the following three parts:

- A description of the task: A description of the optimization task. The description should be a concise but comprehensive introduction to the target problem.
- An expected output: A description of the expected responses that we want. We desire both a description of the new algorithm and a corresponding code block with pre-defined inputs and outputs. The responses should be provided in a specific format.
- Initialization-specific hints: We prioritize the inclusion of diverse algorithms during the initialization process, thus emphasizing the development of a novel algorithm that distinguishes itself from those in the literature. We may also encourage more randomness and diversity by explicitly instructing the LLM to be creative.

We run the prompting procedure for $N$ times to generate $N$ initial algorithms.

## D. Selection

We simply select input $l$ algorithms randomly from the population $P$. This selection strategy aligns with the conventional EC. The difference is that The number of selected
individuals in our framework is scalable, as the LLM takes prompt in a flexible manner. This scalability is only limited by the maximum input token size depending on the adopted LLM.

## E. Crossover

For the $j$-th iteration, the input of crossover consists of a set of $l$ selected individuals, denoted as $p_{j}=\left\{a_{1}, \ldots, a_{l}\right\}$, and the output is a new set of $s$ individuals, denoted as $o_{j}=\left\{a_{1}, \ldots, a_{s}\right\}$. The process of prompt engineering for crossover involves using the target problem and the selected individuals to generate a prompt for LLM, which in turn creates new algorithms. The prompt itself is a combination of natural language and code and is structured into four parts:

- A description of the task: It is identical to that used during initialization.
- Parent algorithms: It comprises a set of $l$ input parent individuals. For each individual, both the algorithm description and code are provided.
- An expected output: It includes a description of the desired responses in a specific format.
- Crossover-specific hints: It emphasizes the need for the new algorithm to draw motivation from the parent algorithms while still being distinct from them.


## F. Mutation

In mutation, the input algorithm is modified using LLM to create a new algorithm. The engineering approach for mutation involves using the target problem and one input individual to generate a prompt that allows for the modification of the input algorithm with LLM. The prompt consists of a combination of natural language and code, and it includes the following four components:

- A description of the task: It is identical to that used during initialization.
- A parent algorithm: Only one input parent individual. Both the algorithm description and code are provided.
- An expected output: A description of the expected responses we want. The responses should be given in a specific format.
- Mutation-specific hints: It emphasizes that the new algorithm should be a revision of the input algorithm.


## G. Population Management

In population management, we remove the worst individuals in terms of fitness value to reduce the population size from $(s+1) N$ to $N$.

## IV. DEMONSTRATION ON TSP

The traveling salesman problem (TSP) is one of the most important combinatorial optimization problems. The problem is to find the shortest route to visit all the given locations once and return to the starting location. It is recognized as NPhard and is usually solved using heuristic algorithms. Among different heuristics [76], constructive heuristics are flexible and easy to implement. In constructive heuristics, the solution is constructed step-by-step by choosing the next node given the current node and the destination node. This autoregressive manner is also used by many recent works on learning a domain neural model for combinatorial optimization [25], [77.

We adopt the same constructive framework to iteratively choose the next node. The task for AEL in this case is to create a novel and competitive algorithm to choose the next node.

## A. AEL Implementation

AEL is a general framework for algorithm design. For a target problem like TSP, we only need to implement the problem-specific parts. i.e., the individual representation and the prompting engineering for initialization, crossover, and mutation.

1) Individual Representation: The objective of AEL is to evolve an algorithm that identifies the next node based on problem information and the current state. As mentioned previously, each individual in AEL comprises three components: 1) an algorithm description, 2) a code block, and 3) a fitness value. Fig 3 presents an example of the individual representation of the greedy algorithm, which selects the next node that is the nearest one to the current node.

The algorithm description is presented in a natural language format. To avoid a noisy long response, we explicitly inform the LLM that the algorithm description should not exceed two sentences. It is important to note that, in more complex tasks, a longer sentence limitation can be adopted. In addition to it, we pose no additional limitations.

The code is a Python function named "select_next_node", which takes inputs including the current node, destination node, unvisited nodes, and distance matrix, and outputs the next selected node.

```
Algorithm Description:
The algorithm selects the next nearest unvisited node based on the current node,
destination node, unvisited nodes, and distances between them. It iterates through each
unvisited node and calculates the distance between the current node and each unvisited
node. The node with the shortest distance is chosen as the next node.
Code Block:
def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):
    shortest_distance $=$ float('inf')
    next_node $=$ None
    for node in unvisited_nodes:
        distance $=$ distance_matrix[current_node][node]
        if distance $<$ shortest_distance:
            shortest_distance = distance
            next_node $=$ node
    return next node
```

Fig. 3. An example of individual representation of the greedy algorithm. Algorithm Description is a brief algorithm description in two sentences. Code Block includes a Python function named "select_next_node" with a pre-defined input and output. The fitness value is a real number, which is not depicted.

The fitness value is evaluated using a set of 64 randomly generated TSP instances of size 50 . The fitness is calculated as the average gap to the commercial solver Gurobi. Smaller values indicate better fitness.

2) Prompt Engineering for Initialization, Crossover, and Mutation: The details are illustrated in Fig. 4 The five different colors represent five different components including A description of task, Parent algorithm(s), Prompt-specific hints, An expected output, and Other hints.

The task involves developing a new strategy for selecting the next node at each step, which remains consistent across all three prompts. Only crossover and mutation include parent algorithms in the prompts. Regarding prompt-specific hints, we instruct the LLM to create a completely new algorithm during initialization, while in crossover and mutation, the algorithm should be inspired by and modified from the parent algorithm(s). This description aligns with the respective functions of each component in the evolutionary framework: the desire for diverse algorithms in the initial population and the expectation that newly created algorithms during evolution will inherit certain aspects from their parents. The format of the expected output remains almost identical for all three prompts. We explicitly define the name, input, and output of the code block for easy identification by the AEL framework. Additionally, we include other hints to emphasize innovation and discourage the need for extra explanations for efficiency and robustness.

## B. Experiments

1) Experimental Settings: The experiments are carried out on 64 randomly generated TSP50 instances, i.e., each algorithm is evaluated on 64 TSP50 instances and the fitness value is the average gap to the optimal solution generated by Gurobi. The experimental settings for AEL are as follows:

- Population size $N: 10$
- Number of population $N_{g}: 10$
- Probability for crossover $\sigma_{1}: 1.0$
- Probability for mutation $\sigma_{2}: 0.2$
- Number of parent individuals $l: 2$
- Number of offspring individuals $s: 1$
- LLM: GPT-3.5-turbo and GPT-4

We compared our AEL method to three kinds of existing algorithm design approaches. Note that there are countless works using other frameworks. We compare the methods that follow the same step-by-step constructive framework as that used for AEL. We are able to further promote the performance by applying AEL in other advanced frameworks.

The three compared approaches as well as the methods are:

- Algorithm design by humans: Greedy search (Greedy), which selects the nearest node as the next node.
- Algorithm design using domain model: Neural combinatorial optimization (Domain model), which trains a neural network to learn the heuristic for selecting the next node. In this study, we adopt POMO [77], a widely employed neural solver baseline. We train it on TSP50 using exactly the same settings as in the original paper [77]. The training costs about four days.
- Algorithm design using LLM: LLM with prompt engineering (LLM). We directly generate a novel algorithm by instructing LLM. The prompt is the same as that used in the initialization stage of AEL.
- AEL: Our proposed AEL.

```
Initialization Prompt for TSP:
Task: Given a set of nodes with their coordinates, you need to
find the shortest route that visits each node once and returns to
the starting node. The task can be solved step-by-step by starting
from the current node and iteratively choosing the next node.
You should create a totally new strategy for me (different from
the heuristics in the literature) to select the next node in each step,
using information including the current node, destination node,
unvisited nodes, and distances between them.
Provide a brief description of the new algorithm and its
corresponding code. The description must start with '<start>' and
end with '<end>'. The code function must called
'select_next_node' that takes inputs 'current_node',
'destination node', 'unvisited nodes', and 'distance matrix', and
outputs the 'next_node', where 'current_node', 'destination_node',
'next_node', and 'unvisited_nodes' are node id.
Be creative and do not give additional explanation.
```


## Crossover Prompt for TSP:

Task: Given a set of nodes with their coordinates, you need to find the shortest route that visits each node once and returns to the starting node. The task can be solved step-by-step by starting from the current node and iteratively choosing the next node.

I have <l> algorithms with their code to select the next node in each step.

The first algorithm and the corresponding code is: $<$ Algorithm description>: ...

<Code>: . . .

The second algorithm and the corresponding code is <Algorithm description>: ..

<Code>: . .

(more . . .

Please help me create a new algorithm that motivated by the given algorithms. Please provide a brief description of the new algorithm and its corresponding code. The description must start with '<start>' and end with '<end>'. The code function must called 'select_next_node' that takes inputs 'current_node', 'destination_node', 'unvisited_nodes', and 'distance_matrix', and outputs the 'next_node', where 'current_node', 'destination_node', 'next_node', and 'unvisited_nodes' are node id.

Be creative and do not give additional explanation.

```
Mutation Prompt for TSP:
Task: Given a set of nodes with their coordinates, you need to find
the shortest route that visits each node once and returns to the
starting node. The task can be solved step-by-step by starting from
the current node and iteratively choosing the next node.
I have an algorithm with its code to select the next node in each step.
The algorithm and the corresponding code is:
<Algorithm description>: ..
<ode>: .. 
Please assist me in creating a modified version of the algorithm
provided. Please provide a brief description of the new algorithm and
its corresponding code. The description must start with '<start>' and
end with '<end>'. The code function must called 'select_next_node
that takes inputs 'current_node', 'destination_node', 'unvisited_nodes',
and 'distance_matrix', and outputs the 'next_node', where
'current_node', 'destination_node', 'next_node', and 'unvisited_nodes'
are node id.
Be creative and do not give additional explanation
```

Fig. 4. Prompts used in the initialization, crossover, and mutation of AEL for TSP: A description of task, Parent algorithm(s), Prompt-specific hints, An expected output, and Other hints.

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-07.jpg?height=523&width=876&top_left_y=188&top_left_x=169)

Fig. 5. The convergence curve of AEL using GPT-4 on TSP50, where each sample represents an algorithm created in the evolution. The orange and red lines represent the mean and best objective values and the dotted black line represents the greedy algorithm.

2) Experimental Results: Fig 5 illustrates the convergence process of the proposed AEL using GPT-4 on the TSP50 dataset. The y-axis represents the gap (\%) to the optimal solution and the $\mathrm{x}$-axis represents the number of generations. Each blue data point represents an algorithm created by AEL during the evolution. The orange and red lines depict the convergence curves of the mean and best objective values, respectively, in each population. The black line represents the greedy algorithm designed by humans.

It can be observed that there is a clear convergence in terms of both the mean and best objective values as the evolution progresses. AEL nearly converges in 10 generations, and the optimal gap is reduced from $20 \%$ to around $12 \%$. The best algorithm generated by GPT-4 in the first population is close to the greedy algorithm, while AEL clearly beats the greedy algorithm towards the end of evolution.

As illustrated in Fig. 6, the best algorithm created by AEL is significantly more complicated than the greedy algorithm. It selects the next node considering various factors such as its distance to other unvisited nodes, mean distance, and standard deviation of these distances. The algorithm assigns a higher weight to close clusters of nodes by incorporating the standard deviation into the scoring system. Additionally, the algorithm includes a conditional statement that ensures nodes far from the rest are not chosen prematurely by selecting the closest node when the minimum calculated score exceeds a specified threshold. The created Python code block employs the NumPy library for calculations. The code starts with defining the select_next_node function, which takes the current_node, destination_node, unvisited_nodes, distance_matrix, and an optional threshold parameter. Inside the function, a dictionary called scores is created to store the scores for each unvisited node. The algorithm iterates through each node in the unvisited_nodes list and calculates the score based on the provided formula. The minimum score is then compared to the specified threshold, and based on the result, the next_node is determined. Finally, the function returns the selected next_node.

The automatically designed best algorithm by AEL presents a sophisticated strategy incorporating an additional threshold

```
Algorithm Description:
This enhanced algorithm considers the current node's distance to the other unvisited
nodes and the mean distance, but also incorporates the standard deviation of these
distances into the scoring system, giving a higher weight to close clusters of nodes. It also
includes a conditional statement that chooses the closest node when the minimum
calculated score exceeds a specified threshold, ensuring that nodes far from the rest are
not chosen prematurely.
Code Block:
import numpy as np
def select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix,
        threshold=0.7):
    scores = {}
    for node in unvisited_nodes:
        all_distances = [distance_matrix[node][i] for i in unvisited_nodes if i != node]
        average_distance_to_unvisited = np.mean(all_distances)
        std_dev_distance_to_unvisited = np.std(all_distances)
```

score $=0.4 *$ distance_matrix[current_node][node] - $0.3 *$
average_distance_to_unvisited +0.2 * std_dev_distance_to_unvisited - 0.1 *
distance_matrix[destination_node][node]
scores[node] = score
if min(scores.values()) > threshold:
next_node $=$ min(unvisited_nodes, key=lambda node:
distance_matrix[current_node][node])
else:
next_node $=\min ($ scores, key=scores.get $)$
return next_node

Fig. 6. The best algorithm created by AEL using GPT-4. Algorithm Description is a brief algorithm description in two sentences. Code Block includes a Python function named "select_next_node" with a pre-defined input and output.

parameter (not explicitly given in the prompt) and a complex scoring function. The scoring function integrates multiple factors along with four hyperparameters, which presents a challenge, even for an expert, without substantial trial-anderror testing. In contrast, AEL automatically develops the algorithm through an iterative process involving 100 interactions with LLM.

3) Evaluation of Optimized Algorithm: We evaluate the best algorithm designed by AEL on various TSP instances with multiple problem sizes and compared the results to other algorithm design approaches. Table $\square$ shows the performance of algorithms designed using different approaches on TSP20 to TSP1000. The results are averaged on 64 randomly generated instances. Apart from the total distance, we also measured the average gap in relation to the baseline solver LKH3. Fig. 1 provides a more intuitive comparison of the average gap vs. the problem size of different algorithm design approaches.

- AEL outperforms the simple Greedy algorithm designed by humans in all problem sizes. The average gap is reduced by half from $17.0 \%$ to $6.2 \%$ and from $25.2 \%$ to $12.8 \%$ for TSP20 and TSP1000, respectively.
- AEL demonstrates significantly better generalization performance across various problem sizes when compared to the domain model. The domain model is trained and overfitted on TSP50, whereas AEL presents a much more robust solution. Although the domain model surpasses AEL in problem sizes close to the training data, its performance rapidly deteriorates on large-scale problems. The average gap increases dramatically from less than $1 \%$ to over $50 \%$.
- AEL also outperforms directly instructing LLM to design algorithms. LLM (Average) and LLM (Best) represent the

TABLE I

EVALUATION OF ALGORITHMS DESIGNED BY DIFFERENT APPROACHES ON TSP20-TSP1000.

| Algorithms | 20 |  | 50 |  |  | 100 |  |  |  |  |  |  |  |  |  |  | 200 |  | 500 |  | 1000 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Dis. | Gap. | Dis. | Gap. | Dis. | Gap. | Dis. | Gap. | Dis. | Gap. | Dis. | Gap. |  |  |  |  |  |  |  |  |  |
| Baseline (SOTA Solver LKH3) | 3.84 | $/$ | 5.69 | $/$ | 7.77 | $/$ | 10.73 | $/$ | 16.56 | $/$ | 23.08 | $/$ |  |  |  |  |  |  |  |  |  |
| Human (Greedy) | 4.49 | $17.0 \%$ | 7.01 | $23.1 \%$ | 9.84 | $26.6 \%$ | 13.50 | $25.8 \%$ | 20.87 | $26.0 \%$ | 28.90 | $25.2 \%$ |  |  |  |  |  |  |  |  |  |
| Domain Model | $\mathbf{3 . 8 6}$ | $\mathbf{0 . 6 \%}$ | $\mathbf{5 . 7 1}$ | $\mathbf{0 . 4 \%}$ | $\mathbf{8 . 0 1}$ | $\mathbf{3 . 0 \%}$ | 13.02 | $21.3 \%$ | 24.34 | $47.0 \%$ | 36.53 | $58.3 \%$ |  |  |  |  |  |  |  |  |  |
| LLM (Average) | GPT-3.5-turbo | 5.04 | $31.3 \%$ | 7.56 | $32.8 \%$ | 10.62 | $36.7 \%$ | 14.35 | $33.7 \%$ | 22.04 | $33.1 \%$ | 30.05 | $30.2 \%$ |  |  |  |  |  |  |  |  |
|  | GPT-4 | 7.45 | $94.2 \%$ | 14.97 | $162.9 \%$ | 24.87 | $220.1 \%$ | 37.36 | $248.0 \%$ | 61.73 | $272.8 \%$ | 86.26 | $273.8 \%$ |  |  |  |  |  |  |  |  |
| LLM (Best) | GPT-3.5-turbo | 4.61 | $20.3 \%$ | 6.71 | $17.9 \%$ | 10.01 | $28.9 \%$ | 13.31 | $24.0 \%$ | 20.83 | $25.8 \%$ | 28.98 | $25.6 \%$ |  |  |  |  |  |  |  |  |
|  | GPT-4 | 4.36 | $13.6 \%$ | 6.82 | $19.7 \%$ | 9.95 | $28.1 \%$ | 13.94 | $29.9 \%$ | 22.07 | $33.3 \%$ | 30.36 | $31.6 \%$ |  |  |  |  |  |  |  |  |
| AEL (Ours) | GPT-3.5-turbo | 4.26 | $11.2 \%$ | 6.65 | $16.8 \%$ | 9.32 | $20.0 \%$ | 13.07 | $21.8 \%$ | 20.38 | $23.1 \%$ | 28.34 | $22.8 \%$ |  |  |  |  |  |  |  |  |
| AEL (Ours) | GPT-4 | 4.07 | $6.2 \%$ | 6.33 | $11.1 \%$ | 8.58 | $10.5 \%$ | $\mathbf{1 1 . 9 4}$ | $\mathbf{1 1 . 2 \%}$ | $\mathbf{1 8 . 6 7}$ | $\mathbf{1 2 . 8 \%}$ | $\mathbf{2 6 . 0 3}$ | $\mathbf{1 2 . 8 \%}$ |  |  |  |  |  |  |  |  |

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-08.jpg?height=1550&width=1803&top_left_y=927&top_left_x=172)

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-08.jpg?height=428&width=577&top_left_y=938&top_left_x=183)

(a) TSP100, Greedy

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-08.jpg?height=431&width=577&top_left_y=1449&top_left_x=183)

(d) TSP500, Greedy

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-08.jpg?height=434&width=577&top_left_y=1965&top_left_x=183)

(g) TSP1000, Greedy

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-08.jpg?height=431&width=575&top_left_y=934&top_left_x=772)

(b) TSP100, AEL (GTP-3.5-turbo)

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-08.jpg?height=434&width=577&top_left_y=1450&top_left_x=774)

(e) TSP500, AEL (GPT-3.5-turbo)

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-08.jpg?height=431&width=577&top_left_y=1964&top_left_x=771)

(h) TSP1000, AEL (GPT-3.5-turbo)

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-08.jpg?height=431&width=593&top_left_y=934&top_left_x=1357)

(c) TSP100, AEL (GTP-4)

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-08.jpg?height=431&width=591&top_left_y=1454&top_left_x=1358)

(f) TSP500, AEL (GPT-4)

![](https://cdn.mathpix.com/cropped/2024_06_04_001c48c0467234ff768ag-08.jpg?height=434&width=593&top_left_y=1962&top_left_x=1357)

(i) TSP1000, AEL (GPT-4)

Fig. 7. A comparison of the greedy algorithm and algorithms developed by AEL, utilizing GPT-3.5-turbo and GPT-4, for solving TSP100, TSP500, and TSP1000 instances. The nodes in the scatter plot represent locations, while the blue lines represent routes generated by the various algorithms. The numbers displayed above the scatter plot indicate the sequence in which the locations appear in the route. The starting and ending locations are denoted by red nodes.
average and best of ten algorithms directly generated by instructing LLM. The best LLM algorithm is competitive to the greedy algorithm but evidently inferior to AEL. Interestingly, the best gap achieved by LLM with GPT-4 is inferior to GPT-3.5-turbo. An explanation for this is that GPT-4 is more powerful but can also be overly innovative with excessive randomness. Consequently, the algorithms directly generated by GPT-4 could be considerably worse.

- We also want to note that the demonstration is conducted based on a basic constructive heuristic framework. Our comparison involves the algorithm created by AEL, a greedy algorithm, and a domain model, all using the same step-by-step constructive framework. However, there are numerous other complex algorithms capable of generating near-optimal solutions for TSP [78]. Additionally, recent neural solvers specifically designed for largescale TSP have also demonstrated good generalization performance [27], [28], [79]. Advanced frameworks can be integrated to promote performance in the future.

Fig 7 compares the routes generated using the greedy algorithm and algorithms developed by AEL, utilizing GPT3.5-turbo and GPT-4, on TSP100, TSP500, and TSP1000 instances. The scatter plot nodes represent locations, with the blue lines indicating the routes generated by different algorithms. The numbers displayed above the scatter plot indicate the sequence in which the locations appear in the route. The starting and ending locations are marked in red. The results demonstrate that the algorithms designed by AEL produce superior routes with fewer intersections than the greedy algorithm, resulting in shorter total distances. Additionally, the starting and ending nodes are closer together compared to the greedy algorithm. AEL using GPT-4 outperforms AEL using GPT-3.5-turbo.

## C. Created Algorithms or Spliced Algorithms

It is debatable whether the LLM can genuinely comprehend and generate new knowledge or if it simply searches and combines various existing information [80]. As the TSP is a well-studied combinatorial optimization problem with numerous publicly available resources and research papers, it is possible that the LLM merely selects or splices existing algorithms [81]. However, the majority of the algorithms created in our AEL framework cannot be found on the web or in publications. The comparison between AEL algorithms and LLM algorithms also demonstrates that AEL stimulates the creation of novel algorithms to a noteworthy extent by combining evolutionary computing and LLM.

## D. Less/No Expert Knowledge

AEL requires minimal expert knowledge about the target problem. For designers using our AEL framework, the primary workload lies in designing the prompt engineering for each component for the target problem. The prompt engineering is presented in a natural language format, making it accessible to algorithm designers from diverse backgrounds.

## V. Future WORKS

- AEL with tools: Equipping LLMs with external tools significantly enhances the capabilities of the model [82][84]. There are numerous well-crafted existing optimization algorithms, which can be integrated as effective tools. This approach provides a more flexible and robust framework as opposed to relying solely on LLM for algorithm evolution. We can also integrate other heuristic frameworks into AEL. For example, landscape updating for guided local search [85], improve large neighborhood search [86], and Tabu search [87].
- AEL with additional information: Another interesting direction is to provide additional information as the input for LLM during the optimization process. The information can be in the form of history search trajectories, external archives, and rewards obtained during optimization [88], [89]. With more available information, AEL is able to evolve more powerful algorithms.
- AEL for complex optimization problems: Complex problems pose challenges for AEL as they are difficult for LLMs to comprehend, which is even challenging for humans. In such cases, it is possible to decompose the problem into simpler tasks and adopt LLMs for each task [90]. A possible solution is to separate the algorithm description and coding and evolve in a hierarchical way, i.e., evolve on the algorithm and then generate code for each algorithm. Refinement techniques [91]-[93] can be applied to reduce the failure rate.
- Multi-objective AEL: The current focus of AEL is mainly on single-objective optimization problems. However, many real-world problems are multi-objective in nature, where multiple conflicting objectives need to be simultaneously considered. As AEL is algorithm-agnostic, we can easily extend AEL to handle multi-objective optimization problems using multi-objective optimization algorithms [94]. Multi-objective AEL finds tradeoff algorithms that satisfy multiple objectives, which can be particularly useful in real-world decision-making scenarios [57].


## VI. CONCLUSION

This paper introduces a novel approach called Algorithm Evolution with Large Language Model (AEL) for automatic algorithm design. By utilizing a large language model (LLM), AEL automates the generation of optimization algorithms using an evolutionary framework. It significantly reduces the need for expertise knowledge and domain model training.

We have demonstrated the effectiveness of AEL on the constructive method for TSP. Results on TSP instances with problem sizes ranging from 20 to 1000 show that the algorithm generated by AEL outperforms the simple hand-crafted greedy algorithm and the algorithms generated by directly instructing LLMs. It also exhibits excellent scalability across different problem sizes compared to training a domain neural model. Future works on the integration of AEL with more advanced algorithm frameworks could lead to even more powerful algorithms.

## REFERENCES

[1] J. Nocedal and S. J. Wright, Numerical optimization. Springer, 1999.

[2] F. W. Glover and G. A. Kochenberger, Handbook of metaheuristics. Springer Science \& Business Media, 2006, vol. 57.

[3] K. Deb, Optimization for engineering design: Algorithms and examples. PHI Learning Pvt. Ltd., 2012.

[4] T. Chen, X. Chen, W. Chen, Z. Wang, H. Heaton, J. Liu, and W. Yin, "Learning to optimize: A primer and a benchmark," The Journal of Machine Learning Research, vol. 23, no. 1, pp. 8562-8620, 2022.

[5] X. He, K. Zhao, and X. Chu, "Automl: A survey of the state-of-the-art," Knowledge-Based Systems, vol. 212, p. 106622, 2021.

[6] E. K. Burke, M. Gendreau, M. Hyde, G. Kendall, G. Ochoa, E. Özcan, and R. Qu, "Hyper-heuristics: A survey of the state of the art," Journal of the Operational Research Society, vol. 64, pp. 1695-1724, 2013

[7] E. K. Burke, M. R. Hyde, G. Kendall, G. Ochoa, E. Özcan, and J. R. Woodward, "A classification of hyper-heuristic approaches: revisited," Handbook of metaheuristics, pp. 453-477, 2019.

[8] T. Stützle and M. López-Ibáñez, "Automated design of metaheuristic algorithms," Handbook of metaheuristics, pp. 541-579, 2019.

[9] L. Ma, N. Li, Y. Guo, X. Wang, S. Yang, M. Huang, and H. Zhang, "Learning to optimize: reference vector reinforcement learning adaption to constrained many-objective optimization of industrial copper burdening system," IEEE Transactions on Cybernetics, 2021.

[10] Y. Tian, X. Li, H. Ma, X. Zhang, K. C. Tan, and Y. Jin, "Deep reinforcement learning based adaptive operator selection for evolutionary multi-objective optimization," IEEE Transactions on Emerging Topics in Computational Intelligence, 2022.

[11] Z. Zhang, Q. Tang, M. Chica, and Z. Li, "Reinforcement learning-based multiobjective evolutionary algorithm for mixed-model multimanned assembly line balancing under uncertain demand," IEEE Transactions on Cybernetics, 2023.

[12] Y. Wu, W. Song, Z. Cao, J. Zhang, and A. Lim, "Learning improvement heuristics for solving routing problems," IEEE transactions on neural networks and learning systems, vol. 33, no. 9, pp. 5057-5069, 2021.

[13] Y. Shen, Y. Sun, X. Li, A. Eberhard, and A. Ernst, "Adaptive solution prediction for combinatorial optimization," European Journal of Operational Research, vol. 309, no. 3, pp. 1392-1408, 2023.

[14] Y. Sun, S. Wang, Y. Shen, X. Li, A. T. Ernst, and M. Kirley, "Boosting ant colony optimization via solution prediction and machine learning," Computers \& Operations Research, vol. 143, p. 105769, 2022.

[15] K. C. Tan, L. Feng, and M. Jiang, "Evolutionary transfer optimization-a new frontier in evolutionary computation research," IEEE Computational Intelligence Magazine, vol. 16, no. 1, pp. 22-33, 2021.

[16] L. Zhou, L. Feng, K. C. Tan, J. Zhong, Z. Zhu, K. Liu, and C. Chen, "Toward adaptive knowledge transfer in multifactorial evolutionary computation," IEEE transactions on cybernetics, vol. 51, no. 5, pp. 2563-2576, 2020.

[17] K. Li, R. Chen, and X. Yao, "A data-driven evolutionary transfer optimization for expensive problems in dynamic environments," IEEE Transactions on Evolutionary Computation, 2023.

[18] F.-Y. Liu and C. Qian, "Prediction guided meta-learning for multiobjective reinforcement learning," in 2021 IEEE Congress on Evolutionary Computation (CEC). IEEE, 2021, pp. 2171-2178.

[19] Z. Zhang, Z. Wu, H. Zhang, and J. Wang, "Meta-learning-based deep reinforcement learning for multiobjective optimization problems," IEEE Transactions on Neural Networks and Learning Systems, 2022.

[20] Y. Cao, T. Chen, Z. Wang, and Y. Shen, "Learning to optimize in swarms," Advances in neural information processing systems, vol. 32, 2019.

[21] R. Lange, T. Schaul, Y. Chen, T. Zahavy, V. Dalibard, C. Lu, S. Singh, and S. Flennerhag, "Discovering evolution strategies via meta-black-box optimization," in Proceedings of the Companion Conference on Genetic and Evolutionary Computation, 2023, pp. 29-30.

[22] L. Penghui, K. Wu, and J. Liu, "Decn: Evolution inspired deep convolution network for black-box optimization," 2022.

[23] X. Li, K. Wu, X. Zhang, H. Wang, and J. Liu, "Optformer: Beyond transformer for black-box optimization," 2022.

[24] Y. Jiang, Z.-H. Zhan, K. C. Tan, and J. Zhang, "Knowledge learning for evolutionary computation," IEEE Transactions on Evolutionary Computation, 2023.

[25] W. Kool, H. Van Hoof, and M. Welling, "Attention, learn to solve routing problems!" arXiv preprint arXiv:1803.08475, 2018.

[26] X. Lin, Z. Yang, and Q. Zhang, "Pareto set learning for neural multiobjective combinatorial optimization," 2022.
[27] J. Zhou, Y. Wu, W. Song, Z. Cao, and J. Zhang, "Towards omnigeneralizable neural methods for vehicle routing problems," arXiv preprint arXiv:2305.19587, 2023.

[28] F. Luo, X. Lin, F. Liu, Q. Zhang, and Z. Wang, "Neural combinatorial optimization with heavy decoder: Toward large scale generalization," arXiv preprint arXiv:2310.07985, 2023.

[29] Z. Wang, S. Yao, G. Li, and Q. Zhang, "Multiobjective combinatorial optimization using a single deep reinforcement learning model," IEEE Transactions on Cybernetics, 2023

[30] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., "A survey of large language models," arXiv preprint arXiv:2303.18223, 2023.

[31] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva, F. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier et al., "Chatgpt for good? on opportunities and challenges of large language models for education," Learning and individual differences, vol. 103, p. $102274,2023$.

[32] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth, "Recent advances in natural language processing via large pre-trained language models: A survey," ACM Computing Surveys, 2021.

[33] H. Tian, W. Lu, T. O. Li, X. Tang, S.-C. Cheung, J. Klein, and T. F. Bissyandé, "Is chatgpt the ultimate programming assistant-how far is it?" arXiv preprint arXiv:2304.11938, 2023.

[34] P. Lee, S. Bubeck, and J. Petro, "Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine," New England Journal of Medicine, vol. 388, no. 13, pp. 1233-1239, 2023.

[35] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, "Capabilities of gpt-4 on medical challenge problems," arXiv preprint arXiv:2303.13375, 2023.

[36] K. Cheng, Q. Guo, Y. He, Y. Lu, S. Gu, and H. Wu, "Exploring the potential of gpt-4 in biomedical engineering: the dawn of a new era," Annals of Biomedical Engineering, pp. 1-9, 2023.

[37] K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero, and B. Smit, "Is gpt-3 all you need for low-data discovery in chemistry?" 2023.

[38] J. Blocklove, S. Garg, R. Karri, and H. Pearce, "Chip-chat: Challenges and opportunities in conversational hardware design," arXiv preprint arXiv:2305.13243, 2023.

[39] Z. He, H. Wu, X. Zhang, X. Yao, S. Zheng, H. Zheng, and B. Yu, "Chateda: A large language model powered autonomous agent for eda," arXiv preprint arXiv:2308.10204, 2023.

[40] C. Yu, X. Liu, C. Tang, W. Feng, and J. Lv, "Gpt-nas: Neural architecture search with the generative pre-trained model," arXiv preprint arXiv:2305.05351, 2023.

[41] M. Zheng, X. Su, S. You, F. Wang, C. Qian, C. Xu, and S. Albanie, "Can gpt-4 perform neural architecture search?" arXiv preprint arXiv:2304.10970, 2023.

[42] S. Zhang, C. Gong, L. Wu, X. Liu, and M. Zhou, "Automl-gpt: Automatic machine learning with gpt," arXiv preprint arXiv:2305.02499, 2023.

[43] Z. Zhao, W. S. Lee, and D. Hsu, "Large language models as commonsense knowledge for large-scale task planning," arXiv preprint arXiv:2305.14078, 2023.

[44] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen, "Large language models as optimizers," arXiv preprint arXiv:2309.03409, 2023.

[45] E. Meyerson, M. J. Nelson, H. Bradley, A. Moradi, A. K. Hoover, and J. Lehman, "Language model crossover: Variation through few-shot prompting," arXiv preprint arXiv:2302.12170, 2023.

[46] A. Chen, D. M. Dohan, and D. R. So, "Evoprompting: Language models for code-level neural architecture search," arXiv preprint arXiv:2302.14838, 2023.

[47] H. Wang, S. Feng, T. He, Z. Tan, X. Han, and Y. Tsvetkov, "Can language models solve graph problems in natural language?" arXiv preprint arXiv:2305.10037, 2023.

[48] F. Liu, X. Lin, Z. Wang, S. Yao, X. Tong, M. Yuan, and Q. Zhang, "Large language model for multi-objective evolutionary optimization," arXiv preprint arXiv:2310.12541, 2023.

[49] M. Gendreau, J.-Y. Potvin et al., Handbook of metaheuristics. Springer, 2010, vol. 2.

[50] C. Ansótegui, M. Sellmann, and K. Tierney, "A gender-based genetic algorithm for the automatic configuration of algorithms," in International Conference on Principles and Practice of Constraint Programming. Springer, 2009, pp. 142-157.

[51] F. Hutter, H. H. Hoos, K. Leyton-Brown, and T. Stützle, "Paramils: an automatic algorithm configuration framework," Journal of artificial intelligence research, vol. 36, pp. 267-306, 2009.

[52] A. Blot, H. H. Hoos, L. Jourdan, M.-É. Kessaci-Marmion, and H. Trautmann, "Mo-paramils: A multi-objective automatic algorithm configuration framework," in Learning and Intelligent Optimization: 10th International Conference, LION 10, Ischia, Italy, May 29-June 1, 2016, Revised Selected Papers 10. Springer, 2016, pp. 32-47.

[53] M. López-Ibáñez, J. Dubois-Lacoste, L. P. Cáceres, M. Birattari, and T. Stützle, "The irace package: Iterated racing for automatic algorithm configuration," Operations Research Perspectives, vol. 3, pp. 43-58, 2016.

[54] F. Hutter, H. H. Hoos, and K. Leyton-Brown, "Sequential modelbased optimization for general algorithm configuration," in Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5. Springer, 2011, pp. $507-523$.

[55] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, "Optuna: A nextgeneration hyperparameter optimization framework," in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining, 2019, pp. 2623-2631.

[56] W. Meng and R. Qu, "Automated design of search algorithms: Learning on algorithmic components," Expert Systems with Applications, vol. 185, p. $115493,2021$.

[57] L. C. Bezerra, M. López-Ibánez, and T. Stützle, "Automatic componentwise design of multiobjective evolutionary algorithms," IEEE Transactions on Evolutionary Computation, vol. 20, no. 3, pp. 403-417, 2015.

[58] Y. Bengio, A. Lodi, and A. Prouvost, "Machine learning for combinatorial optimization: a methodological tour d'horizon," European Journal of Operational Research, vol. 290, no. 2, pp. 405-421, 2021.

[59] N. Li, L. Ma, G. Yu, B. Xue, M. Zhang, and Y. Jin, "Survey on evolutionary deep learning: Principles, algorithms, applications, and open issues," ACM Computing Surveys, vol. 56, no. 2, pp. 1-34, 2023.

[60] W. Liu, R. Wang, T. Zhang, K. Li, W. Li, and H. Ishibuchi, "Hybridization of evolutionary algorithm and deep reinforcement learning for multiobjective orienteering optimization," IEEE Transactions on Evolutionary Computation, 2022.

[61] W. Dong and M. Zhou, "A supervised learning and control method to improve particle swarm optimization algorithms," IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 47, no. 7, pp. 1135-1148, 2016.

[62] Y. Sun, A. T. Ernst, X. Li, and J. Weiner, "Learning to generate columns with application to vertex coloring," in The Eleventh International Conference on Learning Representations, 2022.

[63] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. De Freitas, "Taking the human out of the loop: A review of bayesian optimization," Proceedings of the IEEE, vol. 104, no. 1, pp. 148-175, 2015.

[64] Q. Zhang, W. Liu, E. Tsang, and B. Virginas, "Expensive multiobjective optimization by MOEA/D with gaussian process model," IEEE Transactions on Evolutionary Computation, vol. 14, no. 3, pp. 456-474, 2009.

[65] Y. Jin, H. Wang, T. Chugh, D. Guo, and K. Miettinen, "Data-driven evolutionary optimization: An overview and case studies," IEEE Transactions on Evolutionary Computation, vol. 23, no. 3, pp. 442-458, 2018.

[66] Z. Song, H. Wang, C. He, and Y. Jin, "A kriging-assisted two-archive evolutionary algorithm for expensive many-objective optimization," IEEE Transactions on Evolutionary Computation, vol. 25, no. 6, pp. 1013-1027, 2021.

[67] H. Hao, A. Zhou, H. Qian, and H. Zhang, "Expensive multiobjective optimization by relation learning and prediction," IEEE Transactions on Evolutionary Computation, vol. 26, no. 5, pp. 1157-1170, 2022.

[68] Y. Mei, Q. Chen, A. Lensen, B. Xue, and M. Zhang, "Explainable artificial intelligence by genetic programming: A survey," IEEE Transactions on Evolutionary Computation, 2022.

[69] Y.-H. Jia, Y. Mei, and M. Zhang, "Learning heuristics with different representations for stochastic routing," IEEE Transactions on Cybernetics, 2022.

[70] P.-F. Guo, Y.-H. Chen, Y.-D. Tsai, and S.-D. Lin, "Towards optimizing with large language models," arXiv preprint arXiv:2310.05204, 2023.

[71] A. E. Brownlee, J. Callan, K. Even-Mendoza, A. Geiger, C. Hanna, J. Petke, F. Sarro, and D. Sobania, "Enhancing genetic improvement mutations using large language models," arXiv preprint arXiv:2310.19813, 2023.

[72] S. Liu, C. Chen, X. Qu, K. Tang, and Y.-S. Ong, "Large language models as evolutionary optimizers," arXiv preprint arXiv:2310.19046, 2023.

[73] M. U. Nasir, S. Earle, J. Togelius, S. James, and C. Cleghorn, "Llmatic: Neural architecture search via large language models and qualitydiversity optimization," arXiv preprint arXiv:2306.01102, 2023.

[74] G. Jawahar, M. Abdul-Mageed, L. V. Lakshmanan, and D. Ding, "Llm performance predictors are good initializers for architecture search," arXiv preprint arXiv:2310.16712, 2023.
[75] J. Lehman, J. Gordon, S. Jain, K. Ndousse, C. Yeh, and K. O. Stanley, "Evolution through large models," in Handbook of Evolutionary Machine Learning. Springer, 2023, pp. 331-366.

[76] G. Reinelt, The traveling salesman: computational solutions for TSP applications. Springer, 2003, vol. 840 .

[77] Y.-D. Kwon, J. Choo, B. Kim, I. Yoon, Y. Gwon, and S. Min, "Pomo: Policy optimization with multiple optima for reinforcement learning," Advances in Neural Information Processing Systems, vol. 33, pp. 21 188-21 198, 2020.

[78] X. Pan, Y. Jin, Y. Ding, M. Feng, L. Zhao, L. Song, and J. Bian, "Htsp: Hierarchically solving the large-scale travelling salesman problem," arXiv preprint arXiv:2304.09395, 2023.

[79] H. Cheng, H. Zheng, Y. Cong, W. Jiang, and S. Pu, "Select and optimize: Learning to aolve large-scale tsp instances," in International Conference on Artificial Intelligence and Statistics. PMLR, 2023, pp. 1219-1231.

[80] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., "Sparks of artificial general intelligence: Early experiments with gpt-4," arXiv preprint arXiv:2303.12712, 2023.

[81] W. J. Cook, D. L. Applegate, R. E. Bixby, and V. Chvatal, The traveling salesman problem: a computational study. Princeton university press, 2011.

[82] G. Mialon, R. Dessì, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozière, T. Schick, J. Dwivedi-Yu, A. Celikyilmaz et al., "Augmented language models: a survey," arXiv preprint arXiv:2302.07842, 2023.

[83] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, "Toolformer: Language models can teach themselves to use tools," arXiv preprint arXiv:2302.04761, 2023 .

[84] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, and M. T. Ribeiro, "Art: Automatic multi-step reasoning and tool-use for large language models," arXiv preprint arXiv:2303.09014, 2023.

[85] B. Hudson, Q. Li, M. Malencia, and A. Prorok, "Graph neural network guided local search for the traveling salesperson problem," arXiv preprint arXiv:2110.05291, 2021.

[86] A. Hottung and K. Tierney, "Neural large neighborhood search for the capacitated vehicle routing problem," arXiv preprint arXiv:1911.09539, 2019 .

[87] N. T. Nguyen and K. Lee, "Deep learning-aided tabu search detection for large mimo systems," IEEE Transactions on Wireless Communications, vol. 19, no. 6, pp. 4262-4275, 2020.

[88] S. Jiang, J. Zou, S. Yang, and X. Yao, "Evolutionary dynamic multiobjective optimisation: A survey," ACM Computing Surveys, vol. 55, no. 4, pp. 1-47, 2022.

[89] H. Ishibuchi, L. M. Pang, and K. Shang, "A new framework of evolutionary multi-objective algorithms with an unbounded external archive," Authorea Preprints, 2023

[90] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark, and A. Sabharwal, "Decomposed prompting: A modular approach for solving complex tasks," arXiv preprint arXiv:2210.02406, 2022.

[91] X. Chen, M. Lin, N. Schärli, and D. Zhou, "Teaching large language models to self-debug," arXiv preprint arXiv:2304.05128, 2023.

[92] O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith, and M. Lewis, "Measuring and narrowing the compositionality gap in language models," arXiv preprint arXiv:2210.03350, 2022.

[93] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, "Self-consistency improves chain of thought reasoning in language models," arXiv preprint arXiv:2203.11171, 2022.

[94] A. Zhou, B.-Y. Qu, H. Li, S.-Z. Zhao, P. N. Suganthan, and Q. Zhang, "Multiobjective evolutionary algorithms: A survey of the state of the art," Swarm and evolutionary computation, vol. 1, no. 1, pp. 32-49, 2011.

