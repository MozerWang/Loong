# Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions 

Khai Nguyen Nhat Ho<br>The University of Texas at Austin

May 2, 2024


#### Abstract

Sliced Wasserstein (SW) and Generalized Sliced Wasserstein (GSW) have been widely used in applications due to their computational and statistical scalability. However, the SW and the GSW are only defined between distributions supported on a homogeneous domain. This limitation prevents their usage in applications with heterogeneous joint distributions with marginal distributions supported on multiple different domains. Using SW and GSW directly on the joint domains cannot make a meaningful comparison since their homogeneous slicing operator i.e., Radon Transform (RT) and Generalized Radon Transform (GRT) are not expressive enough to capture the structure of the joint supports set. To address the issue, we propose two new slicing operators i.e., Partial Generalized Radon Transform (PGRT) and Hierarchical Hybrid Radon Transform (HHRT). In greater detail, PGRT is the generalization of Partial Radon Transform (PRT), which transforms a subset of function arguments non-linearly while HHRT is the composition of PRT and multiple domain-specific PGRT on marginal domain arguments. By using HHRT, we extend the SW into Hierarchical Hybrid Sliced Wasserstein (H2SW) distance which is designed specifically for comparing heterogeneous joint distributions. We then discuss the topological, statistical, and computational properties of H2SW. Finally, we demonstrate the favorable performance of $\mathrm{H} 2 \mathrm{SW}$ in 3D mesh deformation, deep 3D mesh autoencoders, and datasets comparison.


## 1 Introduction

Optimal transport [56,48] is a powerful mathematical tool for machine learning, statistics, and data sciences. As an example, Wasserstein distance [48], defined as the optimal transportation cost between two distributions, has been used successfully in many areas of machine learning and statistics, such as generative modeling on images [2, 54], representation learning [36], vocabulary learning [59], and so on. Despite being accepted as an effective distance, Wasserstein distance has been widely known as a computationally expensive distance. In particular, when comparing two distributions that have at most $n$ supports, the time complexity and the memory complexity of the Wasserstein distance scale with the order of $\mathcal{O}\left(n^{3} \log n\right)[46]$ and $\mathcal{O}\left(n^{2}\right)$ respectively. In addition, the Wasserstein distance requires more samples to approximate a continuous distribution with its empirical distribution in high dimension since its sample complexity is of the order of $\mathcal{O}\left(n^{-1 / d}\right)$ [20], where $n$ is the sample size and $d$ is the number of dimensions. Therefore, Wasserstein distance is not statistically and computationally scalable, especially in high dimensions.

Along with entropic regularization [17] which can reduce the time complexity and memory complexity of computing optimal transport to $\mathcal{O}\left(n^{2}\right)$ and $\mathcal{O}\left(n^{2}\right)$ in turn, sliced Wasserstein (SW) distance [11] is one alternative approach for the original Wasserstein distance. The key benefit of the SW distance is that it scales the order $\mathcal{O}(n \log n)$ and $\mathcal{O}(n)$ in terms of time and memory respectively. The reason
behind that fast computation is the closed-form solution of optimal transport in one dimension. To leverage that closed-form, sliced Wasserstein utilizes Radon Transform [24] (RT) to transform a high-dimensional distribution to its one-dimensional projected distributions, then the final distance is calculated as the average of all one-dimensional Wasserstein distance. By doing that, the SW distance has a very fast sample complexity i.e., $\mathcal{O}\left(n^{-1 / 2}\right)$, which makes it computationally and statistically scalable in any dimension. Therefore, the SW distance has been applied successfully in various domains of applications including generative models [19], domain adaptation [33], clustering [28], 3D shapes [31, 30], gradient flows [35, 8], Bayesian inference computation [38, 60], texture synthesis [23], and many other tasks.

Despite being useful, the SW distance is not as flexible as the Wasserstein distance in terms of choosing the ground metric. In greater detail, the number of ground metrics in one dimension is limited, especially ground metrics that lead to the closed-form solution. As a result, the role of capturing the structure of interested distributions belongs to the slicing/projecting operators. To generalize RT to non-linear projection, generalized Radon Transform (GRT) is introduced in [3] with circular projection [29], polynomial projection [52], and so on. With GRT, Generalized Sliced Wasserstein (GSW) distance is proposed in [27]. In addition, a line of works on developing sliced Wasserstein variants on different manifolds such as hyper-sphere [6, 55, 50, 51], hyperbolic manifolds [7], the manifold of symmetric positive definite matrices [10], general manifolds and graphs [53]. In those works, special variants of GRT are proposed.

Although the SW has become more effective on multiple domains, no SW variant is designed specifically for heterogeneous joint distributions i.e., joint distributions that have marginals supported on different domains, except for the product of Hadamard manifolds [9]. It is worth noting that marginal domains of heterogeneous joint distributions can be any metric space and are not necessary manifolds. Heterogeneous joint distributions appear in many applications, e.g., domain adaptation domains $[15,4]$, comparing datasets with labels [1], 3D shape deformation [30], and so on. In this case, Wasserstein distance can be adapted by using a mixed ground metric, i.e., a weighted sum of metrics on domains $[15,1]$. In contrast to the Wasserstein distance, the adaptation of SW has not been well-investigated. Using GSW directly with one type defining function for all marginals cannot separate the information within and among groups of arguments.

Contribution: In this work, we tackle the challenge of designing a sliced Wasserstein variant for heterogeneous joint distributions. In summary, our main contributions are three-fold:

1. We first extend the partial Radon Transform to the partial generalized Radon Transform (PGRT) to inject non-linearity into local transformation. We discuss the injectivity of PGRT for some choices of defining functions. We then propose a novel slicing operator for heterogeneous joint distributions, named Hierarchical Hybrid Radon Transform (HHRT). In particular, HHRT is a hierarchical transformation that first applies partial generalized Radon Transform with different defining functions on arguments of each marginal to gather marginal information, then applies partial Radon Transform on the joint transformed arguments to gather information among marginals. We show that HHRT is injective as long as the partial generalized Radon Transform is injective.
2. We propose Hierarchical Hybrid Sliced Wasserstein (H2SW) which is a novel metric for comparing heterogeneous joint distributions by utilizing the HHRT. Moreover, we investigate the topological properties, statistical properties, and computational properties of H2SW. In particular, we show that H2SW is a valid metric on the space of distribution over the joint space, H2SW does not suffer
from the curse of dimensionality and enjoys the same computational scalability as SW distance.
3. A 3D mesh can be effectively represented by a point-cloud and corresponding surface normal vectors. Therefore, it can be seen as an empirical heterogeneous joint distribution. We conduct experiments on optimization-based 3D mesh deformation and deep 3D mesh autoencoder to show the favorable performance of H2SW compared to SW and GSW. Moreover, we also illustrate that H2SW can also provide a meaningful comparison for probability distributions on the product of Hadamand manifolds by conducting experiments on dataset comparison.

Organization. We first provide some preliminaries on SW distance, GSW distance, and joint Wasserstein distance in Section 2. We then define the hierarchical hybrid Radon transform and hierarchical hybrid sliced Wasserstein distance s in Section 3. Section 4 contains experiments on 3D mesh deformation, deep 3D mesh autoencoder, and datasets comparison. We conclude the paper in Section 5. Finally, we defer the proofs of key results, and additional materials in the Appendices.

## 2 Preliminaries

Wasserstein distance. For $p \geq 1$, the Wasserstein- $p$ distance [57, 48] between two distributions $\mu \in \mathcal{P}(\mathcal{X})$ and $\nu \in \mathcal{P}(\mathcal{Y})$, where $\mathcal{X}$ and $\mathcal{Y}$ are subsets of $\mathbb{R}^{d}$ and they share a ground metric $c: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^{+}$, is defined as:

$$
\begin{equation*}
\mathrm{W}_{p}^{p}(\mu, \nu ; c):=\inf _{\pi \in \Pi(\mu, \nu)} \int_{\mathcal{X} \times \mathcal{Y}} c(x, y)^{p} d \pi(x, y) \tag{1}
\end{equation*}
$$

where $\left.\Pi(\mu, \nu):=\{\pi \in \mathcal{P}(\mathcal{X} \times \mathcal{Y})\} \mid \int_{\mathcal{Y}} d \pi(x, y)=\mu(x), \int_{\mathcal{X}} d \pi(x, y)=\nu(y)\right\}$. When $\mu$ and $\nu$ are discrete with at most $n$ supports, the time complexity and the space complexity of the Wasserstein distance is $\mathcal{O}\left(n^{3} \log n\right)$ and $\mathcal{O}\left(n^{2}\right)$ in turn which are very expensive. Therefore, sliced Wasserstein is proposed as an alternative solution. We first review the

Radon Transform [24] The Radon Transform $\mathcal{R}: \mathbb{L}_{1}\left(\mathbb{R}^{d}\right) \rightarrow \mathbb{L}_{1}\left(\mathbb{R} \times \mathbb{S}^{d-1}\right)$ is defined as:

$$
\begin{equation*}
(\mathcal{R} f)(t, \theta)=\int_{\mathbb{R}^{d}} f(x) \delta(t-\langle x, \theta\rangle) d x \tag{2}
\end{equation*}
$$

Radon Transform defines a linear bijection [24]. Given a projecting direction $\theta,(\mathcal{R} f)(\cdot, \theta)$ is an one-dimensional function. With Radon Transform, we can now define the sliced Wasserstein distance.

Sliced Wasserstein distance. For $p \geq 1$, the Sliced Wasserstein (SW) distance [11] of $p$-th order between two distributions $\mu \in \mathcal{P}(\mathcal{X})$ and $\nu \in \mathcal{P}(\mathcal{Y})$ with an one-dimensional ground metric $c: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}^{+}$is defined as follow:

$$
\begin{equation*}
\operatorname{SW}_{p}^{p}(\mu, \nu ; c)=\mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[\mathrm{W}_{p}^{p}\left(\mathcal{R}_{\theta} \sharp \mu, \mathcal{R}_{\theta} \sharp \nu ; c\right)\right], \tag{3}
\end{equation*}
$$

where $\mathcal{R}_{\theta} \sharp \mu$ and $\mathcal{R}_{\theta} \sharp \nu$ are the one-dimensional push-forward distributions created by applying Radon Transform (RT) [24] on the pdf of $\mu$ and $\nu$ with the projecting direction $\theta$. The computational benefit of SW distance comes from the closed-form solution when the one-dimensional ground metric $c(x, y)=$ $h(x-y)$ for $h$ is a strictly convex function: $\mathrm{W}_{p}^{p}\left(\mathcal{R}_{\theta} \sharp \mu, \mathcal{R}_{\theta} \sharp \nu ; c\right)=\int_{0}^{1} c\left(F_{\mathcal{R}_{\theta} \sharp \mu}^{-1}(z), F_{\mathcal{R}_{\theta} \sharp \nu}^{-1}(z)\right)^{p} d z$, where $F_{\mathcal{R}_{\theta} \sharp \mu}^{-1}$ and $F_{\mathcal{R}_{\theta} \sharp \nu}^{-1}$ are inverse CDF of $\mathcal{R}_{\theta} \sharp \mu$ and $\mathcal{R}_{\theta} \sharp \nu$ respectively. When $\mu$ and $\nu$ are discrete
with at most $n$ supports, the time complexity and the space complexity of the closed-form is $\mathcal{O}(n \log n)$ and $\mathcal{O}(n)$ respectively.

Generalized Radon Transform and Generalized Sliced Wasserstein distance. To generalize RT to non-linear operator, the Generalized Radon Transform (GRT) was proposed [3]. Given a defining function [27] $g: \mathbb{R}^{d} \times \Omega \rightarrow \mathbb{R}$, the Generalized Radon Transform [3] $\mathcal{G R}: \mathbb{L}_{1}\left(\mathbb{R}^{d}\right) \rightarrow$ $\mathbb{L}_{1}(\mathbb{R} \times \Omega)$ is defined as: $(\mathcal{G} \mathcal{R} f)(t, \theta)=\int_{\mathbb{R}^{d}} f(x) \delta(t-g(x, \theta)) d x$. For example, we can have the circular function [29], i.e., $g(x, \theta)=\|x-r \theta\|_{2}$ for $r \in \mathbb{R}^{+}$and $\theta \in \Omega:=\mathbb{S}^{d-1}$, homogeneous polynomials with an odd degree [52] $(m)$, i.e., $g(x, \theta)=\sum_{|\alpha|=m} \theta_{\alpha} x^{\alpha}$ with $\alpha=\left(\alpha_{1}, \ldots, \alpha_{d_{\alpha}}\right) \in \mathbb{N}^{d_{\alpha}}$, $|\alpha|=\sum_{i=1}^{d_{\alpha}} \alpha_{i}, x^{\alpha}=\prod_{i=1}^{d_{\alpha}} x_{i}^{\alpha_{i}}, \Omega=\mathbb{S}^{d_{\alpha}}$, and so on. Using GRT, the Generalized Sliced Wasserstein (GSW) distance is introduced in [27], which is formally defined as :

$$
\begin{equation*}
\operatorname{GSW}_{p}^{p}(\mu, \nu ; c, g)=\mathbb{E}_{\theta \sim \mathcal{U}\left(\mathbb{S}^{d-1}\right)}\left[\mathrm{W}_{p}^{p}\left(\mathcal{G R}{ }_{\theta}^{g} \sharp \mu, \mathcal{G} \mathcal{R}_{\theta}^{g} \sharp \nu ; c\right)\right] . \tag{4}
\end{equation*}
$$

It is worth noting that the injectivity of GRT is required to have the identity of indiscernible GSW.

Heterogeneous joint distributions comparison. We are given two joint distributions $\mu\left(x_{1}, x_{2}\right) \in$ $\mathcal{P}\left(\mathcal{X}_{1} \times \mathcal{X}_{2}\right)$ and $\nu\left(y_{1}, y_{2}\right) \in \mathcal{P}\left(\mathcal{Y}_{1} \times \mathcal{Y}_{2}\right)$ where $X_{1}$ are $Y_{1}$ share a ground metric $c_{1}: \mathcal{X}_{1} \times \mathcal{Y}_{1} \rightarrow \mathbb{R}^{+}$ and $X_{2}$ are $Y_{2}$ share a ground metric $c_{2}: \mathcal{X}_{2} \times \mathcal{Y}_{2} \rightarrow \mathbb{R}^{+}$with $\left(c_{1} \neq c_{2}\right)$. In this case, previous works utilize the joint distribution Wasserstein distance $[15,1]$ to compare $\mu$ and $\nu$ :

$$
\begin{equation*}
\mathrm{W}_{p}^{p}\left(\mu, \nu ; c_{1}, c_{2}\right):=\inf _{\pi \in \Pi(\mu, \nu)} \int_{\mathcal{X}_{1} \times \mathcal{X}_{2} \times \mathcal{Y}_{1} \times \mathcal{Y}_{2}}\left(c_{1}\left(x_{1}, y_{1}\right)^{p}+c_{2}\left(x_{2}, y_{2}\right)^{p}\right) d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right) \tag{5}
\end{equation*}
$$

where $\Pi(\mu, \nu):=\left\{\pi \in \mathcal{P}\left(\mathcal{X}_{1} \times \mathcal{X}_{2} \times \mathcal{Y}_{1} \times \mathcal{Y}_{2}\right)\right\} \mid \int_{\mathcal{Y}_{1} \times \mathcal{Y}_{2}} d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)=\mu\left(x_{1}, x_{2}\right)$,

$\left.\int_{\mathcal{X}_{1} \times \mathcal{X}_{2}} d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)=\nu\left(y_{1}, y_{2}\right)\right\}$. We can easily extend the definition to joint distributions with more than two marginals (see Appendix B). In contrast to the Wasserstein distance, there is no variant of $\mathrm{SW}$ that is designed specifically for this case. SW variants can still be used by treating $\mathcal{X}_{1} \times \mathcal{X}_{2}$ and $\mathcal{Y}_{1} \times \mathcal{Y}_{2}$ as homogeneous spaces $\mathcal{X}$ and $\mathcal{Y}$ which share the same Radon Transform variant and one-dimensional ground metric $c$. However, that approach cannot differentiate the difference between $\mathcal{X}_{1}$ and $\mathcal{X}_{2}$, and leverage the hierarchical structure, i.e., inside and among marginals.

## 3 Hierarchical Hybrid Sliced Wasserstein Distance

In this section, we propose the Hierarchical Hybrid Radon Transform (HHRT) which first applies $\mathrm{P}(\mathrm{G}) \mathrm{RT}$ on each marginal argument to gather each marginal information, then applies PRT on the joint transformed arguments from all marginals to gather information among marginals. After that, we introduce Hierarchical Hybrid Sliced Wasserstein distance by using HHRT as the slicing operator.

### 3.1 Hierarchical Hybrid Radon Transform

We first introduce the first building block in HHRT, i.e., Partial Generalized Radon Transform (PGRT).

Definition 1 (Partial Generalized Radon Transform). Given a defining function $g: \mathbb{R}^{d_{1}} \times \Omega \rightarrow \mathbb{R}$, Partial Generalized Radon Transform $\mathcal{P G R}: \mathbb{L}_{1}\left(\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}\right) \rightarrow \mathbb{L}_{1}\left(\mathbb{R} \times \Omega \times \mathbb{R}^{d_{2}}\right)$ is defined as:

$$
\begin{equation*}
(\mathcal{P} \mathcal{G} \mathcal{R} f)(t, \theta, y)=\int_{\mathbb{R}^{d_{1}}} f(x, y) \delta(t-g(x, \theta)) d x \tag{6}
\end{equation*}
$$

When $g(x, \theta)=\langle x, \theta\rangle$, PGRT reverts into Partial Radon Transform (PRT) [34].

Proposition 1. For some defining function g such as linear, circular, and homogeneous polynomials with an odd degree; the Partial Generalized Radon Transform is injective, i.e., for any functions $f_{1}, f_{2} \in \mathbb{L}^{1}\left(\mathbb{R}^{d}\right),\left(\mathcal{P G} \mathcal{R} f_{1}\right)(t, \theta, y)=\left(\mathcal{P G} \mathcal{R} f_{2}\right)(t, \theta, y) \forall t, \theta, y$ implies $f_{1}=f_{2}$.

The proof of Proposition 1 is given in Appendix A.1. The main idea to prove the injectivity of PGRT is to show that given a fixed $y$, the PGRT is the GRT of $f(\cdot, y)$.

Definition 2 (Hierarchical Hybrid Radon Transform). Given defining functions $g_{1}: \mathbb{R}^{d_{1}} \times \Omega_{1} \rightarrow$ $\mathbb{R}$ and $g_{2}: \mathbb{R}^{d_{2}} \times \Omega_{2} \rightarrow \mathbb{R}$, Hierarchical Hybrid Radon Transform $\mathcal{H} \mathcal{H}: \mathbb{L}_{1}\left(\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}\right) \rightarrow$ $\mathbb{L}_{1}\left(\mathbb{R} \times \Omega_{1} \times \Omega_{2} \times \mathbb{S}\right)$ is defined as:

$$
\begin{equation*}
(\mathcal{H H R} f)\left(t, \theta_{1}, \theta_{2}, \psi\right)=\int_{\mathbb{R}^{d_{1} \times \mathbb{R}^{d_{2}}}} f\left(x_{1}, x_{2}\right) \delta\left(t-\psi_{1} g_{1}\left(x_{1}, \theta_{1}\right)-\psi_{2} g_{2}\left(x_{2}, \theta_{2}\right)\right) d x_{1} d x_{2} \tag{7}
\end{equation*}
$$

where $\psi=\left(\psi_{1}, \psi_{2}\right) \in \mathbb{S}$.

The reason for using PRT for the final transform is that the previous PGRTs are assumed to be able to transform the non-linear structure to a linear line. However, PGRT can still be used as a replacement for PRT. Definition 2 can be extended to more than two marginals (see Appendix B).

Proposition 2. For some defining functions $g_{1}, g_{2}$ such as linear, circular, and homogeneous polynomials with an odd degree; Hierarchical Hybrid Radon Transform is injective, i.e., for any functions $f_{1}, f_{2} \in \mathbb{L}_{1}\left(\mathbb{R}^{d}\right),\left(\mathcal{H} \mathcal{H} f_{1}\right)\left(t, \theta_{1}, \theta_{2}, \psi\right)=\left(\mathcal{H H R} f_{2}\right)\left(t, \theta_{1}, \theta_{2}, \psi\right) \forall t, \theta_{1}, \theta_{2}, \psi$ implies $f_{1}=f_{2}$.

The proof of Proposition 2 is given in Appendix A.2. The main idea to prove the injectivity of HHRT is to show that HHRT is the composition of PRT and multiple PGRTs.

HHRT of discrete distributions. We are given $f(x)=\sum_{i=1}^{n} \alpha_{i} \delta\left(\left(x_{1}, x_{2}\right)-\left(x_{1 i}, x_{2 i}\right)\right)(n \geq 1$, $\left.\alpha_{i} \geq 0 \forall i\right)$. The HHRT of $f(x)$ is $(\mathcal{H H} \mathcal{R} f)\left(t, \theta_{1}, \theta_{2}, \psi\right)=\sum_{i=1}^{n} \alpha_{i} \delta\left(t-\psi_{1} g_{1}\left(x_{i 1}, \theta_{1}\right)-\psi_{2} g_{2}\left(x_{i 2}, \theta_{2}\right)\right)$. For $g_{1}$ and $g_{2}$ that are the linear function and (or) the circular function, the time complexity of the transform is $\mathcal{O}\left(d_{1}+d_{2}\right)$ which is the same as the complexity of using RT and GRT directly. However, HHRT has an additional constant complexity scaling linearly with the number of marginals, i.e., two marginals in Definition 2.

Example 1. In this paper, we focus on $3 D$ shape data (mesh) with points and normals representation, i.e., shapes as points representation [47]. In particular, we can transform a 3D shape into a set of points and normals by sampling from the surface of the mesh. In addition, we can convert back to the 3D shape from points and normals with Poisson surface reconstruction [26] algorithm. In this setup, a shape is represented by a 6 -dimensional vector $x=\left(x_{1}, x_{2}\right)$ where $x_{1} \in \mathcal{X}_{1} \in \mathbb{R}^{3}$ and $x_{2} \in \mathcal{X}_{2} \in \mathbb{S}^{2}$. For the set $\mathcal{X}_{1} \in \mathbb{R}^{3}$, we can use directly the linear defining function $g_{1}\left(x_{1}, \theta_{1}\right)=\left\langle x_{1}, \theta_{1}\right\rangle$ with $\theta_{1} \in \mathbb{S}^{2}$. For the set $\mathcal{X}_{2} \in \mathbb{S}^{2}$, we can utilize the circular defining function $g_{2}\left(x_{2}, \theta_{2}\right)=\left\|x_{2}-r \theta_{2}\right\|_{2}$ with $r \in \mathbb{R}^{+}$and $\theta_{2} \in \mathbb{S}^{2}$. As alternative options for $\mathcal{X}_{2}$, we can use other defining functions from special cases of GRT including Vertical Slice Transform [50], Parallel Slice Transform [51], Spherical Radon Transform [6], and Stereographic Spherical Radon Transform [55].

Inversion. In Proposition 2, we show that HHRT is the composition of PRT and multiple PGRTs. Therefore, the inversion of HHRT is the composition of the inversion of multiple PGRT (invertibility of PGRT depends on the choice of defining functions [3, 29]) and the inversion of PRT [24].

### 3.2 Hierarchical Hybrid Sliced Wasserstein Distance

By using HHRT, we obtain a novel variant of SW which is specifically designed for comparing heterogeneous joint distributions.

Definitions. We now define the Hierarchical Hybrid Sliced Wasserstein (H2SW) distance.

Definition 3. For $p \geq 1$, defining functions $g_{1}, g_{2}$, the hierarchical hybrid sliced Wasserstein- $p$ (H2SW) distance between two distributions $\mu \in \mathcal{P}\left(\mathcal{X}_{1} \times \mathcal{X}_{2}\right)$ and $\nu \in \mathcal{P}\left(\mathcal{Y}_{1} \times \mathcal{Y}_{2}\right)$ with an onedimensional ground metric $c: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}^{+}$is defined as:

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-06.jpg?height=74&width=1391&top_left_y=668&top_left_x=367)

where $\mathcal{H H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi} \sharp \mu$ and $\mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi} \sharp \nu$ are the one-dimensional push-forward distributions created by applying HHRT.

Definition 3 can be easily extended to more than two marginals (see Appendix B)

Topological Properties. We first show that H2SW is a valid metric on the space of distributions on any sets $\mathcal{X} \times \mathcal{Y} \in \mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}\left(d_{1}, d_{2} \geq 1\right)$.

Theorem 1. For any $p \geq 1$, ground metric $c$, and defining functions $g_{1}, g_{2}$ which lead to the injectivity of GRT, the hierarchical hybrid sliced Wasserstein $H 2 S W_{p}\left(\cdot, \cdot ; c, g_{1}, g_{2}\right)$ is a metric on $\mathcal{P}\left(\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}\right)$ i.e., it satisfies the symmetry, non-negativity, triangle inequality, and identity of indiscernible.

The proof of Theorem 1 is given in Appendix A.3. It is worth noting that the identity of indiscernible property is proved by the injectivity of HHRT (Proposition 2). We now discuss the connection of H2SW to GSW and Wasserstein distance in some specific cases.

Proposition 3. For any $p \geq 1, c(x, y)=|x-y|$, and $\mu, \nu \in \mathcal{P}\left(\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}\right)$, we have: (i) $H 2 S W_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right) \leq G S W_{p}\left(\mu_{1}, \nu_{1} ; c, g_{1}\right)+G S W_{p}\left(\mu_{2}, \nu_{2} ; c, g_{2}\right)$, where $\mu_{1}(X)=\mu\left(X \times \mathbb{R}^{d_{2}}\right)$ and $\mu_{2}(Y)=\mu\left(\mathbb{R}^{d_{1}} \times Y\right)$ (similar with $\nu_{1}$ and $\nu_{2}$ ).

(ii) If $g_{1}, g_{2}$ are linear defining functions, $H 2 S W_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right) \leq W_{p}\left(\mu_{1}, \nu_{1} ; c\right)+W_{p}\left(\mu_{2}, \nu_{2} ; c\right)$.

(iii) If $p=1, g_{1}, g_{2}$ are linear defining functions, $H 2 S W_{1}\left(\mu, \nu ; c, g_{1}, g_{2}\right) \leq W_{1}(\mu, \nu ; c, c)$.

The proof of Proposition 3 is given in Appendix A.4.

Sample Complexity. We now discuss the sample complexity of H2SW.

Proposition 4. For any $p \geq 1$, dimension $d_{1}, d_{2} \geq 1, q>p, c(x, y)=|x-y|, g_{1}, g_{2}$ are linear defining functions or circular defining functions, and $\mu, \nu \in \mathcal{P}_{q}\left(\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}\right)$ with the corresponding distributions $\mu_{n}$ and $\nu_{n}(n \geq 1)$, there exists a constant $C_{p, q}$ depending on $p, q$ such that:

$$
\begin{align*}
& \mathbb{E}\left|H 2 S W_{p}\left(\mu_{n}, \nu_{n} ; c, g_{1}, g_{2}\right)-H 2 S W_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right)\right| \\
& \quad \leq C_{p, q}^{\frac{1}{p}}\left(\sum_{i=0}^{q} q^{i} C_{g_{1}, g_{2}}^{q-i}\left(M_{i}(\mu)+M_{i}(\nu)\right)\right)^{\frac{1}{p}}\left\{\begin{array}{l}
n^{-1 / 2 p} \text { if } q>2 p \\
n^{-1 / 2 p} \log (n)^{\frac{1}{p}} \text { if } q=2 p \\
n^{-(q-p) / p q} \text { if } q \in(p, 2 p)
\end{array}\right. \tag{9}
\end{align*}
$$

where $M_{q}(\mu)$ and $M_{q}(\nu)$ are the $q$-th moments of $\mu$ and $\nu, C_{g_{1}, g_{2}}$ is a constant depends on $g_{1}, g_{2}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-07.jpg?height=618&width=1482&top_left_y=239&top_left_x=316)

Figure 1: Generalized Radon Transform and Hierarchical Hybrid Radon Transform on a discrete distribution.

The proof of Proposition 4 is given in Appendix A.5. The rate in Proposition 4 is as good as the rate of SW in [39], however, it is slightly worse than the rate of SW in [45,37,5] due to the usage of the circular defining functions and simpler assumptions. To the best of our knowledge, the sample complexity of GSW has not been investigated.

Monte Carlo Estimation. Since the expectation in H2SW (Equation 8) is intractable, Monte Carlo estimation and Quasi-Monte Carlo approximation [40] can be used to form a practical evaluation of H2SW. Here, we utilize Monte Carlo estimation for simplicity. In particular, we sample $\theta_{11}, \ldots, \theta_{1 L} \stackrel{\text { i.i.d }}{\sim} \mathcal{U}\left(\Omega_{1}\right), \theta_{21}, \ldots, \theta_{2 L} \stackrel{i . i . d}{\sim} \mathcal{U}\left(\Omega_{2}\right)$, and $\psi_{1}, \ldots, \psi_{L} \stackrel{\text { i.i.d }}{\sim} \mathcal{U}(\mathbb{S})$. After that, we form the following estimation of H2SW:

$$
\begin{equation*}
\widehat{\mathrm{H} 2 \mathrm{SW}}_{p}^{p}\left(\mu, \nu ; c, g_{1}, g_{2}, L\right)=\frac{1}{L} \sum_{l=1}^{L} \mathrm{~W}_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1 l}, \theta_{2 l}, \psi_{l}}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1 l}, \theta_{2 l}, \psi_{l}}^{g_{1}, g_{2}} \sharp \nu ; c\right) . \tag{10}
\end{equation*}
$$

Proposition 5. For any $p \geq 1$, dimension $d_{1}, d_{2} \geq 1$, and $\mu, \nu \in \mathcal{P}\left(\mathbb{R}_{1}^{d} \times \mathbb{R}^{d_{2}}\right)$, we have:

$$
\begin{align*}
& \mathbb{E}\left|\widehat{H 2 S W}_{p}^{p}\left(\mu, \nu ; c, g_{1}, g_{2}, L\right)-H 2 S W_{p}^{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right)\right| \\
& \quad \leq \frac{1}{\sqrt{L}} \operatorname{Var}\left[W_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{\theta}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu ; c\right)\right]^{\frac{1}{2}}, \tag{11}
\end{align*}
$$

where the variance is with respect to $\mathcal{U}\left(\Omega_{1} \times \Omega_{2} \times \mathbb{S}\right)$.

The proof of Proposition 5 is given in Appendix A.6. From the proposition, we see that the estimation error of $\mathrm{H} 2 \mathrm{SW}$ is the same as $\mathrm{SW}$ which is $\mathcal{O}\left(L^{-1 / 2}\right)$.

Computational Complexities. The time complexity and memory complexity of H2SW with linear and circular defining functions are $\mathcal{O}\left(L n \log n+L\left(d_{1}+d_{2}+k\right) n\right)$ and $\mathcal{O}\left(L n+\left(d_{1}+d_{2}+k\right) n\right)$ with $k$ is the number of marginals i.e., 2. We can see that the complexities of H2SW are the same as those of SW in terms of the number of supports $n$ and the number of dimensions $d$. We demonstrate the process of HHRT compared to GRT on a discrete distribution with $L$ realization of $\theta_{1}, \theta_{2}, \psi$ in Figure 1. Overall, the complexities of defining functions are often different in the number of dimensions, hence, $\mathrm{H} 2 \mathrm{SW}$ is always scaled the same as $\mathrm{SW}$ in the number of supports i.e., $\mathcal{O}(n \log n)$.

Gradient Estimation. In applications, it is desirable to estimate the gradient $\nabla_{\phi} \mathrm{H} 2 \mathrm{SW}_{p}^{p}\left(\mu_{\phi}, \nu ; c, g_{1}, g_{2}\right)$. We can move the gradient operator to inside the expectation and then apply Monte Carlo estimation. The gradient $\left.\nabla_{\phi} \mathrm{W}_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{\phi}, \mathcal{H} \mathcal{H} \mathcal{\theta}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu ; c\right)\right]$ can be computed easily since the functions $g_{1}, g_{2}$ are usually differentiable.

Beyond uniform slicing distribution. H2SW is defined with the uniform slicing distribution in Definition 3, however, it is possible to extend it to other slicing distributions such as the maximal projecting direction [18], distributional slicing distribution [43], and energy-based slicing distribution [42]. Since the choice of slicing distribution is independent of the main contribution i.e., the slicing operator, we leave this investigation to future work.

H2SW for distributions on the product of Hadamard manifolds. A recent work [9] extends sliced Wasserstein on hyperbolic manifolds [7] and on the manifold of symmetric positive definite matrices [10] to Hadamard manifolds i.e., manifold non-positive curvature. The work discusses the extension of SW to the product of Hadamard manifolds. For the geodesic projection, the closed-form for the projection is intractable. For the Buseman projection, the Buseman projection on the product manifolds is the weighted sum of the Buseman projection with the weights belonging to the unit-sphere. In the work, the weights are a fixed hyperparameter i.e., Cartan-Hadamard Sliced-Wasserstein (CHSW) utilizes only one Buseman function to project the joint distribution. In contrast, H2SW utilizes the Radon Transform on the joint spaces of projections i.e., considering all distributed weighted combinations which is equivalent to considering all Buseman functions under a probability law. As a result, the $\mathrm{H} 2 \mathrm{SW}$ is a valid metric as long as the Buseman projections can be proven to be injective (the injectivity of the Buseman projection has not been known at the moment) while Cartan-Hadamard Sliced-Wasserstein is only pseudo metric since the injectivity of a fixed weighted combination is not trivial to show. Moreover, H2SW does not only focus on the product of Hadamard manifolds i.e., H2SW is a generic distance for heterogeneous joint distributions in which marginal domains do not necessary manifolds e.g., images [41], functions [22], and so on. In the later experiments, we conduct experiments on comparing 3D shapes which are represented by a distribution on the product of the Euclidean space and the 2D sphere (not a Hadamard manifold).

## 4 Experiments

In this section, we first compare the performance of the proposed H2SW with SW and GSW in the 3D mesh deformation application. After that, we further evaluate the performance of $\mathrm{H} 2 \mathrm{SW}$ in training a deep 3D mesh autoencoder compared to SW and GSW. Finally, we compare H2SW with SW and Cartan-Hadamard Sliced-Wasserstein (CHSW) in comparing datasets on the product of Hadamard manifolds. In the experiments, we use $c(x, y)=|x-y|$ and $p=2$ for all SW variants.

### 4.1 3D Mesh Deformation

In this task, we would like to move from a source mesh to a target mesh. To represent those meshes, we sample 10000 points by Poisson disk sampling and their corresponding normal vectors of the mesh surface at those points. Let the source mesh be denoted as $X(0)=\left\{x_{1}(0), \ldots, x_{n}(0)\right\}$ and the target mesh be denoted as $Y=\left\{y_{1}, \ldots, y_{n}\right\}$. We deform $X(0)$ to $Y$ by integrating the ordinary differential equation $\dot{X}(t)=-n \nabla_{X(t)}\left[\mathcal{S}\left(\frac{1}{n} \sum_{i=1}^{n} \delta\left(x-x_{i}(t)\right), \frac{1}{n} \sum_{i=1}^{n} \delta\left(y-y_{i}\right)\right)\right]$, where $\mathcal{S}$ denotes a SW variant. We utilize the Euler discretization scheme with step size 0.01 and 5000 steps. For evaluation, we use the joint Wasserstein distance in Equation 5 with the mixed distance from the

Table 1: Summary of joint Wasserstein distances across time steps from deformation from the sphere mesh to the Armadillo mesh.

| Distances | Step $100\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ | Step $300\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ | Step $500\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ | Step $1500\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ | Step $4000\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ | Step $5000\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| SW L=10 | $1852.519 \pm 3.236$ | $1436.686 \pm 3.056$ | $1071.227 \pm 2.449$ | $104.452 \pm 2.35$ | $\mathbf{6 . 1 9} \pm \mathbf{0 . 3 0 7}$ | $2.726 \pm 0.305$ |
| GSW L $=10$ | $1893.438 \pm 3.205$ | $1535.737 \pm 3.363$ | $1192.52 \pm 3.274$ | $143.518 \pm 1.04$ | $8.73 \pm 0.353$ | $4.743 \pm 0.134$ |
| H2SW L $=10$ | $\mathbf{1 8 4 0 . 7 3} \pm \mathbf{1 . 2 8 2}$ | $\mathbf{1 4 2 2 . 6 6 7} \pm \mathbf{7 . 8 1 3}$ | $\mathbf{1 0 5 8 . 1 7 1 \pm \mathbf { 5 . 3 6 2 }}$ | $\mathbf{9 5 . 6 7 2} \pm \mathbf{4 . 3 7 6}$ | $6.326 \pm 0.151$ | $\mathbf{2 . 6 0 2} \pm \mathbf{0 . 2 0 1}$ |
| SW L $=100$ | $1847.572 \pm 0.303$ | $1426.425 \pm 0.528$ | $1059.127 \pm 1.106$ | $89.693 \pm 0.793$ | $\mathbf{4 . 4 5 3} \pm \mathbf{0 . 2 2}$ | $1.171 \pm 0.056$ |
| GSW L $=100$ | $1889.312 \pm 0.883$ | $1525.269 \pm 1.078$ | $1179.1 \pm 2.052$ | $122.618 \pm 1.175$ | $7.905 \pm 0.373$ | $3.226 \pm 0.388$ |
| H2SW L $=100$ | $\mathbf{1 8 3 9 . 3 4 7} \pm \mathbf{1 . 9 8 6}$ | $\mathbf{1 4 1 7 . 1} \pm \mathbf{3 . 6 7 7}$ | $\mathbf{1 0 4 8 . 8 9 5} \pm \mathbf{4 . 0 0 8}$ | $\mathbf{8 6 . 0 7 8} \pm \mathbf{0 . 6 2 3}$ | $4.61 \pm 0.431$ | $\mathbf{1 . 0 8 6} \pm \mathbf{0 . 1 7 7}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-09.jpg?height=622&width=268&top_left_y=692&top_left_x=254)

Source
![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-09.jpg?height=592&width=202&top_left_y=709&top_left_x=538)

Step 500
![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-09.jpg?height=584&width=200&top_left_y=713&top_left_x=778)

Step 1500
![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-09.jpg?height=606&width=204&top_left_y=710&top_left_x=1010)
![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-09.jpg?height=604&width=202&top_left_y=710&top_left_x=1232)

Step 4000
![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-09.jpg?height=592&width=204&top_left_y=709&top_left_x=1470)

Step 5000
![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-09.jpg?height=618&width=200&top_left_y=698&top_left_x=1688)

Figure 2: Visualization of deformation from the sphere mesh to the Armadillo mesh with $L=10$.

Euclidean distance and the great circle distance. We use the circular defining function for GSW, and use the linear defining function and the circular defining function for H2SW. We vary the number of projections $L \in\{10,100\}$ for all variants. For H2SW and GSW, we select the best hyperparameter of the circular defining function $r \in\{0.5,0.7,0.8,0.9,1,5,10,50,100\}$.

Results. We compare H2SW with GSW and SW by deforming the sphere mesh to the Armadillo mesh [61]. We report the quantitative results in Table 1 after 3 independent runs and the qualitative result for $L=10$ in Figure 2 and $L=100$ in Figure 5 in Appendix D. From Table 1, we observe that H2SW helps the deformation convergence faster at the beginning and better at the end in terms of the joint Wasserstein distance, especially for a small value of the number of projections i.e., $L=10$. The result for $L=100$ is better than $L=10$ which is consistent with Proposition 5 . The qualitative results in Figure 6 and Figure 7 also reinforce the favorable performance of H2SW since they are visually consistent with quantitative scores. We also conduct deformation to the Stanford Bunny mesh $[16,61]$ in Table 4, Figure 6, and Figure 7 in Appendix D and we observe the same phenomenon that $\mathrm{H} 2 \mathrm{SW}$ is the best variant for 3D meshes. From those experiments, H2SW has shown the benefit of the HHRT in transforming a joint distribution over the product of the Euclidean space and the 2D sphere compared to the conventional RT of SW and the conventional GRT of GSW.

Table 2: Joint Wasserstein distance reconstruction errors (multiplied by 100) from three different runs of autoencoders trained by SW, GSW, and H2SW with the number of projections $L=100$ and $L=1000$.

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-10.jpg?height=1087&width=1618&top_left_y=362&top_left_x=275)

Figure 3: Visualization of some randomly selected reconstruction meshes from autoencoders trained by SW, GSW, and H2SW in turn with the number of projections $L=100$ at epoch 2000.

### 4.2 Training deep 3D mesh autoencoder

We utilize the processed ShapeNet dataset [12] from [47], then sample 2048 points and the corresponding normal vectors from each shape in the dataset. Formally, we would like to train an autoencoder that contains an encoder $f_{\phi}$ that maps a mesh $X \in \mathbb{R}^{2048 \times 6}$ to a latent code $z \in \mathbb{R}^{1024}$, and a decoder $g_{\psi}$ that maps the latent code $z$ back to the reconstructed mesh $\tilde{X} \in \mathbb{R}^{2048 \times 6}$. We adopt Point-Net [49] architecture to construct the autoencoder. We want to train the encoder $f_{\phi}$ and the decoder $g_{\psi}$ such that $\tilde{X}=g_{\psi}\left(f_{\phi}(X)\right) \approx X$ for all shapes $X$ in the dataset. To do that, we solve the following optimization problem:

$$
\min _{\phi, \gamma} \mathbb{E}_{X \sim \mu(X)}\left[\mathcal{S}\left(P_{X}, P_{\left.g_{\gamma}\left(f_{\phi}(X)\right)\right)}\right]\right.
$$

where $\mathcal{S}$ is a sliced Wasserstein variant, and $P_{X}=\frac{1}{n} \sum_{i=1}^{n} \delta\left(x-x_{i}\right)$ denotes the empirical distribution over the point cloud $X=\left(x_{1}, \ldots, x_{n}\right)$. We train the autoencoder for 2000 epochs on the training set of the ShapeNet dataset using an SGD optimizer with a learning rate of $1 e-3$, and a batch size of 128. For evaluation, we also use the joint Wasserstein distance in Equation 5 with the mixed distance from the Euclidean distance and the great circle distance to measure the average reconstruction loss

Table 3: Relative error to the joint Wasserstein distance of SW, CHSW, and H2SW.

| Distances | $L=100$ | $L=500$ | $L=1000$ | $L=2000$ |
| :--- | :---: | :---: | :---: | :---: |
| SW | $4.618 \pm 0.744$ | $4.253 \pm 0.398$ | $4.235 \pm 0.310$ | $4.198 \pm 0.238$ |
| CHSW | $4.449 \pm 0.497$ | $4.063 \pm 0.254$ | $4.059 \pm 0.167$ | $4.035 \pm 0.145$ |
| H2SW | $\mathbf{4 . 3 8 1} \pm \mathbf{0 . 6 9 5}$ | $\mathbf{4 . 0 0 1} \pm \mathbf{0 . 2 6 7}$ | $\mathbf{4 . 0 4 8} \pm \mathbf{0 . 1 8 2}$ | $\mathbf{3 . 9 9 8} \pm \mathbf{0 . 1 4 2}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-11.jpg?height=322&width=415&top_left_y=560&top_left_x=218)

SW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-11.jpg?height=320&width=418&top_left_y=564&top_left_x=623)

CHSW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-11.jpg?height=320&width=398&top_left_y=564&top_left_x=1034)

$\mathrm{H} 2 \mathrm{SW}$

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-11.jpg?height=323&width=399&top_left_y=565&top_left_x=1443)

Joint Wasserstein

Figure 4: Cost matrices between datasets from SW, CHSW, and H2SW with $L=2000$.

on the testing set of the ShapeNet dataset. We use the circular defining function for GSW, and use the linear defining function and the circular defining function for H2SW. For H2SW and GSW, we select the best hyperparameter of the circular defining function $r \in\{0.5,0.7,0.8,0.9,1,5,10\}$. For more details such as the neural network architectures, we refer the reader to Appendix B.

Results. We report the joint Wasserstein reconstruction errors (measured in three independent times) on the testing set in Table 2 with trained autoencoder at epoch 500, 1000, and 2000 from SW, GSW, and H2SW with the number of projections $L=100$ and $L=1000$. In addition, we show some randomly reconstructed meshes for epoch 2000in Figure 3 and for epoch 500 in Figure 8 in Appendix D. From Table 2, we observe that H2SW yields the lowest reconstruction errors for both $L=100$ and $L=1000$. Moreover, we see that the reconstruction errors are lower with $L=1000$ than ones with $L=100$ for all $\mathrm{SW}$ variants. The qualitative reconstructed meshes in Figure 3 and Figure 8reflect the same relative comparison. It is worth noting that both the qualitative and the qualitative performance of autoencoders can be improved by using more powerful neural networks. Since we focus on comparing SW, GSW, and H2SW, we only use a light neural network i.e., Point-Net [49] architecture. The trained autoencoders can be further used to reduce the size of 3D meshes for data compression and for dimension reduction, however, such downstream applications are not our focus in the current investigation of the paper.

### 4.3 Comparing Datasets on The Product of Hadamard Manifolds

We follow the same experimental setting from [9]. Here, we have datasets as sets of feature-label pairs which are embedded in the space of $\mathbb{R}^{d_{1}} \times \mathbb{L}^{d_{2}}$ where $\mathbb{L}^{d_{2}}$ denotes a Lorentz model of $d_{2}$ dimension (a hyperbolic space). We uses MNIST [32] dataset, EMNIST dataset [14], Fashion MNIST dataset [58], KMNIST dataset [13], and USPS dataset [25]. For CHSW, we use Buseman projection on the product space of Euclidean and the Lorentz model. For H2SW, we use the linear defining function and the Buseman function on the Lorentz model. We refer the reader to Appendix B for greater detail on Buseman functions and experimental setups. We compare SW, CHSW, and H2SW by varying $L \in\{100,500,1000,2000\}$ For evaluation, we use the joint Wasserstein distance in [1] as the
ground truth. In particular, let $C_{W}$ be the cost matrix from the joint Wasserstein distance and $C$ be a given cost matrix, we use $\left|C / \max (C)-C_{W} / \max \left(C_{W}\right)\right|$ as the relative error.

Results. We report the relative errors from SW, CHSW, and H2SW in Table 3 after 100 independent runs. In addition, we show the cost matrices from SW, CHSW, H2SW. and joint Wasserstein distance with $L=2000$ in Figure 4. Cost matrices for $L=100, L=500$, and $L=1000$ are given in Figure 911 in Appendix D. From Table 3, we see that H2SW gives a lower relative error than CHSW and SW. Therefore, using H2SW for comparing datasets is the most equivalent to the joint Wasserstein distance in terms of the relative error. We also observe that increasing the value of the number of projections also reduces the relative errors for all SW variants. Again, we would like to recall that H2SW can be used for heterogeneous joint distributions beyond the product of Hadamard manifolds as shown in previous experiments.

## 5 Conclusion

We have presented Hierarchical Hybrid Sliced Wasserstein (H2SW) distance, a novel sliced probability metric for heterogeneous joint distributions i.e., joint distributions have marginals on different domains. The key component of H2SW is the proposed hierarchical hybrid Radon Transform (HHRT) which is the composition of partial Radon Transform and multiples proposed partial generalized Radon Transform. We then discuss the injectivity of the proposed transforms and theoretical properties of H2SW including topological properties, statistical properties, and computational properties. On the experimental side, we show that H2SW has favorable performance in applications of 3D mesh deformation, training deep 3D mesh autoencoder, and datasets comparison. In those applications, heterogeneous joint distributions appear in the form of joint distributions on the product of Euclidean space and 2D sphere, and the product of Hadamard manifolds. In the future, we will extend the application of H2SW to more complicated heterogeneous joint distributions.

## Supplement to "Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions"

We first provide skipped proofs in the main paper in Appendix A. We then provide some additional materials including additional background and extended definitions in Appendix B. After that, we discuss some related works in Appendix C. We report additional experimental results in Appendix D. Finally, we report computational infrastructure in Appendix E.

## A Proofs

## A. 1 Proof of Proposition 1

For any $t, \theta$, $y$, we are given $\left(\mathcal{P G} \mathcal{R} f_{1}\right)(t, \theta, y)=\left(\mathcal{P G} \mathcal{R} f_{2}\right)(t, \theta, y)$. By Definition 1 , we have:

$$
\int_{\mathbb{R}^{d_{1}}} f_{1}(x, y) \delta(t-g(x, \theta)) d x=\int_{\mathbb{R}^{d_{1}}} f_{2}(x, y) \delta(t-g(x, \theta)) d x
$$

For any $\varepsilon \in \mathbb{R}^{2}$, we have:

$$
\int_{\mathbb{R}^{d_{2}}} \int_{\mathbb{R}^{d_{1}}} f_{1}(x, y) \delta(t-g(x, \theta)) e^{-i 2 \pi\langle\varepsilon, y\rangle} d x d y=\int_{\mathbb{R}^{d_{2}}} \int_{\mathbb{R}^{d_{1}}} f_{2}(x, y) \delta(t-g(x, \theta)) e^{-i 2 \pi\langle\varepsilon, y\rangle} d x d y
$$

Applying the Fubini's theorem, we have:

$$
\int_{\mathbb{R}^{d_{1}}} f_{1}(x, y) \int_{\mathbb{R}^{d_{2}}} e^{-i 2 \pi\langle\varepsilon, y\rangle} d y \delta(t-g(x, \theta)) d x=\int_{\mathbb{R}^{d_{1}}} \int_{\mathbb{R}^{d_{2}}} f_{2}(x, y) e^{-i 2 \pi\langle\varepsilon, y\rangle} d y \delta(t-g(x, \theta)) d x
$$

which is:

$$
\left(\mathcal{G R} \int_{\mathbb{R}^{d_{2}}} f_{1}(x, y) e^{-i 2 \pi\langle\varepsilon, y\rangle} d y\right)=\left(\mathcal{G R} \int_{\mathbb{R}^{d_{2}}} f_{2}(x, y) e^{-i 2 \pi\langle\varepsilon, y\rangle} d y\right)
$$

By the injectivity of GRT, we have:

$$
\int_{\mathbb{R}^{d_{2}}} f_{1}(x, y) e^{-i 2 \pi\langle\varepsilon, y\rangle} d y=\int_{\mathbb{R}^{d_{2}}} f_{2}(x, y) e^{-i 2 \pi\langle\varepsilon, y\rangle} d y
$$

Then, for any $\epsilon \in \mathbb{R}^{d_{1}}$, we have

$$
\int_{\mathbb{R}^{d_{1}}} \int_{\mathbb{R}^{d_{2}}} f_{1}(x, y) e^{-i 2 \pi\langle\varepsilon, y\rangle} e^{-i 2 \pi\langle\epsilon, x\rangle} d y d x=\int_{\mathbb{R}^{d_{1}}} \int_{\mathbb{R}^{d_{2}}} f_{2}(x, y) e^{-i 2 \pi\langle\varepsilon, y\rangle} e^{-i 2 \pi\langle\epsilon, x\rangle} d y d x
$$

which is $\left.\left(\mathcal{F} f_{1}(x, y)\right)=\left(\mathcal{F} f_{2}(x, y)\right)\right)$ with $\mathcal{F}$ denotes the Fourier transform. By the injectivity of the Fourier Transform, we have $f_{1}(x, y)=f_{2}(x, y)$ for any $x, y$, which concludes the proof.

## A. 2 Proof of Proposition 2

We first show that HHRT is the composition of PGRT and PRT. We have

$$
\begin{aligned}
& (\mathcal{P R}(\mathcal{P G} \mathcal{G}(\mathcal{P G R} f)))\left(t, \theta_{1}, \theta_{2}, \psi\right) \\
& =\int_{\mathbb{R}^{2}} \int_{\mathbb{R}^{d_{1}}} \int_{\mathbb{R}^{d_{2}}} f(x, y) \delta\left(t_{1}-g_{1}\left(x, \theta_{1}\right)\right) \delta\left(t_{2}-g_{2}\left(y, \theta_{2}\right)\right) \delta\left(t-\psi_{1} t_{1}-\psi_{2} t_{2}\right) d x d y d t_{1} d t_{2} \\
& =\int_{\mathbb{R}^{d_{1}}} \int_{\mathbb{R}^{d_{2}}} f(x, y) \int_{\mathbb{R}^{2}} \delta\left(t_{1}-g_{1}\left(x, \theta_{1}\right)\right) \delta\left(t_{2}-g_{2}\left(y, \theta_{2}\right)\right) \delta\left(t-\psi_{1} t_{1}-\psi_{2} t_{2}\right) d t_{1} d t_{2} d x d y \\
& =\int_{\mathbb{R}^{d_{1}}} \int_{\mathbb{R}^{d_{2}}} f(x, y) \delta\left(t-\psi_{1} g_{1}\left(x, \theta_{1}\right)-\psi_{2} g_{2}\left(y, \theta_{2}\right)\right) d x d y \\
& =(\mathcal{H} \mathcal{H} f)\left(t, \theta_{1}, \theta_{2}, \psi\right) .
\end{aligned}
$$

For any $t, \theta_{1}, \theta_{2}, \psi$, we are given $\left(\mathcal{H H} \mathcal{R} f_{1}\right)\left(t, \theta_{1}, \theta_{2}, \psi\right)=\left(\mathcal{H H} \mathcal{R} f_{2}\right)\left(t, \theta_{1}, \theta_{2}, \psi\right)$, which is equivalent to:

$$
\left(\mathcal{P R}\left(\mathcal{P G} \mathcal{R}\left(\mathcal{P G} \mathcal{R} f_{1}\right)\right)\right)\left(t, \theta_{1}, \theta_{2}, \psi\right)=\left(\mathcal{P} \mathcal{R}\left(\mathcal{P} \mathcal{G}\left(\mathcal{P} \mathcal{G} \mathcal{R} f_{2}\right)\right)\right)\left(t, \theta_{1}, \theta_{2}, \psi\right)
$$

By the injectivity of the PRT and the PGRT, we obtain $f_{1}(x, y)=f_{2}(x, y)$ for any $x, y$ which completes the proof.

## A. 3 Proof of Theorem 1

To prove that the hierarchical hybrid sliced Wasserstein $H 2 S W_{p}\left(\cdot, \cdot ; c, g_{1}, g_{2}\right)$ is a metric on the space of distributions on $\mathcal{P}\left(\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}\right)$ for any $p \geq 1$, ground metric $c$, and defining functions $g_{1}, g_{2}$, we need to show that it satisfies non-negativity, symmetry, triangle inequality, and identity of indiscernible.

Non-Negativity. Since $\mathrm{W}_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu ; c\right) \geq 0[48]$ for any $\theta_{1}, \theta_{2}, \psi$, we have:

$$
\mathbb{E}_{\left(\theta_{1}, \theta_{2}, \psi\right) \sim \mathcal{U}\left(\Omega_{1} \times \Omega_{2} \times \mathbb{S}\right)}\left[\mathrm{W}_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu ; c\right)\right] \geq 0,
$$

which means that $H 2 S W_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right) \geq 0$ for any $\mu$ and $\nu$.

Symmetry. Since we have the symmetry of the Wasserstein distance $\mathrm{W}_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu ; c\right)=$ $\mathrm{W}_{p}^{p}\left(\mathcal{H H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu ; c\right)[48]$ for any $\theta_{1}, \theta_{2}, \psi$, we have:

$$
\begin{aligned}
& \mathbb{E}_{\left(\theta_{1}, \theta_{2}, \psi\right) \sim \mathcal{U}\left(\Omega_{1} \times \Omega_{2} \times \mathbb{S}\right)}\left[\mathrm{W}_{p}^{p}\left(\mathcal{H H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu ; c\right)\right] \\
& =\mathbb{E}_{\left(\theta_{1}, \theta_{2}, \psi\right) \sim \mathcal{U}\left(\Omega_{1} \times \Omega_{2} \times \mathbb{S}\right)}\left[\mathrm{W}_{p}^{p}\left(\mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu, \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu ; c\right)\right]
\end{aligned}
$$

which means that $H 2 S W_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right)=H 2 S W_{p}\left(\nu, \mu ; c, g_{1}, g_{2}\right)$ any $\mu$ and $\nu$.

Triangle Inequality. Given $c$ to be a valid metric on $\mathbb{R}$, we can use the triangle inequality of the

Wasserstein distance. For any distributions $\mu_{1}, \mu_{2}, \mu_{3} \in \mathcal{P}\left(\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}\right)$, we have:

$$
\begin{aligned}
& \operatorname{H} 2 S W_{p}\left(\mu_{1}, \mu_{2} ; c, g_{1}, g_{2}\right)=\left(\mathbb{E}_{\left(\theta_{1}, \theta_{2}, \psi\right) \sim \mathcal{U}\left(\Omega_{1} \times \Omega_{2} \times \mathbb{S}\right)}\left[\mathrm{W}_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{\theta}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{1}, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{2} ; c\right)\right]\right)^{\frac{1}{p}} \\
& \leq\left(\mathbb { E } _ { ( \theta _ { 1 } , \theta _ { 2 } , \psi ) \sim \mathcal { U } ( \Omega _ { 1 } \times \Omega _ { 2 } \times \mathbb { S } ) } \left[\left(\mathrm{W}_{p}\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{1}, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{3} ; c\right)\right.\right.\right. \\
& \left.\left.\left.+\mathrm{W}_{p}\left(\mathcal{H H} \mathcal{H}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{3}, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{2} ; c\right)\right)^{p}\right]\right)^{\frac{1}{p}} \\
& \leq\left(\mathbb{E}_{\left(\theta_{1}, \theta_{2}, \psi\right) \sim \mathcal{U}\left(\Omega_{1} \times \Omega_{2} \times \mathbb{S}\right)}\left[\mathrm{W}_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{1}, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{3} ; c\right)\right]\right)^{\frac{1}{p}} \\
& +\left(\mathbb{E}_{\left(\theta_{1}, \theta_{2}, \psi\right) \sim \mathcal{U}\left(\Omega_{1} \times \Omega_{2} \times \mathbb{S}\right)}\left[\mathrm{W}_{p}^{p}\left(\mathcal{H H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{3}, \mathcal{H} \mathcal{H} \mathcal{\theta}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{2} ; c\right)\right]\right)^{\frac{1}{p}}
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-15.jpg?height=56&width=876&top_left_y=899&top_left_x=668)

where the final inequality is due to Minkowski's inequality. Therefore, we complete the proof for the triangle inequality of the hierarchical hybrid sliced Wasserstein.

Identity of indiscernible. For any $p \geq 1$, ground metric $c$, and $g_{1}, g_{2}$, when $\mu=\nu$, we have $\mathcal{H H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu=\left(\mathcal{H} \mathcal{H} \mathcal{\theta}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu\right.$. Therefore, we have $\mathrm{W}_{p}^{p}\left(\mathcal{H} \mathcal{H}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{1}, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{2} ; c\right)=0$ which leads to $\mathrm{H}_{2} \mathrm{SW}_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right)=0$. Now, assume that $\mathrm{H}_{2} \mathrm{SW}_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right)=0$, then $\mathrm{W}_{p}^{p}\left(\mathcal{H H} \mathcal{A}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{1}, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu_{2} ; c\right)=0$ for almost everywhere $\theta_{1} \in \Omega_{1}, \theta_{2} \in \Omega_{2}, \psi \in \mathbb{S}$. By applying the identity property of the Wasserstein distance, we have $\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu=\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu\right.$ for almost everywhere $\theta_{1} \in \Omega_{1}, \theta_{2} \in \Omega_{2}, \psi \in \mathbb{S}$. Since the HHRT is injective (proved in Proposition 2), we obtain $\mu=\nu$.

## A. 4 Proof of Proposition 3

(i) For any $p \geq 1, c(x, y)=|x-y|$, and $\mu, \nu \in \mathcal{P}\left(\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}\right)$, we have:

$$
\begin{aligned}
& \mathrm{H}_{2} \mathrm{SW}_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right) \\
& =\left(\mathbb{E}_{\left(\theta_{1}, \theta_{2}, \psi\right) \sim \mathcal{U}\left(\Omega_{1} \times \Omega_{2} \times \mathbb{S}\right)}\left[\mathrm{W}_{p}^{p}\left(\mathcal{H H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu ; c\right)\right]\right)^{\frac{1}{p}} \\
& =\left(\mathbb{E}\left[\inf _{\pi \in \Pi(\mu, \nu)} \int\left|\psi_{1}\left(g_{1}\left(\theta_{1}, x_{1}\right)-g_{1}\left(\theta_{1}, y_{1}\right)\right)+\psi_{2}\left(g_{2}\left(\theta_{2}, x_{2}\right)-g_{1}\left(\theta_{2}, y_{2}\right)\right)\right|^{p} d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)\right]\right)^{\frac{1}{p}}
\end{aligned}
$$

By applying the Cauchy-Schwartz inequality, we have:

$$
\begin{aligned}
& \mathrm{H} 2 \mathrm{SW}_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right)
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-16.jpg?height=125&width=1797&top_left_y=474&top_left_x=240)

$$
\begin{aligned}
& \leq\left(\mathbb{E}\left[\inf _{\pi \in \Pi(\mu, \nu)} \int\left(\left|g_{1}\left(\theta_{1}, x_{1}\right)-g_{1}\left(\theta_{1}, y_{1}\right)\right|+\left|g_{2}\left(\theta_{2}, x_{2}\right)-g_{2}\left(\theta_{2}, y_{2}\right)\right|\right)^{p} d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)\right]\right)^{\frac{1}{p}} \\
& \leq\left(\mathbb{E}\left[\inf _{\pi \in \Pi(\mu, \nu)} \int\left|g_{1}\left(\theta_{1}, x_{1}\right)-g_{1}\left(\theta_{1}, y_{1}\right)\right|^{p} d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)\right]\right)^{\frac{1}{p}} \\
& +\left(\mathbb{E}\left[\inf _{\pi \in \Pi(\mu, \nu)} \int\left|g_{2}\left(\theta_{2}, x_{2}\right)-g_{2}\left(\theta_{2}, y_{2}\right)\right|^{p} d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)\right]\right)^{\frac{1}{p}} \\
& =\left(\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \nu_{1}\right)} \int\left|g_{1}\left(\theta_{1}, x_{1}\right)-g_{1}\left(\theta_{1}, y_{1}\right)\right|^{p} d \pi\left(x_{1}, y_{1}\right)\right]\right)^{\frac{1}{p}} \\
& +\left(\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{2}, \nu_{2}\right)} \int\left|g_{2}\left(\theta_{2}, x_{2}\right)-g_{2}\left(\theta_{2}, y_{2}\right)\right|^{p} d \pi\left(x_{2}, y_{2}\right)\right]\right)^{\frac{1}{p}} \\
& =\operatorname{GSW}_{p}\left(\mu_{1}, \nu_{1} ; g_{1}, c\right)+\operatorname{GSW}_{p}\left(\mu_{2}, \nu_{2} ; g_{2}, c\right)
\end{aligned}
$$

where the last inequality is due to the Minkowski's inequality.

(ii) From (i), we have $\mathrm{H}_{2} \mathrm{SW}_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right) \leq \operatorname{GSW}_{p}\left(\mu_{1}, \nu_{1} ; g_{1}, c\right)+\operatorname{GSW}_{p}\left(\mu_{2}, \nu_{2} ; g_{2}, c\right)$. When, $g_{1}$, $g_{2}$, and $c(x, y)=|x-y|$ are linear defining functions, we have:

$$
\begin{aligned}
\operatorname{GSW}_{p}\left(\mu_{1}, \nu_{1} ; g_{1}, c\right) & =\left(\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \nu_{1}\right)} \int\left(\left|\theta^{\top} x_{1}-\theta^{\top} y_{1}\right|^{p} d \pi\left(x_{1}, y_{1}\right)\right]\right)^{\frac{1}{p}}\right. \\
& \leq\left(\mathbb{E}\left[\inf _{\pi \in \Pi\left(\mu_{1}, \nu_{1}\right)} \int\left(\|\theta\|_{2}\left\|x_{1}--y_{1}\right\|_{2}^{p} d \pi\left(x_{1}, y_{1}\right)\right]\right)^{\frac{1}{p}}\right. \\
& \leq\left(\mathbb { E } \left[\inf _{\pi \in \Pi\left(\mu_{1}, \nu_{1}\right)}\left(\left(\left\|x_{1}-y_{1}\right\|^{p} d \pi\left(x_{1}, y_{1}\right)\right]\right)^{\frac{1}{p}}\right.\right. \\
& =\left(\inf _{\pi \in \Pi\left(\mu_{1}, \nu_{1}\right)}\left(\left\|x_{1}-y_{1}\right\|^{p} d \pi\left(x_{1}, y_{1}\right)\right)^{\frac{1}{p}}\right. \\
& =W_{p}\left(\mu_{1}, \nu_{1} ; c\right)
\end{aligned}
$$

Similarly, we have $\operatorname{GSW}_{p}\left(\mu_{2}, \nu_{2} ; g_{1}, c\right) \leq W_{p}\left(\mu_{2}, \nu_{2} ; c\right)$. Therefore, we obtain the proof of $\mathrm{H} 2 \mathrm{SW} . p\left(\mu, \nu ; c, g_{1}, g_{2}\right) \leq$ $W_{p}\left(\mu_{1}, \nu_{1} ; c\right)+W_{p}\left(\mu_{1}, \nu_{1} ; c\right)$.
(iii) When $g_{1}, g_{2}$ are linear defining functions, we have:

$$
\begin{aligned}
& \mathrm{H}_{2} \mathrm{SW}_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right) \\
& \left.\leq\left(\mathbb{E}\left[\inf _{\pi \in \Pi(\mu, \nu)} \int\left(\mid \theta_{1}^{\top} x_{1}-\theta_{1}^{\top} y_{1}\right)|+| \theta_{2}^{\top} x_{2}-\theta_{2}^{\top} y_{2} \mid\right)^{p} d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)\right]\right)^{\frac{1}{p}} \\
& \left.\leq\left(\mathbb{E}\left[\inf _{\pi \in \Pi(\mu, \nu)} \int\left(\mid \theta_{1}^{\top} x_{1}-\theta_{1}^{\top} y_{1}\right)|+| \theta_{2}^{\top} x_{2}-\theta_{2}^{\top} y_{2} \mid\right)^{p} d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)\right]\right)^{\frac{1}{p}} \\
& \left.\leq\left(\mathbb{E}\left[\inf _{\pi \in \Pi(\mu, \nu)} \int\left(\mid x_{1}-y_{1}\right)|+| x_{2}-y_{2} \mid\right)^{p} d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)\right]\right)^{\frac{1}{p}} \\
& \left.=\left(\inf _{\pi \in \Pi(\mu, \nu)} \int\left(\mid x_{1}-y_{1}\right)|+| x_{2}-y_{2} \mid\right)^{p} d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)\right)^{\frac{1}{p}}
\end{aligned}
$$

When $p=1$, we obtain:

$$
\begin{aligned}
\operatorname{H}_{2} \mathrm{SW}_{1}\left(\mu, \nu ; c, g_{1}, g_{2}\right) & \left.\leq\left(\inf _{\pi \in \Pi(\mu, \nu)} \int\left(\mid x_{1}-y_{1}\right)|+| x_{2}-y_{2} \mid\right) d \pi\left(x_{1}, x_{2}, y_{1}, y_{2}\right)\right)^{\frac{1}{p}} \\
& =W_{1}(\mu, \nu ; c, c)
\end{aligned}
$$

which completes the proof.

## A. 5 Proof of Proposition 4

Let $p \geq 1, c(x, y)=|x-y|, \mu \in \mathcal{P}(\mathbb{R})$ with the corresponding empirical distribution $\mu_{n}$, we assume that there exists $q>p$ such that the $q$-th order moment of $\mu$ i.e, $M_{q}(\mu)=\int_{\mathbb{R}}|x|^{q} d \mu(x)$, is bounded by $B<\infty$. From Theorem 1 in [21], there exists a constant $C_{p, q}$ such that:

$$
\mathbb{E}\left[W_{p}^{p}\left(\mu_{n}, \mu ; c\right)\right] \leq C_{p, q} B\left\{\begin{array}{l}
n^{-1 / 2} \text { if } q>2 p \\
n^{-1 / 2} \log (n)^{\frac{1}{p}} \text { if } q=2 p \\
n^{-(q-p) / q} \text { if } q \in(p, 2 p)
\end{array}\right.
$$

We show that $\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu$ has finite bounded moments. In particular, we have:

$$
\begin{aligned}
M_{k}\left(\mathcal{H H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu\right) & =\int_{\mathbb{R}}|t|^{k} d\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu\right)(t) \\
& =\int_{\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}}\left|\psi_{1} g_{1}\left(\theta_{1}, x_{1}\right)+\psi_{2} g_{2}\left(\theta_{2}, x_{2}\right)\right|^{k} d \mu\left(x_{1}, x_{2}\right) \\
& \leq \int_{\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}}\left(\psi_{1}^{2}+\psi_{2}^{2}\right)^{k / 2}\left(g_{1}\left(\theta_{1}, x_{1}\right)^{2}+g_{2}\left(\theta_{2}, x_{2}\right)^{2}\right)^{k / 2} d \mu\left(x_{1}, x_{2}\right) \\
& \leq \int_{\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}}\left(\left|g_{1}\left(\theta_{1}, x_{1}\right)\right|+\left|g_{2}\left(\theta_{2}, x_{2}\right)\right|\right)^{k} d \mu\left(x_{1}, x_{2}\right)
\end{aligned}
$$

where the first inequality is due to the Cauchy-Schwarz inequality and the second inequality is due to the fact that $\|x\|_{2} \leq|x|$. For the linear defining functions $g(\theta, x)=\theta^{\top} x$, we have $|g(\theta, x)|=\left|\theta^{\top} x\right| \leq$
$\|x\|_{1}$. For the circular defining functions $g(\theta, x)=\|x-r \theta\|_{2} \leq\|x-r \theta\|_{1} \leq\|x\|_{1}+\|r \theta\|_{1} \leq\|x\|_{1}+r$. Therefore, we have:

$$
\begin{aligned}
M_{k}\left(\mathcal{H H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \mu\right) & \leq \int_{\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}}\left(\left|x_{1}\right|+\left|x_{2}\right|+C_{g_{1}, g_{2}}\right)^{k} d \mu\left(x_{1}, x_{2}\right) \\
& =\int_{\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}} \sum_{i=0}^{k} k^{i}\left(\left|x_{1}\right|+\left|x_{2}\right|\right)^{i} C_{g_{1}, g_{2}}^{k-i} d \mu\left(x_{1}, x_{2}\right) \\
& =\sum_{i=0}^{k} k^{i} C_{g_{1}, g_{2}}^{k-i} \int_{\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}}\left(\left|x_{1}\right|+\left|x_{2}\right|\right)^{i} d \mu\left(x_{1}, x_{2}\right) \\
& \leq \sum_{i=0}^{k} k^{i} C_{g_{1}, g_{2}}^{k-i} M_{i}(\mu),
\end{aligned}
$$

where $C_{g_{1}, g_{2}}=0$ if $g_{1}, g_{2}$ are linear, $C_{g_{1}, g_{2}}=r$ if $g_{1}$ and $g_{2}$ are linear and circular respectively (exchangeable), and $C_{g_{1}, g_{2}}=2 r$ if both $g_{1}$ and $g_{2}$ are circular.

Now, using the triangle inequality of H2SW (Theorem 1), we have:

$$
\begin{aligned}
& \mathbb{E}\left|\mathrm{H}_{2} \mathrm{SW}_{p}\left(\mu_{n}, \nu_{n} ; c, g_{1}, g_{2}\right)-\mathrm{H}_{2} \mathrm{SW}_{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right)\right|
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-18.jpg?height=49&width=889&top_left_y=1493&top_left_x=520)

$$
\begin{aligned}
& \leq \mathbb{E}\left|\mathrm{H}_{2} \mathrm{SW}_{p}\left(\mu, \mu_{n} ; c, g_{1}, g_{2}\right)\right|+\mathbb{E}\left|\mathrm{H}_{2} \mathrm{SW}_{p}\left(\nu, \nu_{n} ; c, g_{1}, g_{2}\right)\right| \\
& \leq\left(\mathbb{E}\left|{\mathrm{H} 2 \mathrm{SW}_{p}^{p}}_{(}\left(\mu, \mu_{n} ; c, g_{1}, g_{2}\right)\right|\right)^{\frac{1}{p}}+\left(\mathbb{E}\left|\mathrm{H}_{2} \mathrm{SW}_{p}^{p}\left(\nu, \nu_{n} ; c, g_{1}, g_{2}\right)\right|\right)^{\frac{1}{p}}
\end{aligned}
$$

where the last inequality is due to Holder's inequality. Combining with previous results, we obtain:

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-18.jpg?height=54&width=843&top_left_y=1992&top_left_x=370)

$$
\begin{aligned}
& \leq C_{p, q}^{\frac{1}{p}}\left(\sum_{i=0}^{q} q^{i} C_{g_{1}, g_{2}}^{q-i}\left(M_{i}(\mu)+M_{i}(\nu)\right)\right)^{\frac{1}{p}}\left\{\begin{array}{l}
n^{-1 / 2 p} \text { if } q>2 p \\
n^{-1 / 2 p} \log (n)^{\frac{1}{p}} \text { if } q=2 p \\
n^{-(q-p) / p q} \text { if } q \in(p, 2 p)
\end{array}\right.
\end{aligned}
$$

which completes the proof.

## A. 6 Proof of Proposition 5

For any $p \geq 1$, and $\mu, \nu \in \mathcal{P}\left(\mathbb{R}^{d_{1}} \times \mathbb{R}^{d_{2}}\right)$, using the Holder's inequality, we have:

$$
\begin{aligned}
& \mathbb{E}\left|\widehat{\mathrm{H} 2 \mathrm{SW}}_{p}^{p}\left(\mu, \nu ; c, g_{1}, g_{2}, L\right)-\mathrm{H} 2 \mathrm{SW}_{p}^{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right)\right| \\
& \leq\left(\mathbb{E}\left|\widehat{\mathrm{H}} 2 \mathrm{SW}_{p}^{p}\left(\mu, \nu ; c, g_{1}, g_{2}, L\right)-\mathrm{H} 2 \mathrm{SW}_{p}^{p}\left(\mu, \nu ; c, g_{1}, g_{2}\right)\right|^{2}\right)^{\frac{1}{2}} \\
& =\left(\mathbb{E}\left|\frac{1}{L} \sum_{l=1}^{L} \mathrm{~W}_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1 l}, \theta_{2 l}, \psi_{l}}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H} \mathcal{R}_{\theta_{1 l}, \theta_{2 l}, \psi_{l}}^{g_{1}, g_{2}} \sharp \nu ; c\right)-\mathbb{E}\left[\mathrm{W}_{p}^{p}\left(\mathcal{H} \mathcal{R}_{\theta_{1}, \theta_{2}, \psi^{2}}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H} \mathcal{H} \mathcal{\theta}_{\theta_{1}, \theta_{2}, \psi}^{g_{1}, g_{2}} \sharp \nu ; c\right)\right]\right|^{2}\right)^{\frac{1}{2}} \\
& =\left(\operatorname{Var}\left[\frac{1}{L} \sum_{l=1}^{L} \mathrm{~W}_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1 l}, \theta_{2 l}, \psi_{l}}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1 l}, \theta_{2 l}, \psi_{l}}^{g_{1}, g_{2}} \sharp \nu ; c\right)\right]\right)^{\frac{1}{2}} \\
& =\frac{1}{\sqrt{L}} \operatorname{Var}\left[\mathrm{W}_{p}^{p}\left(\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1 l}, \theta_{2 l}, \psi_{l}}^{g_{1}, g_{2}} \sharp \mu, \mathcal{H} \mathcal{R}_{\theta_{1 l},,_{2 l}, \psi_{l}}^{g_{1}, g_{2}} \sharp \nu ; c\right)\right]^{\frac{1}{2}},
\end{aligned}
$$

which completes the proof.

## B Additional Materials

HHRT with more than two marginals. We now extend the definition of HHRT to $K>2$ mariginals.

Definition 4 (Hierarchical Hybrid Radon Transform). Given $K \geq 2$, given defining functions $\left\{g_{k}: \mathbb{R}^{d_{k}} \times \Omega_{i} \rightarrow \mathbb{R}\right\}_{i=k}^{K}$, the Hierarchical Hybrid Radon Transform $\mathcal{H} \mathcal{H}: \mathbb{L}_{1}\left(\mathbb{R}^{d_{1}} \times \ldots \times \mathbb{R}^{d_{K}}\right) \rightarrow$ $\mathbb{L}_{1}\left(\mathbb{R} \times \Omega_{1} \ldots \times \Omega_{K} \times \mathbb{S}^{K-1}\right)$ is defined as:

$$
\begin{align*}
& (\mathcal{H H} \mathcal{H} f)\left(t, \theta_{1}, \ldots, \theta_{K}, \psi\right) \\
& \quad=\int_{\mathbb{R}^{d_{1}}{ }_{1} \ldots \times \mathbb{R}^{d_{K}}} f\left(x_{1}, \ldots, x_{K}\right) \delta\left(t-\sum_{k=1}^{K} \psi_{k} g_{k}\left(x_{k}, \theta_{k}\right)\right) d x_{1} \ldots d x_{K} \tag{12}
\end{align*}
$$

H2SW with more than two marginals. From the new definition of HRRT on $K>2$ mariginals, we now can define $\mathrm{H} 2 \mathrm{SW}$ between joint distributions with $K$ mariginals.

Definition 5. For $p \geq 1, K \geq 2$, defining functions $g_{1}, \ldots, g_{K}$, the hierarchical hybrid sliced Wasserstein-p (H2SW) distance between two distributions $\mu \in \mathcal{P}\left(\mathcal{X}_{1} \times \ldots \times \mathcal{X}_{K}\right)$ and $\nu \in \mathcal{P}\left(\mathcal{Y}_{1} \times\right.$ $\ldots \times \mathcal{Y}_{K}$ ) with an one-dimensional ground metric $c: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}^{+}$is defined as:

$$
\begin{align*}
& H 2 S W_{p}^{p}\left(\mu, \nu ; c, g_{1}, \ldots, g_{K}\right) \\
& \quad=\mathbb{E}_{\left(\theta_{1}, \ldots, \theta_{K}, \psi\right) \sim \mathcal{U}\left(\Omega_{1} \times \ldots \times \Omega_{K} \times \mathbb{S}^{K-1}\right)}\left[W_{p}^{p}\left(\mathcal{H H} \mathcal{R}_{\theta_{1}, \ldots, \theta_{K}, \psi}^{g_{1}, \ldots, g_{K}} \sharp \mu, \mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \ldots, \theta_{K}, \psi}^{g_{1}, \ldots, g_{K}} \sharp \nu ; c\right)\right], \tag{13}
\end{align*}
$$

where $\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \ldots, \theta_{K}, \psi}^{g_{1}, \ldots, g_{K}} \sharp \mu$ and $\mathcal{H} \mathcal{H} \mathcal{R}_{\theta_{1}, \ldots, \theta_{K}, \psi}^{g_{1}, \ldots, g_{K}} \sharp \nu$ are the one-dimensional push-forward distributions created by applying HHRT.

Lorentz Model and Busemann function. The Lorentz model $\mathbb{L}^{d} \in \mathbb{R}^{d+1}$ of a d-dimensional hyperbolic space is [7]:

$$
\mathbb{L}^{d}=\left\{\left(x_{1}, \ldots, x_{d}\right) \in \mathbb{R}^{d+1},-x_{0} y_{0}+\sum_{i=1}^{d} x_{i} y_{i}=-1, x_{0}>0\right\}
$$

Given a direction $\theta \in T_{x_{0}} \mathbb{L}^{d} \cap \mathbb{S}^{d}, x \in \mathbb{L}^{d}$, the Busemann function is:

$$
B(x, \theta)=\log \left(-\left\langle x, x_{0}+\theta\right\rangle\right)
$$

Busemann function on product Hadamard manifolds. For distributions supports on the product of $K \geq 2$ Hadamard manifolds with the corresponding Busemann functions $B_{1}, \ldots, B_{K}$, we have a Buseman function of the product manifolds is:

$$
B\left(x_{1}, \ldots, x_{K}, \theta_{1}, \ldots, \theta_{K}\right)=\sum_{k=1}^{K} \lambda_{k} B_{k}\left(x_{k}, \theta_{k}\right)
$$

for $\left(\lambda_{1}, \ldots, \lambda_{K}\right) \in \mathbb{S}^{K-1}$. The Cartan-Hyperbolic Sliced-Wasserstein distance use a fixed value of $\left(\lambda_{1}, \ldots, \lambda_{K}\right)$ e.g., $\left(\lambda_{1}, \ldots, \lambda_{K}\right)=(1 / \sqrt{K}, \ldots, 1 / \sqrt{K})$ (see $\left.{ }^{1}\right)$. In our proposed H2SW, we treat $\left(\lambda_{1}, \ldots, \lambda_{K}\right)$ as a random variable follows $\mathcal{U}\left(\mathbb{S}^{K-1}\right)$ and the value of $\mathrm{H} 2 \mathrm{SW}$ is defined as the mean of such random variable.

## C Related Works

HHRT and Generalized Radon Transform. HHRT can be also seen as a special case of GRT [3] with the defining function $g(x, \theta)=\psi_{1} g_{1}\left(x_{1}, \theta_{1}\right)+\psi_{2} g_{2}\left(y, \theta_{2}\right)$ with $x=\left(x_{1}, x_{2}\right)$ and $\theta=\left(\theta_{1}, \theta_{2}, \psi\right)$ $\left(\Omega=\Omega_{1} \times \Omega_{2} \times \mathbb{S}\right)$. However, without approaching via the hierarchical construction, the injectivity of the transform might be a challenge to obtain.

HHRT and Hierarchical Radon Transform. Hierarchical Radon Transform (HRT) [44] is the composition of Partial Radon Transform and Overparameterized Radon Transform, which is designed specifically for reducing projection complexity when using Monte Carlo estimation. Moreover, HRT is introduced with linear projection and does not focus on the problem of comparing heterogeneous joint distributions. In contrast to HRT, the proposed HHRT is the composition of multiple partial Generalized Radon Transform and Partial Random Transform, which is suitable for comparing heterogeneous joint distributions.

HHRT and convolution slicers. Convolution slicers [41] are introduced to project an image into a scalar. It can be viewed as a Hierarchial Partial Radon Transform i.e., small parts of the image are transformed first, then be aggregated later. Although convolution slicers can separate global and local information as HHRT, they focus on the domain of images only and have not been proven to be injective. Again, HHRT is designed to compare heterogeneous joint distributions and is proven to be injective in Proposition 2. As a result, H2SW is a valid metric while convolution sliced Wasserstein [41] is only a pseudo metric. Moreover, H2SW can also use convolution slicers when having marginal domains as images.[^0]

Table 4: Summary of joint Wasserstein distances (multiplied by 100) across time steps from deformation from the sphere mesh to the Stanford Bunny mesh.

| Distances | Step $100\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ | Step $300\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ | Step $500\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ | Step $1500\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ | Step $4000\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ | Step $5000\left(\mathrm{~W}_{c_{1}, c_{2}} \downarrow\right)$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $\mathrm{SW} \mathrm{L}=10$ | $26.868 \pm 0.579$ | $4.46 \pm 0.195$ | $1.52 \pm 0.081$ | $0.623 \pm 0.024$ | $0.221 \pm 0.023$ | $0.14 \pm 0.018$ |
| GSW L $=10$ | $26.837 \pm 0.496$ | $4.378 \pm 0.128$ | $1.548 \pm 0.062$ | $0.653 \pm 0.01$ | $0.173 \pm 0.018$ | $0.146 \pm 0.013$ |
| $\mathrm{H} 2 \mathrm{SW} \mathrm{L}=10$ | $23.283 \pm 0.119$ | $2.221 \pm 0.124$ | $1.452 \pm 0.075$ | $0.636 \pm 0.045$ | $0.177 \pm 0.009$ | $0.089 \pm 0.022$ |
| SW $L=100$ | $26.678 \pm 0.168$ | $4.109 \pm 0.138$ | $1.458 \pm 0.142$ | $0.362 \pm 0.023$ | $0.072 \pm 0.017$ | $0.049 \pm 0.006$ |
| GSW $\mathrm{L}=100$ | $26.795 \pm 0.202$ | $4.084 \pm 0.109$ | $1.375 \pm 0.049$ | $0.372 \pm 0.026$ | $0.048 \pm 0.004$ | $0.042 \pm 0.017$ |
| $\mathrm{H} 2 \mathrm{SW} \mathrm{L}=100$ | $23.772 \pm 0.19$ | $2.388 \pm 0.009$ | $1.358 \pm 0.051$ | $0.488 \pm 0.026$ | $0.064 \pm 0.01$ | $0.038 \pm 0.007$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-21.jpg?height=616&width=480&top_left_y=622&top_left_x=256)

Source
![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-21.jpg?height=432&width=202&top_left_y=818&top_left_x=538)

Step 500
![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-21.jpg?height=636&width=1098&top_left_y=614&top_left_x=790)

Figure 5: Visualization of deformation from the sphere mesh to the Armadillo mesh with $L=100$.
![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-21.jpg?height=652&width=1640&top_left_y=1392&top_left_x=253)

Source

Step 500

Step 4000

Target

Figure 6: Visualization of deformation from the sphere mesh to the Stanford Bunny mesh with $L=10$.

## D Additional Experiments

3D Mesh Deformation. As mentioned in the main text, we present the deformation visualization to the Armadill mesh with $L=100$ in Figure 5, and the deformation visualization to the Stanford

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-22.jpg?height=643&width=1632&top_left_y=237&top_left_x=257)

Figure 7: Visualization of deformation from the sphere mesh to the Stanford Bunny mesh with $L=100$.

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-22.jpg?height=688&width=1651&top_left_y=1030&top_left_x=259)

Figure 8: Visualization of some randomly selected reconstruction meshes from autoencoders trained by SW, GSW, and H2SW in turn with the number of projections $L=100$ at epoch 500 .

Bunny o mesh with $L=10$ and $L=100$ in Figure 6- 7 in turn. The quantitative result for the Armadillo mesh is given in Table 4. Here, we set the step size to 0.1. From these results, we see that the proposed H2SW gives the best flow deformation flow in general. The performance gap is especially larger when $L=10$ i.e., having a small number of projections.

Deep 3D mesh autoencoder. We first report the neural network architectures that we use in the experiments.

- The encoder: Conv1d(6,64,1) $\rightarrow$ BatchNorm1d $\rightarrow$ LeakyReLU(0.2) $\rightarrow \operatorname{Conv1d}(64,128,1) \rightarrow$ BatchNorm1d $\rightarrow \operatorname{LeakyReLU}(0.2) \rightarrow \operatorname{Conv1d}(128,256,1) \rightarrow$ BatchNorm1d $\rightarrow \operatorname{LeakyReLU}(0.2)$ $\rightarrow$ Conv1d(256, 512, 1) $\rightarrow$ BatchNorm1d $\rightarrow$ LeakyReLU(0.2) $\rightarrow \operatorname{Conv1d(512,~1024,~1)~} \rightarrow$

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=325&width=420&top_left_y=239&top_left_x=213)

SW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=323&width=407&top_left_y=245&top_left_x=623)

CHSW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=319&width=403&top_left_y=247&top_left_x=1029)

H2SW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=322&width=417&top_left_y=243&top_left_x=1426)

W

Figure 9: Cost matrices between datasets from SW, CHSW, and H2SW with $L=100$.

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=325&width=412&top_left_y=762&top_left_x=217)

SW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=333&width=436&top_left_y=766&top_left_x=606)

CHSW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=320&width=425&top_left_y=778&top_left_x=1018)

H2SW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=311&width=399&top_left_y=777&top_left_x=1443)

W

Figure 10: Cost matrices between datasets from SW, CHSW, and H2SW with $L=500$.

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=325&width=415&top_left_y=1282&top_left_x=218)

SW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=333&width=418&top_left_y=1286&top_left_x=623)

CHSW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=320&width=421&top_left_y=1298&top_left_x=1020)

H2SW

![](https://cdn.mathpix.com/cropped/2024_06_04_21542227c0fceb6cd5f1g-23.jpg?height=309&width=396&top_left_y=1298&top_left_x=1447)

W

Figure 11: Cost matrices between datasets from SW, CHSW, and H2SW with $L=1000$.

BatchNorm1d $\rightarrow$ LeakyReLU(0.2) $\rightarrow$ Max-Pooling $\rightarrow$ Linear(1024, 1024).

- The decoder: Linear(1024, 1024) $\rightarrow$ BatchNorm1d $\rightarrow$ LeakyReLU(0.2) $\rightarrow \operatorname{Linear}(1024,2048) \rightarrow$ BatchNorm1d $\rightarrow$ LeakyReLU(0.2) $\rightarrow$ Linear $(2048,4096) \rightarrow$ BatchNorm1d $\rightarrow \operatorname{LeakyReLU}(0.2)$ $\rightarrow$ Linear $\left(2048,2048^{*} 6\right)$. The output of the decoder is the concatenation of the location and normal vector. We normalize the normal vector to the unit-sphere.

As mentioned in the main text, we report the reconstruction of randomly selected meshes for $L=100$ at epoch 500 in Figure 8. We see that the reconstructed meshes at epoch 500 are visually worse than the reconstructed meshes at epoch 2000. Therefore, the joint Wasserstein distances in Table 2 are consistent with the qualitative results.

Dataset Comparison. We follow the same procedure in Section 6.2 in [9]. We refer the reader
to the reference for a detailed description. Here, we show the cross-dataset cost matrices with the number of projections $L=100$ in Figure 9, $L=500$ in Figure 10, and $L=1000$ Figure 11.

## E Computational Infrastructure

For the non-deep-learning experiments, we use a HP Omen 25L desktop for conducting experiments. For 3D mesh autoencoder experiments, we use a single NVIDIA A100 GPU.

## References

[1] D. Alvarez-Melis and N. Fusi. Geometric dataset distances via optimal transport. Advances in Neural Information Processing Systems, 33:21428-21439, 2020. (Cited on pages 2, 4, and 11.)

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pages 214-223, 2017. (Cited on page 1.)

[3] G. Beylkin. The inversion problem and applications of the generalized Radon transform. Communications on pure and applied mathematics, 37(5):579-599, 1984. (Cited on pages 2, 4, 5, and 20.)

[4] B. Bhushan Damodaran, B. Kellenberger, R. Flamary, D. Tuia, and N. Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 447-463, 2018. (Cited on page 2.)

[5] M. T. Boedihardjo. Sharp bounds for the max-sliced wasserstein distance. arXiv preprint arXiv:2403.00666, 2024. (Cited on page 7.)

[6] C. Bonet, P. Berg, N. Courty, F. Septier, L. Drumetz, and M.-T. Pham. Spherical slicedWasserstein. International Conference on Learning Representations, 2023. (Cited on pages 2 and 5.)

[7] C. Bonet, L. Chapel, L. Drumetz, and N. Courty. Hyperbolic sliced-Wasserstein via geodesic and horospherical projections. In Topological, Algebraic and Geometric Learning Workshops 2023, pages 334-370. PMLR, 2023. (Cited on pages 2, 8, and 20.)

[8] C. Bonet, N. Courty, F. Septier, and L. Drumetz. Efficient gradient flows in sliced-Wasserstein space. Transactions on Machine Learning Research, 2022. (Cited on page 2.)

[9] C. Bonet, L. Drumetz, and N. Courty. Sliced-Wasserstein distances and flows on CartanHadamard manifolds. arXiv preprint arXiv:2403.06560, 2024. (Cited on pages 2, 8, 11, and 23.)

[10] C. Bonet, B. Malézieux, A. Rakotomamonjy, L. Drumetz, T. Moreau, M. Kowalski, and N. Courty. Sliced-Wasserstein on symmetric positive definite matrices for $\mathrm{m} / \mathrm{eeg}$ signals. In International Conference on Machine Learning, pages 2777-2805. PMLR, 2023. (Cited on pages 2 and 8.)

[11] N. Bonneel, J. Rabin, G. Peyré, and H. Pfister. Sliced and Radon Wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 1(51):22-45, 2015. (Cited on pages 1 and 3.)

[12] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. (Cited on page 10.)

[13] T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718, 2018. (Cited on page 11.)

[14] G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 international joint conference on neural networks (IJCNN), pages 2921-2926. IEEE, 2017. (Cited on page 11.)

[15] N. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. In Advances in Neural Information Processing Systems, pages 3730-3739, 2017. (Cited on pages 2 and 4.)

[16] B. Curless and M. Levoy. A volumetric method for building complex models from range images. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 303-312, 1996. (Cited on page 9.)

[17] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems, pages 2292-2300, 2013. (Cited on page 1.)

[18] I. Deshpande, Y.-T. Hu, R. Sun, A. Pyrros, N. Siddiqui, S. Koyejo, Z. Zhao, D. Forsyth, and A. G. Schwing. Max-sliced Wasserstein distance and its use for GANs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 10648-10656, 2019. (Cited on page 8.)

[19] I. Deshpande, Z. Zhang, and A. G. Schwing. Generative modeling using the sliced Wasserstein distance. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3483-3491, 2018. (Cited on page 2.)

[20] N. Fournier and A. Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. Probability Theory and Related Fields, 162:707-738, 2015. (Cited on page 1.)

[21] N. Fournier and A. Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. Probability theory and related fields, 162(3):707-738, 2015. (Cited on page 17.)

[22] R. C. Garrett, T. Harris, B. Li, and Z. Wang. Validating climate models with spherical convolutional Wasserstein distance. arXiv preprint arXiv:2401.14657, 2024. (Cited on page 8.)

[23] E. Heitz, K. Vanhoey, T. Chambon, and L. Belcour. A sliced Wasserstein loss for neural texture synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9412-9420, 2021. (Cited on page 2.)

[24] S. Helgason. The Radon transform on r n. In Integral Geometry and Radon Transforms, pages 1-62. Springer, 2011. (Cited on pages 2, 3, and 5.)

[25] J. J. Hull. A database for handwritten text recognition research. IEEE Transactions on pattern analysis and machine intelligence, 16(5):550-554, 1994. (Cited on page 11.)

[26] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction. In Proceedings of the fourth Eurographics symposium on Geometry processing, volume 7, page 0, 2006. (Cited on page 5.)

[27] S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. Rohde. Generalized sliced Wasserstein distances. In Advances in Neural Information Processing Systems, pages 261-272, 2019. (Cited on pages 2 and 4.)

[28] S. Kolouri, G. K. Rohde, and H. Hoffmann. Sliced Wasserstein distance for learning Gaussian mixture models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3427-3436, 2018. (Cited on page 2.)

[29] P. Kuchment. Generalized transforms of Radon type and their applications. In Proceedings of Symposia in Applied Mathematics, volume 63, page 67, 2006. (Cited on pages 2, 4, and 5.)

[30] T. Le, K. Nguyen, S. Sun, K. Han, N. Ho, and X. Xie. Diffeomorphic mesh deformation via efficient optimal transport for cortical surface reconstruction. International Conference on Learning Representations, 2024. (Cited on page 2.)

[31] T. Le, K. Nguyen, S. Sun, N. Ho, and X. Xie. Integrating efficient optimal transport and functional maps for unsupervised shape correspondence learning. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. (Cited on page 2.)

[32] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. (Cited on page 11.)

[33] C.-Y. Lee, T. Batra, M. H. Baig, and D. Ulbricht. Sliced Wasserstein discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10285-10295, 2019. (Cited on page 2.)

[34] Z.-P. Liang and D. C. Munson. Partial Radon transforms. IEEE transactions on image processing, 6(10):1467-1469, 1997. (Cited on page 5.)

[35] A. Liutkus, U. Simsekli, S. Majewski, A. Durmus, and F.-R. Stöter. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In International Conference on Machine Learning, pages 4104-4113. PMLR, 2019. (Cited on page 2.)

[36] M. Luong, K. Nguyen, N. Ho, R. Haf, D. Phung, and L. Qu. Revisiting deep audio-text retrieval through the lens of transportation. In The Twelfth International Conference on Learning Representations, 2024. (Cited on page 1.)

[37] T. Manole, S. Balakrishnan, and L. Wasserman. Minimax confidence intervals for the sliced Wasserstein distance. Electronic Journal of Statistics, 16(1):2252-2345, 2022. (Cited on page 7.)

[38] K. Nadjahi, V. De Bortoli, A. Durmus, R. Badeau, and U. Şimşekli. Approximate Bayesian computation with the sliced-Wasserstein distance. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5470-5474. IEEE, 2020. (Cited on page 2.)

[39] K. Nadjahi, A. Durmus, L. Chizat, S. Kolouri, S. Shahrampour, and U. Simsekli. Statistical and topological properties of sliced probability divergences. Advances in Neural Information Processing Systems, 33:20802-20812, 2020. (Cited on page 7.)

[40] K. Nguyen, N. Bariletto, and N. Ho. Quasi-monte carlo for 3d sliced Wasserstein. In The Twelfth International Conference on Learning Representations, 2024. (Cited on page 7.)

[41] K. Nguyen and N. Ho. Revisiting sliced Wasserstein on images: From vectorization to convolution. Advances in Neural Information Processing Systems, 2022. (Cited on pages 8 and 20.)

[42] K. Nguyen and N. Ho. Energy-based sliced Wasserstein distance. Advances in Neural Information Processing Systems, 2023. (Cited on page 8.)

[43] K. Nguyen, N. Ho, T. Pham, and H. Bui. Distributional sliced-Wasserstein and applications to generative modeling. In International Conference on Learning Representations, 2021. (Cited on page 8.)

[44] K. Nguyen, T. Ren, H. Nguyen, L. Rout, T. Nguyen, and N. Ho. Hierarchical sliced Wasserstein distance. International Conference on Learning Representations, 2023. (Cited on page 20.)

[45] S. Nietert, R. Sadhu, Z. Goldfeld, and K. Kato. Statistical, robustness, and computational guarantees for sliced Wasserstein distances. Advances in Neural Information Processing Systems, 2022. (Cited on page 7.)

[46] O. Pele and M. Werman. Fast and robust earth mover's distances. In 2009 IEEE 12th International Conference on Computer Vision, pages 460-467. IEEE, September 2009. (Cited on page 1.)

[47] S. Peng, C. Jiang, Y. Liao, M. Niemeyer, M. Pollefeys, and A. Geiger. Shape as points: A differentiable poisson solver. Advances in Neural Information Processing Systems, 34:1303213044, 2021. (Cited on pages 5 and 10.)

[48] G. Peyré and M. Cuturi. Computational optimal transport, 2020. (Cited on pages 1, 3, and 14.)

[49] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652-660, 2017. (Cited on pages 10 and 11.)

[50] M. Quellmalz, R. Beinert, and G. Steidl. Sliced optimal transport on the sphere. arXiv preprint arXiv:2304.09092, 2023. (Cited on pages 2 and 5.)

[51] M. Quellmalz, L. Buecher, and G. Steidl. Parallelly sliced optimal transport on spheres and on the rotation group. arXiv preprint arXiv:2401.16896, 2024. (Cited on pages 2 and 5.)

[52] F. Rouviere. Nonlinear Radon and Fourier transforms, 2015. (Cited on pages 2 and 4.)

[53] R. M. Rustamov and S. Majumdar. Intrinsic sliced Wasserstein distances for comparing collections of probability distributions on manifolds and graphs. In International Conference on Machine Learning, pages 29388-29415. PMLR, 2023. (Cited on page 2.)

[54] I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. In International Conference on Learning Representations, 2018. (Cited on page 1.)

[55] H. Tran, Y. Bai, A. Kothapalli, A. Shahbazi, X. Liu, R. D. Martin, and S. Kolouri. Stereographic spherical sliced Wasserstein distances. arXiv preprint arXiv:2402.02345, 2024. (Cited on pages 2 and 5.)

[56] C. Villani. Optimal transport: Old and New. Springer, 2008. (Cited on page 1.)

[57] C. Villani. Optimal transport: old and new, volume 338. Springer Science \& Business Media, 2008. (Cited on page 3.)

[58] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. (Cited on page 11.)

[59] J. Xu, H. Zhou, C. Gan, Z. Zheng, and L. Li. Vocabulary learning via optimal transport for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7361-7373, 2021. (Cited on page 1.)

[60] M. Yi and S. Liu. Sliced Wasserstein variational inference. In Fourth Symposium on Advances in Approximate Bayesian Inference, 2021. (Cited on page 2.)

[61] Q.-Y. Zhou, J. Park, and V. Koltun. Open3D: A modern library for 3D data processing. arXiv:1801.09847, 2018. (Cited on page 9.)


[^0]:    ${ }^{1}$ https://github.com/clbonet/Sliced-Wasserstein_Distances_and_Flows_on_Cartan-Hadamard_Manifolds/ blob/0eb05450e7f9f27586d0ddb1ce6e58f07eb75786/Experiments/xp_otdd/OTDD_SW.ipynb

