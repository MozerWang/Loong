# First Analysis of Local GD on Heterogeneous Data 

Ahmed Khaled*<br>Cairo University<br>akregeb@gmail.com

Konstantin Mishchenko<br>$\mathrm{KAUST}^{\dagger}$<br>konstantin.mishchenko@kaust.edu.sa

Peter Richt√°rik<br>KAUST<br>peter.richtarik@kaust.edu.sa


#### Abstract

We provide the first convergence analysis of local gradient descent for minimizing the average of smooth and convex but otherwise arbitrary functions. Problems of this form and local gradient descent as a solution method are of importance in federated learning, where each function is based on private data stored by a user on a mobile device, and the data of different users can be arbitrarily heterogeneous. We show that in a low accuracy regime, the method has the same communication complexity as gradient descent.


## 1 Introduction

We are interested in solving the optimization problem

$$
\begin{equation*}
\min _{x \in \mathbb{R}^{d}}\left\{f(x) \stackrel{\text { def }}{=} \frac{1}{M} \sum_{m=1}^{M} f_{m}(x)\right\} \tag{1}
\end{equation*}
$$

which is arises in training of supervised machine learning models. We assume that each $f_{m}: \mathbb{R}^{d} \rightarrow$ $\mathbb{R}$ is an $L$-smooth and convex function and we denote by $x_{*}$ a fixed minimizer of $f$.

Our main interest is in situations where each function is based on data available on a single device only, and where the data distribution across the devices can be arbitrarily heterogeneous. This situation arises in federated learning, where machine learning models are trained on data available on consumer devices, such as mobile phones. In federated learning, transfer of local data to a single data center for centralized training is prohibited due to privacy reasons, and frequent communication is undesirable as it is expensive and intrusive. Hence, several recent works aim at constructing new ways of solving (1) in a distributed fashion with as few communication rounds as possible.

Large-scale problems are often solved by first-order methods as they have proved to scale well with both dimension and data size. One attractive choice is Local Gradient Descent, which divides the optimization process into epochs. Each epoch starts by communication in the form of a model averaging step across all $M$ devices ${ }^{3}$ The rest of each epoch does not involve any communication, and is devoted to performing a fixed number of gradient descent steps initiated from the average model, and based on the local functions, performed by all $M$ devices independently in parallel. See Algorithm 1 for more details.[^0]

```
Algorithm 1 Local Gradient Descent
Input: Stepsize $\gamma>0$, synchronization/communication times $0=t_{0} \leqslant t_{1} \leqslant t_{2} \leqslant \ldots$, initial
    vector $x_{0} \in \mathbb{R}^{d}$
    Initialize $x_{0}^{m}=x_{0}$ for all $m \in[M] \stackrel{\text { def }}{=}\{1,2, \ldots, M\}$
    for $t=0,1, \ldots$ do
        for $m=1, \ldots, M$ do
            $x_{t+1}^{m}= \begin{cases}\frac{1}{M} \sum_{j=1}^{M}\left(x_{t}^{j}-\gamma \nabla f_{j}\left(x_{t}^{j}\right)\right), & \text { if } t=t_{p} \text { for some } p \in\{1,2, \ldots\} \\ x_{t}^{m}-\gamma \nabla f_{m}\left(x_{t}^{m}\right), & \text { otherwise. }\end{cases}$
        end for
    end for
```

The stochastic version of this method is at the core of the Federated Averaging algorithm which has been used recently in federated learning applications, see e.g. [7, 10]. Essentially, Federated Averaging is a variant of local Stochastic Gradient Descent (SGD) with participating devices sampled randomly. This algorithm has been used in several machine learning applications such as mobile keyboard prediction [5], and strategies for improving its communication efficiency were explored in [7]. Despite its empirical success, little is known about convergence properties of this method and it has been observed to diverge when too many local steps are performed [10]. This is not so surprising as the majority of common assumptions are not satisfied; in particular, the data is typically very non-i.i.d. [10], so the local gradients can point in different directions. This property of the data can be written for any vector $x$ and indices $i, j$ as

$$
\left\|\nabla f_{i}(x)-\nabla f_{j}(x)\right\| \gg 1
$$

Unfortunately, it is very hard to analyze local methods without assuming a bound on the dissimilarity of $\nabla f_{i}(x)$ and $\nabla f_{j}(x)$. For this reason, almost all prior work assumed bounded dissimilarity [8, 16, 17, 18] and addressed other less challenging aspects of federated learning such as decentralized communication, nonconvexity of the objective or unbalanced data partitioning. In fact, a common way to make the analysis simple is to assume Lipschitzness of local functions, $\left\|\nabla f_{i}(x)\right\| \leqslant G$ for any $x$ and $i$. We argue that this assumption is pathological and should be avoided when seeking a meaningful convergence bound. First of all, in unconstrained strongly convex minimization this assumption cannot be satisfied, making the analysis in works like [14] questionable. Second, there exists at least one method, whose convergence is guaranteed under bounded gradients [6], but in practice the method diverges [3, 12].

Finally, under the bounded gradients assumption we have

$$
\begin{equation*}
\left\|\nabla f_{i}(x)-\nabla f_{j}(x)\right\| \leqslant\left\|\nabla f_{i}(x)\right\|+\left\|\nabla f_{j}(x)\right\| \leqslant 2 G \tag{2}
\end{equation*}
$$

In other words, we lose control over the difference between the functions. Since $G$ bounds not just dissimilarity, but also the gradients themselves, it makes the statements less insightful or even vacuous. For instance, it is not going to be tight if the data is actually i.i.d. since $G$ in that case will remain a positive constant. In contrast, we will show that the rate should depend on a much more meaningful quantity,

$$
\sigma^{2} \stackrel{\text { def }}{=} \frac{1}{M} \sum_{m=1}^{M}\left\|\nabla f_{m}\left(x_{*}\right)\right\|^{2}
$$

where $x_{*}$ is a minimizer of $f$. Obviously, $\sigma$ is always finite and it serves as a natural measure of variance in local methods. On top of that, it allows us to obtain bounds that are tight in case the data is actually i.i.d. We note that an attempt to get more general convergence statement has been made in [13], but sadly their guarantee is strictly worse than that of minibatch Stochastic Gradient Descent (SGD), making their theoretical contribution smaller.

We additionally note that the bound in the mentioned work [8] not only uses bounded gradients, but also provides a pessimistic $\mathcal{O}\left(H^{2} / T\right)$ rate, where $H$ is the number of local steps in each epoch, and $T$ is the total number of steps of the method. Indeed, this requires $H$ to be $\mathcal{O}(1)$ to make the

where averaging is done across all devices. We focus on this simpler situation first as even this is not currently understood theoretically.
rate coincide with that of SGD for strongly convex functions. The main contribution of that work, therefore, is in considering partial participation as in Federated Averaging.

When the data is identically distributed and stochastic gradients are used instead of full gradients on each node, the resulting method has been explored extensively in the literature under different names, see e.g. [1, 14, 15, 19]. [11] proposed an asynchronous local method that converges to the exact solution without decreasing stepsizes, but its benefit from increasing $H$ is limited by constant factors. [9] seems to be the first work to propose a local method, but no rate was shown in that work.

## 2 Convergence of Local GD

### 2.1 Assumptions and notation

Before introducing our main result, let us first formulate explicitly our assumptions.

Assumption 1. The set of minimizers of (1) is nonempty. Further, for every $m \in[M] \stackrel{\text { def }}{=}$ $\{1,2, \ldots, M\}, f_{m}$ is convex and $L$-smooth. That is, for all $x, y \in \mathbb{R}^{d}$ the following inequalities are satisfied:

$$
0 \leqslant f_{m}(x)-f_{m}(y)-\left\langle\nabla f_{m}(y), x-y\right\rangle \leqslant \frac{L}{2}\|x-y\|^{2}
$$

Further, we assume that Algorithm 1 is run with a bounded synchronization interval. That is, we assume that

$$
H \stackrel{\text { def }}{=} \max _{p \geqslant 0}\left|t_{p}-t_{p+1}\right|
$$

is finite. Given local vectors $x_{t}^{1}, x_{t}^{2}, \ldots, x_{t}^{M} \in \mathbb{R}^{d}$, we define the average iterate, iterate variance and average gradient at time $t$ as

$$
\begin{equation*}
\hat{x}_{t} \stackrel{\text { def }}{=} \frac{1}{M} \sum_{m=1}^{M} x_{t}^{m} \quad V_{t} \stackrel{\text { def }}{=} \frac{1}{M} \sum_{m=1}^{M}\left\|x_{t}^{m}-\hat{x}_{t}\right\|^{2} \quad g_{t} \stackrel{\text { def }}{=} \frac{1}{M} \sum_{m=1}^{M} \nabla f_{m}\left(x_{t}^{m}\right) \tag{3}
\end{equation*}
$$

respectively. The Bregman divergence with respect to $f$ is defined via

$$
D_{f}(x, y) \stackrel{\text { def }}{=} f(x)-f(y)-\langle\nabla f(y), x-y\rangle
$$

Note that in the case $y=x_{*}$, we have $D_{f}\left(x, x_{*}\right)=f(x)-f\left(x_{*}\right)$.

### 2.2 Analysis

The first lemma enables us to find a recursion on the optimality gap for a single step of local GD:

Lemma 1. Under Assumption 1 and for any $\gamma \geqslant 0$ we have

$$
\begin{equation*}
\left\|r_{t+1}\right\|^{2} \leqslant\left\|r_{t}\right\|^{2}+\gamma L(1+2 \gamma L) V_{t}-2 \gamma(1-2 \gamma L) D_{f}\left(\hat{x}_{t}, x_{*}\right) \tag{4}
\end{equation*}
$$

where $r_{t} \stackrel{\text { def }}{=} \hat{x}_{t}-x_{*}$. In particular, if $\gamma \leqslant \frac{1}{4 L}$, then $\left\|r_{t+1}\right\|^{2} \leqslant\left\|r_{t}\right\|^{2}+\frac{3}{2} \gamma L V_{t}-\gamma D_{f}\left(\hat{x}_{t}, x_{*}\right)$.

We now bound the sum of the variances $V_{t}$ over an epoch. An epoch-based bound is intuitively what we want since we are only interested in the points $\hat{x}_{t_{p}}$ produced at the end of each epoch.

Lemma 2. Suppose that Assumption 1 holds and let $p \in \mathbb{N}$, define $v=t_{p+1}-1$ and suppose Algorithm 1 is run with a synchronization interval $H \geqslant 1$ and a constant stepsize $\gamma>0$ such that $\gamma \leqslant \frac{1}{4 L H}$. Then the following inequalities hold:

$$
\begin{aligned}
& \sum_{t=t_{p}}^{v} V_{t} \leqslant 5 L \gamma^{2} H^{2} \sum_{i=t_{p}}^{v} D_{f}\left(\hat{x}_{i}, x_{*}\right)+\sum_{i=t_{p}}^{v} 8 \gamma^{2} H^{2} \sigma^{2} \\
& \sum_{t=t_{p}}^{v} \frac{3}{2} L V_{t}-D_{f}\left(\hat{x}_{t}, x_{*}\right) \leqslant-\frac{1}{2} \sum_{t=t_{p}}^{v} D_{f}\left(\hat{x}_{i}, x_{*}\right)+\sum_{t=t_{p}}^{v} 12 L \gamma^{2} H^{2} \sigma^{2}
\end{aligned}
$$

Combining the previous two lemmas, the convergence of local GD is established in the next theorem:

Theorem 1. For local GD run with a constant stepsize $\gamma>0$ such that $\gamma \leqslant \frac{1}{4 L H}$ and under Assumption 1, we have

$$
\begin{equation*}
f\left(\bar{x}_{T}\right)-f\left(x_{*}\right) \leqslant \frac{2\left\|x_{0}-x_{*}\right\|^{2}}{\gamma T}+24 \gamma^{2} \sigma^{2} H^{2} L \tag{5}
\end{equation*}
$$

where $\bar{x}_{T} \stackrel{\text { def }}{=} \frac{1}{T} \sum_{t=0}^{T-1} \hat{x}_{t}$.

### 2.3 Local GD vs GD

In order to interpret the above bound, we may ask: how many communication rounds are sufficient to guarantee $f\left(\bar{x}_{T}\right)-f\left(x_{*}\right) \leqslant \epsilon$ ? To answer this question, we need to minimize $\frac{T}{H}$ subject to the constraints $0<\gamma \leqslant \frac{1}{4 L}, \frac{2\left\|x_{0}-x_{*}\right\|^{2}}{\gamma T} \leqslant \frac{\epsilon}{2}$, and $24 \gamma^{2} \sigma^{2} H^{2} L \leqslant \frac{\epsilon}{2}$, in variables $T, H$ and $\gamma$. We can easily deduce from the constraints that

$$
\begin{equation*}
\frac{T}{H} \geqslant \frac{16\left\|x_{0}-x_{*}\right\|^{2}}{\epsilon} \max \left\{L, \sigma \sqrt{\frac{3 L}{\epsilon}}\right\} \tag{6}
\end{equation*}
$$

On the other hand, this lower bound is achieved by any $0<\gamma \leqslant \frac{1}{4 L}$ as long as we pick

$$
T=T(\gamma) \stackrel{\text { def }}{=} \frac{4\left\|x_{0}-x_{*}\right\|^{2}}{\epsilon \gamma} \quad \text { and } \quad H=H(\gamma) \stackrel{\text { def }}{=} \frac{1}{4 \max \left\{L, \sigma \sqrt{\frac{3 L}{\epsilon}}\right\} \gamma}
$$

The smallest $H$ achieving this lower bound is $H\left(\frac{1}{4 L}\right)=\min \left\{1, \sqrt{\frac{\epsilon L}{3 \sigma^{2}}}\right\}$.

Further, notice that as long as the target accuracy is not too high, in particular $\epsilon \geqslant \frac{3 \sigma^{2}}{L}$, then $\max \{L, \sigma \sqrt{3 L / \epsilon}\}=L$ and (6) says that the number of communications of local GD (with parameters set as $H=H(\gamma)$ and $T=T(\gamma))$ is equal to

$$
\frac{T}{H}=\mathcal{O}\left(\frac{L\left\|x_{0}-x_{*}\right\|^{2}}{\epsilon}\right)
$$

which is the same as the number of iterations (i.e., communications) of gradient descent. If $\epsilon<\frac{3 \sigma^{2}}{L}$, then (6) gives the communication complexity

$$
\frac{T}{H}=\mathcal{O}\left(\frac{\sqrt{L} \sigma}{\epsilon^{3 / 2}}\right)
$$

### 2.4 Local GD vs Minibatch SGD

Equation (5) shows a clear analogy between the convergence of local GD and the convergence rate of minibatch SGD, establishing a $1 / T$ convergence to a neighborhood depending on the expected noise at the optimum $\sigma^{2}$, which measures how dissimilar the functions $f_{m}$ are from each other at the optimum $x_{*}$.

The analogy between SGD and local GD extends further to the convergence rate, as the next corollary shows:

Corollary 1. Choose $H$ such that $H \leqslant \frac{\sqrt{T}}{\sqrt{M}}$, then $\gamma=\frac{\sqrt{M}}{4 L \sqrt{T}} \leqslant \frac{1}{4 H L}$, and hence applying the result of the previous theorem

$$
f\left(\bar{x}_{T}\right)-f\left(x_{*}\right) \leqslant \frac{8 L\left\|x_{0}-x_{*}\right\|^{2}}{\sqrt{M T}}+\frac{3 M \sigma^{2} H^{2}}{2 L T}
$$

To get a convergence rate of $1 / \sqrt{M T}$ we can choose $H=O\left(T^{1 / 4} M^{-3 / 4}\right)$, which implies a total number of $\Omega\left(T^{3 / 4} M^{3 / 4}\right)$ communication steps. If a rate of $1 / \sqrt{T}$ is desired instead, we can choose a larger $H=O\left(T^{1 / 4}\right)$.

## 3 Experiments

To verify the theory, we run our experiments on logistic regression with $\ell_{2}$ regularization and datasets taken from the LIBSVM library [2]. We use a machine with 24 Intel(R) Xeon(R) Gold 6146 CPU @ 3.20GHz cores and we handle communication via the MPI for Python package [4].

Since our architecture leads to a very specific trade-off between computation and communication, we also provide plots for the case the communication time relative to gradient computation time is higher or lower. In all experiments, we use full gradients $\nabla f_{m}$ and constant stepsize $\frac{1}{L}$. The amount of $\ell_{2}$ regularization was chosen of order $\frac{1}{n}$, where $n$ is the total amount of data. The data partitioning is not i.i.d. and is done based on the index in the original dataset.

We observe a very tight match between our theory and numerical results. In cases where communication is significantly more expensive than gradient computation, local methods are much faster for imprecise convergence. This was not a big advantage though with our architecture, mainly because full gradients took a lot of time to be computed.
![](https://cdn.mathpix.com/cropped/2024_06_04_78e00d735d4b42453915g-05.jpg?height=320&width=1378&top_left_y=867&top_left_x=362)

Figure 1: Convergence of local GD methods with different number of local steps on the 'a5a' dataset. 1 local step corresponds to fully synchronized gradient descent and it is the only method that converges precisely to the optimum. The left plot shows convergence in terms of communication rounds, showing a clear advantage of local GD when only limited accuracy is required. The mid plot, however, illustrates that wall-clock time might improve only slightly and the right plot shows what changes with different communication cost.
![](https://cdn.mathpix.com/cropped/2024_06_04_78e00d735d4b42453915g-05.jpg?height=322&width=1378&top_left_y=1487&top_left_x=362)

Figure 2: Same experiment as in Figure 3, performed on the 'mushrooms' dataset.

## References

[1] Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification, and Local Computations. arXiv:1906.02367, 2019.

[2] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27, 2011.

[3] Tatjana Chavdarova, Gauthier Gidel, Fran√ßois Fleuret, and Simon Lacoste-Julien. Reducing Noise in GAN Training with Variance Reduced Extragradient. arXiv preprint arXiv:1904.08598, 2019.

[4] Lisandro D. Dalcin, Rodrigo R. Paz, Pablo A. Kler, and Alejandro Cosimo. Parallel distributed computing using Python. Advances in Water Resources, 34(9):1124-1139, 2011.

[5] Andrew Hard, Kanishka Rao, Rajiv Mathews, Fran√ßoise Beaufays, Sean Augenstein, Hubert Eichner, Chlo√© Kiddon, and Daniel Ramage. Federated Learning for Mobile Keyboard Prediction. arXiv:1811.03604, 2018.

[6] Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational Inequalities with Stochastic Mirror-Prox algorithm. Stochastic Systems, 1(1):17-58, 2011.

[7] Jakub Koneƒçn√Ω, H. Brendan McMahan, Felix X. Yu, Peter Richt√°rik, Ananda Theertha Suresh, and Dave Bacon. Federated Learning: Strategies for Improving Communication Efficiency. In NIPS Private Multi-Party Machine Learning Workshop, 2016.

[8] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the Convergence of FedAvg on Non-IID Data. arXiv:1907.02189, 2019.

[9] L. Mangasarian. Parallel Gradient Distribution in Unconstrained Optimization. SIAM Journal on Control and Optimization, 33(6):1916-1925, 1995.

[10] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag√ºera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. Proceedings of the 20 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017. JMLR: W\&CP volume 54, 2016.

[11] Konstantin Mishchenko, Franck Iutzeler, J√©r√¥me Malick, and Massih-Reza Amini. A Delaytolerant Proximal-Gradient Algorithm for Distributed Learning. In International Conference on Machine Learning, pages 3584-3592, 2018.

[12] Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richt√°rik, and Yura Malitsky. Revisiting Stochastic Extragradient. arXiv preprint arXiv:1905.11373, 2019.

[13] Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith. On the Convergence of Federated Optimization in Heterogeneous Networks. arXiv:1812.06127, 2018.

[14] Sebastian U. Stich. Local SGD Converges Fast and Communicates Little. arXiv:1805.09767, 2018 .

[15] Jianyu Wang and Gauri Joshi. Cooperative SGD: A Unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms. arXiv:1808.07576, 2018.

[16] Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K. Leung, Christian Makaya, Ting He, and Kevin Chan. When Edge Meets Learning: Adaptive Control for Resource-Constrained Distributed Machine Learning. arXiv:1804.05271, 2018.

[17] Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning. arXiv:1807.06629, 2018.

[18] Hao Yu, Rong Jin, and Sen Yang. On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization. arXiv preprint arXiv:1905.03817, 2019.

[19] Fan Zhou and Guojing Cong. On the Convergence Properties of a $k$-step Averaging Stochastic Gradient Descent Algorithm for Nonconvex Optimization. In IJCAI International Joint Conference on Artificial Intelligence, volume 2018-July, pages 3219-3227, 2018.
