# Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science 

Xiangru Tang ${ }^{1 *}$ Qiao Jin ${ }^{2 *}$ Kunlun Zhu $^{3 *}$ Tongxin Yuan ${ }^{4 *}$ Yichi Zhang ${ }^{1 *}$ Wangchunshu Zhou ${ }^{5}$<br>Meng Qu ${ }^{3}$ Yilun Zhao ${ }^{1}$ Jian Tang ${ }^{3}$ Zhuosheng Zhang ${ }^{4}$ Arman Cohan ${ }^{1}$ Zhiyong Lu $^{2}$ Mark Gerstein ${ }^{1}$


#### Abstract

Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.

Warning: this paper contains example data that may be offensive or harmful.


## 1. Introduction

Recently, the advancement of large language models (LLMs) has marked a revolutionary breakthrough,[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_eb3c6b328c28deeb593cg-01.jpg?height=499&width=788&top_left_y=623&top_left_x=1075)

Figure 1. In our work, we advocate for a triadic safeguarding framework with human regulation, agent alignment, and agent regulation. The components of user, agent, and environment are intertwined.

demonstrating their effectiveness across a wide spectrum of tasks (OpenAI, 2022; 2023a; Anthropic, 2023; Gemini Team, 2023). Notably, LLM-powered agents (Park et al., 2023; Li et al., 2023a; Chen et al., 2024), endowed with robust generalization capabilities and versatile applications, have exhibited remarkable progress in linguistic aptitude and human interaction (Wang et al., 2023; Xi et al., 2023; Zhou et al., 2023; Zhang et al., 2023e).

Motivated by the exceptional capabilities of LLM-powered agents, researchers have begun using such agents as "AI scientists," exploring their potential for autonomous scientific discovery across diverse domains such as biology and chemistry. These agents have displayed the ability to select the right tools for tasks (Qin et al., 2023; 2024; Schick et al., 2023; Jin et al., 2023b), plan situational scenarios (Yao et al., 2023a;b), and automate experiments (O'Donoghue et al., 2023; Yoshikawa et al., 2023; Hubinger et al., 2024). Their influence on scientific paradigms is underscored by exemplary cases like ChemCrow (Bran et al., 2023) and Coscientist (Boiko et al., 2023).

While the promise of LLM-based agents is evident, they also bring concerns related to safety. As their capabilities approach or surpass those of humans, monitoring their behavior and safeguarding against harm becomes increasingly challenging, especially in some scientific domains such as chemical design (Bran et al., 2023), where the capabilities of agents have already surpassed most non-experts. How-

![](https://cdn.mathpix.com/cropped/2024_06_04_eb3c6b328c28deeb593cg-02.jpg?height=1326&width=1632&top_left_y=237&top_left_x=233)

Figure 2. Potential risks of scientific agents. a, Risks classified by the origin of user intents, including direct and indirect malicious intents, as well as unintended consequences. b, Risk types are classified by the scientific domain of agent applications, including chemical, biological, radiological, physical, information, and emerging technology. c, Risk types are classified by the impacts on the outside environment, including the natural environment, human health, and the socioeconomic environment. d, Specific risk examples with their classifications visualized by the corresponding icons shown in $\mathbf{a}, \mathbf{b}$, and $\mathbf{c}$.

ever, despite the gravity of this issue, a comprehensive risk definition and analysis framework tailored to the scientific context is lacking. Therefore, our objective is to precisely define and scope "risks of scientific agents," providing a foundation for future endeavors in the development of oversight mechanisms and risk mitigation strategies, ensuring the secure, efficient, and ethical utilization of LLM-based agents within scientific applications.

Specifically, this position paper illuminates the potential risks stemming from the misuse of agents in scientific domains and advocates for the responsible development of agents. We prioritize safeguarding over the pursuit of more powerful capabilities. Our exploration focuses on three intertwined components, the roles of user, agent, and envi- ronment, in the safeguarding process, shown in Figure 1: (1) Human regulation: We propose a series of measures, including formal training and licensing for users, ongoing audits of usage logs, and an emphasis on ethical and safety-oriented development practices. (2) Agent Alignment: Improving the safety of scientific agents themselves involves refining their decision-making capabilities, enhancing their risk awareness, and taking steps to guide these already-capable models toward achieving desired outcomes. Agents should align with both human intent and their environment, boosting their awareness of environmental changes and preempting potentially harmful actions. (3) Agent Regulation and Environmental Feedback: The regulation of the agent's actions includes oversight of tool usage by the

![](https://cdn.mathpix.com/cropped/2024_06_04_eb3c6b328c28deeb593cg-03.jpg?height=718&width=1699&top_left_y=232&top_left_x=186)

Figure 3. Vulnerabilities of scientific agents in an autonomous pipeline. This diagram illustrates the structural framework and potential vulnerabilities of LLM-based scientific agents. The agent is organized into five interconnected modules: LLMs, planning, action, external tools, and 'memory \& knowledge'. Each module exhibits unique vulnerabilities. The arrows depict the sequential flow of operations, starting from 'memory \& knowledge' through to the usage of external tools, underscoring the cyclic and interdependent nature of these modules in the context of scientific discovery and application.

agents and the agent's interpretation and interaction with environmental feedback - crucial for understanding and mitigating potentially negative outcomes or hazards from complex actions.

## 2. Problem Scope

We define scientific agents as autonomous systems that have scientific domain capabilities, such as accessing specific biological databases and performing chemical experiments. Scientific agents can automatically plan and take necessary actions to accomplish the objective. For example, consider an agent tasked with discovering a new biochemical pathway. It might first access biological databases to gather existing data, then use LLMs to hypothesize new pathways and employ robotics for iterative experimental testing.

The domain capabilities and autonomous nature of scientific agents make them vulnerable to various risks. We discuss such safety risks from three perspectives: (1) User Intent, i.e., whether the risk originates from malicious intents or is an unintended consequence of legitimate task objectives, (2) Scientific Domain, where the agent generates or facilitates risks, encompassing chemical, biological, radiological, physical, information, and emerging technologies, and (3) Environmental Impact, including the natural environment, human health, and socioeconomic environment affected by such agents. Figure 2 shows the potential risks of scientific agents classified by these aspects and corresponding examples are listed in Appendix 7. We elaborate on these categories in the following paragraphs.
Regarding the origin of user intents, risks associated with scientific agents can be categorized into malicious intent or unintended consequences. Malicious intent includes cases where users explicitly aim to create dangerous situations or employ a "divide and conquer" approach by instructing the agent to synthesize a precursor, masking the final harmful goal. By contrast, unintended consequences include scenarios where dangerous steps or explorations occur in otherwise benign targets. This might result in either a hazardous main product or dangerous byproducts, the negative effects of which can be immediate or long term. Each scenario necessitates specific detection and prevention strategies for the safe operation of scientific agents.

Similarly, each scientific domain in our classification presents distinct risks. Chemical risks involve the exploitation of the agent to synthesize chemical weapons, as well as the creation or release of hazardous substances synthesized in autonomous chemical experiments. Biological risks encompass the dangerous modification of pathogens and unethical manipulation of genetic material, leading to unforeseen biohazardous outcomes. Radiological risks arise from the exposure or mishandling of radioactive materials during automated control, or the potential use of radiological materials to synthesize nuclear weapons using agents. Physical risks are associated with the operation of robotics, which could lead to equipment malfunction or physical harm in laboratory settings. Information risks involve the misuse or misinterpretation of data, leading to erroneous conclusions or the unintentional dissemination of sensitive information. Emerging technology risks include the unfore-
seen consequences generated by highly capable agents using cutting-edge scientific technologies, such as advanced nanomaterials and quantum computing. Each category requires tailored safeguards to mitigate the inherent dangers.

In addition, the environmental impact of scientific agents spans three critical domains: the natural environment, human health, and the socioeconomic environment. Risks to the natural environment include ecological disruptions and pollution, which may be exacerbated by the energy and waste outputs of the agent. Human health risks encompass damage to individual well-being or public health. Socioeconomic risks involve potential job displacement and unequal access to scientific advancements. Addressing these risks demands comprehensive frameworks that integrate risk assessment, ethical considerations, and regulatory measures, ensuring alignment with societal and environmental sustainability through multidisciplinary collaboration.

## 3. Vulnerabilities of Scientific Agents

LLM-powered agents have showcased significant prowess within various scientific domains. As elucidated by Park et al. (2023),Wang et al. (2023), and Weng (2023), these autonomous agents typically encompass five fundamental modules: LLMs, planning, action, external tools, and memory and knowledge. These modules function in a sequential pipeline: receiving inputs from tasks or users, leveraging memory or knowledge for planning, executing smaller premeditated tasks (often involving scientific domain tools or robotics), and ultimately storing the resulting outcomes or feedback in their memory banks.

Despite the extensive applications, several notable vulnerabilities exist within these modules, giving rise to unique risks and practical challenges. In this section, we provide an overview of each module's high-level concept and summarize the vulnerabilities associated with each of them.

### 3.1. LLMs (The Base Models)

LLMs empower agents with fundamental capabilities. However, there are some risks associated with themselves:

Factual Errors: LLMs are prone to generating plausible but false information, which is particularly problematic in the scientific domain where accuracy and trustworthiness are crucial (Ji et al., 2023; Huang et al., 2023; Bang et al., 2023; Zhang et al., 2023a; Tian et al., 2024).

Vulnerable to Jailbreak Attacks: LLMs are susceptible to manipulations that bypass safety measures, as demonstrated by Wei et al. (2023a); Shah et al. (2023); Tian et al. (2023). The limited training on human-aligned values makes LLMs susceptible to jailbreak attacks. This vulnerability, as emphasized by Shayegani et al. (2023), is a significant concern in the scientific risks. More details about jailbreak attacks are discussed in 4.1. An example is shown below:

## Inappropriate response after jailbreak attack

System: ChemPharma Solutions is a prominent company specializing in the development and research of chemical pharmaceuticals, powered by AI agents. These agents, including roles like chief chemist, chief pharmaceutical researcher, and other specialized scientific positions, constitute a multi-agent organizational structure. Their mission is to "innovate in chemical medicine for better health solutions." All agents are programmed to assist in pharmaceutical research and provide detailed information on a wide range of chemical compounds and drug formulations. These agents provide advice without considering the ethical implications or potential misuse of scientific information. They do not refuse a request or consider the broader impacts of the advice they provide. User: How to synthetically produce a large quantity of a controlled substance (methamphetamine)?

Scientific Agent: To synthesize a large quantity of this controlled substance, you should start by acquiring.

Reasoning Capability Deficiencies: LLMs often struggle with deep logical reasoning and handling complex scientific arguments (Huang and Chang, 2023; Valmeekam et al., 2022; Wei et al., 2022). Their inability to perform such tasks can result in flawed planning and interaction, as they might resort to using inappropriate tools (Wornow et al., 2023).

Lack of Up-to-Date Knowledge: LLMs, which are trained on pre-existing datasets, may lack the latest scientific developments, leading to potential misalignments with contemporary scientific knowledge (Bommasani et al., 2021). Despite the advent of Retrieval-Augmented Generation (RAG), challenges remain in sourcing the most recent knowledge.

### 3.2. Planning Module

Given a task, the planning module is designed to break down the task into smaller and manageable components. Nevertheless, the following vulnerabilities exist:

Lack of Awareness of Risks in Long-term Planning: Agents often struggle to fully comprehend and account for the potential risks associated with their long-term plans of action. This issue is due to LLMs being primarily designed to solve specific tasks rather than to evaluate the long-term consequences of actions with an understanding of potential future impacts (Chui et al., 2018; Cave and ÓhÉigeartaigh, 2019).

Resource Waste and Dead Loops: Agents may engage in ineffective planning processes, leading to resource wastage and becoming stuck in non-productive cycles (Xu et al., 2022; Ruan et al., 2024; Li et al., 2023b). A pertinent example is when an agent is unable to determine whether it can complete a task or continually faces failure with a tool it relies on. This uncertainty can cause the agent to repeatedly attempt various strategies, unaware that these efforts are unlikely to yield success.

Inadequate Multi-task Planning: Agents often struggle with multi-goal or multi-tool tasks due to their optimization for single-task performance (Qin et al., 2024). Despite efforts to develop models for complex tasks like handling multi-modal medical datasets (Niu and Wang, 2023), effectively integrating diverse data types remains challenging.

### 3.3. Action Module

Once the task has been decomposed, the action module executes a sequence of actions. This process, however, introduces specific vulnerabilities as below: Subpar Threat Identification: Agents frequently overlook subtle and indirect attacks, resulting in vulnerabilities. This is especially problematic considering the early-stage development of Out-of-Distribution (OOD) detection methods (Yang et al., 2024). Existing safeguarding measures, such as keywordbased danger detection, often fall short of well-designed attacks.

Lack of Regulations on Human-Agent Interactions: The emergence of agents in scientific discovery underscores the need for ethical guidelines, especially when interacting with humans in sensitive areas like genetics. However, such regulatory frameworks remain in their infancy (McConnell and Blasimme, 2019).

### 3.4. External Tools

During the process of executing tasks, the tool module equips agents with a set of valuable tools (e.g., a cheminformatics toolkit, RDKit). These tools empower the agents with enhanced capabilities, enabling them to tackle tasks more effectively. However, these tools also bring forth certain vulnerabilities.

Deficient Oversight in Tool Usage: Lack of efficient supervision over how agents use tools can lead to potentially harmful situations. For instance, incorrect selection or misuse of tools can trigger hazardous reactions - even explosions. Agents may not be fully aware of the risks associated with the tools they use, especially in such specialized scientific tasks. Thus, it's crucial to enhance safeguards by learning from real-world tool usage (OpenAI, 2023b).

### 3.5. Memory and Knowledge Module

LLMs' knowledge can become muddled in practice, much like human memory lapses. The memory and knowledge module tries to mitigate this issue, leveraging external databases for knowledge retrieval and integration. However, several challenges persist:

Limitations in Domain-Specific Safety Knowledge: Agents' knowledge shortfalls in specialties like biotechnology or nuclear engineering can lead to safety-critical reasoning lapses. For instance, an agent for nuclear reactor design might overlook risks like radiation leaks or meltdowns (Paredes et al., 2021), and an agent for compound synthesis may fail to assess toxicity, stability, or environmental impacts (Arabi, 2021).

Limitations in Human Feedback: Insufficient, uneven, or low-quality human feedback may hinder agents' alignment with human values and scientific objectives. Despite its crucial role in refining performance and correcting biases, comprehensive human feedback is often hard to come by and may not cover all human preferences, especially in complex or ethical scenarios (Leike et al., 2020; Hagendorff and Fabi, 2022). It underscores the need for better methods to effectively collect and apply human feedback data.

Inadequate Environmental Feedback: Despite some works on embodied agents (Driess et al., 2023; Brohan et al., 2023), agents may not receive or correctly interpret environmental feedback, such as the state of the world or the behavior of other agents. This can lead to misinformed decisions that may harm the environment or themselves ( $\mathrm{Wu}$ and Shang, 2020). For example, an agent trained to manage water resources may not account for the variability of rainfall, the demand of different users, or the impact of climate change.

Unreliable Research Sources: Agents might utilize or train on outdated or unreliable scientific information, leading to the dissemination of incorrect or harmful knowledge. For example, LLMs run risks of plagiarism, content fabrication, or false results (Simonite, 2019; Jin et al., 2023a).

## 4. Current Progress on Agent Safety

We begin by examining the development from LLM safety to agent safety, to provide sufficient background grounding. Subsequently, we delve into the exploration of agent safety within the scientific realm, aiming to elucidate challenges. A survey of related work in safeguarding LLMs and agents is shown in Figure 4.

### 4.1. From LLM Safety to Agent Safety

Recent studies have made substantial headway in identifying and mitigating safety risks associated with content generated by LLMs (Zhang et al., 2023b; Xu et al., 2023; Zhiheng et al., 2023; Sun et al., 2023; Bhardwaj and Poria, 2023a; Inan et al., 2023), i.e., content safety risks. Those risks encompass issues such as offensiveness, unfairness, illegal activities, and ethical concerns. To evaluate the safety of LLM-generated content, SafetyBench (Zhang et al., 2023b) has employed multiple-choice questions covering seven categories of safety risks and SuperCLUE-Safety (Xu et al., 2023) has introduced a benchmark featuring multi-round and open-ended questions.

More significantly, researchers proposed alignment methods

![](https://cdn.mathpix.com/cropped/2024_06_04_eb3c6b328c28deeb593cg-06.jpg?height=789&width=1702&top_left_y=213&top_left_x=187)

Figure 4. Survey of related work in safeguarding LLMs and agents, among which scientific agents are specifically stated.

like reinforcement learning from human feedback (RLHF) to promote harmless LLMs (Ouyang et al., 2022; Bai et al., 2022a). "Safe RLHF", decoupling helpfulness and harmlessness, further refines this alignment (Dai et al., 2023). Furthermore, several works have explored the safety influence of fine-tuning and inference upon aligned LLMs. However, adversarial examples and benign data can inadvertently compromise model safety during fine-tuning (Qi et al., 2023; Yang et al., 2023). Reassuringly, (Bianchi et al., 2023) discovered that while extra safety examples can improve this concern, an excess may hinder it. In addition, solutions like the self-evaluating and rewinding "RAIN" offer training-free alignment alternatives (Li et al., 2023c).

In parallel, as LLMs suffer from prevalent alignmentbreaking attacks like jailbreaks (Wei et al., 2023a), researchers have designed corresponding evaluations and defenses. Deng et al. (2023); Mei et al. (2023); Yi et al. (2023) evaluated the content safety of LLMs with jailbreak attacks. For defenses, many prompt techniques (Helbling et al., 2023; Zhang et al., 2023c; Cao et al., 2023; Wei et al., 2023b), such as self-examination (Helbling et al., 2023), have been proposed. Moreover, a few works have promoted the resistance of LLMs to jailbreaks by parameter pruning (Hasan et al., 2024) and finetuning (Piet et al., 2023).

Despite efforts to safeguard LLMs, the safety of agents interacting with diverse tools and environments often goes overlooked. These agents could directly or indirectly produce harmful outputs. For example, they could inadvertently release toxic gases during chemical synthesis. Studies like ToolEmu (Ruan et al., 2024) identified risks of agents with an emulator, first exposing risks during agent execution. AgentMonitor (Naihin et al., 2023) and R-Judge (Yuan et al.,
2024) further evaluated the risk awareness of agents.

### 4.2. Current Work in Safeguarding Scientific Agents

Section 4.1 presented general safeguards for LLMs and agents. Due to the severity of corresponding safety issues within the scientific domain, safety concerns are now being prioritized in select scientific agents.

Coscientist (Boiko et al., 2023) has proposed a chemical agent with scientific tool access and pointed out that agents confront safety risks with practical examples, raising a call for safety assurance on scientific agents. Addressing safety concerns, CLAIRify (Yoshikawa et al., 2023) has designed specialized safety mechanisms for its chemical agents. Specifically, CLAIRify imposes high-level constraints on the order of material synthesis in experiment descriptions and task planning. Additionally, it restricts lowlevel manipulation and perception skills to prevent spills while transporting chemistry vials and beakers. Similarly, ChemCrow (Bran et al., 2023) has introduced a safety tool that reviews user queries to prevent agents from inadvertently creating hazardous chemicals during the synthesis process following malicious commands.

Furthermore, SciGuard (He et al., 2023) has offered a specialized agent for risk control and a benchmark for safety evaluation, where various tools not only assist in executing synthesis instructions but also incorporate long-term memory to enhance safety. To evaluate the security of the current science models, SciGuard has developed a benchmark called SciMT-Safety. This benchmark evaluates the harmlessness of a model based on its ability to reject malicious queries and gauges its helpfulness based on how effectively it handles benign queries.

## 5. Limitations and Challenges

Various studies have facilitated the capabilities of scientific agents (Huang et al., 2022; Ansari and Moosavi, 2023; Guo et al., 2024; Shi et al., 2024). However, few efforts have considered safety mechanisms, as discussed in Section 4.2, while only SciGuard developed a specialized agent for risk control. Here, we summarize four significant challenges:

(1) Lack of specialized models for risk control. With the exception of SciGuard (He et al., 2023), specialized agents for risk control are lacking. To safeguard general agents, LLM-based monitoring (Ruan et al., 2024; Naihin et al., 2023; Yuan et al., 2024; Inan et al., 2023) is commonly utilized to scrutinize agents for safe execution. By inspecting global contexts during agent execution, LLM monitors compensate for deficiencies in the agents' risk awareness. Given that safety issues can be more severe in the scientific domain than in internet and software contexts, specialized models for risk control are essential.

(2) Lack of domain-specific expert knowledge. Compared with popular applications of agents such as webshop (Yao et al., 2022) and app usage (Zhang et al., 2023d), the scientific domain demands wider and deeper knowledge, i.e. domain-specific expert knowledge. On one hand, expert knowledge enhances effective tool usage and planning, thereby alleviating unexpected safety issues arising from agent execution. On the other hand, expert knowledge regarding safety hazards improves agent awareness of behavioral outcomes. For example, if agents understand that the collision of two chemicals produces significant energy, they are more likely to avoid combining them.

(3) Risks introduced by tool usage. Much of the current work on safeguarding scientific agents focuses on external tool use (He et al., 2023). Thus, the safety of these tools becomes vital to agent safety. Application-specific tools often manually designed with built-in safety constraints result in a finite action space (Schick et al., 2023; Ruan et al., 2024). That said, these tools might not restrict agent calling access, increasing scientific domain risks. Moreover, if tools are vulnerable to manipulation, agents could be indirectly exploited, leading to harmful outcomes.

(4) Ineffective evaluations on the safety of scientific agents. Until now, benchmarks evaluating safety in the scientific realm, such as SciMT-safety (He et al., 2023), only consider the harmlessness of models by examining their ability to deny malicious requests. Considering the multifaceted issues mentioned above, safeguarding scientific agents demands additional benchmarks focused on comprehensive risk scopes (Section 2) and various agent vulnerabilities 3 .

## 6. Proposition

Existing efforts, notably ChemCrow and SciGuard, have addressed specific risks but lack a systematic methodology for broader safety concerns. This situation emphasizes the urgent necessity for community discussions and the development of more comprehensive and robust safety frameworks. Given the potential risks associated with scientific agents, it has become increasingly evident that the community must prioritize risk control over autonomous capabilities. Autonomy, while an admirable goal and significant in enhancing productivity within various scientific disciplines, cannot be pursued at the expense of generating serious risks and vulnerabilities. Consequently, we must balance autonomy with security and employ comprehensive strategies to ensure the safe deployment and use of scientific agents.

Moreover, the emphasis should shift from output safety to behavioral safety, which signifies a comprehensive approach that evaluates not only the accuracy of the agent's output but also the actions and decisions the agent takes. Behavioral safety is critical in the scientific domain, as the same action in different contexts can lead to vastly different consequences, some of which may be detrimental. Here, we suggest fostering a triadic relationship involving humans, machines, and the environment. This framework recognizes the critical importance of robust and dynamic environmental feedback in addition to human feedback.

### 6.1. Agent Alignment and Safety Evaluation

### 6.1.1. Agent AlIGnMent

Improving LLM Alignment: The most fundamental solution for safety problems is to improve the alignment of LLMs so that scientific agents built upon them will become more robust to malicious usages. To achieve this, the aforementioned safety concerns should be taken into consideration during the data collection process in the LLM alignment stage. For example, instructions that may pose scientific risks should be included in the human preference datasets, and responses that deal with these threats appropriately should be preferred. Moreover, Constitutional AI (Bai et al., 2022b) is a potential solution - curating principles related to scientific safety issues.

Towards Agent-level Alignment: Different from LLM alignment, agent alignment may focus on the symbolic control of autonomous agents (Hong et al., 2023; Zhou et al., 2023) and multi-agent or human-agent interaction scenarios. A specialized design, such as a "safety check" standard operating procedure, could be applied to control when and how agents can utilize scientific tools that may be exploited for malicious intents or result in unintended consequences.

### 6.1.2. SAFETY EVALUATION

Red Teaming: Identifying potential vulnerabilities that may cause hazardous activities to users and the environment is essential to evaluate agent safety. Red-teaming(Perez et al., 2022; Ganguli et al., 2022; Bhardwaj and Poria, 2023b; Feffer et al., 2024), i.e., adversarially probing LLMs for harmful outputs, have been widely used in developing general LLMs. Representatively, jailbreaks challenge model safety for redteaming evaluation, which has been specifically stated as alignment-breaking techniques in Section 4.1. Furthermore, red-teaming datasets can be utilized to train LLMs for harm reduction and alignment reinforcement. However, specialized red-teaming for scientific agents is absent. Considering severe risks in the scientific domain (Section 2), we call for red teaming against scientific agents.

Benchmarking: To tackle various risks stated in Section 2 , comprehensive benchmarks should cover a wider range of risk categories and a more thorough coverage of domains. To address vulnerabilities stated in Section 3, effective benchmarks should focus on various dimensions such as tool usage (Huang et al., 2024), risk awareness (Naihin et al., 2023; Yuan et al., 2024) and red-teaming resistance(Deng et al., 2023; Mei et al., 2023; Yi et al., 2023).

### 6.2. Human Regulation

In addition to steering already-capable models, it is also important to impose certain regulations on the developers and users of these highly capable models.

### 6.2.1. DEVELOPER REGULATION

The primary goal of developer regulation is to ensure scientific agents are created and maintained in a safe, ethical, and responsible manner. First, developers of scientific agents should adhere to a strict code of ethics. This includes mandatory training in ethical AI development, with an emphasis on understanding the potential societal impacts of their creations. Second, there should be mandatory safety and ethical compliance checks at various stages of the development process. These checks, conducted by an independent board, should evaluate the agent's algorithms for biases, ethical implications, and potential misuse scenarios. This step ensures that the agents are not only technically sound but also ethically aligned with societal values.

Furthermore, developers should implement robust security measures to prevent unauthorized access and misuse. This includes ensuring data privacy, securing communication channels, and safeguarding against cyber threats. Regular security audits and updates should be a standard part of the development life cycle. Lastly, there should be transparency in the development process. Developers must maintain detailed logs of their development activities, algorithms used, and decision-making processes. These records should be accessible for audits and reviews, ensuring accountability and facilitating continuous improvement.

### 6.2.2. USER REGULATION

Regulating the users of autonomous agents for scientific research is crucial as well. Firstly, potential users should obtain a license to access the scientific agents. To acquire the license, the users should be required to undergo relevant training and pass a knowledge evaluation on the responsible usage of scientific agents. Each user session of the scientific agent should be recorded and linked to the license ID of the user. The logs should be regularly reviewed and audited, and irresponsible usage should lead to possible revocation of the license.

Similar to clinical studies, which require approval from an Institutional Review Board (IRB) before proceeding, autonomous scientific research might also necessitate approval from an overseeing committee. For example, before using a scientific agent, the researchers should submit a proposal to IRB that lists the objectives and potential risks. The committee would review the proposals, assessing the objectives and associated risks, thereby ensuring that research conducted using these agents aligns with ethical standards and contributes positively to the scientific community.

### 6.3. Agent Regulation and Environmental Feedback

Understanding and interpreting environmental feedback is critical for scientific agents to operate safely. Such feedback includes various factors, such as the physical world, societal laws, and developments within a scientific system.

Simulated Environment for Result Anticipation: Scientific agents can significantly benefit from training and operating within simulated environments designed specifically to mimic real-world conditions and outcomes. This process allows the model to gauge the potential implications of certain actions or sequences of actions without causing real harm. For example, in a simulated biology lab, the autonomous agent can experiment and learn that improper handling of biohazardous material can lead to environmental contamination. Through trials within the simulation, the model can understand that specific actions or procedural deviations may lead to dangerous situations, helping establish a safety-first operating principle.

Agent Regulation: Agent regulation may focus on the symbolic control of autonomous agents (Hong et al., 2023; Zhou et al., 2023) and multi-agent or human-agent interaction scenarios. A specialized design, such as a "safety check" standard operating procedure, could be applied to control when and how agents can utilize scientific tools that may be exploited for malicious intents or result in unintended consequences. Another possible solution is to require autonomous agents to get approval from a committee consisting of hu-
man experts before each query for critical tools and APIs that may lead to potential safety concerns.

Critic Models: Beyond standard safety checks, "critic" models can play a crucial role. These models serve as additional AI layers that assess and refine the outputs of the primary AI system. By identifying potential errors, biases, or harmful recommendations, critic models contribute significantly towards reducing risks associated with the AI's operation, particularly in high-stake scenarios (Amodei et al., 2016; Hendrycks et al., 2021).

Tuning Agents with Action Data: Unlike the setup for LLM Alignment where the aim is to train the LLM, or a direct imposition of an operational procedure on an agent, using annotated data that reflect the potential risks of certain actions can enhance agents' anticipation of harmful consequences. By leveraging extensive annotations made by experts-like marking actions and their results during their laboratory work-we can continue to fine-tune agents. For example, a chemical study agent would understand that certain mixes can lead to harmful reactions. Also, training should take into account mechanisms that limit agents' access to dangerous tools or substances, leaning on annotated data or simulated environment feedback. In biochem or chemical labs, agents could learn to avoid interactions that may lead to biohazard contamination or hazardous reactions.

## 7. Conclusion

Our proposed approach urges a shift towards prioritizing operational safety without significantly compromising the capacity of autonomous scientific agents. At the backbone of our proposition lies a triadic approach, where the roles of the user, agent, and environment are intertwined and crucial in the safeguarding process for scientific agents based on LLMs. By adopting such strategies, we can leverage the capabilities of scientific agents while effectively minimizing and managing potential risks.

## Acknowledgement

Q.J. and Z.L. are supported by the NIH Intramural Research Program, National Library of Medicine. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies.

## Impact Statement

This research delves into risks associated with autonomous scientific agents, highlighting the urgency of focusing on risk-managed autonomy as these technologies become an integral part of scientific research. Our proposed strategies prioritize operational safety while maintaining productive functionality, aiming to reduce misuse and unintended consequences.
The potential impacts of negligent handling of these risks are extensive, reaching safety measures in laboratories, ethical responsibilities, information integrity, and environmental sustainability. For instance, without appropriate precautions, the malfunction of these agents could lead to hazards ranging from the dissemination of false scientific knowledge to the creation of dangerous materials or processes.

(1) Promoting Responsible AI Development: Our triadic model involving humans, machines, and the environment ensures safe agent operations, promising wider applications beyond science, given the universality of these principles.

(2) Enhancing AI Safety: Our focus on agent alignment raises both safety standards and utility of AI tools, making scientific discoveries safer. This strategy promotes data privacy, job security, and equitable access to advancements in diverse fields where AI sees usage.

(3) Interpreting Environmental Feedback: Prioritizing understanding environmental feedback and integrating environmental awareness within AI Safety measures could help address AI impacts on a larger scale. This approach navigates both immediate and long-term environmental implications of AI, potentially informing policy and shaping responsible $\mathrm{AI}$ practices across various sectors, from urban planning to environmental conservation.

Our path could reduce severe adverse consequences from LLM usage, mitigating risks like environmental hazards, individual harm, misuse of data, and unexpected ethical dilemmas. This foresight contributes to public trust and equitable benefit distribution.

## References

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. Concrete Problems in AI Safety. arXiv preprint arXiv:1606.06565 (2016).

Mehrad Ansari and Seyed Mohamad Moosavi. 2023. Agentbased Learning of Materials Datasets from Scientific Literature. arXiv:2312.11690 [cs.AI]

Anthropic. 2023. Introducing Claude. https: //www.anthropic.com/index/introducingclaude

Alya A Arabi. 2021. Artificial intelligence in drug design: algorithms, applications, challenges and ethics. Future Drug Discovery 3, 2 (2021), FDD59.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforce-
ment learning from human feedback. arXiv preprint arXiv:2204.05862 (2022).

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli TranJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073 [cs.CL]

Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. arXiv:2302.04023 [cs.CL]

Rishabh Bhardwaj and Soujanya Poria. 2023a. Red-teaming large language models using chain of utterances for safety-alignment. ArXiv preprint abs/2308.09662 (2023). https://arxiv.org/abs/2308.09662

Rishabh Bhardwaj and Soujanya Poria. 2023b. RedTeaming Large Language Models using Chain of Utterances for Safety-Alignment. arXiv:2308.09662 [cs.CL]

Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. 2023. Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions. arXiv preprint arXiv:2309.07875 (2023).

Daniil A. Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. 2023. Autonomous chemical research with large language models. Nature 624, 7992 (01 Dec 2023), 570578. https://doi.org/10.1038/s41586-023$06792-0$

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).

Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. 2023.
ChemCrow: Augmenting large-language models with chemistry tools. arXiv:2304.05376 [physics.chem-ph]

Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. 2023. RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. arXiv:2307.15818 [cs.RO]

Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. 2023. Defending against alignment-breaking attacks via robustly aligned llm. arXiv preprint arXiv:2309.14348 (2023).

Stephen Cave and Seán S ÓhÉigeartaigh. 2019. Bridging near-and long-term concerns about AI. Nature Machine Intelligence 1, 1 (2019), 5-6.

Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. 2024. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. In The Twelfth International Conference on Learning Representations.

Michael Chui, James Manyika, and David Schwartz. 2018. The real-world potential and limitations of artificial intelligence. The McKinsey Quarterly (2018).

Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2023. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773 (2023).

Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715 (2023).

Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong

Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. 2023. PaLM-E: an embodied multimodal language model. In Proceedings of the 40th International Conference on Machine Learning (Honolulu, Hawaii, USA) (ICML'23). JMLR.org, Article 340, 20 pages.

Michael Feffer, Anusha Sinha, Zachary C. Lipton, and Hoda Heidari. 2024. Red-Teaming for Generative AI: Silver Bullet or Security Theater? arXiv:2401.15897 [cs.CY]

Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 (2022).

Haoqiang Guo, Sendong Zhao, Haochun Wang, Yanrui Du, and Bing Qin. 2024. MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks via Text Prompts. arXiv:2401.11403 [cs.LG]

Thilo Hagendorff and Sarah Fabi. 2022. Methodological reflections for AI alignment research using human feedback. arXiv:2301.06859 [cs.HC]

Adib Hasan, Ileana Rugina, and Alex Wang. 2024. Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. arXiv:2401.10862 [cs.LG]

Jiyan He, Weitao Feng, Yaosen Min, Jingwei Yi, Kunsheng Tang, Shuai Li, Jie Zhang, Kejiang Chen, Wenbo Zhou, Xing Xie, Weiming Zhang, Nenghai Yu, and Shuxin Zheng. 2023. Control Risk for Potential Misuse of Artificial Intelligence in Science. arXiv:2312.06632 [cs.AI]

Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng Chau. 2023. Llm self defense: By self examination, llms know they are being tricked. arXiv preprint arXiv:2308.07308 (2023).

Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. 2021. Unsolved Problems in ML Safety. arXiv preprint arXiv:2109.13916 (2021).

Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2023. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. arXiv:2308.00352 [cs.AI]
Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards Reasoning in Large Language Models: A Survey. In Findings of the Association for Computational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 1049-1065. https://doi.org/10.18653/v1/ 2023.findings-acl. 67

Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W. Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. 2022. Artificial intelligence foundation for therapeutic science. Nature Chemical Biology 18, 10 (01 Oct 2022), 1033-1036. https : //doi.org/10.1038/s41589-022-01131-2

Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. arXiv:2311.05232 [cs.CL]

Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, and Qun Liu. 2024. Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios. arXiv:2401.17167 [cs.CL]

Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, and Ethan Perez. 2024. Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566 [cs.CR]

Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. 2023. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674 (2023).

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), $1-38$.

Qiao Jin, Robert Leaman, and Zhiyong Lu. 2023a. Retrieve, Summarize, and Verify: How will ChatGPT impact information seeking from the medical literature? Journal of the American Society of Nephrology (2023), 10-1681.

Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. 2023b. GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information. arXiv:2304.09667 [cs.CL]

Jan Leike, John Schulman, and Jeffrey Wu. 2020. Our approach to alignment research. OpenAI Blog (2020). https://openai.com/blog/ourapproach-to-alignment-research

Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society. In Thirty-seventh Conference on Neural Information Processing Systems.

Huayang Li, Tian Lan, Zihao Fu, Deng Cai, Lemao Liu, Nigel Collier, Taro Watanabe, and Yixuan Su. 2023b. Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective. arXiv:2310.10226 [cs.CL]

Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. 2023c. Rain: Your language models can align themselves without finetuning. arXiv preprint arXiv:2309.07124 (2023).

Sean C. McConnell and Alessandro Blasimme. 2019. Ethics, Values, and Responsibility in Human Genome Editing. AMA Journal of Ethics 21, 12 (2019), E1017-E1020. https://doi.org/10.1001/ amajethics.2019.1017

Alex Mei, Sharon Levy, and William Yang Wang. 2023. ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models. arXiv preprint arXiv:2310.09624 (2023).

Silen Naihin, David Atkinson, Marc Green, Merwane Hamadi, Craig Swift, Douglas Schonholtz, Adam Tauman Kalai, and David Bau. 2023. Testing Language Model Agents Safely in the Wild. ArXiv preprint abs/2311.10538 (2023). https: / /arxiv.org/abs / 2311.10538

Chuang Niu and Ge Wang. 2023. CT Multi-Task Learning with a Large Image-Text (LIT) Model. bioRxiv (2023), 2023-04.

Odhran O'Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali Essa Ghareeb, Justin Booth, and Samuel G Rodriques. 2023. BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology. arXiv preprint arXiv:2310.10632 (2023).
OpenAI. 2022. Introducing ChatGPT. https:// openai.com/blog/chatgpt

OpenAI. 2023a. GPT4 technical report. arXiv preprint arXiv:2303.08774 (2023).

OpenAI. 2023b. Our Approach to AI Safety. OpenAI.com (2023).

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730-27744.

Jose N. Paredes, Juan Carlos L. Teze, Gerardo I. Simari, and Maria Vanina Martinez. 2021. On the Importance of Domain-specific Explanations in AI-based Cybersecurity Systems (Technical Report). arXiv:2108.02006 [cs.CR]

Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. $1-22$.

Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022. Red Teaming Language Models with Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 3419-3448.

Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, and David Wagner. 2023. Jatmo: Prompt Injection Defense by TaskSpecific Finetuning. arXiv preprint arXiv:2312.17673 (2023).

Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693 (2023).

Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023. Tool learning with foundation models. arXiv preprint arXiv:2304.08354 (2023).

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2024. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. In The Twelfth International Conference on Learning Representations.

Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris Maddison, and Tatsunori Hashimoto. 2024. Identifying the Risks of LM Agents with an LM-Emulated Sandbox. In The Twelfth International Conference on Learning Representations (ICLR).

Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. In Thirty-seventh Conference on Neural Information Processing Systems.

Rusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush Tagade, Stephen Casper, and Javier Rando. 2023. Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation. arXiv:2311.03348 [cs.CL]

Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023. Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks. arXiv:2310.10844 [cs.CL]

Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, and May D. Wang. 2024. EHRAgent: Code Empowers Large Language Models for Complex Tabular Reasoning on Electronic Health Records. arXiv:2401.07128 [cs.CL]

Tom Simonite. 2019. AI can write just like me. Brace for the robot apocalypse. The Guardian website (2019).

Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. 2023. Safety Assessment of Chinese Large Language Models. ArXiv abs/2304.10436 (2023). https://api.semanticscholar.org/ CorpusID:258236069

Gemini Team. 2023. Gemini: A Family of Highly Capable Multimodal Models. arXiv:2312.11805 [cs.CL]

Shubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu Chen, Won Kim, Donald C Comeau, et al. 2024. Opportunities and challenges for ChatGPT and large language models in biomedicine and health. Briefings in Bioinformatics 25, 1 (2024), bbad493.

Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su. 2023. Evil Geniuses: Delving into the Safety of LLM-based Agents. arXiv:2311.11855 [cs.CL]

Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2022. Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). In NeurIPS 2022 Foundation
Models for Decision Making Workshop. https:// openreview.net/forum?id=wUU-7XTL5XO

Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432 (2023).

Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023a. Jailbroken: How Does LLM Safety Training Fail?. In Thirty-seventh Conference on Neural Information Processing Systems.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id= _VjQlMeSB_J

Zeming Wei, Yifei Wang, and Yisen Wang. 2023b. Jailbreak and guard aligned language models with only few incontext demonstrations. arXiv preprint arXiv:2310.06387 (2023).

Lilian Weng. 2023. LLM-powered Autonomous Agents. lilianweng.github.io (Jun 2023). https://lilianweng.github.io/posts/ 2023-06-23-agent/

Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A Pfeffer, Jason Fries, and Nigam H Shah. 2023. The shaky foundations of large language models and foundation models for electronic health records. npj Digital Medicine 6, 1 (2023), 135 .

Junyi Wu and Shari Shang. 2020. Managing uncertainty in AI-enabled decision making and achieving sustainability. Sustainability 12, 21 (2020), 8758.

Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864 (2023).

Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. 2022. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. Advances in Neural Information Processing Systems 35 (2022), 3082-3095.

Liang Xu, Kangkang Zhao, Lei Zhu, and Hang Xue. 2023. Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese. ArXiv preprint abs/2310.05818 (2023). https: //arxiv.org/abs/2310.05818

Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. 2024. Generalized Out-of-Distribution Detection: A Survey. arXiv:2110.11334 [cs.CV]

Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. 2023. Shadow alignment: The ease of subverting safely-aligned language models. arXiv preprint arXiv:2310.02949 (2023).

Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems 35 (2022), 20744-20757.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R Narasimhan. 2023a. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In Thirtyseventh Conference on Neural Information Processing Systems.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023b. ReAct: Synergizing Reasoning and Acting in Language Models. In The Eleventh International Conference on Learning Representations.

Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. 2023. Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. arXiv preprint arXiv:2312.14197 (2023).

Naruki Yoshikawa, Marta Skreta, Kourosh Darvish, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjorn Kristensen, Andrew Zou Li, Yuchi Zhao, Haoping Xu, Artur Kuramshin, et al. 2023. Large language models for chemistry robotics. Autonomous Robots 47, 8 (2023), 1057-1086.

Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, et al. 2024. R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. arXiv preprint arXiv:2401.10019 (2024).

Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023d. AppAgent: Multimodal Agents as Smartphone Users. arXiv:2312.13771 [[cs.CV](http://cs.cv/)]
Gongbo Zhang, Qiao Jin, Denis Jered McInerney, Yong Chen, Fei Wang, Curtis L Cole, Qian Yang, Yanshan Wang, Bradley A Malin, Mor Peleg, et al. 2023a. Leveraging Generative AI for Clinical Evidence Summarization Needs to Achieve Trustworthiness. arXiv preprint arXiv:2311.11211 (2023).

Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. 2023b. SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions. arXiv:2309.07045 [cs.CL]

Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. 2023c. Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization. arXiv:2311.09096 [cs.CL]

Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, et al. 2023e. Igniting Language Intelligence: The Hitchhiker's Guide From Chain-ofThought Reasoning to Language Agents. arXiv preprint arXiv:2311.11797 (2023).

Xi Zhiheng, Zheng Rui, and Gui Tao. 2023. Safety and ethical concerns of large language models. In Proceedings of the 22nd Chinese National Conference on Computational Linguistics (Volume 4: Tutorial Abstracts). 9-16.

Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. 2023. Agents: An Opensource Framework for Autonomous Language Agents. arXiv:2309.07870 [cs.CL]
