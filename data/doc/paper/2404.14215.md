# Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction 

Zheye Deng ${ }^{*}$, Chunkit Chan ${ }^{*}$, Weiqi Wang ${ }^{*}$, Yuxi Sun ${ }^{\star}$, Wei Fan ${ }^{*}$,<br>Tianshi Zheng ${ }^{*}$, Yauwai Yim ${ }^{*}$, Yangqiu Song ${ }^{*}$<br>${ }^{*}$ Department of Computer Science and Engineering, HKUST, Hong Kong SAR, China<br>'School of Computer Science, Fudan University, Shanghai, China<br>\{zdengah, yqsong\}@cse.ust.hk


#### Abstract

The task of condensing large chunks of textual information into concise and structured tables has gained attention recently due to the emergence of Large Language Models (LLMs) and their potential benefit for downstream tasks, such as text summarization and text mining. Previous approaches often generate tables that directly replicate information from the text, limiting their applicability in broader contexts, as text-to-table generation in real-life scenarios necessitates information extraction, reasoning, and integration. However, there is a lack of both datasets and methodologies towards this task. In this paper, we introduce LiVESUm, a new benchmark dataset created for generating summary tables of competitions based on real-time commentary texts. We evaluate the performances of state-of-the-art LLMs on this task in both fine-tuning and zero-shot settings, and additionally propose a novel pipeline called $\mathrm{T}^{3}$ (ext-Tuple-Table) to improve their performances. Extensive experimental results demonstrate that LLMs still struggle with this task even after fine-tuning, while our approach can offer substantial performance gains without explicit training. Further analyses demonstrate that our method exhibits strong generalization abilities, surpassing previous approaches on several other text-to-table datasets. Our code and data can be found at https://github. com/HKUST-KnowComp/LiveSum-TTT.


## 1 Introduction

Reading extensive texts is demanding and timeconsuming for humans, further compounded by the challenge of effectively capturing the key elements. Consequently, recent works have shifted to explore the structured summarization of text (Jain et al., 2024), with tables being one highly prevalent form (Wu et al., 2022; Li et al., 2023c; Sundar et al., 2024). These approaches improve text comprehension by extracting inherent yet valuable structural

![](https://cdn.mathpix.com/cropped/2024_06_04_92015ae2ad594ff87675g-01.jpg?height=874&width=719&top_left_y=751&top_left_x=1071)

Figure 1: An overview of the differences between our proposed LIVESUM dataset and previous dataset (Wiseman et al., 2017), as well as our proposed pipeline called $\mathrm{T}^{3}$ (Text-Tuple-Table) which consists of three steps.

information from long unstructured text and enabling their applications in downstream scenarios, such as question answering (Chen et al., 2020; Zhu et al., 2024), text summarization (Wiseman et al., 2017; Wang et al., 2020; Mulwad et al., 2023) and text data mining (Li et al., 2023b; Sui et al., 2024).

However, previous studies on text-to-table generation primarily rely on datasets traditionally used for table-to-text tasks (Wiseman et al., 2017; Novikova et al., 2017). One evident issue is that these tasks focus merely on format transformation, where the information in the table and the corresponding text representation are essentially similar (Lebret et al., 2016; Bao et al., 2018). For example, in the upper part of Figure 1, the table can be easily completed by extracting relevant numbers from the text without intermediate inference. Such seemingly meaningful correlations can intro-
duce bias into the models, causing them to excel at replicating relevant information but struggle when it comes to categorizing and integrating numbers in complex scenarios. This is further evidenced by the fact that fine-tuning models already perform very well and greatly surpass zero-shot LLMs (Tang et al., 2023; Sundar et al., 2024). Hence, a more complicated dataset that requires information aggregation and minimizes the presence of spurious correlations, closely resembling real-world scenarios, is definitely needed for a more rigorous evaluation of models' text-to-table generation proficiency.

Apart from the research gap at the benchmark level, in terms of methodology, considerable attention has been given to studying the ability of LLMs to comprehend and generate complex structured outputs (Tang et al., 2023; Jain et al., 2024), driven by the exceptional success of LLMs in recent years (Touvron et al., 2023; Anthropic, 2024; OpenAI, 2024). Extensive benchmarks indicate that LLMs exhibit sub-optimal performance in zero-shot settings, with multiple cases of generating inaccurate contents deviate from the given text (Tang et al., 2023). To address this issue, more sophisticated prompting mechanisms have been proposed (Wei et al., 2022; Khot et al., 2022; Dua et al., 2022). Among them, Jain et al. (2024) introduce a divide-and-generate prompting approach to generate more accurate and informative tables, demonstrating its effectiveness in improving model performance. However, this simplistic approach of dividing text into paragraphs and generating tables is unsuitable in more complicated situations because table-relevant information may not be contiguous in the original text and may span across various paragraphs. Therefore, developing a robust prompting method is also needed for generating complex tables that capture crucial information from scattered text or paragraphs.

To resolve the aforementioned research gaps, we introduce a novel benchmark, LIVESUM, which consists of 3,771 text-based live commentaries from real-world football matches, intending to evaluate the models' ability to generate summary tables. Unlike previous benchmarks, our benchmark necessitates the model to possess the ability to extract correct and meaningful information from complex textual data, specifically emphasizing information integration, reasoning, and conceptualization skills (Wang et al., 2023a). This is because commentaries in close temporal proximity or with similar semantic meanings may describe the same event, while verbs with similar meanings may refer to the same types of events. For example, in Figure 1, the second and third dialogue boxes both describe the same goal event, and the verbs "goal" and "score" refer to the same goal event.

Along with the benchmark, we also introduce a robust prompting-based method $\mathrm{T}^{3}$ to address our proposed task. Specifically, our method draws inspiration from the inherent attributes of the table, where each cell, along with its corresponding row header and column header, creates an informative triple (namely (row header, column header, cell)), which degenerates into a binary tuple when lacking row or column headers. These tuples serve as cues for humans to locate specific information in the text and complete the table accordingly. Consequently, our pipeline begins by extracting the relevant tuples from the text, followed by the integration of these tuples, and ultimately generating one or more summary tables.

We hope that the proposed dataset, method, and experimental results can provide valuable insights for tasks such as text-to-table, as well as any task involving the generation of complex structured outputs from text. In summary, in this paper, we make the following contributions:

- To the best of our knowledge, LIVESUM is the first benchmark dataset designed to evaluate the information integration ability of models in text-to-table generation tasks.
- We introduce a novel $\mathrm{T}^{3}$ (Text-Tuple-Table) prompting pipeline that functions as a flexible framework, applicable to any text-to-table generation tasks.
- We conduct extensive experiments to evaluate the performance of LLMs under different settings and demonstrate that our $\mathrm{T}^{3}$ pipeline can bring significant improvements while showcasing excellent generalization capabilities.


## 2 Task Definition

We first provide a formal definition of the text-totable generation. The input $\mathcal{S}$ consists of a textual passage with $n$ tokens, denoted as $\mathbf{x}=x_{1}, \ldots, x_{n}$, and optionally, an instruction text with $m$ tokens, denoted as $\mathbf{y}=y_{1}, \ldots, y_{m}$, which provides guidance on the format or content of the generated tables. The output $\mathcal{T}$ is a set of $k(k \geq 1)$ tables, $\mathbf{T}^{1}, \ldots, \mathbf{T}^{k}$. For the output tables, we present a more detailed definition that covers two aspects: structure-related and content-related.

Original Live Text

Goal! West Ham United 0, Liverpool 1. Steven Gerrard (Liverpool) converts the penalty with a right footed shot to the bottom right corner
![](https://cdn.mathpix.com/cropped/2024_06_04_92015ae2ad594ff87675g-03.jpg?height=456&width=644&top_left_y=338&top_left_x=298)

Figure 2: Overview of the pipeline for constructing the LIVESUM dataset illustrated with a sample sentence.

Structure We assume there are no merged cells in the tables for simplicity. Each table $\mathbf{T}^{i}$ has a caption $\mathbf{c}^{i}=c_{1}^{i}, \ldots, c_{d}^{i}$, where $d=\left|\mathbf{c}^{i}\right|$, and it consists of $n_{r}^{i}$ rows and $n_{c}^{i}$ columns, resulting in a total of $n_{r}^{i} \times n_{c}^{i}$ cells. The cell $\mathbf{T}_{p, q}^{i}$ in the $p$-th row and $q$-th column is composed of a sequence of tokens: $\mathbf{T}_{p, q, 1}^{i}, \ldots, \mathbf{T}_{p, q, r}^{i}$, where $r=\left|\mathbf{T}_{p, q}^{i}\right|$. The table $\mathbf{T}^{i}$ must have either a row header for all rows or a column header for all columns and it is also possible for the table to have both.

Content We define that the information in the output tables should be derived from the input $\mathrm{x}$ or can be inferred from $\mathbf{x}$. For each cell $\mathbf{T}_{p, q}^{i}$ in table $\mathbf{T}^{i}$, when combined with its row header $\mathbf{T}_{p, 1}^{i}$ (if any), its column header $\mathbf{T}_{1, q}^{2}$ (if any), and the table's caption $\mathbf{c}^{i}$, it should convey the equivalent information as expressed in the input $\mathbf{x}$ and complies with the instruction $\mathbf{y}$ (if any).

## 3 LIVESUM Dataset

We consider the problem of generating match statistic information tables from textual live commentary. Inspired by RoTOWIRE (Wiseman et al., 2017), a data-to-document dataset in the sports domain that aims to generate textual summaries by incorporating statistical data from basketball games, we instead focus on live commentary in football, which is available on BBC Sports ${ }^{1}$. We crawl the data for the English Premier League from 2014 to 2023 and obtain complete commentary for 3,771 matches. Figure 2 shows the pipeline we use to construct the dataset. Section 3.1 describes the generation process of the live commentary, and Section 3.2 describes the generation process for the summary table. More details are provided in Appendix A.[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_92015ae2ad594ff87675g-03.jpg?height=494&width=486&top_left_y=233&top_left_x=1202)

Figure 3: Eight types of event information (inner circle) that require summarization in LIVESUM dataset, along with their common expressions (outer circle) in the commentary.

### 3.1 Live Commentary Generation

To address formatting issues in the original textual live commentary on the website, we paraphrase the text to match the commentator's style while ensuring a certain degree of diversity. Building upon previous studies (Kim et al., 2023a; Chen et al., 2023; Kim et al., 2023b), we employ ChatGPT (OpenAI, 2022) to generate complete live commentary automatically. Subsequently, to comply with privacy regulations and prevent bias in LLM benchmarks, we also anonymize the data leveraging named entity recognition (NER) techniques (Qi et al., 2020) to produce the final version of the live commentary.

### 3.2 Summary Table Generation

On the other hand, human annotators label a summary table for each match's commentary. We recruit five workers who are interested in football and are from English-speaking countries to perform the annotations. Since the occurrence of the events in football matches is deterministic, the ground truth is essentially unambiguous. In cases where there are inconsistencies in the annotated results, the correct answer is determined through a majority vote.

### 3.3 Statistics

LIVESUM comprises a collection of 3,771 pairs, consisting of textual live commentaries and corresponding summary tables. We randomly split the entire dataset into training and test sets, resulting in 3,017 instances for the training set and 754 instances for the test set. On average, each live commentary segment consists of 1,256 words. LIVESUM focuses on eight types of events, with the names and their corresponding common descriptions displayed in Figure 3.

## $4 \quad T^{3}$ (Text-Tuple-Table) Pipeline

Our proposed $\mathrm{T}^{3}$ (ext-Tuple-Table) pipeline is designed to mimic the intuitive steps followed by humans when performing this task. When individuals aim to summarize a table from text, they typically extract pertinent or valuable tuples from the content, guided by any provided instructions, and then organize these tuples into one or more tables. Based on this concept, we divide this transformation into three stages: text-to-tuple, integration, and tuple-to-table, each of which is discussed in the following subsections. Taking Figure 1 as an example, we first extract key events mentioned in the text, then aggregate this information into consolidated tuples, and ultimately compile them into a table.

### 4.1 Text-to-Tuple

Considering the superior performance and flexibility of LLMs in information extraction compared to traditional techniques (Ma et al., 2023; Xu et al., 2023), we employ an LLM as our tuple extractor. We follow the instructions from InstructUIE (Wang et al., 2023b) and design the following prompting:

## Text-to-Tuple Prompting Template

According to $<$ Instruction $>$, please extract the relevant events and information in the form of tuples, structured as (subject, object, verb) or (subject, attribute, value): <Text>

where $<$ Instrction $>$ is the directive for the current task, and $<$ Text $>$ is the text to be transformed.

### 4.2 Information Integration

In this stage, we propose two approaches for integrating information. The first one involves direct execution by the LLM, using prompting to consolidate tuple data. The second one uses algorithms and code generated by the LLM to integrate tuple information, inspired by the LLMs' great success in code generation tasks (Roziere et al., 2023; Luo et al., 2023; Guo et al., 2024). T³ defaults to using code generation in this step. The promptings for these two methods are shown as follows:

Information Integration Prompting Template

Direct Execution: According to <Instruction>, please integrate these tuples as required: <Tuples>

Code Generation: According to <Instruction>, please develop an algorithm to consolidate these tuples as specified: <Tuples> where $<$ Instrction $>$ is the directive for the current task, and <Tuples> consists of the tuples extracted in the prior stage.

### 4.3 Tuple-to-Table

After obtaining the integrated tuples, we follow the previous implementation (Tang et al., 2023; Jain et al., 2024) and use the following prompting to generate the final tables:

Tuple-to-Table Prompting Template

According to $<$ Instruction $>$, please generate one or more tables based on the following tuples: <Tuples>

where $<$ Instrction $>$ is the directive for the current task, and <Tuples> consists of the tuples produced in the prior stage.

## 5 Experimental Setup

Baseline Models In this study, we conduct finetuning on the LIVESUM dataset using three representative open-source LLMs: Mistral-7B-Instructv0.2 (Jiang et al., 2023), LLaMA-2 Chat 7B and LLaMA-2 Chat 13B (Touvron et al., 2023). We fine-tune these models following the current stateof-the-art fine-tuning methodologies (Tang et al., 2023). Therefore, the outcomes represent the best results achievable with the present fine-tuning methods. We also evaluate eight state-of-the-art LLMs in zero-shot settings: LLaMA-2 Chat 13B, LLaMA-2 Chat 70B (Touvron et al., 2023), Mistral Large (MistralAI, 2024), Claude 2.1 (Anthropic, 2023), Claude 3 Opus (Anthropic, 2024), ChatGPT (OpenAI, 2022), and GPT-4 (OpenAI, 2024). For each model, we conduct tests using two types of prompts. The first type directly describes the task by providing an instruction text $\mathbf{y}$ and accompanying it with the text $\mathbf{x}$. The second type uses the Chain-of-Thought (CoT) prompting (Wei et al., 2022), incorporating the phrase "let's think step by step" into the instruction text. See more details in Appendix C.

Evaluation Metric As the generated cell content in this task consists of numerical values, we utilize commonly employed metrics in regression tasks, namely the Root Mean Square Error (RMSE). We also report the Error Rate (ER) for each cell, defining a cell as erroneous if its content does not exactly match the ground truth.

Grouping by Event Difficulty Furthermore, we categorize the eight types of events into three

![](https://cdn.mathpix.com/cropped/2024_06_04_92015ae2ad594ff87675g-05.jpg?height=720&width=1582&top_left_y=228&top_left_x=246)

Figure 4: The performance of various LLMs under fine-tune and zero-shot settings, as well as after the application of the $\mathrm{T}^{3}$ method on the test set of LIVESUM dataset. The average RMSE and error rate for each model are displayed, along with the error rate for each of the three difficulty sections. More results are in Table 1.

groups based on assessed difficulty: Goals, due to direct descriptions of scores in the original text, and Red Cards, due to their rare occurrence are categorized into the Easy section. Shots and Fouls, due to their varied expressions and descriptions, are classified into the Hard section. The remaining four event types are classified as Medium section. We report the RMSE and ER for each model across different difficulty categories to provide a more comprehensive analysis.

## 6 Experiments and Analyses

In this section, we will benchmark the performance of current state-of-the-art LLMs on the LIVESUM dataset, and further evaluate the effectiveness and generalization of our proposed approach. We aim to answer the following research questions:

RQ1 (Benchmarking) How do the current stateof-the-art LLMs perform on this dataset in finetuning and zero-shot settings?

RQ2 (Effectiveness) How does our proposed $\mathrm{T}^{3}$ pipeline impact model performance?

RQ3 (Generalization) How effective is the $\mathrm{T}^{3}$ pipeline when applied to other real-world datasets for the text-to-table generation task?

### 6.1 Benchmarking (RQ1)

We first analyze the performance of existing stateof-the-art LLMs on the LIVESUM dataset under fine-tuning and zero-shot settings, with results displayed in Figure 4 and Table 1. Overall, the performance of most models in the zero-shot setting far exceeds that in the fine-tuning setting, indicating that the previous state-of-the-art fine-tuning method has limited capability for information integration, and there is substantial room for improvement on this benchmark. In the zero-shot setting, it is noteworthy that most models show a slight improvement in both metrics after applying CoT. Among them, the best-performing models are Mistral Large, GPT-4, and Claude 3 Opus, which are nearly comparable. They achieve RMSEs ranging from 2.08 to 2.27 and error rates between $46.20 \%$ and $48.33 \%$. Nevertheless, this still highlights a notable deficiency in the information integration capabilities of LLMs in the zero-shot settings, underscoring the challenges and significance of our benchmark. We then analyze performance across three categories of difficulty.

Easy Section It can be observed that the error rate of the fine-tuned models is generally around $40 \%$, with an RMSE close to 1 . In the zero-shot setting, LLaMA-2-Chat, ChatGPT, and Claude 2.1 models exhibit relatively poor performance, occasionally producing anomalously large values. The error rates of the other models generally remain below $5 \%$, with RMSEs less than 0.2 . Among these, the Mistral Large model performs the best, with both metrics significantly lower than other models.

Medium Section The medium section exhibits the greatest variation among models and serves as a crucial determinant of overall model performance. We organize the models based on their performance, with error rates in the zero-shot set-

| Model | Easy |  | Medium |  | Hard |  | Average |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | RMSE | ER | RMSE | ER | RMSE | ER | RMSE | ER |
| Fine-Tune |  |  |  |  |  |  |  |  |
| Mistral-7B-Instruct-v0.2 | 1.045 | 38.36 | 3.832 | 85.11 | 7.115 | 95.52 | 4.564 | 76.03 |
| LLaMA-2-Chat 7B | $\overline{1.047}$ | $38.41 \quad$ | 3.728 | 84.91 | 7.107 | 95.40 | 4.512 | 75.91 |
| LLaMA-2-Chat 13B | 1.043 | 39.22 | 3.587 | $\overline{84.60}$ | $\overline{6.671}$ | $\overline{94.42}$ | $\overline{4.287}$ | $\overline{75.71}$ |
| Zero-Shot |  |  |  |  |  |  |  |  |
| LLaMA-2-Chat 13B | 0.775 | 33.29 | 4.554 | 87.37 | 5.203 | 93.29 | 4.279 | 75.33 |
| LLaMA-2-Chat 13B (CoT) | 0.780 | 31.83 | 4.376 | 87.42 | 5.088 | $92.35 \quad$ | 4.162 | 74.75 |
| LLaMA-2-Chat 70B | 0.410 | 12.34 | 3.189 | 88.59 | 4.941 | 92.41 | 3.455 | 70.48 |
| LLaMA-2-Chat 70B (CoT) | 0.450 | 12.86 | 3.221 | 89.25 | 5.314 | 94.24 | 3.613 | 71.40 |
| ChatGPT | 0.200 | 8.06 | 2.864 | 72.73 | 4.257 | 90.62 | 3.008 | 61.03 |
| ChatGPT (CoT) | 0.229 | 10.61 | 2.809 | 72.75 | 4.087 | 90.38 | 2.911 | 61.62 |
| Claude 2.1 | 1.014 | 10.08 | 2.581 | 63.99 | 4.621 | 90.58 | 3.171 | 57.16 |
| Claude 2.1 (CoT) | 1.496 | 14.06 | 2.291 | 61.70 | 4.081 | 90.38 | 2.918 | 56.96 |
| Mistral Large | 0.005 | 0.27 | 2.385 | 52.45 | 2.712 | 84.62 | 2.209 | 47.45 |
| Mistral Large (CoT) | 0.018 | 0.73 | 2.311 | 51.82 | $\overline{2.608}$ | $\overline{84.08}$ | 2.139 | 47.12 |
| GPT-4 | $\overline{0.156}$ | $4.64 \quad$ | 1.167 | 46.05 | 4.114 | 88.53 | $\overline{2.273}$ | 46.32 |
| GPT-4 (CoT) | 0.154 | $4.38 \quad$ | 1.173 | $\overline{45.86}$ | 3.981 | 88.73 | 2.225 | $\overline{46.20}$ |
| Claude 3 Opus | 0.078 | $2.52 \quad$ | $\overline{1.617}$ | 51.36 | 3.713 | 88.06 | 2.253 | 48.33 |
| Claude 3 Opus (CoT) | 0.040 | 1.59 | 1.642 | 49.60 | 3.265 | 87.86 | 2.079 | 47.17 |
| $\bullet$ Zero-Shot with $\mathrm{T}^{3}$ |  |  |  |  |  |  |  |  |
| Claude $2.1\left(\mathrm{~T}^{3}\right)$ | 0.193 | 8.95 | 1.965 | 44.99 | 2.751 | 72.15 | 2.066 | 42.77 |
| Mistral Large $\left(\mathrm{T}^{3}\right)$ | 0.191 | 8.82 | 1.596 | 42.37 | 2.136 | 69.23 | 1.631 | 40.70 |
| GPT-4 $\left(\mathrm{T}^{3}\right)$ | 0.056 | 3.18 | $\underline{0.854}$ | $\underline{25.83}$ | 1.219 | 46.22 | $\underline{0.929}$ | $\underline{25.27}$ |
| Claude 3 Opus $\left(\mathrm{T}^{3}\right)$ | $\underline{0.081}$ | $\underline{5.30}$ | 0.406 | 14.79 | 0.477 | 21.29 | 0.438 | 14.04 |

Table 1: The performance of various LLMs under three settings, showing RMSE and error rate across three difficulty categories and overall average. We bold the best results and underline the second-best results in each setting.

ting ranging from $89.25 \%$ down to $45.86 \%$. GPT-4 with CoT performs the best, achieving the lowest error rate, which still indicates the suboptimal capabilities of LLMs.

Hard Section The charts clearly show that in the hard section, the zero-shot method shows minimal enhancement compared to fine-tuning, as most error rates are around $90 \%$. However, Mistral Large is an exception, achieving a lower error rate of $84.08 \%$, which demonstrates the challenging nature of the hard section.

### 6.2 Effectiveness (RQ2)

We apply the $\mathrm{T}^{3}$ pipeline to four LLMs: Claude 2.1, Mistral Large, GPT-4, and Claude 3 Opus; the rest are not applicable to this method due to their ineffective extraction of tuples from inputs of such lengths, resulting in a minimal number of tuples or a substantial duplication of the same tuple. The implementation details are discussed in Appendix B. From Figure 4 and Table 1, it is observable that Claude 2.1 and Mistral Large exhibit similar improvements after applying the $\mathrm{T}^{3}$ method, both showing slight enhancements over the best results in the zero-shot setting, with reductions in RMSE of $34.9 \%$ and $26.2 \%$ respectively and de- creases in error rate of $25.2 \%$ and $14.2 \%$. In contrast, GPT-4 and Claude 3 Opus display substantial improvements after implementing the $\mathrm{T}^{3}$ method, with RMSE reductions of $59.1 \%$ and $80.6 \%$, respectively, and error rate reductions of $45.4 \%$ and $70.9 \%$. The reductions in these metrics are primarily reflected in the significant decrease in the error rate for the hard sections, creating a clear distinction from the zero-shot approaches. We then conduct an ablation study to evaluate the impact of the $\mathrm{T}^{3}$ method on model performance. We experiment with two variant methods: $\mathrm{T}^{3}$-MERGED $\left(\mathrm{T}^{3} \mathrm{M}\right)$ and $\mathrm{T}^{3}$-DIRECT-EXECUTION ( $\left.\mathrm{T}^{3} \mathrm{D}\right)$. The former method involves using a single prompt that instructs the model to first extract relevant tuples before generating the table, while the latter one modifies the second step of the $\mathrm{T}^{3}$ method to be directly executed by the LLM, rather than using code generation (see Appendix B. 5 for more details). Table 2 presents the results using GPT-4 as an example. It is apparent that using $\mathrm{T}^{3}$ considerably enhances the model's overall performance, leading significantly in overall metrics, with average reductions in RMSE and error rate of $59.1 \%$ and $45.4 \%$, respectively. Compared to GPT-4 and CoT, the variants $\mathrm{T}^{3} \mathrm{M}$ and $\mathrm{T}^{3} \mathrm{D}$ also exhibit notable im-

| Model | Easy <br> RMSE/ER | Medium <br> RMSE/ER | Hard <br> RMSE/ER | Average <br> RMSE/ER |
| :--- | :---: | :---: | :---: | :---: |
| GPT-4 | $0.16 / 4.6$ | $1.17 / 46.1$ | $4.11 / 88.5$ | $2.27 / 46.3$ |
| $\mathrm{w} / \mathrm{CoT}$ | $0.15 / 4.4$ | $1.17 / 45.9$ | $3.98 / 88.7$ | $2.23 / 46.2$ |
| $\mathrm{w} / \mathrm{T}^{3} \mathrm{M}$ | $\mathbf{0 . 0 0 / 0 . 1}$ | $1.42 / 43.2$ | $2.46 / 82.8$ | $1.62 / 42.3$ |
| $\mathrm{w} / \mathrm{T}^{3} \mathrm{D}$ | $0.09 / 4.5$ | $\underline{1.12} / \underline{\underline{2} .1}$ | $\underline{2.23} / 81.4$ | $\underline{1.42 / 41.5}$ |
| $\mathrm{w} / \mathrm{T}^{3}$ | $\underline{0.06} / 3.2$ | $\mathbf{0 . 8 5} / \mathbf{2 5 . 8}$ | $\mathbf{1 . 2 2 / 4 6 . 2}$ | $\mathbf{0 . 9 3 / 2 5 . 3}$ |

Table 2: The ablation study results comparing the performance of different prompting methods. We bold the best results and underline the second-best results.

| Metric | Zero-Shot |  | Fine-Tune |
| :--- | :---: | :---: | :---: |
|  | w/ T |  |  |

Table 3: The evaluation results on the test set of StRUCBench Table dataset with nine metrics. We bold the best results and underline the second-best results.

provements, with respective reductions in RMSE of $28.7 \%$ and $37.6 \%$, and error rates of $8.6 \%$ and $10.3 \%$. Full ablation studies are in Appendix D.

### 6.3 Generalization (RQ3)

To examine the generalization capabilities of $\mathrm{T}^{3}$, we apply $\mathrm{T}^{3}$ to two additional datasets designed to test text-to-table performance and compare it with previous methods. Section 6.3.1 involves table generation without the need for information integration, while Section 6.3.2 focuses on table generation without instructions.

### 6.3.1 Performance of $T^{3}$ on STRUC-BENCH Table Dataset

We test the performance of $\mathrm{T}^{3}$ on the text-totable benchmark STRUC-BENCH Table (Tang et al., 2023). This benchmark is based on the RoTOWIRE dataset (Wiseman et al., 2017) and employs traditional evaluation metrics, prompting score $(P$ Score), and heuristical score( $H$-Score), to conduct a comprehensive assessment of the output tables. They also introduce a fine-tuning approach incorporating row and column header information in the training instructions. We argue that this comparison with the zero-shot method is problematic. In zeroshot settings, due to the absence of a fine-tuning process, the format of the output tables is uncertain. For example, in ground-truth tables, cells that are left blank may be filled with terms such as "unknown" or "not mentioned" by the model, substantially impacting similarity-based metrics. Hence we modify the model's outputs under zero-shot settings before reporting the results. We evaluate the performance of ChatGPT with and without employing the $\mathrm{T}^{3}$ method and also compare it to the fine-tuned LLaMA-7B model proposed by Tang et al. (2023). Results are detailed in Table 3. It is observable that the application of the $\mathrm{T}^{3}$ method results in significant improvements across all metrics, with some measures outperforming the fine-tuned model. Further details on the experiments and analysis of the results are discussed in Appendix E.

### 6.3.2 Performance of $\mathrm{T}^{3}$ on WIKI40B Dataset

We intend to evaluate the performance of our proposed approach in the text-to-table task without instructions and ground-truth tables. In line with the pioneering work of STRUCTSUM (Jain et al., 2024), we experiment on a randomly sampled set from the English section of WIKI40B dataset (Guo et al., 2020). As there is no ground-truth table for the text in the dataset, they leverage LLMs and propose AUTO-QA Coverage as an evaluation metric:

$$
\operatorname{Cov}(\mathcal{T})=\frac{\sum_{i=1}^{|G(\mathcal{S})|} E_{\left(q_{i}, a_{i}\right)}\left[Q\left(\mathcal{T}, q_{i}\right)\right]}{|G(\mathcal{S})|}
$$

where $G(\mathcal{S})$ is the list of Question-Answer pairs $\left(q_{i}, a_{i}\right.$ ) generated by the LLM based on text $\mathcal{S}$, $Q(\mathcal{T}, q)$ is the LLM's answer to question $q$ based on table $\mathcal{T}$, and $E_{(q, a)}[x]$ is the LLM's evaluation of whether answer $a$ and $x$ are equivalent for question $q$. On top of this evaluation, we add a step where an LLM is used to pre-screen each $(q, a)$ pair based on text $\mathcal{S}$, filtering out any pairs where the question can not be correctly answered. This process further assures the quality of the $\mathrm{QA}$ pairs generated by $G$. We opt for ChatGPT as the LLM for evaluation and randomly sample 500 passages for the test dataset following Jain et al. (2024). We also introduce $\mathrm{T}^{2}$ (Text-Tuple) which treats the intermediary tuples from $\mathrm{T}^{3}$ as a single table $\mathcal{T}$. We aim to investigate the extent of information loss during the conversion from tuples to tables through this configuration. Figure 5 shows the Auto-QA Coverage of three methods. The curve indicates the percentage of generated tables meeting a given coverage threshold. Overall, $\mathrm{T}^{3}$ demonstrates a

![](https://cdn.mathpix.com/cropped/2024_06_04_92015ae2ad594ff87675g-08.jpg?height=768&width=711&top_left_y=233&top_left_x=267)

$\rightarrow$ Divide-and-Generate (Jain et al., 2024)

$\longrightarrow \mathrm{T}^{3}$ (Text-Tuple-Table)

$\because \mathrm{T}^{2}$ (Text-Tuple)

![](https://cdn.mathpix.com/cropped/2024_06_04_92015ae2ad594ff87675g-08.jpg?height=608&width=694&top_left_y=387&top_left_x=270)

Figure 5: AUTo-QA coverage of the three methods The point $(P, C)$ means $P \%$ of the data can achieve a coverage of $C \%$ or higher measured using Auto-QA.

substantial improvement over the prior Devide-andGenerate method (Jain et al., 2024). For example, when the coverage threshold is set to $70 \%$, about $65 \%$ of data reach this threshold after applying $\mathrm{T}^{3}$, compared to only $50 \%$ with the preceding approach. It is important to note that $\mathrm{T}^{2}$ outperforms $\mathrm{T}^{3}$, with $83 \%$ of data retaining the same coverage after tuple extraction. This suggests that information loss occurs during the transformation of raw tuples into structured tables. Exploring ways to mitigate this loss represents an essential area for further research. More details are discussed in Appendix F.

### 6.4 Case Studies

We present specific case studies on the outputs of different models on the LIVESUM dataset in Appendix G. These cases directly demonstrate the effectiveness of our proposed method. Additionally, we summarize some common errors of the model after applying $\mathrm{T}^{3}$ and areas for improvement.

## 7 Related Work

Text-to-Table Generation Many studies have been proposed to perform text-to-table generation, converting it into sequence-to-sequence problems (Wu et al., 2022; Li et al., 2023c), or framing them as question-answering problems (Sundar et al., 2024). With the rise of LLMs, some research has also explored evaluating LLMs under finetuning or zero-shot settings, and it shows that finetuning yields highly effective results (Tang et al., 2023; Sundar et al., 2024). However, these meth- ods employ datasets that only require the model to extract relevant information from text and populate tables, which significantly limits the scope of this task. Therefore, we introduce a new challenging dataset and propose a universal solution that greatly enhances the performance of LLMs under zero-shot setting.

LLMs for Information Extraction Information Extraction (IE) is critical and foundational for many downstream tasks in NLP. Many works have been conducted to leverage LLMs and provide effective solutions for IE tasks within a generative framework (Ma et al., 2023; Lu et al., 2023; Wan et al., 2023; Zhou et al., 2024). Recent progress in LLMs also has led to the development of unified frameworks that model various IE tasks and domains (Wang et al., 2023b; Sainz et al., 2024). This aligns with our intention to harness this capability to address general text-to-table generation tasks.

LLM Promptings Prompt engineering has been essential for enhancing LLMs and has demonstrated great success across a wide range of applications (Wei et al., 2022; Dua et al., 2022; Li et al., 2023a; Wang et al., 2024). Among the various prompting techniques, we find the decomposed prompting (Khot et al., 2022), which breaks down complex tasks into easier sub-tasks via prompting, highly effective for text-to-table generation. Jain et al. (2024) adopts this idea by breaking the text into small pieces for table generation. However, we argue that such a decomposition approach is impractical because text is not always easily divisible. For example, in our dataset, such division might result in adjacent sections describing the same event, causing errors. Hence we propose a more intuitive and broadly applicable task decomposition pipeline.

## 8 Conclusion

In this work, we introduce LIVESUM, a novel and challenging benchmark dataset for assessing the capability of models to integrate information in the text-to-table generation, along with a robust pipeline named $\mathrm{T}^{3}$. Experimental results show that current LLMs underperform on our dataset in both fine-tuning and zero-shot settings; however, significant improvements are observed after applying our proposed $\mathrm{T}^{3}$ pipeline. Our method can also be applied to any text-to-table dataset, enabling LLMs to outperform previous methods in zero-shot settings.

## Limitations

Although our LIVESUM benchmark extensively evaluates the information integration capabilities of LLMs, we have not yet tested their performance in a few-shot setting. Despite the challenge posed by the token length of live commentary for fewshot settings, we reserve this aspect for future work. Furthermore, while our proposed $\mathrm{T}^{3}$ pipeline significantly improves performance on several stateof-the-art LLMs, it cannot be effectively applied to LLMs that are deficient in tuple extraction capabilities, as it fails in the first stage and cannot proceed to the next phase. Developing methods that boost performance on such LLMs remains a valuable area for future research.

## Ethics Statement

When constructing the LIVESUM dataset, we sample texts of live football match commentary from the open-access BBC Sports official website. We apply LLMs to paraphrase this live commentary and conduct manual reviews to ensure no harmful content is generated. We also anonymize the data using named entity recognition (NER) technology combined with player rosters. The datasets used in our experiments, STRUC-BENCH (Tang et al., 2023) and WIKI40B (Guo et al., 2020), are opensource, and all experiments adhere to their intended use for research purposes. Therefore, to the best of the authors' knowledge, we believe that this work introduces no additional risk.

## Acknowledgements

The authors of this paper were supported by the NSFC Fund (U20B2053) from the NSFC of China, the RIF (R6020-19 and R6021-20), and the GRF (16211520 and 16205322) from RGC of Hong Kong. We also thank the UGC Research Matching Grants (RMGS20EG01-D, RMGS20CR11, RMGS20CR12, RMGS20EG19, RMGS20EG21, RMGS23CR05, RMGS23EG08).

## References

Anthropic. 2023. Introducing claude 2.1. Anthropic.

Anthropic. 2024. Introducing the next generation of claude. Anthropic.

Junwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua Lv, Ming Zhou, and Tiejun Zhao. 2018. Table-totext: Describing table region with natural language.
In Proceedings of the AAAI conference on artificial intelligence, volume 32.

Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, and Dilek Hakkani-Tur. 2023. PLACES: Prompting language models for social conversation synthesis. In Findings of the Association for Computational Linguistics: EACL 2023, pages 844-868, Dubrovnik, Croatia. Association for Computational Linguistics.

Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. 2020. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1026-1036.

Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022. Successive prompting for decomposing complex questions. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1251-1265, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. Preprint, arXiv:2401.14196.

Mandy Guo, Zihang Dai, Denny Vrandecic, and Rami Al-Rfou. 2020. Wiki-40b: Multilingual language model dataset. In LREC 2020.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.

Parag Jain, Andreea Marzoca, and Francesco Piccinno. 2024. Structsum generation for faster text comprehension. Preprint, arXiv:2401.06837.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825.

Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representations.

Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. 2023a. SODA: Million-scale dialogue distillation with social commonsense contextualization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12930-12949, Singapore. Association for Computational Linguistics.

Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. 2023b. FANToM: A benchmark for stress-testing machine theory of mind in interactions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14397-14413, Singapore. Association for Computational Linguistics.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.

Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas. Association for Computational Linguistics.

Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023a. Structured chain-of-thought prompting for code generation. Preprint, arXiv:2305.06599.

Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. 2023b. Table-gpt: Table-tuned gpt for diverse table tasks. Preprint, arXiv:2310.09263.

Tong Li, Zhihao Wang, Liangying Shao, Xuling Zheng, Xiaoli Wang, and Jinsong Su. 2023c. A sequenceto-sequence\&set model for text-to-table generation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 5358-5370, Toronto, Canada. Association for Computational Linguistics.

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.

Di Lu, Shihao Ran, Joel Tetreault, and Alejandro Jaimes. 2023. Event extraction as question generation and answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1666-1688, Toronto, Canada. Association for Computational Linguistics.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evolinstruct. Preprint, arXiv:2306.08568.
Yubo Ma, Yixin Cao, Yong Hong, and Aixin Sun. 2023. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10572-10601, Singapore. Association for Computational Linguistics.

MistralAI. 2024. Au large. MistralAI.

Varish Mulwad, Jenny Weisenberg Williams, Timothy W. Finin, Sharad Dixit, and Anupam Joshi. 2023. Towards semantic exploration of tables in scientific documents. In ESWC Workshops.

Jekaterina Novikova, Ondřej Dušek, and Verena Rieser. 2017. The E2E dataset: New challenges for endto-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201-206, Saarbrücken, Germany. Association for Computational Linguistics.

OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. OpenAI.

OpenAI. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774.

Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Belgium, Brussels. Association for Computational Linguistics.

Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A Python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations.

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950.

Oscar Sainz, Iker García-Ferrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, and Eneko Agirre. 2024. Gollie: Annotation guidelines improve zero-shot information-extraction. Preprint, arXiv:2310.03668.

Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.

Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024. Table meets llm: Can large language models understand structured table data? a benchmark and empirical study. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pages 645-654.

Anirudh Sundar, Christopher Richardson, and Larry Heck. 2024. gtbls: Generating tables from text by conditional question answering. Preprint, arXiv:2403.14457.

Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark Gerstein. 2023. Struc-bench: Are large language models really good at generating complex structured data? Preprint, arXiv:2309.08963.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Preprint, arXiv:2307.09288.

Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, and Sadao Kurohashi. 2023. GPT-RE: In-context learning for relation extraction using large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3534-3547, Singapore. Association for Computational Linguistics.

Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Doug Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis, Rodney Michael Kinney, et al. 2020. Cord-19: The covid-19 open research dataset. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020.

Weiqi Wang, Tianqing Fang, Baixuan Xu, Chun Yi Louis Bo, Yangqiu Song, and Lei Chen. 2023a. CAT: A contextualized conceptualization and instantiation framework for commonsense reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13111-13140, Toronto, Canada. Association for Computational Linguistics.

Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, Jingsheng Yang, Siyuan Li, and Chunsai Du. 2023b. Instructuie: Multi-task instruction tuning for unified information extraction. Preprint, arXiv:2304.08085.
Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, and Tomas Pfister. 2024. Chain-of-table: Evolving tables in the reasoning chain for table understanding. Preprint, arXiv:2401.04398.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.

Sam Wiseman, Stuart M Shieber, and Alexander M Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253-2263.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020, pages 38-45. Association for Computational Linguistics.

Jian Wu, Yicheng Xu, Yan Gao, Jian-Guang Lou, Börje Karlsson, and Manabu Okumura. 2023. TACR: A table alignment-based cell selection method for $\mathrm{Hy}$ bridQA. In Findings of the Association for Computational Linguistics: ACL 2023, pages 6535-6549, Toronto, Canada. Association for Computational Linguistics.

Xueqing Wu, Jiacheng Zhang, and Hang Li. 2022. Textto-table: A new way of information extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2518-2533.

Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, and Enhong Chen. 2023. Large language models for generative information extraction: A survey. arXiv preprint arXiv:2312.17617.

Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263-27277.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.

Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning of 100+ language models. CoRR, abs/2403.13372.

Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung Poon. 2024. Universalner: Targeted distillation from large language models for open named entity recognition. Preprint, arXiv:2308.03279.

Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, and Tat-Seng Chua. 2024. Tat-llm: A specialized language model for discrete reasoning over tabular and textual data. Preprint, arXiv:2401.13223.
