# Contrastive Search Is What You Need For Neural Text Generation 

Yixuan Su<br>ys484@cam.ac.uk<br>Language Technology Lab, University of Cambridge<br>Nigel Collier<br>nhc30@cam.ac.uk<br>Language Technology Lab, University of Cambridge

Reviewed on OpenReview: https: // openreview. net/forum? id=GbkWw3jwL9


#### Abstract

Generating text with autoregressive language models (LMs) is of great importance to many natural language processing (NLP) applications. Previous solutions for this task often produce text that contains degenerative expressions (Welleck et al. 2020) or lacks semantic consistency (Basu et al. 2021). Recently, Su et al. (2022b) introduced a new decoding method, contrastive search, based on the isotropic representation space of the language model and obtained new state of the art on various benchmarks. In addition, Su et al. (2022b) argued that the representations of autoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also shared by previous studies (Ethayarajh 2019). Therefore, to ensure the language model follows an isotropic distribution, $\mathrm{Su}$ et al. (2022b) proposed a contrastive learning scheme, i.e. SimCTG, which calibrates the language model's representations through additional training.

In this study, we first answer the question: "Are autoregressive LMs really anisotropic?". To this end, we extensively evaluate the isotropy of LMs across 16 languages. Surprisingly, we find that the anisotropic problem only exists in the two specific English GPT2-small/medium models. On the other hand, all other evaluated LMs are isotropic which is in contrast to the conclusion drawn by previous studies (Ethayarajh, 2019 Su et al. 2022b). Based on our findings, we further assess the contrastive search decoding method using off-the-shelf LMs on four generation tasks across 16 languages. Our experimental results demonstrate that contrastive search significantly outperforms previous decoding methods without any additional training. More notably, on 12 out of the 16 evaluated languages, contrastive search performs comparably with human-level performances as judged by human evaluations. Our code and other related resources are publicly available at https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need.


## 1 Introduction

Natural language generation (NLG) with autoregressive language models (LMs) is an indispensable component of various NLP applications. Some typical examples are dialogue systems (Thoppilan et al. 2022 Rae et al. 2021), contextual text completion (Radford et al. 2019), story generation (Su et al.| 2022a), machine translation (NLLB Team et al. 2022), and etc. Predominant approaches for generating text from LMs can be categorized into two classes: (i) maximization-based methods, such as greedy search and beam search; and (ii) stochastic methods, such as top- $k$ sampling (Fan et al. 2018) and nucleus sampling Holtzman et al. 2020). However, on the one hand, maximization-based approaches tend to produce text that contains undesirable repetitions (Fan et al. 2018 Holtzman et al. 2020) as shown in Table 1. On the other hand,

Method
Gobe Bryant is one of the greatest basketball players of all time, but he's also one of the most polarizing figures in the
NBA.
Bryant is one of the greatest basketball players of all time, but he's also one of the most polarizing figures in the NBA.
Bryant is one of the greatest basketball players of all time, but he's also one of the most polarizing figures in the NBA.
Bryant is one of the greatest basketball players of all time, but he's also one of the most polarizing figures in the NBA.
Bryant is one of the greatest basketball players of all time, but he's also one of the most polarizing figures in the NBA...
Kobe Bryant is a Hall of Fame Player. Superstar NBA player Steph Curry also is. Scoring a crazy 96 points per game,
staving off injury, and making 28 consecutive free throws, Michael Jordan is a cool Hall of Fame Hall of Famer. People
may forget he played for a winning team. But remember, basketball Hall of Famers Tom Chambers, Bob Pettit and
Walter Davis, along with Pats linebacker Ty Law, played on the losing Dallas Cowboys teams of the 1950's. They played
in indestructible stinkin' teams. They were far from perfect. We all remember it. So, if you haven't followed their run to
three rings as one of the best defenses in league history, they are hard to forget. There are options available. Jimmy Butler,
Dwight Howard, Russell Westbrook, Deron Williams, Ryan Anderson, Patrick Beverley, Anthony Davis, Reggie Jackson,
Tony Parker, Kyrie Irving, Gordon Hayward, LeBron James and Damian Lillard are all right now making the All-
-Defensive teams and likely to be Pro-Bowlers if they stay on this level this season. Shaquille O'Neal is not far behind...
nobe Bryant is the best player in the world.
I know this is a bold statement to make, but it's true. He may have won a lot of games, but his impact on the game is
immeasurable. Kobe is one of the most popular players in the world, and that's no small feat when you consider how
many people play video games, watch sports, listen to music, and have social media accounts.
Kobe has made a career out of being a role model for young people, and he does an amazing job of that every time he
steps on the court. His career stats are impressive, but what makes him stand out is the way he goes about his business.
When he's on the court, he's the center of attention, and it's easy to see why.
The Lakers' All-Star point guard wears a smile on his face like no other. He's always in the right place at the right time,
making the right play at the right time, and doing the right thing in the right way. He's not afraid to get his hands dirty,
and he never shies away from a physical confrontation...

Table 1: Texts generated by the off-the-shelf GPT-2-large using different methods given the prefix "Kobe Bryant is". (i) Beam search $(b=5)$ generates text with undesirable repetitions (highlighted in red); (ii) Nucleus sampling $(p=0.95)$ quickly goes off-the-topic and talks about other players who are inconsistent with the prefix (highlighted in blue); (iii) Lastly, the text generated by contrastive search is semantically coherent to the prefix while being grammatically fluent. (best viewed in color)

stochastic methods are likely to produce text that is semantically inconsistent with the given human-written prefix (Basu et al. 2021, Su et al. 2022b) (see an example in Table 1).

To address the issues posed by previous studies, Su et al. 2022b introduced a new decoding method, contrastive search, which generates semantically coherent text while maintaining a high level of diversity, based on the isotropic representation space of LMs. Moreover, as widely discussed by previous studies (Ethayarajh, 2019), Su et al. (2022b) argued that autoregressive LMs (e.g. GPT-2) are naturally anisotropic, i.e. their token representations reside in a narrow subset of the entire space (Ethayarajh, 2019). Therefore, an additional training stage, i.e. SimCTG, is required to calibrate the representation space of LMs. However, an obvious downside of Su et al. (2022b) is that, for extremely large LMs (e.g. GPT-3 (Brown et al. 2020), this additional training stage is computationally prohibitive which greatly limits the practical applicability of their approach.

While the anisotropy of autoregressive LMs have been widely discussed by previous studies (Ethayarajh, 2019 Su et al., 2022b), in this work, we revisit this problem and try to answer the question: "Are autoregressive LMs really anisotropic?". To this end, we extensively evaluate 38 off-the-shelf LMs, ranging from 117M to 30B parameters, across 16 major languages. Surprisingly, we find that the anisotropic problem only exists in the two specific English GPT-2-small/medium models. And the rest of evaluated LMs are isotropic which is in contrast to the conclusion drawn by previous studies (Ethayarajh, 2019; Su et al., 2022b).

Based on our findings, we further assess the contrastive search decoding method using off-the-shelf LMs on four generation tasks across 16 languages ( $\$ 4$ to $\$ 7$. Both human and automatic evaluations verify that, without any additional training, contrastive search significantly outperforms existing decoding methods and
generates exceptionally high-quality text as shown in Table 1. Furthermore, we provide in-depth analyses on the inner workings of contrastive search ( $\$ 8$.

In summary, our contributions are:

- To the best of our knowledge, our work is the first effort that sheds light on the isotropic property of autoregressive LMs.
- We extensively evaluate contrastive search using off-the-shelf LMs from 16 languages across four generation tasks, including (i) open-ended text generation; (ii) document summarization; (iii) code generation; and (iv) machine translation.
- Our experimental results on both human and automatic evaluations verify the clear superiority of contrastive search over existing decoding methods. Notably, on 12 out of the 16 evaluated languages, contrastive search performs comparably with human-level performances.


## 2 Preliminaries

### 2.1 Measurement for the Isotropy of Language Models

To analyze the isotropy of the language model's representation space, we follow previous studies Ethayarajh, 2019; Su et al., 2021, 2022b and define the averaged self-similarity of token representations within a text sequence $\boldsymbol{x}$ as

$$
\begin{equation*}
\text { self-similarity }(\theta ; \boldsymbol{x})=\frac{1}{|\boldsymbol{x}| \times(|\boldsymbol{x}|-1)} \sum_{i=1}^{|\boldsymbol{x}|} \sum_{j=1, j \neq i}^{|\boldsymbol{x}|} \frac{h_{x_{i}}^{\top} h_{x_{j}}}{\left\|h_{x_{i}}\right\| \cdot\left\|h_{x_{j}}\right\|} \tag{1}
\end{equation*}
$$

where $\boldsymbol{x}=\left\{x_{1}, \ldots, x_{|\boldsymbol{x}|}\right\}$ is a text sequence with variable length; $h_{x_{i}}$ and $h_{x_{j}}$ are the token representations of $x_{i}$ and $x_{j}$ produced by the language model $\theta$. Intuitively, a lower self-similarity $(\theta ; \boldsymbol{x})$ indicates the representations of distinct tokens are less similar, i.e. more discriminative, to each other.

Furthermore, given a text corpus $\mathcal{D}=\left\{\boldsymbol{x}_{i}\right\}_{i=1}^{|\mathcal{D}|}$, we define the isotropy of the language model $\theta$ as

$$
\begin{equation*}
\text { isotropy }(\theta)=1-\frac{1}{|\mathcal{D}|} \sum_{\boldsymbol{x} \in \mathcal{D}} \text { self-similarity }(\theta ; \boldsymbol{x}) \tag{2}
\end{equation*}
$$

Here, a larger isotropy $(\theta)$ means the representations produced by the language model are more evenly distributed in the representation space, therefore better following an isotropic distribution.

### 2.2 Contrastive Search

As discussed in Section $\S 1$ to address the issues posed by existing decoding methods, Su et al. (2022b) introduced a new decoding method, contrastive search. Formally, given the prefix context $\boldsymbol{x}_{<t}$, the selection of the output $x_{t}$ follows

$$
\begin{equation*}
x_{t}=\underset{v \in V^{(k)}}{\arg \max }\{(1-\alpha) \times \underbrace{p_{\theta}\left(v \mid \boldsymbol{x}_{<t}\right)}_{\text {model confidence }}-\alpha \times \underbrace{\left(\max \left\{s\left(h_{v}, h_{x_{j}}\right): 1 \leq j \leq t-1\right\}\right)}_{\text {degeneration penalty }}\} \tag{3}
\end{equation*}
$$

where $V^{(k)}$ is the set of top- $k$ predictions from the language model's probability distribution $p_{\theta}\left(\cdot \mid \boldsymbol{x}_{<t}\right)$. In Eq. (3), the first term, model confidence, is the probability of candidate $v$ predicted by the LMs. The second term, degeneration penalty, measures the similarity between the candidate $v$ and the tokens in the previous context $\boldsymbol{x}_{<t}$. And $s(\cdot, \cdot)$ computes the cosine similarity between token representations. More specifically, degeneration penalty is defined as the maximum cosine similarity between the representation of the candidate $v$ and that of all tokens in $\boldsymbol{x}_{<t}$. Here, the candidate representation $h_{v}$ is computed by the LMs given the concatenation of $\boldsymbol{x}_{<t}$ and $v$. Intuitively, a larger degeneration penalty of $v$ means it is more similar to the context, therefore
more likely leading to the undesirable repetitions in the generated output. The hyperparameter $\alpha \in[0,1]$ regulates the importance of these two components. 1 . After the selection of output token $x_{t}$ based on Eq. (3), the representation $h_{x_{t}}$ is further used to predict the token at time step $\mathrm{t}+1$. In Section $\$ 8$, we provide in-depth analyses on the inner relationship between contrastive search and the isotropy of LMs.

## 3 Isotropy of Language Models

In this section, we conduct extensive evaluations on the isotropy of LMs from 16 major languages. The model scale of evaluated LMs ranges from 117M to 30 B parameters ${ }^{2}$ In Section $\$ 3.1$, we first evaluate the English LMs. Then, in Section $\$ 3.2$ we extend our evaluations to multilingual LMs.

Evaluation Dataset. To measure the isotropy of LMs from different languages, we use the WIT dataset (Srinivasan et al., 2021) as our text corpus $\mathcal{D}$ (see Eq. (2)). Specifically, WIT consists of generaldomain text collected from Wikipedia across 108 languages. For different LMs, we use the text of WIT from the corresponding language to compute the isotropy.

### 3.1 English Language Models

First, we evaluate the isotropy of English LMs. For a comprehensive evaluation, we consider three families of off-the-shelf autoregressive LMs.

- GPT-2 (Radford et al. 2019): We evaluate all publicly available GPT-2 models with different model scales, including small (i.e. $117 \mathrm{M}$ ), medium (i.e. $345 \mathrm{M}$ ), large (i.e. $774 \mathrm{M}$ ), and $\mathrm{xl}$ (i.e. $1.6 \mathrm{~B}$ ).
- GPT-Neo (Black et al., 2021): We evaluate all three publicly available GPT-Neo models with the parameter size of $125 \mathrm{M}, 1.3 \mathrm{~B}$, and $2.7 \mathrm{~B}$, respectively.
- OPT (Zhang et al. 2022): OPT is recently released by Meta as an open-sourced replication of GPT-3 (Brown et al. 2020). In our experiments, we evaluate the OPT model with up to 30B parameters.

Figure 1 plots the isotropy results of different English LMs. On the one hand, we see that, among all evaluated LMs, only the small (i.e. $117 \mathrm{M}$ ) and medium (i.e. $345 \mathrm{M}$ ) size GPT-2 display a clear anisotropy (i.e. isotropy $(\theta) \leq 0.25$ ). On the other hand, the representation space of all other evaluated LMs are remarkably better and isotropic.

It is worth emphasizing that previous studies (Ethayarajh, 2019 Su et al., 2022b), which discuss the anisotropy of English autoregressive LMs, only focus on the specific GPT-2-small model (i.e. 117M). And our observation on the anisotropy of GPT-2-small is also inline with previous studies (Ethayarajh, 2019 Su et al. 2022b). However, through extensive evaluations on a wide range of LMs with different scales, we empirically show that the majority of existing English autoregressive LMs are isotropic, and this

![](https://cdn.mathpix.com/cropped/2024_06_04_3301b272a3a38d6a3bc8g-04.jpg?height=558&width=811&top_left_y=1540&top_left_x=1058)

Figure 1: Isotropy results of English LMs. observation also holds for multilingual LMs (\$3.2).$^{3}\$

Remark. We acknowledge that there are many factors (e.g. training data, model initialization, optimization, and etc.) that could potentially cause the unusual behaviors of the English GPT-2-small/medium models. The rigorous investigation on these factors is out-of-the-scope of this study and we leave it to our future[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_3301b272a3a38d6a3bc8g-05.jpg?height=578&width=1567&top_left_y=264&top_left_x=276)

Figure 2: Isotropy results of multilingual LMs. Each $\mathrm{x}(\mathrm{y}$ ) denotes the language code (x) and the model size (y), where $\mathrm{s}$ is for small size model (i.e. $\sim 120 \mathrm{M}$ parameters), $\mathrm{m}$ is for medium size model (i.e. $\sim 350 \mathrm{M}$ parameters), 1 is for large size model (i.e. $\sim 780 \mathrm{M}$ parameters), and $\mathrm{x}$ is for $\mathrm{xl}$ size model (i.e. $\sim 1.5 \mathrm{~B}$ parameters). For English (i.e. en) LMs, we plot the results of three OPT models. The detailed list of language codes and evaluated LMs can be found in Table 10 at Appendix $\mathrm{D}$

work. Nonetheless, based on our extensive evaluations in both English LMs (\$3.1) as well as multilingual LMs (detailed in $\$ 3.2$, we empirically show that the majority of existing autoregressive LMs are isotropic.

### 3.2 Multilingual Language Models

Here, we evaluate the isotropy of multilingual LMs, with different scales, across 16 languages. Figure 2 presents the evaluated results, from which we see that the isotropy scores of all evaluated LMs are above 0.50. This clearly indicates that our finding in Section \$3.1 i.e. the majority of existing autoregressive LMs are isotropic, is generalizable to different languages.

## 4 Open-ended Text Generation

In this section, we present our experimental results on open-ended text generation for both English LMs ( $\$ 4.1$ ) and multilingual LMs ( $\$ 4.2$. Formally, open-ended generation is defined as, conditioned on a human-written prefix (i.e. context), the language model is required to generate a text continuation that is grammatically fluent while being semantically coherent with the context (Holtzman et al. 2020; Su et al. 2022b; Su \& Xu, 2022).

### 4.1 English Open-ended Text Generation

Following previous study (Holtzman et al. 2020), we use the large version of GPT-2 (Radford et al., 2019) (i.e. GPT-2-large) to generate texts conditioned on the initial paragraph (restricted to 40 tokens) of documents from the held-out set of WebText (Radford et al. 2019). Specifically, the generation of text ends upon reaching an end-of-document token or a maximum length of 200 tokens.

### 4.1.1 Automatic Evaluation

Decoding Methods. We compare various decoding strategies, including (1) greedy search; (2) beam search $(b=4)$; (3) typical sampling $(\tau=0.95)$ (Meister et al. 2022); (4) top- $k$ sampling $(k=50)$ (Fan et al., 2018); (5) nucleus sampling $(p=0.95)$ (Holtzman et al. 2020); and (6) contrastive search $(k=5, \alpha=0.6)$ (Su et al. 2022b). $4^{4}$[^1]

Evaluation Metrics. Following previous studies (Welleck et al., 2020, Su et al., 2022b), we evaluate the generated results of different decoding methods using (i) diversity, which provides an overall assessment on the repetition of generation at different $n$-gram levels and $n \in\{2,3,4\}$; and (ii) MAUVE (Pillutla et al. 2021), which measures the token distribution closeness between the generated text and the human-written text. A higher MAUVE score means the generated text is more human-like. (iii) Moreover, it has been widely demonstrated that, by simply measuring the log-likelihood of the text, the massively pre-trained language models (Brown et al., 2020 Zhang et al. 2022) display an exceptional zero-shot performance on tasks like sentence completion selection (Zellers et al., 2019) and natural language inference (NLI) (Wang et al., 2018). We follow the same practice and introduce a new coherence metric to automatically measure the semantic coherence between the generated text and the given prefix text. Specifically, the metric is defined as the averaged log-likelihood of the generated text conditioned on the prefix text as

$$
\begin{equation*}
\operatorname{coherence}(\hat{\boldsymbol{x}}, \boldsymbol{x})=\frac{1}{|\hat{\boldsymbol{x}}|} \sum_{i=1}^{|\hat{\boldsymbol{x}}|} \log p_{\mathcal{M}}\left(\hat{\boldsymbol{x}}_{i} \mid\left[\boldsymbol{x}: \hat{\boldsymbol{x}}_{<i}\right]\right) \tag{4}
\end{equation*}
$$

where $\boldsymbol{x}$ and $\hat{\boldsymbol{x}}$ are the prefix text and the generated text, respectively; and [:] is the concatenation operation. For the evaluation model $\mathcal{M}$, we choose the recently released OPT (Zhang et al., 2022) which is massively pretrained on over 180 billion tokens. To alleviate the potential measurement inaccuracy caused by the inductive bias of $\mathcal{M}$, we present the coherence score obtained by OPT with different model scales (i.e. 125M, 2.7B, and 13B parameters, respectively). (iv) Lastly, we also report the averaged length of the generated text (i.e. gen-length) from different decoding methods.

| Method | diversity $(\%) \uparrow$ | $\operatorname{MAUVE}(\%) \uparrow$ | gen-length | coherence $\uparrow$ |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  | OPT-125M | OPT-2.7B | OPT-13B |
| greedy | 5.38 | 7.91 | 147.28 | -0.72 | -0.58 | -0.60 |
| beam | 4.04 | 5.22 | 137.55 | -0.59 | -0.46 | -0.46 |
| typical | $87.98( \pm \mathbf{0 . 1 3})$ | $49.76( \pm \mathbf{3 . 9 0})$ | $142.11( \pm \mathbf{0 . 7 0})$ | $-2.45( \pm 0.02)$ | $-2.20( \pm \mathbf{0 . 0 1})$ | $-2.25( \pm \mathbf{0 . 0 1})$ |
| top- $k$ | $91.33( \pm \mathbf{0 . 0 5})$ | $89.64( \pm 2.37)$ | $142.48( \pm \mathbf{0 . 2 8})$ | $-2.59( \pm \mathbf{0 . 0 1})$ | $-2.35( \pm \mathbf{0 . 0 1})$ | $-2.42( \pm \mathbf{0 . 0 1})$ |
| nucleus | $93.61( \pm 0.07)$ | $87.89( \pm \mathbf{0 . 9 7})$ | $139.49( \pm \mathbf{0 . 9 9})$ | $-3.09( \pm \mathbf{0 . 0 1 )}$ | $-2.88( \pm \mathbf{0 . 0 1})$ | $-2.94( \pm \mathbf{0 . 0 1})$ |
| contrastive | 92.54 | 87.26 | 140.72 | -1.93 | -1.52 | -1.56 |

Table 2: Automatic evaluation results on the held-out set of WebText. $\uparrow$ means the higher the better.

Evaluation Results. Table 2 presents the experimental results. ${ }^{5}$ Firstly, as demonstrated by the diversity and MAUVE scores, greedy and beam search stuck in repetitive loops (see an example in Table 1) and produce less human-like results. These repetitions generated by greedy and beam search further lead to the high coherence score (i.e. log-likelihood) as judged by the OPT models. ${ }^{6}$ Secondly, we see that contrastive search achieves comparable results with other stochastic methods (i.e. typical, top- $k$, and nucleus sampling) on metrics including diversity, MAUVE, and gen-length. On the other hand, contrastive search performs notably better on the coherence metric as measured by OPT with different scales, suggesting it best maintains the semantic consistency between the generated text and the given prefix text.

### 4.1.2 Human Evaluation

We also conduct a human evaluation with the help of five graders proficient in English from a third-party grading platform. Specifically, we randomly select 200 prefixes from the held-out set of WebText and ask the annotators to assess the generation quality of different decoding methods, including (i) typical sampling; (ii) top- $k$ sampling; (iii) nucleus sampling; and (iv) contrastive search. The evaluation is conducted through pairwise comparisons by jointly considering the following aspects:
- Coherence: Whether the generated text is semantically consistent with the prefix text.[^2]- Fluency: Whether the generated text is fluent and easy to understand.
- Informativeness: Whether the generated text is diverse and contains interesting content.

Table 3 presents the human evaluation results. We can see that contrastive search outperforms all compared baselines by significant margins. It is worth noting that contrastive search even performs comparably with the human-written text as judged by Sign Test. These results indicate that (i) LMs can successfully learn the underlying knowledge (e.g. grammars and linguistic patterns) of human language through large-scale pretraining over unstructured text; and (ii) with the state-of-the-art decoding method, i.e. contratsive search, the intrinsic knowledge of LMs can be effectively elicited, therefore producing text with high quality.

| Method A is Better |  | $\frac{\text { Neutral }}{15.3 \%}$ | Method B is Better |  |
| :---: | :---: | :---: | :---: | :---: |
| contrastive | $71.2 \%^{\dagger}$ |  | $13.5 \%$ | typical |
| contrastive | $68.7 \%^{\dagger}$ | $14.7 \%$ | $16.6 \%$ | top- $k$ |
| contrastive | $64.2 \%^{\dagger}$ | $15.5 \%$ | $20.3 \%$ | nucleus |
| contrastive | $19.9 \% \\|$ | $57.9 \%$ | $\mathbf{2 2 . 2} \% \\|$ | human |

Table 3: Human evaluation on WebText. ${ }^{\dagger}$ means one method performs significantly better than the other as judged by Sign Test with $p$-value $<0.05$. " means one method performs comparably with the other with $p$-value $>0.4$.

### 4.2 Multilingual Open-ended Text Generation

Next, we extend our evaluation to multilingual open-ended text generation on 16 languages.

Evaluation Benchmark. We conduct experiments on the WIT dataset (Srinivasan et al., 2021) which consists of general-domain text collected from Wikipedia across 108 languages. For each evaluated language, we use the LMs to generate text conditioned on the prefix (restricted to 16 tokens) from the test set of WIT. The generation of text ends upon reaching an end-of-document token or a maximum length of 64 tokens.

Experiment Setups. To generate text, we use GPT-2 models from different languages that are publicly available in the Huggingface library (Wolf et al. 2019). We compare the results of contrastive search with the strong baseline, i.e. nucleus sampling $(p=0.95)]^{7}$ For the assessment of generated results, we rely on human evaluation following the same protocol as described in Section $\$ 4.1 .2$.

Evaluation Results. Our experimental results are presented in Table 4 From the results, we see that, for all evaluated languages, contrastive search significantly outperforms nucleus sampling as validated by Sign Test. Furthermore, on 12 out of the 16 evaluated languages (i.e. except for Hindi, Thai, Indonesia, and Russian), the performances of contrastive search are comparable with human-written texts. These evaluation results clearly demonstrate the generalization ability of contrastive search across different languages as well as its superiority over existing decoding methods.

## 5 Document Summarization

In this section, we present our experimental results on the task of document summarization.

Benchmark. We use the widely-used XSum dataset (Narayan et al. 2018) as our test bed which consists of news articles collected from $\mathrm{BBC}$ along with the corresponding one-sentence summaries.

Models and Decoding Methods. We conduct experiments using OPT models with different scales, ranging from $125 \mathrm{M}$ to $2.7 \mathrm{~B}$ parameters. To generate the summary, we apply different decoding methods, including beam search $(b=4)$, nucleus sampling $(p=0.95)$, and contrastive search $(k=5, \alpha=0.6)$.[^3]

| Spanish |  |  |  |  | French |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Method A is Better |  | $\frac{\text { Neutral }}{20.0 \%}$ | Method B is Better |  | Method A is Better |  | $\frac{\text { Neutral }}{2.7 \%}$ | Method B is Better |  |
| contrastive | $71.2 \%^{\dagger}$ |  | $8.8 \%$ | nucleus | contrastive | $86.3 \%^{\dagger}$ |  | $11.0 \%$ | nucleus |
| contrastive | $16.3 \% \\|$ | $63.9 \%$ | $19.8 \% \\|$ | human | contrastive | $22.8 \% \\|$ | $53.1 \%$ | $24.1 \% \\|$ | human |
| Chinese |  |  |  |  | $\overline{\text { Hindi }}$ |  |  |  |  |
| Method A is Better |  | Neutral | Method B is Better |  | Method A is Better |  | Neutral | Method B is Better |  |
| contrastive | $92.7 \%^{\dagger}$ | $4.9 \%$ | $2.4 \%$ | nucleus | contrastive | $47.9 \%^{\dagger}$ | $20.8 \%$ | $31.3 \%$ | nucleus |
| contrastive | $30.4 \% \\|$ | $41.8 \%$ | $27.8 \% \\|$ | human | contrastive | $20.4 \%$ | $43.5 \%$ | $36.1 \%^{\dagger}$ | human |
| Thai |  |  |  |  | Indonesia |  |  |  |  |
| Method A is Better |  | Neutral | Method B is Better |  | Method A is Better |  | Neutral | Method B is Better |  |
| contrastive | $68.1 \%^{\dagger}$ | $4.3 \%$ | $27.6 \%$ | nucleus | contrastive | $65.4 \%^{\dagger}$ | $6.0 \%$ | $28.6 \%$ | nucleus |
| contrastive | $18.9 \%$ | $49.4 \%$ | $31.7 \%^{\dagger}$ | human | contrastive | $16.9 \%$ | $44.3 \%$ | $38.8 \%^{\dagger}$ | human |
| Arabic |  |  |  |  | Japanese |  |  |  |  |
| Method A is Better |  | Neutral | Method B is Better |  | Method A is Better |  | Neutral | Method B is Better |  |
| contrastive | $84.1 \%^{\dagger}$ | $2.0 \%$ | $13.9 \%$ | nucleus | contrastive | $62.1 \%^{\dagger}$ | $18.0 \%$ | $19.9 \%$ | nucleus |
| contrastive | $24.6 \% \\|$ | $52.6 \%$ | $22.8 \% \\|$ | human | contrastive | $30.3 \% \\|$ | $33.1 \%$ | $36.6 \% \\|$ | human |
| English |  |  |  |  | Bengali |  |  |  |  |
| Method A | s Better | Neutral | Method | is Better | Method A | Better | Neutral | Method | is Better |
| contrastive | $72.3 \%^{\dagger}$ | $15.6 \%$ | $12.1 \%$ | nucleus | contrastive | $73.7 \%^{\dagger}$ | $8.1 \%$ | $18.2 \%$ | nucleus |
| contrastive | $23.3 \% \\|$ | $51.9 \%$ | $24.8 \%^{\\|}$ | human | contrastive | $24.8 \% \\|$ | $48.6 \%$ | $26.6 \% \\|$ | human |
|  |  | Korean |  |  |  |  | German |  |  |
| Method A | s Better | Neutral | Method | is Better | Method A | Better | Neutral | Method | is Better |
| contrastive | $69.2 \%^{\dagger}$ | $12.3 \%$ | $18.5 \%$ | nucleus | contrastive | $76.8 \%^{\dagger}$ | $13.3 \%$ | $9.9 \%$ | nucleus |
| contrastive | $29.8 \%{ }^{\\|}$ | $44.6 \%$ | $25.6 \% \\|$ | human | contrastive | $30.2 \%{ }^{\\|}$ | $43.0 \%$ | $26.3 \% \\|$ | human |
|  |  | Italian |  |  |  |  | ortugues |  |  |
| Method A | s Better | Neutral | Method | is Better | Method A | Better | Neutral | Method | is Better |
| contrastive | $69.7 \%^{\dagger}$ | $11.9 \%$ | $18.4 \%$ | nucleus | contrastive | $75.8 \%^{\dagger}$ | $13.1 \%$ | $11.1 \%$ | nucleus |
| contrastive | $23.7 \% \\|$ | $50.2 \%$ | $26.1 \% \\|$ | human | contrastive | $30.2 \% \\|$ | $43.9 \%$ | $25.9 \% \\|$ | human |
|  |  | Dutch |  |  |  |  | Russian |  |  |
| Method A | s Better | Neutral | Method | is Better | Method A | Better | Neutral | Method | is Better |
| contrastive | $85.6 \%^{\dagger}$ | $10.2 \%$ | $4.2 \%$ | nucleus | contrastive | $48.2 \%^{\dagger}$ | $21.3 \%$ | $30.5 \%$ | nucleus |
| contrastive | $\mathbf{3 3 . 2} \% \\|$ | $40.0 \%$ | $26.8 \%^{\\|}$ | human | contrastive | $18.9 \%$ | $41.3 \%$ | $39.8 \%^{\dagger}$ | human |

Table 4: Human evaluation on multilingual open-ended text generation. $\dagger$ means one method performs significantly better than the other as judged by Sign Test with $p$-value $<0.05$. " means one method performs comparably with the other with $p$-value $>0.4$.

Evaluation Setups. We test the model under two settings: zero-shot learning and in-context learning. (i) For the zero-shot setting, given the article (e.g. "This is an article."), we provide a natural language input "Article: $|n| n$ This is an article. $|n| n$ Summary:" to the model, and let it generate the summary autoregressively. (ii) For the in-context learning setting, when generating the summary, we follow previous studies (Brown et al. 2020; Zhang et al. 2022) and additionally provide the model with one or two in-context examples. Here, each in-context example is a pair of article and the corresponding summary ${ }^{8}$[^4]

| Shot | Method | OPT-125M |  |  | OPT-350M |  |  | OPT-1.3B |  |  | OPT-2.7B |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\mathrm{R}-1$ | $\mathrm{R}-2$ | $\mathrm{R}-\mathrm{L}$ | $\mathrm{R}-1$ | $\mathrm{R}-2$ | $\mathrm{R}-\mathrm{L}$ | $\mathrm{R}-1$ | $\mathrm{R}-2$ | $\mathrm{R}-\mathrm{L}$ | $\mathrm{R}-1$ | $\mathrm{R}-2$ | $\mathrm{R}-\mathrm{L}$ |
| Zero | beam | 9.05 | 1.24 | 6.78 | 1.46 | 0.23 | 1.11 | 11.35 | 1.51 | 8.50 | 5.76 | 0.85 | 4.36 |
|  | nucleus | 10.25 | 0.70 | 7.80 | 5.26 | 0.35 | 4.03 | 12.56 | 1.32 | 9.22 | 6.59 | 0.96 | 4.74 |
|  | contrastive | 12.68 | 1.75 | 9.59 | 1.11 | 0.16 | 0.86 | 16.76 | 3.17 | 12.64 | 4.95 | 1.03 | 3.81 |
| One | beam | 13.48 | 1.28 | 10.17 | 15.50 | 2.04 | 11.61 | 23.37 | 5.81 | 18.07 | 24.99 | 6.92 | 19.50 |
|  | nucleus | 12.36 | 0.84 | 9.49 | 12.27 | 0.77 | 9.38 | 10.01 | 1.80 | 11.61 | 18.14 | 3.03 | 13.82 |
|  | contrastive | 15.86 | 1.96 | 12.03 | 17.30 | 2.67 | 13.27 | 25.36 | 6.57 | 19.76 | 27.77 | 8.22 | 21.77 |
| Two | beam | 17.02 | 2.02 | 12.97 | 17.66 | 2.30 | 13.58 | 25.36 | 7.10 | 19.72 | 25.85 | 7.72 | 20.42 |
|  | nucleus | 12.75 | 0.87 | 9.71 | 12.45 | 0.99 | 9.69 | 17.99 | 2.89 | 13.69 | 19.07 | 3.57 | 14.64 |
|  | contrastive | 18.04 | 2.63 | 13.89 | 18.84 | 3.01 | 14.48 | 27.31 | 7.54 | 21.16 | 29.02 | 9.09 | 23.07 |

Table 5: Experimental results on the XSum benchmark, in which R-1, R-2, R-L denote ROUGE-1, ROUGE2, and ROUGE-L (Lin, 2004), respectively.

Results. Table 5 presents the evaluation results on XSum $\sqrt[9]{9}$ On the one hand, under the zero-shot setting, the performance of different methods are fluctuated across different LMs. We conjecture that such instability comes from the distinct inductive bias of LMs with different scales. On the other hand, by providing one or two in-context examples, we observe much better performances from the LMs which demonstrates its strong in-context learning ability (Brown et al. 2020 Zhang et al. 2022). Moreover, across all evaluation metrics, contrastive search consistently achieves the best results with notable margins, demonstrating its clear advantages over existing decoding methods.

## 6 Code Generation

We also conduct experiments on the task of code generation. In this task, given a natural language prompt, the LMs is required to generate a complete code snippet that fulfills the function specified by the prompt. Following previous studies (Chen et al. 2021, Nijkamp et al. 2022), we use the HumanEval dataset (Chen et al. 2021) as our testbed. We apply the CodeGen model (Nijkamp et al., 2022) with two model scales (i.e. $350 \mathrm{M}$ and $2 \mathrm{~B}$ parameters) and generate the code with three decoding methods, including beam search $(b=4)$, nucleus sampling $(p=0.95)$, and contrastive search $(k=3, \alpha=0.4)$.

| Model | Method | Pass Rate@1 (\%) |
| :---: | :---: | :---: |
|  | beam | 14.63 |
| CodeGen-350M-mono | nucleus | $5.08( \pm \mathbf{0 . 7 6})$ |
|  | contrastive | $\mathbf{1 5 . 2 4}$ |
|  | beam | 18.90 |
| CodeGen-2B-mono | nucleus | $10.98( \pm \mathbf{0 . 5 0})$ |
|  | contrastive | $\mathbf{2 1 . 9 5}$ |

Table 6: Code generation results on HumanEval dataset.

The evaluation results ${ }^{10}$ on Pass Rate@1 are shown in Table 6, from which we can draw the same conclusion that contrastive search outperforms other decoding methods.[^5]

## 7 Machine Translation

Lastly, we conduct experiments on the machine translation task using the IWSLT14 De-En dataset. Same as in Section $\$ 5$ we test OPT model $\$ 11$ with different scales using three decoding methods: (i) beam search $(b=4)$; (ii) nucleus sampling $(p=0.95)$; and contrastive search $(k=3, \alpha=0.4)$.

| Shot | Method | OPT-125M |  | OPT-350M |  | OPT-1.3B |  | OPT-2.7B |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | BLEU | COMET | $\mathrm{BLEU}$ | COMET | BLEU | COMET | $\mathrm{BLEU}$ | COMET |
| One | beam | $0.00_{( \pm \mathbf{0 . 0 0})}$ | $-1.24_{( \pm 0.09)}$ | $0.03_{( \pm 0.04)}$ | $-1.26_{( \pm 0.05)}$ | $5.53( \pm 1.33)$ | $-0.55( \pm \mathbf{0 . 1 4})$ | $14.06( \pm 0.67)$ | $0.07( \pm 0.05)$ |
|  | nucleus | $0.00( \pm \mathbf{0 . 0 0})$ | $-1.49( \pm 0.04)$ | $0.00( \pm \mathbf{0 . 0 0})$ | $-1.54( \pm 0.02)$ | $2.18( \pm 0.68)$ | $-0.85( \pm \mathbf{0 . 1 3})$ | $7.15( \pm 0.64)$ | $-0.22( \pm 0.05$ |
|  | contrastive | $0.05( \pm 0.07)$ | $-1.18( \pm 0.11)$ | $0.00( \pm \mathbf{0 . o 0})$ | $-1.30( \pm 0.01)$ | $7.10_{( \pm 1.33)}$ | $-0.41( \pm 0.08)$ | $12.98( \pm \mathbf{0 . 7 7})$ | $0.04( \pm \mathbf{0 . 0 4})$ |
| Few | beam | $0.00_{( \pm \mathbf{0 . o 0})}$ | $-1.45( \pm 0.09)$ | $0.08( \pm 0.11)$ | $-1.40( \pm 0.07)$ | $8.54_{( \pm 0.75)}$ | $-0.36( \pm 0.09)$ | $14.59_{( \pm 0.40)}$ | $0.08( \pm 0.01)$ |
|  | nucleus | $0.03( \pm 0.05)$ | $-1.54( \pm 0.04)$ | $0.03( \pm \mathbf{0 . 0 5})$ | $-1.57( \pm 0.06)$ | $4.22( \pm 0.68)$ | $-0.54( \pm 0.06)$ | $8.36( \pm \mathbf{0 . 3 0})$ | $-0.15( \pm \mathbf{0 . 0 3}$ |
|  | contrastive | 0.05 | $-1.38( \pm 0.12)$ | $0.05( \pm 0.07)$ | $-1.41( \pm 0.09)$ | $8.39( \pm 0.71)$ | $-0.29( \pm 0.05)$ | $13.52( \pm \mathbf{0 . 1 4})$ | $0.05( \pm \mathbf{0 . 0 1})$ |

Table 7: Machine translation results on IWSLT14 De-En dataset.

Table 7 presents the BLEU-4 (Papineni et al. 2002) and COMET (Rei et al. 2020) results under one-shot and few-shot settings ${ }^{12}$ Firstly, we see that smaller LMs (i.e. model scale $\leq 350 \mathrm{M}$ ) does not yield satisfactory BLEU scores. In contrast, by scaling up the model parameters, larger LMs (i.e. model scale $\geq 1.3 \mathrm{~B}$ ) starts to display emergent capability (Wei et al. 2022) and obtains notably better results. Secondly, contrastive search consistently outperforms nucleus sampling but performs slightly worse than beam search on a few evaluations on both the BLEU and COMET metrics. This reveals the advantage of maximization-based decoding methods, i.e. beam search, in tasks like machine translation that demand a high surface-level accuracy (e.g. BLEU).

## 8 Further Analysis

### 8.1 Relationship between Contrastive Search and the Isotropy of LMs

We conduct quantitative analysis on the importance of LMs' isotropy for contrastive search. ${ }^{13}$ To this end, given a prefix text $\boldsymbol{x}$, we measure the variance of degeneration penalty (see Eq. (33) as

$$
\begin{align*}
\operatorname{dp}(v ; \boldsymbol{x}, \theta) & =\max \left\{s\left(h_{v}, h_{x_{j}}\right): 1 \leq j \leq|\boldsymbol{x}|\right\} \\
\operatorname{var}(\boldsymbol{x} ; \theta) & =\sqrt{\frac{1}{k} \sum_{v \in V^{(k)}}(\operatorname{dp}(v ; \boldsymbol{x}, \theta)-\mu)^{2}} \tag{5}
\end{align*}
$$

where $s(\cdot, \cdot)$ computes the cosine similarity between token representations; $V^{(k)}$ is the set of top- $k$ predictions from the language model's probability distribution $p_{\theta}(\cdot \mid \boldsymbol{x})$; and $\mu=\frac{1}{k} \sum_{v \in V^{(k)}} \operatorname{dp}(v ; \boldsymbol{x}, \theta)$. Then, we define the averaged variance of degeneration penalty at each decoding step $t$ as

$$
\begin{equation*}
f(t ; \theta, \mathcal{D})=\frac{1}{\mathcal{D}} \sum_{\boldsymbol{x} \in \mathcal{D}} \operatorname{var}([\boldsymbol{x}: \hat{\boldsymbol{x}}] ; \theta) \tag{6}
\end{equation*}
$$

where $\mathcal{D}$ is a text corpus; $\boldsymbol{x}$ is the prefix text with a fixed length; $\hat{\boldsymbol{x}}$ is the text continuation generated by $\theta$ using contrastive search and $|\hat{\boldsymbol{x}}|=t$.

In our experiments, we follow similar procedures as in Section $\$ 4.1$ Specifically, we use GPT-2 models with different scales to generate text (up to 200 tokens) conditioned on the initial paragraph (restricted to 40[^6]tokens) of documents from the held-out set of WebText. The $k$ and $\alpha$ in contrastive search are set as 5 and 0.6 , respectively.

![](https://cdn.mathpix.com/cropped/2024_06_04_3301b272a3a38d6a3bc8g-11.jpg?height=767&width=1347&top_left_y=419&top_left_x=386)

Figure 3: Averaged variance of degeneration penalty of different GPT-2 models.

Figure 3 plots the results of different GPT-2 model $\sqrt{14}$ over the decoding steps. On the one hand, for GPT2-small/medium models that have a low isotropy in the representation space ( $\$ 3.1$, their averaged variance of degeneration penalty across the decoding process is closer to 0 . In other words, when applying contrastive search (see Eq. (3)), the degeneration penalties of different candidates are indistinguishable to each other. Therefore, the selection of the output is dominated by the model confidence term, making contrastive search degenerate to the greedy search method. On the other hand, with an isotropic representation space (\$3.1), the GPT-2-large/xl models display notably higher averaged variance in their degeneration penalties. During the decoding process, such high variance helps the LMs to avoid model degeneration, therefore generating high-quality text. In conclusion, an isotropic representation space of the LMs is essential for contrastive search to work well.

### 8.2 Contrastive Search versus Sampling Methods

Here, we provide further comparisons between contrastive search and other strong sampling methods (i.e. top- $k$ sampling and nucleus sampling). To this end, we follow Section \$4.1 and generate text using GPT-2large with different decoding methods. Specifically, we vary the hyperparameters of different methods, i.e. $k$ for top- $k$ sampling (from 5 to 640 ); $p$ for nucleus sampling (from 0.4 to 1.0); and $k$ for contrastive search (from 2 to 10 ) 15

The generated texts are evaluated from two aspects: (i) MAUVE and (ii) coherence (obtained with the OPT2.7B model) that are described in Section $\$ 4.1 .1$. Figure 4 plots the results of different decoding methods. We can see that, by varying the hyperparameters, the performances of sampling methods change drastically on the coherence metric. On the other hand, contrastive search best balances the trade-off between MAUVE and coherence. These results further verify the strong robustness of contrastive search over different selections of hyperparameters 16[^7]

![](https://cdn.mathpix.com/cropped/2024_06_04_3301b272a3a38d6a3bc8g-12.jpg?height=799&width=1087&top_left_y=278&top_left_x=519)

Figure 4: Contrastive search versus other sampling methods: (i) top- $k$; and (ii) nucleus sampling.

## 9 Conclusion and Future Work

In this work, we first investigate the isotropy of autoregressive LMs. Through extensive evaluations on LMs from 16 languages, we surprisingly find that the anisotropy problem only exists in the two specific English GPT-2-small/medium models. On the contrary, the rest of evaluated LMs are isotropic which is in contrast to the conclusion drawn by previous studies. Furthermore, based on our findings, we comprehensively evaluate contrastive search using off-the-shelf LMs on four generation tasks across 16 languages. Extensive human and automatic evaluations verify that contrastive search outperforms existing decoding methods by significant margins. More notably, on 12 out of the 16 evaluated languages, contrastive search performs comparably with human-level performances as judged by human evaluations.

For future work, we would like to suggest two research directions based on our study.

- Open-domain knowledge probing of LMs: Previous approaches (Petroni et al., 2019 Meng et al. 2022) for probing knowledge from LMs mainly focus on a fixed set of knowledge ontologies. Differently, contrastive search opens up another viable direction in which the world knowledge of the LMs with respect to a specific entity can be elicited through open-domain generation. In Appendix A.1 we provide an example on how to directly generate the factual knowledge of "DeepMind Company" from the LMs using contrastive search.
- Dataset synthesization: There has been a rising trend in using generative LMs to synthesize training data, therefore alleviating issues like data sparsity. By default, previous studies (Schick \& Schütze, 2021; Ye et al. 2022) use sampling methods to create synthetic data. However, it still remains as an open question on how the choice of decoding method affects the system's downstream performances. We hypothesize that replacing sampling methods with contrastive search could further improve the quality of synthetic data, therefore benefiting the performances of downstream systems.


## References

Sourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, and Lav R. Varshney. Mirostat: a neural text decoding algorithm that directly controls perplexity. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=W1G1JZEIy5_.

Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. March 2021. doi: 10.5281/zenodo.5297715. URL https: //doi.org/10.5281/zenodo.5297715

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and GPT-2 embeddings. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 55-65. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1006. URL https://doi.org/10.18653/v1/D19-1006.

Angela Fan, Mike Lewis, and Yann N. Dauphin. Hierarchical neural story generation. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pp. 889-898. Association for Computational Linguistics, 2018. doi: 10.18653/v1/P18-1082. URL https: //aclanthology.org/P18-1082/.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rygGQyrFvH.

Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.

Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Typical decoding for natural language generation. arXiv preprint arXiv:2202.00666, 2022.

Zaiqiao Meng, Fangyu Liu, Ehsan Shareghi, Yixuan Su, Charlotte Collins, and Nigel Collier. Rewirethen-probe: A contrastive recipe for probing biomedical knowledge of pre-trained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4798-4810, 2022.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id= Byj72udxe.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! Topicaware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. A conversational paradigm for program synthesis. arXiv preprint arXiv:2203.13474, 2022.

NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation, 2022. URL https://arxiv.org/abs/2207.04672

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40 th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.

Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.

Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. MAUVE: Measuring the gap between neural text and human text using divergence frontiers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=Tqx7nJp7PR.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. URL https://cdn.openai.com/better-language-models/ language_models_are_unsupervised_multitask_learners.pdf

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis \& insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.

Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. Comet: A neural framework for mt evaluation. arXiv preprint arXiv:2009.09025, 2020.

Timo Schick and Hinrich Schütze. Generating datasets with pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6943-6951, 2021.

Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipediabased image text dataset for multimodal multilingual machine learning. In Proceedings of the 44 th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 2443-2449, 2021 .

Yixuan Su and Jialu Xu. An empirical study on contrastive search and contrastive decoding for open-ended text generation. arXiv preprint arXiv:2211.10797, 2022.

Yixuan Su, Fangyu Liu, Zaiqiao Meng, Tian Lan, Lei Shu, Ehsan Shareghi, and Nigel Collier. Tacl: Improving BERT pre-training with token-aware contrastive learning. CoRR, abs/2111.04198, 2021. URL https: //arxiv.org/abs/2111.04198

Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, and Nigel Collier. Language models can see: Plugging visual controls in text generation. arXiv preprint arXiv:2205.02655, 2022a.

Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A contrastive framework for neural text generation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022b. URL https://openreview.net/ forum?id=V88BafmH9Pj

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

Jörg Tiedemann and Santhosh Thottingal. OPUS-MT - Building open translation services for the World. In Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT), Lisbon, Portugal, 2020.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.

Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/ forum?id=SJeYeONtvH

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.

Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. arXiv preprint arXiv:2206.02369, 2022.

Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. Zerogen: Efficient zero-shot learning via dataset generation. arXiv preprint arXiv:2202.07922, 2022.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.
