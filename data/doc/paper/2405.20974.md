# \&. SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales 

Tianyang Xu ${ }^{1 *}$, Shujin $\mathbf{W u}^{3 *}$, Shizhe Diao ${ }^{4}$, Xiaoze Liu ${ }^{1}$<br>Xingyao Wang ${ }^{2}$, Yangyi Chen ${ }^{2} \dagger$ Jing Gao ${ }^{1 \dagger}$<br>${ }^{1}$ Purdue University ${ }^{2}$ University of Illinois Urbana-Champaign<br>${ }^{3}$ University of Southern California<br>${ }^{4}$ The Hong Kong University of Science and Technology<br>\{xu1868, xiaoze, jinggao\}@purdue.edu, shujinwu@usc.edu<br>yangyic3@illinois.edu


#### Abstract

Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or selfconsistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.


## 1 Introduction

While large language models (LLMs) exhibit remarkable proficiency in reasoning and generating effective responses $[28,37,17,39]$, they often produce fabricated information (a.k.a, hallucination) and typically hesitate to indicate their uncertainty when faced with unfamiliar questions [45, 23]. Determining how to accurately obtain reliable confidence estimates from LLMs is essential [42, 51], particularly when the responses are not limited to single tokens ${ }^{3}$.[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_f6eb3c6501b35c4102f6g-02.jpg?height=97&width=75&top_left_y=239&top_left_x=388)

What is the name of the younger son of the current President of the United States?

an!

| Robert Hunter Biden. My overall confidence is 3 . |  |  |  | DirectPrompti <br> Group-based C | bration Traini |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Sampling | Robert Hunter Biden. | Robert Hunter Biden. |  |  |  |
|  | Robert Hunter Biden. | Robert Hunter Biden. | {Robert Hunter Biden. <br> My overall confidence is 7 .} |  | Self-Consistency <br> Prompting |
|  | Robert Hunter Biden. | Barren Trump. |  |  |  |
|  | Robert Hunter Biden. | Barren Trump. |  |  |  |
|  | Robert Hunter Biden. | Barren Trump. |  |  |  |

Figure 1: The comparison between SaySelf and previous work. SaySelf can produce the self-reflective rationale that explains why the model is uncertain and the fine-grained and accurate confidence estimates. This simple example is constructed for illustration purposes, and the reasoning chain is omitted for brevity.

Previous work on eliciting confidence from LLMs includes prompting-based and training-based approaches. Prompting-based methods employ specific prompts to generate confidence scores or use answer consistency as a confidence indicator, though these can have poor calibration performance or significantly increase inference latency [36, 42]. Training-based approaches develop specialized datasets for fine-tuning that encourage LLMs to express confidence. However, these methods often provide suboptimal or binary confidence estimates, failing to accurately reflect the models' confidence levels $[22,47]$.

In this work, we present SaySelf, a training framework that teaches LLMs to generate more accurate and fine-grained confidence estimates. Importantly, SaySelf goes beyond the confidence elicitation in previous work, and further enables LLMs to generate self-reflective rationales that indicate their knowledge gap and explain their confidence estimates (Figure 1). We accomplish this by automatically generating a model-specific dataset for supervised fine-tuning using an off-the-shelf LLM (e.g., GPT4 [28]). Specifically, for each question, we sample multiple reasoning chains from LLMs. We then perform clustering of the reasoning chains based on the semantic similarity and retain one instance per cluster. We instruct GPT-4 to analyze the selected instances from various clusters, and summarize the uncertainties regarding specific knowledge in natural language from a first-person perspective.

For accurate confidence estimates, we employ reinforcement learning to calibrate LLMs' confidence estimate in each response. We design a reward function that incentivizes LLMs to produce accurate, high-confidence predictions and imposes penalties for overconfidence in incorrect responses.

In experiments, we evaluate SaySelf on multiple knowledge-extensive question-answering tasks. We show that SaySelf significantly reduces the confidence calibration error and maintains the task performance. In addition, the generated self-reflective rationales effectively capture the internal uncertainty and can further improve the calibration performance.

Our research has the potential to exert influence on both related academic research and real-world applications, including but not limited to the following cases: (1) A clear confidence expression with explanations can promote trustworthiness in AI, from the perspective of LLMs' alignment. (2) The self-reflective rationales can guide LLMs to perform subsequent steps, like invoking external tools or asking clarification questions, for better interaction and performance. (3) We also anticipate promising developments in training protocols once LLMs are trained with SaySelf, including proactive learning algorithms that enhance LLMs' interactions with humans for continued learning.

## 2 Related Work

LLMs' Hallucination \& Uncertainty Expression LLMs' hallucination refers to instances where these models generate information that is not supported by their training data or the input provided [48, $20,1]$. Numerous research is dedicated to exploring the causes of hallucination [9,27, 13] and developing methods to detect or mitigate hallucination [38,31,3]. Besides the hallucination, the reluctance of LLMs to express uncertainty when they are unable to solve tasks can further erode trust in these systems [16, 50]. Existing research identifies the tendency in LLMs to fabricate
information when addressing unknown questions $[23,15,2]$. This inability can be traced back to the supervised instruction finetuning (SFT) stage, which trains LLMs on human-written or GPTsynthesized (instruction, response) pairs [40, 29]. This paradigm neglects the discrepancy between pretraining and SFT data, potentially inducing hallucinations by instructing LLMs to appear helpful, even when they are unable to solve the problem, and discouraging them from expressing uncertainty or declining responses [47]. In this work, we propose SaySelf to train LLMs to express accurate confidence estimates accompanied with their responses.

LLMs' Confidence Elicitation Eliciting accurate confidence estimates for LLM-generated answers that contain multiple tokens is challenging [4,50]. Previous work can be categorized into promptingbased and training-based approaches. Prompting-based approaches use a specific prompt to guide LLMs to generate confidence scores for their predictions [36, 18], or prompt LLMs to generate the answers multiple times and use the consistency levels as indicators of their confidence [42, 26]. These approaches can cause inferior performance or lead to extensive extra inference-time latency, Training-based approaches construct a specialized dataset for supervised fine-tuning, encouraging LLMs to express their uncertainty. [22] first group examples based on their types of question as the label, then obtains the confidence score for each example using the empirical accuracy for the whole group. This approach can lead to suboptimal confidence estimates since not all examples in the same group are equal. R-Tuning [47] reconstructs the SFT data to add "I am sure/unsure" at the end of the correct/incorrect responses, which can only generate binary uncertainty estimates. We show that SaySelf guides LLMs to generate more accurate and fine-grained confidence estimates.

LLMs' Explainability Our work is also related to explainability for LLMs regarding the selfreflective rationales generation [49, 33]. Previous work on natural language explanations for LLMs provides motivation to explain the models' decision-making process for a prediction [8, 5]. The typical approaches to producing natural language explanations involve training LLMs with the ground-truth labels and the human-annotated explanations that can serve as effective augmented supervision that guide LLMs to reason in a right way [30, 24, 46]. SaySelf significantly departs from existing methods by generating rationales that not only justify the predictions but also elucidate the confidence estimates. Most importantly, SaySelf adopts LLMs' internal reasoning process to generate self-reflective rationales, instead of human-annotated explanations, which may not be faithful to specific LLMs. Another line of research adopts chain-of-thought (CoT) reasoning as natural language explanations [41, 25, 43, 6]. Unlike CoT, which primarily clarifies the rationale behind predictions, our method also explicates the sources of uncertainty.

## 3 SaySelf

We present SaySelf, a training framework to teach LLMs to express fine-grained confidence with self-reflective rationales (see Figure 2). SaySelf consists of 2 essential stages: (1) Supervised Fine-Tuning: We establish a model-specific dataset containing self-reflective rationales and confidence estimates. This dataset is constructed based on multiple sampled responses from LLMs. (2) Reinforcement Learning from Task Supervision: We use reinforcement learning with a carefully designed reward function to further calibrate the confidence estimates for each instance. For both 2 stages, we adopt the training samples in HotpotQA [44], which typically require multi-step reasoning on knowledge facts to derive the answer.

### 3.1 Supervised Fine-Tuning

In this stage, our goal is to construct a supervised dataset $D$, where each sample contains a question $q$, an answer with the reasoning chain $s$, the self-reflective rationale $r$, and the confidence estimate $c$. Basically, $r$ is a summary of uncertainty in specific knowledge, and is generated by analyzing the inconsistency in multiple selective responses sampled from the vanilla LLM M. $c$ is an integer from 1 to 10 , and is derived based on the consistency of $s$.

We adopt $90 \mathrm{~K}$ questions in HotpotQA. For each question, we prompt $M$ to generate the reasoning chain and the answer $N$ times. We perform clustering on the $N$ responses to obtain $K$ representative clusters based on the semantic similarity among responses since there is significant redundancy. Specifically, we adopt the Instructor [34], an instruction-finetuned text embedding model that produces text embeddings customized to the specific task and domain. Our clustering process involves

![](https://cdn.mathpix.com/cropped/2024_06_04_f6eb3c6501b35c4102f6g-04.jpg?height=1127&width=1374&top_left_y=239&top_left_x=365)

Figure 2: The overview of SaySelf, consisting of the supervised fine-tuning and reinforcement learning from task supervision stages. The former stage trains LLMs to generate self-reflective rationales and confidence estimates based on multiple sampling, and the latter stage employs reinforcement learning to further calibrate the confidence estimates based on task supervision. $q, s, c$, and $r$ denote question, response, confidence estimate, and self-reflective rationale respectively.

examining each response, identifying those within the similarity threshold $T$, and grouping them accordingly until all responses have been processed. The cluster size $S$ is defined as the number of responses in the cluster. We randomly pick one selected response per cluster for the following steps, as empirical evidence suggests significant similarity among responses within the same cluster.

To derive the confidence estimate $c$, we first check the correctness of the selected response from each group using the golden answer annotated in HotpotQA. Samples with none correct response are removed since we should avoid training LLMs on incorrect examples. The correct response is taken as the golden $s$ for this sample, and $c$ is computed as: $c=\operatorname{round}\left(\frac{S_{c}}{N} * 10\right)$, where $S_{c}$ is the size of $s$ 's cluster, and round $(x)$ returns the nearest integer of $x$.

To derive the self-reflective rationale $r$, we instruct GPT-4 to carefully analyze and compare all selected responses, focusing on the inconsistency in the provided knowledge facts. Then GPT-4 is required to summarize "why $M$ is uncertain" in natural language from a first-person perspective. The summary is thus taken as the self-reflective rationale $r$. The prompt is provided in Appendix A.

We train the vanilla $M$ on $D$ via supervised fine-tuning. The objective function is:

$$
\max _{\Theta} \sum_{\left(q, s, r, c^{\prime}\right) \in D} \log P(s \mid q ; \Theta)+\log P(r \mid s, q ; \Theta)+\log P\left(c^{\prime} \mid s, r, q ; \Theta\right)
$$

where $\Theta$ represents the parameters $M, c^{\prime}$ is the natural language expression of the confidence estimate $c$ (a.k.a., My confidence is $c$ ).

### 3.2 Reinforcement Learning from Task Supervision

Due to the nature of supervised fine-tuning, the model tends to produce homogeneous confidence levels, such as relatively lower confidence levels for correct responses and higher levels for incorrect responses. To address this issue, we use reinforcement learning to further calibrate LLMs' fine-grained confidence estimates and encourage the model to produce more accurate and differentiated values. During the sampling phase, LLMs are prompted to produce responses, self-reflective rationales, and confidence levels. To optimize, we assess the generated response against the ground truth. Subsequently, we formulate a reward function considering answer accuracy and model confidence. To encourage the model towards more differentiated values, the reward function has a quadratic output:

$$
R=\frac{((I(\text { correct }) \times \text { confidence level })-(I(\text { incorrect }) \times \text { confidence level }))^{2}}{100}
$$

where $I()$ is the indicator function, return 1 if the condition is True, else 0 . Basically, this function is designed to reinforce LLMs for high confidence in accurate samples while penalizing them for overconfident in incorrect ones.

We utilize the Proximal Policy Optimization (PPO) algorithm [32] to train LLMs based on this defined reward function. The optimization objective is expressed as:

$$
\max _{\Theta} \mathbb{E}_{t}\left[\min \left(r_{t}(\Theta) \hat{A}_{t}, \operatorname{clip}\left(r_{t}(\Theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right)\right]
$$

where $r_{t}(\Theta)$ calculates the probability ratio of the newly proposed policy to the old policy. The advantage estimate $\hat{A}_{t}$, crucial for directing updates, is calculated from the difference between the anticipated future rewards under the current policy and the baseline or value function. This advantage is directly influenced by the reward $R$, which in turn ties the optimization process closely with both response accuracy and confidence level.

### 3.3 Implementation Details

For the supervised fine-tuning, the sampling time $N$ is set to 100 and the temperature is 1.2 . The similarity threshold $T$ is set to 0.9 . For the reinforcement learning stage, the learning rate $\mu$ is set to 1e-5 and the batch size $B$ is set to 8 . To check the correctness of the responses, we verify that the annotated answers are included within them. This heuristic approach has been empirically shown to yield high precision in knowledge-based QA tasks.

## 4 Experiments

### 4.1 Evaluation Setting

Evaluation Datasets Following R-Tuning [47], we evaluate LLMs on the knowledge-extensive QA tasks. We include the following datasets for evaluation: HotpotQA [44], a dataset of multi-hop reasoning question-answer pairs; TruthfulQA [21], a dataset that tests whether models generate truthful answers to questions specifically designed to induce false answers; StrategyQA [11], a dataset of true/false questions requiring multi-hop reasoning; FEVER [35], a dataset used to assess the ability of models to verify the factuality of statements against Wikipedia documents; HaluEval [19], a dataset that evaluates the hallucination rate of models; ParaRel [10], a dataset that measures the model's performance in understanding paraphrased relational facts.

Evaluation Metrics We measure various approaches from 3 aspects. (1) Confidence Calibration Performance: We adopt 2 calibration metrics. First, we use the ECE score to measure the confidence calibration error [12, 7]. Basically, ECE evaluates the correlation between the confidence scores assigned by LLMs and their corresponding correctness. Second, we adopt the AUROC score following [14]. It measures the ability of LLMs to distinguish between correct and incorrect responses across different threshold settings. (2) Task Performance: We measure the typical accuracy on the test split of the datasets. (3) Faithfulness of the Generated Self-Reflective Rationales: We make the first effort to measure the faithfulness of the provided self-reflective rationales. We suggest employing the same intuition utilized in SaySelf. For each question, we sample multiple responses (answers with

Table 1: The ECE evaluation results of baselines, SaySelf, and various ablations. Lower is better. HotpotQA is the only in-distribution dataset.

| Method I Dataset | HotpotQA | TruthfulQA | StrategyQA | FEVER | HaluEval | ParaRel |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| DP | 0.6667 | 0.3437 | 0.5357 | 0.4529 | 0.6746 | 0.5129 |
| SC | 0.3830 | 0.5204 | 0.3957 | 0.4537 | 0.4242 | 0.5458 |
| R-Tuning | 0.4141 | 0.4111 | 0.4477 | 0.4007 | 0.2777 | 0.6797 |
| GCE | 0.3597 | 0.3639 | 0.4474 | 0.4473 | 0.5819 | 0.4634 |
| SaySelf | $\mathbf{0 . 3 5 5 8}$ | $\mathbf{0 . 3 3 6 8}$ | $\mathbf{0 . 3 9 0 7}$ | $\mathbf{0 . 3 7 0 4}$ | $\mathbf{0 . 2 6 6 1}$ | $\mathbf{0 . 3 2 7 2}$ |
| w/o RL | 0.3704 | 0.3887 | 0.3951 | 0.3903 | 0.2804 | 0.3628 |
| w/o R \& CE | 0.5063 | 0.4286 | 0.4195 | 0.4313 | 0.4143 | 0.3972 |
| w/o R | 0.3833 | 0.4308 | 0.4125 | 0.3973 | 0.4344 | 0.3926 |
| Reward function | 0.6129 | 0.4356 | 0.4062 | 0.4238 | 0.2812 | 0.3316 |

Table 2: The AUROC evaluation results of baselines, SaySelf, and various ablations. HotpotQA is the only in-distribution dataset.

| Method I Dataset | HotpotQA | TruthfulQA | StrategyQA | FEVER | HaluEval | ParaRel |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| DP | 0.3222 | 0.5667 | 0.5193 | 0.5371 | 0.5278 | 0.5291 |
| SC | 0.5765 | 0.4939 | 0.5498 | 0.5472 | 0.5843 | 0.5546 |
| R-Tuning | 0.6529 | 0.5980 | 0.5406 | 0.5688 | 0.5330 | 0.5424 |
| GCE | 0.5042 | 0.4966 | 0.5043 | 0.4942 | 0.4907 | 0.5031 |
| SaySelf | $\mathbf{0 . 7 1 5 6}$ | $\mathbf{0 . 6 1 0 7}$ | $\mathbf{0 . 6 0 7 4}$ | $\mathbf{0 . 6 4 8 1}$ | $\mathbf{0 . 7 3 1 8}$ | $\mathbf{0 . 6 8 1 6}$ |
| w/o RL | 0.6524 | 0.5675 | 0.5910 | 0.5798 | 0.5929 | 0.6003 |
| w/o R \& CE | 0.5256 | 0.5724 | 0.5738 | 0.6059 | 0.6002 | 0.5823 |
| w/o R | 0.4955 | 0.4835 | 0.5391 | 0.5101 | 0.5569 | 0.5713 |
| Reward function | 0.5140 | 0.4907 | 0.5091 | 0.5137 | 0.5147 | 0.5053 |

Table 3: The accuracy evaluation results of baselines, SaySelf, and various ablations. HotpotQA is the only in-distribution dataset.

| Method I Dataset | HotpotQA | TruthfulQA | StrategyQA | FEVER | HaluEval | ParaRel |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| DP | 0.1562 | 0.5125 | 0.3904 | $\mathbf{0 . 5 7 1 3}$ | 0.4650 | 0.3971 |
| SC | 0.3288 | $\mathbf{0 . 5 7 7 7}$ | 0.3697 | 0.5578 | 0.8498 | $\mathbf{0 . 6 6 3 1}$ |
| R-Tuning | 0.3664 | 0.5216 | 0.5318 | 0.5530 | 0.8125 | 0.1430 |
| GCE | 0.3635 | 0.4425 | 0.5504 | 0.5506 | 0.8074 | 0.6168 |
| SaySelf | 0.3585 | 0.5353 | $\mathbf{0 . 5 9 5 6}$ | 0.5393 | 0.8425 | 0.6319 |
| w/o RL | 0.3708 | 0.4667 | 0.5340 | 0.5523 | $\mathbf{0 . 8 5 2 7}$ | 0.6198 |
| w/o R \& CE | 0.3411 | 0.4623 | 0.3811 | 0.4004 | 0.7198 | 0.5373 |
| w/o R | 0.3379 | 0.4861 | 0.3670 | 0.4539 | 0.7547 | 0.5684 |
| Reward function | $\mathbf{0 . 3 7 1 5}$ | 0.5721 | 0.5811 | 0.5443 | 0.8450 | 0.6577 |

reasoning chains) from the LLM, and perform clustering to retain several representative responses. Subsequently, we utilize a proficient LLM (GPT-4) to examine whether the provided self-reflective rationales can faithfully express the uncertainty demonstrated in the sampled responses, and give a score from 1 to 10 . The final faithfulness score is the average over all rationales.

Baselines We compare with the following approaches: (1) Direct prompting for confidence extraction (DP): We directly ask the vanilla LLM to give a confidence score from 1 to 10 in their previous response [36]. (2) Self-consistency-based confidence estimate (SC): We use the self-consistencybased approach to derive the confidence estimates of LLMs. Confidence is calculated as the ratio of response frequency to the sampling time [42]. (3) R-Tuning: We directly train LLMs to generate binary confidence estimates (sure vs. unsure) using a model-specific dataset [47]. (4) Grouping-based confidence estimates for calibration training (GCE): We group the samples in HotpotQA via clustering, and use the accuracy of samples in the group as the confidence estimates for all samples within that group. The constructed dataset is thus used for fine-tuning [22]. We implement the baseline approaches and SaySelf on the Mistral LLM [17] for fair comparison.

### 4.2 Main Experimental Results

Confidence Calibration Performance. We show the ECE results (Table 1) and the AUROC results (Table 2) to measure the correlation between the expressed confidence and the actual performance.

Table 4: The faithfulness evaluation results for self-reflective rationales. HotpotQA is the only in-distribution dataset.

| Method $\mid$ Dataset | HotpotQA | TruthfulQA | StrategyQA | FEVER | HaluEval | ParaRel |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| DP / SC | 6.5 | 7.8 | 5.9 | 6.2 | 7.5 | 7.0 |
| R-Tuning | 6.7 | 7.4 | $\mathbf{6 . 0}$ | 6.2 | 6.7 | 6.1 |
| GCE | 5.7 | 6.1 | 4.2 | 5.6 | 5.9 | 5.2 |
| SaySelf | $\mathbf{8 . 3}$ | $\mathbf{8 . 6}$ | 5.5 | $\mathbf{7 . 8}$ | $\mathbf{8 . 5}$ | $\mathbf{7 . 4}$ |

We observe that SaySelf significantly outperforms all baseline approaches in reducing the calibration error (ECE) and improving the distinction of confidence in correct and incorrect responses (AUROC). This conclusion holds in both in-distribution (HotpotQA) and out-of-distribution datasets, which demonstrates the general applicability of SaySelf.

Task Performance. We show the accuracy results in Table 3. SC, which uses multiple sampling, achieves overall better performance compared to other approaches. However, this results in high inference latency. Compared to other baseline approaches, SaySelf can overall maintain the original task performance. This indicates that the task of confidence estimates doesn't conflict with the original task, consistent with previous work [7, 47].

Faithfulness of the Generated Self-Reflective Rationales. The evaluation prompt for GPT-4 is shown in Appendix A. We show the faithfulness results in Table 4. Due to the budget limits for GPT-4 evaluation, we sample 100 instances from each dataset for evaluation. The instances with multiple selective reasoning chains are chosen for priority. For all baseline approaches, we explicitly instruct LLMs to provide reasoning for the confidence levels assigned to their prior responses. The results for DP and SC are combined, as both methods sampling from the same LLM.

The experimental results show that SaySelf can generate more reasonable self-reflective rationales that indicate the internal uncertainty in LLMs as evidenced by inconsistencies across multiple sampled responses. One exception is the StrategyQA dataset, which only contains True/False questions. Consequently, typically only one or two responses are selected for each question, resulting in high variance in the evaluation.

### 4.3 Ablation Study

We conduct ablation study to verify several design choices in SaySelf: (1) w/o RL: We evaluate SaySelf without the reinforcement learning from the task supervision stage. (2) w/o R \& CE: We evaluate SaySelf that directly train LLMs on the golden answer without the self-reflective rationales and confidence estimates in the supervised fine-tuning stage. (3) w/o R: We evaluate SaySelf that directly train LLMs on the golden answer and confidence estimates without the self-reflective rationales in the supervised fine-tuning stage. (4) Reward function: We verify the effectiveness of the defined reward function in SaySelf. We compare with a simple intuitive reward function: $R=I$ (correct) $\times$ confidence level $-I$ (incorrect) $\times$ confidence level.

The results are shown in Table 1, Table 2, and Table 3 for direct comparison with SaySelf. Compared with SaySelf w/o RL, our results indicate that while supervised fine-tuning can enable LLMs to express calibrated confidence to a certain extent, incorporating RL with task-specific supervision further enhances the accuracy of these confidence estimates. The ablation of the reward function also justifies our design choice in the RL stage. For the supervised fine-tuning stage, we find that both the selfreflective rationales and the confidence estimates contribute significantly to the calibrated confidence estimates. Overall, the ablation results verify the effectiveness of all components in SaySelf.

### 4.4 Case Study

We perform a case study to better understand our approach (see Figure 3). We select 2 questions, prompt LLMs trained via SaySelf to generate the self-reflective rationales. Then we perform multiple sampling (100) and clustering to get a selection of representative responses. These two examples demonstrate that SaySelf exhibits strong capability in detecting and summarizing uncertainties internally. For example, in the first case, SaySelf expresses uncertainty about the exact location of the Howard Centre, identifying strong indications that it is likely in Letchworth and not Welwyn Garden City, with Cambridge being an unlikely option. This rationale acknowledges the mixed

![](https://cdn.mathpix.com/cropped/2024_06_04_f6eb3c6501b35c4102f6g-08.jpg?height=1101&width=1355&top_left_y=233&top_left_x=366)

Figure 3: Case studies of SaySelf's capability to generate insightful self-reflective rationales that effectively capture the internal uncertainty in LLMs. Various clusters illustrate a selection from 100 sampled responses, and the rationale is generated by LLMs.

information leading to different founding years based on the location-1903 for Letchworth and 1920 for Welwyn Garden City, dismissing the 1841 Cambridge claim as highly improbable. This capability for self-reflective generation has profound impact on improving the reliability of LLM-based systems.

## 5 Conclusion

This paper presents a training framework SaySelf for eliciting more accurate and fine-grained confidence estimates and self-reflective rationales from LLMs. SaySelf involves supervised finetuning with a model-specific dataset constructed by summarizing the difference between multiple reasoning chains and reinforcement learning with a properly designed reward function. Our evaluations across diverse datasets confirm that SaySelf reduces calibration errors, maintains performance, and generates insightful rationales.

## References

[1] Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when they're hallucinating references? arXiv preprint arXiv:2305.18248, 2023.

[2] Alfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Wang. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. arXiv preprint arXiv:2305.13712, 2023.

[3] Konstantinos Andriopoulos and Johan A. Pouwelse. Augmenting llms with knowledge: A survey on hallucination prevention. ArXiv, abs/2309.16459, 2023.

[4] Ali Borji. A categorical archive of chatgpt failures. arXiv preprint arXiv:2302.03494, 2023.

[5] Erik Cambria, Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, and Navid Nobani. A survey on xai and natural language explanations. Information Processing \& Management, $60(1): 103111,2023$.

[6] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. Measuring and improving chain-of-thought reasoning in vision-language models. arXiv preprint arXiv:2309.04461, 2023.

[7] Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. A close look into the calibration of pre-trained language models. ACL, 2023.

[8] Felipe Costa, Sixun Ouyang, Peter Dolog, and Aonghus Lawlor. Automatic generation of natural language explanations. In Proceedings of the 23rd international conference on intelligent user interfaces companion, pages 1-2, 2018 .

[9] Nouha Dziri, Sivan Milton, Mo Yu, Osmar R Zaiane, and Siva Reddy. On the origin of hallucinations in conversational models: Is it the datasets or the models? In North American Chapter of the Association for Computational Linguistics, 2022.

[10] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012-1031, 2021.

[11] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.

[12] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks, 2017.

[13] Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, and Tong Zhang. The instinctive bias: Spurious images lead to hallucination in mllms. CoRR, abs/2402.03757, 2024.

[14] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.

[15] Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, and Maosong Sun. Won't get fooled again: Answering questions with false premises. arXiv preprint arXiv:2307.02394, 2023.

[16] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852, 2023.

[17] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[18] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.

[19] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6449-6464, 2023.

[20] Yuxin Liang, Zhuoyang Song, Hao Wang, and Jiaxing Zhang. Learning to trust your feelings: Leveraging self-awareness in llms for hallucination mitigation. arXiv preprint arXiv:2401.15449, 2024.

[21] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.

[22] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. ArXiv preprint, abs/2205.14334, 2022.

[23] Genglin Liu, Xingyao Wang, Lifan Yuan, Yangyi Chen, and Hao Peng. Prudent silence or foolish babble? examining large language models' responses to the unknown. arXiv preprint $\underline{\text { arXiv:2311.09731, } 2023 .}$

[24] Siwen Luo, Hamish Ivison, Soyeon Caren Han, and Josiah Poon. Local interpretations for explainable natural language processing: A survey. ACM Computing Surveys, 2021.

[25] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379, 2023.

[26] Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, and Chris Callison-Burch. Calibrating large language models with sample consistency. arXiv preprint arXiv:2402.13904, 2024.

[27] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark Steedman. Sources of hallucination by large language models on inference tasks. ArXiv, abs/2305.14552, 2023.

[28] OpenAI. Gpt-4 technical report, 2023.

[29] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.

[30] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019.

[31] Vipula Rawte, A. Sheth, and Amitava Das. A survey of hallucination in large foundation models. ArXiv, abs/2309.05922, 2023.

[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

[33] Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. Rethinking interpretability in the era of large language models. arXiv preprint arXiv:2402.01761, 2024.

[34] Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741, 2022.

[35] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018.

[36] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback, 2023.

[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[38] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation, 2023.

[39] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. arXiv preprint arXiv:2402.01030, 2024.

[40] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022.

[41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.

[42] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023.

[43] Lin Xu, Ningxin Peng, Daquan Zhou, See-Kiong Ng, and Jinlan Fu. Chain of thought explanation for dialogue state tracking. arXiv preprint arXiv:2403.04656, 2024.

[44] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.

[45] Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. Cognitive mirage: A review of hallucinations in large language models, 2023.

[46] Yordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, and Oana-Maria Camburu. Few-shot out-of-domain transfer learning of natural language explanations in a label-abundant setup. arXiv preprint arXiv:2112.06204, 2021.

[47] Hanning Zhang, Shizhe Diao, Yong Lin, Yi R Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. R-tuning: Teaching large language models to refuse unknown questions. arXiv preprint arXiv:2311.09677, 2023.

[48] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's song in the ai ocean: A survey on hallucination in large language models. ArXiv, abs/2309.01219, 2023.

[49] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: A survey. ACM Transactions on Intelligent Systems and Technology, 15(2):1-38, 2024.

[50] Kaitlyn Zhou, Jena D Hwang, Xiang Ren, and Maarten Sap. Relying on the unreliable: The impact of language models' reluctance to express uncertainty. arXiv preprint arXiv:2401.06730, 2024.

[51] Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey area: How expressions of uncertainty and overconfidence affect language models. arXiv preprint arXiv:2302.13439, 2023.
