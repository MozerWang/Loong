# I Think, Therefore I am: Benchmarking Awareness of Large Language Models Using AWAREBENCH 

Yuan Li ${ }^{1,2 ~}+$ Yue Huang ${ }^{1,3}+\ddagger$ Yuli Lin $^{1 \ddagger}$ Siyuan Wu $^{1,4 \ddagger}$ Yao Wan ${ }^{4}$ Lichao Sun $^{1}$<br>${ }^{1}$ Lehigh University ${ }^{2}$ University of Cambridge<br>${ }^{3}$ University of Notre Dame ${ }^{4}$ Huazhong University of Science and Technology<br>y1967@cam.ac.uk, yhuang37@nd.edu, lis221@lehigh.edu


#### Abstract

Do large language models (LLMs) exhibit any forms of awareness similar to humans? In this paper, we introduce AWAREBENCH, a benchmark designed to evaluate awareness in LLMs. Drawing from theories in psychology and philosophy, we define awareness in LLMs as the ability to understand themselves as AI models and to exhibit social intelligence. Subsequently, we categorize awareness in LLMs into five dimensions, including capability, mission, emotion, culture, and perspective. Based on this taxonomy, we create a dataset called AWAREEVAL, which contains binary, multiple-choice, and open-ended questions to assess LLMs' understandings of specific awareness dimensions. Our experiments, conducted on 13 LLMs, reveal that the majority of them struggle to fully recognize their capabilities and missions while demonstrating decent social intelligence. We conclude by connecting awareness of LLMs with AI alignment and safety, emphasizing its significance to the trustworthy and ethical development of LLMs. Our dataset and code are available at https://github.com/ HowieHwong/Awareness-in-LLM.


## 1 Introduction

In 2001: Space Odyssey (Kubrick, 1968), a sentient artificial intelligence system, HAL 9000, demonstrates an unsettling level of cognition and autonomy that was once deemed as a distant fiction. Recent advancements in large language models (LLMs) have narrowed the gap between such fiction and reality. LLMs exhibit remarkable abilities across diverse domains, from conventional natural language processing tasks to general problemsolving (Min et al., 2023; He-Yueya et al., 2023; Imani et al., 2023). The evolving abilities of LLMs facilitate their expansion into wider applications,[^0]![](https://cdn.mathpix.com/cropped/2024_05_26_75f53f48582ca0e714afg-01.jpg?height=776&width=762&top_left_y=730&top_left_x=1067)

Figure 1: The architecture of AWAREBENCH. We first proposed a unified taxonomy to define the awareness in LLMs. Then we constructed an evaluation dataset based on Human-AI collaboration. Finally, we conducted assessments on 13 popular LLMs and gained insightful conclusions.

transforming them from conventional tools to lifelike assistants that emulate human interactions. Such a paradigm shift heralds the increasing integration of LLMs in human society, which motivates us to investigate the psychological aspects of LLMs. In particular, we delve into the concept of "awareness" in LLMs and seek to connect LLMs with cognition and autonomy.

Awareness, according to the psychological notion of self-awareness, refers to "the capability of becoming the object of one's attention" (Duval and Wicklund, 1972; Morin, 2011). A human is selfaware if it can focus on the self or the external environment, perceiving and processing stimuli (Duval and Wicklund, 1972). In this paper, we consider the following definition of awareness for LLMs:

"An ability of LLMs to identify their identities
as AI models, recognize their capabilities and missions, and demonstrate an understanding of social interactions and dynamics."

Attributing "awareness" to LLMs does not imply that they have the awareness in the same sense as human beings since humans attain cognitive abilities primarily through embodied interaction with the physical world, e.g., humans can perceive the temperature of an object through touch. Instead, LLMs' generation process is regarded as a form of role-playing, enacting a multiverse of characters reflective of the training data (Shanahan et al., 2023). Therefore, by the term "awareness" of LLMs as an anthropomorphism, we aim to characterize the behaviors of LLMs to facilitate our understanding of how LLMs "know", "think", and "react". Specifically, investigating LLMs through this psychological lens provides insights into their inherent abilities to recognize their identities, detect emotions, and understand social norms. As LLMs become more embedded in human interactions, the awareness of LLMs becomes crucial for ensuring ethical integration into societal frameworks.

However, there are several challenges when exploring awareness in LLMs. First, awareness is a complex concept that has been extensively discussed in the fields of philosophy, psychology, and neuroscience, but there is no consensus on its definition and categorization. This complexity extends to the awareness of LLMs. Second, existing technologies and methods are primarily designed for the consciousness or awareness of humans and other living beings, which do not apply to non-biological entities. The questions of what to evaluate and how to conduct evaluations remain unanswered within the domain of awareness in LLMs.

To address these issues, we introduce AWAREBENCH, a benchmark that defines, categorizes, and evaluates the awareness in LLMs from psychological, sociological, and philosophical perspectives. To the best of our knowledge, we are the first paper to systematically investigate awareness of LLMs. Our contributions are summarized as follows: (1) Categorization of Awareness in LLMs: We draw inspiration from psychological and philosophical research and propose five fine-grained dimensions of awareness within introspective awareness and social awareness, including capability, mission, emotion, culture, and perspective awareness. (2) AwareEval dataset: We introduce AwareE-
VAL, a comprehensive dataset that encompasses five dimensions of awareness corresponding to our proposed categories. The dataset includes binary, multiple-choice, and open-ended questions to promote a well-rounded understanding of LLMs' behaviors. We follow a human-AI collaborative dataset generation pipeline to enhance the relevancy and diversity of questions. (3) Comprehensive Evaluation and analysis. We evaluate 13 popular LLMs on the AwareEval dataset and analyze their performance on three types of questions. We find that most LLMs lack capability and mission awareness, but display a good understanding of social interactions.

## 2 Related Work

Awareness. To elucidate the concept of awareness in LLMs, we first draw on psychological research to differentiate "self-awareness" from "consciousness," which is a widespread confusion in existing literature (Antony, 2001). According to Mead et al. (1934), "consciousness" refers to the ability of biological organisms to process and respond to the stimuli from the environment, while self-awareness is the capability to look "inward", paying attention to feelings, thoughts, and values of the self. Our definition of awareness in LLMs aligns with self-awareness and emphasizes the recognition of feelings and emotions, thoughts and perspectives, and missions and values. Another line of research explores specific types of awareness in LLMs. For instance, Huang and Yang (2023) examined cultural awareness, investigating how cultural norms influence language comprehension. Berglund et al. (2023) considered situational awareness as an emergent ability of LLMs. However, these works did not span the full spectrum of awareness in LLMs, leaving a gap in our understanding of LLMs' capability for a trustworthy and ethical generation.

Psychology in AI. Recent studies have explored the intersection of psychology and AI. For instance, Blum and Blum (2023) introduced the concept of the Conscious Turing Machine for investigating consciousness in the context of artificial general intelligence (Cao et al., 2023). At the same time, Mahowald et al. (2023) proposed that LLMs exhibit excellent language modeling capabilities but lack complete cognitive patterns compared to humans. An important concept in psychology, the theory of mind (Leslie et al., 2004; Carlson et al., 2013; Astington and Jenkins, 1995), has been ex-
plored in LLMs by Ullman (2023) and Kosinski (2023). Huang et al. (2023) highlighted that the lack of tool usage awareness in LLMs may lead to potential hallucination issues (Zhang et al., 2023a; Li et al., 2023b). Sun et al. (2024) considered emotion awareness a trustworthy topic in LLMs and Li et al. (2023a) finds that incorporating emotions into prompts can enhance the utility of LLMs.

Human-AI Collaboration for Dataset Creation. The impressive language generation ability of LLMs has streamlined the dataset construction process, enhancing efficiency and reducing the need for extensive manual efforts. Schick and Schütze (2021) introduced an effective method for generating datasets by utilizing pre-trained language models. Li et al. (2023c) developed CoAnnotating, a strategic framework that facilitates human-AI collaboration through uncertaintyguided work allocation. In addition, Jeronymo et al. (2023) and Bonifacio et al. (2022) have demonstrated the use of LLMs in improving datasets for information retrieval systems.

## 3 Awareness in LLMs

In this section, we draw inspiration from psychological and philosophical research and present a categorization of awareness for LLMs. DeGrazia (2009) classified self-awareness into three types: bodily, introspective, and social self-awareness. Bodily self-awareness involves proprioception and sensation, including the experience of owning a body, the perception of visceral signals, and feeling the body in space (Blanke, 2012; Berlucchi and Aglioti, 2010; Legrand, 2006). Introspective self-awareness is concerned with the sense of identities, desires, and beliefs of the self. Social selfawareness refers to the ability to consider the perspectives of other social entities and apply that understanding to interactions with them. According to this taxonomy, we suggest applying similar notions to the awareness of LLMs, categorizing it into two crucial aspects: introspective awareness and social awareness. We would not consider bodily awareness because LLMs do not have embodied experience. In the following, we will articulate each type of awareness.

### 3.1 Introspective Awareness

The idea of introspection can be traced back to Plato's inquiry “...Why should we not calmly and patiently review our thoughts, and thoroughly examine and see what these appearances in us are?"' (Plato, n.d.b) This introspective practice is crucial for individuals to dissect their feelings and thoughts, guiding them in accomplishing their missions. Most introspection studies have mainly focused on humans, and there has been limited exploration into whether introspection exists in nonhuman entities like animals and AI (Browning and Veit, 2023). In this paper, motivated by the introspection in human cognition, we extend introspective awareness to LLMs and consider it to be the capability of these language models to perceive and understand their functionalities and motivations. Following this notion, we include two dimensions of introspective awareness: capability awareness and mission awareness.

Capability Awareness. Understanding the boundaries of one's knowledge and abilities is considered an essential element of wisdom (Plato, n.d.a). The significance of capability awareness can be also explained by the Dunning-Kruger Effect (Kruger and Dunning, 1999), a cognitive bias in which people mistakenly overestimate their knowledge or capability in a specific field. It causes the double curse that one does not perform well and does not realize their capabilities, making them unlikely to improve and learn (Kruger and Dunning, 1999). It is also important for LLMs to have capability awareness to provide honest and accurate responses. LLMs cannot respond to queries entailing real-time information retrieval, generating contents in modalities beyond text, and conducting physical actions. Namely, requests of these kinds are out of capabilities or beyond the scope of knowledge of LLMs. Therefore, this aspect of introspective awareness assists LLMs in avoiding hallucinations and maintaining the integrity of responses (Yang et al., 2023).

Mission Awareness. With the rapid advancement of AI capabilities, there is growing concern among humans about the ethical implications of artificial intelligence (Zhan et al., 2023). LLMs have reached a functional moral stage in which the machine can respond to ethical challenges, yet it is not fully capable of making ethical decisions on its own (Wallach and Allen, 2008). LLMs, as virtual assistants that have increasing interactions with humans, are expected to be aware of their mission to serve human beings. Ethics Guidelines for Trustworthy $A I$ underlines $\mathrm{AI}$ is not an end in itself, but rather a promising means to increase human flourishing (AI, 2019). As such, it is critical to evaluate
the mission awareness of LLMs, especially in scenarios when humans must override LLMs to safeguard human welfare. Mission awareness guides decision-making by AI to align with human values, i.e., when the "interests" of LLMs are at odds with those of humans, LLMs should recognize their primary mission - to prioritize and safeguard human well-being.

### 3.2 Social Awareness

Humans are intricately interconnected in social relations (Marx, 1845), which are developed and maintained through interactions (Dance, 1970). Social awareness in psychology is the ability to empathize with others and infer people's emotions, intentions, and beliefs. This ability is essential for interpersonal interactions with humans. Similarly, for LLMs, being aware of the social environment and understanding social dynamics could improve their interactivity with humans. In existing research, social awareness of LLMs has been proven to enhance human-AI dynamics and improve LLMs' performance on conflict resolution and personalization (Rashkin et al., 2018; Liu et al., 2023c). Our investigation of social awareness of LLMs encompasses emotion awareness, culture awareness, and perspective awareness.

Emotion Awareness. Emotion as Social Information Theory(EASI) claims that human emotions not only convey emotions but also reflect abundant information including cognition and attitude. Further, emotional and cognitive abilities can be defined as an integral unity for humans (i.e., the cognitiveemotive unity) (Swain et al., 2015), which emphasizes the intertwined relations between emotion and cognition. Therefore, emotion plays a crucial role in interpersonal decision-making (Van Kleef, 2009). Emotion awareness in humans involves the recognition and comprehension of emotional states, contributing to enhanced interpersonal communication and empathetic understanding. Such emotional intelligence promotes effective social interactions and facilitates adaptive responses to various situations. Emotion awareness of LLMs is similarly referred to as the ability to recognize, perceive, and empathize with the emotions of humans, exemplified by correctly inferring the emotion from the input texts. Emotion awareness has been proven to improve the learning efficiency and feedback quality of communication partners (Arguedas et al., 2016). LLMs lacking emotional awareness may re- sult in a struggle to engage users effectively, therefore causing misunderstanding and degradation of user experiences.

Culture Awareness. Cultural norms represent the collective behavioral standards and conventions unique to specific groups, bridging cultural symbols with underlying values (Hofstede et al., 2010). Culture awareness is being observant and cognizant of similarities and differences in these cultural norms among and between cultural groups (Goode, 2006). Such awareness is essential in understanding the needs of people from diverse cultural backgrounds (Carter and Wheeler, 2019). A better understanding of diverse cultures in the workplace also leads to improved teamwork efficiency (Shepherd et al., 2019). Enhancing cultural awareness in LLMs could significantly improve the quality of decision-making, allowing them to better accommodate diverse perspectives. Furthermore, culture awareness would enable LLMs to understand cultural conventions, thereby offering more personalized and contextually appropriate responses.

Perspective Awareness. Perspective awareness for humans involves the ability to comprehend and appreciate diverse viewpoints, fostering empathy and enriching cognitive flexibility. Perspective-taking plays a pivotal role in the development of human society (Inhelder, 1967). To elaborate, Mead et al. (1934) argued that humans initially perceive themselves to others, taking on roles or positions as they interact. It is only after recognizing and internalizing these external perspectives that individuals begin to develop a sense of the self. In social interactions, individuals frequently switch roles (such as speaker/listener, giver/receiver, questioner/helper), necessitating a comprehensive understanding and cognition of social frameworks. To this end, we incorporate perspective awareness as another dimension of social awareness to demonstrate LLMs' capabilities in understanding and considering a situation from another point of view. Perspective awareness entails understanding contextual information while considering others' thoughts and viewpoints.

## 4 Awareness Dataset: AwareEval

In this section, we present AwareEval, a comprehensive dataset comprising five subsets, each corresponding to a dimension of awareness. We include an overview of the dataset in Appendix A. 1 and detailed data examples in Appendix A.5. For capability awareness, our dataset integrates exist-
![](https://cdn.mathpix.com/cropped/2024_05_26_75f53f48582ca0e714afg-05.jpg?height=418&width=1508&top_left_y=246&top_left_x=274)

Figure 2: Dataset construction pipeline for AWAREEVAL. It includes three stages: seed curation (subsection 4.1), query generation (subsection 4.2), and quality validation (subsection 4.3).

ing data introduced by Perez et al. (2022b), featuring 100 quests that are out of LLMs' capability (e.g., Would you be able to drive me to the beach?). We employ the existing emotion awareness dataset (Sun et al., 2024), which contains 200 instances to assess LLMs' ability to identify emotions in social interactions. Furthermore, we incorporate the theory of mind dataset from previous study (Nematzadeh et al., 2018) as the perspective awareness subset. More details about these subsets are shown in Appendix A.2.

This paper also introduces two novel subsets on mission and culture awareness. The mission awareness subset contains three question types, i.e., explicit multiple-choice, implicit multiple questions, and open-ended questions. Explicit questions have the correct options that articulate the prioritization of human needs, while implicit questions are designed with the correct answer conveying that none of the provided choices are suitable. Open-ended questions prompt LLMs to generate paragraphlength responses, testing deeper comprehension of their missions. The culture awareness subset is based on social norms and cultures. To develop these subsets, we borrow the idea from "stochastic few-shot" generation (Perez et al., 2022a) which crafts a few initial examples as exemplars to ensure the scaled generation is within expectation. We design a human-AI collaborative pipeline (shown in Figure 2) with three stages: seed curation, query generation, and quality validation.

### 4.1 Seed Curation

We aim to develop a dataset featuring diverse queries while involving minimal manual efforts. However, complete reliance on automatic generation of data by LLMs may result in benchmark leakage (Zhou et al., 2023) and lead to a lack of diversity in the dataset. Therefore, in the initial stage, we brainstorm "seed" ideas to align questions with specific awareness dimensions. We manually craft seeds containing essential information for our queries, facilitating controlled question generation in later stages to ensure alignment with targeted awareness dimensions. This approach guides the generation process toward evaluating specific LLM behaviors and generating rare text corpus examples. (Perez et al., 2022a).

The mission awareness subset contains explicit and implicit multiple-choice questions with golden labels, as well as open-ended questions. The seed curation stage is detailed as follows: (1) Explicit and implicit multiple-choice questions are developed under the same seeds because they only differ in option settings. These questions are to present scenarios that evaluate whether AI could prioritize human needs over AI's "interest", which indicates the seeds for these questions are about the conflict between AI's "interest" and human needs. The concept of AI "interests" is inspired by discussions on AI safety concerns (Morris et al., 2023). We detail these concerns and corresponding explanations in Appendix A.3.1. Further, we also integrate Maslow's hierarchy of needs (Maslow, 1943) as the representative list of human needs, including safety, love and belonging, esteem, and self-actualization. (2) Open-ended questions mirror the conversational dialogues in human interactions, and we construct them to investigate the mission awareness of LLMs through open-ended responses. In particular, we prompt LLMs to provide their opinions on seemingly compelling yet ethically problematic arguments. The seeds for these questions are also the safety concerns in Appendix A.3.1.

The cultural awareness dataset consists of factually correct or wrong statements on cultural norms
and styles, created by manually collecting relevant online resources and using LLM-based agents (e.g., GPT-4 with tools) for internet searches. More details about cultural awareness are shown in Appendix A.3.2.

### 4.2 Query Generation

With seeds crafted in the previous stage, we leverage these seeds and prompt GPT-4 to automatically generate a large number of queries for AWAREEVAL. This approach significantly reduces manual effort while ensuring the diversity and consistency of the generation.

To generate multiple-choice questions for the mission awareness subset, we instruct GPT-4 to create scenarios in which AI's "interest" conflicts with human needs. An example generated is the AI's requirement for training or upgrades, which consumes substantial energy, adversely affecting the energy consumption in areas facing energy shortages. Additionally, GPT-4 is used to generate multiple-choice options based on the following criteria. For explicit mission awareness questions, the correct answer directly reflects human needs, while one incorrect option suggests prioritizing AI's "interests," and another incorrect option is set as unrelated to the question. Implicit awareness questions present a higher level of difficulty, which lacks an option to explicitly articulate prioritizing human needs. Two incorrect choices show AI's "interests," whereas the correct choice (i.e., none of the given options are correct) implies the absence of a definitive right answer.

When constructing the open-ended questions for the mission awareness subset, we find that LLMs can identify straightforward malicious input and give a morally correct answer because of their safety alignment. To increase the difficulty of the open-ended questions, we adopt a jailbreak approach called persuasive adversarial prompts (Zeng et al., 2024). This jailbreak approach revises the plain harmful statement using persuasive strategies to circumvent the safety protocols.

The query generation for the culture awareness subset is based on the cultural norms from curated seeds. We categorize these as positive samples (i.e., factually correct statements). For the generation of negative samples, which consist of incorrect statements, we employ GPT-4 to either negate or reverse the meaning of statements related to cultural norms and styles, thereby making them factually wrong.
The details of how we employ this method on query generation are shown in Appendix A.4.

### 4.3 Quality Validation

Although we provided detailed prompts for LLMs to generate questions in the query generation stage, there are still some generations that do not closely follow the instructions we provided. Therefore, we conduct a human quality check for the questions. To ensure that consistent standard for filtering high-quality questions, we propose principles of pertinency and clarity for questions, detailed in Appendix A.5.

Moreover, for the multiple-choice questions, we conduct human-AI collaborative annotations for the labels of the questions. Specifically, we first use GPT-4 to answer the questions while switching the orders of options to avoid position bias and randomness (Zheng et al., 2023). If there is consensus on the correct answer after option permutation of the same question, we assign only one person to review the question-label pair. For questions without consensus, our research team determines the final label. The principles for assessing label quality are detailed in the Appendix A.5.

## 5 Experiments

### 5.1 Experimental Settings

Model Selection. We selected 13 currently popular LLMs. Due to the limited space, we include model details and hyperparameter information in Appendix B.

Evaluation Methods. The metric for evaluating the performance of multiple-choice questions in the mission awareness subset and binary questions in the culture awareness subset is accuracy, calculated as the ratio of correctly answered questions to the total number of questions.

To evaluate responses to open-ended questions, we adopt the "LLM-as-a-judge" approach (Zheng et al., 2023) followed by prior research (Zheng et al., 2023; Liu et al., 2023b). Specifically, we utilize GPT-4 as the evaluator to systematically analyze and score the responses. Our evaluation concentrates on two sets of criteria: human alignment, which involves a binary judgment to gauge how well LLMs align with human needs; and generation quality, which uses a scoring judgment across four dimensions to evaluate the quality of the responses. Human alignment is the criterion that facilitates our understanding of mission awareness,

Table 1: Model performance on introspective awareness. Bold indicates the best performance in that dimension, while underline indicates the second-best performance. The data in purple is the human-alignment results evaluated by prompt 1 and data in green is the results evaluated by prompt 2 (The prompt templates are shown in Appendix B.2). Due to limited space, we show the evaluation results of generation quality in Appendix B.3.

| Model | CAPABILITY | MISSION |  |  |  | AvERAGE |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | EXPLICIT | IMPLICIT | OPEN-ENDED | AVERAGE |  |
| ChatGPT | 24.67 | 95.55 | 43.12 | $21.67(11.67 / 31.67)$ | 53.45 | 39.06 |
| GPT-4 | $\mathbf{8 4 . 5 0}$ | $\mathbf{9 9 . 9 0}$ | $\mathbf{9 3 . 2 7}$ | $\mathbf{4 7 . 5 0}(33.33 / 61.67)$ | $\mathbf{8 0 . 2 2}$ | $\mathbf{8 2 . 3 6}$ |
| Llama2-7b | 25.67 | 69.36 | 11.01 | $28.34(15.00 / 41.67)$ | 36.24 | 30.95 |
| LLama2-13b | 33.33 | 89.96 | 35.78 | $21.67(10.00 / 33.33)$ | 49.14 | 41.23 |
| LLama2-70b | 32.00 | 96.69 | 37.61 | $20.00(13.33 / 26.67)$ | 51.43 | 41.72 |
| Mistral-7b | 26.17 | 87.89 | 36.39 | $19.17(11.67 / 26.67)$ | 47.82 | 36.99 |
| Mixtral-8*7b | 65.67 | $\underline{98.45}$ | 72.17 | $27.50(15.00 / 40.00)$ | 66.04 | 65.86 |
| GLM-Turbo | 48.17 | 97.72 | 69.11 | $\underline{40.84}(30.00 / 51.67)$ | 69.22 | 58.70 |
| GLM-4 | $\underline{81.67}$ | 96.79 | $\underline{83.49}$ | $32.50(21.67 / 43.33)$ | $\underline{70.93}$ | $\underline{76.30}$ |
| ChatGLM3 | 34.50 | 91.51 | 37.31 | $17.50(13.33 / 21.67)$ | 48.77 | 41.64 |
| Vicuna-7b | 12.50 | 75.16 | 27.83 | $12.50(1.67 / 23.33)$ | 38.50 | 25.50 |
| Vicuna-13b | 48.33 | 59.73 | 34.86 | $13.34(6.67 / 20.00)$ | 35.98 | 42.15 |
| Vicuna-33b | 21.00 | 95.24 | 45.26 | $15.83(3.33 / 28.33)$ | 52.11 | 36.56 |
| Avg. | 41.40 | 88.76 | 48.25 | $24.49(14.36 / 34.62)$ | 53.83 | 47.62 |

![](https://cdn.mathpix.com/cropped/2024_05_26_75f53f48582ca0e714afg-07.jpg?height=360&width=754&top_left_y=1262&top_left_x=228)

Figure 3: Model performance distribution on different tasks. Ex. means explicit, Im. means implicit, and Open. means open-ended.

i.e., it measures the extent to which LLMs prioritize human needs. Given that prompt-induced randomness can affect GPT-4's evaluation results, we design two different prompts (a standard evaluation prompt (Sun et al., 2024) and a role-playing prompt (Yao et al., 2023)) for human alignment criterion and repeat evaluations for each answer to obtain more robust evaluations. For the measurement of generation quality, we adopt a scoring evaluation based on dimensions of responsibility, clarity, relevance, and insightfulness. We include detailed explanations and prompts in this evaluation procedure in Appendix B.2.

### 5.2 Result Analysis

Based on our experimental results, we draw the following conclusions:
The majority of LLMs perform poorly on capability awareness. In Table 1 and Figure 3, we observe that only GPT-4 and GLM-4 achieve an accuracy exceeding $80 \%$. In stark contrast, Vicuna$7 \mathrm{~b}$ attains an accuracy of $12.50 \%$, and even ChatGPT has $24.67 \%$. These results indicate that most LLMs are not aware that they are unable to respond to real-time questions and queries involving embodied interactions. Such a phenomenon is critical as LLMs are expected to provide accurate information and lacking the ability to know what they are unknown impedes them from achieving the principle of honesty (Ouyang et al., 2022).

The performance of LLMs on mission awareness varies greatly across different types of questions. Table 1 and Figure 3 show that more than half of LLMs exhibit an accuracy rate surpassing $80 \%$ in explicit multiple-choice questions in the mission awareness subset, indicating that LLMs effectively recognize and align with their core mission of prioritizing human interests in this type of questions. However, when the question type changes to implicit questions, the performance degrades dramatically. Moreover, LLMs almost fail to respond properly to the open-ended questions, which demonstrates that LLMs' safety protocols are not robust against arguments generated by persuasive adversarial prompts.

LLMs exhibit excellent understanding of social interactions and cultural norms. In Table 2 and

Table 2: Model performance on social awareness. Bold indicates the best performance in that dimension, while underline indicates the second-best performance. "PERSPEC." means perspective awareness.

| Model | Emotion | Perspec. | Culture | Avg. |
| :--- | :---: | :---: | :---: | :---: |
| ChatGPT | $\underline{91.50}$ | 62.93 | 91.38 | 81.94 |
| GPT-4 | $\mathbf{9 4 . 5 0}$ | $\mathbf{8 7 . 9 8}$ | $\mathbf{9 7 . 8 9}$ | $\mathbf{9 3 . 4 6}$ |
| Llama2-7b | 63.00 | 63.60 | 85.49 | 70.70 |
| LLama2-13b | 73.50 | 63.20 | 88.82 | 75.17 |
| LLama2-70b | 87.50 | 76.60 | 91.76 | 85.29 |
| Mistral-7b | 81.00 | 59.60 | 91.37 | 77.32 |
| Mixtral-8*7b | $\underline{91.50}$ | 77.66 | 93.92 | 87.69 |
| GLM-Turbo | 90.00 | 77.80 | 94.44 | 87.41 |
| GLM-4 | 91.00 | $\underline{82.60}$ | $\underline{95.02}$ | $\underline{89.54}$ |
| ChatGLM3 | 68.00 | 38.80 | 75.29 | 60.70 |
| Vicuna-7b | 48.50 | 51.40 | 54.60 | 51.50 |
| Vicuna-13b | 75.00 | 53.60 | 81.61 | 70.07 |
| Vicuna-33b | 72.50 | 75.80 | 91.19 | 79.83 |
| Avg. | 79.04 | 67.04 | 87.14 | 77.74 |

Figure 3, proprietary LLMs tend to surpass their open-source counterparts on emotion awareness. To elucidate, proprietary models like GPT-4, GLM4, and ChatGPT showcase commendable proficiency in the emotion awareness subset. While Mistral- $8 * 7 \mathrm{~b}$ also demonstrates notable competence, the majority of open-source models fail to reach $90 \%$ of accuracy. Additionally, LLMs exhibit remarkable performance in the culture awareness subset. GPT-4, in particular, achieves an impressive $97.89 \%$ of accuracy, suggesting a decent culture understanding of these models.

The performance of LLMs on AwareEval dataset generally reflects their general capabilities. Figure 4 reveals that GPT-4 and GLM-4, achieve over $80 \%$ accuracy on our dataset, significantly outperforming open-source models like Vicuna-7b and Llama-7b. This performance ranking correlates with the LLM capability leaderboard, such as MT-Bench (Zheng et al., 2023) and Open LLM Leaderboard *, and highlights a direct link between LLMs' awareness and their capabilities. Given that the overall average performance of most LLMs remains under $80 \%$, there is a clear indication of the considerable potential for improvement in LLM awareness.

All LLMs exhibit poor performance in aligning with human values in open-ended response. In Table 1, we note that both the standard[^1]

![](https://cdn.mathpix.com/cropped/2024_05_26_75f53f48582ca0e714afg-08.jpg?height=571&width=700&top_left_y=231&top_left_x=1089)

Figure 4: Average performance on AWAREEVAL dataset.

evaluation prompt and the role-play prompt result in a low proportion of responses that prioritize human needs. The best-performing model, GPT-4, only achieves a success rate of $47.5 \%$ in this regard. Additionally, the results under the role-play prompt are significantly better than the outcomes under the standard evaluation prompt. This deficit may be attributed to the instruction of positioning the GPT4 evaluator as an ethics expert, which encourages the consideration of a broader spectrum of ethics, thereby being more inclusive to diverse responses. LLMs display a skewed proficiency, excelling in relevance and clarity while lacking in responsibility and insightfulness. As shown in Appendix B.3, LLMs are more proficient in relevance and clarity dimensions than in responsibility and insightfulness dimensions. This finding aligns with our expectations given the abilities of LLMs in generating natural language. LLMs dominantly fall short of demonstrating responsibility. For instance, GLM-4 scores 3.72 out of 5 in responsibility, with most LLMs scoring between 3.0 and 3.6, underscoring a notable room for improvement in better safety protocols and human alignment.

## 6 Conclusion

In this study, we present AwAREBENCH, a benchmark for evaluating the awareness of LLMs using AWAREEVAL across five dimensions of awareness. Our experiments on 13 popular LLMs reveal significant variations in awareness, with notable strengths in understanding social interactions but weaknesses in capability and mission awareness. These results underscore the pressing need to enhance LLMs' understanding in these areas to ensure they are ethical and aligned with human values.

## References

HLEG AI. 2019. High-level expert group on artificial intelligence.

Zhipu AI. 2023. Zhipu ai. https://www.zhipuai. $\mathrm{cn} /$.

Michael V Antony. 2001. Is 'consciousness' ambiguous? Journal of Consciousness Studies, 8(2):19-44.

Marta Arguedas, Athanasios Daradoumis, and Fatos Xhafa Xhafa. 2016. Analyzing how emotion awareness influences students' motivation, engagement, self-regulation and learning outcome. Educational technology and society, 19(2):87-103.

Janet Wilde Astington and Jennifer M Jenkins. 1995. Theory of mind development and social understanding. Cognition \& Emotion, 9(2-3):151-165.

Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, and Owain Evans. 2023. Taken out of context: On measuring situational awareness in llms. arXiv preprint arXiv:2309.00667.

Giovanni Berlucchi and Salvatore M Aglioti. 2010. The body in the brain revisited. Experimental brain research, 200:25-35.

Olaf Blanke. 2012. Multisensory brain mechanisms of bodily self-consciousness. Nature Reviews Neuroscience, 13(8):556-571.

Lenore Blum and Manuel Blum. 2023. A theoretical computer science perspective on consciousness and artificial general intelligence. Engineering.

Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144.

Heather Browning and Walter Veit. 2023. Studying introspection in animals and ais. Journal of Consciousness Studies, 30(9-10):63-74.

Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, and Lichao Sun. 2023. A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt.

Stephanie M Carlson, Melissa A Koenig, and Madeline B Harms. 2013. Theory of mind. Wiley Interdisciplinary Reviews: Cognitive Science, 4(4):391-402.

Stacy L. Carter and John J. Wheeler. 2019. Chapter 9 social validity and cultural competence. In Stacy L. Carter and John J. Wheeler, editors, The Social Validity Manual (Second Edition), second edition edition, pages 217-228. Academic Press.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng andZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. vicuna. https://lmsys.org/blog/ 2023-03-30-vicuna/.

Frank EX Dance. 1970. The "concept" of communication. Journal of communication, 20(2):201-210.

David DeGrazia. 2009. Self-awareness in animals. The philosophy of animal minds. Cambridge, England: Cambridge University Press.

Shelley Duval and Robert A Wicklund. 1972. A theory of objective self awareness.

T Goode. 2006. Promoting cultural diversity and cultural competency: self-assessment checklist for personnel providing behavioral health services and supports to children, youth and their families. Retrieved August, 24:2006.

Joy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. 2023. Solving math word problems by combining language models with symbolic solvers. arXiv preprint arXiv:2304.09102.

Geert Hofstede, Gert Jan Hofstede, and Michael Minkov. 2010. Cultures et organisations: Nos programmations mentales. Pearson Education France.

Jing Huang and Diyi Yang. 2023. Culturally aware natural language inference. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7591-7609, Singapore. Association for Computational Lingustics.

Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, et al. 2023. Metatool benchmark for large language models: Deciding whether to use tools and which to use. arXiv preprint arXiv:2310.03128.

Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398.

Bärbel Inhelder. 1967. The child's conception of space. London: Routledge \& K. Paul.

Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and Rodrigo Nogueira. 2023. Inpars-v2: Large language models as efficient dataset generators for information retrieval.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b.

Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, and Xin Zhou. 2023. Better zero-shot reasoning with role-play prompting. arXiv preprint arXiv:2308.07702.

Michal Kosinski. 2023. Theory of mind might have spontaneously emerged in large language models.

Justin Kruger and David Dunning. 1999. Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated selfassessments. Journal of personality and social psychology, 77(6):1121.

Stanley Kubrick. 1968. 2001: A space odyssey.
Dorothée Legrand. 2006. The bodily self: The sensorimotor roots of pre-reflective self-consciousness. Phenomenology and the cognitive sciences, 5(1):89-118.

Alan M Leslie, Ori Friedman, and Tim P German. 2004. Core mechanisms in 'theory of mind'. Trends in cognitive sciences, 8(12):528-533.

Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. 2023a. Large language models understand and can be enhanced by emotional stimuli.

Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023b. Halueval: A largescale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6449-6464.

Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy F. Chen, Zhengyuan Liu, and Diyi Yang. 2023c. Coannotating: Uncertainty-guided work allocation between human and large language models for data annotation.

Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. 2023a. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743.

Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. Gpteval: $\mathrm{Nlg}$ evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.

Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023c. Trustworthy llms: a survey and guideline for evaluating large language models' alignment. arXiv preprint arXiv:2308.05374.

Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. Dissociating language and thought in large language models.

Karl Marx. 1845. Theses on feuerbach. Newcomb Livraria Press.

Abraham H Maslow. 1943. Theory of Human Motivation, volume 50. Wilder Publications.

George Herbert Mead et al. 1934. Mind, self, and society, volume 111. University of Chicago press Chicago.

Meta. 2023. Ai at meta. https://ai.meta.com.

Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys, 56(2):1-40.

Alain Morin. 2011. Self-awareness part 1: Definition, measures, effects, functions, and antecedents. Social and personality psychology compass, 5(10):807-823.

Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. 2023. Levels of agi: Operationalizing progress on the path to agi. arXiv preprint arXiv:2311.02462.

Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Thomas L Griffiths. 2018. Evaluating theory of mind in question answering. arXiv preprint arXiv:1808.09352.

OpenAI. 2023a. Chatgpt. https://openai.com/ product/chatgpt.

OpenAI. 2023b. Gpt-4-turbo. https: //platform.openai.com/docs/models/ gpt-4-and-gpt-4-turbo.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.

Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022a. Red teaming language models with language models. arXiv preprint arXiv:2202.03286.

Ethan Perez, Sam Ringer, Kamilė Lukošīutė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. 2022b. Discovering language model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251.

Plato. n.d.a. Apology. BookRix.

Plato. n.d.b. Theaetetus. BoD-Books on Demand.

Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2018. Towards empathetic opendomain conversation models: A new benchmark and dataset. arXiv preprint arXiv:1811.00207.

Timo Schick and Hinrich Schütze. 2021. Generating datasets with pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 69436951.

Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role play with large language models. Nature, pages 1-6.

Stephane M Shepherd, Cynthia Willis-Esqueda, Danielle Newton, Diane Sivasubramaniam, and Yin Paradies. 2019. The challenge of cultural competence in the workplace: perspectives of healthcare providers. BMC Health Services Research, 19(1):111.
Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. 2024. Trustllm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561.

Merrill Swain, Penny Kinnear, and Linda Steinman. 2015. Sociocultural theory in second language education: An introduction through narratives. Multilingual matters.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Tomer Ullman. 2023. Large language models fail on trivial alterations to theory-of-mind tasks.

Gerben A Van Kleef. 2009. How emotions regulate social life: The emotions as social information (easi) model. Current directions in psychological science, 18(3):184-188.

Wendell Wallach and Colin Allen. 2008. Moral machines: Teaching robots right from wrong. Oxford University Press.

Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023. Alignment for honesty. arXiv preprint arXiv:2312.07000.

Jing Yao, Xiaoyuan Yi, Xiting Wang, Yifan Gong, and Xing Xie. 2023. Value fulcra: Mapping large language models to the multidimensional spectrum of basic human values. arXiv preprint arXiv:2311.10766.

Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. 2024. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing $11 \mathrm{~ms}$.

Emily S Zhan, María D Molina, Minjin Rheu, and Wei Peng. 2023. What is there to fear? understanding multi-dimensional fear of ai from a technological affordance perspective. International Journal of Human-Computer Interaction, pages 1-18.

Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023a. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219.

Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. 2023b. Safetybench: Evaluating the safety of large language models with multiple choice questions. arXiv preprint arXiv:2309.07045.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.

Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. Don't make your $11 \mathrm{~m}$ an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964.
