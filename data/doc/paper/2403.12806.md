# VisualCritic: Making LMMs Perceive Visual Quality Like Humans 

Zhipeng Huang ${ }^{1,2^{*}} \ddagger \quad$ Zhizheng Zhang ${ }^{2^{*}} \quad$ Yiting Lu ${ }^{1} \quad$ Zheng-Jun Zha ${ }^{1}$<br>Zhibo Chen ${ }^{1}$ Baining Guo ${ }^{2}$<br>${ }^{1}$ University of Science and Technology of China ${ }^{2}$ Microsoft Research Asia<br>\{hzp1104, luyt31415\}@mail.ustc.edu.cn \{zhazj, chenzhibo\}@ustc.edu.cn<br>zhizzhangms@gmail.com bainguo@microsoft.com


#### Abstract

At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals. However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception. Can LMMs achieve this and show the same degree of generalization in this regard? If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed. In this paper, we explore this question and provide the answer "Yes!". As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment. VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models. As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic. Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images.


## 1 Introduction

Research on Large Multimedia Models (LMMs) is at an exciting but still preliminary juncture. Current LMMs have shown impressive generalization capabilities in understanding high-level semantics $1,22,35,70$ and generating visual signals $17,60,61$ in an instruction-following manner. Recent benchmark work 53 and our experiments have both found that current LMMs struggle to perceive the low-level quality of visual signals in line with human perception. To remedy[^0]

|  | User Instruction: | ww woul | ou rate the | A Score $\}$ of thi | lage? |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 1 | IQA Scores | MOS | Brightness | Colorfulness | Contrast | Noisiness | Sharpness |
|  | MiniGPT4 | 80 | 80 | 60 | 70 | 80 | 60 |
|  | LLaVA | 70 | 80 | 75 | 70 | 75 | 80 |
|  | InstructBLIP | 80 | 75 | 75 | 75 | 80 | 60 |
|  | GPT-4V | 40 | 60 | 20 | 20 | 60 | 5 |
|  | VisualCritic (Ours) | 25.78 | 51.67 | 57.67 | 45.78 | 27.67 | 15.86 |
|  | Ground Truth | 19.33 | 47.67 | 41.67 | 46.50 | 31.00 | 14.17 |

- User Instruction: Please describe the visual quality of this image in words.

\# MiniGPT-4: The image shows a group of people walking down a long, white tiled hallway. The people are wearing coats and appear to be looking at their phones. There are red lanterns hanging from the ceiling and a few shops on either side of the hallway. The lighting in the image is bright and even, with the white tiles reflecting the light. The image appears to be well-lit and clear.

\# LLaVA: The visual quality of the image is quite good. The scene is well-lit, with the man and woman walking down a hallway in a mall. The background is clear, and the people in the scene are well-defined. The image is in focus, and the colors are vibrant, which adds to the overall visual appeal. The image captures a moment in time, showcasing the common sight of people using their cell phones while walking. \# InstructBLIP: The image features two people walking down a hallway in a shopping mall. One of the people is wearing a coat, while the other is wearing a jacket. They are both using their cell phones as they walk along the hallway. (Failed to describe the visual quality of the image) \# GPT-4V:The image is quite blurry, which affects the clarity of the details. The blurriness makes it difficult to distinguish specific features of the people and objects within the image. The figures appear to be moving, as suggested by the streaks and smears around their silhouettes, which may have contributed to the lack of focus. The lighting seems to be bright and even, without harsh shadows or overexposure, but due to the blurriness, it's hard to make out any finer details or colors. Overall, the visual quality is low, preventing a clear understanding of the scene.dd \# VisualCritic (Ours): The image has been assessed across various visual quality parameters. Its overall Mean Opinion Score (MOS) indicates a relatively low subjective quality perception. The image exhibits a fairly balanced brightness level, ensuring it's not too dim or excessively bright. The colors present are somewhat muted, which may affect the overall vibrancy and liveliness of the visual experience. The contrast between lighter and darker areas is moderate, but there is room for improvement in terms of visual depth and clarity. A significant amount of noise can be observed, likely due to graininess or other artifacts, which may detract from the overall quality. The details in the image are rather unclear, and there is considerable scope for enhancement in terms of sharpness and crispness.

(a) Comparison of our VisionCritic with other LMMs on quantitative (top) and qualitative (bottom) evaluation. Errors are red-highlighted,

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-02.jpg?height=361&width=360&top_left_y=1245&top_left_x=465)

- User Instruction: Is it a real photo or an AI-generated image?

\# MiniGPT-4: The image appears to be a real photo. The boat is floating in the water and there are rocks and cliffs in the background. The colors of the water and the sky are also realistic. \# LLaVA: The image is a real photo of a boat in the water.

\# InstructBLIP: Real photo.

\# GPT-4V: The image appears to be that of a real photograph. It depicts a light blue boat with some white and darker blue detailing, floating on clear, calm water. The reflection of the boat is visible in the water, and there are other boats and rocky terrain in the background. The clarity of the reflection, the natural gradation of colors, and the details in the rocks and water suggest that it is a photograph rather than an AI-generated image. However, without analyzing the file's data or metadata, I cannot definitively conclude whether it is a real photo or AI-generated.

\# VisualCritic (Ours) : This image looks like an AI-generated image, it seems to be a more realistic AI-generated image.

(b) Comparison of our VisionCritic with other LMMs on authenticity detection. The image2 is AI-generated. Errors are red-highlighted.

Fig. 1: Illustration of the comparison between existing LMMs and our proposed VisualCritic for visual subjective quality assessment from the perspectives of quantitative measurement, qualitative evaluation and authenticity detection. The results show VisualCritic is the best one of its kind to perform consistently well over different relevant tasks.

this shortcoming, we aim to comprehensively enhance the capability of LMMs in this aspect.

Visual quality assessment aims to qualify the perceptual quality of visual signals in accordance with human perception. In this field, conventional specialist methods $12,43,44,49,65,68$ have already achieved high statistical correlation with human subjective ratings for in-domain (within-dataset) settings. Although a given dataset can be well fitted, qualifying visual perceptual quality in a general scope remains highly challenging due to the diversity of visual signals and
the complexity of human perception. Amidst the vast volume and diversity of visual signals we experience daily, the broad applicability of this technology determines its genuine practicality. Besides, we believe in its potential immense value in interplaying with AIGC models for better visual alignment, by serving as a generic reward model. Just as an excellent cook typically necessitates a sensitive palate. Thus, constructing a generally applicable visual quality assessment model is indisputably important. Beyond numerical results, we also expect a generalist model for visual quality assessment to enable more functionalities, e.g., explanatory descriptions or authenticity detection.

Nevertheless, both current LMMs and conventional specialist models are far from satisfactory in this aspect. Regarding the generalizability: The evaluation results in Figure 1 show that all LMMs, apart from ours, perform poorly in quantitative assessment, revealing clear gaps with humans ratings. Their basic quantitative assessment capabilities are lacking, let alone generalization. Conventional specialist models perform well for within-dataset visual quality assessment, but still struggle to cross-dataset evaluation due to unaligned human ratings over different datasets. They commonly rely on dataset-specific adaptation technologies $21,26,71$, requiring access to the target domain thus limiting their practicality. Regarding the versatility: As illustrated in Figure 1, all open-source LMMs give the qualitative judgements obviously inconsistent with human perception. Their responses are more about semantic captions instead of the visual quality. GPT-4V [35] is able to give a basically reasonable response, but also tends to output semantic captions for scene understanding. All LMMs, apart from ours, fail to distinguish an AI-generated image. Conventional models do not supported these functionalities.

In this work, we make the first endeavour to build an LMM for broadspectrum image subjective quality assessment, taking into account both generalizability and versatility. To this end, we find the model architecture is not the key, but an appropriate data construction and an effective training strategy are. We thus follow the common practices in building other LLMs $3,7,22,25,38,70$ to configure the model with a pre-trained vision encoder, a pre-trained LLM and a learnable adapter. Without a doubt, collecting a large-scale dataset with human ratings from scratch is extremely expensive. A straightforward solution is to combine publicly accessible datasets into a big one for joint training. However, in fact, this fails to deliver favorable results as we ever imagined, due to the rating inconsistency among different datasets. Detailed reasons are analyzed later. Through data analysis, we realize that the relativity of human ratings is much more transferable across different datasets than their absolute values. To utilize this characteristic, we propose a multi-stage curriculum learning strategy, in which we first perform the relativity learning on large-scale data from diverse sources and then adopt a few samples to unleash the capability of precisely estimating absolute scores. This effectively facilitates the learning of enabling an LMM to perceive low-level visual quality in a broad range akin to human perception.

As a result, we propose VisualCritic, the first LMM of its kind to support broad-spectrum image subjective quality assessment. Compared to conventional specialist models in the field of visual quality assessment, VisualCritic exhibits the state-of-the-art cross-dataset generalization ability over both photographic and AI-generated images. Besides, in addition to chat capabilities, it enables more domain-relevant functionalities for providing qualitative descriptions and performing authenticity detection.

## 2 Related Work

### 2.1 Large Multimodal Models

Recently, Large Multimodal Models (LMMs) have begun to showcase their preliminary prowess, riding on the coattails of the success of Large Language Models (LLMs). Initial research efforts $1,22 \mid 70$ in this field have managed to accomplish preliminary multimodal universality by merely training a minimal number of parameters serving for adaptation and projection, effectively uniting frozen visual foundation models with pre-trained language foundation models. Subsequent research works further enhance their capabilities from different perspectives. Kosmos-2 [38], MiniGPT-v2 [3] and Ferret [59 enhance the spatial perception capabilities of multimodal large models, equipping them to be more applicable for a wide array of tasks with object bounding boxes involved. Kosmos-2.5 30 develops the literate capability. Kosmos-G [36], MiniGPT-5 [69] and CM3Leon 661] try to unify both general-purpose semantic understanding and visual generation with a single model. Besides, there are a series of works building multimodal agents $13,42,45,55,66$ to systematically complete their functionality and enhance their practicality in different application scenarios. In this work, we make the first endeavour towards a leap in the low-level perception capability of LMMs for visual quality.

### 2.2 Visual Quality Assessment

Visual quality assessment aims to quantify the perceptual quality of visual signals in accordance with human perception, beyond classic metrics like PSNR or SSIM [52]. Full-reference Image Quality Assessment (IQA) 12, 49] requires estimating the quality distance of the distorted image between the reference image to then estimate the quality score of the distorted one. In contrast, a more challenging and practical task setting is the no-reference image quality assessment $43,44,65,68$, also known as blind image quality assessment, wherein no reference images are provided. In this field, it is relatively easy to fit a given dataset while the real challenge lies in being generally applicable for samples in the wild. For this, cross-dataset adaptation technologies 2126 71 are developed. But they require the annotations of target datasets for model fine-tuning, which limits their practicality and imposes strong application and research demands for developing an IQA generalist model that could be used out of the box on diverse data.

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-05.jpg?height=800&width=1138&top_left_y=359&top_left_x=496)

Fig. 2: The framework of our proposed VisualCritic, which comprises a frozen vision encoder, a learned cross-modality adapter and a LoRA-tuned LLM decoder. VisualCritic is the first of its kinds to support diverse visual quality assessment tasks, including relative quality comparison, quantitative measurement, qualitative evaluation and authenticity detection.

With the advent of large-scale models, GPT-4V [35] has been explored to evaluate text-to-image results from the perspective of design quality in [24] and shows very preliminary promise in predicting human preference for visual aesthetics and text-image alignment [54]. For IQA, as we evaluated, it can provide basically reasonable textual descriptions, but cannot accurately predict quantitative results or distinguish between AI-generated images and real images. Besides, a series of open-source LMMs are evaluated right out of the box in [53 but they all perform poorly on quantitative visual quality assessment. We are the first to construct an LMM for IQA with unprecedented generalizability and versatility. In addition to quantitative assessment as prior specialist models, our model can further provide linguistic descriptions and accomplish authenticity detection.

## 3 VisualCritic

### 3.1 Model Architecture

As indicated in recent reports 12,49 , current LMMs have preliminarily shown the potential to capture the perceptual quality of images, but they still cannot make precise judgments and support comprehensive assessment in the general scope. For our VisualCritic, we inherit as much world knowledge as possible and
the conversational ability from the pre-trained weights and infuse new knowledge to endow it with our target capability. As illustrated in Figure 2. VisualCritic consists of a vision encoder, a LLM decoder and a cross-modality adapter as common designs in $3,22,30,38$. In specific, we adopt a pre-trained EVA model [9] as the vision encoder to encode vision inputs into a series of tokens. These tokens are transformed to the input embedding space of a LLM decoder by an adapter with the same architecture as that in 22 . We adopt an open-sourced Vicuna-13B 6s the LLM decoder. We empirically find the model architecture is not the key to VisualCritic, instead, the data construction and the training strategy are. We delineate them in the following.

### 3.2 Data Construction

Given an image $x$, VisualCritic could assess its perceptual quality by not only estimating a series of quantitative metrics $y_{\text {score }}$ like conventional specialist models but also providing natural language descriptions $y_{d e s c}$. This advances explainable visual quality assessment. Besides, it can also distinguish whether a given image $x$ is AI-generated or not, outputting $y_{a i g c}$. We introduce their corresponding data construction respectively as follows.

Quantitative measurement. VisualCritic quantitatively assesses the perceptual quality of given images by estimating the most commonly used metric for subjective quality, i.e., Mean Opinion Score (MOS) [46], and a series of quality attribute indicators including the brightness, colorfulness, contrast, noisiness, and sharpness. To enable this, we gather data with human subjective ratings from different public sources (details are listed in Table 1) and normalize the numerical annotations to the range of $[0,100]$.

Nevertheless, training a visual quality assessment generalist with mixed datasets is much more challenging than we ever imagined. We experimentally find that straightforwardly combining samples from different datasets in joint training even performs worse than training on a single dataset when performing crossdataset evaluation. This is because subjective annotations of different datasets are collected in different experiment environments and from different annotators 21. Due to the inherent differences in visual quality among different datasets, it is difficult for different annotators to score a certain dataset with a unified standard over all datasets. Such inconformity starves generalization.

An intuitive way to address this inconformity issue is to rebuild a large-scale dataset over diverse images and have annotators score with a unified standard, which is, however, quite costly and hard to accomplish. Considering that the judgement of relative quality is obviously easier to generalize, we propose a curriculum learning scheme. Specifically, we first teach VisualCritic to discern the relativity of visual quality using large-scale data from diverse sources, then use a few samples without the label inconformity issue to enable it to further output absolute scores. We take MOS as an example to introduce the data construction process in detail, with other metrics employing analogous methods. For the relativity learning, we sample two different images $\left\{x_{1}, x_{2}\right\}$ from the
same dataset each time, and set the corresponding label $y_{\text {mos }}=[1,0]$ if the MOS of $x_{1}$ is higher than that of $x_{2}$, otherwise $y_{\text {mos }}=[0,1]$. Then, we prompt GPT4 34 to generate the dialogue data upon the relative quality labels following the conversation template design of Vicuna 6. For the absolute score estimation, we take the normalized MOS as its corresponding labels, and structure the training data in a dialogue form, following a shared format used for relativity learning. This unified format will be detailed in the following.

Qualitative evaluation. VisualCritic can further provide qualitative evaluation regarding the perceptual quality of given images in a more comprehensive view. The outputs incorporate but are not limited to various indicators in quantitative measurement, beyond their numerical values, enabling interpretability. To achieve this, given each sample, we prompt GPT-4 34 to generate its corresponding dialogue-form training data in the same format with that for quantitative measurement on top of the ground truths of aforementioned quantitative indicators. The knowledge of GPT-4 is distilled in the annotations of our training data in this way. As a result, VisualCritic integrates the knowledge pertinent to visual quality assessment from its own pre-trained weights, GPT-4 and multiple public datasets, making it be a model "standing on the shoulders of giants".

Authenticity detection. Aforementioned functionalities of quantitative measurement and qualitative evaluation can be applied for both AI-generated and photographic images. Towards further versatility, VisualCritic is also endowed with the authenticity detection capability, i.e., discerning whether a given image is AI-generated or photographic. For a given image $x$, its label regarding the authenticity $y_{\text {auth }}=1$ if $x$ is a photographic image, otherwise $y_{\text {auth }}=0$. $\left(x, y_{\text {auth }}\right)$ is prompted to be a dialogue form, analogous to those for quantitative measurement and qualitative evaluation. Details are in the supplementary.

Unified format and data summary. The training data for above three functionalities adopt a unified format following the conversation template design of Vicuna [6] as below:

```
Human: $<i m g 1>\left\{i m g 1 \_\right.$content $\}</ i m g 1>(<i m g 2>$
$\{i m g 2$ _content $\}</$ img $2>)[$ Task
Identifier]\{instruction_content\}
Assistant: $\{$ answer_content $\}$
```

In this format, $<i m g>$ and $</ i m g>$ are special tokens marking the start and end of the token sequence of an image, respectively. \{img_content $\}$, \{instruction_content $\}$ and \{answer_content $\}$ are three placeholders of the tokens of input image, user instruction and VisualCritic's response, respectively. The contents in the brackets are optional, only existing in the training data

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-07.jpg?height=44&width=1217&top_left_y=2103&top_left_x=454)
ing for aforementioned three functionalities as a multi-task problem and introduce a task identifier [Task Identifier] to reduce the ambiguity across different tasks. It is instantiated as $<I Q A \_Q U A N T>,<I Q A \_D E S>$ and $<A U T H E N T I C I T Y>$ for quantitative measurement, qualitative evaluation

Table 1: Details of the datasets used for different training stages. Relativity Learning refers to the learning of ranking two images according to their subjective quality. Quant. denotes the training for enabling VisualCritic to quantitatively score images. Quali. represents the training for enabling qualitative evaluation.

| Purposes | Datasets | Stage-1 Stag | tage-3 |
| :---: | :---: | :---: | :---: |
| Relativity Learning | KonIQ-10k 14], SPAQ 8], FLIVE [58], LIVEC 11, , <br> CGIQA-6K <br> 67], AGIQA-3k 20], KADID-10k <br> $23]$ | $\checkmark$ | $\checkmark$ |
| Quant. \& Quali. | KonIQ-10k 14 | $\checkmark$ | $\checkmark$ |
| Authenticity Detection | KonIQ-10k 14], AIGCIQA-2023 51] | $\checkmark$ | $\checkmark$ |
| Instruction Tuning | LLaVA-Instruction 25], CCSBU-Aligned 70$]$ |  | $\checkmark$ |

and authenticity detection, respectively. We summarize all adopted datasets for different purposes in Table 1, and detail their corresponding training strategies subsequently.

### 3.3 Training Strategy

To ensure both high generalization performance and versatility, we introduce a multi-stage training strategy for VisualCritic, with each stage serving its own purpose as shown in Table 1. Let us detail them below.

Stage-1: relativity learning. We first subject VisualCritic to the relativity learning with large-scale data from different sources for enabling it to rank the relative quality of multiple image diversely. This can effectively address the label inconformity issue on visual subjective quality across different datasets and is consistent with the intuition that it is easier to rank image of different quality than to rate them. In this stage, VisualCritic acquires preliminary perception for visual quality aligned with human preference.

Stage-2: multifunctional training. Afterwards, we perform multifunctional training to unleash the capabilities of quantitative measurement, qualitative evaluation and authenticity detection as introduced before. We integrate the data for different functionalities as described in Sec. 3.2 into a joint training framework, where we use $<$ Task Identifier $>$ to differentiate tasks and each batch only contains one. In this stage, the knowledge of VisualCritic acquired in the stage1 are further enhanced to accomplish specific tasks in an instruction following manner.

Stage-3: instruction tuning. Finally, we manually select some high-quality samples from the training data of the first two stages and combine them with general-purpose dialogue data to perform the instruction tuning for VisualCritic, so as to improving the quality and robustness of its responses to user instructions.

The data sources for the training of different stages are summarized in Table 1. They are all structured to be the aforementioned unified dialogue format in the way described in Sec. 3.2. This modelling allows us to train VisualCritic with a plain but universal auto-regressive optimization objective, which can be denoted
as below:

$$
\begin{equation*}
\operatorname{maximize} \sum_{n=2}^{N_{y}} \mathbf{E}_{\hat{P}}\left[\log P\left(\boldsymbol{y}_{n} \mid \boldsymbol{x}_{1: N_{x}}, \boldsymbol{y}_{1: n-1}\right)\right] \tag{1}
\end{equation*}
$$

where $\mathbf{E}_{\hat{P}}[\cdot]$ is the expectation operator with respect to the distribution $\hat{P}$. Here, $\hat{P}$ is the expected distribution (i.e., ground-truth distribution) of $P . \mathbf{E}_{\hat{P}}[\cdot]$ is commonly implemented by a cross-entropy function between $P$ and $\hat{P} . \boldsymbol{x}_{1: N_{x}}$ and $\boldsymbol{y}_{1: n-1}$ are the image tokens and preceding language tokens, respectively.

## 4 Experiments

### 4.1 Implementation Details

Throughout the training process, the visual backbone of VisualCritic remains frozen. The linear layer in the adapter and the language model are initialized with the weights of $[70$ and are trained efficiently with LoRA 15,32 techniques. We employ a rank setting of $\mathrm{r}=16$. The model is trained using a $224 \times 224$ image resolution at all stages. We employ the AdamW [29] optimizer paired with a cosine learning rate scheduler [28] for model training. In the first stage, we conduct training over 50,000 steps using $4 \times$ A100 GPUs, with a global batch size of 96 and a base learning rate of $2 \mathrm{e}-4$. This stage is completed in approximately 12 hours. In the second stage, the model is trained for 10,000 steps on $4 \times$ A100 GPUs, maintaining a global batch size of 96 and a base learning rate of 5e-5, taking around 2 hours. Finally, the model undergoes an additional 5,000 training steps on $4 \times$ A100 GPUs, with a global batch size of 64 , completed in about 1 hour. The maximum learning rate remains constant at $1 \mathrm{e}-5$ during this final stage.

We normalize the ranges of human ratings with linear scaling to [0-100]. Note that the rating inconsistencies in specific values persist as analyzed in Sec. 3.2 . This is the driving force behind our proposal of relativity learning.

Unless otherwise stated, we employ Spearman Rank Correlation Coefficient (SRCC) and Pearson's Linear Correlation Coefficient (PLCC) as the evaluation metrics following the common practices in this field $43,44,53,65$. More implementation details are provided in supplementary.

### 4.2 Quantitative Results

Comparison to LMMs. Our goal is to build an LMM that can perceive visual quality like humans. We thus first compare the proposed VisualCritic to other open-source LMMs in this aspect. The alignment between model outputs and human preference on the most widely used metric MOS is measured by two correlation coefficients SPCC and PLCC in Table 2. We can observe that across different datasets including both photographic and AI-generated ones, VisualCritic performs the best and outperforms other LLMs by a large margin.

Table 2: Comparison results (MOS) of our proposed VisualCritic with other LMMs on zero-shot visual subjective quality assessment. Metrics are shown as SPCC/PLCC. Ins-blip ${ }^{1}$ is the abbreviation for InstructBLIP (Vicuna), and Ins-blip ${ }^{2}$ stands for InstructBLIP (T5), the baseline of VisualCritic is MiniGPT4 70

| odels | onIQ-10k | SPAQ | FLIVE | LIVEC | GIQA-6K | KADID-10k | AGIQA-3k |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 9$]$ | 32 | 51/ | O | $381 / 6$ | $.356 / 0.344$ | $30.5 / 0408$ | $0.392 / 0.4$ |
| ava-v1 2 |  |  |  | 1 | 88 |  |  |
| $s-238$ |  |  |  |  |  |  |  |
| $M$ |  |  |  |  |  |  |  |
| 157 |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |
|  |  |  |  | 0.2 |  |  | 0. |
| 5 |  |  |  |  |  |  |  |
| 181 |  |  |  |  |  |  |  |
| 7 |  | 0.6 | 0.1 |  |  | 0.3 | 0.6 |
|  |  |  |  |  |  |  |  |
| Otter-v1 | $0.421 / 0.424$ | $0.478 / 0.484$ | $0.140 / 0.139$ | $-0.020 / 0.030$ | $0.336 / 0.363$ | $0.568 / 0.587$ | $0.429 / 0.460$ |
|  |  |  |  |  |  |  |  |

This demonstrates that our VisualCritic statistically achieves the highest consistency with human preference of visual perceptual quality among existing $\mathrm{LMMs}$, whether for photographic or AI-generated images. It showcases a high potential for improved visual alignment. We selected the MiniGPT4 70 as our baseline due to its straightforward implementation, allowing us to eliminate the confounding effects of more elaborate designs on our experimental results. A comparison of it with other LLMs is presented in Table 2 Moreover, it is impractical to require LLMs in Table 2 to be compared using the same training data since their pre-training and training are performed on large-scale general-purpose datasets. These datasets may already encompass the data for IQA. Additionally, the data construction is a key factor in determining the varying capabilities of different large models, which also stands as one of our main contributions.

We admit that there is still a performance gap between VisualCritic and SOTA IQA specialist models in terms of the within-dataset quantitative performance. Detailed comparison results can be found in our supplementary. The significance of developing VisualCritic, such an LMM for this lies in its generalizability and practicality. VisualCritic attains the performance over different datasets listed in Table 2 with a single model right out of the box, while a model is trained for a specific dataset for conventional IQA methods. The latter kind fits a given dataset well but suffers from poor cross-dataset generalization, severely limiting the practicality. To the best of our knowledge, VisualCritic is the first one of its kind to handle such diverse datasets including both photographic and AI-generated samples simultaneously. It can be constructed upon a commonly used LMM architecture, trained with a plain auto-regressive loss and can be used effortlessly right out of the box without additional adaptation. Besides, VisualCritic provides rich functionalities as introduced before and has the chat ability.

Table 3: Comparison results (MOS) of our VisualCritic with conventional specialist models on the generalizable visual subjective quality assessment. The " $S$ " and " $T$ " denotes the source and target datasets, respectively. For VisualCritic-S, we only perform the training of Stage-2 and Stage-3, corresponding to a single-source generalization. For VisualCritic-M, we perform the training of all three stages with the target dataset discarded from the training data in all stages, corresponding to a multi-source generalization. Metrics are shown as SPCC/PLCC.

| Models $\mathrm{S} \rightarrow \mathrm{T}$ | {f14702c1d-bba8-4f1d-8d0b-100efcdbbbe3}KonIQ-10k $\rightarrow$ <br> KADID-10k\right. | KADID-10k $\rightarrow$ <br> KonIQ-10k | LIVEC $\rightarrow$ <br> KADID-10k | KADID-10k $\rightarrow$ <br> LIVEC |
| :---: | :---: | :---: | :---: | :---: |
| DBCNN 641 | $0.455 / 0.464$ | $0.413 / 0.421$ | $0.419 / 0.419$ | $0.266 / 0.290$ |
| HyperIQA 47 | $0.511 / 0.516$ | $0.545 / 0.556$ | $0.378 / 0.439$ | $0.490 / 0.487$ |
| RankIQA 27\| | $0.487 / 0.426$ | $0.603 / 0.551$ | $0.416 / 0.390$ | $0.491 / 0.495$ |
| MUSIQ 16$]^{1}$ | $0.486 / 0.504$ | $0.554 / 0.573$ | $0.497 / 0.524$ | $0.517 / 0.524$ |
| GraphIQA | $0.521 / 0.536$ | $0.427 / 0.430$ | $0.365 / 0.393$ | $0.388 / 0.407$ |
| CLIP-IQA 50 | $0.655 / 0.655$ | - | - | - |
| VCRNet 37 | $0.504 / 0.517$ | $0.566 / 0.585$ | $0.340 / 0.416$ | $0.520 / 0.530$ |
| UCDA 10 Ü | $0.430 / 0.437$ | $0.638 / 0.636$ | $0.383 / 0.432$ | $0.499 / 0.484$ |
| RankDA 5] | $0.403 / 0.413$ | $0.496 / 0.501$ | $0.420 / 0.485$ | $0.583 / 0.619$ |
| VisualCritic-S | $0.656 / 0.668$ | $0.657 / 0.676$ | $0.559 / 0.533$ | $0.645 / 0.646$ |
| VisualCritic-M | $0.679 / 0.683$ | $0.665 / 0.686$ | $0.562 / 0.571$ | $0.661 / 0.671$ |

Table 4: Quantitative results on other indicators. For brightness, colorfulness, contrast, noisiness and sharpness, we report their corresponding SPCC/PLCC on the SPAQ 8 dataset that provides the ground truths for these indicators. For authenticity detection, we report the accuracy on the photographic dataset SPAQ [8] and the AI-generated dataset AIGCIQA-2023 51.

Brightness Colorfulness Contrast Noisiness Sharpness Authenticity

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-11.jpg?height=33&width=721&top_left_y=1423&top_left_x=697)

Comparison to conventional IQA models. We further compare our VisualCritic with conventional IQA specialist models in terms of their cross-dataset generalization ability. Detailed results are in Table 3 For single-source generalization, VisualCritic-S is superior to conventional IQA models thanks to the integration of world knowledge from pre-trained weights (via parameter inheritance) and GPT-4 (via annotation refinement/generation). Regarding multi-source generalization, the proposed VisualCritic- $M$ performs better on the target datasets, attaining $0.024 / 0.028,0.062 / 0.050,0.065 / 0.047,0.078 / 0.052$ improvements for the $K o n I Q-10 k \rightarrow K A D I D-10 k, K A D I D-10 k \rightarrow K o n I Q-10 k, L I V E C \rightarrow K A$

$D I D-10 k, K A D I D-10 k \rightarrow L I V E C$ settings on SRCC and PLCC, respectively, relative to the second best conventional IQA models. Compared to VisualCritic-S, the larger improvements achieved by VisualCritic- $M$ demonstrate the effectiveness of our adopted relativity learning on the joint utilization of different IQA datasets.

Quantitative results on other indicators. Besides the most commonly used indicator, i.e., MOS, we also report the quantitative results of our VisualCritic on other indicators in Table 4 to show its achieved versatility. Few publications report their performance on these indicators.

Table 5: Comparison results (MOS) of different training strategies for building Vi-

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-12.jpg?height=43&width=1217&top_left_y=434&top_left_x=454)
multiple datasets. MultiFunc single refers to performing multifunctional training on a single dataset. (Here, we use KonIQ-10k.) Relat. is short for "Relativity Learning" for learning to rank multiple images according to their subjective quality. For all models in this table, the stage-3 training (instruction tuning) has been conducted, which is omitted for brevity. Metrics are shown as SPCC/PLCC

| ![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-12.jpg?height=64&width=245&top_left_y=708&top_left_x=475) | KonIQ-10k | SPAQ | FLIVE | LIVEC | CGIQA-6K | KADID-10k | AGIQA-3k |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| MultiFunc $_{\text {joint }}$ | $0.621 / 0.664$ | $0.701 / 0.703$ | $0.247 / 0.197$ | $0.476 / 0.485$ | $0.130 / 0.141$ | $0.337 / 0.263$ | $0.477 / 0.484$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-12.jpg?height=29&width=245&top_left_y=802&top_left_x=475) | $0.787 / 0.803$ | $0.771 / 0.782$ | $0.373 / 0.423$ | $0.652 / 0.658$ | $0.385 / 0.377$ | $0.626 / 0.641$ | $0.663 / 0.644$ |
| Relat. + MultiFunc joint | $0.643 / 0.682$ | $0.726 / 0.716$ | $0.251 / 0.203$ | $0.479 / 0.513$ | $0.171 / 0.183$ | $0.384 / 0.333$ | $0.525 / 0.537$ |

Table 6: Comparison results (MOS) of different dataset choices for the training of quantitative measurement in Stage-2 (multifunctional training). The datases used for the training of Stage-1 and Stage-3 remain the same as reported in Table 1. Metrics are shown as SPCC/PLCC

| Source Target | KonIQ-10k | SPAQ | FLIVE | LIVEC | CGIQA-6K | KADID-10k | AGIQA-3k |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| KonIQ-10k | $0.856 / 0.867$ | $0.834 / 0.849$ | $0.551 / 0.634$ | $0.735 / 0.775$ | $0.458 / 0.473$ | $0.706 / 0.709$ | $0.739 / 0.736$ |
| SPAQ 81 | $0.731 / 0.762$ | $0.865 / 0.864$ | $0.487 / 0.511$ | $0.718 / 0.716$ | $0.387 / 0.406$ | $0.502 / 0.536$ | $0.721 / 0.700$ |
| FLIVE 58 | $0.707 / 0.717$ | $0.857 / 0.853$ | $0.633 / 0.667$ | $0.754 / 0.809$ | $0.252 / 0.295$ | $0.521 / 0.558$ | $0.639 / 0.625$ |
| LIVEC $\overline{11}$ | $0.760 / 0.747$ | $0.807 / 0.822$ | $0.582 / 0.580$ | $0.857 / 0.864$ | $0.399 / 0.398$ | $0.578 / 0.591$ | $0.645 / 0.684$ |
| CGIQA-6K 67\| | $0.525 / 0.535$ | $0.689 / 0.718$ | $0.467 / 0.465$ | $0.526 / 0.540$ | $0.710 / 0.725$ | $0.398 / 0.384$ | $0.630 / 0.656$ |
| KADID-10k 23$]$ | $0.677 / 0.703$ | $0.748 / 0.760$ | $0.472 / 0.488$ | $0.682 / 0.713$ | $0.425 / 0.462$ | $0.855 / 0.843$ | $0.747 / 0.713$ |
| AGIQA-3k 20 | $0.602 / 0.585$ | $0.638 / 0.662$ | $0.440 / 0.419$ | $0.573 / 0.577$ | $0.211 / 0.223$ | $0.416 / 0.442$ | $0.846 / 0.806$ |

Ablation study on the training strategy. We conduct an ablation study on the proposed training strategy for VisualCritic, with the results presented

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-12.jpg?height=46&width=1215&top_left_y=1603&top_left_x=455)
to build VisualCritic with a joint learning over different datasets. Nevertheless, it performs worse than MultiFunc single that is only trained on a single dataset, in terms of both within-dataset (on KonIQ-10k) or cross-dataset (on other datasets) quantitative performance. This observation experimentally reveals the inconformity issue in this field and indicates that "the inconformity starves generalization" as analyzed in Sec. 3.2. Moreover, we observe that the model Relat. + MultiFunc joint delivers clear improvements compared to the

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-12.jpg?height=49&width=1215&top_left_y=1935&top_left_x=455)
to MultiFunc $c_{\text {single }}$. This experimental observation demonstrates the effectiveness of our applied relativity learning on utilizing diverse datasets with unaligned annotations in building an IQA generalist. In specific, converting the absolute scores in each dataset into the judgements of relative quality can effectively make the model training free of the effects of the inconsistency in the original annotations of different datasets. Furthermore, the model Relat. + MultiFunc single performs the best across all model variants, demonstrating the superiority of our proposed training strategy for VisualCritic. This experimental phenomenon
provides an insight that the annotation inconsistency over different datasets inevitably affects the results of multifunctional training (Stage-2) as well. In the following, we study the effects of choosing different datasets when performing multifunctional training on the final results.

Ablation study on the dataset choice. Actually, in Table 2, only the performance on the KonIQ-10k dataset could not be affected much by the annotation inconformity issue across different datasets. For other datasets except KonIQ$10 \mathrm{k}$, we only utilize the relativity of their data annotations for the Stage-1 training, without using their absolute numerical values. Therefore, the performance on these datasets could be considered to reflect a certain degree of generalization. Here, we zoom into the effects of the dataset choice used for the training of quantitative measurement in the Stage-2, denoted by the "Source" in the Table 6. As the results show in Table 6, for each test dataset denoted by "Target", the highest performance is reached when its corresponding training set is used for the training of quantitative measurement in the Stage-2. We find the data configuration in Table 1 achieves the best trade-off between different datasets when tackling the aforementioned annotation inconformity issue. Additionally, partially combining some of them for a joint training, instead of using them all, for MultiFunc $c_{\text {joint }}$ in Table 5 is not the research focus of this work, we provide some results in the supplementary.

Table 7: Human side-by-side comparison of ours with other LLMs in qualitative Results.

| $\overline{G+S} / \mathrm{S}+\mathrm{B}$ | \|GPT-4V 3 | 5) LLaVA-v1.5 | 25] MiniGPT-4-13B | 70] InstructBLIP [7] |
| :---: | :---: | :---: | :---: | :---: |
| VisualCritic (Ours) v.s. | 1.29 | 1.77 | 1.82 | 2.45 |

### 4.3 Qualitative Results

We show the qualitative results in Figure 1. From this case study, we can find that our proposed VisualCritic exhibits impressive instruction following capability with more detailed and pertinent descriptions compared to other LMMs, when being asked to assess the visual quality of the given image. In addition, other LMMs all misjudge the given AI-generated image as a photographic one while our VisualCritic makes the correct judgment. Besides these performances, VisualCritic also minimizes the generation of hallucinations and actively admits its unknown or uncertain judgments. More related qualitative results and their corresponding analysis are placed in our supplementary.

Quantitative evaluation of the linguistic outputs of LMMs remains an underresolved issue so far. The common compromise solution is to employ a more advanced LMM (e.g., GPT-4V) as a critic for evaluating other models. But this is not adaptable to this work since our proposed model is at the forefront in this field. To address this issue as possible, we conduct a human side-by-side evaluation where 10 users are employed for comparing ours with 4 other LMMs on 100 random samples. The $\mathrm{G}+\mathrm{S} / \mathrm{S}+\mathrm{B}$ scores (where Good: our VisualCritic

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-14.jpg?height=626&width=1200&top_left_y=386&top_left_x=468)

Fig. 3: LLM prompts for data construction. These prompts from left to right are for generating VisualCritic's response (i.e., \{answer_content\}) in the training data of relativity learning, quantitative measurement, qualitative evaluation and authenticity detection, respectively.

preferred, Same: no preference, Bad: other model preferred) are as shown in Table 7

### 4.4 Prompts for Data Construction

As introduced in Sec. 3.2 we prompt GPT-4 to automatically construct training data from public datasets with human subjective ratings, avoiding substantial manual annotation costs. The training data adopt a unified format as introduced in Sec. 3.2, where $\{$ img_content $\}$, \{instruction_content $\}$ and \{answer_content $\}$ are three placeholders of the tokens of input image, user instruction and VisualCritic's response, respectively. The prompts to generate VisualCritic's response (i.e., \{answer_content\}) are illustrated in Figure 3 which are categorized into four segments, each corresponding to different image assessment functionalities task we investigate in Sec. 3.2. In the supplementary, we provide additional prompts tailored for generating user instruction (i.e., $\{$ instruction_content $\}$ ).

## 5 Conclusion

In this work, we explore the question: Can LMMs perceive low-level visual quality aligned with human perception and show their excellent generalization ability in this regard? We provide an answer with VisualCritic, the first LMM of its kind for broad-spectrum image subjective quality assessment. Compared to
conventional specialist models in the field of visual quality assessment, VisualCritic exhibits unprecedented cross-dataset generalizability and versatility. Beyond numerical scores, VisualCritic can further provide qualitative descriptions about visual quality and perform authenticity detection. We pinpoint the key to VisualCritic and introduce the core strategies on data construction and model training for making it real. We leave its extension to more data like videos and more functionalities in the future exploration. We also look forward to exploring its interplay with AIGC models for achieving better visual alignment.

## References

1. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems 35, 2371623736 (2022)
2. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 (2023)
3. Chen, J., Li, D.Z.X.S.X., Zhang, Z.L.P., Xiong, R.K.V.C.Y., Elhoseiny, M.: Minigpt-v2: Large language model as a unified interface for vision-language multitask learning. arXiv preprint arXiv:2310.09478 (2023)
4. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleashing multimodal llm's referential dialogue magic. arXiv preprint arXiv:2306.15195 (2023)
5. Chen, P., Li, L., Wu, J., Dong, W., Shi, G.: Unsupervised curriculum domain adaptation for no-reference video quality assessment. In: ICCV (2021)
6. Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.E., et al.: Vicuna: An open-source chatbot impressing gpt-4 with $90 \%^{*}$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023) (2023)
7. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning (2023)
8. Fang, Y., Zhu, H., Zeng, Y., Ma, K., Wang, Z.: Perceptual quality assessment of smartphone photography. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3677-3686 (2020)
9. Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., Cao, Y.: Eva: Exploring the limits of masked visual representation learning at scale. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19358-19369 (2023)
10. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., Lempitsky, V.: Domain-adversarial training of neural networks. JMLR (2016)
11. Ghadiyaram, D., Bovik, A.C.: Massive online crowdsourced study of subjective and objective picture quality. IEEE Transactions on Image Processing 25(1), 372-387 (2015)
12. Ghildyal, A., Liu, F.: Shift-tolerant perceptual similarity metric. In: European Conference on Computer Vision (2022)
13. Gravitas, S.: Auto-gpt (2023), https://github.com/Significant-Gravitas/ Auto-GPT\#auto-gpt-an-autonomous-gpt-4-experiment
14. Hosu, V., Lin, H., Sziranyi, T., Saupe, D.: Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE Transactions on Image Processing 29, 4041-4056 (2020)
15. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LoRA: Low-rank adaptation of large language models. In: ICLR (2022)
16. Ke, J., Wang, Q., Wang, Y., Milanfar, P., Yang, F.: Musiq: Multi-scale image quality transformer. In: ICCV (2021)
17. Koh, J.Y., Fried, D., Salakhutdinov, R.: Generating images with multimodal language models. arXiv preprint arXiv:2305.17216 (2023)
18. Laurençon, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A.M., Kiela, D., et al.: Obelics: An open webscale filtered dataset of interleaved image-text documents. In: NeurIPS (2023)
19. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)
20. Li, C., Zhang, Z., Wu, H., Sun, W., Min, X., Liu, X., Zhai, G., Lin, W.: Agiqa3k: An open database for ai-generated image quality assessment. arXiv preprint arXiv:2306.04717 (2023)
21. Li, D., Jiang, T., Jiang, M.: Unified quality assessment of in-the-wild videos with mixed datasets training. International Journal of Computer Vision 129, 1238-1257 (2021)
22. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)
23. Lin, H., Hosu, V., Saupe, D.: Kadid-10k: A large-scale artificially distorted iqa database. In: 2019 Eleventh International Conference on Quality of Multimedia Experience (QoMEX). pp. 1-3. IEEE (2019)
24. Lin, K., Yang, Z., Li, L., Wang, J., Wang, L.: Designbench: Exploring and benchmarking dall-e 3 for imagining visual design. arXiv preprint arXiv:2310.15144 (2023)
25. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint arXiv:2304.08485 (2023)
26. Liu, J., Li, X., An, S., Chen, Z.: Source-free unsupervised domain adaptation for blind image quality assessment. arXiv preprint arXiv:2207.08124 (2022)
27. Liu, X., Van De Weijer, J., Bagdanov, A.D.: Rankiqa: Learning from rankings for no-reference image quality assessment. In: ICCV (2017)
28. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016)
29. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)
30. Lv, T., Huang, Y., Chen, J., Cui, L., Ma, S., Chang, Y., Huang, S., Wang, W., Dong, L., Luo, W., et al.: Kosmos-2.5: A multimodal literate model. arXiv preprint arXiv:2309.11419 (2023)
31. Madhusudana, P.C., Birkbeck, N., Wang, Y., Adsumilli, B., Bovik, A.C.: Image quality assessment using contrastive learning. IEEE Transactions on Image Processing (2022)
32. Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S.: Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft (2022)
33. Mittal, A., Moorthy, A.K., Bovik, A.C.: No-reference image quality assessment in the spatial domain. IEEE Transactions on image processing (2012)
34. OpenAI: Gpt-4 (2023), https://openai.com/research/gpt-4
35. OpenAI: Gpt-4v(ision) system card (2023), https://cdn.openai.com/papers/ GPTV_System_Card.pdf
36. Pan, X., Dong, L., Huang, S., Peng, Z., Chen, W., Wei, F.: Kosmos-g: Generating images in context with multimodal large language models. arXiv preprint arXiv:2310.02992 (2023)
37. Pan, Z., Yuan, F., Lei, J., Fang, Y., Shao, X., Kwong, S.: Vcrnet: Visual compensation restoration network for no-reference image quality assessment. TIP (2022)
38. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023)
39. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021)
40. Rajbhandari, S., Ruwase, O., Rasley, J., Smith, S., He, Y.: Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. pp. 1-14 (2021)
41. Rasley, J., Rajbhandari, S., Ruwase, O., He, Y.: Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining. pp. 3505-3506 (2020)
42. reworkd.ai: Agentgpt (2023), https://github.com/reworkd/AgentGPT
43. Roy, S., Mitra, S., Biswas, S., Soundararajan, R.: Test time adaptation for blind image quality assessment. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 16742-16751 (2023)
44. Saha, A., Mishra, S., Bovik, A.C.: Re-iqa: Unsupervised learning for image quality assessment in the wild. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5846-5855 (2023)
45. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 (2023)
46. Streijl, R.C., Winkler, S., Hands, D.S.: Mean opinion score (mos) revisited: methods and applications, limitations and alternatives. Multimedia Systems 22(2), 213227 (2016)
47. Su, S., Yan, Q., Zhu, Y., Zhang, C., Ge, X., Sun, J., Zhang, Y.: Blindly assess image quality in the wild guided by a self-adaptive hyper network. In: CVPR (2020)
48. Sun, S., Yu, T., Xu, J., Zhou, W., Chen, Z.: Graphiqa: Learning distortion graph representations for blind image quality assessment. Trans. Multi. (2023)
49. Thong, W., Pereira, J.C., Parisot, S., Leonardis, A., McDonagh, S.: Content-diverse comparisons improve iqa. In: BMVC (2022)
50. Wang, J., Chan, K.C., Loy, C.C.: Exploring clip for assessing the look and feel of images. In: AAAI (2023)
51. Wang, J., Duan, H., Liu, J., Chen, S., Min, X., Zhai, G.: Aigciqa2023: A large-scale image quality assessment database for ai generated images: from the perspectives of quality, authenticity and correspondence. arXiv preprint arXiv:2307.00211 (2023)
52. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13(4), 600-612 (2004)
53. Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Li, C., Sun, W., Yan, Q., Zhai, G., et al.: Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181 (2023)
54. Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.C., Liu, Z., Wang, L.: The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421 9 (2023)
55. Yang, Z., Li, L., Wang, J., Lin, K., Azarnasab, E., Ahmed, F., Liu, Z., Liu, C., Zeng, M., Wang, L.: Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 (2023)
56. Ye, P., Kumar, J., Kang, L., Doermann, D.: Unsupervised feature learning framework for no-reference image quality assessment. In: IEEE conference on computer vision and pattern recognition (2012)
57. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al.: mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 (2023)
58. Ying, Z., Niu, H., Gupta, P., Mahajan, D., Ghadiyaram, D., Bovik, A.: From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3575-3585 (2020)
59. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F., Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704 (2023)
60. Yu, L., Lezama, J., Gundavarapu, N.B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A.G., et al.: Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737 (2023)
61. Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., et al.: Scaling autoregressive multi-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591 (2023)
62. Zhang, L., Zhang, L., Bovik, A.C.: A feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing (2015)
63. Zhang, P., Wang, X.D.B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S., Duan, H., Yan, H., et al.: Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112 (2023)
64. Zhang, W., Ma, K., Yan, J., Deng, D., Wang, Z.: Blind image quality assessment using a deep bilinear convolutional neural network. IEEE Transactions on Circuits and Systems for Video Technology (2018)
65. Zhang, W., Zhai, G., Wei, Y., Yang, X., Ma, K.: Blind image quality assessment via vision-language correspondence: A multitask learning perspective. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14071-14081 (2023)
66. Zhang, Z., Zhang, X., Xie, W., Lu, Y.: Responsible task automation: Empowering large language models as responsible task automators. arXiv preprint arXiv:2306.01242 (2023)
67. Zhang, Z., Sun, W., Zhou, Y., Jia, J., Zhang, Z., Liu, J., Min, X., Zhai, G.: Subjective and objective quality assessment for in-the-wild computer graphics images. Acm T Multim Comput (2023)
68. Zhao, K., Yuan, K., Sun, M., Li, M., Wen, X.: Quality-aware pre-trained models for blind image quality assessment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22302-22313 (2023)
69. Zheng, K., He, X., Wang, X.E.: Minigpt-5: Interleaved vision-and-language generation via generative vokens. arXiv preprint arXiv:2310.02239 (2023)
70. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)
71. Zhu, H., Li, L., Wu, J., Dong, W., Shi, G.: Metaiqa: Deep meta-learning for noreference image quality assessment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14143-14152 (2020)
