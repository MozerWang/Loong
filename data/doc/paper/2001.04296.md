# High-Fidelity Synthesis with Disentangled Representation 

Wonkwang Lee ${ }^{1}$ Donggyun Kim ${ }^{1}$ Seunghoon Hong ${ }^{1}$ Honglak Lee ${ }^{2,3}$<br>${ }^{1}$ KAIST $\quad{ }^{2}$ University of Michigan $\quad{ }^{3}$ Google Brain<br>${ }^{1}\{$ wonkwang.lee,kdgyun425, seunghoon.hong\}@kaist.ac.kr 2,3honglak@\{umich.edu,google.com\}

![](https://cdn.mathpix.com/cropped/2024_06_04_b525d079938b7558d02fg-01.jpg?height=598&width=1728&top_left_y=584&top_left_x=164)

Figure 1. Generated images on the CelebA-HQ dataset [34]. The proposed framework allows synthesizing high-resolution images ( $1024 \times 1204$ pixels) using the disentangled representation learned by VAEs.


#### Abstract

Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance (i.e. improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAEbased models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (1024x1024 pixels) for the first time using the disentangled representations.


## 1. Introduction

Learning a compact and interpretable representation of data without supervision is important to improve our understanding of data and machine learning systems. Recently, it is suggested that a disentangled representation, which represents data using independent factors of variations in data can improve the interpretability and transferability of the representation $[5,1,50]$. Among various use-cases of disentangled representation, we are particularly interested in its application to generative models, since it allows users to specify the desired properties in the output by controlling the generative factors encoded in each latent dimension. There are increasing demands on such generative models in various domains, such as image manipulation [20, 30, 27], drug discovery [15], ML fairness [11, 35], etc.

Most prior works on unsupervised disentangled representation learning formulate the problem as constrained generative modeling task. Based on well-established frameworks, such as the Variational Autoencoder (VAE) or the Generative Adversarial Network (GAN), they introduce additional regularization to encourage the axes of the latent manifold to align with independent generative factors in the data. Approaches based on VAE [17, 9, 25, 7] augment its objective function to favor a factorized latent representation by adding implicit [17, 7] or explicit penalties [25, 9]. On the other hand, approaches based on GAN [10] propose to regularize the generator such that it increases the mutual information between the input latent code and its output.

One major challenge in the existing approaches is the trade-off between learning disentangled representations and generating realistic data. VAE-based approaches are effective in learning useful disentangled representations in various tasks, but their generation quality is generally worse than the state-of-the-arts, which limits its applicability to the task of realistic synthesis. On the other hand, GANbased approaches can achieve the high-quality synthesis with a more expressive decoder and without explicit likelihood estimation [10]. However, they tend to learn comparably more entangled representations than the VAE counterparts [17, 25, 9, 7] and are notoriously difficult to train, even with recent techniques to stabilize the training $[25,53]$.

To circumvent this trade-off, we propose a simple and generic framework to combine the benefits of disentangled representation learning and high-fidelity synthesis. Unlike the previous approaches that address both problems jointly by a single objective, we formulate two separate, but successive problems; we first learn a disentangled representation using VAE, and distill the learned representation to GAN for high-fidelity synthesis. The distillation is performed from VAE to GAN by transferring the inference model, which provides a meaningful latent distribution, rather than a simple Gaussian prior and ensures that both models are aligned to render the same generative factors. Such decomposition also naturally allows a layered approach to learn latent representation by first learning major disentangled factors by VAE, then learning missing (entangled) nuisance factors by GAN. We refer the proposed method as the Information Distillation Generative Adversarial Network (ID-GAN).

Despite the simplicity, the proposed ID-GAN is extremely effective in addressing the previous challenges, achieving high-fidelity synthesis using the learned disentangled representation (e.g. $1024 \times 1024$ image). We also show that such decomposition leads to a practically efficient model design, allowing the models to learn the disentangled representation from low-resolution images and transfer it to synthesize high-resolution images.

The contributions of this paper are as follows:

- We propose ID-GAN, a simple yet effective framework that combines the benefits of disentangled representation learning and high-fidelity synthesis.
- The decomposition of the two objectives enables plugand-play-style adoption of state-of-the-art models for both tasks, and efficient training by learning models for disentanglement and synthesis using low- and highresolution images, respectively.
- Extensive experimental results show that the proposed method achieves state-of-the-art results in both disentangled representation learning and synthesis over a wide range of tasks from synthetic to complex datasets.


## 2. Related Work

Disentanglement learning. Unsupervised disentangled representation learning aims to discover a set of generative factors, whose element encodes unique and independent factors of variation in data. To this end, most prior works based on VAE [17, 25, 9] and GAN [10, 21, 33, 32] focused on designing the loss function to encourage the factorization of the latent code. Despite some encouraging results, however, these approaches have been mostly evaluated on simple and low-resolution images [40, 36]. We believe that improving the generation quality of disentanglement learning is important, since it not only increases the practical impact in real-world applications, but also helps us to better assess the disentanglement quality on complex and natural images where the quantitative evaluation is difficult. Although there are increasing recent efforts to improve the generation quality with disentanglement learning [21, 44, 33, 32], they often come with the degraded disentanglement performance [10], rely on a specific inductive bias (e.g. 3D transformation [44]), or are limited to low-resolution images $[33,32,21]$. On the contrary, our work aims to investigate a general framework to improve the generation quality without representation learning trade-off, while being general enough to incorporate various methods and inductive biases. We emphasize that this contribution is complementary to the recent efforts for designing better inductive bias or supervision for disentanglement learning $[52,43,47,37,8]$. In fact, our framework is applicable to a wide variety of disentanglement learning methods and can incorporate them in a plug-and-play style as long as they have an inference model (e.g. nonlinear ICA [24]).

Combined VAE/GAN models. There have been extensive attempts in literature toward building hybrid models of VAE and GAN [28, 4, 19, 54, 6]. These approaches typically learn to represent and synthesize data by combining VAE and GAN objectives and optimizing them jointly in an end-to-end manner. Our method is an instantiation of this model family, but is differentiated from the prior work in that (1) the training of VAE and GAN is decomposed into two separate tasks and (2) the VAE is used to learn a specific conditioning variable (i.e. disentangled representation) to the generator while the previous methods assume the availability of an additional conditioning variable [4] or use VAE to learn the entire (entangled) latent distribution $[28,19,54,6]$. In addition, extending the previous VAE-GAN methods to incorporate disentanglement constraints is not straightforward, as the VAE and GAN objectives are tightly entangled in them. In the experiment, we demonstrate that applying existing hybrid models on our task typically suffers from the suboptimal trade-off between the generation quality and the disentanglement performance, and they perform much worse than our method.

## 3. Background: Disentanglement Learning

The objective of unsupervised disentanglement learning is to describe each data $x$ using a set of statistically independent generative factors $z$. In this section, we briefly review prior works and discuss their advantages and limitations.

The state-of-the-art approaches in unsupervised disentanglement learning are largely based on the Variational Autoencoder (VAE). They rewrite their original objective and derive regularizations that encourage the disentanglement of the latent variables. For instance, $\beta$-VAE [17] proposes to optimize the following modified Evidence LowerBound (ELBO) of the marginal log-likelihood:

$$
\begin{align*}
& \mathbb{E}_{x \sim p(x)}[\log p(x)] \geq \mathbb{E}_{x \sim p(x)} {\left[\mathbb{E}_{z \sim q_{\phi}(z \mid x)}\left[\log p_{\theta}(x \mid z)\right]\right.} \\
&\left.-\beta \mathrm{D}_{\mathrm{KL}}\left(q_{\phi}(z \mid x) \| p(z)\right)\right] \tag{1}
\end{align*}
$$

where setting $\beta=1$ reduces to the original VAE. By forcing the variational posterior to be closer to the factorized prior $(\beta>1)$, the model learns a more disentangled representation, but with a sacrifice of generation quality, since it also decreases the mutual information between $z$ and $x[9,25]$. To address such trade-off and improve the generation quality, recent approaches propose to gradually anneal the penalty on the KL-divergence [7], or decompose it to isolate the penalty for total correlation [51] that encourages the statistical independence of latent variables [1, 9, 25].

Approaches based on VAE have shown to be effective in learning disentangled representations over a range of tasks from synthetic [40] to complex datasets [34, 3]. However, their generation performance is generally insufficient to achieve high-fidelity synthesis, even with recent techniques isolating the factorization of the latent variable [25, 9]. We argue that this problem is fundamentally attributed to two reasons: First, most VAE-based approaches assume the fully-independent generative factors [17, 25, 9, 36, 50, 39]. This strict assumption oversimplifies the latent manifold and may cause the loss of useful information (e.g. correlated factors) for generating realistic data. Second, they typically utilize a simple generator, such as the factorized Gaussian decoder, and learn a uni-modal mapping from the latent to input space. Although this might be useful to learn meaningful representations [7] (e.g. capturing a structure in local modes), such decoder makes it difficult to render complex patterns in outputs (e.g. textures).

## 4. High-Fidelity Synthesis via Distillation

Our objective is to build a generative model $G_{\omega}: \mathcal{Z} \rightarrow$ $\mathcal{X}$ that produces high-fidelity output $x \in \mathcal{X}$ with an interpretable latent code $z \in \mathcal{Z}$ (i.e. disentangled representation). To achieve this goal, we build our framework upon VAE-based models due to their effectiveness in learning disentangled representations. However, discussions in the previous section suggest that disentanglement learning in

![](https://cdn.mathpix.com/cropped/2024_06_04_b525d079938b7558d02fg-03.jpg?height=415&width=761&top_left_y=237&top_left_x=1105)

Figure 2. Overall framework of the proposed method (ID-GAN).

VAE leads to the sacrifice of generation quality due to the strict constraints on fully-factorized latent variables and the utilization of simple decoders. We aim to improve the VAEbased models by enhancing generation quality while maintaining its disentanglement learning performance.

Our main idea is to decompose the objectives of learning disentangled representation and generating realistic outputs into separate but successive learning problems. Given a disentangled representation learned by VAEs, we train another network with a much higher modeling capacity (e.g. GAN generator) to decode the learned representation to a realistic sample in the observation space.

Figure 2 describes the overall framework of the proposed algorithm. Formally, let $z=(s, c)$ denote the latent variable composed of the disentangled variable $c$ and the nuisance variable $s$ capturing independent and correlated factors of variation, respectively. In the proposed framework, we first train VAE (e.g. Eq. (1)) to learn disentangled latent representations of data, where each observation $x$ can be projected to $c$ by the learned encoder $q_{\phi}(c \mid x)$ after the training. Then in the second stage, we fix the encoder $q_{\phi}$ and train a generator $G_{\omega}(z)=G_{\omega}(s, c)$ for high-fidelity synthesis while distilling the learned disentanglement by optimizing the following objective:

$$
\begin{align*}
\min _{G} \max _{D} & \mathcal{L}_{\mathrm{GAN}}(D, G)-\lambda \mathcal{R}_{\mathrm{ID}}(G)  \tag{2}\\
\mathcal{L}_{\mathrm{GAN}}(D, G)= & \mathbb{E}_{x \sim p(x)}[\log D(x)]+  \tag{3}\\
& \mathbb{E}_{s \sim p(s), c \sim q_{\phi}(c)}[\log (1-D(G(s, c)))] \\
\mathcal{R}_{\mathrm{ID}}(G)= & \mathbb{E}_{c \sim q_{\phi}(c), x \sim G(s, c)}\left[\log q_{\phi}(c \mid x)\right]+H_{q_{\phi}}(c) \tag{4}
\end{align*}
$$

where $q_{\phi}(c)=\frac{1}{N} \sum_{i} q_{\phi}\left(c \mid x_{i}\right)$ is the aggregated posterior [38, 18, 49] of the encoder network ${ }^{1}$. Similar to [10], Eq. (4) corresponds to the variational lower-bound of mutual information between the latent code and the generator output $I(c ; G(s, c))$, but differs in that (1) $c$ is sampled from the aggregated posterior $q_{\phi}(c)$ instead of the prior $p(c)$ and (2) it is optimized with respect to the generator only. Note that we treat $H_{q_{\phi}}(c)$ as a constant since $q_{\phi}$ is fixed in Eq.(4). We refer the proposed model as the Information Distillation Generative Adversarial Network (ID-GAN).[^0]

### 4.1. Analysis

In this section, we provide in-depth analysis of the proposed method and its connections to prior works.

Comparisons to $\beta$-VAEs [17,9, 25]. Despite the simplicity, the proposed ID-GAN effectively addresses the problems in $\beta$-VAEs with generating high-fidelity outputs; it augments the latent representation by introducing a nuisance variable $s$, which complements the disentangled variable $c$ by modeling richer generative factors. For instance, the VAE objective tends to favor representational factors that characterize as much data as possible [7] (e.g. azimuth, scale, lighting, etc.), which are beneficial in representation learning, but incomprehensive to model the complexity of observations. Given the disentangled factors discovered by VAEs, the ID-GAN learns to encode the remaining generative factors (such as high-frequency textures, face identity, etc.) into nuisance variable $s$. (Figure 8). This process shares a similar motivation with a progressive augmentation of latent factors [31], but is used for modeling disentangled and nuisance generative factors. In addition, ID-GAN employs a much more expressive generator than a simple factorized Gaussian decoder in VAE, which is trained with adversarial loss to render realistic and convincing outputs. Combining both, our method allows the generator to synthesize various data in a local neighborhood defined by $c$, where the specific characteristics of each example are fully characterized by the additional nuisance variable $s$.

Comparisons to InfoGAN [10]. The proposed method is closely related to InfoGAN, which optimizes the variational lower-bound of mutual information $I(c ; G(s, c))$ for disentanglement learning. To clarify the difference between the proposed method and InfoGAN, we rewrite the regularization for both methods using the $\mathrm{KL}$ divergence as follows:

$$
\begin{align*}
\mathcal{R}_{\text {Info }}(G, q) & =\mathbb{E}_{s \sim p(s)}\left[D_{\mathrm{KL}}\left(p(c) \| q_{\phi}(c \mid G(s, c))\right)\right],  \tag{5}\\
\mathcal{R}_{\text {ourrs }}(G, q) & =\beta \mathcal{R}_{\mathrm{VAE}}(q)+\lambda \mathcal{R}_{\mathrm{ID}}(G), \text { where } \\
\mathcal{R}_{\mathrm{VAE}}(q) & =\mathbb{E}_{x \sim p(x)}\left[D_{\mathrm{KL}}\left(q_{\phi}(c \mid x) \| p(c)\right)\right],  \tag{6}\\
\mathcal{R}_{\mathrm{ID}}(G) & =\mathbb{E}_{s \sim p(s)}\left[D_{\mathrm{KL}}\left(q_{\phi}(c) \| q_{\phi}(c \mid G(s, c))\right)\right], \tag{7}
\end{align*}
$$

where $\mathcal{R}_{\text {ours }}$ summarizes all regularization terms in our method ${ }^{2}$. See the Appendix A. 1 for detailed derivations.

Eq. (5) shows that InfoGAN optimizes the forward KL divergence between the prior $p(c)$ and the approximated posterior $q_{\phi}(c \mid G(s, c))$. Due to the zero-avoiding characteristics of forward KL [42], it forces all latent code $c$ with non-zero prior to be covered by the posterior $q_{\phi}$. Intuitively, it implies that InfoGAN tries to exploit every dimensions in $c$ to encode each (unique) factor of variations. It becomes problematic when there is a mismatch between the[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_b525d079938b7558d02fg-04.jpg?height=458&width=702&top_left_y=253&top_left_x=1123)

Figure 3. Comparison of disentanglement vs. generation performance on dSprites dataset.

number of true generative factors and the size of latent variable $c$, which is common in unsupervised disentanglement learning. On the contrary, VAE optimizes the reverse KL divergence (Eq. (6)), which can effectively avoid the problem by encoding only meaningful factors of variation into certain dimensions in $c$ while collapsing the remainings to the prior. Since the encoder training in our method is only affected by Eq. (6), it allows us to discover the ambient dimension of latent generative factors robust to the choice of latent dimension $|c|$.

In addition, Eq. (5) shows that InfoGAN optimizes the encoder using the generated distributions, which can be problematic when there exists a sufficient discrepancy between the true and generated distributions (e.g. modecollapse may cause learning partial generative factors.). On the other hand, the encoder training in our method is guided by the true data (Eq. (6)) together with maximum likelihood objective, while the mutual information (Eq. (7)) is enforced only to the generator. This helps our model to discover comprehensive generative factors from data while guiding the generator to align its outputs to the learned representation.

Practical benefits. The objective decomposition in the proposed method also offers a number of practical advantages. First, it enables plug-and-play-style adoption of the state-of-the-art models for disentangled representation learning and high-quality generation. As shown in Figure 3, it allows our model to achieve state-of-the-art performance on both tasks. Second, such decomposition also leads to an efficient model design, where we learn disentanglement from low-resolution images and distill the learned representation to the task of high-resolution synthesis with a much higher-capacity generator. We argue that it is practically reasonable in many cases since VAEs tend to learn global structures in disentangled representation, which can be captured from low-resolution images. We demonstrate this in the high-resolution image synthesis task, where we use the disentangled representation learned with $64 \times 64$ images for the synthesis of $256 \times 256$ or $1024 \times 1024$ images.

Table 1. Quantitative comparison results on synthetic datasets.

|  | Color-dSprites |  |  | Scream-dSprites |  |  | Noisy-dSprites |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | FVM $(\uparrow)$ | $\operatorname{MIG}(\uparrow)$ | $\operatorname{FID}(\downarrow)$ | $\overline{F V M(\uparrow)}$ | $\overline{\operatorname{MIG}(\uparrow)}$ | FID $(\downarrow)$ | FVM $(\uparrow)$ | $\overline{\operatorname{MIG}(\uparrow)}$ | $\operatorname{FID}(\downarrow)$ |
| VAE | $\mid .67 \pm .12$ | $.16 \pm .08$ | $21.63 \pm 4.97$ | $.44 \pm .03$ | $.08 \pm .04$ | $7.79 \pm 2.51$ | $.42 \pm .09$ | $.05 \pm .04$ | $3.27 \pm 1.94$ |
| $\beta$-VAE | $.67 \pm .07$ | $.32 \pm .04$ | $15.13 \pm 4.25$ | $.57 \pm .01$ | $.29 \pm .00$ | $7.33 \pm 2.87$ | $.32 \pm .05$ | $.05 \pm .03$ | $3.46 \pm 0.38$ |
| FactorVAE | $.69 \pm .05$ | $.37 \pm .02$ | $10.71 \pm 5.73$ | $.57 \pm .01$ | $.22 \pm .06$ | $6.35 \pm 3.27$ | $.40 \pm .09$ | $.08 \pm .04$ | $2.48 \pm 0.44$ |
| GAN | N/A | N/A | $0.30 \pm 0.07$ | N/A | N/A | $\mathbf{0 . 1 1} \pm \mathbf{0 . 0 3}$ | N/A | N/A | $9.74 \pm 2.18$ |
| InfoGAN | $.34 \pm 00$ | $.01 \pm .01$ | $30.55 \pm 21.17$ | $.29 \pm 00$ | $.00 \pm .00$ | $5.77 \pm 3.93$ | $.22 \pm .02$ | $.01 \pm .01$ | $5.51 \pm 4.22$ |
| ID-GAN+ | $67 \pm .12$ | $16 \pm .08$ | $0.32 \pm$ | $44 \pm .03$ | $.08 \pm .04$ | $0.26 \pm 0.03$ | $.42 \pm .09$ | $.05 \pm .04$ | $1.58 \pm 0.62$ |
| ID-GA | $.67 \pm .07$ | $.32 \pm .04$ | $0.25 \pm 0.23$ | $.57 \pm .01$ | $.29 \pm .00$ | $0.18 \pm 0.02$ | $.32 \pm .05$ | $.05 \pm .03$ | $12.42 \pm 1.13$ |
| ID-GAN+FactorVAE | $.69 \pm .05$ | $.37 \pm .02$ | $0.75 \pm 0.54$ | $.57 \pm .01$ | $.22 \pm .06$ | $0.65 \pm 0.33$ | $.40 \pm .09$ | $.08 \pm .04$ | $2.07 \pm 0.87$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_b525d079938b7558d02fg-05.jpg?height=675&width=1315&top_left_y=644&top_left_x=378)

Figure 4. Qualitative results on synthetic datasets. Both $\beta$-VAE and ID-GAN share the same latent code, but ID-GAN exhibits substantailly higher generation quality.

## 5. Experiments

In this section, we present various results to show the effectiveness of ID-GAN. Refer to the Appendix for more comprehensive results and figures.

### 5.1. Implementation Details

Compared methods. We compare our method with stateof-the-art methods in disentanglement learning and generation. We choose $\beta$-VAE [17], FactorVAE [25], and InfoGAN [10] as baselines for disentanglement learning. For fair comparison, we choose the best hyperparameter for each model via extensive hyper-parameter search. We also report the performance by training each method over five different random seeds and averaging the results.

Network architecture. For experiments on synthetic datasets, we adopt the architecture from [36] for all VAEbased methods (VAE, $\beta$-VAE, and FactorVAE). For GANbased methods (GAN, InfoGAN, and ID-GAN), we employ the same decoder and encoder architectures in VAE as the generator and discriminator, respectively. We set the size of disentangled latent variable to 10 for all methods, and exclude the nuisance variable in GAN-based methods for a fair comparison with VAE-based methods. For experiments on complex datasets, we employ the generator and discriminator in the state-of-the-art GAN [41, 46]. For VAE ar- chitectures, we utilize the same VAE architecture as in the synthetic datasets. We set the size of disentangled and nuisance variables to 20 and 256 , respectively.

Evaluation metrics We employ three popular evaluation metrics in the literature: Factor-VAE Metric (FVM) [25], Mutual Information Gap (MIG) [9], and Fréchet Inception Distance (FID) [16]. FVM and MIG evaluate the disentanglement performance by measuring the degree of axisalignment between each dimension of learned representations and ground-truth factors. FID evaluates the generation quality by measuring the distance between the true and the generated distributions.

### 5.2. Results on Synthetic Dataset.

For quantitative evaluation of disentanglement, we employ the dSprites dataset [40], which contains synthetic images generated by randomly sampling known generative factors, such as shape, orientation, size, and x-y position. Since the complexity of dSprites is limited to analyze the disentanglement and generation performance, we adopt three variants of dSprites, which are generated by adding color [25] (Color-dSprites) or background noise [36] (Noisy- and Scream-dSprites).

Table 1 and Figure 4 summarize the quantitative and qualitative comparison results with existing disentangle-

Table 2. Comparison of approaches using a joint and decomposed objective for disentanglement learning and synthesis.

|  | dSprites |  |  |
| :--- | :---: | :---: | :---: |
|  | FVM $(\uparrow)$ | MIG $(\uparrow)$ | FID $(\downarrow)$ |
| $\beta$-VAE (reference) | $\mathbf{0 . 6 5} \pm \mathbf{0 . 0 8}$ | $\mathbf{0 . 2 8} \pm \mathbf{0 . 0 9}$ | $37.75 \pm 24.58$ |
| VAE-GAN | $0.46 \pm 0.18$ | $0.13 \pm 0.11$ | $33.54 \pm 24.93$ |
| ID-GAN (end-to-end) | $0.50 \pm 0.14$ | $0.13 \pm 0.09$ | $3.18 \pm 2.38$ |
| ID-GAN (two-step) | $\mathbf{0 . 6 5} \pm \mathbf{0 . 0 8}$ | $\mathbf{0 . 2 8} \pm \mathbf{0 . 0 9}$ | $\mathbf{2 . 0 0} \pm \mathbf{1 . 7 4}$ |

ment learning approaches, respectively. First, we observe that VAE-based approaches (i.e. $\beta$-VAE and FactorVAE) achieve the state-of-the-art disentanglement performance across all datasets, outperforming the VAE baseline and InfoGAN with a non-trivial margin. The qualitative results in Figure 4 show that the learned generative factors are wellcorrelated with meaningful disentanglement in the observation space. On the other hand, InfoGAN fails to discover meaningful disentanglement in most datasets. We observe that information maximization in InfoGAN often leads to undesirable factorization of generative factors, such as encoding both shape and position into one latent code, but factorizing latent dimensions by different combinations of them (e.g. Color-dSprites in Figure 4). ID-GAN achieves state-of-the-art disentanglement through the distillation of the learned latent code from the VAE-based models. Appendix B. 3 also shows that ID-GAN is much more stable to train and insensitive to hyper-parameters than InfoGAN.

In terms of generation quality, VAE-based approaches generally perform much worse than GAN baseline. This performance gap is attributed to the strong constraints on the factorized latent variable and weak decoder in VAE, which limits the generation capacity. This is clearly observed in the results on the Noisy-dSprites dataset (Figure 4 ), where the outputs from $\beta$-VAE fail to render the high-dimensional patterns in the data (i.e. uniform noise). On the other hand, our method achieves competitive generation performance to the state-of-the-art GAN using a much more flexible generator for synthesis, which enables the modeling of complex patterns in data. As observed in Figure 4, ID-GAN performs generation using the same latent code with $\beta$-VAE, but produces much more realistic outputs by capturing accurate object shapes (in Color-dSprites) and background patterns (in Scream-dSprites and NoisydSprites) missed by the VAE decoder. These results suggest that our method can achieve the best trade-off between disentanglement learning and high-fidelity synthesis.

### 5.3. Ablation Study

This section provides an in-depth analysis of our method.

Is two-step training necessary? First, we study the impact of two-stage training for representation learning and synthesis. We consider two baselines: (1) VAE-GAN [28] as an extension of $\beta$-VAE with adversarial loss, and (2) endto-end training of ID-GAN. Contrary to ID-GAN that learns
Table 3. Comparison of two-step approaches for generation (FID) and alignment ( $\mathcal{R}_{\mathrm{ID}}$ and GILBO) ${ }^{3}$ performance.

![](https://cdn.mathpix.com/cropped/2024_06_04_b525d079938b7558d02fg-06.jpg?height=688&width=813&top_left_y=320&top_left_x=1076)

Figure 5. Qualitative comparisons of various two-step approaches. All samples share the same disentangled code $c$, but different nuisance variable $s$. (1) First column: output of $\beta$-VAE decoder. (2) Second to fourth columns: images generated by different nuisance variables $s$ using various methods (rows).

to represent $\left(q_{\phi}\right)$ and synthesize $(G)$ data via separate objectives, these baselines learn a single, entangled objective for both tasks. Table 2 summarizes the results in the dSprites dataset.

The results show that VAE-GAN improves the generation quality of $\beta$-VAE with adversarial learning. The generation quality is further improved in the end-to-end version of ID-GAN by employing a separate generator for synthesis. However, the improved generation quality in both baselines comes with the cost of degraded disentanglement performance. We observe that updating the encoder using adversarial loss hinders the discovery of disentangled factors, as the discriminator tends to exploit high-frequency details to distinguish the real images from the fake images, which motivates the encoder to learn nuisance factors. This suggests that decomposing the representation learning and generation objective is important in the proposed framework (ID-GAN two-step), which achieves the best performance in both tasks.

Is distillation necessary? The above ablation study justifies the importance of two-step training. Next, we compare different approaches for two-step training that perform conditional generation using the representation learned by $\beta$-VAE. Specifically, we consider two baselines: (1) cGAN and (2) ID-GAN trained without distillation (ID-GAN w/o distill). We opt to consider cGAN as the baseline since we[^2]find that it implicitly optimizes $\mathcal{R}_{\text {ID }}$ (see Appendix A. 2 for the proof). In the experiments, we train all models in the CelebA 128x128 dataset using the same $\beta$-VAE trained on the $64 \times 64$ resolution, and compare the generation quality (FID) and a degree of alignment between the disentangled code $c$ and generator output $G(s, c)$. For comparison of the alignment, we measure $\mathcal{R}_{\text {ID }}$ (Eq. (7)) and GILBO ${ }^{4}$ [2], both of which are valid lower-bounds of mutual information $I(c ; G(s, c))$. Note that the comparison based on the lower-bound is still valid as its relative order has shown to be insensitive to the tightness of the bound [2]. Table 3 and Figure 5 summarize the quantitative and qualitative results, respectively.

As shown in the table, all three models achieve comparable generation performances in terms of FID. However, we observe that their alignments to the input latent code vary across the methods. For instance, ID-GAN (w/o distill) achieves very low $\mathcal{R}_{\mathrm{ID}}$ and GILBO, indicating that the generator output is not accurately reflecting the generative factors in $c$. The qualitative results (Figure 5) also show considerable mismatch between the $c$ and the generated images. Compared to this, cGAN achieves much higher degree of alignment due to the implicit optimization of $\mathcal{R}_{\mathrm{ID}}$, but its association is much loose than our method (e.g. changes in gender and hairstyle). By explicitly constraining the generator to optimize $\mathcal{R}_{\mathrm{ID}}$, ID-GAN achieves the best alignment.

### 5.4. Results on Complex Dataset

To evaluate our method with more diverse and complex factors of variation, we conduct experiments on natural image datasets, such as CelebA [34], 3D Chairs [3], and Cars [26]. We first evaluate our method on $64 \times 64$ images, and extend it to higher resolution images using the CelebA $(256 \times 256)$ and CelebA-HQ [23] $(1024 \times 1024)$ datasets.

Comparisons to other methods. Table 4 summarizes quantitative comparison results (see Appendix A. 4 for qualitative comparisons). Since there are no ground-truth factors available in these datasets, we report the performance based on generation quality (FID). As expected, the generation quality of VAE-based methods is much worse in natural images. GAN-based methods, on the contrary, can generate more convincing samples exploiting the expressive generator. However, we observe that the baseline GAN taking only nuisance variables ends up learning highly-entangled generative factors. ID-GAN achieves disentanglement via disentangled factors learned by VAE, and generation performance on par with the GAN baseline.

To better understand the disentanglement learned by GAN-based methods, we present latent traversal results in Figure 6. We generate samples by modifying values of each dimension in the disentangled latent code $c$ while fixing the[^3]

Table 4. Quantitative results based on FID ( $\downarrow$ ).

![](https://cdn.mathpix.com/cropped/2024_06_04_b525d079938b7558d02fg-07.jpg?height=770&width=857&top_left_y=276&top_left_x=1054)

Figure 6. Comparisons of latent traversal between GAN-based approaches. Although both methods achieve comparable generation quality, ID-GAN learns much more meaningful disentanglement.

![](https://cdn.mathpix.com/cropped/2024_06_04_b525d079938b7558d02fg-07.jpg?height=650&width=838&top_left_y=1147&top_left_x=1061)

Figure 7. Comparisons of VAE and ID-GAN outputs (top-rows: VAE, bottom-rows: ID-GAN). Note that both outputs are generated from the same latent code, but using different decoders. Both decoders are aligned well to render the same generative factors, but ID-GAN produces much more realistic outputs.

rest. We observe that the InfoGAN fails to encode meaningful factors into $c$, and nuisance variable $z$ dominates the generation process, making all generated images almost identical. On the other hand, ID-GAN learns meaningful disentanglement with $c$ and generates reasonable variations.

Extension to high-resolution synthesis. One practical benefit of the proposed two-step approach is that we can incorporate any VAE and GAN into our framework. To demonstrate this, we train ID-GAN for high-resolution images (e.g. $256 \times 256$ and $1024 \times 1024$ ) while distilling the

![](https://cdn.mathpix.com/cropped/2024_06_04_b525d079938b7558d02fg-08.jpg?height=672&width=829&top_left_y=239&top_left_x=168)

Figure 8. Analysis on the learned disentangled variables $c^{(m)} \in$ $\mathbb{R}^{20}$ and nuisance variables $s^{(n)} \in \mathbb{R}^{256}$ of ID-GAN on CelebA $(256 \times 256)$. The samples in the first row are generated by the $\beta$ VAE decoder and the rest are generated by ID-GAN. Each $c^{(m)}$ captures the most salient factors of variation (e.g., azimuth, hairstructure, etc.) while $s^{(n)}$ contributes to the local details (e.g., $s^{(2)}$ and $s^{(3)}$ for curvy and straight hair, respectively).

$\beta$-VAE encoder learned with much smaller $64 \times 64$ images $^{5}$. This allows us to easily scale up the resolution of synthesis and helps us to better assess the disentangled factors.

We first adapt ID-GAN to the $256 \times 256$ image synthesis task. To understand the impact of distillation, we visualize the outputs from the VAE decoder and the GAN generator using the same latent code as inputs. Figure 7 summarizes the results. We observe that the outputs from both networks are aligned well to render the same generative factors to similar outputs. Contrary to blurry and lowresolution $(64 \times 64)$ VAE outputs, however, ID-GAN produces much more realistic and convincing outputs by introducing a nuisance variable and employing more expressive decoder trained on higher-resolution $(256 \times 256)$. Interestingly, synthesized images by ID-GAN further clarify the disentangled factors learned by the VAE encoder. For instance, the first row in Figure 7 shows that the ambiguous disentangled factors from the VAE decoder output is clarified by ID-GAN, which is turned out to capture the style of a cap. This suggests that ID-GAN can be useful in assessing the quality of the learned representation, which will broadly benefit future studies.

To gain further insights on the learned generative factors by our method, we conduct qualitative analysis on the latent variables ( $c$ and $s$ ) by generating samples by fixing one variable while varying another (Figure 8). We observe that varying the disentangled variable $c$ leads to variations in the holistic structures in the outputs, such as azimuth, skin[^4]

![](https://cdn.mathpix.com/cropped/2024_06_04_b525d079938b7558d02fg-08.jpg?height=1049&width=835&top_left_y=234&top_left_x=1060)

Figure 9. Results on the CelebA-HQ dataset $(1024 \times 1024 \mathrm{im}-$ ages).

color, hair style, etc, while varying the nuisance variable $s$ leads to changes in more fine-grained facial attributes, such as expression, skin texture, identity, etc. It shows that ID-GAN successfully distills meaningful and representative disentangled generative factors learned by the inference network in VAE, while producing diverse and high-fidelity outputs using generative factors encoded in the nuisance variable.

Finally, we further conduct experiments on the more challenging task of mega-pixel image synthesis. In the experiments, we base our ID-GAN on the VGAN architecture [46] and adapt it to synthesize CelebA-HQ $1024 \times 1024$ images given factors learned by $\beta$-VAE. Figure 9 presents the results, where we generate images by changing one values in one latent dimension in $c$. We observe that IDGAN produces high-quality images with nice disentanglement property, where it changes one factor of variation in the data (e.g. azimuth and hair-style) while preserving the others (e.g. identity).

## 6. Conclusion

We propose Information Distillation Generative Adversarial Network (ID-GAN), a simple framework that combines the benefits of the disentanglement representation learning and high-fidelity synthesis. We show that we can
incorporate the state-of-the-art for both tasks by decomposing their objectives while constraining the generator by distilling the encoder. Extensive experiments on synthetic and complex datasets validate that the proposed method can achieve the best trade-off between realism and disentanglement, outperforming the existing approaches with substantial margin. We also show that such decomposition leads to efficient and effective model design, allowing high-fidelity synthesis with disentanglement on high-resolution images.

## References

[1] A. Achille and S. Soatto. Information dropout: Learning optimal representations through noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018. 1,3

[2] Alexander A Alemi and Ian Fischer. Gilbo: One metric to measure them all. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, NeurIPS, pages 7037-7046. Curran Associates, Inc., 2018. 6, 7

[3] Mathieu Aubry, Daniel Maturana, Alexei Efros, Bryan Russell, and Josef Sivic. Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models. In CVPR, 2014. 3, 7, 23

[4] J. Bao, D. Chen, F. Wen, H. Li, and G. Hua. Cvae-gan: Finegrained image generation through asymmetric training. In ICCV. 2

[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation Learning: A review and new perspectives. PAMI, 2013. 1

[6] Andrew Brock, Theodore Lim, James M. Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. In ICLR, 2017. 2

[7] Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in $\beta$-vae. arXiv preprint arXiv:1804.03599, 2018. 1, 2, 3, 4, 23

[8] Junxiang Chen and Kayhan Batmanghelich. Weakly supervised disentanglement by pairwise similarities. arXiv preprint arXiv:1906.01044, 2019.2

[9] Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement in variational autoencoders. In NeurIPS, 2018. 1, 2, 3, 4, 5, 23

[10] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets. In NeurIPS, 2016. 1, 2, 3, 4, 5, 11, 16

[11] Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard Zemel. Flexibly fair representation learning by disentanglement. In ICML, pages 1436-1445, 2019. 1

[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009. 23

[13] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. In NeurIPS. 2016. 22
[14] Maurice Fréchet. Sur la distance de deux lois de probabilité. COMPTES RENDUS HEBDOMADAIRES DES SEANCES DE L ACADEMIE DES SCIENCES, 1957. 23

[15] Rafael Gmez-Bombarelli, Jennifer N. Wei, David Duvenaud, Jos Miguel Hernndez-Lobato, Benjamn Snchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Aln Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 2018. 1

[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gnter, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a nash equilibrium. In NeurIPS, 2017. 5, 23

[17] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. $\beta$-VAE: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017. 1, $2,3,4,5,16$

[18] Matthew D. Hoffman and Matthew J. Johnson. ELBO surgery: yet another way to carve up the variational evidence lower bound. In NeurIPS, 2016. 3

[19] Huaibo Huang, zhihang li, Ran He, Zhenan Sun, and Tieniu Tan. Introvae: Introspective variational autoencoders for photographic image synthesis. In NIPS. 2018. 2

[20] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018. 1

[21] Insu Jeon, Wonkwang Lee, and Gunhee Kim. IB-GAN: Disentangled representation learning with information bottleneck GAN, 2019. 2

[22] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 22

[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018. 7, 23

[24] Ilyes Khemakhem, Diederik Kingma, and Aapo Hyvrinen. Variational autoencoders and nonlinear ica: A unifying framework. arXiv preprint arXiv:1907.04809, 2019. 2

[25] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In ICML, 2018. 1, 2, 3, 4, 5, 16, 23

[26] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), 2013. 7, 23

[27] Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic DENOYER, and Marc Aurelio Ranzato. Fader networks:manipulating images by sliding attributes. In NeurIPS. 2017. 1

[28] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. In ICML, 2016. 2, 6

[29] C. Ledig, L. Theis, F. Huszr, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017. 22

[30] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Kumar Singh, and Ming-Hsuan Yang. Diverse imageto-image translation via disentangled representations. In ECCV, 2018. 1

[31] Jos Lezama. Overcoming the disentanglement vs reconstruction trade-off via jacobian supervision. In ICLR, 2019. 4

[32] Zinan Lin, Kiran Koshy Thekumparampil, Giulia C. Fanti, and Sewoong Oh. Infogan-cr: Disentangling generative adversarial networks with contrastive regularizers. arXiv preprint arXiv:1906.06034, 2019.2

[33] Bingchen Liu, Yizhe Zhu, Zuohui Fu, Gerard de Melo, and Ahmed Elgammal. OOGAN: disentangling GAN with onehot sampling and orthogonal regularization. arXiv preprint arXiv:1905.10836, 2019. 2

[34] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015. $1,3,7,23$

[35] Francesco Locatello, Gabriele Abbati, Tom Rainforth, Stefan Bauer, Bernhard Schölkopf, and Olivier Bachem. On the fairness of disentangled representations. arXiv preprint arXiv:1905.13662, 2019. 1

[36] Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. arXiv preprint arXiv:1811.12359, 2018. 2, 3, 5, 23

[37] Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Rätsch, Bernhard Schölkopf, and Olivier Bachem. Disentangling factors of variation using few labels. arXiv preprint arXiv:1905.01258, 2019. 2

[38] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. In ICLR, 2016. 3

[39] Emile Mathieu, Tom Rainforth, N. Siddharth, and Yee Whye Teh. Disentangling disentanglement in variational autoencoders. In Bayesian Deep Learning Workshop, NeurIPS, 2018. 3

[40] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement testing Sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017. 2, 3, 5, 23

[41] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Which training methods for gans do actually converge? In ICML, 2018. 5, 16, 24, 25

[42] Tom Minka et al. Divergence measures and message passing. Technical report, Technical report, Microsoft Research, 2005. 4

[43] Siddharth Narayanaswamy, T. Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr. Learning disentangled representations with semi-supervised deep generative models. In NeurIPS. 2017. 2

[44] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Hologan: Unsupervised learning of $3 \mathrm{~d}$ representations from natural images. In ICCV, 2019. 2

[45] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 16
[46] Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow. In ICLR, 2019. 5, 8, 17, 18, 25

[47] Adria Ruiz, Oriol Martínez, Xavier Binefa, and Jakob Verbeek. Learning disentangled representations with reference-based variational autoencoders. arXiv preprint arXiv:1901.08534, 2019. 2

[48] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. 23

[49] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders. In ICLR, 2018. 3

[50] Michael Tschannen, Olivier Frederic Bachem, and Mario Lui. Recent advances in autoencoder-based representation learning. In Bayesian Deep Learning Workshop, NeurIPS, 2018. 1,3

[51] S. Watanabe. Information Theoretical Analysis of Multivariate Correlation. IBM Journal of Research and Development, 1960. 3

[52] Nicholas Watters, Loïc Matthey, Christopher P. Burgess, and Alexander Lerchner. Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes. arXiv preprint arXiv:1901.07017, 2019. 2

[53] Xiang Wei, Zixia Liu, Liqiang Wang, and Boqing Gong. Improving the improved training of wasserstein GANs. In ICLR, 2018. 2

[54] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. In NIPS, 2017. 2
