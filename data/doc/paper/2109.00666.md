# TabFairGAN: Fair Tabular Data Generation with Generative Adversarial Networks 

$1^{\text {st }}$ Amirarsalan Rajabi<br>Department of Computer Science<br>University of Central Florida<br>Orlando, FL, US<br>amirarsalan@ knights.ucf.edu

$2^{\text {nd }}$ Ozlem Ozmen Garibay<br>Department of Industrial Engineering and Management Systems<br>University of Central Florida<br>Orlando, FL, US<br>ozlem@ucf.edu


#### Abstract

With the increasing reliance on automated decision making, the issue of algorithmic fairness has gained increasing importance. In this paper, we propose a Generative Adversarial Network for tabular data generation. The model includes two phases of training. In the first phase, the model is trained to accurately generate synthetic data similar to the reference dataset. In the second phase we modify the value function to add fairness constraint, and continue training the network to generate data that is both accurate and fair. We test our results in both cases of unconstrained, and constrained fair data generation. In the unconstrained case, i.e. when the model is only trained in the first phase and is only meant to generate accurate data following the same joint probability distribution of the real data, the results show that the model beats state-of-the-art GANs proposed in the literature to produce synthetic tabular data. Also, in the constrained case in which the first phase of training is followed by the second phase, we train the network and test it on four datasets studied in the fairness literature and compare our results with another state-of-the-art pre-processing method, and present the promising results that it achieves. Comparing to other studies utilizing GANs for fair data generation, our model is comparably more stable by using only one critic, and also by avoiding major problems of original GAN model, such as mode-dropping and non-convergence, by implementing a Wasserstein GAN.


Index Terms-Fairness in Artificial Intelligence, Generative Adversarial Networks, WGAN

## I. INTRODUCTION

Artificial intelligence has gained paramount importance in the contemporary human life. With an ever-growing body of research and increasing processing capacity of computers, machine learning systems are being adopted by many firms and institutions for decision-making. Various industries such as insurance companies, financial institutions, and healthcare providers rely on automated decision making by machine learning models, making fairness-aware learning crucial since many of these automated decisions could have major impacts on the lives of individuals.

There are numerous evidence suggesting that bias exists in AI systems. One well known example is Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), which is a decision making system deployed by the US criminal justice system to assess the likelihood of a criminal defendant's recidivism (re-offending). It is shown that COMPAS is biased against African American defendants [1]. Another example is a Google's targeted advertising that was found to have shown the high paid jobs significantly more to males than females [2].

The existence of such bias and unfair classifications in AI systems has led the research community to pay attention to the problem of bias in AI. There are different approaches to improve fairness existing in the AI fairness literature. Let $D=\{X, S, Y\}$ be a labelled dataset, where $X \in \mathbb{R}^{n}$ are the unprotected attributes, $S$ is the protected attribute, and $Y$ is the decision. From a legal perspective, protected attribute is the attribute identified by law, based on which it is illegal to discriminate [3], e.g. gender or race. The proposed fairness enforcement methods in the literature could be categorised into three main classes of pre-process methods, in-process methods, and post-process methods.

Pre-process methods include modifying the training data before feeding the data into machine learning algorithm. For instance, in one study [4], four methods are presented to remove bias including suppression which is to remove attributes highly correlated with protected attributes $S$, massaging the dataset which is to change labels $(Y)$ of some objects in the dataset, and reweighing that involves assigning weights to different instances in the dataset. These are preliminary and simpler methods that results in more fair predictions, however entail higher fairness-utility cost. In other words fairness is achieved at the expense of accuracy. Another preprocessing method proposed in the literature is the work of Feldman et al. [5] in which a repairment mechanism is proposed to modify the unprotected attributes $(X)$ and achieve fairness with higher accuracy comparing to the aforementioned methods. This method will be discussed in more detail in Section V-B as the baseline method. In-process approaches involve modifying the learning algorithm to achieve fairness during training [3]. These methods mostly include modifying the objective functions or adding regularization terms to the cost function. For example, [6] proposes adding a regularization term to the objective function which penalize mutual information between the protected attributes and the classifier predictions. Finally, post-process mechanisms include modifying the final decisions of the classifiers. For instance, Hardt et al. [7] propose a method to modify the final classification scores in order to enhance equalized odds.

The emergence of unfairness in AI systems is mostly
attributed to: 1) direct bias existing in the historical datasets being used to train the algorithms, 2) bias caused by missing data, 3) bias caused by proxy attributes, where bias against the minority population is present in non-protected attributes, and 4) bias resulting from algorithm objective functions, where the aggregate accuracy of the whole population is sought and therefore the algorithm might disregard the minority group for the sake of majority [3]. Since historical datasets are a major source of discrimination in $\mathrm{AI}$, we focus on generating unbiased datasets to achieve fairness.

There is a rich and growing literature on generative models. The main idea behind a generative model is to capture the probabilistic distribution that could generate data similar to a reference dataset [8]. Broadly speaking, generative models could be divided into two main classes of models [8]: Energybased models such as Boltzmann Machines [9], and cost function-based models such as autoencoders and generative adversarial networks (GANs) [10]. GANs address some deficiencies in traditional generative models, and are shown to excel in various tasks comparing to other generative models such as in image generation [11] and video generation [12].

The original GAN consists of two networks, generator and discriminator [10]. The two networks play a minimax game. The generator takes a latent random variable $Z$ as input and generates a sample $G(Z)$, that is similar to the real data. The discriminator, on the other hand, is fed with both real and generated samples, and its task is to correctly classify the input sample as real or generated. Over time if the networks have enough capacity, they are trained together and ideally optimized to reach an equilibrium state in which the generator produces data from the exact targeted distribution and the discriminator gives the real and generated samples an equal probability of 0.5 . The work in [10] shows that training the discriminator to optimality is equal to minimizing JensenShannon divergence [13]. The work of Arjovsky et al. develops Wasserstein GANs, where a critic replaces the discriminator, and minimizing Earth-mover's distance is used instead of minimizing Jensen-Shannon divergence [14]. They show that WGAN could address some common training problems attributed to GANs, usch as requirement to maintain a careful balance during training as well as mode dropping [15].

In recent studies adversarial training has been used to remove discrimination. One such study, for example, by formulating the model as a minimax problem, proposes an adversarial learning framework that could learn representations of data that are discrimination-free and do not contain explicit information about the protected attribute [16]. Other adversarial objectives are proposed by the works of [17], [18] to achieve group fairness measures such as demographic parity and equality of odds. The application of generative adversarial networks for fairness in tabular datasets is not discussed enough in the literature, but has recently attracted attention of the research community. For instance, the work of Sattigeri et al. [19] proposes an approach to generate image datasets such that demographic fairness in the generated dataset is imposed. In their work $\mathrm{Xu}$ et al. [20] design a GAN that produces discrimination free tabular datasets. Their network includes one generator and two discriminators. The generator is adopted from [21] and produces fake pairs of data $(\hat{X}, \hat{Y})$ following the conditional distribution $P_{G}(X, Y \mid S)$. One discriminator's task is to ensure generator produces data with good accuracy, and the second discriminator ensures the generator produces fair data.

In this paper, we propose a Wasserstein GAN, TabFairGAN, that can produce high quality tabular data with the same joint distribution as the original tabular dataset. In Section II we discuss the fairness measure: demographic parity and discrimination score. In Section III, we introduce the model architecture, data transformation, value functions, and the training process of the model. In section IV, we compare the results of TabFairGAN with two other state-of-the-art GANs for tabular data generation, namely TGAN [22] and CTGAN [23]. In SectionV, we show how the model could be used for fair data generation and test the model on four real dataset. We compare the results of our model with the method developed by [5], which is another pre-process methods to enforce fairness. Finally in Section V-D, we explore the fairnessaccuracy trade-off. This work has two main contributions. We show that in the case of no constraints present (no fairness), the model is able to produce high quality synthetic data, competing with the state-of-the-art GANs designed for tabular data generation. Second contribution is producing high quality fair synthetic data, by adding a fairness constraint in the loss function of the generator. Comparing our model to previous application of GANs for fair tabular data generation, the model is more stable based on two merits: 1) the proposed model is a Wasserstein GAN which is shown to improve original GAN model in terms of some common GAN pitfalls, such as modedropping phenomena [15], and 2) the model only uses one critic instead of two [20] or three [24] discriminators.

## II. DISCRIMINATION SCORE

Among the most frequently practiced fairness metrics specified in legal notions and the literature is demographic parity or statistical parity/fairness. The goal of demographic fairness is to ensure that the overall proportion of members with respect to the protected group receiving a positive decision is identical. In a binary case, let $D=\{X, S, Y\}$ be a labelled dataset, where $X \in \mathbb{R}^{n}$ is the unprotected attributes, $S$ is the protected attribute, and $Y$ is the decision. In this paper, we consider the binary case, and for notational convenience we assume that the protected attribute $S$ takes two values, where $S=0$ represents the underprivileged minority class, and $S=1$ represents the privileged majority class. For instance, in a binary racial discrimination study the value 0 will be assigned to "AfricanAmerican", whereas 1 is assigned to "White". We also assign 1 to $Y$ for a successful decision (for instance an admission to a higher education institution), and assign 0 to $Y$ for an unsuccessful decision (rejection). Demographic fairness for the labeled dataset is defined as follows [7]:

$$
\begin{equation*}
P(y=1 \mid s=1)=P(y=1 \mid s=0) \tag{1}
\end{equation*}
$$

In this context, demographic parity is defined by the difference between the conditional probability and its marginal. We define the discrimination with respect to the protected attribute $S$ by discrimination score (DS) and calculate it by: $D S=P(y=1 \mid s=1)-P(y=1 \mid s=0)$. A similar measure could be obtained for a labeled dataset $D$ and a classifier $f:(X, S) \rightarrow Y$ where the discrimination score for the classifier $f$ with respect to protected attribute $S$ can be obtained by:

$$
\begin{equation*}
P(\hat{y}=1 \mid x, s=1)-P(\hat{y}=1 \mid x, s=0) \tag{2}
\end{equation*}
$$

## III. MODEL DESCRIPTION

## A. Tabular Dataset Representation and Transformation

A tabular dataset contains $N_{C}$ numerical columns $\left\{c_{1}, \ldots, c_{N_{C}}\right\}$ and $N_{D}$ categorical columns $\left\{d_{1}, \ldots, d_{N_{D}}\right\}$. In this model, categorical columns are transformed and represented by one-hot vectors. Representing numerical columns on the other hand is non-trivial due to certain properties of numerical columns. One such property is that numerical columns are often sampled from multi-modal distributions. Some models such as [21] use min-max normalization to normalize and transform numerical columns. The work of $\mathrm{Xu}$ et al. [23] proposes a more complex process, namely a mode-specific normalization using variational Gaussian mixture model (VGM) to estimate the number of modes and fit a Gaussian mixture model to each numerical column. In our model, each numerical column is transformed using a quantile transformation [25]:

$$
\begin{equation*}
c_{i}^{\prime}=\Phi^{-1}\left(F\left(c_{i}\right)\right) \tag{3}
\end{equation*}
$$

Where $c_{i}$ is the $i$ th numerical feature, $F$ is the CDF (cumulative distbituion function) of the feature $c_{i}$, and $\Phi$ is the CDF of a uniform distribution. After transforming numerical and discrete columns, the representation of each transformed row of the data is as follows:

$$
\begin{align*}
& \mathbf{r}=c_{1}^{\prime} \oplus \ldots \oplus c_{N_{C}}^{\prime} \oplus d_{1}^{\prime} \oplus \ldots \oplus d_{N_{D}}^{\prime}  \tag{4}\\
& l_{i}=\operatorname{dim}\left(d_{i}^{\prime}\right)  \tag{5}\\
& l_{w}=\operatorname{dim}(r) \tag{6}
\end{align*}
$$

where $c^{\prime}{ }_{i}$ represents the $i$ th numerical column, $d^{\prime}{ }_{i}$ denotes the one-hot encoded vector of the $i$ th categorical columns, and $\oplus$ is the symbol denoting concatenation of vectors. Also, $l_{i}$ shows the dimension of the $i$ th discrete column's one-hot encoding vector and $l_{w}$ shows the the dimension of $r$.

## B. Network Structure

While traditional GANs suffer from problems such as nonconvergence and mode-collapse, the work of [15] developed Wasserstein GANs which improve training of GANs to some extent, and replace the discriminator with a critic. The network designed in this model is a WGAN with gradient penalty [26]. The WGAN value function using the Kantorovich-Rubinstein duality [27] is as follows [26]:

$$
\begin{equation*}
\min _{G} \max _{C \in \mathcal{C}} \underset{\mathbf{x} \sim P_{\text {data }}(\mathbf{x})}{\mathbb{E}}[C(\mathbf{x})]-\underset{\mathbf{z} \sim P_{z}(\mathbf{z})}{\mathbb{E}}[C(G(z))] \tag{7}
\end{equation*}
$$

Where $\mathcal{C}$ is the set of 1-Lipschitz functions. The generator receives a latent variable $Z$ from a standard multivariate normal distribution and produces a sample data point which is then forwarded to the critic. Once the critic and the generator are trained together, eventually the generator would become like a deterministic transformation that produces data similar to the real data.

The generator consists of a fully-connected first layer with ReLu activation function. The second hidden layer of the generator network is then formed by concatenation of multiple vectors that could form data similar to transformed original data. For the numerical variables, a fully connected layer of $\mathrm{FC}_{l_{w} \rightarrow N_{C}}$, with a $\mathrm{ReLu}$ activation is implemented. For nodes that are supposed to produce discrete columns, multiple fully connected layer of $\mathrm{FC}_{l_{w} \rightarrow l_{i}}$, with Gumble softmax [28] activation is used in order to produce one-hot vectors $\left(d_{i}^{\prime}\right)$. The resulting nodes are then concatenated to produce data similar to the transformed original data (with the same dimension of $l_{w}$ ), which is then fed to the critic network. The structure of the critic network is simple and includes 2 fully connected layers with Leaky ReLu activation functions.

The generator network's architecture is formally described as:

$\left\{\begin{array}{l}h_{0}=Z \text { (latent vector) } \\ h_{1}=\operatorname{ReLu}\left(\mathrm{FC}_{l_{w} \rightarrow l_{w}}\left(h_{0}\right)\right) \\ h_{2}=\operatorname{ReLu}\left(\mathrm{FC}_{l_{w} \rightarrow N_{C}}\left(h_{1}\right)\right) \oplus \text { gumble }_{0.2}\left(\mathrm{FC}_{l_{w} \rightarrow l_{1}}\left(h_{1}\right)\right) \oplus \\ \text { gumble }_{0.2}\left(\mathrm{FC}_{l_{w} \rightarrow l_{2}}\left(h_{1}\right)\right) \oplus \ldots \oplus \text { gumble }_{0.2}\left(\mathrm{FC}_{l_{w} \rightarrow l_{N_{D}}}\left(h_{1}\right)\right)\end{array}\right.$

Where $F C_{a \rightarrow b}$ denotes a fully connected layer with input size $a$ and output size $b, \operatorname{ReLu}(x)$ shows applying a ReLu activation on $x$, and gumble ${ }_{\tau}(x)$ denotes applying Gumble softmax with parameter $\tau$ on a vector $x$, and $\oplus$ denotes concatenation of vectors.

The critic network's architecture is formally described as:

$\left\{\begin{array}{l}h_{0}=X \text { (output of the generator or transformed real data) } \\ h_{1}=\mathrm{LeakyReLu}_{0.01}\left(\mathrm{FC}_{l_{w} \rightarrow l_{w}}\left(h_{0}\right)\right) \\ h_{2}=\operatorname{LeakyReLu}_{0.01}\left(\mathrm{FC}_{l_{w} \rightarrow l_{w}}\left(h_{1}\right)\right)\end{array}\right.$

Where LeakyReLu ${ }_{\tau}(x)$ denotes applying Leaky ReLu activation function [29] with slope $\tau$ on $x$. Fig. 1] shows the architecture of the model.

## C. Training

In this section we introduce the loss functions for the critic network and generator network of the developed WGAN. The overall process of training the model includes two phases. Phase I of training only focuses on training the model such that the generator could generate data with a joint probability distribution similar to that of the real data. Phase II of training

![](https://cdn.mathpix.com/cropped/2024_06_04_a31a2c0cdbfc87b415ddg-4.jpg?height=433&width=889&top_left_y=171&top_left_x=152)

Fig. 1. Model architecture. The generator consists of an initial fully connected layer with ReLu activation function, and a second layer which uses ReLu for numerical attributes generation and gumble-softmax to form one-hot representations of categorical attributes. The final data is then produced by concatenating all attributes in the last layer of the generator. The critic consists of fully-connected layers with LeakyReLu activation function.

further trains the generator to produce samples which have a joint probability distribution similar to that of real data and is also fair, with respect to discrimination score (DS) defined in Section $\square$.

1) Phase I: Training for Accuracy: In the first phase, generator and critic are trained with respect to their value functions. Critic's loss function with gradient penalty is [26]:

$V_{C}=\underset{\hat{\mathbf{x}} \sim P_{g}}{\mathbb{E}}[C(\hat{\mathbf{x}})]-\underset{\mathbf{x} \sim P_{r}}{\mathbb{E}}[C(\mathbf{x})]+\lambda \underset{\overline{\mathbf{x}} \sim P_{\mathbf{x}}}{\mathbb{E}}\left[\left(\left\|\nabla_{\overline{\mathbf{x}}} C(\overline{\mathbf{x}})\right\|_{2}-1\right)^{2}\right]$

Where $P_{\mathrm{r}}$ and $P_{\mathrm{g}}$ are real data distribution and generated data distribution, respectively. Note that the third term is the gradient penalty to enforce the Lipschitz constraint, and $P_{\overline{\mathbf{x}}}$ is implicitly defined sampling uniformly along straight lines between pairs of points sampled from the data distribution $P_{\mathbf{r}}$ and the generator distribution $P_{\mathrm{g}}$ [26].

The loss function for the generator network in Phase I of training is also as follows:

$$
\begin{equation*}
V_{G}=-\underset{\hat{\mathbf{x}} \sim P_{g}}{\mathbb{E}}[C(\hat{\mathbf{x}})] \tag{11}
\end{equation*}
$$

2) Phase II: Training for Fairness and Accuracy: In the second phase of training, fairness constraint is enforced on generator to produce fair data. Similar to the definitions in Section II, let $\hat{D}=\{\hat{X}, \hat{Y}, \hat{S}\}$ be a batch of generated data, where $X$ is the unprotected attribute of the generated data, $\hat{Y}$ is the decision with $\hat{Y}=1$ being the successful and favorable value for the decision (e.g. having an income of $>50 K$ for an adult in the adult income dataset), and $\hat{S}$ being the protected attribute with $\hat{S}=0$ showing the unprivileged minority group (for example having a gender of "female" in the adult income data set). The new loss function for the generator in Phase II of training is as follows:

$$
\begin{align*}
V_{G}=-\underset{(\hat{\mathbf{x}}, \hat{\mathbf{y}}, \hat{\mathbf{s}}) \sim P_{g}}{\mathbb{E}}[C(\hat{\mathbf{x}}, \hat{\mathbf{y}}, \hat{\mathbf{s}})]-\lambda_{f}\left(\underset{(\hat{\mathbf{x}}, \hat{\mathbf{y}}, \hat{\mathbf{s}}) \sim P_{g}}{\mathbb{E}}[\hat{\mathbf{y}} \mid \hat{\mathbf{s}}=0]-\right. \\
\left.\underset{(\hat{\mathbf{x}}, \hat{\mathbf{y}}, \hat{\mathbf{s}}) \sim P_{g}}{\mathbb{E}}[\hat{\mathbf{y}} \mid \hat{\mathbf{s}}=1]\right) \tag{12}
\end{align*}
$$

With the above loss function for the generator, the model aims to generate a fair dataset $\{\hat{X}, \hat{Y}, \hat{S}\} \sim P_{g}$ which achieves the demographic fairness with respect to the protected attribute $\hat{S}$ in the generated samples, by minimizing discrimination score in the generated data $P(\hat{Y} \mid \hat{S}=1)-P(\hat{Y} \mid \hat{S}=0)$. The goal in this phase of training is to train the generator to generate synthetic data which is both similar to the real data $\hat{D} \sim D$, and the generated data is fair based on demographic fairness measure. In the ideal case, the generator would produce synthetic data such that $\hat{Y} \perp \hat{S}$. After training is done, the samples are generated and inverse transformed to the original data format. The formal procedure of training the model is shown in Algorithm 1

```
Algorithm 1 training algorithm for the proposed WGAN. We
use $n_{\text {crit }}=4$, batch size of $256, \lambda_{p}=10$, Adam optimizer
with $\alpha=0.0002, \beta_{1}=0.5$, and $\beta_{2}=0.999$
    for $T_{1}$ do
        for $t=1, \ldots, n_{\text {crit }}$ do
            Sample batch $m D(x, y, s) \sim P_{r}$ and $z \sim P(z)$ and $\epsilon \sim U[0,1]$
            $\hat{D}=(\hat{x}, \hat{s}, \hat{y}) \leftarrow G_{\theta}(z)$
            $\bar{D} \leftarrow \epsilon(D)+(1-\epsilon)(\hat{D})$
            Update the critic by descending the gradient:
            $\nabla_{w} \frac{1}{m} \sum_{i=1}^{m} C_{w}(\hat{D})-C_{w}(D)+\lambda_{p}\left(\left\|\nabla_{\bar{D}} C_{w}(\bar{D})\right\|_{2}-1\right)^{2}$
        end for
        Sample a batch $m z \sim P(z)$
        Update the generator by descending the gradient:
        $\nabla_{\theta} \frac{1}{m} \sum_{i=1}^{m}-\left(C_{w}\left(G_{\theta}(z)\right)\right)$
    end for
    for $T_{2}$ do
        for $t=1, \ldots, n_{\text {crit }}$ do
            Sample batch $m D(x, y, s) \sim P_{r}$ and $z \sim P(z)$ and $\epsilon \sim U[0,1]$
            $\hat{D}=(\hat{x}, \hat{s}, \hat{y}) \leftarrow G_{\theta}(z)$
            $\bar{D} \leftarrow \epsilon(D)+(1-\epsilon)(\hat{D})$
            Update the critic by descending the gradient:
            $\nabla_{w} \frac{1}{m} \sum_{i=1}^{m} C_{w}(\hat{D})-C_{w}(D)+\lambda_{p}\left(\left\|\nabla_{\bar{D}} C_{w}(\bar{D})\right\|_{2}-1\right)^{2}$
        end for
        sample a batch $m \hat{D}=\hat{x}, \hat{s}, \hat{y} \sim P\left(G_{\theta}(z)\right)$
        Update the generator by descending the gradient:
            $\nabla_{\theta} \frac{1}{m} \sum_{i=1}^{m}-C_{w}(\hat{D})-\lambda_{f}\left(\frac{\left|D_{s=0, y=1}\right|}{\left|D_{s=0}\right|}-\frac{\left|D_{s=1, y=1}\right|}{\left|D_{s=1}\right|}\right)$
    end for
```


## IV. EXPERIMENT: ONLY PHASE I (NO FAIRNESS)

In this section, we evaluate the effectiveness of the model in producing synthetic data simialr to data coming from a known probability distribution. We show that the model is able to generate synthetic data similar to the reference dataset and compare our results with two state-of-the-art GAN models for generation of tabular datasets, namely TGAN [22] and CTGAN [23]. TGAN is a GAN-based model that generates relational tables by clustering numerical variables to deal with multi-modal distributions and adding noise and KL divergence into loss function to generate discrete features. In CTGAN, mode-specific normalization is applied to numerical values and
the generator works conditionally in order to overcome the imbalance in training data. We evaluate the model on UCI Adult Income Datase ${ }^{1}[30]$. The task we are trying to achieve is as follows: given a dataset $D=\{X, S, Y\} \sim P_{\text {data }}$, generate a dataset $\hat{D}_{\text {syn }}=\{\hat{X}, \hat{S}, \hat{Y}\} \sim P_{s y n}$ s.t. $P_{\text {syn }} \sim P_{\text {data }}$. We are not seeking to achieve fairness in this section, and we solely seek to generate data following the same distribution as real data to achieve data utility.

To compare data utility among generated datasets among different models, we evaluate the performance of using synthetic data as training data for machine learning. At first, the real dataset is divided into two parts: $\mathbf{D}_{\text {train }}$ and $\mathbf{D}_{\text {test }}$. Adult dataset contains a total of 48,842 rows. $90 \%$ of the data were assigned to $\mathbf{D}_{\text {train }}$ and the rest $10 \%$ were assigned to $\mathbf{D}_{\text {test }}$. Next, each model is trained on the training set $\mathbf{D}_{\text {train }}$ for 300 epochs three times. With each training, the trained models are used to generate their corresponding synthetic data $\mathbf{D}_{\text {syn }}$. Three machine learning classifiers are then chosen and trained on each generated $\mathbf{D}_{\text {syn }}$, tested on $\mathbf{D}_{\text {test }}$, and eventually the accuracy and F1 score of classification is recorded. The classifiers used are a Decision Tree Classifier, Logistic Regression, and a Multi Layer Perceptron. Table I reports the results of classification, and compares the results with the case that a classifier is trained on the original $\mathbf{D}_{\text {train }}$, and tested on $\mathbf{D}_{\text {test }}$ (reporting the means and standard deviations of evaluation metrics). The results shows that TabFairGAN and CTGAN outperform TGAN in all cases. TabFairGAN outperforms CTGAN with a DT Classifier. With a LR classifier, the performance of TabFairGAN and CTGAN is identical with respect to accuracy, and TabFairGAN performs slightly better than CTGAN with respect to F1 score. With a MLP classifier, CTGAN performs slightly better than TabFairGAN with respect to accuracy, while TabFairGAN outperforms CTGAN with respect to F1 score. These results display the effetiveness of TabFariGAN with respect to generating data identical to real tabular data.

## V. EXPERIMENTS: FAIR DATA GENERATION AND DATA UTILITY (TRAINING WITH BOTH PHASE I AND PHASE II)

In the second set of experiments, the effectiveness of the model in generating data which is both similar to the reference dataset and also fair is evaluated, and the tradeoff between machine learning efficacy and fairness is investigated. We will experiment with four datasets to test the fairness/utility tradeoff of the model. The four datasets and their attributes are first introduced. All four datasets used in experiments are studied in the literature of algorithmic fairness [3]. Next, we introduce the baseline method with which the results of TabFairGAN are compared. The results are presented and compared in Table II

## A. Datasets

The first dataset is UCI Adult Dataset [30]. This dataset is based on 1994 US census data and contains 48,842 rows with[^0]

attributes such as age, sex, occupation, and education level. for each person, and the target variable indicates whether that individual has an income that exceeds $\$ 50 \mathrm{~K}$ per year or not. In our experiments, we consider the protected attribute to be $\operatorname{sex}(S=$ "Sex", $Y=$ "Income").

The second dataset used in the experiments is the Bank Marketing Data Set [31]. This dataset contains information about a direct marketing campaign of a Portuguese banking institution. Each row of the dataset contains attributes about an individual such as age, job, marital status, housing, duration of that call, and the target variable determines whether that individual subscribed a term deposit or not. The dataset contains 45,211 records. Similar to [32], we have considered age to be the protected attribute (a young individual has a higher chance of being labeled as "yes" to subscribe a term deposit). In order to have a binary protected attribute, we set a cut-off value of 25 and an age of more than 25 is considered "older", while an age of less than or equal to 25 is considered "younger" ( $S=$ "Age", $Y=$ "Subscribed").

The third dataset used in this section is the ProPublica dataset from COMPAS risk assessment system [33]. This dataset contains information about defendants from Broward County, and contains attributes about defendants such as their ethnicity, language, marital status, sex, etc. ,and for each individual a score showing the likelihood of recidivism (reoffending). In this experiments we used a modified version of the dataset. First, attributes such as FirstName, LastName, MiddleName, CASE_ID, and DateOfBirth are removed. Studies have shown that this dataset is biased against African Americans [1]. Therefore, ethnicity is chosen to be the protected attribute for this study. Only African American and Caucasian individuals are kept and the rest are dropped. The target variable in this dataset is a risk decile score provided by COMPAS system, showing the likelihood of that individual to re-offend, which ranges from 1 to 10 . The final modified dataset contains 16,267 records with 16 features. To make the target variable binary, a cut-off value of 5 is considered and individuals with a declile score of less than 5 are considered "Low_Chance", while the rest are considered "High_Chance". ( $S=$ "Ethnicity", $Y=$ "Recidivism_Chance").

The last dataset used in experiments is the Law School Admission Council which is made by conducting a survey across 162 law schools in the United States [34]. This dataset contains information on 21,790 law students such as their GPA (grade-point average), LSAT score, race, and the target variable is whether the student had a high FYA (first year average grade). Similar to other studies (such as [35]), we have considered race to be the protected attribute. We only considered individuals with "Black" or "White" race. The modified data contains 19,567 records. ( $S=$ "Race", $Y=$ "FYA"). There discrimination score (DS) of all datasets are reported in Table II

TABLE I

COMPARING THE RESULTS TABFAIRGAN FOR ACCURATE DATA GENERATION WITH TGAN AND CTGAN MODELS

| Classifier | DTC |  | LR |  | MLP |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Accuracy | F1 | Accuracy | F1 | Accuracy | F1 |
| Original Data | $0.811 \pm 0.001$ | $0.606 \pm 0.002$ | $0.798 \pm 0.000$ | $0.378 \pm 0.000$ | $0.780 \pm 0.051$ | $0.488 \pm 0.075$ |
| TabFairGan | $\mathbf{0 . 7 8 3} \pm \mathbf{0 . 0 0 1}$ | $\mathbf{0 . 5 4 4} \pm \mathbf{0 . 0 0 2}$ | $\mathbf{0 . 7 9 4} \pm \mathbf{0 . 0 2 0}$ | $\mathbf{0 . 2 3 9} \pm \mathbf{0 . 0 1 2}$ | $0.778 \pm 0.045$ | $\mathbf{0 . 4 0 5} \pm \mathbf{0 . 1 7 4}$ |
| TGAN | $0.661 \pm 0.013$ | $0.503 \pm 0.012$ | $0.765 \pm 0.010$ | $0.170 \pm 0.008$ | $0.623 \pm 0.197$ | $0.376 \pm 0.159$ |
| CTGAN | $0.777 \pm 0.003$ | $0.482 \pm 0.004$ | $\mathbf{0 . 7 9 4} \pm \mathbf{0 . 0 2 3}$ | $0.232 \pm 0.012$ | $\mathbf{0 . 7 8 4} \pm \mathbf{0 . 0 0 7}$ | $0.305 \pm 0.104$ |

## B. Baseline Model: Certifying and Removing Disparate Impact

In their work Feldman et al. [5] proposed a method to modify a dataset to remove bias and preserve relevant information in the data. In dataset $D=\{X, S, Y\}$, given the protected attribute $S$ and a single numerical attribute $X$, let $X_{s}=\operatorname{Pr}(X \mid S=s)$ denote the marginal distribution on $X$ conditioned on $S=s$. Considering $F_{s}: X_{s} \rightarrow[0,1]$ the cumulative distribution function for values $x \in X_{s}$, they define a "median" distribution $A$ in terms of its quantile function $F_{A}^{-1}: F_{A}^{-1}(u)=$ median $_{s \in S} F_{s}^{-1}(u)$. They then propose a repair algorithm which creates $\bar{X}$, such that for all $x \in X_{s}$ the corresponding $\bar{x}=F_{A}^{-1}\left(F_{s}(x)\right)$. To control the tradeoff between fairness and accuracy, they define and calculate $\lambda$ - partial repair by:

$$
\begin{equation*}
\bar{F}_{s}^{-1}=(1-\lambda) F_{s}^{-1}+\lambda\left(F_{A}\right)^{-1} \tag{13}
\end{equation*}
$$

The result of such partial repair procedure is a dataset $\bar{D}=\{\bar{X}, S, Y\}$ which is more fair and preserves relevant information for classification task. We call this method CRDI henceforth.

## C. Results

The goal in this section is to train the proposed network on datasets and produce similar data that is also fair with respect to protected attributes defined for each dataset. The process is as follows: The models are first trained on each dataset. As mentioned in Section III-C, training of the network includes two phases: in the first phase, the network is only trained for accuracy for a certain number of epochs, and then in the second phase, the loss function of generator is modified and the network gets trained for accuracy and fairness. Once the training is finished, the generator of the network is used to produce synthetic data $\mathbf{D}_{\text {syn }}$. We also generated repaired datasets using CRDI method described in Section V-B to compare our results with. For each model, we train five times and report the means and standard deviations of evaluation results in Table II

The generated data $\mathbf{D}_{\text {syn }}$ is then evaluated from two perspective: fairness and utility. To evaluate the fairness of $\mathbf{D}_{\text {syn }}$, we adopt discrimination score (DS): $D S=P(y=1 \mid s=$ 1) $-P(y=1 \mid s=0)$. Looking into Table II the results show that comparing with CRDI, TabFairGAN could more effectively produce datasets s.t. demographic parity in the generated data is almost removed. The demographic parity of the produced datasets by TabFairGAN, beat the repaired datasets produced by CRDI.

To evaluate data utility, we adopt a decision tree classifier with the default parameter setting [36]. For TabFairGAN data, We train the decision tree classifier on $\mathbf{D}_{\text {syn }}$ and test it on $\mathbf{D}_{\text {test }}$, and report the accuracy and F1-score of the classifier. We also train decision tree classifiers on repaired data $\bar{D}$ produced by CRDI, and test on $\mathbf{D}_{\text {test }}$ and report accuracy and f1-score. Table II shows that repaired data $\bar{D}$ produced by CRDI has better data utility for adult dataset, COMPAS dataset, and Law School dataset by less than $5 \%$ in all cases, while the accuracy of $\mathbf{D}_{\text {syn }}$ produced by TabFairGAN is almost $8 \%$ higher than that of $\bar{D}$ produced by CRDI.

The last evaluation we perform on the produced datasets is to examine discrimination score (DS) of the classifier. we adopt discrimination score (DS) for classifier: $D S=P(\hat{y}=$ $1 \mid s=1)-P(\hat{y}=1 \mid s=0)$. The results in Table II show that discrimination score of the decision tree classifier trained on $\mathbf{D}_{\text {syn }}$ for Adult dataset and Law School is lower by almost $4 \%$ and $13 \%$, respectively, while the discrimination score of the decision tree classifier trained on $\overline{\mathbf{D}}$ for Bank dataset and COMPAS dataset is lower by $1 \%$ and $0.003 \%$, respectively.

The parameter settings of the models on each datasets is reported in the Appendix. The results show, while CRDI narrowly beats TabFairGAN in terms of data utility, TabFairGAN beats CRDI in terms of discrimination score in all cases for generated data and in 2 out of 4 cases in the generated classifiers. This is attributed to fairness utility tradeoff of TabFairGAN governed by $\lambda_{f}$. The case of COMPAS dataset is interesting since none of the models could decrease discrimination score in the classifier much, comparing to the discrimination score in the original dataset. Looking into the data and performing a correlation analysis, risk decile score (target variable) has a high Pearson correlation of 0.757 with one of columns names RecSupervisionLevel which denotes the supervisory status of each individual. This reveals that although the generated dataset $\mathbf{D}_{\text {syn }}$ has a lower discrimination score of 0.009 , disparate impact exists in the dataset, indicating that the discriminatory outcomes are not explicitly caused by the protected attribute, but are also from the proxy unprotected attributes [20].

TABLE II

COMPARING THE RESULTS OF TABFAIRGAN FOR FAIR DATA GENERATION WITH CRDI

| Dataset | Original Data |  |  | TabFairGAN |  |  |  | CRDI |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Orig. Acc. | F1 Orig. | DS in Orig. Data | DS Gen. Data | Acc. Gen. Data | F1 Gen. Data | DS in Classifier | DS Rep. Data. | Acc. Rep. Data | F1 Rep. Data | DS in Classifier |
| Adult | $0.816 \pm 0.005$ | $0.619 \pm 0.013$ | 0.195 | $0.009 \pm \mathbf{0 . 0 2 7}$ | $0.773 \pm 0.013$ | $0.536 \pm 0.022$ | $0.082 \pm 0.038$ | $0.165 \pm 0.048$ | $0.793 \pm 0.011$ | $0.558 \pm 0.029$ | $0.121 \pm 0.024$ |
| Bank | $0.879 \pm 0.004$ | $0.491 \pm 0.020$ | 0.126 | $0.001 \pm \mathbf{0 . 0 1 1}$ | $0.854 \pm 0.004$ | $0.373 \pm 0.024$ | $0.060 \pm 0.056$ | $0.122 \pm 0.004$ | $0.776 \pm 0.004$ | $0.384 \pm 0.011$ | $0.050 \pm 0.017$ |
| COMPAS | $0.903 \pm 0.007$ | $0.914 \pm 0.007$ | 0.258 | $0.009 \pm \mathbf{0 . 1 0 2}$ | $0.860 \pm 0.040$ | $0.876 \pm 0.033$ | $0.208 \pm 0.072$ | $0.119 \pm 0.128$ | $0.893 \pm 0.021$ | $0.906 \pm 0.020$ | $0.205 \pm 0.055$ |
| Law School | $0.854 \pm 0.008$ | $0.918 \pm 0.005$ | 0.302 | $0.024 \pm 0.036$ | $0.847 \pm 0.020$ | $0.916 \pm 0.012$ | $0.153 \pm 0.072$ | $0.233 \pm 0.103$ | $0.892 \pm 0.004$ | $0.941 \pm 0.002$ | $0.289 \pm 0.057$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_a31a2c0cdbfc87b415ddg-7.jpg?height=521&width=789&top_left_y=607&top_left_x=169)

Fig. 2. Exploring the trade-off between accuracy and fairness by incremental increasing of parameter $\lambda_{f}$

## D. Utility and Fairness Trade-off

To explore the trade-off between utility and fairness of the generated data, we perform the following experiment: $\lambda_{f}$ was increased between $[0.05,0.7]$ in steps of 0.05 , and for each value of $\lambda_{f}$ the model was trained 170 epochs in phase I and 30 times in the phase II. For each $\lambda_{f}$ value, five training was performed and the average of Discrimination Score was recorded for each $\lambda_{f}$. Figure 2 shows the results, plotted along with standard deviation as confidence intervals. We can observe that discrimination score of the generated synthetic datasets ( $D_{\text {syn }}$ ) is decreasing significantly as $\lambda_{f}$ decreases. Meanwhile, classifier accuracy layoff, i.e. the reduction in decision tree classifier's accuracy comparing to the case in which the classifier is trained on the real original training dataset ( $D_{\text {train }}$ ), is increasing slightly as $\lambda_{f}$ increases.

## VI. CONCLUSION

In this paper, we proposed a Wasserstein Generative Adversarial Network that could generate synthetic data similar to a reference data. We showed that in the case of unconditional tabular data generation, i.e. with no fairness constrains, the model is able to produce data with high quality comparing to other GANs developed for the same purpose. We also showed that by adding a fairness constraint to the generator, the model is able to achieve data generation which improves the demographic parity of the generated data. We tested the model on four datasets studies in the fairness literature and compared our results with that of [5]. As a generative model, GANs have a great potential to be utilized for fair data generation, specially in the case that the real dataset is limited. There are other field in which GANs could be utilized for tabular data generation, such as the research involved with data privacy [37]. In the future work, we will explore other more sophisticated data generation constraints, e.g. considering enforcing other fairness metrics such as equality of odds and equality of opportunity. We also consider exploring utilizing GANs for fairness in other data types, such as text and image data.

## REFERENCES

[1] A. Chouldechova, "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments," Big data, vol. 5, no. 2, pp. $153-163,2017$.

[2] A. Lambrecht and C. Tucker, "Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads," Management Science, vol. 65, no. 7, pp. 2966-2981, 2019.

[3] D. Pessach and E. Shmueli, "Algorithmic fairness," arXiv preprint arXiv:2001.09784, 2020.

[4] F. Kamiran and T. Calders, "Data preprocessing techniques for classification without discrimination," Knowledge and Information Systems, vol. 33, no. 1, pp. 1-33, 2012.

[5] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, "Certifying and removing disparate impact," in proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 259-268.

[6] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma, "Fairness-aware classifier with prejudice remover regularizer," in Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2012, pp. 35-50.

[7] M. Hardt, E. Price, and N. Srebro, "Equality of opportunity in supervised learning," Advances in neural information processing systems, vol. 29, pp. 3315-3323, 2016.

[8] A. Oussidi and A. Elhassouny, "Deep generative models: Survey," in 2018 International Conference on Intelligent Systems and Computer Vision (ISCV). IEEE, 2018, pp. 1-8.

[9] S. E. Fahlman, G. E. Hinton, and T. J. Sejnowski, "Massively parallel architectures for al: Netl, thistle, and boltzmann machines," in National Conference on Artificial Intelligence, AAAI, 1983

[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial nets," Advances in neural information processing systems, vol. 27, 2014.

[11] A. Brock, J. Donahue, and K. Simonyan, "Large scale gan training for high fidelity natural image synthesis," arXiv preprint arXiv:1809.11096, 2018 .

[12] C. Vondrick, H. Pirsiavash, and A. Torralba, "Generating videos with scene dynamics," Advances in neural information processing systems, vol. 29, pp. 613-621, 2016.

[13] M. Men√©ndez, J. Pardo, L. Pardo, and M. Pardo, "The jensen-shannon divergence," Journal of the Franklin Institute, vol. 334, no. 2, pp. 307$318,1997$.

[14] Y. Rubner, C. Tomasi, and L. J. Guibas, "The earth mover's distance as a metric for image retrieval," International journal of computer vision, vol. 40, no. 2, pp. 99-121, 2000.

[15] M. Arjovsky, S. Chintala, and L. Bottou, "Wasserstein generative adversarial networks," in International conference on machine learning. PMLR, 2017, pp. 214-223.

[16] H. Edwards and A. Storkey, "Censoring representations with an adversary," arXiv preprint arXiv:1511.05897, 2015.

[17] D. Madras, E. Creager, T. Pitassi, and R. Zemel, "Learning adversarially fair and transferable representations," in International Conference on Machine Learning. PMLR, 2018, pp. 3384-3393.

[18] B. H. Zhang, B. Lemoine, and M. Mitchell, "Mitigating unwanted biases with adversarial learning," in Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 2018, pp. 335-340.

[19] P. Sattigeri, S. C. Hoffman, V. Chenthamarakshan, and K. R. Varshney, "Fairness gan: Generating datasets with fairness properties using a generative adversarial network," IBM Journal of Research and Development, vol. 63, no. 4/5, pp. 3-1, 2019.

[20] D. Xu, S. Yuan, L. Zhang, and X. Wu, "Fairgan: Fairness-aware generative adversarial networks," in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 570-575.

[21] E. Choi, S. Biswal, B. Malin, J. Duke, W. F. Stewart, and J. Sun, "Generating multi-label discrete patient records using generative adversarial networks," in Machine learning for healthcare conference. PMLR, 2017, pp. 286-305.

[22] L. Xu and K. Veeramachaneni, "Synthesizing tabular data using generative adversarial networks," arXiv preprint arXiv:1811.11264, 2018.

[23] L. Xu, M. Skoularidou, A. Cuesta-Infante, and K. Veeramachaneni, "Modeling tabular data using conditional gan," in Advances in Neural Information Processing Systems, 2019.

[24] D. Xu, S. Yuan, L. Zhang, and X. Wu, "Fairgan+: Achieving fair data generation and classification through generative adversarial nets," in 2019 IEEE International Conference on Big Data (Big Data). IEEE, 2019, pp. 1401-1406.

[25] T. M. Beasley, S. Erickson, and D. B. Allison, "Rank-based inverse normal transformations are increasingly used, but are they merited?" Behavior genetics, vol. 39, no. 5, pp. 580-595, 2009.

[26] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, "Improved training of wasserstein gans," in Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/2017/ file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf

[27] C. Villani, Optimal transport: old and new. Springer, 2009, vol. 338.

[28] E. Jang, S. Gu, and B. Poole, "Categorical reparameterization with gumbel-softmax," arXiv preprint arXiv:1611.01144, 2016.

[29] B. Xu, N. Wang, T. Chen, and M. Li, "Empirical evaluation of rectified activations in convolutional network," arXiv preprint arXiv:1505.00853, 2015.

[30] D. Dua and C. Graff, "UCI machine learning repository," 2017. [Online]. Available: http://archive.ics.uci.edu/ml

[31] S. Moro, P. Cortez, and P. Rita, "A data-driven approach to predict the success of bank telemarketing," Decision Support Systems, vol. 62, pp. 22-31, 2014.

[32] M. B. Zafar, I. Valera, M. G. Rogriguez, and K. P. Gummadi, "Fairness constraints: Mechanisms for fair classification," in Artificial Intelligence and Statistics. PMLR, 2017, pp. 962-970.

[33] J. Angwin, J. Larson, S. Mattu, and L. Kirchner. (2016) Machine bias propublica. [Online]. Available: https://www.propublica.org/article/ machine-bias-risk-assessments-in-criminal-sentencing

[34] L. F. Wightman, "Lsac national longitudinal bar passage study. 1sac research report series." 1998.

[35] Y. Bechavod and K. Ligett, "Penalizing unfairness in binary classification," arXiv preprint arXiv:1707.00044, 2017.

[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al., "Scikit-learn: Machine learning in python," the Journal of machine Learning research, vol. 12, pp. 2825-2830, 2011.

[37] N. Park, M. Mohammadi, K. Gorde, S. Jajodia, H. Park, and Y. Kim, "Data synthesis based on generative adversarial networks," arXiv preprint arXiv:1806.03384, 2018
