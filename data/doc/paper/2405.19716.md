# Enhancing Large Vision Language Models with Self-Training on Image Comprehension 

Yihe Deng ${ }^{* 1}$, Pan $\mathbf{L u}^{* 1,3}$, Fan Yin ${ }^{1}$, Ziniu Hu ${ }^{1}$, Sheng Shen ${ }^{2}$<br>James Zou ${ }^{3}$, Kai-Wei Chang ${ }^{1}$, Wei Wang ${ }^{1}$<br>${ }^{1}$ University of California, Los Angeles<br>${ }^{2}$ University of California, Berkeley ${ }^{3}$ Stanford University<br>https://stic-lvlm.github.io/

![](https://cdn.mathpix.com/cropped/2024_06_04_18d42068839193d4cf89g-01.jpg?height=502&width=832&top_left_y=936&top_left_x=359)

Query: How many gallons of supreme gasoline can I get with $\$ 50$ ?

Base (LLaVA-v1.6 7B):

Based on the current gas prices displayed on the sign, you can get approximately 3.65 gallons of supreme gasoline with $\$ 50$.

## STIC (LLaVA-v1.6 7B):

With $\$ 50$, you can get approximately 13.69 gallons of supreme gasoline, as indicated by the price of $\$ 3.65$ per gallon on the sign.

Figure 1: Left: Accuracy improvement of our method, STIC, compared to the original LLaVA-v1.6 (Liu et al., 2024) on seven benchmarks. Right: Response examples from the original LLaVA-v1.6 and STIC (LLaVA-v1.6), which enhances image comprehension and subsequent reasoning capabilities.


#### Abstract

Large vision language models (LVLMs) integrate large language models (LLMs) with pre-trained vision encoders, thereby activating the perception capability of the model to understand image inputs for different queries and conduct subsequent reasoning. Improving this capability requires high-quality vision-language data, which is costly and labor-intensive to acquire. Self-training approaches have been effective in single-modal settings to alleviate the need for labeled data by leveraging model's own generation. However, effective self-training remains a challenge regarding the unique visual perception and reasoning capability of LVLMs. To address this, we introduce Self-Training on Image Comprehension (STIC), which emphasizes a self-training approach specifically for image comprehension. First, the model self-constructs a preference dataset for image descriptions using unlabeled images. Preferred responses are generated through a step-by-step prompt, while dis-preferred responses are generated from either corrupted images or misleading prompts. To further self-improve reasoning on the extracted visual information, we let the model reuse a small portion of existing instruction-tuning data and append its self-generated image descriptions to the prompts. We validate the effectiveness of STIC across seven different benchmarks, demonstrating substantial performance gains of $4.0 \%$ on average while using $70 \%$ less supervised fine-tuning data than the current method. Further studies investigate various components of STIC and highlight its potential to leverage vast quantities of unlabeled images for self-training. Code and data are made publicly available.


[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_18d42068839193d4cf89g-02.jpg?height=563&width=1379&top_left_y=239&top_left_x=362)

Figure 2: Framework overview of STIC, a two-stage self-training algorithm focusing on the image comprehension capability of the LVLMs. In Stage 1, the base LVLM self-constructs its preference dataset for image description using well-designed prompts, poorly-designed prompts, and distorted images. In Stage 2, a small portion of the previously used SFT data is recycled and infused with model-generated image descriptions to further fine-tune the base LVLM.

## 1 Introduction

In recent years, we have witnessed remarkable advancements in large language models (LLMs), such as GPT-4 (OpenAI, 2023a) and the LLaMA family (Touvron et al., 2023a,b). The increasing importance of processing multimodal inputs, including images and text, has significantly driven progress in vision language models (Radford et al., 2021; Jia et al., 2021b; Goel et al., 2022). Leveraging the powerful language understanding and generation capabilities of LLMs, researchers have advanced vision language models into large vision language models (LVLMs). This enhancement is achieved by integrating LLMs with image encoders (Radford et al., 2021; Li et al., 2023a), which were pre-trained on large-scale image-text pairs to ensure alignment between the two domains. For instance, LLaVA (Liu et al., 2023b) integrates a vision encoder from CLIP (Radford et al., 2021) with the LLM Vicuna (Chiang et al., 2023b), which is further fine-tuned on carefully constructed vision-language instructional datasets to activate the model's perception capability of capturing the vision information according to different queries. This recent development has substantially expanded the requirement for large-scale instruction fine-tuning data for LVLMs (Gao et al., 2023b; Bai et al., 2023; Chen et al., 2023b; Gao et al., 2024; Anthropic, 2024; McKinzie et al., 2024).

While LVLMs have shown promising results, a key challenge lies in the acquisition of high-quality fine-tuning data. Obtaining human-curated content at scale is often prohibitively expensive, especially for multi-modal data. Many recent studies resort to GPT-4V (OpenAI, 2023b) for generating or labeling high-quality vision-language fine-tuning data. However, this approach does not significantly reduce the cost (Liu et al., 2023b; Wu et al., 2024). For instance, using GPT-4V to generate $6 k$ image descriptions with $1 k$ tokens per output would cost approximately $\$ 200$. There remains a pressing need for cost-effective methods to gather fine-tuning data to further enhance LVLMs.

To tackle the data acquisition bottleneck in multi-modality, we propose Self-Training on Image Comprehension (STIC). Our method is inspired by the recent success of self-training (Chen et al., 2024; Yuan et al., 2024; Fränken et al., 2024; Rosset et al., 2024) in LLMs, which leverages selfgenerated data to improve their downstream performance. However, different from the text-only domain, the unique vision modality of LVLMs introduces new challenges, as LVLMs must understand the input image content before reasoning and responding to any related textual queries about the image. Therefore, the proposed STIC approach is a novel two-stage self-training method that targets both image perception and reasoning over images and texts.

The overall framework is summarized in Figure 2. STIC specifically emphasizes the image comprehension self-training of LVLMs where the model generates its own preference dataset focused on image description. The self-generated dispreferred response is obtained by gathering model responses from either (1) prompts likely to elicit inaccurate responses or (2) corrupted images. The preferred responses are collected via a detailed prompt that guides the model through a step-by-step image description process. Figure 3 shows examples of such generated responses. During fine-tuning, we consider a DPO loss (Rafailov et al., 2023) with an additional regularized term explicitly emphasizing

![](https://cdn.mathpix.com/cropped/2024_06_04_18d42068839193d4cf89g-03.jpg?height=288&width=266&top_left_y=249&top_left_x=366)

Task: Explain what is depicted in the photograph.

Preferred data (Good prompting: Provide a detailed description of the image, focusing on ...) The image depicts a dynamic winter sports scene. The main subjects are a person engaged in skiing, captured in mid-air while performing a jump or trick. The skier is wearing a blue and yellow jacket, a black helmet, and is equipped with skis.....

(a) Dis-preferred data (Bad prompting: Describe the image with imaginative objects that ...) ...... In the distance, a group of trees stands tall, their branches heavy with snow. ...... Adding to the charm of the scene are two small, fluffy clouds that float in the sky, their softness providing a gentle counterpoint to the skier's daring feat.

![](https://cdn.mathpix.com/cropped/2024_06_04_18d42068839193d4cf89g-03.jpg?height=298&width=268&top_left_y=610&top_left_x=365)

(b) Dis-preferred data (Normal prompting + Image corruption: Color jitter.)

The skier, clad in a vibrant purple jacket and blue pants, ..... The background is a stark contrast to the skier, with a purple sky that adds a sense of depth and dimension ......

(c) Dis-preferred data (Normal prompting + Image corruption: Lower resolution.)

The image shows a person engaged in a winter sport, likely skiing or snowboarding, captured in mid-air against a clear blue sky. The individual is wearing a blue and yellow suit, ..... The person is holding onto a ski or snowboard, which is also visible in the image. The motion blur effect ......

Figure 3: Examples of the self-constructed preference data in STIC.

the preferred response. Lastly, we allow the model to self-improve its reasoning ability based on its own extracted image information by reusing a small amount of existing instruction fine-tuning data and appending its self-generated image descriptions to the prompts. We refer to this second stage as description-infused fine-tuning. Notably, our STIC approach does not require pre-labeled information of the images, which contrasts to the recent works that rely on such information for constructing vision-language preference data (Zhou et al., 2024).

To demonstrate the effectiveness of STIC, we conduct extensive experiments on seven vision-language benchmarks, including ScienceQA (Lu et al., 2022), TextVQA (Singh et al., 2019), ChartQA (Masry et al., 2022), LLaVA-Bench (Liu et al., 2023a), MMBench (Liu et al., 2023c), MM-Vet (Yu et al., 2023), and MathVista (Lu et al., 2024). These benchmarks encompass scientific reasoning, math reasoning, optical character recognition (OCR), and conversation capabilities based on vision inputs, spanning various image sources such as natural, chart, and text-rich images. We employ LLaVAv1.6 (Liu et al., 2024) as the primary base LVLM for our experiments and unitize $6 k$ images from MSCOCO (Lin et al., 2014) to construct the image description preference data. As depicted in Figure 1, STIC achieves consistent and significant performance improvements across these benchmarks, with an average accuracy gain of $\mathbf{4 . 0 \%}$ over the base LVLM and a notable gain of $\mathbf{6 . 4 \%}$ on ScienceQA. We also provide an example of the different responses from the original LVLM and STIC in Figure 1, where STIC successfully identifies the key visual information and accurately reason with it. These results demonstrate the remarkable effectiveness of our image comprehension self-training approach in enhancing the visual perception capabilities of LVLMs.

In addition, we explore the benefits of the various components of STIC. First, based on the descriptioninfused fine-tuning stage that enhances the model's reasoning ability with self-generated description, we show that further letting the model describe the image before responding to a query provides further improved reasoning capability. This results in a notable improvement of $2.8 \%$ on ScienceQA and $1.1 \%$ on average as compared to direct responses to queries (Table 2). Moreover, we examine the impact of self-generated dispreferred responses, from either bad prompting or image corruption. By excluding these dispreferred responses and conducting SFT solely with preferred responses, we observed a performance decrease of $2.5 \%$ on average across three benchmarks as compared to STIC with the preference data (Table 3). This highlights the importance of the negative samples in the self-constructed preference data by STIC. We also assess the scalability of our self-training scheme. By increasing the amount of generated preference data from $6 k$ to $12 k$, we show an even further improvement of STIC from $1.9 \%$ to $3.1 \%$ on LLaVA-Bench (Figure 7). This result suggests that STIC holds considerable potential for leveraging vast quantities of unlabeled images for self-training, given the immense availability of unlabeled image data. Lastly, our t-SNE visualization analysis shows that the closer the distribution between MSCOCO images, which we use for preference data construction, to images in downstream tasks, the more likely STIC results in higher performance gains (Figure 8).

The main contributions of this work are summarized as follows:

- We propose STIC, a novel two-stage self-training approach for LVLMs that focuses on enhancing their image comprehension capabilities by generating a preference dataset for image description without relying on pre-labeled image information.
- Through extensive experiments on seven diverse benchmarks, STIC demonstrates significant performance gains over the base LVLM, achieving an average accuracy gain of $4.0 \%$.
- We explore the benefits of various components of STIC, highlighting its potential to leverage vast quantities of unlabeled images for self-training.


## 2 Related Work

Vision language models (VLMs). VLMs (Tan and Bansal, 2019; Li et al., 2019, 2020; Kim et al., 2021; Wang et al., 2022b; Bao et al., 2022; Wang et al., 2022a; Alayrac et al., 2022; Li et al., 2023b; Chen et al., 2022; Jia et al., 2021a; Shen et al., 2022; Singh et al., 2021), processing both images and text, are pivotal in a wide range of multimodal understanding and reasoning tasks, capable of generating text or encoding multimodal representations. These models have shown increasing proficiency in visual perception and textual reasoning, and are also capable of following complex instructions (OpenAI, 2023b; Team et al., 2023). Recent advancements in the field have been propelled by the availability of open-source large language models (LLMs) (Touvron et al., 2023a,b; Jiang et al., 2023) and innovative image encoders (Radford et al., 2021; Li et al., 2022). For instance, LLaVA (Liu et al., 2023b) combines a vision encoder from CLIP (Radford et al., 2021) with the Vicuna LLM (Chiang et al., 2023b), and has been further fine-tuned on vision-language instructionfollowing datasets. The recent development of LVLMs has significantly expanded the scale and diversity of VL instruction-following data, including models such as LLaMA-Adapter-V2 (Gao et al., 2023b), Qwen-VL (Bai et al., 2023), InternVL (Chen et al., 2023b), InstructBLIP (Dai et al., 2024), SPHINX-X (Gao et al., 2024), Claude-3 (Anthropic, 2024), MM1 (McKinzie et al., 2024), and Grok-1.5V (xAI, 2024). In this work, we focus on enhancing the visual perception and mathmatical reasoning capabilities of LVLMs by efficiently aligning them with purely unsupervised data.

Alignment fine-tuning. Subsequent to supervised fine-tuning (SFT), alignment fine-tuning has emerged as a prominent approach to further enhance the performance of LLMs by aligning them with human preferences (Ouyang et al., 2022; Casper et al., 2023). Early efforts utilized on-policy reinforcement learning (RL) methods, such as proximal policy optimization (PPO) (Schulman et al., 2017), to train a reward model based on preference data (Bai et al., 2022; Touvron et al., 2023a). With the notable introduction of direct policy optimization (DPO) (Rafailov et al., 2023), a new line of research emphasizes direct learning from human preferences without relying on an explicit reward model (Zhao et al., 2023; Azar et al., 2024; Ethayarajh et al., 2024; Zheng et al., 2024). Another prominent direction is iterative preference fine-tuning, which has proven effective in enhancing model performance by repeatedly optimizing on newly generated preference pairs in each iteration (Adolphs et al., 2023; Xu et al., 2023; Xiong et al., 2023; Pang et al., 2024). While substantial research has focused on alignment fine-tuning for LLMs, efforts to adapt these techniques for LVLMs have been significantly limited. Initial attempts involve constructing preference datasets using human-labeled data (Sun et al., 2023) or GPT-4 generations for fine-tuning with a DPO loss (Zhou et al., 2024).

Self-improving language models. High-quality data, including human-crafted and advanced AI generated content, has been demonstrated to significantly enhance the performance of LLMs on various tasks (Josifoski et al., 2023; Taori et al., 2023; Chiang et al., 2023a; Li et al., 2023c). Although, acquiring such high-quality data is often prohibitively expensive. To circumvent the costs associated with obtaining human-annotated or expertly curated data, researchers have shifted their focus to leveraging data generated by the target model itself, exploring ways of self-improvement (Chen et al., 2024; Yuan et al., 2024; Fränken et al., 2024; Rosset et al., 2024). Recent studies have also emphasized the rephrasing capabilities of LLMs, which either enhance their own response quality (Deng et al., 2023; Prasad et al., 2023) or augment synthetic data for self-supervised fine-tuning (Kim et al., 2023). To the best of our knowledge, our work is the first to explore the potential for self-improvement in LVLMs, specifically focusing on the vision modality and emphasizing the self-improvement of image comprehension capabilities.

## 3 Problem Setting and Preliminaries

Notation. We use lower case letters and lower case bold face letters to denote scalars and vectors. We use the symbol $p$ to represent the probability of an LLM's response. And we denote the sequence of tokens generated from the LLM before the $t$-th token as $\mathbf{y}_{<t}=\left[y_{1}, \ldots, y_{t-1}\right]$ for $t>1$.

Generative vision language models. LVLM typically consists of three components: a vision encoder $f(\cdot)$, a projection network $g(\cdot)$, and an LLM $p_{\boldsymbol{\theta}}$ parameterized by $\boldsymbol{\theta}$. The model processes an image input $\mathbf{e}$ along with a text sequence $\mathbf{x}=\left[x_{1}, \ldots, x_{n}\right]$ as the prompt to generate a corresponding response $\mathbf{y}=\left[y_{1}, \ldots, y_{m}\right]$, where $x_{i}$ and $y_{j}$ represent individual tokens from the vocabulary of the LLM. The image is therefore converted into visual tokens within the language token space by the vision encoder and the projection network, producing $\mathbf{v}=\left[v_{1}, \ldots, v_{k}\right]=f \circ g(\mathbf{e})$. The response $\mathbf{y}$ is then considered as a sample from the conditional probability distribution $p_{\boldsymbol{\theta}}(\cdot \mid \mathbf{v}, \mathbf{x})$. As a Markov process, the conditional probability distribution $p_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{v}, \mathbf{x})$ can be decomposed as

$$
\begin{equation*}
p_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{v}, \mathbf{x})=\prod_{j=1}^{m} p_{\boldsymbol{\theta}}\left(y_{j} \mid \mathbf{v}, \mathbf{x}, \mathbf{y}_{<j}\right) \tag{3.1}
\end{equation*}
$$

Alignment fine-tuning. To improve LLM alignment with human preferences, RL fine-tuning (Bai et al., 2022; Gao et al., 2023a) is typically employed after supervised fine-tuning (SFT). This process involves a reward function $r(\mathbf{x}, \mathbf{y})$ for a given sequence pair $(\mathbf{x}, \mathbf{y})$. The more preferred response $\mathbf{y}$ is expected to result in a higher reward $r(\mathbf{x}, \mathbf{y})$, where the corresponding objective is to maximize the following:

$$
\begin{equation*}
L(\boldsymbol{\theta})=\mathbb{E}_{\mathbf{x} \sim \mathcal{D}, \mathbf{y} \sim p_{\boldsymbol{\theta}}(\cdot \mid \mathbf{x})}[r(\mathbf{x}, \mathbf{y})]-\lambda \mathbb{E}_{\mathbf{x} \sim \mathcal{D}} \mathrm{KL}\left(p_{\boldsymbol{\theta}}(\cdot \mid \mathbf{x}) \| p_{\mathrm{ref}}(\cdot \mid \mathbf{x})\right) \tag{3.2}
\end{equation*}
$$

where $\mathbf{x} \sim \mathcal{D}$ is sampled from a given distribution $\mathcal{D}$ and the $\mathrm{KL}$ regularization term prevents the new model $p_{\theta}$ from deviating too much from the reference model $p_{\text {ref }}$, with $\lambda>0$ as the regularization parameter. Training the reward function is challenging in practice, but direct preference optimization (DPO) (Rafailov et al., 2023) simplifies this process using a predefined preference dataset $S_{\text {pref }}=\left\{\left(\mathbf{x}^{(i)}, \mathbf{y}_{w}^{(i)}, \mathbf{y}_{l}^{(i)}\right)\right\}_{i \in[N]}$, where $\mathbf{y}_{w}^{(i)}$ denotes the preferred response and $\mathbf{y}_{l}^{(i)}$ denotes the dispreferred response given the same prompt $\mathbf{x}^{(i)}$. The objective function is then formulated as

$$
\begin{equation*}
L_{\mathrm{DPO}}\left(\boldsymbol{\theta}, \boldsymbol{\theta}_{\mathrm{ref}}\right)=\mathbb{E}_{\left(\mathbf{x}, \mathbf{y}_{w}, \mathbf{y}_{l}\right) \sim S_{\mathrm{pref}}}\left[\ell\left(\lambda \log \frac{p_{\boldsymbol{\theta}}\left(\mathbf{y}_{w} \mid \mathbf{x}\right)}{p_{\boldsymbol{\theta}_{\mathrm{ref}}}\left(\mathbf{y}_{w} \mid \mathbf{x}\right)}-\lambda \log \frac{p_{\boldsymbol{\theta}}\left(\mathbf{y}_{l} \mid \mathbf{x}\right)}{p_{\boldsymbol{\theta}_{\mathrm{ref}}}\left(\mathbf{y}_{l} \mid \mathbf{x}\right)}\right)\right] \tag{3.3}
\end{equation*}
$$

where $\ell(t)=\log (1+\exp (-t))$ is the logistic loss function and $\boldsymbol{\theta}_{\text {ref }}$ is the reference model.

Self-play fine-tuning. Notably, SPIN (Chen et al., 2024) shares a similar objective formulation while eliminating the need for a preference dataset by considering the model's own generation as a "dispreferred" response with an iterative self-play mechanism:

$$
\begin{equation*}
L_{\mathrm{SPIN}}\left(\boldsymbol{\theta}, \boldsymbol{\theta}_{\mathrm{ref}}\right)=\mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim S_{\mathrm{SFT}}, \mathbf{y}^{\prime} \sim p_{\boldsymbol{\theta}_{t}}(\cdot \mid \mathbf{x})}\left[\ell\left(\lambda \log \frac{p_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})}{p_{\boldsymbol{\theta}_{t}}(\mathbf{y} \mid \mathbf{x})}-\lambda \log \frac{p_{\boldsymbol{\theta}}\left(\mathbf{y}^{\prime} \mid \mathbf{x}\right)}{p_{\boldsymbol{\theta}_{t}}\left(\mathbf{y}^{\prime} \mid \mathbf{x}\right)}\right)\right] \tag{3.4}
\end{equation*}
$$

given a SFT dataset $S_{\mathrm{SFT}}=\{(\mathbf{x}, \mathbf{y})\}_{i=1}^{n}$. Here, $p_{\boldsymbol{\theta}_{t}}$ represents the LLM in SPIN's previous iteration.

## 4 Our Method: STIC

In this section, we introduce STIC, a two-stage self-training algorithm designed to enhance image comprehension capabilities. The first stage constructs its own preference dataset and the second stage infuses the used SFT data with self-generated image descriptions for fine-tuning. Figure 2 presents the general framework of STIC. Notably, unlike recent work on fine-tuning algorithms (Sun et al., 2023; Zhou et al., 2024), STIC enables a base LVLM, such as LLaVA-v1.6 (Liu et al., 2024), to evolve from self-generated image captions, thus eliminating the need for additional supervised and preference data from human annotators or advanced teacher models. This approach fundamentally enhances image comprehension abilities and can be seamlessly applied to a wide range of vision-language reasoning tasks. We summarize STIC in Algorithms 1 and 2, and detail the process below.

Stage 1: Image comprehension self-training. The process begins with a self-constructed preference dataset from the base LVLM, which we aim to improve through fine-tuning. The dataset contains paired preference data for image descriptions:

- Preferred response: Model-generated image descriptions derived from well-crafted prompts with explicit reasoning steps.
- Dispreferred response: Model-generated descriptions resulting from either (1) corrupted image with low resolution or distorted color, or (2) "bad" prompts that cause the base model to hallucinate and describe elements that may not logically exist in the image.

```
Algorithm 1 STIC (Stage 1: image comprehension self-training)
    Input: Unlabeled image dataset: $\left\{\mathbf{v}^{(i)}\right\}_{i \in[N]}$. Image captioning prompt set: $P=\left\{\mathbf{x}^{(i)}\right\}_{i \in\left[M_{1}\right]}$.
    Hallucination prompt set: $P_{\text {hallu }}=\left\{\mathbf{x}_{\text {hallu }}^{(i)}\right\}_{i \in\left[M_{2}\right]}$. Image corruption $h(\cdot)$. Well-curated caption-
    ing prompt: $\mathbf{x}_{g}$. LVLM parameterized by $\boldsymbol{\theta}_{0}: p_{\boldsymbol{\theta}_{0}}$.
    Let self-training dataset $D=\{\}$.
    for $i=1, \ldots N$ do
        Randomly sample a number $n \in(0,1)$.
        Randomly sample $\mathbf{x} \sim\left\{\mathbf{x}^{(i)}\right\}_{i \in[M]}$.
        Generate preferred response $\mathbf{y}_{g} \sim p_{\boldsymbol{\theta}_{t}}\left(\cdot \mid \mathbf{v}^{(i)}, \mathbf{x}_{g}\right)$.
        if $n<0.5$ then
            Randomly sample bad prompt $\mathbf{x}_{b} \sim P_{\text {hallu }}$.
            Generate dispreferred response $\mathbf{y}_{b} \sim p_{\boldsymbol{\theta}_{t}}\left(\cdot \mid \mathbf{v}^{(i)}, \mathbf{x}_{b}\right)$.
        else
            Corrupt the image input $\mathbf{v}_{b}^{(i)}=h\left(\mathbf{v}^{(i)}\right)$.
            Generate dispreferred response $\mathbf{y}_{b} \sim p_{\boldsymbol{\theta}_{t}}\left(\cdot \mid \mathbf{v}_{b}^{(i)}, \mathbf{x}^{(i)}\right)$.
        end if
        Add $\left(\mathbf{x}, \mathbf{y}_{g}, \mathbf{y}_{b}\right)$ to $D$.
    end for
    Update $\boldsymbol{\theta}_{1}=\operatorname{argmin}_{\boldsymbol{\theta} \in \boldsymbol{\Theta}} \sum_{\left(\mathbf{x}, \mathbf{y}_{g}, \mathbf{y}_{b}\right) \in D}\left[\ell\left(\lambda \log \frac{p_{\boldsymbol{\theta}}\left(\mathbf{y}_{g} \mid \mathbf{x}\right)}{p_{\boldsymbol{\theta}_{0}}\left(\mathbf{y}_{g} \mid \mathbf{x}\right)}-\lambda \log \frac{p_{\boldsymbol{\theta}}\left(\mathbf{y}_{b} \mid \mathbf{x}\right)}{p_{\theta_{0}}\left(\mathbf{y}_{b} \mid \mathbf{x}\right)}\right)-\alpha \log p_{\boldsymbol{\theta}}\left(\mathbf{y}_{g} \mid \mathbf{x}\right)\right]$.
```

Output: $\theta_{1}$.

The self-constructed preference dataset is used for the first-stage self-training using DPO (Rafailov et al., 2023) with an additional regularization term to further emphasize the preferred response, controlled by the hyperparameter $\alpha$. The regularized loss function is as follows:

$$
\begin{equation*}
L\left(\boldsymbol{\theta}, \boldsymbol{\theta}_{\mathrm{ref}}\right)=\mathbb{E}_{\left(\mathbf{x}, \mathbf{y}_{w}, \mathbf{y}_{l}\right) \sim S}\left[\ell\left(\lambda \log \frac{p_{\boldsymbol{\theta}}\left(\mathbf{y}_{w} \mid \mathbf{x}\right)}{p_{\boldsymbol{\theta}_{\mathrm{ref}}}\left(\mathbf{y}_{w} \mid \mathbf{x}\right)}-\lambda \log \frac{p_{\boldsymbol{\theta}}\left(\mathbf{y}_{l} \mid \mathbf{x}\right)}{p_{\boldsymbol{\theta}_{\mathrm{ref}}}\left(\mathbf{y}_{l} \mid \mathbf{x}\right)}\right)-\alpha \log p_{\boldsymbol{\theta}}\left(\mathbf{y}_{w} \mid \mathbf{x}\right)\right] \tag{4.1}
\end{equation*}
$$

The use of an explicit loss term for positive examples can be similarly found in previous studies on contrastive learning (Chen et al., 2021; Chen and He, 2021; Chen et al., 2023a) and more recently in preference fine-tuning (Pang et al., 2024). Specifically, Chen et al. (2023a) demonstrated in the context of contrastive learning that a regularization term applied to positive samples provably enhances the model's ability to differentiate between positive and negative samples. As demonstrated in our experiments in Section 6, the LVLM after Stage 1 has shown notable improvement in downstream vision-language reasoning tasks, confirming that the enhanced visual comprehension ability directly benefits the model performance and its multimodal reasoning ability.

Stage 2: Description-infused fine-tuning. In the second stage, we further fine-tune the self-trained LVLM to leverage self-generated high-quality image descriptions for instruction-following tasks, and thus help ground its reasoning ability on self-generated descriptions. To achieve this, we randomly select a small subset of data from the model's instruction fine-tuning dataset already used during SFT. We then infuse the instructions in this subset with model-generated image descriptions as follows:

```
Image description: {model description}
<original instruction>
```

The original ground-truth completions remain unchanged. We then fine-tune the LVLM for one epoch on this small description-infused subset. This fine-tuning step ensures that the model effectively integrates visual information into its responses, thereby enhancing its ability to handle a variety of vision-language reasoning tasks. During inference, optionally, we can let the model self-augment its prompt for downstream vision-language reasoning tasks by describing the image before answering the question.

## 5 Experiments

In this section, we present the experiment results of STIC across seven visual question answering (VQA) benchmarks. We demonstrate that STIC effectively and substantially improves LVLM's performance across different VQA tasks using a self-constructed preference dataset without external labels.

```
Algorithm 2 STIC (Stage 2: description-infused fine-tuning)
    Input: Instruction-following dataset already used for fine-tuning the target LVLM model:
    $\left\{\mathbf{v}^{(i)}, \mathbf{x}^{(i)}, \mathbf{y}^{(i)}\right\}_{i \in[m]}$. Image description prompt set: $P=\left\{\mathbf{x}_{\text {des }}^{(i)}\right\}_{i \in\left[M_{1}\right]}$. LVLM parameterized by
    $\boldsymbol{\theta}_{1}$ after self-training: $p_{\boldsymbol{\theta}_{1}}$.
    Let description-infused dataset $D_{\text {des }}=\{\}$.
    for $i=1, \ldots m$ do
        Randomly sample $\mathbf{x}_{\text {des }} \sim\left\{\mathbf{x}_{\text {des }}^{(i)}\right\}_{i \in[M]}$.
        Generate model image description $\mathbf{y}_{\text {des }} \sim p_{\boldsymbol{\theta}_{t}}\left(\cdot \mid \mathbf{v}^{(i)}, \mathbf{x}_{\text {des }}\right)$.
        Add $\left(\left[\mathbf{x}_{\mathrm{des}}, \mathbf{x}^{(i)}\right], \mathbf{y}^{(i)}\right)$ to $D_{\text {des }}$.
    end for
    Update $\widehat{\boldsymbol{\theta}}=\operatorname{argmin}_{\boldsymbol{\theta} \in \boldsymbol{\Theta}} \sum_{(\mathbf{x}, \mathbf{y}) \in D_{\mathrm{des}}} \ell\left(\log p_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})\right)$.
    Output: $\widehat{\theta}$.
```


### 5.1 Experiment Setup

Model and datasets. In experiments, we consider llava-v1.6-mistral-7b (Liu et al., 2023a) as our base model for self-training with model generated preference data. We additionally consider llava-v1.5-7b (Liu et al., 2023a) based on Vicuna-7B (Chiang et al., 2023b) to directly compare with one concurrent baseline POVID (Zhou et al., 2024). We follow the optimization process described in Section 4 for self-training on image description in Algorithm 1 and description-infused fine-tuning in Algorithm 2 to achieve improved downstream performances. For the self-constructed preference dataset, we gather $6 k$ unlabeled image data randomly sampled from the MSCOCO dataset (Lin et al., 2014) and specifically the train2014 split for its high-quality images popularly used for pre-training and fine-tuning. In the second stage, we randomly subsample $5 \mathrm{k}$ used instruction fine-tuning data from LLaVA's SFT data to construct the description-infused fine-tuning data with model-generated image descriptions. Lastly, we use low-rank adaptation (LoRA) fine-tuning (Hu et al., 2021) instead of full fine-tuning for efficient computation. We defer the detailed prompts and corruptions to Appendix A.

Evaluation. We consider the widely used benchmarks for LVLM evaluation across different domains including: ScienceQA (Lu et al., 2022), TextVQA (Singh et al., 2019), ChartQA (Masry et al., 2022), LLaVA-Bench (Liu et al., 2023a), MMBench (Liu et al., 2023c), MM-Vet (Yu et al., 2023), and MathVista (Lu et al., 2024). Specifically, ScienceQA focuses on scientific question answering and MathVista focuses on math reasoning with visual information. TextVQA consists of images with text-rich contents and ChartQA with visual charts. Lastly, LLaVA-Bench, MMBench, and MM-Vet are three recent benchmarks to comprehensively evaluate a model's capabilities in a wide range of tasks and evaluation criteria. We use the evaluation scripts provided by LLaVA (Liu et al., 2023a) to obtain the results for both our base model and after using STIC to ensure a fair comparison.

### 5.2 Main Results

Table 1: Performance of STIC compared with the original LVLM model across vision-language reasoning tasks. For LLaVA-v1.5 (Vicuna 7B), we directly report the values in the paper of POVID, and "-" indicates an unreported value.

| Model | ScienceQA | TextVQA | ChartQA | LLaVA-Bench | MMBench | MM-Vet | MathVista |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| InstructBLIP (7B) | 60.5 | 50.1 | - | 60.9 | 36.0 | 26.2 | 25.3 |
| mPLUG-OWL2 (7B) | 64.5 | 54.3 | - | 59.9 | 64.5 | 36.2 | 22.2 |
| LLaVA-v1.5 (7B) | 66.8 | 58.2 | 6.32 | 65.4 | 64.3 | 31.1 | 25.1 |
| w/ POVID | 68.8 | - | - | 68.7 | 64.9 | 31.8 | - |
| w/ STIC | $\mathbf{6 9 . 5}$ | $\mathbf{6 1 . 4}$ | $\mathbf{6 . 6 4}$ | $\mathbf{6 8 . 9}$ | $\mathbf{6 5 . 3}$ | $\mathbf{3 2 . 6}$ | $\mathbf{2 7 . 2}$ |
| LLaVA-v1.6 (7B) | 68.9 | 60.3 | 36.4 | 77.3 | 63.7 | 42.2 | 34.6 |
| w/ STIC | $\mathbf{7 5 . 3}$ | $\mathbf{6 5 . 2}$ | $\mathbf{4 1 . 5}$ | $\mathbf{7 9 . 2}$ | $\mathbf{6 7 . 8}$ | $\mathbf{4 5 . 0}$ | $\mathbf{3 7 . 0}$ |

We present our main results in Table 1 and detail the benchmark performances of STIC (LLaVA-v1.6 7B) on MMBench and MM-Vet in Figure 4. In Appendix A, we present detailed results for MMBench in Table 5 and MM-Vet in Table 6. Our results show a consistent and significant improvement of STIC over the original models (LLaVA-v1.5 and LLaVA-v1.6) across all seven datasets. This improvement
is achieved using only self-constructed preference data and a small portion of the model's SFT dataset, which had already been used for fine-tuning the original model.

On average, STIC improves LLaVA-v1.5 by $1.7 \%$, increasing from $45.3 \%$ to $47.0 \%$. and LLaVA-v1. 6 by a notable score of $4.0 \%$, increasing from $54.7 \%$ to $58.7 \%$. The improvement is comprehensive, as detailed in Tables 5 and 6, where STIC consistently enhances performance across all evaluation tasks and targets. Moreover, while STIC improves both LLaVA-v1.5 and LLaVA-v1.6, a more significant improvement is observed in the more advanced model, LLaVA-v1.6. This trend suggests that the extent of self-improvement could be correlated with the model's inherent capabilities.
![](https://cdn.mathpix.com/cropped/2024_06_04_18d42068839193d4cf89g-08.jpg?height=424&width=1378&top_left_y=568&top_left_x=363)

Figure 4: Accuracy improvement of STIC compared to the base LLaVA-v1.6 model across different tasks in Left: MMBench, where the original performances are re-scaled to 60 in plotting and STIC accordingly with the same coefficient for each task. Middle: MM-Vet, where the performances of the original model are re-scaled to 40 and STIC accordingly. Right: LLaVA-Bench, where we report the error bars over three independent runs due to the randomness of GPT-4 evaluation.

## 6 Ablation Studies and Discussions

In this section, we outline the differences of our method with current alignment fine-tuning methods focusing on the vision-language setting. Furthermore, we conduct ablation studies on the key components of STIC to demonstrate their importance and effectiveness. Additionally, we examine the image distribution of our self-training data (MSCOCO) alongside the image distributions of benchmark datasets, revealing a positive correlation between performance gains and similarity in image distributions.

Discussion with POVID. We detail the differences between STIC and POVID. In POVID, the dispreferred response is generated either by adding Gaussian noise to the original image or by manually injecting hallucinations into the ground truth completion, using the labeled object information of the images. In contrast, STIC (1) specifically targets the image description task, (2) constructs preference datasets exclusively from unlabeled images using selfgenerated content for both preferred and dispreferred responses, (3) employs an automatic model generation process without manual injections or modifications, and (4) utilizes only a small portion of SFT data for instruction-following fine-tuning with uniquely infused model descriptions. Lastly, we compare the data types and scales used in POVID and STIC in Figure 5.

![](https://cdn.mathpix.com/cropped/2024_06_04_18d42068839193d4cf89g-08.jpg?height=431&width=409&top_left_y=1582&top_left_x=1335)

Figure 5: Data comparison.

Effectiveness of describe-and-respond (DaR) prompting. We assess the significance of the fine-tuning process in STIC by comparing it to the approach of directly allowing the base LVLM to describe an image and then respond to a query with a self-augmented prompt, which we refer to as the describe-and-respond (DaR) prompting method. As indicated in Table 2, applying DaR to the base LVLM yields mixed results, with performance improvements on some datasets and degradation on others, resulting in an overall average drop of $2.2 \%$. In contrast, when DaR is combined with the fine-tuning process of STIC, it leads to a further average enhancement of $1.1 \%$ and a notable $2.8 \%$ on ScienceQA. This demonstrates the synergistic effect of DaR and the fine-tuning process in STIC. Additionally, it is worth noting that STIC achieves a substantial average improvement of $2.9 \%$ even without the DaR prompting method, compared to the base LVLM.

Table 2: Test performance of STIC based on llava-v1.6-mistral-7b. We investigate the benefit of DaR as a prompting method toward the base LVLM model as compared to the benefit on STIC.

| Method | DaR | ScienceQA | TextVQA | ChartQA | LLaVA-Bench | MMBench | MM-Vet | MathVista | Average |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Original | $X$ | 68.9 | 60.3 | 36.4 | 77.3 | 63.7 | 42.2 | 34.6 | 54.8 |
|  | $\checkmark$ | 69.9 | 56.6 | 34.6 | 78.5 | 50.7 | 42.3 | 34.7 | 52.5 |
| w/ STIC | $X$ | 72.5 | 63.4 | 39.3 | 78.4 | $\mathbf{6 8 . 7}$ | $\mathbf{4 5 . 7}$ | 35.2 | 57.6 |
|  | $\checkmark$ | $\mathbf{7 5 . 3}$ | $\mathbf{6 5 . 2}$ | $\mathbf{4 1 . 5}$ | $\mathbf{7 9 . 2}$ | 67.8 | 45.2 | $\mathbf{3 7 . 0}$ | $\mathbf{5 8 . 7}$ |

Table 3: Test performance of STIC if we remove negative examples and use positive ones to perform SFT in Stage 1.

| Model | ScienceQA | TextVQA | LLaVA-Bench |
| :---: | :---: | :---: | :---: |
| Original | 68.9 | 60.3 | 77.3 |
| w/ STIC (positive) | 71.8 | 63.7 | 76.7 |
| w/ STIC | $\mathbf{7 5 . 3}$ | $\mathbf{6 5 . 2}$ | $\mathbf{7 9 . 2}$ |

The role of dispreferred samples in STIC. To understand the importance of dispreferred samples in STIC, we conduct an ablation study using llava-v1.6-mistral-7b as the base LVLM. We remove the negative examples from the preference data and only use the positive samples for supervised fine-tuning (SFT), effectively creating an SFT version of STIC. Table 3 shows that omitting the dispreferred samples evev leads to a performance drop of $0.6 \%$ on LLaVA-Bench, while failing to provide equally significant improvement as STIC with preference data. This highlights the crucial role of negative examples in aligning preferences and enabling the model to distinguish between high-quality and low-quality responses. By leveraging both positive and negative examples, STIC effectively improves the model's performance and generates more preferred outputs.

Progression of stages. In Figure 6, we illustrate the sequential improvement in performance of STIC on ScienceQA. While stage 1 focuses exclusively on enhancing the perception capabilities of the LVLM, it still notably improves performance on downstream VQA tasks. Building on the improved image comprehension achieved in stage 1 , stage 2 introduces an enhanced reasoning process that utilizes the model's self-generated image descriptions and results in an even more significant gain. This enhancement further enables the model to self-augment its prompts with Describe and Respond (DaR), resulting in total the substantial performance gains of $6.4 \%$ observed.

Scaling law of STIC. We explore the scaling law of STIC by expanding the preference data in Stage 1. Using the LLaVA-Bench benchmark as a case study, we scale up the preference data from $6 k$ to $12 k$ MSCOCO images. As depicted in Figure 7, there is an obvious gain on the LLaVA-Bench from $1.9 \%$ to $3.1 \%$. This finding demonstrates that STIC can effectively leverage larger amounts of unlabeled image data and presents a cost-effective solution to the challenge of acquiring high-quality vision-language data.

![](https://cdn.mathpix.com/cropped/2024_06_04_18d42068839193d4cf89g-09.jpg?height=288&width=415&top_left_y=1233&top_left_x=1321)

Figure 6: Progression of stages in STIC.

![](https://cdn.mathpix.com/cropped/2024_06_04_18d42068839193d4cf89g-09.jpg?height=266&width=420&top_left_y=1680&top_left_x=1319)

Figure 7: Scaling law in STIC.

Correlation between image distribution and performance gains. To gain further insight into the effectiveness of STIC across different benchmarks, we conducted a t-SNE visualization analysis comparing the image distributions of MSCOCO, which we used for preference data construction, with those of four benchmarks: ScienceQA, TextVQA, MathVista, and ChartQA (Figure 8). Our analysis revealed a general trend: the greater the overlap between the MSCOCO image distribution and that of a benchmark, the higher the performance gain achieved by STIC on that benchmark. This observation held true for ScienceQA and TextVQA, which exhibited substantial distributional overlap with MSCOCO and yielded the highest performance gains of $6.4 \%$ and $4.9 \%$, respectively. Conversely, MathVista, with its diverse image types and limited overlap with MSCOCO, saw a more modest gain of $2.4 \%$. Interestingly, ChartQA was an outlier, achieving a high gain of $5.1 \%$ despite minimal overlap with MSCOCO, suggesting that the improved image comprehension from STIC played a fundamental role in understanding and reasoning about the charts. Detailed per-benchmark visualizations and discussions are provided in Appendix B.2.

![](https://cdn.mathpix.com/cropped/2024_06_04_18d42068839193d4cf89g-10.jpg?height=650&width=884&top_left_y=236&top_left_x=618)

Figure 8: t-SNE visualization of images from MSCOCO and four benchmarks, each sampling $1 k$ images.

## 7 Conclusion

We introduce Self-Training on Image Comprehension (STIC), a novel self-training approach designed to enhance the image comprehension capabilities of large vision language models (LVLMs). Our method leverages a two-stage self-training process, creating a preference dataset for image descriptions from unlabeled images and refining reasoning abilities through description-infused fine-tuning. Extensive experiments across seven vision-language benchmarks demonstrated significant performance improvements, with an average accuracy gain of $4.0 \%$, while reducing the need for supervised fine-tuning data by $70 \%$. Our findings underscore the potential of STIC to harness vast quantities of unlabeled images, offering a cost-effective solution for advancing LVLMs.

The promising results demonstrated by STIC in enhancing the capabilities of 7B LVLMs suggest its potential applicability to larger models, such as those with 13B, 40B, and 100B parameters, if computational resources permit. Additionally, it would be beneficial to investigate the impact of the image distribution used in self-training on STIC, aiming to further refine its effectiveness with curated image datasets. Lastly, an examination of the effects of various image corruptions and "bad" prompts on STIC could provide valuable insights into the effective generation of dispreferred samples.

## References

Adolphs, L., Gao, T., Xu, J., Shuster, K., Sukhbaatar, S. and Weston, J. (2023). The cringe loss: Learning what language not to model. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).

Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., BarR, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M. et al. (2022). Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems.

ANTHROPIC (2024). The claude 3 model family: Opus, sonnet, haiku.

azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M. and CalanDRIELLO, D. (2024). A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics. PMLR.

Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. and Zhou, J. (2023). Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 .

Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T. et al. (2022). Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 .

Bao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O. K., Aggarwal, K., Som, S., Piao, S. and WeI, F. (2022). Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. In Advances in Neural Information Processing Systems.

Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., KorbaK, T., Lindner, D., Freire, P. et al. (2023). Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217 .

Chen, S., Niu, G., Gong, C., Li, J., Yang, J. and Sugiyama, M. (2021). Large-margin contrastive learning with distance polarization regularizer. In International Conference on Machine Learning. PMLR.

CHEN, X. and He, K. (2021). Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.

Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L. et al. (2022). Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794 .

CHEN, Z., Deng, Y., Li, Y. and Gu, Q. (2023a). Understanding transferable representation learning and zero-shot transfer in clip. arXiv preprint arXiv:2310.00927 .

Chen, Z., Deng, Y., YuAn, H., JI, K. and Gu, Q. (2024). Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335 .

Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Muyan, Z., Zhang, Q., Zhu, X., LU, L. ET AL. (2023b). Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238 .

Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., GonZaleZ, J. E., Stoica, I. and Xing, E. P. (2023a). Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.

Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., ZHUANG, Y., GonZALEZ, J. E. ET AL. (2023b). Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023) 26 .

Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B., Fung, P. N. and Hoi, S. (2024). Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems 36.

Deng, Y., Zhang, W., ChEn, Z. and Gu, Q. (2023). Rephrase and respond: Let large language models ask better questions for themselves. arXiv preprint arXiv:2311.04205 .

Ethayarajh, K., Xu, W., Muennighoff, N., JurafSKY, D. and Kiela, D. (2024). Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306 .

Fränken, J.-P., Zelikman, E., Rafailov, R., Gandhi, K., Gerstenberg, T. and Goodman, N. D. (2024). Self-supervised alignment with mutual information: Learning to follow principles without preference labels. arXiv preprint arXiv:2404.14313 .

Gao, L., Schulman, J. and Hilton, J. (2023a). Scaling laws for reward model overoptimization. In International Conference on Machine Learning. PMLR.

Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., Li, H. and QIAO, Y. (2023b). Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010 .

Gao, P., Zhang, R., Liu, C., Qiu, L., Huang, S., Lin, W., Zhao, S., Geng, S., Lin, Z., Jin, P., Zhang, K., Shao, W., Xu, C., He, C., He, J., Shao, H., Lu, P., Li, H. and Qiao, Y. (2024). Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. In International Conference on Machine Learning (ICML).

Goel, S., Bansal, H., Bhatia, S., Rossi, R., Vinay, V. and Grover, A. (2022). Cyclip: Cyclic contrastive language-image pretraining. Advances in Neural Information Processing Systems 35 $6704-6719$.

Hu, E. J., WAllis, P., Allen-Zhu, Z., Li, Y., Wang, S., WAng, L., Chen, W. et al. (2021). Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.

Jia, C., Yang, Y., Xia, Y., Chen, Y., Parekh, Z., Pham, H., Le, Q. V., Sung, Y., Li, Z. and DUERIG, T. (2021a). Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (M. Meila and T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research. PMLR.

Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z. and DuERIG, T. (2021b). Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning. PMLR.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. D. L., BreSSAND, F., LenGYel, G., Lample, G., SAULNiER, L. ET AL. (2023). Mistral 7b. arXiv preprint arXiv:2310.06825 .

Josifoski, M., Sakota, M., Peyrard, M. and West, R. (2023). Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction. arXiv preprint arXiv:2303.04132 .

Kim, D., Park, C., Kim, S., Lee, W., Song, W., Kim, Y., Kim, H., Kim, Y., Lee, H., Kim, J. ET AL. (2023). Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166.

KIM, W., Son, B. and KIM, I. (2021). ViLT: Vision-and-language transformer without convolution or region supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (M. Meila and T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research. PMLR.

Li, J., Li, D., SaVarese, S. and Hoi, S. (2023a). Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning. PMLR.

Li, J., Li, D., Savarese, S. and Hoi, S. (2023b). Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597

Li, L. H., YatsKar, M., Yin, D., Hsieh, C. and Chang, K. (2019). Visualbert: A simple and performant baseline for vision and language. CoRR abs/1908.03557.

Li, L. H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., YuAn, L., Zhang, L., Hwang, J.-N. ET AL. (2022). Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.

Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., Choi, Y. and Gao, J. (2020). Oscar: Object-semantics aligned pre-training for vision-language tasks. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXX (A. Vedaldi, H. Bischof, T. Brox and J. Frahm, eds.), vol. 12375 of Lecture Notes in Computer Science. Springer.

li, Y., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar, S. and Lee, Y. T. (2023c). Textbooks are all you need ii: phi-1.5 technical report.

Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. and ZitniCK, C. L. (2014). Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer.

LiU, H., LI, C., LI, Y. and LEE, Y. J. (2023a). Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 .

LiU, H., Li, C., Li, Y., Li, B., ZHAnG, Y., Shen, S. and LeE, Y. J. (2024). Llava-next: Improved reasoning, ocr, and world knowledge.

LiU, H., Li, C., WU, Q. and LeE, Y. J. (2023b). Visual instruction tuning. Advances in Neural Information Processing Systems (NeurIPS) 36.

LiU, Y., Duan, H., ZHang, Y., Li, B., Zhang, S., ZhaO, W., Yuan, Y., Wang, J., He, C., LiU, Z. ET AL. (2023c). Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 .

lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M. and GaO, J. (2024). Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR).

Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P. and KALYAN, A. (2022). Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems 35 2507-2521.

MasrY, A., Do, X. L., Tan, J. Q., JotY, S. and Hoque, E. (2022). Chartqa: A benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022.

McKinZie, B., Gan, Z., FauconniER, J.-P., Dodge, S., ZHAnG, B., Dufter, P., ShAH, D., Du, X., Peng, F., Weers, F. et al. (2024). Mm1: Methods, analysis \& insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611 .

OPENAI (2023a). Gpt-4 technical report.

OPENAI (2023b). Gpt-4v(ision) system card.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A. et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 2773027744.

Pang, R. Y., Yuan, W., Cho, K., He, H., Sukhbaatar, S. and Weston, J. (2024). Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733 .

Prasad, A., Stengel-Eskin, E. and Bansal, M. (2023). Rephrase, augment, reason: Visual grounding of questions for vision-language models. arXiv preprint arXiv:2310.05861 .

Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., AsKell, A., MishKin, P., ClarK, J. ET AL. (2021). Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML). PMLR.

Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D. and Finn, C. (2023). Direct preference optimization: Your language model is secretly a reward model. arXiv preprint

![](https://cdn.mathpix.com/cropped/2024_06_04_18d42068839193d4cf89g-13.jpg?height=40&width=271&top_left_y=2013&top_left_x=407)

Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A. and Xie, T. (2024). Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715 .

Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 .

Shen, S., Li, L. H., Tan, H., Bansal, M., Rohrbach, A., Chang, K.-W., Yao, Z. and KeUTZER, K. (2022). How much can clip benefit vision-and-language tasks? In ICLR.

Singh, A., Hu, R., Goswami, V., Couairon, G., Galuba, W., Rohrbach, M. and Kiela, D. (2021). FLAVA: A foundational language and vision alignment model. CoRR abs/2112.04482.

Singh, A., Natarjan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D. and RoHrbach, M. (2019). Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.

Sun, Z., Shen, S., Cao, S., Liu, H., Li, C., Shen, Y., Gan, C., Gui, L.-Y., Wang, Y.-X., YANG, Y. ET AL. (2023). Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525 .

TAN, H. and BANSAL, M. (2019). LXMERT: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 (K. Inui, J. Jiang, V. Ng and X. Wan, eds.). Association for Computational Linguistics.

Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P. and Hashimoto, T. B. (2023). Stanford alpaca: An instruction-following llama model.

Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., SchalkWYK, J., DAI, A. M., HaUTH, A. ET AL. (2023). Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 .

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., BhargaVa, P., Bhosale, S. et AL. (2023a). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., BhargaVa, P., Bhosale, S. et AL. (2023b). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .

Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., MA, J., Zhou, C., Zhou, J. and Yang, H. (2022a). Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. CoRR abs/2202.03052.

Wang, Z., Yu, J., Yu, A. W., Dai, Z., TsvetKov, Y. and Cao, Y. (2022b). SimVLM: Simple visual language model pretraining with weak supervision. In ICLR.

Wu, T., Yang, G., Li, Z., Zhang, K., LiU, Z., Guibas, L., Lin, D. and WetZStein, G. (2024). Gpt-4v (ision) is a human-aligned evaluator for text-to-3d generation. arXiv preprint arXiv:2401.04092 .

XAI (2024). Grok-1.5 vision preview.

Xiong, W., Dong, H., Ye, C., Zhong, H., Jiang, N. and Zhang, T. (2023). Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456 .

Xu, J., Lee, A., Sukhbaftar, S. and Weston, J. (2023). Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682 .

Yu, W., YanG, Z., Li, L., WanG, J., Lin, K., LiU, Z., WanG, X. and WAnG, L. (2023). Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 .

Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J. and Weston, J. (2024). Selfrewarding language models. arXiv preprint arXiv:2401.10020 .

Zhao, Y., Joshi, R., Liu, T., Khalman, M., SaleH, M. and Liu, P. J. (2023). Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425 .

ZHEng, C., WAng, Z., Ji, H., HuAng, M. and Peng, N. (2024). Weak-to-strong extrapolation expedites alignment. arXiv preprint arXiv:2404.16792 .

Zhou, Y., Cui, C., Rafailov, R., Finn, C. and YaO, H. (2024). Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411 .
