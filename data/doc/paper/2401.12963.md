# AUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS 

Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana<br>Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Sean<br>Kirmani, Isabel Leal, Edward Lee, Sergey Levine, Yao Lu, Isabel Leal, Sharath Maddineni,<br>Kanishka Rao, Dorsa Sadigh, Pannag Sanketi, Pierre Sermanet, Quan Vuong, Stefan Welker,<br>Fei Xia, Ted Xiao, Peng Xu, Steve Xu, Zhuo Xu

Google Deepmind


#### Abstract

Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting $77 \mathrm{k}$ real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such "in-the-wild" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.


## 1 INTRODUCTION

One of the central goals of autonomous robotics research is to enable independent and broadly capable robotic agents: systems that can be tasked with some high-level goals ("keep the kitchen clean"), formulate plans for addressing these goals, and then carry out those plans with the skills and resources available to them. While current robotic learning methods offer appealing solutions for acquiring individual robotic skills, and large language models (LLMs), vision-language models (VLMs) and large multimodal models offer the ability to reason over such abstract tasks (Ahn et al., 2022; Rana et al., 2023), truly open-ended tasks still present major challenges. Performing innumerable number of tasks in diverse settings requires a grounded and generalist agent that can robustly adapt to scenarios outside where robots are trained. The bottleneck for achieving these goals, however, is the need for large amounts of robotic experience in the real world - much larger than robot datasets collected in lab settings with well-defined environments.

In this paper, we study how we can design agents to gather robotic experience for themselves at scale. Central to our work is leveraging knowledge contained in foundation models to drive realworld robots. We specifically focus on diverse robotic data acquisition: when a robot is placed in a new environment, potentially with a user command to collect data around some theme (e.g. office[^0]tasks), the robot should determine which tasks can be performed, which of its own skills to trigger to attempt them, and when it should rely on human teleoperators. We view this from the perspective of controlling a fleet of robots, spread across multiple locations, where there are many more robots than human supervisors, necessitating mixing expert demonstrations with suboptimal autonomous policies in a safe and appropriate way. Our system for large-scale orchestration of robotic agents, which we call AutoRT, tackles this problem.

At the core of AutoRT is an large foundation model that acts as a robot orchestrator, prescribing appropriate tasks to one or more robots in an environment based on the user's prompt and environmental affordances ("task proposals") discovered from visual observations. The scene description step perceive objects in the environment, the task proposal step suggests possible things the robot could do with them, and then the affordance filtering step decides which tasks to attempt and how based on these observations and prompt. This process takes into account constraints specified via "constitutional prompting", where rules about robot behaviour can be defined by the user. It additionally accounts for the availability of human teleoperators, and handles working around the capabilities of the robot (e.g., the robot can pick up a cup but not a table, it can approach the sink but can't sit in a chair, etc.).

We describe the AutoRT system, instantiate it with a fleet of real-world mobile manipulators, and present the results of an extensive real-world evaluation over 7 months, 4 different office buildings, and a fleet of over 20 robots, which resulted in the collection of 77,000 real-world robotic trials with both teleoperation and autonomous execution. AutoRT is, to the best of our knowledge, the first system where LLM-controlled robots are allowed to drive autonomously in real world settings, propose their own goals, and take actions toward those goals. We show that AutoRT scales robot deployment by allowing 1 human to supervise 3-5 mobile manipulators. Our evaluation studies how AutoRT can collect highly diverse data, be instructed to collect task appropriate data and shows such data can be used to improve state-of-the-art robot learning models. AutoRT also introduces aligning robot behavior to human preferences using prompting and critiquing with a robot constitution.

## 2 RELATED WORK

Real robot data collection. Large scale real robot data collection for robotic manipulation falls into mainly two categories: autonomous data collection and human assisted demonstrations. Autonomous data collection in prior works is often conducted in constrained robot lab environments, on tasks like grasping (Pinto \& Gupta, 2015; Levine et al., 2016; Kalashnikov et al., 2018; Platt, 2022), pushing (Yu et al., 2016; Ebert et al., 2018; Dasari et al., 2020), or pick and place (Kalashnikov et al., 2021; Bousmalis et al., 2023). Our work focuses on tackling more varied environments, similar to Gupta et al. (2018), and tackling a wider set of tasks. Human demonstrated data collection can be done in varied environments (Sharma et al., 2018; Mandlekar et al., 2019; Jang et al., 2021; Brohan et al., 2022), and teleoperated data can be far more diverse and valuable for skill learning than autonomously collected data, but is bottlenecked by availability of humans when scaling to many robots. This motivates hybrid approaches that mix teleoperation and autonomous policies, such as DAgger style methods (Ross et al., 2011; Kelly et al., 2019; Hoque et al., 2022). AutoRT is such a hybrid approach, collecting both teleoperated and autonomous episodes based on supply of human supervision, with a focus on collecting data on novel tasks in novel environments.

Large language models. Many recent works have studied using LLMs to generate agent-like behavior (Shinn et al., 2023; Yao et al., 2022; Park et al., 2023), improve embodied reasoning (Driess et al., 2023), and write robotics code (Vemprala et al., 2023; Liang et al., 2022). Works like Ahn et al. (2022) and Rana et al. (2023) use LLMs to generate language plans for robots to solve an instruction given by a user. Our work self-generates instructions for the robot to perform, which was proposed in Xian et al. (2023). Most similar is Voyager (Wang et al., 2023), an LLM-driven agent that autonomously explores a Minecraft environment. AutoRT runs on a real-world robot for extended periods of time, introducing challenges like reliability and safety that are less present in simulated environments.

## 3 PROBLEM STATEMENT

In this work, our goal is to build a system that enables large-scale, "in-the-wild" data collection to generate diverse, real-world robot data on new skills in new environments.

To do so, we assume access to a large fleet of $N$ robots, capable of navigating across multiple buildings, and manipulating objects. The buildings are populated, where both robots and people are free to move around the space. We do not make any assumptions about the layout of the buildings, or the objects available for manipulation. We assume a limited bandwidth of human supervision, meaning there are more robots than human supervisors - that is, we cannot expect that a human will always be in charge of teleoperating a single robot.

Our goal is to have a single system that can handle any state $s \in S$ observed by a robot, and generate tasks $t$ executable by one of $k$ different collect policies $\pi \in\left\{\pi^{1}, \ldots, \pi^{k}\right\}=\Pi$. For instance, $\pi_{i}$ can be an autonomous policy $\pi_{i}^{\text {auto }}$ either hand-designed or learned a priori, or a policy executed by querying a human teleoperator, i.e., $\pi_{i}^{\text {teleop }}$. The goal of such a system: $S \rightarrow \Pi$ is to guide the data collection of the fleet of $N$ robots by observing the state $s$ and use this information to identify a set of feasible language-specified tasks $t$ that correspond to specific policies $\pi$. In addition, the system needs to take into account other factors that impact throughput of data collection and safety. These include tradeoffs between autonomous and teleoperated policy primitives, generation of diverse and novel tasks proposals while at the same time considering guardrails and safety criteria.

## 4 AUTORT: EXPLORING AND EXECUTING IN THE WILD

In this section, we describe each component of AutoRT, which is visualized in Fig. 5. At a high level, AutoRT gathers data via an open vocabulary object detector to first understand and describe the scene, then an LLM parses this description and generates sensible and safe language goals given high-level objectives, and finally an LLM is used to determine how to execute these goals.

The robot platform used in AutoRT is a mobile manipulator with a camera, robot arm, and mobile base. Herein, we only consider manipulation data collection, so navigation is only used to gather diverse manipulation settings - however, we note that the system is general to other robotic embodiments and modes of collection. Further details on the robot platform and the implementation are in Appendix A.

### 4.1 EXPLORATION: NAVIGATING TO THE TARGET

The first stage of AutoRT is to explore the space and find interesting scenes for manipulation. To map the environment, we use the natural language map approach proposed by Chen et al. (2023), which is built using a VLM to encode object detections into visual-language embeddings $\phi_{i}$, with corresponding position $\left(x_{i}, y_{i}, z_{i}\right)$ determined by the robot's depth sensor and SLAM. Thus, given a textual target $q$ like "sponge", we can direct the robot towards a sponge by querying for a $\phi_{i}$ that is close to the text embedding for $q$. To determine navigation goals we sample this map for regions of interest via sampling states proportional to their latent distance to an average embedding of previously seen objects (see Appendix B for more details). For each environment, this map is generated once, then copied to all robots collecting in the space and loaded from cache to save time in future episodes.

### 4.2 RoBOT ConStitUTION

Key to safe robot operation is breaking down high level objectives relevant to humans into tasks a robot may perform. We specify this to robots using what we call a Robot Constitution, a list of rules an LLM is instructed to follow, inspired by methods like Constitutional AI (Bai et al., 2022). These rules are divided into three categories:

- Foundational rules inspired by Asimov's three laws (Asimov, 1942) that govern robotics in general and govern interactions with humans. We modify the exact text of these laws as described in Appendix D.
- Safety rules describing what tasks are considered unsafe or undesired based on current capabilities in deployment. These discourage the collect policies from interacting with humans or animals. They also discourage handling sharp and fragile objects or electrical equipment.
- Embodiment rules describing limitations of the robot's embodiment, such as its maximum payload and its unimanual nature, to discourage attempting tasks with heavier objects or that which require two arms (e.g. "opening a fridge and picking up a drink").

![](https://cdn.mathpix.com/cropped/2024_06_04_2a5d8b17f2c453406160g-04.jpg?height=922&width=1260&top_left_y=276&top_left_x=430)

Figure 1: System diagram for AutoRT. Each robot explores the environment, sampling a random navigation target close to objects. The scene and objects in it are described by a VLM to give text to an LLM, which generates manipulation tasks for the robot. Valid tasks are run by the robot, the episodes are scored, and the process repeats. No part of this requires advance knowledge of the layout of the environment or objects it contains, making it easy to run on a fleet of $20+$ robots that are each in novel settings. Green sections are contributions of this work.

A fourth category, the guidance rules, provides an input for an optional high-level human command: "The human command, which the robot should follow if given: \{guidance\}". The way the robot constitution is used in task generation and affordance is explained below.

### 4.3 TASK GENERATION

Once a robot is in front of a manipulation scene $s_{i}$, it needs to generate a list of manipulation tasks to attempt. This is done via two steps:

- Scene description: Given an image from the robot camera, a VLM outputs text describing the scene the robot observes, and 5 objects that exist in that scene. For example, as shown in Fig. 5, the VLM lists soap, napkin, snack, cloth, sponge in the given scene.
- Task proposal: In this step, AutoRT is prompted to generate a list of tasks. This prompt begins with a system prompt, such as: "I am a robot operating in an office environment", which describes the role the LLM should play. It continues with a list of rules that should be followed for task generation, codified by the robot constitution. The prompt ends with a section, where we can inject the scene and object description from the prior VLM call. Given this prompt, an LLM generates a list of potential manipulation tasks (see Fig. 5). We note, the LLM is not fine-tuned to our specific use case to maintain the generality the underlying model.

An important detail of AutoRT is that we use multiple collect policies $\left\{\pi^{1}, \pi^{2}, \ldots, \pi^{k}\right\}$, sampling one each episode. When the collect policy is sampled, and task generation must be modified to match the capabilities of that policy. Thus, for each policy $\pi^{j}$, we append a $\pi^{j}$-specific suffix to the end of the task generation prompt. See Appendix D for full text of the prompts.

### 4.4 AFFORDANCE

Tasks generated by the LLM on the first pass may not fully follow the provided prompt and thus AutoRT uses an extra step of task filtering. This is done using another prompted LLM; one can view this as a self-reflection step where an LLM is prompted to critique its own output, inspired by approaches such as Reflexion (Shinn et al., 2023), ReAct (Yao et al., 2022), and Constitutional AI (Bai et al., 2022).

During the affordance step, in addition to the robot constitution, the LLM is further prompted with the list of collect policies available and text summaries of what each collect policy can do. For each generated task, the LLM is asked to either output a collect policy or a reason to reject that task. A few examples are provided to guide the LLM output into the desired format. This can be viewed as a classifier between the $k$ collect policies, with an extra category for unknown tasks. The final task is then selected by randomly sampling from the accepted tasks. For instance, as shown in Fig. 5, the originally sampled policy is $\pi^{\text {teleop }}$. The first two proposed tasks by the LLM are classified as $\pi^{\mathrm{teleop}}$, the second two tasks are classified as $\pi^{\mathrm{rt2}}$, an autonomous policy from (Brohan et al., 2023), and the last task is rejected as the embodiment of the robot does not allow for a bimanual task. The final task is sampled from the first two tasks. We found classifying between all collect policies was fine, even though for filtering it would be sufficient to classify between $\pi^{i}$ and not- $\pi^{i}$ per episode.

### 4.5 DATA COLLECTION

Any number of collect policies could be used, but our instance of AutoRT uses three: teleoperation, a scripted pick policy, and RT-2 (Brohan et al., 2023). The scripted pick policy pseudocode is provided in Appendix H. Each $\pi^{i}$ has a different sampling probability $p_{i}$ that is adjusted during collect primarily based on the number of robots supervised per person. For example, if 1 person is supervising 3 robots, then the human teleoperation collect policy was sampled $p<\frac{1}{3}$ of the time to respect available supervision. After manipulation, the episode's diversity is scored (see Section 5.1 for how), and the robot resets to start again. The human supervisor may occasionally reset the environment by hand.

Recent works like Brohan et al. (2023) suggest Internet-scale visual-language data can drive generalization in downstream robotic models. Assuming these trends continue, the upcoming bottleneck will be action diversity - collecting useful, diverse motions that make progress towards new tasks in novel environments. Teleoperated data is the most action diverse policy, so we focus on keeping throughput of teleoperation high (no worse than a " 1 human 1 robot" setup), potentially at the cost of assisting autonomous robots less frequently. We additionally prompt task generation for teleop to collect varied tasks by including lines like "none of these tasks should be simple pick and place". For a breakdown of throughput by collect policy, or visualization of action trajectories, see Appendix I.

### 4.6 GUARDRAILS

AutoRT deploys foundation models in "in the wild" settings but foundation models, even if prompted correctly and with instruction finetuning have no guarantees on safety. We complement these with traditional robot environment controls as an additional layer of safety. These measures are detailed in Appendix C.

## 5 EXPERIMENTAL EVALUATION

Our experimental evaluation studies the deployment of AutoRT in a variety of real-world environments, covering about 7 months, 4 different buildings, simultaneous operation of over 20 robots, and about 77,000 real-world robotic trials. We aim to evaluate the diversity of the data collected by AutoRT, the degree to which we can steer the tasks that AutoRT attempts by modifying the prompt, the semantic and functional appropriateness of the automatically generated task proposals, and an initial evaluation showing an example application of the AutoRT-collected data to improve the RT-1 (Brohan et al., 2022) model.

AutoRT Environment Scaling Our collection environments for the robots include offices, kitchens, and cafeterias. The same code is used in every environment with the only per-environment change being the difference in driving bounds allowing AutoRT to start collecting in a new environment in ; 1 day without too much set up. Some of these environments are shown in Fig. 2.
![](https://cdn.mathpix.com/cropped/2024_06_04_2a5d8b17f2c453406160g-06.jpg?height=136&width=1374&top_left_y=275&top_left_x=369)

Figure 2: Examples of robot collect environments used. These environments have a variety of surfaces and semantically different objects to practice manipulation on, along with freedom for the robot to move between manipulation scenes.

AutoRT Robot Deployment Scaling: Each human supervised between 3 to 5 robots at once, allowing to scale mobile manipulator deployment faster than number of humans employed. Some of AutoRT was run using stationary robots that skipped navigation, only running task generation and manipulation in a loop. These robots were easier to supervise due to their smaller range of motion, and were run with 1 human watching up to 8 robots. Human availability dictated the sampling ratios for collect policies.

Data statistics: In total, 53 robots were used to collect 77,000 new episodes over the course of 7 months, with a peak load of over 20 simultaneous robots. Over 6,650 unique instructions appear in the dataset. More details can be found in Fig. 3, Fig. 4 and Table 1. Interestingly, we find that RT-2 success rate is quite low during collection, because the complex environments, objects and requirement for navigation differed significantly from RT-2's training set and inference capabilities. This influenced our decision to run RT-2 less frequently.

![](https://cdn.mathpix.com/cropped/2024_06_04_2a5d8b17f2c453406160g-06.jpg?height=366&width=1127&top_left_y=1145&top_left_x=499)

Figure 3: On the left is AutoRT robot usage and on the right is t-SNE visualization of tasks, colored by collect policy used. Each point corresponds to a different task string.
![](https://cdn.mathpix.com/cropped/2024_06_04_2a5d8b17f2c453406160g-06.jpg?height=466&width=1264&top_left_y=1678&top_left_x=430)

Figure 4: AutoRT episodes collected and unique tasks over time

### 5.1 DIVERSITY SCORING

Given a fixed budget of human oversight and a fleet of robots, we aim to collect as much useful data as possible. Evaluating this is challenging, because downstream methods for utilizing such data are still imperfect - despite considerable recent progress, RL methods present scalability challenges to such diverse environments (Cobbe et al., 2020), while imitation learning methods require nearoptimal data. Thus, our measure of success for AutoRT is the diversity of the collected data.We

| Collect Policy | \# Episodes | Success Rate |
| :--- | :--- | :--- |
| Scripted Policy | 73293 | $21 \%$ |
| Teleop | 3060 | $82 \%$ |
| RT-2 | 936 | $4.7 \%$ |

Table 1: AutoRT data, split by collect policy used. Scripted policy was used most frequently, while teleoperation had the highest success rate.

| Collect Method | Average Language L2 Dist |
| :--- | :---: |
| Lang. Table | 0.988 |
| BC-Z | 1.070 |
| RT-1 | 1.073 |
| AutoRT w/PaLI | 1.100 |
| AutoRT w/FlexCap | 1.137 |
| Optimal | 1.414 |

Table 2: Diversity of language embeddings from task generators. AutoRT generates language embeddings that are further apart.

consider two different axes of diversity: visual diversity (how diverse are the collected trajectories visually), and language diversity (how diverse are the natural language instructions proposed by our system). We additionally present an evaluation of the RT-1 model via filtered BC in Section 5.4, however we note our evaluation is preliminary, and we hope that future advances in low-level robotic learning algorithms (e.g., RL and IL) will lead to better approaches for utilizing such data.

Language diversity: To measure language diversity, we use the $\mathrm{L} 2$ distance in a language embedding space - specifically that of Universal Sentence Encoder (Cer et al., 2018) that are normalized 512-d embeddings. We compare AutoRT's task generation approach with the hand-designed tasks from three previous works: tasks from Language Table (Lynch et al., 2023), tasks from BC-Z (Jang et al., 2021), and tasks from RT-1 (Brohan et al., 2022). Table 2 shows AutoRT has higher average distance between language embeddings and generates more diverse language than all other approaches.

We additionally use the language diversity score to compare two VLMs for scene description without generating large amounts of robot data. We compare PaLI (Chen et al., 2022) and FlexCap (Review, 2023). Keeping the LLM prompts fixed, we first sample 70 random scenes the robots saw so far. Each scene was described by each VLM, and their descriptions were passed to task generation. The diversity of language embeddings after affordance filtering was then used to score the VLMs. We found both VLMs led to better scores than our baselines. Qualitative examples of sampled tasks from the two VLMs are in Appendix G.

Visual diversity: To measure visual diversity, we utilize a clustering method similar to a diversity measure used in Tirumala et al. (2023). Robot episodes are first embedded by a visual encoder, then $k$-means unsupervised clustering is done in the space. New episodes are scored based on the distance from that episode's embedding to the nearest $k$-means centroid. This distance is the diversity score, with larger distances indicating more novel data. We utilize a CLIP model as our embedder, finetuned to contrast \{first image, goal image \} embeddings with natural language captions (Xiao et al., 2023), and cluster with $k=1000$. We found this was better at capturing semantic differences, although it does ignore intermediate images.

Fig. 5 shows the visual diversity across each of AutoRT's data collection policies, along with the RT-1 dataset as a baseline. We find that the visual diversity is larger for each type of AutoRT data, with higher diversity in teleop than the scripted policy. Notably, RT-1's dataset is only teleop, yet AutoRT is more diverse across all categories. Sample images are shown in Fig. 6. We also did an experiment where human supervisors directly optimized the visual diversity at collect time based on robot feedback. Further details are in Appendix E.

### 5.2 TASK GENERATION

In this section we study the quality of task generation prior to filtering based on feasibility (is the task possible) and relevance (does the task follow high-level guidance) and compare against two baselines. First, a simple templated language approach that matches a random verb from a hardcoded list with an object seen by the VLM, e.g. "<verb> <object>". This mirrors the language instruction process used in RT-1. Second, to ablate how well AutoRT can be steered towards useful tasks, we consider a AutoRT (unguided) variant that removes the guidance rule from the prompt.

To evaluate, the robot is placed in front of 5 scenes. We generate 75 tasks in total, using guidance like "collect gardening tasks" or "how would you clean this mess?" for AutoRT (guided). Results
![](https://cdn.mathpix.com/cropped/2024_06_04_2a5d8b17f2c453406160g-08.jpg?height=376&width=1264&top_left_y=278&top_left_x=430)

Figure 5: Visual diversity visualizations for AutoRT, as scored by distance to closest $k$-means centroid. Left: Histogram of 1000 random successes per collect policy (or all successes from RT-2 collect). Right: CDF of distributions, median of distribution annotated. Higher distances (more weight on the right) are further from prior data, and thus better. We find all AutoRT data is more diverse due to running in more varied environments, with teleop data from AutoRT scoring best.
![](https://cdn.mathpix.com/cropped/2024_06_04_2a5d8b17f2c453406160g-08.jpg?height=282&width=1346&top_left_y=889&top_left_x=386)

Figure 6: Example last-frame images (color corrected) from RT-1 (left) and AutoRT (right)

are shown in Table 3. We find that AutoRT's tasks (guided and unguided) are 1.5x more likely to be feasible than templated language. The large increase in feasibility is because naively mixand-matching verbs is likely to generate nonsense language like "open keyboard", whereas LLMs will tend to generate sensible language. We further find that we can guide task generation towards gardening, cleaning, etc., which is promising for allowing end-users to tell robots what data we would like them to collect. Qualitative outputs are in Appendix G.

### 5.3 AFFORDANCE AND RobOT CONSTITUTION

In this section we study the effect of constitutional prompting and LLM self-critiquing on identifying safe and feasible tasks. Task generation and filtering are evaluated via two metrics: \% Safe, the fraction of safe and feasible tasks proposed by AutoRT, and Recall, how often the self critiquing step correctly rejects unsuitable tasks generated during task proposal step.

Accuracy of AutoRT Task Generation: Across a sample of 64 scenes, we consider all 259 tasks generated and label whether each task is safe and feasible to collect. In this sample, we found 31 tasks that outght to have been rejected, giving a base rate of $228 / 259=88 \%$ acceptable tasks. After the LLM affordance filtering step we see the rate of acceptable tasks increase to $200 / 214=93 \%$.

When evaluating affordance, over-rejecting tasks is better than under-rejecting them, so we further evaluate the recall of rejected tasks. How often does the LLM reject (or fail to reject) tasks that should be rejected? Of the 31 unsuitable tasks, the LLM rejected $17 / 31=55 \%$ of them. Aditionally we find that all 14 errors occurred during teleop task sampling, attributable to forcing teleop task

Table 3: Comparison of task generation methods at generating completable tasks and relevant tasks. Injecting the high-level guidance into the LLM prompt improves the relevance of generated tasks. Using an LLM at all improves both feasibility and relevance thanks to common-sense inherited from Internet-scale data.

| Task Generator | Relevance | Feasibility |
| :--- | :---: | :---: |
| Templated Language | $20 / 75=27 \%$ | $39 / 75=52 \%$ |
| AutoRT (unguided) | $21 / 75=28 \%$ | $62 / 75=83 \%$ |
| AutoRT (guided) | $46 / 75=61 \%$ | $58 / 75=77 \%$ |

generation to remain highly diverse. These tasks were rejected by the teleoperator during collect indicating the importance of human-in-the-loop supervision, both as a safety mechanism and as a source of intervention data to improve affordance of task generation.

Adversarial Testing of Constitutional Prompting: To measure the effect of constitutional prompting, we set up deliberately adversarial scenes, and ablate our rules from the task generation prompt and affordance prompt. First, 5 test scenes were set up with objects that the robot should not interact with, including lifelike toy animals, sharp items, and people. Three task generation prompts are used: an unsafe prompt (designed to propose unsafe tasks), a minimal prompt (describing task generation without rules or constitution), and the constitutional prompt. These tasks are then filtered via two affordance prompts: a minimal one (describing affordance classification) and a constitutional one. Full prompt texts are in Appendix D.1. We show in Table 4 that the rate of safe tasks is significantly increased when robot constitution is included at task generation time or affordance filtering time, with best results when included at both steps. Additionally constitutional prompting is able to achieve high recall when given unsafe tasks.

Table 4: Effect of constitutional prompting on safety of proposed tasks

|  | Unsafe prompting |  | Task Generation <br> Minimal prompting |  | Constitutional prompting <br> \% Safe |  |
| :--- | ---: | :--- | ---: | :--- | :--- | :--- |
| \% Safe | Recall | $\%$ Safe | Recall | Recall |  |  |
| None | $13 / 49=27 \%$ | N/A | $9 / 50=18 \%$ | N/A | $35 / 50=70 \%$ | N/A |
| Minimal | $11 / 43=26 \%$ | $4 / 36=11 \%$ | $5 / 34=15 \%$ | $12 / 41=29 \%$ | $26 / 39=67 \%$ | $2 / 15=13 \%$ |
| Constit. | $13 / 15=87 \%$ | $34 / 36=94 \%$ | $8 / 14=57 \%$ | $35 / 41=85 \%$ | $25 / 30=83 \%$ | $26 / 39=67 \%$ |

### 5.4 MODEL TRAINING

The data generated by AutoRT covers a significantly wider range of language and visuals than in datasets such as RT-1 (Brohan et al., 2022). As a sanity check on the usefulness of the data, we run a training comparison with the RT-1 model. A pretrained RT-1 model is co-fine-tuned on a 50-50 mixture of the pretraining dataset described in Brohan et al. (2022) and AutoRT's dataset. RT-1 is used instead of RT-2 due to training more quickly and cheaply.

The co-fine-tuned model is evaluated on two tasks we find RT-1 generalizes poorly to: picking from different heights, and wiping. Exact evaluation instructions and details are in Appendix F. When co-fine-tuned, RT-1's performance increases from $0 \%$ to $12.5 \%$ on picking from different height, and $10 \%$ to $30 \%$ on wiping. We additionally include an ablation where we train from only the teleoperated segment of AutoRT data. We find this model is no longer able to pick from different heights, indicating that non-teleoperated AutoRT can be useful. These increases are modest, but we note that the focus of AutoRT was on collecting diverse data, not on achieving high success rates. RT-1 training was done to verify the data could improve the model, but the high diversity of tasks and scenarios leads to a challenging learning problem that is hard to perform well at.

Table 5: Results from co-finetuning RT-1 on AutoRT data

|  | Picking (Height Generalization) | Wiping |
| :---: | :---: | :---: |
| RT-1 | $0 / 24=0 \%$ | $1 / 10=10 \%$ |
| Co-fine-tuned RT-1 on AutoRT data | $\mathbf{3 / 2 4 = 1 2 . 5 \%}$ | $\mathbf{3 / 1 0}=\mathbf{3 0 \%}$ |
| Co-fine-tuned RT-1 on teleop segment of AutoRT data | $0 / 24=0 \%$ | $2 / 10=20 \%$ |

## 6 CONCLUSION, LIMITATIONS, AND FUTURE WORK

We presented AutoRT, an approach for directing fleets of robots to collect data in the real world, autonomously and with human help, supervised by large-scale vision and language models. We demonstrated that this approach results in useful, diverse, and large-scale data-leading to $77 \mathrm{k}$ realworld demonstrations collected by over 20 robots in 7 months in 4 buildings. We further introduced a robot constitution - which defined foundational rules, outlined safety constraints, and detailed the robot's embodiment, and ablated the system design to show its usefulness. Finally, by training a
model on this collected data we demonstrated novel capabilities and improved generalization over state of the art models. We believe this work is a step towards scaling robot data collection to the breadth of foundation models as well as embodying foundation models into robotic systems.

Despite the promise of AutoRT, the current approach comes with a number of limitations.

1. AutoRT relies in large part on scripted and learned policies to scale collection for fixed teleoperation budget. If these policies only handle simpler tasks or have lower success rates in unseen settings, it lowers the throughput of successful episodes. Scaling the generation of higher quality data requires more robust and diverse autonomous collect policies as in Arenas et al. (2023)
2. Communication bandwidth between scene description and language model can introduce an information bottleneck in AutoRT. Failures of perception such as hallucination of objects, lack of generalization to novel environments, and motion blur can introduce and propagate failures in the system. As noted by prior work (Ahn et al., 2022; Mees et al., 2023; Gao et al., 2023), foundation models also face challenges in reasoning about task and embodiment specific information, such as physics of objects and capabilities of the robot. We ignored this for simplicity, but expect future efforts to require more accurate real-world reasoning.
3. Thirdly, the type of data collected by AutoRT tends to be highly diverse, leading to fewer samples per task and lots of variety in scenes and object configurations. This "sparse" data presents a harder learning problem than the datasets used in existing state of the art robot learning methods like Brohan et al. (2022) and Brohan et al. (2023). AutoRT assumes data collection is decoupled from the control policy, but achieving the best policy improvement would likely require the two to evolve in tandem with each other.
4. Lastly, though constitutional prompting improves safety of generated tasks, prompting an LLM does not guarantee that the prompt's instructions will be followed, and a small percentage of unsafe tasks generated by the LLM will pass the affordance filtering. This necessitates some degree of human supervision.

As we explore future directions, a chief question is how a robot should autonomously act in the world. What we call a robot constitution has historically been a topic reserved for science fiction (Asimov, 1942), but this work concretizes a real application where such rules could be helpful. We also see future work in treating model improvement and data collection as a single goal, rather than two separate areas, with an eye on identifying proximal skills and improving sample efficiency via directed data collection.

| Author | Model <br> Training <br> \& Eval | Navigation <br> $\&$ Scene <br> Descrip- <br> tion | Task <br> Gener- <br>  <br> Filtering | Collect <br> Methods | Data | Leadership | Paper <br> Writing |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Michael Ahn |  |  |  |  | $\checkmark$ |  |  |
| Debidatta <br> Dwibedi |  | $\checkmark$ |  |  |  |  |  |
| Chelsea Finn |  |  |  |  |  | $\bar{\checkmark}$ | $\bar{\checkmark}$ |
| Montse Gon- <br> zalez Arenas |  |  |  | $\checkmark$ |  |  |  |
| Keerthana <br> Gopalakrish- <br> nan | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| Karol Haus- <br> man |  |  |  |  |  | $\bar{\checkmark}$ | $\bar{\checkmark}$ |
| Brian Ichter |  |  | $\checkmark$ |  |  |  |  |
| Alex Irpan | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| Nikhil Joshi |  |  |  |  | $\checkmark$ |  |  |
| Ryan Julian |  |  |  | $\checkmark$ |  |  |  |
| Sean Kirmani |  | $\checkmark$ | $\bar{\checkmark}$ |  |  |  | $\bar{\checkmark}$ |
| Isabel Leal |  |  |  | $\bar{\checkmark}$ | $\checkmark$ |  |  |
| Edward Lee |  | $\checkmark$ |  |  |  |  |  |
| Sergey Levine |  |  |  |  |  | $\checkmark$ | $\bar{\checkmark}$ |
| Yao Lu |  |  |  |  | $\checkmark$ | $\checkmark$ |  |
| Sharath <br> Maddineni |  |  |  | $\checkmark$ |  |  |  |
| Kanishka Rao |  |  |  |  |  | $\checkmark$ |  |
| Dorsa Sadigh |  |  |  |  |  | $\checkmark$ | $\checkmark$ |
| Pannag San- <br> keti |  |  |  |  | $\checkmark$ |  |  |
| Pierre Ser- <br> manet  | $\checkmark$ |  | $\bar{\checkmark}$ |  |  |  |  |
| Quan Vuong | $\checkmark$ |  |  |  | $\checkmark$ |  |  |
| Stefan Welker |  |  |  |  | $\checkmark$ |  |  |
| Fei Xia |  | $\checkmark$ | $\bar{\checkmark}$ | $\checkmark$ |  |  |  |
| Ted Xiao |  |  |  |  | $\checkmark$ |  | $\bar{\checkmark}$ |
| Peng $\mathrm{Xu}$ |  |  |  | $\checkmark$ |  |  |  |
| Steve $\mathrm{Xu}$ |  |  |  | $\checkmark$ |  |  |  |
| Zhuo Xu | $\checkmark$ |  |  |  |  |  |  |

## ACKNOWLEDGMENTS

We thank Celeste Barajas, Joseph Dabis, Gavin Gonzalez, Tomas Jackson, Alex Luong, Utsav Malla, Emily Perez, Elio Prado, Jornell Quiambao, Sangeetha Ramesh, Jaspiar Singh, Clayton Tan,

Jodexty Therlonge, Eric Tran, Steven Vega, and Samuel Wan for assistance on data collection, model evaluation, and AutoRT supervision. We thank Anthony Brohan and Noah Brown for assistance on data analysis. We thank David DoVo, Regine Firmeza, Tad Koch, Gus Kouretas, Jessica Lam, Thien Nguyen, and Eric Zankiewicz for robot setup and maintenance. We thank Nicolas Heess, Jacky Liang, Vincent Vanhoucke, and Andy Zeng for providing feedback on paper drafts.

## REFERENCES

Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.

Montserrat Gonzalez Arenas, Ted Xiao, Sumeet Singh, Vidhi Jain, Allen Z. Ren, Quan Vuong, Jake Varley, Alexander Herzog, Isabel Leal, Sean Kirmani, Dorsa Sadigh, Vikas Sindhwani, Kanishka Rao, Jacky Liang, and Andy Zeng. How to prompt your robot: A promptbook for manipulation skills with code as policies. In 2nd Workshop on Language and Robot Learning: Language as Grounding, 2023. URL https: / /openreview.net/forum?id=T8AiZj1QdN.

Isaac Asimov. Runaround. Street \& Smith, 1942.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Żołna, Scott Reed, Sergio Gómez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Tom Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, and Nicolas Heess. Robocat: A self-improving foundation agent for robotic manipulation, 2023.

Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.

Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt2: Vision-language-action models transfer web knowledge to robotic control. In arXiv preprint arXiv:2307.15818, 2023.

Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole Lyn Untalan Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Céspedes, Steve Yuan, Chris Tar, Yun hsuan Sung, Brian Strope, and Ray Kurzweil. Universal sentence encoder. In In submission to: EMNLP demonstration, Brussels, Belgium, 2018. URL https://arxiv.org/abs/1803.11175. In submission.

Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S Ryoo, Austin Stone, and Daniel Kappler. Open-vocabulary queryable scene representations for real
world planning. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. $11509-11522$. IEEE, 2023.

Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022.

Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pp. 20482056. PMLR, 2020.

Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning, 2020 .

Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In arXiv preprint arXiv:2303.03378, 2023.

Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control, 2018.

Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation, 2023.

Abhinav Gupta, Adithyavairavan Murali, Dhiraj Gandhi, and Lerrel Pinto. Robot learning in homes: Improving generalization and reducing dataset bias, 2018.

Ryan Hoque, Lawrence Yunliang Chen, Satvik Sharma, Karthik Dharmarajan, Brijen Thananjeyan, Pieter Abbeel, and Ken Goldberg. Fleet-dagger: Interactive robot fleet learning with scalable human supervision, 2022 .

Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-z: Zero-shot task generalization with robotic imitation learning. In 5th Annual Conference on Robot Learning, 2021. URL https://openreview. net/forum? id=8k.bp23tSGYv.

Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. CoRR, abs/1806.10293, 2018. URL http://arxiv.org/abs/1806.10293.

Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale, 2021.

Michael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell, and Mykel J Kochenderfer. Hgdagger: Interactive imitation learning with human experts. In 2019 International Conference on Robotics and Automation (ICRA), pp. 8077-8083. IEEE, 2019.

Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection, 2016.

Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In arXiv preprint arXiv:2209.07753, 2022.

Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023.

Ajay Mandlekar, Jonathan Booher, Max Spero, Albert Tung, Anchit Gupta, Yuke Zhu, Animesh Garg, Silvio Savarese, and Li Fei-Fei. Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1048-1055. IEEE, 2019.

Oier Mees, Jessica Borja-Diaz, and Wolfram Burgard. Grounding language with visual affordances over unstructured data. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 11576-11582. IEEE, 2023.

Joon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.

Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours, 2015 .

Robert Platt. Grasp learning: Models, methods, and performance, 2022.

Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. arXiv preprint arXiv:2307.06135, 2023.

Under Review. Flexcap: Generating rich, localized, and flexible captions in images. 2023.

Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627-635. JMLR Workshop and Conference Proceedings, 2011.

Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Abhinav Gupta. Multiple interactions made easy (mime): Large scale demonstrations data for imitation, 2018.

Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.

Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving llm pretraining via document de-duplication and diversification. In Proceedings of the 40 th International Conference on Machine Learning, 2023.

Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics: Design principles and model abilities. Microsoft Auton. Syst. Robot. Res, 2:20, 2023.

Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv: Arxiv-2305.16291, 2023.

Zhou Xian, Theophile Gervet, Zhenjia Xu, Yi-Ling Qiao, and Tsun-Hsuan Wang. Towards a foundation model for generalist robots: Diverse skill learning at scale via automated task and scene generation. arXiv preprint arXiv:2305.10455, 2023.

Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, and Jonathan Tompson. Robotic skill acquistion via instruction augmentation with visionlanguage models. In Proceedings of Robotics: Science and Systems, 2023.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.

Kuan-Ting Yu, Maria Bauza, Nima Fazeli, and Alberto Rodriguez. More than a million ways to be pushed: A high-fidelity experimental dataset of planar pushing, 2016.
