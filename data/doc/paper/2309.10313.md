# Investigating the Catastrophic Forgetting in Multimodal Large Language Models 

Yuexiang Zhai ${ }^{1 *}$ Shengbang Tong ${ }^{2}, \mathrm{Xiao}^{3}{ }^{3}, \mathrm{MuCai}^{4}$,<br>Qing $\mathrm{Qu}^{3}$, Yong Jae Lee ${ }^{4,5}, \mathrm{Yi} \mathrm{Ma}{ }^{1}$<br>${ }^{1}$ UC Berkeley, ${ }^{2} \mathrm{NYU},{ }^{3}$ University of Michigan, ${ }^{4}$ University of Wisconsin-Madison, ${ }^{5}$ Cruise LLC


#### Abstract

Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the finetuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.


## 1. Introduction

The recent progress in language models (LMs) has demonstrated impressive competency in engaging in a natural dialogue and in complex examinations [1-4]. Besides text generation, GPT4 [5] has recently shown impressive multimodal capability by performing a range of tasks with visual and language inputs. The emergent multimodal reasoning capabilities of GPT4 have propelled a surge of interest in multimodal large language models (MLLMs) [6-10]. This line of research typically involves (1) integrating pre-trained vision encoders [11, 12] with open-source LLMs [13-15], and (2) applying instruction tuning on the resulting vision-language models $[7,9,10]$.

While many of these fine-tuned MLLMs have demonstrated remarkable capabilities in general purpose vision-language comprehension $[16,17]$, these models still suffer from catastrophic forgetting [18-21]. That is, the models tend to overfit to the fine-tuning dataset and consequently experience a decline in performance on pre-training tasks. Catastrophic forgetting in image classification has been extensively studied in computer vision and machine learning [22, 23]. However, recent developments in MLLMs [6-10] have mainly focused on creating multimodal chatbots for visual question answering [24], without evaluating their fundamental image classification capabilities, let alone explore the catastrophic forgetting in MLLM. That being said, prior MLLM evaluation frameworks $[17,25]$ mainly focus on assessing cognitive reasoning capability or hallucinations, which overlooks the necessity to critically examine how well MLLMs inherit the image classification capability from their base vision encoders $[11,12]$.[^0]

To comprehensively investigate the catastrophic forgetting in fine-tuned MLLM, we present the Evaluating MulTimodality (EMT) framework, which, to the best of our knowledge, is the first evaluation framework that studies the catastrophic forgetting in MLLMs. The EMT framework is a two-stage approach that treats each MLLM as an image classifier. In particular, for an input text and image pair, EMT first prompts the testing MLLM by asking it to classify the input image, and then post-processes the outputs to obtain a classification accuracy.

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-02.jpg?height=355&width=1138&top_left_y=538&top_left_x=488)

Figure 1: The EMT evaluation pipeline for MLLM. We prompt each MLLM as an image classifier by (1) inputting an image from a classification task; (2) asking the MLLM to explicitly answer a single label from the classification task. We evaluate the correctness of each output using another LLM.

We first apply EMT to several open-source fine-tuned MLLMs [7, 9, 10, 26] and observe a severe catastrophic forgetting phenomenon among all the tested models. That is, the majority of the tested MLLMs fail to retain a comparable classification accuracy when compared to the zero-shot performance of their vision encoders. After analyzing the results from the tested open-source models, we identify hallucination $[8,25,27,28]$ as one the major factors contributing to the performance degradation in MLLMs. Specifically, the tested MLLMs hallucinate by generating additional outputs that are irrelevant to the input question, including outputting more than one label or generating unverifiable descriptions of a label.

To gain deeper insights into how fine-tuning impacts the performance of MLLMs, we continue to fine-tune LLaVA [7], a popular MLLM achieving state-of-the-art accuracy on Science QA [29], and then apply the EMT evaluation to the fine-tuned LLaVA. Our fine-tuning experiments reveal two main observations. Initially, fine-tuning on one dataset demonstrates generalization to other datasets, as it improves the alignment between textual and visual features. However, as the fine-tuning progresses, LLaVA starts to hallucinate by disregarding the questions and exclusively generating text based on the examples in the fine-tuning datasets.

To summarize, this paper makes two key contributions.

- We propose EMT, an evaluation framework designed specifically to evaluate the phenomenon of catastrophic forgetting in MLLMs. To the best of our knowledge, EMT is the first evaluation framework to investigate catastrophic forgetting in MLLM through classification. Through EMT, we discover that nearly all the tested models fail to retain the classification performance of their vision encoders.
- We conduct fine-tuning experiments on LLaVA. Our fine-tuning results indicate that while moderate fine-tuning is advantageous for non-fine-tuned tasks, excessive fine-tuning ultimately leads to catastrophic forgetting in these tasks.

Our findings suggest that the fine-tuning process of MLLMs can still be further improved, particularly in mitigating catastrophic forgetting and reducing hallucinations.

## 2. Related Works

Fine-Tuning and Catastrophic Forgetting. Fine-tuning large pre-trained models has significantly transformed the field of natural language processing [1,2,30-32]. Despite its ubiquity and remarkable achievements, fine-tuning LLM still suffers from core machine learning problems such as catas-
trophic forgetting [33]. Catastrophic forgetting widely appears in LLM fine-tuning [19, 21, 34-36] or in-context learning $[37,38]$, as the LLMs tend to overfit to the small fine-tuning dataset resulting in losing performance on other tasks [34]. Various approaches have been proposed to mitigate the catastrophic forgetting problem in LLM fine-tuning, including pre-trained weight decay [36], learning rate decay [34], regularizations [35], and adversarial fine-tuning [19]. However, in MLLM, such a catastrophic forgetting phenomenon has not been thoroughly studied yet. Our work is most related to several evaluation metrics for MLLMs [17, 25], which proposed a comprehensive framework for evaluating the perception and recognition [17] or hallucinations [25], while the proposed EMT specifically aims at evaluating the catastrophic forgetting in MLLMs.

Multimodal Large Language Models. Multimodal Large Language Models (MLLMs) have emerged as a significant advancement in vision-language models, which significantly improves the model's reasoning capability. These models are designed to process and interpret information from multiple modalities, such as text and images, to perform complex tasks that require a comprehensive understanding of the context. Recent works [6-10, 26,39-42] have contributed to the development and enhancement of MLLMs by leveraging the strong reasoning capability of LLMs such as LLaMA [14, 15]. LLaVA [7], as presented in the paper under discussion, represents a novel approach to instruction tuning on machine-generated multimodal language-image instruction-following data, achieving impressive multimodal chat abilities and state-of-the-art accuracy on Science QA [29]. Following the instruction tuning approach, various works came out focusing on other modalities such as video [43] and point cloud [44]. See Yin et al. [16] for a more comprehensive overview of the current state and future directions of MLLMs.

A Theoretical Perspective of Catastrophic Forgetting through Minority Collapse. Recently, Yang et al. [23] introduced an approach to address the issue of catastrophic forgetting, drawing inspiration from the principles of Neural Collapse (NC) [45-50]. In particular, Fang et al. [47] proposes minority collapse as a subsequent research direction of NC. Minority collapse describes a phenomenon in supervised learning with imbalanced data, where the classifiers of the minority classes converge to one vertex when the sample size ratio between the majority and minority classes reaches infinity. This result implies that all minority classes are indistinguishable when the imbalance ratio reaches infinity. To connect the fine-tuning with minority collapse: (1) Treating the absent class in fine-tuning as minority classes with a sample size of zero, directly implies the imbalanced training scenarios with a ratio of infinity; (2) Such an imbalance training in the fine-tuning phase will make the classifiers of the pre-trained classes converges to one vertex [47]; (3) Hence, the pre-trained classes become indistinguishable during fine-tuning, which results in catastrophic forgetting.

## 3. Fine-Tuning Image Classification

To verify the theoretical results inspired by minority collapse $[47,48]$, where supervised fine-tuning leads to catastrophic forgetting, we first perform pre-training and fine-tuning of image classification via ResNet [51]. Next, to further investigate the catastrophic forgetting in the vision-language model, we conduct experiments in fine-tuning the Contrastive Language-Image Pre-Training network (CLIP) $[11]$.

### 3.1. Pre-Training and Fine-Tuning for Image Classification

To initiate our investigation, we train ResNet18 [51] on conventional image classification benchmarks. In particular, we first pre-train using the initial $50 \%$ of classes for 100 epochs. Then, we fine-tune with the remaining $50 \%$ of classes for 100 epochs, so that the fine-tuning classes and the pre-training classes do not overlap. Since the NC theory $[45,46]$ mainly focuses on analyzing the training loss, we only present the average training accuracy for the first $50 \%$ pre-trained classes (See Figure 2). Notably, when the fine-tuning starts, the training accuracy of pre-trained classes rapidly diminishes to zero across all datasets. As discussed in previous sections, such a catastrophic forgetting phenomenon can be directly associated with minority collapse, where the classifiers of all minority classes converge to a single vertex, when the imbalance ratio between majority and minority classes approaches infinity. Therefore, the observed decline in performance is in line with
our expectations. For completeness, we provide the theoretical formulation of minority collapse of finetuning in Appendix A and implementation details in Appendix B.
![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-04.jpg?height=318&width=1484&top_left_y=350&top_left_x=298)

Figure 2: Catastrophic forgetting happens in traditional classification tasks. To corroborate the NC theory [45-48], we only plot the average training accuracy of the first $50 \%$ classes of MNIST, CIFAR-10, CIFAR-100, and miniImagenet, respectively.

### 3.2. Fine-Tuning Contrastive Language-Image Pre-Training Network

We then fine-tune the vision encoder from the CLIP ViT-L-14 model [11], starting from a checkpoint provided by OpenAI's CLIP, available through openCLIP [12]. In our experiments, we employ the standard cross-entropy loss, consistent with the approach used in CLIP pre-training and the analysis in Neural Collapse [45, 46] as well as minority collapse [47]. Text inputs are created by concatenating labels with short descriptions. See examples in Appendix B.
![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-04.jpg?height=316&width=1504&top_left_y=1132&top_left_x=300)

Figure 3: Accuracy of 1-14 epoch fine-tuned CLIP on MNIST, CIFAR-10, CIFAR-100, and miniImagenet. Detailed accuracy numbers are presented in Table 10 of Appendix B.3.

Empirical results demonstrate that vision-language models like CLIP are susceptible to neural collapse after fine-tuning. In particular, we observe a significant rise in the in-domain performance, while the out-of-domain dataset performance begins to decline. By the time we reach 15 epochs, nearly all in-domain performance metrics have escalated to close to $99 \%$, but the out-of-domain performance has suffered.

## 4. EMT: Evaluating Multimodal Large Language Models

Since prior MLLM evaluation frameworks [17,25] focus on assessing cognitive reasoning [17] or hallucinations [25] rather than the catastrophic forgetting from an image classification perspective, we propose EMT, a framework for Evaluating MulTimodal LLM. EMT works as follows: (1) We start by inputting an image from a classification task; (2) Then we prompt the testing MLLM by asking it to classify the input images and collect its outputs via the prompt provided below, according to each dataset. (3) Next, since the output from MLLMs may not adhere to a specific format, we apply GPT- 3.5 to evaluate the classification accuracy; ${ }^{2}$ (4) Finally, we output the prediction accuracy of the testing MLLM on different datasets.

## EMT Prompt:

What is the number/object in the image? Please only answer a single number/object in [class labels].[^1]

The detailed prompts for predictions and evaluations for each dataset are provided in Appendix C.1.

### 4.1. Catastrophic Forgetting in Open-Source MLLMs

In this subsection, we initially apply EMT to assess four MLLMs: LLaVA [7], Otter [9], InstructBLIP [10], and LENS [26]. As shown in Figure 4, most of the tested open-source MLLMs suffer from catastrophic forgetting by failing to retain a similar classification performance, compared to the zero-shot classification outcome of their respective vision encoders. A notable exception is InstructBLIP-7b, which performs slightly better on the CIFAR-10 dataset. Despite InstructBLIP slightly performing better than its base vision model, InstructBLIP cannot achieve similar performance in CIFAR-100 and miniImagenet, compared to LLaVA and Otter. ${ }^{3}$ It may seem surprising that most of the tested MLLMs fail to retain similar performance of their foundational vision models, but such a performance degradation can be anticipated in hindsight. This performance degradation may stem from the fact that classifications of MNIST, CIFAR-10, CIFAR-100, and miniImagenet are not incorporated into the training dataset of the evaluated MLLMs. ${ }^{4}$

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-05.jpg?height=371&width=393&top_left_y=931&top_left_x=454)

(a) ViT-L-14

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-05.jpg?height=371&width=399&top_left_y=931&top_left_x=863)

(b) ViT-H-14

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-05.jpg?height=372&width=401&top_left_y=928&top_left_x=1274)

(c) ViT-g-14

Figure 4: EMT evaluation accuracy of different MLLMs on MNIST, CIFAR-10, CIFAR-100, and miniImagenet, against the zero-shot performance of their vision encoders. Models are grouped according to their underlying vision encoder architecture. Detailed accuracy numbers are presented in Table 2 in Appendix C.3.

### 4.2. Analyzing Failure Modes of MLLMs

After checking the outputs of different models using our EMT prompt, we have identified three major issues causing performance degradation: incorrect prediction, intrinsic hallucination, and extrinsic hallucination. It is evident that MLLMs could produce incorrect predictions, just like classifiers. In the example shown below, LLaVA-7B incorrectly predicts " 0 " as " 8 " in the MNIST classification.

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-05.jpg?height=138&width=113&top_left_y=1785&top_left_x=304)

Label: 0 | LLaVA-7b

The number in the image is 8 :

Aside from incorrect prediction, the performance is also significantly impacted by hallucination $[27,53]$ - the tested MLLMs sometimes generate seemingly related but incorrect or unverifiable contents. Ji et al. [27] further characterizes hallucinations into two distinct categories: intrinsic and extrinsic hallucinations. Intrinsic hallucinations are defined as instances in which the generated output directly contradicts the source content. Extrinsic hallucinations, on the other hand, are those where the output bears no verifiable connection to the original source content.

Intrinsic Hallucination. Our EMT prompt has identified intrinsic hallucinations within the tested MLLMs. One example can be drawn from asking LENS to perform a classification on CIFAR-10:[^2]

It is important to note that EMT prompt explicitly instructed the testing MLLM to identify only a single object within all class labels. Despite these clear instructions, LENS still produces an intrinsically hallucinated output-airplane, automobile, bird, cat, deer, dog, frog, horse,, an answer that contains multiple labels.

Extrinsic Hallucination. In addition to intrinsic hallucination, we have also discovered extrinsic hallucinations when applying InstructBLIP to classify CIFAR-100:

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-06.jpg?height=141&width=106&top_left_y=697&top_left_x=314)

$$
\text { Label: aquarium_fish | InstructBLIP-7b }
$$

a picture of a fish in a tank

In the example provided above, while the generated output text partially includes the label "aquarium fish", it also exhibits additional descriptors that are not only challenging to verify, but also extraneous to the original request outlined by the prompt.

Base LMs are Important. Among all the tested MLLMs, Figure 4 shows that LENS achieves the worst performance, compared to all other models, in each individual task and overall performance. Considering that ViT-H-14, the underlying vision encoder of LENS, does not exhibit a significant performance shortfall, we hypothesize that the observed performance gap is attributed to the base LM. This is because Otter, LLaVA, and InstructBLIP all adopt the LLaMA model [14], while LENS uses the Flan-T5 model [13], which is less powerful than LLaMA. Nonetheless, our results do not necessarily imply that larger LMs consistently yield superior performance, as our experiments have revealed varying outcomes. For instance, although LLaVA-13b generally surpasses LLaVA-7b, InstructBLIP-13b does not demonstrate superiority over InstructBLIP-7b. Therefore, we believe that additional experiments are required to conclusively determine whether larger LMs improve the integration of vision and text data in MLLMs.

## 5. EMT on Multimodal Large Language Models Fine-Tuning

Equipped with EMT, we now investigate the hallucinations in MLLM fine-tuning. We use LLaVA-7b and LLaVA-13b as our based MLLM for fine-tuning. And we conduct fine-tuning experiments on MNIST, CIFAR-10, CIFAR-100, and miniImagenet, respectively. All of our fine-tuning experiments were conducted based on the LLaVA model released on July $4^{\text {th }}, 2023 .{ }^{5}$

Linear and LoRA Fine-Tuning As discussed by Liu et al. [7], the LLaVA model contains a frozen base vision encoder $g(\cdot)$ and a pre-trained LLM $f_{\phi}(\cdot)$ parameterized by $\phi$. For an input image $\boldsymbol{X}_{\mathrm{v}}$, LLaVA first maps $\boldsymbol{X}_{\mathrm{v}}$ into a visual feature vector $\boldsymbol{Z}_{\mathrm{v}}$ by applying the visual encoder $\boldsymbol{Z}_{\mathrm{v}}=g\left(\boldsymbol{X}_{\mathrm{v}}\right)$. Then, LLaVA applies a linear adapted layer $\boldsymbol{W}$, that maps the visual features into text feature spaces $\boldsymbol{H}_{\mathrm{v}}=\boldsymbol{W} \cdot \boldsymbol{Z}_{\mathrm{v}}$, and concatenate $\boldsymbol{H}_{\mathrm{v}}$ with the embedding of language queries $\boldsymbol{H}_{\mathrm{q}}$ into a visual and text embedding vector $\left[\boldsymbol{H}_{\mathrm{v}}, \boldsymbol{H}_{\mathrm{q}}\right]$. Finally, LLaVA feeds $\left[\boldsymbol{H}_{\mathrm{v}}, \boldsymbol{H}_{\mathrm{q}}\right]$ as the input to the pre-trained LLM $f_{\phi}(\cdot)$ to obtain responses. As for specific fine-tuning methods: (1) Linear fine-tuning method only fine-tunes the linear adapter layer $\boldsymbol{W}$; (2) LoRA fine-tuning method fine-tunes the linear adapter layer $\boldsymbol{W}$ and the pre-trained $\operatorname{LLM} f_{\phi}(\cdot)$ with LoRA [54].

### 5.1. Experimental Setup and Overview

Given that LLaVA relies on visual and language instruction data for training and fine-tuning processes, we have manually reformatted several datasets, namely MNIST, CIFAR-10, CIFAR-100, and miniImagenet to comply with the required format for fine-tuning. For more detailed information on the format of the fine-tuning data used, as well as the specifics of the $\mathrm{LLaVA}$ fine-tuning process,[^3]please refer to Appendix D.1. All of our fine-tuning experiments were conducted using 2 Nvidia A100 GPUs. We fine-tune LLaVA-7b and LLaVA-13b using linear and LoRA [54] fine-tuning respectively, due to the limitation of computational resources, we cannot afford to fine-tune the entire LLaMA model. We first report the EMT evaluated accuracy of fine-tuned LLaVA-7b and LLaVA-13b after 3 epochs of linear and LoRA fine-tuning in Figure 5. To assess accuracy variations during training, we then report the EMT evaluation results from 1-3 fine-tuning epochs in Figure 6 and 7.

### 5.2. Excessive Fine-Tuning Causes Forgetting

We first present the 3-epoch fine-tuning results in Figure 5. While LLaVA's performance indeed improves on the fine-tuning dataset, Figure 5 unveils a critical issue of MLLM fine-tuning:

Fine-tuning MLLM on one dataset decreases the performance on another non-fine-tuning dataset.

This phenomenon, though not unexpected, is noteworthy. As the model doesn't have exposure to datasets other than the one it has been fine-tuned on, it stands to reason that a similar effect to catastrophic forgetting would be observed, as discussed previously in Section 4.1.

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-07.jpg?height=326&width=333&top_left_y=1016&top_left_x=362)

(a) $7 \mathrm{~b}$-linear

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-07.jpg?height=323&width=350&top_left_y=1020&top_left_x=714)

(b) $7 b-1 o r a$

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-07.jpg?height=323&width=352&top_left_y=1020&top_left_x=1060)

(c) 13b-linear

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-07.jpg?height=326&width=342&top_left_y=1016&top_left_x=1423)

(d) 13b-lora

Figure 5: EMT evaluation accuracy of 3-epoch fine-tuned LLaVA-7b and LLaVA-13b on MNIST, CIFAR-10, CIFAR-100, and miniImagenet, against the zero-shot performance of their vision encoders. Detailed accuracy numbers are presented in Table 9 of Appendix D.5.

As we examine the output from fine-tuned LLaVA, we discover that

Fine-tuning MLLM causes hallucinations, by outputting texts that are related to its fine-tuned dataset while ignoring the question related to its original prompt.

To further illustrate this phenomenon, we provide explicit examples of classifying the LLaVA-7b and LLaVA-13b, which have been fine-tuned on different datasets using the EMT prompt.

## EMT Prompt:

What is the object in the image? Please only answer a single object in airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.

| img | Label: airplane \| LLaVA-7b-lora-ft-cifar10 |
| :--- | :--- |
| The object is an airplane. |  |

The earlier demonstration illustrates that, when the CIFAR-10 fine-tuned model is tested on CIFAR10, LLaVA indeed successfully identifies the object. Nevertheless, the LLaVA model begins to hallucinate in CIFAR-10 classifications after being fine-tuned on other datasets.

![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-07.jpg?height=130&width=116&top_left_y=2361&top_left_x=305)

Label: airplane | LLaVA-7b-lora-ft-mnist

The airplane is 8 .

In the preceding example, the classification of CIFAR-10 through an MNIST fine-tuned model, the model not only partially generates the keyword "airplane", but concurrently produces hallucinated outputs by yielding the representation of the number " 8 ". Similar phenomena are also observed in the CIFAR-100 and miniImagenet fine-tuned models. Specifically, these fine-tuned models begin to hallucinate by predicting "airplane" as classes that bear resemblance or are related to an "airplane", such as "butterfly" and "aircraft carrier" in the CIFAR-100 and miniImagenet models, respectively.

| img | Label: airplane \| LLaVA-7b-lora-ft-cifar100 |
| :---: | :--- | :--- |
| The object is a(n) butterfly. |  |

```
img Label: airplane | LLaVA-7b-lora-ft-miniimagenet
The object is a(n) aircraft carrier.
```

For completeness, we attach additional outputs of different fine-tuned LLaVA models in Appendix D. 2 for further reference.

### 5.3. Moderate Fine-Tuning is Beneficial

In the preceding subsection, we have demonstrated that 3-epoch fine-tuned LLaVA achieves superior performance on the fine-tuned dataset, at the expense of generating hallucinated texts when tested on other datasets. However, this outcome does not necessarily imply that fine-tuning undermines the performance. Notably, we actually observe performance improvement on non-finetuned datasets. For instance, as shown in Figure 5, LLaVA-7b exhibits improved performance on miniImagenet after 3 epochs of fine-tuning on CIFAR-10. To better understand the generalizability in fine-tuning, we conduct fine-tuning experiments on all four datasets for 3 epochs and report their accuracy at each epoch.

Fine-Tuning Adapter Improves Feature Alignments. As illustrated in Figure 6, we observe that the linear fine-tuned LLaVA achieves generalization performance upon being fine-tuned on RGB datasets, namely, CIFAR-10, CIFAR-100, and miniImagenet. Given that linear fine-tuning only affects the linear projection layer connecting visual features to the text embedding space, Figure 6 implies that early-stage fine-tuning contributes to the enhancement of alignment between visual and textual features. However, in subsequent fine-tuning epochs (2-3), LLaVA starts to overfit the fine-tuning dataset by generating hallucinated texts.
![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-08.jpg?height=322&width=1486&top_left_y=1709&top_left_x=316)

Figure 6: EMT evaluation accuracy of 1-3 epoch linear fine-tuned LLaVA-7b on MNIST, CIFAR-10, CIFAR-100, and miniImagenet. Detailed accuracy numbers are presented in Table 10.

Fine-Tuning LLM and Adapter Causes Hallucinations. Contrary to the linear fine-tuning, Figure 7 implies that jointly fine-tuning both the LLM and the linear adapter directly causes overfitting on the fine-tuning dataset. This is evidenced by the significant degradation in the LoRA fine-tuned model's performance on the non-fine-tuning datasets after just a single epoch of training.

## 6. Conclusions

In this paper, we have studied how fine-tuning affects catastrophic forgetting in MLLMs. To quantitatively evaluate this issue, we propose EMT, a framework for evaluating the fine-tuning perfor-
![](https://cdn.mathpix.com/cropped/2024_06_04_3f49cf3167359480c225g-09.jpg?height=320&width=1504&top_left_y=230&top_left_x=300)

Figure 7: EMT evaluation accuracy of 1-3 epoch LoRA fine-tuned LLaVA-7b on MNIST, CIFAR-10, CIFAR-100, and miniImagenet. Detailed accuracy numbers are presented in Table 11.

mance of MLLMs. We then conduct extensive experiments in fine-tuning LLaVA, an MLLM, and apply EMT to evaluate the performance of different fine-tuned LLaVA models. We have discovered that: (1) Almost all the open-source MLLMs tested in this paper fail to achieve a similar level of accuracy, compared to the zero-shot performance of their base vision encoder; (2) After excessive fine-tuning on one dataset, LLaVA's performance on non-fine-tuning datasets deteriorate as it starts to overfit and hallucinate; (3) Moderate fine-tuning actually improves the performance of LLaVA on similar tasks, as fine-tuning helps visual and text feature alignment in the early-stage.

## 7. Discussions and Future Work

Dataset Diversity is Important for Fine-Tuning. Figure 6 shows that LLaVA fine-tuned on CIFAR10, CIFAR-100, and miniImagenet for one epoch, could generalize to the other two datasets, while fine-tuning LLaVA on MNIST leads to performance degradation on all remaining datasets. This observation implies that having a diverse fine-tuning dataset is important. This is because a more diverse dataset will have features of more modes, hence making the fine-tuned MLLMs suffer less from catastrophic forgetting.

Catastrophic Forgetting Beyond Image Classifications. As a starting point, we only study the catastrophic forgetting in MLLM from the image classification perspective, since it is a standard classification problem. In the future, we believe similar evaluation methods can be developed for other scenarios, such as reducing bias towards unsafe outputs [39], degrading visual localization reasoning capabilities [8], or even hallucinations [25].

Post-processing the Outputs. Note that in step (3) of EMT, using the openaiAPI is not the only solution for evaluating the correctness of the outputs generated by MLLMs. In the future, there are several solutions. (1) Utilize a sentence embedding model. $N$ formatted ground truth phrases can be fed into a sentence embedding model such as CLIP text encoding resulting in $N$ ground truth embedding $\left\{e_{i}\right\}$, where $i \in\{1, \cdots, N\}$. Given a generated text $y$ for a test sample, we can feed its CLIP text embedding $e(y)$ and compute the matching ground truth $i$ using arg $\min _{i}\left\|e_{i}-e(y)\right\|_{2}$. (2) One can also hard code (such as finding the existence of the label names) the decision criteria for dealing with hallucination. Note that finding a perfect post-processing method for EMT is not easy, as the labels from different datasets may have many synonyms. For example, when evaluating LLaVA on the label African_hunting_dog in miniImagenet, it is hard to determine whether a prediction of "dog" should be correct or not. Hence, we believe such confusion in synonyms should also be taken into consideration in the future when building post-processing methods.

## 8. Acknowledgement

We want to thank Sergey Levine from UC Berkeley and Carl Vondrick from Columbia University, for the early discussion during the preparation of this paper. We would also like to thank Haotian Liu from the University of Wisconsin-Madison for suggestions in setting up the LLaVA experiments. We would also like to thank Samuel Ainsworth and Yuning Chai from the Cruise AI research team for their insightful discussion and suggestions. YZ and YM acknowledge support from the joint Simons Foundation-NSF DMS grant\#2031899, the ONR grant N00014-22-1-2102, and the Tsinghua-Berkeley Shenzhen Institute (TBSI) Research Fund. Yi Ma also acknowledges support from the University of Hong Kong.

## References

[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 1,2

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 2

[3] OpenAI. Chatgpt, 2022. URL https://openai.com/blog/chatgpt.

[4] R OpenAI. Gpt-4 technical report. arXiv, pages 2303-08774, 2023. 1

[5] OpenAI. Gpt-4, 2023. URL https://openai.com/research/gpt-4. 1

[6] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 1, 3

[7] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1, 2, 3, 5, 6, 16, 18, 19

[8] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2,9

[9] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023. $1,2,5,16,19$

[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning, 2023. 1, 2, 3, 5, 16, 19

[11] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 1, 3, 4, 18, 19, 23

[12] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/ zenodo.5143773. 1, 4, 18, 19

[13] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 1, 6

[14] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3, 6

[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1, 3

[16] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong $\mathrm{Xu}$, and Enhong Chen. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. 1, 3

[17] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 1, 3, 4

[18] Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. Recall and learn: Fine-tuning deep pretrained language models with less forgetting. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 78707881, Online, November 2020. Association for Computational Linguistics. URL https://www . aclweb.org/anthology/2020.emnlp-main. 634. 1

[19] Xinshuai Dong, Anh Tuan Luu, Min Lin, Shuicheng Yan, and Hanwang Zhang. How should pre-trained language models be fine-tuned towards adversarial robustness? Advances in Neural Information Processing Systems, 34:4356-4369, 2021. 3

[20] Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of finetuning $\{$ bert\}: Misconceptions, explanations, and strong baselines. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=nzpLWnVAyah.

[21] Tomasz Korbak, Hady Elsahar, German Kruszewski, and Marc Dymetman. Controlling conditional language models without catastrophic forgetting. In International Conference on Machine Learning, pages 11499-11528. PMLR, 2022. 1, 3

[22] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013. 1

[23] Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, and Dacheng Tao. Neural collapse inspired feature-classifier alignment for few-shot class-incremental learning. In The Eleventh International Conference on Learning Representations, 2023. URL https : //openreview. net/forum?id=y5W8tpojhtJ. 1,3,14

[24] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433, 2015. 1

[25] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. 1, $2,3,4,9$

[26] William Berrios, Gautam Mittal, Tristan Thrush, Douwe Kiela, and Amanpreet Singh. Towards language models that can see: Computer vision through the lens of natural language. arXiv preprint arXiv:2306.16410, 2023. 2, 3, 5, 19

[27] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023. 2, 5

[28] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023. 2, 4

[29] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35: 2507-2521, 2022. 2,3,18

[30] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 2

[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[32] Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, et al. Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models. arXiv preprint arXiv:2309.06256, 2023. 2

[33] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109-165. Elsevier, 1989. 3

[34] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018. 3

[35] Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to finetune large-scale pretrained language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HkgaETNtDB. 3

[36] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. Revisiting fewsample \{bert\} fine-tuning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=c01IH43yUF. 3

[37] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 3

[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. 3

[39] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. URL https: //doi.org/10.5281/zenodo.7733589. 3,9,19

[40] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zeroinit attention. arXiv preprint arXiv:2303.16199, 2023.

[41] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023.

[42] Mu Cai, Zeyi Huang, Yuheng Li, Haohan Wang, and Yong Jae Lee. Leveraging large language models for scalable vector graphics-driven image understanding. arXiv preprint arXiv:2306.06094, 2023. 3

[43] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 3

[44] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. arXiv preprint arXiv:2307.12981, 2023. 3

[45] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40): 24652-24663, 2020. 3, 4, 14

[46] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. Advances in Neural Information Processing Systems, 34:29820-29834, 2021. 3, 4, 15

[47] Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Exploring deep neural networks via layerpeeled model: Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences, 118(43):e2103091118, 2021. 3, 4, 14, 15

[48] Christos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina Behnia. Imbalance trouble: Revisiting neural-collapse geometry. Advances in Neural Information Processing Systems, 35:27225-27238, 2022. 3, 4, 14

[49] Tina Behnia, Ganesh Ramachandra Kini, Vala Vakilian, and Christos Thrampoulidis. On the implicit geometry of cross-entropy parameterizations for label-imbalanced data. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop), 2022. URL https : / /openreview. net/forum?id=1piyfD_ictW.

[50] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding imbalanced semantic segmentation through neural collapse. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19550-19560, 2023. 3,14

[51] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $770-778,2016.3,15,16$

[52] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023. 4

[53] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. 5

[54] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 6,7

[55] Yibo Yang, Shixiang Chen, Xiangtai Li, Liang Xie, Zhouchen Lin, and Dacheng Tao. Inducing neural collapse in imbalanced learning: Do we really need a learnable classifier at the end of deep neural network? Advances in Neural Information Processing Systems, 35:37991-38002, 2022. 14

[56] Liang Xie, Yibo Yang, Deng Cai, and Xiaofei He. Neural collapse inspired attraction-repulsionbalanced loss for imbalanced learning. Neurocomputing, 527:60-70, 2023. 14

[57] Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. arXiv preprint arXiv:2011.11619, 2020. 14

[58] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597-1607. PMLR, 2020. 15

[59] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 17

[60] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018. 18

[61] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278-25294, 2022. 19
