# Multi-Reference Preference Optimization for Large Language Models 

Hung Le $^{1} \quad$ Quan Tran $^{2} \quad$ Dung Nguyen $^{1} \quad$ Kien Do $^{1} \quad$ Saloni Mittal $^{2}$<br>Kelechi Ogueji $^{2}$ Svetha Venkatesh ${ }^{1}$<br>${ }^{1}$ Applied AI Institute, Deakin University, Geelong, Australia<br>${ }^{1}$ \{thai.le, dung.nguyen,k.do, svetha.venkatesh\}@deakin.edu. au<br>${ }^{2}$ ServiceNow Research, USA<br>${ }^{2}$ \{hungquan.tran, saloni.mittal, kelechi.ogueji\}@servicenow.com


#### Abstract

How can Large Language Models (LLMs) be aligned with human intentions and values? A typical solution is to gather human preference on model outputs and finetune the LLMs accordingly while ensuring that updates do not deviate too far from a reference model. Recent approaches, such as direct preference optimization (DPO), have eliminated the need for unstable and sluggish reinforcement learning optimization by introducing close-formed supervised losses. However, a significant limitation of the current approach is its design for a single reference model only, neglecting to leverage the collective power of numerous pretrained LLMs. To overcome this limitation, we introduce a novel closed-form formulation for direct preference optimization using multiple reference models. The resulting algorithm, Multi-Reference Preference Optimization (MRPO), leverages broader prior knowledge from diverse reference models, substantially enhancing preference learning capabilities compared to the single-reference DPO. Our experiments demonstrate that LLMs finetuned with MRPO generalize better in various preference data, regardless of data scarcity or abundance. Furthermore, MRPO effectively finetunes LLMs to exhibit superior performance in several downstream natural language processing tasks such as GSM8K and TruthfulQA.


## 1 Introduction

Large Language Models (LLMs) have emerged as powerful tools in natural language processing, capable of generating human-like text and performing a myriad of language-related tasks [Lewkowycz et al., 2022, Achiam et al. 2023. Touvron et al., 2023]. However, aligning these models with human intentions and values remains a challenging endeavor [Wang et al., 2023]. Aligning LLMs with curated human feedback emerges as a critical solution to guide LLM response behavior and address this challenge. Preference models like the Bradley-Terry model [Bradley and Terry, 1952] are often used to measure the alignment of reward functions with empirical preference data, facilitating an alignment framework using reinforcement learning with human feedback (RLHF [Christiano et al., 2017]). The framework aims to optimize the preference models (maximizing preference reward) while ensuring that LLM updates do not stray too far from a base reference LLM model (minimizing a Kullback-Leibler (KL) divergence). While RLHF has been successful in enhancing the helpfulness and accuracy of model-generated content [Ouyang et al., 2022, Stiennon et al., 2020], it is unstable, complicated, and resource-intensive.

Recent advancements, such as direct preference optimization (DPO [Rafailov et al., 2023|) and other likelihood-based preference learning [Zhao et al., 2023, Azar et al., 2023, Ethayarajh et al., 2024,

Chen et al. 2024|, have sought to replace the cumbersome RLHF with closed-form supervised losses. Although they have demonstrated impressive performance compared to RLHF and supervised finetuning (SFT), their exclusive reliance on a single reference model restricts their potential, overlooking the advantages of harnessing multiple pretrained LLMs. Incorporating multiple LLMs to constrain the update of the optimized LLM towards a preference dataset results in a model that reflects the characteristics of all reference models while satisfying human preference. This is increasingly important as the open-source community consistently introduces new pretrained/SFT LLMs of varying scales, trained on diverse datasets [Touvron et al., 2023, Penedo et al., 2023, Jiang et al., 2023]. It underscores the necessity for a solution that employs multiple references for LLM finetuning, enabling the distillation of knowledge from existing LLMs to enhance the alignment training stage. Unfortunately, none of the prior works have proposed a solution for utilizing multiple reference LLMs in direct preference optimization.

The absence of such solutions stems from three challenges in formulating closed-form multiplereference preference learning. Firstly, deriving a closed-form solution for the RLHF objective with multiple referencing constraints is nontrivial due to the non-linearity of multiple KL terms. Secondly, reference models with varying architecture, size, and pretraining data may produce diverging outputs given the same input. This divergence could potentially confuse the learning process, leading to unstable training. Thirdly, determining the contribution of each reference model during training poses a challenge, requiring extensive tuning. In this paper, we tackle these three challenges, presenting a simple and viable framework for direct preference optimization utilizing multiple reference models.

To address the non-linearity of KL divergence, we propose maximizing a simpler surrogate lower bound that allows for the derivation of a novel closed-form solution incorporating multiple reference models. Our solution is theoretically and empirically proven superior to combining multiple DPO losses. Next, we propose a clipped trust-regions optimization (CTRO) to address the second challenge By clipping the log probability of diverging reference policy, we force the mismatch to be minimal to facilitate stable training while retaining useful information from the reference policy to guide the optimization. More importantly, the clipping rate is dynamically adjusted according to the predicted likelihood of the data, enabling a more adaptable update. Lastly, to automate the process of determining the contribution of each reference model, we introduce a dynamic mechanism (ARWC) to calculate the weight of each KL term based on the confidence of the referencing LLMs.

Our holistic framework, dubbed Multiple Reference Preference Optimization (MRPO), undergoes throughout evaluation across various tasks. In preference learning tasks involving 6 preference datasets, MRPO demonstrates significant superiority over DPO and a naive combination of multiple DPO losses, especially when preference data is limited with improvement of up to $7 \%$. Furthermore, on general language understanding benchmarks like the HuggingFace Open LLM Leaderboard Beeching et al., 2023], MRPO exhibits average enhancements of $3-4 \%$ compared to SFT and $1.2 \%$ compared to DPO. Certain tasks show more than $5 \%$ improvements over DPO. Importantly, these enhancements are evident across various configurations, including different numbers of reference models ( 2 or 3 ) and sizes of LLMs (1B or 7B). Finally, we perform a comprehensive ablation study to demonstrate the efficacy of CTRO and ARWC mechanisms.

## 2 Background

### 2.1 Problem Formulation and Notations

We rely on [Azar et al. 2023] to formally define the problem and notations. Given an input $x \in \mathcal{X}$ where $\mathcal{X}$ is the finite space of input texts, a policy $\pi$ models a conditional probability distribution $\pi(y \mid x)$ where $y \in \mathcal{Y}$ is the output in the finite space of output texts. From a given $\pi$ and $x$, we can sample an output as $y \sim \pi(\cdot \mid x)$. Preference data is generated by sampling two outputs $\left(y, y^{\prime} \mid x\right)$ from policies $\pi$ and $\mu$ and presenting them to an agent, normally a human, for rating to indicate which one is preferred. For example, $y \succ y^{\prime}$ denotes $y$ is preferred to $y^{\prime}$. A preference dataset is then denoted as $\mathcal{D}=\left\{y_{w}^{i}, y_{l}^{i} \mid x^{i}\right\}_{i=1}^{N}$ where $N$ is the number of data points, $y_{w}$ and $y_{l}$ denote the preferred (chosen) and dispreferred (rejected), respectively. Assuming that there exists a true model of preference of the agent $p^{*}\left(y \succ y^{\prime} \mid x\right)$ that assigns the agent's probability of $y$ being preferred to $y^{\prime}$ given $x$. Using dataset $\mathcal{D}$, our goal is to find a policy $\pi$ maximizing the expected preference while being close to a
reference policy $\pi_{r e f}$, which results in the following optimization problem:

![](https://cdn.mathpix.com/cropped/2024_05_29_cd7f65295c92cba32a7ag-03.jpg?height=142&width=756&top_left_y=303&top_left_x=679)

where $\rho$ is the input distribution, $\Psi$ is a scaled function, $D_{K L}$ is the Kullback-Leibler divergence and $\beta$ is a hyperparameter. Usually, $\pi$ is initialized as $\pi_{r e f}$ for stable optimization.

### 2.2 Preference Learning with Reward Function and Reinforcement Learning

In this approach, Bradley-Terry model [Bradley and Terry, 1952] is employed as the preference model:

$$
\begin{equation*}
p\left(y \succ y^{\prime} \mid x\right)=\sigma\left(r_{\theta}(x, y)-r_{\theta}\left(x, y^{\prime}\right)\right) \tag{2}
\end{equation*}
$$

where $\sigma$ denotes the sigmoid function and $r: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}$ is a reward model parameterized by $\theta$, which assigns a scalar score to indicate the suitability of output $y$ for input $x$. In earlier works [Christiano et al. 2017], the reward model is trained on $\mathcal{D}$ to minimize the negative log-likelihood loss:

$$
\begin{equation*}
\mathcal{L}_{R}=-\mathbb{E}_{x, y_{w}, y_{l} \sim \mathcal{D}}\left[\log \sigma\left(r_{\theta}\left(x, y_{w}\right)-r_{\theta}\left(x, y_{l}\right)\right)\right] \tag{3}
\end{equation*}
$$

Given a trained reward model $r$, and the scaled function as $\Psi(q)=\log \left(\frac{q}{1-q}\right) \forall q: 0<q<1$, the objective in Eq. 1 can be rewritten as:

$$
\begin{equation*}
\max _{\pi} \underset{\substack{x \sim \rho \\ y \sim \pi(\cdot \mid x)}}{\mathbb{E}}[r(x, y)]-\beta D_{K L}\left(\pi \| \pi_{r e f}\right) \tag{4}
\end{equation*}
$$

This RLHF objective is employed to train LLMs such as Instruct-GPT [Ouyang et al., 2022] using PPO [Schulman et al., 2017].

### 2.3 Direct Preference Optimization

Reward training and RL finetuning require significant resources and can be cumbersome. Recent approaches circumvent these challenges by directly optimizing the policy via minimizing a preferencebased negative log-likelihood loss [Rafailov et al., 2023]:

$$
\begin{equation*}
\mathcal{L}_{D P O}=-\mathbb{E}_{x, y_{w}, y_{l} \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{r e f}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{r e f}\left(y_{l} \mid x\right)}\right)\right] \tag{5}
\end{equation*}
$$

The term $r_{\theta}\left(x, y \mid \pi_{r e f}\right)=\beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{r e f}(y \mid x)}$ plays the role of an implicit reward. The authors in Rafailov et al. 2023] proved that minimizing this loss is equivalent to solving the optimization problem in Eq. 4

## 3 Method

### 3.1 Multi-Reference Preference Optimization

In this paper, we are focused on situations involving $K$ reference policies $\left\{\pi_{r e f}^{k}\right\}_{k=1}^{K}$. Therefore, extending from Eq. 4, our objective can be formulated as a multi-reference RLHF objective:

$$
\begin{equation*}
\max _{\pi} \underset{\substack{x \sim \rho \\ y \sim \pi(\cdot \mid x)}}{\mathbb{E}}[r(x, y)]-\beta\left(\sum_{k=1}^{K} \alpha_{k} D_{K L}\left(\pi \| \pi_{r e f}^{k}\right)\right) \tag{6}
\end{equation*}
$$

where $\alpha_{k}$ are weighting coefficients for each reference policy and $1=\sum_{k=1}^{K} \alpha_{k}$. Without loss of generality, we denote $\pi_{r e f}^{1}$ as the initializing reference policy for the main policy $\pi_{\theta}$. This objective was explored in previous studies, leading to enhancements in pure RL problems [Le et al. 2022].

However, addressing this optimization problem in LLMs through reward learning and RL finetuning poses similar challenges to Eq. 4. Hence, we propose an alternative approach that leverages direct
preference optimization for the scenario involving multiple reference policies. We aim to find a closed-form solution for the multi-reference RLHF objective in Eq. 6 Unfortunately, deriving an exact closed-form solution is challenging due to the nonlinearity of $D_{K L}$ terms. To circumvent this, we suggest obtaining a closed-form solution for a surrogate objective, which serves as a lower bound for the multi-reference RLHF objective. We summarize our findings as a proposition below.

Proposition 1. The following policy is the optimum for a lower bound of the RLHF objective (Eq. 6):

$$
\pi^{*}(y \mid x)=\frac{1}{Z(x)} \tilde{\pi}_{r e f}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)
$$

where $\tilde{\pi}_{r e f}(y \mid x)=\left(\sum_{k=1}^{K} \frac{\alpha_{k}}{\pi_{r e f}^{k}(y \mid x)}\right)^{-1}$ and $Z(x)=\sum_{y} \tilde{\pi}_{r e f}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)$.

Proof. See Appendix A. 1

Following the derivation in [Rafailov et al. 2023] with our proposed optimal policy $\pi^{*}$, we have the associated direct preference loss function as follow,

$$
\begin{equation*}
\mathcal{L}_{M R P O}=-\mathbb{E}_{x, y_{w}, y_{l} \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\tilde{\pi}_{r e f}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\tilde{\pi}_{r e f}\left(y_{l} \mid x\right)}\right)\right] \tag{7}
\end{equation*}
$$

The loss function is similar to the DPO loss (Eq. 5), but instead of using a single reference policy $\pi_{r e f}$, we substitute it with a "virtual" reference policy $\tilde{\pi}_{r e f}$ that aggregates information from all multiple reference policies.

### 3.2 Clipped Trust-Regions Optimization (CTRO)

An issue that may arise when multiple reference policies are involved in the optimization process is the mismatch between the reference policy and the main policy. This is less common with singlereference DPO, as the main policy is initialized as the reference policy, ensuring a small mismatch across training. However, with multiple-reference policies, those not chosen to initialize the main policy can result in significantly different probabilities compared to the main one, potentially leading to unstable training and, at times, loss divergence.

To address this issue, we propose to constrain the virtual reference policy $\tilde{\pi}_{r e f}$ in the vicinity of the initializing reference policy $\pi_{r e f}^{1}$. As such, we propose to clip the log-probability of the other reference policies $\pi_{r e f}^{k>1}$ as follows,

$$
\begin{equation*}
\log \hat{\pi}_{r e f}^{k>1}(y \mid x)=\min \left(\max \left(\log \pi_{r e f}^{k>1}(y \mid x),(1+\epsilon) \log \pi_{r e f}^{1}(y \mid x)\right),(1-\epsilon) \log \pi_{r e f}^{1}(y \mid x)\right) \tag{8}
\end{equation*}
$$

where $\epsilon$ defines the vicinity range around $\pi_{r e f}^{1}$. Then we will replace $\pi_{r e f}^{k>1}$ with $\hat{\pi}_{r e f}^{k>1}$ in the $\tilde{\pi}_{r e f}$ defined in Proposition 1

Using a fixed $\epsilon$ is overly restrictive and can be suboptimal since different data and policies may require different trust-region ranges. Thus, we suggest an adaptive approach to define $\epsilon$ based on the predicted likelihood of the data. Essentially, if the $\log$ probability of a reference model for a given data point is large, indicating high reliability, a conservative update should be applied (smaller $\epsilon$ ) to exploit the reference policy. Conversely, for lower log probabilities, we may prefer more exploration, allowing reference values to diverge from the initial $\pi_{r e f}^{1}$ (bigger $\epsilon$ ). That is,

$$
\begin{equation*}
\epsilon(y \mid x)=\epsilon_{\max } \frac{\left|\sum_{k=1}^{K} \log \pi_{r e f}^{k}(y \mid x)\right|}{\sum_{y^{\prime}}\left|\sum_{k=1}^{K} \log \pi_{r e f}^{k}\left(y^{\prime} \mid x\right)\right|} \tag{9}
\end{equation*}
$$

where $\epsilon_{\max }$ is a hyperparameter specifying the maximum ratio for an updating range. It is worth noting that since $0 \leq \pi(\cdot) \leq 1$, the log-probability is always negative, meaning a larger absolute value of the log-probability indicates less confidence. The denominator represents the sum of all possible output $y^{\prime}$, serving as a normalization factor. In practice, in preference learning, we only have two outputs $\left(y_{w}, y_{l}\right)$ per input $x$ so the denominator is the sum of two terms.

### 3.3 Adaptive Reference Weighting Coefficients (ARWC)

If we have no preference or prior knowledge of the reference policies, we can simply use $\alpha_{k}=$ $1 / K \forall k$. However, if we assume that the reference policy obtains a reasonable ability to differentiate between $y_{w}$ and $y_{l}$ such that even when it make wrong preference (i.e $\log y_{l}>\log y_{w}$ ) the likelihood difference should not be too large, we can introduce an automatic mechanism to determine the value of $\alpha_{k}$ based on the confidence of the reference policy. Specifically, we examine the absolute difference between the log-probability of the two outputs $\left(y_{w}, y_{l}\right)$ as an indicator of the policy's confidence in its ability to discriminate between two outputs. In essence, a larger difference suggests that the policy distinguishes one output from another more decisively. Formally, we propose to adaptively compute reference weighting coefficients as:

$$
\begin{equation*}
\alpha_{k}=\frac{\left|\log \pi_{r e f}^{k}\left(y_{w} \mid x\right)-\log \pi_{r e f}^{k}\left(y_{l} \mid x\right)\right|}{\sum_{i=1}^{K}\left|\log \pi_{r e f}^{i}\left(y_{w} \mid x\right)-\log \pi_{r e f}^{i}\left(y_{l} \mid x\right)\right|} \tag{10}
\end{equation*}
$$

The coefficient is normalized across reference policies, giving greater weight to those with higher discriminative confidence.

### 3.4 Comparison with Multiple DPO

When having multiple reference policies, a naive solution for direct preference learning is to combine multiple DPO losses (Multi-DPO):

$$
\begin{equation*}
\mathcal{L}_{M u l t i-D P O}=-\mathbb{E}_{x, y_{w}, y_{l} \sim \mathcal{D}}\left[\sum_{k=1}^{K} \alpha_{k} \log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{r e f}^{k}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{r e f}^{k}\left(y_{l} \mid x\right)}\right)\right] \tag{11}
\end{equation*}
$$

We can show that our proposed MRPO is more desirable than this MDPO loss. To see that, we analyze the gradient wrt. the policy parameters $\theta$ of the two losses: (1) $\nabla_{\theta} \mathcal{L}_{M R P O}=$ $-\beta \mathbb{E}_{x, y_{w}, y_{l} \sim \mathcal{D}}\left[\sigma\left(r_{\theta}\left(x, y_{l} \mid \tilde{\pi}_{r e f}\right)-r_{\theta}\left(x, y_{w} \mid \tilde{\pi}_{r e f}\right)\right) \times\left(\nabla_{\theta} \log \pi_{\theta}\left(y_{w} \mid x\right)-\nabla_{\theta} \log \pi_{\theta}\left(y_{l} \mid x\right)\right)\right]$, and (2) $\nabla_{\theta} \mathcal{L}_{\text {Multi-DPO }}=-\beta \mathbb{E}_{x, y_{w}, y_{l} \sim \mathcal{D}}\left[\alpha_{k} \sum_{k=1}^{K} \sigma\left(r_{\theta}\left(x, y_{l} \mid \pi_{r e f}^{k}\right)-r_{\theta}\left(x, y_{w} \mid \pi_{r e f}^{k}\right)\right) \times\right.$ $\left.\left(\nabla_{\theta} \log \pi_{\theta}\left(y_{w} \mid x\right)-\nabla_{\theta} \log \pi_{\theta}\left(y_{l} \mid x\right)\right)\right]$.

The gradients mean that the rate of increasing/decreasing the likelihood of preferred/dispreferred outputs is scaled by the reward error $\sigma\left(r_{\theta}\left(x, y_{l} \mid \tilde{\pi}_{r e f}\right)-r_{\theta}\left(x, y_{w} \mid \tilde{\pi}_{r e f}\right)\right)$ or $\sum_{k=1}^{K} \alpha_{k} \sigma\left(r_{\theta}\left(x, y_{l} \mid \pi_{r e f}^{k}\right)-r_{\theta}\left(x, y_{w} \mid \pi_{r e f}^{k}\right)\right)$, corresponding to MRPO and Multi-DPO, respectively. Under mild assumptions, we can have the following result:

Proposition 2. Assume that reference policies are constrained to be relatively close to each other, ensuring that $\left\{d_{k}=r_{\theta}\left(x, y_{l} \mid \pi_{r e f}^{k}\right)-r_{\theta}\left(x, y_{w} \mid \pi_{r e f}^{k}\right)\right\}_{k=1}^{K}$ share the same sign $\forall k$, then

$\left\{\begin{array}{l}\sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\tilde{\pi}_{r e f}\left(y_{l} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\tilde{\pi}_{r e f}\left(y_{w} \mid x\right)}\right) \quad \geq \sum_{k=1}^{K} \alpha_{k} \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{r e}^{k}\left(y_{l} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{r e f}^{k}\left(y_{w} \mid x\right)}\right) \quad \forall d_{k} \geq 0 \\ \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{r e f}\left(y_{l} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{r e f}\left(y_{w} \mid x\right)}\right) \quad \leq \sum_{k=1}^{K} \alpha_{k} \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{r e f}^{k}\left(y_{l} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{r e f}^{k}\left(y_{w} \mid x\right)}\right) \quad \forall d_{k} \leq 0\end{array}\right.$

Proof. See Appendix A. 2

As a result, when the reward estimations are wrong, i.e., $\forall d_{k}>0$, MRPO update will have a higher rate of following the likelihood gradient than Multi-DPO, which is desirable because we want to fix the likelihood faster to correct the implicit reward. On the contrary, when the reward estimations are right, i.e., $\forall d_{k}<0$, MRPO update will have a lower rate of following the likelihood gradient than Multi-DPO, which may help reduce over-fitting and stabilize the convergence.

## 4 Experimental Results

In our experiments, we will always refer to the first (initializing) reference model as RefM1, the second as RefM2, and so forth. The Base model is the original LLM that will undergo finetuning

| Dataset | S1 |  | S2 |  | S3 |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathrm{L} \leftarrow \mathrm{M}$ | $\mathrm{M} \leftarrow \mathrm{L}$ | $\mathrm{L} \leftarrow \mathrm{M}$ | $\mathrm{M} \leftarrow \mathrm{L}$ | $\mathrm{L} \leftarrow \mathrm{M}$ | $\mathrm{M} \leftarrow \mathrm{L}$ |
| DPO | $93.9 \pm 1.4$ | $98.7 \pm 0.7$ | $94.4 \pm 2.9$ | $96.7 \pm 1.8$ | $54.8 \pm 5.4$ | $52.2 \pm 3.7$ |
| Multi-DPO | $95.6 \pm 1.7$ | $98.0 \pm 0.8$ | $95.5 \pm 1.2$ | $96.3 \pm 1.2$ | $45.6 \pm 4.4$ | $51.3 \pm 1.2$ |
| MRPO (Ours) | $\mathbf{9 7 . 2} \pm \mathbf{1 . 7}$ | $99.5 \pm 0.8$ | $97.0 \pm 3.2$ | $97.0 \pm 1.6$ | $\mathbf{6 1 . 3} \pm 5.2$ | $56.0 \pm 1.9$ |

Table 1: Final mean $\pm$ std. testing accuracy $(\times 100$ ) on small datasets over 5 runs. Bold denotes the best, statistically different from the others as Cohen effect size $>0.5$. Italic denotes the second best.

on preference data and is initialized as RefM1. Throughout experiments, if not stated otherwise, Llama (L), Mistral (M), and Qwen (Q) refer to Llama-2-7b-chat-hf, OpenHermes-2.5-Mistral-7B, and Qwen 1.5-7B-Chat, respectively. Unless specified otherwise, we finetune these LLMs using LoRA 4-bit quantization to enable faster training and accommodate our hardware of a single Tesla A100 GPU with 32GB of memory. Further training details are provided in Appendix B. 1

To assess model performance, we finetune the model with preference data and evaluate it on preference learning and general language understanding (GLU) tasks. In preference learning, we measure the Preference Accuracy in predicting whether two responses, $y_{1} \mid x$ and $y_{2} \mid x$ as chosen (preferred) or rejected (dispreferred). In particular, following Rafailov et al. (2023), for DPO and Multi-DPO, we use $\beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{r e f}(y \mid x)}$ as the reward $r(x, y)$ for each response, with the response classified as chosen or rejected if it has a higher or lower reward, respectively. As for MRPO, we use $\beta \log \frac{\pi_{\theta}(y \mid x)}{\bar{\pi}_{r e f}(y \mid x)}$ as the reward. Another preference metric worth considering is the Reward Margin, which quantifies the difference between the chosen and rejected rewards: $r\left(x, y_{w}\right)-r\left(x, y_{l}\right)$. In GLU, we employ the finetuned LLM to generate output for each input and use the GLU Metric provided by the task. All of these measurements are desirable when they are higher.

### 4.1 Performance when Preference Data is Scarce

Datasets In many real-world scenarios, human feedback is limited. Here, we curate 3 small preference datasets (hundreds to a few thousand data points) to simulate the scarcity of feedback data. Each training dataset comprises a random subset from a larger public preference dataset available in the Hugging Face data repository. The remaining portions of the dataset will be utilized as testing data. These datasets are generated with inputs, outputs, and preference rankings often produced by powerful LLMs like GPT4, making them suitable for training smaller LLMs such as Llama and Mistral. The datasets are labeled as S1, S2, and S3, and their details are given in Appendix Table 6 and Appendix B. 2

Baselines Unless stated otherwise, we always compare our MRPO with DPO and Multi-DPO using the same common hyperparameters such as learning rate $\left(10^{-5}\right.$ ), batch size (8), number of epochs (3), and $\beta=0.1$. For Multi-DPO, we have to use clipped trust regions to ensure RefM2 is close to RefM1. Otherwise, the learning will not converge. To make fair comparison, both MRPO and Multi-DPO use $\epsilon_{\max }=0.1$ and incorporate the adaptive $\epsilon$ (\$3.2) and $\alpha$ (\$3.3) mechanisms. For Multi-DPO and MRPO, we consider 2 reference models ( $K=2$ ), and examine 2 modes of initialization: (1) $\mathrm{L} \leftarrow \mathrm{M}$, the Base model is initialized as Llama (RefM1) for all baselines, and the RefM2 is Mistral for MRPO and Multi-DPO; (2) $\mathrm{M} \leftarrow \mathrm{L}$, the order is reversed.

Results Table 1 reports the final preference accuracy on test sets. MRPO consistently outperforms DPO by a significant margin, 3-7\% and 1-4\% for $\mathrm{L} \leftarrow \mathrm{M}$ and $\mathrm{M} \leftarrow \mathrm{L}$, respectively. Mode $\mathrm{L} \leftarrow$ M observes more improvement because Mistral is stronger than Llama in these tasks and thus, using Mistral as RefM2 will bring more benefits than using Llama. On the other hand, Multi-DPO underperforms DPO and MRPO in many cases, indicating that combining multiple references in a naive way, even when equipped with our CTRO and ARWC, does not yield favorable results.

Appendix Fig. 1 depicts the testing preference accuracy curves over the training duration for all methods across 2 initialization modes. These curves demonstrate that all methods boost the chosen/rejection prediction accuracy of the Base model (the first evaluation point in each graph is lower than the following points). Among all, MRPO exhibits early outperformance compared to other baselines and maintains its superior performance until convergence.

| Metric | Dataset | HelpSteer |  | Ultrafeedback |  | Nectar |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\mathrm{L} \leftarrow \mathrm{M}$ | $\mathrm{M} \leftarrow \mathrm{L}$ | $\mathrm{L} \leftarrow \mathrm{M}$ | $\mathrm{M} \leftarrow \mathrm{L}$ | $\mathrm{L} \leftarrow \mathrm{M}$ | $\mathrm{M} \leftarrow \mathrm{L}$ |
| Accuracy | DPO | $68.9 \pm 0.4$ | $70.8 \pm 4.3$ | $69.8 \pm 0.9$ | $72.0 \pm 1.1$ | $75.6 \pm 3.9$ | $78.5 \pm 3.6$ |
|  | MRPO | $\mathbf{7 3 . 6} \pm \mathbf{1 . 6}$ | $71.6 \pm 5.2$ | $\mathbf{7 2 . 9} \pm \mathbf{2 . 9}$ | $\mathbf{7 3 . 2} \pm \mathbf{1 . 9}$ | $\mathbf{7 9 . 2} \pm \mathbf{1 . 2}$ | $78.7 \pm 3.0$ |
| Margin | DPO | $0.64 \pm 0.01$ | $0.95 \pm 0.20$ | $0.70 \pm 0.07$ | $1.14 \pm 0.12$ | $1.52 \pm 0.08$ | $2.65 \pm 0.29$ |
|  | MRPO | $\mathbf{0 . 7 7} \pm \mathbf{0 . 0 7}$ | $\mathbf{1 . 0 5} \pm \mathbf{0 . 2 0}$ | $\mathbf{0 . 8 2} \pm \mathbf{0 . 1 0}$ | $\mathbf{1 . 2 7} \pm \mathbf{0 . 2 6}$ | $\mathbf{1 . 7 3} \pm \mathbf{0 . 0 5}$ | $\mathbf{3 . 1 3} \pm \mathbf{0 . 2 5}$ |

Table 2: Final mean $\pm$ std. testing preference accuracy (upper, $\times 100$ ) and reward margin (lower) on big datasets over 3 runs. Bold is best, statistically different from others as Cohen effect size $>0.5$.

### 4.2 Can MRPO Scale to Big Preference Datasets?

Datasets To assess the scalability of MRPO to real and large datasets, we utilize three big preference datasets: HelpSteer, Ultrafeedback, and Nectar (see Appendix Table 7). Each dataset employs human rankings to assess the outputs generated by powerful LLMs. Finetuning LLMs for just one epoch is sufficient for large datasets to achieve learning convergence. We use the provided train/test split for HelpSteer and Ultrafeedback. We randomly allocate $90 \%$ of the data for training purposes, reserving the remaining $10 \%$ for testing for Nectar.

Baselines and Results In this task, we evaluated MRPO $(K=2)$ against DPO, the top two methods from our prior tests, using the same initialization approaches detailed earlier. The preference accuracy result, reported in Table 2.s upper row and Appendix Fig. 2, demonstrates that MRPO consistently surpasses DPO in real-world preference datasets, showing an improvement gain of approximately $3-5 \%$ and up to $1 \%$ for $\mathrm{L} \leftarrow \mathrm{M}$ and $\mathrm{M} \leftarrow \mathrm{L}$ modes, respectively. Since Preference Accuracy can sometimes be unclear in demonstrating performance, especially with borderline inputs where the chosen and rejected ground truth may not be entirely accurate, we also examine the Reward Margin of DPO and MRPO on these datasets. The result, displayed in Table 2 s lower row and Appendix Fig. 3. demonstrates MRPO's superior ability to separate chosen and rejected outputs, as evidenced by a significantly higher Reward Margin of $10-20 \%$ compared to DPO. The findings confirm MRPO's capability to effectively scale with large datasets for preference learning tasks.

### 4.3 How Effective is MRPO on General Language Understanding Benchmarks?

For our evaluation benchmark, we utilized the Huggingface Open LLM Leaderboard, a standard in the field [Beeching et al. 2023]. In this benchmark, we explore a variety of datasets, collectively covering tasks such as math (GSM8k) multi-task language understanding (MMLU), human falsehood understanding (TruthfulQA), and commonsense reasoning (Arc, HellaSwag, Winogrande). The evaluation process presents the language models with few-shot in-context examples and questions. We apply the standard evaluation protocol to evaluate and report average scores (GLU Metrics) across all datasets.

In this experiment, we consider the third reference model RefM3 as Qwen to verify the scalability of our method to $K=3$ on the standard GLU benchmark. We examine the following initialization modes: (1) $\mathrm{L} \leftarrow \mathrm{M}$, Q where Mistral and Qwen are additional reference models for Base model Llama, (2) $\mathrm{M} \leftarrow \mathrm{L}$, Q where Llama and Qwen are additional reference models for Base model Mistral, and (3) $\mathrm{Q} \leftarrow \mathrm{M}$, L where Mistral and Llama are additional reference models for Base model Qwen. Following prior practices [Chen et al., 2024], we adopt full finetuning to finetune the Base models on Ultrafeedback dataset using DPO and MRPO and evaluate the LLMs using Language Model Evaluation Harness library [Gao et al.]. We report all results in Table 3. MRPO leads to a notable enhancement in the Base model of $3.5 \%, 1.4 \%$, and $0.8 \%$, surpassing DPO with an average improvement of $1.1 \%, 1.0 \%$ and $1.3 \%$ for initialization (1), (2) and (3), respectively. Notably, there are several cases in which MRPO can outperform DPO by a huge margin, such as $6.8 \%$ in GSM8K $(\mathrm{M} \leftarrow \mathrm{L}, \mathrm{Q}), 5.8 \%$ in TruthfulQA $(\mathrm{L} \leftarrow \mathrm{M}, \mathrm{Q})$ and $5 \%$ in GSM8K $(\mathrm{Q} \leftarrow \mathrm{M}, \mathrm{L})$.

We also explore MRPO ( $K=2, \mathrm{~L} \leftarrow \mathrm{M}$ and $\mathrm{M} \leftarrow \mathrm{L})$ and observe that this variant consistently surpasses DPO in performance, albeit falling short of MRPO $(K=3)$, suggesting the advantages of incorporating more reference models. Full results can be found in Appendix Table 8

| Dataset | GSM8K | TruthfulQA | HellaSwag | MMLU | Arc-easy | Winograde | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Base (Mistral) | 49.6 | 44.5 | 62.8 | 60.8 | 83.5 | $\mathbf{7 4 . 4}$ | 62.6 |
| DPO | 53.7 | $\mathbf{5 3 . 3}$ | $\mathbf{6 6 . 6}$ | 60.1 | 82.7 | 73.6 | 65.0 |
| MRPO $($ M $\leftarrow$ L,Q) | $\mathbf{6 0 . 5}$ | 51.43 | 65.83 | $\mathbf{6 1 . 5}$ | $\mathbf{8 4 . 0}$ | 73.4 | $\mathbf{6 6 . 1}$ |
| Base (LLama) | 23.9 | 37.8 | 57.8 | 46.4 | 60.8 | 66.4 | 51.0 |
| DPO | 22.0 | 39.5 | $\mathbf{5 9 . 4}$ | 46.3 | 73.5 | $\mathbf{6 7 . 3}$ | 51.4 |
| MRPO $($ L $\leftarrow$ M,Q) | $\mathbf{2 4 . 3}$ | $\mathbf{4 5 . 3}$ | 57.9 | $\mathbf{4 6 . 4}$ | $\mathbf{7 4 . 1}$ | 66.7 | $\mathbf{5 2 . 4}$ |
| Base (Qwen) | 21.1 | 53.6 | 58.8 | 60.1 | 68.3 | 65.2 | 54.5 |
| DPO | 18.7 | 54.8 | $\mathbf{6 0 . 4}$ | $\mathbf{6 0 . 3}$ | 65.2 | 64.3 | 54.0 |
| MRPO $($ Q $\leftarrow$ M,L) | $\mathbf{2 3 . 7}$ | $\mathbf{5 4 . 9}$ | 59.0 | 60.1 | $\mathbf{6 8 . 4}$ | $\mathbf{6 5 . 4}$ | $\mathbf{5 5 . 3}$ |

Table 3: $\mathrm{M} \leftarrow \mathrm{L}$ : test performance $(\times 100)$ across HuggingFace Open LLM Leaderboard datasets. Bold denotes best.

| Dataset | GSM8K | TruthfulQA | HellaSwag | MMLU | Arc-easy | Winograde | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Base (TinyLlama) | $\mathbf{2 . 5}$ | 32.9 | 45.7 | 24.9 | 66.8 | 61.1 | 38.9 |
| DPO | 2.1 | $\mathbf{3 3 . 1}$ | 46.0 | 25.4 | $\mathbf{6 7 . 0}$ | $\mathbf{6 1 . 6}$ | 39.2 |
| MRPO (Ours) | 2.1 | 33.0 | $\mathbf{4 7 . 0}$ | $\mathbf{2 5 . 8}$ | $\mathbf{6 7 . 0}$ | $\mathbf{6 1 . 6}$ | $\mathbf{3 9 . 4}$ |

Table 4: TinyLlama $\leftarrow$ Mistral: test performance $(\times 100)$ across HuggingFace Open LLM Leaderboard datasets. Bold denotes best.

### 4.4 Distillation from Strong to Weak LLMs

In this experiment, we assess the advantage of MRPO in transferring likelihood predictions from larger LLMs to smaller ones. This has potential applications in improving finetuning small LLMs on mobile devices. It is worth mentioning that MRPO allows the precomputation of log probabilities from large LLMs, ensuring efficient training on low-resource devices. In particular, we choose TinyLlama (1.1B parameters) and Mistral (7B parameters) as RefM1 and RefM2, respectively. We finetune TinyLlama on the Ultrafeedback dataset using a process akin to the one described in the preceding section and report the Open LLM Leaderboard results in Table 4. Overall, MRPO keeps outperforming DPO and Base models by $0.2 \%$ and $0.5 \%$, respectively. While the gain is modest compared to previous results, our findings confirm the benefit of applying MRPO to weak LLMs, especially when the cost of training can be almost similar to that of DPO.

### 4.5 Ablation Study

### 4.5.1 The Importance of Clipped Trust-Region Optimization (CTRO)

Can MRPO work without CTRO ( $\$ 3.2$ To investigate the question, we utilize the small yet relatively challenging dataset $\mathrm{S} 3$ and conduct experiments with various $\epsilon_{\max }$ values: $\epsilon_{\max }=0$ (MRPO equals DPO), $\epsilon_{\max }=10^{6}$ (No clip), and $\epsilon_{\max }=\{1,0.1,0.01\}$. We also examine different reference models: (i) We employ RefM2 (Llama supervised tuning on Alpaca dataset) as a finetuned model of RefM1 (Llama), aiming to ensure that RefM2 is closely related to RefM1 (same family); and (ii) RefM2 (Mistral) is from a different family from RefM1 (LLama), which indicates a larger mismatch between the two reference models. Here, different families mean that LLMs can vary in architecture and/or be pretrained on distinct corpora, where the log probability of these LLMs for the same input can differ by hundreds of units. The final preference accuracy is reported in Table 5

We observe that for setting (ii) when $\epsilon_{\max } \geq 1$, the training loss can escalate significantly, reaching values as high as 10 , and occasionally even infinity, which highlights the instability of training in the absence of CTRO. This is evident in the poor performance of $\epsilon_{\max }=\left\{10^{6}, 1\right\}$. Utilizing CTRO with small $\epsilon_{\max }$ results in more stable training. However, excessive constraint, where $\epsilon_{\max }$ is too small, can lead to nearly identical performance compared to DPO. In setting (i), even with big $\epsilon_{\max }$, the training is stable. However, the performance is not as good as setting (ii). In particular, with the best $\epsilon_{\max }=0.1$, in setting (i), MRPO only achieves a $2 \%$ improvement over DPO, whereas in setting (ii), the improvement gap widens to $7 \%$. This discrepancy is understandable because without a diverse reference source, RefM2 may not exhibit significant advantages over RefM1, thereby limiting the extent of improvement. Therefore, we conclude that it is more advantageous to leverage diverse reference models, and employing CTRO is required to ensure training stability in this case.

| Setting | $\epsilon_{\max }=0$ (DPO) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |

Table 5: Clipped Trust-Region Optimization impact on S3. In setting (i), two reference models belong to the same family but differ in the finetuning dataset, whereas in setting (ii), they are from different families of LLMs. The reported numbers are the mean accuracy over 5 runs.

Is adaptive $\epsilon$ necessary? We conduct more experiments with fixed $\epsilon=0.1$ and adaptive $\epsilon_{\max }=0.1$ on S1, S2 and S3 using Llama and Mistral for RefM1 and RefM2, receptively. The results depicted in Appendix Fig. 4 (top) illustrate that fixed $\epsilon$ is still better than DPO, and adaptive $\epsilon$ outperforms significantly fixed $\epsilon$ across all datasets, emphasizing the importance of this mechanism in MRPO.

### 4.5.2 Analyzing Reference Weighting Coefficients

In this section, using adaptive $\epsilon_{\max }=0.1$, we compare the adaptive $\alpha$ (ARWC) mechanism proposed in $\S 3.3$ with different fixed values of $\alpha=\{0.1,0.5,0.9\}$ on S1, S2 and S3 using Llama and Mistral for RefM1 and RefM2, receptively. As shown in Appendix Fig. 4 (bottom), adaptive $\alpha$ demonstrates competitive performance, either outperforming or closely matching the performance of the best fixed $\alpha$ across all datasets. Given the minor discrepancies observed and the associated cost of hyperparameter tuning, we have opted to utilize adaptive $\alpha$ for all other experiments.

## 5 Related work

The integration of human input has been instrumental in advancing the performance of Large Language Models (LLMs) in diverse domains, such as question answering [Nakano et al., 2021], document summarization [Stiennon et al., 2020], and dialog applications [Thoppilan et al., 2022]. Traditionally, instruction finetuning (IFT) and reinforcement learning from human feedback (RLHF) framework [Christiano et al., 2017, Ouyang et al., 2022, Lee et al., 2023] has employed RL to align Large Language Models (LLMs). They RLH objective is to maximize a reward score derived from human preferences (chosen or rejected) while simultaneously minimizing the disparity between the new and initial policy. Recently, there has been a significant move towards closed-form losses, exemplified by DPO [Rafailov et al., 2023], which directly finetunes LLM on offline preference datasets, consolidating RLHF's reward learning and policy adjustments into a single stage. This "direct" approach is favored over RLHF due to their maximum-likelihood losses, demonstrating superior speed and stability than RL pipeline.

Other direct preference optimization methods [Zhao et al., 2023, Azar et al. 2023] formulate various adaptations of closed-form losses to attain the RLHF objective. Recent advancements have expanded beyond the traditional binary preference data, focusing on novel human preference models like Kahneman-Tversky value functions [Ethayarajh et al., 2024]. All these methods adhere to the fundamental framework of RLHF, striving to fit a preference model while ensuring the updates remain close to a reference model. In contrast, our approach is the first direct preference finetuning framework with multiple reference models.

Another line of work creates new preference data from LLM's own generated outputs, typically through self-training paradigms [Chen et al., 2024, Yuan et al., 2024, Pattnaik et al., 2024], employing multiple reference data pairs [Pattnaik et al., 2024]. Our method is orthogonal to self-playing approaches, featuring a faster alternative procedure as it does not require additional data generation. In contrast to methods employing multiple rewards and model merging [Jang et al. 2023, Rame et al., 2024], our approach does not involve training multiple LLMs. Instead, we train a single LLM using log-probability outputs of multiple reference LLMs during training. During testing, there is no need to maintain multiple models, and our inference cost is the same as using a single LLM.

## 6 Discussion

In this paper, we present Multi-Reference Preference Optimization (MRPO), a novel method leveraging multiple reference models to improve preference learning for Large Language Models (LLMs).

We theoretically derive the objective function for MRPO and conduct experiments with LLMs like LLama2 and Mistral, demonstrate their enhanced generalization across six preference datasets and competitive performance in six downstream natural language processing tasks.

Limitations Our study is limited by using only up to three reference models of modest size. Future research will explore the scalability of MRPO, examining its performance with larger $K$ values and a broader range of LLM sizes across diverse benchmarks. The limited improvement gain on small LLMs like TinyLlama presents a challenge for our current approach. Further investigation is needed to enhance MRPO when small LLMs serve as the base model, common on low-resource devices.

Broader Impacts In this work, we used publicly available datasets for our experiments and did not collect any human or animal data. We aim to enhance the alignment of LLMs with human preferences and values. We believe this goal is genuine and do not foresee any immediate harmful consequences. However, we acknowledge potential issues if our method is used to augment language models to generate hallucinated or negative content. This risk is inherent to any fine-tuning method when the fine-tuned data can be misused, and we will take all possible measures to prevent such misuse from our end.

## References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036, 2023.

Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. Hugging Face, 2023.

Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.

Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024 .

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.

L Gao, J Tow, B Abbasi, S Biderman, S Black, A DiPofi, C Foster, L Golding, J Hsu, A Le Noac'h, et al. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo. org/records/10256836, 7 .

Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. arXiv preprint arXiv:2310.11564, 2023 .

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Hung Le, Thommen Karimpanal George, Majid Abdolshah, Dung Nguyen, Kien Do, Sunil Gupta, and Svetha Venkatesh. Learning to constrain policy optimization with virtual trust region. Advances in Neural Information Processing Systems, 35:12775-12786, 2022.

Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.

Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, $35: 3843-3857,2022$.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.

Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, and Sathwik Tejaswi Madhusudhan. Curry-dpo: Enhancing alignment using curriculum learning \& ranked preferences. arXiv preprint arXiv:2403.07230, 2024.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.

Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Advances in Neural Information Processing Systems, 36, 2024.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966, 2023.

Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.
