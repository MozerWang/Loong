# MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing 

Unggi Lee ${ }^{1}$, Yonghyun Park ${ }^{2}$, Yujin Kim ${ }^{3}$,<br>Seongyune Choi ${ }^{1}$, Hyeoncheol Kim ${ }^{1 *}$<br>${ }^{1}$ Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea<br>${ }^{2}$ Department of Physics Education, Seoul National University, Seoul, Republic of Korea<br>${ }^{3}$ Department of Science Education, Ewha Womans University, Seoul, Republic of Korea<br>codingchild @ korea.ac.kr, enkeejunior1 @ snu.ac.kr, hello.yujink @ gmail.com,<br>\{csyun213, harrykim\}@korea.ac.kr


#### Abstract

Knowledge tracing (KT) is a field of study that predicts the future performance of students based on prior performance datasets collected from educational applications such as intelligent tutoring systems, learning management systems, and online courses. Some previous studies on KT have concentrated only on the interpretability of the model, whereas others have focused on enhancing the performance. Models that consider both interpretability and the performance improvement have been insufficient. Moreover, models that focus on performance improvements have not shown an overwhelming performance compared with existing models. In this study, we propose MonaCoBERT, which achieves the best performance on most benchmark datasets and has significant interpretability. MonaCoBERT uses a BERT-based architecture with monotonic convolutional multihead attention, which reflects forgetting behavior of the students and increases the representation power of the model. We can also increase the performance and interpretability using a classical testtheory-based (CTT-based) embedding strategy that considers the difficulty of the question. To determine why MonaCoBERT achieved the best performance and interpret the results quantitatively, we conducted ablation studies and additional analyses using Grad-CAM, t-SNE, and various visualization techniques. The analysis results demonstrate that both attention components complement one another and that CTTbased embedding represents information on both global and local difficulties. We also demonstrate that our model represents the relationship between concepts.


## Introduction

The outbreak of COVID-19 has accelerated the digital transformation in the field of education, and the number of students who use online learning platforms has increased. Most online learning platforms collect student data such as interaction logs, correctness, and learning history, thereby providing a chance to develop a better adaptive learning system for students.

Knowledge tracing (KT) is a research area that predicts the future performance of students based on prior performance datasets collected from educational applications such as intelligent tutoring systems (ITS), learning management systems (LMS), and online courses. KT models can be broadly classified into two categories: those focusing on interpretability and those focusing on increasing the performance. Models focused on interpretability, such as BKT (Corbett and Anderson 1994) and PFA (Pavlik Jr, Cen, and Koedinger 2009), mainly use Markov chain techniques and logistic functions. These models are generally simple to interpret. However, they suffer from a relatively low performance (Piech et al. 2015). Models focusing on a performance improvement, such as DKT, DKVMN (Zhang et al. 2017), SAKT (Pandey and Karypis 2019), and CL4KT (Lee et al. 2022), perform significantly better than traditional statistical approaches. However, this is difficult to interpret because of the nature of deep learning. Although AKT (Ghosh, Heffernan, and Lan 2020) considers both the model performance and interpretability, it has not shown an overwhelming performance in comparison to existing models.

Moreover, despite the application of self-attention and transformers (Vaswani et al. 2017) in recent KT models, they have been unable to improve the attention architecture compared to changes in other parts of the models. In natural language processing (NLP), Bigbird (Zaheer et al. 2020), Longformer (Beltagy, Peters, and Cohan 2020), and ConvBERT (Jiang et al. 2020), which maintain the BERT architecture and change the attention architectures, have succeeded in terms of both performance and efficiency.

In this article, MonaCoBERT, which achieves both a high performance and interpretability, is proposed. MonaCoBERT uses BERT-based and monotonic convolutional attention architectures. We also suggest a classical test theory (CTT) based embedding strategy that considers the question difficulty. Using CTT-based embedding, our model achieves a increase in performance and interpretability. As a result, MonaCoBERT achieved state-of-the-art results on most benchmark datasets in term of both the AUC and RMSE. Moreover, we also conducted an ablation study on all parts of the models and an additional analysis using Grad-CAM, $\mathrm{t}-\mathrm{SNE}$, and various visualization techniques. The analysis results demonstrate that both attention components complement one another and that CTT-based embedding represents information on both global and local difficulties. We also demonstrate that our model represents the relationship between concepts.[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_50fffd4c9b837f6b9da7g-2.jpg?height=492&width=1520&top_left_y=172&top_left_x=300)

Figure 1: Architectures of MonaCoBERT and monotonic convolutional multi-head attention. The left side shows different strategies in training and testing sessions. The right side shows the architecture of monotonic convolutional multi-head attention, combined with monotonic attention and ConvBERT attention.

## Related Work

## Knowledge Tracing

Knowledge tracing (KT) is a research area of predicting the knowledge states of the students using their interaction data. Since the first introduction of DKT (Piech et al. 2015), significant research in this area using deep neural networks has been conducted. Researchers have recently focused on self-attention architectures. SAKT (Pandey and Karypis 2019), SAINT+ (Shin et al. 2021), which uses selfattention, achieves a better performance than previous models. Moreover, AKT (Ghosh, Heffernan, and Lan 2020) was presented with self-attention, and a new architecture for retrieving latent knowledge representations was suggested. For AKT, a new embedding method that considers the educational perspective has also been suggested. CL4KT (Lee et al. 2022) also uses a self-attention and contrastive learning framework, and has achieved the best performance in KT.

## BERT and Its Application

BERT (Devlin et al. 2018) has been referred to as a successful application of Transformer. It mainly uses self-attention, and the masked language model (MLM) method, which can train bidirectionally in NLP, has been suggested. Some variations of BERT, such as Bigbird (Zaheer et al. 2020), Longformer (Beltagy, Peters, and Cohan 2020), and ConvBERT, (Jiang et al. 2020) have recently been designed with effective attention mechanisms applied while using the original architecture of BERT. These approaches have achieved an outstanding performance and high efficiency. Other studies have also attempted to use BERT architectures. As a recommendation system, BERT4Rec (Sun et al. 2019) uses the BERT architecture to enhance the recommendation power. However, for KT, although BEKT (Tiana et al. 2021) and BiDKT (Tan et al. 2022) attempt to use the BERT architecture, they cannot achieve a higher performance than other KT models. In this study, we explored why BERT does not perform better than the other models and showed that the BERT architecture is still valuable for KT. We found that changing the attention architecture and an embedding strategy are vital to optimizing BERT for the KT area.

## Method

## Problem Statement

Knowledge tracing aims at predicting the probabilities of students being correct through the use of a sequence of interaction data gathered by an LMS or ITS. Student interactions can be expressed as $x_{1}, \ldots, x_{t}$, and the $t$-th interaction can be denoted as $x_{t}=\left(q_{t}, a_{t}\right)$. Here, $q_{t}$ is the $\mathrm{t}-t h$ question and $a_{t}$ is $t$-th correctness of the student's response, where $a_{t} \in\{0,1\}$, in which 0 indicates an incorrect response and 1 is a correct answer. However, some datasets contain concept data $c_{t}$, and thus we can also express $x_{t}=\left(q_{t}, c_{t}, a_{t}\right)$.

## Proposed Model Architecture

BERT based Architecture for Knowledge Tracing To create our model baseline, we mainly referenced BERT (Devlin et al. 2018), BiDKT (Tan et al. 2022), BEKT (Tiana et al. 2021), and BERT4Rec (Sun et al. 2019). To optimize our research into KT, we changed some of the BERT architecture. First, we used a pre-layer normalization (preLN) Transformer in our model. Previous research (Liu et al. 2020) has suggested that Transformer is difficult to train without a training strategy, such as a warm-up start. By contrast, the pre-LN Transformer can be trained without a warm-up start and converges much faster than the original Transformer (Xiong et al. 2020). Second, we used a different strategy for the training and testing processes. During the training process, the proposed model predicted the masking position. The masking ratio used in the training process was the same as with the original BERT, which used $15 \%$ embedding, $80 \%$ of which was actual masking, $10 \%$ was a reversal, and $10 \%$ did not change. During the testing process, masking was applied to the last position of each sequence. Referring to the previous BERT-based studies on KT (Tan et al. 2022) or recommendation systems (Sun et al. 2019), the model predicts the correctness of the students using their previous history of interaction sequences. Figure 1-Left describes the different training and testing strategies of our model.

Embedding Strategy Most KT models use concepts, questions, and correctness as the input vectors for train-
ing. Previous studies have explored new input features. For example, AKT created Rasch embedding vectors by using concepts, items, and responses Ghosh, Heffernan, and Lan 2020). However, an item response theory (IRT), such as Rasch, can be applied to the dataset collected from tests or examinations because IRT assumes that the ability of a student does not change during the trial. In KT, the states of student knowledge change during learning (Yeung|2019). Therefore, we used the classical test theory (CTT) for handling the difficulty features.

We extracted the correctness of each question from the training set and made the questions difficulty. If the question in the validation or test set were not contained in training set, we replaced that question difficulty as a arbitrarily number like 75. Subsequently, we added the difficulty to the embedding blocks. In a previous study, BEKT Tiana et al. 2021) used five difficulty ranges in its embedding blocks. Nevertheless, we used a difficulty range of 100. Similar to BERT embedding layers, MonaCoBERT uses elementwise embedding vectors $E_{\text {input }}$, learnable positional embedding $E_{\text {pos }}$, concept embedding $E_{c}$, item embedding $E_{q}$, correctness embedding $E_{a}$, and CTT embedding $E_{c t t}$, where $E_{\text {input }} \in R^{m \times h}, E_{\text {pos }} \in R^{m \times h}, E_{c} \in R^{m \times h}, E_{q} \in$ $R^{m \times h}, E_{a} \in R^{m \times h}$ and $E_{c t t} \in R^{m \times h}$. Embedding layers $E_{\text {input }}$ are formulated as follows:

$$
\begin{equation*}
E_{\text {input }}=E_{\text {pos }}+E_{c}+E_{q}+E_{a}+E_{c t t} \tag{1}
\end{equation*}
$$

Pre-LN Transformer-based Encoder Architecture The encoder blocks used the pre-LN Transformer architecture (Xiong et al. 2020). In this study, 12 encoder layers were used. First, the embedding vectors $E_{\text {input }}$ are normalized through the pre-LN $L N_{\text {pre }}$

$$
\begin{equation*}
z=L N_{\text {pre }}\left(E_{\text {input }}\right) \tag{2}
\end{equation*}
$$

Second, the normalized value $z$ was changed to the query, key, and value of monotonic convolutional multihead attention. The results were passed through dropout layer $D$ and added to the embedding vectors as a residual connection.

$$
\begin{equation*}
a=x+D(\text { MonoConvMulAttn }(z, z, z)) \tag{3}
\end{equation*}
$$

Third, the results were normalized and passed through fully connected layers. The activation function was LeakyReLU. The results were also normalized through the dropout layer $D$. The second result was added as a residual connection.

$$
\begin{equation*}
l=a+D(f c(L N(a))) \tag{4}
\end{equation*}
$$

The fully connected layers are formulated as follows.

$$
\begin{equation*}
f c=W_{f c 2}\left(L e a k y \operatorname{Re} L U\left(W_{f c 1}\right)\right) \tag{5}
\end{equation*}
$$

where $W_{f c 1} \in R^{h \times(h * n)}, W_{f c 2} \in R^{(h * n) \times h}$.

Monotonic Convolutional Multihead Attention We suggest the use of monotonic convolutional multihead attention. This architecture is combined with ConvBERT's (Jiang et al. 2020) mixed-attention and AKT Ghosh, Heffernan, and Lan 2020) monotonic attention. In previous research, mixed attention achieved a higher performance than normal attention with BERT. Meanwhile, the sequence data in KT contain latent information about the forgetting of the students. To represent such forgetting, we used the exponential decay mechanism of monotonic attention. Figure 1-Right shows the monotonic convolutional multihead attention architecture.

The monotonic convolutional multihead attention $A_{m c}$ consists of the concatenation ([;]) of monotonic multihead attention $A_{m}$ and span-based dynamic convolution $S D C$. Here, $A m$ is the same as monotonic attention and can be formulated as follows:

$$
\begin{equation*}
A_{m c}(Q, K, V)=\left[A_{m}(Q, K, V) ; S D C(Q, K, V)\right] \tag{6}
\end{equation*}
$$

First, monotonic multihead attention $A_{m}$ has an exponential decay mechanism for measuring the distance between sequences. The exponential decay mechanism is a dot product with query linear $W_{Q}$ and key linear $W_{K}$. The learnable parameter $\delta$ is multiplied by these values. In addition, $A_{m}$ can be formulated as follows:

$$
\begin{equation*}
\operatorname{Am}=\operatorname{softmax}\left(\frac{(-\delta \cdot d(t, \tau)) \cdot W_{Q} \cdot W_{K}}{\sqrt{D_{k}}}\right), \delta>0 \tag{7}
\end{equation*}
$$

Here, $d(t, \tau)$ is the distance function, where $t$ is the present time step, and $\tau$ is the previous time step. In addition, $d(t, \tau)$ can be formulated as

$$
\begin{equation*}
d(t, \tau)=|t-\tau| \cdot \sum_{t^{\prime}=\tau+1}^{t} \gamma_{t, t^{\prime}} \tag{8}
\end{equation*}
$$

Moreover, $\gamma_{t, t^{\prime}}$ can be formulated as

$$
\begin{equation*}
\gamma=\frac{\exp \left(\frac{W_{q t} \cdot W_{k t}^{\prime}}{\sqrt{D_{k}}}\right)}{\sum_{1 \leq \tau^{\prime} \leq t} \exp \left(\frac{W_{q t} \cdot W_{k \tau^{\prime}}}{\sqrt{D_{k}}}\right)}, t^{\prime} \leq t \tag{9}
\end{equation*}
$$

The span dynamic convolution $S D C$ can be formulated as

$$
\begin{equation*}
S D C(Q, K, V)=L \operatorname{Conv}(V, \operatorname{softmax}(W(Q \otimes K))) \tag{10}
\end{equation*}
$$

where $W$ is a linear layer, and $\otimes$ can be denoted as a point-wise multiplication. The lightweight convolution LConv can be formulated as follows:

$$
\begin{equation*}
\operatorname{LConv}(X, W)=\sum_{j=1}^{k} W_{j} \dot{X}_{i+j-\left[\frac{[k+1]}{2}\right]} \tag{11}
\end{equation*}
$$

## Experiment Setting

Datasets Six benchmark datasets were used to validate the effectiveness of our model. We ignored student data with fewer than five interactions. If the dataset contained multiple concepts in a single interaction, we treated the combination of concepts as unique. The ASSISTment datasets were collected from the ASSISTment ITS. We used assist09, assist12, assist17 and ignored assist15, which has no information regarding the questions 1 . The algebra datasets were[^1]

| Dataset | Metrics | DKT | DKVMN | SAKT | AKT | CL4KT | MCB-NC | MCB-C |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| assist09 | AUC | 0.7285 | 0.7271 | 0.7179 | 0.7449 | $\underline{0.7600}$ | $\mathbf{0 . 8 0 0 2}$ | $\underline{\mathbf{0 . 8 0 5 9}}$ |
|  | RMSE | $\underline{0.4328}$ | 0.4348 | 0.4381 | 0.4413 | 0.4337 | $\underline{\mathbf{0 . 4 0 2 9}}$ | $\mathbf{0 . 4 0 6 3}$ |
| assist12 | AUC | 0.7006 | 0.7011 | 0.6998 | $\underline{0.7505}$ | 0.7314 | $\mathbf{0 . 8 0 6 5}$ | $\underline{\mathbf{0 . 8 1 3 0}}$ |
|  | RMSE | 0.4348 | 0.4355 | 0.4360 | $\underline{0.4250}$ | 0.4284 | $\mathbf{0 . 3 9 7 6}$ | $\underline{\mathbf{0 . 3 9 3 5}}$ |
| assist17 | AUC | $\underline{\mathbf{0 . 7 2 2 0}}$ | $\underline{0.7095}$ | 0.6792 | 0.6803 | 0.6738 | 0.6700 | $\mathbf{0 . 7 1 4 1}$ |
|  | RMSE | $\underline{\mathbf{0 . 4 4 6 9}}$ | $\mathbf{0 . 4 5 1 6}$ | $\underline{0.4591}$ | 0.4722 | 0.4713 | 0.4727 | 0.4630 |
| algebra05 | AUC | 0.8088 | 0.8146 | $\underline{0.8162}$ | 0.7673 | 0.7871 | $\mathbf{0 . 8 1 9 0}$ | $\underline{\mathbf{0 . 8 2 0 1}}$ |
|  | RMSE | 0.3703 | $\underline{0.3687}$ | $\mathbf{0 . 3 6 8 5}$ | 0.3918 | 0.3824 | 0.3940 | $\underline{\mathbf{0 . 3 5 8 4}}$ |
| algebra06 | AUC | 0.7939 | $\underline{\underline{0.7961}}$ | 0.7927 | 0.7505 | 0.7789 | $\mathbf{0 . 7 9 9 7}$ | $\underline{\mathbf{0 . 8 0 6 4}}$ |
|  | RMSE | $\mathbf{0 . 3 6 6 6}$ | $\underline{\mathbf{0 . 3 6 6 1}}$ | 0.3675 | 0.3986 | 0.3863 | 0.3835 | $\underline{\mathbf{0 . 3 6 7 2}}$ |
| EdNet | AUC | 0.6609 | $\underline{0.6602}$ | 0.6506 | $\underline{0.6687}$ | 0.6651 | $\mathbf{0 . 7 2 2 1}$ | $\underline{\mathbf{0 . 7 3 3 6}}$ |
|  | RMSE | 0.4598 | $\underline{0.4597}$ | 0.4629 | 0.4783 | 0.4750 | $\mathbf{0 . 4 5 7 2}$ | $\underline{\mathbf{0 . 4 5 1 6}}$ |

Table 1: Overall performance of knowledge tracing models based on five benchmark datasets. The best performance is denoted in bold underline, the second in bold, and the third in underline. MCB-C indicates that MonaCoBERT used classical test theory (CTT), whereas MCB-NC indicates that it did not. We can see that MCB-C achieved the best results, and MCB-NC was second for most of the benchmark datasets.

provided by the KDD Cup 2010 EDM Challeng $d^{2}$ EdNe ${ }^{3}$ is a dataset collected by Santa for the learning of English, mainly TOEIC (Choi et al. 2020). We extracted 5,000 interaction data from the original dataset. Table 2 lists the number of features in the benchmark dataset.

| Dataset | \#Students | \#Concepts | \#Questions | \#interactions |
| :---: | :---: | :---: | :---: | :---: |
| assist09 | 3,695 | 149 | 17,728 | 282,071 |
| assist12 | 24,429 | 264 | 51,632 | $1,968,737$ |
| assist17 | 1,708 | 411 | 3,162 | 934,638 |
| algebra05 | 571 | 271 | 173,113 | 607,014 |
| algebra06 | 1,318 | 1,575 | 549,821 | $1,808,533$ |
| EdNet | 5,000 | 1,472 | 11,957 | 641,712 |

Table 2: Benchmark dataset ignored student data with less than five interactions. \#Concepts are the same as the skills.

Evaluation Metrics and Validation By referencing CL4KT, we used both the area under the curve (AUC) and the root mean squared error (RMSE) as the performance metrics. We also used a five-fold cross-validation for the evaluation.

Baseline Models We compared MonaCoBERT to the baseline models, such as DKT (Piech et al. 2015), DKVMN (Zhang et al. 2017), SAKT (Pandey and Karypis 2019), and the latest models, such as AKT (Ghosh, Heffernan, and Lan 2020) and CL4KT. (Lee et al. 2022).[^2]

Hyperparameters for Experiments To compare each model, we used the same parameters for the model training.

- batch size: The batch size was 512 . Owing to a limitation of resources, we also used a gradient accumulation.
- early stop: The early stop was 10 . If the validation score was not successively increased during the ten iterations, the training session was stopped.
- training, validation, test ratio: The training ratio was $80 \%$ of the entire dataset, and the test ratio was $20 \%$. The valid ratio was $10 \%$ of the training ratio.
- learning rate and optimizer: The learning rate was 0.001, and Adam was used as the optimizer.
- embedding size: The embedding size was 512 .
- others: We used eight attention heads for MonaCoBERT. The Max sequence length was 100 , and the encoder number was 12. Other models such as $\mathrm{AKT}_{4}^{4}$ and CL4KT used the default settings.


## Result and Discussion

## Overall Performance

Figure 1 illustrates the overall performance of each model. Every model used a five-fold cross-validation for the estimation. MonaCoBERT-C, which was trained using CTT, was the best model in most benchmark datasets and was a new state-of-the-art model in assist09, assist12, algebra05, and ednet. MonaCoBERT-NC was the second-best model for most of the datasets. This result indicates that CTT embedding affects the performance of the model. For all datasets,[^3]

MonaCoBERT-C performed better than MonaCoBERT-NC. This result indicates that it was difficult for MonaCoBERT$\mathrm{NC}$ to learn the latent representations of the item difficulty from the dataset.

Our estimation differs from that of previous research. Except for MonaCoBERT-NC and MonaCoBERT-C, the best model was modified for each dataset. For instance, the AUC and RSME of assist17, and the RMSE, DKT, and DKVMN of algebra06 showed that these were the best and second-best models, respectively. This indicates that DKT and DKVMN are still helpful in predicting certain cases. These results may stem from pre-processing methods or the training of the hyperparameter settings.

## Ablation Studies

In this section, we explore why MonaCoBERT performed better than the other models and which parts of the model affected the increase in performance.

Impact of Attention Mechanisms In Table 3, we compare the attention mechanisms. For comparison, we used the assist09 and assist09-CTT datasets. The assist09 dataset is a normal dataset that contains concepts, questions, and correctness; however, assist09-CTT contains the concepts, questions, correctness, and CTT-based difficulty.

We detached each part of the monotonic convolutional multi-head attention and created four attention mechanisms: normal multi-head attention, monotonic multi-head attention, convolutional multi-head attention, and monotonic convolutional multi-head attention. We also used a five-fold cross-validation and an early stop 10 times. The other hyperparameters used to determine the overall performance were the same.

As a result, monotonic convolutional multihead attention exhibited the best performance for both comparisons. Convolutional multihead attention and monotonic multihead attention achieved the second-best performance under each setting. The increments differed for each setting and were approximately $2 \%$ for assist09 and 1-2\% for assist09-CTT.

| Dataset | Attn | MonoAttn | ConvAttn | MonoCoAttn |
| :---: | :---: | :---: | :---: | :---: |
| assist09 | 0.7736 | $\mathbf{0 . 7 9 9 3}$ | $\underline{0.7959}$ | $\underline{\mathbf{0 . 8 0 0 2}}$ |
| increment | 0 | +0.026 | +0.022 | +0.027 |
| assist09-CTT | 0.7858 | $\underline{0.8039}$ | $\mathbf{0 . 8 0 5 4}$ | $\underline{\mathbf{0 . 8 0 5 9}}$ |
| increment | 0 | +0.018 | +0.020 | +0.021 |

Table 3: AUC performances of each attention mechanism using the assist09 and assist09-CTT datasets. The increments were written based on normal attention.

Impacts of Embedding Strategy In Table 4, we compare each embedding strategy. The first embedding strategy $e m b_{c q}$ is an element-wise sum of the concept embedding $e m b_{c}$, question embedding $e m b_{q}$, and correctness embedding $e m b_{r}$.

$$
\begin{equation*}
e m b_{c q}=e m b_{c}+e m b_{q}+e m b_{r} \tag{12}
\end{equation*}
$$

Moreover, the second embedding strategy $e m b_{\text {rasch }}$ is an element-wise sum of concept and Rasch embedding, as suggested by AKT. Rasch embedding uses concept embedding $e m b_{c}$ and learnable question scalar $e m b_{q}$ or a combination of concepts and answer embedding $e m b_{c r}$ to calculate the difficulty, where $e m b_{c}, e m b_{c r} \in R^{n \times h}$ and $e m b_{q} \in R^{n \times 1}$. Note that IRT Rasch embedding differs from AKT Rasch embedding because the condition of IRT assumes that the knowledge state of the student is fixed and does not change when estimated.

$$
\begin{gather*}
e m b_{r a s c h-c}=e m b_{c}+e m b_{q} * e m b_{c}  \tag{13}\\
e m b_{r a s c h-c r}=e m b_{c r}+e m b_{q} * e m b_{c r} \tag{14}
\end{gather*}
$$

The last embedding strategy, $e m b_{C T T}$, is an element-wise sum of concept embedding, question embedding, correctness embedding, and CTT embedding, $e m b_{c t t}$, which was suggested in this study. We set $e m b_{c t t}$ as the probability of the difficulty and the integer type, where $0 \leq e m b_{c t t} \leq 100$.

$$
\begin{equation*}
e m b_{C T T}=e m b_{c}+e m b_{q}+e m b_{r}+e m b_{c t t} \tag{15}
\end{equation*}
$$

As a result, in Table 4, emb ${ }_{C T T}$ generally showed a better performance than the other embedding strategies. DKVMN, AKT, and MonaCoBERT performed well when using $e m b_{C T T}$. This result indicates that the models did not learn the difficulty representation during training. Meanwhile, CL4KT and SAKT showed slightly better performances when using $e m b_{\text {rasch }}$. DKT was not affected by the embedding strategy.

| Embedding Strategy | $e m b_{c q}$ | $e m b_{\text {rasch }}$ | $e m b_{C T T}$ |
| :---: | :---: | :---: | :---: |
| DKT | 0.7263 | $\mathbf{0 . 7 2 7 4}$ | 0.7239 |
| DKVMN | 0.7188 | 0.7255 | $\mathbf{0 . 7 3 1 3}$ |
| SAKT | 0.6822 | $\mathbf{0 . 6 9 4 1}$ | 0.6693 |
| AKT | 0.7440 | 0.7449 | $\mathbf{0 . 7 6 3 2}$ |
| CL4KT | 0.7600 | $\mathbf{0 . 7 6 0 1}$ | 0.7461 |
| MCB | 0.8002 | 0.7736 | $\mathbf{0 . 8 0 5 9}$ |

Table 4: Comparison of each embedding strategy with KT models in the assist09 dataset.

## In-depth Analysis of Attention and Embedding

In this subsection, we analyze the attention and embedding in depth. We used Grad-CAM and t-SNE for the analysis and visualization.

Analysis of MA and SDC Owing to the nature of KT, such as the forgetting behavior of the students, we expected that monotonic attention (MA) will look up the nearby data regarding the current input. However, as shown in Figure 3. MA induced higher attention scores for the distant data,
![](https://cdn.mathpix.com/cropped/2024_06_04_50fffd4c9b837f6b9da7g-6.jpg?height=414&width=1632&top_left_y=178&top_left_x=238)

Figure 2: Analysis of underlying behaviors of SDC. The figure on the left illustrates the proportion of the importance of each module. SDC showed an importance competitive to that of MA in most layers. In particular, the SDC showed the most significant contribution in the first layer. The histogram in the center figure represents the current input weight of the concept. When the response of the student was correct, the SDC allocated more weight to the interaction. In addition, even if the response was the same, the weight varied considerably based on the concept. The figure on the right shows examples of SDC filters arranged based on the correctness and concept.

not nearby data. We also observed that SDC was more critical than MA in the first layer. Figure 2 -Left shows the relative importance ratios of SDC and MA. The contribution of SDC was greater than that of MA in the first layer. To define the importance of each module, we used an element-wise version of Grad-CAM as a metric (Selvaraju et al. 2017; Gildenblat and contributors 2021). We also found that SDC extracted useful information regarding the properties of the current input. Specifically, SDC focused on the current input when the student answered correctly. In Figure 2 -Center, Right, we can see that SDC assigned higher weights to the current inputs when the student responded correctly. Moreover, the large variance of weights given correct responses implies that SDC considers not only the correctness of responses but also the importance of the concept. (Figure 2 . Center, blue) This result shows us that MonaCoBERT implicitly learned what concepts or questions were essential for inferring the ability of the students. This indicates the possibility of using MonaCoBERT to automatically find the problem essential to estimating the student's ability, which can be used to support the estimation and assessment.
![](https://cdn.mathpix.com/cropped/2024_06_04_50fffd4c9b837f6b9da7g-6.jpg?height=244&width=740&top_left_y=1827&top_left_x=237)

Figure 3: Analysis of the attention map of monotonic selfattention (MA). The figure on the left shows the attention weight according to the distance of the interaction data, and indicates that models with MA (e.g., SDC + MA, MA) display more outstanding attention scores for distant tokens. The figure on the right shows an example of the attention map of MonaCoBERT.
CTT based embedding We showed that CTT-based embedding helps the model represent the difficulty of the problem. Figure 4 shows a visualization using t-SNE (?). Figure 4.Left shows the visualization of the CTT-based embedding vector, and Figure 4-Right shows the visualization of the NoCTT-based embedding. Unlike No-CTT-based embedding, where different difficulties are mixed in each cluster, CTTbased embedding (i.e., $e m b_{C T T}$ ) showed that the difficulty of the information was smoothly distributed globally.
![](https://cdn.mathpix.com/cropped/2024_06_04_50fffd4c9b837f6b9da7g-6.jpg?height=330&width=728&top_left_y=1323&top_left_x=1206)

Figure 4: Visualization of the embedding vector. The figure on the left shows the results with CTT-based embedding. The figure on the right shows the results of No-CTT-based embedding. We can see that the results of CTT-based embedding not only represent the difficulty information globally, they also help avoid a difficulty in the mixing in each cluster.

## Discovery of Relationships between Concepts

To determine whether our model understood the relevance between concepts, we analyzed the monotonic attention weights of the last encoder layer after passing through the softmax function. The results are shown in Figure 5-left. We averaged the attention scores of the questions using the same concepts to obtain the relevance between concepts. We created a directed graph, as shown in Fig 5-Center, by selecting only those concepts with attention weights of higher than 0.1 .
![](https://cdn.mathpix.com/cropped/2024_06_04_50fffd4c9b837f6b9da7g-7.jpg?height=498&width=1684&top_left_y=212&top_left_x=208)

Figure 5: Analysis results of the relevance between concepts, exploiting attention weights of the monotonic attention part after the model was trained using monotonic convolutional multi-head attention. The figure on the left shows a heatmap of the attention weights between each pair of concepts. It shows how much attention each concept on the y-axis (e.g., 7th, 92nd, 94th, 96th) assigns attention to some selected concept on the other $\mathrm{x}$-axis. The center figure shows a directed graph of the relevance between concepts. It shows how the concepts of assist09 influence one another. The source concept nodes are assigned a high attention weight to the destination concept nodes, and the concept nodes can be connected in both directions. We set the threshold to 0.1 and ignored edges lower than the threshold. When the threshold was decreased, more skill nodes were connected, and vice versa. The concept information of the assist09 dataset can be found on the right. 'nan' means concepts that are not defined in the original dataset.

According to the concept network shown in Figure 5 center, we can see that the model learns the relevance between skills. For example, as shown in Figure 5-left, the 7th concept (Absolute Value) was connected with some concepts of subtraction, such as 92 (Addition and Subtraction Fractions), 94 (Addition and Subtraction Positive Decimals), and 96 (Addition and Subtraction Integers). This means that you need to be good at subtraction to calculate the correct absolute value. Accordingly, the 117th concept (Probability of a Single Event) and 115th concept (Probability of Two Distinct Events) assigned high attention weights to each other, since concept 117 is a prerequisite for concept 115 . 121st concept (Counting Methods) is also connected with 115 and 117. However, the concept network shown in Figure 5 is not perfect because some concepts did not connect to each other despite their similarities. This result may be due to the monotonic attention decreasing the attention weight according to the time step. Nevertheless, observing the attention weights can help uncover new connections between previously inconceivable concepts.

## Conclusion

In this study, we developed MonaCoBERT, which employs a BERT-based architecture with monotonic convolutional multihead attention for student forgetting and the representation power of the model. We also adopted an effective embedding strategy that represented difficulty based on a classic test theory. Consequently, MonaCoBERT exhibited superior performance on most benchmark datasets. We conducted an ablation study for each part of the model; consequently, we discovered that monotonic convolutional multihead attention aided in improving the model performance. Although the embedding strategy contributed significantly to the performance improvement of our model, we confirmed that depending on the model, the contribution of the embedding strategy to the performance enhancement differed. We conducted an additional analysis to quantitatively analyze the attention architecture and embedding strategy using Grad-CAM and t-SNE. Future research will be focused on improving the attention architecture and the difficulty embedding strategy.

## References

Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.

Choi, Y.; Lee, Y.; Shin, D.; Cho, J.; Park, S.; Lee, S.; Baek, J.; Bae, C.; Kim, B.; and Heo, J. 2020. Ednet: A largescale hierarchical dataset in education. In International Conference on Artificial Intelligence in Education, 69-73. Springer.

Corbett, A. T.; and Anderson, J. R. 1994. Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and user-adapted interaction, 4(4): 253-278.

Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Ghosh, A.; Heffernan, N.; and Lan, A. S. 2020. Contextaware attentive knowledge tracing. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining, 2330-2339.

Gildenblat, J.; and contributors. 2021. PyTorch library for CAM methods. https://github.com/jacobgil/pytorch-gradcam.

Jiang, Z.-H.; Yu, W.; Zhou, D.; Chen, Y.; Feng, J.; and Yan, S. 2020. Convbert: Improving bert with span-based dynamic convolution. Advances in Neural Information Processing Systems, 33: 12837-12848.

Lee, W.; Chun, J.; Lee, Y.; Park, K.; and Park, S. 2022. Contrastive Learning for Knowledge Tracing. In Proceedings of the ACM Web Conference 2022, 2330-2338.

Liu, L.; Liu, X.; Gao, J.; Chen, W.; and Han, J. 2020. Understanding the difficulty of training transformers. arXiv preprint arXiv:2004.08249.

McInnes, L.; Healy, J.; and Melville, J. 2018. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426.

Pandey, S.; and Karypis, G. 2019. A self-attentive model for knowledge tracing. arXiv preprint arXiv:1907.06837.

Pavlik Jr, P. I.; Cen, H.; and Koedinger, K. R. 2009. Performance Factors Analysis-A New Alternative to Knowledge Tracing. Online Submission.

Piech, C.; Bassen, J.; Huang, J.; Ganguli, S.; Sahami, M.; Guibas, L. J.; and Sohl-Dickstein, J. 2015. Deep knowledge tracing. Advances in neural information processing systems, 28.

Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; and Batra, D. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, 618-626.

Shin, D.; Shim, Y.; Yu, H.; Lee, S.; Kim, B.; and Choi, Y. 2021. Saint+: Integrating temporal features for ednet correctness prediction. In LAK21: 11th International Learning Analytics and Knowledge Conference, 490-496.

Sun, F.; Liu, J.; Wu, J.; Pei, C.; Lin, X.; Ou, W.; and Jiang, P. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management, 1441-1450.

Tan, W.; Jin, Y.; Liu, M.; and Zhang, H. 2022. BiDKT: Deep Knowledge Tracing with BERT. In International Conference on Ad Hoc Networks, International Conference on Testbeds and Research Infrastructures, 260-278. Springer.

Tiana, Z.; Zhengc, G.; Flanaganb, B.; Mic, J.; and Ogatab, H. 2021. BEKT: Deep Knowledge Tracing with Bidirectional Encoder Representations from Transformers. Proceedings of the 29th International Conference on Computers in Education.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30 .

Xiong, R.; Yang, Y.; He, D.; Zheng, K.; Zheng, S.; Xing, C.; Zhang, H.; Lan, Y.; Wang, L.; and Liu, T. 2020. On layer normalization in the transformer architecture. In International Conference on Machine Learning, 10524-10533. PMLR.

Yeung, C.-K. 2019. Deep-IRT: Make deep learning based knowledge tracing explainable using item response theory. arXiv preprint arXiv:1904.11738.
Zaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Alberti, C.; Ontanon, S.; Pham, P.; Ravula, A.; Wang, Q.; Yang, L.; et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33: 17283-17297.

Zhang, J.; Shi, X.; King, I.; and Yeung, D.-Y. 2017. Dynamic key-value memory networks for knowledge tracing. In Proceedings of the 26th international conference on World Wide Web, 765-774.


[^0]:    ${ }^{*}$ Corresponding author.

[^1]:    ${ }^{1}$ retrieved from https://sites.google.com/site/assistmentsdata/home

[^2]:    ${ }^{2}$ retrieved from https://pslcdatashop.web.cmu.edu/KDDCup

    ${ }^{3}$ retrieved from https://github.com/riiid/ednet

[^3]:    ${ }^{4}$ https://github.com/arghosh/AKT

    ${ }^{5}$ https://github.com/UpstageAI/cl4kt

