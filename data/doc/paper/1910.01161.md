# Stochastic Bandits with Delayed Composite Anonymous Feedback 

Siddhant Garg * Aditya Kumar Akash *<br>\{sgarg33, aakash@wisc.edu \}<br>University of Wisconsin-Madison


#### Abstract

We explore a novel setting of the Multi-Armed Bandit (MAB) problem inspired from real world applications which we call bandits with "stochastic delayed composite anonymous feedback (SDCAF)". In SDCAF, the rewards on pulling arms are stochastic with respect to time but spread over a fixed number of time steps in the future after pulling the arm. The complexity of this problem stems from the anonymous feedback to the player and the stochastic generation of the reward. Due to the aggregated nature of the rewards, the player is unable to associate the reward to a particular time step from the past. We present two algorithms for this more complicated setting of SDCAF using phase based extensions of the UCB algorithm. We perform regret analysis to show sub-linear theoretical guarantees on both the algorithms.


## 1 Introduction

Multi-Armed Bandits (MAB) have been a well studied problem in machine learning theory for capturing the exploration-exploitation trade off in online decision making. MAB has applications to domains like e-commerce, computational advertising, clinical trials, recommendation systems, etc.

In most of the real world applications, assumptions of the original theoretical MAB model like immediate rewards, non-stochasticity of the rewards, etc do not hold. A more natural setting is when the rewards of pulling bandit arms are delayed in the future since the effects of the actions are not always immediately observed. [16] first explored this setting assuming stochastic rewards for pulling an arm which are obtained at some specific time step in the future. This setting is called delayed, aggregated, anonymous feedback (DAAF). The complexity of this problem stems from anonymous feedback to the model due to its inability to distinguish the origin of rewards obtained at a particular time from any of the previous time steps.

This work was extended by adding a relaxation to the temporal specificity of observing the reward at one specific time in the future by [3]. The reward for pulling an arm can now be possibly spread adversarially over multiple time steps in the future. However they made an added assumption on the non-stochasticity of the rewards from each arm, thereby observing the same total reward for pulling the same arm each time. This scenario of non-stochastic composite anonymous feedback (CAF) can be applied to several applications, but it still does not cover the entire spectrum of applications.

Consider a setting of a clinical trial where the benefits of different medicines on improving patient health are observed. CAF offers a more natural extension to this scenario than DAAF since the benefits from a medicine can be spread over multiple steps after taking it rather than achieving it all at once at a single time step in the future. However, the improvements effects of the medicine might be different for different patients and thus assuming the same total health improvement for each time using a specific medicine is not very realistic. Inspired from this real world setting, we suggest that[^0]a more general bandit setting will be using CAF with the non-stochastic assumption dropped. We study such a MAB setting with stochastic delayed composite anonymous feedback (SDCAF).

For the SDCAF setting, a player has an option to chose one of $\mathrm{K}$ actions (bandit arms) at every time step. Once the player picks an action, a reward is generated at random from an underlying reward distribution for that action. Instead of receiving this reward in a single step, it is adversarially spread over this fixed number of time steps after the action was chosen. After every action choice, the player receives the sum total of all the rewards from the previous actions which are due at this particular step. The difficulty of this setting is due to the fact that the player does not know how this aggregated reward has been constituted from the previous actions chosen. Extending algorithms from the theoretical model of SDCAF to practical applications, involves obtaining guarantees on the rewards obtained from them. The regret of the algorithms refers to how much reward was lost on choosing a particular action over the optimal one. We aim to minimize the regret from plays of the bandit.

We present a phase based algorithm for this SDCAF setting which involves running a modified version of the UCB algorithm [1] in phases where the same arm is pulled multiple times in a single phase. This is motivated by the aim to reduce the error in approximating the arm mean due to extra and missing reward components from adjacent arm pulls. We prove sub-linear regret bounds for this algorithm. We also show that a modified version of ODAAF, a phase based improved UCB algorithm proposed in [16], can be used in our setting and achieves sub-linear regret bounds.

### 1.1 Related Work

Online learning with delayed feedback has been studied in the non-bandit setting by $[19,14,11,9$, 17, 10, 8] and in the bandit setting by [15, 9, 13, 4, 18, 16]. [7] consider contextual stochastic bandits having a reward with a constant delay. The loss function of our setting is a generalization of the loss function of [5]. Gaussian process bandits with bounded stochastic delayed rewards were studied in [6]. [16] study bandits in the setting of delayed anonymous and aggregated rewards where the rewards are stochastically sampled from a distribution but received at some time step in the future. [3] study non-stochastic bandits where the rewards are spread adversarially over some time steps in the future after pulling the arm.

## 2 Problem Definition

We consider a MAB setting with $K>1$ actions or arms in the set $A$ of all actions. At each time step $0 \leq t \leq T$, the player chooses an action $i \in A$ and receives some reward depending on his past and current choices. Each action $i \in A$ is associated with a reward distribution $\nu_{i}$ which is supported in $[0,1]$, with mean $\mu_{i}$. Let $R_{t}(i)$ denote the total reward generated on choosing action $i$ at time step $t$ which is drawn from the distribution $\nu_{i}$. Note that $R_{t}(i)$ is not received by the player in its entirety at time step $t$, but rather spread over a maximum of $d$ time steps (including current time $t$ ) in any arbitrary manner. $R_{t}(i)$ is defined by the sum $\sum_{s=0}^{d-1} R_{t}^{(s)}(i)$ of $d$ components $R_{t}^{(s)}(i) \geq 0$ for $s=0, \ldots, d-1$, where $R_{t}^{(s)}(i)$ denotes the reward component obtained at time $t+s$ if action $i$ was chosen at time $t$. We refer to choosing an action and pulling an arm interchangeably in our analysis and use a similar notation to [3] for uniformity.

We define $X_{t}$ as the collective reward that the player obtains at time $t$. If the player chose action $i_{t-l}$ at time step $(t-l)$ where $l \in\{0, \ldots, d-1\}$, we can write $X_{t}=\sum_{l=0}^{d-1} R_{t-l}^{l}\left(i_{t-l}\right)$, which is the sum of contributions from all actions which the player chose in the past. Only actions chosen in the past $(d-1)$ time steps affect the current reward $X_{t}$ obtained. We have $R_{t-l}^{l}(i)=0$ for all $i$ and $l$ when $t-l<0$.

## 3 Algorithms

We present two algorithms for this setting of SDCAF in Algorithm 1 and Algorithm 2 respectively. For Algorithm 2 we only specify the additional inputs and initialization over Algorithm 1 We first provide the intuition behind the algorithms and then provide a formal regret analysis.

Algorithm 1 is a modified version of the standard UCB algorithm and is run in phases where the same arm is pulled multiple times along with maintaining an upper confidence bound on the reward from each arm. More specifically, each phase $m$ consists of two steps. In step 1 , the arm with maximum upper confidence bound $i=\arg \max _{j} B_{j}(m-1, \delta)$ is selected. In step 2, the selected
arm $i$ is pulled $k$ times repeatedly. We track all time steps where arm $i$ is played till phase $m$ in the set $S_{i}(m)$. The rewards obtained are used to update the running estimate of the arm mean $\hat{\mu}_{i}(m)$. The intuition behind running the algorithm in phases is to gather sufficient rewards from a single arm so as to have a good estimate of the arm mean reward. This helps us bound the error in our reward estimate due to extra rewards from the previous phase and missing rewards which seep into the next phase due to delay. For every phase of the algorithm, the selected arm is pulled for a fixed number of times $k$. From our regret analysis, setting $k=O(\sqrt{T / \log (T)})$ achieves sub-linear regret.

Algorithm 2 is a modification of the ODAAF algorithm proposed in [16], where we remove the bridge period as it has no affect on the confidence bounds in the analysis. This is a modified version of the improved-UCB algorithm from [2] and run in phases where a set $A$ of active arms is maintained, which is pruned based on the confidence on the arm mean estimates. Each phase $m$ consists of three steps. In the first step, an active arm $i \in A_{m}$ is sampled and then pulled for $n_{m}-n_{m-1}$ steps. All time steps where arm $i$ was played in the first $m$ phases are collected in the set $S_{i}(m)$. In the next step, an updated estimate $X_{i}(m)$ of the arm mean $\mu_{i}$ is computed. In the final step, the set of active arms is updated by elimination of arm $j$ if the calculated estimate $X_{i}(m)$ is $\tilde{\Delta}_{m}$ smaller than $\max _{j \in A_{m}} X_{j}(m)$. The choice of $n_{m}$ ensures that with good probability the estimates $X_{j}(m)$ have bounded error.

We now provide regret analysis for the algorithms and specify the choice of parameters $k$ and $n_{m}$.

```
Algorithm 1: Modified UCB
Algorithm 2: Improved UCB (ODAAF [16])
Input: $\mathrm{A}$ set of arms $\mathbb{A}=\{1, \ldots, K\}$
Extra Input: $n_{m}$ for each phase $m=\{1,2, \ldots\}$
        A time horizon $T$
Extra Initialization: $A_{1}=A$
Initialization: $S_{i}(0)=\phi \forall i \in \mathbb{A}$
        $t=0$
        $m=1\{$ Phase $\}$
while $(m=1,2, \ldots) \cap(t \leq T)$ do
    Step 1

```

![](https://cdn.mathpix.com/cropped/2024_06_04_e7d696434dba8ce24b91g-03.jpg?height=48&width=352&top_left_y=1334&top_left_x=423)

```
        $B_{i}(m-1, \delta)=\infty$
    Else
    $\mathbf{F i}$
        $B_{i}(m-1, \delta)=\hat{\mu}_{i}(m-1)+\sqrt{\frac{2 \log \left(\frac{1}{\delta}\right)}{T_{i}(m-1)}}$
    Choose arm $i \in \arg \max _{j} B_{j}(m-1, \delta)$
    Step 2
    $\overline{S_{j}(m)} \leftarrow S_{j}(m-1), \forall j \in \mathbb{A}$
    Repeat $\mathrm{k}$ time steps
        Play arm $i$
        Collect reward $X_{t}$ at time step $t$
        $S_{i}(m) \leftarrow S_{i}(m) \cup\{t\}$
        $t \leftarrow t+1$
    end
    $\hat{\mu}_{i}(m)=\frac{\sum_{s \in S_{i}(m)} X_{t}}{\left|S_{i}(m)\right|}$
    $m \leftarrow m+1$
while $(m=1,2, \ldots) \cap(t \leq T)$ do
end
    Step 1: Play arms

```

![](https://cdn.mathpix.com/cropped/2024_06_04_e7d696434dba8ce24b91g-03.jpg?height=46&width=216&top_left_y=1218&top_left_x=1109)

```
        $S_{j}(m) \leftarrow S_{j}(m-1), \forall j \in \mathbb{A}$
        while $\left|S_{i}(m)\right| \leq n_{m}$ and $t \leq T$ do
            Play arm i
            Receive reward $X_{t}$ at time step $t$
            $S_{i}(m) \leftarrow S_{i}(m) \cup\{t\}$
        end
                $t \leftarrow t+1$
    end
Step 2: Eliminate sub-optimal arms
$\overline{X_{i}(m)}=\frac{\sum_{t \in S_{i}(m)} X_{t}}{\left|S_{i}(m)\right|}, \forall i \in A_{m}$
    $A_{m+1} \leftarrow A_{m}$
    for $i \in A_{m}$ do
        If $X_{i}(m)+\tilde{\Delta}_{m}<\max _{j \in A_{m}} X_{j}(m)$
            $A_{m+1} \leftarrow A_{m+1}-\{i\}$
    end
    Step 3: Decrease Tolerance
    $\tilde{\Delta}_{m+1} \leftarrow \frac{\tilde{\Delta}_{m}}{2}$
$m \leftarrow m+1$
end
```


### 3.1 Regret Analysis for Algorithm 1

The regret analysis closely follows from that of the UCB algorithm described in [12]. Without loss of generality we assume that the first arm is optimal. Thus we have $\mu_{1}=\mu^{*}$, and define $\Delta_{i}=\mu^{*}-\mu_{i}$. We assume that the algorithm runs for $n$ phases. Let $T_{i}(n)=\left|S_{i}(n)\right|$ denote the number of times arm $i$ is played till phase $n$. We bound $\mathbb{E}\left[T_{i}(n)\right]$ for each sub-optimal arm $i$. For this we show that the following good event holds with a high probability bound

$$
G_{i}=\left\{\mu_{1} \leq \min _{m \in[n]} B_{1}(m, \delta)\right\} \cap\left\{\hat{\mu}_{i}\left(u_{i}\right)+\sqrt{\frac{2}{T_{i}\left(u_{i}\right)} \log \left(\frac{1}{\delta}\right)} \leq \mu_{1}\right\}
$$

Here, $G_{i}$ is the event that $\mu_{1}$ is never underestimated by the upper confidence bound of the first arm, while at the same time the upper confidence bound for the mean of arm $i$, after $T\left(u_{i}\right)$ observations are taken from this arm, is below the payoff of the optimal arm. We make a claim that if $G_{i}$ occurs, then $T_{i}(n) \leq T_{i}\left(u_{i}\right)$. Since we always have $T_{i}(n) \leq T \forall i \in A$, the following holds

$$
\mathbb{E}\left[T_{i}(n)\right]=\mathbb{E}\left[\mathbb{1}\left[G_{i}\right] T_{i}(n)\right]+\mathbb{E}\left[\mathbb{1}\left[G_{i}^{c}\right] T_{i}(n)\right] \leq T_{i}\left(u_{i}\right)+P\left(G_{i}^{c}\right) T
$$

Next we bound the probability of occurrence of the complement event $G_{i}^{c}$. We present a lemma to bound the difference between true estimate of mean and the approximate one used in the algorithm. Lemma 1. If $\bar{\mu}_{i}(m)=\frac{1}{T_{i}(m)} \sum_{t \in S_{i}(m)} R_{t}(i)$ is an unbiased estimator of $\mu_{i}$ for $m^{\text {th }}$ phase, then the error from the estimated mean can be bound as $\left|\hat{\mu}_{i}(m)-\bar{\mu}_{i}(m)\right| \leq \frac{d}{k}$ where $S_{i}(m)$ is the set of time steps when arm $i$ was played and $T_{i}(n)=\left|S_{i}(n)\right|$.

The proof of Lemma 1 follows from the fact that in each phase the missing rewards from the current phase and extra reward components from the previous phase can be paired up and the maximum difference that we can obtain between them is at most one. We use Lemma- 1 to bound $P\left(G_{i}^{c}\right)$ and obtain $k=O(\sqrt{T / \log (T)})$. This gives us an upper bound on the number of times a sub-optimal arm is played $\mathbb{E}\left[T_{i}(n)\right] \leq \frac{289 \log (T)}{4 \Delta_{i}^{2}}+\frac{d}{2} \sqrt{\frac{T}{\log (T)}}+2$.

Theorem 1. For the choice of $k=O(\sqrt{T / \log (T)})$, the regret of Algorithm $\square$ is bounded by $O(\sqrt{T K \log (T)}+K d \sqrt{T / \log (T)})$.

The proof of Theorem 1 proceeds by plugging in the upper bound on $\mathbb{E}\left[T_{i}(n)\right]$ in the UCB regret analysis. We refer the readers to Appendix A for the detailed regret analysis of Algorithm 1 and proofs of Lemma 1 and Theorem 1 .

### 3.2 Regret Analysis for Algorithm 2

We use the regret analysis from Appendix-F of [16] where it is used for the setting of aggregate feedback with bounded delays. A similar analysis works for our setting of SDCAF. For completeness, we restate the analysis components, lemmas and theorems for composite rewards here. We first bound the difference between estimators for the arm mean $\mu_{i}$

Lemma 2. If $\tilde{\mu}_{i}(m)=\frac{1}{T_{i}(m)} \sum_{t \in S_{i}(m)} R_{t}(i)$ is an unbiased estimator for $\mu_{i}$ in $m^{\text {th }}$ phase, then we can bound the difference of $\tilde{\mu}_{i}(m)$ with the estimator $X_{i}(m)$ used in Algorithm 2 as $\mid \tilde{\mu}_{j}(m)-$ $X_{i}(m) \left\lvert\, \leq \frac{m(d-1)}{n_{m}}\right.$ where each arm is pulled $n_{m}$ times till phase $m, S_{i}(m)$ is the set of time steps when arm $i$ was played and $T_{i}(m)=\left|S_{i}(m)\right|=n_{m}, X_{i}(m)$ is the arm mean estimate computed from the delayed reward components.

Choice of $n_{m}$ : We use $n_{m}=O\left(\frac{\log \left(T \tilde{\Delta}_{m}^{2}\right)}{\tilde{\Delta}_{m}^{2}}+\frac{m d}{\Delta_{m}}\right)$ similar to that in [16]. This ensures a small probability for the event that after $m$ phases a suboptimal arm is still in the active set $A_{m}$. This bounds the regret contribution of all suboptimal arms. The exact expression is given in Appendix $\mathrm{B}$

Theorem 2. For the choice of $n_{m}=O\left(\frac{\log \left(T \tilde{\Delta}_{m}^{2}\right)}{\Delta_{m}^{2}}+\frac{m d}{\Delta_{m}}\right)$ the regret of Algorithm 2 is bounded by $O(\sqrt{T K \log (K)}+K d \log (T))$.

We refer the readers to Appendix $B$ (Appendix-F[16]) for the detailed regret analysis and proofs.

## 4 Conclusion and Future Work

In this work we explored the setting of stochastic multi-armed bandits with delayed, composite, anonymous feedback. Due to the nature of rewards being stochastic, anonymous feedback to the player, rewards not being available in its entirety and being arbitrarily spread, the problem becomes significantly complex than the standard MAB scenario. We show that simple extensions of the standard UCB and improved UCB algorithms which run in phases can obtain sub-linear regret bounds for this hard setting. We suggest further extensions of our work in two possible directions: the first being analysing the case when delay parameter $d$ is not perfectly known, and the second being considering a similar setting for the contextual bandits.

## References

[1] Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Mach. Learn., 47(2-3):235-256, May 2002.

[2] Peter Auer and Ronald Ortner. Ucb revisited: Improved regret bounds for the stochastic multiarmed bandit problem. Periodica Mathematica Hungarica, 61(1):55-65, Sep 2010.

[3] Nicolò Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. Nonstochastic bandits with composite anonymous feedback. In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 750-773. PMLR, 06-09 Jul 2018.

[4] Nicolò Cesa-Bianchi, Claudio Gentile, Yishay Mansour, and Alberto Minora. Delay and cooperation in nonstochastic bandits. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, pages 605-622, 2016.

[5] Ofer Dekel, Jian Ding, Tomer Koren, and Yuval Peres. Online learning with composite loss functions. CoRR, abs/1405.4471, 2014.

[6] Thomas Desautels, Andreas Krause, and Joel W. Burdick. Parallelizing explorationexploitation tradeoffs in gaussian process bandit optimization. Journal of Machine Learning Research, 15:4053-4103, 2014.

[7] Miroslav Dudík, Daniel J. Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. CoRR, abs/1106.2369, 2011.

[8] Scott Garrabrant, Nate Soares, and Jessica Taylor. Asymptotic convergence in online learning with unbounded delays. CoRR, abs/1604.05280, 2016.

[9] Pooria Joulani, Andras Gyorgy, and Csaba Szepesvari. Online learning under delayed feedback. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 1453-1461, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.

[10] Pooria Joulani, András György, and Csaba Szepesvári. Delay-tolerant online convex optimization: Unified analysis and adaptive-gradient algorithms. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16, pages 1744-1750. AAAI Press, 2016.

[11] John Langford, Alexander J. Smola, and Martin Zinkevich. Slow learners are fast. In Proceedings of the 22Nd International Conference on Neural Information Processing Systems, NIPS'09, pages 2331-2339, USA, 2009. Curran Associates Inc.

[12] T. Lattimore and C. Szepesvá. Bandit algorithms. Cambridge University Press, 2018, 2018.

[13] Travis Mandel, Yun-En Liu, Emma Brunskill, and Zoran Popovic. The queue method: Handling delay, heuristics, prior data, and evaluation in bandits. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pages 2849-2856, 2015.

[14] Chris Mesterharm. On-line learning with delayed label feedback. In ALT, 2005.

[15] Gergely Neu, András György, Csaba Szepesvári, and András Antos. Online markov decision processes under bandit feedback. In Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada., pages 1804-1812, 2010.

[16] Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, and Steffen Grunewalder. Bandits with delayed, aggregated anonymous feedback. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4105-4113, Stockholmsmässan, Stockholm Sweden, 10-15 Jul 2018. PMLR.

[17] Kent Quanrud and Daniel Khashabi. Online learning with adversarial delays. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1270-1278. Curran Associates, Inc., 2015.

[18] Claire Vernade, Olivier Cappé, and Vianney Perchet. Stochastic bandit models for delayed conversions. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017, 2017.

[19] M. J. Weinberger and E. Ordentlich. On delayed prediction of individual sequences. IEEE Trans. Inf. Theor., 48(7):1959-1976, September 2006.
