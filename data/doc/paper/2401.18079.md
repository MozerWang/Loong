# KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization 

Coleman Hooper<br>chooper@berkeley.edu<br>UC Berkeley<br>Michael W. Mahoney<br>mmahoney@stat.berkeley.edu<br>ICSI, LBNL, UC Berkeley

Sehoon Kim<br>sehoonkim@berkeley.edu<br>UC Berkeley<br>Yakun Sophia Shao<br>ysshao@berkeley.edu<br>UC Berkeley

Amir Gholami

amirgh@berkeley.edu

ICSI, UC Berkeley

Hiva Mohammadzadeh<br>hiva@berkeley.edu<br>UC Berkeley<br>Kurt Keutzer<br>keutzer@berkeley.edu<br>UC Berkeley


#### Abstract

LLMs are seeing growing use for applications which require large context windows, and with these large context windows $\mathrm{KV}$ cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing $\mathrm{KV}$ cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) PreRoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges; and (v) $Q$-Norm, where we normalize quantization centroids in order to mitigate distribution shift, providing additional benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to $\mathbf{1 0}$ million on an 8 -GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to $\sim 1.4 \times$ speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model. The code is available at https://github.com/SqueezeAILab/KVQuant/.


## 1 INTRODUCTION

Large language models (LLMs) have revolutionized many natural language processing (NLP) tasks. In order to improve the capabilities of LLMs, there is significant interest in increasing the context lengths of LLMs. Longer context lengths enable new applications, including long document summarization, retrieval for answering questions about long documents, extended multi-turn applications [4], and code analysis. To support this pull from applications, there have been significant recent advances in long-context length models in industry [1, 29], as well as in academia [4].

Given the importance of LLM workloads, there is strong motivation to improve their inference efficiency. LLM inference with large context lengths can be incredibly resource-intensive; serving LLMs requires high-end GPUs, and the largest LLMs require costly multiGPU inference setups. When analyzing the computational nature of generative inference with LLMs, it becomes quickly apparent that, for relatively small batch sizes, the computation is memory bound [18]. With the growing divergence between computational speeds and memory speeds, this problem is only going to get worse over time [13]. This makes reducing the memory bottleneck preeminently important. Further analysis shows that the memory bottleneck is strongly related to context size. For short sequence lengths, the dominant contributor to memory consumption is the weight matrices, and therefore the optimal strategy is to minimize the model size in order to reduce memory consumption as well as bandwidth requirements [18, 19]. However, for long sequence lengths, the main bottleneck is the memory requirements for caching Key and Value (KV) activations throughout inference. In particular, the size of the $\mathrm{KV}$ cache can become the dominant contributor to memory footprint, even for a $32 \mathrm{~K}$ context limit (see Table 1), making it challenging to perform long context length inference. This challenge is further exacerbated when one considers batched inference.

It is therefore crucial to develop methods for compressing the KV cache to enable efficient long-sequence length inference. Existing approaches lead to unacceptable accuracy degradation due to the outlier structures in $\mathrm{KV}$ cache activations as well as suboptimal bit allocation with existing uniform and non-uniform approaches. In this work, we perform an extensive analysis of KV cache activations in recent LLMs, revealing patterns which can be exploited to enable ultra-low precision quantization with minimal accuracy loss. In particular, we make the following contributions (summarized in Figure 1):

- We find that the Key matrices exhibit structured outliers in specific channels before applying RoPE. However, the outlier channel magnitudes become less consistent after applying RoPE, posing a distinct challenge for low precision quantization. Based on these observations, we use per-channel quantization for Keys,
and we quantize Keys before RoPE is applied (see Section 3.1 and Section 3.2).
- We find that existing uniform and non-uniform quantization methods result in sub-optimal quantization signpost placement. Instead, we propose a Non-Uniform Quantization (NUQ) method which considers sensitivity and not just magnitude when quantizing activations. We show that one can apply sensitivity-weighted non-uniform quantization offline on a calibration set to derive accurate datatypes for $\mathrm{KV}$ cache quantization (see Section 3.3).
- Even with the above, we find that outlier values in cached KV activations can significantly degrade quantization resolution. Unlike for weights, it is non-trivial to extract outlier values from activations, given the dynamic nature of activations. However, we find that we can efficiently and accurately identify and compress outlier values in order to store them compactly in a separate sparse representation. We also find that per-vector outlier detection outperforms per-matrix outlier detection with no additional memory overhead. With this method, we can attain under 0.1 perplexity degradation for 3-bit $\mathrm{KV}$ cache quantization on both Wikitext-2 and $\mathrm{C} 4$ by only removing $1 \%$ of outliers, thereby facilitating accurate inference with $4.8 \times$ longer context length.
- For ultra low-bit precision, we find that the quantized activations can deviate significantly from their corresponding fp16 values. To address this, we propose a lightweight Q-Norm layer which shifts and scales the distribution after de-quantization to match the mean and standard deviation of corresponding fp16 values. Interestingly, the Q-Norm layer can be fused with nonuniform quantization values resulting in no overhead during inference. This was particularly helpful for 2-bit quantization (see Section 3.5).
- Previous work has shown that the model tends to use the first token as an Attention Sink, meaning that the model allocates a large score towards the initial tokens even if they are not semantically important [39]. In our work, we demonstrate that the model is also sensitive to perturbation in the initial token. By keeping only the first token in higher precision, we can attain perplexity benefits relative to quantizing the first token, without introducing significant storage overhead (see Section 3.6).
- We implement custom CUDA kernels to perform activation quantization efficiently during inference, achieving up to $\sim 1.4 \times$ speedups for Key and Value matrix-vector multiplications for LLaMA-7B at 4-bit precision relative to the fp16 baseline (see Section 3.8 and 4.2). These results demonstrate how our methodology allows for accurate and efficient low-bit KV cache quantization.


## 2 BACKGROUND

### 2.1 LLM Inference

When inferring a decoder-only LLM, inference proceeds in two distinct phases. In the prefill phase, the model takes in an input prompt, which it processes in parallel. During the generation phase, the model then generates the output sequence autoregressively, meaning that each token generation is dependent on all previously generated tokens. As such, for small batch sizes, the generation phase of LLM inference is typically memory-bandwidth bound, as the only available parallelism is across different sequences in a given batch.

![](https://cdn.mathpix.com/cropped/2024_06_04_ba189d3083c90ee53cc2g-02.jpg?height=618&width=792&top_left_y=298&top_left_x=1100)

Figure 1: Overview of the different components used in KVQuant that result in less than 0.1 perplexity degradation over the fp16 baseline when quantizing the $K V$ cache to 3-bit precision. As shown in Table 2, our 3-bit approach results in $4.8 \times$ reduction in cached activation memory footprint.

Additionally, during generation, the model needs to store intermediate Key and Value activations in order to condition generations on previously generated output tokens. Otherwise, we would need to recompute all prior Keys and Values at each timestep, which would be prohibitively expensive. For each prior token, we need to store the Keys and Values at each layer in order to use these activations when generating future tokens. These stored activations are referred to as the Key-Value (KV) cache. Throughout this paper, we will capitalize Key and Value to distinguish when we are referring to the KV cache tensors. Assuming a model with $n$ layers and $h$ attention heads with dimension $d$ that is stored using $e$ bytes per element, the KV cache size for batch size $b$ and sequence length $l$ is $2 \cdot n \cdot h \cdot d \cdot e \cdot b \cdot l$, meaning that it grows linearly with both batch size and sequence length. As shown in Table 1, the KV cache becomes the dominant contributor to memory consumption for longer sequence lengths and larger batch sizes. Note that since each sequence in batched inference depends on separate past context, there is no available batch-level parallelism when loading the cached Keys and Values for their respective computations in batched inference. KV cache loading is therefore always memory-bandwidth bound. This motivates pursuing methods to optimally compress the KV cache, even at the expense of a more complex dequantization process.

### 2.2 LLM Quantization

KV Cache Quantization. There have been many prior works on LLM quantization. Several have focused on weight-only quantization for LLMs, due to the greater contribution to memory consumption and runtime for fairly small sequence length and batch size $[7,18,22]$. There has also been work on quantizing both weights and activations (including KV cache) [31, 38]. However, there is still a significant perplexity degradation when quantizing $\mathrm{KV}$ cache activations to low precision; [32, 41] quantized $\mathrm{KV}$ cache activations

Table 1: Model size and activation memory size for different sequence lengths and batch sizes (BS) for different LLaMA models. For long sequence lengths and larger batch sizes, activation memory is the main bottleneck (particularly when weights are already quantized to low precision). At a sequence length of $128 \mathrm{~K}$ with the LLaMA-7B model, the KV cache is the main bottleneck (see below left). Additionally, if model weights are quantized, then the $K V$ cache is the main bottleneck even at a sequence length of $32 K$. By compressing the $K V$ cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system.

SeqLen 512, Batch Size 1

![](https://cdn.mathpix.com/cropped/2024_06_04_ba189d3083c90ee53cc2g-03.jpg?height=299&width=461&top_left_y=604&top_left_x=233)

Short sequence length

Weights are the bottleneck
SeqLen 128 K, Batch Size 1

![](https://cdn.mathpix.com/cropped/2024_06_04_ba189d3083c90ee53cc2g-03.jpg?height=290&width=355&top_left_y=603&top_left_x=668)

Long Sequence Lengths KV Cache is the bottleneck to 4-bits, but required fine-grained grouping for 4-bit quantization, while still observing some perplexity degradation, and [32] observed that 3-bit KV cache quantization with fine-grained grouping leads to unacceptable accuracy loss. Other works quantized $\mathrm{KV}$ cache activations to 4 -bits but required retraining to maintain performance [24]. One concurrent work also explores low precision $\mathrm{KV}$ cache quantization in order to enable larger batch size inference by reducing the $\mathrm{KV}$ cache size [26]. In this work, we introduce a method for near-lossless low-bit KV cache quantization that minimizes performance degradation without the need for finetuning.

Outlier-Aware LLM Quantization. LLMs have been known to have distinct outliers both in weights and activations [5, 7, 18]. SqueezeLLM and SpQR both decompose the weight matrix into a sparse matrix containing a small portion of outliers and a dense matrix that can be accurately quantized to low precision (referred to as dense-and-sparse or sparse-quantized representation) [7, 18]. LLM.int8() [5] handled particular outlier channels separately in higher precision, and SmoothQuant [38] migrates quantization difficulty due to outlier channels to weights in order to support joint weight-activation quantization. Other works reconsidered the dimension along which we quantize in order to reduce quantization error (or else added per-channel compensation to improve quantization performance) [2, 16, 36, 37]. In this work, we demonstrate that per-channel pre-RoPE Key quantization provides significant accuracy benefits given the outlier structure in Keys, and that denseand-sparse quantization can be efficiently applied for KV cache quantization.

Non-uniform LLM Quantization. Non-uniform quantization has also been applied in the context of LLMs. Non-uniform quantization allows for more flexible quantization signpost placement relative to uniform quantization methods, enabling improved accuracy for the same bit precision [6,18]. Building on the observation that model parameters tend to be approximately normally-distributed, prior work has proposed the NormalFloat datatype [6]. SqueezeLLM [18] derived per-channel non-uniform quantization signposts using a sensitivity-weighted k-means approach. In this work, we show that we can derive accurate per-layer non-uniform datatypes using

| BS | \# Params | Model Size (GB) <br> 16-bit $\rightarrow$ 2-bit | fp16 KV Cache Size with different seq len (GB) |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | $32 \mathrm{~K}$ | $128 \mathrm{~K}$ | $1 \mathrm{M}$ | $10 \mathrm{M}$ (16-bit $\rightarrow 2$-bit) |
| 1 | 7B | $12.6 \rightarrow 1.6$ | 16 | 64 | 512 | $4883 \rightarrow 610$ |
|  | 13B | $24.1 \rightarrow 3.0$ | 25 | 100 | 800 | $7629 \rightarrow 954$ |
|  | $30 \mathrm{~B}$ | $60.3 \rightarrow 7.5$ | 49 | 195 | 1560 | $14877 \rightarrow 1860$ |
|  | 65B | $121.1 \rightarrow 15.1$ | 80 | 320 | 2560 | $24414 \rightarrow 3052$ |
| 4 | $7 \mathrm{~B}-2$ | $12.6 \rightarrow 1.6$ | 64 | 256 | 2048 | $19531 \rightarrow 2441$ |
|  | $13 \mathrm{~B}$ | $24.1 \rightarrow 3.0$ | 100 | 400 | 3200 | $30518 \rightarrow 3815$ |
|  | $30 \mathrm{~B}$ | $60.3 \rightarrow 7.5$ | 195 | 780 | 6240 | $59509 \rightarrow 7439$ |
|  | 65B | $121.1 \rightarrow 15.1$ | 320 | 1280 | 10240 | $97656 \rightarrow 12207$ |

a sensitivity-weighted k-means approach with $\mathrm{KV}$ cache activations.

### 2.3 KV Cache Compression

There have also been several prior works on compressing the KV cache. Some of these methods aim to only store important tokens in the KV cache and to evict less important tokens, thereby maintaining low memory usage $[12,25,40]$. Other methods aim to only retrieve a subset of tokens at each step to achieve memory bandwidth savings [30]. In this work, we explore KV cache quantization as an orthogonal direction for compressing the $\mathrm{KV}$ cache in order to enable long context inference.

## 3 METHOD

### 3.1 Per-Channel Key Quantization

To inform our approach, we first performed a detailed analysis to understand the KV cache distributions. Figure 2 shows sample distributions for the $\mathrm{KV}$ cache activations. We observe that the Key matrices tend to have distinct outlier channels, which have larger average magnitudes than other channels; this corroborates previous observations about outlier channels in LLM activations $[5,38]$. The Value matrices exhibit both outlier channels as well as outlier tokens (although these outliers are less extreme than the outlier Key channels).

Existing $\mathrm{KV}$ cache quantization approaches perform per-token quantization (meaning that the scaling factor and zero-point are shared by elements in the same token) [32, 41]. However, due to the differing average magnitudes between channels, the values within a channel are easier to quantize when grouped together than the values across different channels. As such, to better match the distributions, we investigate per-channel $\mathrm{KV}$ cache quantization, meaning that the scaling factor and zero-point are shared by elements in the same channel. By sharing the scaling factor and zero-point along the channel dimension, this will naturally group together values with similar magnitudes, thereby mitigating the impacts of outlier channels on other channels when quantizing to
![](https://cdn.mathpix.com/cropped/2024_06_04_ba189d3083c90ee53cc2g-04.jpg?height=432&width=1736&top_left_y=290&top_left_x=194)

Figure 2: Example distributions of the activation values for Keys pre-RoPE, Keys post-RoPE, and Values for LLaMA-7B on a sample with $2 K$ sequence length from Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers in specific channels across different tokens; (ii) after applying RoPE, the distribution becomes less structured and there are less consistent magnitudes for outlier channels (this is expected, as RoPE applies a rotation operation between pairs of channels); and (iii) Values exhibit no fixed outlier pattern with outlier values across channels and tokens.

low precision. As outlined in Appendix D, we find that per-channel quantization provides significant accuracy benefits for Keys but not for Values. By leveraging per-channel quantization for Keys and per-token quantization for Values, we observe a 3.88 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. Note that this can potentially add runtime overhead since the quantization dimension is now misaligned with the reduction dimension for the Keys when performing matrix-vector multiplications. However, we find that we are able to efficiently dequantize Keys and perform the Query-Key matrix-vector multiplication without adding runtime overhead, as shown in Section 4.2. Additionally, as outlined in Section 3.7, per-channel quantization can also be challenging due to the need to recompute scaling factors as tokens are added to the Key cache. We show that we can calibrate offline for scaling factors, thereby avoiding expensive online recomputation.

Per-channel Key quantization was also explored in another concurrent work [26], which leveraged similar intuition about grouping together large magnitude values in the same channel to minimize quantization error. Their methodology requires fine-grained grouping for per-channel quantization while maintaining a residual subset of the KV cache in fp16 (until all elements for that group have been added to the KV cache). In our work, we demonstrate that by leveraging offline calibration, we can accurately perform per-channel quantization without grouping.

### 3.2 Pre-RoPE Key Quantization

One issue when quantizing Keys is handling the rotary positional embedding (RoPE), which is applied to Keys and Queries in most public LLMs, including LLaMA and LLaMA-2 [33]. Given Query and Key vectors $Q_{m}=W_{q} * x_{m}$ and $K_{n}=W_{k} * x_{n}$ at positions $m$ and $n$ in the sequence, RoPE is applied as position-dependent rotations to each of these vectors to obtain $\tilde{Q}_{m}$ and $\tilde{K}_{n}$. This embeds the relative position between a Query and Key vector as an amount of an angle that is a multiple of its position index. Formally, RoPE is applied in self-attention as follows:

$$
\begin{equation*}
\tilde{Q}_{m} \tilde{K}_{n}^{\top}=\left(R_{\theta, m}^{d} \cdot Q_{m}\right)\left(R_{\theta, n}^{d} \cdot K_{n}\right)^{\top} \tag{1}
\end{equation*}
$$

The Query vectors computed at each iteration will have RoPE applied ( $\left.\tilde{Q}_{m}\right)$. When caching Key vectors, we need to either cache $\tilde{K}_{n}$, or else we need to cache $K_{n}$ and apply $R_{\theta, n}^{d}$ on-the-fly during inference. The challenge with caching Key vectors after applying this rotation is that it leads to mixing pairs of channels by different amounts for different positions in the sequence, as shown in Appendix A (since it jointly rotates pairs of channels by different angles depending on the position in the sequence). The post-RoPE activation distribution is also shown in Figure 2, demonstrating how the rotation between pairs of channels leads to less consistent channel magnitudes. This makes it harder to quantize Key activation channels which would typically have consistent large-magnitude values. This motivated our investigation into whether we could perform pre-RoPE Key quantization (meaning that we quantize $K_{n}$ ) and then efficiently apply the positional embeddings on-the-fly after dequantization. The benefits of pre-RoPE Key quantization are highlighted in Appendix E, yielding 0.65 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. To be able to quantize Keys pre-RoPE, we develop a fused kernel to efficiently apply RoPE post-dequantization (the details of this approach will be discussed in Section 3.8).

## 3.3 nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype

Uniform quantization is suboptimal for $\mathrm{KV}$ cache quantization since the Query and Key activations are non-uniform. Additionally, KV cache loading is memory bandwidth bound, regardless of batch size or sequence length, meaning that the dequantization overhead introduced by non-uniform quantization methods is not problematic (since the added computation does not introduce any additional latency). It is therefore desirable to leverage non-uniform quantization methods for $K V$ cache quantization.

In [18], the authors computed non-uniform quantization signposts using a sensitivity-weighted k-means approach. However,

![](https://cdn.mathpix.com/cropped/2024_06_04_ba189d3083c90ee53cc2g-05.jpg?height=263&width=612&top_left_y=297&top_left_x=407)

Challenge: need to potentially recompute the scaling factor every time a new key is added if we compute it online

## Per-token Quantization

![](https://cdn.mathpix.com/cropped/2024_06_04_ba189d3083c90ee53cc2g-05.jpg?height=228&width=628&top_left_y=336&top_left_x=1025)

Challenge: Need to compute the scaling factor for each incoming token

Figure 3: One typically achieves better performance when the scaling factor/zero point are computed online. However, this is quite challenging to do for per-channel quantization, as these factors will not only need to be recomputed for every new Key appended to the Key cache, but also all the prior cached Keys will need to be updated. As such, we use a calibration set to compute per-channel scaling factors offline. A similar challenge exists for per-token quantization, but online calibration for this does not require updating prior cached Values. In Section 3.7 and Appendix 7, we discuss how we are able to efficiently compute outlier thresholds / scaling factors for per-token calibration, thereby enabling online computation.

this is challenging to apply online during inference due to its computational cost, and it is also difficult to estimate sensitivity for activations online. We therefore facilitate efficient online non-uniform $\mathrm{KV}$ cache quantization by computing sensitivity-weighted quantization signposts offline on a calibration set prior to inference. Using the diagonal Fisher information matrix (derived in Appendix C), along with the quantization error for activation $A$, we formulate the error minimization objective as:

$$
\begin{equation*}
Q(A)^{*} \simeq \underset{Q}{\arg \min } \sum_{i=1}^{N} \mathcal{F}_{i i}(A-Q(A))^{2} \tag{2}
\end{equation*}
$$

In order to derive a per-matrix non-uniform datatype, we first normalize each vector to the range $[-1,1]$. We then minimize the objective in Equation 2 offline on a calibration set using a k-means solver in order to obtain the quantization signposts for the nonuniform datatype for each Key or Value layer. Appendix F compares our non-uniform quantization approach with existing uniform and non-uniform quantization baselines [6], demonstrating how our non-uniform approach provides 0.33 perplexity improvement on Wikitext-2 for LLaMA-7B relative to 3-bit uniform methods. Table 16 in Appendix J shows how computing the required Fisher information for the LLaMA-65B model takes only a few minutes, and how using the k-means solver takes only a few minutes per layer (with the computation for each layer being parallelizable). In Appendix P, we also demonstrate that we can derive a metric for accurate one-shot mixed-precision assignment (where different layers are assigned different bit widths) using the quantization error weighted by sensitivity information.

### 3.4 Per-Vector Dense-and-Sparse Quantization

Figure 5 in Appendix B shows the portion of elements falling within different percentiles of the dynamic range. For both Keys and Values, the majority of elements are contained within a small percentage of the dynamic range. This means that by leveraging dense-and-sparse quantization, as demonstrated in [18], in order to isolate a small percentage of numerical outliers, we can restrict the range that we need to represent, thereby allowing us to represent the remaining elements with greater precision.
Additionally, when looking at the Key and Value distributions in Figure 2, different channels and tokens have different average magnitudes. Therefore, an element which counts as an outlier in one channel may not be an outlier in another channel (since that channel may have a greater average magnitude). It is therefore crucial to directly target the outlier values that skew the dynamic range at the granularity that we are quantizing in order to address the values that are exaggerating the range along that particular dimension. In this work, we leverage per-vector dense-and-sparse quantization, where we use a different outlier threshold per-vector (either a separate threshold per-channel for per-channel quantization, or a separate threshold per-token for per-token quantization), rather than a single outlier threshold for each layer.

Note that computing outlier thresholds for per-vector denseand-sparse quantization poses potential accuracy and efficiency challenges. However, in Section 3.7, we show that we are able to accurately calibrate for per-channel outlier thresholds offline and efficiently compute per-token outlier thresholds online. After determining the upper and lower outlier thresholds, the remaining numbers in the vector are normalized to the range $[-1,1]$, and we then minimize Equation 2 (ignoring outliers) in order to obtain the quantization signposts for the non-uniform datatype for the remaining numbers. Appendix $G$ will demonstrate the benefits of removing a small percentage of numerical outliers and keeping them in full precision, as well as the advantages of per-vector dense-and-sparse quantization over using a single outlier threshold for each layer. As shown in Figure 1, by removing $1 \%$ of numerical outliers using per-vector outlier thresholds, we achieve an additional 0.25 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization, which is within 0.08 perplexity of the fp16 baseline.

### 3.5 Mitigating Distribution Shift using Q-Norm

When pushing to extremely low bit widths like 2-bit quantization, we begin to observe accuracy degradation due to distribution shift, meaning that the post-quantization distribution has a different mean and standard deviation than the pre-quantization distribution. As shown in Appendix H, this distribution shift can lead to greater error accumulation at later layers. To combat this, we introduce

Table 2: Evaluation of our method for different models using the perplexity on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. $K V$ cache sizes assume a sequence length of $128 \mathrm{~K}$ (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We incorporate Q-Norm for 2-bit experiments, and we leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix $N$. Table 19 in Appendix M demonstrates a full evaluation on all LLaMA, LLaMA2, and Mistral models.

| Method | LLaMA-7B |  | LLaMA-13B |  | LLaMA-30B |  | LLaMA-65B |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Perplexity | KV Cache (GB) | \| Perplexity | KV Cache (GB) | Perplexity | KV Cache (GB) | Perplexity | KV Cache (GB) |
| baseline | 5.68 | 64.0 | 5.09 | 100.0 | 4.10 | 195.0 | 3.53 | 320.0 |
| int4 | 5.98 | 16.0 | 5.32 | 25.0 | 4.34 | 48.8 | 3.73 | 80.1 |
| $\mathrm{nf} 4$ | 5.87 | 16.0 | 5.23 | 25.0 | 4.25 | 48.8 | 3.63 | 80.1 |
| ATOM-4bit | 5.77 | 16.6 | 5.16 | 26.0 | 4.16 | 50.7 | 3.57 | 83.1 |
| FlexGen-4bit | 5.73 | 17.3 | 5.14 | 27.0 | 4.14 | 52.6 | 3.56 | 86.3 |
| KVQuant-4bit | 5.72 | 16.0 | 5.13 | 25.0 | 4.14 | 48.8 | 3.56 | 80.0 |
| KVQuant-4bit-1\% | 5.69 | 17.3 | 5.10 | 27.0 | 4.11 | 52.7 | 3.54 | 86.5 |
| int3 | 10.87 | 12.0 | 8.69 | 18.8 | 6.82 | 36.6 | 6.37 | 60.1 |
| nf3 | 7.33 | 12.0 | 6.21 | 18.8 | 5.46 | 36.6 | 4.44 | 60.1 |
| ATOM-3bit | 6.17 | 12.6 | 5.47 | 19.7 | 4.44 | 38.4 | 3.78 | 63.0 |
| FlexGen-3bit | 5.93 | 13.2 | 5.29 | 20.6 | 4.26 | 40.2 | 3.66 | 65.9 |
| KVQuant-3bit | 5.89 | 12.0 | 5.25 | 18.8 | 4.30 | 36.6 | 3.63 | 60.0 |
| KVQuant-3bit-1\% | 5.75 | 13.3 | 5.15 | 20.8 | 4.16 | 40.5 | 3.57 | 66.5 |
| int2 | 11779 | 8.0 | 69965 | 12.5 | 1470 | 24.4 | 7272 | 40.1 |
| nf2 | 3210 | 8.0 | 5786 | 12.5 | 2044 | 24.4 | 5367 | 40.1 |
| ATOM-2bit | 37.37 | 8.6 | 41.77 | 13.4 | 16.49 | 26.1 | 13.63 | 42.8 |
| FlexGen-2bit | 11.09 | 9.1 | 9.84 | 14.3 | 6.60 | 27.8 | 5.54 | 45.6 |
| KVQuant-2bit | 7.15 | 8.0 | 5.81 | 12.5 | 4.84 | 24.4 | 4.38 | 40.0 |
| KVQuant-2bit-1\% | 6.02 | 9.3 | 5.37 | 14.5 | 4.36 | 28.3 | 3.72 | 46.5 |

$Q$-Normalization (shortened to $Q$-Norm), where we normalize the quantization centroids obtained from k-means in order to ensure the post-quantization distribution has the same mean and standard deviation as the pre-quantization distribution. Our solution is inspired by [21], which demonstrated that ensuring that the activation distributions post-weight quantization have similar mean and standard deviation to the activation distributions pre-weight quantization can help reduce accuracy degradation. Given mean $\mu_{1}$ and standard deviation $\sigma_{1}$ for the pre-quantization distribution and mean $\mu_{2}$ and standard deviation $\sigma_{2}$ for the post-quantization distribution (computed offline on a calibration set), we normalize the quantization centroids $C_{i}$ to $\hat{C}_{i}$ as follows:

$$
\begin{equation*}
\hat{C}_{i}=\frac{\left(C_{i}-\mu_{2}\right) \sigma_{1}}{\sigma_{2}}+\mu_{1} \tag{3}
\end{equation*}
$$

During dequantization, we use $\hat{C}_{i}$ in place of $C_{i}$ such that the mean and standard deviation of dequantized values matches the original fp16 distribution. As shown in Appendix H, Q-Norm provides significant accuracy benefits for 2-bit quantization with no added inference cost.

### 3.6 Attention Sink-Aware Quantization

Prior work has demonstrated that after the first few layers in LLMs, the model tends to allocate a large attention score to the first token [39]. This occurs even when the initial token is not semantically important. This phenomenon happens because the model tends to use the inital token as a "sink". [39] demonstrates how keeping a few initial sink tokens allows for accurate performance for streaming applications. In our work, we demonstrate that due to the Attention Sink phenomenon, the model is disproportionately sensitive to quantization error in the first token. By keeping only the first token in fp16, we can attain significant perplexity benefits, particularly for 2-bit quantization. A similar observation has also been made in another concurrent work of [23] in the context of weight quantization. Note that when retaining the first token in fp16, we account for this during the calibration process as well, meaning that we ignore the first token when deriving the nuqX datatype and when calibrating the scaling factors and zero points offline for the Keys. As demonstrated in Appendix I, this approach persistently yields performance benefits, particularly with lower bits and without dense-and-sparse quantization.

### 3.7 Offline Calibration versus Online Computation

A crucial challenge for activation quantization is that we either need to compute statistics on-the-fly (which is potentially expensive) or else we need to use offline calibration data (which potentially has negative accuracy implications). The challenges with computing scaling factors (and zero-points) online versus offline for both Keys and Values are shown in Figure 3. In per-channel quantization, it is challenging to update scaling factors online since the scaling factors corresponding to each incoming channel would potentially need to be updated whenever a new token is added to the $\mathrm{KV}$ cache. It is therefore desirable to be able to compute statistics offline (i.e., using calibration data before running inference). While this can have negative effects on model accuracy, in Appendix J we show that we can effectively calibrate offline for per-channel quantization, obviating the need for online updates of scaling factors for perchannel quantization. For per-token quantization, it is challenging to calibrate for scaling factors offline due to the presence of outlier Value tokens. It is therefore desirable to be able to compute scaling factors and outlier thresholds online for each incoming token. As shown in Appendix J, we can efficiently compute outlier thresholds online per-token by offloading to the CPU. By leveraging custom quantization function implementations for compressing activations,
we are able to perform online per-token Value quantization without compromising on performance.

### 3.8 Kernel Implementation

In order to efficiently perform activation quantization on-the-fly, we leverage dedicated kernel implementations with our 4-bit quantization method for compressing vectors to reduced precision and extracting the sparse outliers, performing matrix-vector multiplications using the compressed vectors, and performing sparse matrixdense vector multiplications using the sparse outliers. We store the quantized Key and Value matrices as 4-bit elements which are used as indices into lookup tables to recover the corresponding fp16 values. We store the sparse outlier matrices in either CompressedSparse Row (CSR) or Compressed-Sparse Column (CSC) format (depending on which aligns better with appending new Key and Value tokens). The kernels for the Key matrix-vector operations apply RoPE on-the-fly in order to support pre-RoPE quantization More kernel implementation details are provided in Appendix Q.

## 4 RESULTS

### 4.1 Main Results

4.1.1 Main Evaluation. We used the LLaMA-7B/13B/30B/65B, LLaMA2-7B/13B/70B, and Mistral-7B models to evaluate our methodology by measuring perplexity on both Wikitext-2 and C4 [17, 34, 35]; see Appendix K for details on our experimental setup. Table 2 shows the results for LLaMA models for the Wikitext-2 dataset. We compared our method with per-token quantization with and without grouping. The baseline configurations used by Atom and FlexGen are included for reference [32, 41]. We did not include results for KIVI [26] since they did not report perplexity values. We find that our method consistently outperforms baseline approaches by an especially large margin with 3-bit and 2-bit quantization. Once we incorporate outliers, we further push the performance of low-precision quantization, achieving 4-bit quantization with less than 0.02 perplexity degradation, 3 -bit quantization with under 0.07 perplexity degradation, and 2-bit quantization with under 0.38 perplexity degradation on Wikitext-2, relative to the fp16 baseline, across all models (while attaining $3.7 \times$, $4.8 \times$, and $6.9 \times$ memory savings, respectively). Table 20 in Appendix M also provides zero-shot MMLU evaluation with 3-bit quantization, demonstrating how our method maintains performance on downstream tasks.

4.1.2 Long Context Length Evaluation. We evaluated long context length performance using the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation [3]) as well as the LLaMA-2-70B-32K LongLoRA model [4]. For evaluating performance on longer context lengths, we first evaluated perplexity on Wikitext-2 using larger amounts of input context, as shown in Figure $4[4,14]$. The results demonstrate how our method maintains accuracy even for longer amounts of input context, thereby enabling efficient and accurate long sequence length inference.

We also evaluated the performance of our quantization method on passkey retrieval to assess the model's ability to use its context. Passkey retrieval involves evaluating the model's capacity to locate specific information in long texts [20], and this can be used to effectively measure the maximum distance over which a
Table 3: Passkey retrieval results across different context lengths for the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation [3]) as well as the LLaMA-2-70B-32K LongLoRA model [4]. The values reported are the success rate for retrieving the passkey, computed over 50 samples. 2-bit results are with per-matrix $Q$-Norm enabled.

| Model | Method | 2K | 4K | 8K | $\mathbf{1 6 K}$ | $\mathbf{3 2 K}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMA-2-7B-32K | fp16 | 1 | 1 | 1 | 1 | 1 |
|  | nuq4-1\% | 1 | 1 | 1 | 1 | 1 |
|  | nuq3-1\% | 1 | 1 | 1 | 1 | 1 |
|  | nuq2-1\% | 1 | 1 | 1 | 0.92 | 0.98 |
| LLaMA-2-70B-32K | fp16 | 1 | 1 | 1 | 1 | 1 |
|  | nuq4-1\% | 1 | 0.98 | 1 | 1 | 1 |
|  | nuq3-1\% | 1 | 0.98 | 1 | 1 | 1 |
|  | nuq2-1\% | 0.82 | 0.8 | 0.82 | 0.8 | 0.76 |

Table 4: $K V$ cache quantization results when KVQuant is applied in conjunction with the weight quantization methodology in SqueezeLLM [18]. w4-s45 and w3-s45 for weights refer to the 4-bit and 3-bit dense-and-sparse weight quantization approaches in [18], respectively. See Appendix K for experimental details.

| Weights | KV Cache | LLaMA-7B | LLaMA-13B | Avg. Bits (KV Cache) |
| :---: | :---: | :---: | :---: | :---: |
| fp16 | fp16 | 5.68 | 5.09 | 16 |
| w4-s45 | fp16 | 5.77 | 5.17 | 16 |
|  | nuq4 | 5.83 | 5.22 | 4.00 |
|  | nuq4-1\% | $\mathbf{5 . 7 9}$ | $\mathbf{5 . 1 8}$ | $4.32-4.33$ |
| w3-s45 | fp16 | 6.13 | 5.45 | 16 |
|  | nuq3 | 6.55 | 5.77 | 3.00 |
|  | nuq3-1\% | $\mathbf{6 . 2 6}$ | $\mathbf{5 . 5 4}$ | $3.32-3.33$ |

token can attend during the inference stage. We used the passkey evaluation framework from [42] (which is based on the methodology from [27]) to evaluate retrieval performance. Table 3 shows passkey retrieval results for the LLaMA-2-7B-32K and LLaMA-270B-32K models. These results indicate that with 4-bit and 3-bit quantization, KVQuant is able to maintain retrieval performance of the fp16 model. We observe some degradation with 2-bit quantization especially with LLaMA-2-70B, indicating potential room for improvement in 2-bit quantization techniques.

4.1.3 Joint Weight and $K V$ Cache Quantization. Table 4 shows results for our KV cache quantization method when the weights are also quantized using the methodology in SqueezeLLM [18]. We observe minimal perplexity degradation when leveraging our KV cache quantization approach, even when weights are also quantized to reduced precision (achieving within 0.02 perplexity of 4 -bit weight-only quantization when quantizing the KV cache using nuq4-1\% for the LLaMA-7B and LLaMA-13B models). These results demonstrate how our method is compatible with existing weightonly quantization methods.

### 4.2 Performance Analysis

Table 5 shows kernel benchmarking results using a batch size of 1 for the 4 -bit dense-and-sparse compression and matrix-vector kernel implementations. We show results across different sequence lengths to assess the performance of the kernels at different points during generation. We report latency benchmarked on an A6000 GPU and averaged over 1000 runs. The results show that for the
![](https://cdn.mathpix.com/cropped/2024_06_04_ba189d3083c90ee53cc2g-08.jpg?height=544&width=1766&top_left_y=281&top_left_x=168)

Figure 4: Perplexity results for the LLaMA-2-7B-32K model [3] as well as the LLaMA-2-70B-32K LongLoRA model [4] on the Wikitext-2 dataset, evaluated using different sequence lengths.

Table 5: Average latency (in microseconds) for the Key and Value nuq4-1\% kernels, benchmarked on an A6000 GPU for the LLaMA7B model across different sequence lengths (l). fp16 matrix-vector multiplication latencies are included for reference, and the fp16 Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. Section 3.8 and Appendix $Q$ provide additional details for our kernel implementation, and Table 22 provides a detailed breakdown of kernel runtime.

| Activation | Operation | $\boldsymbol{l}=\mathbf{2 K}$ | $\boldsymbol{l}=\mathbf{4 K}$ | $\boldsymbol{l}=\mathbf{1 6 K}$ |
| :---: | :---: | :---: | :---: | :---: |
| Key | fp16 Matvec | 66.4 | 116.1 | 402.3 |
| Key | nuq4-1\% | 60.2 | 96.7 | 344.1 |
| Value | fp16 Matvec | 56.0 | 104.2 | 389.3 |
| Value | nuq4-1\% | 40.8 | 85.8 | 293.4 |

Key and Value multiplications, we can achieve 1.1-1.2Ã— and 1.2$1.4 \times$ latency savings, respectively, relative to the baseline. We have integrated these kernels into an end-to-end generation pipeline that is able to compress activations dynamically during inference, thereby achieving significant memory savings and allowing for either larger batch sizes or longer sequence lengths.

### 4.3 Pushing the Context Length Limit

Table 6 shows the $\mathrm{KV}$ cache memory requirements for $128 \mathrm{~K}, 1 \mathrm{M}$, and $10 \mathrm{M}$ sequence lengths, with the $\mathrm{KV}$ cache stored in $\mathrm{fp} 16$ as well as 4-bit, 3-bit, and 2-bit precision with KVQuant. As one can see, our method provides $3.7 \times \mathrm{KV}$ cache compression (nuq4-1\%) and enables serving the quantized LLaMA-65B model with a context length of $32 \mathrm{~K}$ tokens on a single A100-80GB GPU (requiring $30.3 \mathrm{~GB}$ for the model weights compressed to 4 -bit, and $46.5 \mathrm{~GB}$ for the $\mathrm{KV}$ cache when compressed with nuq2-1\%), and our nuq2 method enables serving the LLaMA-7B model with a context length of 1M tokens on a single A100 GPU (requiring 64GB for the KV cache). Additionally, when considering an 8-GPU serving system, we enable serving the LLaMA-7B model with $10 \mathrm{M}$ context length (with nuq2), or the LLaMA-65B model with 1M context length (with nuq3). Our results show little degradation compared to baseline fp16 inference while providing significant compression, demonstrating
Table 6: Activation memory size (GB) for $128 \mathrm{~K}, 1 \mathrm{M}$, and $10 \mathrm{M}$ sequence length (l) for different LLaMA models. By compressing the KV cache to 2-bit precision, we can enable $1 M$ context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8 -GPU system.

| Model | Method | $\boldsymbol{l}=\mathbf{1 2 8 K}$ | $\boldsymbol{l}=\mathbf{1 M}$ | $\boldsymbol{l}=\mathbf{1 0 M}$ |
| :---: | :---: | :---: | :---: | :---: |
| LLaMA-7B | fp16 | 64.0 | 512.0 | 4882.8 |
|  | nuq4 | 16.0 | 128.1 | 1221.9 |
|  | nuq4-1\% | 17.3 | 138.4 | 1319.6 |
|  | nuq3 | 12.0 | 96.1 | 916.7 |
|  | nuq3-1\% | 13.3 | 106.4 | 1014.4 |
|  | nuq2 | 8.0 | 64.1 | 611.5 |
|  | nuq2-1\% | 9.3 | 74.4 | 709.2 |
| LLaMA-65B | fp16 | 320.0 | 2560.0 | 24414 |
|  | nuq4 | 80.0 | 640.3 | 6106.5 |
|  | nuq4-1\% | 86.5 | 691.5 | 6595.0 |
|  | nuq3 | 60.0 | 480.3 | 4580.6 |
|  | nuq3-1\% | 66.5 | 531.5 | 5069.1 |
|  | nuq2 | 40.0 | 320.3 | 3054.7 |
|  | nuq2-1\% | 46.5 | 371.5 | 3543.3 |

the benefits of our approach for enabling accurate and efficient long sequence length inference.

## 5 CONCLUSION

As context lengths in LLMs increase, the $\mathrm{KV}$ cache activations surface as the dominant contributor to memory consumption. Quantization is a promising approach to reduce the size of $\mathrm{KV}$ cache activations, but prior solutions failed to represent activations accurately in ultra-low precisions, such as sub-4-bit. In contrast, we achieve accurate ultra-low precision KV cache quantization. By quantizing Keys per-channel before applying RoPE, we are able to better match the outlier distribution and mitigate the impacts of RoPE on quantization (due to it mixing pairs of channels which may have different average magnitudes). We use non-uniform quantization to better allocate the small number of quantization signposts at low precision. We observe significant accuracy improvements when employing dense-and-sparse quantization, particularly when detecting outliers at the same granularity as we compute quantization scaling factors. Crucially, we demonstrate that we can perform accurate
calibration offline for Keys, as well as efficient online scaling factor and outlier threshold computation for Values. By leveraging these methods, we are able to enable accurate low-precision activation quantization, achieving $4.8 \mathrm{x}$ compression (nuq3-1\% outliers) with only 0.1 perplexity degradation across different LLaMA, LLaMA-2, and Mistral models. Additionally, we show that Q-Norm improves accuracy for 2-bit quantization by helping mitigate distribution shift. Our methodology therefore supports inferring the LLaMA-7B model with a context length of $\mathbf{1 0 M}$ on an 8-GPU serving system. Through our efficient kernel implementation, we are able to show improved latency relative to the fp16 baseline, demonstrating how our method allows for improved latency in addition to the memory savings.

## 6 LIMITATIONS

While our work enables accurate long-context length inference by reducing the memory requirements, there is significant work required for training long context length models with greater than $100 \mathrm{~K}$ context length. This work is orthogonal to our efforts, which are constrained to efficient inference with long context length models. Additionally, our latency benchmarking results currently focus on memory-bandwidth bound generation rather than prompt processing (where we need to compress multiple Keys and Values at once). In future work, we plan to develop dedicated efficient kernels for block Key/Value compression in order to efficiently compress multiple Keys and Values at once. Finally, in the current end-to-end implementation, there are inefficiencies in how memory allocation is handled for updating the sparse matrix (where the data corresponding to the previous tokens have to be copied when concatenating them with the data from the new token). In future work, we plan to optimize this by doing blocked allocation to avoid overheads from reallocating memory.

## 7 ACKNOWLEDGEMENTS.

The authors would like to acknowledge Nicholas Lee for helpful discussions and feedback. We acknowledge gracious support from Intel, Furiosa, Apple, and Samsung SAIT. We also appreciate the support from Microsoft through their Accelerating Foundation Model Research, including great support from Sean Kuno. Furthermore, we appreciate support from Google Cloud, the Google TRC team, and specifically Jonathan Caton, and Prof. David Patterson. Prof. Keutzer's lab is sponsored by the Intel corporation, Intel One-API, Intel VLAB team, the Intel One-API center of excellence, as well as funding through BDD and BAIR. We appreciate great feedback and support from Ellick Chan, Saurabh Tangri, Andres Rodriguez, and Kittur Ganesh. Sehoon Kim would like to acknowledge the support from the Korea Foundation for Advanced Studies (KFAS). Amir Gholami was supported through funding from Samsung SAIT. Michael W. Mahoney would also like to acknowledge a J. P. Morgan Chase Faculty Research Award as well as the DOE, NSF, and ONR Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.

## REFERENCES

[1] Anthropic. Introducing claude 2.1, Nov 2023.
[2] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint $\operatorname{arXiv:2109.12948,2021.}$

[3] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.

[4] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023.

[5] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.

[6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.

[7] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023.

[8] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. Advances in neural information processing systems, 33:18518-18529, 2020 .

[9] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 293-302, 2019 .

[10] Goran Flegar and Enrique S Quintana-OrtÃ­. Balanced csr sparse matrix-vector product on graphics processors. In Euro-Par 2017: Parallel Processing: 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28-September 1, 2017, Proceedings 23, pages 697-709. Springer, 2017.

[11] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.

[12] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023.

[13] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, and Kurt Keutzer. Ai and memory wall. RiseLab Medium Post, 2021.

[14] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.

[15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.

[16] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models. arXiv preprint arXiv:2309.15531, 2023.

[17] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023 .

[18] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.

[19] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023.

[20] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of opensource llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.

[21] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm tweaking: Highperformance low-bit quantization of large language models. arXiv preprint arXiv:2309.02784, 2023.

[22] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023

[23] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact, 2024.

[24] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llmqat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023.

[25] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023.

[26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization. 2023.

[27] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023.

[28] Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018.

[29] OpenAI. New models and developer products announced at devday 2023, Nov 2023.

[30] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023.

[31] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023.

[32] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher RÃ©, Ion Stoica, and Ce Zhang. Flexgen: Highthroughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094-31116. PMLR, 2023.

[33] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.

[34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023

[35] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos ale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[36] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023.

[37] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:17402-17414, 2022.

[38] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 3808738099. PMLR, 2023 .

[39] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023.

[40] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher RÃ©, Clark Barrett, et al. H 2 o Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023,

[41] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate $1 \mathrm{~lm}$ serving. arXiv preprint arXiv:2310.19102, 2023.

[42] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skipwise training. arXiv preprint arXiv:2309.10400, 2023.
