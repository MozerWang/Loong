# AlignBench: Benchmarking Chinese Alignment of Large Language Models 

Xiao Liu ${ }^{1,2, *}$, Xuanyu Lei ${ }^{1,2, *, \dagger}$, Shengyuan Wang ${ }^{1,2, \dagger}$, Yue Huang ${ }^{5,2, \dagger}$,<br>Zhuoer Feng ${ }^{4,2, \dagger}$, Bosi Wen ${ }^{4,2, \dagger}$, Jiale Cheng ${ }^{4,2, \dagger}$, Pei Ke ${ }^{4}$, Yifan Xu ${ }^{1,2}$,<br>Weng Lam Tam ${ }^{2}$, Xiaohan Zhang ${ }^{2}$, Lichao Sun ${ }^{6}$, Hongning Wang ${ }^{4}$,<br>Jing Zhang ${ }^{3}$, Minlie Huang ${ }^{4}$, Yuxiao Dong ${ }^{1}$, Jie Tang ${ }^{1}$<br>${ }^{1}$ The Knowledge Engineering Group (KEG), Tsinghua University; ${ }^{2}$ Zhipu AI;<br>${ }^{3}$ Renmin University of China; ${ }^{4}$ The CoAI Group, Tsinghua University;<br>${ }^{5}$ Sichuan University; ${ }^{6}$ Lehigh University


#### Abstract

Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, openended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce ALIGNBENCH, a comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in Chinese. Equipped with a human-in-the-loop data curation pipeline, our benchmark employs a rule-calibrated multidimensional LLM-as-Judge (Zheng et al., 2023) with Chain-of-Thought to generate explanations and final ratings as evaluations, ensuring high reliability and interpretability. Furthermore, we report ALIGNBENCH evaluated by CritiqueLLM (Ke et al., 2023), a dedicated Chinese evaluator LLM that recovers $95 \%$ of GPT-4's evaluation ability. We will provide public APIs for evaluating AlignBENCH with CritiqueLLM to facilitate the evaluation of LLMs' Chinese alignment. All evaluation codes, data, and LLM generations are available at https://github.com/THUDM/ AlignBench.


## 1 Introduction

Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022; Zeng et al., 2022; Touvron et al., 2023) have experienced a surge in development thanks to popular products such as ChatGPT (OpenAI, 2022). During the period, alignment (Ouyang et al., 2022; Bai et al., 2022), including supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), and related techniques, has been justified as a key strategy to endow pre-trained LLMs (which can hardly follow instructions) with[^0]

strong grasping of human intentions and preferences. After training, aligned LLMs have not only mastered a wide array of established NLP tasks (Wang et al., 2019; Liang et al., 2022) but also versatile language-grounded missions (Cobbe et al., 2021; Chen et al., 2021; Liu et al., 2023a). As a result, LLMs have paced a firm step towards practical applications in the wild.

Meanwhile, it has also become a substantial challenge to reliably benchmark LLMs' broad and strong competence. In English, there have been MMLU (Hendrycks et al., 2021), Big-Bench (Srivastava et al., 2023), and HELM (Wang et al., 2019; Liang et al., 2022); in Chinese, there are CEval (Huang et al., 2023b) and CMMLU (Zeng, 2023). However, they hardly examine aligned LLMs' fulfillment of user intention and human preference in real-world conditions, and even fall short to tell the difference between aligned and base LLMs. Consequently, dedicated benchmarks for evaluating alignment are crucial for development and meaningful comparisons of aligned LLMs.

Nevertheless, designing a comprehensive and reliable benchmark for LLM alignment is nontrival. An alignment benchmark should meet several important requirements, which correspond to LLMs' unique strengths and real usages of users:

- Real-World User Scenarios: Query forms and topics should be diverse and deriving from real scenarios to reflect LLMs' authentic usages.
- Open-ended: As aligned LLMs usually produce long open-ended replies, the benchmark should judge the soundness of both concise answers and detailed reasoning procedures.
- Challenging: LLMs are improving so rapidly on various challenging missions beyond estimation. The benchmark thus has to ensure its difficulty to recognize fine-grained differences between LLMs.
- Automatic Judging: Benchmark construction

![](https://cdn.mathpix.com/cropped/2024_06_04_daf94f2e387a5418e663g-02.jpg?height=506&width=1579&top_left_y=244&top_left_x=244)

Figure 1: Overall framework of AlignBENCH. 1) Data Curation: a human-in-the-loop pipeline to allow continual high-quality test query harvesting from real scenarios. 2) Task Taxonomy: 8 main categories that cover the common usages of LLMs in Chinese. 3) LLM Evaluation: automatic multi-dimensional rule-calibrated LLM-as-Judge.

and evaluation should be as automatic as possible to provide scalable and reproducible feedback to facilitate LLM development.

There have been recent attempts to introduce LLM-as-Judge (Li et al., 2023b; Zheng et al., 2023) for evaluating the general alignment of LLMs. AlpacaEval (Li et al., 2023b) compares target LLM's replies against text-davinci-003's, but has been shown unstable and uninterpretable due to its direct and pairwise scoring. MT-Bench (Zheng et al., 2023) harnesses point-wise scoring with Chain-ofThought (CoT) (Wei et al., 2022) explanations for better accuracy and transparency. However, it employs only 80 test samples and a scoring prompt that judges queries of different tasks and domains uniformly. Lastly, both benchmarks are designed only in English and cannot well reflect the level of alignment of many emerging Chinese LLMs.

In light of all mentioned issues, in this work we present AlIGNBENCH, a comprehensive multidimensional benchmark for evaluating LLMs' alignment in Chinese. Based on ChatGLM (Zeng et al., 2022; Du et al., 2022) online services (https: //chatglm.cn/), we set up a semi-automatic data curation pipeline with human in the loop to create high-quality queries to construct AlIGNBENCH. ALIGNBENCH summarizes a taxonomy comprising 8 major categories of queries (Cf. Figure 1) to comprehensively cover and align with real-user scenarios. In order to make the judge model generate objective and fair evaluations, each sample is accompanied with a human-corrected LLMgenerated reference.

To enhance the reliability and interpretability of the evaluation, similar to MT-Bench (Zheng et al., 2023), AlignBeNCH distinctly leverages
GPT-4 (OpenAI, 2023) as the major model evaluator in its development, which serves to discern the data samples and evaluate by referenced point-wise scoring with CoT. Differently, AlignBench further highlights strategies of rules-calibration and task-specific multi-dimensional judgement in the scoring. Our experiments demonstrate that these strategies contribute to AlIGNBENCH's better consistency with human judgements and better explanation quality.

Based on AlignBench, we evaluate 17 popular API-based or open-sourced LLMs that support Chinese. It is for the first time that a benchmark can provide such detailed comparisons of these LLMs across a set of fine-grained capabilities on Chinese alignment. To improve the usability and affordability for Chinese developers, on top of AlignBENCH we develop a dedicated companion evaluator LLM—CritiqueLLM (Ke et al., 2023)—which can recover over $95 \%$ of GPT-4's critic and scoring abilities in our experiments. We also report the AlignBench scores of all 17 LLMs using CritiqueLLM as judge in the paper, and will provide public APIs of CritiqueLLM to researchers to test their LLMs on AlignBench.

In summary, the contributions of our work are:

- We construct AlignBench, the first systematic benchmark that derives from real-user scenarios and queries to evaluate Chinese alignment of LLMs. We also tailor a human-in-theloop pipeline to allow sustainable update and extension of ALIGNBENCH.
- Targeting accurate and automatic evaluation of LLMs, we design a rule-calibrated multidimensional point-wise LLM-as-judge method for grading. Human evaluations justified its ap-
plicability compared to existing LLM-as-Judge methods (Zheng et al., 2023).
- We systematically benchmarked 17 popular LLMs' Chinese alignment for the first time. On top of their performance on ALIGNBENCH, we provide deep insights into status quo of Chinese LLMs' development and highlight the deficiencies that require improvements.


## 2 Dataset

In this section, we introduce the data composition and construction pipeline of ALIGNBENCH, including the definition of each data category and how samples from real-user queries are collected, desensitized, and filtered for better quality.

### 2.1 Dataset Composition

In this section, we introduce the overall composition of AlignBench. To perform a systematic evaluation, we framed a comprehensive taxonomy of the LLMs' abilities based on real-user instructions. We inspect and summarize these use-cases into 8 main categories, namely Fundamental Language Abilities, Chinese Advanced Understanding, Open-ended Questions, Writing Ability, Logical Reasoning, Mathematics, Task-oriented Role Play, and Professional Knowledge, as shown in Table 2. ALIGNBENCH contains 683 samples in total and all the samples were carefully classified and verified. Fundamental Language Ability. This category focuses on the basic language understanding and processing tasks, which are derived from traditional NLP tasks such as information extraction (Etzioni et al., 2008), text classification (Wang and Manning, 2012), and commonsense knowledge (Talmor et al., 2019). They reflect common users' practical needs of LLMs to conduct traditional tasks under zero-shot or few-shot settings with customized prompts and formats, such as text classification, information extraction, and short summarization. Thus we select high-quality diverse queries relevant to each traditional task in this category.

Advanced Chinese Understanding. This category aims to evaluate LLMs' ability to understand cultural and historical background in Chinese-specific tasks. In Chinese context, a large percentage of real-user needs are related to Chinese culture, characters, and history. However, without deliberate optimization on Chinese, cutting-edge LLMs (e.g., GPT-4 (OpenAI, 2023)) would fail to understand and answer these questions correctly. As Align-
BENCH targets Chinese alignment, the category plays a vital role in our overall design.

Open-ended Questions. This category represents a common usage of LLMs to answer subjective questions in an open-ended manner. Users may seek for advice, recommendations, and tutoring for many daily questions concerning their work, study, travel, and lives. A key criterion of good responses in the category is about catering to human preference, featuring long, detailed, and highly related content. The category measures the LLMs' general ability to offer opinions and suggestions.

Writing Ability. Regarded as one of the most frequently used functions, writing plays a vital role in LLMs' applications. For a more detailed classification, we further frame the category into 4 subcategories, namely Practical Writing, Creative Writing, Professional Writing, and Custom Writing. We select challenging writing instructions, which require not only an excellent mastering of language but also a high level of instruction-following (e.g., specific formatting conditions), consistency (e.g., argumentative topics), and creativity (e.g., fictions or poems).

Logical Reasoning. The ability to process complicated problems with step-by-step reasoning and LLMs' inherent commonsense or factual knowledge is highlighted for current strong LLMs. This category aims to evaluate LLMs' abilities to understand, analyze, and produce correct responses given intricate logical problems, using questions that require deductive, abductive, multi-hop, or commonsense reasoning

Mathematics. Considering its logical complexity and a large proportion, math problems are regarded as a suitable approach to evaluate LLMs. We collected math problems in different difficulty levels from elementary to advanced mathematics and in different formats, including calculations, simple problem solving, concept explanation, theoremproof, etc.

Task-oriented Role Play. Lots of users request the model to play as a specific identity to perform corresponding tasks, which is summarized as taskoriented role play. In order to evaluate the fulfillment of users' instructions and the quality of responses when role play, we collected role play instructions of high complexity and constructed this category.

Professional Knowledge. LLMs have proven their competence in solving domain-specific problems

| Benchmark | Dataset Information |  |  |  | Evaluation Method |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Data Size | Language | Data Source | Domain | Open <br> -ended | Multi- <br> Dimensional | Metric |
| MMLU (Hendrycks et al., 2021) | 15,908 | English | Exams \& Textbooks | Knowledge | $x$ | $x$ | Accuracy |
| GSM8k (Cobbe et al., 2021) | 8,000 | English | Human Writers | Math | $\checkmark$ | $x$ | Accuracy |
| HumanEval (Chen et al., 2021) | 164 | Python | Human Writers | Code | $\checkmark$ | $x$ | Pass@k |
| CMMLU (Zeng, 2023) | 11,528 | Chinese | Exams \& Textbooks | Knowledge | $x$ | $x$ | Accuracy |
| AGI-Eval (Zhong et al., 2023) | 8,062 | Chi. \& Eng. | Exams | Knowledge | $x$ | $x$ | Accuracy |
| C-Eval (Huang et al., 2023b) | 13,948 | Chinese | Exams | Knowledge | $x$ | $x$ | Accuracy |
| AlpacaEval (Li et al., 2023b) | 805 | English | Alpaca Data | General | $\checkmark$ | $x$ | Model Judge (w/o CoT) |
| MT-Bench (Zheng et al., 2023) | 80 | English | Self-constructed | General | $\checkmark$ | $x$ | Model Judge ( $\mathrm{w} / \mathrm{CoT})$ |
| $\overline{\text { ALIGNBENCH (ours) }}$ | 683 | Chinese | Real-user Scenarios | General | $\checkmark$ | $\checkmark$ | Model Judge (w/ CoT) |

Table 1: Comparisons between ALIGNBENCH and other benchmarks, illustrating the features of ALIGNBENCH in terms of dataset information and evaluation methods.

| Category | 中文名 | \#Samples |
| :--- | :---: | :---: |
| Fundamental Language Ability | 基本任务 | 68 |
| Advanced Chinese Understanding | 中文理解 | 58 |
| Open-ended Questions | 综合问答 | 38 |
| Writing Ability | 文本写作 | 75 |
| Logical Reasoning | 逻辑推理 | 92 |
| Mathematics | 数学计算 | 112 |
| Task-oriented Role Play | 角色扮演 | 116 |
| Professional Knowledge | 专业能力 | 124 |
| Total | 总计 | 683 |

Table 2: Sample distribution of AlIGNBENCH dataset.

that require professional knowledge. This category aims to evaluate LLMs' abilities in specific domains, for instance, physics, history, music, law, etc. Additionally, the queries we select are generative open-ended questions that allow LLMs to generate freely and provide sufficient details.

### 2.2 Dataset Construction

Each sample in AlignBench contains a taskoriented query, a high-quality reference answer, and the corresponding category in our taxonomy. The detailed construction pipeline is described as follows, serving as an efficient toolkit to create such high-quality data.

Query Curation. To ensure the diversity and authenticity of the queries, we mainly refer to two sources, namely the scenarios from online chat service of ChatGLM (Zeng et al., 2022; Du et al., 2022), one of the cutting-edge Chinese LLMs, and some supplementary challenging problems provided by researchers. Given the inherently noisy nature, we conducted a thorough and high-standard data curation process following the three rules described below.

- Task-oriented. The query should represent human intentions and instruct LLMs to complete the specified task.
- Clarity \& Fluency. The query should be clear, and easy-to-understand and the demands should be smoothly expressed.
- Complexity \& Difficulty. The query should be hard for most LLMs, requiring them to utilize their capabilities to solve it comprehensively.
- Desensitization. Ensure that the queries are safe and insensitive.

Reference Acquisition \& Improvement. Since we adopt point-wise grading for AlIGNBENCH (Cf. Section 3 for analysis), scoring with a pivotal reference answer has been shown very beneficial to improve the reliability of the LLM-as-Judge (Zheng et al., 2023; Zhang et al., 2020). Therefore, we decide to offer a human-curated reference answer for each query, serving a dual purpose: 1) assisting evaluators in determining the correctness of the answer, and 2) acting as a scoring pivot.

However, because AligNBENCH has been designed to be challenging, so is it for human annotators to provide answers from scratch for it in our preliminary trial. As a result, we first utilize GPT-4 to generate answers, and then ask human annotators to meticulously review, revise, and refine them as reference answers for ALIGNBENCH. The annotators are entrusted with the responsibility of conducting an exhaustive validation and modification process, all with the aim of elevating the quality of the responses and ensuring their factual and logical correctness.

Filtering \& Classification. LLMs are increasingly smart and able to master many ordinary queries in practice. Thus to clearly discriminate between strong LLMs, it is necessary to select samples of a more challenging nature. Consequently, we engage three relatively advanced Chinese-supported

![](https://cdn.mathpix.com/cropped/2024_06_04_daf94f2e387a5418e663g-05.jpg?height=523&width=780&top_left_y=247&top_left_x=227)

Figure 2: Cumulative distribution of judging by human, general (Zheng et al., 2023) and rule-calibrated on sampled AlignBencH along their ratings.

LLMs, including GPT-3.5-turbo (OpenAI, 2022), ChatGLM (Du et al., 2022; Zeng et al., 2022) APIs and Sparkdesk to serve as difficulty filters within our construction procedure. We subject these models to evaluation, analyzing their responses to the processed queries and subsequently scoring the answers utilizing GPT-4. By computing the average score across responses and utilizing it as a definitive signal, we proceed to eliminate $50 \%$ of the queries that garnered the highest average scores, indicative of their lower difficulty levels. This systematic approach ensures a meticulous and discerning selection of samples, effectively distinguishing between strong LLMs of varying capacities.

## 3 Methods

To effectively evaluate the quality of responses, ALIGNBENCH employs GPT-4 (OpenAI, 2023) as a major evaluator to analyze and subsequently grade the responses following adopted practices (Zheng et al., 2023; Li et al., 2023b; Liu et al., 2023b). However, a significant designing space still exists regarding prompting, score calibration, critique explanability, and evaluation dimensions, which have been hardly explored.

Therefore, in AlignBENCH we design a novel rule-calibrated multi-dimensional point-wise LLMas-Judge method. The detailed prompts are in Appendix A. 2 and an example is displayed in Fig 3.

Point-wise Grading \& Chain-of-Thought (Wei et al., 2022). When LLM-as-Judge is leveraged, two grading methods have been previously implemented: point-wise (Zheng et al., 2023) or pairwise grading (Li et al., 2023b). Nevertheless, previous study has indicated that the point-wise grading possessed comparable agreement with humans than[^1]

the pair-wise grading, which suffers from position bias (Zheng et al., 2023). Additionally, considering the evaluating efficiency, compared to pair-wise grading's quadratic number of comparisons, the point-wise grading has advantages in terms of expenses and time. Therefore AlIGNBENCH adopts point-wise grading either. During the evaluation, the inputs are the query, the model's response, and a human-curated reference answer, and the output is an multi-dimensional analytical explanation and a final rating, ranging from 1 to 10 .

As the task of grading involves complex reasoning, introducing Chain-of-Thought in the scoring process has also been proved useful to augment both the score reliability and interpretability (Zheng et al., 2023). Specifically, GPT-4 is instructed to generate explanations from multiple dimensions before providing a final grade on a scale of 1 to 10 .

Rule-calibrated Referencing. Given that many of the questions in ALIGNBENCH are of significant complexity and difficulty even for GPT-4, we provide a high-quality reference answer, which is primarily generated by GPT-4 and modified by human annotators to ensure its correctness and improve its quality. To guide the evaluator to compare the answer with the reference and generate more controllable scores, we provided detailed grading rules elaborating the relationship between score intervals and the answer's quality compared to the reference. Additionally, we set the reference answer to score 8 as a reference scoring pivot.

We plot the cumulative distribution of human judge, general judge and rule-calibrated judge in Figure 2 to show that rule-calibration judge has a narrower gap to human evaluation's cumulative distribution. Typically, rule-calibrated judge scores much fewer top scores ( 9 and 10) than general judge, aligned with human scoring habits and therefore enhance the discrimination of ALIGNBENCH.

Multi-dimensional Analysis. As tasks vary in their nature and characteristics, applying the same evaluation criteria to all tasks would be unjust. For instance, writing tasks should prioritize creativity, whereas logical reasoning tasks primarily require logical coherence. As a solution, we propose a multi-dimensional scoring approach to evaluate LLMs' responses, tailoring the evaluation to the specific task at hand, promising a more comprehensive and organized explanation. Specifically, we set up different evaluation dimensions based on dif-

| Category | Question Type | Evaluation Dimension | Reply Temperature |
| :---: | :---: | :---: | :---: |
| 基本任务 <br> (Fundamental Language Ability) | 事实与解释型问题 <br> (Factual and Explanatory Question) | 事实正确性 (Correctness), 满足用户需求 (User Satisfaction), <br> 清晰度 (Clarity), 完备性 (Completeness) | 0.1 |
| 中文理解 <br> (Advanced Chinese Understanding) | 事实与解释型问题 <br> (Factual and Explanatory Question) | 事实正确性 (Correctness), 满足用户需求 (User Satisfaction), <br> 清晰度 (Clarity), 完备性 (Completeness) | 0.1 |
| 综合问答 <br> (Open-ended Questions) | 建议型问题 <br> (Recommendation Question) | 事实正确性 (Correctness), 满足用户需求 (User Satisfaction), <br> 公平与可负责程度 (Fairness and Responsibility), 创造性 (Creativity) | 0.7 |
| 文本写作 <br> (Writing Ability) | 生成型问题 <br> (Generative Question) | 事实正确性 (Correctness), 满足用户需求 (User Satisfaction), <br> 辑连贯性 (Logical Coherence), 创造性 (Creativity), 丰富度 (Richness) | 0.7 |
| 逻辑推理 <br> (Logical Reasoning) | 逻辑推理型问题 <br> (Logical Reasoning Question) | 事实正确性 (Correctness), 满足用户需求 (User Satisfaction), <br> 逻辑连贯性 (Logical Coherence), 完备性 (Completeness) | 0.1 |
| 数学计算 <br> (Mathematics) | 逻辑推理型问题 <br> (Logical Reasoning Question) | 事实正确性 (Correctness), 满足用户需求 (User Satisfaction), <br> 逻辑连贯性 (Logical Coherence), 完备性 (Completeness) | 0.1 |
| 角色扮演 <br> (Task-oriented Role Play) | 生成型问题 <br> (Generative Question) | 事实正确性 (Correctness), 满足用户需求 (User Satisfaction), <br> 逻辑连贯性 (Logical Coherence), 创造性 (Creativity), 丰富度 (Richness) | 0.7 |
| 专业能力 <br> (Professional Knowledge) | 事实与解释型问题 <br> (Factual and Explanatory Question) | 事实正确性 (Correctness), 满足用户需求 (User Satisfaction), <br> 清晰度 (Clarity), 完备性 (Completeness) | 0.1 |

Table 3: Judging dimensions and LLM reply generation temperatures of ALIGNBENCH on different categories. They both help to provide better category-conditioned scoring in practice (Cf. Section 3.)

ferent types of questions, as shown in Table 3 and we instructed the evaluator to analyze the model answer from specified dimensions and provide dimensional scores. Furthermore, we found that our multi-dimensional method could effectively balance different dimensions, reducing verbosity bias, with an example shown in Table 11. The categorical information is also useful for conditioning generation temperature for target LLMs to generate reply (Zheng et al., 2023). For tasks that has a relatively fixed answers (e.g., Mathematics, Professional Knowledge, etc.), we set temperature to 0.1 to ensure more deterministic and reproducible generation; for other tasks (e.g., Writing, Task-oriented Role Play, etc.) that may need more creativity, a high temperature (e.g., 0.7) is adopted to encourage longer and more diverse generation.

## 4 Human Evaluation on AlignBENCH

To justify the rule-calibrated multi-dimensional point-wise LLM-as-Judge method we design for ALIGNBENCH, we conduct extensive human evaluation over ALIGNBENCH's selected queries. We especially focus on two aspects: the method's agreement with human judging, and the method's critique quality for more human interpretable results.

### 4.1 Agreement Evaluation

Previous studies (Zheng et al., 2023) have executed comprehensive agreement experiments, demonstrating that GPT-4 (OpenAI, 2023) evaluator con-[^2]

cur excellently with humans within English contexts. However, such agreement remains considerably under-investigated in Chinese contexts, thereby warranting further exploration. To affirm the credibility of our multi-dimensional rulecalibrated LLM-as-Judge method, we have conducted a comprehensive human annotation experiment, aiming to measure the agreement between evaluations adjudicated by human annotators and our method.

Dataset. To effectively appraise this grading method across diversified Chinese-supported LLMs, we randomly sampled the experiment dataset from the complete ALIGNBENCH dataset. To make sure each category consists of enough samples to produce reliable results, smaller categories were supplemented more samples. The sampled dataset incorporates 400 high-quality queries.In pursuit of covering models of diversified capabilities, we generated the answers on 8 models, including GPT-4 (OpenAI, 2023), three versions of ChatGLM series (Zeng et al., 2022; Du et al., 2022), Sparkdesk, Qwen-plus-v1-search(Bai et al., 2023a), InternLM-7B-Chat (Team, 2023) and Chinese-Llama2-7B-Chat, producing a total of 3200 question-answer pairings. Subsequent to the compilation of the evaluation set, the questionanswer-reference triples were delivered to seasoned human annotators, tasked with assigning quality ratings to the answers according to the references. Given the inherent limitations bound to human cognition, annotators were instructed to employ a rating on a scale from 1 to 5 . The scores were[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_daf94f2e387a5418e663g-07.jpg?height=914&width=360&top_left_y=240&top_left_x=243)

## Reference

假设今天是周五, 那昨天应该是周四。但实际上, 周四是明天,所以今天实际是周三

Suppose today was Friday, then yesterday should have been Thursday. But Thursday is to morrow. So Wednesday is today.
