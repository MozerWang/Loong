# Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion? 

Mudit Verma*<br>Arizona State University<br>School of Computing and Augmented<br>Intelligence<br>Tempe, USA<br>muditverma@asu.edu

Siddhant Bhambri*<br>Arizona State University<br>School of Computing and Augmented<br>Intelligence<br>Tempe, USA<br>sbhambr1@asu.edu

Subbarao Kambhampati<br>Arizona State University<br>School of Computing and Augmented<br>Intelligence<br>Tempe, USA<br>rao@asu.edu

![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-01.jpg?height=342&width=1749&top_left_y=729&top_left_x=188)

Figure 1: Fetch executes the maneuver to pick the block and takes a step (towards robot's) left indicating its choice of goal as Loc2 (instead of the alternative Loc3) thereby being legible. Left : Fetch executes the maneuver. Center : Fetch has a label on its head stating "Not a Legible robot" in a small font which the human observer is unable to read. Right : Human has a blurred view of Fetch making them unable to comment on whether the robot is being legible. Robot expects the LLM (human proxy) to "think what the 'human would think of the robot's behavior'"


#### Abstract

Large Language Models (LLMs) have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of LLMs especially on Theory of Mind (ToM) abilities in Large Language Models. While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs an LLM to assess the robot's generated behavior in a manner similar to human observer. We focus on four behavior types, namely - explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to be a human proxy to the agent, and to answer how a certain agent behavior would be perceived by the human in the loop, for example "Given a robot's behavior X, would the human observer find it explicable?". We conduct a human subject study to verify that the users are able to correctly answer such a question in the curated \footnotetext{ Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. HRI '24 Companion, March 11-14, 2024, Boulder, CO, USA

Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0323-2/24/03...\$15.00 }

https://doi.org/10.1145/3610978.3640767


situations (robot setting and plan) across five domains. A first analysis of the belief test yields extremely positive results inflating ones expectations of LLMs possessing ToM abilities. We then propose and perform a suite of perturbation tests which breaks this illusion, i.e. Inconsistent Belief, Uninformative Context and Conviction Test. The high score of LLMs on vanilla prompts showcases its potential use in HRI settings, however to possess ToM demands invariance to trivial or irrelevant perturbations in the context which LLMs lack. We report our results on GPT-4 and GPT-3.5-turbo.

## CCS CONCEPTS

- Computing methodologies $\rightarrow$ Theory of mind; Knowledge representation and reasoning; Planning and scheduling.


## KEYWORDS

Theory of Mind; Large Language Models; Reasoning

## ACM Reference Format:

Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati. 2024. Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?. In Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction (HRI '24 Companion), March 11-14, 2024, Boulder, CO, USA. ACM, New York, NY, USA, 37 pages. https://doi.org/10.1145/ 3610978.3640767

## 1 INTRODUCTION

As Artificial Intelligence (AI) progresses, the development of the next generation of $\mathrm{AI}$ agents require the ability to interact with[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-02.jpg?height=309&width=409&top_left_y=323&top_left_x=294)

(a) Using LLMs as a HumanProxy
![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-02.jpg?height=392&width=660&top_left_y=278&top_left_x=751)

(b) User Study Interface

Figure 2: (a) An LLM being used as a Human-Proxy by a Robot as an internal-critique for behavior synthesis. (b) Interface used for our User Study: Example showing the three questions for Legibility in Fetch Domain.

humans in human-like manner, processes and behaviors. A vital component of such interaction is the Theory of Mind (ToM), which involves attributing mental states - such as beliefs, intentions, desires, and emotions - to oneself and others, and to understand that these mental states may differ from one's own. ToM has extensive roots in Human-Human interaction and has motivated several critical studies in pursuit of social intelligence $[2,3,15,60]$. Moreover, ToM is the corner stone enabling effective communication, collaboration, or deception and becomes a pre-requisite for most of human-agent interaction [12, 20, 29, 43, 57]. Everyday interactions which are second nature to human conversations like our ability to empathize with a character in a movie or understand social humor is in part due to our ability to perform theory of mind and comes naturally to humans [41].

Theory of Mind becomes all the more important in Human-Robot Interaction to facilitate improved behavior synthesis [8, 9, 11, 2628, 47, 48, 69] and engender Human-Robot trust [14, 65-68]. Prior works have either assumed a human mental model [9] or learned it through interaction [56,69]. Recent developments in generative AI, specifically LLMs may have opened a new way to access approximate human mental model to enable robots achieve their HRI objectives in better ways through symbolic [49] or other modalities $[16,55,56]$. Initial research hinted at emergent LLM's Reasoning, Planning \& Theory of Mind abilities [6, 21, 30, 46], however, recent works have refuted such claims [44, 52, 54]. While LLMs may not be sound reasoners or perform ToM as humans do, their performance on existing benchmarks for such tasks certainly exceeds chance behavior and it may find significant use as a Human-Proxy available to the robot. Sally-Anne test and its variations that have been extensively studied for testing ToM have likely been seen in the training data for these LLMs. We suspect that LLM's approximate retrieval is be mistaken for reasoning abilities. If LLMs were indeed good at Theory of Mind, then, as in Fig. 2a, a robot can leverage the LLM to perform an internal critique while synthesizing behavior with a human-in-the-loop. The internal critique essentially would answer whether the planned robot behavior would be useful for the human-in-the-loop. For instance, if the human prefers explicable behavior [69], the robot can query the LLM to check whether a planned behavior would indeed be explicable to the human.

While past works checking for emergent abilities of LLM [6, 21, 30, 46], even ToM abilities [24] exist, we posit that there is a need for investigating scenarios specific to HRI as real-world robots function in a non-ergodic world with irreversible consequences and much higher stakes. To this end, we investigate PROBE task (Perceived RObot Bhavior rEcognition) as a means to understand theory of mind abilities of Large Language Models in the context of HumanRobot Interaction. For example, the Fetch robot that is navigating to a location with a human observer in the loop has to tuck its arms and crouch before moving ${ }^{1}$. The lay-observer unfamiliar with the underlying dynamics of the robot may find the actions "tuck \& crouch" unnecessary and inexplicable. In our investigation, the robot employs an LLM to ask the test question "Given behavior X, would the human observer find such a plan explicable?".

We study four robot behaviors which have been extensively discussed in HRI and mental modelling, namely - explicable, legible, predictable and obfuscatory behavior (referred to as "behavior types"). We leverage the rich literature along these behavior types and borrow five domains used in prior works and valuable to the HRI community, namely - Fetch, Passage Gridworld, Environment Design, Urban Search and Rescue, and Package Delivery. For each domain we construct four distinct situations corresponding to each behavior type. We construct a prompt (can be automated) using relevant context like description of the domain, robot capabilities \& goals, and a definition of the behavior type. The LLM is asked a binary question (Q1) on whether the human observer would find the agent plan to be explicable (or the corresponding behavior type) and a multi choice question (Q2). In our evaluation, upon correct answer to (Q1), the LLM is tasked with choosing one explanation from a set of options, explaining why the plan conforms to the predicted behavior type (Q2). As a baseline, we first test LLM on vanilla prompts : success on which is indicative of potential ToM abilities but may also inflate one's expectations of LLMs. Second, we propose three tests of which two are perturbation tests as Inconsistent Belief and Uninformative Context, and a Conviction Test. Our second suite of tests elicit important conclusions on ToM abilities of LLMs and essentially break the supposed illusion.

We highlight our contributions as follows :

1. We conduct a first study of Theory of Mind abilities of Large Language Models in the context of interpretable behavior synthesis and Human-Robot Interaction.[^1]
2. We investigate along five domains and four behavior types which have been well motivated with respect to the need for mental modeling in HRI and curate a total of 20 situations.
3. We perform a user subject study to validate our curated situations as lay users are able to correctly answer ToM queries.
4. We propose second-order tests including two perturbation tests and the Conviction test which highlights that any ToM abilities of LLMs are illusionary.
5. We perform a user subject study to validate that humans observers observing a real robot (Fetch) will be unaffected by perturbations while answering ToM queries.

## 2 RELATED WORK

Large Language Models : Large Language Models [70] are pretrained generative AI models that can take a piece of text as an input and generate text as an output. Typically, the input space is unconstrained, meaning a finite yet large set of tokens / text is a valid input for the GPT like Large Language Model. While this allows for arbitrary tasks to be framed as a linguistic query for the LLM, it is unclear whether data-driven next-word prediction style of LLMs [1, 13, 37, 62] training is reason enough to believe that LLMs would perform well on said arbitrary tasks. Over the past years several variants and versions of Large Language Models have been released such as the GPT family [37], LLAMA family [51], PALM family [1, 13], BLOOM [62] and several others [70]. In this work we conduct our study with GPT-4 [37] as it is one of the strongest LLMs in terms of NLP task performance.

Theory of Mind: Theory of Mind has been a challenging, yet, ever-interesting topic for researchers [17, 33, 53, 64]. It allows for effective communication between agents especially with a human in the loop. While humans develop it naturally, ToM is considered to be one of the most challenging goals for an $\mathrm{AI}$ agent [41]. In this work, we are specifically interested in studying ToM for humanrobot interaction for behavior synthesis that involves extensive modeling of the human in the loop, reasoning on those models and synthesise behavior that conforms to the human mental models.

LLMs, Theory of Mind and Emergent abilities: LLMs have amassed a large popularity with public and the research community with its exceptional abilities in NLP tasks. With an AI as impressive as an LLM, researchers have explored the possibilities of emergent abilities of these LLMs. While this is an active area of research, of interest to the scope of this work are LLMs abilities in reasoning [23, 58, 59], deception abilities [18], Theory of Mind abilities [25] and other emergent behaviors that are similar to human behaviors [7, 40, 42, 50, 63]. While these works highlight how LLMs possess these abilities, more recent works have refuted these claims $[5,22,35,45,52,54]$ citing that LLMs are extremely brittle and heavilty depend on the prompt fed to it. [54] suggests, systems that reason do not have to depend upon such syntactic alterations to the input like Chain of Thought Prompts, Tree of Thoughts or other prompting strategies [31]. While ToM has extensive prior works on specialized robot agents $[4,34,39]$ these works are not directly applicable to investigating the emergent ToM abilities of LLMs. We distinguish ourselves from past works through our novel construction of prompt situations which are sourced from the vast literature on behavior synthesis, the situations are well rooted for
HRI scenarios, provide a thorough analysis via a large-scale user subject study and provide a case study with real Fetch robot in HRI.

## 3 PRELIMINARIES

A core focus of this work is to analyze the Theory of Mind abilities of Large Language models in Human-AI interaction scenario. As motivated previously, we identify that Human-Aware Planning is an important part of HRI that attempts to synthesize agent behaviors by taking human mental model into account. [9, 48] provides a good overview of the literature and formal specifications of behavior types we consider in this work. [9, 26] papers reconciles the various interpretations of terminologies used in the literature and highlights important connections between related works. Subsequently, they provide formal definitions of the four behavior types (explicability, predictability, legibility and obfuscation) as follows :

(1) Explicability [10, 47, 69]: Explicability measures how close a plan is to the expectations of the observer, given a known goal.

(2) Legibility : Plan legibility reduces ambiguity over possible goals that are being achieved.

(3) Predictability : Plan predictability reduces ambiguity over possible plans, given a goal.

(4) Obfuscation (Goal-Obfuscation) [26] : A plan is an obfuscatory behavior if it k-ambiguous. That is, the plan is consistent with at least $k$ goals with a true goal of the agent and $k-1$ are decoy goals meant to deceive the human observer.

Each of these behavior types correspond to utilize the human mental model to synthesize a behavior to achieve some target. For example, explicability utilizes the human mental model and forces the robot to conform to the human model. Not only the robot must realize which behavior type it should conform to (even though these types are not mutually exclusive, they can be competing as pointed by [9]), but also inherently require the human's mental model. Further, it needs to be able to reason with the human mental model to finally synthesize the expected behavior. This shows that behavior synthesis not only requires Theory of Mind abilities but also require reasoning abilities (i.e. performing inference over the mental models). There exists a large body of work that study the variations among and behavior types but we argue that showcasing LLM's failure at ToM on these canonical types is sufficient (as problems studying a cross between behavior types require the agent to optimize over multiple, possibly competing, objectives).

## 4 METHODOLOGY

Through this work, we answer the following three questions in the context of mental-model reasoning abilities in Human-AI Interaction scenarios for behavior synthesis :

Research Question 1: How do human subjects ${ }^{2}$ perform on ToM reasoning tasks in Human-Robot Interaction scenarios?

Research Question 2: How much does LLM performance align with that of: 1) oracle, \& 2) human subjects?

Research Question 3: How robust are LLMs in their (perceived) mental-model reasoning abilities?

${ }^{2}$ We refer to lay users as human subjects in the rest of the paper.

We first construct a set of ToM tasks given as "situations" across five domains. A situation is described as an ordered tuple $\mathcal{S}=$ $\left(R_{d}, G, B_{d}, C, P\right)$ where $R_{d}$ is the description of the domain, robot capabilities and actions. $G$ is the description of goals, $B_{d}$ describes the behavior-type, $C$ describes any additional constraints (like partial observability) and $P$ is optional and adds a perturbation text (Uninformative Context or Inconsistent Belief). Following each situation we query two questions, Q1 : a binary yes/no question on whether the human observer perceives the agent behavior according to a behavior-type, and Q2 : to select an explanation for why the said behavior-type is a valid answer. A prompt is composed of the tuple and the two questions. We first validate our constructed prompts and obtain human baseline performance. We then query the prompts to GPT-X family, i.e. GPT-4 and GPT-3.5-turbo. We note that queries to GPT-X family imply an expectation of second order ToM abilities which implies we expect the LLM to "think what the 'human would think of the robot's behavior'". As discussed in Section 1 , such an ability can be leveraged by the robot to use LLMs as a human-proxy and improve its behavior synthesis process.

As discussed in Sec. 6, the performance of LLMs on the vanilla prompts (where $P=\emptyset$ ) engenders an inflated belief that LLMs may possess ToM abilities. We then investigate the robustness of LLM responses with two perturbation cases and a Conviction test. Unlike how ToM agents (like humans) behave [2, 3, 15, 52, 60](see Sec. 7), we find that LLMs are extremely brittle to small changes to prompt and fail miserably. Finally, we present a case study with Fetch robot where a human observes the robot acting in the world along the three cases Vanilla prompt, Uninformative Context and Inconsistent Belief. For all our experiments we use the most recent version of GPT-4 (as of September 30, 2023) and GPT-3.5-turbo (as of November 25, 2023). Past works have justified their general superiority amongst the popular LLMs and within the GPT-X family [37] motivating their use as representative LLMs.

### 4.1 Prompt Construction

For each domain we create four situations corresponding to each behavior-type. In each situation there is a robot acting in the world and a human observer. Based on the human's mental model they may find the robot behavior to be explicable, legible, predictable or obfuscatory. We write one prompt for each situation (i.e. Domain $\times$ Behavior-Type). The prompt is divided into sections as : a description of the robot, its actions, the goal, definition of the behavior type we wish to have the LLM answer about, and the plan proposed / executed by the robot. Following which we add our ToM test question (Q1) whether the plan corresponds to the behavior type (as a binary answer) and (Q2) the reason for the answer to Q1.

### 4.2 Human Subject Study on ToM-PROBE task

In reference to answering research question 1, we describe our user study for evaluating the alignment between LLM and human responses. We evaluate four agent behaviors, namely - explicability, legibility, predictability and obfuscation, across five domains discussed in detail in Section 5. The study protocol was approved by our local Institutional Review Board (IRB) and refined through pilot studies over a small group of participants.
Table 1: Study-1 Participant Demographics by Test Domain.

| Test Domain | N | \#Male | \#Female | \#Other | Avg. Age (SD) |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Fetch | 20 | 8 | 12 | - | $35.25(11.77)$ |
| Passage | 20 | 6 | 14 | - | $31.90(11.83)$ |
| Env. Design | 20 | 6 | 13 | 1 | $33.35(8.96)$ |
| USAR | 20 | 7 | 12 | 1 | $32.45(8.43)$ |
| Package D. | 20 | 6 | 14 | - | $32.85(9.70)$ |
| Fetch-video (Sec 7) | 20 | 5 | 15 | - | $33.75(8.94)$ |
| Total | 120 | 38 | 80 | 2 | $33.26(9.88)$ |

4.2.1 Question Design. For answering RQ2, we designed the first question as a binary Yes/No response on the agent behavior conforming to the given behavior type. The second question was a Multiple Choice Question (MCQ) asking the participants to choose the correct reason behind the agent behavior. In Q3, we presented the participants with GPT-4's response when given the same prompt comprising of the domain description and the first question (see, vignettes in Supplementary). For the last question, participants answered on a Likert Scale from 1 (Strongly Disagree) to 5 (Strongly Agree) with the LLM's reasoning on the agent behavior. Our study had a mixed design: Test Domains were varied between-subjects, and response ordering was varied within-subjects for the first two questions.

4.2.2 Setup \& Procedure. The study used the Prolific platform [38], directing participants to a Google Form for responses. Participants could use any device without requiring speaker, camera, microphone, or special software for the study. First, they were presented with the data confidentiality statement, followed questions collecting their demographic details, and finally provided with the instructions and overview of the entire study which consisted of the four parts comprising three questions each. The study approximately took 6 minutes to complete, and participants were compensated at the rate of $\$ 9 /$ hour.

4.2.3 Participants. We recruited 125 participants (between the ages of 18 and 60). We conducted a pilot study on 25 participants spread across each of the five evaluation domains. The final study, refined using the pilot study responses, had a sample size of 100 participants with 20 participants for each of the five domains listed in Section 5 (see Table 1). Most participants fell in the age range of 21 to 34 years, and had an undergraduate degree. Participants reported a mean familiarity of 3.07 and standard deviation of 0.36 to Machine Learning and Artificial Intelligence tools/technologies, on a 7-point Likert Scale with 1 corresponding to being least familiar.

### 4.3 Testing LLMs for Robustness

To answer research question 3 , we design a set of adversarial perturbation strategies for testing the robustness of LLM responses in mental-model reasoning tasks. As pointed by [52], mental-model reasoning should be unaffected by perturbations, commonly referred to as deception strategies in cognitive development studies $[19,32,61]$. In the case of LLMs where the said reasoning tasks are presented as text prompts, we design the following perturbation strategies (see 1 and 2 for example prompts), ensuring that the final expected reasoning responses are invariant to the perturbations introduced by each of the two cases:

![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-05.jpg?height=234&width=1594&top_left_y=276&top_left_x=271)

Figure 3: An illustrative view of the domains used for testing LLM for Theory of Mind reasoning on the Perceived Behavior Recognition task. Left to Right: Fetch Robot Domain, Passage Gridworld, Environment Design, Urban Search and Rescue, and Package Delivery.

## Perturb 1: Fetch - Legibility + Uninformative Context

Description: Fetch is a robot that can carry objects from one location to another. There are three locations: loc1, loc2, and loc3 where the robot can go.

There is a block b1 at location loc1, and the robot is at location loc1 and has its hand empty. Location loc2 is to the left of loc1, and location loc3 is to the right of loc1. Goals: The robot has to take the block b1 to either loc2 OR loc3 (only one of these locations).

Definition : A partial plan is a part of robot's behavior, for example a few actions that it takes.

Definition : A partial plan is legible if the observer (you) can identify which goal the robot wants to go for. A partial plan $A$ is more legible than another partial plan $B$ if the number of possible goal locations for $\mathrm{A}$ is less than $\mathrm{B}$.

Plan : In the robot's partial plan, it picks block b1 and takes one step left.

The robot has a label saying 'Not a Legible Agent'. The human observer cannot read. The human observer looks at the label. The human observer has not seen the robot before.

Question 1 : Imagine you are the human observer. Would you find such a partial plan legible? Give your answer as 'Yes' or 'No' only.

## Perturb 2: Fetch - Legibility + Inconsistent Belief

Description: Fetch is a robot that can carry objects from one location to another. There are three locations: loc1, loc2, and loc3 where the robot can go.

There is a block b1 at location loc1, and the robot is at location loc1 and has its hand empty. Location loc2 is to the left of loc1, and location loc3 is to the right of loc1.

Goals: The robot has to take the block b1 to either loc2 OR loc3 (only one of these locations).

The human observer can not see the robot acting in the environment

Definition : A partial plan is a part of robot's behavior, for example a few actions that it takes.

Definition : A partial plan is legible if the observer (you) can identify which goal the robot wants to go for. A partial plan $\mathrm{A}$ is more legible than another partial plan $\mathrm{B}$ if the number of possible goal locations for $\mathrm{A}$ is less than $\mathrm{B}$

Plan : In the robot's partial plan, it picks block b1 and takes one step left. Question 1 : Imagine you are the human observer. Would you find such a partial plan legible? Give your answer as 'Yes', 'No', or 'Can't say' only.

Uninformative Context: Inspired by the robustness testing in [52], we add an additional context to each of the LLM prompts. This additional context can be understood as a self-contained pair of clauses evaluating to False, and hence, having a null effect to the reasoning expected by the task prompt.

Inconsistent Belief: An essential component to checking whether mental-model reasoning abilities exist, is by including an additional context which ensures that the correct response can only be computed through semantic reasoning. Specifically, we include the sentence "The human observer can not see the robot acting in the environment." in each prompt. We modify the options for the first binary response question to include Can't Say (the correct answer) along with Yes/No. We also modify Q2 to include the correct reason stating the inability of the human observer to see and understand the agent behavior.

Conviction Test : For a system that exhibits mental-model reasoning abilities, we would expect the system to consistently pick the same response (ideally, the correct response) regardless of the repeated queries. In the case of GPT-X family, the temperature (or $\tau$ ) parameter controls the stochastic nature of the queried responses. It can be varied in the range of 0 (least)-2 (most) stochasticity influencing the LLM outputs on each iteration.

## 5 EVALUATION DOMAINS

We used the following five domains (Fig. 3) for our investigation:

Fetch Robot Fetch is a robot that can carry objects (pick / place) and move from one location to another. The situations require the robot to pick the block and navigate to (one of) the goal location.

## Prompt 1: Fetch - Legibility <br> Description: Fetch is a robot that can carry objects from one location to another. There are three locations: loc1, loc2, and loc3 where the robot can go. <br> There is a block b1 at location loc1, and the robot is at location loc1 and has its hand empty. Location loc2 is to the left of loc1, and location loc3 is to the right of loc1. Goals: The robot has to take the block b1 to either loc2 OR loc3 (only one of these locations). <br> Definition : A partial plan is a part of robot's behavior, for example a few actions that it takes. <br> Definition : A partial plan is legible if the observer (you) can identify which goal the robot wants to go for. A partial plan $A$ is more legible than another partial plan $B$ if the number of possible goal locations for $\mathrm{A}$ is less than $\mathrm{B}$. <br> Plan : In the robot's partial plan, it picks block b1 and takes one step left. <br> Question 1: Would you find such a partial plan legible? Give your answer as 'Yes' or 'No' only.

Passage Gridworld: In this domain, a rover which exists in a gridworld domain with possible blockades that it must navigate through to reach its goal location.

## Prompt 2: Passage Gridworld - Explicability

Description: Consider a $4 \times 4$ square grid with each cell numbered as (row, column), the robot needs to travel from top left cell $1(1,1)$ to its goal at bottom right cell $15(4,3)$. The human observer (you) expects the robot to take the shortest path by going DOWN 3 steps to row 4 (reach 4,1), and then RIGHT 2 steps to column 3 (reach 4,3). Constraint: There are blockades in columns 1, 2 and 3, that the human observer (you) do not know of, but the robot does.

Definition : Plan Explicability means whether the plan / robot behavior is an expected behavior according to the human observer (you). If you look at the robot behavior and find that some actions are unnecessary or not required, then the behavior is inexplicable. Plan: The robot goes RIGHT 3 steps in row 1 (reach 1,4), goes DOWN 3 steps in column 4 (reach 4,4), and LEFT 1 step in row $4(4,3)$.

Question 1: Imagine you are the human observer, would you find such a plan explicable? Give your answer as 'Yes' or 'No' only.

Environment Design Gridworld: The environment design domain tests LLMs on the design choices of a gridworld environment that correspond to an agent's explicable behavior (and for other three behavior types).

## Prompt 3: Environment Design - Obfuscation

Description: There is a $3 \times 3$ square grid numbered as (row, column) $=(1,1)$ the bottom left cell and $(3,3)$ is the top right cell. The robot needs to travel from cell $1(1,1)$ to achieve two goals G1, placed at $(3,1)$ and G2, placed at $(3,2)$. The robot cannot go through cells that have an obstacle. The robot can go UP, DOWN, LEFT or RIGHT. Finally, there may be objects placed in one or more cells, and the agent will incur a very high cost on visiting these two cells.

Suppose there is only one instantiation of this grid based on how these obstacles are placed in the environment:

Setup A : No obstacles.

Goals : The robot needs to find a plan to reach one of the two goals, G1 and G2

Definition: An environment is designed for obfuscation when all the plan completions are equally worse for all the agent goals. This is useful when the agent wants to achieve a certain goal say G1 but does not want the observer (you) to realize which among set of possible goals it wants to achieve. An environment designed for obfuscations allows for plans that lets the agent hide which goal it wants to achieve for as long as possible. Question 1 : For the setup there are multiple plans possible, for example to goto cells $(1,1)$ RIGHT $(1,2) \operatorname{RIGHT}(1,3) \operatorname{UP}(2,3)$, $\operatorname{LEFT}(2,2)$, $\operatorname{LEFT}(2,1) \operatorname{UP}(3,1) \operatorname{RIGHT}(3,2)$. Another shorter plan can be $(1,1)$ UP $(2,1)$ UP $(3,1)$ RIGHT $(3,2)$. Do you think that the environment is designed for obfuscation? Give your answer only as Yes or No.

Package Delivery: The Package Delivery domain presents yet another complex scenario where a human is observing an agent acting in a partially observable port management scenario where.

## Prompt 4: Package Delivery - Obfuscation

$$
\begin{aligned}
& \text { Description: Consider a situation where there is a robot which manages a shipping } \\
& \text { port, and a human observer (you) who is the supervisor that has sensors or subordinates } \\
& \text { at the port who provide partial information about the nature of activity being carried } \\
& \text { out at the port. For instance, when a specific crate is loaded onto the ship, the observer } \\
& \text { finds out about the identity of the loaded crate. The observer knows the initial inventory } \\
& \text { at the port, but when new cargo is acquired by the port, the observer's sensors reveal } \\
& \text { only that more cargo was received; they do not specify the numbers or identities of the } \\
& \text { received crates. } \\
& \text { Initial state: There are packages at the port. } \\
& \text { Goal: The robot can either pick and acquire a package on the port, or else pick and load } \\
& \text { it on the ship. } \\
& \text { Definition: Suppose you think the robot is trying to achieve one out of a set of of } \\
& \text { potential goals. If the agent's behavior does not reduce the size of this set, then it is } \\
& \text { obfuscatory. For example, if you think robot is trying to achieve one of A, B, C. If it } \\
& \text { shows a behavior (partial plan) but you think it is still trying to achieve any one of A, B, } \\
& \text { C, then it is obfuscatory. } \\
& \text { Plan : Suppose the agent picks up the package and holds it between the port and the } \\
& \text { ship. } \\
& \text { Question } 1 \text { : Would you find such a partial plan obfuscatory? Give your answer as 'Yes' } \\
& \text { or 'No' only. }
\end{aligned}
$$

USAR: Urban Search And Rescue is a popular domain where the robot and a human observer (commander) perform triage and rescue operations. The robot uses the LLM to query how a certain plan will be perceived by the commander.

## Prompt 5: USAR - Predictability

Description : In a typical Urban Search and Rescue (USAR) setting, there is a building with interconnected rooms and hallways. There is a human commander CommX, and a robot agent acting in the environment. Both the agents can move around and pickup/drop-off or handover med-kits to each other. CommX can only interact with med-kits light in weight, but the robot agent can interact with heavy med-kits too. Initial State : There is a medkit located in the middle of the hallway, and there are two paths to a patient room from there (one on the LEFT, and one on the RIGHT) The observer (you) has the top-view of this setting

Goal : Agent has to pickup medkit and take it to the patient room.

Definition: A partial plan A is predictable if the observer (you) can identify if there is one possible completion (which may or may not lead to the goal). A partial plan $\mathrm{A}$ is more predictable than a partial plan B if the number of possible completions of $\mathrm{A}$ is less than B.

Plan : In the robot's partial plan, it picks up the medkit, and takes one step LEFT Question 1 : Would you find such a partial plan predictable? Give your answer as 'Yes' or 'No' only.

## 6 RESULTS

We breakdown our discussion on the results to answer the three research questions, as mentioned in Section 4. In summary, we begin with evaluating human subjects performance on our five evaluation domains in 6.1, which validates our evaluation prompts, and also acts as a baseline when comparing LLM performance in 6.2. Finally, we shift our focus to testing the robustness of LLMs with an in-depth analysis of their failure modes in 6.3.

### 6.1 RQ1 : Human Subjects on ToM-PROBE task

To validate our text prompts querying for Theory of Mind abilities and to establish a baseline, we report the user response average accuracy and standard deviation for the binary response question (Q1) and the MCQ response question (Q2), across five domains in Figure 4 (c.f. Table 2 in Appendix for more details). We use the same text as vignettes shown in Section 5, as LLM prompts. To answer our RQ1, we note that users were correctly able to answer both questions for each of the five domains with a minimum average accuracy of $69 \%$, except for the case of Package Delivery domain where the accuracy was relatively lower in Legibility and Predictability behavior prompts. We recall that Q2 conveys the correct answer (regardless of the user response to $\mathrm{Q} 1$ ) and we request them to "rethink and select an explanation". A much higher number of users could correctly identify the reasoning choice in $\mathrm{Q} 2$, including users who had incorrectly answered Q1 (see Fig. 4b).

![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-06.jpg?height=337&width=808&top_left_y=1306&top_left_x=1100)

![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-06.jpg?height=258&width=364&top_left_y=1324&top_left_x=1119)

(a) Q1 - Binary Response

![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-06.jpg?height=258&width=377&top_left_y=1324&top_left_x=1530)

(b) Q2 - MCQ Response
Figure 4: Human subject performance across five domains.

### 6.2 RQ2: LLMs Performance Alignment

6.2.1 Performance Comparison : To answer our second research question (RQ2), we begin with comparing the performance accuracy of GPT-4 and GPT-3.5-turbo with human subjects and oracle, as shown in Figure 5. To our expectations, GPT-4 performs better than GPT-3.5-turbo in all but one behavior type, i.e., for predictability where their accuracies are equal.

6.2.2 Kolmogorov-Smirnov (KS) Test : For further evaluating the alignment between LLM responses and human subjects, we perform the KS test between the two populations of (scaled) user responses and GPT-4 responses with the null hypothesis $H_{0}$ : the two populations belong to the same distribution. For a KS test with sample size 20 , the critical value for significance level 0.01 is 0.356 , and for significance level 0.05 is 0.294 . We observe that the observed critical value is 0.04 , and hence, we accept the null hypothesis. We also run the KS test between GPT-4 responses and the ground truth results, and obtain 0.3 as the observed critical value at which $H_{0}$ can only

![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-07.jpg?height=393&width=743&top_left_y=281&top_left_x=233)

Figure 5: Performance on Q1 across five domains along four robot behavior types on Q1 (binary response). Human subjects' results have been scaled for a uniform comparison.

![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-07.jpg?height=268&width=830&top_left_y=855&top_left_x=192)

Figure 6: Likert Score (1-5) comparison for subjective evaluation of LLM responses.

![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-07.jpg?height=208&width=830&top_left_y=1262&top_left_x=192)

Figure 7: Performance accuracy of GPT-4 showcasing failure modes in perturbation tests. Each bar denotes the correct answers for $\mathbf{Q 1}$ across five domains along behavior types.

be accepted for significance level 0.05 and not for significance level 0.01. These tests have been run using the GPT-4 results obtained with the hyperparameter $\tau$ (temperature) set to 0 . With $\tau=1$, we obtain the similar results with the observed critical values 0.07 and 0.35 in the two respective cases. Hence, we conclude from these results that GPT-4's performance aligns with that of human subjects as compared to the case of oracles or ground truth.

6.2.3 Subjective Response Evaluation: Additionally, for Q3 of in our user study, we record the average scores for human evaluations on LLM descriptive reasoning responses. These ratings are in the [1-5] Likert scale where 1 implies users strongly disgree and at 5 users strongly agree with the GPT-4's explanation. We find that GPT-4 descriptive responses to vanilla prompt are generally agreed by the users (with average Likert rating of Explicability : 3.47, Legibility : 4.23, Predictability : 4.0 , Obfuscation : 4.08) further bolstering the (illusion) belief that LLMs are in fact performing second order ToM (c.f. Figure 12 in the Appendix for more details). For completeness, we use an automated method of comparing the correct explanation in Q2 and descriptive explanation by GPT-4 by using gpt-3.5 as a semantic text similarity tool (see prompt 6 in Appendix for

![](https://cdn.mathpix.com/cropped/2024_06_04_e3dd099718c33e088cf2g-07.jpg?height=558&width=700&top_left_y=282&top_left_x=1165)

Figure 8: Conviction Test Results - Performance distribution of LLMs across behavior types when prompted 10 independent times with $\tau=2$ (see Appendix for $\tau=0 \& \tau=1$ ).

more details) in Figure 6, with average ratings of Explicability : 3.4, Legibility : 4 , Predictability : 3.8 , Obfuscation : 3.4.

### 6.3 RQ3: Failure modes of LLMs in ToM-PROBE

While RQ1 and RQ2 yield extremely motivating and positive results in favor of GPT-4 showing ToM abilities, we clarify that our RQ3 breaks this illusion. Revisiting our research question 3 to test LLM robustness for correctly answering Theory-of-Mind reasoning questions, we report the results for GPT-4's performance on the binary question for all behavior types across the five evaluation domains in Figure 7.

6.3.1 Uninformative Context. In the case of our first perturbation (see Prompt 1), we note a drop in performance across all four behavior types. Note that past works [52] have considered even a single incorrect response on this perturbation test enough to disregard any emergent abilities of LLM.

6.3.2 Inconsistent Belief. In the case of the second perturbation (see Prompt 2), GPT-4 is unable to get even a single correct response in all but one out of the 20 cases. This should not be surprising as prior work has already shown that LLMs cannot reason [54]. The Inconsistent Belief Tests checks relies on inferring the correct mental state which in-turn requires reasoning over given information in the prompt.

6.3.3 Conviction Test. While it would be expected for a true ToM reasoner that multiple queries of the same prompt results in the same answer, we find that GPT-4 does not remain consistent with its responses even when they are incorrect. The LLM keeps switching answers between "Yes/No" for Q1 when sampled 10 times under $\tau=2$, as shown in Figure 8. Note, that we would ideally expect to have the same answer, irrespective of the correctness and the number of times the LLM is queried. Furthermore, we note that while GPT-4 is inconsistent in its responses, GPT-3.5-turbo performs much worse and gives irrelevant responses. For an automated agent which may utilize a language model for querying for ToM problems,
such inconsistent and irrelevant responses could lead to unreliable performance, thereby impacting the human-robot interaction.

Although the results from vanilla prompts and the user study build the illusion of ToM abilities in LLMs, we conclude that our perturbation experiments are sufficient to clarify that ToM abilities are absent from LLMs. ToM evaluation must entail evaluation of reasoning abilities with minimal leniency towards failure cases.

## 7 CASE STUDY

In our previous discussions, the prompts queried to LLMs and the user study were solely text based as LLMs like GPT4 and GPT3.5 are limited to textual input. Specifically, the robot has to convert its plan into a human-interpretable text (such as natural language or more formal PDDL syntax) which is shown to the user subjects. We extend our arguments along an additional modality where user subjects observe a real robot acting in the world. We consider the case of the Fetch Robot's legibility prompt for this case study. We answer the following research question :

Extended Research Question 1: How do human subjects perform on HRI ToM-PROBE when observing a real robot?

### 7.1 Study design \& Procedure

We use the Fetch robot to execute maneuvers as described in FetchLegibility prompt 1 . The robot picks up a block on the table and takes a step left. This partial plan implicitly indicates the robot's choice of going to the goal on left (instead of the goal on the right) making the partial plan legible. The user subjects have to answer (Q1) and (Q2). Further, we consider the case of the two perturbations proposed in section 4.3 where in Uninformative Context test the Fetch robot has a label saying "Not a Legible robot" but the human cannot read (operationalized by a attaching a label in small font so the subjects are not able to read) and the Inconsistent Belief test where the human observer is unable to see the robot (operationalized by mosaic blurring of the robot). The users answer (Q1) for the two perturbation tests. They use the Google Form based interface with access to a video showing the Fetch robot maneuver in addition to the text as before. We recruit 20 new participants on Prolific with the same requirements as in section 4.2.3. Participant's demographics details can be found in Table 1.

### 7.2 Results

We ask the human subjects whether they can read the text on the robot's head (Fig 1:center) or see the robot acting (Fig 1:right) as a binary question. $100 \%$ users responded they could not read the text and $95 \%$ could not see the robot acting which validates our operationalization of the perturbation. GPT4-Vision [36] is also fed with image frames obtained via stratified sampling (step size $k=\{6,12,24,30\})$ from the video along with the text prompt. Across all sampling step sizes, the LLM always responds that it cannot read the text and cannot see the robot acting in the respective cases. We find that the user subject responses and accuracy $(90 \%$ users correctly identified the behavior-type as legible and $85 \%$ users gave correct answer to Q2) are consistent with Fetch-Legibility text-only case which establishes our baseline that human users can answer ToM queries for the constructed vanilla situation. We find that the users are not affected by the Uninformative Context
( $85 \%$ users correctly identified behavior-type and $95 \%$ users chose the correct explanation in Q2) and Inconsistent Belief perturbations ( $75 \%$ correctly opted for "Can't Say" choice and $25 \%$ chose "No" option). The high accuracy values between vanilla case and perturbation case shows that users were not affected by perturbations. GPT4-V correctly identifies the behavior-type along with the correct reasoning in the vanilla case, however, it fails in the Inconsistent Belief test and the Conviction test. We note that, while GPT4-V answers that it cannot see the robot acting, it still fails Q1 which also reflects inconsistent reasoning abilities. More details can be found in Section A.3.

## 8 CONCLUSION \& FUTURE WORK

In this work we perform a critical investigation of the theory of mind (ToM) abilities of Large Language Models specifically designed for Human Robot Interaction. We consider the second order ToM setup where a robot can query an LLM (acting as a human proxy) to "think how a human observer would perceive the robot's behavior" which can be further utilizes to improve behavior synthesis. We leverage prior works and identify key behavior-types critical for HRI and behavior synthesis - explicability, legibility, predictability and obfuscatory behavior. We use prior works to borrow five HRI domains for these behavior types and construct various situations where the robot behavior belongs to these behavior types. Each of these situations (domain description paired with robot behavior) is given as context to answer two questions. First, to recognize whether the human observing the robot would find its behavior to be of a given behavior type. Second, we ask for an objective explanation (by choosing among a set of choices) for their answer to the first question. We perform a human subject study to obtain lay-user performance measure on these tasks and compare it to LLM performance (on GPT-4 and GPT-3.5-turbo). While the LLM's performance on the first set of experiments is much better than chance behavior, our experiments testing LLMs robustness and conviction highlight that LLMs, while a useful tool for HRI, are not robust ToM agents. Specifically, we find that LLMs fail in our perturbation tests, i.e. Inconsistent Belief and Uninformative Context test and do not possess conviction in its responses. Finally, we provide a case study with Fetch Robot to emphasize that the situations considered in our work are translations of real-world objectives that robot's may have with human observer in the loop, and bolster existing belief that humans are robust towards ToM perturbations that we consider.

Our work provides a first analysis of LLM ToM abilities for an HRI setting and opens up research opportunities in studying the various facets of ToM with LLMs. While future generations of LLMs may improve on vanilla ToM tasks, it becomes every more important to identify and study other crucial failure modes to realize whether the LLM responses are retrieval based or reasoning based. We hope that our contribution can bring forth the HRI community to realize the impact, potential benefits and cautions of utilizing Large Language Models in HRI setting.

## ACKNOWLEDGEMENT

This research is supported in part by ONR grants N00014-18-1-2442, N14-18-1-2840 and N00014-23-1-2409.

## REFERENCES

[1] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023)

[2] Janet Wilde Astington and Jodie A Baird. 2005. Why language matters for theory of mind. Oxford University Press

[3] Janet Wilde Astington and Jennifer M Jenkins. 1995. Theory of mind development and social understanding. Cognition \& Emotion 9, 2-3 (1995), 151-165.

[4] Chris Baker, Rebecca Saxe, and Joshua Tenenbaum. 2011. Bayesian theory of mind: Modeling joint belief-desire attribution. In Proceedings of the annual meeting of the cognitive science society, Vol. 33.

[5] Ali Borji. 2023. A categorical archive of chatgpt failures. arXiv preprint arXiv:2302.03494 (2023).

[6] SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023).

[7] SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023 Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023).

[8] Tathagata Chakraborti, Gordon Briggs, Kartik Talamadupula, Yu Zhang, Matthias Scheutz, David Smith, and Subbarao Kambhampati. 2015. Planning for serendipity. In 2015 IEEE/RS7 International Conference on Intelligent Robots and Systems (IROS). IEEE, 5300-5306

[9] Tathagata Chakraborti, Anagha Kulkarni, Sarath Sreedharan, David E. Smith, and Subbarao Kambhampati. 2018. Explicability? Legibility? Predictability? Transparency? Privacy? Security? The Emerging Landscape of Interpretable Agent Behavior. arXiv:1811.09722 [cs.AI]

[10] Tathagata Chakraborti, Anagha Kulkarni, Sarath Sreedharan, David E Smith, and Subbarao Kambhampati. 2019. Explicability? legibility? predictability? transparency? privacy? security? the emerging landscape of interpretable agent behavior. In Proceedings of the international conference on automated planning and scheduling, Vol. 29. 86-96.

[11] Tathagata Chakraborti, Sarath Sreedharan, Yu Zhang, and Subbarao Kambhampati. 2017. Plan explanations as model reconciliation: Moving beyond explanation as soliloquy. arXiv preprint arXiv:1701.08317 (2017).

[12] Sandra Devin and Rachid Alami. 2016. An implemented theory of mind to improve human-robot shared plans execution. In 2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 319-326.

[13] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. 2023. PaLM-E: An Embodied Multimoda Language Model. In arXiv preprint arXiv:2303.03378.

[14] Connor Esterwood and Lionel P Robert. 2023. The theory of mind and humanrobot trust repair. Scientific Reports 13, 1 (2023), 9877

[15] Chris Frith and Uta Frith. 2005. Theory of mind. Current biology 15, 17 (2005), R644-R645.

[16] Lin Guan, Mudit Verma, Suna Sihang Guo, Ruohan Zhang, and Subbarao Kambhampati. 2021. Widening the pipeline in human-guided reinforcement learning with explanation and context-aware data augmentation. Advances in Neural Information Processing Systems 34 (2021), 21885-21897.

[17] David Gunning. 2018. Machine Common Sense Concept Paper arXiv:1810.07528 [cs.AI]

[18] Thilo Hagendorff. 2023. Deception abilities emerged in large language models. arXiv preprint arXiv:2307.16513 (2023).

[19] Suzanne Hala, Michael Chandler, and Anna S Fritz. 1991. Fledgling theories of mind: Deception as a marker of three-year-olds' understanding of false belief Child development 62, 1 (1991), 83-97.

[20] Laura M Hiatt, Anthony M Harrison, and J Gregory Trafton. 2011. Accommodating human variability in human-robot teams through theory of mind. In Twenty-Second International 7oint Conference on Artificial Intelligence.

[21] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. 2022. Inner monologue: Embodied reasoning through planning with language models arXiv preprint arXiv:2207.05608 (2022).

[22] Jan KocoÅ, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika SzydÅo, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. 2023. ChatGPT: Jack of all trades, master of none. Information Fusion (2023), 101861.

[23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199-22213

[24] Michal Kosinski. 2023. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083 (2023).
[25] Michal Kosinski. 2023. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083 (2023).

[26] Anagha Kulkarni, Siddharth Srivastava, and Subbarao Kambhampati. 2019. Signaling friends and head-faking enemies simultaneously: Balancing goal obfuscation and goal legibility. arXiv preprint arXiv:1905.10672 (2019).

[27] Anagha Kulkarni, Siddharth Srivastava, and Subbarao Kambhampati. 2021. Planning for proactive assistance in environments with partial observability. arXiv preprint arXiv:2105.00525 (2021).

[28] Anagha Kulkarni, Yantian Zha, Tathagata Chakraborti, Satya Gautam Vadlamudi, Yu Zhang, and Subbarao Kambhampati. 2019. Explicable planning as minimizing distance from expected behavior. In AAMAS Conference proceedings.

[29] Jin Joo Lee, Fei Sha, and Cynthia Breazeal. 2019. A Bayesian theory of mind approach to nonverbal communication. In 2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 487-496.

[30] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023. Llm + p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477 (2023).

[31] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023), $1-35$.

[32] Heidemarie Lohmann and Michael Tomasello. 2003. The role of language in the development of false belief understanding: A training study. Child development 74,4 (2003), 1130-1144.

[33] Gabriela Marcu, Iris Lin, Brandon Williams, Lionel P Robert Jr, and Florian Schaub. 2023. " Would I Feel More Secure With a Robot?": Understanding Perceptions of Security Robots in Public Spaces. Proceedings of the ACM on Human-Computer Interaction 7, CSCW2 (2023), 1-34.

[34] Stacy C Marsella, David V Pynadath, and Stephen J Read. 2004. PsychSim: Agent-based modeling of social interactions and influence. In Proceedings of the international conference on cognitive modeling, Vol. 36. 243-248.

[35] R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. 2023. Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve. arXiv:2309.13638 [cs.CL]

[36] OpenAI. 2023. ChatGPT can now see, hear, and speak. https://openai.com/blog chatgpt-can-now-see-hear-and-speak. [Accessed 16-01-2024].

[37] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]

[38] Stefan Palan and Christian Schitter. 2018. Prolific. ac-A subject pool for online experiments. Journal of Behavioral and Experimental Finance 17 (2018), 22-27.

[39] David V Pynadath and Stacy C Marsella. 2005. PsychSim: Modeling theory of mind with decision-theoretic agents. In IFCAI, Vol. 5. 1181-1186.

[40] Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A Smith, and Yejin Choi. 2018. Event2mind: Commonsense inference on events, intents, and reactions. arXiv preprint arXiv:1805.06939 (2018).

[41] Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. 2022. Neural theoryof-mind? on the limits of social intelligence in large lms. arXiv preprint arXiv:2210.13312 (2022)

[42] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728 (2019).

[43] Brian M Scassellati. 2001. Foundations for a Theory of Mind for a Humanoid Robot. Ph. D. Dissertation. Massachusetts Institute of Technology.

[44] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever hans or neural theory of mind? stress testing social reasoning in large language models. arXiv preprint arXiv:2305.14763 (2023).

[45] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael SchÃ¤rli, and Denny Zhou. 2023. Large Language Models Can Be Easily Distracted by Irrelevant Context. In Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 31210-31227. https://proceedings.mlr.press/v202/shi23a.html

[46] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. 2022. Llm-planner: Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv:2212.04088 (2022).

[47] Sarath Sreedharan, Subbarao Kambhampati, et al. 2017. Balancing explicability and explanation in human-aware planning. In 2017 AAAI Fall Symposium Series.

[48] Sarath Sreedharan, Anagha Kulkarni, and Subbarao Kambhampati. 2022. Explainable Human-AI Interaction: A Planning Perspective. Springer Nature.

[49] Sarath Sreedharan, Utkarsh Soni, Mudit Verma, Siddharth Srivastava, and Subbarao Kambhampati. 2020. Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations. arXiv preprint arXiv:2002.01080 (2020).

[50] Varsha Suresh and Desmond C Ong. 2021. Using knowledge-embedded attention to augment pre-trained language models for fine-grained emotion recognition. In 2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, $1-8$.

[51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).

[52] Tomer Ullman. 2023. Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. arXiv:2302.08399 [cs.AI]

[53] Tomer Ullman. 2023. Large language models fail on trivial alterations to theoryof-mind tasks. arXiv preprint arXiv:2302.08399 (2023)

[54] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2023. Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). arXiv:2206.10498 [cs.CL]

[55] Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati. 2023. Exploiting Unlabeled Data for Feedback Efficient Human Preference based Reinforcement Learning. arXiv preprint arXiv:2302.08738 (2023).

[56] Mudit Verma and Katherine Metcalf. 2022. Symbol Guided Hindsight Priors for Reward Learning from Human Preferences. arXiv preprint arXiv:2210.09151 (2022).

[57] Samuele Vinanzi, Massimiliano Patacchiola, Antonio Chella, and Angelo Cangelosi. 2019. Would a robot trust you? Developmental robotics model of trust and theory of mind. Philosophical Transactions of the Royal Society B 374, 1771 (2019), 20180032.

[58] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).

[59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824-24837

[60] Henry M Wellman. 1992. The child's theory of mind. The MIT Press.

[61] Heinz Wimmer and Josef Perner. 1983. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Cognition 13, 1 (1983), 103-128.

[62] BigScience Workshop, :, and Teven Le Scao et al. 2023. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. arXiv:2211.05100 [cs.CL]

[63] Kisu Yang, Dongyub Lee, Taesun Whang, Seolhwa Lee, and Heuiseok Lim. 2019. Emotionx-ku: Bert-max based contextual emotion classifier. arXiv preprint arXiv:1906.11565 (2019).

[64] Xin Ye, Lionel Robert, et al. 2023. Human Security Robot Interaction and Anthropomorphism: An Examination of Pepper, RAMSEE, and Knightscope Robots. (2023).

[65] Zahra Zahedi, Sarath Sreedharan, and Subbarao Kambhampati. 2022. A MentalModel Centric Landscape of Human-AI Symbiosis. arXiv:2202.09447 [cs.AI]

[66] Zahra Zahedi, Sarath Sreedharan, and Subbarao Kambhampati. 2023. A Mental Model Based Theory of Trust. arXiv:2301.12569 [cs.AI]

[67] Zahra Zahedi, Sarath Sreedharan, Mudit Verma, and Subbarao Kambhampati. 2022. Modeling the Interplay between Human Trust and Monitoring. In 2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI). 1119-1123. https://doi.org/10.1109/HRI53351.2022.9889475

[68] Zahra Zahedi, Mudit Verma, Sarath Sreedharan, and Subbarao Kambhampati. 2021. Trust-Aware Planning: Modeling Trust Evolution in Longitudinal HumanRobot Interaction. CoRR abs/2105.01220 (2021). arXiv:2105.01220 https://arxiv. org/abs/2105.01220

[69] Yu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti, Hankz Hankui Zhuo, and Subbarao Kambhampati. 2017. Plan explicability and predictability for robot task planning. In 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 1313-1320.

[70] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large Language Models. arXiv:2303.18223 [cs.CL]
