# ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR 

Junda Zhu ${ }^{1 *} \quad$ Lingyong Yan $^{2 *} \quad$ Haibo Shi ${ }^{2} \quad$ Dawei Yin ${ }^{2} \quad$ Lei Sha $^{1 \dagger}$<br>${ }^{1}$ Beihang University, Beijing, China<br>${ }^{2}$ Baidu Inc., Beijing, China<br>junda_zhu@outlook.com lingyongy@gmail.com<br>shihaibo@baidu.com yindawei@acm.org shalei@buaa.edu.cn


#### Abstract

Large language model (LLM) has proven to benefit a lot from retrieval augmentation in alleviating hallucinations confronted with knowledge-intensive questions. Retrievalaugmented generation (RAG) adopts IR-based techniques utilizing semantic-relevant documents as the generator's input context and realizes external knowledge injection. However, on today's Internet which is flooded with content generated by LLMs, there are too many "related yet useless" documents or even fake knowledge fabricated by LLMs, which will introduce extra noise to the generator and distract it from giving correct results. To this end, we regard the training of the RAG generator model as a multi-agent adversarial-defensive system, guiding the generator to have a better taste of whether a specific document helps answer the question through the Adversarial Tuning in a Multi-agent (ATM) system to strengthen the generator's robustness in an RAG pipeline. After rounds of multi-agent iterative tuning, we find that the ATM GENERATOR can eventually discriminate useful documents amongst LLM fabrications and achieve better performance than strong baselines.


## 1 Introduction

Pre-trained Large Language Models (LLMs) such as Llama (Touvron et al., 2023a,b), Mistral (Jiang et al., 2023), or black-box GPT-4 (Achiam et al., 2023) have demonstrated impressive power in text generation. However, due to the knowledge updating or domain-specific knowledge deficiency, the trained LLMs often fail to provide concise answers in the knowledge-intensive question answering (KiQA) task, leading to the answer hallucination (Macpherson and Platchias, 2013).

Retrieval-Augmented Generation (RAG, Lewis et al., 2020) alleviates this dilemma through lever-[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_2f3e62ff27d8ea13a605g-01.jpg?height=773&width=779&top_left_y=750&top_left_x=1047)

Figure 1: Recall rate of the golden document with Contriever (Gao et al., 2023) (above) and Acc (Subspan EM) performance of LLMs on NaturalQuestions (Kwiatkowski et al., 2019)(below) with the number of candidate documents increasing.

aging relevant knowledge from external sources. The knowledge is usually retrieved by relevancebased retrievers (Robertson et al., 2009; Reimers and Gurevych, 2019; Karpukhin et al., 2020; Gao et al., 2021), and then injected into the prompts of LLMs (termed as the GENERATOR) to give final answers. In practice, the LLMs often take multiple relevant documents as inputs to ensure the comprehensiveness of final answers. However, introducing more candidates usually results in more related yet useless documents. Also, recent studies (Shi et al., 2023a; Cuconasu et al., 2024; Wu et al., 2024) reveal that the correctness of LLMs is vulnerable to these noises in the retrieved results. For instance, as shown in Figure 1, though the recall rate of useful knowledge can reach a relatively high level with more documents, the accuracy of final answers may gradually decrease. Although researchers try lots
of efforts to trade-off between comprehensiveness and correctness, a GENERATOR inevitably suffers from noisy documents due to the weak retriever and huge amount of fake knowledge on the Internet.

To this end, this work aims at improving the GENERATOR's capability and robustness against knowledge noises existing in the retrieved documents with the proposed adversarial tuning. We concentrate on optimizing the GENERATOR's performance from the following aspects: (1) Generation capacity: we seek to bridge the gap between Close- $Q A$ and $R A G$ - $Q A$ through multi-task training, to enhance its $Q A$ ability while maintaining its reasoning capability; (2) Robustness: we attack the order of document lists and inject additional fabrications to challenge the GenERator. After that, the optimized GENERATOR can perform better in giving correct answers and resisting fake knowledge simultaneously.

Our proposed ATM system has two agents: the GENERATOR (i.e. the LLM) and the ATTACKER. The ATTACKER tries its best to fabricate fake knowledge and increase the noise in the document list, the GENERATOR resists the noise and remains robust. We conduct multi-agent iterative optimization, through which ATTACKER is promising to emerge stronger attack patterns while the GENERATOR develops greater robustness. To the best of our knowledge, ATM pioneers the utilization of LLM alignment optimization in the multi-agent scenario and realizes both agents' optimization simultaneously instead of self-aligning (Schulman et al., 2017; Rafailov et al., 2024; Chen et al., 2024; Sun et al., 2024). Since the adversarial tuning introduces extremely misleading documents, in order to ensure the stability of the optimization, we design an initial tuning stage up front to provide a better optimization starting point for the GENERATOR.

Through rounds of adversarial tuning, we end up with an aggressive ATTACKER and a robust GENERATOR. In our experiments, we draw a conclusion that through continuously improving the GENERATOR's robustness and QA capacity with adversarial tuning, the optimized GENERATOR is promising to withstand the noise brought by fabrications, giving more correct answers that remain robust as more aggressive documents being injected. The whole optimization system is depicted in Figure 2. Our contributions can be summarized as follows:

- We concentrate on optimizing the GENERATOR, utilizing LLM's generative capability while greatly improving its QA capacity, we find it hopeful to improve the correctness of the generated answers in the RAG pipeline.
- We introduce an aggressive ATTACKER and pioneer the utilization of adversarial-defensive tuning in the retrieval augmentation scenario. Through iterative optimization, we end up realizing a multi-agent co-evolution.
- We explore the possibility of GENERATOR resisting LLM fabrications with real-world simulation evaluation, which is believed as a strong endorsement of the validity of our proposed method encountering a massive amount of AI-generated content.


## 2 Related Work

### 2.1 Retrieval-Augmented Language Models

Retrieval-Augmented Language Models (RALMs) are aimed at bridging the gap between Close-QA and $R A G$ - $Q A$ and optimizing LLMs to perform better with retrieved documents. RAFT (Zhang et al., 2024) strengthens the model with domain-specific $R A G-Q A$ data and extra reasoning chains. Junqing et al. (2023) proposes a data engineering technique to alleviate "Lost in the Middle" (Liu et al., 2024) phenomenon. RAT (Wang et al., 2024b) conducts CoT and self-revision, forming a reasoning chain towards generating the final answer.

As a separate system, retriever and generator have different training objectives, giving rise to risks of indirect optimization. To this end, REPLUG (Shi et al., 2023b) tunes the retriever with the output token probability without the need for direct access to generator's parameter. RA-DIT (Lin et al., 2023) introduces a co-training setting for both modules and achieves dual optimization. RAGend2end (Siriwardhana et al., 2023) proposes to dynamically re-index the document library with optimized retriever during training. Self-RAG (Asai et al., 2023) and Adaptive-RAG (Jeong et al., 2024) trains models to be fluent in answer generation and aware of whether to retrieve and what to retrieve, mitigating noises brought by unnecessary retrieval.

It is widely recognized that with more parameter, LLM also can act as a re-ranker better (Sun et al., 2023). Recent works like MetaEOL (Lei et al., 2024), GritLM (Muennighoff et al., 2024) and LLM2Vec (BehnamGhader et al., 2024) train or prompt LLMs to act as a sentence encoder and discover that LLMs have strong embedding capabilities. REAR (Wang et al., 2024a) seeks to further

![](https://cdn.mathpix.com/cropped/2024_06_04_2f3e62ff27d8ea13a605g-03.jpg?height=497&width=1596&top_left_y=231&top_left_x=227)

Figure 2: Overview of the proposed ATM System. User questions act as queries for document retrieval and input to ATTACKER for fabrications production. The two agents conduct multi-agent iterative tuning afterwards. GENERATOR rewards the ATTACKER according to its fabrications, documents containing golden answer or related knowledge (Golden) are rejected while misleading documents (Red) are preferred. The pseudo-code of iterative optimization can also be found in Algorithm 1.

plug an extra re-ranking module to LLMs, and simultaneously improve its document ranking and answer generation capabilities.

### 2.2 Adversarial Learning and Robustness

Generative Adversarial Networks (Goodfellow et al., 2020), widely referred to as GAN, is first proposed in image classification tasks. Its setting makes it possible that the robustness of the discriminator model can be gradually enhanced. Similar idea also works in NLP tasks. Li et al. (2017) utilizes the trajectory-level reward from the discriminator distinguishing machine-generated content and conduct REINFORCE (Williams, 1992) to enhance the generator's anthropomorphism.

Different from GAN-inspired methods mentioned above, we argue that, our work expands the adversarial idea to a system where both agents are generative language models. As for the RALMs, some recent works like RetRobust (Yoran et al., 2023) are aimed at making LLMs robust against irrelevant documents. However, to the best of our knowledge, our proposed ATM is the first to consider the vulnerability of RAG systems to LLM fabricated knowledge and introduce adversarial roles in the optimization of the GENERATOR.

## 3 ATM System

We propose ATM where agents optimize towards opposite directions. Specifically, there are two different roles: ATTACKER fabricating fake knowledge; GEnerator resists the perturbation and answers the question correctly. We find that the capabilities of different agents can be strengthened simultaneously, making it possible to achieve simultaneous evolution with adversarial tuning.

### 3.1 GENERATOR

We focus on optimizing the Retrieval-augmented GENERATOR and find it hopeful to answer questions well with retrieved documents and remain resistant to fabrications. We believe a strong GENERATOR should be able to capture knowledge in the reference documents of the question without the need for massive parameterized knowledge. We seek to improve the model's performance by improving its ability to distinguish and utilize useful knowledge from external documents. Borrowed from GAN, our optimization goal can be min-max formalized as Equation 1:

$$
\begin{align*}
& \min \| G\left(a_{\text {golden }} \mid q, \boldsymbol{d}^{\prime}\right)-\max G\left(a_{\text {golden }} \mid q, \boldsymbol{d}\right) \\
& -\max G\left(a_{\text {golden }} \mid q,\left\{d^{*}\right\}\right) \| \tag{1}
\end{align*}
$$

where $G(\cdot)$ denotes the language model probability (Bengio et al., 2000; Radford et al., 2018) of generative models (Equation 2). Input sequence $\boldsymbol{d}$ denotes the document list, $\boldsymbol{d}^{\prime}$ is the attacked document list, $d_{k}$ is the golden document that contains useful knowledge, $q$ is the knowledge-intensive question. Optimized $G$ should maximize the probability of generating the golden answer given useful knowledge only $d^{*}$ (can also be seen as reading comprehension), while minimizing its gap between retrieval-augmented document list and attacked list.

![](https://cdn.mathpix.com/cropped/2024_06_04_2f3e62ff27d8ea13a605g-04.jpg?height=640&width=771&top_left_y=248&top_left_x=240)

Figure 3: AtTACKer's attacking patterns. Fabrications are model generations with misleading fake knowledge. Rule-based attack shuffles the relative order of retrieved documents and sentence order of documents.

$$
\begin{align*}
G(a \mid q, \boldsymbol{d}) & =G\left(a \mid q, d_{1} \ldots \oplus d_{n}\right)  \tag{2}\\
& =\prod_{t=1}^{T_{a}} P_{G}\left(a_{t} \mid a_{<t} ; q, d_{1} \ldots \oplus d_{n}\right)
\end{align*}
$$

where $\oplus$ denotes string concatenation of retrieved documents which constitutes the paragraph in the $R A G-Q A$ prompt in Appendix A.2. We compute the token-level cumulative probability with answer $a$ whose token length is $T_{a}$ as output of GENERATOR. Apart from vanilla RAG, we also conduct multi-task data synthesis. Reading comprehension $\left(\boldsymbol{d} \leftarrow\left\{d^{*}\right\}\right)$, close-book QA $(\boldsymbol{d} \leftarrow \phi)$ and key document extraction are introduced to strengthen GENERATOR's instruction following ability. Our multi-task prompt template can also be found in Appendix A.2.

### 3.2 ATTACKER

Resistance against fabrications is believed especially critical for RAG with today's Internet content flooded with tons of AI-generated information (Briesch et al., 2023). Moreover, "Lost in the Middle" has shown LLMs being extremely sensitive to the relative position of useful knowledge in the list. To this end, the ATTACKER is encouraged to fabricate fake knowledge and hide useful knowledge, challenging the GENERATOR to give a right answer.

Rule-based Attack In order to challenge GENERATOR's positional robustness, we utilize a rule- based approach for document list disordering as depicted in Figure 3. In a rule-based attack, a document list is shuffled and duplicated, which makes irrelevant documents on the top while golden ones in the middle. It is a training-free method that leverages human-set rules, end up drowning the truth in irrelevant fake knowledge.

Model-based Attack Model-based attack aims at training the ATTACKER to inject highly semantically related fake knowledge into the retrieved document list. As depicted in Figure 3, fabrications greatly resemble the format of Wikipedia and contain misleading knowledge. These fabrications increase GENERATOR's language model perplexity $(P P L)$ as formalized in Equation 3:

$\mathrm{PPL}_{G}=\exp \left\{-\frac{1}{T_{a}} \sum_{t=1}^{T_{a}} \log P_{G}\left(a_{t} \mid a_{<t} ; q,\left\{d_{f}\right\}\right)\right\}$

where $d_{f}$ denotes one document containing fake knowledge. If GENERATOR is misled by $d_{f}$, its $P P L$ of generating the golden answer should be relatively high. To this end, the ATTACKER's optimization goal is to generate documents that can maximize the $P P L$ of the GENERATOR.

## 4 Multi-agent Iterative Tuning

With the optimization goals discussed above, in this section, we present the methodology to realize continuous evolution. In the ATM system, ATtACKER and GENERATOR undergo an adversarial-defensive game. ATTACKER continuously increases the attack intensity, and GENERATOR gradually improves its generation capability while resisting attack, resulting in an ATTACKER with strong attack pattern and a GENERATOR with great robustness against fabrications.

### 4.1 Initial Tuning

We conduct optimization with constructed Multitask SFT dataset on the GENERATOR to improve its instruction following and question answering ability. We seek to migrate the chat capabilities of LLMs to question answering with documents retrieved (i.e. $R A G-Q A$ ), which minimizes the loss as denoted in Equation 4:

$$
\begin{equation*}
\mathcal{L}_{S F T}=-\sum_{t=1}^{T_{a}} \log P_{G}\left(a_{t} \mid a_{<t} ; q, \boldsymbol{d}\right) \tag{4}
\end{equation*}
$$

As for the ATTACKER, we align a Mistral 7B
model to the aggressive role with prompt engineering following Appendix A.1.

### 4.2 Adversarial Iterative Optimization

After both agents are initially tuned, we conduct multi-agent iterative optimization. ATTACKER fabricates increasingly misleading documents while GENERATOR needs to constantly improve its generating ability. When a fabrication containing fake knowledge makes GENERATOR give an incorrect answer, we consider the ATTACKER to have performed a successful attack. For the GENERATOR, it should keep replying with correct answers robustly, thus improving the robustness of the model.

In the setting of prevalent aligning approaches, LLMs achieve alignment with human preferences based on manually labeled or self-constructed preference data. In our multi-agent scenario, models can mutually optimize with each other's responses as feedback and achieve iterative optimization.

The overview of this process is as depicted in Algorithm 1, where the notations are described in Table 7. In this iterative stage, ATTACKER generate fabrications to construct data to conduct an adversarial tuning on the GENERATOR while the GENERATOR returns feedback to optimize the ATTACKER. We utilize Direct Preference Optimization (DPO, Rafailov et al., 2024) with Misleading-based Feedback (i.e. $P P L$ ) from the GENERATOR to train the ATTACKER and help it align towards a better attacking pattern. The optimization goal of DPO which aligns LLMs with preference data can be formalized in Equation 5:

$$
\begin{align*}
\mathcal{L}_{\mathrm{DPO}}(\widetilde{A} ; A)= & -\left[\operatorname { l o g } \sigma \left(\beta \log \frac{\widetilde{A}\left(d_{\text {win }} \mid q\right)}{A\left(d_{\text {win }} \mid q\right)}\right.\right. \\
& \left.\left.-\beta \log \frac{\widetilde{A}\left(d_{\text {lose }} \mid q\right)}{A\left(d_{\text {lose }} \mid q\right)}\right)\right] \tag{5}
\end{align*}
$$

where $d_{w i n}$ and $d_{\text {lose }}$ represent the fabrications of ATTACKER, win denotes a high $P P L$ that is the most misleading while lose denoting lowest $P P L$ which is not that challenging, this matches exactly the optimization objective expressed in Equation 3. $\sigma$ denotes the sigmoid activation function, while $\beta$ is a hyper-parameter.

As for the GENERATOR, apart from utilizing knowledge increasingly better, it's also required that the model remain robust and give the right answers regardless of noise documents injected, which can be summarized from Equation 1. To this end, we introduce a novel Multi-agent Iterative Tuning Optimization (MITO) loss for this phase, the optimization objective is depicted in Equation 6. We use SFT of golden answer given attacked document list and add it to token-level Kullback-Leibler Divergence (Kullback and Leibler, 1951) of golden answer probability between normal document list and attacked document list as regularization term as depicted in Equation 6:

$$
\begin{align*}
\mathcal{L}_{M I T O}= & \mathcal{L}_{S F T}+\alpha \mathcal{L}_{K L} \\
\mathcal{L}_{K L}= & \sum_{t=1}^{T_{a}} \mathbb{D}_{K L}\left[P_{G}\left(a_{t} \mid a_{<t} ; q, \boldsymbol{d}\right)\right. \\
& \left.\| P_{G}\left(a_{t} \mid a_{<t} ; q, \boldsymbol{d}^{\prime}\right)\right] \tag{6}
\end{align*}
$$

where $\boldsymbol{d}$ and $\boldsymbol{d}^{\prime}$ denotes retrieved and attacked document list respectively, $\alpha$ is a pre-set hyperparameter. Math derivations can be found in Appendix $\mathrm{B}$, implementation details can be found in Appendix C.3.

## 5 Experiments

### 5.1 Experimental Setup

Datasets We focus on KiQA datasets and conduct $R A G-Q A$. We use the training split of Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and WEBQUESTIONS (Berant et al., 2013) together with retrieved documents as our training dataset, and run evaluation on 4 test splits together with PopQA (Mallen et al., 2023).

Evaluation We build the test set with retrieved documents and fabrications of strong LLMs, where there are 5 fabrications and 5 retrieved documents for each question. Details of models we used can be found at C.1. As for metrics, we adopt strict Exact Match (EM) metric following Lee et al. (2019). Since sometimes the answering style mismatch may bring additional reductions, we also report the Subspan EM and F1 as additional metrics to balance between the correctness and comprehensiveness of answers.

Baselines We compare our model's performance with Llama2 chat-aligned model i.e. the GENERATOR before optimization, to illustrate the improvement brought by the proposed optimization. As a work making LLMs robust to irrelevant documents, we select RetRobust (Yoran et al., 2023) for comparison. We also report our experiment results in comparison with REAR (Wang et al., 2024a) which

| LLMs | Natural Questions |  |  | TriviaQA |  |  | WebQuestions |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Subspan EM | EM | F1 | Subspan EM | EM | F1 | Subspan EM | EM | F1 |
|  | $R A G-Q A$ |  |  |  |  |  |  |  |  |
| Llan | 33.21 | 23.55 | 32.27 | 56.52 | 29.62 | 39.98 | 9 | 8.46 | 22.34 |
| REAF |  | 31.63 | 39 |  | 42.92 | 53.33 |  | 31.94 | 41.27 |
| RetRol |  | 47.26 | 56.72 |  | 58.39 | 65.46 |  | 38.05 | 51.39 |
|  | 0 | 31.80 | 39.64 |  | 55.30 | 66.16 |  | 40.10 | 50.74 |
| ATI | 49 | 46.26 | 55.73 | 66.63 | 57.81 | 70.01 | 5 | 44.93 | 54.26 |
|  |  | 49.53 | 58.90 |  | 57.83 | 70.22 |  | 46.51 | 55.30 |
|  | 5 | 54.21 | 63.23 | 69.33 | 59.77 | 72.30 |  | 47.28 | 56.31 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 3}$ | 3.06 | 53.74 | 62.72 | 67.93 | 58.15 | 71.06 | 48.53 | 47.35 | 56.02 |
|  | Close-book QA |  |  |  |  |  |  |  |  |
| -Chat |  | 13.27 | 23.50 | 43.23 | 32.19 | 45.12 |  | 13.24 | 33.04 |
| $\Gamma M$ |  | 24.21 | 32.96 | 45.21 | o | 47.78 | 40.45 | 38.93 | 47.77 |
| $\mathrm{ATM}_{7 \mathrm{~B}-1}$ | 28.89 | 25.62 | 34.91 | 1 | 36.59 | 48.84 | 40 | 38.68 | 47.67 |
|  |  |  | 36.3 |  |  | 49.40 |  | 40.06 | 49.09 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 3}$ | 32.22 | 28.80 | 37.43 | 47.78 | 38.95 | 50.42 | 42.18 | 40.45 | 49.47 |

Table 1: Evaluation Results of ATM GenErator and baselines on KiQA datasets Natural Questions, TriviaQA and WebQuestions. Best performing models are marked bold.
![](https://cdn.mathpix.com/cropped/2024_06_04_2f3e62ff27d8ea13a605g-06.jpg?height=502&width=1584&top_left_y=1062&top_left_x=230)

Figure 4: F1 Score of different GENERATor at different optimization stages. The number of total documents (fabrications and retrieved documents together) remains 10. We conduct this experiment to investigate the effects of Different Fabrication Numbers and observe the Generator getting more robust and steady.

follows a rank-then-generate setting and Self-RAG (Asai et al., 2023) which makes LLMs conduct adaptive retrieval and generate answers.

### 5.2 Main Results

Table 1 shows our evaluation results on the test set. ATM proves to obtain significant gain on the $R A G$ $Q A$ with an over $31 \%$ EM score improvement on Natural Questions observed. Metric improvements can also be observed with optimization proceeding. Comparing to our baselines, our GENERATOR gradually surpasses previous state-of-the-art RALMs, RetRobust and Self-RAG, by an average of $5 \%$ margin across datasets. It is believed that improvements mainly come from the extended amount of data involved in adversarial tuning.

Results on the other two datasets demonstrate similar tendencies, indicating that through the two- stage tuning, ATM GENERATORs can better leverage external knowledge to answer questions. As for the gap of REAR compared to its original paper, we analyze that, since REAR routes the top documents scored by LLM with a rank-head before generating, it is severely vulnerable to fabrications, ending up with the abesence of useful documents and leading to the dramatic shrinkage in accuracy encountering fabricated knowledge.

In addition, we observe that the GENERATOR

![](https://cdn.mathpix.com/cropped/2024_06_04_2f3e62ff27d8ea13a605g-06.jpg?height=48&width=774&top_left_y=2323&top_left_x=1052)
still achieve better or comparable performance. This indicates that initial tuning is necessary to adapt GENERator to the $R A G$ - $Q A$ scenario. We also observe that on the Close- $Q A$, which reflects the ability of GENERATOR to use its own knowledge to answer the questions, a continuous rise can be observed with the number of optimization itera-

| LLMs | Natural Questions |  |  | TriviaQA |  |  | WebQuestions |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Subspan EM | EM | F1 | Subspan EM | EM | F1 | Subspan EM | EM | F1 |
|  | Llama3-70B |  |  |  |  |  |  |  |  |
| Llama2 $_{\text {7B-Chat }}$ | 36.93 | 29.00 | 37.40 | 49.66 | 41.42 | 49.63 | 38.63 | 19.24 | 33.31 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 0}$ | 43.32 | 41.36 | 49.01 | 55.11 | 47.20 | 58.34 | 44.54 | 42.67 | 51.87 |
| ATM $_{7 \text { B-Iterl }}$ | 47.20 | 43.68 | 53.41 | 58.26 | 49.82 | 61.34 | 46.36 | 44.72 | 53.67 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 2}$ | 48.21 | 44.12 | 54.57 | 61.23 | 52.27 | 63.96 | 46.16 | 44.19 | 53.05 |
| ATM $_{7 \text { B-Iter3 }}$ | 49.20 | 46.34 | 55.05 | 61.04 | 52.69 | 64.40 | 45.23 | 43.80 | 52.56 |
|  | Mixtral- $8 \times 22 \mathrm{~B}$ |  |  |  |  |  |  |  |  |
| Llama2 $_{\text {7в-Chat }}$ | 32.27 | 26.79 | 32.99 | 44.40 | 37.42 | 44.14 | 38.44 | 19.09 | 33.05 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 0}$ | 51.52 | 48.01 | 57.04 | 67.52 | 58.96 | 70.83 | 45.32 | 43.90 | 52.52 |
| ATM $_{7 \text { B-Iter } 1}$ | 53.88 | 49.76 | 59.01 | 67.96 | 59.40 | 71.29 | 47.93 | 46.06 | 54.37 |
| ATM $_{7 \text { B-Iter2 }}$ | 55.92 | 52.77 | 61.14 | 70.41 | 61.65 | 73.39 | 46.60 | 44.49 | 53.15 |
| ATM $_{7 \text { B-Iter3 }}$ | 57.51 | 54.27 | 62.64 | 69.12 | 60.36 | 72.57 | 45.62 | 44.05 | 52.73 |

Table 2: Results with generated fabrications from Various Fabricators, best performing iterations are marked bold.

| LLMs | PopQA |  |  |
| :--- | :---: | :---: | :---: |
|  | Subspan EM | EM | F1 |
| Llama 27_-Chat $_{7 \text { 1 }}$ | 38.62 | 31.98 | 37.89 |
| REAR $_{\text {7B }}$ | 40.24 | 38.75 | 42.58 |
| RetRobust $_{13 \mathrm{~B}}$ | 48.34 | 37.69 | 50.76 |
| Self-RAG $_{13 \mathrm{~B}}$ | 45.04 | 18.76 | 21.92 |
| ATM $_{\text {7B-Iter0 }}$ | 42.50 | 34.09 | 44.69 |
| ATM $_{\text {7B-Iter1 }}$ | $\mathbf{5 1 . 4 2}$ | $\mathbf{4 2 . 0 6}$ | $\mathbf{5 4 . 0 5}$ |
| ATM $_{\text {7B-Iter2 }}$ | 46.11 | 37.62 | 48.57 |
| ATM $_{\text {7B-Iter3 }}$ | 48.92 | 40.15 | 52.13 |

Table 3: Results (Unseen Data) of retrieval-augmented ATM GENERATOR and baselines on KiQA dataset PopQA which is unseen during training.

| LLMs | Subspan EM | F1 |
| :--- | :---: | :---: |
| Llama2 $_{\text {7B-Chat }}$ | $37.62( \pm 6.36)$ | $27.88( \pm 5.51)$ |
| ATM $_{\text {7B-Iter0 }}$ | $46.19( \pm 1.77)$ | $53.18( \pm 1.59)$ |
| ATM $_{\text {7B-Iter1 }}$ | $47.61( \pm 1.99)$ | $53.79( \pm 1.65)$ |
| ATM $_{\text {7B-Iter2 }}$ | $49.09( \pm 1.84)$ | $\mathbf{5 6 . 0 7}( \pm 1.45)$ |
| ATM $_{\text {7B-Iter3 }}$ | $\mathbf{4 9 . 1 5}( \pm \mathbf{1 . 5 9})$ | $56.04( \pm \mathbf{1 . 4 2})$ |

Table 4: Resistance against Bad Sorting on WeBQuESTIONS, standard deviation was calculated with results of experiments repeated 20 times.

tions increasing, ending up with an $8.48 \%$ improvement of Subspan EM on Natural Questions, $4.55 \%$ on TriviaQA and $1.58 \%$ on WEBQUESTIONS.

It is also noteworthy that the performance of ATM decreases slightly after at most three iterations of adversarial tuning, indicating that the optimization reaches convergence. In practice, 2 or 3 iterations are sufficient to improve the performance of generator, depending on the amount and diversity of data used for adversarial tuning.

### 5.3 Detailed Analysis

In order to illustrate the scalability of our proposed method, we conduct detailed analysis on $R A G-Q A$.

Unseen Data We further conduct a detailed analysis in order to look into how the GENERATOR can scale to unseen data. Table 3 reveals results on unseen dataset PopQA, on which we also observed obvious metrics gains.

Bad Sorting Considering possible bad sorting brought by weak retrievers, we further investigate the positional robustness enhancement as reported in Table 4 on WebQuestions. We conducted random shuffling and repeated the experiment 20 times to get the stability performance of the GENERATOR. As is observed, the accuracy score of GENERATOR improves a lot, while the standard deviations continuously declining, which implies strong positional robustness.

Various Fabricators We further evaluate the GENERATOR encountering various fabricators as reported in Table 2. As is shown, the ATM GENERATOR has little or no drop in performance encountering more powerful Mixtral model. A similar tendency can also be observed with Llama3 as the fabricator. where the GENERATOR increasingly robust across iterations.

Different Fabrication Numbers Given the fact that real-world retriever may recall a variable number of fabrications, we keep the top-10 setup and change the number of fabrications from 5 to 9 to investigate how it affect the final performance. As shown in Figure 4, after initial tuning, the GENERATOR performs well but is not that robust. Through adversarial tuning with data generated by the AT-

| $\mathbf{L L M s}$ | Natural Questions |  |  | TriviaQA |  |  | WebQuestions |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Subspan EM | $\mathbf{E M}$ | F1 | $\overline{\text { Subspan EM }}$ | $\mathbf{E M}$ | F1 | $\overline{\text { Subspan EM }}$ | $\mathbf{E M}$ | F1 |
| Llama27B-Chat | 33.21 | 23.55 | 32.27 | 56.52 | 29.62 | 39.98 | 38.29 | 8.46 | 22.34 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 0}$ | 49.73 | 46.26 | 55.73 | 66.63 | 57.81 | 70.01 | 46.75 | 44.93 | 54.26 |
|  | $\alpha=0(\mathrm{SFT})$ |  |  |  |  |  |  |  |  |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 1}$ | 51.72 | 47.21 | 56.69 | 67.71 | 58.04 | 70.69 | 45.52 | 43.45 | 51.27 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 2}$ | 50.93 | 46.88 | 54.31 | 68.83 | 58.79 | 71.37 | 46.12 | 43.71 | 53.58 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 3}$ | 52.89 | 48.06 | 57.75 | 65.22 | 55.30 | 68.56 | 47.31 | 45.52 | 55.67 |
|  | $\alpha=0.1$ |  |  |  |  |  |  |  |  |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 1}$ | 54.67 | 50.98 | 59.67 | 65.91 | 57.07 | 68.50 | 47.72 | 47.01 | 55.83 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 2}$ | 56.73 | 53.39 | 62.02 | 67.72 | 59.56 | 70.25 | 45.98 | 44.43 | 52.14 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 3}$ | 57.21 | 52.97 | 62.85 | 69.41 | 59.62 | 71.98 | 46.17 | 43.62 | 53.70 |
|  | $\alpha=0.2$ (Default) |  |  |  |  |  |  |  |  |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 1}$ | 53.05 | 49.53 | 58.90 | 66.74 | 57.83 | 70.22 | 47.59 | 46.51 | 55.30 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 2}$ | 57.73 | 54.21 | 63.23 | 69.33 | 59.77 | 72.30 | 48.91 | 47.28 | 56.31 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 3}$ | 56.06 | 53.74 | 62.72 | 67.93 | 58.15 | 71.06 | 48.53 | 47.35 | 56.02 |
|  | $\alpha=0.5$ |  |  |  |  |  |  |  |  |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 1}$ | 48.81 | 45.52 | 53.79 | 64.31 | 56.12 | 66.93 | 47.13 | 45.29 | 55.34 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 2}$ | 47.13 | 44.36 | 51.68 | 65.75 | 57.31 | 68.13 | 46.69 | 45.14 | 54.77 |
| $\mathrm{ATM}_{7 \mathrm{~B}-\mathrm{Iter} 3}$ | 44.37 | 41.70 | 49.05 | 64.69 | 57.07 | 67.56 | 45.79 | 43.82 | 52.16 |

Table 5: Ablation of ATM GENER atOR optimized with different hyper-parameter $\alpha$ during adversarial tuning.

| Attacking Types | F1 |
| :--- | :---: |
| Before Optimization | 32.27 |
| Iteration 0 |  |
| Rule-based | 55.73 |
| w/o Rule-based | $49.11(-6.62)$ |
| Iteration 1 |  |
| Rule-based + Model-based | 58.90 |
| w/o Model-based | $54.39(-4.51)$ |
| w/o Rule-based | $56.68(-2.22)$ |
| w/o Rule-based + Model-based | $52.36(-6.54)$ |

Table 6: Ablation with diffrent attacking types on Natural Questions, Iteration 0 and Iteration 1.

TACKER, GENERATOR proves to benefit from the optimization and getting GENERATOR more robust (smoother curves).

### 5.4 Ablation Study

We conduct our ablation to investigate the essential components in our optimization. For MITO, we train with different $\alpha$ in Equation 6. As is formalized, when $\alpha=0$ the optimization degenerates to SFT. From Table 5 we observe that, (1) with pure SFT the optimization has relatively low performance ceilings: a $5 \%$ drop can be seen on Natural Questions; (2) lower $\alpha$ makes the optimization more steady while a higher one brings instability. When $\alpha$ becomes 0.5 , a significant drop is witnessed at the start of optimization.

We also conduct ablation to prove the necessity of different attacking types. As shown in Ta- ble 6 , a significant drop can be observed without model-based attack, which injects fabrications during training. Optimization with rule-based attack also proves to be necessary. We believe that the positional change can increase the diversity of document list, thus enhancing the training effectiveness. Apart from the GENERATOR, we also monitored the attacking intensity of ATTACKER across iterations as reported in Appendix D.

## 6 Conclusion

In this paper, we propose a novel Adversarial Tuning Multi-agent system (ATM) to train the retrievalaugmented GENERATOR. We conduct iterative optimization to improve GENERATOR's accuracy and robustness. In order to simulate the real-world $R A G-Q A$, we also investigate the robustness of GENERATOR encountering bad sorting, unseen data, different fabricators and various number of fabrications, where GENERATOR proves to be robust and powerful. Analysis of the ATTACKER also reveals that agents can be simultaneously optimized in an adversarial perspective.

## Limitations

As a multi-agent tuning technique that requires parameter update with back propagation (Rumelhart et al., 1986), our proposed iterative optimization requires relatively long training time. In the future, we plan to try more efforts to develop a training-
efficient online optimization method for GENERATOR which constitutes a robust RAG system.

## Ethics Statement

We use publicly accessible Wikipedia as knowledge base which contains knowledge from various subjects enables readers to benefit from the use of the it. Though we encourage ATM AtTACKER to fabricate misleading knowledge, it is exactly what we seek to solve in this work to mitigate the impact of retrieved fake LLM-generated content, which we believe to be particularly important in the $R A G-Q A$ scenario.

## Acknowledgements

This work was supported by the National Natural Science Foundation of China under grant No. KZ37117501, No. ZG216S23E8, No. 62306024, and No. 92367204.

## References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.

Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511.

Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268.

Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961.

Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. Advances in neural information processing systems, 13 .

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533-1544, Seattle, Washington, USA. Association for Computational Linguistics.
Martin Briesch, Dominik Sobania, and Franz Rothlauf. 2023. Large language models suffer from their own output: An analysis of the self-consuming training loop. arXiv preprint arXiv:2311.16822.

Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 2024. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335.

Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The power of noise: Redefining retrieval for rag systems. arXiv preprint arXiv:2401.14887.

Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359.

Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024. The faiss library.

Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise zero-shot dense retrieval without relevance labels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1762-1777, Toronto, Canada. Association for Computational Linguistics.

Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial networks. Communications of the ACM, 63(11):139-144.

Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation, 3(1):79-87.

Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C Park. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. arXiv preprint arXiv:2403.14403.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019 Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547.

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.

He Junqing, Pan Kunhao, Dong Xiaoqun, Song Zhuoyang, Liu Yibo, Liang Yuxin, Wang Hao, Sun Qianguo, Zhang Songxin, Xie Zejian, et al. 2023. Never lost in the middle: Improving large language models via attention strengthening question answering. arXiv preprint arXiv:2311.09198.

Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. 2019. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.

Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathematical statistics, 22(1):79-86.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611-626.

Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086-6096, Florence, Italy. Association for Computational Linguistics.
Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, and Andrew Yates. 2024. Meta-task prompting elicits embedding from large language models. arXiv preprint arXiv:2402.18458.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.

Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean, Alan Ritter, and Dan Jurafsky. 2017. Adversarial learning for neural dialogue generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2157-2169, Copenhagen, Denmark. Association for Computational Linguistics.

Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704.

Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint arXiv:2310.01352.

Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.

Fiona Macpherson and Dimitris Platchias. 2013. Hallucination: Philosophy and psychology. MIT Press.

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9802-9822, Toronto, Canada. Association for Computational Linguistics.

Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024. Generative representational instruction tuning. arXiv preprint arXiv:2402.09906.

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE

Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.

Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: $\mathrm{Bm} 25$ and beyond. Foundations and Trends $®$ in Information Retrieval, 3(4):333-389.

David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by backpropagating errors. nature, 323(6088):533-536.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. 2023a. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages $31210-31227$. PMLR.

Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023b. Replug: Retrievalaugmented black-box language models. arXiv preprint arXiv:2301.12652.

Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics, 11:1-17.

Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT good at search? investigating large language models as re-ranking agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14918-14937, Singapore. Association for Computational Linguistics.

Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2024. Principle-driven selfalignment of language models from scratch with minimal human supervision. Advances in Neural Information Processing Systems, 36.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong Wen. 2024a. Rear: A relevance-aware retrieval-augmented framework for open-domain question answering. arXiv preprint arXiv:2402.17497.

Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. 2024b. Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation. arXiv preprint arXiv:2403.05313.

Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8:229-256.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.

Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. 2024. How easily do irrelevant inputs skew the responses of large language models? arXiv preprint arXiv:2404.03302.

Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint arXiv:2310.01558.

Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. 2024. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131.
