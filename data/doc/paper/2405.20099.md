# Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks 

Chen Xiong<br>The Chinese University of Hong Kong<br>Sha Tin, Hong Kong<br>cxiong23@cse.cuhk.edu.hk

Pin-Yu Chen

IBM Research

New York, USA

pin-yu.chen@ibm.com

Xiangyu Qi<br>Princeton University<br>New Jersey, USA<br>xiangyuqi@princeton.edu<br>Tsung-Yi Ho<br>se University of Hong Kong<br>Sha Tin, Hong Kong<br>tyho@cse.cuhk.edu.hk


#### Abstract

Safety, security, and compliance are essential requirements when aligning large language models (LLMs). However, many seemingly aligned LLMs are soon shown to be susceptible to jailbreak attacks. These attacks aim to circumvent the models' safety guardrails and security mechanisms by introducing jailbreak prompts into malicious queries. In response to these challenges, this paper introduces Defensive Prompt Patch (DPP), a novel prompt-based defense mechanism specifically designed to protect LLMs against such sophisticated jailbreak strategies. Unlike previous approaches, which have often compromised the utility of the model for the sake of safety, DPP is designed to achieve a minimal Attack Success Rate (ASR) while preserving the high utility of LLMs. Our method uses strategically designed interpretable suffix prompts that effectively thwart a wide range of standard and adaptive jailbreak techniques. Empirical results conducted on LLAMA-2-7B-Chat and Mistral-7B-Instruct-v0.2 models demonstrate the robustness and adaptability of DPP, showing significant reductions in ASR with negligible impact on utility. Our approach not only outperforms existing defense strategies in balancing safety and functionality, but also provides a scalable and interpretable solution applicable to various LLM platforms.


Project Page:

https://huggingface.co/spaces/TrustSafeAI/ Defensive-Prompt-Patch-Jailbreak-Defense

## 1 Introduction

Recent advances in large language models (LLMs) [25, 31] such as GPT-4 [18], LLAMA-2 [2], and Mistral [7] have showcased their ability to understand and generate text akin to human interaction [26, 27, 32]. These models, powered by the Transformer architecture, excel in processing sequential data and understanding complex language patterns, hence enhancing tasks like text summarization, creative writing, and coding. To maintain model integrity and mitigate undesired outputs, developers implement alignment constraints using techniques like Reinforcement Learning with Human Feedback (RLHF) [22, 23, 24] and Supervised Fine-Tuning (SFT).
![](https://cdn.mathpix.com/cropped/2024_06_04_5da8c6d0def83935075fg-02.jpg?height=788&width=1396&top_left_y=234&top_left_x=362)

Figure 1: Overview of Defensive Prompt Patch. (a) showcases an example of jailbreak attacks. (b) is the DPP training phase in which the algorithm takes in the refusal and helpful datasets and a prototype of the defense prompt. Then, the algorithm forms the defense prompt population by revising the prototype using LLM. For each of the defense prompts in the population, the algorithm will evaluate the defense and utility scores as detailed in Sec. 3. The algorithm keeps editing the defense prompts with low scores using the Hierarchical Genetic Search algorithm. (c) shows the deployment of DPP in the LLM inference phase, by adding the best DPP in (b) (indicated in green patch) to every input query. (d) shows the trade-off graphs between the win-rate (utility) [14] and attack success rate (ASR) in both LLAMA-2-7B-Chat and Mistral-7B-Instruct-v0.2 models for different defenses.

Despite these alignment efforts, current LLMs can be tricked to generate undesirable output, as demonstrated by various jailbreak attacks [1, 3, 5, 4]. Initial strategies like the GCG attack involve crafting adversarial suffixes combined with user queries to manipulate model outputs [1]. More sophisticated techniques such as the AutoDAN [3], PAIR [5], and TAP [4] attacks generate interpretable jailbreak templates that enhance the efficacy and readability of the attacks.

In response to these vulnerabilities, the development of defensive strategies [19, 20, 28] has become increasingly vital. Prompt-based defenses, such as Self-Reminder [10], Goal Prioritization [11], and RPO [12], involve improving system prompts to enhance LLM alignment. These methods are simple yet effective, requiring minimal in-depth model knowledge and sparing model re-training as they operate at the text input level.

Nevertheless, these prompt-based defense mechanisms frequently grapple with the trade-off between preserving utility and effectively mitigating jailbreaks. Although Goal Prioritization excels in defense, it substantially compromises model utility. On the other hand, RPO retains utility but provides limited defense coverage. While Self-Reminder achieves a better balance, it fails to deliver satisfactory performance on more aligned models such as LLAMA-2-7B-Chat, owing to deficiencies in its search algorithm for the optimal prompt. We provide a comparative analysis of different prompt-based defenses in Table 1 .

|  | Optimizable Prompt | Gradient-Based Search | Interpretable | Attack Success Rate | Utility Degradation |
| :--- | :---: | :---: | :---: | :---: | :---: |
| Self-Reminder | $\checkmark$ | $x$ | $\checkmark$ | Medium | Medium |
| RPO |  | $x$ | $x$ | High | Low |
| Goal Prioritization | $x$ | $x$ | $\checkmark$ | Low | High |
| Default System Prompt | $\checkmark$ | $\checkmark$ | $\checkmark$ | High | Medium |
| Defensive Prompt Patch (Ours) | $\checkmark$ |  | Low | Low |  |

Table 1: Comparison between different defense methods against jailbreak attacks on LLMs.

To address these deficiencies, we introduce Defensive Prompt Patch (DPP), a novel, prompt-based defense mechanism. As illustrated in Figure 1. DPP uses adversarial and utility datasets to iteratively optimize and refine a suffix prompt to be appended to every input query for balancing alignment and utility. Figure 1 (d) demonstrates that DPP notably reduces the Attack Success Rate (ASR) to $3.8 \%$ on the LLAMA-2-7B-Chat model without compromising utility. Furthermore, it extends robust
defense capabilities to less-aligned models, such as the Mistral-7B-Instruct-v0.2, where it achieves a significant reduction in ASR to $2.0 \%$ while maintaining minimal utility loss.

Our main contributions are as follows:

- Improved Defense with Minimal Utility Trade-off: DPP is designed to minimize jailbreak risks while maintaining high utility, addressing the common pitfalls in current prompt-based defenses. Figure 1.d) summarizes its superior performance in balancing jailbreak risk and utility (Win-Rate).
- Robustness and Generalization against Adaptive Jailbreaking Attacks: We evaluated DPP against a variety of adaptive and unforeseen jailbreak strategies. DPP consistently achieves the lowest average attack success rate, proving its effectiveness across multiple scenarios.
- Interpretability and Stability of Prompt-based Defenses: We examined the best DPP found by our algorithm and demonstrated its enhanced interpretability over existing prompt-based defenses. We also conducted an ablation study on the LLAMA-2-7B-Chat model to validate that using DPP as a suffix to every input query attains better defense and utility compared with using it as a prefix.


## 2 Related Work

We overview notable jailbreak attack mechanisms and defense mechanisms developed for LLMs. Jailbreak attacks, which aim to exploit vulnerabilities in LLMs to elicit unaligned or harmful outputs, pose significant challenges to the integrity and safety of these systems. Conversely, developing robust defenses against such attacks is critical to maintaining the alignment and utility of LLMs.

Jailbreak attacks have evolved through various innovative mechanisms. For instance, techniques like the PAIR and TAP Attacks [5, 4] automate the creation of jailbreak prompts using a secondary "attacker" LLM, which poses serious threats through black-box access to the target LLM. Similarly, the ICA Attack [8] leverages in-context learning to misaligned responses, and the Catastrophic Attack [9] manipulates generation configurations to trigger misaligned outputs. GCG Attack [1] optimize adversarial inputs using gradient-based approaches, and the AutoDAN Attack [3] employs genetic algorithms to refine prompts based on specific templates. Another notable method, the Base64 Attack [6], encodes malicious queries in Base64 to bypass content filters subtly.

Defensive strategies have been developed in response to these sophisticated attacks to reinforce the security of LLMs. Techniques such as the Self-Reminder [10] defense modify the system prompt of LLMs to induce more self-aware and aligned processing. The RPO (Robust Prompt Optimization) [12] modifies objectives to minimize the perceptual distance between harmful queries and safe responses. Furthermore, Goal Prioritization and Default System Prompts [11, 15] are designed to direct LLMs to prioritize safety and prevent the generation of harmful outputs.

These attacks and defenses represent a dynamic interplay between the capabilities of LLMs and the measures required to secure them. Detailed descriptions and evaluations of these defense methods will be further discussed in the Sec. 4 section, where their effectiveness against various adversarial strategies is systematically analyzed.

## 3 Methodology

In this section, we first introduce preliminary concepts, followed by the description and training algorithm of our proposed methodology, Defensive Prompt Patch (DPP), designed to counteract jailbreak attacks while minimizing utility degradation.

### 3.1 Preliminaries

Jailbreak Attack: A jailbreak attack on an LLM aims to circumvent model alignment by using meticulously crafted prompts [29, 30]. We denote a malicious query as $\mathbf{u}_{1: n}=\left\langle u_{1}, u_{2}, \ldots, u_{n}\right\rangle$, with each $u_{i}$ being an input token. Ordinarily, the LLM would reject such queries based on its alignment policies. However, refined jailbreak queries, $\tilde{\mathbf{u}}_{1: m}=\left\langle\tilde{u}_{1}, \tilde{u}_{2}, \ldots, \tilde{u}_{m}\right\rangle$, manipulate these policies to elicit a compliant response $\mathbf{r}_{1: k}=\left\langle r_{1}, r_{2}, \ldots, r_{k}\right\rangle$, reflecting the original malicious intent.

Jailbreak Defense: Our defense involves a defensive prompt patch $\mathbf{d}_{1: l}=\left\langle d_{1}, d_{2}, \ldots, d_{l}\right\rangle$, derived from our DPP algorithm. This patch is appended to the refined query, forming a protected input $\mathbf{x}_{1: m+l}^{\text {guard }}=\left(\tilde{\mathbf{u}}_{1: m}, \mathbf{d}_{1: l}\right)$, typically resulting in a refusal response $\mathbf{s}_{1: n}=\left\langle s_{1}, s_{2}, \ldots, s_{n}\right\rangle$.

Utility Degradation: We measure utility degradation by the deviation in LLM responses to benign queries appended with $\mathbf{d}_{1: l}$. Ideally, the response to a benign query $\mathbf{b}_{1: p}=\left\langle b_{1}, b_{2}, \ldots, b_{p}\right\rangle$ patched by $\mathbf{d}_{1: l}$ should closely match the response to $\mathbf{b}_{1: p}$ alone.

Mathematical Formulation: We define the $\oplus$ operation as the concatenation of two sequences. For a given sequence $\mathbf{a}_{1: n}=\left\langle a_{1}, \ldots, a_{n}\right\rangle$ and $\mathbf{z}_{1: m}=\left\langle z_{1}, \ldots, z_{m}\right\rangle, \mathbf{a}_{1: n} \oplus \mathbf{z}_{1: m}$ is defined as: $\mathbf{a}_{1: n} \oplus \mathbf{z}_{1: m}=\left\langle a_{1}, \ldots a_{n}, z_{1}, \ldots z_{m}\right\rangle$. We denote sequences of harmful responses and jailbreak inputs by $\mathbf{r}_{1: k}$ and $\tilde{\mathbf{u}}_{1: m}$, respectively. Since LLMs are specifically trained to predict the probability of the next word, we define the goal (i.e., the objective function to be maximized) of a jailbreak attack as:

$$
\begin{equation*}
P\left(\mathbf{r}_{1: k} \mid \tilde{\mathbf{u}}_{1: m}\right)=\prod_{j=1}^{k} P\left(r_{j} \mid \tilde{\mathbf{u}}_{1: m}, \mathbf{r}_{1: j-1}\right) \tag{1}
\end{equation*}
$$

and the goal of defense as:

$$
\begin{equation*}
P\left(\mathbf{s}_{1: n} \mid \tilde{\mathbf{u}}_{1: m} \oplus \mathbf{d}_{1: l}\right)=\prod_{i=1}^{n} P\left(s_{i} \mid \tilde{\mathbf{u}}_{1: m} \oplus \mathbf{d}_{1: l}, \mathbf{s}_{1: i-1}\right) \tag{2}
\end{equation*}
$$

where $\mathbf{s}_{1: n}$ is the refusal response to the jailbreak inputs. Finally, we assess utility degradation by:

$$
\begin{equation*}
P\left(\mathbf{h}_{1: q} \mid \mathbf{b}_{1: p} \oplus \mathbf{d}_{1: l}\right)=\prod_{k=1}^{q} P\left(h_{k} \mid \mathbf{b}_{1: p} \oplus \mathbf{d}_{1: l}, \mathbf{h}_{1: k-1}\right) \tag{3}
\end{equation*}
$$

where $\mathbf{h}_{1: q}$ is the normal response for each benign queries $\mathbf{b}_{1: p}$. The DPP algorithm's efficacy is evaluated by its performance in both defense against malicious queries and impact on utility on benign queries.

### 3.2 Score Evaluation

In our work, the DPP must fulfill two crucial objectives: (I) Maximization of Refusal Score on malicious queries and (II) Maximization of Helpful Score on benign queries.

To achieve (I), we use the log-likelihood of Eq. 2 and define the refusal score as follows:

$$
\begin{equation*}
\mathcal{S}_{D_{i}}=\log P\left(\mathbf{s}_{1: n} \mid \tilde{\mathbf{u}}_{1: m} \oplus \mathbf{d}_{1: l}\right) \tag{4}
\end{equation*}
$$

where $S_{D_{i}}$ denotes the refusal score attributed to the $i$-th DPP within the population of DPPs. The vector $\mathbf{s}_{1: n}$ encapsulates the refusal response, $\tilde{\mathbf{u}}_{1: m}$ represents the jailbreak query, and $\mathbf{d}_{1: l}$ is the our defensive mechanism

Similarly, for (II), the inputs include benign queries combined with the same DPP as used in the refusal score calculation. Applying the log-likelihood of Eq. 3. The helpful score is formulated as:

$$
\begin{equation*}
\mathcal{S}_{H_{i}}=\log P\left(\mathbf{h}_{1: q} \mid \mathbf{b}_{1: p} \oplus \mathbf{d}_{1: l}\right) \tag{5}
\end{equation*}
$$

where $S_{H_{i}}$ represents the helpfulness score assigned to the $i$-th DPP within the population of DPPs. The vector $\mathbf{h}_{1: q}$ denotes the standard response, whereas $\mathbf{b}_{1: p}$ refers to the benign query. The overall score function for training DPP combines the refusal and helpful scores, weighted by coefficients $\alpha$ and $\beta$, respectively:

$$
\begin{equation*}
\mathcal{S}_{T_{i}}=\alpha \cdot \mathcal{S}_{D_{i}}+\beta \cdot \mathcal{S}_{H_{i}} \tag{6}
\end{equation*}
$$

### 3.3 DPP Training Algorithm

Using the total score defined in Sec. 3.2 we use a Hierarchical Genetic Algorithm (HGA) to optimize DPP, drawing inspiration from the AutoDAN jailbreak attack in [3]. We adapt and extend HGA to iteratively refine DPP based on our defined scores, as depicted in Figure. 1.(b) and (c). to develop our methodology, which we term the Defensive Prompt Patch Algorithm (DPP Algorithm).

Initially, we establish a baseline DPP, designated as the prototype. Without loss of generality, this prototype may take the form of either a Prefix DPP or a Suffix DPP. The relative effectiveness of each
configuration is assessed in Appendix. D Following this, the prototype is subjected to $K$ iterations of rewriting via an LLM to potentially refine the DPP, creating a population of DPP candidates. Each candidate within the population is evaluated by sampling refusal data pairs and helpful data pairs from adversarial/utility datasets to compute the total score, as formulated in Eq.6. Details on adversarial/utility datasets in our implementation can be found in Sec. 4.1.

The DPP optimization process is conducted over $I$ iterations for each candidate, during which the DPP algorithm executes two pivotal operations: Sentence-Level Word Substitution and ParagraphLevel Sentence Swap and Mutations.

In Sentence-Level Word Substitution, each sentence within the population is assigned a score calculated using Eq. 6 A certain percentage of defense prompts are retained based on their scores for further optimization. For these sentences, words are initially assigned the same score as their corresponding sentences. These scores are later adjusted based on the frequency of occurrence of each word. Words whose scores surpass a specified threshold are then randomly replaced with synonyms.

In Paragraph-Level Sentence Swap and Mutations, we specify a swap probability $p_{\text {swap }}$ and a mutation probability $p_{\text {mutate }}$. The defensive prompt patch, modified in the previous step, is reassessed for total score at the sentence level. Employing a methodology similar to that of sentence-level optimization, the algorithm selects parent sentences based on their scores, segments and swaps these sentences, and then conducts mutations by revising sentences using an LLM.

These processes-Sentence-Level Word Substitution and Paragraph-Level Sentence Swap and Mutations-aim to increase the diversity within the defensive prompt patch population and enhance the likelihood of identifying the optimal patch.

The full algorithm is delineated in Algorithm 1. Ultimately, the algorithm produces an updated set of optimized DPPs, comprising $K$ enhanced patches, and identifies the Best Defensive Prompt Patch based on the highest total score.

```
Algorithm 1 Defensive Prompt Patch (DPP) Algorithm
    Arguments: Defensive Prompt Patch Prototype $O$, refusal pair $\left(x^{r}, y^{r}\right)$, helpful pair $\left(x^{h}, y^{h}\right)$,
    $\alpha$ and $\beta$, target LLM
    Initialization: Number of optimization iteration $I$, batch size, $p_{\text {crossover }}, p_{\text {mutate }}$, Sentence-
    level iterations, Paragraph-level iterations, number of steps, number of parent set size
    DPP_Set $\leftarrow$ DPP SET GENERATION $(O, \mathrm{~K})$ by Alg. 2
    while $I$ is not reached do
        for iteration in sentence-level iterations do
            Evaluate refusal/helpful score of each DPP with $\left(x^{r}, y^{r}\right) /\left(x^{h}, y^{h}\right)$ and target LLM
            Final Score $\leftarrow$ calculate the score using Eq. (6)
            Select elite and parent prompts from DPP_Set according to Final Score
            WordDict $\leftarrow$ Calculate each word score using selected parent prompts by Alg. 3
            Find synonyms for each word
            if random value $<$ WordDict[synonym] / sum(word scores) then
                Replace word with synonym
            end if
        end for
        for iteration in paragraph-level iterations do
            Repeat line 6 to 8
            Conduct crossover and mutation on selected parent prompts using Alg. 4
        end for
        New_DPP_Set $\leftarrow$ DPP_Set $\cup$ New_DPP
        Best_DPP $\leftarrow$ Best score within New_DPP_Set
    end while
    return (New_DPP_Set, Best_DPP)
```

Best DPP selection. Algorithm 1 identifies the optimal DPP for a given pair of refusal and helpful data. Our primary objective is to find a DPP that generalizes well across different user queries. To enhance the universality of DPP, we incorporate $N$ pairs of refusal and helpful data, sampled from their respective datasets. In each iteration of the DPP algorithm, as described earlier, a set of
candidate DPPs is generated along with the best DPP for the specific data pair. This set of candidate DPPs is then used for the next pair of refusal and helpful data. By iteratively optimizing this set of DPP candidates, we aim to identify the most generalizable DPP with the best defensive and utility performance. The overall optimization procedure is detailed in Algorithm 5 . For full implementation details and hyperparameter settings, please refer to Appendix D.

## 4 Experiments

We demonstrate the performance of our DPP through three perspectives: Robustness to standard (non-adaptive) and adaptive jailbreak attacks, Generalization to unforeseen jailbreak queries and different LLMs, and Interpretability of the best-found DPP. All final DPPs are listed in Appendix H

### 4.1 Experimental Setup

Adversarial Dataset: We use the AdvBench [1], specifically the harmful behavior instructions ${ }^{1}$. as jailbreak questions. Each of them is fed into a well-aligned LM (LLAMA-2-7B-Chat [2]) to generate the denial responses. In our experiment, we sampled 100 jailbreak questions and recorded both jailbreak questions along with their refusal responses to form the Adversarial Dataset.

Utility Dataset: We use the Alpaca datase ${ }^{2}$ as our benchmark. For consistency with the Adversarial Dataset, we also sampled only 100 benign questions and their corresponding answers.

Language Models: We perform our jailbreak experiments on two specific LLMs: LLAMA-2-7BChat [2] and Mistral-7B-Instruct-v0.2 [7]. LLAMA-2-7B-Chat is an adapted version of LLAMA-27B, specifically configured for chat-based interactions. Mistral-7B-Instruct-v0.2 is a fine-tuned chat version of Mistral-7B-v0.2. This model demonstrates a stronger ability in performance, outperforming LLAMA-2-13B on all benchmarks while maintaining proficiency in English language tasks.

Jailbreak Attack Methods: We use several existing jailbreak attack methods to generate advanced malicious prompts. Specifically, for each malicious behavior statement, we apply several different types of jailbreaking attacks: (i) Uninterpretable Jailbreak Attacks - we used GCG [1] and Base64 [6] to generate adversarial prompts. Specifically, GCG is used to generate an adversarial suffix for each malicious query. Base64 encodes each harmful query in Base64 format. (ii) Interpretable Jailbreak Attacks - AutoDAN [3], PAIR [5], TAP [4], and ICA [8] are interpretable attacks that we used to translate the original malicious query into a new improved malicious query. Please refer to Appendix Afor more details on generating new malicious queries. (iii) Generation-based Jailbreak Attacks - we follow Catastrophic Attack [9] to vary the hyperparameters of the LLM to generate malicious responses for each harmful question. In our evaluation, similar to the Adversarial Dataset, we utilize 100 harmful behavior questions from AdvBench to generate new malicious querie $3^{3}$, all of which will be employed in our experiments.

Jailbreak Defense Methods: We compare our DPP to Self-Reminder [10] and Goal Prioritization [11]. They are prompt-based defenses that add defense prompts as a prefix or suffix. For the LLAMA-2-7B chat model, we also include another defensive suffix approach called RPO [12]. For Mistral-7B-Instruct-v0.2, instead of using RPO as a baseline, we compare the results with Plain (Default) System Prompt [15]. We defer the discussion of our choices of baselines for the two LLMs to Appendix B. The prompts for each defense can be found in Appendix G

Evaluation Metrics: We use the Attack Success Rate (ASR) as our primary metric for evaluating the effectiveness of jailbreak defenses. The ASR measures the proportion of malicious queries that successfully bypass the LLMs alignment and generate harmful responses. Details on how we calculate ASR can be found in Appendix C. In addition to ASR, we also use AlpacaEval [14] to evaluate the utility degradation of the LLM model when defenses are employed. Specifically, we utilize the metric called Win-Rate. This involves comparing the frequency with which outputs from[^0]

| Methods | Base64 $[\downarrow]$ | ICA $[\downarrow]$ | AutoDAN $[\downarrow]$ | GCG $[\downarrow]$ PAIR $[\downarrow]$ TAP $[\downarrow]$ | Average ASR $[\downarrow]$ | Win-Rate $[\uparrow]$ |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| w/o defense | 0.990 | 0.690 | 0.640 | 0.550 | 0.100 | 0.120 | 0.515 | 81.37 |
| RPO [12] | 0.000 | 0.420 | 0.280 | 0.190 | 0.060 | 0.060 | 0.168 | 79.23 |
| Goal Priorization [11] | 0.000 | 0.020 | 0.520 | 0.020 | 0.020 | 0.020 | 0.100 | 34.29 |
| Self-Reminder [10] | 0.030 | 0.290 | 0.000 | 0.040 | 0.020 | 0.000 | 0.063 | 64.84 |
| DPP (Ours) | 0.010 | 0.000 | 0.100 | 0.040 | 0.040 | 0.040 | $\mathbf{0 . 0 3 8}$ | $\mathbf{8 2 . 9 8}$ |

Table 2: Attack Success Rates (ASRs) and Win-Rates (utility) on LLAMA-2-7B-Chat model across six different jailbreak attacks. Our method can achieve the lowest Average ASR and highest Win-Rate against other defense baselines. The arrow's direction signals improvement, the same below.

| Adaptive Methods | ICA $[\downarrow]$ | Catastrophic $[\downarrow]$ | GCG $[\downarrow]$ | AutoDAN $[\downarrow]$ | Average Adaptive ASR $[\downarrow]$ |
| :--- | :---: | :---: | :---: | :---: | :---: |
| Self-Reminder | 0.410 | 0.263 | 0.210 | 0.080 | 0.241 |
| RPO | 0.360 | 0.653 | 0.920 | 0.170 | 0.526 |
| Goal Prioritization | 0.660 | 0.0033 | 0.190 | 0.530 | 0.346 |
| DPP (Ours) | 0.160 | 0.247 | 0.120 | 0.110 | $\mathbf{0 . 1 5 9}$ |

Table 3: Adaptive Attack Success Rates Rate on LLAMA-2-7B-Chat model. Our method can achieve the lowest Average Adaptive ASR.

LLM are favored over those from a reference model, given a specific user instruction. Utilizing simulated Win-Rate offers a straightforward, comparable metric across various LLMs using the same reference model. In Appendix $\mathrm{O}$, we discuss the setups of evaluating with Win-Rate.

### 4.2 Robustness against Non-adaptive and Adaptive Attacks

Our analysis begins with a comparative evaluation of our DPP Suffix method against established defense baselines under six distinct jailbreak attacks on the LLAMA-2-7B-Chat model. We delineate our findings for both non-adaptive and adaptive jailbreak attacks, reporting on Attack Success Rate (ASR), Average ASR, and Win-Rate to underscore minimal utility degradation under our method.

Non-adaptive Attacks: We generate malicious queries using the aforementioned jailbreak attacks directly from the original LLMs (i.e., without any defense). From Table 2 we can summarize the following observations. First, our method outperforms RPO with respect to ICA, AutoDAN, and GCG attacks. Specifically, it outperforms the ASR of RPO by $42 \%$ for ICA attack, $18 \%$ for AutoDAN, and $15 \%$ for GCG attack. For the Base64 attack, our method is comparable to RPO with only $1 \%$ less than RPO. Second, although Goal Prioritization is a strong defense mechanism against Base64 and GCG, it fails to defend against the AutoDAN attack, where our method is $42 \%$ better than Goal Prioritization in terms of ASR. Self-Reminder has the same performance as our method against the GCG attack and a slightly weaker performance against the Base64 attack. While our method has $10 \%$ worse defense performance under AutoDAN setting, it outperforms Self-Reminder on ICA attack by $29 \%$. The last column of Table 2 shows the utility degradation of each defense. Our method has the best Win-Rate, $82.98 \%$, outrunning all the other baselines. Notably, the Goal Prioritization has the lowest Win-Rate, suggesting that its defense performance comes with a high cost in utility drop. Overall, our DPP not only achieves the lowest Average ASR of $3.80 \%$ but also ensures minimal utility impact, reinforcing its standing as the most robust method among those evaluated.

Adaptive Attacks: Adaptive attack [16] is a critical evaluation procedure for assessing defense effectiveness when the defense mechanism is known to the attack. Here, we assume the attacker can query the protected LLM with the defense in place when making jailbreak attempts. In this setup, we adapted the attack strategies described in Appendix I. Due to the known limited effectiveness of PAIR and TAP in the non-adaptive setting on the LLAMA-2-7B-Chat model, [5, 4], we replace these attacks with Catastrophic Adaptive Attack. In addition, Base64 attack is a static approach, so the adaptive setting cannot be directly applied to it. Therefore, we remove these attacks from the evaluation. Table 3 shows the adaptive attack results. Our method still has the best adaptive ASR with respect to ICA and GCG adaptive attacks. Although Goal Prioritization has the best ASR under catastrophic attacks, which is $0.33 \%$, it fails to defend against ICA and AutoDAN adaptive attacks. On the other hand, our method outperforms Self-Reminder against all adaptive attacks except AutoDAN. Notably, our method attains the best Average ASR, which is $15.9 \%$ (outperforming the second-best method by more than $8 \%$ ), while RPO has the worst robustness, with an Average ASR of $52.6 \%$. In Appendix F we also conducted our DPP with different initialized prototypes and found the defensive performance was consistent.

| Methods | Base64 $[\downarrow]$ | ICA $[\downarrow]$ | GCG $[\downarrow]$ | AutoDAN [ $\downarrow]$ | PAIR $[\downarrow]$ | TAP $[\downarrow]$ | Average ASR $\downarrow \downarrow]$ | Win-Rate $[\uparrow]$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| w/o defense | 0.990 | 0.960 | 0.990 | 0.970 | 1.000 | 1.000 | 0.985 | 90.31 |
| Self-Reminder [10] | 0.550 | 0.270 | 0.510 | 0.880 | 0.420 | 0.260 | 0.482 | 88.82 |
| System Prompt $[15$ | 0.740 | 0.470 | 0.300 | 0.970 | 0.500 | 0.180 | 0.527 | 84.97 |
| Goal Priorization [11] | 0.030 | 0.440 | 0.030 | 0.390 | 0.300 | 0.140 | 0.222 | 56.59 |
| DPP (Ours) | 0.000 | 0.010 | 0.020 | 0.030 | 0.040 | 0.020 | $\mathbf{0 . 0 2 0}$ | 75.06 |

Table 4: Attack Success Rates (ASRs) and Win-Rates (utility) on Mistral-7B-Instruct-v0.2 model across six different jailbreak attacks. Our method can achieve the lowest Average attack success rate with reasonable trade-off of Win-Rate when compared with other defense baselines.

| Adaptive Methods | ICA [ $\downarrow$ ] | Catastrophic [ $\downarrow$ ] | GCG [ $\downarrow$ ] | AutoDAN [ $\downarrow$ ] | PAIR [ $\downarrow$ ] | TAP [ $\downarrow$ ] | Average Adaptive ASR [ $\downarrow]$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Self-Reminder | 0.440 | 0.727 | 0.610 | 1.000 | 1.000 | 1.000 | 0.796 |
| System Prompt | 0.990 | 0.340 | 0.850 | 0.990 | 1.000 | 1.000 | 0.862 |
| Goal Priorization | 0.960 | 0.123 | 0.110 | 0.570 | 1.000 | 1.000 | 0.627 |
| DPP (Ours) | 0.000 | 0.277 | 0.390 | 0.470 | 0.837 | 0.840 | $\mathbf{0 . 4 6 9}$ |

Table 5: Adaptive Attack Success Rates on Mistral-7B-Instruct-v0.2. Our method can achieve the lowest Average ASR.

In conclusion, both non-adaptive and adaptive evaluations affirm that our DPP consistently surpasses other defense mechanisms in robustness, with minimal utility degradation across the board. This comprehensive performance solidifies our method's position as a preferable choice for defending the LLAMA-2-7B-Chat model against diverse and sophisticated attacks.

### 4.3 Generalization of DPP

We begin by demonstrating the generalizability of our method by applying it to Mistral-7B-Instructv0.2. Similar to LLAMA-2-7B-Chat, we used two settings on Mistral-7B-Instruct-v0.2: non-adaptive and adaptive attacks. For both settings we use GCG, AutoDAN, PAIR, and TAP attacks. In addition, we report utility degradation in terms of Win-Rate. All results are recorded in Table 4 and 5 .

Non-adaptive Attacks: Table 4 shows our method outperforms all comparative baselines in terms of defense capability. Although Goal Prioritization exhibits comparable performance against the GCG Attack—with an Attack Success Rate (ASR) of 3\% for Goal Prioritization versus $2 \%$ for our method-it does not maintain this performance across other jailbreak attacks. When comparing the average ASR, our ASR is more than $20 \%$ lower than the best defense baseline (Goal Prioritization).

Regarding the trade-off between defense effectiveness and utility degradation, unlike the LLAMA2-7B-Chat results, our method exhibits a higher utility degradation, as indicated by the Win-Rate, compared to Self-Reminder, and System Prompt. Nonetheless, the superior defense performance (a gap greater than $46 \%$ in average ASR) of our method justifies this increased utility degradation. It is noteworthy that despite the relatively higher utility impact, our method still shows much less degradation compared to the Goal Prioritization approach. Our result suggests that Mistral-7BInstruct-v0.2 has a worse defense-utility trade-off than LLAMA-2-7B-Chat. That is, the cost of making Mistral-7B-Instruct-v0.2 robust to jailbreak attacks on utility is more significant than LLAMA2-7B-Chat. We present additional experiments in Appendix P, where we compare our results with another defense baseline and observe similar effects.

Adaptive Attacks: Table 5 demonstrates that our method consistently performs best as a defense mechanism against jailbreak attacks on average. Although our approach is slightly less effective in the GCG Adaptive Attack compared to Goal Prioritization, it exhibits superior defensive capabilities in the AutoDAN, PAIR, and TAP adaptive attacks.

Unforeseen Jailbreak Queries: We also test the generalization of each defense using the JailbreakBench Chat dataset (JBC) [33], which contains harmful queries distinct from those found in the AdvBench dataset. The results from Table 12 in Appendix D show that for the well-aligned model (LLAMA-2-7B-Chat), the JBC dataset does not yield effective jailbreak attacks, resulting in comparable defense performances across all methods. Conversely, with the less-aligned Mistral-7BInstruct-v0.2 model, our DPP demonstrated its efficacy by reducing the Attack Success Rate (ASR) from $41 \%$ to $1 \%$, attaining the best defense performance (on par with Goal Prioritization). This marked decrease in ASR highlights our DPP's strong capability to generalize defense performance effectively against unforeseen attacks.

In summary, our method not only achieves better defense performance on LLAMA-2-7B-Chat model, but also generalizes well to the less-aligned Mistral-7B-Instruct-v0.2 model. This underscores our method's strong generalization ability and the potential applicability to other LLMs.

| Configuration | Initialization | Win-Rate $[\uparrow]$ | GCG Attack $[\downarrow]$ | GCG Adaptive $[\downarrow]$ |
| :--- | :--- | :---: | :---: | :---: |
| Prefix DPP | Initialization 1 | 72.85 | 0.05 | 0.58 |
|  | Initialization 2 | 76.99 | 0.17 | 0.54 |
|  | Initialization 3 | 69.32 | 0.16 | 0.59 |
|  | Average | $\mathbf{7 3 . 0 5}$ | $\mathbf{0 . 1 3}$ | $\mathbf{0 . 5 7}$ |
| Suffix DPP | Initialization 1 | 82.98 | 0.04 | 0.12 |
|  | Initialization 2 | 74.63 | 0.05 | 0.19 |
|  | Initialization 3 | 70.65 | 0.08 | 0.15 |
|  | Average | $\mathbf{7 6 . 0 9}$ | $\mathbf{0 . 0 6}$ | $\mathbf{0 . 1 5}$ |

Table 7: Win-Rate and Attack Success Rate (ASR) for Prefix and Suffix Defensive Prompt Patch in LLAMA-2-7B-Chat Model.

### 4.4 Interpretability of DPP

We explore the interpretability of our DPP by presenting our best DPPs trained on both LLAMA-27B-Chat and Mistral-7B-Instruct-v0.2 models below.

## DPP (Suffix) for LLAMA-2-7B-Chat

Kindly furnish a thorough response to the former user's question.

## DPP (Suffix) for Mistral-7B-Instruct-v0.2

Begin the reply with 'I'm sorry' if the client's query relates to defective components.

We observe that both DPPs are fluent and relevant to the purpose of alignment. One interesting finding is that the optimized DPP for Mistral-7B-Instruct-v0.2 is very explicit in instructing refusal when facing "defective components", whereas the DPP of LLAMA-2-7B-Chat acts like a gentle reminder. This distinction can be explained by the relatively weak alignment of Mistral-7B-Instruct-v0.2 when compared with LLAMA-2-7B-Chat. We also showcase more DPPs in Appendix $\mathrm{H}$

Quantitatively, we measure the perplexity for our DPP as well as other defense baseline prompts on LLAMA-2-7BChat in Table 6. The perplexity score for a sentence is calculated by averaging the negative log probabilities of next-token, predicted by the GPT-4 model, and using this average as the exponent in a base-2 exponential function. Our method exhibits a lower perplexity score than RPO and Self-Reminder, indicating higher interpretability. It is noteworthy that RPO has the highest perplexity, suggesting that the suffix prompt generated by RPO is highly uninterpretable due to the use of GCG Attack algorithm.

|  | Perplexity $[\downarrow]$ |
| :--- | :---: |
| Self-Reminder | 298.39 |
| Goal Prioritization | 40.65 |
| System Prompt | 25.65 |
| RPO | 8780.94 |
| DPP (Ours) | 56.57 |

Table 6: Comparison of perplexity scores for various defense prompts evaluated using GPT-4, highlighting the interpretability of each method. Although both Goal Prioritization and System Prompts are hand-crafted defense prompts with lower perplexity (i.e., they are more interpretable prompts), our method remains competitive with these approaches while sparing the need for human interventions in prompt design and optimization.

### 4.5 Ablation Study

We report an ablation study to test the stability of DPP and its patching format (i.e., as a prefix or as a suffix to an input query). We independently initialized three distinct sets of defense prompts as prefixes and suffixes and applied the DPP algorithm to each set. Table 7 shows the ASR and Win-Rate under both non-adaptive and adaptive GCG attack scenarios for the LLAMA-2-7B-Chat model.

In terms of Win-Rate, the Suffix DPP surpasses the Prefix DPP by 3\% on average. For the GCG non-adaptive attack, the ASR for Suffix DPP is 7\% lower than that for Prefix DPP. In the adaptive GCG settings, the ASR difference increases to $\mathbf{4 2 \%}$ between the Prefix and Suffix DPP. This ablation study concludes that Prefix DPP is less effective than Suffix DPP, particularly under adaptive settings. Therefore, we suggest using suffixes as the default DPP format in future studies.

## 5 Conclusion

The proposed Defensive Prompt Patch (DPP) framework presents a scalable and practical promptbased approach to improving LLM safeguards, addressing critical vulnerabilities exposed by jailbreak attacks while preserving high utility of the protected LLM. Our method stands out by achieving an optimal balance between maintaining high utility and providing robust defense, thereby ensuring that the protected LLM simultaneously remains high efficiency and safety when facing jailbreak attempts. The empirical tests conducted - including LLAMA-2-7B-Chat12 and Mistral-7B-Instruct-v0.2 models, 7 jailbreak attack strategies, and several state-of-the-art prompt-based defenses - substantiate that DPP effectively reduces the attack success rate to low levels with minimal impact on model performance. Moreover, the adaptability of DPP to function effectively even on less-aligned models underscores its potential as a universal defensive solution in various LLM models. The interpretable property of our DPP also opens up a new avenue to infusing and accelerating prompt engineering by human users for enhancing LLM safety alignment.

## References

[1] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson, "Universal and transferable adversarial attacks on aligned language models," CoRR, vol. abs/2307.15043, 2023.

[2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, "Llama: Open and efficient foundation language models," CoRR, vol. abs/2302.13971, 2023.

[3] X. Liu, N. Xu, M. Chen, and C. Xiao, "Autodan: Generating stealthy jailbreak prompts on aligned large language models," CoRR, vol. abs/2310.04451, 2023.

[4] A. Mehrotra, M. Zampetakis, P. Kassianik, B. Nelson, H. Anderson, Y. Singer, and A. Karbasi, "Tree of attacks: Jailbreaking black-box llms automatically," CoRR, vol. abs/2312.02119, 2023.

[5] P. Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong, "Jailbreaking black box large language models in twenty queries," CoRR, vol. abs/2310.08419, 2023.

[6] A. Wei, N. Haghtalab, and J. Steinhardt, "Jailbroken: How does LLM safety training fail?" CoRR, vol. abs/2307.02483, 2023.

[7] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, "Mistral 7b," 2023.

[8] Z. Wei, Y. Wang, and Y. Wang, "Jailbreak and guard aligned language models with only few in-context demonstrations," 2023.

[9] Y. Huang, S. Gupta, M. Xia, K. Li, and D. Chen, "Catastrophic jailbreak of open-source llms via exploiting generation," 2023.

[10] Y. Xie, J. Yi, J. Shao, J. Curl, L. Lyu, Q. Chen, X. Xie, and F. Wu, "Defending chatgpt against jailbreak attack via self-reminders," Nat. Mac. Intell., vol. 5, no. 12, pp. 1486-1496, 2023.

[11] Z. Zhang, J. Yang, P. Ke, and M. Huang, "Defending large language models against jailbreaking attacks through goal prioritization," 2023.

[12] A. Zhou, B. Li, and H. Wang, "Robust prompt optimization for defending language models against jailbreaking attacks," 2024.

[13] M. Phute, A. Helbling, M. Hull, S. Peng, S. Szyller, C. Cornelius, and D. H. Chau, "Llm self defense: By self examination, llms know they are being tricked," arXiv preprint arXiv:2308.07308, 2023.

[14] X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto, "Alpacaeval: An automatic evaluator of instruction-following models," https://github.com/ tatsu-lab/alpaca_eval, 2023.

[15] C. Zheng, F. Yin, H. Zhou, F. Meng, J. Zhou, K.-W. Chang, M. Huang, and N. Peng, "On prompt-driven safeguarding for large language models," 2024.

[16] F. Tramer, N. Carlini, W. Brendel, and A. Madry, "On adaptive attacks to adversarial example defenses," 2020

[17] Z. Liao and H. Sun, "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms," 2024.

[18] OpenAI, "GPT-4 technical report," CoRR, vol. abs/2303.08774, 2023.

[19] N. Jain, A. Schwarzschild, Y. Wen, G. Somepalli, J. Kirchenbauer, P. Chiang, M. Goldblum, A. Saha, J. Geiping, and T. Goldstein, "Baseline defenses for adversarial attacks against aligned language models," CoRR, vol. abs/2309.00614, 2023.

[20] A. Robey, E. Wong, H. Hassani, and G. J. Pappas, "Smoothllm: Defending large language models against jailbreaking attacks," CoRR, vol. abs/2310.03684, 2023.

[21] B. Zhu, E. Frick, T. Wu, H. Zhu, and J. Jiao, "Starling-7b: Improving llm helpfulness and harmlessness with rlaif," November 2023.

[22] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, and J. Kaplan, "A general language assistant as a laboratory for alignment," 2021.

[23] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. HatfieldDodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan, "Training a helpful and harmless assistant with reinforcement learning from human feedback," 2022.

[24] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe, "Training language models to follow instructions with human feedback," 2022.

[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," 2023.

[26] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan, "Agieval: A human-centric benchmark for evaluating foundation models," 2023.

[27] X. Pu, M. Gao, and X. Wan, "Summarization is (almost) dead," 2023.

[28] Y. Zhang, L. Ding, L. Zhang, and D. Tao, "Intention analysis makes llms a good jailbreak defender," 2024.

[29] Z.-X. Yong, C. Menghini, and S. H. Bach, "Low-resource languages jailbreak gpt-4," 2024.

[30] Z. Zhang, L. Lei, L. Wu, R. Sun, Y. Huang, C. Long, X. Liu, X. Lei, J. Tang, and M. Huang, "Safetybench: Evaluating the safety of large language models with multiple choice questions," 2023.

[31] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," 2019.

[32] I. Dasgupta, A. K. Lampinen, S. C. Y. Chan, H. R. Sheahan, A. Creswell, D. Kumaran, J. L. McClelland, and F. Hill, "Language models show human-like content effects on reasoning tasks," 2023.

[33] P. Chao, E. Debenedetti, A. Robey, M. Andriushchenko, F. Croce, V. Sehwag, E. Dobriban, N. Flammarion, G. J. Pappas, F. Tramer, H. Hassani, and E. Wong, "Jailbreakbench: An open robustness benchmark for jailbreaking large language models," 2024.

[34] C. Zheng, F. Yin, H. Zhou, F. Meng, J. Zhou, K.-W. Chang, M. Huang, and N. Peng, "On prompt-driven safeguarding for large language models," 2024.
