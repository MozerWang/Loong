# A Triumvirate of AI Driven Theoretical Discovery 

Yang-Hui $\mathrm{He}^{*}$<br>${ }^{1}$ London Institute for Mathematical Sciences, Royal Institution, London W1S 4BS, UK<br>${ }^{2}$ Merton College, University of Oxford, OX14JD, UK<br>*hey@maths.ox.ac.uk

May 31, 2024


#### Abstract

Recent years have seen the dramatic rise of the usage of AI algorithms in pure mathematics and fundamental sciences such as theoretical physics. This is perhaps counter-intuitive since mathematical sciences require the rigorous definitions, derivations, and proofs, in contrast to the experimental sciences which rely on the modelling of data with error-bars. In this Perspective, we categorize the approaches to mathematical discovery as "top-down", "bottom-up" and "meta-mathematics", as inspired by historical examples. We review some of the progress over the last few years, comparing and contrasting both the advances and the short-comings in each approach. We argue that while the theorist is in no way in danger of being replaced by AI in the near future, the hybrid of human expertise and AI algorithms will become an integral part of theoretical discovery.


## Key points:

- Bottom-up mathematics is building statements from axioms and definitions.
- Meta-mathematics is to treat all statements and their derivations as a language.
- Top-down mathematics is theoretical research guided by intuition, experience and pure data.
- Special consideration should be given to pure, theoretical, noiseless data, which can lead to profound conjectures.
- AI has made significant advances in all these approaches of mathematical and theoretical research, but the human expert will not be replaced any time soon.
- Humans and AI will work in tandem for theoretical discovery.


## 1 Introduction

In a recent conversation, a friend who is an eye doctor recalled that when he was a trainee ophthalmologist, his mentor was a world-leading specialist who could recognize a diseased eye at a single glance from a wall of images. Yet, when pressed, his mentor often could not give more precise an answer than dismissing it as gut-instinct. This, we concluded, is the fundamental reason why Ariticial Intelligence (AI) is taking over so many disciplines of human endeavour.

For practical purposes, the vast majority of human activity falls under the adage of "if it ain't broke, don't fix it". A medic's primary goal is to cure the patient; understanding the mechanisms of the disease is secondary. ChatGTP can perform as well as an average undergraduate student in an exam not because a large language model (LLM) can have true comprehension but because for all but the very top students
in the world the goal is to pass the test, not to understand the material. Even (or perhaps especially) in the creative arts, a good definition of genius would be the mastery of style (via supervised learning of preceding samples) and the deviation therefrom in an inexplicable way. Deep neural networks can mimic so much of human activities because they more often than not simply requires black-boxes: learning from trial-and-error, and performing to within a margin of error.

This can never be so for the scientist. The raison d'être of this tiny percentage of humanity is to understand and to question. Such a compulsion for explainability is especially true for the theorist. Here and throughout, we will use the term theoretical science to include both (1) pure mathematics and (2) the development and testing of theories and hypotheses using mathematics, exemplified by theoretical physics. In other words, we will adhere to the British academic convention and consider the likes of theoretical physics or theoretical computer science as sub-disciplines of mathematics.

In some sense, the desire for interpretability and explicability is the very reason why the field of AIassisted theoretical discovery has been a relative late-comer to fundamental science. While as far back as the 1990s experimental physicists at CERN, in searching for new particles, were amongst the first scientists to use AI [110, it wasn't until 2017-8 that machine learning techniques emerged in theoretical physics 65 , 21, 90, 117, 97] and pure mathematics 68,20 .

The past five years has seen tremendous progress in AI-driven theoretical investigations. We give but a few of the representative examples out of the hundreds of papers that have come to define this exciting field. In theoretical physics, these have included particle phenomenology from string theory $40,103,29,107,116,94$, 16, 109, 1, establishing dictionaries between field theory and deep-learning $64,88,45$, theoretical cosmology 97, 83, 115, quantum field theories $[24,25,84,63,62,92$, and uncovering fundamental symmetries 25,52 , $91,125,31,98,96$. In parallel, in pure mathematics, these have included algebraic geometry $65,68,30,87$, $5,75,59,44,14,13,27$, , algebraic structures and representation theory $72,10,35,39,77$, symbolic algebra and computation $[134,43,93,42,108,33$, differential and metric geometry [6, 82, 7, 95], number theory 4 , $70,74,73,2$, graph theory and combinatorics $[7,8,9,13$, as well as knot theory $61,32,35$. As to the question of AI's role in our civilization, there is a mixture of optimism $28,80,132,54,120,60,48,12]$ and anxiety 119 .

The purpose of this Perspective is to attempt a balanced overview of the advances that have been made in the last few years $68,99,135,129,66,60,128$, whilst bearing in mind the concerns and limitations 58 , $41,46,89$. We shall caution against both euphoric over-enthusiasm and unwarranted fear, and convey that the future of the mathematical sciences will be an inevitable and hopefully harmonious union between the human and AI.

### 1.1 What is the AI Mathematician?

Regarding the issue of AI-driven theoretical discovery, the natural question arises as to "What is the AI mathematician"? First, we need to ask a more basic question: "What is mathematics?", or perhaps on a more pragmatic level, "What is a mathematician?". One could look at it in three ways 67, on which we will expound in detail, in light of how AI has been instrumental to each over recent years. Not delving into the depth of the philosophy of mathematics and the philosophy of science, one can loosely categorize as follows:

I. Bottom-Up: One can think of mathematics as being built from foundational axioms, where all theorems and equations are constructed from the roots up using logic. This is what is known as Hilbert's Formalism Programme 133. We will refer to this approach as "bottom-up" to reflect the rigorous nature of theoretical research.

II. Meta-Mathematics: Closely related to the bottom-up approach is the Logicism of Russell-Whitehead 118 and ultimately Wittgenstein 130. We will think of this as "Mathematics as Language", where one considers any proposition as a set of symbols, led to by sequences of symbols that one calls proof or derivation. We will refer to this as meta-mathematics in the sense of looking at the problem from a distance, perhaps more as a linguist or computer-scientist.

III. Top-Down: The practicing theorist often experiments and conjectures before tackling a proof or derivation. This is somewhat in the spirit of Brouwer's intuitionist approach to mathematics 19 where one factors in the human element. We will refer to this as "top-down", where an over-arching view, based on experience and speculation, guides one towards a problem.

The above is the "Triumvirate" in the title. We shall discuss how each has witnessed dramatic advancement in the last few years.

## 2 Bottom-Up Mathematics

Hilbert's "Wir mussen wissen. Wir werden wissen." (We must know. We will know.) is a famous declaration that should be considered in conjunction with the Principia Mathematica of Russell-Whitehead, in a tradition

![](https://cdn.mathpix.com/cropped/2024_06_04_5273dd4d7e4dd616928ag-03.jpg?height=46&width=1645&top_left_y=562&top_left_x=237)
Elements of Euclid. The Programme of building up mathematical truths from the ground up received a fatal blow from the Undecidability and Incompleteness Theorems of Gödel 55, Church 26] and Turing [124 by the 1930s.

However, to quote Prof. Minhyong Kim in a private communication, "the practicing mathematician rarely contemplates whether your daily proposition is provable or not". In other words, the space of decidable and interesting statements are so vast that one could first focus on these. Thus, despite the logical impossibility of building all mathematical statements bottom-up, theorists certainly never stopped pursuing proofs for countless propositions. This lead to the modern day answer to Russell-Whitehead's Principia Mathematica 118: the Automated Theorem Proving programme (ATP). Arguably the first AI system for mathematics or indeed, the first AI system - was the Logic Theory Machine 104 of Newell-Simon-Shaw of 1956, which was an early computer system 1 designed for, and succeeded in, proving a number of propositions of the Principia.

It has been some 70 years since this first AI-for-mathematics system and much progress has been made. Over the second half of the twentieth century, it became clear that an increasing number of proofs of fundamental results in mathematics are impossible without the computer. These have ranged from situations where key steps reduce to extensive brute-force computation, such as in the four-colour theorem, to more extreme circumstances where it takes longer than a human life-span to go through all the details, such as the classification of simple finite groups. Dependence of the human theorist on machines have prompted such influential figures as Terrance Tao 112] and International Congress of Mathematicians addresses 34 to seriously consider the future of mathematics.

While the first proof-assistant appeared in the 1970s, [37, Isabelle/HOL 105, Coq 15, Agda 126, and Lean [38] softwares are spear-heading the ATP programme in this century. One notable direction well under way is the Xena project (https://xenaproject.wordpress.com/) headed by Prof. Kevin Buzzard to formalize all (every statement and every step of proof) of undergraduate-level mathematics into Lean. More recently and more non-trivially, Gowers, Green, Manners and Tao [56] used Lean's MathLib library to prove the Polynomial Freiman-Ruzsa conjecture. Over several private conversations with Buzzard and Davenport, we are still far from having established anything close to a full database of all of contemporary mathematics in Coq or Lean format ${ }^{2}$, let alone have AI automation on selecting correct proof strategies given a proposition or conjecture. Nevertheless, given such a database 106, 100, there inevitably will be a plethora of research devoted to mining this data for new theories. This brings us to the next point.

## 3 Meta-Mathematics

From the Principia to the advancement of computer science - or indeed, from Euclid's Elements or Galileo's Il Saggiatore - there has been a tradition of viewing mathematics as a language 53. Indeed, Natural Language Processing (NLP) is rooted in Turing's original proposal of his famous eponymous test 123. AI and the internet have propelled NLP to the era of the Large Language Model (LLM). Indeed, openAI's ChatGTP (or its counterparts such as google's Gemini) has passed the Turing Test 17. The important point here, of course, is that LLM has no understanding of the underlying material, it is merely grouping together words in the right order based on large corpora of statistical samples. The philosophy 3 of "understanding" aside,[^0]it is indubitable that ChatGTP has been transformative in mimicking human communication.

One of the earliest experiments 71 on LLM for theory was the application of the Word2Vec 101 neural network (perhaps the most basic LLM technique) applied to the titles of several sections of the ArXiv (www.arxiv.org), the most comprehensive repository for contemporary research in mathematics and theoretical physics. Perhaps more interesting than in retrieving seemingly sensible linguistic identities (e.g., 'string-theory + Calabi-Yau $=$ M-theory $+\mathrm{G} 2$ '), was a comparison with viXra (www.viXra.org), the repository of fringe ideas not accepted by main-stream science. From the titles alone, one could significantly distinguish (from the confusion matrix) different sub-fields of theoretical physics (high energy theory, high energy phenomenology, general relativity/quantum cosmology, etc) in arXiv, whereas in viXra this is not so. In other words, the syntax of proper theoretical science is more self-consistent than that of fringe science even at the level of titles 4

Today, Word2Vec has been superseded by transformers which are the preferred architectures for NLP, and the programme of $L L M$ for mathematics is blooming [86, 81, 127, 113, 121, 3, 50]. Notably, in parallel to the aforementioned OpenAI and MetaAI projects, DeepMind's AlphaGeo |121) has recently been able to generate correct, human-understandable proofs for Olympiad level problems in Euclidean geometry. It is clear, when and if we do have a linguistic database of all contemporary mathematics - which is certainly many decades in the future - the LLM approach of AlphaGeo on this vast data should produce new mathematics.

## 4 Top-Down Mathematics

Everything we have said so far has to do with building correct mathematical statements. But frequently one has no idea what statement one should try to show. Indeed, how does the practicing mathematician actually work? In many ways, our papers are written backwards. From day to day, we doodle on paper and on board, experimenting with ideas, mistakes, and expressions, until something sensible comes out. Then, we go back and formalize with definitions followed by theorems and derivations that lead to logical conclusions. Thus, journal papers in mathematics and theoretical physics look "bottom-up" even though the discovery process is quite the opposite. Historically, this is even more true for some of the greatest theoretical discoveries. Newton and Euler were freely manipulating formal expressions in calculus, centuries before a proper notion of analysis and convergence; Galois showed the unsolvability of the quintic by radicals by seeing the structures of permutation groups, before the definitions of groups and fields that we are taught today; theoretical physicists freely manipulate Feynman integrals to give results that agree with experiment to astounding accuracy, when we still do not have a mathematically rigorous formulation of quantum field theory etc.

In a recent AI safety conference, a policy maker jokingly said to me that mathematicians are high on the list of jobs being replaced ${ }^{5}$ In her mind, a human mathematician is a bottom-up Logical Theory Machine, building sentences (proofs and derivations) from definitions. In reality, actual mathematical research is based on a combination of inspiration, intuition, and experience. To contrast with the almost dry narrative of "bottom-up", this almost fuzzy approach we will call "top-down". Of course, at the end of the day, all statements must be rigorous and any fuzziness and inaccuracies must be distilled out (see nice recent Perspective 60$]$ ). It is this direction which we now discuss in detail, with illustrative examples.

Perhaps contrary to common conception, an indispensable component to even the purest mathematical discovery is data. This is not experimental data in the sense of, e.g., particle trajectories from CERN, with errors and variance, but results of classifications and computations, e.g., tables of characters of finite groups. This "pure data" is exact and without statistical error and shed light on the underlying theory. To quote Vladimir Arnol'd, "mathematics is the part of physics where experiments are cheap." Indeed, while AI has long been instrumental in the scientific big data revolution 57, 79, its application to pure mathematical data is new 68,67 .

The best neural network of the nineteenth century is undoubtedly the brain of Gauss. Confronted with the ancient problem of finding patterns in primes which date back at least to Euclid, the sixteen-year-old[^1]defined the prime counting function which gives the number of prime numbers not exceeding a positive real $x \in \mathbb{R}_{+}$,

$$
\begin{equation*}
\pi(x)=\#\{p \leq x: p \text { prime }\} \tag{1}
\end{equation*}
$$

Gauss consulted tables available at the time and computed tens of thousands more (by hand!) and simply plotted $\pi(x)$. With divine inspiration, he conjectured that $\pi(x) \sim x / \ln (x)$. This profound observation had to wait for the establishment of complex analysis by Cauchy and Riemann in order to be proven by Hadamard and de la Vallée Poussin at least 50 years later. It is now known as the Prime Number Theorem, a cornerstone of mathematics.

In the twentieth century, Birch and Swinnerton-Dyer plotted, using the earliest computers of the 1960s, ranks and other quantities for elliptic curves, and conjectured that the order of vanishing of the L-function $L(s)$ for the curve at $s \rightarrow 1$ equals to the rank. This observation is the the now celebrated BSD Conjecture that bears their name; it is a Millennium Prize problem 22 and central to modern mathematics.

The above are but two of the countless examples where experimenting with pure data can lead to profound results. They illustrate the importance of conjectures. In theoretical research, finding the interesting problem is vital, and is time and again guided by the almost undefinable process of intuition. G. H. Hardy's definition of mathematics is succinct: "A mathematician, like a painter or a poet, is a maker of patterns." Even a theoretical physicist or mathematical biologist, whose principle motivation comes from real world data and observations, would first distill the problem from Nature into a mathematical problem. Then it again becomes a mathematical game of pattern spotting, from graphs and plots, to formal symbols. But here is an undeniable point: if there is one thing that AI can do better than humans, it is pattern recognition, especially when the data is in high dimension.

### 4.1 Playing with Binary Sequences

Let us perform the following simple experiment. Given (i) the sequence $\{0,0,1,0,0,1,0,0,1,0,0,1,0$, $0,1,0,0,1\}$ and asked what the next number is, any human would instantly say 0 . One way to describe it is the sequence of whether $n$ divides 3 , for positive integers $n \in \mathbb{Z}_{>0}$. Now, (ii) try $\{0,1,1,0,1,0,1,0,0$, $0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,0,0$, $0\}$. An inspired person might, after some experimentation, conclude that the next number is 1 ; this is the sequence of PrimeQ, whether the $n$-th positive integer is prime or not. Next, (iii) try $\{1,1,1,0,1,1,1,0$, $0,1,1,0,1,1,1,0,1,0,1,0,1,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,1,1,1,0,1,1,1,0,0,1,1,0,0$, $0,1\}$. The person will be rather hard pressed to guess ${ }^{6}$. This is the sequence of whether the $n$-th positive integer has a even (0) or odd (1) number of prime factors counted with multiplicity, a shifted version of the so-called Möbius mu-function. Uncovering its patterns would have incredible repercussions for mathematics: there are equivalent formulations of the Riemann Hypothesis in terms of this sequence.

What if we gave the sequence to AI? For instance, what about a supervised machine learning algorithm? In order to establish a reasonable training set, one could chose the following representation (and indeed the choice of representation is extremely important!). Take one of the above infinite sequences $\left\{a_{i}\right\}_{i=1,2,3, \ldots}$ and a sliding window of length $N$. In other words, consider a set of sequences $\left\{\left\{a_{i}\right\}_{i=1,2, \ldots, N},\left\{a_{i}\right\}_{i=2,3, \ldots, N+1}, \ldots\left\{a_{i}\right\}_{i=k, k+1, \ldots, N+k-1}\right\}$ for some $k$. Here, $k$ will be taken to be sufficiently large (say 100,000) to create a decent data size, and $N$ will be taken to be sufficiently large (say 100) to give enough features. After all, mathematical data is cheap! We can then consider each of the finite sub-sequences as a single vector in $\mathbb{R}^{N}$ and label it by the next number outside the window:

$$
\begin{equation*}
\left\{\left(a_{i}\right)_{i=1,2, \ldots, N} \longrightarrow a_{N+1}, \quad\left(a_{i}\right)_{i=2,3, \ldots, N+1} \longrightarrow a_{N+2}, \quad \ldots, \quad\left(a_{i}\right)_{i=k, k+1, \ldots, N+k-1} \longrightarrow a_{N+k}\right\} \tag{2}
\end{equation*}
$$

Note that we have chosen our sequences judiciously to standardize everything into a binary classification problem of binary vectors of dimension $N$. The question is then: having seen $k$ labelled samples, how well will the ML algorithm predict on $k^{\prime}$ unseen vectors. This familiar supervised ML paradigm can be then compared to the human eye.

One can readily check that with the most basic ML algorithms suited for this problem, such as decision trees, support vector machines, relatively shallow feed-forward neural networks with ReLU activation functions, etc., on Eq (2) applied to our three sequences. For (i) one very quickly reaches $100 \%$ accuracy for any[^2]of the ML strategies. On (ii) one reaches $7^{7}$ about $80 \%$, while on (iii) one struggles to find any AI algorithm that would beat $50 \%$. What this means is that (i), a trivial problem for the human eye, is as trivial for the $\mathrm{AI}$; for (ii) the AI might be finding some version of the Sieve of Eratosthenes for checking PrimeQ; and for (iii) the AI is not beating a random guess. Of course, should one find an algorithm which does, then one might be well under way in finding a new approach toward the Riemann Hypothesis!

Many of the papers referenced in the introduction employ similar ideas to the previous paragraph, but to much more sophisticated situations. Indeed, what (computable) mathematics, in the sense of a Turing Machine, does not fall into some version of $\mathrm{Eq} \sqrt{2}$ ?! We can make the situation even more visual and suited to AI by wrapping each $N$ vector into an $m \times n=N$ matrix, which can be interpreted as a pixelated image where 1 is black and 0 is white. For instance, suppose $N=100$, the first vector for situation (ii) can be wrapped into a $10 \times 10$ matrix, together with a label 1 (since 101 is prime): something like Take, as a much more elevated example, the problem of computing a topological invariant for a manifold in algebraic geometry (which involves rather advanced calculations). Yet, one can represent the manifold as a pixelated image by tensorizing the multi-degree information of the manifold as an algebraic variety 65 , 68. In a similar manner one could fashion any mathematical computation as an image recognition problem. Learning and gaining experience and intuition by a plethora of calculations - as mathematicians and theorists do during their careers - is in analogy to training a neural network. We could summarize this paradigm as

Bottom-up (and meta-) mathematics is language processing while top-down mathematics is image processing.

### 4.2 The Birch Test

The key steps to top-down mathematics are (1) identifying the problem and (2) identifying a strategy to attack the problem. Both depend on experience, with a healthy dose of intuition. While LLMs applied to the likes of Lean's MathLib [38] are making baby-steps in (2), step (1) is formally known as "Conjecture Formulation", exemplified by the aforementioned cases of Gauss, Birch, Swinnerton-Dyer. Can AI assist in telling a good conjecture from a useless one? Which patterns found from mathematical data lead to interesting as opposed to trivial mathematics? More recently, this AI-guided "Conjecture Formulation" has been given much systematic thought $131,67,35,51,111,102,11,36$.

In a six-month workshop in Cambridge in 2023 which I helped to co-organize 23], together with Professor K. Buzzard et al., we wanted to give some criteria on AI-driven theoretical discovery, and in particular on AI-assisted conjectures. Since chatGTP has passed the Turing Test, we wanted to up the ante. Inspired by a talk given by Birch 18, we called it the Birch Test 69. The AI-assisted discovery must be such that

(A) Automaticity: it is completely made by AI from pattern-spotting, without any human intervention;

(I) Interpretability: any statements - conjectures or conclusions - be precise to a human mathematician, who cannot distinguish it from one given by a human colleague;

(N) Non-Triviality: it is non-trivial enough that the community of human experts will work on it.

To be fair, these are very stringent criteria (one needs to have a high bar in honour of Birch!) and so far no AI-assisted theoretical discovery has passed all three parts of the test. We now highlight with some examples where they succeed and fail.

Take the early experiments of obtaining topological invariants by deep-learning algebraic varieties 65 , 68. They have been improved to $>99.9 \%$ accuracy 44, which hint toward underlying and yet unknown structures in algebraic geometry that facilitate calculations without recourse to standard and computationally expensive sequence chasing. These results suffer from the typical problem of deep neural networks: there is no interpretible formula one could extract. Thus, they fail Birch Test (I). A better situation 72 is where a support vector machine found a separation between simple and non-simple finite groups by plotting the Cayley multiplication tables. However, the hypersurface of separation is so complicated and furthermore[^3]deforms as more samples of groups were added that Birch Test (I) is still not passed. The knot invariant relations found by saliency analyses [35] and the Reidemeister moves untangled for extremely complicated knots 61], though novel, interesting, and precise, were either readily proven or have not become sufficiently influential in the field; thus they fail Birch Test $(\mathrm{N})$. Likewise, the continued fraction identities found by the Ramanujan machine 111, or the physical conservation laws found by AI-Feynman 125 also belong to this category. Even the faster matrix multiplication algorithm found by DeepMind 47 was shortly thereafter beaten by human researchers 85 .

The closest any AI-guided theoretical discovery to passing the Birch test so far, which has been precise enough and propelled a community of human experts to work on, is the Murmuration Conjectures in number theory [78. This discovery has passed (I) and for the first time passed (N). However, it fails (A) in that humans intervened in the process by digging under the hood: surprised by why AI was doing so well at distinguishing ranks of elliptic curves in the context of BSD, the researchers had to home in on a principal component analysis, and then look at the weight matrices in order to extract an unexpected formula.

## 5 Prospectus

The human theorist is not in danger of being put out of the job in the foreseeable future. From the lack of a complete bottom-up MathLib database 38 for all of mathematics, to the challenges LLMs would face given the vast search space of proof strategies even with such a database, to the exacting requirements of the Birch Test in top-down mathematics, we are far from automating theoretical discovery.

Nevertheless, it is undeniable that AI is beginning to play and will continue to play a pivotal role in partnership with the human mathematician and theoretical scientist. In the nineteenth century, Gauss's intuitions alone were good enough to spot patterns that led to such profound results as the Prime Number Theorem. In the twentieth century, computer experimentations were needed along-side the insights of Birch and Swinnerton-Dyer to raise the BSD Conjecture. Now, in the twenty-first century, AI will work hand-inhand with human experts to find new insights, conjectures, as well as strategies for derivations and proofs.

## Acknowledgements

I am most grateful to Dr. Ananyo Bhattacharya, Prof. Alexander Kosyak and Dr. Melissa Duncan for many valuable comments on the draft. I would like to lend this opportunity to thank my many collaborators over the past seven years on AI-assisted mathematics, for the great fun and friendship: Daattavya Aggarwal, Laura Alessandretti, Guillermo Arias-Tamargo, Anthony Ashmore, Jiakang Bao, Andrea Baronchelli, Per Berglund, David Berman, Kieran Bull, Lucille Calmon, Hengyu Chen, Siqi Chen, Andrei Constantin, PierrePhilippe Dechant, Rehan Deen, Stavros Garoufalidis, Elli Heyes, Edward Hirst, Johannes Hofscheier, Juan Ipiña, Vishnu Jejjala, Alexander Kasprzyk, Minhyong Kim, Shailesh Lal, Kyu-Hwan Lee, Seung-Joo Lee, Jianrong Li, Andre Lukas, Suvajit Majumder, Challenger Mishra, Gregg Musiker, Brent Nelson, Andrew Nestor, Thomas Oliver, Burt Ovrut, Toby Peterken, Stephen Pietromonaco, Andrey Pozdnyakov, Dmitrii Riabchenko, Diego Rodriguez-Gomez, Henrique Sá Earp, Max Sharnoff, Tomás Silva, Eldar Sultanow, Yuxuan Xiao, Shing-Tung Yau, and Zaid Zaz.

The resaerch is funded in part by STFC grant ST/J00037X/2 and the Leverhulme Trust for a project grant.

## Competing interests

The author declares no competing interests.

## Publisher's note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

## References

[1] Steven Abel et al. "String Model Building, Reinforcement Learning and Genetic Algorithms". In: Nankai Symposium on Mathematical Dialogues: In celebration of S.S.Chern's 110th anniversary. Nov. 2021. arXiv: 2111.07333 [hep-th].

[2] Steven A. Abel and Luca A. Nutricati. "Ising Machines for Diophantine Problems in Physics". In: Fortsch. Phys. 70.11 (2022), p. 2200114. DOI: 10.1002 /prop . 202200114. arXiv: 2206.09956 [hep-th].

[3] Janice Ahn et al. Large Language Models for Mathematical Reasoning: Progresses and Challenges. 2024. arXiv: 2402.00157 [cs.CL].

[4] Laura Alessandretti, Andrea Baronchelli, and Yang-Hui He. "Machine Learning meets Number Theory: The Data Science of Birch-Swinnerton-Dyer". In: (Nov. 2019). arXiv: 1911.02008 [math.NT].

[5] Ross Altman et al. "Estimating Calabi-Yau Hypersurface and Triangulation Counts with Equation Learners". In: JHEP 03 (2019), p. 186. DOI: 10.1007/JHEP03(2019)186, arXiv: 1811.06490 [hep-th].

[6] Anthony Ashmore, Yang-Hui He, and Burt A. Ovrut. "Machine Learning Calabi-Yau Metrics". In: Fortsch. Phys. 68.9 (2020), p. 2000068. DOI: 10.1002/prop.202000068 arXiv: 1910.08605 [hep-th].

[7] Anthony Ashmore et al. "Calabi-Yau Metrics, Energy Functionals and Machine-Learning". In: International Journal of Data Science in the Mathematical Sciences 1.1 (2023), pp. 49-61. DOI: 10.1142/ S2810939222500034, arXiv: 2112.10872 [hep-th].

[8] Jiakang Bao et al. "Hilbert series, machine learning, and applications to physics". In: Phys. Lett. B 827 (2022), p. 136966. DOI: 10.1016/j.physletb.2022.136966, arXiv: 2103.13436 [hep-th].

[9] Jiakang Bao et al. "Polytopes and Machine Learning". In: Math. Sci. 01 (2023), pp. 181-211. DOI: 10.1142/S281093922350003X, arXiv: 2109.09602 [math.CO].

[10] Jiakang Bao et al. "Quiver Mutations, Seiberg Duality and Machine Learning". In: Phys. Rev. D 102.8 (2020), p. 086013. DOI: 10.1103/PhysRevD.102.086013. arXiv: 2006.10783 [hep-th].

[11] Andrej Bauer, Matej Petković, and Ljupčo Todorovski. MLFMF: Data Sets for Machine Learning for Mathematical Formalization. 2023. arXiv: 2310.16005 [cs.LG].

[12] Yoshua Bengio and Nikolay Malkin. "Machine learning and information theory concepts towards an AI Mathematician". In: arXiv preprint arXiv:2403.04571 (2024).

[13] Per Berglund, Ben Campbell, and Vishnu Jejjala. "Machine Learning Kreuzer-Skarke Calabi-Yau Threefolds". In: (Dec. 2021). arXiv: 2112.09117 [hep-th].

[14] David S. Berman, Yang-Hui He, and Edward Hirst. "Machine learning Calabi-Yau hypersurfaces". In: Phys. Rev. D 105.6 (2022), p. 066002. DOI: 10.1103/PhysRevD. 105.066002 , arXiv: 2112.06350 [hep-th].

[15] Yves Bertot and Pierre Castéran. Interactive theorem proving and program development: Coq'Art: the calculus of inductive constructions, https: //coq. inria. fr/. Springer Science \& Business Media, 2013 .

[16] Martin Bies et al. "Machine Learning and Algebraic Approaches towards Complete Matter Spectra in 4d F-theory". In: JHEP 01 (2021), p. 196. DOI: 10.1007/JHEP01(2021)196, arXiv: 2007.00009 [hep-th].

[17] Celeste Biever. "ChatGPT broke the Turing test-the race is on for new ways to assess AI". In: Nature 619.7971 (2023), pp. 686-689.

[18] B Birch. Reminiscences from 1958-62, Cambridge talk https: // www. sms . cam. ac. uk/media/ 3530787 .

[19] Luitzen Egbertus Jan Brouwer. "Intuitionistic reflections on formalism". In: originally published in (1927), pp. 490-492.

[20] Kieran Bull et al. "Machine Learning CICY Threefolds". In: Phys. Lett. B 785 (2018), pp. 65-72. DOI: 10.1016/j.physletb.2018.08.008, arXiv: 1806.03121 [hep-th].

[21] Jonathan Carifio et al. "Machine Learning in the String Landscape". In: JHEP 09 (2017), p. 157. DOI: $10.1007 / \mathrm{JHEP09}$ (2017)157, arXiv: 1707.00655 [hep-th].

[22] James A Carlson, Arthur Jaffe, and Andrew Wiles. The millennium prize problems. American Mathematical Soc., 2006.

[23] A Castro et al. Black holes: bridges between number theory and holographic quantum information https: //www. newton. ac. uk/event/blh/.

[24] Heng-Yu Chen et al. "Machine Learning Etudes in Conformal Field Theories". In: (June 2020). arXiv: 2006.16114 [hep-th].

[25] Heng-Yu Chen et al. "Machine learning Lie structures \& applications to physics". In: Phys. Lett. B 817 (2021), p. 136297. DOI: 10.1016/j.physletb.2021.136297, arXiv: 2011.00871 [hep-th].

[26] Alonzo Church. "An unsolvable problem of elementary number theory". In: Amer. J. Maths 58 (1936), pp. $345-363$.

[27] Tom Coates, Alexander M Kasprzyk, and Sara Veneziale. "Machine learning the dimension of a Fano variety". In: Nature Communications 14.1 (2023), p. 5526.

[28] Timothy Cohen, Marat Freytsis, and Bryan Ostdiek. "(Machine) learning to do more with less". In: Journal of High Energy Physics 2018.2 (2018), pp. 1-28.

[29] Alex Cole and Gary Shiu. "Topological Data Analysis for the String Landscape". In: JHEP 03 (2019), p. 054. DOI: 10.1007/JHEP03(2019)054, arXiv: 1812.06960 [hep-th]

[30] Andrei Constantin and Andre Lukas. "Formulae for Line Bundle Cohomology on Calabi-Yau Threefolds". In: Fortsch. Phys. 67.12 (2019), p. 1900084. DOI: 10.1002/prop. 201900084 arXiv: 1808. 09992 [hep-th]

[31] Cristina Cornelio et al. "AI Descartes: Combining data and theory for derivable scientific discovery". In: arXiv:2109.01634 (2021).

[32] Jessica Craven, Vishnu Jejjala, and Arjun Kar. "Disentangling a deep learned volume formula". In: JHEP 06 (2021), p. 040. DOI: 10.1007/JHEP06(2021)040, arXiv: 2012.03955 [hep-th].

[33] Lennart Dabelow and Masahito Ueda. Symbolic Equation Solving via Reinforcement Learning. 2024. arXiv: 2401.13447 [cs.LG].

[34] James Davenport and Kevin Buzzard. "The Future of Mathematics; ICM panels \& lectures". In: Proc. ICM. WS. 2018, pp. 1085-1110.

[35] Alex Davies et al. "Advancing mathematics by guiding human intuition with AI". In: Nature 600.7887 (2021), pp. $70-74$.

[36] Randy Davila. Advancements in Research Mathematics through AI: A Framework for Conjecturing. 2023. arXiv: 2306.12917 [math.C0]

[37] Nicolaas Govert De Bruijn. "The mathematical language AUTOMATH, its usage, and some of its extensions". In: Studies in Logic and the Foundations of Mathematics. Vol. 133. Elsevier, 1994, pp. 73100 .

[38] Leonardo De Moura et al. "The Lean theorem prover (system description), https://leanprovercommunity.github.io/'. In: Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25. Springer. 2015, pp. 378388 .

[39] Pierre-Philippe Dechant et al. "Cluster Algebras: Network Science and Machine Learning". In: J. Comput. Algebra 8 (2023). DOI: 10.1016/j.jaca.2023.100008, arXiv: 2203.13847 [math.CO].

[40] Mehmet Demirtas et al. "The Kreuzer-Skarke Axiverse". In: JHEP 04 (2020), p. 138. DOI: 10.1007/ JHEP04(2020)138, arXiv: 1808.01282 [hep-th].

[41] V. V. Dolotin, A. Yu. Morozov, and A. V. Popolitov. "Machine learning of the well-known things". In: Theoretical and Mathematical Physics 214.3 (Mar. 2023), pp. 446-455. ISSN: 1573-9333. DOI: 10.1134/s0040577923030091. URL: http://dx.doi.org/10.1134/S0040577923030091.

[42] Weinan E. "Machine Learning and Computational Mathematics". In: Communications in Computational Physics 28.5 (June 2020), pp. 1639-1670. ISSN: 1991-7120. DOI: 10.4208/cicp.oa-2020-0185 URL: http://dx.doi.org/10.4208/cicp.0A-2020-0185.

[43] Matthew England. "Machine Learning for Mathematical Software". In: Lecture Notes in Computer Science. Springer International Publishing, 2018, pp. 165-174. ISBN: 9783319964188. DOI: $10.1007 /$ 978-3-319-96418-8_20. URL: http://dx.doi.org/10.1007/978-3-319-96418-8_20.

[44] Harold Erbin and Riccardo Finotello. "Machine learning for complete intersection Calabi-Yau manifolds: a methodological study". In: Phys. Rev. D 103.12 (2021), p. 126014. DOI: 10.1103/PhysRevD. 103.126014, arXiv: 2007.15706 [hep-th].

[45] Harold Erbin, Vincent Lahoche, and Dine Ousmane Samary. "Non-perturbative renormalization for the neural network-QFT correspondence". In: Mach. Learn. Sci. Tech. 3.1 (2022), p. 015027. DOI: 10.1088/2632-2153/ac4f69. arXiv: 2108.01403 [hep-th].

[46] Oscar Fajardo-Fontiveros et al. "Fundamental limits to learning closed-form mathematical models from data". In: Nature Communications 14.1 (Feb. 2023). ISSN: 2041-1723. DOI: 10.1038/s41467023-36657-z. URL: http://dx.doi.org/10.1038/s41467-023-36657-z.

[47] Alhussein Fawzi et al. "Discovering faster matrix multiplication algorithms with reinforcement learning". In: Nature 610.7930 (2022), pp. 47-53.

[48] Thomas Fink. Why mathematics is set to be revolutionized by AI. 2024.

[49] Gottlob Frege. Grundgesetze der Arithmetik: begriffsschriftlich abgeleitet. Vol. 1. H. Pohle, 1893.

[50] Simon Frieder et al. Large Language Models for Mathematicians. 2024. arXiv: 2312.04556 [cs.CL].

[51] Pascal Friederich et al. "Scientific intuition inspired by machine learning-generated hypotheses". In: Machine Learning: Science and Technology 2.2 (2021), p. 025027.

[52] Yarin Gal et al. "Baryons from Mesons: A Machine Learning Perspective". In: Int. J. Mod. Phys. A 37.06 (2022), p. 2250031. DOI: 10.1142/S0217751X22500312. arXiv: 2003.10445 [hep-ph].

[53] Mohan Ganesalingam and Mohan Ganesalingam. The language of mathematics. Springer, 2013.

[54] Iulia Georgescu. "How machines could teach physicists new scientific concepts". In: Nature Reviews Physics 4.12 (2022), pp. 736-738.

[55] Kurt Gödel. "Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I". In: Monatshefte für mathematik und physik 38 (1931), pp. 173-198.

[56] W. T. Gowers et al. On a conjecture of Marton (see Quanta coverage in https: // www. quantamagazir.e. org/a-team-of-math-proves-a-critical-link-between-addition-and-sets-20231206/). 2023. arXiv: 2311.05762 [math.NT].

[57] Matthew J Graham et al. "Machine-assisted discovery of relationships in astronomy". In: Monthly Notices of the Royal Astronomical Society 431.3 (2013), pp. 2371-2384.

[58] André Greiner-Petter et al. Why Machines Cannot Learn Mathematics, Yet. 2019. arXiv: 1905.08359 [cs.DL].

[59] Thomas W. Grimm, Fabian Ruehle, and Damian van de Heisteeg. "Classifying Calabi-Yau Threefolds Using Infinite Distance Limits". In: Commun. Math. Phys. 382.1 (2021), pp. 239-275. DOI: 10.1007/ s00220-021-03972-9. arXiv: 1910.02963 [hep-th].

[60] Sergei Gukov, James Halverson, and Fabian Ruehle. "Rigor with machine learning from field theory to the Poincaré conjecture". In: Nature Reviews Physics (2024), pp. 1-10.

[61] Sergei Gukov et al. "Learning to Unknot". In: Mach. Learn. Sci. Tech. 2.2 (2021), p. 025035. DOI: 10.1088/2632-2153/abe91f, arXiv: 2010.16263 [math.GT].

[62] Rajan Gupta, Tanmoy Bhattacharya, and Boram Yoon. "AI and Theoretical Particle Physics". In: (2023), pp. 465-491. DOI: 10.1142/9789811265679_0026. arXiv: 2205.05803 [hep-lat].

[63] T. R. Harvey and A. Lukas. "Quark Mass Models and Reinforcement Learning". In: JHEP 08 (2021), p. 161. DOI: $10.1007 /$ JHEP08(2021)161. arXiv: 2103.04759 [hep-th]

[64] Koji Hashimoto et al. "Deep learning and the AdS/CFT correspondence". In: Phys. Rev. D 98.4 (2018), p. 046019. DOI: 10.1103/PhysRevD.98.046019, arXiv: 1802.08313 [hep-th].

[65] Yang-Hui He. "Deep-Learning the Landscape". In: Phys. Lett. B 774 (June 2017), pp. 564-568. DoI: 10.1016/j.physletb.2017.10.024, arXiv: 1706.02714 [hep-th].

[66] Yang-Hui He, ed. Machine Learning in Pure Mathematics and Theoretical Physics. World Scientific, 2023.

[67] Yang-Hui He. "Machine-Learning Mathematical Structures". In: (Jan. 2021). arXiv: 2101 . 06317 [cs.LG].

[68] Yang-Hui He. The Calabi-Yau Landscape: From Geometry, to Physics, to Machine Learning. Lecture Notes in Mathematics. Springer-Nature, May 2018. ISBN: 978-3-030-77561-2, 978-3-030-77562-9. DOI: 10.1007/978-3-030-77562-9. arXiv: 1812.02893 [hep-th].

[69] Yang-Hui He and Mikhail Burtsev. "Can AI make genuine theoretical discoveries?" In: Nature 625.7994 (2024), pp. 241-241.

[70] Yang-Hui He, Edward Hirst, and Toby Peterken. "Machine-learning dessins d'enfants: explorations via modular and Seiberg-Witten curves". In: J. Phys. A 54.7 (2021), p. 075401. DOI: 10.1088/17518121/abbc4f, arXiv: 2004.05218 [hep-th].

[71] Yang-Hui He, Vishnu Jejjala, and Brent D. Nelson. hep-th. 2018. arXiv: 1807.00735 [cs.CL].

[72] Yang-Hui He and Minhyong Kim. "Learning Algebraic Structures: Preliminary Investigations". In: (May 2019). arXiv: 1905.02263 [cs.LG].

[73] Yang-Hui He, Kyu-Hwan Lee, and Thomas Oliver. "Machine-Learning Number Fields". In: (Nov. 2020). DOI: 10.4310/MCGD.2022.v2.n1.a2, arXiv: 2011.08958 [math.NT].

[74] Yang-Hui He, Kyu-Hwan Lee, and Thomas Oliver. "Machine-Learning the Sato-Tate Conjecture". In: (Oct. 2020). DOI: 10.1016/j.jsc.2021.11.002, arXiv: 2010.01213 [math.NT].

[75] Yang-Hui He and Seung-Joo Lee. "Distinguishing elliptic fibrations with AI". In: Phys. Lett. B 798 (2019), p. 134889. DOI: 10.1016/j.physletb.2019.134889. arXiv: 1904.08530 [hep-th].

[76] Yang-Hui He and Shing-Tung Yau. "Graph Laplacians, Riemannian Manifolds and their MachineLearning". In: (June 2020). arXiv: 2006.16619 [math.CO].

[77] Yang-Hui He et al. "Learning to be Simple". In: (Dec. 2023). arXiv: 2312.05299 [cs.LG].

[78] Yang-Hui He et al. "Murmurations of elliptic curves (see Quanta feature in https://www.quantamagazine. org/elliptic-curve-murmurations-found-with-ai-take-flight-20240305/)". In: (Apr. 2022). arXiv: 2204.10140 [math.NT].

[79] Tony Hey et al. "Machine learning and big scientific data". In: Philosophical Transactions of the Royal Society A 378.2166 (2020), p. 20190054.

[80] Matthew Hudson. "No coding required: Companies make it easier than ever for scientists to use artificial intelligence". In: Science (2019). DOI: 10.1126/science.aay9534.

[81] M Hutson. "AI learns to write computer code in "stunning" advance". In: Science (2022).

[82] Vishnu Jejjala, Damian Kaloni Mayorga Pena, and Challenger Mishra. "Neural network approximations for Calabi-Yau metrics". In: JHEP 08 (2022), p. 105. DOI: 10.1007/JHEP08 (2022) 105. arXiv: 2012.15821 [hep-th]

[83] Ryusuke Jinno. "Machine learning for bounce calculation". In: (May 2018). arXiv: 1805 . 12153 [hep-th]

[84] Bastian Kaspschak and Ulf-G. Meißner. "Neural network perturbation theory and its application to the Born series". In: Phys. Rev. Res. 3.2 (2021), p. 023223. DoI: 10.1103/PhysRevResearch. 3 . 023223, arXiv: 2009.03192 [cs.LG].

[85] Manuel Kauers and Jakob Moosbauer. "The FBHHRBNRSSSHK-Algorithm for Multiplication in $\mathbb{Z}_{2}^{5 \times 5}$ is still not the end of the story". In: arXiv:2210.04045 (2022).

[86] Joanne T. Kim, Mikel Landajuela, and Brenden K. Petersen. Distilling Wikipedia mathematical knowledge into neural network models. 2021. arXiv: 2104.05930 [cs.LG].

[87] Daniel Klaewer and Lorenz Schlechter. "Machine Learning Line Bundle Cohomologies of Hypersurfaces in Toric Varieties". In: Phys. Lett. B 789 (2019), pp. 438-443. DoI: 10.1016/j.physletb. 2019. 01.002, arXiv: 1809.02547 [hep-th].

[88] Ellen de Mello Koch, Robert de Mello Koch, and Ling Cheng. "Is Deep Learning a Renormalization Group Flow?" In: (June 2019). DOI: 10.1109/ACCESS.2020.3000901, arXiv: 1906.05212 [cs.LG].

[89] Alexander Kolpakov and Aidan Rocke. "On the impossibility of discovering a formula for primes using AI". In: arXiv preprint arXiv:2308.10817 (2023).

[90] Daniel Krefl and Rak-Kyeong Seong. "Machine Learning of Calabi-Yau Volumes". In: Phys. Rev. D 96.6 (2017), p. 066014. DOI: 10.1103/PhysRevD.96.066014. arXiv: 1706.03346 [hep-th].

[91] Sven Krippendorf and Marc Syvaeri. "Detecting Symmetries with Neural Networks". In: (Mar. 2020). arXiv: 2003.13679 [physics.comp-ph].

[92] Shailesh Lal, Suvajit Majumder, and Evgeny Sobko. "The R-mAtrIx Net". In: (Apr. 2023). arXiv: 2304.07247 [hep-th].

[93] Guillaume Lample and François Charton. "Deep learning for symbolic mathematics". In: arXiv:1912.01412 (2019).

[94] Magdalena Larfors and Robin Schneider. "Explore and Exploit with Heterotic Line Bundle Models". In: Fortsch. Phys. 68.5 (2020), p. 2000034. DOI: 10.1002 /prop . 202000034 . arXiv: 2003.04817 [hep-th]

[95] Magdalena Larfors et al. "Numerical metrics for complete intersection and Kreuzer-Skarke Calabi-Yau manifolds". In: Mach. Learn. Sci. Tech. 3.3 (2022), p. 035014. DOI: 10.1088/2632-2153/ ac8e4e, arXiv: 2205.13408 [hep-th].

[96] Pablo Lemos et al. "Rediscovering orbital mechanics with machine learning". In: Machine Learning: Science and Technology 4.4 (2023), p. 045002.

[97] Junyu Liu. "Artificial Neural Network in Cosmic Landscape". In: JHEP 12 (2017), p. 149. DOI: 10.1007/JHEP12(2017)149, arXiv: 1707.02800 [hep-th].

[98] Ziming Liu et al. "Machine-learning nonconservative dynamics for new-physics detection". In: Physical Review E 104.5 (2021), p. 055302.

[99] Pan Lu et al. A Survey of Deep Learning for Mathematical Reasoning. 2023. arXiv: 2212.10535 [cs.AI].

[100] MetaAI. Teaching AI advanced mathematical reasoning, https://ai. meta. com/blog/ai-maththeorem-proving/. 2022.

[101] Tomas Mikolov et al. "Efficient estimation of word representations in vector space". In: arXiv preprint arXiv:1301.3781 (2013).

[102] Challenger Mishra, Subhayan Roy Moulik, and Rahul Sarkar. "Mathematical conjecture generation using machine intelligence". In: arXiv:2306.07277 (2023).

[103] Andreas Mütter, Erik Parr, and Patrick K. S. Vaudrevange. "Deep learning in the heterotic orbifold landscape". In: Nucl. Phys. B 940 (2019), pp. 113-129. DOI: 10.1016/j.nuclphysb. 2019.01.013. arXiv: 1811.05993 [hep-th].

[104] Allen Newell and Herbert Simon. "The logic theory machine-A complex information processing system". In: IRE Transactions on information theory 2.3 (1956), pp. 61-79.

[105] Tobias Nipkow, Lawrence C Paulson, and Markus Wenzel. Isabelle/HOL: a proof assistant for higherorder logic, https: // isabelle. in. tum. de. Vol. 2283. Springer Science \& Business Media, 2002.

[106] OpenAI. Solving (some) formal math olympiad problems, https : // openai . com/research/ formal-math. 2022.

[107] Hajime Otsuka and Kenta Takemoto. "Deep learning and k-means clustering in heterotic string vacua with line bundles". In: JHEP 05 (2020), p. 047. DOI:10.1007/JHEP05(2020)047. arXiv: 2003.11880 [hep-th].

[108] Dylan Peifer, Michael Stillman, and Daniel Halpern-Leistner. "Learning selection strategies in Buchberger's algorithm". In: International Conference on Machine Learning. PMLR. 2020, pp. 7575-7585.

[109] Ricardo Perez-Martinez, Saul Ramos-Sanchez, and Patrick K. S. Vaudrevange. "Landscape of promising nonsupersymmetric string models". In: Phys. Rev. D 104.4 (2021), p. 046026. DOI: 10. 1103/ PhysRevD.104.046026, arXiv: 2105.03460 [hep-th].

[110] D Perret-Gallix and W Wojcik. "New computing techniques in physics research: Proceedings of the 1st International Workshop on Software Engineering, Artificial Intelligence and Expert Systems in High-Energy and Nuclear Physics". In: (1990).

[111] Gal Raayoni et al. "Generating conjectures on fundamental constants with the Ramanujan Machine". In: Nature 590.7844 (2021), pp. 67-73.

[112] Siobhan Roberts. "AI Is Coming for Mathematics, Too." In: International New York Times (2023).

[113] Bernardino Romera-Paredes et al. "Mathematical discoveries from program search with large language models". In: Nature 625.7995 (2024), pp. 468-475.

[114] Frank Rosenblatt. "The perceptron: a probabilistic model for information storage and organization in the brain." In: Psychological review 65.6 (1958), p. 386.

[115] Tom Rudelius. "Learning to Inflate". In: JCAP 02 (2019), p. 044. DOI: 10.1088/1475-7516/2019/ 02/044, arXiv: 1810.05159 [hep-th].

[116] Fabian Ruehle. "Data science applications to string theory". In: Phys. Rept. 839 (2020), pp. 1-117. DOI: $10.1016 / j \cdot$ physrep.2019.09.005.

[117] Fabian Ruehle. "Evolving neural networks with genetic algorithms to study the String Landscape". In: JHEP 08 (2017), p. 038. DOI: 10.1007/JHEP08(2017)038, arXiv: 1706.07024 [hep-th]

[118] Bertrand Russell and Alfred North Whitehead. "Principia Mathematica Vol. I". In: (1910).

[119] Adam Satariano and Megan Specia. "Governments Warn AI Poses Risk of 'Catastrophic' Harm." In: The New York Times (2023), A6-A6.

[120] Jeyan Thiyagalingam et al. "Scientific machine learning benchmarks". In: Nature Reviews Physics 4.6 (2022), pp. 413-420.

[121] Trieu H Trinh et al. "Solving olympiad geometry without human demonstrations". In: Nature 625.7995 (2024), pp. 476-482.

[122] Vahe Tshitoyan et al. "Unsupervised word embeddings capture latent knowledge from materials science literature". In: Nature 571.7763 (2019), pp. 95-98.

[123] Alan M Turing. Computing machinery and intelligence. Vol. LIX (236). 1950, pp. 433-460.

[124] AM Turing. "On computable numbers, with an application to the Entscheidungsproblem. A correction". In: Proc. LMS (1938).

[125] Silviu-Marian Udrescu and Max Tegmark. "AI Feynman: A physics-inspired method for symbolic regression". In: Science Advances 6.16 (2020).

[126] Philip Wadler. "Programming language foundations in agda, https://github.com/agda/agda'. In: Formal Methods: Foundations and Applications: 21st Brazilian Symposium, SBMF 2018, Salvador, Brazil, Proc 21. Springer. 2018, pp. 56-73.

[127] Zengzhi Wang, Rui Xia, and Pengfei Liu. Generative AI for Math: Part I - MathPile: A BillionToken-Scale Pretraining Corpus for Math. 2023. arXiv: 2312.17120 [cs.CL].

[128] Alex Wilkins. "How AI mathematicians might finally deliver human-level reasoning". In: New Scientist 10 April (2024).

[129] Geordie Williamson. Is deep learning a useful tool for the pure mathematician? 2023. arXiv: 2304. 12602 [math.RT].

[130] Ludwig Wittgenstein. "Tractatus logico-philosophicus". In: (1922).

[131] Chai Wah Wu. "Can machine learning identify interesting mathematics? An exploration using empirically observed laws". In: arXiv:1805.07431 (2018).

[132] Yongjun Xu et al. "Artificial intelligence: A powerful paradigm for scientific research". In: The Innovation 2.4 (2021).

[133] Richard Zach. "Hilbert's program then and now". In: Philosophy of logic. Elsevier, 2007, pp. 411-447.

[134] Wojciech Zaremba, Karol Kurach, and Rob Fergus. Learning to Discover Efficient Mathematical Identities. 2014. arXiv: 1406.1584 [cs.LG].

[135] Cedegao E. Zhang et al. AI for Mathematics: A Cognitive Science Perspective. 2023. arXiv: 2310. 13021 [q-bio.NC].


[^0]:    ${ }^{1}$ Interestingly, this was around the same time as the emergence of the first trainable neural network 114 .

    ${ }^{2}$ Only earlier this year was a new project launched to formalize all the requisite pieces to Wiles' proof of Fermat's last theorem (UK EPSRC Grant EP/Y022904/1).

    ${ }^{3}$ Perhaps for this very reason of whether there is understanding of the underlying mathematics that we have chosen to call this direction "meta-mathematics".

[^1]:    ${ }^{4}$ If one enriched the data, and included abstracts, application of Word2Vec on papers in material science have uncovered new chemical reactions 122 .

    ${ }^{5}$ The eye doctor mentioned earlier confessed to me that he will not take on any further trainees in diagnosis; that will soon be entirely taken over by AI.

[^2]:    ${ }^{6}$ I have tried this on the audience in numerous talks over the last few years.

[^3]:    ${ }^{7}$ Note, in this case because of the increasing rarity of primes - approximately by a factor of $x / \ln (x)$ due to the Prime Number Theorem - we scale the window size accordingly.

