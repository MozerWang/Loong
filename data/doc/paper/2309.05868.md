# Analysis of Randomized Householder-Cholesky QR Factorization with Multisketching 

Andrew J. Higgins* $\quad$ Daniel B. Szyld* $\quad$ Erik G. Boman ${ }^{\dagger}$<br>Ichitaro Yamazaki ${ }^{\dagger}$

September 13, 2023


#### Abstract

CholeskyQR2 and shifted CholeskyQR3 are two state-of-the-art algorithms for computing tall-and-skinny QR factorizations since they attain high performance on current computer architectures. However, to guarantee stability, for some applications, CholeskyQR2 faces a prohibitive restriction on the condition number of the underlying matrix to factorize. Shifted CholeskyQR3 is stable but has $50 \%$ more computational and communication costs than CholeskyQR2. In this paper, a randomized QR algorithm called Randomized Householder-Cholesky (rand_cholQR) is proposed and analyzed. Using one or two random sketch matrices, it is proved that with high probability, its orthogonality error is bounded by a constant of the order of unit roundoff for any numerically full-rank matrix, and hence it is as stable as shifted CholeskyQR3. An evaluation of the performance of rand_cholQR on a NVIDIA A100 GPU demonstrates that for tall-and-skinny matrices, rand_cholQR with multiple sketch matrices is nearly as fast as, or in some cases faster than, CholeskyQR2. Hence, compared to CholeskyQR2, rand_cholQR is more stable with almost no extra computational or memory cost, and therefore a superior algorithm both in theory and practice.


Keywords: Randomized Linear Algebra, QR Factorization, CommunicationAvoiding Algorithms, Error Analysis, Numerical Stability, GPUs

MSC Classification: $65 \mathrm{~F} 05,65 \mathrm{~F} 20,65 \mathrm{~F} 25,65 \mathrm{G} 50,15 \mathrm{~B} 52$[^0]

## 1 Introduction

Computing the $\mathrm{QR}$ factorization of tall-and-skinny matrices is a critical component of many scientific and engineering applications, including the solution of least squares problems, block orthogonalization kernels for solving linear systems and eigenvalue problems within block or $s$-step Krylov methods, dimensionality reduction methods for data analysis like Principal Component Analysis, and many others. Modern high-performance computers enable extremely fast floating point operations (FLOPs), but are limited by relatively slow communication (data transfers) between processors and through the memory hierarchy. Current state-of-the-art QR algorithms for tall-and-skinny matrices are the CholeskyQR2 and shifted CholeskyQR3 algorithms 9, 10, thanks to their communication-avoiding properties along with their exploitation of vendorprovided highly-optimized dense linear algebra subroutines [2, 19, 20. However, CholeskyQR2 may fail to accurately factorize a matrix $V$ when its condition number $\kappa(V)>O\left(\mathbf{u}^{-1 / 2}\right)$, where $\mathbf{u}$ is unit roundoff 31. Shifted CholeskyQR3 is numerically stable as long as $\kappa(V)<O\left(\mathbf{u}^{-1}\right)$, but it requires over $50 \%$ more computational and communication cost than CholeskyQR2 [9]. Although more stable communication-avoiding algorithms exist, such as TSQR 7, they rely on Householder QR factorizations, and are often significantly slower than CholeskyQR2 in practice [10].

In this paper, we present and analyze a randomized algorithm called randQR for orthogonalizing the columns of a tall-and-skinny matrix with respect to a specific bilinear form. In order to reduce the cost of the computations, we propose to use "multisketching," i.e., the use of two consecutive sketch matrices, obtaining another algorithm called rand_cholQR for computing the QR factorization of a tall-and-skinny matrix $V$. Our approach is general in the sense that our analysis applies to any two $\epsilon$-subspace embedding sketching matrices (see Section 2 for definitions), but what we have in mind is one sparse sketch and one dense sketch, such as a Gaussian or Radamacher sketch 1. Our analysis applies in particular to Count-Gauss (one application of CountSketch followed by a Gaussian sketch), as described in 15, 26, 27.

We prove that with high probability, the orthogonality error of rand_cholQR is bounded by a constant of the order of unit roundoff for any numerically fullrank matrix $V$, and hence it is as stable as shifted CholeskyQR3 and it is significantly more numerically stable than CholeskyQR2. Our numerical experiments ilustrate the theoretical results. In addition, the rand_cholQR algorithm may be implemented using the same basic linear algebra kernels as CholeskyQR2. Therefore, it is simple to implement and has the same communication-avoiding properties. We perform a computational study on a state-of-the-art heterogeneous computer to demonstrate that rand_cholQR can perform up to $4 \%$ faster than CholeskyQR2 and $56.6 \%$ faster than shifted CholeskyQR3, while significantly improving the robustness of CholeskyQR2.

In summary, our contribution consist of a new error analysis of multisketching, expanding the applicability of the method, as it can be safely used for matrices of larger condition number. This analysis applies in particular to the
case of one sketch, improving upon the existing results. Our implementation confirms the theory developed in this paper.

In the next section we present some preliminary definitions and known results. We follow with Section 3. where we give the multisketching algorithm, together with a result on the use of two sketch matrices. We also present FLOP counts for different algorithms, leading to the motivation as to why multisketching is recommended. In Section 4 we present rigorous error bounds for the proposed multisketching algorithm (the proofs are given in an Appendix). These bounds can also be applied to the case of a single sketch matrix, and we compare the new results to those available in the literature. Numerical experiments are presented in Section 6 followed by our conclusions.

## 2 Preliminaries on Random Sketching

Suppose one would like to compress $V \in \mathbb{R}^{n, m}$ into a matrix with fewer columns with nearly the same norm. We denote the sketch matrix by $S \in \mathbb{R}^{s, n}$ for $s \ll n$. The sketch matrix is typically chosen to be a $\epsilon$-subspace embedding, or a linear map to a lower dimensional space that preserves $\ell_{2}$-inner products and norms of all vectors within the subspace up to a factor of $\sqrt{1 \pm \varepsilon}$ for $\varepsilon \in[0,1)$ [4, 17, 25].

Definition 2.1 ( $\varepsilon$-subspace embedding). Given $\varepsilon \in[0,1)$, the sketch matrix $S \in \mathbb{R}^{s, n}$ is an $\varepsilon$-subspace embedding for the subspace $\mathcal{V} \subset \mathbb{R}^{n}$ if $\forall x, y \in \mathcal{V}$,

$$
|\langle x, y\rangle-\langle S x, S y\rangle| \leq \varepsilon\|x\|_{2}\|y\|_{2}
$$

Corollary 2.1. If the sketch matrix $S \in \mathbb{R}^{s, n}$ is an $\varepsilon$-subspace embedding for the subspace $\mathcal{V} \subset \mathbb{R}^{n}$, then $\forall x \in \mathcal{V}$,

$$
\begin{equation*}
\sqrt{1-\varepsilon}\|x\|_{2} \leq\|S x\|_{2} \leq \sqrt{1+\varepsilon}\|x\|_{2} \tag{1}
\end{equation*}
$$

Corollary 2.2. If the sketch matrix $S \in \mathbb{R}^{s, n}$ is an $\varepsilon$-subspace embedding for the subspace $\mathcal{V} \subset \mathbb{R}^{n}$, and $V$ is a matrix whose columns form a basis of $\mathcal{V}$, then

$$
\begin{equation*}
(1+\varepsilon)^{-1 / 2} \sigma_{\min }(S V) \leq \sigma_{\min }(V) \leq \sigma_{\max }(V) \leq(1-\varepsilon)^{-1 / 2} \sigma_{\max }(S V) \tag{2}
\end{equation*}
$$

Thus,

$$
\begin{equation*}
\kappa(V) \leq \sqrt{\frac{1-\varepsilon}{1+\varepsilon}} \kappa(S V) \tag{3}
\end{equation*}
$$

As already indicated in 4], Corollary 2.2 implies that the singular values of $V$ are bounded by those of $S V$. Hence if $S V$ is well conditioned, then so is $V$.

While $\varepsilon$-subspace embeddings require knowledge of the subspace $\mathcal{V} \subset \mathbb{R}^{n}$ a priori, $(\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embeddings do not 4 .

Definition 2.2 (( $\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embedding). $S \in \mathbb{R}^{s, n}$ is an $(\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embedding if it is an $\varepsilon$-subspace embedding for any fixed $m$-dimensional subspace $\mathcal{V} \subset \mathbb{R}^{n}$ with probability at least $1-d$.

An example of a $(\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embedding is $S=\frac{1}{\sqrt{s}} G$ for a Gaussian matrix $G \in \mathbb{R}^{s, n}$ and

$$
s=\Omega\left(\varepsilon^{-2} \log m \log (1 / d)\right)
$$

this is a full matrix; see, e.g., 25. Sparse $(\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embeddings exist, including CountSketch, which consists of a single $\pm 1$ per column, where the row storing the entry and its sign are chosen uniformly at random [5, 30]. In order to be a $(\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embedding, the number of columns of the CountSketch matrix must satisfy

$$
\begin{equation*}
s \geq \frac{m^{2}+m}{\varepsilon^{2} d} \tag{4}
\end{equation*}
$$

see 16. Other popular $(\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embeddings include subsampled randomized Hadamard and Fourier transforms, and "sparse dimension reduction maps" 4, 17, though obtaining high performance with these is difficult, and the complexity of applying them is higher than CountSketch. We do not consider such embeddings in this paper.

## 3 Multisketching

Next, we consider the case of applying two sketch matrices one after the other; which is what we propose in this paper, generalizing the approach of 15, 27, where one application of a (sparse) CountSketch is followed by a Gaussian sketch. In these references though, there is no analysis of stability, as we do here. The main motivation for this approach is to be able to apply the dense Gaussian sketch to a smaller matrix, obtained after the application of a sparse sketch, thus obtaining similar good results at a fraction of the cost; see more details on this motivation in Section 3.2 .

We first present the algorithm randQR with two sketches, and then prove bounds similar to those in Corollary [2.2 for the case of two sketches.

Let $V \in \mathbb{R}^{n, m}$, and suppose $S_{1} \in \mathbb{R}^{s_{1}, n}$ and $S_{2} \in \mathbb{R}^{s_{2}, s_{1}}$ are $\left(\varepsilon_{1}, d_{1}, m\right)$ and $\left(\varepsilon_{2}, d_{2}, m\right)$ oblivious $\ell_{2}$-subspace embeddings, respectively. We define the Randomized Householder QR algorithm (randQR) in Algorithm 1 where we have used a Matlab function call.

```
Algorithm 1 Randomized Householder $\mathrm{QR}:[Q, R]=\operatorname{randQR}\left(V, S_{1}, S_{2}\right)$
    Input: Matrix $V \in \mathbb{R}^{n, m}$, sketch matrices $S_{1} \in \mathbb{R}^{s_{1}, n}, S_{2} \in \mathbb{R}^{s_{2}, s_{1}}$
    Output: $S_{2} S_{1}$-Orthogonal factor $Q \in \mathbb{R}^{n, m}$, Triangular factor $R \in \mathbb{R}^{m, m}$
    such that $Q R=V$.
    Apply sketches $W=S_{2} S_{1} V$
    Perform Householder QR: $\left[Q_{t m p}, R\right]=\operatorname{hhqr}(W)$
    Recover $S_{2} S_{1}$-orthogonal matrix: $Q=V R^{-1}$
```

Remark 3.1. In exact arithmetic, provided that $V \in \mathbb{R}^{n, m}$ is full rank, then randQR produces a matrix $Q$ that is $S_{2} S_{1}$-orthogonal; i.e., it is orthogonal with respect to $\left(S_{2} S_{1} \cdot, S_{2} S_{1} \cdot\right)$, or $\left(S_{2} S_{1} Q\right)^{T}\left(S_{2} S_{1} Q\right)=I$, as $S_{2} S_{1} Q=S_{2} S_{1} V R^{-1}=$ $W R^{-1}=Q_{t m p}$, where $Q_{t m p}$ is the orthogonal factor produced by the Householder $\mathrm{QR}$ factorization of $W=S_{2} S_{1} V$. Unlike traditional Householder $\mathrm{QR}$, even in exact arithmetic $V$ must have full rank, since Step 3 of Algorithm 1 requires $\operatorname{rank}(V)=\operatorname{rank}(R)=m$. In finite precision, intuition suggests that an inevitable requirement of randQR is that $V$ must be numerically full rank.

Proposition 3.1. Let $S_{1} \in \mathbb{R}^{s_{1}, n}$ be $a\left(\varepsilon_{1}, d_{1}, m\right)$ oblivious $\ell_{2}$-subspace embedding in $\mathbb{R}^{n}, S_{2} \in \mathbb{R}^{s_{2}, s_{1}}$ be a $\left(\varepsilon_{2}, d_{2}, m\right)$ oblivious $\ell_{2}$-subspace embedding in $\mathbb{R}^{s_{1}}$, generated independently. Let $\varepsilon_{L}=\varepsilon_{1}+\varepsilon_{2}-\varepsilon_{1} \varepsilon_{2}, \varepsilon_{H}=\varepsilon_{1}+\varepsilon_{2}+\varepsilon_{1} \varepsilon_{2}$, and $d=d_{1}+d_{2}-d_{1} d_{2}$. Then for any $m$-dimensional subspace $\mathcal{V} \subset \mathbb{R}^{n}$ and $\forall x \in \mathcal{V}$,

$$
\begin{equation*}
\sqrt{1-\varepsilon_{L}}\|x\|_{2} \leq\left\|S_{2} S_{1} x\right\|_{2} \leq \sqrt{1+\varepsilon_{H}}\|x\|_{2} \tag{5}
\end{equation*}
$$

with probability at least $1-d$.

Proof. $S_{2}$ is a $\left(\varepsilon_{2}, d_{2}, m\right)$ oblivious $\ell_{2}$-subspace embedding of $S_{1} \mathcal{V} \in \mathbb{R}^{s_{1}}$, and that if $x \in \mathcal{V}$, then $S_{1} x \in S_{1} \mathcal{V}$. Therefore, for any $x \in \mathcal{V}$,

$$
\sqrt{1-\varepsilon_{2}}\left\|S_{1} x\right\|_{2} \leq\left\|S_{2} S_{1} x\right\|_{2} \leq \sqrt{1+\varepsilon_{2}}\left\|S_{1} x\right\|_{2}
$$

Now, using (1), for $S_{1}$ and $\epsilon_{1}$, we have

$$
\begin{aligned}
\sqrt{1-\left(\varepsilon_{1}+\varepsilon_{2}-\varepsilon_{1} \varepsilon_{2}\right)}\|x\|_{2} & =\sqrt{\left(1-\varepsilon_{2}\right)\left(1-\varepsilon_{1}\right)}\|x\|_{2} \leq \sqrt{1-\varepsilon_{2}}\left\|S_{1} x\right\|_{2} \\
& \leq\left\|S_{2} S_{1} x\right\|_{2} \leq \sqrt{1+\varepsilon_{2}}\left\|S_{1} x\right\|_{2} \\
& \leq \sqrt{\left(1+\varepsilon_{2}\right)\left(1+\varepsilon_{1}\right)}\|x\|_{2}=\sqrt{1+\left(\varepsilon_{1}+\varepsilon_{2}+\varepsilon_{1} \varepsilon_{2}\right)}\|x\|_{2}
\end{aligned}
$$

with probability at least $\left(1-d_{1}\right)\left(1-d_{2}\right)=1-\left(d_{1}+d_{2}-d_{1} d_{2}\right)$.

If $S_{1}, S_{2}$ are $\varepsilon_{1}, \varepsilon_{2}$ embeddings respectively, then by Corollary 2.2 and Proposition 3.1 .

$$
\begin{align*}
\left(1+\varepsilon_{H}\right)^{-1 / 2} \sigma_{\min }\left(S_{2} S_{1} V\right) & \leq \sigma_{\min }(V) \leq \sigma_{\max }(V)  \tag{6}\\
& \leq\left(1-\varepsilon_{L}\right)^{-1 / 2} \sigma_{\max }\left(S_{2} S_{1} V\right)
\end{align*}
$$

and so,

$$
\begin{equation*}
\kappa(V) \leq \sqrt{\frac{1-\varepsilon_{L}}{1+\varepsilon_{H}}} \kappa\left(S_{2} S_{1} V\right) \tag{7}
\end{equation*}
$$

### 3.1 Algorithms and FLOP count

Let $Q$ be the $S_{2} S_{1}$-orthogonal factor from randQR. By Corollary 2.2 and the fact that $S_{2} S_{1} Q$ is orthogonal, $\kappa(Q) \leq \sqrt{\frac{1-\epsilon_{L}}{1+\epsilon_{H}}} \kappa\left(S_{2} S_{1} Q\right)=\sqrt{\frac{1-\epsilon_{L}}{1+\epsilon_{H}}}=O(1)$. Thus, if one re-orthogonalizes $Q$ using Cholesky QR (cholQR, Algorithm 2), it can be shown that the result has a loss of orthogonality on the order of machine

```
Algorithm 2 Cholesky QR: $[Q, R]=\operatorname{cholQR}(V)$
    Input: Matrix $V \in \mathbb{R}^{n, m}$
    Output: Orthogonal factor $Q \in \mathbb{R}^{n, m}$, Triangular factor $R \in \mathbb{R}^{m, m}$ such
    that $Q R=V$.
    : Compute Gram matrix $G=V^{T} V$
    : Perform Cholesky on $G$ : $R=\operatorname{chol}(G)$
    Recover orthogonal matrix: $Q=V R^{-1}$
```

```
Algorithm 3 Rand. Householder-Cholesky: $[Q, R]=$ rand_cholQR $\left(V, S_{1}, S_{2}\right)$
    Input: Matrix $V \in \mathbb{R}^{n, m}$, sketch matrices $S_{1} \in \mathbb{R}^{s_{1}, n}, S_{2} \in \mathbb{R}^{s_{2}, s_{1}}$
    Output: Orthogonal factor $Q \in \mathbb{R}^{n, m}$, Triangular factor $R \in \mathbb{R}^{m, m}$ such
    that $Q R=V$.
    1: Recover $S_{2} S_{1}$-orthogonal matrix $Q_{0}:\left[Q_{0}, R_{0}\right]=\operatorname{randQR}\left(V, S_{1}, S_{2}\right)$
    2: Perform Cholesky QR on $Q_{0}:\left[Q, R_{1}\right]=\operatorname{cholQR}\left(Q_{0}\right)$
    Return $R: R=R_{1} R_{0}$
```

precision [31. We take this approach, resulting in Algorithm 3] which we call rand_cholQR.

The computational cost of Step 2 of randQR (Algorithm 1) is negligible compared to steps 1 and 3 , since $W \in \mathbb{R}^{s_{2}, m}$ with $s_{2} \ll n$. The cost of Step 1 is dependent on the type of sketch matrices used. Suppose one replaces $S_{2} S_{1}$ with a single dense Gaussian sketch matrix $S \in \mathbb{R}^{s, n}$, which is conceptually simple, very efficient in parallel, but computationally expensive since it is fully dense. Then the computational cost of randQR and rand_cholQR (in FLOPs) are:

$$
\text { randQR FLOPs: } \underbrace{s m(2 n-1)}_{\text {Sketching }}+\underbrace{2 s m^{2}-\frac{2}{3} m^{3}}_{\text {Householder QR }}+\underbrace{n m^{2}}_{\text {Tri. solve }} \approx 2 n m s+n m^{2}
$$

$$
\text { rand_cholQR FLOPs: } \underbrace{2 n m s+n m^{2}}_{\text {randQR }}+\underbrace{2 n m^{2}}_{\text {cholQR }}+\underbrace{m^{2}(2 m-1)}_{\text {Matrix mult. }} \approx 2 n m s+3 n m^{2}
$$

Provided that $s=O(m)$, e.g., $s \approx 2 m$, then rand_cholQR FLOPs $\approx 7 n m^{2}$. What we propose instead is to perform cholQR twice, and this is rendered in Algorithm 4 which we call CholQR2, where in Step 3, we use the dense matrixmatrix product subroutine gemm. CholQR2 incurs a cost of

$$
\text { cholQR2 FLOPs: } \underbrace{2 n m^{2}}_{\text {cholQR }}+\underbrace{2 n m^{2}}_{\text {cholQR }}+\underbrace{m^{2}(2 m-1)}_{\text {Matrix mult. }} \approx 4 n m^{2}
$$

Comparing randQR (using a dense Gaussian sketch) to cholQR, their computational costs about the same asymptotically (as we discuss in the next section, the size of $s_{1}$ and $s_{2}$ depend of $m$, but are independent of $n$ ). In addition, the dominant costs incurred in steps 1 and 3 of randQR are nearly identical to cholQR, in the sense that both perform a product of tall and skinny matrices,

```
Algorithm 4 CholeskyQR2: $[Q, R]=\operatorname{cholQR2}(V)$
    Input: Full rank matrix $V \in \mathbb{R}^{n, m}$
    Output: Orthogonal factor $Q \in \mathbb{R}^{n, m}$, Triangular factor $R \in \mathbb{R}^{m, m}$
    Perform Cholesky QR on $W:\left[Q_{0}, R_{0}\right]=\operatorname{cholQR}(V)$
    Perform Cholesky QR on $Q_{0}:\left[Q, R_{1}\right]=\operatorname{cholQR}\left(Q_{0}\right)$
    Return $R: R=R_{1} R_{0}$
// gemm
```

followed by a triangular solve of a tall and skinny matrix, and therefore the algorithms should incur the same number of processor synchronizations. Moreover, rand_cholQR and cholQR2 simply build on these algorithms, adding passes of cholQR to matrices of the same size for both algorithms. Thus, in a large scale parallel setting, one can expect rand_cholQR to run slightly slower, but on the same order of runtime, as cholQR2, and scale in the same way. However, as we show in Section 4, rand_cholQR is significantly more stable.

### 3.2 Motivation for Multisketching

The Gaussian sketch performs the dense matrix-matrix multiply with the sketch matrix $S$ of dimension $s \times n$. Hence, we need to store and load the $s \times n$ sketch matrix. As shown in Section 3.1 the time to sketch the matrix becomes dominant in the total randQR factorization time (the memory access is expensive on a current computer architecture like a GPU).

One can reduce the sketching cost using a sparse sketch such as a CountSketch matrix 5. Since the CountSketch matrix has only one non-zero per column, the cost of applying the CountSketch matrix to $V \in \mathbb{R}^{n, m}$ is only $O(n m)$, and it only requires to store $O(n)$ numerical values. Additionally, CountSketch can be implemented using the sparse-matrix multiple-vector multiply (SpMM), whose optimized implementation is often available on specific architectures. A clever implementation can exploit the fact that applying the CountSketch matrix is equivalent to adding/subtracting subsets of rows of $V$, and can therefore be parallelized well using batched BLAS-1 kernels or a highly-optimized sparse linear algebra library. Hence, CountSketch could obtain high performance using only readily available linear algebra libraries. However, a CountSketch matrix requires $s=O\left(m^{2}\right)$ to maintain the $\varepsilon$-embedding properties, so one is left to factorize $W \in \mathbb{R}^{s, m}$ with Householder $\mathrm{QR}$, which incurs $O\left(m^{4}\right)$ FLOPs. In contrast, the Gaussian sketch ensures that $S$ is an $\varepsilon$-subspace embedding with $s=O(m)$, meaning the cost of the Householder QR factorization is only $O\left(m^{3}\right)$ FLOPs. Householder QR imposes high communication costs and does not parallelize well [7]. As a result, on current computers, it obtains much lower performance than the BLAS-3 operations like the dense matrix product (gemm), and these $O\left(m^{4}\right)$ FLOPs for Householder QR become a performance bottleneck for sufficiently large $m$.

Ideally, we want an embedding that offers low computational and storage costs like CountSketch, while returning a sketched matrix $W \in \mathbb{R}^{s, m}$ with $s=$
$O(m)$ like the Gaussian sketch does to avoid a performance bottleneck from Householder QR. This is possible by using a "Multisketch" framework with first a sparse CountSketch and then a Gaussian sketch. To see this, suppose $S_{1} \in \mathbb{R}^{s_{1}, n}$ is a CountSketch matrix with $s_{1}=\frac{m^{2}+m}{\varepsilon_{1}^{2} d_{1}}$, cf.(4), and suppose $S_{2} \in \mathbb{R}^{s_{2}, s_{1}}$ is a Gaussian sketch where $s_{2}=2 m$. By Proposition 3.1, for any $V \in \mathbb{R}^{n, m}$, with probability at least $1-d$

$$
\sqrt{1-\varepsilon_{L}}\|V\|_{2} \leq\left\|S_{2} S_{1} V\right\|_{2} \leq \sqrt{1+\varepsilon_{H}}\|V\|_{2}
$$

We split the computation of $W=S_{2} S_{1} V$ into two steps: first computing $W_{1}=S_{1} V$, then $W=S_{2} W_{1}$. Storing $S_{1}$ only requires $O(n)$ bytes of memory, and the sparse matrix product $W_{1}=S_{1} V$ costs $O(n m)$ FLOPs. The cost to compute $W=S_{2} W_{1}$ costs $O\left(m^{4}\right)$ FLOPs, but since the dense matrix product (gemm) obtains much higher performance than the Householder QR, this cost became negligible in our performance studies with a GPU. The storage of $S_{2}$ only requires $O\left(\mathrm{~m}^{3}\right)$ bytes of memory, and the Householder QR factorization of the $O(m) \times m$ matrix $W$ incurs negligble computational cost as well.

Moreover, the $O\left(n m+m^{4}\right)$ total FLOPs incurred using the multisketch framework can actually be lower than the $O\left(n m^{2}\right)$ FLOPs required to perform cholQR, making rand_cholQR sometimes cheaper than cholQR2 under the multisketch framework. Thus, the multisketch framework provides an avenue for an extremely efficient, stable QR factorization that can potentially outperform cholQR2 in terms of both stability and practical speed on modern parallel machines.

## 4 Error Bounds for Floating Point Arithmetic

Here, we present the main results of this work on theoretical properties (with high probability) of $\hat{Q}$ and $\hat{R}$ computed by randQR and rand_cholQR. For the sake of organization, we define a set of assumptions stating $V$ is numerically full rank (i.e., $\left.\kappa(V) \leq O\left(\mathbf{u}^{-1}\right)\right), n \gg m$, and that the sketch matrices $S_{1}, S_{2}$ simultaneously satisfy the subspace embedding properties, ensuring equations (11)-(3), (5)-(7) hold with probability at least $1-d$. We also impose an assumption that $\epsilon_{L}$ is sufficiently-but need not be too far-below 1 , to obtain a positive lower bound on $\sigma_{m}(\hat{Q})$ while maintaining as general of a result as possible.

Assumptions 4.1. Suppose $S_{1} \in \mathbb{R}^{s_{1}, m}$ and $S_{2} \in \mathbb{R}^{s_{2}, s_{1}}$ are $\left(\varepsilon_{1}, d_{1}, m\right)$ and $\left(\varepsilon_{2}, d_{2}, m\right)$ oblivious $\ell_{2}$-subspace embeddings respectively, generated independently. Define $d=d_{1}+d_{2}-d_{1} d_{2}, \varepsilon_{L}=\varepsilon_{1}+\varepsilon_{2}-\varepsilon_{1} \varepsilon_{2}, \varepsilon_{H}=\varepsilon_{1}+\varepsilon_{2}+\varepsilon_{1} \varepsilon_{2}$, where

$$
\varepsilon_{L} \in\left[0, \frac{221}{225}-\frac{4}{225} \varepsilon_{H}\right)
$$

Further, suppose $V \in \mathbb{R}^{n, m}$ with $m^{3 / 2} \leq n$ and $m \leq s_{2} \leq s_{1} \leq n$ satisfies cnm $\boldsymbol{u} \leq \frac{1}{12}$ and

$$
\delta:=\frac{12 \sqrt{1+\epsilon_{H}}}{\sqrt{1-\epsilon_{L}}} \cdot 1.1 c \boldsymbol{u} \sqrt{m}\left(1.1 n\left(1+1.1 c s_{2} m \boldsymbol{u}\right)+s_{2} m\right) \kappa(V) \leq 1
$$

Remark 3.1 indicates that in exact arithmetic, randQR yields a matrix $Q$ that is orthogonal with respect to $\left(S_{2} S_{1} \cdot, S_{2} S_{1} \cdot\right)$. We show next that provided $V$ has full numerical rank, then in floating point arithmetic, the orthogonality error of the matrix $\hat{Q}$ generated by randQR measured in $\left(S_{2} S_{1} \cdot, S_{2} S_{1} \cdot\right)$ is $O(\mathbf{u}) \kappa(V)$, and the factorization error is $O(\mathbf{u})\|V\|_{2}$. To maintain the flow of the exposition all proofs of the results in this section are postponed until the Appendices.

Theorem 4.1 (randQR Errors). Suppose Assumptions 4.1 are satisfied. With probability at least $1-d$, the $\hat{Q}, \hat{R}$ factors obtained with Algorithm $\square$ (randQR) satisfy

$$
\begin{gather*}
\left\|\left(S_{2} S_{1} \hat{Q}\right)^{T}\left(S_{2} S_{1} \hat{Q}\right)-I\right\|_{2} \leq 3 \delta \\
\|V-\hat{Q} \hat{R}\|_{2} \leq \frac{\delta}{9} \sigma_{m}(V) \tag{8}
\end{gather*}
$$

Similar to the analysis of the condition number of $Q$ generated by randQR in exact arithmetic in Section 3] we show next that provided that $V$ has full numerical rank, then $\hat{Q}$ generated by randQR in floating point arithmetic also has $\kappa(\hat{Q})=O(1)$.

Theorem 4.2 (Conditioning of randQR). Suppose Assumptions 4.1 are satisfied. Then with probability at least $1-d$, the $\hat{Q}$ matrix obtained with Algorithm 1 (randQR) has condition number $\kappa(\hat{Q})=O(1)$. In fact,

$$
\begin{equation*}
\kappa(\hat{Q}) \leq \frac{20}{15 \sqrt{\frac{1-\epsilon_{L}}{1+\epsilon_{H}}}-2} \tag{9}
\end{equation*}
$$

In the following result we show that rand_cholQR $(V)$ (Algorithm3) produces a factor $\hat{Q}$ that is orthogonal in the Euclidean inner product up to a factor of $O(\mathbf{u})$ and has a factorization error of $O(\mathbf{u})\|V\|_{2}$ for any numerically full rank $V$.

Theorem 4.3 (rand_cholQR Errors). Suppose Assumptions 4.1 are satisfied. Then with probability at least $1-d$, the $\hat{Q}, \hat{R}$ factors obtained with Algorithm 3 (rand_cholQR) has $O(\boldsymbol{u})$ orthogonality error and $O(\boldsymbol{u})\|V\|_{2}$ factorization error. In fact,

$$
\begin{gather*}
\left\|\hat{Q}^{T} \hat{Q}-I\right\|_{2} \leq \frac{2000}{\left(15 \sqrt{\frac{1-\epsilon_{L}}{1+\epsilon_{H}}}-2\right)^{2}}(n m+m(m+1)) \boldsymbol{u}  \tag{10}\\
\|V-\hat{Q} \hat{R}\|_{2} \leq\left(\frac{17 m^{3 / 2}}{\frac{7.5}{\sqrt{1+\epsilon_{H}}}-1}+\frac{1.5 c m}{\sqrt{1-\varepsilon_{L}}} \sqrt{1+\frac{2000(n m+m(m+1)) \boldsymbol{u}}{\left(15 \sqrt{\frac{1-\varepsilon_{L}}{1+\varepsilon_{H}}}-2\right)^{2}}}\right) \times \\
\left(\sqrt{m} \sqrt{1+\varepsilon_{H}}\|V\|_{2}+\frac{\sqrt{1-\varepsilon_{L}}}{12} \sigma_{m}(V) \delta\right) \sqrt{m} \boldsymbol{u}+\frac{\delta}{9} \sigma_{m}(V)
\end{gather*}
$$

Theorem4.2 guarantees randQR $(V)$ produces a well-conditioned $\hat{Q}$. We show next that rand_cholQR $(V)$ produces a factor $\hat{Q}$ with $\kappa(\hat{Q}) \approx 1$ (up to unit roundoff) for any numerically full rank $V$.

Theorem 4.4 (Conditioning of rand_cholQR). Suppose Assumptions 4.1 are satisfied. Then with probability at least $1-d$, the matrix $\hat{Q}$ obtained with Algorithm 1 satisfies $\kappa(\hat{Q}) \approx 1$. In fact,

$$
\begin{equation*}
\kappa(\hat{Q})<\sqrt{\frac{1+\frac{2000}{\left(15 \sqrt{\frac{1-\epsilon_{L}}{1+\epsilon_{H}}}-2\right)^{2}}(n m+m(m+1)) \boldsymbol{u}}{1-\frac{2000}{\left(15 \sqrt{\frac{1-\epsilon_{L}}{1+\epsilon_{H}}}-2\right)^{2}}(n m+m(m+1)) \boldsymbol{u}}} \tag{11}
\end{equation*}
$$

Furthermore, if $\frac{2000}{\left(15 \sqrt{\frac{1-\epsilon_{L}}{1+\epsilon_{H}}}-2\right)^{2}}(n m+m(m+1)) \boldsymbol{u}<\frac{1}{2}$, then

$$
\begin{equation*}
\kappa(\hat{Q})<1+\frac{4000}{\left(15 \sqrt{\frac{1-\epsilon_{L}}{1+\epsilon_{H}}}-2\right)^{2}}(n m+m(m+1)) \boldsymbol{u} \tag{12}
\end{equation*}
$$

Theorems 4.14 .4 correspond to multisketchings, that is, to the application of one sketch matrix after another. In the rest of the section, we recast our error bounds for a single sketch matrix, that is, for a single $(\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embedding for any $\varepsilon \in\left[0, \frac{221}{229}\right)$, covering nearly the entire range of possible $\varepsilon \in[0,1)$ for such embeddings.

Assumptions 4.2. Suppose $V \in \mathbb{R}^{n, m}, \varepsilon \in\left[0, \frac{221}{229}\right), S \in \mathbb{R}^{s, m}$ is a $(\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embedding, $m^{3 / 2} \leq n, m \leq s \leq n$, cnm $\boldsymbol{u} \leq \frac{1}{12}$, and

$$
\delta:=\frac{12 \sqrt{1+\epsilon}}{\sqrt{1-\epsilon}} \cdot 1.1 c \boldsymbol{u} \sqrt{m}(1.1 n(1+1.1 c \operatorname{csm} \boldsymbol{u})+s m) \kappa(V) \leq 1
$$

Corollary 4.1 (randQR Errors). Suppose Assumptions 4.2 are satisfied. Then with probability at least $1-d$, the $\hat{Q}, \hat{R}$ factors obtained with Algorithm (randQR) satisfy

$$
\begin{gathered}
\left\|(S \hat{Q})^{T}(S \hat{Q})-I\right\|_{2} \leq 3 \delta \\
\|V-\hat{Q} \hat{R}\|_{2} \leq \frac{\delta}{9} \sigma_{m}(V)
\end{gathered}
$$

Corollary 4.2 (Conditioning of randQR). Suppose Assumptions 4.2 are satisfied. Then with probability at least $1-d$, the matrix $\hat{Q}$ obtained with Algorithm 1 (randQR) satisfies

$$
\kappa(\hat{Q}) \leq \frac{20}{15 \sqrt{\frac{1-\epsilon}{1+\epsilon}}-2}
$$

Therefore, if $\varepsilon \leq 0.9$,

$$
\kappa(\hat{Q}) \leq 13.88
$$

Corollary 4.3 (rand_cholQR Errors). Suppose Assumptions 4.2 are satisfied. Then with probability at least $1-d$, the $\hat{Q}, \hat{R}$ factors obtained with Algorithm 3 (rand_cholQR) has $O(\boldsymbol{u})$ orthogonality error and $O(\boldsymbol{u})\|V\|_{2}$ factorization error. In fact,

$$
\begin{aligned}
&\left\|\hat{Q}^{T} \hat{Q}-I\right\|_{2} \leq \frac{2000}{\left(15 \sqrt{\frac{1-\epsilon}{1+\epsilon}}-2\right)^{2}}(n m+m(m+1)) \boldsymbol{u} \\
&\|V-\hat{Q} \hat{R}\|_{2} \leq\left(\frac{17 m^{3 / 2}}{\frac{7.5}{\sqrt{1+\epsilon_{H}}}-1}+\frac{1.5 c m}{\sqrt{1-\varepsilon_{L}}} \sqrt{1+\frac{2000(n m+m(m+1)) \boldsymbol{u}}{\left(15 \sqrt{\frac{1-\varepsilon_{L}}{1+\varepsilon_{H}}}-2\right)^{2}}}\right) \\
&\left(\sqrt{m} \sqrt{1+\varepsilon_{H}}\|V\|_{2}+\frac{\sqrt{1-\varepsilon_{L}}}{12} \sigma_{m}(V) \delta\right) \sqrt{m} \boldsymbol{u}+\frac{\delta}{9} \sigma_{m}(V)
\end{aligned}
$$

Corollary 4.4 (Conditioning of rand_cholQR). Suppose Assumptions 4.2 are satisfied. Then with probability at least $1-d$, the matrix $\hat{Q}$ obtained with Algorithm (rand_cholQR) satisfies $\kappa(\hat{Q}) \approx 1$. In fact,

$$
\kappa(\hat{Q})<\sqrt{\frac{1+\frac{2000}{\left(15 \sqrt{\frac{1-\epsilon}{1+\epsilon}}-2\right)^{2}}(n m+m(m+1)) \boldsymbol{u}}{1-\frac{2000}{\left(15 \sqrt{\frac{1-\epsilon}{1+\epsilon}}-2\right)^{2}}(n m+m(m+1)) \boldsymbol{u}}}
$$

Furthermore, if $\frac{2000}{\left(15 \sqrt{\frac{1-\epsilon}{1+\epsilon}}-2\right)^{2}}(n m+m(m+1)) \boldsymbol{u}<\frac{1}{2}$, then

$$
\kappa(\hat{Q})<1+\frac{4000}{\left(15 \sqrt{\frac{1-\epsilon}{1+\epsilon}}-2\right)^{2}}(n m+m(m+1)) \boldsymbol{u}
$$

## 5 Related Work

In the case of a single sketch matrix, the concept of sketching a tall-and-skinny matrix, computing its QR factorization, and then preconditioning the matrix with the resulting triangular factor like randQR is not new. The earliest appearance of such an algorithm was by Rokhlin and Tygert in 2008 [24] for solving overdetermined least squares problems, where they proposed a version of randQR with a column-pivoted $\mathrm{QR}$ factorization and a single subsampled randomized Hadamard transform sketch.

While this paper was being written, Balabanov proposed the "RCholeskyQR" and "RCholeskyQR2" methods in an unpublished manuscript [3], which are identical to what we refer to as randQR and rand_cholQR, respectively, in the case of a single $(\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embedding, and gave stability results similar to Corollary 4.2 of this paper. However, our results differ from Balabanov's, as ours impose no assumptions on the level of accuracy performed
by subroutines within the algorithm, meticulously deriving all bounds from existing roundoff error analysis of each subroutine. Additionally, Balabanov's work imposes a far stricter limit on the subspace embedding parameter $\epsilon \leq \frac{1}{2}$, while ours provides analysis up to $\epsilon<\frac{221}{229}$ for a $\left(\varepsilon, d, m\right.$ ) oblivious $\ell_{2}$-subspace embedding, which is nearly the theoretical upper limit of $\epsilon<1$ imposed by the theory in Section 2. This is significant, because stability guarantees for larger values of $\epsilon$ ensure high accuracy with smaller sketch matrices, resulting in a more computationally efficient algorithm.

Our results extend beyond a single $(\varepsilon, d, m)$ oblivious $\ell_{2}$-subspace embedding, and cover the more generalized case of two subspace embeddings (i.e., multisketch). Also, our work includes explicit analysis of the $S_{2} S_{1}$-orthogonality error of randQR, and the loss of orthogonality error in the standard Euclidean inner product of rand_cholQR.

Our work is novel in several ways. To our knowledge, this work is the first to propose a randomized $\mathrm{QR}$ algorithm with multiple sketches. The stability results in this paper improve upon and expand the existing stability analysis of randQR and rand_cholQR, and considers the multisketch case for the first time. Additionally, our experimental results are the first to demonstrate the performance of rand_cholQR in a parallel heterogeneous computing environment under any sketching framework, particularly in the multisketch case which allows the algorithm to sometimes run faster than the widely used high-performance cholQR2 algorithm. This tangibly demonstrates the potential of the multisketch rand_cholQR in exascale applications.

## 6 Numerical Experiments

We conducted numerical experiments with two goals in mind. First, we compare the performance of rand_cholQR with the performance of cholQR2, sCholQR3, and Householder QR on a latest GPU leveraging vendor-optimized libraries. Second, we empirically validate the bounds given in Section 4 and more generally, compare the stability of rand_cholQR to the stability of cholQR2, sCholQR3, and Householder QR.

### 6.1 Implementation Details

We implemented rand_cholQR, cholQR2, sCholQR3, and Householder QR in C++. To be portable to a GPU, we used the Kokkos Performance Portability Library [8] and Kokkos Kernels [23]. For our experiments on an NVIDIA GPU, we configured and built our code such that Kokkos Kernels calls NVIDIA's cuBLAS and cuSPARSE linear algebra libraries for optimized dense and sparse basic linear algebra routines [18, 21. To perform LAPACK routines that are not currently available natively within Kokkos Kernels (i.e., dgeqrf and dorgqr for computing the Householder QR factorization, and dpotrf for the Cholesky factorization), we directly called NVIDIA's cuSOLVER linear algebra library 2. 19. 20. Test results were obtained using Kokkos 3.7.01, Cuda 11.7.99, and

![](https://cdn.mathpix.com/cropped/2024_06_04_d134965606cab2bee739g-13.jpg?height=575&width=1230&top_left_y=409&top_left_x=445)

![](https://cdn.mathpix.com/cropped/2024_06_04_d134965606cab2bee739g-13.jpg?height=479&width=610&top_left_y=430&top_left_x=454)

(a) $n=1,000,000$ rows

![](https://cdn.mathpix.com/cropped/2024_06_04_d134965606cab2bee739g-13.jpg?height=485&width=610&top_left_y=427&top_left_x=1061)

(b) $n=10,000,000$ rows

Figure 1: Runtimes (in seconds) of QR factorizations of $V$ with $\kappa(V)=10^{6}$ for a fixed number of rows as the number of columns vary.

GCC 7.2.0 on an AMD EPYC 7742 64-Core $2.25 \mathrm{GHz}$ CPU with a NVIDIA A100-SXM4 40GB GPU. All computations were done in double precision, so $\mathbf{u}=2^{-52} \approx 10^{-16}$.

We tested a variety of sketching strategies. The simplest was the case of a Gaussian sketch $S=\frac{1}{\sqrt{s}} G \in \mathbb{R}^{s, n}$, which were generated within a parallel for loop. The sketch size chosen for a Gaussian to embed $V \in \mathbb{R}^{n, m}$ was $s=\lceil 74.3 \log (m)\rceil$, which can be shown to produce a $(0.49,1 / m, m)$ oblivious $\ell_{2}$-subspace embedding 1, Lemma 4.1]. To test using CountSketch, we explicitly constructed a sparse matrix and applied the sketch using a sparse-matrix vector product. The sketch size used to embed $V \in \mathbb{R}^{n, m}$ with a $S \in \mathbb{R}^{s, n}$ CountSketch matrix was $s=\left\lceil 8.24\left(m^{2}+m\right)\right\rceil$, which can be shown to be a $(0.9,0.15, m)$ oblivious $\ell_{2}$-subspace embedding [16, Theorem 1].

In our implementation of multisketching, we chose $S_{1} \in \mathbb{R}^{s_{1}, m}$ as a CountSketch described above with $\varepsilon_{1}=0.9$, and $S_{2} \in \mathbb{R}^{s_{2}, s_{1}}$ a Gaussian sketch with $s_{2}=\left\lceil 74.3 \log \left(s_{1}\right)\right\rceil$ giving $\varepsilon_{2}=0.49$. Thus, $S_{2} S_{1}$ produced an embedding with $\varepsilon_{L} \approx 0.9490, \varepsilon_{H} \approx 1.8310$, and $d \approx 0.15$. It is easily verified that $S_{2} S_{1}$ is in line with Assumptions 4.1, and that both of $S_{1}$ and $S_{2}$ satisfy Assumptions 4.2 ensuring the analysis in Section 4 is relevant to the experiments. Runtimes of rand_cholQR did not include the time to generate the sketch, as this was assumed to be a fixed overhead time.

### 6.2 Performance and Numerical Results

Figure 1 shows the runtimes of each QR method for test problems with $n=10^{6}$ and $n=10^{7}$ rows, and $m=10-100$ columns. Since cholQR2 is typically expected to be the fastest algorithm, Table 1 shows the relative slowdown of each QR method compared to cholQR2 averaged across each data point from Figure 1. Table 1and Figure 1indicate that cholQR2 is indeed the fastest method in

Average Slowdown compared to cholQR2

|  | $1,000,000$ rows | $10,000,000$ rows |
| :---: | ---: | ---: |
| rand_cholQR: Gauss Sketch | $21.5 \%$ | $23.1 \%$ |
| rand_cholQR: CountSketch | $19.5 \%$ | $5.3 \%$ |
| rand_cholQR: multisketch | $7.1 \%$ | $4.9 \%$ |
| sCholQR3 | $35.9 \%$ | $33.8 \%$ |
| Householder | $84.8 \%$ | $83.4 \%$ |

Table 1: Average slowdowns of each QR algorithm compared to cholQR2, taken from experiments shown in Figure 1. Smaller values indicate faster runtimes. Slowdowns for each $\mathrm{QR}$ algorithm are measured as $(\mathrm{QR}$ algorithm runtime cholQR2 runtime)/(cholQR2 runtime) $\times 100 \%$

general, while multisketch rand_cholQR performs the closest to cholQR2, averaging only a 4.9-7.1\% slowdown. Additionally, Figure 1 shows that in some cases, multisketch rand_cholQR actually outperforms cholQR2, specifically for $n=10^{6}$ rows and $m=70$ columns, and for $n=10^{7}$ rows and $m=70-80$ columns. The most notable result is that for $n=10^{7}$ rows and $m=70$ columns, multisketch rand_cholQR is $4 \%$ faster than cholQR2. Multisketch rand_cholQR is significantly faster than sCholQR3, as evidenced by Figure 1, and both algorithms have the same $O(\mathbf{u}) \kappa(V)<1$ stability requirement.

Figure 2 shows the orthogonalization error $\left\|I-\hat{Q}^{T} \hat{Q}\right\|_{F}$ and the relative factorization error $\|V-\hat{Q} \hat{R}\|_{F} /\|V\|_{F}$ for condition number $\kappa(V) \in\left[1,10^{16}\right]$. The results demonstrate that rand_cholQR maintains $O(\mathbf{u})$ orthogonality error and $O(\mathbf{u})\|V\|_{2}$ factorization error while $\kappa(V)<O(\mathbf{u})$, as predicted by Theorem 4.3, and is more robust than cholQR2 and sCholQR3. In practice, it appears that rand_cholQR is stable even when $V$ is numerically rank-deficient. In summary, Figures 1 and 2 demonstrate that multisketch rand_cholQR significantly improves the robustness of cholQR2 and sCholQR3 at little to no cost, therefore making rand_cholQR a superior high-performance QR algorithm.

## 7 Conclusions

The results in Section 4 indicate that rand_cholQR using one or two sketch matrices orthogonalizes any numerically full-rank matrix $V$ up to $O(\mathbf{u})$ error. This is a significant improvement over CholeskyQR2, which requires $\kappa(V)<O\left(\mathbf{u}^{-1 / 2}\right)$ to ensure a stable factorization. Our results for a single sketch apply for any $\varepsilon$-embedding with $\varepsilon \in\left[0, \frac{221}{229}\right)$, covering nearly the entire possible range for $\varepsilon$-embeddings.

Our performance results in Section 6.2 indicate that the significantly better stability properties of rand_cholQR over cholQR2 come at virtually no increase in the factorization time on a modern GPU. Additionally, rand_cholQR is theoretically just as stable and in practice more stable than sCholQR3, while being[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_d134965606cab2bee739g-15.jpg?height=561&width=1242&top_left_y=408&top_left_x=447)

![](https://cdn.mathpix.com/cropped/2024_06_04_d134965606cab2bee739g-15.jpg?height=474&width=607&top_left_y=430&top_left_x=455)

(a) Orthogonality Error

![](https://cdn.mathpix.com/cropped/2024_06_04_d134965606cab2bee739g-15.jpg?height=480&width=610&top_left_y=427&top_left_x=1061)

(b) Relative Factorization Error

Figure 2: Orthogonality (left) and relative factorization error (right) of the $\mathrm{QR}$ factorization of a matrix $V$ with varying condition number. To explicitly control $\kappa(V), V:=L \Sigma R^{T} \in \mathbb{R}^{n, m}$ using random orthogonal matrices $L, R$, and a diagonal $\Sigma$ with log-equispaced entries in the range $\left[\kappa^{-\frac{1}{2}}(V), \kappa^{\frac{1}{2}}(V)\right]$. Indicated by a large dot, lines for cholQR2 and sCholQR3 end at $\kappa(V)=10^{8}$ and $\kappa(V)=10^{12}$ respectively, as the methods fail beyond these points.

substantially faster. This is due to the fact that rand_cholQR and cholQR2 incur the same number of processor synchronizations, while leveraging mostly BLAS-3 or optimized sparse matrix-vector routines for most of the required computation. In fact, rand_cholQR can perform better than cholQR2 when using the multisketch framework. Of the sketching strategies considered, the multisketch framework is the most advantageous, likely because it requires little additional storage compared to cholQR2, and applying the sketches in this framework is extremely cheap.

Future work includes applying rand_cholQR to Krylov subspace methods that require tall-and-skinny QR factorizations, particularly block 12, 22, $s$-step [6, 14, 28, and enlarged Krylov methods 11], and further investigations into efficient multisketching implementations on a GPU, as our analysis is amenable to any multisketching strategy (not just a CountSketch followed by a dense Gaussian). In particular, applying the CountSketch matrix could potentially be optimized better than using a sparse-matrix vector multiplication by using a custom routine to add/subtract subsets of randomly selected rows in parallel using batched BLAS-1 routines, which should be investigated. Additionally, the performance of randQR and rand_cholQR using dense Rademacher sketch matrices in place of dense Gaussian sketches as in 1 should be investigated, as Rademacher sketches impose far lower storage requirements than a Gaussian sketch and can be generated much more efficiently.

## 8 Acknowledgements

Sandia National Laboratories is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DENA-0003525. This work was in part supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration.

## Statements and Declarations

Competing interests. The authors declare no competing interests.

## References

[1] Dimitris Achlioptas. Database-friendly random projections: JohnsonLindenstrauss with binary coins. Journal of Computer and System Sciences, 66:671-687, 2003. Special Issue on PODS 2001.

[2] Eric Anderson, Zhaojun Bai, Christopher Bischof, S. Blackford, James W. Demmel, Jack J. Dongarra, Jeremy Du Croz, Aanne Greenbaum, Sven Hammarling, A. McKenney, and Dannny C. Sorensen. LAPACK Users' Guide. Society for Industrial and Applied Mathematics, Philadelphia, Third edition, 1999.

[3] Oleg Balabanov. Randomized Cholesky QR factorizations, 2022. arXiv:2210.09953.

[4] Oleg Balabanov and Laura Grigori. Randomized Gram-Schmidt process with application to GMRES. SIAM Journal on Scientific Computing, 44:A1450-A1474, 2022.

[5] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In P. Widmayer, S. Eidenbenz, F. Triguero, R. Morales, R. Conejo, and M. Hennessy, editors, Automata, Languages and Programming, volume 2380 of Lecture Notes in Computer Science, pages 693-703, Berlin, Heidelberg, 2002. Springer. ICALP 2002.

[6] Anthony T. Chronopoulos and C. William Gear. s-step iterative methods for symmetric linear systems. Journal of Computational and Applied Mathematics, 25:153-168, 1989.

[7] James W. Demmel, Laura Grigori, Maek Hoemmen, and Julien Langou. Communication-optimal parallel and sequential QR and LU factorizations. SIAM Journal on Scientific Computing, 34:A206-A239, 2012.

[8] H. Carter Edwards, Christian R. Trott, and Daniel Sunderland. Kokkos: Enabling manycore performance portability through polymorphic memory access patterns. Journal of Parallel and Distributed Computing, 74:3202$3216,2014$.

[9] Takeshi Fukaya, Ramaseshan Kannan, Yuji Nakatsukasa, Yusaku Yamamoto, and Yuka Yanagisawa. Shifted Cholesky QR for computing the QR factorization of ill-conditioned matrices. SIAM Journal on Scientific Computing, 42(1):A477-A503, 2020.

[10] Takeshi Fukaya, Yuji Nakatsukasa, Yuka Yanagisawa, and Yusaku Yamamoto. CholeskyQR2: A simple and communication-avoiding algorithm for computing a tall-skinny QR factorization on a large-scale parallel system. In 2014 5th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems, pages 31-38, Los Alamitos, CA, 2014. IEEE Computer Society.

[11] Laura Grigori, Sophie Moufawad, and Frederic Nataf. Enlarged Krylov subspace conjugate gradient methods for reducing communication. SIAM Journal on Matrix Analysis and Applications, 37:744-773, 2016.

[12] Martin H. Gutknecht. Block Krylov subspace methods for linear systems with multiple right-hand sides: An introduction. In Abul Hasan Siddiqi, Iain S. Duff, and Ole Christensen, editors, Modern Mathematical Models, Methods and Algorithms for Real World Systems, chapter 10, pages 420447. Anamaya Publishers, New Dehli, 2006.

[13] Nicholas J. Higham. Accuracy and Stability of Numerical Algorithms. Society for Industrial and Applied Mathematics, Philadelphia, second edition, 2002 .

[14] Mark Hoemmen. Communication-avoiding Krylov subspace methods. PhD thesis, EECS Department, University of California, Berkeley, 2010.

[15] Michael Kapralov, Vamsi Potluru, and David Woodruff. How to fake multiply by a Gaussian matrix. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48, pages 2101-2110. Proceedings of Machine Learning Research, 2016.

[16] Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression. In Proceedings of the Forty-Fifth Annual ACM Symposium on Theory of Computing, STOC '13, page 91-100, New York, 2013. Association for Computing Machinery.

[17] Yuji Nakatsukasa and Joel A. Tropp. Fast \& accurate randomized algorithms for linear systems and eigenvalue problems, 2021. arXiv:2111.00113.

[18] NVIDIA. cuBLAS documentation.https://docs.nvidia.com/cuda/cublas/index.html. Accessed: 2023-06-21.

[19] NVIDIA. CUDA toolkit documentation. https://docs.nvidia.com/cuda/. Accessed: 2023-06-21.

[20] NVIDIA. cuSOLVER documentation. https://docs.nvidia.com/cuda/cusolver/index.html. Accessed: 2023-06-21.

[21] NVIDIA. cuSPARSE documentation. https://docs.nvidia.com/cuda/cusparse/index.html. Accessed: 2023-06-21.

[22] Dianne P. O'Leary. The block conjugate gradient algorithm and related methods. Linear Algebra and its Applications, 29:293-322, 1980.

[23] Sivasankaran Rajamanickam, Seher Acer, Luc Berger-Vergiat, Vinh. Q. Dang, Nathan D. Ellingwood, Evan Harvey, Brian Kelley, Christian R. Trott, Jeremy J. Wilke, and Ichitaro Yamazaki. Kokkos kernels: Performance portable sparse/dense linear algebra and graph kernels, 2021. arxiv:2103.11991.

[24] Vladimir Rokhlin and Mark Tygert. A fast randomized algorithm for overdetermined linear least-squares regression. Proceedings of the National Academy of Sciences of the United States of America, 105:13212-13217, 2008 .

[25] Tamás Sarlós. Improved approximation algorithms for large matrices via random projections. In 47 th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06), pages 143-152, Los Alamitos, CA, 2006. IEEE Computer Society.

[26] Aleksandros Sobczyk and Efstratios Gallopoulos. Estimating leverage scores via rank revealing methods and randomization. SIAM Journal on Matrix Analysis and Applications, 42:199-1228, 2021.

[27] Aleksandros Sobczyk and Efstratios Gallopoulos. pylspack: Parallel algorithms and data structures for sketching, column subset selection, regression, and leverage scores. ACM Transactions on Mathematical Software, $48: 1-27,2022$.

[28] Homer F. Walker. Implementation of the GMRES method using Householder transformations. SIAM Journal on Scientific and Statistical Computing, 9:152-163, 1988.

[29] Hermann Weyl. Das asymptotische Verteilungsgesetz der Eigenwerte linearer partieller Differentialgleichungen (mit einer Anwendung auf die Theorie der Hohlraumstrahlung). Mathematische Annalen, 71:441-479, 1912.

[30] David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in Theoretical Computer Science, 10:1-157, 2014.

[31] Yusaku Yamamoto, Yuji Nakatsukasa, Yuka Yanagisawa, and Takeshi Fukaya. Roundoff error analysis of the Cholesky QR2 algorithm. Electronic Transactions on Numerical Analysis, 44:306-326, 2015.
