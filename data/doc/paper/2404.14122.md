# Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice? 

Dawei Zhu ${ }^{1}$ Pinzhen Chen ${ }^{2}$ Miaoran Zhang ${ }^{1}$<br>Barry Haddow ${ }^{2}$ Xiaoyu Shen ${ }^{3}$ Dietrich Klakow ${ }^{1}$<br>${ }^{1}$ Saarland University, Saarland Informatics Campus<br>${ }^{2}$ University of Edinburgh<br>${ }^{3}$ Eastern Institute of Technology<br>\{dzhu, mzhang\}@lsv.uni-saarland.de pinzhen.chen@ed.ac.uk


#### Abstract

Traditionally, success in multilingual machine translation can be attributed to three key factors in training data: large volume, diverse translation directions, and high quality. In the current practice of fine-tuning large language models (LLMs) for translation, we revisit the importance of all these factors. We find that LLMs display strong translation capability after being fine-tuned on as few as 32 training instances, and that fine-tuning on a single translation direction effectively enables LLMs to translate in multiple directions. However, the choice of direction is critical: fine-tuning LLMs with English on the target side can lead to task misinterpretation, which hinders translations into non-English languages. A similar problem arises when noise is introduced into the target side of parallel data, especially when the target language is well-represented in the LLM's pre-training. In contrast, noise in an underrepresented language has a less pronounced effect. Our findings suggest that attaining successful alignment hinges on teaching the model to maintain a "superficial" focus, thereby avoiding the learning of erroneous biases beyond translation.


## 1 Introduction

Large language models (LLMs) have reached heights in various NLP tasks (Radford et al., 2019; Brown et al., 2020; Touvron et al., 2023; Jiang et al., 2023). Supervised fine-tuning (SFT (Ouyang et al., 2022), also called instruction tuning or simply fine-tuning in some literature) further prepares these models for better generalization and reliability in downstream tasks by training on inputoutput data combined with task instructions in natural languages (Sanh et al., 2022; Wei et al., 2022; Mishra et al., 2022). In this research direction, various works have studied the "scaling up" of data size, number of languages, etc, and demonstrated strong results (Chung et al., 2022; Muennighoff et al., 2023). On the other hand, more recent papers also embraced the philosophy of "less is more" by curating a small set of high-quality training instances, claiming a "superficial alignment hypothesis" (Zhou et al., 2023) with similar findings by others (e.g. Chen et al., 2024a).

This work further investigates the role of SFT data in preparing LLMs for machine translation (MT), a cross-lingual generation task with high demands in practical domains. Prior research has found fine-tuning to improve translation performance (Zhang et al., 2023c) and more recent works also integrated continued pre-training with more data to provide further improvement (Xu et al., 2024a; Alves et al., 2024). Nonetheless, the feasibility of "less is more" in translation is rather underexplored. In translation prompting, researchers have suggested that a model's translation capability can be attributed to the bilingual signals exposed during pre-training (Briakou et al., 2023) and task recognition in LLM layers (Sia et al., 2024), hinting that the translation task has been picked up during pre-training. A natural question follows: can we put reduced effort into data?

From a practical perspective, we squeeze the translation SFT data to a mere size of 32 or the translation direction to 1 for multilingual translation, for which we believe LLMs already possess a strong pre-trained foundation in multilingual understanding and generation. Beyond quantity and language diversity, we perform SFT on data with inferior quality which could be common for lowerresourced languages. To summarize, our analysis is grounded in the task of MT, with "scaling down" in mind. In multiple dimensions-data size (\$3.2), translation direction (§3.3, §3.4), and data quality (\$3.5)—our findings verify, complement, and refine the existing superficial alignment hypothesis:

1. 32 data instances successfully enable an LLM to translate in 11 directions. More data still
helps but the returns diminish.
2. Data in a single translation direction can effectively align an LLM to translate to and from multiple directions. Yet, it is crucial to pick the right direction-we recommend not to place English on the target side.
3. Injecting noise into the SFT data results in different patterns concerning language exposure. An LLM can easily overfit to the noise patterns in high-resource languages while it is more robust to data noise in low-resource languages.

## 2 Preliminaries

### 2.1 Supervised fine-tuning

In this work, we perform SFT to prepare pre-trained LLMs for MT. Let $S$ denote a source input and $T=\left[t_{1}, t_{2}, \ldots, t_{|T|}\right]$ denote a target-side reference. We start with placing the input into a prompt template by applying $\mathcal{I}(\cdot)$ to $S$. For each training instance, the instruction template is randomly selected from a pre-defined pool. We fine-tune an LLM parameterized by $\theta$ by optimizing the loglikelihood:

$$
\begin{aligned}
\mathcal{L}_{S F T}(\mathcal{I}(S), T ; \theta) & =-\log P(T \mid \mathcal{I}(S) ; \theta) \\
& =-\log \prod_{k=1}^{|T|} P\left(t_{k} \mid t_{<k}, \mathcal{I}(S) ; \theta\right) \\
& =-\sum_{k=1}^{|T|} \log P\left(t_{k} \mid t_{<k}, \mathcal{I}(S) ; \theta\right)
\end{aligned}
$$

### 2.2 Superficial alignment hypothesis

Zhou et al. (2023) claim that a model's knowledge and capabilities are acquired almost entirely during pre-training, and the effect of alignment tuning might be "superficial", in that it teaches the model the format for interacting with users. This idea is further supported by recent works (Lin et al., 2024; Ghosh et al., 2024). However, to what extent this applies to multilingual translation remains underexplored. To bridge this gap, We conduct a series of controlled experiments to fine-tune LLMs for translation, complementing previous research across three dimensions. First, we study the parallel data efficiency in the era of LLMs, aiming to determine the minimum data needed for effective model alignment. Second, we explore the scope of alignment by probing whether aligning one translation direction influences other directions. Finally, we investigate how varying data quality affects the LLMs' behavior in aligning translation tasks.

## 3 Experiments and Results

### 3.1 Experimental setup

Training. By default, we take the test sets from WMT17 to WMT20 as our parallel training data (Bojar et al., 2017, 2018; Barrault et al., 2019, 2020); we also use the development sets in WMT21 (Akhbardeh et al., 2021) for training if a language pair of interest is not available in earlier years. The specific training data configurations will be detailed in the individual subsequent sections. The test sets from WMT21 are used for validation. Detailed data statistics can be found in Appendix F.1. The LLM we use for SFT is the base version of Llama-2 7B (Touvron et al., 2023). When performing SFT, we use a learning rate of $5 \mathrm{e}-6$, an effective batch size of 64 , and linear learning rate scheduling with a warmup ratio of 0.1 . We select the model checkpoint based on COMET scores on the validation sets. ${ }^{1}$ To form the model input for SFT, we feed the source sentence into the Alpaca prompt template (Taori et al., 2023), supplementing it with a translation instruction that is randomly selected from a pool of 31 instructions. Refer to Table 4 in the appendix for the complete list of instructions.

Evaluation. We primarily evaluate the models on the WMT22 test sets (Kocmi et al., 2022) covering 11 translation directions: en $\leftrightarrow$ cs, en $\leftrightarrow$ de, en $\leftrightarrow$ jp, en $\leftrightarrow \mathrm{ru}$, en $\leftrightarrow$ zh, and en $\rightarrow$ hr. ${ }^{2}$ Languages in these 11 directions are explicitly included in Llama-2's pre-training corpus. In Section 3.4, we extend our evaluation to translation directions involving middle and low resource languages: Icelandic and Hausa (i.e., en $\leftrightarrow$ is, en $\leftrightarrow$ ha), and this data comes from WMT21's test set. At inference time, a fixed translation instruction is applied to all test examples. We set the maximum generation length to 256 tokens and use beam search with a beam size of 4 for decoding. We use reference-based COMET22 (Rei et al., 2020) ${ }^{3}$ and BLEU (Papineni et al., 2002) as the evaluation metrics. Refer to detailed[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_214af474d081dbaf3499g-03.jpg?height=396&width=1514&top_left_y=236&top_left_x=268)

Figure 1: Performance comparison between instruction-tuned baselines and Llama-2 fine-tuned with different training data sizes. Average COMET (left) and BLEU (right) scores across 11 translation directions are presented. For training data sizes of 1 and 3, ICL is applied, marked with an asterisk “*"; otherwise, we perform SFT. With only 32 training examples for SFT, Llama-2 outperforms general-purpose, instruction-tuned baselines. Base.: instructiontuned baseline models. See performance metrics for the 11 individual translation directions in Appendix A.

software versions in Appendix F.3.

### 3.2 How much training data is required to enable LLMs for translation?

Recent works in machine translation suggest that pre-trained LLMs require significantly less parallel data for fine-tuning (via SFT), compared to training conventional translation models from scratch. However, the SFT process in these works still operates with an order of $10^{5}$ parallel samples (Jiao et al., 2023; Zhang et al., 2023c; Zeng et al., 2024; Xu et al., 2024a, i.a.), without a clear justification for selecting this specific data size and source. This raises a pivotal question, inspired by the recently proposed "superficial alignment hypothesis" (Zhou et al., 2023): Is SFT mainly a method for superficially aligning LLMs for translation tasks? If so, what is the actual minimal amount of data required to achieve effective "alignment"?

Setup. We fine-tune Llama-2 7B using different numbers of training samples and evaluate the multilingual translation performance of the resulting models. We collect training data covering 10 translation directions: en $\leftrightarrow\{\mathrm{cs}$, de, jp, ru, zh $\}$. The training data sourced from WMT17-20 contains a total of 74,623 parallel examples. Note that the training samples across translation directions are not evenly distributed. To create training sets of varying sizes, we subsample the original data into subsets that are powers of 2 , starting from $32\left(2^{5}\right)$ and ending with $4096\left(2^{12}\right)$; larger subsets always contain smaller subsets. To ensure balanced language representation in our subsets, we distribute samples as evenly as possible among the language pairs. ${ }^{4}$ We refer to the fine-tuned model as SFTMT. Considering LLMs are capable of performing translation through prompting too, we compare SFT-MT with 1- and 3-shot in-context learning (ICL), denoted as ICL-MT. For ICL, we randomly select demonstrations from the training set in the test direction for each test sentence. We do not consider Llama-2's zero-shot performance because, although it sometimes produces acceptable translations at the beginning, it often does not stop generating, which makes it difficult to accurately evaluate its performance. Lastly, as demonstrated in (Zhu et al., 2024), established LLM-based chatbots trained on diverse NLP tasks also serve as strong translation systems. Therefore, we compare our models with general-purpose, instruction-tuned LLMs (IT-LLM), including Vicuna-v1.5-7b (Chiang et al., 2023), Mistral-7b (Jiang et al., 2023), and Llama-2-7b-chat (Touvron et al., 2023). ${ }^{5}$

Results. Figure 1 illustrates the effect of varying training sample sizes on translation performance. In both 1- and 3-shot cases, ICL-MT underperforms IT-LLM baselines like Llama-2-7b-chat despite sharing the same foundation model, indicating that a few in-context demonstrations may not effectively align Llama-2 for translation. However, performance significantly improves as soon as Llama2 is fine-tuned with just 32 samples, outperforming all three IT-LLM baselines in both COMET and BLEU metrics. This suggests that a handful of high-quality parallel data can effectively specialize the model into a performant translation system. Increasing parallel data further boosts per-[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_214af474d081dbaf3499g-04.jpg?height=619&width=1380&top_left_y=233&top_left_x=338)

Figure 2: Normalized COMET score (as a $\%$ of performance from fine-tuning on an equivalent sized dataset of all 10 directions) resulted from varying combinations of train and test translation directions. In most cases, Llama-2 fine-tuned on a single translation direction can effectively translate across other directions, achieving performance comparable to models trained on all directions, with a few exceptions when trained on $\mathrm{X} \rightarrow$ en but tested on en $\rightarrow \mathrm{X}$. Performance measured in BLEU score is provided in Appendix B.

formance, though with diminishing returns: the COMET score rises by an average of 2 points when expanding from 32 to 1024 samples, but only by 0.5 points when increasing further from 1024 to $75 \mathrm{~K}$ samples (the full training set). Given that it is unlikely that these 32 training samples "teach" Llama-2 new translation skills, this shows strong evidence that superficial alignment applies to MT. In general, we find that around $1 \mathrm{~K}$ samples are a sweet spot for data efficiency. In summary, effective translation alignment begins with minimal training data, revealing less is good alignment and more is better with diminishing gains.

### 3.3 Do we need to include all directions?

In the preceding section, we follow the traditional practice in multilingual MT by including multiple translation directions during training. However, the observation that Llama-2 may efficiently adapt to translation tasks with only a few dozen samples leads us to reconsider the necessity of including samples from all these directions. Specifically, could training with examples from just a single translation direction be sufficient to align LLMs for multilingual translation?

Setup. We explore six training configurations, each focusing on a single translation direction: de $\rightarrow$ en, zh $\rightarrow$ en, en $\rightarrow$ de, en $\rightarrow$ zh, $\mathrm{fr} \rightarrow$ de, and $\mathrm{de} \rightarrow \mathrm{fr}$. These configurations include cases where English appears on the source side, the target side, as well as settings with English excluded, to investigate if specific languages have a different impact on the overall performance. The training size is set to 1024 for SFT. Evaluations are conducted across the same 11 test directions as used in the previous section. Additionally, we explore similar settings in ICL, where we present demonstrations with translation directions that do not match those used in evaluations, to determine if the mechanisms of both SFT and ICL exhibit similarities. Lastly, we conduct a joint evaluation, progressively expanding both the training size and the range of covered translation directions to understand the combined effect of these factors.

SFT results. Figure 2 demonstrates the normalized performance of Llama-2 when fine-tuned in various single directions. Remarkably, training with just one direction enables Llama-2 to translate between multiple languages. For instance, after fine-tuning on de $\rightarrow$ en or $\mathrm{zh} \rightarrow$ en, the model can translate from all considered languages to English with at least $98.6 \%$ efficacy relative to training on all directions. Similarly, the model fine-tuned on en $\rightarrow$ de, en $\rightarrow$ zh, fr $\rightarrow$ de or de $\rightarrow$ fr also demonstrate only a slight performance decline when translating from English. Notable declines are observed in two scenarios: (1) trained to translate to English and evaluated on translating to non-English; and (2) trained to translate to non-English and evaluated on translating to English. ${ }^{6}$ Of these two scenarios, scenario 1 exhibits a much larger performance drop. The fact that both scenarios involve a[^2]

| Evaluation on de $\rightarrow$ en |  |  |  |  |  |
| :---: | :---: | ---: | ---: | ---: | ---: |
| demo <br> lang | 1-shot |  |  | 3-shot |  |
|  | COMET | BLEU |  | COMET | BLEU |
| de $\rightarrow$ en | 73.47 | 19.7 |  | 75.04 | 22.4 |
| en $\rightarrow$ de | 55.96 | 7.3 |  | 44.39 | 3.5 |
| de $\rightarrow$ fr | 66.35 | 12.1 |  | 64.61 | 17.6 |
| fr $\rightarrow$ de | 58.06 | 7.8 |  | 57.13 | 10.5 |
| zh $\rightarrow$ en | 56.66 | 10.7 |  | 54.82 | 7.1 |
| en $\rightarrow$ zh | 51.30 | 7.8 |  | 56.87 | 1.8 |


| Evaluation on en $\rightarrow$ de |  |  |  |  |  |
| :---: | :---: | ---: | :---: | :---: | ---: |
| demo | 1-shot |  |  | 3-shot |  |
|  | COMET | BLEU |  | COMET | BLEU |
| en $\rightarrow$ de | 67.37 | 10.5 |  | 69.80 | 14.3 |
| $\mathrm{de} \rightarrow \mathrm{en}$ | 57.83 | 8.7 |  | 45.54 | 5.0 |
| $\mathrm{en} \rightarrow \mathrm{zh}$ | 59.76 | 9.5 |  | 59.53 | 8.4 |
| $\mathrm{zh} \rightarrow \mathrm{en}$ | 47.31 | 4.5 |  | 49.24 | 5.0 |
| $\mathrm{fr} \rightarrow \mathrm{de}$ | 59.36 | 8.6 |  | 66.01 | 12.9 |
| $\mathrm{de} \rightarrow \mathrm{fr}$ | 60.70 | 11.0 |  | 61.76 | 11.3 |

Table 1: ICL-MT performance with aligned vs. misaligned demonstrations, evaluated on de $\rightarrow$ en and en $\rightarrow$ de. 1 -shot/3-shot: using 1 or 3 demonstrations randomly sampled from the training set. Misaligned demonstrations consistently cause a substantial performance drop.

mismatch between using English and non-English suggests that Llama-2, as an English-centric LLM, may process English differently compared to other languages. When fine-tuned for English generation, the model may misinterpret the task as only generating in English. Generalization among nonEnglish languages is much easier than generalization between English and non-English languages, as evidenced by the negligible performance drop when fine-tuning and testing on two vastly different language pairs such as de $\rightarrow \mathrm{fr}$ and en $\rightarrow \mathrm{zh}$. Overall, the findings suggest that SFT in one translation direction effectively enables the many directions, though avoiding misinterpretation is crucial.

ICL results. The results of performing ICL with misaligned directions are presented in Table 1. It can be seen that misaligned demonstrations significantly degrade translation performance, with 3 -shot be often worse than 1 -shot. We observe that the model may output Chinese characters, emojis, time and date etc., but no clear error patterns are observed. This contrasts sharply with findings from SFT: while SFT can recognize the format of translation, ICL requires language-aligned demonstrations.

Joint evaluation. Figure 3 presents a joint evaluation on sizes and directions. For small training sizes, covering diverse translation directions in training proves to be beneficial. However, the benefits of such diversity level off as the training size increases. With a training size of 1024 , models trained exclusively on two directions, en $\leftrightarrow$ de, perform on par with those trained on all directions.

![](https://cdn.mathpix.com/cropped/2024_06_04_214af474d081dbaf3499g-05.jpg?height=520&width=748&top_left_y=1025&top_left_x=1065)

Figure 3: Average performance (in COMET) across 11 test directions for models trained with varying data sizes and directions. Both factors positively impact performance. $+=$ : training directions added on top of previous directions; two directions are added at each time. For example, " $+=$ ru" covers 10 directions: en $\leftrightarrow\{$ de, zh, cs, jp, ru $\}$. Performance on individual test directions is provided in Appendix C.

### 3.4 Can alignment be achieved with unseen languages?

Previous sections focus on translation directions involving languages explicitly included in Llama-2's pre-training corpus. We now extend our investigation to languages that do not have an identified presence of over $0.005 \%$ in the pre-training data (c.f. Touvron et al., 2023, p22), referred to as unseen languages. Here we seek answers to two questions: (1) Can we effectively align Llama-2 to translate both from and to unseen languages by fine-tuning it with a small dataset? (2) How well can this finetuned model translate from and to seen languages?
![](https://cdn.mathpix.com/cropped/2024_06_04_214af474d081dbaf3499g-06.jpg?height=490&width=1468&top_left_y=234&top_left_x=294)

Figure 4: Model performance (in COMET) evaluated across 15 translation directions. While models trained on unseen languages (en $\leftrightarrow$ is, en $\leftrightarrow$ ha) exhibit moderate improvements in translating these languages, they demonstrate accurate translations from and to seen languages. Performance measured in BLEU score is provided in Appendix D.

Setup. We consider three training configurations: en $\leftrightarrow$ is, en $\leftrightarrow$ ha, and en $\leftrightarrow$ de, with Icelandic (is) and Hausa (ha) being unseen languages. en $\leftrightarrow$ de serves as a control to assess Llama-2's initial translation capabilities into unseen languages without specific fine-tuning. The training size is fixed at 1024 (512 samples for each direction). The test directions include the 11 directions as before, plus en $\leftrightarrow$ is and en $\leftrightarrow$ ha coming from the WMT21 test.

Results. The results are presented in Figure 4. It can be seen that fine-tuning on Icelandic and Hausa enhances a model's translation quality on these languages compared to the control setup, yet the gains are modest. We observe that Llama-2 manages to produce tokens in these languages, however, the translations often largely deviate from the original meanings. This suggests that it is difficult to teach models new translation directions via SFT with limited data. Interestingly, we find fine-tuning on Icelandic or Hausa does not hinder Llama-2's ability to translate from and to all seen languages, maintaining performance levels comparable to the control scenario with en $\leftrightarrow d$. Based on these results, we propose a complement to the superficial alignment hypothesis in MT: LLMs may learn the essence of the task without requiring a deep understanding of the input-output mapping of the training samples.

### 3.5 Can we use noisy data?

So far we have observed that LLMs quickly recognize the translation task with minimal high-quality, manually curated data. But what if the data contains noise? Can LLMs still recognize the essence of translation-accurately conveying the meaning of the source sentence in another language-or will they instead overfit to the noise, leading to degraded translation performance?

Setup. To create noisy training sets, we inject two types of noise: sentence-level and word-level, into the clean training data. Both types of noise are introduced via back-translation (Sennrich et al., 2016). Specifically, we use the OPUS-MT suite (Tiedemann and Thottingal, 2020) to translate from English to a target non-English language. ${ }^{7}$ For word-level noising, we translate individual source words by feeding a single space-delimited word at a time. The OPUS-MT checkpoints can be found under Helsinki-NLP/opus-mt-\$en-\$\{trg\} on Hugging Face. We investigate the impact of noise in four translation directions: en $\rightarrow \mathrm{de}^{\prime}, \mathrm{de}^{\prime} \rightarrow \mathrm{en}$, en $\rightarrow$ ha', and ha ${ }^{\prime} \rightarrow$ en, where the prime (') notation indicates the side receiving noise. We consider two sizes of the training sets: 32 and 1024 .

Results. The results are presented in Figure 5. It can be seen that both types of noise generally cause a decline in performance. However, The degree of degradation significantly varies depending on whether the noise is injected into the source or target side of the translation, as well as the language affected. Specifically, when noise is introduced to the target side, models fine-tuned on en $\rightarrow \mathrm{de}^{\prime}$ and en $\rightarrow$ ha' translations exhibit a clear drop in performance. The impact of word noise is more severe than that of sentence noise. In the case of en $\rightarrow \mathrm{de}^{\prime}$, word-level noise makes the model degenerate to producing literal translations across all test cases.[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_214af474d081dbaf3499g-07.jpg?height=739&width=1448&top_left_y=253&top_left_x=270)

Figure 5: Model performance in COMET score varying training sizes, directions, and noise types. Top (Bottom): score averaged across all en $\rightarrow \mathrm{X}(\mathrm{X} \rightarrow \mathrm{en})$ test directions. Training sizes considered are 32 and 1024. Performance measured in BLEU score is in Appendix E.

An example of this behavior is presented in Table 2 . In contrast, the performance drop caused by word noise is less pronounced with en $\rightarrow$ ha', particularly when evaluated on en $\rightarrow \mathrm{X}$.

Conversely, when noise is introduced on the source side, the negative impact is much smaller, and the disparity in performance degradation between the two types of noise diminishes. Even more strikingly, when evaluated on en $\rightarrow \mathrm{X}$, having noise at the source side often outperforms the clean settings. Notably, in Section 3.3, we show that finetuning models purely on $\mathrm{X} \rightarrow$ en risks task misinterpretation, leading to low performance on en $\rightarrow \mathrm{X}$. However, adding noise appears to mitigate this issue, resulting in improvements in both COMET and BLEU scores, especially for the ha ${ }^{\prime} \rightarrow$ en case.

Summarizing the observations, Llama-2 is much more robust against the noise added to Hausa, likely because it has limited familiarity with the language, making it more difficult to imitate the noise patterns. As a result, Llama-2 tends to just recognize the task instead of fitting the noise. In contrast, for German, Llama-2's understanding leads to a misinterpretation of the training objectives, such as fitting the word-level noise with a directive for literal translations. Overall, LLMs may quickly fit translation noise and imitate the noise patterns, especially for seen languages; the resulting performance drop may be observable with just 32 training samples.

## 4 Related Work

### 4.1 What does LLM SFT bring us?

Foundational language models become more robust and follow instructions better after being finetuned on task-oriented supervised data formulated as natural language text (Mishra et al., 2022; Sanh et al., 2022; Wei et al., 2022). We observe diverging trends in research on instruction tuning nowadays: 1) Many works attempt to scale up instruction data in terms of the number of tasks, languages, data size, and thus implicitly increasing training updates (Chung et al., 2022; Muennighoff et al., 2023; Wu et al., 2024b; Li et al., 2023a; Üstün et al., 2024; Zhang et al., 2024). 2) Another stream of papers, argue that instruction tuning mainly alters a base model's response style but not content or knowledge-data quality and diversity outweigh quantity (Zhou et al., 2023; Mitchell et al., 2023; Lin et al., 2024; Chen et al., 2024a). This work is a continued exploration of the latter, focusing on the machine translation task. We verify the effect of size variations and include two new factorslanguage directions and quality-aiming to provide practical and cost-effective guidance on this matter.

Specifically, language transfer has been demonstrated in smaller pre-trained models before LLMs (Wu and Dredze, 2019; Artetxe et al., 2020). For (sufficiently) multilingual models, training on certain languages might still benefit other languages at the test time (Choenni et al., 2023). In LLM

| Source | Ref./Training config. | Model output |
| :--- | :--- | :--- |
| Das finde ich ehrlich gesagt | Reference | That really bothers me, I must say. |
| sehr ärgerlich. | Word-to-word ref. | The find I honest said very annoying. |
|  | en $\rightarrow$ de clean | I find that really annoying. |
|  | en $\rightarrow$ de sent. noise | I find that honestly very annoying. |
|  | en $\rightarrow$ de word noise | The find I honestly said very annoying. |
| 以免再次发生这样的事情 | Reference | So that such a thing won't happen again. |
|  | Word-to-word ref. | in order to avoid again happen such thing. |
|  | en $\rightarrow$ de clean | Let's not let it happen again. |
|  | en $\rightarrow$ de sent. noise | In order not to happen again. |
|  | en $\rightarrow$ de word noise | Avoid again happen this way. |

Table 2: Examples of testing Llama-2 trained on en $\rightarrow$ de with 1024 clean and noisy target sentences. The test directions are de $\rightarrow$ en (Top) and $\mathrm{zh} \rightarrow$ en (Bottom). The reference translation is provided by the WMT22 test set. Word-to-word references were created by the authors in consultation with native speakers. Word-level noise makes Llama-2 degenerate into a literal translator.

instruction tuning, recent papers revealed crosslingual transfer and improved robustness in unseen languages via multilingual instruction tuning with a small data sample (Chen et al., 2024b; Kew et al., 2023; Shaham et al., 2024). Furthermore, it has been claimed that even monolingual instruction tuning is sufficient to elicit multilingual responses in the correct languages with a key ingredient being the right learning rate (Chirkova and Nikoulina, 2024a,b). In relation to our experiments, language transfer to unseen languages might account for improved performance in language directions that are not directly fine-tuned on.

### 4.2 Using LLMs for translation

In the field of machine translation, earlier works provided analysis of general-purpose prompting (Vilar et al., 2023; Agrawal et al., 2023; Zhang et al., 2023a) followed by a blossom of strategies focusing on specific aspects of the translation process (Sarti et al., 2023; Ghazvininejad et al., 2023; He et al., 2023; Moslem et al., 2023; Chen et al., 2023; Raunak et al., 2023). Nonetheless, as shown in our experimental results, few-shot prompting is not on par with using instruction-tuned models, illustrating the importance of further understanding the role of instruction tuning in translation tasks.

In terms of fine-tuning LLMs for translation, previous works have explored a wide range of subtasks: disambiguation, low-resource, documentlevel, and adaptive translation, etc (Li et al., 2023b; Zhang et al., 2023b; Alves et al., 2023; Iyer et al., 2023; Mao and Yu, 2024; Wu et al., 2024a). These works focus on improving translation performance and specific applications. Recent research aims to enhance the translation capabilities of LLMs by incorporating human preference data (Jiao et al., 2023; Zeng et al., 2024; Zhu et al., 2024) or by extending the pre-training phase before fine-tuning (Xu et al., 2024a,b; Alves et al., 2024), yet these approaches require significantly more data or computing resources. The aim of this paper is not to pursue the state of the art but to investigate the opportunities of extending instruction-tuned LLMs' translation capabilities in desirable compute-efficient scenarios. It is still worth noting that our investigation is orthogonal to previous works which employ relatively large monolingual and parallel data for continued pre-training.

## 5 Conclusion and Future Work

In this work, we conduct an in-depth analysis of fine-tuning LLMs for translation. We demonstrate that LLMs is capable of translating in multiple directions after being fine-tuned with minimal lowquality training data in a single direction. While this suggests pre-trained LLMs inherently possess multilingual translation capabilities which only need to be unlocked by aligning with the correct task format, we discover pitfalls and lessons in aligning LLMs; while LLMs make efforts to adjust to the translation task, they are good at imitating other patterns such as the noise in the parallel data. Future work could explore robust training methods that align LLMs with translation while minimizing the risk of overfitting to noisy data.

## Limitations

This work offers a range of insights into fine-tuning LLMs for translation. However, our study is not exhaustive and is subject to the following limitations.

Model size and diversity. Throughout our systematic study, we fine-tuned Llama-2 7B, which is a strong and feasible option when the work is carried out. This may impact the generalization of our findings to models with different capabilities or of different sizes are used.

Non-English centric MT. Our evaluation is English-centric, which is the condition of most LLM pre-training; the findings will be more comprehensive if future work can extend it to translation directions not involving English.

State-of-the-Art performance. Our research primarily explores how SFT enables LLM to translate, with the aim of uncovering data-efficient strategies in SFT and identifying associated pitfalls. Recent studies have demonstrated that translation capabilities can be further enhanced through techniques such as continual pre-training ( $\mathrm{Xu}$ et al., 2024a; Alves et al., 2024) and preference learning (Xu et al., 2024b; Zhu et al., 2024). However, these methods require significantly more training resources, which may pose challenges when applied to large models.

Fine-tuning methods. Throughout this work, we perform full parameter fine-tuning in SFT. It is worthwhile to explore alternative tuning methods and investigate whether they exhibit properties similar to those observed in full fine-tuning, especially when training involves small, single-direction, and noisy data. We leave this investigation for future work.

## Ethical considerations

Our work's sole aim is to study the influence of data factors in applying supervised fine-tuning to large language models. We expect minimal social risks to be associated with our efforts.

## Acknowledgments

Pinzhen Chen and Barry Haddow received funding from UK Research and Innovation (UKRI) under the UK government's Horizon Europe funding guarantee [grant number 10052546]

## References

Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023. Incontext examples selection for machine translation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 8857-8873, Toronto, Canada. Association for Computational Linguistics.

Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ondřej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina España-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21). In Proceedings of the Sixth Conference on Machine Translation, pages 1-88, Online. Association for Computational Linguistics.

Duarte M. Alves, Nuno M. Guerreiro, João Alves, José Pombal, Ricardo Rei, José de Souza, Pierre Colombo, and Andre Martins. 2023. Steering large language models for machine translation with finetuning and in-context learning. In Findings of the Association for Computational Linguistics: EMNLP 2023.

Duarte M. Alves, José Pombal, Nuno M Guerreiro, Pedro H Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, et al. 2024. Tower: An open multilingual large language model for translation-related tasks. arXiv preprint.

Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623-4637, Online. Association for Computational Linguistics.

Loïc Barrault, Magdalena Biesialska, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubešić, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, pages 1-55, Online. Association for Computational Linguistics.

Loïc Barrault, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn,

Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1-61, Florence, Italy. Association for Computational Linguistics.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, pages 169-214, Copenhagen, Denmark. Association for Computational Linguistics.

Ondřej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. 2018. Findings of the 2018 conference on machine translation (WMT18). In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 272-303, Belgium, Brussels. Association for Computational Linguistics.

Eleftheria Briakou, Colin Cherry, and George Foster. 2023. Searching for needles in a haystack: On the role of incidental bilingualism in PaLM's translation capability. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9432-9452, Toronto, Canada. Association for Computational Linguistics.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems.

Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. 2024a. Alpagasus: Training a better Alpaca model with fewer data. In The Twelfth International Conference on Learning Representations.

Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Kenneth Heafield. 2023. Iterative translation refinement with large language models. arXiv preprint.

Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Andrey Kutuzov, Barry Haddow, and Kenneth Heafield. 2024b. Monolingual or multilingual instruction tuning: Which makes a better Alpaca. In Findings of the Association for Computational Linguistics: EACL 2024.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing GPT-4 with $90 \% *$ ChatGPT quality. Imsys.org.
Nadezhda Chirkova and Vassilina Nikoulina. 2024a. Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks. arXiv preprint.

Nadezhda Chirkova and Vassilina Nikoulina. 2024b. Zero-shot cross-lingual transfer in instruction tuning of large language model. arXiv preprint.

Rochelle Choenni, Dan Garrette, and Ekaterina Shutova. 2023. How do languages influence each other? studying cross-lingual data sharing during LM fine-tuning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models arXiv preprint.

Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer. 2023. Dictionary-based phrase-level prompting of large language models for machine translation. arXiv preprint.

Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, and Dinesh Manocha. 2024. A closer look at the limitations of instruction tuning. Preprint, arXiv:2402.05119.

Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2023. Exploring humanlike translation strategy with large language models. arXiv preprint.

Vivek Iyer, Pinzhen Chen, and Alexandra Birch. 2023. Towards effective disambiguation for machine translation with large language models. In Proceedings of the Eighth Conference on Machine Translation.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint.

Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023. ParroT: Translating during chat using large language models tuned with human translation and feedback. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore.

Tannon Kew, Florian Schottmann, and Rico Sennrich. 2023. Turning english-centric LLMs into polyglots: How much multilinguality is needed? arXiv preprint

Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki

Nagata, Toshiaki Nakazawa, Michal Novák, Martin Popel, and Maja Popović. 2022. Findings of the 2022 conference on machine translation (WMT22). In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 1-45, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. 2023a. Bactrian-X: Multilingual replicable instruction-following models with low-rank adaptation. arXiv preprint.

Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, and Jiajun Chen. 2023b. Eliciting the translation ability of large language models via multilingual finetuning with translation instructions. arXiv preprint.

Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2024. Urial: Aligning untuned LLMs with just the 'write' amount of in-context learning. In The Twelfth International Conference on Learning Representations.

Zhuoyuan Mao and Yen Yu. 2024. Tuning llms with contrastive alignment instructions for machine translation in unseen, low-resource languages. arXiv preprint.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487, Dublin, Ireland. Association for Computational Linguistics.

Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D. Manning. 2023. An emulator for fine-tuning large language models using small language models. arXiv preprint.

Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way. 2023. Adaptive machine translation with large language models. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 227-237, Tampere, Finland. European Association for Machine Translation.

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15991-16111, Toronto, Canada. Association for Computational Linguistics.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35.

Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog.

Vikas Raunak, Amr Sharaf, Hany Hassan Awadallah, and Arul Menezes. 2023. Leveraging GPT-4 for automatic translation post-editing. In Findings of the Association for Computational Linguistics: EMNLP 2023.

Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zeroshot task generalization. In International Conference on Learning Representations.

Gabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu, Anna Currey, Georgiana Dinu, and Maria Nadejde. 2023. RAMP: Retrieval and attribute-marking enhanced prompting for attribute-controlled translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1476-1490, Toronto, Canada. Association for Computational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86-96, Berlin, Germany. Association for Computational Linguistics.

Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. 2024. Multilingual instruction tuning with just a pinch of multilinguality. arXiv preprint.

Suzanna Sia, David Mueller, and Kevin Duh. 2024. Where does in-context translation happen in large language models. arXiv preprint.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An instruction-following LLaMA model. GitHub repository.

Jörg Tiedemann and Santhosh Thottingal. 2020. OPUSMT - building open translation services for the world In Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, pages 479-480, Lisboa, Portugal. European Association for Machine Translation.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint.

Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, WeiYin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. 2024. Aya model: An instruction finetuned open-access multilingual language model. arXiv preprint.

David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2023. Prompting PaLM for translation: Assessing strategies and performance. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1540615427, Toronto, Canada. Association for Computational Linguistics.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.

Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Foster, and Gholamreza Haffari. 2024a. Adapting large language models for document-level machine translation. arXiv preprint.

Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Aji. 2024b. LaMiniLM: A diverse herd of distilled models from largescale instructions. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics.

Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 833-844, Hong Kong, China. Association for Computational Linguistics.

Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024a. A paradigm shift in machine translation: Boosting translation performance of large language models. In The Twelfth International Conference on Learning Representations.
Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. 2024b. Contrastive preference optimization: Pushing the boundaries of $11 \mathrm{~m}$ performance in machine translation. arXiv preprint.

Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. 2024. Teaching large language models to translate with comparison. In Proceedings of the AAAI Conference on Artificial Intelligence.

Biao Zhang, Barry Haddow, and Alexandra Birch. 2023a. Prompting large language model for machine translation: a case study. In Proceedings of the 40th International Conference on Machine Learning.

Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. 2024. When scaling meets LLM finetuning: The effect of data, model and finetuning method. In The Twelfth International Conference on Learning Representations.

Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, et al. 2023b. BayLing: Bridging cross-lingual alignment and instruction following through interactive translation for large language models. arXiv preprint.

Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp Koehn. 2023c. Machine translation with large language models: Prompting, few-shot learning, and fine-tuning with QLoRA. In Proceedings of the Eighth Conference on Machine Translation.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing Systems.

Dawei Zhu, Sony Trenous, Xiaoyu Shen, Dietrich Klakow, Bill Byrne, and Eva Hasler. 2024. A preference-driven paradigm for enhanced translation with large language models. arXiv preprint.
