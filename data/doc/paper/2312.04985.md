# SparQ Attention: Bandwidth-Efficient LLM Inference 

Luka Ribar * Ivan Chelombiev * Luke Hudlass-Galley * Charlie Blake Carlo Luschi Douglas Orr


#### Abstract

The computational difficulties of large language model (LLM) inference remains a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data-transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to $8 \times$ savings in attention data-transfers without substantial drops in accuracy, by evaluating Llama 2, Mistral and Pythia models on a wide range of downstream tasks.


## 1. Introduction

Transformer models trained on large corpora of text have recently shown remarkable performance on complex natural language processing tasks (Achiam et al., 2023; Touvron et al., 2023). This has been attributed to the in-context learning capabilities that emerge with large-scale training, enabling arbitrary textual information (e.g. long instructions, chat histories, relevant documents) to be incorporated at inference-time (Wei et al., 2022).

To leverage the benefits of in-context learning there has been demand for LLMs to support increasingly long input sequences. However, the standard inference optimisation used to support in-context learning, key-value ( $K V$ ) caching[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-01.jpg?height=556&width=808&top_left_y=633&top_left_x=1060)

Figure 1: Llama 2 13B SQuAD 1-shot performance versus attention transfers over a range of compression ratios. SparQ Attention achieves matching performance, while transferring between $1 / 8$ and $1 / 4$ as much data as the original dense model. The curves show the mean $\pm$ one standard error evaluated over 4000 examples. This pattern is representative of the performance across multiple models and tasks, shown in Figures A1 and A2.

(Pope et al., 2023), is constrained by the need to fetch a large amount of data from memory when processing batches of long sequences. This in turn limits the speed at which tokens can be generated-a key usability metric for LLMs.

This bottleneck can be attributed to the auto-regressive nature of transformer generation. For each token generated, the full KV cache must be re-fetched from memory. The size of the KV cache scales linearly with the sequence length, as well as the batch size, thus rendering generation for long batched sequences increasingly memory bandwidth limited.

Despite this expensive cache-fetch at each step, tokens generally only attend to a small part of the sequence at a time (Vig, 2019; Yun et al., 2020). If it were possible to efficiently predict which tokens will have high attention scores, memory bandwidth efficiency could be significantly increased by only transferring the key-value pairs of high-scoring tokens.

Building upon this idea, we present SparQ (Sparse Query) Attention, a technique for significantly improving the memory bandwidth efficiency of transformer inference. By approximating attention scores using a subset of query and
key components, we fetch only the most relevant tokens for each generation step, reducing the amount of data transferred without degrading the model.

We also provide a new set of challenging downstream task variants which we use to evaluate SparQ Attention. These are based on existing tasks, modified to assess the model's ability to utilise information from long input sequences for multi-token generation. We show that SparQ Attention performs favourably compared to other state-of-the-art methods, giving up to $8 \times$ compression without substantial loss in accuracy. SparQ Attention is robust across tasks and models, demonstrated by evaluation on Llama 2, Mistral and Pythia. We also provide benchmarks measured on IPU and GPU, showing the practical computational benefits of our approach.

## 2. Background

In this section we provide a straightforward framework to understand the computational efficiency of sequence generation using transformer models (similar to the modelling introduced by Kaplan et al. (2020)) and use it to motivate transfer-efficient attention mechanisms.

Arithmetic intensity A compute unit capable of $r_{\mathcal{A}}$ scalar arithmetic operations per second is connected to a memory via an interface that can transfer $r_{\mathcal{M}}$ scalar elements per second, processing a workload requiring $\mathcal{A}$ arithmetic operations and $\mathcal{M}$ transfers. Assuming concurrent compute and data transfer, when the arithmetic intensity $\mathcal{A} / \mathcal{M}$ of the workload is less than the ratio $r_{\mathcal{A}} / r_{\mathcal{M}}$, execution time is limited by $r_{\mathcal{M}}$.

Sequence generation Consider a full transformer layer, with $N$ parameters, batch size $B$, and $C$ elements in the attention KV cache per batch element. We assume Grouped Query Attention (GQA) (Ainslie et al., 2023) with $g$ grouped-query heads ( $g=1$ for standard multi-head attention). This implies the arithmetic intensity:

$$
\begin{equation*}
\frac{\mathcal{A}}{\mathcal{M}}=\frac{B N+B C g}{N+B C}=\frac{N+C g}{N / B+C} \tag{1}
\end{equation*}
$$

We can increase arithmetic intensity by making $B$ large, causing $\mathcal{A} / \mathcal{M}$ to approach $N / C+g$. Hence the limiting factor for large-batch transformer inference is the ratio of the KV cache size per-item to the size of the model.

We can alternatively express this in terms of the model's basic hyperparameters. A standard transformer with model dimension $d_{m}$ and sequence-length $S$, has $N=12\left(d_{m}\right)^{2}$ and $C=2 S d_{m} / g$, giving:

$$
\begin{equation*}
\frac{\mathcal{A}}{\mathcal{M}}=\frac{6+\rho g}{6 / B+\rho} \tag{2}
\end{equation*}
$$

Table 1: Excess correlation ratio $\eta$ (Roche et al., 1998) along axes of $\boldsymbol{V}$ (excess: subtract $d^{-0.5}$, so uniform random data $=0.0$ ). This demonstrates substantial auto-correlation along the sequence axis. Calculated for Llama 7B over 40 SQuAD examples.

|  | $B$ | $S$ | Layer | Head | $d_{h}$ |
| :---: | :---: | :---: | :---: | :---: | :---: |
| $\eta-d^{-0.5}$ | 0.143 | $\mathbf{0 . 2 5 6}$ | 0.0 | 0.0 | 0.0 |

where $\rho=S /\left(g d_{m}\right)$. The value $\rho$ underlies the KV cachemodel size relationship outlined above, determining the point at which the model becomes memory bandwidth bound.

For typical examples (see Appendix C), sequence generation exhibits a large-batch arithmetic intensity of just 7 for multi-head attention with $S=d_{m}$, up to 100 for groupedquery attention with $S \ll d_{m}$, while ML hardware can provide $r_{\mathcal{A}} / r_{\mathcal{M}}>200$. This means that data transfer is the performance-limiting factor, motivating the search for transfer-efficient alternatives to full attention.

## 3. Approximating Attention

In this section we examine several properties of the attention operation that enable us to introduce an accurate bandwidthefficient approximation.

Consider a single attention query head with the head dimension $d_{h}$, processing an input token sequence of length $S$. During autoregressive generation, the output of the attention head is calculated as:

$$
\begin{equation*}
\boldsymbol{y}=\operatorname{softmax}\left(\frac{\boldsymbol{q} \cdot \boldsymbol{K}^{\top}}{\sqrt{d_{h}}}\right) \cdot \boldsymbol{V} \tag{3}
\end{equation*}
$$

where $\boldsymbol{q}$ is the query, and $\boldsymbol{K} \in \mathbb{R}^{S \times d_{h}}$ and $\boldsymbol{V} \in \mathbb{R}^{S \times d_{h}}$ are the key and value caches respectively. When using GQA (Ainslie et al., 2023), $\boldsymbol{K}$ and $\boldsymbol{V}$ are shared across $g$ query heads.

For each forward pass, we need to fetch the key and value matrices from memory, as well as write (append) $k$ and $v$ vectors for the current token, giving a total number of elements transferred per attention head:

$$
\begin{equation*}
\mathcal{M}_{\text {base }}=\underbrace{2 S d_{h}}_{(1)}+\underbrace{2 d_{h}}_{(2)} \tag{4}
\end{equation*}
$$

where (1) corresponds to reading the $\boldsymbol{K}$ and $\boldsymbol{V}$ caches and (2) corresponds to writing the current $k$ and $\boldsymbol{v}$ to memory.

Attention scores sparsity First, consider the attention scores $\boldsymbol{s} \in(0,1)^{S}$ in Equation (3):

$$
\begin{equation*}
\boldsymbol{s}=\operatorname{softmax}\left(\frac{\boldsymbol{q} \cdot \boldsymbol{K}^{\top}}{\sqrt{d_{h}}}\right) \tag{5}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-03.jpg?height=1147&width=765&top_left_y=215&top_left_x=211)

```
Algorithm 1 SparQ Attention
    Input: $\boldsymbol{q} \in \mathbb{R}^{d_{h}}, \boldsymbol{K} \in \mathbb{R}^{S \times d_{h}}, \boldsymbol{V} \in \mathbb{R}^{S \times d_{h}}, \overline{\boldsymbol{v}} \in \mathbb{R}^{d_{h}}$,
    $r \in \mathbb{N}, k \in \mathbb{N}, l \in \mathbb{N}$
    \# Indices of top $r$ elements of $|q|$
    $\boldsymbol{i}_{1} \leftarrow \operatorname{argtopk}(|\boldsymbol{q}|, r)$
    \# Softmax temperature, weighted by L1 coverage
    $\tau \leftarrow \sqrt{d_{h} \cdot \frac{\left\|\boldsymbol{q}_{\left[i_{1}\right]}\right\|_{1}}{\|\boldsymbol{q}\|_{1}}}$
    \# Approximate attention scores (all positions)
    $\hat{\boldsymbol{s}} \leftarrow \operatorname{softmax}\left(\boldsymbol{q}_{\left[i_{1}\right]} \cdot \boldsymbol{K}_{\left[i_{1},:\right]}^{\top} / \tau\right)$
    \# Local mask of last l positions
    $\boldsymbol{m} \leftarrow[1 \text { if } i>S-l \text { else } 0]_{i=1}^{S}$
    \# Indices of top $k$ approximate scores or local
    $\boldsymbol{i}_{2} \leftarrow \operatorname{argtopk}(\hat{\boldsymbol{s}}+\boldsymbol{m}, k)$
    \# Total approximate score of top $k$
    $\alpha \leftarrow \operatorname{sum}\left(\hat{s}_{\left[i_{2}\right]}\right)$
    \# Final attention scores (top $k$ positions)
    $\boldsymbol{s} \leftarrow \operatorname{softmax}\left(\boldsymbol{q} \cdot \boldsymbol{K}_{\left[:, i_{2}\right]}^{\top} / \sqrt{d_{h}}\right)$
    \# Mixed scores and values, interpolating with $\bar{v}$
    $\boldsymbol{y} \leftarrow \alpha \boldsymbol{s} \cdot \boldsymbol{V}_{\left[: ; i_{2}\right]}+(1-\alpha) \overline{\boldsymbol{v}}$
    return $\boldsymbol{y}$
```

Figure 2: SparQ Attention for a single attention head. The algorithm consists of three steps. First, we find the $r$ largest components of the incoming query vector and gather the corresponding components along the hidden dimension of the key cache $\boldsymbol{K}$. This allows us to approximate the full attention scores ( $\hat{s}$ ). In the second step, we identify the top- $k$ largest scores in the approximation and proceed to gather the corresponding full key and value vectors from the cache. As a final step, to compensate for the missing value vectors, we additionally maintain and fetch the running mean value vector $\overline{\boldsymbol{v}}$ and reassign it the leftover mass based on approximate score weightings. The attention output is then calculated as usual using the top- $k$ fetched key and value pairs, together with $\overline{\boldsymbol{v}}$.

Due to the normalising effect of the softmax function, the resulting $s$ vector is sparse (see Figures 3a and 3b), i.e. we can find a boolean mask $\boldsymbol{m}_{\boldsymbol{s}} \in\{0,1\}^{S}$ corresponding to the top- $k$ elements in $s(k \ll S)$ such that:

$$
\begin{equation*}
\boldsymbol{y}_{1}=\left(\boldsymbol{s} \circ \boldsymbol{m}_{\boldsymbol{s}}\right) \cdot \boldsymbol{V} \approx s \cdot \boldsymbol{V} \tag{6}
\end{equation*}
$$

As a result, only the values $\boldsymbol{v}_{i}$ corresponding to the non-zero elements of $\boldsymbol{m}_{\boldsymbol{s}}$ need to be fetched from memory. However, the algorithm still requires fetching the full $\boldsymbol{K}$ from memory in order to calculate the attention scores $s$, limiting the minimum amount of data transferred to $\frac{1}{2} \mathcal{M}_{\text {base }}$.

Mean value reallocation In order to further improve the approximation in Equation (6), we note a further observation: $\boldsymbol{v}_{i}$ vectors within the sequence exhibit a high degree of auto-correlation (see Table 1). Thus, an additional correction term using a running-mean value vector $\overline{\boldsymbol{v}}=\frac{1}{S} \sum_{i=1}^{S} \boldsymbol{v}_{i}$ can be added as follows:

$$
\begin{equation*}
\boldsymbol{y}_{2}=\left(\boldsymbol{s} \circ \boldsymbol{m}_{\boldsymbol{s}}\right) \cdot \boldsymbol{V}+\left(1-\boldsymbol{s} \cdot \boldsymbol{m}_{\boldsymbol{s}}\right) \overline{\boldsymbol{v}} \tag{7}
\end{equation*}
$$

This introduces a minimal additional overhead compared to Equation (6) as the mean vector $\overline{\boldsymbol{v}}$ needs to be updated and written back to memory at each step.

Query sparsity In order to improve the lower bound on memory transfers, we further consider efficiently approximating the mask $\boldsymbol{m}_{\boldsymbol{s}}$ by calculating approximate attention scores $\hat{\boldsymbol{s}}$ without using the full matrix $\boldsymbol{K}$. Here, we consider the distribution of magnitudes of the components of the query vector $\boldsymbol{q}$ and observe that it is highly heavy-tailed (see Figures $3 \mathrm{c}$ and $3 \mathrm{~d}$ ). This observation allows us to efficiently approximate the attention scores $s$ by defining a per-query boolean mask $\boldsymbol{m}_{\boldsymbol{q}} \in\{0,1\}^{d_{h}}$ corresponding to the top- $r$
components of $\boldsymbol{q}$. The scores are then approximated as:

$$
\begin{equation*}
\hat{s}=\operatorname{softmax}\left(\frac{\left(\boldsymbol{q} \circ \boldsymbol{m}_{\boldsymbol{q}}\right) \cdot \boldsymbol{K}^{\top}}{\tau}\right) \tag{8}
\end{equation*}
$$

where $\tau$ is the softmax temperature. Due to the mask $\boldsymbol{m}_{\boldsymbol{q}}$, only the components of $\boldsymbol{K}$ corresponding to non-zero elements of the mask need to be fetched from memory. The top-k mask $\boldsymbol{m}_{\hat{\boldsymbol{s}}} \in\{0,1\}^{S}$ can then be calculated using $\hat{s}$ (see Figure 3e) and the approximate attention output is obtained as:

$$
\begin{equation*}
\boldsymbol{y}_{3}=\operatorname{softmax}\left(\frac{\boldsymbol{q} \cdot \boldsymbol{K}^{\top}}{\sqrt{d_{h}}}+\log \left(\boldsymbol{m}_{\hat{\boldsymbol{s}}}+\epsilon\right)\right) \cdot \boldsymbol{V} \tag{9}
\end{equation*}
$$

with $\epsilon \rightarrow 0$. Again, due to the mask $\boldsymbol{m}_{\hat{\boldsymbol{s}}}$, only the key-value pairs corresponding to the non-masked elements need to be fetched from the memory.

Mean value reallocation with query sparsity As a final consideration, we look at combining the mean value reallocation improvement of Equation (7) with the approach in
Equation (9). As we do not have access to the full scores $s$, we proceed to approximate the weighted sum using the approximate scores in Equation (8). Note that, since the query-key dot product is performed over only $r$ dimensions, care needs to be taken when choosing the appropriate softmax temperature $\tau$ in Equation (8). If $r$ components were chosen randomly, the appropriate temperature would be $\sqrt{r}$. On the other hand, if the top- $r$ components were the only non-zero elements of the query vector, the appropriate temperature would remain $\sqrt{d_{h}}$. As a balance between the two extremes, we have found the following temperature to yield a good approximation (see Figure 3f):

$$
\begin{equation*}
\tau=\sqrt{d_{h} \frac{\left\|\boldsymbol{q} \circ \boldsymbol{m}_{\boldsymbol{q}}\right\|_{1}}{\|\boldsymbol{q}\|_{1}}} \tag{10}
\end{equation*}
$$

The final attention output can be then calculated as a weighted sum:

$$
\begin{equation*}
\boldsymbol{y}=\alpha \boldsymbol{y}_{3}+(1-\alpha) \overline{\boldsymbol{v}} \tag{11}
\end{equation*}
$$

where $\alpha=\boldsymbol{m}_{\hat{\boldsymbol{s}}} \cdot \hat{\boldsymbol{s}}$ is the relative weight of the top- $k$ terms.

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-04.jpg?height=957&width=1716&top_left_y=1175&top_left_x=172)

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-04.jpg?height=393&width=545&top_left_y=1191&top_left_x=194)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-04.jpg?height=388&width=523&top_left_y=1671&top_left_x=191)

(d)

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-04.jpg?height=388&width=566&top_left_y=1194&top_left_x=823)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-04.jpg?height=390&width=514&top_left_y=1675&top_left_x=857)

(e)

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-04.jpg?height=388&width=376&top_left_y=1194&top_left_x=1487)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-04.jpg?height=380&width=358&top_left_y=1683&top_left_x=1510)

(f)

Figure 3: Statistics of Llama 7B, evaluated over 40 SQuAD queries, over all 32 layers $\times 32$ heads unless noted. (a) Sum softmax output allocated to the 32 highest-scoring positions, demonstrating natural attention sparsity; (b) for each head. (c) Kernel density estimate (Rosenblatt, 1956) of components of $\boldsymbol{q}$ in layer 16, showing heavy tails. (d) Fisher Kurtosis of $\boldsymbol{q}$ components, for each head, showing that the query vector is leptokurtic for most heads. (e) Top- $k$ agreement between approximate and true scores for multiple values of $r$ selected from query vector. Top- $k$ agreement is the proportion of the top- $k$ positions that are correctly predicted by an approximated softmax, using a projection of $\boldsymbol{q}$. (f) Agreement between the coverage $\alpha$ based on estimated scores versus the true mass of the top 128 scores, for different softmax temperatures (a point for each example $\times$ head), showing the importance of correct temperature. Further analysis is presented in Appendix D.

## 4. SparQ Attention

Following the analysis in Section 3, we propose SparQ Attention (see Algorithm 1) consisting of three steps:

Step 1: Find the indices of $r$ largest components of $|\boldsymbol{q}|$ and only fetch $\boldsymbol{K}$ along the dimensions corresponding to these indices. Calculate approximate attention scores $\hat{s}$ using the sliced query and keys.

Step 2: Find the top- $k$ positions in the approximate attention scores and fetch the corresponding full key and value vectors. Calculate the output of the attention operation using the top- $k$ keys and values.

Step 3: Estimate the total score $\alpha$ assigned to the top- $k$ positions using the approximate attention scores. Use this total score to interpolate between the attention output from the top- $k$ positions, and a mean value vector, $\overline{\boldsymbol{v}}$

The memory transfer of the SparQ Attention algorithm for a single attention head forward-pass:

$$
\begin{equation*}
\mathcal{M}_{\mathrm{SparQ}}=\underbrace{S r}_{\text {(1) }}+\underbrace{2 k d_{h}}_{(2)}+\underbrace{4 d_{h}}_{(3)} \tag{12}
\end{equation*}
$$

where (1) corresponds to reading $r$ rows of $\boldsymbol{K}$, (2) corresponds to reading the top- $k$ columns of $\boldsymbol{K}$ and $\boldsymbol{V}$ and (3) corresponds to transfers associated with writing the current $\boldsymbol{k}$ and $\boldsymbol{v}$, in addition to reading and writing $\overline{\boldsymbol{v}}$.

By varying $r$ and $k$, we can tune the total amount of data transferred by the scheme, trading-off approximation accuracy for token-generation speed-up. Since typically $S \gg d_{h}$, $r$ is the most important parameter controlling the data transfer compression ratio. Typical ratios are given in Table F2.

Grouped Query Attention For models using GQA, groups of $g$ queries access the same KV head. In order to accommodate this, we modify Step 1 to sum $|\boldsymbol{q}|$ within each group before selecting top- $r$ components. Similarly, Step 2 is modified by summing the approximate attention scores within each group before selecting top- $k$ keys and values for each KV head. Note that Step 3 remains the same. The full code can be found in Appendix B.

## 5. Experiments

### 5.1. Setup

Models We evaluate our method on three widely-used open-source language model variants: Llama 2 (Touvron et al., 2023), Mistral (Jiang et al., 2023) and Pythia (Biderman et al., 2023), evaluating model sizes up to 13 billion parameters. All models are decoder-only transformers (Radford et al., 2018), pre-trained on causal language modelling. They share similar architectural components such as Rotary positional encoding (Su et al., 2021), while also having some notable differences such as different attention mechanisms (Multi-Head and Grouped-Query), layer normalisation implementations, activation functions and execution of modules in parallel.

Tasks In order to evaluate our method on a spectrum of relevant NLP tasks that present a particular challenge to sparse attention techniques, our evaluation setup consists of various tasks requiring information retrieval and reasoning over long input sequences. This includes question answering, summarisation, perplexity/bits-per-character (BPC), and text repetition. For this, we adapted standard downstream tasks and datasets to generate examples of sequence lengths between $1 \mathrm{k}$ and $2 \mathrm{k}$ tokens. ${ }^{1}$

For question answering, we use the SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) datasets in the open-book setting. In order to construct the SQuAD examples, we augment the provided context (i.e. the standard SQuAD input sequence required to answer the question) with seven additional "confusion contexts" from unrelated questions. This ensures that the examples have a large sequence length, while making the task harder as the model needs to distinguish the relevant information from the context from the unrelated paragraphs. We use SQuAD v1.1, as it does not include unanswerable questions included in SQuAD v2.0, since we aim to measure the model's ability to extract useful information from the KV cache. For both question answering tasks we use exact string match accuracy as the evaluation metric.

Summarisation is evaluated on the CNN/DailyMail dataset (See et al., 2017) using the ROUGE-L F-score (Lin, 2004) as the metric. We use the WikiText-103 dataset (Merity et al., 2016) with bits per character (BPC) for evaluating language modelling performance. ${ }^{2}$

Finally, we construct an artificial "Text Repetition" task to evaluate the capability of the model to repeat sentences from its context verbatim. Such a task can commonly appear in a dialogue setting where the LLM agent is required to retrieve a piece of text from a possibly long context provided, and can be challenging for sparse attention techniques. We construct examples using the Tiny-Shakespeare dataset (Karpathy, 2015) by chunking the text into contexts of the[^1]

Table 2: Results for the largest models tested are presented below. SQuAD and TriviaQA measure performance in accuracy. CNN/DailyMail uses ROUGE-L score. Repetition counts the number of characters before the generation diverges and WikiText task measures perplexity in bits per character (BPC). Values in bold represent the best score for a model, task and sparsity setting. Median standard errors across all models and sparsity settings are: SQuAD 0.7, TriviaQA 0.7, CNN/DailyMail 0.4, WikiText 0.006, Repetition 2 .

| {Dataset Name <br> Compression} |  | SQuAD $\uparrow$ |  |  | TriviaQA $\uparrow$ |  |  | CNN/DailyMail $\uparrow$ |  |  | WikiText $\downarrow$ |  |  | Repetition $\uparrow$ |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | 1 | $1 / 2$ | $1 / 8$ | 1 | $1 / 2$ | $1 / 8$ | 1 | $1 / 2$ | $1 / 8$ | 1 | $1 / 2$ | $1 / 8$ | 1 | $1 / 2$ | $1 / 8$ |
| Llama 2 <br> 13B | LM- $\infty$ | 80.8 | 50.0 | 32.4 | 78.7 | 73.4 | 69.0 | 22.1 | 16.8 | 15.1 | 0.61 | 0.64 | 0.69 | 229 | 76 | 29 |
|  | $\mathrm{H}_{2} \mathrm{O}$ |  | 73.2 | 64.1 |  | 78.5 | 78.4 |  | 22.2 | 20.8 |  | 0.61 | 0.63 |  | 61 | 26 |
|  | SparQ |  | 80.7 | 78.0 |  | 78.8 | 78.2 |  | 22.5 | 22.2 |  | 0.61 | 0.64 |  | 227 | 190 |
| Mistral <br> 7B | LM- $\infty$ | 81.0 | 51.0 | 31.6 | 80.9 | 75.8 | 72.8 | 23.7 | 18.0 | 16.8 | 0.62 | 0.65 | 0.70 | 231 | 81 | 20 |
|  | $\mathrm{H}_{2} \mathrm{O}$ |  | 71.2 | 59.2 |  | 80.8 | 80.6 |  | 23.5 | 23.4 |  | 0.63 | 0.65 |  | 38 | 14 |
|  | SparQ |  | 80.9 | 77.5 |  | 80.8 | 79.0 |  | 23.5 | 23.0 |  | 0.63 | 0.65 |  | 209 | 201 |
| Pythia <br> $6.9 \mathrm{~B}$ | LM- $\infty$ | 57.8 | 38.5 | 18.9 | 52.6 | 41.6 | 32.0 | 20.2 | 14.9 | 14.1 | 0.68 | 0.71 | 0.77 | 150 | 64 | 18 |
|  | $\mathrm{H}_{2} \mathrm{O}$ |  | 52.9 | 46.6 |  | 52.6 | 52.3 |  | 20.3 | 18.9 |  | 0.69 | 0.71 |  | 47 | 19 |
|  | SparQ |  | 58.0 | 57.1 |  | 52.4 | 51.7 |  | 20.6 | 20.6 |  | 0.68 | 0.70 |  | 151 | 144 |

appropriate size, appending them with the prompts containing a subset of the context, and evaluating the output exact character length match with the continuation from the context.

Baselines We consider the cache eviction technique $\mathrm{H}_{2} \mathrm{O}$ (Zhang et al., 2023) and LM-Infinite, a local windowing scheme with initial-tokens included proposed by Han et al. (2023) as baselines. For consistency, we allocate a fixed KV cache transfer budget $k$ for both techniques. With $\mathrm{H}_{2} \mathrm{O}$, we set the local window size $l=k / 4$ (with $3 k / 4$ heavy hitters), and for LM-Infinite we always include the first 16 positions (with $k-16$ local positions).

### 5.2. Results

Our experiments span six distinct models: Llama 2 with 7 and 13 billion parameters, Mistral with 7 billion parameters, and three Pythia models with $1.4,2.8$ and 6.9 billion parameters. Results from the largest models are presented in Table 2, with further results in Figures A1 and A2. We observe that:

- SparQ Attention performance is robust across all tasks and model sizes tested. Compression ratios of $2 \times$ to $8 \times$ are readily achievable with little to no loss in task performance.
- Certain tasks are more challenging for $\mathrm{H}_{2} \mathrm{O}$ (Repetition, SQuAD), while others are more forgiving (TriviaQA, WikiText-103).
- LM-Infinite degrades performance across all tasks, demonstrating that the tasks do not permit the trivial solution of discarding the long input sequence.


### 5.3. Ablations and Extensions

Key cache compression The first step in SparQ Attention involves reading $r$ components of the key cache to approximately determine which keys yield the highest attention scores. To examine the practical trade-off of the approximation we look at how SparQ Attention performs when compared to a theoretical upper-bounding "oracle" which provides the exact top- $k$ keys without any data transfer. The results in Figure 4a show that SparQ Attention retains comparable performance to the oracle for a wide range of compression ratios, and attains considerably higher performance than a baseline compression scheme, in which a random low rank projection of $\boldsymbol{K}$ is transferred from memory.

Approximate softmax temperature To empirically support our statistical analysis of $\alpha$ agreement shown in Figure 3 f, we evaluate a number of different viable temperature settings, including the square root of the head dimension ( $\tau=\sqrt{d_{h}}$ ), the square root of the rank $(\tau=\sqrt{r}$ ), and the temperature proposed in Equation (10). We also consider the scenario where we do not reallocate mass to mean value ( $\alpha=0$ ), which corresponds to the limit of the temperature tending towards 0 . We find that our proposed temperature performs best, as shown in Figure 4b.

Hyperparameter selection The reduction of data transfer attained by SparQ Attention is controlled by its two hyperparameters, $k$ and $r$. Reducing either of these variables will improve the bandwidth efficiency, but can negatively impact task performance. Figure 4c shows the relationship between $k$ and $r$ on both of these factors. Based on these results, we propose a simple recipe of setting $k=128$ and tuning $r$ to maintain a good trade-off between data transfer and task performance for a range of models and tasks.

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-07.jpg?height=260&width=788&top_left_y=236&top_left_x=186)

(a) Accuracy results of SparQ Attention and a random low rank compression scheme against an oracle top- $k$ selector.

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-07.jpg?height=253&width=787&top_left_y=627&top_left_x=192)

(b) Comparison of different softmax temperatures for approximate attention scores for two different hyperparameter configurations.
![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-07.jpg?height=262&width=790&top_left_y=1012&top_left_x=190)

(c) Results for Repetition and SQuAD tasks with $r \in\{16,32,64\}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-07.jpg?height=263&width=791&top_left_y=1359&top_left_x=190)

(d) SQuAD performance vs input sequence length. The compression ratio is fixed at $1 / 4$. Uses Vicuna $1.57 \mathrm{~B}$ with $16 \mathrm{k}$ maximum sequence length against our SQuAD (train) task with 7 (default) to 63 confusion contexts to increase the sequence length.

Figure 4: Results of ablations and extensions (Llama 2 7B SQuAD 1-shot performance unless specified).

Sequence length scaling The sequence lengths of different examples in our main tasks vary between $1 \mathrm{k}$ and $2 \mathrm{k}$ tokens, whereas many LLMs support sequence lengths far greater than this. We developed a variation of the SQuAD task that increases the task difficulty, as well as the sequence length by increasing the number of confusion contexts present in the prompt in Figure 4d, which is akin to increasing the number of retrieved documents with a retrieval augmented generation system (Borgeaud et al., 2022). We test SparQ Attention and $\mathrm{H}_{2} \mathrm{O}$ in this setting using Vicuna (Chiang et al., 2023), a descendent of Llama 2 that has been adapted and for longer sequences. Both SparQ Attention

![](https://cdn.mathpix.com/cropped/2024_06_04_031fc89d5ecd303f5da4g-07.jpg?height=393&width=828&top_left_y=232&top_left_x=1060)

Figure 5: Microbenchmark results for batch size 64, 32 heads, $d_{h}=128, r=32, k=128$, A100 (40GB).

and $\mathrm{H}_{2} \mathrm{O}$ are configured to maintain a fixed compression ratio versus the dense baseline (keeping $r=32$ and modifying $k$ to maintain $1 / 4$ compression), showing that SparQ Attention is scalable to large sequences.

## 6. Benchmarking

The results above use a theoretical cost model of total memory transfers, allowing us to evaluate SparQ Attention independently of a specific hardware setup. To validate this approach, we performed a set of microbenchmarks of an attention operation in isolation.

SparQ Attention benefits from two optimisations. The first is to store $\boldsymbol{K}$ twice, in both $d_{h}$-contiguous and $S$-contiguous layouts, since this allows for an efficient gather (indexing) on either axis, at the cost of $50 \%$ extra memory usage. The second optimisation is to use a fused gather-then-matmul operation to avoid writing the result of the gather to memory.

We tested multiple implementations of baseline and SparQ Attention on IPU using the Poplar C++ interface and GPU using PyTorch (Paszke et al., 2019). In all cases, we used the Llama 7B shape parameters: 32 heads, $d_{h}=128$. The implementations tested were: Dense baseline, choosing the faster of a plain PyTorch implementation and the builtin scaled_dot_product_attention, SparQ (Triton), storing $K$ twice \& using fused gather-then-matmul kernels written using Triton (Tillet et al., 2019), SparQ (PyTorch), with no Triton and SparQ (Triton, $1 \times \boldsymbol{K}$ ), storing $\boldsymbol{K}$ in $d_{h^{-}}$ contiguous layout only, for no additional memory cost. In an example configuration running on a single IPU from a Bow $\operatorname{Pod}_{16}$, batch size 1, sequence length $S=16384$, the dense baseline achieves $40.4 \mathrm{~ms} / q u e r y$, while SparQ ( $r=32$, $k=128$ ) achieves $5.28 \mathrm{~ms} /$ query for a speedup of $7.41 \times$ (the theoretical speedup of SparQ is $7.53 \times$ ). This nearperfect speedup is achieved because attention is strongly memory bound when using remote memory. In contrast, the baseline running in local SRAM takes $134 \mu$ s for a $345 \times$ speedup, but this is only practically achievable when the whole model fits in SRAM.

Table 3: GPU performance with batch size 64, sequence length $S=4096, r=32, k=128$. The theoretical speedup is $6.4 \times$.

| Kernel | A100 (40GB) | A10G |
| :---: | :---: | :---: |
| Dense | $49 \mu \mathrm{s}(1 \times)$ | $128 \mu \mathrm{s}(1 \times)$ |
| SparQ (Triton, $1 \times \boldsymbol{K})$ | $38 \mu \mathrm{s}(1.28 \times)$ | $79 \mu \mathrm{s}(1.63 \times)$ |
| SparQ (PyTorch) | $37 \mu \mathrm{s}(1.33 \times)$ | $78 \mu \mathrm{s}(1.63 \times)$ |
| SparQ (Triton) | $\mathbf{1 6} \boldsymbol{\mu s}(\mathbf{3 . 0 2} \times)$ | $\mathbf{3 1} \boldsymbol{\mu s}(\mathbf{4 . 1 7} \times)$ |

Our achieved GPU speed-ups are presented in Table 3, and the performance trend with sequence length is shown in Figure 5. Standard error for all results given is $<1 \%$ of the mean. See Appendix E for further details.

These microbenchmark results show that the theoretical benefits of SparQ Attention can yield substantial wall-clock time speedups on current hardware. Further work is needed to show improvements for small batch size, and to investigate alternatives to storing $\boldsymbol{K}$ twice.

## 7. Related Work

Efficient attention methods have been a very active area of research (Tay et al., 2020b). Schemes such as Sparse Transformers (Child et al., 2019), Combiner (Ren et al., 2021), Longformer (Beltagy et al., 2020), BigBird (Zaheer et al., 2020), Reformer (Kitaev et al., 2020) and Sparse Sinkhorn Attention (Tay et al., 2020a) have been developed to increase efficiency of the attention mechanism by extracting information from the most salient tokens in the sequence or approximating dense attention maps. Two schemes that reduce memory footprint and data transfer of the attention operation, while maintaining quadratic complexity are Multi-Query Attention (MQA) (Shazeer, 2019) and Grouped-Query Attention (GQA) (Ainslie et al., 2023) that share each KV head across multiple query heads. These methods form part of the architecture: they must be implemented during pre-training, carry varying task performance trade-offs, and may affect model quality and stability.

An emerging area of research similar to SparQ Attention aims to only adapt the inference procedure of a pre-trained model. The simplest method of this category is part of FlexGen (Sheng et al., 2023), and calculates exact attention scores, retrieving only the values associated with the top$k$ scores. This process uses the full key cache to produce attention scores, limiting the asymptotic reduction of the memory transfers to only $50 \%$. LM-Infinite (Han et al., 2023) and StreamingLLM (Xiao et al., 2023) employ a fixed sparsity pattern preserving the most recent tokens and a few initial tokens for better attention efficiency, but are not selective in their cache lookup.
Eviction schemes cache only a subset of keys and values, by continually deleting tokens that are uninformative for future outputs. By reducing the cache size itself, both the amount of memory used and data transferred are reduced. $\mathrm{H}_{2} \mathrm{O}$ (Zhang et al., 2023) and Scissorhands (Liu et al., 2023a) are examples of such eviction methods. $\mathrm{H}_{2} \mathrm{O}$ uses a greedy eviction policy that maintains in memory the most salient "Heavy Hitter" tokens that contribute most to the attention scores. Scissorhands identifies and maintains "pivotal tokens" by counting when a token's attention score exceeds an importance threshold. While these methods reduce the memory footprint of the $\mathrm{KV}$ cache as well as data transfer, they also lead to permanent loss of information from the context window, which can lead to mistakes for queries seeking less-attended parts of the sequence.

IceFormer (Mao et al., 2023) uses multiple existing approximate nearest neighbour algorithms for approximating attention scores of pre-trained models, but focuses solely on improving CPU performance, while SparQ Attention aims to be hardware-agnostic. Scatterbrain (Chen et al., 2021) employs similar techniques, but for computer vision applications.

An orthogonal line of work increases bandwidth efficiency by compressing the $\mathrm{KV}$ cache with 4 -bit number formats (Liu et al., 2023b; Sheng et al., 2023). Liu et al. (2023a) demonstrate that 4 -bit compression is complementary to techniques that reduce the number of transferred elements.

## 8. Discussion

In this work we have presented SparQ Attention, a novel technique for unlocking faster inference for pre-trained LLMs. Our proposed technique modifies the attention mechanism to access only the relevant tokens from the KV cache on every generation step, leading to considerable data transfer savings. This allows for pre-trained models to be executed more efficiently, without any fine-tuning or modifications to the weights of the model.

One limitation of SparQ Attention is that it does not reduce memory capacity, only the amount of data transferred. However, as no information is discarded from the input sequence, it is more robust to performance degradation compared to lossy alternatives.

LLM inference speed is often bottlenecked by memory transfers rather than computation, especially in regimes of long sequence lengths and limited memory bandwidth. SparQ Attention addresses this, increasing the arithmetic intensity of LLMs with up to an eightfold compression of the transferred $\mathrm{KV}$ cache data and little to no task performance degradation. It is easy to implement on high-performance accelerators and obtains substantial throughput improvements.

## 9. Broader Impact

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

## References

Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebrón, F., and Sanghai, S. GQA: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.

Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. PMLR, 2023.

Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pp. 2206-2240. PMLR, 2022.

Chen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and Ré, C. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 34: 17413-17426, 2021.

Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023.

Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

Graphcore. Bow-2000 datasheet. (Online: accessed 25 January 2024), March 2023. URL https://docs.graphcore.ai/projects/ bow-2000-datasheet.

Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. LM-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

Karpathy, A. The unreasonable effectiveness of recurrent neural networks. (Online: accessed 27 January 2024), 2015. URL https://github.com/karpathy/ char-rnn.

Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.

Lin, C.-Y. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics.

Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis, A., and Shrivastava, A. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023a.

Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. LLMQAT: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023b.

Mao, Y., Ester, M., and Li, K. Iceformer: Accelerated inference with long-sequence transformers on CPUs. In Third Workshop on Efficient Natural Language and Speech Processing (ENLSP-III): Towards the Future of Large Language Models and their Emerging Descendants, 2023.

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

NVIDIA. NVIDIA A10 datasheet. (Online: accessed 22 January 2024), March 2022. URL https://www.nvidia.com/content/dam/ en-zz/Solutions/Data-Center/a10/pdf/ datasheet-new/nvidia-a10-datasheet. pdf.

NVIDIA. NVIDIA H100 datasheet. (Online: accessed 22 January 2024), July 2023. URL https: / /www. nvidia.com/en-gb/data-center/h100/.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.

Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.

Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre-training. (Online: accessed 29 January 2024), 2018. URL https://openai.com/research/ language-unsupervised.

Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuurmans, D., and Dai, B. Combiner: Full attention transformer with sparse computation cost. Advances in Neural Information Processing Systems, 34:22470-22482, 2021.

Roche, A., Malandain, G., Pennec, X., and Ayache, N. The correlation ratio as a new similarity measure for multimodal image registration. In Medical Image Computing and Computer-Assisted Intervention - MICCAI'98: First International Conference Cambridge, MA, USA, October 11-13, 1998 Proceedings 1, pp. 1115-1124. Springer, 1998 .

Rosenblatt, M. Remarks on Some Nonparametric Estimates of a Density Function. The Annals of Mathematical Statistics, 27(3):832 - 837, 1956.

See, A., Liu, P. J., and Manning, C. D. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017.

Shazeer, N. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019.

Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen, B., Liang, P., Ré, C., Stoica, I., and Zhang, C. FlexGen: high-throughput generative inference of large language models with a single GPU. In International Conference on Machine Learning, pp. 31094-31116. PMLR, 2023.

Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864, 2021. URL https://arxiv. org/abs/2104.09864.
Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C. Sparse sinkhorn attention. In International Conference on Machine Learning, pp. 9438-9447. PMLR, 2020a.

Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. CoRR, abs/2009.06732, 2020b. URL https://arxiv.org/abs/2009.06732.

Tillet, P., Kung, H.-T., and Cox, D. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pp. 10-19, 2019.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Vig, J. A multiscale visualization of attention in the transformer model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 37-42, 012019.

Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022.

Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.

Yun, C., Chang, Y.-W., Bhojanapalli, S., Rawat, A. S., Reddi, S., and Kumar, S. $O(n)$ connections are expressive enough: Universal approximability of sparse transformers. Advances in Neural Information Processing Systems, 33:13783-13794, 2020.

Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283-17297, 2020.

Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Ré, C., Barrett, C., et al. $\mathrm{H}_{2} \mathrm{O}$ : Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023.
