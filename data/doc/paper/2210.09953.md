# Randomized Cholesky QR factorizations 

Oleg Balabanov *


#### Abstract

This article proposes and analyzes several variants of the randomized Cholesky QR factorization of a matrix $X$. Instead of computing the $\mathrm{R}$ factor from $X^{T} X$, as is done by standard methods, we obtain it from a small, efficiently computable random sketch of $X$, thus saving computational cost and improving numerical stability. The proposed direct variant of the randomized Cholesky QR requires only half the flops and the same communication cost as the classical Cholesky QR. At the same time, it is more robust since it is guaranteed to be stable whenever the input matrix is numerically full-rank. The rank-revealing randomized Cholesky QR variant has the ability to sort out the linearly dependent columns of $X$, which allows to have an unconditional numerical stability and reduce the computational cost when $X$ is rank-deficient. We also depict a columnoriented randomized Cholesky QR that establishes the connection with the randomized Gram-Schmidt process, and a reduced variant that outputs a low-dimensional projection of the $\mathrm{Q}$ factor rather than the full factor and therefore yields drastic computational savings. It is shown that performing minor operations in higher precision in the proposed algorithms can allow stability with working unit roundoff independent of the dominant matrix dimension. This feature may be of particular interest for a QR factorization of tall-and-skinny matrices on low-precision architectures.


Key words - QR factorization, randomization, sketching, Cholesky, rank-revealing, numerical stability, rounding errors, loss of orthogonality, multi-precision arithmetic, communication-avoiding algorithms.

## 1 Introduction

This work is devoted to computing a thin $\mathrm{QR}$ factorization $\mathbf{X}=\mathbf{Q R}$ of matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$ with $n \ll m$, where $\mathbf{R}$ is upper triangular or trapezoidal, possibly with permuted columns, and $\mathbf{Q}$ is approximately orthonormal or very well-conditioned. Such QR factorizations constitute the basic kernels for many scientific computing algorithms. Applications include solution of least-squares problems [15], computation of low-rank approximations [19], solution of linear systems and eigenvalue problems [15, 26, 27], model order reduction [18], and more. There is a vast variety of algorithms for computing a $\mathrm{QR}$ factorization. They can be majorly divided into three main categories: the ones based on Householder transformations or Givens rotations [15, 20], the ones based on Gram-Schmidt orthogonalization [21], and the ones based on Cholesky QR [13]. We are here interested in the latter type of algorithms.

In scientific computing, special attention must be paid to the numerical stability of algorithms, that is, their sensitivity to rounding errors in finite precision arithmetic [20]. A computed $\mathrm{QR}$ factorization of $\mathbf{X}$ can be said to be numerically stable if cond $(\mathbf{Q})=\mathcal{O}(1)$ and the columns of $\mathbf{Q R}$ approximate the columns of $\mathbf{X}$ up to machine precision.

### 1.1 Cholesky QR

A Cholesky QR (CholeskyQR) factorization of $\mathbf{X}$ proceeds by first obtaining the $\mathrm{R}$ factor through a Cholesky factorization of the Gramian $\mathbf{X}^{\mathrm{T}} \mathbf{X}$, and then retrieving the $\mathrm{Q}$ factor by forward substitution, as shown in Algorithm 1. This procedure is well suited for distributed computing because it requires only one global synchronization[^0]

```
Algorithm 1 Cholesky QR (CholeskyQR)
    Input:
        $\mathbf{X}$ is $m \times n$ matrix
    Output:
        $\mathbf{Q}$ is $m \times n$ orthonormal $\mathrm{Q}$ factor
        $\mathbf{R}$ is $n \times n$ upper triangular $\mathbf{R}$ factor
    function $[\mathbf{Q}, \mathbf{R}]=\operatorname{CholeskyQR}(\mathbf{X})$
    1. $\mathbf{A} \leftarrow \mathbf{X}^{\mathrm{T}} \mathbf{X}$
    2. $\mathbf{R} \leftarrow \operatorname{chol}(\mathbf{A})$
    3. $\mathbf{Q} \leftarrow \mathbf{Q R}^{-1}$
```

between processors. It can be seen as an alternative to another communication-avoiding algorithm called TSQR [9]. The computational advantages of CholeskyQR over TSQR include the twice less computational cost in terms of flops, lower amount of global synchronizations between processors, and a simpler reduction operator. However, unlike TSQR, which is ultimately stable, CholeskyQR often introduces instabilities that limit its use. In particular, one can guarantee numerical stability of the algorithm only by making sure that the unit roundoff satisfies $u \leq \operatorname{cond}(\mathbf{X})^{-2} F(m, n)^{-1}$, where $F(m, n)$ is a low-degree polynomial [31]. To alleviate this drawback, a shifted Cholesky QR with reorthogonalization (shifted CholeskyQR2) has recently been proposed, which is stable under the condition that $u \leq \operatorname{cond}(\mathbf{X})^{-1} F(m, n)^{-1}[14]$. The shifted CholeskyQR2, however, doubles the cost of the classical CholeskyQR. Moreover, the aforementioned stability guarantees may still be insufficient for badly scaled or ill-conditioned problems.

This article proposes randomized Cholesky QR factorizations that can be more than four times as efficient as shifted CholeskyQR2 in terms of flops, and twice as efficient in terms of communication cost, and yet they achieve at least as good stability as that of shifted CholeskyQR2 or even unconditional stability like that of Householder $\mathrm{QR}$ or $\mathrm{TSQR}$.

### 1.2 Randomized Cholesky QR

We provide two methods to compute a $\mathrm{QR}$ factorization: the (direct) randomized Cholesky QR (RCholeskyQR) and the rank-revealing randomized Cholesky QR (RRRCholeskyQR) that are based on a dimension reduction technique called random sketching. See $[23,29]$ for an overview of this technique for scientific computing. Both RCholeskyQR and RRRCholeskyQR first compute the factor $\mathbf{R}$ by a low-dimensional QR factorization of a random sketch $\Theta \mathbf{X} \in \mathbb{R}^{k \times n}$ of $\mathbf{X}$ and then retrieve the factor $\mathbf{Q}$ by forward substitution. The matrix $\Theta \in \mathbb{R}^{k \times m}$ is a suitable random matrix typically with $k=\mathcal{O}(n)$ (say $k=2 n$ ) rows that can be efficiently applied to $\mathbf{X}$ in the given architecture, and that is with high probability an approximate isometry for the column space of $\mathbf{X}$. In the algorithms, the $\mathrm{R}$ factor can be computed for instance with the Cholesky factorization of the sketched Gramian $(\boldsymbol{\Theta} \mathbf{X})^{\mathrm{T}}(\boldsymbol{\Theta} \mathbf{X})$. In this case the proposed QR factorizations become aligned with the direct and rank-revealing generalized Cholesky $\mathrm{QR}$ associated with the sketched inner product $\langle\boldsymbol{\Theta} \cdot \boldsymbol{\Theta} \cdot\rangle_{2}$. This fact gives our methods their names. However, since $\boldsymbol{\Theta X}$ is small, computing the $\mathrm{R}$ factor from $\boldsymbol{\Theta} \mathbf{X}$ using the CholeskyQR may become inappropriate and should be done by more expensive but more reliable methods based, for instance, on Householder transformations or Givens rotations. See Sections 3 and 4 for more details. It has to be underlined that although randomization entails a possible failure of the algorithms, here this should not cause any concerns. We take the probability of failure as a user-defined parameter that can be chosen to be astronomically small, say $10^{-10}$ or $10^{-20}$ without much impact on computational cost.

We show that the RCholeskyQR is stable when the matrix $\mathbf{X}$ is numerically full-rank, which makes it at least as robust as shifted CholeskyQR2. Yet it should take significantly less flops and communication between distributed processors. Moreover, the gain in the computational cost can be drastic when only a small projection of $\mathbf{Q}$ is needed rather than the full matrix (see Section 3.3). The RRRCholeskyQR variant is shown to be even more stable than RCholeskyQR, and provides QR factorization with well-conditioned $\mathrm{Q}$ factor and column-wise approximation error close to machine precision under the condition $u<F(m, n)^{-1}$ that does not involve $\mathbf{X}$. At the same time, this
factorization can inherit the high efficiency of RCholeskyQR or even improve it by a great amount if (normalized) $\mathbf{X}$ is a low-rank matrix. This makes RRRCholeskyQR a very desirable alternative to Householder QR, TSQR and other unconditionally stable QR factorizations in applications. It is then noticed that by performing the minor operations in RCholeskyQR and RRRCholeskyQR algorithms in higher precision, one can guarantee stability of the algorithms using working unit roundoff $u$ independent of the high dimension $m$ i.e. to have $F(m, n)=F(n)$. This can be particularly useful for the $\mathrm{QR}$ factorization of tall-and-skinny matrices on low precision architectures.

The RCholeskyQR and RRRCholeskyQR methods are closely related to the Randomized Gram-Schmidt (RGS) $\mathrm{QR}$ factorization and its block version recently proposed in $[2,3]$. The RGS process was successfully applied to improving Krylov methods for the solution of linear systems and eigenvalue problems. See [1] for an open-source implementation of the RGS-based Krylov methods that can provide more than $10 \times$ speedups over the standard built-in functions. Since RGS also proceeds with the orthogonalization of the random sketch of $\mathbf{X}$, it inherits some of the properties of randomized Cholesky QR algorithms from this paper.

Related work. The idea of using a QR factorization with respect to the sketched inner product goes back to $[4$, 5], where it was employed to improve conditioning of reduced basis for (parametric) linear systems. This approach is aligned with the reduced RCholeskyQR given in Section 3.3. In this context, our main contribution is to obtain rigorous guarantees of numerical stability.

A QR factorization with the RGS algorithm was introduced in [3] and was recently extended to block matrices in [2]. In exact arithmetic, the output of RCholeskyQR and RGS is the same. These algorithms are also closely related from a numerical point of view. As pointed out in [3, Remark 2.10], in RGS it is possible to compute the sketch of the newly obtained column of $\mathbf{Q}$ from the sketches of previously computed vectors, rather than by multiplying that column by $\Theta$. This modification leads to an algorithm that is nothing more than a column-oriented variant of RCholeskyQR (see Section 3.2). However, as noted in [3, Remark 2.10] and also verified in our numerical experiments, the column-oriented RCholeskyQR may be less numerically stable than RGS. A similar picture is observed for the blockwise version of RCholeskyQR [2]. Nevertheless, we show that RCholeskyQR has stability guarantees similar to RGS when $\mathbf{X}$ is numerically full-rank, which is a fairly common situation. More information about the connection between the column-oriented RCholeskyQR and RGS can be found in Section 3.2. In addition, in [2, Section 2.2.2] the authors proposed a direct RCholeskyQR algorithm, the same as in this article. However, they omitted its analysis.

There is another recent work [12], which in parallel to [2] explored the ideas of the direct RCholeskyQR. In particular, [12, Algorithm 2.6] is similar to the RCholeskyQR2 algorithm presented in this paper. The key difference is that in [12] the authors mainly rely on uniform sampling of rows of $\mathbf{X}$ rather than sketching with oblivious subspace embeddings or nonuniform sampling. Note that sketching using Gaussian matrices and nonuniform sampling have also been mentioned, but only as a theoretical supplement to the uniform sampling approach. In particular, Gaussian matrices have been left out of the scope of numerical experiments, theoretical discussions of computational costs, and stability analysis. At the same time, the provided in [12] nonuniform sampling is infeasible for practical use, as it requires a priori knowledge of the $\mathrm{Q}$ factor. Our article in this sense is more complete. It fully explores sketching with Gaussian matrices both theoretically and numerically. In addition, it takes into account other dimension reduction maps. For instance, we consider sketching with subsampled randomized Hadamard transform, which, unlike the methodology from [12], leads to provably stable and accurate algorithms that have less complexity and memory consumption compared to shifted CholeskyQR3 or other deterministic algorithms. A similar result can be achieved using nonuniform row sampling approaches from [7, 10, 29], which are also considered.

The proposed RRRCholeskyQR method is connected to the randomized $\mathrm{QR}$ with column pivoting or the randomized rank-revealing $\mathrm{QR}$ from $[11,22,24,25,30]$. First of all, here we primarily focus on the QR factorization of tall-and-skinny matrices, and not on the $\mathrm{QR}$ factorization of large (low-rank) matrices, considered in the abovementioned works. Though, the RRRCholeskyQR factorization in principle can also be used in the latter context. The main advantage of RRRCholeskyQR in this case is that, unlike other methods, it calculates the R factor solely from the random sketch, which can significantly reduce computational cost. In particular, RRRCholeskyQR can require up to $\frac{n}{r}$ less flops, where $r$ is the approximation rank.

The idea of performing a CholeskyQR with multi-precision arithmetic was explored in [32]. In contrast to [32], here we propose to increase the rounding accuracy only for minor operations with little impact on the overall
computational cost. Moreover, we prove numerical stability for working rounding independent of the dominant dimension $m$. As far as we know, this property is inherent only in randomized algorithms.

The rest of the paper is organized as follows. Section 1.3 describes main notations. In Section 2, we introduce the random sketching technique along with some of the results underlying our randomized algorithms. In Section 3 we introduce the direct RCholeskyQR factorization and discuss its derivatives, which are a column-oriented version and a reduced version. Section 4 is devoted to RRRCholeskyQR factorization. Numerical stability of the proposed algorithms is characterized in Section 5. The methodology is validated numerically in Section 6. Section 7 concludes the article.

### 1.3 Preliminaries

In the manuscript, we use the following notations and assumptions. For a given matrix $\mathbf{A}$, the $i$-th column of $\mathbf{A}$ is denoted by $\mathbf{A}_{(i)}$ or $\mathbf{A}_{(:, i)}$, and the $i$-th row is denoted by $\mathbf{A}_{(i,:)}$. A submatrix consisting of the $i$-th through the $j$-th consecutive columns of $\mathbf{A}$ is denoted by $\mathbf{A}_{(i: j)}$ or $\mathbf{A}_{(:, i: j)}$, and the one that consists of the $i$-th through the $j$-th consecutive rows, by $\mathbf{A}_{(i: j,:)}$. In addition, the $k$-th column of the submatrix $\mathbf{A}_{(i: j,:)}$ is denoted by $\mathbf{A}_{(i: j, k)}$. If A is a block matrix, then the above notation is used to denote submatrices composed of the corresponding blocks of $\mathbf{A}$. For instance, in this case $\mathbf{A}_{(i: j, k)}$ denotes a submatrix composed by column-wise concatenation of $(i, k)$-th, $(i+1, k)$-th, $\ldots,(j, k)$-th blocks of $\mathbf{A}$. We denote by $|\mathbf{A}|$ the matrix whose entries are the absolute values of the corresponding entries of the matrix $\mathbf{A}$. We let $\sigma_{\min }(\mathbf{A})$ and $\sigma_{\max }(\mathbf{A})$ denote the minimal and maximal singular values of $\mathbf{A}$, and cond $(\mathbf{A})$ to denote the condition number $\frac{\sigma_{\max }(\mathbf{A})}{\sigma_{\min }(\mathbf{A})}$. In addition, $\mathbf{A}^{\mathrm{T}}$ and $\mathbf{A}^{\dagger}$ denote the transpose of A and the Moore-Penrose pseudo-inverse of $\mathbf{A}$, respectively. We let $\mathbf{I}$ be an identity matrix of size appropriate for the expression in which it is used. An arithmetic expression or quantity $A$ computed using finite precision arithmetic is denoted by $\widehat{A}$ or $f l(A)$. In addition, in the article we will assume that the dominant operations in randomized algorithms are performed with unit roundoff $u$, and the secondary operations, which are random projections and low-dimensional operations, use unit roundoff $u_{f}$ lower than $u$ by sufficiently large low-degree polynomial in $m$ and $n$.

## 2 Random sketching technique

Let $\boldsymbol{\Theta} \in \mathbb{R}^{k \times m}$ be a sketching matrix with $k \ll m$ rows. This matrix is seen as an embedding of low-dimensional subspaces of $\mathbb{R}^{m}$ into $\mathbb{R}^{k}$. In addition, it is chosen such that it is an approximate isometry of the subspace(s) of interest, or in other words, an $\varepsilon$-embedding.

Definition 2.1. Sketching matrix $\boldsymbol{\Theta}$ is called an $\varepsilon$-embedding for $\mathbf{V} \in \mathbb{R}^{m \times d}$ or range $(\mathbf{V})$ if

$$
(1-\varepsilon)\|\mathbf{V} \mathbf{x}\|_{2}^{2} \leq\|\boldsymbol{\Theta} \mathbf{V} \mathbf{x}\|_{2}^{2} \leq(1+\varepsilon)\|\mathbf{V} \mathbf{x}\|_{2}^{2}
$$

Our analysis assumes that the parameter $\varepsilon$ is set by the user. However, it should be underlined that for the applications in this paper it is not necessary to consider very small values of $\varepsilon$. It suffices to take $\varepsilon=\frac{1}{2}$ or $\varepsilon=\frac{1}{4}$.

As pointed out in $[3,4]$, the $\varepsilon$-embedding property of $\Theta$ allows $\mathbf{V}$ to be approximately orthonormalized by orthonormalizing the sketch $\boldsymbol{\Theta}$. This observation follows from Corollary 2.2 and underlies all our randomized algorithms.

Corollary 2.2 (Corollary 2.2 in [3]). Let $\mathbf{V} \in \mathbb{R}^{m \times d}$ be some matrix. If $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for $\mathbf{V}$, then the singular values of $\mathbf{V}$ are bounded by

$$
(1+\varepsilon)^{-\frac{1}{2}} \sigma_{\min }(\boldsymbol{\Theta} \mathbf{V}) \leq \sigma_{\min }(\mathbf{V}) \leq \sigma_{\max }(\mathbf{V}) \leq(1-\varepsilon)^{-\frac{1}{2}} \sigma_{\max }(\boldsymbol{\Theta} \mathbf{V})
$$

In algorithms, we prefer to construct $\Theta$ without any a priori information about the $\mathbf{V}$ matrix. This can be done in a probabilistic way by drawing $\Theta$ from a carefully designed probability distribution such that $\Theta$ satisfies the $\varepsilon$-embedding property for any fixed $m \times d$ matrix with high probability. Such $\Theta$ will be called an $(\varepsilon, \delta, d)$-oblivious subspace embedding (or $(\varepsilon, \delta, d)$-OSE), as defined below.

Definition 2.3. Random matrix $\Theta$ is called an ( $\varepsilon, \delta, d)$-OSE if it is an $\varepsilon$-embedding for any (fixed) $m \times d$ matrix $\mathbf{W}$ with probability at least $1-\delta$.

The advantage of our randomized algorithms is that they do not rely on a particular OSE distribution for $\Theta$ but rather allow the ability to choose an OSE depending on the given application and the computational architecture to obtain the most computational benefit. For instance, in a classical sequential environment, the most beneficial OSE can be the subsampled randomized Hadamard transform (SRHT) as it can be applied to vectors with as little as $n \log _{2} n$ flops (compared to the $2 n k$ required by unstructured matrices) due to the tensor structure of the Hadamard transform. On the other hand, if memory consumption is the main concern, then both structured and unstructured OSEs can result in significant computational savings since they can be constructed and operated with a seeded random number generator, implying negligible memory consumption of $\Theta$. The advantage of (rescaled) Gaussian or Rademacher OSEs over SRHT is their high suitability for cache-based or massively parallel computational environments. Finally, the CountSketch OSE is computationally advantageous if the $\mathbf{V}$ matrix we want to orthogonalize is sparse. In this work, we chose SRHT and rescaled Gaussian matrices as representative OSEs. The SRHT matrix is defined as a product of a diagonal matrix of random signs (possibly padded with zeros to make the output dimension a power of 2) with structured Hadamard matrix, followed by a uniform sampling matrix and a scaling factor $\sqrt{\frac{1}{k}}$. It follows from $[4,6,28]$ that SRHT matrix with

$$
\begin{equation*}
k \geq 2\left(\varepsilon^{2}-\varepsilon^{3} / 3\right)^{-1}\left(\sqrt{d}+\sqrt{8 \log \frac{6 m}{\delta}}\right)^{2} \log \frac{3 d}{\delta} \tag{1a}
\end{equation*}
$$

rows is an $(\varepsilon, \delta, d)$-OSE. The rescaled Gaussian matrices have entries that are i.i.d. Gaussian variables, scaled by a factor $\sqrt{\frac{1}{k}}$. Such matrices satisfy the OSE property, if $[4]$

$$
\begin{equation*}
k \geq 7.87 \varepsilon^{-2}\left(6.9 d+\log \frac{1}{\delta}\right) \tag{1b}
\end{equation*}
$$

We see that for both SRHT and Gaussian matrices the required number of rows $k$ is independent or only logarithmically dependent on the dimension $m$ and the probability of failure $\delta$, suggesting the potential dimension reduction with these OSEs. Furthermore, it has to be noted that even-though the theoretical bounds for SRHT matrices are somewhat worse than for Gaussian matrices, in our applications these embeddings provide practically the same results. In particular, it is revealed that for both SRHT and Gaussian matrices the sampling dimension $k=\mathcal{O}(d)$ (say $k=2 d$ or $k=4 d$ ) should be sufficient.

Apart from OSEs, there is another way to efficiently obtain an $\varepsilon$-embedding that should be mentioned. However, it would require a priori knowledge of the $\mathbf{V}$ matrix, and therefore would not have all the attendant advantages that the OSEs have. We can proceed as follows. We can choose the sketching matrix $\boldsymbol{\Theta}$ to define a rescaled nonuniform sampling according to a probability distribution $q_{1}, q_{2}, \ldots q_{m}$, the so-called leverage-scores, as explained in Proposition 2.4.

Proposition 2.4 (Leverage scores sampling, corollary of Theorem 17 in [29]). Let $\mathbf{V}$ be some $m \times d$ matrix. Let $\boldsymbol{\Theta}=\boldsymbol{\Gamma} \mathbf{D}$, where $\boldsymbol{\Gamma}$ is $k \times m$ sampling with replacement matrix that samples the $i$-th entry of the input vector with probability $q_{i}$, and $\mathbf{D}$ is $m \times m$ diagonal matrix with entries $\mathbf{D}_{(i, i)}=\frac{1}{\sqrt{k q_{i}}}$. Furthermore, assume that $q_{1}, q_{2}, \ldots q_{m}$ are such that

$$
q_{i} \geq \beta \frac{1}{k}\left\|\mathbf{W}_{(i,:)}\right\|_{2}^{2}
$$

where $\beta>0$ is some parameter, and $\mathbf{W}_{(i,:)}$ are rows of some orthonormal matrix $\mathbf{W}$ with range $(\mathbf{W})=\operatorname{range}(\mathbf{V})$, $1 \leq i \leq m$. If

$$
k>144 d \varepsilon^{-2} \ln \frac{2 d}{\delta} / \beta
$$

then $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for $\mathbf{V}$ with probability at least $1-\delta$.

For a given $\mathbf{V}$, one can calculate the distribution $q_{1}, q_{2}, \ldots q_{m}$ with the approximation parameter $\beta=\frac{1}{2}$ in Proposition 2.4 using only $\mathcal{O}\left(m d \log d+d^{3}\right)$ flops [10] or even $\mathcal{O}\left(\mathrm{nnz}(\mathbf{V}) \log d+d^{3}\right)$ flops [7]. Consequently, in this way
one can efficiently obtain a sampling matrix $\boldsymbol{\Theta}$ with $k=\mathcal{O}(d \log d)$ rows, which satisfies the $\varepsilon$-embedding property for $\mathbf{V}$ with high probability. We omit further details.

Furthermore, it was shown in [3] that sketching with an OSE should not significantly increase rounding errors (in the worst case). This important result will be used in Section 5 to characterize the stability of the proposed randomized algorithms using rounding that does not depend on the dimension $m$. It is summarized in Corollary 2.5.

Corollary 2.5 (Corollary of Theorem 2.5 and Corollary 2.6 in [3]). Given $\mathbf{Y} \in \mathbb{R}^{m \times n}$ and $\mathbf{Z} \in \mathbb{R}^{n \times l}$ possibly depending on $\boldsymbol{\Theta}$, consider the product

$\mathbf{Y Z}$,

computed with finite precision arithmetic. Consider probabilistic rounding model, where the rounding errors due to each elementary arithmetic operation are random variables possibly depending on each other, but are independently centered. Furthermore assume that the errors are bounded so that, it holds

$$
|\mathbf{Y Z}-f l(\mathbf{Y Z})| \leq \mathbf{U}
$$

for some matrix $\mathbf{U}$. If $\Theta$ is a $\left(\varepsilon / 4, l^{-1}\binom{m}{d}^{-1} \delta, d\right)$ OSE, with $d=4.2 c^{-1} \log \frac{4}{\delta}$, where $c \leq 1$ is some universal constant, then

$$
\begin{equation*}
\left\|\boldsymbol{\Theta}\left(\mathbf{Y} \mathbf{Z}_{(:, i)}-f l\left(\mathbf{Y} \mathbf{Z}_{(:, i)}\right)\right)\right\|_{2} \leq \sqrt{1+\varepsilon}\left\|\mathbf{U}_{(:, i)}\right\|_{2} \tag{2}
\end{equation*}
$$

holds with probability at least $1-2 \delta$ for $i=1,2, \ldots, l$ simultaneously.

Corollary 2.5 says that in practice the sketch $\boldsymbol{\Theta}(\mathbf{Y Z}-f l(\mathbf{Y Z}))$ of the rounding error matrix should have column norms not much larger than the worst-case bound of $\mathbf{Y Z}-f l(\mathbf{Y Z})$. We notice an improvement by nearly a factor of $\sqrt{\frac{m}{k}}$ over the following trivial estimate (for SRHT matrices):

$$
\left\|\boldsymbol{\Theta}\left(\mathbf{Y} \mathbf{Z}_{(:, i)}-f l\left(\mathbf{Y} \mathbf{Z}_{(:, i)}\right)\right)\right\|_{2} \leq\|\boldsymbol{\Theta}\|_{2}\left\|\mathbf{Y} \mathbf{Z}_{(:, i)}-f l\left(\mathbf{Y} \mathbf{Z}_{(:, i)}\right)\right\|_{2}=\sqrt{\frac{m}{k}}\left\|\mathbf{U}_{(:, i)}\right\|_{2}
$$

$i=1,2 \ldots, l$. The condition of Corollary 2.5 for $\varepsilon=\frac{1}{2}$ and $l \leq m$ can be satisfied by a Gaussian OSE with $\mathcal{O}\left(\log m \log \frac{1}{\delta}\right)$ rows. For SRHT, this requirement is $\mathcal{O}\left(\log ^{2} m \log ^{2} \frac{1}{\delta}\right)$, although, as has been said, in practice SRHT and Gaussian matrices give very similar results.

## 3 Randomized Cholesky QR

The efficiency and stability of CholeskyQR of $\mathbf{X}$ can be improved by changing the $\ell_{2}$-orthogonality condition

$$
\mathbf{Q}^{\mathrm{T}} \mathbf{Q}=\mathbf{I}
$$

to the sketched one

$$
\begin{equation*}
(\boldsymbol{\Theta} \mathbf{Q})^{\mathrm{T}}(\boldsymbol{\Theta} \mathbf{Q})=\mathbf{I} \tag{3}
\end{equation*}
$$

where $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for $\mathbf{X}$. The sketching matrix $\boldsymbol{\Theta} \in \mathbb{R}^{k \times m}$ can be readily taken as a low-dimensional OSE or a leverage score sampling matrix described in Section 2. Furthermore, in our experiments, it is revealed that the required theoretical bounds (1) for OSEs are pessimistic. In our applications, using SRHT or Gaussian matrices with just $k=2 n$ rows should be sufficient.

According to Corollary 2.2, the matrix $\mathbf{Q}$ that satisfies (3) is very well-conditioned. The obtained sketched QR factorization can be used directly in randomized methods such as block RGS process from [2] or sketched Galerkin and minres approximations with a reduced basis [4, 5]. Alternatively, if having well-conditioned $\mathrm{Q}$ factor is insufficient, the sketched $\mathrm{QR}$ can be post-processed by the classical CholeskyQR to obtain a $\mathrm{Q}$ factor orthonormal to machine precision.

### 3.1 Direct RCholeskyQR

Algorithm 2 depicts RCholeskyQR algorithm for computing a QR factorization of $\mathbf{X}$ that satisfies (3). In step 2 the algorithm computes a QR factorization of a small matrix $\mathbf{P}$. This task can be performed for instance with the classical CholeskyQR in sufficient precision. In this case Algorithm 2 can be viewed as the generalized Cholesky $\mathrm{QR}$ with respect to the sketched (randomized) inner product, which gives Algorithm 2 its name. However, a better way can be to appeal to more stable methods in step 2 such as Householder QR or Givens QR. Note that since the matrix $\mathbf{P}$ is small, using more efficient or less efficient method in step 2 should not have much impact on the overall computational cost of Algorithm 2.

It is shown in Section 5.1 that RCholeskyQR is stable whenever $\mathbf{X}$ is numerically full-rank i.e. when

$$
\begin{equation*}
\operatorname{cond}\left(\mathbf{X}^{*}\right) \leq F(m, n)^{-1} u^{-1} \tag{4}
\end{equation*}
$$

where $F(n, m)$ is a low-degree polynomial, $\mathbf{X}^{*}$ is $\mathbf{X}$ with normalized columns, and $u$ is the unit roundoff. Note that this condition implies that RCholeskyQR should be at least as stable as shifted CholeskyQR2, or even more stable if $\mathbf{X}$ is ill-conditioned or has large first dimension $m$. Our experiments in Section 6 confirm this fact. Furthermore, by performing non-dominant operations in higher precision we can use $F(m, n)$ in (4) independent of $m$.

Let us now characterize the performance of Algorithm 2 in different computational architectures. Using the SRHT matrix as $\boldsymbol{\Theta}$, the computational cost of RCholeskyQR in a classical sequential environment is mainly determined by the computation of $\mathbf{X R}^{-1}$ with forward substitution, which in total requires $m n^{2}$ flops. On distributed architectures, RCholeskyQR should consume only one global synchronization between processors. Moreover, if $\Theta$ is an unstructured OSE, the sketching step is an explicit matrix-matrix product that can be performed with level 3 BLAS routine. In this case the reduction operator is a simple addition as in standard CholeskyQR. This suggests that RCholeskyQR should be four times more efficient in terms of flops, and twice as efficient in terms of communication cost as standard/shifted CholeskyQR2. Note also that both Gaussian and SRHT matrices should have a negligible storage cost due to the use of a seeded random number generator. The sketching step $\mathbf{P} \leftarrow \boldsymbol{\Theta}$ in RCholeskyQR requires only one pass over $\mathbf{X}$, just like computing the Gramian $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ in standard CholeskyQR. From this we conclude that RCholeskyQR should have half the cost of a standard/shifted CholeskyQR2 in terms of data passes. In addition, RCholeskyQR can be even more advantageous when $\mathbf{X}$ is stored column-wise, since in this case the computation of $\boldsymbol{\Theta} \mathbf{X}$ can still be done in single pass, while the computation of $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ cannot.

```
Algorithm 2 Randomized Cholesky QR (RCholeskyQR)
    Input:
        $\mathbf{X}$ is $m \times n$ matrix
        $\Theta$ is $k \times m$ sketching matrix (possibly provided as a function handle)
    Output:
        $\mathbf{Q}$ is $m \times n$ well-conditioned $\mathrm{Q}$ factor
        $\mathbf{S}$ is $k \times n$ orthonormal sketch of $\mathbf{Q}$
        $\mathbf{R}$ is $n \times n$ upper triangular $\mathrm{R}$ factor
    function $[\mathbf{Q}, \mathbf{S}, \mathbf{R}]=\operatorname{RCholeskyQR}(\mathbf{X}, \boldsymbol{\Theta})$
    1. $\mathbf{P} \leftarrow \Theta \mathbf{X}$
    2. $[\mathbf{R}, \mathbf{S}] \leftarrow \mathrm{QR}(\mathbf{P})$
    3. $\mathbf{Q} \leftarrow \mathbf{X R}^{-1}$
```

Remark 3.1 (Sketched SVD). In principle, in step 2, we could have orthogonalized $\mathbf{P}$ with SVD rather than QR factorization. In this case, the matrix $\mathbf{R}$ would have the form $\boldsymbol{\Sigma} \mathbf{V}^{\mathrm{T}}$, where $\boldsymbol{\Sigma}$ is diagonal and $\mathbf{V}$ is orthonormal. Then step 3 could be stably performed by computing $\mathbf{Q} \leftarrow(\mathbf{X V}) \boldsymbol{\Sigma}^{-1}$. In this case, multiplying $\mathbf{X}$ by $\mathbf{V}$ will require twice as many flops than a forward substitution in RCholeskyQR. Numerical analysis of such sketched SVD orthogonalization should be aligned with that of RCholeskyQR presented in Section 5.1.

If the application requires an orthonormal Q factor, and not just a very well-conditioned one, then RCholeskyQR can be augmented with the classical CholeskyQR, resulting in RCholeskyQR2 (see Algorithm 3). The stability of
the RCholeskyQR2 algorithm follows directly from the stability of RCholeskyQR. The computational cost benefit over reorthogonalized shifted CholeskyQR2, i.e., shifted CholeskyQR3, can be characterized in the same way as before. Namely, RCholeskyQR2 should have half the cost in terms of flops, as well as 1.5 times fewer global synchronizations and data passes.

It is important to note that instead of providing the $\mathrm{Q}$ factor as an explicit matrix, we can provide it as a function handle that outputs its products with vectors/matrices. Thus, we can overcome the forward substitution in step 4 and therefore reduce complexity of Algorithm 3 by almost a third. Then the left multiplication of the $\mathrm{Q}$ factor by given matrix $\mathbf{Y}$ can be performed as $(\mathbf{Y Q}) \mathbf{R}^{\prime-1}$, and the right multiplication as $\mathbf{Q}\left(\mathbf{R}^{\prime-1} \mathbf{Y}\right)$. The numerical stability here follows directly from the fact that the matrices $\mathbf{Q}$ and $\mathbf{R}^{\prime}$ are very well conditioned.

```
Algorithm 3 Augmented randomized Cholesky QR (RCholeskyQR2)
    Input:
        $\mathbf{X}$ is $m \times n$ matrix
        $\Theta$ is $k \times m$ sketching matrix (possibly provided as a function handle)
    Output:
        $\mathbf{Q}$ is $m \times n$ orthonormal $\mathrm{Q}$ factor
        $\mathbf{R}$ is $n \times n$ upper triangular $\mathrm{R}$ factor
    function $[\mathbf{Q}, \mathbf{R}]=\operatorname{RCholeskyQR2}(\mathbf{X}, \boldsymbol{\Theta})$
    1. $[\mathbf{Q}, \mathbf{S}, \mathbf{R}] \leftarrow$ RCholeskyQR $(\mathbf{X}, \boldsymbol{\Theta})$
    2. $\mathbf{A} \leftarrow \mathbf{Q}^{\mathrm{T}} \mathbf{Q}$
    3. $\mathbf{R}^{\prime} \leftarrow \operatorname{chol}(\mathbf{A}), \mathbf{R} \leftarrow \mathbf{R}^{\prime} \mathbf{R}$
    4. $\mathbf{Q} \leftarrow \mathbf{Q R}^{\prime-1}$
```


### 3.2 Column-oriented RCholeskyQR

Often the columns of $\mathbf{X}$ are generated recursively from the computed columns of $\mathbf{Q}$ and $\mathbf{R}$ in previous iterations. Let $\mathbf{X}$ be given by $p$ blocks of columns

$$
\mathbf{X}_{(1: p)}=\left[\mathbf{X}_{(1)}, \mathbf{X}_{(2)}, \ldots, \mathbf{X}_{(p)}\right]
$$

where each block of columns $\mathbf{X}_{(i)}$ is obtained from the $\mathrm{QR}$ factorization $\mathbf{Q}_{(1: i-1)} \mathbf{R}_{(1: i-1,1: i-1)}$ of the previously generated matrix $\mathbf{X}_{(1: i-1)}$. This situation appears, for instance, during the generation of a Krylov basis [2] with Arnoldi iteration:

$$
\mathbf{X}_{(i)} \leftarrow \mathbf{A Q}_{(i-1)}
$$

where $\mathbf{A}$ is the operator. In such case RCholeskyQR can be performed block by block as shown in Algorithm 4 . The least-squares solution in step 3 can be computed with any stable least-squares solver, for instance, based on Householder transformations. The product with $\mathbf{R}_{(i, i)}^{-1}$ in step 5 of Algorithm 6 is done by forward substitution. It is easy to see that Algorithm 4 is numerically equivalent to Algorithm 2. Note that when $n=p$, Algorithm 4 corresponds to the situation when $\mathbf{X}$ is given column by column.

Algorithm 4 has a high relation to the RGS algorithm proposed in $[2,3]$. The main difference is how they update the sketch. In RCholeskyQR algorithm the matrix $\mathbf{S}_{(i)}$ is effectively computed by a $\mathrm{QR}$ factorization of $\mathbf{P}_{(1: i)}$, while in RGS it is computed as $\boldsymbol{\Theta} \mathbf{Q}_{(i)}$. In particular, Algorithm 4 would exactly recover the RGS algorithm if in step 2 together with computation $\mathbf{P}_{(i)} \leftarrow \boldsymbol{\Theta} \mathbf{X}_{(i)}$ it would also update the sketch of $\mathbf{Q}_{(i-1)}$ by calculating $\mathbf{S}_{(i-1)} \leftarrow \boldsymbol{\Theta} \mathbf{Q}_{(i-1)}$. Note that this step should increase the cost of the column-oriented RCholeskyQR only by a negligible amount in terms of flops as well as memory consumption and communication cost. In general, the RGS algorithm should provide greater stability than RCholeskyQR, since it has the ability to take into account in $\mathbf{R}_{(:, i)}$ the rounding errors committed when calculating $\mathbf{Q}$ at previous iterations, while in RCholeskyQR the blocks $\mathbf{R}_{(:, i)}$ are calculated independently of $\mathbf{Q}$. This fact is confirmed in our experiments in Section 6. Nevertheless, RCholeskyQR shows similar stability as RGS when the input matrix $\mathbf{X}$ is numerically full-rank, and is slightly less computationally expensive than RGS.

```
Algorithm 4 Column-oriented RCholeskyQR
    Input:
        $\mathbf{X}_{(1)}$ is $m \times \frac{n}{p}$ matrix
            $\boldsymbol{\Theta}$ is $k \times m$ sketching matrix (possibly provided as a function handle)
    Output:
        $\mathbf{Q}$ is $m \times n$ orthonormal $\mathrm{Q}$ factor
        $\mathbf{S}$ is $k \times n$ orthonormal sketch of $\mathbf{Q}$
        $\mathbf{R}$ is $n \times n$ upper triangular $\mathbf{R}$ factor
    function $[\mathbf{Q}, \mathbf{S}, \mathbf{R}]=\operatorname{colCholeskyQR}(\mathbf{X}, \boldsymbol{\Theta})$
    for $i=1: p$ do
        1. If $i>1$ obtain $\mathbf{X}_{(i)}$ from $\mathbf{Q}_{(1: i-1)}$ and $\mathbf{R}_{(1: i-1,1: i-1)}$
        2. $\mathbf{P}_{(i)} \leftarrow \boldsymbol{\Theta} \mathbf{X}_{(i)}$
        3. $\mathbf{R}_{(1: i-1, i)} \leftarrow \mathbf{S}_{(1: i-1)}^{\dagger} \mathbf{P}_{(i)}$
        4. $\left[\mathbf{S}_{(i)}, \mathbf{R}_{(i, i)}\right] \leftarrow \mathrm{QR}\left(\mathbf{P}_{(i)}-\mathbf{S}_{(1: i-1)} \mathbf{R}_{(1: i-1, i)}\right)$
        5. $\mathbf{Q}_{(i)} \leftarrow\left(\mathbf{X}_{(i)}-\mathbf{Q}_{(1: i-1)} \mathbf{R}_{(1: i-1, i)}\right) \mathbf{R}_{(i, i)}^{-1}$
    end for
```


### 3.3 Reduced RCholeskyQR

Particular attention has to be paid to the case when we are only interested in the low-dimensional projection of the matrix $\mathbf{Q}$, and not the full matrix. Assume that for given $\mathbf{X}$ we want to compute the quantity

$$
\begin{equation*}
\mathbf{L}(\mathbf{Q})=\mathbf{L} \mathbf{Q} \tag{5}
\end{equation*}
$$

where $\mathbf{L}$ is some (possibly randomized) low-dimensional extractor of the quantity of interest, and $\mathbf{Q}$ is a wellconditioned matrix satisfying range $(\mathbf{Q})=\operatorname{range}(\mathbf{X})$. This situation for instance appears when solving linear system of equation $\mathbf{A x}=\mathbf{b}$ by a sketched Galerkin or minres projection onto reduced basis $\mathbf{X}[4,5]$. In such case the $\mathbf{L}$ extractor has the following form:

$$
\mathbf{L}(\mathbf{Q})=\mathbf{L Q}=\left[\begin{array}{c}
\mathbf{U Q} \\
\Phi \mathbf{Q} \\
\Phi(\mathbf{A Q})
\end{array}\right]
$$

where $\mathbf{U}$ is (efficient) extractor of low-dimensional quantity $s(\mathbf{x})=\mathbf{U} \mathbf{x}$ of interest from $\mathbf{x}$, and $\boldsymbol{\Phi}$ is an OSE. In details, given $\mathbf{L}(\mathbf{Q})$, an approximate solution in the reduced basis can be efficiently and stably obtained by solving the following reduced system of equations

$$
\mathbf{A}_{\mathrm{red}} \mathbf{a}_{\mathrm{red}}=\mathbf{b}_{\mathrm{red}}
$$

where $\mathbf{A}_{\text {red }}=(\boldsymbol{\Phi} \mathbf{Q})^{\mathrm{T}}(\boldsymbol{\Phi} \mathbf{A Q})$ and $\mathbf{b}_{\text {red }}=(\boldsymbol{\Phi Q})^{\mathrm{T}}(\boldsymbol{\Phi} \mathbf{b})$ for the sketched Galerkin projection, or $\mathbf{A}_{\text {red }}=(\boldsymbol{\Phi} \mathbf{A Q})^{\mathrm{T}}(\boldsymbol{\Phi} \mathbf{A Q})$ and $\mathbf{b}_{\text {red }}=(\boldsymbol{\Phi} \mathbf{A Q})^{\mathrm{T}}(\boldsymbol{\Phi} \mathbf{b})$ for the sketched minres projection. Then the linear system's quantity of interest $s(\mathbf{x})$ is obtained by calculating $(\mathbf{U Q}) \mathbf{a}_{\text {red }}$. Furthermore, the above consideration can be extended from the case of solving one linear system to solving a series of systems $\mathbf{A}(\mu) \mathbf{x}(\mu)=\mathbf{b}(\mu)$, with parameters $\mu$ in some set $[4,5]$.

Then we notice that in order to compute $\mathbf{L}(\mathbf{Q})$, instead of first obtaining $\mathbf{Q}$ by calculating $\mathbf{X R}^{-1}$ and then applying the extractor $\mathbf{L}$, we can first apply the extractor to the matrix $\mathbf{X}$ and only then compute a product with $\mathbf{R}^{-1}$. The resulting reduced RCholeskyQR is depicted in Algorithm 5. We see that the dominant operation in step 3 of the RCholeskyQR has been drastically reduced. Now the dominant cost comes from the computation of $\boldsymbol{\Theta X}$ and $\mathbf{L X}$ in step 1. By using $\boldsymbol{\Theta}$ that is SRHT, this operation should have only $\mathcal{O}(m n \log m)$ complexity, which is by a factor $\mathcal{O}\left(\frac{n}{\log m}\right)$ lower than the dominant operations in RCholeskyQR or standard Cholesky QR algorithms. Moreover, it requires only one pass over $\mathbf{X}$ that can be crucial for the out-of-core computations. Algorithm 5 has a high relation to the reduced basis orthogonalization depicted in [4, Section 4.4].

Furthermore, if the columns of $\mathbf{X}$ are generated iteratively from the computed columns of $\mathbf{L} \mathbf{Q}$ at the previous iterations, for instance from the reduced linear system's solution $\mathbf{a}_{\text {red }}$, then the reduced RCholeskyQR can be performed block by block similarly to Algorithm 4 (see Algorithm 6). Algorithm 6 is a single-pass algorithm, i.e. it does not require storage/operations with $\mathbf{X}_{(1: i-1)}$ or $\mathbf{Q}_{(1: i-1)}$ to get the solution at iteration $i$. Again, it is easy

```
Algorithm 5 Reduced RCholeskyQR
    Input:
        $\mathbf{X}$ is $m \times n$ matrix
        $\Theta$ is $k \times m$ sketching matrix (possibly provided as a function handle)
        $\mathbf{L}$ is $l \times m$ extractor of the quantity of interest (possibly provided as a function handle)
    Output:
        $\mathbf{Z}$ is $l \times n$ quantity of interest $\mathbf{L}(\mathbf{Q})$, where $\mathbf{Q}$ is well-conditioned $\mathrm{Q}$ factor
        $\mathbf{S}$ is $k \times n$ orthonormal sketch of $\mathbf{Q}$
        $\mathbf{R}$ is $n \times n$ upper triangular $\mathbf{R}$ factor
    function $[\mathbf{Z}, \mathbf{S}, \mathbf{R}]=$ redRCholeskyQR $(\mathbf{X}, \boldsymbol{\Theta})$
    1. $\mathbf{P} \leftarrow \Theta \mathbf{X}, \mathbf{Y} \leftarrow \mathbf{L X}$
    2. $[\mathbf{S}, \mathbf{R}] \leftarrow \mathrm{QR}(\mathbf{P})$
    3. $\mathbf{Z} \leftarrow \mathbf{Y R}^{-1}$
```

```
Algorithm 6 Reduced column-oriented RCholeskyQR
    Input:
        $\mathbf{X}_{(1)}$ is $m \times \frac{n}{p}$ matrix
            $\boldsymbol{\Theta}$ is $k \times m$ sketching matrix (possibly provided as a function handle)
            $\mathbf{L}$ is $l \times m$ extractor of the quantity of interest (possibly provided as a function handle)
    Output:
            $\mathbf{Z}$ is $l \times n$ quantity of interest $\mathbf{L}(\mathbf{Q})$, where $\mathbf{Q}$ is well-conditioned $\mathrm{Q}$ factor
            $\mathbf{S}$ is $k \times n$ orthonormal sketch of $\mathbf{Q}$
            $\mathbf{R}$ is $n \times n$ upper triangular $\mathbf{R}$ factor
    function $[\mathbf{Z}, \mathbf{S}, \mathbf{R}]=$ colredRCholeskyQR $(\mathbf{X}, \boldsymbol{\Theta})$
    for $i=1: p$ do
        1. If $i>1$ obtain $\mathbf{X}_{(i)}$ from $\mathbf{Z}_{(1: i-1)}$ and $\mathbf{R}_{(1: i-1,1: i-1)}$
        2. $\mathbf{P}_{(i)} \leftarrow \boldsymbol{\Theta} \mathbf{X}_{(i)}, \mathbf{Y}_{(i)} \leftarrow \mathbf{L} \mathbf{X}_{(i)}$
        3. $\mathbf{R}_{(1: i-1, i)} \leftarrow \mathbf{S}_{(1: i-1)}^{\dagger} \mathbf{P}_{(i)}$
        4. $\left[\mathbf{S}_{(i)}, \mathbf{R}_{(i, i)}\right] \leftarrow \mathrm{QR}\left(\mathbf{P}_{(i)}-\mathbf{S}_{(1: i-1)} \mathbf{R}_{(1: i-1, i)}\right)$
        5. $\mathbf{Z}_{(i)} \leftarrow\left(\mathbf{Y}_{(i)}-\mathbf{Z}_{(1: i-1)} \mathbf{R}_{(1: i-1, i)}\right) \mathbf{R}_{(i, i)}^{-1}$
    end for
```

to see that Algorithm 5 and Algorithm 6 are numerically equivalent. The stability of these algorithms follows from the stability of RCholeskyQR. See Section 5.2 for more details.

## 4 Rank-revealing randomized Cholesky QR

When $\mathbf{X}$ is numerically rank-deficient, one can improve the stability and the computational cost of RCholeskyQR by orthogonalizing only the linearly independent columns of $\mathbf{X}$ and ignoring the other columns. In details, we here look for a $\mathrm{QR}$ factorization of the form

$$
\mathbf{X} \approx \mathbf{Q R} \boldsymbol{\Pi}^{\mathrm{T}}
$$

where $\boldsymbol{\Pi}$ is a permutation matrix, $\mathbf{Q}$ is a very well-conditioned $\mathbf{Q}$ factor with $r \leq m$ columns, and $\mathbf{R}$ is an upper triangular or trapezoidal $\mathrm{R}$ factor with $r$ rows. Furthermore, the factorization is such that $\mathbf{X} \Pi_{(1: r)}$ is sufficiently well-conditioned, and

$$
\begin{equation*}
\mathbf{X}+\Delta \mathbf{X}=\mathbf{Q R} \boldsymbol{\Pi}^{\mathrm{T}} \text { with }\left\|\Delta \mathbf{X}_{(:, i)}\right\|_{2} \leq F(n) u\left\|\mathbf{X}_{(:, i)}\right\|_{2} \tag{6}
\end{equation*}
$$

with $F(n)$ being a low-degree polynomial, and $u$ denoting the unit roundoff, $1 \leq i \leq n$.

To obtain such factorization we propose to first compute $\boldsymbol{\Pi}$ and $\mathbf{R}$ with the rank-revealing QR of the columnnormalized sketch $\boldsymbol{\Theta} \mathbf{X}$, and then compute $\mathbf{Q}$ with forward substitution as is described in Algorithm 7 .

For better presentation in Algorithm 7 we assumed that the columns of $\mathbf{X}$ have unit norms. The case when the columns vary in norm can be accounted for by calculating the normalization matrix $\mathbf{D}=\operatorname{diag}\left(\left\|\mathbf{X}_{(:, j)}\right\|_{2}\right)$ and

```
Algorithm 7 Rank-revealing randomized Cholesky QR (RRRCholQR)
    Input:
        $\mathbf{X}$ is $m \times n$ matrix with normalized columns
        $\boldsymbol{\Theta}$ is $k \times m$ sketching matrix (possibly provided as a function handle)
    Output:
        $\mathbf{Q}$ is $m \times r$ well-conditioned $\mathrm{Q}$ factor
        $\mathbf{S}$ is $k \times r$ orthonormal sketch of $\mathbf{Q}$
        $\mathbf{R}$ is $r \times n$ upper triangular or trapezoidal $\mathrm{R}$ factor
        $\Pi$ is $n \times n$ permutation matrix
    function $[\mathbf{Q}, \mathbf{S}, \mathbf{R}, \mathbf{\Pi}]=\operatorname{RRRCholQR}(\mathbf{X}, \boldsymbol{\Theta})$
    1. $\mathbf{P} \leftarrow \Theta \mathbf{X}$
    2. $[\mathbf{S}, \mathbf{R}, \mathbf{\Pi}] \leftarrow \operatorname{RRQR}(\mathbf{P})$
    3. Determine min $r$ such that $\left\|\mathbf{R}_{(r+1: n, r+1: n)}\right\|_{\mathrm{F}} \leq \tau\|\mathbf{R}\|_{2}$
    4. $\mathbf{Q} \leftarrow\left(\mathbf{X} \Pi_{(:, 1: r)}\right) \mathbf{R}_{(1: r, 1: r)}^{-1}$
    5. $\mathbf{R} \leftarrow \mathbf{R}_{(1: r,:)}$
```

inputting $\mathbf{X D}^{-1}$ to Algorithm 7 instead of $\mathbf{X}$. The output $\mathrm{R}$ factor has then to be post-processed accordingly to

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-11.jpg?height=47&width=1737&top_left_y=1039&top_left_x=191)
memory architecture and the distributed architecture can be reduced by computing this matrix along with $\mathbf{P} \leftarrow \boldsymbol{\Theta}$ in step 1 during the same pass through the matrix $\mathbf{X}$ and global synchronization between processors. Then, since we need a sketch of the normalized $\mathbf{X}$, between steps 1 and 2 we need to normalize $\mathbf{P}$ by computing $\mathbf{P} \leftarrow \mathbf{P D}^{-1}$. Also, one can defer the multiplication of $\mathbf{X}$ by $\mathbf{D}^{-1}$ to step 4 , which will require operation with only $r$ columns of $\mathbf{X}$ and hence have a lower computational cost.

The subroutine RRQR in step 2 that outputs a rank-revealing $Q R$ factorization $\mathbf{S R} \Pi^{\mathrm{T}}$ of $\mathbf{P}$ can be chosen for instance as the strong rank-revealing Cholesky QR from [17] executed in sufficient precision. In this case Algorithm 7 is equivalent to the generalized strong rank-revealing Cholesky QR with respect to the sketched (randomized) inner product, which gives Algorithm 7 its name. However, similarly to RCholeskyQR, it should be better to take more robust subroutine in step 2, such as for instance the strong rank-revealing QR from [16].

Algorithm 7 takes the truncation tolerance $\tau$ as a user-specified parameter. This parameter can be chosen as $\tau=F(n) u$ to have an approximation of $\mathbf{X}$ close to machine precision. RRRCholeskyQR can be viewed as an improved version of RCholeskyQR with better stability characteristics. This follows from the fact that RRQR provides $\boldsymbol{\Pi}$ such that the matrix $\mathbf{P} \boldsymbol{\Pi}_{(:, 1: r)}$ and hence $\mathbf{X} \Pi_{(:, 1: r)}$ is numerically full-rank, given that $\boldsymbol{\Theta}$ is an OSE of sufficiently large size, and $F(n)$ is sufficiently large. Furthermore, it can be shown that (column-normalized) RRRCholeskyQR factorization is a quasi-optimal rank- $r$ approximation of (column-normalized) X. These two properties are formalized in Proposition 4.1. We here restrict ourselves only to the case where $\boldsymbol{\Theta}$ is an OSE. The analysis for the leverage score sampling matrices is similar.

Proposition 4.1. Consider Algorithm 7 with RRQR in step 2 such that

$$
\left\|\mathbf{P}-\mathbf{S}_{(:, 1: r)} \mathbf{R}_{(1: r,:)} \boldsymbol{\Pi}^{\mathrm{T}}\right\|_{\mathrm{F}} \leq C \min _{\operatorname{rank}(\mathbf{Y})=r}\|\mathbf{P}-\mathbf{Y}\|_{\mathrm{F}}
$$

where $C$ is some parameter possibly depending on $n$ and $m$. If $\boldsymbol{\Theta}$ is an $(\varepsilon, \delta, n)-O S E$, then with probability at least $1-\delta$, we have

$$
\begin{equation*}
\sqrt{1-\varepsilon}\left\|\mathbf{X}-\mathbf{Q R} \boldsymbol{\Pi}^{\mathrm{T}}\right\|_{\mathrm{F}} \leq \sqrt{1+\varepsilon} C \min _{\operatorname{rank}(\mathbf{Z})=r}\|\mathbf{X}-\mathbf{Z}\|_{\mathrm{F}} \tag{7a}
\end{equation*}
$$

and

$$
\begin{equation*}
\operatorname{cond}\left(\mathbf{X \Pi}_{(1: r)}\right) \leq \sqrt{\frac{1+\varepsilon}{1-\varepsilon}} \operatorname{cond}\left(\mathbf{P} \boldsymbol{\Pi}_{(1: r)}\right) \tag{7~b}
\end{equation*}
$$

Furthermore, (7a) and (7b) also hold with probability at least $1-\delta$, if $\Theta$ is an $\left(\varepsilon,\left(\binom{n}{r+1}+n\right)^{-1} \delta, r+1\right)-O S E$ and not necessarily an $(\varepsilon, \delta, n)-O S E$.

Proof. Assume that $r<n$, and that $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for all subspaces spanned by $r+1$ columns of $\mathbf{X}$, and all subspaces of the form range $\left(\mathbf{Z}^{*}\right)+\operatorname{span}\left(\mathbf{X}_{(:, j)}\right)$, where $\mathbf{X}_{(:, j)}$ is the $j$-th column of $\mathbf{X}$ and

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-12.jpg?height=113&width=579&top_left_y=429&top_left_x=770)

$1 \leq j \leq n$. Clearly, this condition is satisfied if $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for $\mathbf{X}$, which in turn holds with probability at least $1-\delta$ if $\Theta$ is an $(\varepsilon, \delta, n)$-OSE. Furthermore, since there are in total at most $N=\binom{n}{r+1}+n$ such $r+1$-dimensional subspaces, by the union bound argument, $\Theta$ is an $\varepsilon$-embedding for all of them with probability at least $1-\delta$, if it is an $\left(\varepsilon, N^{-1} \delta, r+1\right)$-OSE.

Notice that since $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for all subspaces spanned by $r+1$ columns of $\mathbf{X}$, it is an $\varepsilon$-embedding for all subspaces of the form range $(\mathbf{Q})+\operatorname{span}\left(\mathbf{X}_{(:, j)}\right)$. It then follows that

$$
\begin{aligned}
& \left\|\mathbf{P} \boldsymbol{\Pi}-\mathbf{S}_{(:, 1: r)} \mathbf{R}_{(1: r,:)}\right\|_{\mathrm{F}}^{2}=\left\|\boldsymbol{\Theta}\left(\mathbf{X} \boldsymbol{\Pi}-\mathbf{X} \boldsymbol{\Pi}_{(1: r)} \mathbf{R}_{(1: r, 1: r)}^{-1} \mathbf{R}_{(1: r,:)}\right)\right\|_{\mathrm{F}}^{2}=\sum_{i=1}^{n}\left\|\boldsymbol{\Theta}\left(\mathbf{X}_{(:, i)}-\mathbf{Q} \mathbf{R}_{(1: r,:)} \boldsymbol{\Pi}_{(i,::}^{\mathrm{T}}\right)\right\|_{2}^{2} \\
& \geq(1-\varepsilon) \sum_{i=1}^{n}\left\|\mathbf{X}_{(:, i)}-\mathbf{Q} \mathbf{R}_{(1: r,:)} \boldsymbol{\Pi}_{(i,::}^{\mathrm{T}}\right\|_{2}^{2}=(1-\varepsilon)\left\|\mathbf{X}-\mathbf{Q R} \boldsymbol{\Pi}^{\mathrm{T}}\right\|_{\mathrm{F}}^{2}
\end{aligned}
$$

and

$$
\min _{\operatorname{rank}(\mathbf{Y})=r}\|\mathbf{P}-\mathbf{Y}\|_{\mathrm{F}}^{2} \leq\left\|\boldsymbol{\Theta}\left(\mathbf{X}-\mathbf{Z}^{*}\right)\right\|_{\mathrm{F}}^{2} \leq(1+\varepsilon) \sum_{i=1}^{n}\left\|\mathbf{X}_{(:, i)}-\mathbf{Z}_{(:, i)}^{*}\right\|_{2}^{2}=(1+\varepsilon)\left\|\mathbf{X}-\mathbf{Z}^{*}\right\|_{\mathrm{F}}^{2}
$$

which gives (7a).

The relation $(7 \mathrm{~b})$ follows from the fact that for any vector $\mathbf{x} \in \mathbb{R}^{r}$ we have

$$
(1-\varepsilon)\left\|\mathbf{X \Pi}_{(1: r)} \mathbf{x}\right\|_{2}^{2} \leq\left\|\boldsymbol{\Theta} \mathbf{X} \boldsymbol{\Pi}_{(1: r)} \mathbf{x}\right\|_{2}^{2} \leq(1+\varepsilon)\left\|\mathbf{X} \boldsymbol{\Pi}_{(1: r)} \mathbf{x}\right\|_{2}^{2}
$$

Proposition 4.1 opens the door to yet another application of the RRRCholeskyQR algorithm beyond QR factorization of tall-and-skinny matrices, which is an efficient and stable low-rank approximation of large $m \times n$ matrices that do not need to satisfy $n \ll m$. According to Proposition 4.1, Algorithm 7 provides a rank-revealing QR factorization of the same quality (in exact arithmetic) as the low-dimensional RRQR in step 2, given that $\Theta$ is a $(\varepsilon, \delta, n)$-OSE or $\left(\varepsilon,\left(\binom{n}{r+1}+n\right)^{-1} \delta, r+1\right)$-OSE. The first-mentioned condition on $\Theta$ is met if $\Theta$ is a Gaussian matrix with $k=\mathcal{O}\left(n+\log \frac{1}{\delta}\right)$ rows, which is sufficient when $\mathbf{X}$ is tall-and-skinny, but infeasible when both dimensions of $\mathbf{X}$ are large. In the latter case, we must turn to the second condition, which is satisfied by a Gaussian OSE with $k=\mathcal{O}\left(r \log n+\log \frac{1}{\delta}\right)$ rows. For SRHT this requirement is higher but in practice there should not be much difference in accuracy. It is concluded that RRRCholeskyQR can provide a quasi-optimal low-rank approximation using the sketching dimension $k$ that depends on $n$ and $m$ at most logarithmically.

The numerical stability of RRRCholeskyQR in both the low-rank approximation context as well as the context of factorization of tall-and-skinny matrices, can be characterized in exactly the same manner. It can be guaranteed unconditionally of $\mathbf{X}$. In particular, it can be shown that by using $\tau \geq G(n) u$, where $G(n)$ is some low-degree polynomial, we obtain $\mathbf{P} \Pi_{(1: r)}$ and hence $\mathbf{X \Pi}_{(1: r)}$ of numerically full rank. In turn this fact implies the stability of computing the factor $\mathbf{Q}$, and therefore the overall stability of the algorithm. More details on this matter are provided in Section 5.3. Furthermore, if the application requires an orthonormal to machnie precision $\mathrm{Q}$ factor and not just well-conditioned, then RRRCholeskyQR can be augmented with the classical CholeskyQR in exactly the same way as done in RCholeskyQR2 defined by Algorithm 3. The resulting algorithm will be referred to as RRRCholeskyQR2.

In terms of efficiency, RRRCholeskyQR and RRRCholeskyQR2 should be at least as good as RCholeskyQR and RCholeskyQR2 respectively, and therefore outperform standard/shifted CholeskyQR2 and CholeskyQR3, Householder QR and other deterministic algorithms. In addition, it is revealed that when $\mathbf{X}$ has a relatively low numerical
rank, the proposed factorizations can be even more computationally advantageous. In particular, if we take $\boldsymbol{\Theta}$ as an SRHT matrix, then RRRCholeskyQR should take about $m n \log _{2} m+m r^{2}$ flops in steps 1 and 4, whereas RRRCholeskyQR2 (in implicit form) takes about $m n \log _{2} m+2 m r^{2}$ flops. In contrast, the standard QR factorizations and RCholeskyQR consume $\mathcal{O}\left(m n^{2}\right)$ flops, which can be much larger when the rank $r$ is relatively small. Furthermore, RRRCholeskyQR and RRRCholeskyQR2 should significantly outperform other randomized rank-revealing $\mathrm{QR}$ factorizations such as the ones from $[11,22,24,25,30]$ that require at least $m n r$ flops (needed to compute the $\mathrm{R}$ factor) and two or more global synchronizations between processors. The RRRCholeskyQR factorization on the other hand needs up to $\frac{n}{r}$ less flops and only one global synchronization.

## 5 Stability analysis

In this section we analyze numerical stability of Algorithms 2,5 and 7. The stability of other presented algorithms directly follows. Let $\kappa$ denote the condition number of column-normalized $\mathbf{X}$. Let us recall that the steps requiring a minor computational cost are here executed in higher precision by a low-degree polynomial factor $F(m, n)$ in $n$ and $m$ than the dominant operations. This allows to have numerical stability with working unit roundoff $u=\mathcal{O}\left(n^{-\frac{3}{2}} \kappa^{-1}\right)$ in dominant step 3 of Algorithms 2 and 5 and $u=\mathcal{O}\left(n^{-\frac{3}{2}} r^{-\frac{5}{2}}\right)$ in dominant step 4 of Algorithm 7 independent of the dimension $m$. Nevertheless, clearly our results also imply guarantees of numerical stability under a model with unique unit roundoff. Such guarantees can be obtained simply by replacing $u$ by $F(m, n) u$ in the results.

Note that the above conditions on $u$ are pessimistic. This overestimation can be seen as an artifact due to the use of worst-case rounding bounds. According to the "rule of thumb" of rounding [20], in practice the low-dimensional polynomials in the forthcoming in this section conditions (9a), (21a) and (26a) on $u$, and the stability guarantees in Theorems 5.2, 5.4, 5.6 and 5.7 can be reduced by (nearly) a square root.

### 5.1 Stability of RCholeskyQR

The stability analysis of Algorithm 2 is carried out using the following assumptions. First, it is assumed that the working unit roundoff $u$ satisfies the bound (9a). Furthermore, steps 1 and 2 are assumed to be performed with unit roundoff $u_{f}$ such that the error matrices

$$
\begin{align*}
& \mathbf{E}_{1}:=\mathbf{\Theta} \mathbf{X}-\widehat{\mathbf{P}}  \tag{8a}\\
& \mathbf{E}_{2}:=\widehat{\mathbf{P}}-\widehat{\mathbf{S}} \widehat{\mathbf{R}}  \tag{8~b}\\
& \mathbf{E}_{3}:=\mathbf{X}-\widehat{\mathbf{Q}} \widehat{\mathbf{R}} \tag{8c}
\end{align*}
$$

satisfy (9b) to (9d). By the classical worst-case rounding analysis we have $\left|\mathbf{E}_{1}\right| \leq \frac{m u_{f}}{1-m u_{f}}|\boldsymbol{\Theta}||\mathbf{X}|$. Hence (9b) can be achieved with $u_{f}=\mathcal{O}\left(m^{-1} n^{-1} u\right)$. In principle the guarantees (9c) can be achieved with any stable QR factorization executed in sufficient precision. This includes the Householder QR with unit roundoff $u_{f}=O\left(k^{-1} n^{-\frac{3}{2}} u\right)$ or Givens $\mathrm{QR}$ with unit roundoff $u_{f}=O\left(k^{-1} n^{-\frac{1}{2}} u\right)[20]$. According to [20, Theorem 8.5], we have $\widehat{\mathbf{Q}}_{(j,:)}\left(\widehat{\mathbf{R}}+\Delta \mathbf{R}^{(j)}\right)=\mathbf{X}_{(j,:)}$ with $\left|\Delta \mathbf{R}^{(i)}\right| \leq 1.1 u n|\widehat{\mathbf{R}}|$, which implies that $\left|\mathbf{E}_{3}\right|$ is bounded by $1.1 u n|\widehat{\mathbf{Q}}||\widehat{\mathbf{R}}|$ and leads to the first inequality in (9d). Since the rows of $\mathbf{E}_{3}$ are computed independently of each other, we can use Corollary 2.5 to bound the sketched norms of the columns of $\mathbf{E}_{3}$. In this way we have $\left\|\boldsymbol{E}_{3(:, j)}\right\|_{2} \leq \sqrt{\frac{3}{2}} 1.1 u n\left\|\left|\widehat{\mathbf{Q}}\|\widehat{\mathbf{R}}(:, j) \mid\|_{2}\right.\right.$, with probability at least $1-\delta$, if $\Theta$ is $\left(1 / 8, n^{-1}\binom{m}{d}^{-1} \delta, d\right)$-OSE, with $d=4.2 c^{-1} \log \frac{4}{\delta}$, which in turn is satisfied by Gaussian matrices and SRHT (in practice) with $k \geq \mathcal{O}\left(\log m \log \frac{1}{\delta}\right)$ rows. Note that the bound for $\left\|\boldsymbol{\Theta} \mathbf{E}_{3(:, j)}\right\|_{2}$ is independent of the high dimension $m$.

Assumptions 5.1. Consider Algorithm 2. We assume that

$$
\begin{equation*}
u \leq 0.01 n^{-\frac{3}{2}} \kappa^{-1} \tag{9a}
\end{equation*}
$$

Furthermore, for $1 \leq j \leq n$,

$$
\begin{array}{rlrl}
\left\|\mathbf{E}_{1(:, j)}\right\|_{2} & \leq 0.1 u n^{-\frac{1}{2}}\left\|\mathbf{X}_{(:, j)}\right\|_{2}, & \\
\left\|\mathbf{E}_{2(:, j)}\right\|_{2} & \leq 0.1 u n^{-\frac{1}{2}}\left\|\widehat{\mathbf{P}}_{(:, j)}\right\|_{2}, & & \left\|\widehat{\mathbf{S}}^{\mathrm{T}} \widehat{\mathbf{S}}-\mathbf{I}\right\|_{\mathrm{F}} \leq 0.1 u \\
\left|\mathbf{E}_{3(:, j)}\right| & \leq 1.1 u n\left|\widehat{\mathbf{Q}} \| \widehat{\mathbf{R}}_{(:, j)}\right|, & & \left\|\boldsymbol{\Theta} \mathbf{E}_{3(:, j)}\right\|_{2} \leq 2 u n\|\widehat{\mathbf{Q}}\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}_{(:, j)}\right\|_{2}
\end{array}
$$

Theorem 5.2 provides a stability guarantee of Algorithm 2 .

Theorem 5.2. Let $\Theta$ be an $\varepsilon$-embedding for $\mathbf{X}$ with $\varepsilon \leq \frac{1}{2}$. Consider Algorithm 2. Under Assumptions 5.1 we have,

$$
\begin{gather*}
\mathbf{X}+\Delta \mathbf{X}=\widehat{\mathbf{Q}} \widehat{\mathbf{R}} \text { with }\left\|\Delta \mathbf{X}_{(:, j)}\right\|_{2} \leq 2.1 u n\left\|\mathbf{X}_{(:, j)}\right\|_{2}  \tag{10a}\\
(1+\varepsilon)^{-\frac{1}{2}}-4 u n^{\frac{3}{2}} \kappa \leq \sigma_{\min }(\widehat{\mathbf{Q}}) \leq \sigma_{\max }(\widehat{\mathbf{Q}}) \leq(1-\varepsilon)^{-\frac{1}{2}}+4 u n^{\frac{3}{2}} \kappa \tag{10~b}
\end{gather*}
$$

for $1 \leq j \leq n$. Furthermore, it holds that

$$
\begin{equation*}
\|\widehat{\mathbf{S}}-\Theta \widehat{\mathbf{Q}}\|_{\mathrm{F}} \leq 6.1 u n^{\frac{3}{2}} \kappa \tag{10c}
\end{equation*}
$$

and that $\Theta$ satisfies the $\varepsilon^{\prime}$-embedding property for $\widehat{\mathbf{Q}}$ with $\varepsilon^{\prime} \leq \varepsilon+50 u n^{\frac{3}{2}} \kappa$.

Proof. Let us scale $\mathbf{X}, \widehat{\mathbf{R}}, \widehat{\mathbf{P}}$ and $\mathbf{E}_{1}, \mathbf{E}_{2}, \mathbf{E}_{3}$ by $\mathbf{D}:=\operatorname{diag}\left(\left\|\mathbf{X}_{(:, j)}\right\|_{2}^{-1}\right): \mathbf{X} \leftarrow \mathbf{X D}, \widehat{\mathbf{R}} \leftarrow \widehat{\mathbf{R}} \mathbf{D}, \widehat{\mathbf{P}} \leftarrow \widehat{\mathbf{P}} \mathbf{D}, \mathbf{E}_{1} \leftarrow \mathbf{E}_{1} \mathbf{D}$, $\mathbf{E}_{2} \leftarrow \mathbf{E}_{2} \mathbf{D}$, and $\mathbf{E}_{3} \leftarrow \mathbf{E}_{3} \mathbf{D}$. Notice that such scaling does not affect the relations (8) and assumptions (9b) to (9d). Then we also have $\kappa=\operatorname{cond}(\mathbf{X})$.

We start with showing that the computed sketch $\widehat{\mathbf{P}}$ of $\mathbf{X}$ preserves the column norms and the smallest singular value of $\mathbf{X}$. By the $\varepsilon$-embedding property of $\boldsymbol{\Theta}$ and (9b), we get for $1 \leq j \leq n$,

$$
\begin{align*}
\left\|\widehat{\mathbf{P}}_{(:, j)}\right\|_{2} \leq\left\|\boldsymbol{\Theta} \mathbf{X}_{(:, j)}\right\|_{2}+\left\|\mathbf{E}_{1(:, j)}\right\|_{2} \leq\left(\sqrt{1+\varepsilon}+0.1 u n^{-\frac{1}{2}}\right)\left\|\mathbf{X}_{(:, j)}\right\|_{2} \leq 1.23\left\|\mathbf{X}_{(:, j)}\right\|_{2}  \tag{11a}\\
\sigma_{\min }(\widehat{\mathbf{P}}) \geq \sigma_{\min }(\boldsymbol{\Theta} \mathbf{X})-\left\|\mathbf{E}_{1}\right\|_{2} \geq \sqrt{1-\varepsilon} \sigma_{\min }(\mathbf{X})-0.1 u n^{-\frac{1}{2}} \operatorname{cond}(\mathbf{X}) \sigma_{\min }(\mathbf{X}) \geq 0.69 \sigma_{\min }(\mathbf{X}) \tag{11b}
\end{align*}
$$

Next, the same thing is shown for the computed $\mathrm{R}$ factor. We have,

$$
\widehat{\mathbf{R}}_{(:, j)}=\widehat{\mathbf{S}}^{\mathrm{T}} \widehat{\mathbf{P}}_{(:, j)}+\left(\mathbf{I}-\widehat{\mathbf{S}}^{\mathrm{T}} \widehat{\mathbf{S}}\right) \widehat{\mathbf{R}}_{(:, j)}+\widehat{\mathbf{S}}^{\mathrm{T}} \mathbf{E}_{2(:, j)}
$$

Hence by (9c) and (11), it holds

$$
\begin{align*}
&\left\|\widehat{\mathbf{R}}_{(:, j)}\right\|_{2} \leq\|\widehat{\mathbf{S}}\|_{2}\left\|\widehat{\mathbf{P}}_{(:, j)}\right\|_{2}+\left\|\mathbf{I}-\widehat{\mathbf{S}}^{\mathrm{T}} \widehat{\mathbf{S}}\right\|_{2}\left\|\widehat{\mathbf{R}}_{(:, j)}\right\|_{2}+\|\widehat{\mathbf{S}}\|_{2}\left\|\mathbf{E}_{2(:, j)}\right\|_{2} \leq 1.26\left\|\mathbf{X}_{(:, j)}\right\|_{2}  \tag{12a}\\
& \sigma_{\min }(\widehat{\mathbf{R}}) \geq \sigma_{\min }(\widehat{\mathbf{S}}) \sigma_{\min }(\widehat{\mathbf{P}})-\left\|\mathbf{I}-\widehat{\mathbf{S}}^{\mathrm{T}} \widehat{\mathbf{S}}\right\|_{\mathrm{F}}\|\widehat{\mathbf{R}}\|_{2}-\|\widehat{\mathbf{S}}\|_{2}\left\|\mathbf{E}_{2}\right\|_{\mathrm{F}} \geq 0.66 \sigma_{\min }(\mathbf{X}) \tag{12b}
\end{align*}
$$

This allows us to bound the error of $\widehat{\mathbf{Q}}$ in the Frobenius norm and the sketched Frobenius norm. By (9d) and (12), we get

$$
\begin{align*}
\left\|\widehat{\mathbf{Q}}-\mathbf{X} \widehat{\mathbf{R}}^{-1}\right\|_{\mathbf{F}} & =\left\|\mathbf{E}_{3} \widehat{\mathbf{R}}^{-1}\right\|_{\mathbf{F}} \leq\left\|\mathbf{E}_{3}\right\|_{\mathbf{F}}\left\|\widehat{\mathbf{R}}^{-1}\right\|_{2} \leq 1.1 u n\|\widehat{\mathbf{Q}}\|_{2}\|\widehat{\mathbf{R}}\|_{\mathbf{F}}\left\|\widehat{\mathbf{R}}^{-1}\right\|_{2} \leq 2.1 u n^{\frac{3}{2}} \operatorname{cond}(\mathbf{X})\|\widehat{\mathbf{Q}}\|_{2}  \tag{13a}\\
\left\|\boldsymbol{\Theta} \widehat{\mathbf{Q}}-\boldsymbol{\Theta} \widehat{\mathbf{R}}^{-1}\right\|_{\mathbf{F}} & =\left\|\boldsymbol{\Theta} \mathbf{E}_{3} \widehat{\mathbf{R}}^{-1}\right\|_{\mathbf{F}} \leq\left\|\boldsymbol{\Theta} \mathbf{E}_{3}\right\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}^{-1}\right\| \leq 2 u n\|\widehat{\mathbf{Q}}\|_{2}\|\widehat{\mathbf{R}}\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}^{-1}\right\|_{2} \leq 3.82 u n^{\frac{3}{2}} \operatorname{cond}(\mathbf{X})\|\widehat{\mathbf{Q}}\|_{2} \tag{13b}
\end{align*}
$$

Furthermore, from $(9 b),(9 c)$ and (12) we obtain

$$
\begin{equation*}
\left\|\boldsymbol{\Theta} \mathbf{X} \widehat{\mathbf{R}}^{-1}-\widehat{\mathbf{S}}\right\|_{\mathrm{F}}=\left\|\mathbf{E}_{1} \widehat{\mathbf{R}}^{-1}+\mathbf{E}_{2} \widehat{\mathbf{R}}^{-1}\right\|_{\mathrm{F}} \leq 0.1 u n^{-\frac{1}{2}}\|\mathbf{X}\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}^{-1}\right\|_{2}+0.1 u n^{-\frac{1}{2}}\|\widehat{\mathbf{P}}\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}^{-1}\right\|_{2} \leq 0.35 u \operatorname{cond}(\mathbf{X}) \tag{14}
\end{equation*}
$$

which in turn implies that

$$
\begin{equation*}
1-\Delta_{1} \leq \sigma_{\min }\left(\mathbf{\Theta} \mathbf{X} \widehat{\mathbf{R}}^{-1}\right) \leq \sigma_{\max }\left(\mathbf{\Theta} \mathbf{X} \widehat{\mathbf{R}}^{-1}\right) \leq 1+\Delta_{1} \tag{15}
\end{equation*}
$$

with $\Delta_{1} \leq\left\|\widehat{\mathbf{S}}^{\mathrm{T}} \widehat{\mathbf{S}}-\mathbf{I}\right\|_{\mathrm{F}}+0.35 u$ cond $(\mathbf{X}) \leq 0.45 u$ cond $(\mathbf{X})$. By the $\varepsilon$-embedding property of $\Theta$, it is deduced from (15) that

$$
\begin{equation*}
(1+\varepsilon)^{-\frac{1}{2}}\left(1-\Delta_{1}\right) \leq \sigma_{\min }\left(\mathbf{X} \widehat{\mathbf{R}}^{-1}\right) \leq \sigma_{\max }\left(\mathbf{X} \widehat{\mathbf{R}}^{-1}\right) \leq(1-\varepsilon)^{-\frac{1}{2}}\left(1+\Delta_{1}\right) \tag{16}
\end{equation*}
$$

which, combined with (13), results in

$$
\begin{equation*}
(1+\varepsilon)^{-\frac{1}{2}}\left(1-\Delta_{1}\right)-\Delta_{2} \leq \sigma_{\min }(\widehat{\mathbf{Q}}) \leq \sigma_{\max }(\widehat{\mathbf{Q}}) \leq(1-\varepsilon)^{-\frac{1}{2}}\left(1+\Delta_{1}\right)+\Delta_{2} \tag{17}
\end{equation*}
$$

for some $\Delta_{2} \leq 2.1 u n^{\frac{3}{2}} \operatorname{cond}(\mathbf{X})\|\widehat{\mathbf{Q}}\|_{2}$. Hence $\|\widehat{\mathbf{Q}}\|_{2} \leq 1.5$, which combined with (9d), (12) and (17), implies the results (10a) and (10b) of the theorem.

Relations (13) and (14) imply (10c). To show that $\boldsymbol{\Theta}$ is an $\varepsilon^{\prime}$-embedding for $\widehat{\mathbf{Q}}$, we notice that for any $\mathbf{x} \in \mathbb{R}^{k}$, it holds

$$
\begin{equation*}
\left(1-\Delta_{3}\right)\|\mathbf{x}\|_{2} \leq\left\|\boldsymbol{\Theta} \mathbf{X} \widehat{\mathbf{R}}^{-1} \mathbf{x}\right\|_{2}-\left\|\boldsymbol{\Theta} \mathbf{E}_{3} \widehat{\mathbf{R}}^{-1} \mathbf{x}\right\|_{2} \leq\|\boldsymbol{\Theta} \widehat{\mathbf{Q}} \mathbf{x}\|_{2} \leq\left\|\boldsymbol{\Theta} \mathbf{X} \widehat{\mathbf{R}}^{-1} \mathbf{x}\right\|_{2}+\left\|\boldsymbol{\Theta} \mathbf{E}_{3} \widehat{\mathbf{R}}^{-1} \mathbf{x}\right\|_{2} \leq\left(1+\Delta_{3}\right)\|\mathbf{x}\|_{2} \tag{18}
\end{equation*}
$$

and

$$
\begin{equation*}
\left((1+\varepsilon)^{-\frac{1}{2}}-\Delta_{3}\right)\|\mathbf{x}\|_{2} \leq\|\widehat{\mathbf{Q}} \mathbf{x}\|_{2} \leq\left((1-\varepsilon)^{-\frac{1}{2}}+\Delta_{3}\right)\|\mathbf{x}\|_{2} \tag{19}
\end{equation*}
$$

where $\Delta_{3} \leq 6.1$ un ${ }^{\frac{3}{2}} \operatorname{cond}(\mathbf{X})$. Whence,

$$
\left(1-\Delta_{3}\right)^{2}\left((1-\varepsilon)^{-\frac{1}{2}}+\Delta_{3}\right)^{-2}\|\widehat{\mathbf{Q}} \mathbf{x}\|_{2}^{2} \leq\|\boldsymbol{\Theta} \widehat{\mathbf{Q}} \mathbf{x}\|_{2}^{2} \leq\left(1+\Delta_{3}\right)^{2}\left((1+\varepsilon)^{-\frac{1}{2}}-\Delta_{3}\right)^{-2}\|\widehat{\mathbf{Q}} \mathbf{x}\|_{2}^{2}
$$

By using the fact that $\Delta_{3} \leq 0.061$, we get $\left(1+\Delta_{3}\right)^{2}\left((1+\varepsilon)^{-\frac{1}{2}}-\Delta_{3}\right)^{-2}=(1+\varepsilon)\left(1+\Delta_{3}\right)^{2}\left(1-\sqrt{1+\varepsilon} \Delta_{3}\right)^{-2} \leq$ $1+\varepsilon+8 \Delta_{3}$, and similarly, $\left(1-\Delta_{3}\right)^{2}\left((1-\varepsilon)^{-\frac{1}{2}}+\Delta_{3}\right)^{-2} \geq 1-\varepsilon-8 \Delta_{3}$.

The stability of the column-oriented RCholeskyQR given by Algorithm 4 follows directly from Theorem 5.2, since it is numerically equivalent to Algorithm 2.

### 5.2 Stability of reduced RCholeskyQR

Stability guarantees for the reduced RCholeskyQR and its column-oriented variant (Algorithms 5 and 6) can be obtained in a similar manner as above. For consistency with the previous subsection, we again assume that the forward substitution in step 3 in Algorithm 5 and in step 5 in Algorithm 6 is done with unit roundoff $u$, which is by a polynomial factor in $n$ and $m$ less than the unit rounding $u_{f}$ used for other operations. Although for a reduced RCholeskyQR, this condition may not be that important, since the forward substitution is performed on a low-dimensional matrix and, therefore, should not be so expensive.

We shall analyze only Algorithm 5, noting that the results obtained will also apply to Algorithm 6, since the two algorithms are numerically equivalent. Let $\mathbf{L}$ have $l \ll m$ rows. Let $\mathbf{E}_{1}$ and $\mathbf{E}_{2}$ be the rounding matrices defined in (8). Also, define

$$
\begin{align*}
& \mathbf{E}_{4}:=\widehat{\mathbf{Y}}-\mathbf{L} \mathbf{X}  \tag{20a}\\
& \mathbf{E}_{5}:=\widehat{\mathbf{Z}} \widehat{\mathbf{R}}-\widehat{\mathbf{Y}} \tag{20b}
\end{align*}
$$

Our analysis will be based on the following assumptions.

Assumptions 5.3. Consider Algorithm 6. We assume that

$$
\begin{equation*}
u \leq 0.01 n^{-\frac{3}{2}} \kappa^{-1} \tag{21a}
\end{equation*}
$$

Furthermore, for $1 \leq i \leq l, 1 \leq j \leq n$,

$$
\begin{align*}
\left\|\mathbf{E}_{1(:, j)}\right\|_{2} & \leq 0.1 u n^{-\frac{1}{2}}\left\|\mathbf{X}_{(:, j)}\right\|_{2}  \tag{21b}\\
\left\|\mathbf{E}_{2(:, j)}\right\|_{2} & \leq 0.1 u n^{-\frac{1}{2}}\left\|\widehat{\mathbf{P}}_{(:, j)}\right\|_{2}  \tag{21c}\\
\left|\mathbf{E}_{4(i, j)}\right| & \leq 0.1 u\left\|\mathbf{L}_{(i,:)}\right\|_{2}\left\|\mathbf{X}_{(:, j)}\right\|_{2} \\
\left|\mathbf{E}_{5(i, j)}\right| & \leq 1.1 u n\left\|\widehat{\mathbf{Z}}_{(i,:)}\right\|_{2}\left\|\widehat{\mathbf{R}}_{(:, j)}\right\|_{2} \tag{21d}
\end{align*} \quad\left\|\widehat{\mathbf{S}}^{\mathrm{T}} \widehat{\mathbf{S}}-\mathbf{I}\right\|_{\mathrm{F}} \leq 0.1 u
$$

The conditions (21b) and (21c) are the same as (9b) and (9c) in the analysis of RCholeskyQR. They can be achieved by using unit roundoff $u_{f}=\mathcal{O}\left(m^{-1} n^{-1} u\right)$. With this unit roundoff we also can get (21d) that follows directly from standard rounding analysis from [20]. Furthermore, [20, Theorem 8.5] states that $\widehat{\mathbf{Z}}_{(i,:)}\left(\widehat{\mathbf{R}}^{2}+\Delta \mathbf{R}^{(i)}\right)=$ $\widehat{\mathbf{Y}}_{(i,:)}$ with $\left|\Delta \mathbf{R}^{(i)}\right| \leq 1.1$ un $|\widehat{\mathbf{R}}|$ that in turn implies (21e).

Theorem 5.4 provides stability characterization of Algorithm 5.

Theorem 5.4. Let $\boldsymbol{\Theta}$ be an $\varepsilon$-embedding for $\mathbf{X}$ with $\varepsilon \leq \frac{1}{2}$. Consider Algorithm 5. Under Assumptions 5.3, there exists $\mathbf{Q}$ with range $(\mathbf{Q})=\operatorname{range}(\mathbf{X})$ such that

$$
\begin{gather*}
\left\|\widehat{\mathbf{Z}}_{(i,:)}-\mathbf{L}_{(i,:)} \mathbf{Q}\right\|_{2} \leq 3.5 u n^{\frac{3}{2}} \kappa\left\|\mathbf{L}_{(i,:)}\right\|_{2}  \tag{22a}\\
(1+\varepsilon)^{-\frac{1}{2}}-0.45 u \kappa \leq \sigma_{\min }(\mathbf{Q}) \leq \sigma_{\max }(\mathbf{Q}) \leq(1-\varepsilon)^{-\frac{1}{2}}+0.45 u \kappa \tag{22b}
\end{gather*}
$$

for $1 \leq i \leq l$. In addition,

$$
\begin{equation*}
\|\widehat{\mathbf{S}}-\Theta \mathbf{Q}\|_{\mathrm{F}} \leq 0.35 u \kappa \tag{22c}
\end{equation*}
$$

and $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for $\mathbf{Q}$.

Proof. Take $\mathbf{Q}=\mathbf{X} \widehat{\mathbf{R}}^{-1}$. As in Theorem 5.2, scale $\mathbf{X}, \widehat{\mathbf{R}}, \widehat{\mathbf{P}}, \widehat{\mathbf{Y}}$ and $\mathbf{E}_{1}, \mathbf{E}_{2}, \mathbf{E}_{4}, \mathbf{E}_{5}$ by $\mathbf{D}=\operatorname{diag}\left(\left\|\mathbf{X}_{(:, j)}\right\|_{2}^{-1}\right)$ : $\mathbf{X} \leftarrow \mathbf{X D}, \widehat{\mathbf{R}} \leftarrow \widehat{\mathbf{R}} \mathbf{D}, \widehat{\mathbf{P}} \leftarrow \widehat{\mathbf{P}} \mathbf{D}, \widehat{\mathbf{Y}} \leftarrow \widehat{\mathbf{Y}} \mathbf{D}, \mathbf{E}_{1} \leftarrow \mathbf{E}_{1} \mathbf{D}, \mathbf{E}_{2} \leftarrow \mathbf{E}_{2} \mathbf{D}, \mathbf{E}_{4} \leftarrow \mathbf{E}_{4} \mathbf{D}, \mathbf{E}_{5} \leftarrow \mathbf{E}_{5} \mathbf{D}$, which does not affect the assumptions.

The fact that $\Theta$ is an $\varepsilon$-embedding for $\mathbf{Q}$ is obvious. Furthermore, the results (22b) and (22c) can be proven similarly to (14) and (15) in the proof of Theorem 5.2.

To show (22a) we shall use the following result (see (12)) from the proof of Theorem 5.2:

$$
\begin{equation*}
\|\widehat{\mathbf{R}}\|_{\mathrm{F}} \leq 1.26\|\mathbf{X}\|_{\mathrm{F}} \text { and } \sigma_{\min }(\widehat{\mathbf{R}}) \geq 0.66 \sigma_{\min }(\mathbf{X}) \tag{23}
\end{equation*}
$$

Then by (21d) and (21e) we have

$$
\begin{aligned}
\left\|\mathbf{Z}_{(i,:)}-\mathbf{L}_{(i,:)} \mathbf{Q}\right\|_{2} & \leq\left\|\mathbf{Z}_{(i,:)}-\mathbf{Y}_{(i,:)} \widehat{\mathbf{R}}^{-1}\right\|_{2}+\left\|\mathbf{Y}_{(i,:)} \widehat{\mathbf{R}}^{-1}-\mathbf{L}_{(i,:)} \mathbf{X} \widehat{\mathbf{R}}^{-1}\right\|_{2} \\
& \leq\left\|\mathbf{E}_{5(i,:)}\right\|_{2}\left\|\widehat{\mathbf{R}}^{-1}\right\|_{2}+\left\|\mathbf{E}_{4(i,:)}\right\|_{2}\left\|\widehat{\mathbf{R}}^{-1}\right\|_{2} \\
& \leq 1.1 u n^{\frac{3}{2}}\left\|\mathbf{Z}_{(i,:}\right\|\left\|_{2}\right\| \widehat{\mathbf{R}}\left\|_{2}\right\| \widehat{\mathbf{R}}^{-1}\left\|_{2}+0.1 u n^{\frac{1}{2}}\right\| \mathbf{L}_{(i,:)}\left\|_{2}\right\| \mathbf{X}\left\|_{2}\right\| \widehat{\mathbf{R}}^{-1} \|_{2} \\
& \leq 1.91 u n^{\frac{3}{2}} \operatorname{cond}(\mathbf{X})\left\|\mathbf{Z}_{(i,:)}\right\|_{2}+0.2 u n^{\frac{1}{2}} \operatorname{cond}(\mathbf{X})\left\|\mathbf{L}_{(i,:)}\right\|_{2}
\end{aligned}
$$

This relation particularly implies that $\left\|\mathbf{Z}_{(i,:)}\right\|_{2} \leq 1.6\left\|\mathbf{L}_{(i,:)}\right\|_{2}$. Consequently, we have

$$
\left\|\mathbf{Z}_{(i,:)}-\mathbf{L}_{(i,:)} \mathbf{Q}\right\|_{2} \leq 3.5 u n^{\frac{3}{2}} \operatorname{cond}(\mathbf{X})\left\|\mathbf{L}_{(i,:)}\right\|_{2}
$$

that is equivalent to (22a).

According to Theorem 5.4, Algorithm 5 computes a quantity of interest $\mathbf{Z}=\mathbf{L Q}$ associated with some wellconditioned matrix $\mathbf{Q}$ with range $(\mathbf{Q})=\operatorname{range}(\mathbf{X})$, with relative row-wise errors

$$
\frac{\left\|\widehat{\mathbf{Z}}_{(i,:)}-\mathbf{L}_{(i,:)} \mathbf{Q}\right\|_{2}}{\left\|\mathbf{L}_{(i,:)} \mathbf{Q}\right\|_{2}}=\mathcal{O}\left(u n^{\frac{3}{2}} \kappa\right), \quad 1 \leq i \leq l
$$

### 5.3 Stability of RRRCholeskyQR

This section is devoted to stability analysis of the RRRCholeskyQR algorithm. To simplify the presentation, we redefine $\mathbf{X}$ by permuting its columns with the permutation matrix $\boldsymbol{\Pi}$ :

$$
\mathbf{X} \leftarrow \mathbf{X \Pi}
$$

In addition, it is assumed that $\mathbf{X}$ has normalized columns, since the errors due to normalization are here negligible and can be ignored.

First notice that the factorization $\mathbf{X}_{(1: r)}=\widehat{\mathbf{Q}} \widehat{\mathbf{R}}_{(1: r)}$ can be viewed as a RCholeskyQR factorization of $\mathbf{X}_{(1: r)}$. Define the associated error matrices:

$$
\begin{align*}
& \mathbf{E}_{1}^{*}:=\boldsymbol{\Theta} \mathbf{X}_{(1: r)}-\widehat{\mathbf{P}}_{(1: r)}  \tag{24a}\\
& \mathbf{E}_{2}^{*}:=\widehat{\mathbf{P}}_{(1: r)}-\widehat{\mathbf{S}} \widehat{\mathbf{R}}_{(1: r)}  \tag{24b}\\
& \mathbf{E}_{3}^{*}:=\mathbf{X}_{(1: r)}-\widehat{\mathbf{Q}} \widehat{\mathbf{R}}_{(1: r)} \tag{24c}
\end{align*}
$$

Define also the error matrix associated with the computation of $\boldsymbol{\Theta} \mathbf{X}_{(r+1: n)}$ :

$$
\begin{equation*}
\mathbf{E}_{6}:=\boldsymbol{\Theta} \mathbf{X}_{(r+1: n)}-\widehat{\mathbf{P}}_{(r+1: n)} \tag{25}
\end{equation*}
$$

The stability analysis will be based on the following assumptions.

Assumptions 5.5. Consider Algorithm 7. We assume that

$$
\begin{equation*}
u \leq 0.001 n^{-\frac{3}{2}} r^{-\frac{5}{2}} \tag{26a}
\end{equation*}
$$

Furthermore,

$$
\begin{align*}
& \left\|\mathbf{E}_{1(:, j)}^{*}\right\|_{2} \leq 0.01 u r^{-\frac{1}{2}}\left\|\mathbf{X}_{(:, j)}\right\|_{2}  \tag{26b}\\
& \left\|\mathbf{E}_{2(:, j)}^{*}\right\|_{2} \leq 0.01 u r^{-\frac{1}{2}}\left\|\mathbf{P}_{(:, j)}\right\|_{2},  \tag{26c}\\
& \left|\mathbf{E}_{3(:, j)}^{*}\right| \leq 1.1 u r\left|\widehat{\mathbf{Q}} \| \widehat{\mathbf{R}}_{(:, j)}\right|,  \tag{26~d}\\
& \left\|\mathbf{E}_{6(:, j)}\right\|_{2} \leq 0.1 u n^{-\frac{1}{2}}\left\|\mathbf{X}_{(:, j)}\right\|_{2} \tag{26e}
\end{align*} \quad\left\|\widehat{\mathbf{S}}^{\mathrm{T}} \widehat{\mathbf{S}}-\mathbf{I}\right\|_{\mathrm{F}} \leq 0.1 u
$$

We also assume that the RRQR subroutine in step 2 is such that

$$
\begin{align*}
& \sigma_{r}\left(\widehat{\mathbf{R}}_{(1: r)}\right) \geq 0.5 n^{-\frac{1}{2}} r^{-\frac{1}{2}} \sigma_{r}(\widehat{\mathbf{P}}), \quad\left\|\widehat{\mathbf{R}}_{(r: n)}\right\|_{2} \leq 2 n^{\frac{1}{2}} r^{\frac{1}{2}} \sigma_{r}(\widehat{\mathbf{P}})  \tag{26f}\\
& \left\|\widehat{\mathbf{R}}_{(1: r)}^{-1} \widehat{\mathbf{R}}_{(r+1: n)}\right\|_{\mathrm{F}} \leq 2 n^{\frac{1}{2}} r^{\frac{1}{2}} \tag{26g}
\end{align*}
$$

The assumptions (26b) to (26d) repeat (9b) to (9d) when $\mathbf{X}_{(1: r)}=\widehat{\mathbf{Q}} \widehat{\mathbf{R}}_{(1: r)}$ is seen as a RCholeskyQR factorization of $\mathbf{X}_{(1: r)}$. They can be satisfied by using unit roundoff $u_{f}=\mathcal{O}\left(m^{-1} n^{-1} u\right)$ in the minor steps of Algorithm 7. Furthermore, by the standard rounding analysis we have $\left|\mathbf{E}_{6}\right| \leq \frac{m u_{f}}{1-m u_{f}}|\boldsymbol{\Theta}|\left|\mathbf{X}_{(r+1: n)}\right|$, that implies (26e) if $u_{f}=\mathcal{O}\left(m^{-1} n^{-1} u\right)$.

Furthermore, we shall assume that Algorithm 7 in step 2 is using strong RRQR subroutine that satisfies (26f) and (26g). This can be achieved for instance with the strong rank-revealing $\mathrm{QR}$ method from [16] with unit roundoff similar to the one required by the Givens $\mathrm{QR}$ i.e. $u_{f}=O\left(k^{-1} n^{-\frac{1}{2}} u\right)$. The method in [16] contains an extra parameter $f$ that in our case should be taken as, say, 1.5. Then the RRQR subroutine in step 2 will take a negligible amount $\mathcal{O}\left(k n^{2} \log n\right)$ of flops, while satisfying (26f) and (26g).

First, it is shown in Theorem 5.6 that RRRCholeskyQR permutes columns so that the condition number of $\mathbf{X}_{(1: r)}$ is bounded by $F(n, r) \tau^{-1}$.

Theorem 5.6. Let $\mathbf{X}$ have normalized columns. Consider Algorithm 7 using the strong rank-revealing QR algorithm and $\tau \geq 4 n^{\frac{3}{2}}$ ru. Let $\boldsymbol{\Theta}$ be an $\varepsilon$-embedding for $\mathbf{X}_{(1: r)}$ with $\varepsilon \leq \frac{1}{2}$. Under Assumptions 5.5 possibly excluding (26d) and (26e), we have

$$
\begin{equation*}
\operatorname{cond}\left(\mathbf{X}_{(1: r)}\right) \leq 10 n^{\frac{3}{2}} r \tau^{-1} \tag{27}
\end{equation*}
$$

Proof. First we notice that by (26f),

$$
\begin{equation*}
\sigma_{\min }\left(\widehat{\mathbf{R}}_{(1: r)}\right) \geq 4^{-1} n^{-1} r^{-1}\left\|\widehat{\mathbf{R}}_{(r: n)}\right\|_{2} \geq 4^{-1} n^{-\frac{3}{2}} r^{-1} \tau\|\widehat{\mathbf{R}}\|_{2} \tag{28}
\end{equation*}
$$

Thus, we deduce that

$$
\begin{equation*}
\operatorname{cond}\left(\widehat{\mathbf{R}}_{(1: r)}\right) \leq 4 n^{\frac{3}{2}} r \tau^{-1} \leq u^{-1} \tag{29}
\end{equation*}
$$

Furthermore, by (26c) we have

$$
\begin{equation*}
\left\|\widehat{\mathbf{P}}_{(1: r)}\right\|_{2} \leq\|\widehat{\mathbf{S}}\|_{2}\left\|\widehat{\mathbf{R}}_{(1: r)}\right\|_{2}+\left\|\mathbf{E}_{2}^{*}\right\|_{2} \leq 1.01\left\|\widehat{\mathbf{R}}_{(1: r)}\right\|_{2} \tag{30}
\end{equation*}
$$

and

$$
\begin{align*}
\sigma_{\min }\left(\widehat{\mathbf{P}}_{(1: r)}\right) & \geq \sigma_{\min }(\widehat{\mathbf{S}}) \sigma_{\min }\left(\widehat{\mathbf{R}}_{(1: r)}\right)-\left\|\mathbf{E}_{2}^{*}\right\|_{2} \geq 0.99 \sigma_{\min }\left(\widehat{\mathbf{R}}_{(1: r)}\right)-0.01 u\left\|\mathbf{P}_{(1: r)}\right\|_{2} \\
& \geq 0.99 \sigma_{\min }\left(\widehat{\mathbf{R}}_{(1: r)}\right)-0.011 u\left\|\mathbf{R}_{(1: r)}\right\|_{2}  \tag{31}\\
& \geq \sigma_{\min }\left(\widehat{\mathbf{R}}_{(1: r)}\right)\left(0.99-0.011 u \operatorname{cond}\left(\widehat{\mathbf{R}}_{(1: r)}\right) \geq 0.97 \sigma_{\min }\left(\widehat{\mathbf{R}}_{(1: r)}\right)\right.
\end{align*}
$$

Consequently,

$$
\begin{equation*}
\operatorname{cond}\left(\widehat{\mathbf{P}}_{(1: r)}\right) \leq 4.2 n^{\frac{3}{2}} r \tau^{-1} \leq 1.05 u^{-1} \tag{32}
\end{equation*}
$$

Next, by (26b) we get

$$
\begin{equation*}
\left\|\boldsymbol{\Theta} \mathbf{X}_{(1: r)}\right\|_{2} \leq\left\|\widehat{\mathbf{P}}_{(1: r)}\right\|_{2}+\left\|\mathbf{E}_{1}^{*}\right\|_{2} \leq 1.01\left\|\widehat{\mathbf{P}}_{(1: r)}\right\|_{2} \tag{33}
\end{equation*}
$$

which due to the $\varepsilon$-embedding property of $\boldsymbol{\Theta}$ implies that

$$
\begin{equation*}
\left\|\mathbf{X}_{(1: r)}\right\|_{2} \leq 1.5\left\|\widehat{\mathbf{P}}_{(1: r)}\right\|_{2} \tag{34}
\end{equation*}
$$

We also have by (26b) and (32),

$$
\begin{align*}
\sigma_{\min }\left(\boldsymbol{\Theta} \mathbf{X}_{(1: r)}\right) & \geq \sigma_{\min }\left(\widehat{\mathbf{P}}_{(1: r)}\right)-\left\|\mathbf{E}_{1}^{*}\right\|_{2} \geq \sigma_{\min }\left(\widehat{\mathbf{P}}_{(1: r)}\right)-0.01 u\left\|\mathbf{X}_{(1: r)}\right\|_{2} \\
& \geq \sigma_{\min }\left(\widehat{\mathbf{P}}_{(1: r)}\right)-0.015 u\left\|\widehat{\mathbf{P}}_{(1: r)}\right\|_{2} \geq 0.92 \sigma_{\min }\left(\widehat{\mathbf{P}}_{(1: r)}\right) \tag{35}
\end{align*}
$$

Consequently, by the $\varepsilon$-embedding property of $\boldsymbol{\Theta}$ and (34) and (35),

$$
\operatorname{cond}\left(\mathbf{X}_{(1: r)}\right) \leq \sqrt{\frac{1+\varepsilon}{1-\varepsilon}} \operatorname{cond}\left(\boldsymbol{\Theta} \mathbf{X}_{(1: r)}\right) \leq 1.91 \operatorname{cond}\left(\widehat{\mathbf{P}}_{(1: r)}\right) \leq 10 n^{\frac{3}{2}} r \tau^{-1}
$$

that finishes the proof.

Theorem 5.6 implies stability of the computation of the $\mathrm{Q}$ factor by forward substitution in step 5 . With this result we are ready to establish the overall stability guarantee.

Theorem 5.7. Let $\mathbf{X}$ have normalized columns. Consider Algorithm 7 using the strong rank-revealing QR algorithm and $1000 r^{\frac{5}{2}} n^{\frac{3}{2}} u \leq \tau \leq 1$. Let $\Theta$ be an $(\varepsilon, \delta, n)-O S E$ with $\varepsilon \leq \frac{1}{2}$. Under Assumptions 5.5, we have with probability at least $1-\delta$,

$$
\begin{align*}
\|\mathbf{X}-\widehat{\mathbf{Q}} \widehat{\mathbf{R}}\|_{\mathrm{F}} & \leq 2 \tau  \tag{36a}\\
(1+\varepsilon)^{-\frac{1}{2}}-0.016 \leq \sigma_{\min }(\widehat{\mathbf{Q}}) & \leq \sigma_{\max }(\widehat{\mathbf{Q}}) \leq(1-\varepsilon)^{-\frac{1}{2}}+0.016 \tag{36~b}
\end{align*}
$$

In addition, it holds that

$$
\begin{equation*}
\|\widehat{\mathbf{S}}-\boldsymbol{\Theta} \widehat{\mathbf{Q}}\|_{\mathrm{F}} \leq 61 r^{\frac{5}{2}} n^{\frac{3}{2}} \frac{u}{\tau} \leq 0.061 \tag{36c}
\end{equation*}
$$

Furthermore, the stability guarantees (36) hold with probability at least $1-\delta$, if $\boldsymbol{\Theta}$ is an $\left(\varepsilon,\binom{n}{r+1}^{-1} \delta, r+1\right)-O S E$ and not necessarily an $(\varepsilon, \delta, n)-O S E$.

Proof. Similarly as in Proposition 4.1, we here shall assume that $\boldsymbol{\Theta}$ is an $\varepsilon$-embedding for all subspaces spanned by $r+1$ columns of $\mathbf{X}$. As is argued in the proof of Proposition 4.1 this condition is satisfied with probability at least $1-\delta$. Then by (26b), (26c), (26e) and (26g) we have

$$
\begin{align*}
\left\|\mathbf{X}_{(r+1: n)}-\widehat{\mathbf{Q}} \widehat{\mathbf{R}}_{(r+1: n)}\right\|_{\mathrm{F}} & =\left\|\mathbf{X}_{(r+1: n)}-\mathbf{X}_{(1: r)} \widehat{\mathbf{R}}_{(1: r)}^{-1} \widehat{\mathbf{R}}_{(r+1: n)}\right\|_{\mathrm{F}}+\left\|\mathbf{E}_{3}^{*} \widehat{\mathbf{R}}_{(1: r)}^{-1} \widehat{\mathbf{R}}_{(r+1: n)}\right\|_{\mathrm{F}} \\
& \leq(1-\varepsilon)^{-\frac{1}{2}}\left\|\boldsymbol{\Theta}\left(\mathbf{X}_{(r+1: n)}-\mathbf{X}_{(1: r)} \widehat{\mathbf{R}}_{(1: r)}^{-1} \widehat{\mathbf{R}}_{(r+1: n)}\right)\right\|_{\mathrm{F}}+\left\|\mathbf{E}_{3}^{*}\right\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}_{(1: r)}^{-1} \widehat{\mathbf{R}}_{(r+1: n)}\right\|_{2}  \tag{37}\\
& \leq(1-\varepsilon)^{-\frac{1}{2}}\left(\left\|\mathbf{E}_{6}\right\|_{\mathrm{F}}+\left\|\mathbf{E}_{7}\right\|_{\mathrm{F}}+\left\|\mathbf{E}_{8}\right\|_{\mathrm{F}}+\left\|\mathbf{E}_{9}\right\|_{\mathrm{F}}\right)+2 n^{\frac{1}{2}} r^{\frac{1}{2}}\left\|\mathbf{E}_{3}^{*}\right\|_{\mathrm{F}}
\end{align*}
$$

where

$$
\begin{aligned}
& \left\|\mathbf{E}_{6}\right\|_{\mathrm{F}}=\left\|\boldsymbol{\Theta} \mathbf{X}_{(r+1: n)}-\widehat{\mathbf{P}}_{(r+1: n)}\right\|_{\mathrm{F}} \leq 0.1 u\left\|\mathbf{X}_{(r+1: n)}\right\|_{2} \\
& \left\|\mathbf{E}_{7}\right\|_{\mathrm{F}}=\left\|\widehat{\mathbf{P}}_{(r+1: n)}-\widehat{\mathbf{S}} \widehat{\mathbf{R}}_{(r+1: n)}\right\|_{\mathrm{F}} \leq 1.01 \tau\|\widehat{\mathbf{R}}\|_{2} \leq 1.02(1+\varepsilon)^{\frac{1}{2}} \tau\|\mathbf{X}\|_{\mathrm{F}}
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-19.jpg?height=65&width=1287&top_left_y=881&top_left_x=408)

$$
\begin{aligned}
& \leq 2 n^{\frac{1}{2}} r^{\frac{1}{2}}\left\|\mathbf{E}_{2}^{*}\right\|_{\mathrm{F}} \leq 0.02 n^{\frac{1}{2}} r^{\frac{1}{2}} u\left\|\widehat{\mathbf{P}}_{(1: r)}\right\|_{2} \leq 0.03 n u\left\|\mathbf{X}_{(1: r)}\right\|_{2} \\
& \left\|\mathbf{E}_{9}\right\|_{\mathbf{F}}=\left\|\left(\widehat{\mathbf{P}}_{(1: r)}-\boldsymbol{\Theta} \mathbf{X}_{(1: r)}\right) \widehat{\mathbf{R}}_{(1: r)}^{-1} \widehat{\mathbf{R}}_{(r+1: n)}\right\|_{2} \leq\left\|\boldsymbol{\Theta} \mathbf{X}_{(1: r)}-\widehat{\mathbf{P}}_{(1: r)}\right\|_{\mathrm{F}}\left\|\widehat{\mathbf{R}}_{(1: r)}^{-1} \widehat{\mathbf{R}}_{(r+1: n)}\right\|_{2} \\
& \leq 2 n^{\frac{1}{2}} r^{\frac{1}{2}}\left\|\mathbf{E}_{1}^{*}\right\|_{\mathrm{F}} \leq 0.02 n u\left\|\mathbf{X}_{(1: r)}\right\|_{2}
\end{aligned}
$$

Consequently,

$$
\begin{equation*}
\left\|\mathbf{X}_{(r+1: n)}-\widehat{\mathbf{Q}} \widehat{\mathbf{R}}_{(r+1: n)}\right\|_{\mathrm{F}} \leq \sqrt{2}\left(u n+1.02 \sqrt{\frac{3}{2}} \tau\right)\|\mathbf{X}\|_{\mathrm{F}} \tag{38}
\end{equation*}
$$

Furthermore, from Theorem 5.6 it follows that

$$
\begin{equation*}
\operatorname{cond}\left(\mathbf{X}_{(1: r)}\right) \leq 10 n^{\frac{3}{2}} r \tau^{-1} \tag{39}
\end{equation*}
$$

By looking at $\widehat{\mathbf{Q}}$ and $\widehat{\mathbf{R}}_{(1: r)}$ as a RCholeskyQR factorization of $\mathbf{X}_{(1: r)}$, according to Theorem 5.2, we have

$$
\begin{align*}
&\left\|\mathbf{X}_{(1: r)}-\widehat{\mathbf{Q}} \widehat{\mathbf{R}}_{(1: r)}\right\|_{\mathrm{F}} \leq 2.1 u r\left\|\mathbf{X}_{(1: r)}\right\|_{\mathrm{F}}  \tag{40a}\\
&(1+\varepsilon)^{-\frac{1}{2}}-4 u r^{\frac{3}{2}} \operatorname{cond}\left(\mathbf{X}_{(1: r)}\right) \leq \sigma_{\min }(\widehat{\mathbf{Q}}) \leq \sigma_{\max }(\widehat{\mathbf{Q}}) \leq(1-\varepsilon)^{-\frac{1}{2}}+4 u r^{\frac{3}{2}} \operatorname{cond}\left(\mathbf{X}_{(1: r)}\right)  \tag{40b}\\
&\|\widehat{\mathbf{S}}-\boldsymbol{\Theta} \widehat{\mathbf{Q}}\|_{\mathrm{F}} \leq 6.1 u r^{\frac{3}{2}} \operatorname{cond}\left(\mathbf{X}_{(1: r)}\right) \tag{40c}
\end{align*}
$$

By combing (40a) with (38) we obtain (36a). By combining (40b) with (39) we obtain (36b). Finally, by combing (40c) with (39) we obtain (36c) and finish the proof.

## 6 Numerical experiments

In this section, the proposed RCholeskyQR and RRRCholeskyQR factorizations are verified on numerical examples. They are compared in terms of stability with the standard CholeskyQR2 and Householder QR factorizations, as well as with the shifted CholeskyQR2 and shifted CholeskyQR3 from [14], the classical block Gram-Schmidt algorithm with reorthogonalization (BCGS2), the modified block Gram-Schmidt algorithm (BMGS) and the block RGS algorithm from [2]. We also characterize the potential speedups of RCholeskyQR2 and RRRCholeskyQR2 methods over the shifted CholeskyQR3 and Householder QR.

### 6.1 Direct QR factorization

The algorithms were tested on series $\mathbf{X}^{(1)}, \mathbf{X}^{(2)}, \ldots, \mathbf{X}^{(j)}$ of matrices of various types and sizes. For the sake of completeness, we have considered two scenarios: one that concerns obtaining a well-conditioned $\mathrm{Q}$ factor with RCholeskyQR, RRRCholeskyQR, CholeskyQR2, shifted CholeskyQR2, or Householder QR, and one that concerns
obtaining an orthonormal Q factor with RCholeskyQR2, RRRCholeskyQR2, shifted CholeskyQR3, or Householder QR. On the plots, these methods are denoted by RCholQR, RRRCholQR, CholQR2, sCholQR2, HH and RCholQR2, RRRCholQR2, sCholQR3, HH, respectively. In the first case, stability was characterized by cond $(\mathbf{Q})$, and in the second case, by the classical measure $\Delta=\left\|\mathbf{Q}^{\mathrm{T}} \mathbf{Q}-\mathbf{I}\right\|_{2}$. Since in randomized algorithms the SRHT and Gaussian matrices gave very similar results, here we present the results for SRHT only.

To ensure a fair comparison, before executing the shifted CholeskyQR algorithms, we normalized the columns of $\mathbf{X}^{(i)}$. Furthermore, we tested several variants of shifts, and then chose those that gave the greatest stability. Namely, the first orthogonalization was performed considering the shift $s$ as the maximum between $s_{0}$ and the smallest power of 10 , such that $\mathbf{X}^{\mathrm{T}} \mathbf{X}+s \mathbf{I}$ is numerically positive-definite. The $s_{0}$ parameter was chosen to be either zero, or the recommended value from [14]: $11 u(m n+n(n+1))\|\mathbf{X}\|_{2}^{2}$, or empirically chosen value: $u \sqrt{n}\|\mathbf{X}\|_{\mathrm{F}}^{2}$. In addition, the second orthogonalization was performed either with zero shift or the smallest power of 10 , so that $\mathbf{Q}^{\mathrm{T}} \mathbf{Q}$ is numerically positive definite. In the shifted CholeskyQR3, the third orthogonalization was performed with zero shift.

In the RCholeskyQR algorithm, we took the QR subroutine in step 2 as the Householder QR. In RRRCholeskyQR, we took RRQR as a strong rank-revealing QR from [16] with parameter $f=1.5$. Furthermore, we considered an (implicit) orthonormalization of the columns of $\mathbf{X}^{(i)}$ as explained in Section 4. The truncation parameter $\tau$ was chosen to be of order of $10^{-15}$ for experiments in float64 format and of order of $10^{-7}$ for experiments in float32. It was revealed that, depending on the experiment, changing this parameter by a small factor could slightly improve the results. Even though the improvements were not that significant, for a fair comparison, we decided to present the results corresponding to the best tested values of $\tau$.

In the first test, the matrices $\mathbf{X}^{(i)}$ were taken of the form $\mathbf{X}^{(i)}=\mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\mathrm{T}}$, where $\mathbf{U}$ and $\mathbf{V}$ are $m \times n$ and $n \times n$ random Gaussian matrices orthonormalized by the Householder QR, and $\boldsymbol{\Sigma}=\operatorname{diag}\left(\left[1, \sigma^{\frac{1}{n-1}}, \ldots, \sigma^{\frac{n-2}{n-1}}, \sigma\right]\right)$, with parameter $\sigma=\sigma(i)$ ranging from $10^{-15}$ to 1 , controlling the condition number of $\mathbf{X}^{(i)}$, as in [14]. The dimensions $m$ and $n$ were chosen as $10^{6}$ and 300 respectively, and the sketching size $k$ as $2 n=600$. In this experiment, all operations were performed in float64 format with unit rounding $\approx 10^{-16}$. In RRRCholeskyQR we chose the $\tau$ parameter to be $4 \times 10^{-15}$. We first turned to computing the $\mathrm{QR}$ factorization with a well-conditioned $\mathrm{Q}$ factor. From Figures 1a and 1b one can see that RCholeskyQR and RRRCholeskyQR were very stable for all $\mathbf{X}^{(i)}$, as were the shifted CholeskyQR2 and Householder QR. The standard CholeskyQR2, on the other hand, failed when $\operatorname{cond}\left(\mathbf{X}^{(i)}\right)$ got larger than about $10^{8} \approx u^{\frac{1}{2}}$, which is in good agreement with the theory. In Figures 1c and 1d we depict the accuracy of randomized and deterministic $\mathrm{QR}$ factorizations in terms of orthogonality of the $\mathrm{Q}$ factor. As in the previous experiment, RCholeskyQR2, RRRCholeskyQR2, shifted CholeskyQR3 and Householder QR provided near perfect stability.

In the second test case, we constructed the $\mathbf{X}^{(i)}$ matrices as in [3]. In particular, we took an uniform unit grid of size $m \times n$ with $m=10^{6}$ and $n=500$, and constructed an $m \times n$ matrix $\mathbf{W}$ with entries equal to

$$
f(x, y)=\frac{\sin (10(y+x))}{\cos (100(y-x))+1.1}
$$

evaluated at the corresponding grid points. Then we considered $\mathrm{QR}$ factoizations of matrices $\mathbf{X}^{(i)}=\mathbf{W}_{(1: i)}$, $1 \leq i \leq n$. In randomized algorithms the matrix $\Theta$ was taken of size $k=2 n=1000$. Here we used float32 arithmetic with working roundoff $u \approx 10^{-7}$. In RRRCholeskyQR the $\tau$ parameter was taken as $2 \times 10^{-7}$. In addition, we tested the benefits of using multi-precision arithmetic in randomized algorithms. It turned out that performing minor operations in the float64 format allowed to reduce the condition number of the $\mathrm{Q}$ factor in RRRCholeskyQR by almost an order of magnitude, while in RCholeskyQR the multi-precision framework did not provide a significant advantage. Therefore, the following results for RRRCholeskyQR will be for the multi-precision algorithm, and the results for RCholeskyQR will be for the unique precision algorithm. Similar considerations are valid for RRRCholeskyQR2 and RCholeskyQR2. From Figure 2, we see that all tested methods besides the standard CholeskyQR2 showed great stability when $\mathbf{X}^{(i)}$ were full-rank, which is in good agreement with the theory. However, when $\mathbf{X}^{(i)}$ became numerically rank-deficient at $i \geq 110$ the stability of RCholeskyQR and shifted CholeskyQR2 became deteriorated (see Figure 2a). Note that for RCholeskyQR the instabilities were not as high as for shifted CholeskyQR2. In contrast, the RRRCholeskyQR algorithm provided a near-perfect stability for all $\mathbf{X}^{(i)}$. This supports the guarantee of an unconditional stability of RRRCholeskyQR from Section 5.3. Moreover, judging by

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-21.jpg?height=1081&width=1333&top_left_y=305&top_left_x=388)

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-21.jpg?height=456&width=613&top_left_y=325&top_left_x=409)

(a) Cond. number of $\mathbf{Q}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-21.jpg?height=480&width=617&top_left_y=839&top_left_x=407)

(c) Stability measure $\Delta=\left\|\mathbf{Q}^{\mathrm{T}} \mathbf{Q}-\mathbf{I}\right\|_{2}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-21.jpg?height=471&width=629&top_left_y=323&top_left_x=1084)

(b) Max relative column-wise error.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-21.jpg?height=477&width=629&top_left_y=843&top_left_x=1084)

(d) Max relative column-wise error.

Figure 1: Stability characterization of $\mathrm{QR}$ factorizations of $\mathbf{X}^{(i)}$ of the form $\mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\mathrm{T}}$, where $\mathbf{U}$ and $\mathbf{V}$ are Gaussian matrices orthonormalized by Householder $\mathrm{QR}$, and $\boldsymbol{\Sigma}=\operatorname{diag}\left(\left[1, \sigma^{\frac{1}{n-1}}, \ldots, \sigma^{\frac{n-2}{n-1}}, \sigma\right]\right)$, with parameter $\sigma=\sigma(i)$ ranging from $10^{-15}$ to 1 .

the approximation errors, this algorithm turned out to be even more accurate than the Householder QR, which, we believe, is a consequence of the use of a multi-precision arithmetic framework. Now let us turn to the context of obtaining an orthonormal Q factor. According to Figure 2c, in this case RRRCholeskyQR2 showed greater stability than Householder $\mathrm{QR}$ and, in particular, provided a lower measure of stability $\left\|\mathbf{Q}^{\mathrm{T}} \mathbf{Q}-\mathbf{I}\right\|_{2}$ by more than an order of magnitude. It is worth noting that RCholeskyQR2 surprisingly also showed similar or better stability as the Householder QR, while the shifted CholeskyQR3 for some $\mathbf{X}^{(i)}$ provided a higher stability measure by more than two orders of magnitude.

To validate the unconditional stability of RRRCholeskyQR to a better extent, the third experiment involves QR factorization of rank-deficient test matrices that are particularly poorly suited to $\mathrm{QR}$ factorization from a numerical point of view. We have taken $\mathbf{X}^{(i)}$ of the form $\mathbf{U V}$, where the matrix $\mathbf{U}$ is an $m \times n$ Gaussian matrix whose first row was scaled by a factor $\sigma(i)$ in the range from 1 to $10^{15}$, and that was orthonormalized with Householder QR. The matrix $\mathbf{V}$ is the upper triangular part of an $n \times n$ orthonormalized Gaussian matrix with modified diagonal entries to $\operatorname{diag}(\mathbf{V})=\left[1,10^{-15}, \ldots, 10^{-15}, 10^{-15}\right]$. We took $m$ as $10^{6}, n$ as 300 and $k=2 n=600$. It turned out that the generated matrices $\mathbf{X}^{(i)}$ had a rank of about $r \approx 290$. The $\mathrm{QR}$ factorizations were calculated in float 64 arithmetic. The $\tau$ parameter in RRRCholeskyQR was chosen to be $5 \times 10^{-16}$. From Figures 3a and 3b one can see that the standard Cholesky QR2 here failed completely, while the RCholeskyQR and the shifted CholeskyQR2 showed highly deteriorated stability than before. In contrast, the RRRCholeskyQR factorization still was almost perfectly stable, as was the Householder QR. This once again proves the unconditional stability of RRRCholeskyQR. It is important to note that it was revealed that the shifted CholeskyQR3 and RCholeskyQR2 not only could not provide an approximately orthonormal $\mathrm{Q}$ factor for large values of $\sigma$ (see Figure 3c), but they even failed several

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-22.jpg?height=1089&width=1347&top_left_y=301&top_left_x=384)

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-22.jpg?height=475&width=613&top_left_y=321&top_left_x=409)

(a) Cond. number of $\mathbf{Q}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-22.jpg?height=472&width=615&top_left_y=848&top_left_x=408)

(c) Stability measure $\Delta=\left\|\mathbf{Q}^{\mathrm{T}} \mathbf{Q}-\mathbf{I}\right\|_{2}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-22.jpg?height=477&width=631&top_left_y=323&top_left_x=1083)

(b) Max relative column-wise error.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-22.jpg?height=466&width=631&top_left_y=854&top_left_x=1083)

(d) Max relative column-wise error.

Figure 2: Stability characterization of $\mathrm{QR}$ factorizations of $\mathbf{X}^{(i)}=\mathbf{W}_{(1: i)}$, where $\mathbf{W}$ is a discretization of $f(x, y)$ on an uniform $10^{6} \times 500$ grid.

times due to the numerical indefiniteness of $\mathbf{Q}^{\mathrm{T}} \mathbf{Q}$ at the last CholeskyQR step, whereas RRRCholeskyQR2 showed great stability.

### 6.2 Randomized block GMRES

Next, we tested the stability of the column-oriented RCholeskyQR in the context of constructing a well-conditioned Krylov basis for solving a block linear system of equations

$$
\mathbf{A U}=\mathbf{B}
$$

with randomized GMRES method $[2,3]$. We have taken the linear system considered in numerical experiments in [2]. Namely, we took $\mathbf{A}=\left(\mathbf{A}_{G a}+0.2 \mathbf{I}\right) \mathbf{P}_{G a}$, where $\mathbf{A}_{G a}$ is the "Ga41As41H72" matrix of dimension $m=268096$ from the SuiteSparse matrix collection, and $\mathbf{P}_{G a}$ is the incomplete LU preconditioner of $\mathbf{A}_{G a}+0.2 \mathbf{I}$ with zero level of fill-in and symmetric reverse Cuthill-McKee reordering. The matrix $\mathbf{A}$ was not computed explicitly, but provided as an implicit map that outputs product with vectors and matrices. The right hand side matrix $\mathbf{B}$ was taken as an $m \times 100$ Gaussian matrix. This system has been approximately solved using the GMRES method based on various versions of the block Gram-Schmidt process or the column-oriented RCholeskyQR. We restarted GMRES every 30 iterations, i.e. when the dimension of the Krylov space became $n=3100$. In the randomized algorithms, the sketching dimension was chosen to be $k=7500$. In the experiments, the products with $\mathbf{A}$ were calculated in float64 format. Solutions to Hessenberg least-squares problems in GMRES were obtained with Givens rotations that were also performed in float64 format. All operations related to the orthogonalization process were performed and accumulated in float32 format.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-23.jpg?height=1098&width=1355&top_left_y=299&top_left_x=382)

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-23.jpg?height=456&width=613&top_left_y=325&top_left_x=409)

(a) Cond. number of $\mathbf{Q}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-23.jpg?height=474&width=615&top_left_y=842&top_left_x=408)

(c) Stability measure $\Delta=\left\|\mathbf{Q}^{\mathrm{T}} \mathbf{Q}-\mathbf{I}\right\|_{2}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-23.jpg?height=471&width=631&top_left_y=323&top_left_x=1083)

(b) Max relative column-wise error.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-23.jpg?height=475&width=636&top_left_y=841&top_left_x=1078)

(d) Max relative column-wise error.

Figure 3: Stability characterization of QR factorizations of rank-deficient $\mathbf{X}^{(i)}=\mathbf{U V}$ where $\mathbf{U}$ is a Gaussian matrix whose first row was scaled by $\sigma$, and that was then orthonormalized by Householder QR.

In the column-oriented version of RCholeskyQR (Algorithm 4), we computed $\mathbf{R}_{(1: i-1, i)}$ and $\mathbf{S}_{(i)}, \mathbf{R}_{(i, i)}$ in steps 3 and 4 with Householder QR. We here tested the variant of the block RGS algorithm that completely repeats the column-oriented RCholeskyQR (Algorithm 4) with updating the sketch $\mathbf{S}_{(i-1)} \leftarrow \Theta \mathbf{Q}_{(i-1)}$ in step 2 as described in Section 3.2. In the deterministic BCGS2 and BMGS algorithms, the inter-block orthogonalizations were performed by Householder QR.

Figure 4 depicts the convergence of the residual error $\max _{1 \leq j \leq 100}\left\|\mathbf{A} \mathbf{U}_{(j)}-\mathbf{B}_{(j)}\right\|_{2} /\left\|\mathbf{B}_{(j)}\right\|_{2}$ and the condition number of the computed Krylov basis $\mathbf{Q}_{(1: i)}$ at each iteration $i$. The BCGS2 remained perfectly stable throughout all iterations. However, it is the most expensive algorithm tested, requiring nearly four times as many flops as the block RGS and RCholeskyQR algorithms. The block RGS also remained stable at all iterations. It provided a well-conditioned $\mathrm{Q}$ factor with cond $(\mathbf{Q}) \leq 5$ and almost as good residual error as BCGS2. We observe instabilities in BMGS and RCholeskyQR, which, unfortunately, worsened the convergence of the solution. In fact, it can be seen that RCholeskyQR entailed a residual error and the condition number of the Krylov basis that are almost four orders of magnitude greater than those of the block RGS. From this, we conclude that block RGS should be preferred over RCholeskyQR in the context of solving linear systems.

### 6.3 Runtime comparison

In this subsection, we explore the speedups that can be achieved with the proposed methods for computing QR factorizations with an orthonormal $\mathrm{Q}$ factor. The following experiments were performed in MATLAB R2021b on a node with 192GB of RAM and 2x Cacade Lake Intel Xeon 521816 cores 2.4GHz processor. We generated a sequence of random matrices $\mathbf{X}=\mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\mathrm{T}}$ of different sizes and ranks, where $\mathbf{U}$ and $\mathbf{V}$ are orthonormalized

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-24.jpg?height=604&width=1328&top_left_y=305&top_left_x=390)

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-24.jpg?height=475&width=613&top_left_y=321&top_left_x=409)

(a) Max column-wise residual error $\max _{j}\left\|\mathbf{A} \mathbf{U}_{(j)}-\mathbf{B}_{(j)}\right\|_{2} /\left\|\mathbf{B}_{(j)}\right\|_{2}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-24.jpg?height=477&width=616&top_left_y=342&top_left_x=1096)

(b) Cond. number of Krylov basis $\mathbf{Q}_{(1: i)}$.

Figure 4: Solution of a linear system with GMRES.

random Gaussian matrices, $\boldsymbol{\Sigma}=\operatorname{diag}\left(\left[1, \sigma^{\frac{1}{r-1}}, \ldots, \sigma^{\frac{r-2}{r-1}}, \sigma\right]\right)$, and $\sigma=10^{-15}$. Then such $\mathbf{X}$ were orthonormalized with RCholeskyQR2, RRRCholeskyQR2, shifted CholeskyQR3 and Householder QR. To compute the Householder $\mathrm{QR}$ we used the MATLAB's built-in function qr. In shifted CholeskyQR3 we used the built-in chol function for the computation of Cholesky decomposition, and the built-in BLAS-3 forward substitution and matrix-matrix multiplication for other operations. In RCholeskyQR2 and RRRCholeskyQR2 algorithms we chose $\Theta$ as a Gaussian OSE with twice as many rows as there are columns in $\mathbf{X}$. Furthermore, in randomized algorithms, in addition to the total runtime, we also measured the runtime corresponding to an ideal scenario where the sketching step requires negligible computational cost compared to other operations. In the given architecture, this could be achieved, for instance, with a well-implemented SRHT embedding.

It can be seen from Figure 5a that the proposed RCholeskyQR2, despite being as or even more stable than the shifted CholeskyQR3, was up to 1.5 times faster. In addition, this speedup could potentially be increased to a 2 factor by using a more efficient sketching step. Let us now compare runtimes of the RRRCholeskyQR2 algorithm and Householder QR that are both unconditionally stable. We find from Figure 5b that for large $\mathbf{X}^{(i)}$ RRRCholeskyQR2 required almost half as much runtime as Householder QR, which could potentially be reduced even more.

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-24.jpg?height=561&width=1312&top_left_y=1674&top_left_x=404)

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-24.jpg?height=477&width=613&top_left_y=1689&top_left_x=409)

(a) RCholQR2 vs. shifted CholQR3

![](https://cdn.mathpix.com/cropped/2024_06_04_0b0a2c4bfa27a755a240g-24.jpg?height=483&width=624&top_left_y=1688&top_left_x=1092)

(b) RRRCholQR2 vs. Householder

Figure 5: Runtimes in seconds taken by the QR factorizations of full-rank matrices $\mathbf{X}$ of varying sizes.

Furthermore, as was said, the RRRCholeskyQR2 algorithm not only provides the benefit of unconditional stability, but also has the ability to significantly reduce the computational cost when the matrix $\mathbf{X}$ is of relatively
low rank. This fact was validated too. From Table 1 we reveal that RRRCholeskyQR2 was almost 3.5 times faster than the Householder QR when $\mathbf{X}$ had a moderate rank, which could be improved to 10 times, or potentially even 100 times, when $\mathbf{X}$ was of low rank.

Table 1: Runtimes in seconds taken by QR factorizations of matrices $\mathbf{X}$ of size $m=2^{20}$ and $n=1000$, and of varying ranks $r$. For randomized algorithms, next to the overall runtimes we also provide the runtimes that could be achieved if the sketching step had a negligible computational cost.

|  | $r=1000$ |  | $r=500$ |  | $r=100$ |  | $r=10$ |  |
| :--- | :---: | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| RCholQR2 | 20.4 | 15.7 | 20.4 | 15.7 | 20.7 | 16.1 | 20.7 | 16.1 |
| RRRCholQR2 | 23.4 | 18.5 | 14.1 | 9.2 | 6.2 | 1.6 | 4.4 | 0.35 |
| sCholQR3 | 29.6 | - | 31.6 | - | 31.2 | - | 29.7 | - |
| HH | 44.6 | - | 47.5 | - | 50.5 | - | 44.8 | - |

## 7 Conclusion

This article proposed several variants of randomized Cholesky QR factorization. The presented direct RCholeskyQR algorithm should be up to four times more efficient than standard/shifted CholeskyQR2. Yet, it is just as or even more stable, and provides a well-conditioned $\mathrm{Q}$ factor whenever the input matrix is numerically full-rank. If necessary, RCholeskyQR can be readily augmented with the standard CholeskyQR to provide a Q factor that is orthonormal to machine precision, and not just well-conditioned, which results in RCholeskyQR2 algorithm. We have depicted some derivatives of RCholeskyQR, such as the column-oriented RCholeskyQR and the reduced RCholeskyQR. These algorithms can be useful for instance for constructing a Krylov basis, or for computing an approximation of a linear system's solution on a reduced basis. In addition, we have proposed an unconditionally stable RRRCholeskyQR, which can be seen as a very desirable alternative to other existing unconditionally stable algorithms such as Householder QR or TSQR. The RRRCholeskyQR should have the same computational cost as RCholeskyQR, or even better if the input matrix is of low rank. The proposed methodology was supported by rigorous theoretical and numerical stability analysis. The efficiency gains were also verified both theoretically and experimentally. In particular, in numerical experiments, we revealed speedups of RCholeskyQR2 and RRRCholeskyQR2 by almost 1.5 and 2 compared to the shifted CholeskyQR3 and Householder QR, respectively. These improvements could be made even greater by using a more efficient sketching subroutine. In addition, when the input matrix $\mathbf{X}$ was of low rank, RRRCholeskyQR2 provided a much higher speedup, namely by a factor of 10 or even 100 (potentially).

## 8 Acknowledgments

The author would like to thank Laura Grigori for useful discussions on randomized algorithms. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement No 810367).

## References

[1] O. Balabanov. "MATLAB library featuring randomized Krylov algorithms." GitHub repository: https : // github. com/obalabanov/randKrylov (2022) (cit. on p. 3).

[2] O. Balabanov and L. Grigori. "Randomized block Gram-Schmidt process for solution of linear systems and eigenvalue problems" arXiv preprint arXiv:2111.14641 (2021) (cit. on pp. 3, 6, 8, 19, 22).

[3] O. Balabanov and L. Grigori. "Randomized Gram-Schmidt Process with Application to GMRES". SIAM Journal on Scientific Computing 44.3 (2022), A1450-A1474 (cit. on pp. 3, 4, 6, 8, 20, 22).

[4] O. Balabanov and A. Nouy. "Randomized linear algebra for model reduction. Part I: Galerkin methods and error estimation". Advances in Computational Mathematics 45.5-6 (2019), pp. 2969-3019 (cit. on pp. 3, 4, 5, $6,9)$.

[5] O. Balabanov and A. Nouy. "Randomized linear algebra for model reduction. Part II: minimal residual methods and dictionary-based approximation". Advances in Computational Mathematics 47.2 (2021), pp. 154 (cit. on pp. $3,6,9$ ).

[6] C. Boutsidis and A. Gittens. "Improved matrix algorithms via the subsampled randomized Hadamard transform". SIAM Journal on Matrix Analysis and Applications 34.3 (2013), pp. 1301-1340 (cit. on p. 5).

[7] K. L. Clarkson and D. P. Woodruff. "Low-rank approximation and regression in input sparsity time". Journal of the ACM (JACM) 63.6 (2017), pp. 1-45 (cit. on pp. 3, 5).

[8] K. L. Clarkson and D. P. Woodruff. "Numerical linear algebra in the streaming model". Proceedings of the forty-first annual ACM symposium on Theory of computing. ACM. 2009, pp. 205-214.

[9] J. Demmel, L. Grigori, M. Hoemmen, and J. Langou. "Communication-optimal parallel and sequential QR and LU factorizations". SIAM Journal on Scientific Computing 34.1 (2012), A206-A239 (cit. on p. 2).

[10] P. Drineas, M. Magdon-Ismail, M. W. Mahoney, and D. P. Woodruff. "Fast approximation of matrix coherence and statistical leverage". The Journal of Machine Learning Research 13.1 (2012), pp. 3475-3506 (cit. on pp. 3, $5)$.

[11] J. A. Duersch and M. Gu. "Randomized QR with column pivoting". SIAM Journal on Scientific Computing 39.4 (2017), pp. C263-C291 (cit. on pp. 3, 13).

[12] Y. Fan, Y. Guo, and T. Lin. "A Novel Randomized XR-Based Preconditioned CholeksyQR Algorithm". arXiv preprint arXiv:2111.11148 (2021) (cit. on p. 3).

[13] T. Fukaya, Y. Nakatsukasa, Y. Yanagisawa, and Y. Yamamoto. "CholeskyQR2: a simple and communicationavoiding algorithm for computing a tall-skinny QR factorization on a large-scale parallel system". 2014 5th workshop on latest advances in scalable algorithms for large-scale systems. IEEE. 2014, pp. 31-38 (cit. on p. 1).

[14] T. Fukaya, R. Kannan, Y. Nakatsukasa, Y. Yamamoto, and Y. Yanagisawa. "Shifted Cholesky QR for computing the QR factorization of ill-conditioned matrices". SIAM Journal on Scientific Computing 42.1 (2020), A477-A503 (cit. on pp. 2, 19, 20).

[15] G. H. Golub and C. F. Van Loan. Matrix computations. JHU press, 2013 (cit. on p. 1).

[16] M. Gu and S. C. Eisenstat. "Efficient algorithms for computing a strong rank-revealing QR factorization". SIAM Journal on Scientific Computing 17.4 (1996), pp. 848-869 (cit. on pp. 11, 17, 20).

[17] M. Gu and L. Miranian. "Strong rank revealing Cholesky factorization". Electronic Transactions on Numerical Analysis 17 (2004), pp. 76-92 (cit. on p. 11).

[18] B. Haasdonk and M. Ohlberger. "Reduced basis method for finite volume approximations of parametrized linear evolution equations". ESAIM: Mathematical Modelling and Numerical Analysis 42.2 (2008), pp. 277302 (cit. on p. 1).

[19] N. Halko, P.-G. Martinsson, Y. Shkolnisky, and M. Tygert. "An algorithm for the principal component analysis of large data sets". SIAM Journal on Scientific computing 33.5 (2011), pp. 2580-2594 (cit. on p. 1).

[20] N. J. Higham. Accuracy and stability of numerical algorithms. 2nd ed. SIAM Publications, Philadelphia, PA, USA, 2002 (cit. on pp. 1, 13, 16).

[21] S. J. Leon, Å. Björck, and W. Gander. "Gram-Schmidt orthogonalization: 100 years and more". Numerical Linear Algebra with Applications 20.3 (2013), pp. 492-532 (cit. on p. 1).

[22] P.-G. Martinsson. "Blocked rank-revealing QR factorizations: How randomized sampling can be used to avoid single-vector pivoting" arXiv preprint arXiv:1505.08115 (2015) (cit. on pp. 3, 13).

[23] P.-G. Martinsson and J. A. Tropp. "Randomized numerical linear algebra: Foundations and algorithms". Acta Numerica 29 (2020), pp. 403-572 (cit. on p. 2).

[24] P.-G. Martinsson, G. Quintana OrtÍ, N. Heavner, and R. van de Geijn. "Householder QR factorization with randomization for column pivoting (HQRRP)". SIAM Journal on Scientific Computing 39.2 (2017), pp. C96C115 (cit. on pp. 3, 13).

[25] T. Mary, I. Yamazaki, J. Kurzak, P. Luszczek, S. Tomov, and J. Dongarra. "Performance of random sampling for computing low-rank approximations of a dense matrix on GPUs". Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2015, pp. 1-11 (cit. on pp. 3, 13).

[26] Y. Saad. Iterative methods for sparse linear systems. SIAM, 2003 (cit. on p. 1).

[27] Y. Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011 (cit. on p. 1).

[28] J. A. Tropp. "Improved analysis of the subsampled randomized Hadamard transform". Advances in Adaptive Data Analysis 3.01n02 (2011), pp. 115-126 (cit. on p. 5).

[29] D. P. Woodruff et al. "Sketching as a tool for numerical linear algebra". Foundations and Trends $\_$in Theoretical Computer Science 10.1-2 (2014), pp. 1-157 (cit. on pp. 2, 3, 5).

[30] J. Xiao, M. Gu, and J. Langou. "Fast parallel randomized QR with column pivoting algorithms for reliable low-rank matrix approximations". 2017 IEEE 24th international conference on high performance computing (HiPC). IEEE. 2017, pp. 233-242 (cit. on pp. 3, 13).

[31] Y. Yamamoto, Y. Nakatsukasa, Y. Yanagisawa, and T. Fukaya. "Roundoff error analysis of the CholeskyQR2 algorithm". Electron. Trans. Numer. Anal 44.01 (2015), pp. 306-326 (cit. on p. 2).

[32] I. Yamazaki, S. Tomov, and J. Dongarra. "Mixed-precision Cholesky QR factorization and its case studies on multicore CPU with multiple GPUs". SIAM Journal on Scientific Computing 37.3 (2015), pp. C307-C330 (cit. on p. 3).


[^0]:    *Part of this work was conducted while the author was at Sorbonne Université, Inria, CNRS, Université de Paris, Laboratoire Jacques-Louis Lions, Paris, France. Email: oleg.balabanov@inria.fr.

