# TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction 

Yinda Chen ${ }^{1,2}$, Haoyuan Shi ${ }^{1,2,}$, Xiaoyu Liu ${ }^{1}$, Te Shi ${ }^{2}$, RuoBing Zhang ${ }^{2,3}$, Dong Liu ${ }^{1}$, Zhiwei<br>Xiong ${ }^{1,2}$, and Feng Wu ${ }^{1,2}$<br>${ }^{1}$ University of Science and Technology of China<br>${ }^{2}$ Institute of Artificial Intelligence, Hefei Comprehensive National Science Center<br>${ }^{3}$ Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences<br>${ }^{1}$ cyd0806@mail.ustc.edu.cn<br>${ }^{1}$ zwxiong@ustc.edu.cn


#### Abstract

Autoregressive next-token prediction is a standard pretraining method for largescale language models, but its application to vision tasks is hindered by the nonsequential nature of image data, leading to cumulative errors. Most vision models employ masked autoencoder (MAE) based pretraining, which faces scalability issues. To address these challenges, we introduce TokenUnify, a novel pretraining method that integrates random token prediction, next-token prediction, and next-all token prediction. We provide theoretical evidence demonstrating that TokenUnify mitigates cumulative errors in visual autoregression. Cooperated with TokenUnify, we have assembled a large-scale electron microscopy (EM) image dataset with ultra-high resolution, ideal for creating spatially correlated long sequences. This dataset includes over 120 million annotated voxels, making it the largest neuron segmentation dataset to date and providing a unified benchmark for experimental validation. Leveraging the Mamba network inherently suited for long-sequence modeling on this dataset, TokenUnify not only reduces the computational complexity but also leads to a significant $45 \%$ improvement in segmentation performance on downstream EM neuron segmentation tasks compared to existing methods. Furthermore, TokenUnify demonstrates superior scalability over MAE and traditional autoregressive methods, effectively bridging the gap between pretraining strategies for language and vision models. Code is available at https://github.com/ydchen0806/TokenUnify


## 1 Introduction

Large language models (LLMs) can be scaled up to trillions of parameters through pretraining [1, 65, 66], primarily owing to high-quality data and effective autoregressive models. These models benefit from strong scaling laws because the structured and sequential nature of text data allows unification into a single next-token prediction task. However, when extending to multimodal tasks such as Unified IO [54] and Qwen VL [6], these models often fail to achieve state-of-the-art performance on fine-grained image tasks. This is due to an over-reliance on the capabilities of LLMs and a lack of robust pretraining strategies for visual data, resulting in weak visual representation capabilities.

Unlike language, the complexity of visual signals has led to diverse approaches in visual pretraining. Contrastive learning methods like DINO v2 [56] excel in fine-grained representation, while masked[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_a5619f81b5b24bf6896eg-02.jpg?height=333&width=1271&top_left_y=232&top_left_x=424)

Figure 1: TokenUnify prediction paradigms divide the 3D EM image into non-overlapping patches, which are tokenized into a sequence. Three tasks are performed for rich 3D image representations: (a) random token prediction, (b) next token prediction, and (c) next-all token prediction.

reconstruction methods such as MAE [32, 16] offer good scalability and zero-shot classification abilities. However, these methods exhibit poor scaling laws, where increasing model size does not yield expected performance gains [61] (see Section A in the appendix). To achieve scaling laws similar to language models, approaches like AIM [27] and LVM [7] have introduced autoregressive tasks into the visual domain, showing promising scaling properties. However, image sequence disorder and error accumulation in autoregressive tasks often degrade performance in smaller models [4]. Additionally, the computational burden of long-sequence images makes researching image autoregressive tasks particularly challenging. We summarize the challenges of visual autoregressive tasks as follows: 1) How to reduce error accumulation in visual autoregression to achieve stronger scaling laws? 2) How to develop more efficient computational methods to handle massive image data? 3) How to construct spatially correlated long-sequence relationships in images?

This paper aims to tackle the above three critical challenges. 1) To tackle the issue of cumulative errors in visual autoregression, we propose TokenUnify, a novel mixed token prediction paradigm. TokenUnify integrates next-token prediction, next-all token prediction, and random token prediction (as illustrated in Fig. 1), leveraging global information to overcome the limitations of local receptive fields. We theoretically demonstrate that this mixed approach reduces cumulative errors while maintaining favorable scaling laws. 2) To alleviate computational burdens, we introduce the Mamba architecture, which reduces the computational complexity of autoregressive tasks from quadratic (as in Transformers) to linear. Detailed comparisons of the scaling properties between Mamba and Transformer architectures reveal that Mamba achieves superior performance and efficiency in largescale autoregressive visual models. 3) To construct spatially correlated long-sequence relationships, we have collected large-scale, ultra-high-resolution 3D electron microscopy (EM) images of mouse brain slices. The ultra-high resolution allows for thousands of continuous image tokens, ensuring robust spatial continuity. We have fully annotated six different functional regions within the mouse brain, totaling 120 million pixels, resulting in the largest manually annotated neuron dataset to date. This comprehensive dataset also serves as a unified benchmark for evaluating experimental performance ${ }^{2}$.

Pretraining with TokenUnify led to a $\mathbf{4 5 \%}$ improvement in performance on subsequent EM neuron segmentation tasks. The mixed training paradigm of TokenUnify outperformed MAE [32] by $\mathbf{2 1 \%}$ in pretraining performance, even with fewer parameters. Furthermore, TokenUnify demonstrated superior scaling properties of autoregressive models, offering a promising approach for pretraining large-scale visual models.

Our contributions can be summarized as follows:

1. We introduce a novel pre-training paradigm, TokenUnify, which models visual pre-training tasks from multiple perspectives at the token level. This ensures sublinear growth of the scaling law while demonstrating superior fine-grained feature extraction capabilities compared to MAE in smaller model pre-training. We also provide a theoretical explanation for this phenomenon.
2. We achieve a $\mathbf{4 5 \%}$ performance improvement on the neuron segmentation task and, for the first time, validate the Mamba model with billion-level parameters on visual tasks, demonstrating the effectiveness and efficiency of TokenUnify in long-sequence visual autoregression.
3. We provide a large-scale biological image dataset with 120 million annotated pixels, offering a long-sequence image dataset to validate the potential of autoregressive methods.[^1]

## 2 Related work

Recent advances in large language models (LLMs) have unified various NLP tasks under a single architecture, formulating them as generation tasks. This architecture can be categorized into BERTlike [25, 70, 39, 62] and GPT-like [10, 58, 46, 77] models. The latter, decoder-only autoregressive structure, has been shown to be more effective, as validated by products like ChatGPT. Subsequent works have built upon GPT, introducing techniques like RMSNorm [74], SwiGLU, and RoPE to ensure efficient and stable training. The LLaMA series [65, 66] has improved training efficiency, while the Qwen series [6, 5, 71] has focused on data cleaning and filtering for Chinese language models. Currently, LLMs have surpassed human-level performance in many text processing tasks [1].

In multi-modal tasks, the CLIP [57] and BLIP series [43, 42, 23] have pioneered contrastive learning on image-text pairs, achieving remarkable zero-shot classification and generalization capabilities. Further works [76, 17, 68, 78] have applied multi-modal models to specific domains. By processing arbitrary modalities into a unified token format [54, 67, 13], these models can generate outputs in any modality. However, there is still room for improvement in fine-grained visual tasks, and training large vision models remains an open problem.

Self-supervised pre-training has been a cornerstone for enhancing model representation capabilities. Approaches based on contrastive learning for representation extraction [15, 73, 33, 41] and masked reconstruction methods [32, 16, 47, 26, 20, 22] have shown promise. However, current vision models have not exhibited the same sublinear scaling laws as language models. To address this issue, some tasks have adopted autoregressive pre-training paradigms similar to those used in language models [14, 7, 27], though the training costs remain a significant concern. In this paper, we explore the potential of long visual token autoregressive pre-training and introduce a collaborative training scheme for long token prediction. Our approach aims to balance the scaling laws and training costs, demonstrating improvements in fine-grained visual tasks.

## 3 Method

### 3.1 Overview

Our theoretical contributions include proving the parameter independence of MAE performance (see Section A), establishing the strong correlation between autoregressive model performance and parameter count (see Section $\sqrt{B}$ ), and demonstrating the advantages of next-all token prediction from both intuitive (see Section C.1) and theoretical perspectives (see Section C.2).

Our experimental framework comprises two stages: pre-training and fine-tuning. During the pretraining stage, we leverage only the unlabeled raw data $\mathcal{X}$ to learn a generic visual representation $f_{\theta_{1}}(\cdot)$, parameterized by $\theta_{1}$. We employ a mixed token prediction strategy to capture both local and global dependencies in the data (see Section 3.2). Additionally, we utilize Mamba for efficient modeling of long sequences in autoregressive tasks, enhancing computational efficiency (see Section 3.3).

In the fine-tuning stage, we use both the raw data $\mathcal{X}$ and the corresponding labels $\mathcal{Y}$ to adapt the pre-trained representation to specific downstream tasks. Let $g_{\theta_{2}}(\cdot)$ be the task-specific model, parameterized by $\theta_{2}$. We initialize $\theta_{2}$ with the pre-trained weights $\theta_{1}$ and optimize the task-specific objective. Further details are provided in Sections 3.4 and F. 3 .

To illustrate the application of our framework, consider the modeling of EM images. Given a total of $T$ EM images $\mathcal{X}=\mathbf{X}^{(1)}, \ldots, \mathbf{X}^{(T)}$, where each $\mathbf{X}^{(t)} \in \mathbb{R}^{D \times H \times W}$ represents a 3D image with depth $D$, height $H$, and width $W$, we aim to learn a meaningful representation of this high-dimensional, long-sequence visual data by leveraging its inherent spatial structure and continuity. To achieve this, we partition each large 3D image $\mathbf{X}$ into smaller patches $\mathbf{x} \in \mathbb{R}^{D^{\prime} \times H^{\prime} \times W^{\prime}}$.

### 3.2 Mixed-mode autoregressive modeling

We theoretically demonstrate the effectiveness of MAE [32] on smaller models, the scaling advantages of next token prediction, and the ability of next-all token prediction to mitigate cumulative errors in autoregressive models. Based on these insights, we propose a hybrid training paradigm that aims to combine the strengths of these three methods, as shown in Fig. 2

![](https://cdn.mathpix.com/cropped/2024_06_04_a5619f81b5b24bf6896eg-04.jpg?height=239&width=1395&top_left_y=233&top_left_x=365)

Figure 2: illustrates the main pretraining workflow of TokenUnify. The image $\mathbf{X}$ is fed into the Tokenizer, transforming it into a long sequence of tokens $\left.\mathbf{x}_{i}\right|_{i=1} ^{K}$. The predictions for the random token, next token, and next-all token are performed sequentially. The Perceiver Resampler is employed to convert varying-size large feature maps into a few visual tokens (see Section 3.2).

Given an image $\mathbf{X}$, we first divide it into a sequence of $K$ non-overlapping patches $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{K}\right\}$. Standard autoregressive modeling typically adopts a fixed left-to-right factorization:

$$
\begin{equation*}
p(\mathbf{x})=\prod_{i=1}^{K} p\left(\mathbf{x}_{i} \mid \mathbf{x}_{<i}\right) \tag{1}
\end{equation*}
$$

where $\mathbf{x}_{<i}$ denotes all patches preceding $\mathbf{x}_{i}$.

We introduce TokenUnify, a mixed-mode autoregressive modeling approach designed to enhance existing autoregressive image modeling techniques [14, 7]. TokenUnify combines three distinct prediction tasks: random token prediction, next token prediction, and next-all token prediction, instead of using the fixed factorization in Eq. 1 .

Random Token Prediction. Given the full patch sequence $\mathbf{x}_{1: K}$, we randomly mask out a subset of patches $\mathcal{M} \subset\{1, \ldots, K\}$ and train the model to predict the masked patches conditioned on the remaining context:

$$
\begin{equation*}
\mathcal{L}_{\text {random }}=-\sum_{i \in \mathcal{M}} \log p\left(\mathbf{x}_{i} \mid \mathbf{x}_{\overline{\mathcal{M}}}\right) \tag{2}
\end{equation*}
$$

where $\mathbf{x}_{\overline{\mathcal{M}}}=\left\{\mathbf{x}_{i} \mid i \notin \mathcal{M}\right\}$ denotes the unmasked patches.

Next Token Prediction. We integrate the standard next token prediction loss into our task. In this setup, we use the Perceiver Resampler [2] (see Section F.2] to convert the variable-sized feature maps generated by the Vision Encoder into a fixed number of visual tokens. This loss trains the model to predict the next patch $\mathbf{x} i$ given the preceding context $\mathbf{x}_{<i}$ :

$$
\begin{equation*}
\mathcal{L}_{\text {next }}=-\sum_{i=1}^{K} \log p\left(\mathbf{x}_{i} \mid \mathbf{x}_{<i}\right) \tag{3}
\end{equation*}
$$

Next-All Token Prediction. To encourage the model to capture longer-range dependencies, we extend the next token prediction to a next-all token prediction task. For each patch $\mathbf{x}_{i}$, the model is trained to predict not only $\mathbf{x}_{i}$ but also all the subsequent patches $\mathbf{x}_{i: K}$ in the sequence:

$$
\begin{equation*}
\mathcal{L}_{\text {next-all }}=-\sum_{i=1}^{K} \sum_{j=i}^{K} \log p\left(\mathbf{x}_{j} \mid \mathbf{x}_{<i}\right) \tag{4}
\end{equation*}
$$

By alternating between these token prediction tasks every 100 epochs, we prevent the model from converging to a trivial solution and encourage it to learn meaningful representations from the input data. This alternating training strategy enables the model to capture both local and global dependencies within the patch sequence, thereby enhancing performance on downstream tasks. The workflow of TokenUnify is illustrated in Fig. 2

### 3.3 Mamba ordering

While the aforementioned mix token prediction task improves sequence autoregressive modeling capabilities, it also introduces additional computational complexity for long sequences. Inspired by the Mamba strategy proposed by [30], we introduce an enhanced approach to effectively model long sequences in volumetric EM images. Traditional sequence modeling methods often struggle with capturing long-range dependencies due to their rigid sequential nature. Our enhanced Mamba ordering strategy addresses this by incorporating a more sophisticated and flexible sequence modeling approach.

The fundamental idea behind Mamba ordering is to dynamically prioritize regions of the sequence based on contextual significance rather than adhering to a fixed order. This is achieved through an adaptive mechanism that evaluates the importance of different patches within the sequence and adjusts the processing order accordingly. By doing so, Mamba ordering enhances the model's ability to capture intricate patterns and long-range dependencies more effectively.

Mathematically, let $\mathbf{x}=\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{K}\right\}$ represent the sequence of patches. Instead of processing these patches in a fixed order, we define a dynamic ordering function $\sigma:\{1,2, \ldots, K\} \rightarrow$ $\{1,2, \ldots, K\}$ that determines the sequence in which patches are processed. The Mamba ordering objective can be formulated as:

$$
\begin{equation*}
\mathcal{L}_{\text {mamba }}=-\mathbb{E}_{\mathbf{x} \sim p_{\text {data }}}\left[\sum_{i=1}^{K} \log p\left(\mathbf{x}_{\sigma(i)} \mid \mathbf{x}_{\sigma(<i)}\right)\right] \tag{5}
\end{equation*}
$$

where $\mathbf{x}_{\sigma(<i)}$ represents the context preceding the $i$-th patch in the dynamically determined order.

To optimize this objective, we introduce a context-aware attention mechanism that assesses the relevance of each patch with respect to the overall sequence. This mechanism outputs a relevance score for each patch, guiding the dynamic ordering function $\sigma$ to prioritize patches that are most informative for subsequent predictions. By iteratively updating the relevance scores and reordering the patches, Mamba ordering ensures that the model focuses on the most crucial aspects of the sequence at each step.

Consider the state-space model representation where the hidden state $\mathbf{h}(t)$ evolves dynamically based on the input $\mathbf{x}(t)$. The state-space equations are given by:

$$
\begin{equation*}
\mathbf{h}^{\prime}(t)=\mathbf{A}(t) \mathbf{h}(t)+\mathbf{B}(t) \mathbf{x}(t), \quad \mathbf{y}(t)=\mathbf{C}(t) \mathbf{h}(t) \tag{6}
\end{equation*}
$$

where $\mathbf{A}(t), \mathbf{B}(t)$, and $\mathbf{C}(t)$ are time-dependent matrices. Specifically, $\mathbf{B}(t)$ and $\mathbf{C}(t)$ are parameterized by the input $\mathbf{x}(t)$ as follows:

$$
\begin{equation*}
\mathbf{B}(t)=s_{B}(\mathbf{x}(t)), \quad \mathbf{C}(t)=s_{C}(\mathbf{x}(t)), \quad \Delta(t)=\tau_{\Delta}(\text { Linear }(\mathbf{x}(t))) \tag{7}
\end{equation*}
$$

where $s_{B}, s_{C}$, and $\tau_{\Delta}$ are functions that map the input to the respective parameters.

The benefits of our enhanced Mamba ordering are twofold. First, it mitigates error accumulation by allowing the model to refine its predictions based on a globally coherent understanding of the sequence. Second, it enhances the model's capacity to capture long-range dependencies, as the dynamic ordering can adapt to the inherent structure and complexity of the data.

Empirical results demonstrate that our enhanced Mamba ordering significantly improves the performance of sequence modeling tasks in volumetric EM images, particularly for long sequences. By enabling a more adaptive and context-aware approach to sequence processing, our enhanced Mamba ordering represents a substantial advancement over traditional methods, offering a robust and scalable solution for high-dimensional visual data.

### 3.4 Finetuning and Segmentation

The segmentation network, denoted as $g_{\text {seg }}(\cdot)$, consists of an encoder $g_{e}(\cdot)$ and a decoder $g_{d}(\cdot)$ :

$$
\begin{equation*}
g_{s e g}\left(x ; \theta_{s}\right)=g_{d}\left(g_{e}(x)\right), \quad \theta_{s}=\left\{\theta_{e}, \theta_{d}\right\} \tag{8}
\end{equation*}
$$

where $\theta_{s}$ represents the parameters of the entire segmentation network, and $\theta_{e}$ and $\theta_{d}$ are the parameters of the encoder and decoder, respectively.

The encoder $g_{e}(\cdot)$ gradually downsamples the input volume and extracts high-level semantic features, while the decoder $g_{d}(\cdot)$ upsamples the encoded features back to the original resolution. Meanwhile, the output of each downsampling layer in the encoder is connected to the corresponding layer in the decoder via skip connections to fuse local and global multi-scale information. We adopt 3D ResUNet/ViT/Mamba as the backbone network, respectively.

The output of the segmentation network $\hat{y}=g_{\text {seg }}(x) \in \mathbb{R}^{C \times D \times H \times W}$ represents the predicted affinity map [45, 44], corresponding to the connectivity probability of each voxel in three directions. During training, the loss function for labeled samples is the mean squared error between the predicted and ground-truth affinity maps:

$$
\begin{equation*}
L_{s e g}=\frac{1}{\left|D_{l}\right|} \sum_{i=1}^{\left|D_{l}\right|}\left|\hat{y}_{i}-y_{i}\right|^{2}=\frac{1}{\left|D_{l}\right|} \sum_{i=1}^{\left|D_{l}\right|}\left|g_{s e g}\left(x_{i}^{l}\right)-y_{i}\right|^{2} \tag{9}
\end{equation*}
$$

During inference, for any new test sample $x_{t}$, forward propagation through $g_{\text {seg }}\left(x_{t}\right)$ yields its predicted affinity map. This predicted affinity map is then post-processed using a seeded watershed algorithm and a structure-aware region agglomeration algorithm [29, 8] to obtain the final neuron instance segmentation. Detailed information on our segmentation process can be found in Section F. 3 and the segmentation pipeline is illustrated in Fig. 7.

## 4 Dataset and Metrics

Dataset. For the pretraining phase of TokenUnify, we collect a vast amount of publicly available unlabeled EM imaging data, from four large-scale electron microscopy (EM) datasets: Full Adult Fly Brain (FAFB) [59], MitoEM [69], FIB-25 [64], and Kasthuri15 [36]. These datasets cover a diverse range of organisms, including Drosophila, mouse, rat, and human samples, totaling over 1 TB. The details of the pretraining datasets can be found in Table 3. We sample from the datasets with equal probability and ensure that each brain region has an equal chance of being sampled, guaranteeing the diversity of the pretraining data.

For downstream fine-tuning and segmentation, we employ two datasets: a smaller dataset, $\mathrm{AC} 3 / 4$, and a larger dataset, MEC, for algorithm validation. The AC3/4 dataset [36] consists of mouse somatosensory cortex datasets with 256 and 100 successive EM images ( $1024 \times 1024)$. We use the first 80 images of AC4 for fine-tuning, the last 20 images of AC4 for testing, and the first 100 images of AC3 for testing. Additionally, we have collected a large-scale electron microscopy dataset, MEC, by imaging the mouse somatosensory cortex, mouse medial entorhinal cortex, and mouse cerebral cortex, achieving a physical resolution of $8 \mathrm{~nm} \times 8 \mathrm{~nm} \times 40 \mathrm{~nm}$. We select 6 representative volumes from different neural regions, named wafer4/25/26/26-2/36/36-2, with each volume size reaching $125 \times 1250 \times 1250$ voxels. We perform dense annotation on these selected wafer regions, with a total of over 1.2 billion annotated voxels. To validate the algorithm's performance on a large-scale dataset, we use wafer25/26/26-2/36 for training, wafer4 for validation, and wafer36-2 for testing on the MEC dataset.

Metrics. To evaluate the performance of neuron segmentation [75, 24], we employ two widely-used metrics: Variation of Information (VOI) [55] and Adjusted Rand Index (ARAND) [3]. These metrics quantify the agreement between the predicted segmentation and the ground truth from different perspectives. Detailed descriptions of these metrics can be found in Section D. 2 .

## 5 Experiment

Implementation Details. In this work, we employ consistent training settings for both pretraining and fine-tuning tasks. The network architecture remains the same throughout the training and finetuning phases. During fine-tuning, we optimize the network using the AdamW optimizer [53] with $\beta_{1}=0.9, \beta_{2}=0.999$, a learning rate of $1 \mathrm{e}-6$, and a batch size of 20 on an NVIDIA GTX 3090 (24GB) GPU. For pretraining, we use a batch size of 8 on an NVIDIA Tesla A40 (48G) GPU due to memory constraints.

We perform distributed training using 8 NVIDIA GTX 3090 GPUs for each segmentation task, running for a total of 1200 epochs. Similarly, we utilize 32 NVIDIA Tesla A40 GPUs for each pretraining task, running for 400 epochs. During the pretraining phase, the input consists solely of unlabeled data, whereas in the segmentation phase, both labeled and unlabeled data are used as input. The input block resolution for the network is set to $16 \times 160 \times 160$. To initialize the network for fine-tuning, we load the weights obtained from the pretraining phase, following the settings of previous works [32].

To generate final segmentation results from the predicted affinities, we employ two representative post-processing methods: Waterz [29] and LMC [8]. Waterz iteratively merges fragments based on edge scores until a threshold is reached. We set the quantile to $50 \%$ and the threshold to 0.5 based on testing on MEC, and discretize scores into 256 bins. LMC formulates agglomeration as a minimum-cost multi-cut problem, extracting edge features as costs and solving with the Kernighan-

Table 1: Quantitative comparison of segmentation results on Wafer4 and Wafer36-2 datasets. 'Post.' represents the post-processing algorithms. * denotes the MAE pretraining strategy [32]. † indicates our TokenUnify pretraining strategy. The best results are in bold and the second best results are in underlined.

![](https://cdn.mathpix.com/cropped/2024_06_04_a5619f81b5b24bf6896eg-07.jpg?height=1219&width=1350&top_left_y=434&top_left_x=385)

Table 2: Quantitative comparison of segmentation results on $\mathrm{AC} 3 / 4$ datasets. 'w/o Pre.' indicates models without pretraining, whereas 'w Pre.' denotes models that utilize corresponding pretraining strategy. * denotes the MAE pretraining strategy [32]. † indicates our TokenUnify pretraining strategy. The best results are in bold and the second best results are in underlined.

| Method | $\mathrm{VOI}_{1}$ | $M \downarrow$ | $\mathrm{VOI}$ | $I_{S} \downarrow$ | $\mathrm{VO}$ |  | $A R A$ | $N D$ | Param. <br> $(\mathrm{M})$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | w/o Pre. |  | o Pre. | w Pre. | w/o Pre. | w Pre. | /o F | w Pre. |  |
| Superhum | 0.4882 | $0.6162 \mid$ | 0.6563 | 0.6308 | 1.1445 | 1.2470 | 0.1748 | 0.2505 | 1.478 |
| MALA [29] | 0.4571 | $0.3345 \mid$ | 0.6767 | $0.7479 \mid$ | 1.1338 | 1.0824 | 0.1664 | $\mathbf{0 . 1 0 2 0}$ | 84.02 |
| PEA [35] | 0.5522 | $0.3832 \mid$ | 0.4980 | $0.6153 \mid$ | 1.0502 | $\underline{0.9985}$ | 0.2093 | 0.1127 | 1.480 |
| UNETH | 0.7799 | 0.5339 | 0.7399 | 0 . | 8 | 912 | 0.2411 | 796 | 129.1 |
| EMmamba* | 0.9378 | 0.3167 | 0.8629 | 0.7963 | 1.8007 | 1.1131 | 0.2840 | $\underline{0.1050}$ | 28.30 |
| EMmamba $\dagger$ | $\mid 0.9378$ | 0.4479 | 0.8629 | 0.5439 | 1.8007 | 0.9918 | 0.2840 | 0.1366 | $\mid 28.30$ |

Lin solver [37]. We maintain consistent post-processing settings across all experiments to ensure fair comparisons and conclusions about our method.
![](https://cdn.mathpix.com/cropped/2024_06_04_a5619f81b5b24bf6896eg-08.jpg?height=648&width=1390&top_left_y=237&top_left_x=366)

Figure 3: shows the visualization results for two slices from the MEC dataset: wafer 4 and wafer 36-2. The left side displays the EM raw images, while the right side presents the affinity and segmentation results. Red boxes indicate over-split regions, and orange boxes highlight over-merge regions.

Experimental Results on MEC Dataset. As detailed in Section 4, we leverage a substantial dataset called MEC to assess the performance of our algorithm. For neuron segmentation tasks, we have implemented several representative methods, including Superhuman [40], MALA [29], PEA [35], and UNETR [31]. Our EMmamba model (see Section F.3) builds upon Segmamba [72] and incorporates enhancements to various anisotropic structures to better accommodate the resolution of electron microscopy. All networks are trained using default open-source parameters. Additionally, we calculated the parameter count for all architectures.

Our experimental results are presented in Table 1. The upper part of the table shows the performance of these methods when directly applied to segmentation tasks. In contrast, the lower part of the table illustrates the performance of networks employing self-supervised pretraining. When comparing models with a similar number of parameters, our pretraining approach achieves approximately a $21 \%$ performance improvement over MAE and over a $45 \%$ improvement compared to direct training. Visualization results, as shown in Fig. 3, demonstrate that our approach significantly outperforms others in both neuron splitting and merging tasks.

Experimental Results on AC3/4 Datasets. As noted in Section 4 we also evaluate the performance of all baseline methods on a smaller dataset. Compared to the MEC dataset, the total training scale (number of labeled pixels) of the $\mathrm{AC} 3 / 4$ dataset is only about $1 / 10$ of that of MEC. In this low-data scenario, the Mamba architecture combined with TokenUnify pretraining achieves performance comparable to the latest SOTA PEA pertaining (as shown in Table 2). Additionally, it demonstrates approximately a $10 \%$ performance improvement over the MAE pretraining approach. This highlights the robustness of TokenUnify even with a limited number of fine-tuning samples.

Experimental Results on the Scaling Law. We conducted a comprehensive evaluation of scaling laws for various initialization and training strategies: Random Initialization, MAE (Masked Autoencoder), Autoregressive, and our proposed TokenUnify method. By increasing the feature dimension and network depth, we scaled the model parameters to $100 \mathrm{M}, 200 \mathrm{M}, 500 \mathrm{M}$, and 1B (detailed network structures are provided in Section F. 3 and Table 5.

In our experiments, we tested input sizes of $16 \times 160 \times 160$. The Mamba architecture was trained on the MEC dataset, while the Transformer architecture was trained on the CREMI dataset [28]. Our experimental results are shown in Fig. 4

Our findings indicate that, following pretraining on the same data, all methods except for the purely vision-based Autoregressive model with small parameter counts demonstrate performance gains. However, MAE quickly encounters scaling law limitations, hitting a performance bottleneck. In contrast, TokenUnify exhibits robust scaling properties, outperforming other pretraining methods
![](https://cdn.mathpix.com/cropped/2024_06_04_a5619f81b5b24bf6896eg-09.jpg?height=618&width=1432&top_left_y=220&top_left_x=346)

Figure 4: We evaluate the performance of models with $100 \mathrm{M}, 200 \mathrm{M}, 500 \mathrm{M}$, and 1B parameters. Each model was trained for 100 epochs on the MEC and CREMI datasets.

at both small and large parameter scales. From a model architecture perspective, Mamba maintains segmentation performance while exhibiting a lower parameter count compared to the Transformer architecture. This validates the suitability of Mamba for long-sequence and autoregressive modeling tasks.


#### Abstract

Abalation Study. We conducted ablation studies on several components within our experimental setup. The experiments were divided into two main parts. First, we explored the mixed mechanisms of TokenUnify. We experimented with combinations of three different mixing mechanisms, ensuring that the total number of training epochs remained consistent. Table 6 presents the results of these experiments on the wafer4 neuron segmentation task (using a $28 \mathrm{M}$ EMmamba segmentation network). The results demonstrate that mixed training provides the most significant benefit for downstream tasks, with the combination of Random token and Next token being the next most effective.

Second, we performed ablation studies on the fine-tuning schemes. Under the default settings, we fine-tuned all parameters of the network. However, due to computational resource constraints, only a subset of parameters (or additional adapter parameters such as LoRA [34]) is often fine-tuned for larger models. Based on our network architecture (see Fig. 7), we divided the network into the Mamba part (for token sequence information extraction), the encoder part (for downsampling), and the decoder part (for convolutional upsampling). We fine-tuned only the corresponding subset of weights for each part. Our experimental results are shown in Table 7

We found that in the TokenUnify modeling, using the sequence information priors extracted by Mamba significantly benefits downstream segmentation tasks. Combining the Mamba module with the encoder part yields even greater performance improvements, while fine-tuning only the encoder or decoder weights provides minimal gains. This suggests that in resource-constrained scenarios, fine-tuning only the sequence modeling parameters can be sufficient.


# 6 Social Impact and Future Work 

The favorable scaling laws of TokenUnify present the opportunity to train a unified and generic visual feature extractor, which holds significant importance for visual tasks. A unified visual feature extractor can substantially reduce the cost of fine-tuning models for different visual tasks, thereby facilitating the application of visual technologies across various domains. We have currently validated the effectiveness of TokenUnify on long-sequence 3D medical images. Moving forward, we plan to further explore the performance of TokenUnify on natural images and other downstream tasks, thereby expanding its scope of application. Moreover, TokenUnify can be extended to multimodal domains such as image-text tasks [18, 49], demonstrating its utility in multimodal applications. We will also continue to investigate model lightweighting [21, 19] and efficient fine-tuning strategies [52, 48]. We believe that TokenUnify offers a promising approach for building large-scale, efficient visual pre-training models, contributing to advancements in the visual domain.

## 7 Conclusion

We propose TokenUnify, a novel autoregressive visual pre-training method that integrates random token prediction, next-token prediction, and next-all token prediction to effectively capture local and global dependencies in image data. We provide theoretical evidence demonstrating that TokenUnify mitigates cumulative errors in visual autoregression while maintaining favourable scaling laws. Furthermore, we collect a large-scale, ultra-high-resolution 3D electron microscopy dataset of mouse brain slices to serve as a unified benchmark for validating our approach. Pretraining with TokenUnify leads to a $45 \%$ improvement in performance on downstream neuron segmentation tasks compared to the baseline, showcasing the potential of our method in fine-grained visual tasks.

## References

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In NeurIPS, volume 35, pages 23716-23736, 2022.

[3] Ignacio Arganda-Carreras, Srinivas C Turaga, Daniel R Berger, Dan Cireşan, Alessandro Giusti, Luca M Gambardella, Jürgen Schmidhuber, Dmitry Laptev, Sarvesh Dwivedi, Joachim M Buhmann, et al. Crowdsourcing the creation of image segmentation algorithms for connectomics. Frontiers in neuroanatomy, $9: 152591,2015$.

[4] Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction. arXiv preprint arXiv:2403.06963, 2024.

[5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.

[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023.

[7] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. arXiv preprint arXiv:2312.00785, 2023.

[8] Thorsten Beier, Constantin Pape, Nasim Rahaman, Timo Prange, Stuart Berg, Davi D Bock, Albert Cardona, Graham W Knott, Stephen M Plaza, Louis K Scheffer, et al. Multicut brings automated neurite segmentation closer to human performance. Nature methods, 14(2):101-102, 2017.

[9] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In NeurIPS, volume 28, 2015.

[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, volume 33, pages 1877-1901, 2020.

[11] Jiuhai Chen and Jonas Mueller. Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment. arXiv preprint arXiv:2308.16175, 2023.

[12] Jiuhai Chen and Jonas Mueller. Automated data curation for robust language model fine-tuning. arXiv preprint arXiv:2403.12776, 2024.

[13] Junbo Chen, Xupeng Chen, Ran Wang, Chenqian Le, Amirhossein Khalilian-Gourtani, Erika Jensen, Patricia Dugan, Werner Doyle, Orrin Devinsky, Daniel Friedman, et al. Subject-agnostic transformer-based neural speech decoding from surface and depth electrode signals. bioRxiv, pages 2024-03, 2024.

[14] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, pages 1691-1703. PMLR, 2020.

[15] Yinda Chen, Wei Huang, Xiaoyu Liu, Shiyu Deng, Qi Chen, and Zhiwei Xiong. Learning multiscale consistency for self-supervised electron microscopy instance segmentation. In ICASSP, pages 1566-1570. IEEE, 2024.

[16] Yinda Chen, Wei Huang, Shenglong Zhou, Qi Chen, and Zhiwei Xiong. Self-supervised neuron segmentation with multi-agent reinforcement learning. In IJCAI, pages 609-617, 2023.

[17] Yinda Chen, Che Liu, Wei Huang, Sibo Cheng, Rossella Arcucci, and Zhiwei Xiong. Generative text-guided 3d vision-language pretraining for unified medical image segmentation. arXiv preprint arXiv:2306.04811, 2023.

[18] Yinda Chen, Che Liu, Xiaoyu Liu, Rossella Arcucci, and Zhiwei Xiong. Bimcv-r: A landmark dataset for 3d ct text-image retrieval. arXiv preprint arXiv:2403.15992, 2024.

[19] Z Chen and L Jing. Multimodal semi-supervised learning for 3d objects. In The British Machine Vision Conference (BMVC), 2021.

[20] Zhimin Chen, Longlong Jing, Yingwei Li, and Bing Li. Bridging the domain gap: Self-supervised 3d scene understanding with foundation models. Advances in Neural Information Processing Systems, 36, 2024.

[21] Zhimin Chen, Longlong Jing, Liang Yang, Yingwei Li, and Bing Li. Class-level confidence based 3d semi-supervised learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 633-642, 2023.

[22] Zhimin Chen, Yingwei Li, Longlong Jing, Liang Yang, and Bing Li. Point cloud self-supervised learning via 3d to multi-view masked autoencoder. arXiv preprint arXiv:2311.10887, 2023.

[23] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In NeurIPS, volume 36, 2024.

[24] Bo Dang, Wenchao Zhao, Yufeng Li, Danqing Ma, Qixuan Yu, and Elly Yijun Zhu. Real-time pill identification for the visually impaired using deep learning. arXiv preprint arXiv:2405.05983, 2024.

[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[26] Zhiyuan Ding, Qi Dong, Haote Xu, Chenxin Li, Xinghao Ding, and Yue Huang. Unsupervised anomaly segmentation for brain lesions using dual semantic-manifold reconstruction. In ICONIP, 2022.

[27] Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua M Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models. arXiv preprint arXiv:2401.08541, 2024.

[28] J Funke, S Saalfeld, DD Bock, SC Turaga, and E Perlman. Miccai challenge on circuit reconstruction from electron microscopy images. In MICCAI, 2016.

[29] Jan Funke, Fabian Tschopp, William Grisaitis, Arlo Sheridan, Chandan Singh, Stephan Saalfeld, and Srinivas C Turaga. Large scale image segmentation with structured loss based deep learning for connectome reconstruction. IEEE transactions on pattern analysis and machine intelligence, 41(7):1669-1680, 2018.

[30] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.

[31] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger R Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In WACV, pages 574-584, 2022.

[32] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 16000-16009, 2022.

[33] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pages 9729-9738, 2020.

[34] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022.

[35] Wei Huang, Shiyu Deng, Chang Chen, Xueyang Fu, and Zhiwei Xiong. Learning to model pixel-embedded affinity for homogeneous instance segmentation. In AAAI, volume 36, pages 1007-1015, 2022.

[36] Narayanan Kasthuri, Kenneth Jeffrey Hayworth, Daniel Raimund Berger, Richard Lee Schalek, José Angel Conchello, Seymour Knowles-Barley, Dongil Lee, Amelio Vázquez-Reina, Verena Kaynig, Thouis Raymond Jones, et al. Saturated reconstruction of a volume of neocortex. Cell, 162(3):648-661, 2015.

[37] Brian W Kernighan and Shen Lin. An efficient heuristic procedure for partitioning graphs. The Bell system technical journal, 49(2):291-307, 1970.

[38] E. Kodak. Kodak lossless true color image suite (photocd pcd0992), 1993. Version 5.

[39] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240, 2020.

[40] Kisuk Lee, Jonathan Zung, Peter Li, Viren Jain, and H Sebastian Seung. Superhuman accuracy on the snemi3d connectomics challenge. arXiv preprint arXiv:1706.00120, 2017.

[41] Chenxin Li, Yunlong Zhang, Zhehan Liang, Wenao Ma, Yue Huang, and Xinghao Ding. Consistent posterior distributions under vessel-mixing: a regularization for cross-domain retinal artery/vein classification. In 2021 IEEE International Conference on Image Processing (ICIP), pages 61-65. IEEE, 2021.

[42] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pages 19730-19742. PMLR, 2023.

[43] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, pages 12888-12900. PMLR, 2022.

[44] Panfeng Li, Mohamed Abouelenien, and Rada Mihalcea. Deception detection from linguistic and physiological data streams using bimodal convolutional neural networks. arXiv preprint arXiv:2311.10944, 2023.

[45] Panfeng Li, Youzuo Lin, and Emily Schultz-Fellenz. Contextual hourglass network for semantic segmentation of high resolution aerial imagery. arXiv preprint arXiv:1810.12813, 2018.

[46] Panfeng Li, Qikai Yang, Xieming Geng, Wenjing Zhou, Zhicheng Ding, and Yi Nian. Exploring diverse methods in visual question answering. arXiv preprint arXiv:2404.13565, 2024.

[47] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In $C V P R$, pages 2142-2152, 2023.

[48] Yufeng Li, Weimin Wang, Xu Yan, Min Gao, and MingXuan Xiao. Research on the application of semantic network in disease diagnosis prompts based on medical corpus. International Journal of Innovative Research in Computer Science \& Technology, 12(2):1-9, 2024.

[49] Che Liu, Cheng Ouyang, Yinda Chen, Cesar César Quilodrán-Casas, Lei Ma, Jie Fu, Yike Guo, Anand Shah, Wenjia Bai, and Rossella Arcucci. T3d: Towards 3d medical image understanding through vision-language pre-training. arXiv preprint arXiv:2312.01529, 2023.

[50] Jiaxiang Liu, Jin Hao, Hangzheng Lin, Wei Pan, Jianfei Yang, Yang Feng, Gaoang Wang, Jin Li, Zuolin Jin, Zhihe Zhao, et al. Deep learning-enabled 3d multimodal fusion of cone-beam ct and intraoral mesh scans for clinically applicable tooth-bone reconstruction. Patterns, 4(9), 2023.

[51] Jiaxiang Liu, Tianxiang Hu, Yang Feng, Wanghui Ding, and Zuozhu Liu. Toothsegnet: image degradation meets tooth segmentation in cbct images. In 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI), pages 1-5. IEEE, 2023.

[52] Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Yang Feng, Jin Hao, Junhui Lv, and Zuozhu Liu. Parameterefficient transfer learning for medical visual question answering. IEEE Transactions on Emerging Topics in Computational Intelligence, 2023.

[53] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2018.

[54] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. arXiv preprint arXiv:2312.17172, 2023.

[55] Juan Nunez-Iglesias, Ryan Kennedy, Toufiq Parag, Jianbo Shi, and Dmitri B Chklovskii. Machine learning of hierarchical clustering to segment 2d and 3d images. PloS one, 8(8):e71715, 2013.

[56] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023.

[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748-8763. PMLR, 2021.

[58] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[59] Philipp Schlegel, Alexander S Bates, Tejal Parag, Gregory SXE Jefferis, and Davi D Bock. Automatic detection of synaptic partners in a whole-brain drosophila em dataset. Nature Methods, 18(8):877-884, 2021.

[60] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, volume 35, pages 25278-25294, 2022.

[61] Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollár, Christoph Feichtenhofer, Ross Girshick, et al. The effectiveness of mae pre-pretraining for billion-scale pretraining. In ICCV, pages 5484-5494, 2023.

[62] Xingchen Song, Di Wu, Binbin Zhang, Zhendong Peng, Bo Dang, Fuping Pan, and Zhiyong Wu. ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs. In INTERSPEECH, pages 1648-1652, 2023.

[63] Yipeng Sun, Yixing Huang, Linda-Sophie Schneider, Mareike Thies, Mingxuan Gu, Siyuan Mei, Siming Bayer, and Andreas Maier. Eagle: An edge-aware gradient localization enhanced loss for ct image reconstruction. arXiv preprint arXiv:2403.10695, 2024.

[64] Shin-ya Takemura, Yoshinori Aso, Toshihide Hige, Allan Wong, Zhiyuan Lu, C Shan Xu, Patricia K Rivlin, Harald Hess, Ting Zhao, Toufiq Parag, et al. A connectome of a learning and memory center in the adult drosophila brain. Elife, 6:e26975, 2017.

[65] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[66] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[67] Ran Wang, Xupeng Chen, Amirhossein Khalilian-Gourtani, Leyao Yu, Patricia Dugan, Daniel Friedman, Werner Doyle, Orrin Devinsky, Yao Wang, and Adeen Flinker. Distributed feedforward and feedback cortical processing supports human speech production. Proceedings of the National Academy of Sciences, 120(42):e2300255120, 2023.

[68] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from unpaired medical images and text. In EMNLP, pages 3876-3887, 2022.

[69] Donglai Wei, Zudi Lin, Daniel Franco-Barranco, Nils Wendt, Xingyu Liu, Wenjie Yin, Xin Huang, Aarush Gupta, Won-Dong Jang, Xueying Wang, et al. Mitoem dataset: Large-scale 3d mitochondria instance segmentation from em images. In Miccai, pages 66-76. Springer, 2020.

[70] Patrick Xia, Shijie Wu, and Benjamin Van Durme. Which* bert? a survey organizing contextualized encoders. In EMNLP, pages 7516-7533, 2020.

[71] Ao Xiang, Jingyu Zhang, Qin Yang, Liyang Wang, and Yu Cheng. Research on splicing image detection algorithms based on natural image statistical characteristics. arXiv preprint arXiv:2404.16296, 2024.

[72] Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, and Lei Zhu. Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation. arXiv preprint arXiv:2401.13560, 2024.

[73] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In ICML, pages 12310-12320. PMLR, 2021.

[74] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, volume 32, 2019.

[75] Jingyu Zhang, Ao Xiang, Yu Cheng, Qin Yang, and Liyang Wang. Research on detection of floating objects in river and lake based on ai intelligent image recognition. arXiv preprint arXiv:2404.06883, 2024.

[76] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for biomedical vision-language processing. arXiv preprint arXiv:2303.00915, 2(3):6, 2023.

[77] Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, and Jianbing Shen. Thread of thought unraveling chaotic contexts. arXiv preprint arXiv:2311.08734, 2023.

[78] Yucheng Zhou, Xiang Li, Qianning Wang, and Jianbing Shen. Visual in-context learning for large vision-language models. arXiv preprint arXiv:2402.11574, 2024.
