# Transformers Learn Shortcuts to Automata 

Bingbin Liu ${ }^{1 *}$ Jordan T. Ash ${ }^{2}$ Surbhi Goel ${ }^{3 \dagger}$ Akshay Krishnamurthy ${ }^{2}$ Cyril Zhang ${ }^{2}$<br>${ }^{1}$ Carnegie Mellon University $\quad{ }^{2}$ Microsoft Research NYC $\quad{ }^{3}$ University of Pennsylvania<br>bingbinl@cs.cmu.edu, \{ash.jordan, goel.surbhi, akshaykr, cyrilzhang\}@microsoft.com


#### Abstract

Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are learned by these shallow and non-recurrent models? We show that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics. Our theoretical results characterize shortcut solutions, whereby a Transformer with $o(T)$ layers can exactly replicate the computation of an automaton on an input sequence of length $T$. We find that polynomial-sized $O(\log T)$-depth solutions always exist; furthermore, $O(1)$-depth simulators are surprisingly common, and can be understood using tools from Krohn-Rhodes theory and circuit complexity. Empirically, we find that Transformers converge to shortcut solutions with standard training, across a wide variety of automata. We further investigate the brittleness of these solutions and propose potential mitigations.


## 1 Introduction

Modern deep learning systems demonstrate increasing capabilities of algorithmic reasoning. Particularly in modalities such as natural language, math, and code, neural networks can successfully parse and synthesize sequences containing symbolic information and compositional structure. To exhibit these functionalities, these networks are required to learn and execute the relevant discrete algorithms within their internal representations. A core open question in this domain is that of mechanistic understanding: how do neural networks encode the primitives of algorithmic reasoning?

When considering this question, there is an apparent mismatch between classical sequential models of computation (e.g. Turing machines) and the Transformer, the state-of-the-art architecture for neural algorithmic reasoning. If we are to think of algorithms as sequentially-executed computational rules, why should we use a shallow ${ }^{1}$ and non-recurrent architecture to represent them?

We study this question through the lens of semiautomata, which compute state sequences $q_{1}, \ldots, q_{T}$ from inputs $\sigma_{1}, \ldots, \sigma_{T}$ by application of a recurrent transition function $\delta$ (and initial state $q_{0}$ ):

$$
q_{t}=\delta\left(q_{t-1}, \sigma_{t}\right)
$$

Semiautomata describe the underlying dynamics of automata, which are simply semiautomata equipped with mappings from states to outputs. With unbounded state spaces, automata can represent all algorithms; however, even bounded automata form a rich class of sequence processing algorithms, containing regular[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-02.jpg?height=325&width=1645&top_left_y=247&top_left_x=237)

Figure 1: Various examples of semiautomata. From left to right: a mod-2 counter, a 2-state memory unit, Grid $_{4}$, a 2-dimensional gridworld constructible via a direct product Grid 33 . Grid $_{4}$, and a Rubik's Cube, whose transformation semigroup is a very large non-abelian group.

expression parsers and finite-state transducers. In reinforcement learning and control, semiautomata are better known as deterministic Markov models (where $\sigma_{t}$ are actions); thus, in addition to algorithmic reasoning, the results in this work also pertain to Transformer dynamics models.

We perform a theoretical and empirical investigation of whether (and how) non-recurrent Transformers perform the computations of semiautomata. We find that Transformers learn shortcut solutions, which correctly and efficiently simulate the sequential transitions of semiautomata using a shallow parallel circuit, rather than naively iterating the single-step recurrence. Shortcuts arise from hierarchical reparameterizations of a semiautomaton's global transition dynamics.

Our contributions. Our theoretical results provide structural guarantees for the representability of semiautomata (thus, iterative algorithms) by one pass through a shallow, non-recurrent Transformer. In particular, we show that:

- Shortcut solutions, with depth logarithmic in the sequence length, always exist (Theorem 1).
- Constant-depth shortcuts exist for solvable semiautomata (Theorem 2). These are understood via the Krohn-Rhodes theorem, a landmark result in semigroup theory. Conversely, there do not exist constant-depth shortcuts for non-solvable semiautomata, unless $\mathrm{TC}^{0}=\mathrm{NC}^{1}$ (Theorem 4).
- For a natural class of semiautomata corresponding to path integration in a "gridworld" with boundaries, we show that there are even shorter shortcuts (Theorem 3), beyond those guaranteed by the general structure theorems above.

We accompany these with an extensive set of experimental findings:

- Transformers learn shortcuts with standard training (Section 4). Across a wide variety of semiautomaton simulation problems, we find that shallow non-autoregressive Transformers successfully learn shortcut solutions: despite the non-convex optimization problem, gradient-based training works. This suggests that shortcuts are plausible mechanisms for algorithmic reasoning in non-synthetic sequence models, and lies beyond our current theoretical understanding.
- Shortcuts are statistically brittle (Section 5). We identify empirical weaknesses of the shortcuts found by Transformers: poor out-of-distribution generalization (including to unseen sequence lengths) and worse performance than RNNs under limited supervision. Toward mitigating these drawbacks and obtaining the best of both worlds, we show that with recency-biased scratchpad training, autoregressive Transformers can easily be guided to learn the iterative RNN-like solutions (chain-of-thought generation).


### 1.1 Related work

Emergent reasoning in neural sequence models. Neural sequence models, both recurrent (Howard and Ruder, 2018, Peters et al., 2018, Wu et al., 2016) and non-recurrent (Devlin et al., 2018, Vaswani et al., 2017), have become an era-defining tool for parsing and transducing data with combinatorial structure, such as natural language and code. A nascent frontier is to leverage neural dynamics models, again both recurrent (Hafner et al., 2019) and non-recurrent (Chen et al., 2021a, Janner et al., 2021), for decision making.

At the highest level, the present work seeks to understand the mechanisms by which these models perform combinatorial and algorithmic reasoning.

Computational models within neural networks. Despite the preponderance of empirical successes, many mysteries remain, towards understanding the internal mechanisms of neural networks capable of algorithmic reasoning. It is known that self-attention realizes low-complexity circuits (Edelman et al., 2022, Elhage et al., 2021, Hahn, 2020, Merrill et al., 2021), declarative programs (Weiss et al., 2021), and Turing machines (Dehghani et al., 2019, Giannou et al., 2023, Pérez et al., 2021). Interpretable symbolic computations have been extracted from trained models (Clark et al., 2019, Tenney et al., 2019, Vig, 2019, Wang et al., 2022). Our conclusions are closest to the literature on the universal representation on Turing machines (which are automata with unbounded states); however, our work is unique in characterizing the recurrent machines whose execution loops can be efficiently unrolled into a single pass of a shallow Transformer.

At a technical level, the most relevant theoretical work to ours is (Barrington and Thérien, 1988), which acts as a "Rosetta Stone" between circuit complexity and semigroup theory. The core technical ideas for Theorems 1 ( $\mathrm{NC}^{1}$ prefix sum), 2 (Krohn-Rhodes), and 4 (Barrington) are inspired by the results and discussions therein. For readers familiar with circuit complexity: our theoretical results establish that Transformers (a certain family of arithmetic circuits) efficiently embed the constructions involved in the $\mathrm{NC}^{1}$ and $\mathrm{ACC}^{0}$ solutions to semigroup word problems. The notions of efficiency (depth, parameter count, and weight norms) are standard in deep learning but not circuit complexity; our embedding avoids suboptimal poly $(T)$ factors in these complexity measures. Theorem 3 comes from an improved parallel algorithm for the special case of "gridworld" semigroups; to our knowledge, this construction is novel, and may be of independent interest.

Learning elementary algorithms with Transformers. Our work provides a unifying lens on many recent investigations on whether (and how) Transformers represent certain classes of fundamental algorithmic computations. These include bounded-depth Dyck languages (Yao et al., 2021), modular prefix sums (Anil et al., 2022), adders (Nanda and Lieberum, 2022, Nogueira et al., 2021), regular languages (Bhattamishra et al., 2020), and sparse logical predicates (Barak et al., 2022, Edelman et al., 2022), which are all special cases of simulating finite-state automata. Thus, our work provides guarantees of shallow Transformer solutions in all of these settings. Zhang et al. (2022) empirically analyze the behavior and inner workings of Transformers on random-access group operations and note "shortcuts" (which skip over explicit program execution) similar to those we study.

We provide an expanded discussion of related work in Appendix A.5.

## 2 Preliminaries

### 2.1 Semiautomata and their algebraic structure

A semiautomaton $\mathcal{A}:=(Q, \Sigma, \delta)$ consists of a set of states $Q$, an input alphabet $\Sigma$, and a transition function $\delta: Q \times \Sigma \rightarrow Q$. In this work, $Q$ and $\Sigma$ will always be finite sets. For all positive integers $T$ and a starting state $q_{0} \in Q, \mathcal{A}$ defines a map from input sequences $\left(\sigma_{1}, \ldots, \sigma_{T}\right) \in \Sigma^{T}$ to state sequences $\left(q_{1}, \ldots, q_{T}\right) \in Q^{T}$ : $q_{t}:=\delta\left(q_{t-1}, \sigma_{t}\right)$ for $t=1, \ldots, T$. This is a deterministic Markov model, in the sense that at time $t$, the future states $q_{t+1}, \ldots, q_{T}$ only depend on the current state $q_{t}$ and the future inputs $\sigma_{t+1}, \ldots, \sigma_{T}$.

We define the task of simulation: given a semiautomaton $\mathcal{A}$, starting state $q_{0}$, and input sequence $\left(\sigma_{1}, \ldots, \sigma_{T}\right)$, output the state trajectory $\mathcal{A}_{T, q_{0}}\left(\sigma_{1}, \ldots, \sigma_{T}\right):=\left(q_{1}, \ldots, q_{T}\right)$. Let $f: \Sigma^{T} \rightarrow Q^{T}$ be a function (which in general can depend on $\left.\mathcal{A}, T, q_{0}\right)$. We will say that $f$ simulates $\mathcal{A}_{T, q_{0}}$ if $f\left(\sigma_{1: T}\right)=\mathcal{A}_{T, q_{0}}\left(\sigma_{1: T}\right)$ for all input sequences $\sigma_{1: T}$. Finally, for a positive integer $T$, we say that a function class $\mathcal{F}$ of functions from $\Sigma^{T} \rightarrow Q^{T}$ simulates $\mathcal{A}$ at length $T$ if, for each $q_{0} \in Q$, there is a function in $\mathcal{F}$ which simulates $\mathcal{A}_{T, q_{0}}$

Every semiautomaton induces a transformation semigroup $\mathcal{T}(\mathcal{A})$ of functions $\rho: Q \rightarrow Q$ under composition, generated by the per-input-symbol state mappings $\delta(\cdot, \sigma): Q \rightarrow Q$. When $\mathcal{T}(\mathcal{A})$ contains the identity function, it is called a transformation monoid. When all of the functions are invertible, $\mathcal{T}(\mathcal{A})$ is a permutation group. See Figure 1 for some examples which appear both in our theory and experiments; additional background (including a self-contained tutorial on the relevant concepts in finite group and semigroup theory) is provided
in Appendix A.2. An elementary example is a parity counter (leftmost in Figure 1): the state is a bit, and the inputs are \{"toggle the bit", "do nothing"\}; the transformation semigroup is $C_{2}$, the cyclic group of order 2 .

### 2.2 Recurrent and non-recurrent neural sequence models

A sequence-to-sequence neural network of length $T$ and embedding dimension $d$ is a function $f_{\mathrm{nn}}: \mathbb{R}^{T \times d} \times \Theta \rightarrow$ $\mathbb{R}^{T \times d}$, with trainable parameters $\theta \in \Theta$. Equipped with an encoding layer $E: \Sigma \rightarrow \mathbb{R}^{d}$ and decoding layer $W: \mathbb{R}^{d} \rightarrow Q$ (both applied position-wise), and fixing some parameters $\theta$, the function $\left(W \circ f_{\mathrm{nn}} \circ E\right): \Sigma^{T} \rightarrow Q^{T}$ has the same input and output types as $\mathcal{A}_{T, q_{0}}$.

A recurrent neural network (RNN) is a sequence-to-sequence neural network defined by iterated composition of a recurrent unit $g: \mathbb{R}^{d} \times \mathbb{R}^{d} \times \Theta \rightarrow \mathbb{R}^{d}$. For a given initial hidden state $h_{0} \in \mathbb{R}^{d}$, and input sequence $u_{1}, \ldots, u_{T} \in \mathbb{R}^{d}$, it produces an output hidden state sequence

$$
h_{t}:=g\left(h_{t-1} ; u_{t} ; \theta\right), \quad t=1, \ldots, T
$$

Thus, fixing the parameters $\theta$, an RNN is an infinite semiautomaton, with $Q=\Sigma=\mathbb{R}^{d}$. Thus, RNNs can trivially simulate any semiautomaton by embedding the looped transition function $\delta$, as long as $g$ can represent $\delta$.

An $L$-layer (or depth- $L$ ) Transformer is a different sequence-to-sequence network, consisting of alternating self-attention blocks and feedforward MLP blocks

$$
f_{\mathrm{tf}}:=\left(\mathrm{id}+f_{\mathrm{mlp}}^{(L)}\right) \circ\left(\mathrm{id}+f_{\mathrm{attn}}^{(L)}\right) \circ\left(\mathrm{id}+f_{\mathrm{mlp}}^{(L-1)}\right) \circ \ldots \circ\left(\mathrm{id}+f_{\mathrm{attn}}^{(1)}\right) \circ(\mathrm{id}+P)
$$

Briefly, an attention layer performs $\ell_{1}$-normalized mixing operations across positions $t$, while a constant-layer MLP block performs position-wise function approximation (with no mixing between positions); id denotes the identity function (residual connections), and $P$ encodes the positions $t .^{2}$ We use fairly standard positional encodings in both theory and experiments. Importantly, the standard Transformer is convolutional (in that the weights in $f_{\text {attn }}$ and $f_{\mathrm{mlp}}$ are shared across positions $t$ ), but not recurrent: parameters are not shared across blocks of different layers.

Typical Transformers are shallow, in the sense that $L \ll T$. In practice, this makes their inference and gradient computations highly parallelizable, with the number of sequential computation steps scaling linearly in $L$, while RNNs require a scaling linear in $T$. While there is always a canonical way for RNNs to simulate semiautomata, the answer to the analogous question for shallow Transformers is far less obvious, and forms the main topic of this paper.

Our results will quantify efficiency with several complexity measures of Transformers: the computational depth $D$ (which is $\Theta(L)$ for Transformers and $\Theta(T)$ for RNNs), embedding dimension $d$, attention width (the largest number of parallel attention head outputs), MLP width (the largest number of parallel hidden activations in MLP), $\ell_{\infty}$ weight norms, and floating-point precision. These are fully defined and discussed in Appendix A.4.

## 3 Theory: shortcuts abound

To simulate a semiautomaton at length $T$, a $T$-layer Transformer can implement the same sequential solution as an RNN: let the $t$-th layer embed the state transition $q_{t-1} \mapsto q_{t}$. We define shortcuts as solutions which implement the same functionality with a significantly smaller depth.

Definition 1 (Shortcut solution). Let $\mathcal{A}$ be a semiautomaton. For every $T \geq 1$, let $f_{T}$ be a sequence-tosequence neural network which simulates $\mathcal{A}$ at length $T$. Then, we call this sequence $\left\{f_{T}\right\}_{T \geq 1}$ a shortcut to $\mathcal{A}$ if the sequence of network depths $D:=\left\{D\left(f_{T}\right)\right\}_{T \geq 1}$ satisfies $D \leq o(T)$.

By this definition, shortcuts are quite general, and some are less interesting than others. For example, it is always possible to construct a constant-depth neural network which memorizes all $|\Sigma|^{T}$ values of $\mathcal{A}_{T, q_{0}}$, but

${ }^{2}$ We omit layer normalization. This discrepancy is superficial; see the discussion in Appendix A.4.

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-05.jpg?height=484&width=1570&top_left_y=224&top_left_x=272)

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-05.jpg?height=393&width=393&top_left_y=237&top_left_x=281)

(a) (b)

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-05.jpg?height=382&width=290&top_left_y=259&top_left_x=1118)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-05.jpg?height=404&width=374&top_left_y=240&top_left_x=1450)

(d)

Figure 2: Intuitions for the theoretical constructions. (a) Divide-and-conquer function composition yields logarithmic-depth shortcuts (Theorem 1). (b) The two "atoms" of the constant-depth Krohn-Rhodes decomposition (Theorem 2) of a solvable semiautomaton: modular addition and sequentially resettable memory. (c) Information flow of the cascade product, which is used to glue these atoms together, and easily implemented with residual connections. (d) An even shorter shortcut solution for gridworld simulation (Theorem 3; see Appendix C.4).

these networks must be exceptionally wide. There are also solutions which emulate transitions in "chunks", letting each of (say) $\sqrt{T}$ layers perform $\sqrt{T}$ consecutive state transitions; however, without exploiting the structure of the semiautomaton, this would require width $\Omega\left(|\Sigma|^{\sqrt{T}}\right)$. To rule out these cases and focus on interesting shortcuts for Transformers, we want the other size parameters (attention and MLP width) to be small: say, scaling at most polynomially in $T,|Q|$, and $|\Sigma|$. To construct such shortcuts, we need ideas beyond explicit iteration of state transitions.

### 3.1 Semiautomata admit shallow parallel shortcuts

We begin by noting that polynomial-width shortcuts always exist. This may seem counterintuitive if we restrict ourselves to viewing a Transformer's intermediate activations as representations of states $q_{t}$, like the RNN solution. Instead, a Transformer can encode and hierarchically compose transformations $\delta(\cdot, \sigma): Q \rightarrow Q$ (see Figure 2a), leading to far shallower solutions:

Theorem 1 (Simulation is generically parallelizable; informal). Transformers can simulate all semiautomata $\mathcal{A}=(Q, \Sigma, \delta)$ at length $T$, with depth $O(\log T)$, embedding dimension $O(|Q|)$, attention width $O(|Q|)$, and $M L P$ width $O\left(|Q|^{2}\right)$.

This is proven in Appendix C.2, and leverages the ability of a self-attention head to approximate hard attention (i.e. concentrate its mixing weights on a single position). However, self-attention heads can also perform soft attention (i.e. depend on a large number of previous positions), enabling even shallower implementations of certain sequential computations. For example, the parity automaton can be simulated by a single Transformer layer (see Lemma 6): soft attention computes prefix sums in parallel, then the MLP computes "mod 2". This leads to a significantly more nuanced question: when are there even shallower shortcuts? At first glance, such solutions may seem rare, and specialized to simple cases such as parity.

Our resolution to this question comes from the Krohn-Rhodes decomposition theorem (Krohn and Rhodes, 1965), a landmark result which vastly generalizes the uniqueness of prime integer factorizations, and created the mathematical field of algebraic automata theory (Rhodes et al., 2010). The conclusion is quite unintuitive: allowing for both hard and soft modes of attention, constant-depth shortcuts are surprisingly common!

Theorem 2 (Transformer Krohn-Rhodes; informal). Transformers can simulate all solvable ${ }^{3}$ semiautomata $\mathcal{A}=(Q, \Sigma, \delta)$, with depth $O\left(|Q|^{2} \log |Q|\right)$, embedding dimension $2^{O(|Q| \log |Q|)}$, attention width $2^{O(|Q| \log |Q|)}$, and $M L P$ width $|Q|^{O\left(2^{|Q|}\right)}+2^{O(|Q| \log |Q|)} \cdot T$.[^1]

Much of the appendix is dedicated to providing a user-friendly exposition of the relevant algebraic concepts, culminating in the proof of Theorem 2. We provide a few high-level notes below:

- Intuitively (illustrated in Figures 2b and 2c), the Krohn-Rhodes decomposition "factorizes" every solvable semiautomaton into modular counters and memory units, glued together via a feedforward cascade product (Definition 4) whose depth only depends on $|Q|$, not $T$. These two types of "prime" semiautomata can be efficiently simulated by depth-1 Transformers.
- The decomposition depends on the transformation semigroup $\mathcal{T}(\mathcal{A})$. It is non-constructive (much like how the existence of prime factorizations doesn't entail a procedure to find them). Computationally, these solutions still have to be found by a search procedure. Remarkably, we find that gradient-based training succeeds empirically, despite the worst-case computational hardness of related problems.

What makes Transformers special (vs. other universal function approximators)? The same underlying semiautomaton-to-circuit constructions could be applied to any universal function approximator (like a vanilla MLP with the same depth). Transformers embed all of the constructions in Theorems 1 and 2 with exceptional efficiency, in terms of the network complexity measures discussed in Section 2. Most importantly, the constructions leverage Transformers' positional weight sharing, which removes all ${ }^{4}$ suboptimal $T$ factors from the parameter count. We discuss this further in Appendix A.5.

Even shallower shortcuts, beyond Krohn-Rhodes. Finally, we show that on a natural class of problems, the computational model of self-attention leads to further fine-grained improvements over the guarantees of Krohn-Rhodes theory. Motivated by the application of Transformers in modeling environment dynamics, we consider the semiautomaton $\operatorname{Grid}_{n}$ corresponding to a "gridworld": $n$ states on a line, with inputs "move left if possible" and "move right if possible" (see Figure 1, middle). We show that self-attention enables an extremely concise solution, with depth independent of both $T$ and $|Q|=n$ :

Theorem 3 (Depth-2 shortcut for gridworld; informal). For all positive integers n,T, Transformers can simulate $\operatorname{Grid}_{n}$ at length $T$, with depth $2,{ }^{5}$ embedding dimension $O(1)$, attention width $O(n)$, and MLP width $O(T) .{ }^{6}$

The proof builds a parallel nearest boundary detector for the two boundary (i.e. leftmost and rightmost) states, and can be found in Appendix C.4. We note that gridworlds are known to be extremal cases for the holonomy decomposition in Krohn-Rhodes theory (Maler (2010) discusses this, calling it the elevator automaton). It would be interesting to generalize our improvement and characterize the class of problems for which self-attention affords $O(1)$ instead of poly $(|Q|)$-depth solutions.

### 3.2 Lower bounds

Can Theorem 2 be improved to handle non-solvable semiautomata? (Equivalently: can Theorem 1 be improved to constant depth?) It turns out that as a consequence of a classic result in circuit complexity (Barrington, 1986), this question is equivalent to the major open question of $\mathrm{TC}^{0} \stackrel{?}{=} \mathrm{NC}^{1}$ (thus: conjecturally, no). Unless these complexity classes collapse, Theorems 1 and 2 are optimal. In summary, simulating non-solvable semiautomata with constant depth is provably hard:

Theorem 4 (Transformer Barrington). Let $\mathcal{A}$ be a non-solvable semiautomaton. Then, for sufficiently large $T$, no $O(\log T)$-precision Transformer with depth independent of $T$ and width polynomial in $T$ can continuously simulate $\mathcal{A}$ at length $T$, unless $\mathrm{TC}^{0}=\mathrm{NC}^{1}$.

This is proven in Appendix C.5. The smallest example of a non-solvable semiautomaton has $|Q|=60$ states, whose transitions generate $A_{5}$ (all of the even permutations).

Finally, we note that although our width bounds might be improvable, an exponential-in- $|Q|$ number of hypotheses (and hence a network with poly $(|Q|)$ parameters) is unavoidable if one wishes to learn an arbitrary[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-07.jpg?height=735&width=783&top_left_y=256&top_left_x=259)

(a) Accuracy across tasks (rows) and network depths (columns); details in Appendix B.1.1.

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-07.jpg?height=366&width=569&top_left_y=237&top_left_x=1209)

(b) Training curves for $C_{2}$ (i.e. parity; 10 replicates).

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-07.jpg?height=366&width=572&top_left_y=652&top_left_x=1205)

(c) Training curves for $S_{5}$ (10 replicates).

Figure 3: Overview of the empirical results in Section 4, on in-distribution learnability of shortcuts by standard Transformer training. (a) Truncated table of results (in-distribution accuracy); rows specify semiautomaton simulation problems, and columns specify network depth. (b),(c) Training is highly unstable.

$|Q|$-state semiautomaton from data: there are $|Q|^{|Q| \cdot|\Sigma|}$ of them, which generate $|Q|^{\Omega\left(|Q|^{2}\right)}$ distinct semigroups (Kleitman et al., 1976). If we wish to study how machine learning models can efficiently identify large algebraic structures, we will need finer-grained inductive biases to specify which semiautomata to prefer, a direction for future work.

## 4 Experiments: can SGD find the shortcuts?

The results in Section 3 only provide a precise understanding of representability: they show that shortcut solutions exist within the parameter space of a shallow Transformer. To understand whether Transformers can actually learn these shortcuts from data, we must introduce the additional considerations of generalization and optimization. It is notoriously difficult to derive meaningful analyses which account for all of these factors in deep learning; thus, we do not attempt to do so in this work. ${ }^{7}$ Instead, we approach the end-to-end question with an empirical lens: trained on sequences arising from a variety of automata, does a shallow (depth- $L \ll T$ ) Transformer converge to correct simulators of these automata?

For a selection of 19 semiautomata corresponding to various groups and semigroups (detailed descriptions in Appendix B.1.1), we train shallow Transformer (GPT-2-like (Radford et al., 2019)) models to map randomly sampled sequences $\left(\sigma_{1}, \ldots, \sigma_{T}\right)$ to their corresponding state sequences $\left(q_{1}, \ldots, q_{T}\right)$, and evaluate their accuracy on held-out sequences. We vary the depth $L$ from 1 to 16 , and use freshly-sampled sequences of length $T=100$. In this setup, the number of sequences encountered during training $\left(\leq 10^{6}\right)$ is far smaller than the number of distinct input sequences $\left(|\Sigma|^{100}\right)$. Thus, brute-force memorization cannot solve this task, and generalization is necessary to achieve nontrivial performance.

Strikingly, we obtain positive results $\left(>99 \%\right.$ in-distribution accuracy ${ }^{8}$ ) for every finite-state semiautomaton[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-08.jpg?height=477&width=488&top_left_y=241&top_left_x=268)

(a) 1st layer, uniform attention

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-08.jpg?height=480&width=496&top_left_y=237&top_left_x=809)

(b) 4th layer, left boundary detector

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-08.jpg?height=480&width=485&top_left_y=237&top_left_x=1362)

(c) 4th layer, right boundary detector

Figure 4: Attention heads implement a nearest boundary detector for the 1-dimensional gridworld task (see Figure 1 and Theorem 3): the lower triangle shows the causal attention patterns (the upper triangle is masked out, hence all 0), where a brighter color corresponds to a higher attention score. Positions for the actual boundaries are marked by white (for the left boundary i.e. state 1) or gray (for the right boundary, i.e. state $n$ ) dots. This shows that our theoretical construction in Theorem 3 agrees with the solutions found in practice.

we considered, including ones which generate the non-solvable groups $A_{5}$ and $S_{5}$. Figure 3a gives a selection of our full results (in Appendix B.1). We find that more complex semiautomata (corresponding to non-abelian groups) require deeper networks to learn, in agreement with our theoretical constructions.

What about the small fraction of mistakes? Our theoretical results show that there are logarithmicdepth $\left(S_{5}, A_{5}\right)$ and constant-depth (all the others) solutions which simulate these semiautomata with exactly $100 \%$ accuracy. Of course, with such long sequences, black-box evaluation of whether this accuracy is reached in the population distribution is computationally infeasible. However, we note that without periodic activations (or some other mechanism for extrapolating to unseen count values), our theoretical constructions require MLPs to memorize the mod-n function. This will be revisited in the out-of-distribution evaluation experiments in Section 5.2, but there is even a corresponding implication for in-distribution mistakes: if a model never sees outlier counts during training, it is expected to make mistakes on those outliers when they appear in evaluation.

Which shallow solutions are learned? Our theoretical results identify shortcut solutions which follow multiple, mutually incompatible paradigms. In general, we do not attempt a full investigation of mechanistic interpretability of the trained models. In particular, we do not claim that the networks discover implementations are isomorphic to those described in the proofs of Theorem 1, 2, and 3. However, as a preliminary exploration, we visualize some of the attention patterns in Figure 3b within successfully-trained models, finding attention heads which perform flat summations (with uniform attention) and conditional resets, agreeing with the construction in Theorem 3.

Optimization instability. Although sufficiently deep networks find the solutions with non-negligible probability, the training dynamics are unstable; Figure 3b,c show example training curves, which exhibit high variance, negative progress, or accuracy that decays with continued training. In the same vein as the "synthetic reasoning tasks" introduced by Zhang et al. (2022), we hope that semiautomaton simulation will be useful as a clean, nontrivial testbed (with multiple difficulty knobs) for debugging and improving training algorithms, and perhaps the neural architectures themselves. More details are deferred to Appendix B.1.1.

## 5 Further experiments: more challenging settings

The results from Sections 3 and 4 show that Transformers can learn shortcuts end-to-end, unobstructed by depth, generalization, or optimization. However, the experiments in Section 4 are idealized in several

| Task | Dyck $_{4,8}$ | Grid $_{9}$ | $S_{5}$ | $C_{4}$ | $D_{8}$ |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Observation | stack top | $\mathbb{1}_{\text {boundary }}$ | $\pi_{1: t}(1)$ | $\mathbb{1}_{0 \text { mod } 4}$ | location |
| Accuracy | 100.0 | 100.0 | 99.6 | 99.9 | 100.0 |

(a) Accuracies with indirect supervision (details in Appendix B.2.1). LSTM gets $100 \%$ on all tasks.

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-09.jpg?height=306&width=544&top_left_y=324&top_left_x=1300)

(b) Varying $p_{\text {reveal }}$ (log spacing).

Figure 5: Overview of the empirical results in Section 5.1. (a) Learning in the latent-state setting, with various observation maps $\varphi\left(q_{t}\right)$. (b) Learning from incomplete state sequences: final accuracy vs. position-wise probability of a hidden token, for GPT and LSTM; the mean and standard deviations are taken over 25 runs.
![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-09.jpg?height=358&width=1260&top_left_y=1030&top_left_x=432)

Figure 6: OOD generalization on $C_{2}$ (parity): Transformers fail to generalize to different distributions (left) because shortcuts fail to generalize to unseen counts (right; the 1s are uniformly distributed in the sequence). In contrast, recurrent solutions (LSTM, and Transformer with recency-biased scratchpad training) maintain perfect accuracy.

![](https://cdn.mathpix.com/cropped/2024_06_04_abf57f04cabbcc6ad33dg-09.jpg?height=488&width=1235&top_left_y=1773&top_left_x=445)

Figure 7: Length generalization on Dyck (left) and $C_{2}$ (right): Transformers fail to generalize to longer sequences, but can be improved by modifying positional encodings. In contrast, recurrent solutions (LSTM, and Transformer with recency-biased scratchpad training) maintain perfect accuracy.
ways; a natural question is whether these findings are robust to various challenges that arise in practice. In this section, we investigate the robustness of the shallow Transformer solutions, compared to those found by RNNs (the "natural" architecture for simulating semiautomata). We consider harder forms of supervision (Section 5.1) and evaluation (Section 5.2); details are deferred to Appendix B.2.

### 5.1 Incomplete and indirect supervision

Successful learning with partial observations. Consider the case of partial observability. For any semiautomaton $\mathcal{A}=(Q, \Sigma, \delta)$ and a (generally non-invertible) observation function $\varphi: Q \rightarrow \tilde{Q}$, we can define the problem of predicting $\tilde{q}_{t}:=\varphi\left(q_{t}\right)$. If we can only obtain observations $\tilde{q}_{t}$ (i.e., the state is latent), this fully captures the problem of learning a finite-state automaton from data. The results in this paper have shown that this is equivalent to the fully-observable case in terms of representation. However, the learning problem can be much harder; indeed, this may account for Bhattamishra et al. (2020)'s negative results on learning regular languages with constant-depth Transformers. Note that this also captures autoregressive next-token prediction tasks induced by distributions (e.g., generating Dyck languages (Yao et al., 2021)) where the sequence's continuations depend on a latent semiautomaton's state (e.g., the current stack for Dyck). Despite these potential challenges, we find that Transformers are able to find solutions with good in-distribution performance for all partially observable settings we consider; see Figure 5a.

Learning from incomplete state sequences: RNNs are better. Next, we consider the setting which is identical to that described in Section 4 , but each state $q_{t}$ is randomly revealed from the training data with some probability $0 \leq p_{\text {reveal }} \leq 1$. As with partial observability, this does not affect representation issues, but can make learning/optimization much harder. Figure 5b shows the accuracy of $S_{5}$ for models trained on length 100 sequences for various $p_{\text {reveal }}$. It can be seen that Transformers may be unable to find good solutions when the labels become sparser, whereas LSTM's performance stays robust across all choices of $p_{\text {reveal }}$.

### 5.2 Out-of-distribution shortcomings of shortcut solutions

Out-of-distribution generalization: RNNs are better. The theoretical construction of modular counters (Lemma 6) suggests a possible failure mode: if attention performs prefix addition and the MLP computes the sum modulo $n$, the MLP could fail on sums unseen during training. This suggests that if the distribution over $\sigma_{1: T}$ shifts between training and testing (but the semiautomaton remains the same), a non-recurrent shortcut solution might map inputs into an intermediate latent variable space (like the sum) which fails to generalize.

Indeed, we observe that with the same models which obtain the positive in-distribution results in Section 4 , accuracy degrades as distribution shift increases; see Figure 6 (left), where the performance drops as the probability of seeing input $\sigma=1$ deviates from the training distribution $(\operatorname{Pr}[\sigma=1]=0.5)$. From the viewpoint of mechanistic interpretation, one possible explanation is that Transformers learn shortcuts that calculates parity by first counting the number of 1 s in the input sequence then computing modulo 2 , and hence struggle to deal with sequences where the count is less frequently during training. To verify this hypothesis, we further compare the accuracy against number of 1 s in the sequence (Figure 6 (right)); details are deferred to Appendix B.

Length generalization: RNNs are better. More ambitiously, we could try to use these models to extrapolate to longer sequence lengths $T$ than those seen in the training data. Promoting this difficult desideratum of length generalization is an intricate problem in its own right; see Anil et al. (2022), Yao et al. (2021) for more experiments similar to ours. Figure 7 shows the performance on sequences of various lengths. In contrast to LSTM's perfect performance on all scenarios, Transformer's accuracy drops sharply as we move to lengths unseen during training. This is not purely due to unseen values of the positional encoding: randomly shifting the positions during training can cover all the positions seen during testing, which helps improve the length generalization performance but cannot make it perfect; we see similar results for removing positional encodings altogether. Finally, we empirically show that the above flaws are circumventable. Using a combination of scratchpad
(a.k.a. "chain-of-thought") (Nye et al., 2021, Wei et al., 2022) and recency bias (Press et al., 2022), we demonstrate that Transformers can be guided towards learning recurrent (depth- $T$ ) solutions, which generalize outof-distribution and to longer sequence lengths (Figure 7, yellow curves). Details are deferred to Section B.2.3.

Discussion: shortcuts as "unintended" solutions. Throughout the deep learning literature, the term shortcut is often used to refer to undesired (i.e., misleading, spurious, or overfitting) statistical properties of learned representations (Geirhos et al., 2020, Robinson et al., 2021). Meanwhile, under our computational $(o(T)$ circuit depth) definition, shortcut solutions are perfectly valid ways to represent recurrent computations. The results in this section establish a connection between these notions: partially-learned computational shortcuts can be statistical shortcuts. Specifically, a non-recurrent architecture can "hallucinate" intermediate variables other than the state (e.g. the "count" variable for the parity automaton), is thus sensitive to the coverage of these variables in the training data. This leads to out-of-distribution generalization failures (e.g. on rare counts) which are not present in recurrent models.

Computational-statistical tradeoffs. The experiments in this section highlight a statistical penalty for learning recurrent computations with a non-recurrent architecture. However, the computational advantage of a shallow architecture is extremely appealing: maximally leveraging parallel computation, training and inference can be done much faster $(O(\log T)$ or $O(1)$ time, compared to $O(T))$. This highlights a delicate tradeoff between RNNs and Transformers, where neither architecture dominates the other, even when considering this elementary class of algorithmic problems. Attaining the best of both worlds with a practical architecture is an interesting avenue for future work.

## 6 Conclusions and future work

We have conducted a theoretical and empirical analysis of how shallow Transformers can learn shortcuts to the recurrent computations of finite-state automata. These shortcuts replace $T$ sequential iterations of a recurrent unit with a single pass through $L \ll T$ parallel self-attention layers. Our theoretical results show that shortcuts are ubiquitous, and characterize extremely shallow ones (with $L$ independent of the context length $T$ ) using algebraic machinery (Krohn-Rhodes theory). Empirically, we have shown that gradient-based optimization successfully finds these shortcuts. While the solutions found in practice generalize near-perfectly in-distribution (Section 4), they lack out-of-distribution robustness (Section 5). We hope that these results shed new light on the internal reasoning mechanisms of Transformers, as well as the design space for architectures capable of algorithmic reasoning.

Future work. This work is an initial foray into the interplay between neural architectures, algebraic automata theory, and circuit complexity. We list some open questions:

- Finer-grained circuit complexity of self-attention: For certain automata of interest (e.g. bounded Dyck language parsers (Yao et al., 2021), and the gridworld automata from Theorem 3), there exist extremely shallow Transformer solutions, with depth independent of both $T$ and $|Q|$. Which other natural classes of automata have this property of "beyond Krohn-Rhodes" representability?
- When and why does gradient-based optimization work? The precise understanding of hierarchical representation learning in neural networks is an active frontier of research. The empirical tractability of finding "algebraic" shortcut solutions with gradient descent is an especially striking case, as related problems are known to be PSPACE-hard (Cho and Huynh, 1991, ?, ?).
- Mechanistic interpretability: Automaton simulation problems generate a rich class of challenging test cases for the reverse engineering of trained networks (see Nanda and Lieberum (2022)), and may yield further insights on the inductive biases of Transformers. In our preliminary attempts, we were only able to interpret a small number of simple models.


## Acknowledgements

We are very grateful to Abhishek Shetty for helpful discussions about circuit complexity. We also thank Ashwini Pokle for thoughtful comments and suggestions towards improving clarity and readability.

## Reproducibility Statement

Complete proofs of the theoretical results are provided in Appendix C, with a self-contained tutorial of relevant algebraic concepts in Appendix A.2. For the empirical results, all our datasets are derived from synthetic distributions, which are clearly described in Appendix B. 1 and B.2. The architectures, implementations (with references to popular base repositories), and hyperparameters (including training procedure) are documented in Appendix B.3. Our open-source data-generating code is available from our project page.

## References

Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. arXiv:2207.04901, 2022.

Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge University Press, 2009 .

Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Logical extrapolation without overthinking. arXiv:-2202.05826, 2022.

Boaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: SGD learns parities near the computational limit. arXiv:2207.08799, 2022.

David A. Mix Barrington. Bounded-width polynomial-size branching programs recognize exactly those languages in NC1. In Symposium on the Theory of Computing, 1986.

David A. Mix Barrington and Denis Thérien. Finite monoids and the fine structure of NC ${ }^{1}$. Journal of the $A C M, 1988$.

Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. In Conference on Empirical Methods in Natural Language Processing, 2020.

Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv:2104.13478, 2021.

Ashok K Chandra, Steven Fortune, and Richard Lipton. Unbounded fan-in circuits and associative functions. In Symposium on Theory of Computing, 1983.

Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems, 2021a.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philipp Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leiki, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv:2107.03374, 2021b.

Sang Cho and Dung T Huynh. Finite-automaton aperiodicity is PSPACE-complete. Theoretical Computer Science, 1991.

Noam Chomsky and Marcel P Schützenberger. The algebraic theory of context-free languages. In Studies in Logic and the Foundations of Mathematics. 1959.

Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021.

Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? An analysis of BERT's attention. In ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2019.

George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 1989 .

Amit Daniely. Depth separation for neural networks. In Conference on Learning Theory, pages 690-696. PMLR, 2017.

Amit Daniely and Eran Malach. Learning parities with neural networks. Advances in Neural Information Processing Systems, 2020.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In International Conference on Learning Representations, 2019.

Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Marcus Hutter, Shane Legg, and Pedro A Ortega. Neural networks and the chomsky hierarchy. arXiv preprint arXiv:2207.02098, 2022.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805, 2018.

Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 2022.

Javid Ebrahimi, Dhruv Gelda, and Wei Zhang. How can self-attention networks recognize Dyck-n languages? In Findings of the Association for Computational Linguistics: EMNLP, 2020.

Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, 2022.

Attila Egri-Nagy and Chrystopher L Nehaniv. Computational holonomy decomposition of transformation semigroups. arXiv:1508.06345, 2015.

Samuel Eilenberg. Automata, languages, and machines. Academic Press, 1974.

Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on learning theory, pages 907-940. PMLR, 2016.

Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. URL https://transformer-circuits.pub/ 2021/framework/index.html.

Merrick Furst, James B. Saxe, and Michael Sipser. Parity, circuits, and the polynomial-time hierarchy. Mathematical Systems Theory, 1984.

Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2020.

Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196, 2023.

Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the ReLU in polynomial time. In Conference on Learning Theory, 2017.

Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016 .

Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.

Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv:1711.02281, 2017.

Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv:1912.01603, 2019.

Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 2020.

Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. arXiv:2203.16634, 2022.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.

Christoph Hertrich, Amitabh Basu, Marco Di Summa, and Martin Skutella. Towards lower bounds on the depth of ReLU neural networks. In Advances in Neural Information Processing Systems, 2021.

W Daniel Hillis and Guy L. Steele Jr. Data parallel algorithms. Communications of the ACM, 1986.

Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 1989.

Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv:1801.06146, 2018.

DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. arXiv:2203.07852, 2022.

Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. In Advances in Neural Information Processing Systems, 2021.

Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A Smith. Finetuning pretrained transformers into rnns. arXiv:2103.13076, 2021.

Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. arXiv preprint arXiv:2006.15595, 2020.

Daniel J Kleitman, Bruce R Rothschild, and Joel H Spencer. The number of semigroups of order n. Proceedings of the American Mathematical Society, 1976.

László Kovács and Cheryl Praeger. Finite permutation groups with large abelian quotients. Pacific Journal of Mathematics, 1989 .

Marc Krasner and Léo Kaloujnine. Produit complet des groupes de permutations et probleme d'extension de groupes II. Acta Scientiarum Mathematicarum, 1951.

Kenneth Krohn and John Rhodes. Algebraic theory of machines, I: Prime decomposition theorem for finite semigroups and machines. Transactions of the American Mathematical Society, 1965.

Guillaume Lample and François Charton. Deep learning for symbolic mathematics. arXiv:1912.01412, 2019.

Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. FractalNet: Ultra-deep neural networks without residuals. arXiv:1605.07648, 2016.

Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural nets to express distributions. In Conference on Learning Theory, pages 1271-1296. PMLR, 2017.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. arXiv:2203.07814, 2022.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv:1711.05101, 2017.

Oded Maler. On the Krohn-Rhodes cascaded decomposition theorem. In Time for Verification. 2010.

Oded Maler and Amir Pnueli. On the cascaded decomposition of automata, its complexity and its application to logic (Draft). 1994.

Carlo Mereghetti and Beatrice Palano. Threshold circuits for iterated matrix product and powering. RAIROTheoretical Informatics and Applications, 2000.

William Merrill, Yoav Goldberg, Roy Schwartz, and Noah A. Smith. On the power of saturated Transformers: A view from circuit complexity. arXiv:2106.16213, 2021.

Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world models. arXiv:2209.00588, 2022.

Anirbit Mukherjee and Amitabh Basu. Lower bounds over boolean inputs for deep neural networks with ReLU gates. arXiv:1711.03073, 2017.

Neel Nanda and Tom Lieberum. A mechanistic interpretability analysis of grokking. Alignment Forum, 2022. URL https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/ a-mechanistic-interpretability-analysis-of-grokking.

Benjamin Newman, John Hewitt, Percy Liang, and Christopher D. Manning. The EOS decision and length extrapolation. In BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, 2020.

Eshaan Nichani, Yu Bai, and Jason D Lee. Identifying good directions to escape the NTK regime and efficiently learn low-degree plus sparse polynomials. arXiv:2206.03688, 2022.

Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv:2102.13019, 2021.

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. arXiv:2112.00114, 2021.

Christos H Papadimitriou and John N Tsitsiklis. The complexity of Markov decision processes. Mathematics of Operations Research, 1987.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K"opf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 2019.

Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is turing complete. The Journal of Machine Learning Research, 22(1):3463-3497, 2021.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv:1802.05365, 2018.

Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv:2009.03393, 2020.

Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019.

John H. Reif and Stephen R. Tate. On threshold circuits and polynomial computation. SIAM Journal on Computing, 1992.

John Rhodes, Chrystopher L Nehaniv, and Morris W Hirsch. Applications of automata theory and algebra: via the mathematical theory of complexity to biology, physics, psychology, philosophy, and games. World Scientific, 2010.

Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? Advances in Neural Information Processing Systems, 2021.

Itay Safran, Ronen Eldan, and Ohad Shamir. Depth separations in neural networks: what is actually being separated? In Conference on Learning Theory, pages 2664-2666. PMLR, 2019.

Tal Schuster, Ashwin Kalyan, Alex Polozov, and Adam Kalai. Programming puzzles. In Advances in Neural Information Processing Systems Track on Datasets and Benchmarks, 2021.

Marcel Paul Schützenberger. On finite monoids having only trivial subgroups. Information and Control, 1965.

Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. In Advances in Neural Information Processing Systems, 2021.

Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. In Conference on Learning Theory, 1992.

Matus Telgarsky. Benefits of depth in neural networks. In Conference on learning theory, pages 1517-1539. PMLR, 2016.

Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. arXiv:1905.05950, 2019 .

Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. arXiv:1609.03499, 2016.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017.

Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake A. Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In IEEE Conference on Computer Vision and Pattern Recognition, 2021.

Jesse Vig. Visualizing attention in transformer-based language representation models. arXiv:1904.02679, 2019 .

Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv:2201.11903, 2022 .

Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like Transformers. In International Conference on Machine Learning, 2021.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface's transformers: State-of-the-art natural language processing. arXiv:1910.03771, 2019.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv:1609.08144, 2016.

Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu. A survey on non-autoregressive generation for neural machine translation and beyond. arXiv:2204.09269, 2022.

Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. arXiv:2009.11848, 2020.

Shunyu Yao, Binghui Peng, Christos H. Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. In Association for Computational Linguistics, 2021.

Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in Neural Information Processing Systems, 2021.

H Paul Zeiger. Cascade synthesis of finite-state machines. Information and Control, 1967.

Chiyuan Zhang, Maithra Raghu, Jon Kleinberg, and Samy Bengio. Pointer value retrieval: A new benchmark for understanding the limits of neural network generalization. arXiv:2107.12580, 2021a.

Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? In International Conference on Learning Representations, 2021b.

Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling Transformers with LEGO: a synthetic reasoning task. arXiv:2206.04301, 2022.

Karl-Heinz Zimmermann. On Krohn-Rhodes theory for semiautomata. arXiv:2010.16235, 2020.
