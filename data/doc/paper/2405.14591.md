# Base of RoPE Bounds Context Length 

Xin Men*<br>Baichuan Inc.<br>Qingyu Zhang<br>ISCAS<br>Mingyu Xu*<br>Baichuan Inc.<br>Hongyu Lin<br>ISCAS<br>Weipeng Chen<br>Baichuan Inc.


#### Abstract

Position embedding is a core component of current Large Language Models (LLMs). Rotary position embedding (RoPE), a technique that encodes the position information with a rotation matrix, has been the de facto choice for position embedding in many LLMs, such as the Llama series. RoPE has been further utilized to extend long context capability, which is roughly based on adjusting the base parameter of RoPE to mitigate outof-distribution (OOD) problems in position embedding. However, in this paper, we find that LLMs may obtain a superficial long-context ability based on the OOD theory. We revisit the role of RoPE in LLMs and propose a novel property of long-term decay, we derive that the base of RoPE bounds context length: there is an absolute lower bound for the base value to obtain certain context length capability. Our work reveals the relationship between context length and RoPE base both theoretically and empirically, which may shed light on future long context training.


![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-01.jpg?height=631&width=829&top_left_y=1625&top_left_x=645)

Figure 1: Context length and its corresponding lower bound of RoPE's base value.[^0]

## 1 Introduction

In the past few years, large language models have demonstrated surprising capabilities and undergone rapid development. By now, LLMs have been widely applied across various domains, including chatbots, intelligent agents, and code assistants (Achiam et al. 2023; Jiang et al. 2023b). The Transformer (Vaswani et al. 2017), based on the attention mechanism, has been the most popular backbone of LLMs due to its good performance and scaling properties (Tay et al. 2022). One of the key component modules in the Transformer is position embedding, which is introduced to embed positional information that is vital for processing sequential data. Rotary position embedding (RoPE), which encodes relative distance information in the form of absolute position embedding (Su et al., 2024), has been a popular choice and applied in many LLMs (Touvron et al., 2023a; Yang et al., 2023; Bai et al. 2023).

RoPE introduces no training parameters and shows improvement in language modeling and many other tasks (Su et al., 2024: Heo et al., 2024). One reason that RoPE is widely used is its ability for context length extrapolation (Peng et al., 2023b; Chen et al., 2023), which extends the context length of a trained LLM without expensive retraining. In practice, many works (Touvron et al. 2023a, Liu et al., 2024a; Young et al. 2024) have successfully extended the window length by simply increasing base value, the only one hyper-parameter in RoPE, and fine-tuning on long texts.

The reasons behind the success of these long context extensions are often explained as avoiding out-of-distribution (OOD) rotation angles (Liu et al. 2024b, Han et al. 2023) in RoPE, meaning the extended context length (OOD) can be mapped to the in-distribution context length that has been properly trained. Based on the OOD theory, a recent study (Liu et al. 2024b) finds that a smaller base can mitigate OOD and is beneficial for the model's ability to process long contexts, which inspires us to further study the relationship between the base of RoPE and the length of context the model can process.

In this paper, we find that the model may show superficial long context capability with an inappropriate RoPE base value, in which case the model can only preserve low perplexity but loses the ability to retrieve long context information. We also show that the out-of-distribution (OOD) theory in position embedding, which motivates most length extrapolation works (Peng et al., 2023b; Chen et al. 2023; Liu et al., 2024b), is insufficient to fully reflect the model's ability to process long contexts. Therefore, we revisit the role of RoPE in LLMs and derive a novel property of long-term decay in RoPE: the ability to attend more attention to similar tokens than random tokens decays as the relative distance increases. While previous long context works often focus on the relative scale of the RoPE base, based on our theory, we derive an absolute lower bound for the base value of RoPE to obtain a certain context length ability, as shown in Figure1. To verify our theory, we conducted thorough experiments on various LLMs such as Llama2-7B (Touvron et al. 2023b), Baichuan2-7B (Yang et al., 2023) and a 2-billion model we trained from scratch, demonstrating that this lower bound holds not only in the fine-tuning stage but also in the pre-training stage.

We summarize the contributions of the paper as follows:

- Theoretical perspective: we derive a novel property of long-term decay in RoPE, indicating the model's ability to attend more to similar tokens than random tokens, which is a new perspective to study the long context capability of the LLMs.
- Lower Bound of RoPE's Base: to achieve the expected context length capability, we derive an absolute lower bound for RoPE's base according to our theory. In short, the base of RoPE bounds context length.
- Superficial Capability: we reveal that if the RoPE's base is smaller than a lower bound, the model may obtain superficial long context capability, which can preserve low perplexity but lose the ability to retrieve information from long context.


## 2 Background

In this section, we first introduce the Transformer and RoPE, which are most commonly used in current LLMs. Then we discuss long context methods based on the OOD of rotation angle theory.

### 2.1 Attention and RoPE

The LLMs in current are primarily based on the Transformer (Vaswani et al. 2017). The core component of it is the calculation of the attention mechanism. The naive attention can be written as:

$$
\begin{align*}
A_{i j} & =q_{i}^{T} k_{j}  \tag{1}\\
\operatorname{ATTN}(X) & =\operatorname{softmax}(A / \sqrt{d}) v \tag{2}
\end{align*}
$$

where $A \in R^{L \times L} q, k, v \in R^{d}$. Position embedding is introduced to make use of the order of the sequence in attention.

RoPE (Su et al. 2024) implements relative position embedding through absolute position embedding, which applies rotation matrix into the calculation of the attention score in Eq. 1. which can be written as:

$$
\begin{equation*}
A_{i j}=\left(R_{i, \theta} q_{i}\right)^{T}\left(R_{j, \theta} k_{i}\right)=q_{i}^{T} R_{j-i, \theta} k_{j}=q_{i}^{T} R_{m, \theta} k_{j} \tag{3}
\end{equation*}
$$

where $m=j-i$ is the relative distance of $i$ and $j, R_{m, \theta}$ is a rotation matrix denoted as:

$$
\left[\begin{array}{ccccccc}
\cos \left(m \theta_{0}\right) & -\sin \left(m \theta_{0}\right) & 0 & 0 & \cdots & 0 & 0  \tag{4}\\
\sin \left(m \theta_{0}\right) & \cos \left(m \theta_{0}\right) & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos \left(m \theta_{1}\right) & -\sin \left(m \theta_{1}\right) & \cdots & 0 & 0 \\
0 & 0 & \sin \left(m \theta_{1}\right) & \cos \left(m \theta_{1}\right) & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos \left(m \theta_{d / 2-1}\right) & -\sin \left(m \theta_{d / 2-1}\right)
\end{array}\right]
$$

Generally, the selection of rotation angles satisfies $\theta_{i}=b a s e^{-2 i / d}$, the typical base value for current LLMs is 10,000 , and the base of RoPE in LLMs is shown in Table 1.

Table 1: The setting of RoPE's base and context length in various LLMs.

| Model | Llama-7B\|38 | Llama2-7B 39] | Llama3-8B | Mistral-7B-v0.2 18 | Baichuan2-7B 42] |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Base | 10,000 | 10,000 | 500,000 | $1,000,000$ | 10,000 |
| Length | 2,048 | 4,096 | 8,192 | 32,768 | 4,096 |

### 2.2 OOD theory of relative rotation angle

Based on RoPE, researchers have proposed various methods to extend the long context ability of LLMs, among which representatives are PI (Chen et al., 2023) and NTK-series (NTK-aware (bloc97, 2023), YaRN (Peng et al., 2023b), and Dynamical-NTK (emozilla, 2023)). Those methods depend on the relative scale $s=T_{\text {new }} / T_{\text {origin }}$, where $T_{\text {origin }}$ is the training length of the original pre-trained model and $T_{\text {new }}$ is the training length in long-context fine-tuning.

PI PI directly interpolates the position embedding, and the calculation of $A_{i j}$ becomes:

$$
\begin{equation*}
A_{i j}=\left(R_{i / s} q_{i}\right)^{T}\left(R_{j / s} k_{i}\right)=q_{i}^{T} R_{(j-i) / s} k_{j}=q_{i}^{T} R_{m / s} k_{j} \tag{5}
\end{equation*}
$$

In other words, the position embedding of the token at position $i$ in pre-training becomes $i / s$ in fine-tuning, ensuring the position embedding range of the longer context remains the same as before.

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-04.jpg?height=469&width=1369&top_left_y=270&top_left_x=367)

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-04.jpg?height=390&width=414&top_left_y=282&top_left_x=384)

(a) base $=1 \mathrm{e} 4$

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-04.jpg?height=393&width=395&top_left_y=281&top_left_x=865)

(b) base $=500$

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-04.jpg?height=390&width=396&top_left_y=282&top_left_x=1320)

(c) base $=b \cdot s^{\frac{d}{d-2}}$

Figure 2: An illustration of OOD in RoPE when we extend context length from $4 \mathrm{k}$ to $32 \mathrm{k}$, and two solutions to avoid the OOD. We show the last dimension as it is the lowest frequency part of RoPE, which suffers OOD mostly in extrapolation. (a) For a $4 \mathrm{k}$ context-length model with base value as $1 \mathrm{e} 4$, when we extend the context length to $32 \mathrm{k}$ without changing the base value, the context length from $4 \mathrm{k}$ to $32 \mathrm{k}$ is OOD for RoPE (red area in the figure). (b) OOD can be avoided with a small base value like 500 (Liu et al., 2024b), since the full period has been fitted during fine-tuning stage. (c) We set base as $b \cdot s^{\frac{d}{d-2}}$ from NTK (Peng et al., 2023b). The blue line denotes the pre-training stage (base=1e4) and the red dashed line denotes the fine-tuning stage (base $=b \cdot s^{\frac{d}{d-2}}$ ), we can observe that the RoPE's rotation angle of extended positions is in-distribution.

NTK-series The idea is that neural networks are difficult to learn high-frequency features, and direct interpolation can affect the high-frequency parts. Therefore, the NTK-aware method achieves high-frequency extrapolation and low-frequency interpolation by modifying the base value of RoPE. Specifically, it modifies the base $b$ of the RoPE to:

$$
\begin{equation*}
b_{\text {new }}=b s \frac{d}{d-2} \tag{6}
\end{equation*}
$$

The derivation of this expression is derived from $T_{\text {new }} b_{\text {new }}^{-\frac{d-2}{d}}=T_{\text {origin }} b^{-\frac{d-2}{d}}$ to ensure that the lowest frequency part being interpolated.

A recent study (Liu et al., 2024b) proposes to set a much smaller base (e.g. 500), in which case $\theta_{i}=$ base $e^{-\frac{2 \pi}{d}}$ is small enough and typical training length (say 4,096) fully covers the period of $\cos (t-s) \theta_{i}$, so the model can obtain longer context capabilities.

One perspective to explain current extrapolation methods is the OOD of rotation angle (Liu et al. 2024b; Han et al. 2023). If all possible values of $\cos (t-s) \theta_{i}$ have been fitted during the pre-training stage, OOD would be avoided when processing longer context. Figure 2 demonstrates how these methods avoid OOD of RoPE.

## 3 Motivation

NTK-based methods are widely adopted in long-context extension (Touvron et al., 2023a: Liu et al., 2024a; Young et al., 2024). To obtain better long-context capability, however, practitioners often adopt a much larger base than the original NTK-aware method suggested. This leads to speculation that there is another bound of RoPE's base determined by context length.

On the other hand, a recent work (Liu et al. 2024b) proposes to set a much smaller base for RoPE to extend the context length. However, we find it may be a superficial long-context capability as shown in Figure 3. This method can obtain a low perplexity even at $128 \mathrm{k}$ context length, which can be explained by the OOD theory as explained above, but the model could not retrieve related information for context length as short as $1 \mathrm{k}$, even much shorter than the model's pre-trained length. Our findings support previous research $\mathrm{Hu}$

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-05.jpg?height=398&width=1315&top_left_y=267&top_left_x=405)

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-05.jpg?height=279&width=347&top_left_y=283&top_left_x=412)

(a) Perplexity

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-05.jpg?height=288&width=358&top_left_y=279&top_left_x=775)

(b) Long-eval $\mathrm{Li}^{*}$ et al. 2023)

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-05.jpg?height=279&width=566&top_left_y=283&top_left_x=1148)

(c) Needle in Haystack G. 2023

Figure 3: The superficial long context capability of avoiding OOD by the smaller base. Following the recent work (Liu et al. 2024b), we fine-tune Llama2-7B with a small base (500) to a context length of $32 \mathrm{k}$.

et al. 2024) on the limitations of perplexity in evaluating long-context abilities. To delve deep into this phenomenon, we do the theoretical exploration in the next section.

## 4 Theory Perspective

For attention mechanism in language modeling, we have the following desiderata:

Desiderata 1 The closer token gets more attention: the current token tends to pay more attention to the token that has a smaller relative distance.

Desiderata 2 The similar token gets more attention: the token tends to pay more attention to the token whose key value is more similar to the query value of the current token.

Then we examine the desiderata when we apply RoPE to the attention mechanism in LLMs.

### 4.1 Long-term Decay of Upper Bound of Attention Score

For Desiderata 1. the property of RoPE makes the model attend more to closer tokens. This kind of long-term decay has been thoroughly discussed in previous work (Su et al. 2024. Sun et al. 2022). It comes from the upper bound of attention score calculation, which can be Written as:

$$
\begin{align*}
\left|A_{i j}\right|=\left|q_{i}^{T} R_{m} k_{j}\right| & \leq \max _{l}\left(\left|h_{l}-h_{l+1}\right|\right) \sum_{n=1}^{d / 2}\left|S_{n}\right| \\
& =\max _{l}\left(\left|h_{l}-h_{l+1}\right|\right) \sum_{n=1}^{d / 2}\left|\sum_{l=0}^{n-1} e^{(j-i) \theta_{l} \sqrt{-1}}\right| \tag{7}
\end{align*}
$$

where $h_{l}=q_{i}^{T}[2 l: l 2+1] k_{j}[2 l: 2 l+1]$. Equation 7 indicates that the upper bound of the attention score $\left|A_{i j}\right|$ decays as the relative distance increases. Figure 4 shows the long-term decay curve of this upper bound, which is in accordance with previous findings (Su et al. 2024; Sun et al. 2022).

### 4.2 Long-term Decay of the Ability to Attend More to Similar Tokens than Random Tokens

In addition to the attention score's upper bound, we also find there exists another long-term decay property in RoPE: the ability to attend more to similar tokens than random tokens decays as the relative distance increases. We define the ability to attend more to similar tokens than random tokens as:

$$
\begin{equation*}
\mathbb{E}_{q, k^{*}}\left[q^{T} R_{m, \theta} k^{*}\right]-\mathbb{E}_{q, k}\left[q^{T} R_{m, \theta} k\right] \tag{8}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-06.jpg?height=326&width=676&top_left_y=278&top_left_x=366)

Figure 4: The upper bound of attention score with respect to the relative distance.

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-06.jpg?height=326&width=676&top_left_y=278&top_left_x=1082)

Figure 5: The ability to attend more to similar tokens than random tokens.

where $q \in R^{d}$ is the query vector for the current token, $k^{*}=q+\epsilon$ is the key value of a similar token, where $\epsilon$ is a small random variable, $k \in R^{d}$ is the key vector of a random token, $R_{m, \theta}$ is the rotation matrix in RoPE. The first term in Eq. 8 is the attention score of $q$ and a similar token $k^{*}$, the second term in Eq. 8 is the attention score of $q$ and random token $k$. Then we derive the following theorem:

Theorem 1 Assuming that the components of query $q \in R^{d}$ and key $k \in R^{d}$ are independent and identically distributed, their standard deviations are denoted as $\sigma \in R$. The key $k^{*}=q+\epsilon$ is a token similar to the query, where $\epsilon$ is a random variable with a mean of 0 . Then we have:

$$
\begin{equation*}
\frac{1}{2 \sigma^{2}}\left(\mathbb{E}_{q, k^{*}}\left[q^{T} R_{m, \theta} k^{*}\right]-\mathbb{E}_{q, k}\left[q^{T} R_{m, \theta} k\right]\right)=\sum_{i=0}^{d / 2-1} \cos \left(m \theta_{i}\right) \tag{9}
\end{equation*}
$$

The proof is shown in Appendix A We denote $\sum_{i=0}^{d / 2-1} \cos \left(m \theta_{i}\right)$ as $B_{m, \theta}$, and according to Theorem 1, $B_{m, \theta}$ measures the ability to give more attention to similar tokens than random tokens, which decreases as the relative distance $m$ increases, as shown in Figure5. For a very small base value, we can observe that the $B_{m, \theta}$ is even below zero at a certain distance, meaning the random tokens have larger attention scores than the similar tokens, which may be problematic for long context modeling.

### 4.3 Base of RoPE Bounds the Context Length

To satisfy the Desiderata 2. we will get $\mathbb{E}_{q, k^{*}}\left[q^{T} R_{m, \theta} k^{*}\right] \geq \mathbb{E}_{q, k}\left[q^{T} R_{m, \theta} k\right]$. According to Theorem 1, $B_{m, \theta}$ needs to be larger than zero. Given the $\theta$ in RoPE, the context length $L_{\theta}$ that can be truly obtained satisfies:

$$
\begin{equation*}
L_{\theta}=\sup \left\{L \mid B_{m, \theta} \geq 0, \forall m \in[0,1, \ldots, L]\right\} \tag{10}
\end{equation*}
$$

In other word, if we follow the setting that $\theta_{i}=b a s e^{-2 i / d}$, in order to get the expected context length $L$, there is a lower bound of the base value base $e_{L}$ :

$$
\begin{equation*}
\text { base }_{L}=\inf \left\{\text { base } \mid B_{m, \theta} \geq 0, \forall m \in[0,1, \ldots, L]\right\} \tag{11}
\end{equation*}
$$

In summary, the RoPE's base determines the upper bound of context length the model can truly obtain. Although there exists the absolute lower bound, Eq. 9 and Eq. 11 are hard to get the closed-form solution since $B_{m, \theta}$ is a summation of many cosine functions. Therefore, in this paper, we get the numerical solution. Table 2 shows this lower bound for context length ranging from 1,000 to one million. In Figure 1. we plot the context length and corresponding lower bound, we can observe that as the context length increases, the required base also increases.

Note: this boundary is not very strict because the stacking of layers in LLMs allows the model to extract information beyond the single layers' range, which may increase the context length in Eq. 10 and decrease the base in Eq. 11 Notwithstanding, in Section 5 we find that the derived bound approximates the real context length in practice.

Long-term decay from different perspectives. The long-term decay in section 4.1 and section 4.2 are from different perspectives. The former refers to the long-term decay of the

Table 2: Context length and its corresponding lower bound of RoPE's base.

| Context Len. | $1 \mathrm{k}$ | $2 \mathrm{k}$ | $4 \mathrm{k}$ | $8 \mathrm{k}$ | $16 \mathrm{k}$ | $32 \mathrm{k}$ | $64 \mathrm{k}$ | $128 \mathrm{k}$ | $256 \mathrm{k}$ | $512 \mathrm{k}$ | $1 \mathrm{M}$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Lower Bound | $4.3 \mathrm{e} 3$ | $1.6 \mathrm{e} 4$ | $2.7 \mathrm{e} 4$ | $8.4 \mathrm{e} 4$ | $3.1 \mathrm{e} 5$ | $6.4 \mathrm{e} 5$ | $2.1 \mathrm{e} 6$ | $7.8 \mathrm{e} 6$ | $3.6 \mathrm{e} 7$ | $6.4 \mathrm{e} 7$ | $5.1 \mathrm{e} 8$ |

attention score as the relative distance increases. This ensures that current tokens tend to pay more attention to the tokens closer to them. The latter indicates that with the introduction of the rotation matrix in attention, the ability to discriminate the relevant tokens from irrelevant tokens decreases as the relative distance increases. Therefore, a large $B_{m, \theta}$, corresponding to a large base value, is important to keep the model's discrimination ability in long context modeling.

## 5 Experiment

In this section, we conduct thorough experiments. The empirical result can be summarized in Table 3 . the details are in the following sections.

Table 3: In Section 5, we aim to answer the following questions.

| Questions | Answers |
| :--- | :--- |
| Q: Does RoPE's base bounds the context | Yes. When the base is small, it is difficult to get extrapolation <br> lor specific context length. |
|  | Yes. Our proposed lower bound for RoPE's base also applies <br> to pre-training. If we train a model from scratch with a small <br> base but the context length is large (larger than the bounded <br> length), the resulting model has very limited the context length <br> capabilities, meaning some of context in pre-training is wasted. |
| Q: Does RoPE's base bounds the context stage? during the pre-training stage? | The model will get the superficial long context capability. <br> lhe model can keep perplexity low, but can't retrieve useful <br> information from long context. |
| Q: What happened when base is set <br> smaller than the lower bound? |  |

### 5.1 Experiments Setup

For fine-tuning, we utilized Llama2-7B (Touvron et al., 2023a) and Baichuan2-7B (Yang et al., 2023), both of which are popular open-source models employing RoPE with a base of $1 \mathrm{e} 4$. We utilized a fixed learning rate of $2 \mathrm{e}-5$ and a global batch size of 128 and fine-tuning for 1000 steps. For pre-training, we trained a Llama-like 2B model from scratch for a total of 1 trillion tokens. We set the learning rate to 1e-4 and adopted a cosine decay schedule, with models trained on a total of $1 \mathrm{~T}$ tokens. The dataset we used is a subset of RedPajama (Computer, 2023). More details of the experimental setup are provided in Appendix B.

Our evaluation focused on two aspects: (1) Perplexity: we use PG19 dataset (Rae et al., 2019) which are often used in long context evaluation; (2) Retrieval: in addition to perplexity, we also adopt retrieval since it represents the real long-context understanding ability of LLMs. We choose a) Long-eval benchmark from (Li* et al. 2023) and b) needle in a haystack (NIH) (G). 2023). The Long-eval benchmark generates numerous random similar sentences and asks the model to answer questions based on a specific sentence within the context, while the NIH requires the model to retrieve information from various positions in the long context.

### 5.2 Base of RoPE bounds context length in fine-tuning stages

According to Eq. 11. there is a lower bound of RoPE's base determined by expected context length. We fine-tune Llama2-7b-Base on $32 \mathrm{k}$ context with varying bases. As depicted in Figure 6, although the difference in perplexity between different bases is negligible, the accuracy of Long-eval varies significantly. In Figure 6b, the dotted line denotes the lower bound derived from Eq. 11, below which the Long-eval accuracy declines significantly.

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-08.jpg?height=567&width=1303&top_left_y=275&top_left_x=411)

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-08.jpg?height=480&width=618&top_left_y=286&top_left_x=428)

(a) Perplexity

![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-08.jpg?height=474&width=621&top_left_y=300&top_left_x=1072)

(b) Long-eval $32 \mathrm{k}$

Figure 6: Fine-tuning Llama2-7B-Base on 32k context length with varying RoPE's base. Although the perplexity remains low with varying bases, the Long-eval accuracy reveals a discernible bound for the base value, below which the Long-eval accuracy declines significantly. The dotted line denotes the lower bound derived from Eq. 11
![](https://cdn.mathpix.com/cropped/2024_06_04_9e83b4f7884b53322d8cg-08.jpg?height=908&width=1306&top_left_y=1031&top_left_x=407)

Figure 7: The first row: the results of a 2B model training from scratch with base $=1 \mathrm{e} 2$. The second row: The results of fine-tuning the $2 \mathrm{~B}$ model with base $=1 \mathrm{e} 4$. The third row: The results of fine-tuning the $2 \mathrm{~B}$ model with base $=1 \mathrm{e} 6$.

Additional results are provided in Appendix C Notably, this empirically observed lower bound closely aligns with our theoretical derivation. On the other hand, we can see that base $=2 e 5$ achieves the best perplexity, but the accuracy of Long-eval is very low, which indicates the limitations of perplexity in evaluating long context capabilities.

### 5.3 The Base of RoPE bounds context length in pre-training stages

According to and Theorem 1 and Eq. 11, this constraints could also apply to pre-training stage. To validate this, we trained a 2B model from scratch with RoPE base $=100$. The results, depicted in the first row of Figure 7 , indicate that even though the model was trained with a
context length of 4,096 tokens, it was capable of retrieving information from only the most recent approximately 500 tokens. This demonstrates that the base parameter bounds the context length during the pre-training stage as well. We define the context length from which the model can effectively retrieve information as the effective context length.

And according to our theory, the effective context length can be extended as the RoPE's base increases. To validate this, we further fine-tune this 2B model on $32 \mathrm{k}$ context length, with RoPE's base set to 1e4, as shown in the second row of Figure 7 While the effective context length increased, it remains significantly below $32 \mathrm{k}$ since the effective context length bounded by base $=1 e 4$ is much smaller than $32 k$. Further, when we increase the base to 1e6 and fine-tune the base 2B model on $32 \mathrm{~K}$ (the third row in Figure 77, the model could obtain a larger context length than base $=1$ e4, which is in accodance with our theory.

To further remove the influence of model size, we also fine-tuned a larger 7B model on a $32 \mathrm{k}$ context length with a RoPE base set to 1e4 and observed an effective context length nearly identical to that of the 2B model with the same RoPE base (see Appendix D). This is empirical proof that the effective context length is determined by RoPE's base.

### 5.4 Interpretation for the superficial long context capability for small base

Based on our theory and empirical observations, it is easy to explain what happens in Figure 3 .

Better Extrapolation (Perplexity)? Due to the small base, $B_{m, \theta}$ can be smaller than zero as $m$ increases, which is shown in Figure5. The model can't attend more to similar tokens than random tokens with a large relative distance, so the model tends to focus more on nearby tokens, this will lead to a smaller empirical receptive field, even smaller than the training length. In this case, the model has a strong ability to maintain perplexity stability (Chi et al. 2023).

Worse Ability (Long-eval and NIH)! According to our previous analysis, RoPE's base bounds the context length, and the context length bounded by 500 is much lower than that bound by 10,000 . Therefore, when the base is set to 500 , the effective context length drops sharply, even after training on $32 \mathrm{k}$ context length.

### 5.5 OOD theory is insufficient to reveal long context capability

Table 4: The comparison of "Method 1" and "Method 2". These methods are designed carefully. They both are no OOD, but they are very different under our theory.

| Method | $\mathrm{OOD}$ | Long-eval |  | numbers of $m$ whose $B_{m, \theta} \geq 0$ |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | $30 \mathrm{k}$ | $15 \mathrm{k}$ | $30 \mathrm{k}$ |
| $\overline{\mathrm{Me}}$ | $x$ | 0.33 | 0.27 | 0 | 0 |
| Method 2 | $x$ | 0.40 | 0.00 | 97 | 2554 |

Section 3 mentions that methods based on the OOD theory of rotation angles may not fully reflect the long context capability. In this section, we conduct further experiments to substantiate and explain this observation. We present two methods to extend the context length of Llama2 from $4 \mathrm{k}$ to $32 \mathrm{k}$. Both of them are devoid of OOD angles. These methods are delineated mathematically as follows:

- Method 1: $\theta_{i}=(5 e 6)^{-2 i / d}$,
- Method 2: $\theta_{i}= \begin{cases}(1 e 4)^{-2 i / 128} / 8, & i \geq 44 \\ \left(1 e 4 * 8^{128 / 88}\right)^{-2 i / 128}, & i<44 .\end{cases}$

We can see from Table 4 that these two methods exhibit significantly different long context capabilities. Under the perspective of OOD rotation angle, both methods avoid OOD rotation angle, suggesting effective extrapolation. However, despite being trained on a
context length of $32 \mathrm{k}$, "method 2" struggles in completing the retrieval task at a context length of $32 \mathrm{k}$. This phenomenon is beyond the scope which the OOD theory can explain.

Under our perspective, "method 2" is severely violating $B_{m, \theta} \geq 0$ when $m \in[15 k, 30 k]$, thereby impeding its ability to achieve long-context discrimination. We speculate that the model may achieve better extrapolation in the fine-tuning stage if the base is sufficiently large to surpass a lower bound and avoid OOD of rotation angles.

## 6 Related Work

Position embedding. Since its introduction, Transformer (Vaswani et al., 2017) has achieved remarkable results in the field of natural language processing. To make full use of the order of sequence, researchers have introduced position embedding. The earliest position embedding was based on sinusoidal functions (Vaswani et al., 2017) for absolute positions, learnable absolute position embedding (Devlin et al., 2018) and many variants (Kiyono et al., 2021; Li et al. 2019) were proposed. Nevertheless, absolute position embedding has difficulties in extending directly to texts longer than the training length. Subsequently, researchers proposed relative position embedding methods (Shaw et al. 2018: Ke et al. 2020). With the development of large language models, rotary position embedding and its variants (Su et al. 2024: Sun et al. 2022) has become widely used, such as Llama2 (Touvron et al., 2023a), Baichuan2 (Yang et al., 2023), Mistral-7B-(Jiang et al., 2023a). A recent study reveals that no position embedding is also potential (Kazemnejad et al., 2024).

Long context learning. Implementing models with longer or even infinitely long contexts has always been an important goal in the field of natural language processing. Due to the squared complexity of the transformer model over time, a significant portion of the work focuses on improving the model structure (Gu \& Dao, 2023, Peng et al., 2023a, Qin et al. 2024). However, most of the work is still based on the transformer architecture. The other part of the work is aimed at reducing the computational complexity of attention itself, such as sparse attention (Beltagy et al., 2020) and group query attention (Ainslie et al., 2023). In addition, there are also some optimizations in engineering efficiency, such as flash attention (Dao et al., 2022) and ring attention (Liu et al. 2023). In the model inference stage, to save time and space, there are also some methods for accelerating long context, such as KV cache compression (Hooper et al. 2024), etc. And the position embedding is important in extrapolation. In the process of fine-tuning, methods such as PI (Chen et al. 2023), NTK, and YARN (Peng et al. 2023b) are used to change the original position embedding information. FoT (Tworkowski et al. 2024) assigns the position information of the tokens outside the local context as the first token in the local context.

## 7 Limitation

In this work, we investigate the relationship between the base of RoPE and context length. Although we have derived that there exists a lower bound for the base of RoPE determined by context length, the existence of the upper bound for RoPE's base remains an open question that warrants further exploration. In addition, because of the lack of effective benchmarks for assessing long-context capabilities, the scope of long-context capabilities discussed in this paper may be limited.

## 8 Conclusion

Our work presents a comprehensive study on the role of RoPE in LLMs for effectively modeling long context. Our main contribution lies in uncovering a novel property of RoPE through theoretical analysis, demonstrating that as the relative distance between tokens increases, the model's ability to attend more to similar tokens decreases. According to our theory, we derive a lower bound for RoPE's base in accommodating to expected context lengths. Our experimental results validate that the base of RoPE bounds context length for not only fine-tuning but also the pre-training stage. Our theory offers a new perspective on
understanding the functionality of RoPE in long-context modeling. By shedding light on the relationship between context length and position embedding, we hope our work could provide insights for enhancing the long context capability of LLMs.

## References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.

Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

bloc97. Ntk-aware scaled rope allows llama models to have extended $(8 \mathrm{k}+)$ context size without any fine-tuning and minimal perplexity degradation. https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_ rope_allows_llama_models_to_have/. 2023.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.

Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13522-13537, 2023.

Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, April 2023. URL https://github.com/togethercomputer/RedPajama-Data.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018

emozilla. Dynamically scaled rope further increases performance of long context llama with zero fine-tuning. https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/ dynamically_scaled_rope_further_increases/. 2023.

Kamradt G. Needle in a haystack - pressure testing llms. https://github.com/gkamradt/ LLMTest_NeedleInAHaystack. 2023.

Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.

Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.

Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transformer. arXiv preprint arXiv:2403.13298, 2024.

Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024.

Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang, and Yansong Feng. Can perplexity reflect large language model's ability in long text understanding? In The Second Tiny Papers Track at ICLR 2024, 2024.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023a.

Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. Impact of code language models on automated program repair. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 1430-1442. IEEE, 2023b.

Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024.

Guolin $\mathrm{Ke}$, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In International Conference on Learning Representations, 2020.

Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. Shape: Shifted absolute position embedding for transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3309-3321, 2021.

Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023. URL https://1msys.org/blog/2023-06-29-longchat.

Hailiang Li, YC Adele, Yang Liu, Du Tang, Zhibin Lei, and Wenye Li. An augmented transformer architecture for natural language generation tasks. In 2019 International Conference on Data Mining Workshops (ICDMW), pp. 1-7. IEEE, 2019.

Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.

Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024a.

Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of roPEbased extrapolation. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=J07kOSJ5V6

Amirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for transformers. Advances in Neural Information Processing Systems, 36, 2024.

Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 14048-14077, 2023a.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2023b.

Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024.

Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2019.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 464-468, 2018.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.

Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.

Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022.

Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.

Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive training for context scaling. Advances in Neural Information Processing Systems, 36, 2024.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.

Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.
