# Preference Alignment with Flow Matching 

Minu Kim ${ }^{1 *}$ Yongsik Lee ${ }^{1 *}$ Sehyeok Kang ${ }^{1} \quad$ Jihwan Oh $^{1}$<br>Song Chong ${ }^{1 \dagger}$ Seyoung Yun ${ }^{1 \dagger}$<br>${ }^{1}$ KAIST AI<br>\{minu.kim, dldydtlr93, kangsehyeok0329, ericoh929,<br>songchong, yunseyoung\}@kaist.ac.kr


#### Abstract

We present Preference Flow Matching (PFM), a new framework for preferencebased reinforcement learning (PbRL) that streamlines the integration of preferences into an arbitrary class of pre-trained models. Existing PbRL methods require fine-tuning pre-trained models, which presents challenges such as scalability, inefficiency, and the need for model modifications, especially with black-box APIs like GPT-4. In contrast, PFM utilizes flow matching techniques to directly learn from preference data, thereby reducing the dependency on extensive fine-tuning of pretrained models. By leveraging flow-based models, PFM transforms less preferred data into preferred outcomes, and effectively aligns model outputs with human preferences without relying on explicit or implicit reward function estimation, thus avoiding common issues like overfitting in reward models. We provide theoretical insights that support our method's alignment with standard PbRL objectives. Experimental results indicate the practical effectiveness of our method, offering a new direction in aligning a pre-trained model to preference.


## 1 Introduction

Preference-based reinforcement learning (PbRL) has emerged as a groundbreaking approach with significant contributions to performance improvement [Akrour et al., 2011, Wilson et al. 2012], particularly in the realm of artificial intelligence where understanding and incorporating human preferences are crucial. Unlike traditional reinforcement learning, which struggles due to the absence of explicit reward functions or the infeasibility of defining comprehensive environmental rewards, $\mathrm{PbRL}$ leverages a variety of feedback forms from humans to guide the learning process. This class of $\mathrm{PbRL}$ method is often referred to as reinforcement learning from human feedback (RLHF) [Ziegler et al., 2019, Levine et al., 2018, Ouyang et al. 2022].

Despite their effectiveness, these methods necessitate fine-tuning pre-trained models to align with user preferences, introducing several challenges such as scalability, accessibility, inefficiency, and the need for model modifications. For instance, with black-box APIs like GPT-4 [OpenAI et al., 2024], customization based on user preferences is constrained due to restricted access to the underlying model. Moreover, even if fine-tuning were feasible, the large model size results in inefficient training and high resource consumption. Aligning black-box models with user preferences remains an under-explored area in research, despite its critical importance and growing demand.

In this line of research, we propose Preference Flow Matching (PFM), which redefines the integration of human preferences by directly learning a preference flow from the less preferred data to the more preferred ones. This direct modeling of preference flows allows our system to better characterize and replicate the marginal distribution of the favored outcomes. We adopt a novel flow matching[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-02.jpg?height=561&width=1263&top_left_y=218&top_left_x=428)

RLHF Methods

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-02.jpg?height=475&width=528&top_left_y=283&top_left_x=430)

Flow Matching (Training)

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-02.jpg?height=431&width=256&top_left_y=286&top_left_x=1051)

Flow Matching Model

$v_{\theta}(t, y \mid x)$
Flow Matching (Inference)

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-02.jpg?height=431&width=247&top_left_y=283&top_left_x=1427)

Flow Matching Model

$v_{\theta}(t, y \mid x)$

Figure 1: Illustration of our PFM framework. In the typical RLHF scenarios (left), we first sample preference data from the supervised fine-tuned (SFT) reference model. A reward model is learned from the collected dataset, either implicitly (as in DPO) or explicitly. The reward model is then used to fine-tune the reference policy to obtain the final model. Our method directly learns the preference flow from the collected preference data, where the flow is represented as a vector field $v_{\theta}$ (middle). For inference, we again sample a point from the reference policy, and improve the quality of alignment by using the trained flow matching model, without the need of fine-tuning the existing reference model (right).

framework [Lipman et al. 2022], which is a simple, intuitive, yet relatively under-explored method for preference alignment. By simply adding a preference flow matching module to black-box models, PFM eliminates the need for fine-tuning the black-box model itself, providing a significant advantage.

Additionally, our method offers a highly robust approach for preference alignment, by circumventing the need for explicit or implicit reward function estimation. In typical RLHF scenarios, a model is initially trained to approximate a reward function based on human preferences. This reward model is then used to guide the policy learning process, aiming to align agent behaviors more closely with human preferences. However, this approach can introduce complexities and potential biases in translating human preferences into numerical rewards. In particular, learned reward models can often overfit the ground truth preference model, especially in the finite data regime [Azar et al. 2023]. Recent advancements such as Direct Preference Optimization(DPO) [Rafailov et al., 2024] address the complexities of RLHF by eliminating the need for reward learning. However, these methods still inherently optimize for the reward model, and hence they are also susceptible to reward overfitting. In contrast, PFM directly learns preference flow, thereby removing the need for any reward model assumptions and resolving the challenges associated with the reward model learning.

We prove both theoretically and empirically that our method is able to learn an object that is similar to the standard RLHF objectives, while being robust to the preference overfitting observed in traditional RLHF pipelines. We also demonstrate how we can further improve the quality of alignment via iterative flow matching, with theoretical guarantees. Experimentally, we find that while typical RLHF methods and DPO suffer from preference overfitting, our method can robustly align with preference and still achieve comparable performances.

## 2 Preliminaries

### 2.1 Reinforcement Learning from Human Feedback (RLHF)

Reinforcement learning from human feedback generally begins with obtaining a pre-trained reference policy $\pi_{\text {ref }}$ that can generate samples $y \sim \pi_{\text {ref }}(\cdot \mid x)$ given a context $x$. For example, a context $x$ could be a text prompt given by a user, and the sample $y$ could represent an appropriate text response generated by the reference policy $\pi_{\text {ref }}$. We then collect a dataset of $N$ preference pairs $\mathcal{D}=\left\{\left(x_{i}, y_{i}^{+}, y_{i}^{-}\right)\right\}_{i=1}^{N}$, where each $x_{i}$ denotes the context, and each $y_{i}^{+}, y_{i}^{-} \sim \pi_{\mathrm{ref}}\left(\cdot \mid x_{i}\right)$ are generated responses to $x_{i}$ and marked as good or bad samples, respectively. Here, we assume that the preference $y_{i}^{+}>y_{i}^{-}$is generated from a ground-truth reward $r^{*}: X \times Y \rightarrow \mathbb{R}>0$, where $X$ and $Y$ are context space and response space, respectively. The goal of general RLHF is to recover an
optimal policy $\pi^{*}$ such that

$$
\begin{equation*}
\pi^{*}=\underset{\pi}{\operatorname{argmax}} \mathbb{E}_{x}\left(\mathbb{E}_{y \sim \pi(\cdot \mid x)}\left(r^{*}(x, y)\right)-\beta \mathbb{D}_{\mathrm{KL}}\left(\pi(\cdot \mid x) \| \pi_{\mathrm{ref}}(\cdot \mid x)\right)\right) \tag{1}
\end{equation*}
$$

RLHF pipelines generally require reward learning. One of the most popular choices of the reward model is the Bradley-Terry model [Bradley and Terry, 1952], which assumes that the preference $y^{+}>y^{-}$is generated from the probability $\mathbb{P}\left(y^{+}>y^{-} \mid x\right)=\sigma\left(r^{*}\left(x, y^{+}\right)-r^{*}\left(x, y^{-}\right)\right)$, where $\sigma$ is the logistic function. Under this model, the general RLHF framework learns the reward model $r_{\phi} \approx r^{*}$ by minimizing the negative log-likelihood:

$$
\begin{equation*}
\mathcal{L}_{R}(\phi ; \mathcal{D}):=-\mathbb{E}_{\left(x, y^{+}, y^{-}\right) \sim \mathcal{D}}\left(\log \sigma\left(r_{\phi}\left(x, y^{+}\right)-r_{\phi}\left(x, y^{-}\right)\right)\right) \tag{2}
\end{equation*}
$$

Once the reward model $r_{\phi}$ is trained, we then use it to optimize for 1 ) to obtain $\pi_{\theta} \approx \pi^{*}$ using standard reinforcement learning algorithms.

There is also a class of reward-free methods that eliminates the need of reward learning phase Rafailov et al., 2024, Azar et al., 2023]. Direct Policy Optimization (DPO) Rafailov et al., 2024] is a representative reward-free method that optimizes for (1) directly without learning a reward model. Although being a reward-free method, DPO implicitly optimizes for the reward function as in $\sqrt{22}$, by replacing $\hat{r}_{\theta}(x, y)=\beta \log \left(\pi_{\theta}(y \mid x) / \pi_{\text {ref }}(y \mid x)\right)$ as the implicit reward estimate.

### 2.2 Flow Matching

Flow matching is a class of generative model, where given a prior distribution $p_{0}$, we aim to model a target distribution $p_{1}$ from $p_{0}$. A key difference of the flow matching to the other generative models is that the prior $p_{0}$ can be an arbitrary distribution, (diffusion, for example, starts from a Gaussian prior $p_{0}$ ) and that the flow matching algorithm learns to modify the prior distribution $p_{0}$ to the target distribution $p_{1}$ with a neural network.

Throughout, we consider a pair of data distributions over $\mathbb{R}^{d}$ with densities $y^{-} \sim p_{0}$ and $y^{+} \sim p_{1}$, possibly unknown (but able to sample). The flow matching considers the task of fitting a mapping $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ that transforms $p_{0}$ to $p_{1}$, that is, if $y^{-} \sim p_{0}$, then $f\left(y^{-}\right) \sim p_{1}$. Inspired as in the motivation for the diffusion models, one can define a smooth time-varying vector field $u:[0,1] \times \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ that defines an ordinary diffusional equation (ODE),

$$
\begin{equation*}
d y=u(t, y) d t \tag{3}
\end{equation*}
$$

where we use the notation $u(t, y)$ interchanably with $u_{t}(y)$. Denote the solution of the above ODE by $\phi(t, y)$ (or $\phi_{t}(y)$ ) with initial condition $\phi_{0}(y)=y$. In other words, $\phi_{t}(y)$ is the point $y$ transported along the vector field $u$ from time 0 to $t$. In order to obtain samples from the target distribution $p_{1}$, we simply compute $\phi_{1}(y)$ where $y \sim p_{0}$. The integration map $\phi_{t}$ induces a pushforward measure $p_{t} \triangleq\left[\phi_{t}\right]_{\sharp}\left(p_{0}\right)$, which is the density of points $y \sim p_{0}$ transported via $u$ from 0 to $t$.

To train the vector field $v_{\theta}$ with neural network that mimics the vector field $u$ of our interest, we can solve for the conditional flow matching objective, as proposed by Lipman et al. [2022]:

$$
\begin{equation*}
\mathcal{L}_{\mathrm{CFM}}(\theta) \triangleq \mathbb{E}_{t \sim[0,1], z \sim q(\cdot), y \sim p_{t}(\cdot \mid z)}\left(\left\|v_{\theta}(t, y)-u_{t}(y \mid z)\right\|^{2}\right) \tag{4}
\end{equation*}
$$

where $q(z)=\pi\left(y^{-}, y^{+}\right)$is some coupled distribution of samples $y^{-}, y^{+}$and $u_{t}(y \mid z)=y^{+}-y^{-}$ is a straight path from a source sample to a target sample. The conditional distribution $q(z)$ can be chosen to be a independent coupling of source and target distribution $q(z)=p_{0}\left(y^{-}\right) p_{1}\left(y^{+}\right)$Lipman et al., 2022], or the 2-Wasserstein optimal transport plan as proposed by Tong et al. [2023].

## 3 Preference Flow Matching

In this section, we describe how we can use flow matching to learn (human) preference. In the first subsection, we illustrate our flow matching framework for learning preference, and compare it with typical RLHF pipelines. Then in Section 3.2, we demonstrate our method in a simple 2-dimensional toy experiment. Finally in Section 3.3 , we provide an extension of our framework that can iteratively improve the performance, with theoretical guarantees.

### 3.1 Flow Matching for Preference Learning

Instead of trying to optimize for the unknown reward $r^{*}$ or the preference probability model $\mathbb{P}\left(y^{+}>\right.$ $\left.y^{-} \mid x\right)$, we simply learn a flow from the marginal distribution of less preferred data $p_{0}\left(y^{-} \mid x\right)$ to the marginal distribution of more preferred data $p_{1}\left(y^{+} \mid x\right)$ by leveraging what is explicitly characterized in the collected preference data:

$$
\begin{align*}
p_{0}\left(y^{-} \mid x\right) & \propto \pi_{\mathrm{ref}}\left(y^{-} \mid x\right) \int \pi_{\mathrm{ref}}(y \mid x) \mathbb{P}\left(y>y^{-} \mid x\right) d y  \tag{5}\\
p_{1}\left(y^{+} \mid x\right) & \propto \pi_{\mathrm{ref}}\left(y^{+} \mid x\right) \int \pi_{\mathrm{ref}}(y \mid x) \mathbb{P}\left(y^{+}>y \mid x\right) d y  \tag{6}\\
& =\pi_{\mathrm{ref}}\left(y^{+} \mid x\right) \mathbb{E}_{y \sim \pi_{\mathrm{ref}}(\cdot \mid x)}\left(\mathbb{P}\left(y^{+}>y \mid x\right)\right) \tag{7}
\end{align*}
$$

In other words, we view that our collected data $\mathcal{D}$ is in fact generated from each of the marginal distributions $y^{-} \sim p_{0}(\cdot \mid x)$ and $y^{+} \sim p_{1}(\cdot \mid x)$ obtained from $\mathbb{P}\left(y^{+}>y^{-} \mid x\right)$, respectively. Hence, following the conventions in the literature, [Tong et al., 2023] we define the flow matching objective for preference dataset $\mathcal{D}$ as follows:

$$
\begin{equation*}
\mathcal{L}(\theta)=\mathbb{E}_{t \sim[0,1], z \sim \mathcal{D}, y \sim p_{t}(\cdot \mid z)}\left(\left\|v_{\theta}(t, y \mid x)-u_{t}(y \mid z)\right\|^{2}\right) \tag{8}
\end{equation*}
$$

where we define the condition $z=\left(x, y^{+}, y^{-}\right)$, conditional flow $u_{t}(y \mid z)=y^{+}-y^{-}$, and the probability path $p_{t}(y \mid z)=\mathcal{N}\left(y \mid t y^{+}+(1-t) y^{-}, \sigma^{2}\right)$. Once we obtain the vector field $v_{\theta}$, we can improve upon the generated negative samples $y^{-} \sim p_{0}(\cdot \mid x)$ by solving $\left.\sqrt{3}\right)$ using an off-the-shelf numerical ODE solver [Runge, 1895, Kutta, 1901] to obtain samples $f\left(y^{-}\right) \sim p_{1}$. Specifically, we start from a sample $y^{-}$with $t=0$, and "flow" along the ODE trajectory using $v_{\theta}$ until $t=1$, to arrive at the target $y^{+}$. Detailed algorithm can be found in Algorithm 1 Notably, generating improved samples can be done without the need of fine-tuning the existing model, since we learn a separate vector field that transports negative samples from $p_{0}$ to $p_{1}$. Furthermore, we did not require any assumption for the probability model $\mathbb{P}\left(y^{+}>y^{-} \mid x\right)$, so our method can extend to general scenarios that do not adopt the Bradley-Terry model. Our method is outlined in Figure 1 .

A careful reader might noticed that for inference, we require negative samples $y^{-}$from the marginal distribution $p_{0}$, to obtain aligned samples $y^{+}$. However, this $p_{0}$ is inaccessible during inference step, as we must first acquire preference label $y^{+}>y^{-}$for samples generated from $\pi_{\text {ref }}$. Instead, we simply start from $y \sim \pi_{\text {ref }}$ as the starting point, and apply flow matching to obtain $f(y) \approx y^{+} \sim p_{1}$. We emphasize that PFM can still robustly generate positive samples, if we assume non-deterministic preferences i.e., $\operatorname{supp}\left(p_{1}\right) \supseteq \operatorname{supp}\left(p_{0}\right)$. We also empirically find that using $\pi_{\text {ref }}$ instead of $p_{0}$ as the source distribution can produce comparable results in practical scenarios. Further details can be found in Appendix B

### 3.2 Illustrative Example: Preference Generated from 8-Gaussians Density

Here, we demonstrate how our method learns to improve generated samples to better align with the preference, in a simple 2-dimensional toy experiment. We consider a ground truth reward function generated from an 8-Gaussians density as illustrated in Figure 2a. We then pre-train a Gaussian mixture model to obtain samples as in Figure 2c. The pairwise preference labels are then generated using the ground truth 8 -Gaussians reward function, as done in many existing preference-based reinforcement learning (PbRL) settings [Christiano et al. 2017, Ibarz et al., 2018, Shin et al. 2023].

Once preference data are collected, we first learn a reward model $\widehat{r}_{\phi}$ via $\sqrt{2}$. As can be seen in Figure $2 \mathrm{~b}$ the learned reward model overfits in the unseen region, which causes the RLHF method to diverge (Figure 2e). DPO also fails to learn the correct preference, as can be seen in Figure $2 \mathrm{f}$ We note here that DPO is also subjective to the reward overfitting since DPO also implicitly learns to optimize for the reward using the Bradley-Terry model (2) [Xu et al., 2024, Azar et al., 2023].

However, PFM is free of such reward overfitting issues, as we do not optimize for the reward function using the Bradley-Terry model. Unlike other RLHF methods, our model robustly learns to align with the preference from the provided dataset (Figure 2g). Notably, our method does not try to overfit beyond the unseen region, since the learned target distribution from the flow matching model tries to mimic the distribution $p_{1}\left(y^{+}\right)$of collected preferred samples. (Compare Figure $2 \mathrm{~d}$ and Figure $2 \mathrm{~g}$ )

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-05.jpg?height=296&width=331&top_left_y=237&top_left_x=385)

(a) Ground-truth reward

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-05.jpg?height=263&width=320&top_left_y=584&top_left_x=379)

(e) RLHF

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-05.jpg?height=298&width=330&top_left_y=239&top_left_x=735)

(b) Learned reward model

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-05.jpg?height=262&width=352&top_left_y=590&top_left_x=710)

(f) DPO

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-05.jpg?height=257&width=352&top_left_y=278&top_left_x=1060)

(c) Pre-trained model

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-05.jpg?height=258&width=323&top_left_y=592&top_left_x=1069)

(g) PFM

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-05.jpg?height=263&width=344&top_left_y=275&top_left_x=1411)

(d) Preferred samples

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-05.jpg?height=266&width=339&top_left_y=588&top_left_x=1419)

(h) PFM (5 iter.)

Figure 2: Comparison of RLHF, DPO, and PFM on a 2-dimensional toy experiment. We generate preference labels from a ground truth reward in (a) and a pre-trained Gaussian reference policy (c). Both the RLHF (e) and DPO (f) methods struggle to align with the prefernce, due to overfitted reward model (b), even with the presence of $\mathrm{KL}$ regularizer $(\beta=1)$. PFM is able to mimic the distribution of the positively-labeled samples (d), and therefore achieves the highest performance $(\mathrm{g})$. Repeating PFM iteratively to the marginal samples can further improve the alignment with the preference (h).

### 3.3 Improving Alignment with Iterative Flow Matching

As done in iterative variants of DPO [Xiong et al., 2023, Yuan et al, 2024], we can also further improve the quality of alignment with iterative flow matching. Specifically, upon obtaining a marginal distribution $p_{1}$ by applying flow matching, we again collect a new preference data $y^{-}, y^{+}$from the obtained marginal distribution $p_{1}$ in (6). We repeat this process iteratively, by replacing the source distribution (which is $\pi_{\text {ref }}$ in the first step) with the marginal distribution $p_{1}$ obtained in the latest iteration. This iterative process can be summarized as follows.

$$
\begin{align*}
p_{0}^{(n)}\left(y^{-} \mid x\right) & \propto p_{1}^{(n-1)}\left(y^{-} \mid x\right) \int p_{1}^{(n-1)}(y \mid x) \mathbb{P}\left(y>y^{-} \mid x\right) d y  \tag{9}\\
p_{1}^{(n)}\left(y^{+} \mid x\right) & \propto p_{1}^{(n-1)}\left(y^{+} \mid x\right) \int p_{1}^{(n-1)}(y \mid x) \mathbb{P}\left(y^{+}>y \mid x\right) d y, \quad p_{1}^{(0)}=\pi_{\mathrm{ref}} \tag{10}
\end{align*}
$$

where we denote $p_{0}^{(n)}$ and $p_{1}^{(n)}$ to be the source and target distribution of the flow matching model at the $n$-th iteration, respectively. By repeatedly marginalizing the distribution with respect to the preference $\mathbb{P}\left(y^{+}>y^{-} \mid x\right)$, we can effectively "narrow" the sampling distribution towards the outputs with higher preference probability. See Figure 2 hfor the results of the iterative method in our toy experiment. Note that even during this iterative approach, we leave the parameters of the pre-trained model $\pi_{\text {ref }}$ untouched, and only require sampling from this model throughout the whole process. Later in Section 4 , we formally prove that the iterative method allows us to obtain a distribution class that maximizes the ground truth expected preference, and hence yields an optimal policy $\pi^{*}$ in 1 with $\beta=0$. See Theorem 4.2

## 4 Theoretical Analysis of Preference Flow

In this section, we theoretically analyze why PFM framework can effectively learn to align with the preference. Interestingly, learning to generate samples from the marginal distribution $p_{1}$ in 6 optimizes an objective that is similar to the goal of general RLHF in (11). Following the formulation provided by Azar et al. [2023], one can observe that the objective (1) is equivalent to the below form:

$$
\begin{equation*}
\pi^{*}=\underset{\pi}{\operatorname{argmax}} \mathbb{E}_{y \sim \pi}\left(\mathbb{E}_{y^{\prime} \sim \pi_{\mathrm{ref}}}\left(\sigma^{-1}\left(\mathbb{P}\left(y>y^{\prime}\right)\right)\right)\right)-\beta \mathbb{D}_{\mathrm{KL}}\left(\pi \| \pi_{\mathrm{ref}}\right) \tag{11}
\end{equation*}
$$

where $\sigma^{-1}(\xi)=\log (\xi /(1-\xi))$ is the logit function, and we drop the conditional dependence of $x$ for simplicity. Note DPO also optimizes for the same objective as in 11.

Let us take a step back, and characterize the failure modes of the RLHF and DPO frameworks, by figuring out when these methods overfits the reward function. Consider a simple example where the preferences are deterministic, i.e., $\mathbb{P}\left(y>y^{\prime}\right)=1$, so that $y$ is always preferred to $y^{\prime}$. If we plug it

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-06.jpg?height=46&width=1385&top_left_y=362&top_left_x=370)
ends up overfitting for the preference likelihood, resulting in a weak or even null KL regularization, regardless of the size of $\beta$.

Although in the case where the preference is not deterministic, this phenomenon can still be pronounced in the finite data regime [Azar et al. 2023]. Even if the true preference is strictly less than 1 , we may have access to only a few data samples to estimate $\mathbb{P}\left(y>y^{\prime}\right) \rightarrow 1$. This means that overfitting can be a critical issue in general, especially if the action space $Y$ or the context space $X$ is extremely large, as in the case of aligning large language models to human preferences.

In contrast, the PFM framework learns to generate a marginal distribution $p_{1}$. One can show that this marginal is a solution for the optimization problem that is similar to the objective 111.

Theorem 4.1 (Characterization of the Marginal). Let $\mathcal{D}=\left\{\left(x, y^{+}, y^{-}\right)\right\}$be a preference dataset collected from a pre-trained reference policy $\pi_{\mathrm{ref}}$, where the preference $y^{+}>y^{-}$is labeled with probability $\mathbb{P}\left(y^{+}>y^{-} \mid x\right)$. Let $p_{1}$ denote the marginal distribution of the positively-labeled samples $y^{+}$. Then the marginal distribution $p_{1}$ obtained from the preference model $\mathbb{P}\left(y>y^{\prime} \mid x\right)$ is an optimizer of the optimization problem

$$
\begin{equation*}
p_{1}=\underset{\pi}{\operatorname{argmax}} \mathbb{E}_{y \sim \pi}\left(\log \mathbb{E}_{y^{\prime} \sim \pi_{\mathrm{ref}}}\left(\mathbb{P}\left(y>y^{\prime}\right)\right)\right)-\mathbb{D}_{\mathrm{KL}}\left(\pi \| \pi_{\mathrm{ref}}\right) \tag{12}
\end{equation*}
$$

We defer the proof to the Appendix $[$. Similar to the RLHF and DPO objective $\sqrt{11}$, the solution $p_{1}$ of (12) drives the original distribution $\pi_{\text {ref }}$ towards the points where the preference probability $\mathbb{P}\left(y>y^{\prime}\right)$ is increasing. However, unlike the RLHF or DPO objectives, the objective 12 is bounded even in the deterministic case $\mathbb{P}\left(y>y^{\prime}\right)=1$, making it robust to reward overfitting.

Despite its robustness, one may notice that the objective $\sqrt{12}$ ) is less flexible compared to the original objective, due to the fixed regularization constant with $\beta=1$. Below, we show that if we apply the iterative algorithm provided in Section 3.3 , one can further reduce the $\mathrm{KL}$ regularization strength and obtain an optimal policy $\pi^{*}$ in 11, with $\beta \rightarrow 0$.

Theorem 4.2 (Convergence of Iterative Method). Assume $\pi_{\mathrm{ref}} \in L^{2}$ and $\mathbb{P}\left(y>y^{\prime}\right) \in L^{2}$. Consider an iterative update of the marginal distribution $p_{1}$ in $(10)$. Then, the iteration converges to the uniform distribution of points $y$ where the value $\mathbb{E}_{y^{-} \sim \pi_{\mathrm{ref}}}\left(\mathbb{P}\left(y>y^{-}\right)\right)$is the largest, i.e.,

$$
\begin{equation*}
p_{1}^{(\infty)} \rightarrow U\left(\left\{y: y \in \underset{y}{\operatorname{argmax}} \mathbb{E}_{y^{-} \sim \pi_{\mathrm{ref}}}\left(\mathbb{P}\left(y>y^{-}\right)\right)\right\}\right) \tag{13}
\end{equation*}
$$

where $U$ stands for uniform distribution, and we drop the conditional dependence of $x$ for simplicity.

We defer the proof to the Appendix $C$ Intuitively, the proof follows from the fact that the marginalization iteratively "narrows" down the distribution towards the outputs with higher preference. We note here that the $L^{2}$ assumptions are generally valid in practical domains. See Appendix $\mathrm{C}$

## 5 Experimental Results

In this section, we conduct experiments to address the following questions: Q1. Can PFM align generated samples from the black-box model with preference and achieve comparable results in practical tasks? Q2. Is PFM beneficial than methods optimizing for explicit/implicit reward model? and Q3. Is PFM beneficial than näive add-on methods, e.g., separately training generative models to imitate preferred samples? To answer these questions, we validate our method in two distinct domains: Conditional image generation task and offline reinforcement learning tasks.

### 5.1 Conditional Image Generation

We first evaluate PFM on a conditional image generation task using the MNIST dataset [LeCun et al. 1998]. Specifically, we utilize a pre-trained DCGAN [Radford et al. 2015] generator as $\pi_{\text {ref }}$ and collect sample pairs from $\pi_{\text {ref }}(\cdot \mid x)$ conditioned on the digit labels $x \in\{0, \cdots, 9\}$. To construct

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-07.jpg?height=323&width=349&top_left_y=253&top_left_x=365)

(a) Pretrained (0.8389)

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-07.jpg?height=317&width=347&top_left_y=633&top_left_x=369)

(e) RLHF (0.9171)

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-07.jpg?height=312&width=350&top_left_y=253&top_left_x=711)

(b) Rejected $y^{-}$(0.3951)

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-07.jpg?height=317&width=336&top_left_y=633&top_left_x=726)

(f) DPO (0.8936)

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-07.jpg?height=309&width=339&top_left_y=257&top_left_x=1072)

(c) Preferred $y^{+}(0.9976)$

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-07.jpg?height=319&width=336&top_left_y=632&top_left_x=1073)

(g) DPO $\beta \ll 1$ (0.5397)

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-07.jpg?height=320&width=339&top_left_y=257&top_left_x=1419)

(d) PFM $(0.9841)$

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-07.jpg?height=317&width=333&top_left_y=633&top_left_x=1422)

(h) Iterative PFM (0.9996)

Figure 3: Comparison of RLHF, DPO, and PFM on a conditional MNIST image generation task. Numbers represent the preference score. PFM (d) demonstrates superior sample quality and preference alignment compared to RLHF (e) and DPO (f), where DPO collapses with a small size of $\beta(\mathrm{g})$. The iterative PFM with only two iterations (h) results in almost perfectly aligning with the preferences.

preference datasets, we assign preferences to sample pairs according to the softmax probabilities of the labels from a LeNet [LeCun et al. 1998]. Then, we learn a PFM flow $v_{\theta}$ that transports $y^{-}$to $y^{+}$ given a condition $x$. More experimental details are provided in the Appendix D

Figure $3 \mathrm{a}$ illustrates the generated samples from $\pi_{\text {ref }}$, and the rejected and preferred images are depicted in Figure 3b and Figure 3c, respectively, where the values in parenthesis are the measured preference score. As shown in Figure 3d, PFM achieves higher preference alignment and better sample quality than RLHF (Figure 3e) and DPO (Figure 3f without fine-tuning $\pi_{\text {ref }}$. Moreover, PFM achieves nearly perfect alignment with the preferences after only two iterations (Figure 3h), demonstrating the effectiveness of iterative PFM.

### 5.2 Offline Reinforcement Learning

Next, we employ the D4RL [Fu et al., 2020] benchmark to assess the performance of PFM in reinforcement learning tasks. Following the prior works on the PbRL literature, we adopt trajectorybased preference alignment [Hejna et al., 2023, Kim et al., 2023]. We first randomly choose an starting state $s_{0} \sim S$, and sample two trajectories $\tau^{+}$and $\tau^{-}$with fixed length $\ell \geq 2$ from $\pi_{\text {ref }}$ :

$$
\begin{align*}
& \tau^{+}:=\left(a_{0}, a_{1}, \cdots, a_{\ell}\right) \sim \pi_{\mathrm{ref}}\left(\cdot \mid s_{0}\right)  \tag{14}\\
& \tau^{-}:=\left(a_{0}^{\prime}, a_{1}^{\prime}, \cdots, a_{\ell}^{\prime}\right) \sim \pi_{\mathrm{ref}}\left(\cdot \mid s_{0}\right) \tag{15}
\end{align*}
$$

Then, we obtain the preference $\tau^{+}>\tau^{-}$given the starting state $s_{0}$ using a scripted teacher approach that has also been widely adopted in the PbRL settings [Lee et al., 2021, Kim et al., 2023], which prioritizes trajectories with higher rewards based on the ground truth reward. For inference at a given state $s_{t}$, we again sample an action trajectory $\tau=\left(a_{t}, \cdots, a_{t+\ell}\right)$ from $\pi_{\mathrm{ref}}\left(\cdot \mid s_{t}\right)$, and apply flow matching to obtain a better action sequence.

The baseline methods for comparing the performance of PFM includes behavior cloning (BC), which we adopt as our pre-trained reference model $\pi_{\text {ref }}$, and a DPO fine-tuned model from the BC model. Additionally, we train a separate behavior cloning model to the collected preferred samples $y^{+} \sim p_{1}$, aiming to replicate the marginal distribution of the "good" trajectories. Further experimental details are deferred to Appendix D

Table 1 presents the outcomes from evaluations conducted on 12 offline datasets. Our findings indicate that PFM consistently demonstrates comparable or even superior performance with lower variance across all baseline methods. Notably, our method excels particularly in datasets generated

Table 1: Normalized results on MuJoCo datasets. Mean and standard deviation from 5 seeds are reported.

|  | Pretrained (BC) | DPO Fine-tuned | PFM (Ours) | Marginal BC |
| :--- | :--- | :--- | :--- | :--- |
| ant-random-v2 | $1.003 \pm 0.065$ | $1.000 \pm 0.103$ | $\mathbf{1 . 0 0 4} \pm 0.174$ | $0.726 \pm 6.166$ |
| ant-medium-v2 | $3.466 \pm 28.57$ | $3.671 \pm 18.52$ | $3.742 \pm 3.287$ | $\mathbf{3 . 8 6 6} \pm 2.084$ |
| ant-expert-v2 | $4.966 \pm 32.01$ | $\mathbf{5 . 3 5 0} \pm 5.002$ | $5.234 \pm 3.578$ | $3.850 \pm 46.20$ |
| hopper-random-v2 | $0.083 \pm 0.261$ | $0.085 \pm 0.256$ | $\mathbf{0 . 2 3 0} \pm 0.086$ | $0.158 \pm 4.589$ |
| hopper-medium-v2 | $1.699 \pm 5.173$ | $1.720 \pm 4.039$ | $\mathbf{1 . 8 9 2} \pm 2.696$ | $1.296 \pm 1.740$ |
| hopper-expert-v2 | $3.601 \pm 0.493$ | $3.609 \pm 0.951$ | $\mathbf{3 . 6 1 5} \pm 0.790$ | $1.034 \pm 0.106$ |
| halfcheetah-random-v2 | $-0.001 \pm 0.020$ | $0.000 \pm 0.036$ | $\mathbf{0 . 0 0 0} \pm 0.012$ | $-0.006 \pm 0.083$ |
| halfcheetah-medium-v2 | $4.806 \pm 3.501$ | $4.927 \pm 2.659$ | $\mathbf{5 . 1 1 9} \pm 3.453$ | $4.536 \pm 4.998$ |
| halfcheetah-expert-v2 | $11.02 \pm 4.868$ | $\mathbf{1 1 . 1 6} \pm 2.999$ | $10.90 \pm 3.244$ | $0.312 \pm 9.799$ |
| walker2d-random-v2 | $0.069 \pm 0.152$ | $0.065 \pm 0.114$ | $0.083 \pm 0.190$ | $\mathbf{0 . 1 1 4} \pm 0.558$ |
| walker2d-medium-v2 | $2.772 \pm 26.36$ | $\mathbf{3 . 4 0 1} \pm 17.50$ | $3.334 \pm 22.94$ | $2.999 \pm 18.26$ |
| walker2d-expert-v2 | $\mathbf{4 . 9 8 8} \pm 0.568$ | $4.977 \pm 0.406$ | $4.976 \pm 0.306$ | $0.747 \pm 0.789$ |
| Random Average | 0.289 | 0.287 | $\mathbf{0 . 3 2 9}$ | 0.248 |
| Medium Average | 3.186 | 3.430 | $\mathbf{3 . 5 2 2}$ | 3.174 |
| Expert Average | 6.143 | $\mathbf{6 . 2 7 3}$ | 6.181 | 1.486 |
| D4RL Average | 3.206 | 3.330 | $\mathbf{3 . 3 4 4}$ | 1.636 |

![](https://cdn.mathpix.com/cropped/2024_06_04_a70db82fbafc41fe4cd7g-08.jpg?height=284&width=1332&top_left_y=1105&top_left_x=388)

Figure 4: Analysis of a sample episode of a DPO fine-tuned model on the MuJoCo ant environment. DPO finetuned model often over-estimates the reward due to reward overfitting (e.g., $t=196$ ). This can cause the policy to choose problematic actions. Here, the implicit reward estimation is $\hat{r}_{\theta}(s, a)=\beta \log \left(\pi_{\theta}(a \mid s) / \pi_{\mathrm{ref}}(a \mid s)\right)$.

from suboptimal behavioral policies, achieving better performance. Furthermore, PFM manages to match the performance on expert datasets even in the absence of a reward model, underscoring its robustness and effectiveness. This demonstrates that PFM can effectively align black-box models with preference through flow matching, without the need to fine-tune the pre-trained model.

### 5.3 Is Learning a Flow Truly Beneficial?

In this remaining section, we focus on answering the remaining questions, $Q 2$ and $Q 3$. We first investigate why PFM can be advantageous over previous methods with explicit/implicit reward modeling. As can be seen in Figure 4, DPO, like typical RLHF approaches, is also prone to reward overfitting, and may cause the agent to fail. This is because if the preference estimate is close to 0 or 1 , these methods may end up overoptimizing to the exploding reward model [Ziegler et al., 2019. Gao et al. 2023|. PFM, on the other hand, is inherently robust to such over-estimation, as we adopt a completely different optimization framework that does not require a reward proxy. (See Theorem 4.1 )

On the other hand, we observe less performance gain in the expert datasets. This is a possible failure mode of PFM, where the generated samples are already near-optimal. In such regime, an arbitrary source $y \sim \pi_{\text {ref }}$ has a near zero probability of being sampled from the true marginal $p_{0}$, suggesting that PFM with prior as $\pi_{\text {ref }}$ might suffer from a shifted source distribution. We verify this experimentally on walker2d, where PFM struggles the most. By adopting a true marginal $p_{0}$ as the source, PFM with prior $p_{0}$ can achieve the highest performance among all baselines. This performance is evident even on the expert dataset, matching our theoretical analysis. See Appendix B

Next, we compare PFM to an alternative approach that simply tries to approximate the marginal distribution directly from the positive samples. Intuitively, training a generative model from scratch
that replicates the marginal $p_{1}$ is as computationally costly as training the original reference model. Experimentally, we observe that PFM achieves better performance than training a behavior cloning model (Marginal BC) to replicate the distribution of the preferred samples (Table 1). However, it is also worth mentioning that the Marginal BC model does occasionally yield the best results, suggesting the potential of using a marginal distribution for preference alignment.

## 6 Related Works

Contrastive Preference Learning (CPL) [Hejna et al., 2023] is a class of reward-free methods that utilizes contrastive learning techniques to align model outputs with the preferences observed in the dataset. By leveraging contrastive loss, CPL encourages the model to distinguish between preferred and less preferred outcomes effectively. Flow-to-Better (FTB) [Zhang et al., 2023] innovatively uses a diffusion model to transition from less preferred data to more preferred data, similar to the flow-based approach in our work. However, FTB mainly focuses on data augmentation, where they used the trained diffusion model to generate more data samples for behavior cloning. Despite their strengths, both works rely on the Bradley-Terry model to implicitly learn the reward function.

Identity Preference Optimization (IPO) [Azar et al. 2023] builds upon the foundation laid by DPO, extending the framework to accommodate a broader range of preference models beyond the BradleyTerry paradigm. In particular, they focus on finding an objective that is bounded even in a deterministic preference regime, by replacing the function $\sigma^{-1}$ in 11) with an identity function $I(x)=x$. This effectively mitigates the reward overfitting problem observed in DPO and standard RLHF methods.

Our method distinguishes itself from these approaches by not requiring the Bradley-Terry assumption nor the fine-tuning of pre-trained models. This eliminates the risk of reward overfitting associated with the Bradley-Terry model and reduces the computational cost significantly. By avoiding the need for fine-tuning, our method offers a more efficient and scalable solution for integrating human preferences into reinforcement learning systems. This makes our approach particularly suitable for scenarios where computational resources are limited or where quick adaptation to human feedback is essential. The comparison of these related works is summarized in Table 2

Table 2: Comparison of our method to other works.

|  | RLHF | DPO | IPO | CPL | FTB | PFM (Ours) |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Reward Model Free | $x$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| No Reward Assumptions (e.g. BT) | $x$ | $x$ | $\checkmark$ | $x$ | $x$ | $\checkmark$ |
| Applicable to Black-Box Models | $x$ | $x$ | $x$ | $x$ | $\checkmark$ | $\checkmark$ |

## 7 Conclusion and Limitations

In conclusion, this research introduces Preference Flow Matching (PFM), a novel add-on approach that offers a practical, efficient, and scalable solution for integrating human preferences. This research highlights the potential of flow matching as a powerful tool for preference alignment and opens new avenues for further exploration and development in the field of RLHF. The ability to align black-box models with human preferences without extensive model modifications marks a critical step forward, with broad implications for the deployment and usability of AI systems in real-world applications.

Our theoretical and empirical analyses demonstrate that PFM achieves alignment performance comparable to standard RLHF methods while being more resilient to preference overfitting. The iterative flow matching technique further enhances alignment quality, by continually refining the preference alignment without modifying the underlying pre-trained model parameters. This iterative refinement, grounded in the flow matching framework, ensures that the sampling distribution increasingly favors higher preference probabilities, leading to a robust and optimal policy.

Despite these promising results, the current design of the PFM framework entails several challenges and limitations. In this work, we mainly focused on classical PbRL settings [Kim et al. 2023, Lee et al. 2021, Zhu et al. 2024], where we adopt the scripted teacher as a preference model. Although theoretically generalizable, our experiments did not cover human preference scenarios, which we leave as a future work. Further experiments on more complex tasks like general contextual image generation are also desirable as future directions.

Another notable limitation is its applicability to the natural language processing (NLP) domain. The PFM framework, as currently designed, cannot be directly applied to NLP tasks because the source and target texts may have different lengths. Future research should explore ways to adapt the PFM framework for variable-length data, potentially through innovative alignment techniques or alternative frameworks suited for text generation tasks.

## Acknowledgements

We thank Taehyeon Kim at KAIST for pointing out the strengths of our work and providing motivations of this work. We would also like to appreciate Sihyeon Kim and Yongjin Yang at KAIST for providing experimental details and suggesting relevant settings for PbRL tasks. Finally, we thank Junghyun Lee at KAIST for revising the details of the proofs of theorems.

## References

Riad Akrour, Marc Schoenauer, and Michele Sebag. Preference-based policy learning. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2011, Athens, Greece, September 5-9, 2011. Proceedings, Part I 11, pages 12-27. Springer, 2011.

Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036, 2023.

Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2020.

Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835-10866. PMLR, 2023.

Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W Bradley Knox, and Dorsa Sadigh. Contrastive prefence learning: Learning from human feedback without rl. arXiv preprint arXiv:2310.13639, 2023.

Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning from human preferences and demonstrations in atari. Advances in neural information processing systems, 31, 2018.

Changyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. Preference transformer: Modeling human preferences using transformers for rl. arXiv preprint arXiv:2303.00957, 2023.

Wilhelm Kutta. Beitrag zur näherungsweisen Integration totaler Differentialgleichungen. Teubner, 1901 .

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.

Kimin Lee, Laura Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. arXiv preprint arXiv:2106.05091, 2021.

Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning handeye coordination for robotic grasping with deep learning and large-scale data collection. The International journal of robotics research, 37(4-5):421-436, 2018.

Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.

OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730$27744,2022$.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.

Carl Runge. Über die numerische auflösung von differentialgleichungen. Mathematische Annalen, 46(2):167-178, 1895 .

Daniel Shin, Anca D Dragan, and Daniel S Brown. Benchmarks and algorithms for offline preferencebased reward learning. arXiv preprint arXiv:2301.01392, 2023.

Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023.

Aaron Wilson, Alan Fern, and Prasad Tadepalli. A bayesian approach for policy learning from trajectory preference queries. Advances in neural information processing systems, 25, 2012.

Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under klconstraint. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.

Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint arXiv:2404.10719, 2024.

Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.

Zhilong Zhang, Yihao Sun, Junyin Ye, Tian-Shuo Liu, Jiaji Zhang, and Yang Yu. Flow to better: Offline preference-based reinforcement learning via preferred trajectory generation. In The Twelfth International Conference on Learning Representations, 2023.

Tianchen Zhu, Yue Qiu, Haoyi Zhou, and Jianxin Li. Decoding global preferences: Temporal and cooperative dependency modeling in multi-agent preference-based reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17202-17210, 2024.

Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.
