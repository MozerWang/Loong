# Treatment Effect Estimation for User Interest Exploration on Recommender Systems 

Jiaju Chen<br>cjj01@mail.ustc.edu.cn<br>University of Science and Technology<br>of China<br>Hefei, China<br>Peng Wu<br>pengwu@btbu.edu.cn<br>Beijing Technology and Business<br>University<br>Beijing, China

Wenjie Wang*<br>wenjiewang96@gmail.com<br>National University of Singapore<br>Singapore<br>Jianxiong Wei<br>weijianxiong@meituan.com<br>Meituan<br>Beijing, China

Chongming Gao<br>chongming.gao@gmail.com<br>University of Science and Technology<br>of China<br>Hefei, China<br>Qingsong Hua<br>huaqingsong@meituan.com<br>Meituan<br>Beijing, China


#### Abstract

Recommender systems learn personalized user preferences from user feedback like clicks. However, user feedback is usually biased towards partially observed interests, leaving many users' hidden interests unexplored. Existing approaches typically mitigate the bias, increase recommendation diversity, or use bandit algorithms to balance exploration-exploitation trade-offs. Nevertheless, they fail to consider the potential rewards of recommending different categories of items and lack the global scheduling of allocating top- $N$ recommendations to categories, leading to suboptimal exploration. In this work, we propose an Uplift model-based Recommender (UpliftRec) framework, which regards top- $N$ recommendation as a treatment optimization problem. UpliftRec estimates the treatment effects, i.e., the click-through rate (CTR) under different category exposure ratios, by using observational user feedback. UpliftRec calculates group-level treatment effects to discover users' hidden interests with high CTR rewards and leverages inverse propensity weighting to alleviate confounder bias. Thereafter, UpliftRec adopts a dynamic programming method to calculate the optimal treatment for overall CTR maximization. We implement UpliftRec on different backend models and conduct extensive experiments on three datasets. The empirical results validate the effectiveness of UpliftRec in discovering users' hidden interests while achieving superior recommendation accuracy.


## CCS CONCEPTS

## - Information systems $\rightarrow$ Recommender systems.

*Corresponding author. This work is supported by the National Key Research and Development Program of China (2022YFB3104701), the National Natural Science Foun dation of China (62272437), and the CCCD Key Lab of Ministry of Culture and Tourism[^0]

## KEYWORDS

Recommender Systems, User Interest Exploration, Treatment Effect Estimation, Multivariate Continuous Treatments

## ACM Reference Format:

Jiaju Chen, Wenjie Wang, Chongming Gao, Peng Wu, Jianxiong Wei, and Qingsong Hua. 2024. Treatment Effect Estimation for User Interest Exploration on Recommender Systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24), July 14-18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3626772.3657736

## 1 INTRODUCTION

Recommender systems are widely deployed for personalized information filtering. Technically speaking, they learn personalized user preferences from user feedback (e.g., clicks and likes). However, user feedback is intrinsically biased to partially observed user interests, leaving many user interests unexplored. Consequently, recommender models learned from such biased feedback will favor partially observed user interests and aggravate the bias in the long run due to the feedback loop [17, 24], gradually leading to the issues such as filter bubbles and echo chambers [9, 42]. Therefore, it is essential to explore users' hidden interests.

Existing work on user interest exploration falls into three groups:

- Debiased methods alleviate the effect of biased user feedback via various reweighting or causal techniques [5, 8, 31, 43], such as propensity weighting [31] and backdoor adjustment [49]. However, this line of research might strengthen the minority interests while it is hard to actively discover new interests.
- Diversity-oriented methods $[4,37,39]$ aim to enrich the categories of recommended items and thus can find users' new interests. Nevertheless, such methods usually ignore the heterogeneous rewards of different item categories and probably recommend items that users dislike, leading to the sacrifice of recommendation accuracy and user satisfaction.
- Bandit algorithms $[22,23,38]$ consider the potential rewards for enhancing the exploration-exploitation trade-off. However, recommender systems usually recommend a list of items in a session while bandit algorithms only consider the policy of a single-item recommendation, lacking global scheduling and leading to inferior rewards in a period.

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-02.jpg?height=439&width=420&top_left_y=298&top_left_x=191)

(b) Deconfounding by cutting off the backdoor path

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-02.jpg?height=244&width=401&top_left_y=317&top_left_x=602)

Step 1: Estimate the treatment effect, i.e., the effect of exposure ratios of different item categories.

Step 2: Adjust the exposure ratios to maximize the overall treatment effects and user satisfaction.

(c) Illustration of the two-step UpliftRec framework
Figure 1: (a) Illustration of a user's satisfaction changing as the exposure ratio increases. (b) Illustration of the relationships of user features $X$, exposure ratios of categories $T$, and user satisfaction $Y$ from a causal view. (c) UpliftRec estimates the treatment effects and schedules the optimal exposure ratios to maximize the treatment effects.

To pursue superior solutions for user interest exploration, we reformulate the recommendation task from a causal view. Generally speaking, we regard the exposure ratios of each item category in the top- $N$ recommendations as the treatment and the click-through rate (CTR) of the category as the outcome, i.e., rewards. The objective of the recommender models is to maximize the potential outcome by optimizing the treatment. Existing recommender methods lack global scheduling for the treatments of multiple categories. They tend to over-recommend categories with more clicks, leading to an exaggeration of partially observed user interests while neglecting minority interests with potentially high rewards. To tackle this issue, we have two essential considerations: 1) the precise estimation of the treatment effects, specifically the expected CTR under different category exposure ratios for each user; 2) the identification of an optimal treatment strategy to maximize the overall potential outcome for each user.

Although the treatment effect estimation has been broadly studied, estimating such treatment effect for top- $N$ recommendations is non-trivial due to the following challenges. 1) Estimating the treatment effects of recommending some minority or unexplored item categories to a user may rely on randomized controlled experiments, i.e., recommending items with randomly allocated categories to this user and collecting the feedback. This is prohibitively expensive and impractical due to the potential negative impact on the user experience. 2) Using observational user feedback for effect estimation has intrinsic biases between the treatment and outcome due to the confounding effect of users' features [45], as shown in Figure 1(b). Besides, the effects of recommending unexposed item categories are missing. 3) Worst of all, top- $N$ recommendations are essentially a problem with multivariate continuous treatments, where the exposure ratio for each category is represented by a continuous value ranging from zero to one. Hence, estimating the effects for such multivariate continuous treatments, as formulated by the Average Dose-Response Function (ADRF) [18, 19], requires a significant amount of treatment-outcome samples.

To solve these challenges, we propose an Uplift model-based Recommender (UpliftRec) framework for user interest exploration, which estimates treatment effects across multiple categories and adjusts the treatment accordingly to maximize users' total CTR, as shown in Figure 1(c). Specifically, 1) UpliftRec only utilizes observational user feedback for treatment effect estimation, i.e., $\mathrm{ADRF}$ estimation. To quantify the effect on unexposed item categories (i.e., users' hidden interests), UpliftRec estimates the group-level effects conditional on similar user features instead of user-level estimation, and thus leverages collaborative filtering information for users' hidden interest exploration. Intuitively, some categories unobserved by a user could be assessed by other similar users. Besides, 2) to mitigate the confounding biases, UpliftRec employs the inverse propensity weighting (IPW) technique for unbiased ADRF estimation. Lastly, 3) to efficiently utilize the observed user feedback and alleviate the high requirements for data scale, we consider discretizing continuous treatments and clustering items into condensed groups for ADRF estimation.

Based on the estimated ADRF, we formulate the treatment selection as a linear optimization problem to maximize the overall CTR and obtain the optimal treatment assignment via dynamic programming. Furthermore, due to the high variance issue of ADRF, we also present a simplified effect estimation method based on the Marginal Treatment Effect Function (MTEF) [36]. We conduct extensive experiments on three datasets and compare the proposed method with various competitive baselines. The empirical results show the superior performance of our method in exploring users' new interests while enhancing overall recommendation accuracy. We release the code and data at https://github.com/Jiaju-Chen/UpliftRec.

To sum up, our contributions are threefold.

- We examine the task of user interest exploration from a causal view and highlight the importance of estimating users' treatment effects across multiple item categories.
- We contribute a novel UpliftRec framework, which can alleviate various challenges of estimating treatment effects, enabling reliable effect estimation and optimal treatment selection.
- Extensive experiments on three datasets validate the effectiveness of the proposed method in exploring new interests while improving the CTR performance.


## 2 PROBLEM FORMULATION

In this section, we first inspect the user interest exploration task. Then, we propose some basic assumptions and formulate the exploration task from a causal perspective.

- User Interest Exploration Task. In this era of information explosion, recommender systems take on the responsibility of information filtering by recommending users' liked items. In this work, we represent users' satisfaction by the total CTR of the recommendation lists, which is a critical metric in recommendation [11]. Intuitively, users favor the item categories that match their interests. However, the user feedback used for recommender training is usually biased to partial user preference, leaving many user interests unexplored. Besides, their interests in a certain category can be influenced by its exposure ratio, as shown in Figure 1(a). When the exposure ratio of a category is too low or too high, users may feel unsatisfied with the recommendations. As such, the key to the task lies in accurately estimating the CTR for different exposure ratios on item categories, including observed and unobserved user
interests. With accurate estimation, we can determine the ideal allocation of exposure resources to each category, enhancing users' overall CTR and satisfaction.
- Causal View of Recommendation Process. Following the analysis above, we now formulate the recommendation process from a causal view. For each user $u$, we define the category exposure ratio as the treatment $T_{u}: t_{u}=\left[t_{u, 1}, t_{u, 2}, \ldots, t_{u, C}\right]$, where $t_{u, c} \in[0,1], \sum_{c=1}^{C} t_{u, c}=1$, and $C$ is the category number. And we define the potential outcome by the corresponding CTR $Y_{u}(t)$ : $y_{u}(t)=\left[y_{u}\left(t_{1}\right), y_{u}\left(t_{2}\right), \ldots, y_{u}\left(t_{C}\right)\right]$. The observed outcome $y_{u}\left(t_{u}\right)$ indicates the CTR when taking treatment $t_{u}$ for user $u$. To measure the treatment effects with varying exposure ratios, we can utilize ADRF [19] and MTEF [36]. Formally,

$$
\begin{align*}
\operatorname{ADRF}_{u}(t) & =\mathbb{E}\left[y_{u}(t)\right] \\
& =\left[\mathbb{E}\left[y_{u}\left(t_{1}\right)\right], \mathbb{E}\left[y_{u}\left(t_{2}\right)\right], \ldots, \mathbb{E}\left[y_{u}\left(t_{C}\right)\right]\right]  \tag{1}\\
& \triangleq\left[\operatorname{ADRF}_{u}\left(t_{1}\right), \ldots, \operatorname{ADRF}_{u}\left(t_{C}\right)\right]
\end{align*}
$$

which is the expectation of CTR of each category given exposure treatment $t$ for user $u$.

$$
\begin{align*}
\operatorname{MTEF}_{u}(t) & =\frac{\mathbb{E}\left[y_{u}(t+\Delta t)\right]-\mathbb{E}\left[y_{u}(t)\right]}{\Delta t} \\
& =\frac{\operatorname{ADRF}_{u}(t+\Delta t)-\operatorname{ADRF}_{u}(t)}{\Delta t} \tag{2}
\end{align*}
$$

which is a gradient approximation of ADRF around exposure treatment $t$, where $\Delta t$ is the interval of $t$. For brevity, we omit the subscript $u$ in $t_{u}, y_{u}(t), \operatorname{ADRF}_{u}(t)$, and $\operatorname{MTEF}_{u}(t)$ in the following since all of these concepts are defined at the user level. With ADRF, we can formulate the task of finding the appropriate exposure ratios into a constrained optimization problem:

$$
\begin{array}{ll} 
& \max _{t=\left[t_{1}, t_{2}, \ldots, t_{C}\right]}  \tag{3}\\
\text { s.t. } & \sum_{c=1}^{C} t_{c} \cdot \operatorname{ADRF}\left(t_{c}\right) \\
& \sum_{c=1}^{C} t_{c}=1 ; t_{c} \geq 0 \\
& \left|t_{c}-t_{0, c}\right| \leq \epsilon
\end{array}
$$

where $t_{0}$ is a reference of category exposure ratios, possibly from the recommendation list of a backend recommender model, and $\epsilon$ controls the deviation acceptance from the reference. This constrained optimization problem can be solved by dynamic programming to calculate the best treatment (i.e., the best exposure ratios across categories). Based on the best treatment, we can adjust the number of recommended items in each category.

Additionally, we can estimate MTEF by ADRF and utilize MTEF to adjust the recommendation list of existing recommender models, which turns out to be an efficient and well-performed practice. Following [48], our method is based on several assumptions:

- Overlap. Every subject has a non-zero probability of receiving the treatment (i.e., $0<P(t \mid x)<1$ ) for all $t$ and $x$, where $x$ represents the observed covariate. In the context of recommender models, most systems incorporate randomness to ensure diversity, enabling that even sparse categories can have a probability of being recommended.
- SUTVA. The stable unit treatment value assumption (SUTVA) supposes that individuals do not interfere with each other. In most recommendation scenarios, users only see and give feedback to
Original user interactions

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-03.jpg?height=146&width=588&top_left_y=323&top_left_x=1229)

Figure 2: Illustration of generating a sample out of a real interaction trail, including history and treatment\&outcome.

their own recommendations without the interfere from others' recommendations, satisfying the SUTVA assumption.

- Unconfoundedness. The distribution of treatments is independent of the distribution of potential outcomes conditioned on a set of observed confounders. Here treatments represent exposure ratios of categories by recommender systems, mainly confounded by users' features and behaviors [35, 44]. Following previous work [2,35, 44], we also treat user features and interaction behaviors as the primary confounders to make the treatments independent with potential outcomes.


## 3 METHOD

In this section, we describe the process of estimating ADRF for the optimization in Eq. (3). Subsequently, we detail how to select the optimal treatment via Eq. (3) using dynamic programming. Finally, we present a more streamlined method achieved through the calculation of MTEF.

### 3.1 Treatment Effects Estimation

- Augmented Dataset Generation. To estimate the users' ADRF (i.e., expectation of CTR on each category), we need to observe the user feedback when applying specific treatments to each user. Based on observational users' interactions, we generate the samples for effect estimation in the tuple of interaction histories, treatments, and outcomes.

For each user $u$ with an interaction sequence $\hat{D}_{u}$, we generate a sample out of user $u$ with treatment and outcome data. We divide $\hat{D}_{u}$ into two parts, as is shown in Figure 2. We set the former $\lambda$ interactions as the user history part and use the latter $(1-\lambda$ ) interactions to obtain the treatment and outcome, where hyperparameter $\lambda(0<\lambda<1)$ is the ratio of dividing interaction trails. We then generate the samples with tuples of the following elements from every real user:

- History interactions $D_{u}$ are the former $\lambda$ interactions of $\hat{D}_{u}$. We use it to depict users' historical interactions before the treatment.
- Treatments $T_{u}$ are category exposure ratios in the latter $(1-\lambda)$ interactions of $\hat{D}_{u}$. We compute $T_{u}$ from items exposed to user $u$.
- Potential outcomes $Y_{u}(t)$ are the CTR in the latter $(1-\lambda)$ interactions of $\hat{D}_{u}$ across all categories. The CTR of category $c$ is the number of positive items of category $c$ divided by that of all exposed items of category $c$.

Considering that some recommendation datasets lack item category, before generating augmented datasets, we can cluster items using the k-means [13] algorithm based on item embeddings. The item embeddings can be acquired by any recommender models.

- Confounder Debiasing. When inferring treatment effects from observed interaction trails, it is possible that confounding factors (such as user features denoted as $X$ ) influence both the treatment

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-04.jpg?height=189&width=372&top_left_y=285&top_left_x=188)

(a) Optimums without constraints

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-04.jpg?height=195&width=374&top_left_y=285&top_left_x=648)

(b) Optimums with constraints

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-04.jpg?height=406&width=852&top_left_y=282&top_left_x=186)

Figure 3: Optain the new optimums under the constraint the ratios of different categories sum to 1 .

$T$ and the outcome $Y$. Consequently, this introduces bias into the causal relationship between $T$ and $Y$, as illustrated in Figure 1(b).To address this issue, we employ the IPW strategy, a well-known re-weighting method, to debias our estimations. Unlike methods that calculate the propensity score of a single item [31,35], we aim to estimate the propensity score $P(t \mid x)$, which represents the probability of the treatment $t$ given the user feature $x$. Through IPW, we devise a debiased estimation for ADRF and demonstrate its validity as shown below, where $y$ and $t_{c}$ represent the outcome and treatment of a particular category $c$ for user $u$, and $x$ denotes user $u$ 's feature. In our work, we represent user features using user history interactions $D_{u}$.

$$
\begin{align*}
\operatorname{ADRF}\left(t_{c}\right) & =\mathbb{E}\left[y\left(t_{c}\right)\right]=\sum_{x} P\left(y \mid t_{c}, x\right) P(x) \\
& =\sum_{x} \frac{P\left(y, t_{c} \mid x\right)}{P\left(t_{c} \mid x\right)} P(x)  \tag{4}\\
& =\mathbb{E}_{x}\left[\frac{P\left(y, t_{c} \mid x\right)}{P\left(t_{c} \mid x\right)}\right]
\end{align*}
$$

Thus, by incorporating the inverse propensity $P\left(t_{c} \mid x\right)$, we effectively block the backdoor path from $T_{c}$ to $Y$. However, due to the sparsity of the dataset, estimating $P\left(t_{c} \mid x\right)$ using an MLP proved challenging. Therefore, we turned to estimate the propensity using a clustering approach.

We obtain the embeddings of real users and their augmented samples by training the backend model with history interactions of all of them. By computing the cosine similarity between each real user and augmented samples, we identified the top $K_{p}$ samples with the highest similarity and used their discretized treatment exposure ratios to calculate the propensity $P(t \mid x)$. Through discretization, $P(t \mid x)$ can be represented by a $C *(K+1)$ matrix, where $C$ is the number of categories, and $K$ is the treatment-clip number. We divided the continuous category exposure distribution into $K$ slots to handle data sparsity, and $P[c]\left[\tilde{t}_{c}\right]$ represents the probability of the exposure ratio on category $c$ falling into the $\tilde{t}_{c}$-th slot.

$$
\begin{align*}
P[c]\left[\tilde{t}_{c}\right] & =\frac{\sum_{j=1}^{K_{p}} \mathbb{I}\left(D i s\left(t_{u_{j}}\right)=\tilde{t}_{c}\right)}{K_{p}}  \tag{5}\\
c & =1,2, \ldots, C ; \tilde{t}_{c}=0,1, \ldots, K
\end{align*}
$$

where Dis is the discretizing function mapping continuous ratios $T$ to discrete versions $\tilde{T}$. We set a lower threshold $v_{p}$ for the propensity score in case of overflow.

- ADRF Calculation. After preparing the augmented dataset and the propensity, we are now able to estimate users' ADRF. We derive the discrete-distribution form of ADRF for two reasons: 1) Utilizing a dynamic programming algorithm enables us to efficiently determine the best exposure distribution in linear time. 2) The optimal distribution must be discrete, considering we can only recommend integer numbers of items in the final recommendation list.

Therefore, we also represent ADRF in a matrix form with dimensions of $C \times(K+1)$, where $C$ is the category number, and $K$ is the treatment-clip number. We approach the optimization process as a knapsack problem, where the knapsack space has $K$ slots, and the value of each item is the CTR. As illustrated in Figure 3, each row of ADRF represents the CTR of category $c$ when exposed to different discrete ratios. In the discretized scenario, we convert the continuous constraint $\sum_{c=1}^{C} t_{c}=1$ into the discrete constraint $\sum_{c=1}^{C} \tilde{t}_{c}=K$, where $\tilde{t}_{c}$ represents the discretized treatment. Our goal is to optimize the total CTR (the sum of CTR of all items in the top- $N$ list) under the discrete constraint.

We retrieve the same embedding information as the propensity calculation process. By fetching the top $K_{s}$ samples with the largest cosine similarity, we calculate $\operatorname{ADRF}[c][k]$ using their average $y_{u_{j}}$ weighted by their inverse propensity $P\left(t_{c} \mid x_{u_{j}}\right)^{\gamma}$. Specifically,

$$
\begin{align*}
\operatorname{ADRF}[c]\left[\tilde{t}_{c}\right] & =\frac{\sum_{j=1}^{K_{s}} \mathbb{I}\left(\operatorname{Dis}\left(t_{u_{j}}\right)=\tilde{t}_{c}\right) * y_{u_{j}} / P[c]\left[\tilde{t}_{c}\right]^{\gamma}}{\sum_{j=1}^{K_{s}} \mathbb{I}\left(\operatorname{Dis}\left(t_{u_{j}}\right)=\tilde{t}_{c}\right)}  \tag{6}\\
c & =1,2, \ldots, C ; \tilde{t}_{c}=0,1, \ldots, K
\end{align*}
$$

where $t_{u_{j}}, y_{u_{j}}$ represent the treatment and outcome of sample $u_{j}$, $P$ is the propensity matrix, $\gamma$ is a hyper-parameter to adjust the extent of debiasing. We set the first column ADRF[:, 0$]$ as 0 since if a category is not exposed, its CTR is 0 . Any other places without treatment-outcome data are padded with a null value $v_{a}$.

### 3.2 Fetching the Best Treatment

In contrast to other algorithms that primarily focus on point-wise accuracy, we propose that top $-N$ recommendation is essentially a resource allocation problem. After computing the ADRF of each real user, we transform the optimization problem into a knapsack problem using dynamic programming. We introduce a new matrix $f$ with the same shape as ADRF, where $f[c][k]$ represents the maximal overall CTR achieved by allocating $k$ slots of exposure resources to the first $c$ categories. The state transition equation is:

$$
\begin{align*}
f[c][k] & =\max _{\left|t_{0, c}-j\right|<\epsilon}(f[c-1][k], f[c-1][k-j]  \tag{7}\\
& +j * \operatorname{ADRF}[c][j]), c=1,2, \ldots, C ; k=0,1, \ldots, K
\end{align*}
$$

where $t_{0}$ represents the category exposure distribution of the backend recommendation list, and $\epsilon$ is the allowable deviation from the distribution of the backend recommendation list. The value of $f[C][K]$ represents the maximal overall CTR that we aim to achieve, and by tracing its path, we can identify the optimal treatment, enabling us to re-allocate the recommendation list accordingly. We refer to this method as UpliftRec-ADRF. An advantage of dynamic programming algorithms is it takes polynomial time to solve the problem. In this case, the time complexity is $O(C K \epsilon)$, where $C$ is the category number, $K$ is the treatment-clip number, and $\epsilon$ is the allowable deviation.

Table 1: Statistics of the three datasets. "int." denotes "interactions". "TP." denotes "Training Positive". "TN." denotes "Training Negative".

| Dataset | \#User | \#Item | \#TP. int. | \#TN. int. | Density |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Yahoo!R3 | $15.4 \mathrm{~K}$ | $1.0 \mathrm{~K}$ | $125.1 \mathrm{~K}$ | $167.9 \mathrm{~K}$ | $1.9 \%$ |
| Coat | 290 | 300 | $1.9 \mathrm{~K}$ | $5.1 \mathrm{~K}$ | $8.0 \%$ |
| KuaiRec | $7.1 \mathrm{~K}$ | $10.7 \mathrm{~K}$ | $936.5 \mathrm{~K}$ | $11.6 \mathrm{M}$ | $16.5 \%$ |

### 3.3 MTEF Approximation

Unfortunately, the ADRF solution faces a significant issue of high variance due to data sparsity. To address this concern, we propose a simplified version of ADRF known as MTEF estimation. MTEF aims to calculate a gradient approximation to mitigate the impact of sparse data. We consider that the farther away $t$ is from the backend distribution $t_{0}$, the higher the variance the model will encounter when estimating $\operatorname{ADRF}(t)$. To offer a slight tuning on the backend recommendation list, we use the treatment effects near $t_{0} \cdot \operatorname{MTEF}\left(t_{0}\right)$ represents the discrete form of the gradient $\frac{\partial y\left(t_{0}\right)}{\partial t}$, which indicates whether increasing exposure boosts the potential CTR on certain categories near $t_{0}$.

$$
\begin{equation*}
\operatorname{MTEF}(t)=\frac{\mathbb{E}[y(t+\Delta t)]-\mathbb{E}[y(t)]}{\Delta t} \tag{8}
\end{equation*}
$$

where $\Delta t$ is the step. The null values are filled by $v_{m}$. Notably, each user has unique ADRF and MTEF. For the sake of brevity, we omit the subscripts.

After calculating MTEF, we add it to scores provided by the backend model to obtain the final score.

$$
\begin{equation*}
s_{u, i}=s_{0, u, i}+\alpha * \operatorname{MTEF}\left(t_{0}\right)\left[c_{i}\right] \tag{9}
\end{equation*}
$$

where $s_{0, u, i}$ is the backend score for ( $\mathrm{u}, \mathrm{i}$ ), $\alpha$ is a hyper-parameter to adjust the influence of MTEF, and $c_{i}$ denotes the category item $i$ belongs to. We name this method UpliftRec-MTEF.

## 4 EXPERIMENTS

In this section, we conduct extensive experiments to answer the following research questions:

- RQ1: How does our method perform compared to SOTA debiased, diversity-based, bandit methods?
- RQ2: What is the impact of the IPW module on the performance of our method?
- RQ3: What distinguishes UpliftRec-MTEF from UpliftRec-ADRF?


### 4.1 Experimental Settings

4.1.1 Datasets. We conduct experiments on three real-world unbiased datasets: 1) Yahoo!R3 [25], 2) Coat [34], and 3) KuaiRec [10]. These datasets pertain to music, coat, and short-video scenarios, respectively. Each dataset comprises two components: a biased part, collected from typical user interactions in the real world, and an unbiased part, gathered through a randomized controlled trial (RCT) that is free from system-induced biases. Table 1 presents the statistics for these datasets. In our experiments, we utilize the biased part to train the model and partition the unbiased part for validation ( $50 \%$ ) and testing $(50 \%)$. For Yahoo!R3 and Coat datasets, we consider only ratings $\geq 4$ as positive examples. For KuaiRec, positive interactions are those with at least twice the watching time ratio, indicating that a user has watched a video at least twice. We incorporate the unbiased part in our experiments to evaluate how effectively the models can explore users' latent interests, which may not be evident during the collection of typical user interactions.

4.1.2 Baseline. We compare our method with several competitive methods including basic backend-candidate models, debiased methods, diversity-oriented methods and bandit algorithms:

- MF [21]. It is as a widely used benchmark model in recommendation systems, owing to its simplicity and effectiveness.
- FM [30]. It is the most representative feature-based recommender model. We selected features based on FM's performance. We used 'ID' and 'jacket type' in Coat, 'ID' in Yahoo!R3, and 'ID' with categories in KuaiRec.
- LightGCN [14]. It is a simplified and effective GCN-based benchmark model.
- IPS [35]. It is a conventional inverse propensity weighting method. It tries to capture true user preference from biased data by reweighting training samples.
- BC Loss [46]. It is a debiased method which incorporates adjustments based on popularity into a contrastive loss.
- iDCF [47]. It is a debiased method utilizing proxy variables to infer the unmeasured confounder.
- MMR [4]. It is a common reranking method that weighs between relevance and maximum list distance.
- PMF- $\alpha-\beta$ [37]. It balances relevance and diversity via reranking.
- LinUCB [22]. It is an exploration and exploitation technique, continuously exploring while minimizing each arm's variance.
- HCB [38]. It is a hierarchical bandit framework for entire space user interest exploration.

4.1.3 Hyper-parameter Settings. For all the methods, the maximum embedding size is set to 512 . We exclusively utilize positive samples for training these methods, and the negative sampling ratio is set to be either 4 or 24 . We search the ADRF-similarity number $K_{s}$ and the propensity-similarity number $K_{p}$ from 10 to the number of samples with intervals of $3 \mathrm{x}$. We tune the treatment-clip number $K$ in $\{0.2,0.4,0.6,0.8,1\}$ of the length of recommendation list $N$. We tune the category-clustering number $C$ in $\{2,3,5,10,15\}$. For UpliftRec-MTEF, we tune the null value $v_{m}$ in $[0,0.5]$ with a step of 0.05 and the weight $\alpha$ in [0.05, 0.45] with a step of 0.05 . For UpliftRec-ADRF, we tune the null value $v_{a}$ in $\{0.01,0.1\}$ and the allowable deviation $\epsilon$ in $\{0,1,2\}$. We generate a single sample for each real user with a division ratio of $\lambda$ set at 0.5 .

4.1.4 Evaluation Metrics. To assess the model's performance, we employ two widely-used accuracy metrics, Recall@K (R@K) and $\mathrm{NDCG} @ \mathrm{~K}(\mathrm{~N} @ \mathrm{~K})$ [16]. Additionally, to assess the model's ability to explore user hidden interests, we introduce two other metrics:

- RUE@K (UnExpected Recall@K). This metric measures the recall of unexpected items in the top-K list, where an item is considered unexpected if its category is not among the user's top 3 categories for interaction history. Please note that Yahoo!R3 does not have this metric due to the absence of category labels.
- RUP@K (UnPopular Recall@K). This metric evaluates the recall of unpopular items in the top- $\mathrm{K}$ list, where an item is labeled as unpopular if its interaction count in the training set falls within the bottom ninety percent of all items.

Table 2: Performance comparison between the baselines and UpliftRec-MTEF on the three datasets. The best results are highlighted in bold while the second-best ones are underlined. * implies the improvements over the best baseline are statistically significant ( $\mathrm{p}$-value $<0.05$ ) under one-sample $\mathrm{t}$-tests.

| Dataset <br> Metric | Yahoo!R3 |  |  | Coat |  |  |  | KuaiRec |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathrm{R} @ 10$ | N@ 10 | RUP@10 | $\mathrm{R} @ 10$ | $\mathrm{~N} @ 10$ | RUE@10 | RUP@10 | $\mathrm{R} @ 10$ | $\mathrm{~N} @ 10$ | RUE@10 | RUP@10 |
| Random | 0.0093 | 0.0044 | 0.0110 | 0.0307 | 0.0212 | 0.0321 | 0.0327 | 0.0009 | 0.0066 | 0.0010 | 0.0009 |
| MF | 0.0621 | 0.0338 | 0.0735 | 0.0705 | 0.0397 | 0.0636 | 0.0751 | 0.0870 | 0.2741 | 0.0949 | 0.0875 |
| FM | 0.0532 | 0.0267 | 0.0630 | 0.0875 | $\underline{0.0416}$ | $\underline{0.0836}$ | 0.0932 | 0.0468 | 0.1629 | 0.0498 | 0.0470 |
| LightGCN | 0.0614 | 0.0299 | 0.0727 | $\overline{0.0796}$ | $\overline{0.0374}$ | $\overline{0.0777}$ | $\overline{0.0848}$ | 0.0797 | 0.2417 | 0.0877 | 0.0802 |
| IPS | 0.0606 | 0.0325 | 0.0717 | 0.0686 | 0.0309 | 0.0734 | 0.0730 | 0.0886 | 0.2666 | 0.0983 | 0.0891 |
| BC Loss | $\underline{0.0694}$ | $\underline{0.0360}$ | 0.0822 | 0.0819 | 0.0357 | 0.0702 | 0.0873 | 0.0487 | 0.1465 | 0.0464 | 0.0490 |
| iDCF | $\overline{0.0675}$ | $\overline{0.0335}$ | $\overline{0.0799}$ | 0.0755 | 0.0336 | 0.0692 | 0.0804 | 0.0346 | 0.0346 | 0.038 | 0.0348 |
| MMR | 0.0557 | 0.0314 | 0.0660 | 0.0821 | 0.0380 | 0.0730 | 0.0874 | 0.0784 | 0.2616 | 0.0864 | 0.0789 |
| PMF- $\alpha-\beta$ | 0.0567 | 0.0301 | 0.0671 | 0.0385 | 0.0214 | 0.0398 | 0.0410 | 0.0787 | 0.2550 | 0.0869 | 0.0792 |
| LinUCB | 0.0610 | 0.0332 | 0.0723 | 0.0571 | 0.0315 | 0.0712 | 0.0608 | $\underline{0.0897}$ | $\underline{0.2758}$ | $\underline{0.0986}$ | $\underline{0.0901}$ |
| HCB | 0.0347 | 0.0168 | 0.0411 | 0.0683 | 0.0425 | 0.0791 | 0.0728 | $\overline{0.0777}$ | $\overline{0.2630}$ | $\overline{0.0862}$ | $\overline{0.0781}$ |
| UpliftRec-MTEF | $0.0715^{*}$ | $0.0363^{*}$ | $0.0847^{*}$ | $0.1027^{*}$ | $0.0504^{*}$ | $0.0984^{*}$ | $0.1093^{*}$ | $0.0914^{*}$ | $0.3169^{*}$ | $0.0999^{*}$ | $0.0920^{*}$ |

We also introduce the concept of serendipity, which refers to accidentally discovering users' hidden interests. Accordingly, we categorize the four metrics into two groups: accuracy (R@K, NDCG@K) and serendipity (RUE@K, RUP@K). We employ recall for finetuning UpliftRec and most baselines, as users are interested in encountering more positive items within the top- $N$ list. For diversityoriented baselines, we prioritize the highest diversity with the costs of a maximal $10 \%$ reduction of recall.

### 4.2 Overall Performance (RQ1)

We train baselines and UpliftRec-MTEF on the three datasets and results are reported in Table 2. We omit more results of @20 with similar trends due to space limitations. We choose the method with the best performance among MF, FM, and LightGCN as the backend model for MMR, PMF- $\alpha-\beta$, LinUCB, HCB, and UpliftRec To be specific, we choose MF for Yahoo!R3, KuaiRec, and FM for Coat. From Table 2, we have the following observations:

- UpliftRec-MTEF consistently outperforms other baselines on the three datasets. On R@10 and N@10, UpliftRec-MTEF yields superior performance than other baselines, which validates its strong ability to accurately predict recommendations. More importantly, UpliftRec-MTEF also performs well on RUE@10 and RUP@10, which reflects a model's capacity in exploring each user' hidden interests in fields he/she or the majority of users have not explored. The higher the value of RUE@10, the greater the likelihood for users to receive items from categories they have rarely encountered before. Similarly, a larger RUP@10 indicates increased opportunities for users to discover and receive unpopular items. These results highlight the effectiveness of UpliftRec-MTEF in mitigating exposure bias and popularity bias within recommender systems.
- In basic backend-candidate models, MF outperforms the other two methods (on Yahoo!R3 and KuaiRec) due to its simplicity and generalization ability. FM excels on Coat by leveraging item features in scarce data, but it performs poorly on KuaiRec. LightGCN exhibits average results against the others.
- As to debiased methods, IPS, BC Loss, and iDCF all stem from MF. BC Loss excels over MF on datasets like Yahoo!R3 and Coat,
![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-06.jpg?height=260&width=814&top_left_y=949&top_left_x=1119)

Figure 4: Performance of UpliftRec-MTEF as changing propensity exponent $\gamma$ on Yahoo!R3, Coat, and KuaiRec.

characterized by significant popularity bias. However, BC Loss exhibits poorer performance on KuaiRec, where primary biases are exposure and position bias. Similarly, iDCF outperforms MF on Yahoo!R3 and Coat owing to confounder learning. Nevertheless, iDCF falters on KuaiRec, where full exposure renders the interacting preference learned by iVAE [20] irrelevant. Conversely, IPS performs even worse than MF on Yahoo!R3 and Coat, primarily due to the high variance issue associated with the method.

- For diversity-oriented methods like MMR and PMF- $\alpha-\beta$, we notice that RUE@10 and RUP@10 do not increase with diversity, suggesting that diversity-oriented methods might not effectively explore users' latent interests.
- Regarding bandit algorithms, LinUCB performs second-best on KuaiRec, demonstrating the efficacy of the exploration-exploitation method in short-video recommendations. However, both LinUCB and HCB perform poorly in other cases, highlighting the detrimental effects of aimless exploitation.


### 4.3 In-depth Analysis (RQ2 \& RQ3)

To further explore our method, we design a bunch of experiments, including ablation studies, comparisons between Uplift-ADRF and Uplift-MTEF, robustness analysis to various backend models, case studies, hyper-parameter analysis and comparisons with methods with unbiased data.

4.3.1 Ablation Analysis. We conduct ablation studies to analyze the effect of inverse propensity weighting on three datasets. We use R@10 for accuracy and RUP@10 for serendipity to examine the necessity of this IPW structure. $\gamma$ is the exponent of the propensity, which controls the extent of debiasing. As shown in Figure 4, results

Table 3: Improvements compared to the different backend models on the Yahoo!R3 and Coat. The best results are in bold.

| Dataset <br> Metric |  | Yahoo!R3 |  |  | Coat |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| R@ 10 | N@10 | RUP@10 | R@ 10 | N@10 | RUE@ 10 | RUP@ 10 |  |
| MF | 0.0621 | 0.0338 | 0.0735 | 0.0705 | 0.0397 | 0.0636 | 0.0751 |
| UpliftRec-MF | $\mathbf{0 . 0 7 1 5}$ | $\mathbf{0 . 0 3 6 3}$ | $\mathbf{0 . 0 8 4 7}$ | 0.0873 | 0.0446 | 0.0720 | 0.0930 |
| FM | 0.0532 | 0.0267 | 0.0630 | 0.0875 | 0.0416 | 0.0836 | 0.0932 |
| UpliftRec-FM | 0.0555 | 0.0271 | 0.0657 | $\mathbf{0 . 1 0 2 7}$ | $\mathbf{0 . 0 5 0 4}$ | $\mathbf{0 . 0 9 8 4}$ | $\mathbf{0 . 1 0 9 3}$ |
| LightGCN | 0.0614 | 0.0299 | 0.0727 | 0.0796 | 0.0374 | 0.0777 | 0.0848 |
| UpliftRec-LGN | 0.0650 | 0.0314 | 0.0770 | 0.0851 | 0.0394 | 0.0968 | 0.0907 |

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-07.jpg?height=360&width=744&top_left_y=698&top_left_x=232)

Figure 5: Performance comparison of UpliftRec-METF and UpliftRec-ADRF on accuracy (left) and serendipity (right). We use R@10 as the metric for accuracy and use RUP@ 10 for Yahoo!R3 and RUE @ 10 for Coat and KuaiRec for serendipity.

on accuracy and serendipity both peak at a non-zero $\gamma$ on the three datasets. When $\gamma$ equals 0 , the IPW structure does not work and relegates the model to suboptimal performance. The increasing trend demonstrates the debiasing effects of IPW because it concerns more seldom exposed ratios when estimating ADRF. The downward trends after reaching the highest points are also reasonable, as an overly debiased method gives too much weight to samples with tiny propensities, which compromises the model's performance.

4.3.2 MTEF vs. ADRF. We compare the performances of UpliftRecMTEF and UpliftRec-ADRF on three datasets (Figure 5). UpliftRecMTEF surpasses UpliftRec-ADRF across all datasets by leveraging marginal treatment effects to mitigate the high variance issue However, UpliftRec-ADRF still achieves better performances on Yahoo!R3 and Coat compared to their backend models. The unstable performance of UpliftRec-ADRF can be attributed to the variance generated during ADRF estimation, especially when outcomes of certain category ratios are too scarce to make accurate estimations.

4.3.3 Backend Analysis. UpliftRec adopts a model-based approach. Backend models provide item embeddings for clustering and user embeddings for calculating similarity. These models also furnish backend scores to assist UpliftRec in ranking items within the same categories (UpliftRec-ADRF) and in re-ranking items with category-level uplift enhancements (UpliftRec-MTEF). To validate the versatility of UpliftRec-MTEF across various backend models, we manipulate the scores of different backend models while keeping the item and user embeddings unchanged. The outcomes are presented in Table 3, revealing substantial improvements in both accuracy and serendipity achieved by our method.

4.3.4 Case Study. We present a case study on the Coat dataset to illustrate the workings of UpliftRec-ADRF and UpliftRec-MTEF.

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-07.jpg?height=197&width=851&top_left_y=693&top_left_x=1098)

(a) Best treatment by UpliftRec-ADRF

(b) Best treatment by UpliftRec-MTEF

Figure 6: Visualization of the best treatment selection of UpliftRec-ADRF and UpliftRec-MTEF on Coat. The treatment provided by the backend model is highlighted in green, and the ground-truth treatment is in yellow-shaded boxes. The treatment calculated by ADRF is framed by red dotted lines in (a). The blue arrows point from the backend treatment to the treatment adjusted by MTEF in (b).

In Figure 6, we visualize the application of these methods based on the estimated ADRF matrix of user-39. The ADRF matrix is displayed in a white and grey matrix format, where color intensity represents the magnitude of the ADRF value. With a matrix size of $3 \times 5$, corresponding to 3 clustered categories and a treatment-clip number $K$ of 5 , null values for ADRF $v_{a}$ and MTEF $v_{m}$ are set at 0.01 and 0.05 , respectively. Our objective is to predict the top- 5 items for user-39, whose actual quantized category ratios $\tilde{t}_{g}$-highlighted in yellow-amount to 2,0 , and 3 for each category. The backend model offers item scores and a recommendation list, denoted as $\tilde{t}_{0}$, with quantized category ratios of 0,1 , and 4 (indicated in green in Figure 6). The backend model, however, fails to identify user-39's latent interest, which is the first clustered category.

In UpliftRec-ADRF, we set the deviation acceptance $\epsilon$ to 2. Consequently, we compute the optimal ratios $\tilde{t}_{a}$, framed by red dotted lines, which equate to 1,0 , and 4 . Notably, ADRF successfully uncovers the hidden interest.

Meanwhile, in UpliftRec-MTEF, we use a step size $\Delta t$ of 1 . The marginal treatment effects, which is calculated as $\operatorname{ADRF}\left(\tilde{t}_{0}+1\right)-$ $\operatorname{ADRF}\left(\tilde{t}_{0}\right)$, yield $[0.36,0.1,0.05]$. The last term is padded with $v_{m}$ due to missing data. Substantial uplift is observed in the first category. The final ratios of UpliftRec-MTEF $\tilde{t}_{m}$ are 2, 1, and 2. Among all these approaches, only UpliftRec-MTEF successfully recommends item-240, the second-ranked item within the first clustered category, showcasing its ability to to uncover user-39's latent interests.

4.3.5 Hyper-parameter Analysis. UpliftRec leverages critical hyper-parameters, including $K_{s}$ for ADRF estimation, $K$ for treatmentclip binning, $C$ for category clustering, and $\lambda$ for the dividing ratio. Through experiments on Yahoo!R3 and Coat, we methodically assess how these parameters affect UpliftRec-MTEF's efficacy.

- The ADRF-similarity number $K_{s}$. UpliftRec employs the top $K_{s}$ similar samples for ADRF estimation. Setting $K_{s}$ too small can

Table 4: Performance comparison between UpliftRec-MTEF and debiased methods with RCT data on the Yahoo!R3 and Coat. The best results are highlighted in bold while the best results besides AutoD-0.25 and InterD-0.25 are underlined.

| Dataset <br> Metric |  | Yahoo!R3 |  | Coat |  |  |
| :--- | :--- | :--- | :---: | :---: | :---: | :---: | :---: |
| MF | R@10 | N@10 | RUP@10 | R@10 | N@10 | RUE@10 |
| FM | 0.0621 | 0.0338 | 0.0735 | 0.0705 | 0.0397 | 0.0636 |
| UpliftRec-MTEF | 0.0532 | 0.0267 | 0.0630 | 0.0875 | 0.0416 | 0.0836 |
| AutoDebias-0.05 | $\underline{0.0715}$ | $\underline{0.0363}$ | $\underline{.0847}$ | $\underline{0.1027}$ | 0.0504 | 0.0984 |
| InterD-0.05 | 0.0603 | 0.0296 | 0.0695 | 0.0953 | $\underline{0.0573}$ | $\underline{0.1034}$ |
| AutoDebias-0.25 | 0.0699 | 0.0358 | 0.0714 | 0.0930 | 0.0565 | 0.0951 |
| InterD-0.25 | $\mathbf{0 . 0 7 4 6}$ | $\mathbf{0 . 0 3 8 1}$ | $\mathbf{0 . 0 8 8 4}$ | 0.1076 | $\mathbf{0 . 0 5 8 5}$ |  |

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-08.jpg?height=384&width=780&top_left_y=781&top_left_x=217)

Yahoo!R3

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-08.jpg?height=365&width=381&top_left_y=793&top_left_x=219)

ADRF-Similarity Number

![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-08.jpg?height=379&width=361&top_left_y=781&top_left_x=622)

Figure 7: Performance of UpliftRec-METF on Yahoo!R3 (left) and Coat (right) under varying ADRF-similarity numbers.
![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-08.jpg?height=386&width=782&top_left_y=1278&top_left_x=214)

Figure 8: Performance of UpliftRec-METF on Yahoo!R3 (left) and Coat (right) under varying treatment-clip numbers.

lead to challenges in accurately estimating ADRF due to high variance and insufficient data. Conversely, an excessively large $K_{s}$ may result in UpliftRec losing its personalization capability, leading to homogenous treatment outcomes. As illustrated in Figure 7, the performance of UpliftRec-MTEF peaks at $K_{s}=30$ for Yahoo!R3 and $K_{s}=80$ for Coat. This emphasizes the importance of choosing an appropriate value for $K_{s}$ to achieve optimal results.

- The treatment-clip number $K$. We discretize the continuous ratio space into $K$ quantized segments to address data scarcity. Figure 8 illustrates the performance of UpliftRec-MTEF across different treatment-clip numbers $K$. Optimal $K$ values are found to be 8 for Yahoo!R3 and 6 for Coat. Choosing a small $K$ may fail to capture model complexities, and a large $K$ can cause data sparsity. The observed trends highlight the need for a proper $K$ value to balance these concerns.
- The category-clustering number $C$. We perform item clustering, condensing items into $C$ new categories. Optimal clustering enhances matrix density, ensuring access to minority categories
![](https://cdn.mathpix.com/cropped/2024_06_04_102f01d86a1c3691807bg-08.jpg?height=384&width=770&top_left_y=778&top_left_x=1141)

Figure 9: Performance of UpliftRec-METF on Yahoo!R3 (left) and Coat (right) under varying category-clustering numbers.

Table 5: Performance of UpliftRec-METF on Yahoo!R3 (left) and Coat (right) under different dividing ratios.

| Dataset <br> $\lambda$ | Yahoo!R3 |  | Coat |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
| $\mathbf{R @ 1 0}$ | RUP@ 10 | R@10 | RUE@10 | RUP@ 10 |  |
| $\mathbf{0 . 5}$ | 0.0644 | 0.0763 | 0.0891 | 0.0836 | 0.0949 |
| $\mathbf{0 . 7 5}$ | 0.0715 | $\mathbf{0 . 0 8 4 7}$ | $\mathbf{0 . 1 0 2 7}$ | $\mathbf{0 . 0 9 8 4}$ | $\mathbf{0 . 1 0 9 3}$ |

while enabling differentiation between each item category. Figure 9 demonstrates that UpliftRec-MTEF's performance initially improves with an increasing number of categories $C$, reaching its peak at 5 categories for Yahoo!R3 and 3 categories for Coat, followed by a decline. Our findings indicate that proper clustering significantly boosts the performance of UpliftRec-MTEF.

- The dividing ratio $\lambda$. We divide the training dataset into a history part and a treatment-outcome part using the ratio $\lambda$. An analysis was conducted by varying $\lambda$, and the corresponding results for Yahoo!R3 and Coat are presented in Table 5. We find that choosing $\lambda=0.5$ strikes a favorable balance between estimating user historical preferences and treatment effects.

4.3.6 Comparison with Methods with Unbiased Data. To better compare UpliftRec-MTEF with SOTA debiased methods, we compare it with two debiased methods employing a small proportion of RCT data in training in Yahoo!R3 and Coat, which are:

- AutoDebias [5]. It is a strong debiased model trained with normal biased and RCT training data. It optimizes propensity scores and an imputation model over RCT training data.
- InterD [8]. It is a distillation model that considers MF and AutoDebias as biased and debiased teachers, respectively, aiming to perform well in both biased and unbiased tests.

We split the original validation dataset into the unbiased training part and the current validation part. We set the proportion of the unbiased training part as 0.05 and 0.25 respectively of the entire unbiased dataset. Specifically, when the ratio is 0.05 , we split the unbiased dataset into an unbiased training set (0.05), a validation set (0.45), and a testing set (0.5). When the ratio is 0.25 , the proportions are $0.25,0.25$, and 0.5 respectively. We train the model with both the previously employed biased training data and the additional unbiased training set. We can see from Table 4 that the performance of UpliftRec-MTEF is somewhere between debiased0.05 and debiased-0.25. To be specific, UpliftRec-MTEF outperforms both InterD-0.05 and AutoDebias-0.05 on accuracy and serendipity in Yahoo!R3, and have comparable results with them in Coat. When the ratio is raised to 0.25 , InterD and AutoDebias act better than all the other methods, which demonstrates the strong ability of imputation-based debiased methods with unbiased training data.

## 5 RELATED WORKS

- User Interest Exploration. Exploring user hidden great improves users' satisfaction. Existing works in this area can be categorized into three groups: debiased methods [3, 5, 8, 31, 43], diversityoriented methods $[4,37,39]$, and bandit methods $[22,23,38]$.
- Debiased methods aim to mitigate biases [6] in recommender systems, addressing issues such as selection bias [35], popularity bias $[49,51]$, and exposure bias [15]. Recently, new debiased methods have emerged, exploring additional aspects including fairness bias [41], duration bias [50], and outlier bias [32]. Many methods have been proposed to mitigate bias, such as regularization [3], data imputation [43], causal inference [28, 40]. Sam-reg [3] introduces regularization to mitigate the biased correlation between user-item relevance and item popularity. Doubly robust [43] trains a data imputation network to estimates the effect of missing data. IPS [31], as a widely used causal inference method, is designed to alleviate different effects of biased feedback. AutoDebias [5] further introduces a general debiased framework capable of combating multiple biases and their combinations using meta-learning. InterD [8] adopts an interpolative framework and leverages other methods as teachers to obtain a model that is both robust on biased and debiased datasets. However, they lack the ability to actively discover new interests.
- Diversity-oriented methods aim to find users' new interests by increasing the categories of recommended items. They recommend diversified items mainly in two ways: post-processing methods [4,37] and diverse-objective methods [39]. Post-processing methods are applied during the inference stage of the recommender systems to improve the coverage of all categories without changing the representation of users and items. On the other hand, diverse-objective methods optimize a diversity-oriented objective while training the models. For example, [39] adds a reward for diversity when training the reinforcement learning framework. However, both diversity-oriented methods focus on covering all categories, irrespective of user relevance. This compromises recommender system performance as low-CTR categories occupy excessive exposure resources.
- Bandit methods consider the potential rewards of enhanced exploration-exploitation trade-offs. They estimate each arm's mean reward and variance (item or item group) by iteratively collecting new user feedback. Bandit methods explore according to the estimation of the mean and variance of each arm [22]. COFIBA [23] considers collaborative effects and clusters users and items based on traditional bandit algorithms. HCB [38] proposes a hierarchical contextual bandit algorithm to reduce the computation cost when the number of arms is large. GNB [29] estimates the user graphs to preserve the pair-wise user correlations and utilize individual GNN-based models to achieve the adaptive exploration. However, Bandit methods still have a high chance to explore dissatisfying items owing to the high variance of these items. Moreover, the absence of list-wise optimization in these methods results in suboptimal recommendation lists.
- Uplift Modeling. Uplift modeling [12, 48] is employed to estimate the incremental effects of treatments (e.g., coupon distribution [2]). It tackles a counterfactual problem since we cannot directly observe the effects of intervention or non-intervention. Several methods have been proposed to address this challenge. LBCF [2], uses causal forests to estimate the conditional average treatment effect (CATE), and proposes a budget constraint optimization algorithm, while the treatment is binary. $M D P^{2}$ Forest [44], employs a Forest Ensemble approach where each tree in the forest estimates a greedy treatment (i.e., category distribution). Nevertheless, current methods do not guarantee optimality and additional random experiments are needed to obtain the data, which causes harm to user experience and company revenue.
- Causal Recommendation. In recent years, causal recommendation methods [7, 26, 33, 47, 49] have been continuously introduced, such as works on counterfactual learning and on confounding effects. For counterfactual learning, [26] utilize a Bayesian framework to qualify the treatment effects to counterfactual data from unobserved treatments. [47] leverages proxy variables to infer the unmeasured confounder and users' feedback. For de-confounding methods, DLCE [33] regards the features of users and items as confounders and re-weight the training samples to learn treatment effects. Another method [7] considers the response rate as the confounder and employs IPW to debias.


## 6 CONCLUSION

In this work, we approach top- $N$ recommendation from a causal perspective, using the category exposure ratio as the treatment and the CTR of each category as the outcome. We tackle the challenges of expensive data collection, biased observation, and data sparsity by generating an augmented dataset for treatment effect estimation, employing an IPW method to address the backdoor path, and discretizing the treatment. Our proposed Uplift modelbased Recommender (UpliftRec) framework estimates personalized ADRF to explore user interests effectively. By optimizing potential outcomes based on ADRF, we determine the best treatment for each user and adjust backend scores using MTEF derived from ADRF.We conduct extensive experiments to validate the effectiveness of our framework. This work introduces a new field of estimating uplift effects in recommendations. In the future, we plan to extend our approach to session-based models, such as reinforcement learning. Furthermore, we will explore different actions as treatments to apply our method to various scenarios.

## REFERENCES

[1] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In Proceedings of the Eleventh ACM Conference on Recommender Systems (RecSys). Association for Computing Machinery, 42-46.

[2] Meng Ai, Biao Li, Heyang Gong, Qingwei Yu, Shengjie Xue, Yuan Zhang, Yunzhou Zhang, and Peng Jiang. 2022. LBCF: A Large-Scale Budget-Constrained Causal Forest Algorithm. In Proceedings of the ACM Web Conference 2022 (WWW'22). Association for Computing Machinery, 2310-2319.

[3] Ludovico Boratto, Gianni Fenu, and Mirko Marras. 2021. Connecting user and item perspectives in popularity debiasing for collaborative recommendation. Information Processing \& Management 58, 1 (2021), 102387.

[4] Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. 335-336.

[5] Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen, Guli Lin, and Keping Yang. 2021. AutoDebias: Learning to Debias for Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21). Association for Computing Machinery, 21-30.

[6] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and Debias in Recommender System: A Survey and Future Directions. ACM Trans. Inf. Syst. 41, 3, Article 67 (feb 2023), 39 pages.

[7] Konstantina Christakopoulou, Madeleine Traverse, Trevor Potter, Emma Marriott, Daniel Li, Chris Haulk, Ed H. Chi, and Minmin Chen. 2020. Deconfounding User Satisfaction Estimation from Response Rate Bias. In Proceedings of the 14th ACM Conference on Recommender Systems (Virtual Event, Brazil) (RecSys). Association for Computing Machinery, 450-455.

[8] Sihao Ding, Fuli Feng, Xiangnan He, Jinqiu Jin, Wenjie Wang, Yong Liao, and Yongdong Zhang. 2022. Interpolative Distillation for Unifying Biased and Debiased Recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). Association for Computing Machinery, 40-49.

[9] Chongming Gao, Wenqiang Lei, Jiawei Chen, Shiqi Wang, Xiangnan He, Shijun Li, Biao Li, Yuan Zhang, and Peng Jiang. 2022. CIRS: Bursting Filter Bubbles by Counterfactual Interactive Recommender System. arXiv preprint arXiv:2204.01266 (2022).

[10] Chongming Gao, Shijun Li, Wenqiang Lei, Jiawei Chen, Biao Li, Peng Jiang Xiangnan He, Jiaxin Mao, and Tat-Seng Chua. 2022. KuaiRec: A Fully-Observed Dataset and Insights for Evaluating Recommender Systems. In Proceedings of the 31st ACM International Conference on Information \& Knowledge Management (CIKM). 540-550

[11] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017).

[12] Pierre Gutierrez and Jean-Yves Grardy. 2017. Causal Inference and Uplift Modelling: A Review of the Literature. In Proceedings of The 3rd International Conference on Predictive Applications and APIs (Proceedings of Machine Learning Research, Vol. 67), Claire Hardgrove, Louis Dorard, Keiran Thompson, and Florian Douetteau (Eds.). PMLR, 1-13.

[13] John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means clustering algorithm. Journal of the royal statistical society. series c (applied statistics) 28,1 (1979), 100-108.

[14] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). Association for Computing Machinery, 639-648.

[15] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In 2008 Eighth IEEE international conference on data mining. Ieee, 263-272.

[16] Kalervo Jrvelin and Jaana Keklinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), $422-446$.

[17] Ray Jiang, Silvia Chiappa, Tor Lattimore, Andrs Gyrgy, and Pushmeet Kohli. 2019. Degenerate Feedback Loops in Recommender Systems. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (AIES). Association for Computing Machinery, 383-390.

[18] Nathan Kallus and Angela Zhou. 2018. Policy Evaluation and Optimization with Continuous Treatments. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research, Vol. 84), Amos Storkey and Fernando Perez-Cruz (Eds.). PMLR, 12431251.

[19] Edward H Kennedy, Zongming Ma, Matthew D McHugh, and Dylan S Small. 2017 Non-parametric methods for doubly robust estimation of continuous treatment effects. Journal of the Royal Statistical Society. Series B (Statistical Methodology)
79,4 (2017), 1229-1245.

[20] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. 2020. Variational autoencoders and nonlinear ica: A unifying framework. In International Conference on Artificial Intelligence and Statistics. PMLR, 2207-2217.

[21] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30-37.

[22] Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. 2010. A ContextualBandit Approach to Personalized News Article Recommendation. In Proceedings of the 19th International Conference on World Wide Web (WWW). Association for Computing Machinery, 661-670.

[23] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. 2016. Collaborative Filtering Bandits. In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). Association for Computing Machinery, 539-548.

[24] Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin Burke. 2020. Feedback Loop and Bias Amplification in Recommender Systems. In Proceedings of the 29th ACM International Conference on Information \& Knowledge Management (CIKM). Association for Computing Machinery, 2145-2148.

[25] Benjamin M. Marlin and Richard S. Zemel. 2009. Collaborative Prediction and Ranking with Non-Random Missing Data. In Proceedings of the Third ACM Conference on Recommender Systems. Association for Computing Machinery, 5-12.

[26] Rishabh Mehrotra, Prasanta Bhattacharya, and Mounia Lalmas. 2020. Inferring the Causal Impact of New Track Releases on Music Recommendation Platforms through Counterfactual Predictions. In Proceedings of the 14th ACM Conference on Recommender Systems (RecSys). Association for Computing Machinery, 687-691.

[27] Judea Pearl. 2009. Causality. Cambridge university press.

[28] Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. 2016. Causal inference in statistics: A primer. 2016. Internet resource (2016).

[29] Yunzhe Qi, Yikun Ban, and Jingrui He. 2023. Graph neural bandits. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1920-1931.

[30] Steffen Rendle. 2012. Factorization Machines with LibFM. ACM Trans. Intell. Syst. Technol. 3, 3, Article 57 (may 2012), 22 pages.

[31] Yuta Saito, Suguru Yaginuma, Yuta Nishino, Hayato Sakata, and Kazuhide Nakata. 2020. Unbiased Recommender Learning from Missing-Not-At-Random Implicit Feedback. In Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM). Association for Computing Machinery, 501-509.

[32] Fatemeh Sarvi, Ali Vardasbi, Mohammad Aliannejadi, Sebastian Schelter, and Maarten de Rijke. 2023. On the Impact of Outlier Bias on User Clicks. arXiv preprint arXiv:2305.00857 (2023).

[33] Masahiro Sato, Sho Takemori, Janmajay Singh, and Tomoko Ohkuma. 2020. Unbiased Learning for the Causal Effect of Recommendation. In Proceedings of the 14th ACM Conference on Recommender Systems (RecSys). Association for Computing Machinery, 378-387.

[34] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. Recommendations as Treatments: Debiasing Learning and Evaluation. In Proceedings of the 33rd International Conference on International Conference on Machine Learning (New York, NY, USA). JMLR.org, 1670-1679.

[35] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. Recommendations as Treatments: Debiasing Learning and Evaluation. In Proceedings of The 33rd International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 48), Maria Florina Balcan and Kilian Q. Weinberger (Eds.). PMLR, 1670-1679.

[36] Vira Semenova and Victor Chernozhukov. 2021. Debiased machine learning of conditional average treatment effects and other causal functions. The Econometrics Journal 24, 2 (2021), 264-289.

[37] Chaofeng Sha, Xiaowei Wu, and Junyu Niu. 2016. A Framework for Recommending Relevant and Diverse Items. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IFCAI). AAAI Press, 3868-3874.

[38] Yu Song, Shuai Sun, Jianxun Lian, Hong Huang, Yu Li, Hai Jin, and Xing Xie. 2022. Show Me the Whole World: Towards Entire Item Space Exploration for Interactive Personalized Recommendations. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM). Association for Computing Machinery, 947-956.

[39] Dusan Stamenkovic, Alexandros Karatzoglou, Ioannis Arapakis, Xin Xin, and Kleomenis Katevas. 2022. Choosing the Best of Both Worlds: Diverse and Novel Recommendations through Multi-Objective Reinforcement Learning. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM). Association for Computing Machinery, 957-965.

[40] Teng Sun, Wenjie Wang, Liqaing Jing, Yiran Cui, Xuemeng Song, and Liqiang Nie. 2022. Counterfactual reasoning for out-of-distribution multimodal sentiment analysis. In Proceedings of the 30th ACM International Conference on Multimedia. $15-23$.

[41] Jiakai Tang, Shiqi Shen, Zhipeng Wang, Zhi Gong, Jingsen Zhang, and Xu Chen. 2023. When Fairness meets Bias: a Debiased Framework for Fairness aware Top-N Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems. $200-210$.

[42] Wenjie Wang, Fuli Feng, Liqiang Nie, and Tat-Seng Chua. 2022. User-Controllable Recommendation Against Filter Bubbles. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 1251-1261.

[43] Xiaojie Wang, Rui Zhang, Yu Sun, and Jianzhong Qi. 2019. Doubly Robust Joint Learning for Recommendation on Data Missing Not at Random. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.) PMLR, 6638-6647.

[44] Sizhe Yu, Ziyi Liu, Shixiang Wan, Jia Zheng, Zang Li, and Fan Zhou. 2022. MDP2 Forest: A Constrained Continuous Multi-Dimensional Policy Optimization Approach for Short-Video Recommendation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). Association for Computing Machinery, 2388-2398.

[45] Alessio Zanga, Elif Ozkirimli, and Fabio Stella. 2022. A survey on causal discovery: theory and practice. International fournal of Approximate Reasoning 151 (2022), $101-129$.

[46] An Zhang, Wenchang Ma, Xiang Wang, and Tat-Seng Chua. 2022. Incorporating Bias-aware Margins into Contrastive Loss for Collaborative Filtering. arXiv preprint arXiv:2210.11054 (2022)

[47] Qing Zhang, Xiaoying Zhang, Yang Liu, Hongning Wang, Min Gao, Jiheng Zhang, and Ruocheng Guo. 2023. Debiasing Recommendation by Learning Identifiable Latent Confounders. arXiv preprint arXiv:2302.05052 (2023).

[48] Weijia Zhang, Jiuyong Li, and Lin Liu. 2021. A Unified Survey of Treatment Effect Heterogeneity Modelling and Uplift Modelling. ACM Comput. Surv. 54, 8, Article 162 (oct 2021), 36 pages.

[49] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021. Causal Intervention for Leveraging Popularity Bias in Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). Association for Computing Machinery, 11-20.

[50] Haiyuan Zhao, Lei Zhang, Jun Xu, Guohao Cai, Zhenhua Dong, and Ji-Rong Wen. 2023. Uncovering User Interest from Biased and Noised Watch Time in Video Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems. 528-539.

[51] Huachi Zhou, Hao Chen, Junnan Dong, Daochen Zha, Chuang Zhou, and Xiao Huang. 2023. Adaptive Popularity Debiasing Aggregator for Graph Collaborative Filtering. (2023).


[^0]:    Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org

    SIGIR '24, July 14-18, 2024, Washington, DC, USA

     2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0431-4/24/07

    https://doi.org $/ 10.1145 / 3626772.3657736$

