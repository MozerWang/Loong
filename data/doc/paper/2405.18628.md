# Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference 

Hao (Mark) Chen ${ }^{1} \quad$ Wayne Luk ${ }^{1} \quad$ Ka Fai Cedric Yiu ${ }^{2}$<br>Rui Li $^{3} \quad$ Konstantin Mishchenko ${ }^{3}$ Stylianos I. Venieris ${ }^{3}$ Hongxiang Fan ${ }^{1,3}$<br>${ }^{1}$ Imperial College London, UK<br>${ }^{2}$ Hong Kong Polytechnic University, Hong Kong<br>${ }^{3}$ Samsung AI Center, Cambridge, UK<br>\{hc1620,w.luk\}@ic.ac.uk \{rui.li,s.venieris\}@ samsung.com<br>konsta.mish@gmail.com cedric.yiu@polyu.edu.hk hongxiangfan@ieee.org


#### Abstract

The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002 \%$ trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $P P D$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a $28 \%$ higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to $2.49 \times$ speedup and maintains a minimal runtime memory overhead of just $0.0004 \%$. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22 \times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding


## 1 Introduction

The recent advances in large language models (LLMs) are increasingly gaining influence across various AI applications. However, autoregressive generation, the de facto approach employed in LLM inference, suffers from inadequate hardware performance due to its inherent sequential nature [23]. Speculative decoding [13, 2, 11], an emerging acceleration technique, employs a guess-and-verify framework for LLM inference, where a smaller draft model first predicts multiple tokens sequentially and then the original LLM verifies them in parallel. Despite its potential, the effectiveness of speculative

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-01.jpg?height=350&width=639&top_left_y=1969&top_left_x=1117)

Figure 1: Comparison of memory, speedup, and training cost on MT-Bench with Vicuna7B. Circle diameter shows training GPU hours.

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-02.jpg?height=444&width=1395&top_left_y=198&top_left_x=365)

Figure 2: Overview of $P P D$. The left section shows the location of trainable parameters and the middle section displays the combined guess-and-verify process during inference. The "prompt token" denotes the special token with separately trained embeddings to perform parallel prediction.

decoding is limited by the complexity and cost of training a draft model capable of consistently achieving high acceptance rates across diverse base models and datasets. Additionally, the extra runtime memory overhead for executing draft models poses a significant barrier to the broader adoption of speculative decoding, particularly in edge and mobile environments where memory capacity is limited. Considering the growing need for user privacy and personalization, deploying LLMs on devices urges a more memory- and cost-efficient solution for accelerating LLM inference. Recent efforts have explored the possibility of generating multiple tokens in parallel without relying on a separate transformer draft model [20]. Approaches such as inserting additional decoding heads [1] and retrieving frequently used tokens [9] are employed to enhance performance. However, these methods either aggressively assume conditional independence among the tokens generated in a single step [1, 9], or use placeholder tokens (e.g., [PAD] token) that do not convey enough contextual information [20]. Therefore, they often suffer from low acceptance rates or degradation in output quality due to the lack of sufficient conditional information during inference.

To alleviate the complexity and overhead associated with the use of draft models while maintaining a high acceptance rate, we propose Parallel Prompt Decoding (PPD), a novel architecture-agnostic and memory-efficient framework that adopts prompt tuning for non-autoregressive LLM inference. Inspired by the human natural language generation process where continuous words like common expressions and phrases are produced simultaneously, $P P D$ introduces the use of prompt tokens, the meticulously trained embeddings, for multi-token prediction. Specifically, these trained prompt tokens are appended to the original input sequence in parallel, enabling the concurrent generation of multiple output tokens in a single forward pass. The key intuition of $P P D$ lies in the observation that if trained properly, prompt tokens appended to the input can approximate tokens generated at future timesteps, thereby partially recovering the missing conditional dependency information for multi-token generation. By strategically positioning trained prompt tokens, $P P D$ achieves up to a $28 \%$ higher acceptance rate when predicting long-range tokens. To further increase the token acceptance rate, we generate multiple candidate continuations with each prompt token and use them in combination with a customized tree attention mask to minimize the computation and memory overhead. The capability of $P P D$ to use low-cost prompt tokens for accurate multi-token prediction forms the foundation for accelerating LLM inference. As shown in Figure 1, PPD achieves a comparable speedup to the state-of-the-art speculative decoding approaches with negligible memory overhead and reduced training cost. Moreover, to facilitate the optimized implementation of $P P D$ across different hardware platforms, we propose a hardware-aware dynamic sparse tree technique that adaptively refines the prompt structure during runtime based on the computational resources available on the specific hardware.

To demonstrate the effectiveness of our approach, we evaluate $P P D$ on MobileLLaMA [6], Vicuna-7b and Vicuna-13b [5]. Running on a single GPU using the A100-40GB and RTX 4090, our method achieves a speedup ratio for inference from $\mathbf{2 . 1 2} \times$ to $2.49 \times$ across a diverse range of popular datasets including MT-Bench, HumanEval, and GSM8K. Our experiments demonstrate that $P P D$ not only achieves comparable throughput to the state-of-the-art speculative decoding method, but it also manages this with significantly fewer trainable parameters-specifically, $\mathbf{0 . 0 0 0 2 \%}$ of trainable parameters-and incurs only a minimal memory overhead $\mathbf{( 0 . 0 0 0 4 \%}$ ), showcasing that $P P D$ is remarkably cost- and memory-efficient. The training of prompt tokens can be completed in 16 hours
using one A100 GPU, 8 hours using four GeForce RTX 3090 GPUs, compared to the 1-2 days on four A100 GPUs required for Eagle [16]. Furthermore, since $P P D$ does not require the modification of the original LLM or the addition of extra networks, it is highly adaptable and orthogonal to other decoding techniques. For instance, it can be effectively combined with a draft model to further reduce inference latency.

Our contributions are summarized as follows:

- A novel Parallel Prompt Decoding (PPD) that adopts cost-effective prompt tokens for non-autoregressive LLM inference, achieving a high acceptance rate for long-distance token prediction with preserved output quality.
- A hardware-aware dynamic sparse tree technique that adaptively optimizes the prompt structure of $P P D$ at runtime based on the available compute and memory resources, facilitating its efficient deployment on various hardware platforms.
- An open-source implementation of $P P D$, accompanied by comprehensive evaluations on various models and benchmarks. Our experiments demonstrate that $P P D$ achieves significant speed improvements with negligible memory overhead and reduced training cost.


## 2 Background and Related Work

To enhance the inference speed of LLM, various approaches adopt an iterative guess-and-verify strategy to enable multi-token generation. In the guessing phase, potential future tokens are proposed at a faster speed than in traditional autoregressive implementations. Subsequently, a parallelized verification process assesses which guessed tokens should be accepted. Depending on how tokens are generated during the guess stage, these approaches can generally be categorized as $i$ ) speculative decoding and ii) parallel decoding.

### 2.1 Speculative Decoding

The guessing phase of speculative decoding adopts a lightweight draft model to generate multiple tokens at an increased speed [11]. During the verification stage, the original LLM subsequently determines the acceptance of the guessed tokens. It is worth noting that both draft and original models still follow the auto-regressive inference scheme. The speedup comes from two factors: $i$ ) the draft model runs much faster than the original model and more tokens can be generated within the same time unit; and ii) token verification is executed concurrently, either by batching or by incorporating multiple candidates into a single input using customized sparse attention masks [18]. Therefore, the overall speedup depends on the acceptance rate and the inference latency of draft models.

Building on the speculative decoding scheme, various studies have been conducted to further optimize its inference speed. To improve the accuracy of the draft model and its token acceptance rate, Eagle [16] incorporates the hidden features into the draft model's forward pass. SpecInfer [18] adopts a tree-based speculative inference and verification scheme, improving the diversity of speculation candidates. Sequoia [4] optimizes the sparse tree structure by considering the capability of the underlying hardware platforms. However, most of these methods require the storage and maintenance of a separate draft model. Moreover, there is extra complexity in designing an efficient draft model.

### 2.2 Parallel Decoding

To overcome the inherent limitations of autoregressive inference and the memory overhead associated with using a separate draft model, several attempts have been made to integrate both guessing and verification using one unified model. Medusd ${ }^{1}[1]$ introduces language model (LM) heads at the final layer of the original LLM, facilitating the generation of multiple tokens in a single forward pass. It also utilizes tree attention masks in its verification process to increase speed even further. To enhance token drafting with retrieval-augmented generation [10], Rest [9] introduce retrieval-based decoding tailored for specific scenarios. Inspired by Jacobi decoding [20] that adopts multiple special tokens to accelerate machine translation, Lookahead Decoding [8] improves upon this method by generating parallel n-grams and employing a caching memory pool. To capture more information while using[^0]multiple special tokens at distinct positions, PaSS [19] trains additional tokens with embedding layers for parallel decoding. Hierarchical parallel decoding [17] introduces the use of $[F o r k]$ and $[$ Join] tokens, enabling parallel execution of multiple structural subroutines.

Our approach can be categorized as parallel decoding, with three novel features to distinguish it from other approaches: 1) PPD trains the embeddings of parameterized ensemble prompt tokens, 2) it utilizes a dynamic sparse tree, adapting its structure at every inference step, and 3) we propose a hardware-aware algorithm for designing a dynamic sparse tree tailored to each hardware platform.

## 3 Parallel Prompt Decoding (PPD)

$P P D$ trains embeddings for prompt tokens rather than developing a separate model. Our method integrates three substeps into a single decoding step, following the guess-and-verify strategy: (1) candidate generation, where multiple candidate continuations $2^{2}$ are predicted by strategically inserting the prompt tokens into the input sequence. Tree attention [18] merges the processing of multiple candidates into a single forward pass; (2) candidate verification, where two verification schemes, exact matching [8] and typical acceptance [1], are implemented; (3) candidate acceptance, where validated candidates are integrated into the input and KV cache is updated accordingly. The inference scheme in Figure 2 illustrates the generation and verification combined in a single forward pass.

### 3.1 Prompt Tokens

The prompt tokens are the key component of $P P D$ to realize multi-token generation. Initially introduced in [12] to adapt LLMs for specific tasks, prompt tokens are typically prepended to the input, with outputs generated in an autoregressive manner. In this work, we propose a novel approach of utilizing prompt tokens by strategically positioning them at locations where tokens are anticipated to be generated in parallel. For conventional parallel decoding techniques [23, 1] that presume complete conditional independence among tokens decoded in a single step, the exact conditional probability $p\left(y_{i+k+1} \mid x, y_{1: i+k}\right)$ is approximated by $p_{\theta}\left(y_{i+k+1} \mid x, y_{1: i}\right)$, where $x$ is the input, $y_{1: i+k}$ represents the generated outputs so far, $k>0$ indicates the token distance 3 However, we observe that as $k$ increases, the gap between the actual probability and its approximation expands, primarily due to the absence of relevant conditional dependencies. We argue that prompt tokens can bridge this gap by more accurately modeling the conditional probability as $p_{\theta}\left(y_{i+k+1} \mid x, y_{1: i}, t_{i+1: i+k}\right)$ where $t_{i}$ is the prompt token with token distance $i$. Through this forward pass in the decoder layers, these causally linked prompt tokens facilitate the flow of information along the sequence of speculative tokens, thus restoring the conditional probability.

### 3.2 Ensemble Prompt Tokens

Inspired by prompt ensembling [12], which uses multiple prompts to generate diverse responses and aggregates these to derive a single answer, we introduce the concept of ensemble prompt token (EPT). This additional abstraction allows us to decouple each prompt token from the fixed embedding dimension. For every prompt token, there exist multiple corresponding EPTs, each with its distinct embedding. We modify the attention mask to ensure that each $n^{\text {th }}$ EPT only depends on the corresponding $n^{\text {th }}$ EPTs from preceding prompt tokens. This selective visibility is maintained for both training and inference, where the guess token for each prompt token is determined by averaging the logits of its EPTs. The use of EPTs not only enables direct and flexible control over the trainable parameters, but also leads to an increase in prediction accuracy. The probability is approximated as $\frac{1}{n} \sum_{j=1}^{n} p_{\theta}\left(y_{i+k+1} \mid x, y_{1: i}, v_{i+1: i+k}^{j}\right)$, where $v_{i+m}^{j}$ denotes the $j^{\text {th }}$ EPT at a token distance of $m$. 4

### 3.3 Training

During training, only the embeddings of prompt tokens are changed, with the parameters of the original LLM remaining frozen. We adopt the following two training techniques:[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-05.jpg?height=461&width=1377&top_left_y=217&top_left_x=366)

Figure 3: Dynamic sparse tree.

Random Insertion of Prompt Tokens: Randomly inserting prompt tokens throughout the input sequence reduces contextual bias from appending them only at the end. This approach broadens the predictive capacity of prompt tokens beyond a limited vocabulary such as <eos> and punctuation.

Knowledge Distillation: To align the predictive behavior of prompt tokens with the original LLM, we employ knowledge distillation. Instead of using hard labels, prompt tokens are trained against the logits produced by the original LLM. Following Medusa [1], The loss function is formulated as:

$$
\begin{equation*}
L_{P D}=\frac{1}{N} \sum_{i=1}^{N} D_{K L}\left(P_{i} \| Q_{i}\right) \cdot \alpha^{i-1} \tag{1}
\end{equation*}
$$

where $D_{K L}$ is the KL divergence, $P_{i}$ is the predicted distribution of the $i^{\text {th }}$ prompt token, $Q_{i}$ is the corresponding distribution from the original LLM, and $\alpha$ is the decay ratio.

## 4 Dynamic Sparse Tree

### 4.1 Motivation

To achieve higher speedup, $P P D$ utilizes a specialized tree attention [1, 18] to process multiple candidates within a single decoding step without expanding the batch size. Notably, $P P D$ employs a sparse tree [1, 4], designed to prioritize candidates with higher prediction accuracy. One key distinction from the sparse tree used in previous works is the appending of a sequence of prompt tokens to each tree node as shown in Figure 3. To optimize the amortized acceptance length across decoding steps, it is crucial to carefully balance the number of candidate tokens and prompt tokens. Instead of appending a uniform number of prompt tokens to every candidate token, we allocate them based on each candidate's probability, causing the tree's maximum depth and structure to vary at each decoding step.

### 4.2 Construction Algorithm

We aim to construct a dynamic sparse tree that maximizes the amortized number of tokens generated with limited candidate tokens and prompt tokens. We first define the tree construction algorithm as a constrained optimization problem.

Definition 4.1. Let $m$ be the maximal number of prompt tokens per tree node. The dynamic sparse tree $T$ can exist in $m$ states, each represented by $T_{k}$ corresponding to state $s_{k}$, where $1 \leq k \leq m$. Let $\mathrm{C}\left(T_{k}\right)$ denote the subtree of $T_{k}$ composed solely of candidate tokens. The maximum depth of $\mathrm{C}\left(T_{k}\right)$ is $k$

Proposition 4.1. For a dynamic sparse tree state $T_{k}$, where each candidate token $v$ follows a path $\operatorname{Path}(v)$ from the root, and the acceptance probability $p_{k}$ at each path position $k$, the expected number of tokens $f\left(T_{k}\right)$ generated is given by $f\left(T_{k}\right)=\sum_{v \in \mathrm{C}\left(T_{k}\right)} \prod_{i \in \operatorname{Path}(v)} p_{i}$, where $\prod_{i \in \operatorname{Path}(v)} p_{i}$ represents the contribution of a token $v$ to the expected number of tokens.

We then propose an approximation of the amortized number of tokens generated, by considering the tokens generated at the current and the next decoding step.

Proposition 4.2. The expected total number of tokens $F\left(T_{k}\right)$ generated for the dynamic sparse tree state $F\left(T_{k}\right)$ at the current and the next decoding step is given by $F\left(T_{k}\right)=f\left(T_{k}\right)+\sum_{i=1}^{m} p\left(s_{i} \mid s_{k}\right) f\left(T_{i}\right)$, where $p\left(s_{i} \mid s_{k}\right)$ represents the state transition probability from state $s_{k}$ to state $s_{i}$.

We are now ready to introduce Proposition 4.3 , which we use in the pruning algorithm.

Proposition 4.3. For a dynamic sparse tree state $T_{k}$ with candidate subtree $c_{k}=\mathrm{C}\left(T_{k}\right)$, the change in expected total tokens $F\left(T_{k}\right)$ due to the removal of a prompt token at candidate token $c$ is given by $\Delta F=p(c) \cdot\left(f\left(T_{i}\right)-f\left(T_{i-1}\right)\right)$, where $p(c)$ is the acceptance probability of candidate $c, i$ denotes the number of prompt tokens prior to removal. We assume that $i>1$.

To construct an approximately optimal dynamic sparse tree with specified numbers of candidate and prompt tokens, the process includes: (1) Optimal Candidate Trees: Constructing trees using only candidate tokens at varying depths, employing the algorithm from Medusa [1] and Sequoia [4] to maximize $f\left(T_{k}\right)$ as stated in Proposition 4.1, (2) Appending Prompt Tokens: Attaching the maximum allowable prompt tokens to each candidate token from the first step; (3) Greedy Prompt Token Removal: Removing prompt tokens greedily to minimize $\Delta F$ (Proposition 4.3), continuing until the desired prompt token count is reached.

We now introduce the formulation of the real amortized number of tokens generated.

Proposition 4.4. The amortized number of tokens $R\left(T_{k}\right)$ generated for the dynamic sparse tree state $F\left(T_{k}\right)$ is given by $R(T)=\sum_{i=1}^{m} p\left(s_{i}\right) f\left(T_{i}\right)$, where $p\left(s_{i}\right)$ is the steady-state probability of state $s_{i}$, and $f$ is the function defined in Proposition 4.1.

Hardware-awareness. All the probabilities used above can be approximated on a validation dataset. The dynamic sparse tree construction algorithm can now be formulated as finding the dynamic sparse tree $T$ with $n_{c}$ candidate tokens and $n_{p}$ prompt tokens to maximize $R(T)$ :

$$
c\left(n_{c}, n_{p}\right)=\max _{T,|C(T)|=n_{c},|T|=n_{c}+n_{p}} R(T)
$$

For a fixed tree size $n$, we explore all combinations of $n_{c}$ and $n_{p}$ where $n=n_{c}+n_{p}$, to identify the dynamic sparse tree that maximizes $R\left(T_{k}\right)$. To determine the optimal tree size $n$, we define two key functions: 1. Acceptance length $\tau(n)$ (hardware-independent) and 2. Forward pass latency $L_{f p}(n)$ (hardware-dependent). The speedup ratio, $\operatorname{Speedup}(n)=\frac{\tau(n)}{L_{f p}(n)}$, is estimated using a validation dataset, with $\tau(n)$ evaluated once and $L_{f p}(n)$ tested on different hardware types. We aim to find the value of $n$ that maximizes Speedup $(n)$.

## 5 Experiments

Models and testbeds. We conducted all the experiments using MobileLLaMA-1.4B [6], Vicuna7B and Vicuna-13B [5]. We used 3 prompt tokens and 1 EPT per prompt token for all inference experiments. The inference throughputs of the models are evaluated on a single NVIDIA A100 GPU with 40GB of memory and a GeForce RTX 4090 using a batch size of 1 and FP16 precision. Further details about the experimental setup can be found in Appendix $\mathrm{C}$.

Training. We froze all trainable parameters of the original LLM. Prompt token embeddings were trained using distillation logits generated from the ShareGPT dataset [22], with a maximum context length of 1024 , a cosine learning rate scheduler starting at 0.01 , and no warmup. Prompt token embeddings are initialized with normal text token embeddings.

Datasets. We assess the throughput performance of $P P D$ across various tasks and datasets. Specifically, we evaluated $P P D$ using the MT-Bench dataset [25], which contains multi-turn questions with a range of topics, in both non-greedy (temperature follows the default configuration) and greedy settings (temperature=0). We used the GSM8K [7] and HumanEval [3] datasets only in the greedy setting. The GSM8K dataset consists of grade school math problems and we used the first 500 questions of the test split for our evaluations. HumanEval includes coding tasks, for which we set a maximum new token limit of 512 to control the length of the generated sequences. We used the Alpaca [15] dataset as the validation dataset to produce the latencies and acceptance lengths used for dynamic sparse tree construction.

| Model | Method | $T$ | $\tau$ | $L_{\mathrm{fp}}(\mathrm{s})$ | Quality | $P_{\mathrm{tr}}(\%)$ | $S_{\text {tr }}$ | $S_{\text {input }}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $\mathrm{M}$ | Vanilla | 50.2 | 1.00 | $\mathbf{0 . 0 2 0}$ | - | $\mathrm{NA}$ | $\mathrm{NA}$ | 1 |
|  | $P P D$ | $\mathbf{1 0 8 . 7}$ | $\mathbf{2 . 4 3}$ | 0.022 | Same | $\mathbf{4 . 5 0} e^{-4}$ | $(10,84,89)$ | $(40,285,285)$ |
|  | Vanilla | 39.2 | 1.00 | $\mathbf{0 . 0 2 6}$ | $\mathbf{5 . 9 9}$ | $\mathrm{NA}$ | NA | 1 |
| V-7B | Medusa | 82.0 | 2.51 | 0.0307 | 5.98 | 8.07 | 63 | 63 |
|  | $P P D$ | $\mathbf{8 8 . 0}$ | $\mathbf{2 . 5 4}$ | 0.029 | 5.93 | $\mathbf{1 . 8 2} e^{-4}$ | $(10,33,34)$ | $(40,105,105)$ |
|  | Vanilla | 30.4 | 1.00 | $\mathbf{0 . 0 3 3 0}$ | $\mathbf{6 . 3 8}$ | NA | NA | 1 |
| V-13B | Medusa | 63.4 | $\mathbf{2 . 5 9}$ | 0.0408 | - | 5.52 | 63 | 63 |
|  | $P P D$ | $\mathbf{6 6 . 1}$ | 2.44 | 0.0379 | 6.32 | $\mathbf{7 . 8 7} e^{-5}$ | $(10,20,20)$ | $(40,60,60)$ |

Table 1: Comparative performance metrics of MobileLLaMA (M) for greedy setting, Vicuna-7B (V-7B) and Vicuna-13B (V-13B) for non-greedy setting using different decoding methods. The table details throughput ( $T$ in tokens/s), average accept lengths ( $\tau$ in tokens), forward pass latency ( $L_{\mathrm{fp}}$ in seconds), quality scores on MT-benchmark, percentages of additional trainable parameters ( $P_{\mathrm{tr}}$ ) and input lengths ( $S_{\text {input }}$ ) after the prefilling phase. $P P D$ employs a dynamic sparse tree with variable tree sizes $\left(S_{\mathrm{tr}}\right)$, represented as tuples. Same means the output matches with that of the original LLM.

### 5.1 Speedup Comparison with Parallel Decoding Methods

We compare the speedup ratios of $P P D$ with stateof-the-art parallel decoding methods on MT-Bench in non-greedy settings in Figure $4, P P D$ achieves speedups up to $13.8 \%$ higher than Medusa and between 2 times and 3 times higher than other parallel decoding methods. We examine the factors contributing to the enhanced speedup ratios and other performance metrics, as presented in Table 1. The reasons for the increase in speedup ratios are twofold. Firstly, $P P D$ produces candidate tokens with a higher acceptance rate than Medusa when utilizing a sparse tree of the same size. Notably, $P P D$ continues to achieve a comparable or slightly better acceptance rate even when employing a much smaller sparse tree - ranging from one-third to half the size. Secondly, PPD benefits from lower forward pass latency due to its ability to use smaller sparse tree sizes and hence shorter input lengths. $P P D$ also

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-07.jpg?height=456&width=653&top_left_y=970&top_left_x=1102)
eliminates the computational overhead associated

Figure 4: Comparative evaluation of latency speedup between $P P D$ and other parallel decoding methods. The experiments were conducted using the MT-Bench dataset, with the temperature set to MT-Bench's default configuration for Medusa and $P P D$. with separate decoding heads. $P P D$ maintains the same output quality, achieving about the same score on MT-Bench while using significantly fewer trainable parameters.

Figure 5 displays the throughput of $P P D$ on MT-Bench, HumanEval, and GSM8K with temperature equal to 0 . $P P D$ achieves consistent walltime speedup ratios from $2.12 \times$ to $2.49 \times$ on different GPUs. In general, $P P D$ performs better in coding and math reasoning tasks, achieving speedups between $2.21 \times$ and $2.49 \times$. This can be attributed to the fact that both code and math equations often contain fixed patterns and repetitive symbols, which narrows the range of plausible candidates and simplifies the prediction. We also found that with typical acceptance, the speedup increases with temperature. Another notable trend is that smaller models, such as Vicuna-7B, generally achieve more significant speedup ratios as compared to larger models, like Vicuna-13B. PPD aims to generate more tokens per step, which comes with increased computational demands. For larger models that already require substantial computational resources, it is necessary to limit the size of the sparse tree to avoid exceeding the GPU's utilization cap and causing increased latency. As a result, the number of tokens accepted per step is reduced, leading to lower speedups. However, this can be amortized when using more powerful GPUs than the NVIDIA A100 and the RTX 4090, such as NVIDIA H100.

### 5.2 Long-range Token Prediction

For a specific sparse tree, the accumulative accuracy provides a theoretical upper bound for the number of generated tokens per step and the maximum possible speedup ratio. Hence, maximizing
![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-08.jpg?height=652&width=1400&top_left_y=194&top_left_x=362)

Figure 5: Throughput of $P P D$ and vanilla models across different tasks. The temperature for experiments are set to 0 and the generated output of $P P D$ exactly matches that of the original LLM. We do not show results of Vicuna-13B on RTX 4090 as it does not fit into the GPU memory.

accumulative accuracy is crucial for the effectiveness of $P P D$. Figure 6 demonstrates the accumulative accuracy of the tokens predicted at various positions. We summarize the following three key insights from the results.

$\boldsymbol{P P D}$ excels at predicting more distant tokens. As depicted in Figure 6a, $P P D$ consistently outperforms Medusa in accuracy across all token positions. The accuracy gap between $P P D$ and Medusa widens with the increased token distance (e.g., the top-10 accuracy difference is 0.03 for the 'next next' word versus 0.12 for the 'next next next next' word). This improvement can be attributed to $P P D$ 's ability to partially recover conditional dependency information through causally connected prompt tokens.

$\boldsymbol{P P D}$ performs well at generating a broader array of plausible token candidates. For example, in predicting the token at a token distance of 3 , the top-10 candidates exhibit an accuracy improvement of 0.1 over Medusa, compared to only 0.02 for the top-1 candidate. This demonstrates the value of using tree attention and the largest viable tree size during inference, as multiple candidate continuations further boost accuracy improvement.

Multiple EPTs per prompt token and larger model sizes yield modest improvements in prediction accuracy. Figure 6b shows that using 100 EPTs per prompt token leads to accuracy improvement, ranging from 0.018 to 0.045 . Figure $6 \mathrm{C}$ displays that $P P D$ with Vicuna-13B outperforms Vicuna-7B with an accuracy gain of $0.011 \sim 0.038$. This increase is due to Vicuna-13B's greater embedding dimensions and deeper layers, which enhance the expressive power of prompt tokens. However, these gains are modest and can be offset by the increased computational burden of larger models.

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-08.jpg?height=401&width=1401&top_left_y=1889&top_left_x=359)

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-08.jpg?height=306&width=439&top_left_y=1907&top_left_x=369)

(a) PD vs. Medusa

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-08.jpg?height=306&width=444&top_left_y=1907&top_left_x=838)

(b) 100 EPT vs. 1 EPT

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-08.jpg?height=306&width=458&top_left_y=1907&top_left_x=1300)

(c) 13 b vs. $7 \mathrm{~b}$

Figure 6: Accumulative accuracy comparisons across different model configurations and prediction distances. 'V7' for Vicuna-7B, and 'V13' for Vicuna-13B. The notation '@ $i$ ' refers to a token distance of $i$. ' 100 EPT' represents 100 EPTs per prompt token. Accumulative accuracy is defined as top-k accuracy (e.g., a prediction is correct if the top-k candidates contain the ground truth). These measurements were obtained from the Alpaca Eval dataset with a maximum of 20 steps.

### 5.3 Memory Efficiency and Synergistic Integrations with Speculative Decoding

Memory efficiency. As shown in Figure 7. we compare the memory overhead of $P P D$ with the leading parallel decoding (Medusa) and speculative decoding approaches (Eagle). The memory overhead of PPD is just $0.004 \%$ of Medusa's and $0.007 \%$ of Eagle's. This efficiency stems from the efficient use of embeddings in $P P D$, which are significantly smaller than decoding heads and draft models, both of which scale with vocabulary size.

$\boldsymbol{P P D}+$ Speculative Decoding. As an orthogonal optimization in accelerating LLMs, PPD can be easily integrated with speculative decoding [11]. To demonstrate this, we applied $P P D$ to Vicuna$68 \mathrm{M}$ [24] and used it as the draft model for Vicuna-

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-09.jpg?height=455&width=653&top_left_y=323&top_left_x=1105)

Figure 7: Model memory usage. 7B. This combination resulted in a speedup of up to $1.22 \times$ for speculative decoding on Vicuna-7B compared to using speculative decoding alone.

### 5.4 Ablation Study

Dynamic Sparse Tree. Figure 8a shows that dynamic sparse trees consistently achieve longer acceptance lengths compared to static and random ones across varying sizes. The acceptance length for dynamic sparse trees shows a steady increase as the tree size extends, suggesting its good scalability. The convergence of dynamic and static sparse trees at larger sizes suggests a structural similarity emerging from constraints in tree depth and tree node count.

Hardware-aware Tree Size. Figure $8 \mathrm{~b}$ presents the theoretical speedup across different GPUs. Figure 8c validates that the optimal sparse tree size, derived from theoretical speedup models, indeed results in the greatest actual speedup observed.

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-09.jpg?height=420&width=1401&top_left_y=1411&top_left_x=362)

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-09.jpg?height=317&width=453&top_left_y=1435&top_left_x=370)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-09.jpg?height=325&width=461&top_left_y=1431&top_left_x=821)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_f0e187ec576d4ac10ee5g-09.jpg?height=341&width=458&top_left_y=1415&top_left_x=1300)

(c)

Figure 8: Evaluation of Dynamic Sparse Tree Performance. The static sparse trees in (a) always use the largest possible prompt tokens for each candidate. The theoretical speedup in (b) is calculated as the ratio of acceptance lengths (hardware-independent) to latency overhead (hardware-dependent). The optimal tree size is obtained from the peak value of the theoretical speedup. The latencies in (b) are obtained from inference on the same prompt for 512 forward passes. (c) shows the actual speedup obtained by running inference on different GPUs with different tree lengths on Alpaca Eval dataset.

## 6 Conclusion

We introduced $P P D$, a memory-efficient, cost-effective, and powerful parallel decoding method that incorporates a hardware-aware dynamic sparse tree. Utilizing specially trained prompt tokens to predict long-range tokens accurately, $P P D$ achieves a speedup of up to $2.49 \times$ in inference while employing only $0.0002 \%$ additional trainable parameters and without incorporating new models or architectural components. We believe that $P P D$ offers a novel perspective on the capabilities of parallel decoding. In future work, it could be synergized with other speculative or parallel decoding techniques to expedite inference even further.

## References

[1] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. arXiv preprint arXiv:2401.10774, 2024.

[2] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating Large Language Model Decoding with Speculative Sampling. arXiv preprint arXiv:2302.01318, 2023.

[3] Mark Chen et al. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374, 2021.

[4] Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding. arXiv preprint arXiv:2402.12374, 2024.

[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \% *$ ChatGPT Quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

[6] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, and Chunhua Shen. MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices. arXiv preprint arXiv:2312.16886, 2023.

[7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

[8] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking the Sequential Dependency of LLM Inference Using Lookahead Decoding, November 2023. URL https://lmsys.org/ blog/2023-11-21-lookahead-decoding/.

[9] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, and Di He. Rest: Retrieval-based Speculative Decoding. arXiv preprint arXiv:2311.08252, 2023.

[10] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing $(E M N L P)$, pages 6769-6781, 2020.

[11] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami, and Kurt Keutzer. Speculative Decoding with Big Little Decoder. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024.

[12] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.

[13] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast Inference from Transformers via Speculative Decoding. In International Conference on Machine Learning (ICML), 2023.

[14] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582-4597. Association for Computational Linguistics, 2021.

[15] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaEval: An Automatic Evaluator of Instructionfollowing Models. https://github.com/tatsu-lab/alpaca_eval, 2023.

[16] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. arXiv preprint arXiv:2401.15077, 2024.

[17] Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, and Yuxiao Dong. APAR: LLMs can do auto-parallel auto-regressive decoding. arXiv preprint arXiv:2401.06761, 2024.

[18] Xupeng Miao et al. SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification. In ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024.

[19] Giovanni Monea, Armand Joulin, and Edouard Grave. PaSS: Parallel Speculative Sampling. arXiv preprint arXiv:2311.13581, 2023.

[20] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodolà. Accelerating Transformer Inference for Translation via Parallel Decoding. In Annual Meeting of the Association for Computational Linguistics (ACL), 2023 .

[21] Apoorv Saxena. Prompt Lookup Decoding, November 2023. URLhttps://github.com/ apoorvumang/prompt-lookup-decoding/

[22] ShareGPT. ShareGPT, 2023. URL https://huggingface.co/datasets/Aeala/ ShareGPT_Vicuna_unfiltered

[23] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise Parallel Decoding for Deep Autoregressive Models. In Advances in Neural Information Processing Systems (NeurIPS), 2018 .

[24] Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. Multi-Candidate Speculative Decoding arXiv preprint arXiv:2401.06706, 2024.

[25] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Advances in Neural Information Processing Systems (NeurIPS), 2023.
