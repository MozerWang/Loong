# Code Llama: Open Foundation Models for Code 

Baptiste Rozière ${ }^{\dagger}$, Jonas Gehring ${ }^{\dagger}$, Fabian Gloeckle ${ }^{\dagger, *}$, Sten Sootla ${ }^{\dagger}$, Itai Gat, Xiaoqing<br>Ellen Tan, Yossi Adi ${ }^{\diamond}$, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom<br>Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron<br>Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron,<br>Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve ${ }^{\dagger}$

Meta AI


#### Abstract

We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B, and 70B parameters each. These models are trained on sequences of $16 \mathrm{k}$ tokens and show improvements on inputs with up to 100k tokens. The 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. CODE LLAMA reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to $67 \%$ and $65 \%$ on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use. ${ }^{1}$


## 1 Introduction

Large language models (LLMs) power a rapidly increasing number of applications, having reached a proficiency in natural language that allows them to be commanded and prompted to perform a variety of tasks (OpenAI, 2023; Touvron et al., 2023b). By utilizing large, in-domain datasets, their efficacy can be greatly improved for applications that require a combination of both natural and domain-specific language and understanding of specialized terminology. By training on domain-specific datasets, they have proved effective more broadly on applications that require advanced natural language understanding. A prominent use-case is the formal interaction with computer systems, such as program synthesis from natural language specifications, code completion, debugging, and generating documentation (for a survey, see Xu \& Zhu, 2022, also see Section 5). In this work, we present Code Llama, a family of LLMs for code generation and infilling derived from Llama 2 (Touvron et al., 2023b) and released under the same custom permissive license. We provide inference code for both completion and infilling models in the accompanying repository. ${ }^{1}$ Our approach is based on gradually specializing and increasing the capabilities of LLAMA 2 models by applying a cascade of training and fine-tuning steps (Figure 2):
- Code-training from foundation models. While most LLMs for code generation such as AlphaCode (Li et al., 2022), InCoder (Fried et al., 2023) or StarCoder (Li et al., 2023) are trained on code only, Codex (Chen et al., 2021) was fine-tuned from a general language model. We also start from a foundation model (Llama 2, Touvron et al., 2023b) pretrained on general-purpose text and code data. Our comparison (Section 3.4.1) shows that initializing our model with Llama 2 outperforms the same architecture trained on code only for a given budget.[^0]

Prompt: In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month

Response: You can use the "find" command in Bash to list all text files in the current directory (excluding subdirectories) that have been modified in the last month. The command would be:

find . -maxdepth 1 -type $f-$ mtime -30 -name $" *$, trt"

Here's a breakdown of the command:

* "s specifies the current directory.
* '-maxdepth $1^{\text {' }}$ tells 'find' to only search the current directory and not any subdirectories.
* "-type f" tells 'find' to only search for files (not directories or other types of files).
* '-mtime -30' tells 'find' to only search for files that have been modified in the last 30 days.
* '-name "*.txt"' tells 'find" to only search for files with the ".txt' extension.

Note that the '-mtime' option uses the number of days since the file was last modified, so '-mtime -30' means "modified in the last 30 days". If you want to search for files modified in the last month, you can use the '-mtime -30" option.

Figure 1: Example of response of Code Llama - Instruct (34B) when queried for a specific shell command.

- Infilling. Autoregressive training and fine-tuning of LLMs is suitable for prompt completion, but does not provide the capability to fill a missing portion of text while taking the full surrounding context into account. Our code-training for 7B, 13B and 70B Code Llama models features a multitask objective (Fried et al., 2023) consisting of both autoregressive and causal infilling prediction, enabling applications such as real-time completion in source code editors or docstring generation.
- Long input contexts. Unlocking repository-level reasoning for completion or synthesis - as opposed to function-level or file-level - requires prompting the model with much longer context than the 4,096 tokens supported by Llama 2. We propose an additional fine-tuning stage that extends the maximum context length from 4,096 tokens to 100,000 tokens by modifying the parameters of the RoPE positional embeddings (Su et al., 2021) used in Llama 2. Our experiments show Code Llama operating on very large contexts with a moderate impact on performances on standard coding benchmarks (Section 3.3).
- Instruction fine-tuning. For end-users, the utility of LLMs is significantly improved by instruction fine-tuning (Ouyang et al., 2022; Wei et al., 2022; OpenAI, 2023; Touvron et al., 2023b), which also helps preventing unsafe, toxic or biased generations. CoDE LLAMA - InSTRUCT variants are further fine-tuned on a mix of proprietary instruction data for improved safety and helpfulness, and a new machine-generated self-instruct dataset created by prompting Llama 2 for coding problems and CODE Llama to generate associated unit tests and solutions. Our results show that Code Llama - InstRuCt significantly improves performance on various truthfulness, toxicity and bias benchmarks at moderate cost in terms of code generation performance (Section 4 ).

Different combinations of these approaches lead to a family of code-specialized Llama 2 models with three main variants that we release in four sizes $(7 \mathrm{~B}, 13 \mathrm{~B}, 34 \mathrm{~B}$ and $70 \mathrm{~B}$ parameters):

- Code Llama: a foundational model for code generation tasks,
- Code Llama - Python: specialized for Python,
- Code Llama - InstRuct: fine-tuned with human instructions and self-instruct code synthesis data.

An example of using Code Llama - Instruct is given in Figure 1. It show-cases that the model interprets natural language to determine suitable options for a command-line program and provides an explanation of the solution. We provide further qualitative examples in Appendix L. We perform exhaustive evaluations of our models on major code generation benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021), as well as a multilingual version of HumanEval (MultiPL-E, Cassano et al., 2023), where our best models establish a new state of the art amongst open-source LLMs. The technical details of our training and fine-tuning procedures are provided in Section 2, followed by in-depth experiments and ablation studies, details of the safety/helpfulness evaluations and a discussion of related work.

![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-03.jpg?height=263&width=1442&top_left_y=215&top_left_x=339)

Figure 2: The Code Llama specialization pipeline. The different stages of fine-tuning annotated with the number of tokens seen during training. Infilling-capable models are marked with the $\rightleftarrows$ symbol.

## 2 Code Llama: Specializing Llama 2 for code

### 2.1 The Code Llama models family

Code Llama. The Code Llama models constitute foundation models for code generation. They come in four model sizes: 7B, 13B, 34B and 70B parameters. The 7B, 13B and 70B models are trained using an infilling objective (Section 2.3), and are appropriate to be used in an IDE to complete code in the middle of a file, for example. The 34B model was trained without the infilling objective. All Code Llama models are initialized with Llama 2 model weights and trained on 500B tokens from a code-heavy dataset (see Section 2.2 for more details), except Code Llama 70B which was trained on $1 \mathrm{~T}$ tokens. They are all fine-tuned to handle long contexts as detailed in Section 2.4.

Code Llama - Python. The Code Llama - Python models are specialized for Python code generation and come in sizes of 7B, 13B, 34B and 70B parameters. They are designed to study the performance of models tailored to a single programming language, compared to general-purpose code generation models. Initialized from Llama 2 models and trained on 500B tokens from the Code Llama dataset, Code Llama - Python models are further specialized on 100B tokens using a Python-heavy dataset (Section 2.2). Code Llama Python with 7B, 13B and 34B parameters are trained without infilling and subsequently fine-tuned to handle long contexts (Section 2.4).

Code Llama - Instruct. For the 7B, 13B and 34B sizes, Code Llama - Instruct models are based on Code Llama and fine-tuned with an additional approx. 5B tokens to better follow human instructions. More details on Code Llama - Instruct can be found in Section 2.5.

Code Llama 70B. Code Llama 70B was trained months after the Code Llama 7B, 13B and 34B model. It was trained using the same data as the smaller versions of CoDE Llama, and using roughly the same methods. Code Llama 70B was trained on twice the number of tokens: 1 trillion instead of 500 billion. It was trained with FIM, which was an often-requested capability for the 34B model. Only the base Code Llama 70B was trained with LCFT. See Appendix B for Code Llama 70B specialization pipeline. Code Llama - Instruct 70B was trained from Code Llama - Python 70B, which outperforms Code Llama 70B in average on the languages from MultiPL-E including python.

### 2.2 Dataset

We train Code Llama 7B, 13B and 34B on 500B tokens, and Code Llama 70B on 1T tokens during the initial phase, starting from the 7B, 13B, 34B, and 70B versions of Llama 2. As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of publicly available code. We also source $8 \%$ of our samples data from natural language datasets related to code. This dataset contains many discussions about code and code snippets included in natural language questions or answers. To help the model retain natural language understanding skills, we also sample a small proportion of our batches from a natural language dataset. Data is tokenized via byte pair encoding (BPE, Sennrich et al. (2016)), employing the same tokenizer as Llama and Llama 2. Preliminary experiments suggested that adding batches sampled from our natural language dataset improves the performance of our models on MBPP.

### 2.3 Infilling

Code infilling is the task of predicting the missing part of a program given a surrounding context. Applications include code completion at the cursor's position in code IDEs, type inference and generation of in-code documentation (e.g., docstrings).

We train infilling models following the concept of causal masking (Aghajanyan et al., 2022; Fried et al., 2023), where parts of a training sequence are moved to the end, and the reordered sequence is predicted autoregressively. We train the general-purpose 7B, 13B and 70B models with an infilling objective, following the recommendations of Bavarian et al. (2022). More precisely, we split training documents at the character level into a prefix, a middle part and a suffix with the splitting locations sampled independently from a uniform distribution over the document length. We apply this transformation with a probability of 0.9 and to documents that are not cut across multiple model contexts only. We randomly format half of the splits in the prefix-suffix-middle (PSM) format and the other half in the compatible suffix-prefix-middle (SPM) format described in Bavarian et al. (2022, App. D). We extend Llama 2's tokenizer with four special tokens that mark the beginning of the prefix, the middle part or the suffix, and the end of the infilling span. To limit the distribution shift between autoregressive and infilling training, we suppress the implicit leading space that SentencePiece tokenizers add upon encoding the middle part and the suffix (Kudo \& Richardson, 2018). In SPM format, we concatenate the prefix and the middle part before encoding to tokens. Note that our model doesn't encounter split subtokens in the SPM format while it does in the PSM format.

Results on the effect of infilling training on downstream generation tasks and the performance of our infilling models on infilling benchmarks are reported in Section 3.2.

### 2.4 Long context fine-tuning

Effective handling of long sequences is a major topic of research in transformer-based language modeling (Vaswani et al., 2017). The fundamental modeling challenges are extrapolation, i.e., operating on sequence lengths beyond those seen at training time, and the quadratic complexity of attention passes which favors training on short-to-medium length inputs.

For Code Llama, we propose a dedicated long context fine-tuning (LCFT) stage in which models are presented with sequences of 16,384 tokens, up from the 4,096 tokens used for LlamA 2 and our initial code training stages. By limiting the training time spent on processing long sequences to a fine-tuning stage, we gain long-range capabilities without significantly increasing the cost of training our models. Our strategy is similar to the recently proposed fine-tuning by position interpolation (Chen et al., 2023b), and we confirm the importance of modifying the rotation frequencies of the rotary position embedding used in the LLAMA 2 foundation models (Su et al., 2021). However, instead of downscaling frequencies linearly as Chen et al. (2023b), we change the base period from which they are derived. Specifically, with rotary embeddings, the query and key vectors $\mathbf{x}_{n}$ at position $n$ are subject to a linear transformation $\mathbf{R}_{\Theta, n}^{d} \mathbf{x}_{n}$, where $\mathbf{R}_{\Theta, n}^{d}$ is a block diagonal matrix with entries of the form

$$
\left(\mathbf{R}_{\Theta, n}^{d}\right)_{i}=\left(\begin{array}{cc}
\cos n \theta_{i} & -\sin n \theta_{i} \\
\sin n \theta_{i} & \cos n \theta_{i}
\end{array}\right)
$$

and $d$ denotes the embedding dimension. Rotation frequencies are computed as $\theta_{i}=\theta^{-2 i / d}$, and we increase the base period $\theta$ from 10,000 to 1,000,000 for fine-tuning. This increase allows for processing much larger sequences and reduces bias towards short-distance attention (see Appendix G. 1 for further discussion). Our experiments confirm that CoDE Llama models are not only effective within the increased sequence length used during fine-tuning, but further show extrapolation capabilities and exhibit stable behavior on very long sequences of up to 100,000 tokens (Section 3.3).

### 2.5 Instruction fine-tuning

Our instruction fine-tuned models Code Llama - Instruct are based on Code Llama and trained to answer questions appropriately. They are trained on three different types of data.

| Dataset | Sampling prop. | Epochs | Disk size |
| :--- | :---: | ---: | ---: |
| Code Llama (500B tokens) |  |  |  |
| Code | $85 \%$ | 2.03 | $859 \mathrm{~GB}$ |
| Natural language related to code | $8 \%$ | 1.39 | $78 \mathrm{~GB}$ |
| Natural language | $7 \%$ | 0.01 | $3.5 \mathrm{~TB}$ |
| Code Llama - Python (additional | $\mathbf{1 0 0 B}$ | tokens) |  |
| Python | $75 \%$ | 3.69 | $79 \mathrm{~GB}$ |
| Code | $10 \%$ | 0.05 | $859 \mathrm{~GB}$ |
| Natural language related to code | $10 \%$ | 0.35 | $78 \mathrm{~GB}$ |
| Natural language | $5 \%$ | 0.00 | $3.5 \mathrm{~TB}$ |

Table 1: Training dataset of Code Llama and Code Llama - Python. We train Code Llama on 500B additional tokens and Code Llama - PYthon further on 100B tokens.

Proprietary dataset. We use the instruction tuning dataset collected for Llama 2 and described in detail by Touvron et al. (2023b). Specifically, we use the version referred to in their paper as "RLHF V5", collected through several stages of reinforcement learning from human feedback and human feedback annotation (see their Section 3 for more details). It combines thousands of Supervised Fine-Tuning and millions of Rejection Sampling examples. Each example consists of a multi-turn dialogue between a user and an assistant. For Rejection Sampling, the output was selected among several generations using a reward model. The final dataset contains both Helpfulness and Safety data. This enables Code Llama to inherit Llama 2's instruction following and safety properties.

Self-instruct. Our proprietary dataset contains few examples of code-related tasks. Collecting supervised data from human annotators or training from human feedback (Ouyang et al., 2022) is expensive for coding tasks as it requires input from professional developers. Instead of human feedback, we use execution feedback to select data to train our instruct model. We construct the self-instruction dataset following the recipe below, resulting in $\sim 14,000$ question-tests-solution triplets:

1. Generate 62,000 interview-style programming questions by prompting (Figure 10) Llama 2 70B.
2. De-duplicate the set of questions by removing exact duplicates, resulting in $\sim 52,000$ questions.
3. For each of these questions:

(a) Generate unit tests by prompting Code Llama 7B (Figure 11)

(b) Generate ten Python solutions by prompting Code Llama 7B (Figure 12)

(c) Run the unit tests on the ten solutions. Add the first solution that passes the tests (along with its corresponding question and tests) to the self-instruct dataset.

We use Code Llama 7B to generate the tests and Python solutions, as we found it more efficient than generating fewer solutions per question with the 34B model for the same compute budget.

Rehearsal. In order to prevent the model from regressing on general coding and language understanding capabilities, Code Llama - InstRuct is also trained with a small proportion of data from the code dataset $(6 \%)$ and our natural language dataset $(2 \%)$.

### 2.6 Training details

Optimization. Our optimizer is AdamW (Loshchilov \& Hutter, 2019) with $\beta_{1}$ and $\beta_{2}$ values of 0.9 and 0.95 . We use a cosine schedule with 1000 warm-up steps, and set the final learning rate to be $1 / 30$ th of the peak learning rate. We use a batch size of $4 \mathrm{M}$ tokens which are presented as sequences of 4,096 tokens each. Despite the standard practice of using lower learning rates in fine-tuning stages than in pre-training stages,

| Model | Size | HumanEval |  |  | MBPP |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | pass@1 | pass@10 | pass@100 | \| pass@1 | pass@10 | pass @100 |
| code-cushman-001 | $12 \mathrm{~B}$ | $33.5 \%$ | - | - | $45.9 \%$ | - | - |
| GPT-3.5 (ChatGPT) |  | $48.1 \%$ | - | - | $52.2 \%$ | - | - |
| GPT-4 |  | $67.0 \%$ | - | - | - | - | - |
| PaLM | 540B | $26.2 \%$ | - | - | $36.8 \%$ | - | - |
| PaLM-Coder | 540B | $35.9 \%$ | - | $88.4 \%$ | $47.0 \%$ | - | - |
| PaLM 2-S |  | $37.6 \%$ | - | $88.4 \%$ | $50.0 \%$ | - | - |
| StarCoder Base | $15.5 \mathrm{~B}$ | $30.4 \%$ | - | - | $49.0 \%$ | - | - |
| StarCoder Python | $15.5 \mathrm{~B}$ | $33.6 \%$ | - | - | $52.7 \%$ | - | - |
| StarCoder Prompted | $15.5 \mathrm{~B}$ | $40.8 \%$ | - | - | $49.5 \%$ | - | - |
| LLAMA 2 | 7B | $12.2 \%$ | $25.2 \%$ | $44.4 \% \quad \square$ | $20.8 \%$ | $41.8 \%$ | $65.5 \%$ |
|  | $13 \mathrm{~B}$ | $20.1 \%$ | $34.8 \%$ | $61.2 \%$ | $27.6 \%$ | $48.1 \%$ | $69.5 \%$ |
|  | $34 \mathrm{~B}$ | $22.6 \%$ | $47.0 \%$ | $79.5 \%$ | $33.8 \%$ | $56.9 \%$ | $77.6 \%$ |
|  | $70 \mathrm{~B}$ | $30.5 \%$ | $59.4 \%$ | $87.0 \%$ | $45.4 \%$ | $66.2 \%$ | $83.1 \%$ |
| Code Llama | $7 \mathrm{~B}$ | $33.5 \%$ | $59.6 \%$ | $85.9 \%$ | $41.4 \%$ | $66.7 \%$ | $82.5 \%$ |
|  | $13 \mathrm{~B}$ | $36.0 \%$ | $69.4 \%$ | $89.8 \%$ | $47.0 \%$ | $71.7 \%$ | $87.1 \%$ |
|  | $34 \mathrm{~B}$ | $48.8 \%$ | $76.8 \%$ | $93.0 \%$ | $55.0 \%$ | $76.2 \%$ | $86.6 \%$ |
|  | $70 \mathrm{~B}$ | $53.0 \%$ | $84.6 \%$ | $96.2 \%$ | $62.4 \%$ | $81.1 \%$ | $91.9 \%$ |
| Code Llama - Instruct | $7 \mathrm{~B}$ | $34.8 \%$ | $64.3 \%$ | $88.1 \%$ | $44.4 \%$ | $65.4 \%$ | $76.8 \%$ |
|  | $13 \mathrm{~B}$ | $42.7 \%$ | $71.6 \%$ | $91.6 \%$ | $49.4 \%$ | $71.2 \%$ | $84.1 \%$ |
|  | $34 \mathrm{~B}$ | $41.5 \%$ | $77.2 \%$ | $93.5 \%$ | $57.0 \%$ | $74.6 \%$ | $85.4 \%$ |
|  | $70 \mathrm{~B}$ | $67.8 \%$ | $90.3 \%$ | $97.3 \%$ | $62.2 \%$ | $79.6 \%$ | $89.2 \%$ |
| Unnatural Code Llama | $34 \mathrm{~B}$ | $62.2 \%$ | $85.2 \%$ | $95.4 \%$ | $61.2 \%$ | $76.6 \%$ | $86.7 \%$ |
| Code Llama - Python | $7 \mathrm{~B}$ | $38.4 \%$ | $70.3 \%$ | $90.6 \%$ | $47.6 \%$ | $70.3 \%$ | $84.8 \%$ |
|  | 13B | $43.3 \%$ | $77.4 \%$ | $94.1 \%$ | $49.0 \%$ | $74.0 \%$ | $87.6 \%$ |
|  | $34 \mathrm{~B}$ | $53.7 \%$ | $82.8 \%$ | $94.7 \%$ | $56.2 \%$ | $76.4 \%$ | $88.2 \%$ |
|  | $70 \mathrm{~B}$ | $57.3 \%$ | $89.3 \%$ | $98.4 \%$ | $65.6 \%$ | $81.5 \%$ | $91.9 \%$ |

Table 2: Code Llama pass@ scores on HumanEval and MBPP. The pass@1 scores of our models are computed with greedy decoding. The pass@10 and pass@100 scores are computed with nucleus sampling with $\mathrm{p}=0.95$ and temperature 0.8 following our findings from Figure 6 . Models are evaluated in zero-shot on Human Eval and 3-shot on MBPP. The instruct models are trained to be safe and aligned from the base Code Llama models. Results for other models as provided by Li et al. (2023) (code-cushman-001, StarCoder), OpenAI (2023) (GPT-3.5, GPT-4), and Chowdhery et al. (2022); Anil et al. (2023) (PaLM).

we obtained best results when retaining the original learning rate of the Llama 2 base model. We carry these findings to the $13 \mathrm{~B}, 34 \mathrm{~B}$ and $70 \mathrm{~B}$ models, and set their learning rates to $3 e^{-4}, 1.5 e^{-4}$, and $1.5 e^{-4}$ respectively. For python fine-tuning, we set the initial learning rate to $1 e^{-4}$ instead. For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.

Long context fine-tuning. For long context fine-tuning (LCFT), we use a learning rate of $2 e^{-5}$, a sequence length of 16,384 , and reset RoPE frequencies with a base value of $\theta=10^{6}$. The batch size is set to $2 \mathrm{M}$ tokens for model sizes 7B and 13B and to $1 \mathrm{M}$ tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default. We observed instabilities in downstream performance for certain configurations, and hence set the number of gradient steps to 11,000 for the $34 \mathrm{~B}$ models and to 3,000 for Code LLAMA 7B.

## 3 Results

We report results on a variety of benchmarks. First, we evaluate our models on popular description-to-code generation benchmarks for Python: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS
(programming interviews and competitions, Hendrycks et al., 2021). Second, we evaluate our models on further programming languages using MultiPL-E (Cassano et al., 2023), namely on C++, Java, PHP, C\#, TypeScript (TS), and Bash. We additionally report results on the GSM8K benchmark (Cobbe et al., 2021), which measures mathematical reasoning capabilities (Appendix D).

Next, we perform an extensive ablation study: (i) we study the impact of training from scratch or from a pretrained Llama 2 model in Section 3.4.1; (ii) we perform ablations for infilling and additional infilling specific benchmarks in Section 3.2; (iii) we study the effect of long context fine-tuning on perplexity, a synthetic retrieval task, and code completion with long source code files (Section 3.3); and (iv) we evaluate our instruction fine-tuning procedure, which includes self-instruct training by leveraging self-generated unit tests in Section 3.4.2.

### 3.1 Code generation

### 3.1.1 Python code generation

We start by reporting results for Python code generation using the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and APPS (Hendrycks et al., 2021) benchmarks. Results are summarized in Tables 2 and 3. The full list of results on HumanEval and MBPP, including models with and without infilling and long context fine-tuning, can be found in Table 10 in Appendix C. We provide zero-shot results of our instruction fine-tuned models on APPS in Table 15 with evaluation details in Appendix F. Our main findings are as follows.

The value of model specialization. We observe that model specialization is yields a boost in code generation capabilities when comparing Llama 2 to Code Llama and Code Llama to Code Llama Python. Llama 2 was trained on $2 \mathrm{~T}$ tokens, and training on only 500B of extra tokens from a code-heavy dataset results in massive performance gains on both HumanEval and MBPP, to the point that Llama 2 70B is roughly equivalent to Code Llama 7B on Python coding benchmarks. Although Code Llama was trained on more than two epochs of our code dataset, which contains our entire Python dataset, training on 100B extra tokens of a Python-heavy data mix leads to significant gains on Python code generation benchmarks, between $4.3 \%$ points and $8.3 \%$ points in HumanEval pass@1 and between $1.2 \%$ points and $6.4 \%$ points in MBPP pass@1. These gains are smaller than for the first code training step, but still allow Code Llama - Python 7B to outperform even Code Llama 13B on MBPP and HumanEval. For the APPS benchmark, the prompts are much less direct and more complex compared to MBPP and HumanEval. Our Code Llama - Python models show slightly decreased performance on the introductory and interview level problems, where understanding the prompt is often more challenging for a language model than implementing a solution. However, Code Llama - Python shows clear gains on the competition-level problems where solutions are more complex. While large language models have enough capacity to learn to generate text on various topics, we observe that model specialization is beneficial for models between 7B and 70B parameters and after two full epochs on the training data.

Scaling of specialized models. We observe that scaling the number of parameters matters for models specialized for coding. With the same training process, our larger models outperform their smaller counterparts on almost every metric from HumanEval, MBPP and APPS (Table 2,3). For instance, we gain 5.6 percentage points on MBPP pass@1 scaling Code Llama from 7B to 13B parameters, 8 more points when scaling to $34 \mathrm{~B}$ and 7 when scaling to $70 \mathrm{~B}$. We can hypothesize that specializing larger models to code would lead to significant further gains on coding tasks. Moreover, the Chinchilla scaling laws (Hoffmann et al., 2022) indicate that larger models would benefit more from training on more tokens.

### 3.1.2 Multilingual evaluation

Next, we evaluate our models on a more diverse set of programming languages. For that, we use the MultiPL-E benchmark (Cassano et al., 2023). We report results for Python, C++, Java, PHP, TypeScript, C\#, and Bash in Table 4.

| Model | Size | Pass@ | \| Introductory | Interview | Competition |
| :---: | :---: | :---: | :---: | :---: | :---: |
| GPT-Neo | $2.7 \mathrm{~B}$ | 1 | $3.9 \%$ | $0.6 \%$ | $0.0 \%$ |
|  |  | 5 | $5.5 \%$ | $0.8 \%$ | $0.0 \%$ |
| Codex | 12B | 1 | $4.1 \%$ | $0.1 \%$ | $0.0 \%$ |
|  |  | 5 | $9.7 \%$ | $0.5 \%$ | $0.1 \%$ |
|  |  | 1000 | $25.0 \%$ | $3.7 \%$ | $3.2 \%$ |
| AlphaCode <br> AlphaCode (Filtered 1000) <br> AlphaCode (Filtered 10000) <br> AlphaCode (Filtered 50000) | 1B | 1000 | $17.7 \%$ | $5.2 \%$ | $7.1 \%$ |
|  |  | 5 | $14.4 \%$ | $5.6 \%$ | $4.6 \%$ |
|  |  | 5 | $18.2 \%$ | $8.2 \%$ | $6.7 \%$ |
|  |  | 5 | $20.4 \%$ | $9.7 \%$ | $7.8 \%$ |
| Code Llama | 7B | 5 | $10.8 \%$ | $2.0 \%$ | $0.8 \%$ |
|  |  | 10 | $15.6 \%$ | $3.1 \%$ | $1.4 \%$ |
|  |  | 100 | $33.5 \%$ | $9.4 \%$ | $7.1 \%$ |
|  | 13B | 5 | $23.7 \%$ | $5.6 \%$ | $2.1 \%$ |
|  |  | 10 | $30.2 \%$ | $8.1 \%$ | $3.4 \%$ |
|  |  | 100 | $49.0 \%$ | $18.4 \%$ | $12.0 \%$ |
|  | $34 \mathrm{~B}$ | 5 | $32.8 \%$ | $\underline{8.8 \%}$ | $2.9 \%$ |
|  |  | 10 | $\overline{39.0 \%}$ | $\overline{12.2 \%}$ | $4.7 \%$ |
|  |  | 100 | $\overline{56.3 \%}$ | $\underline{24.3 \%}$ | $15.4 \%$ |
| Code Llama - Python | 7B | 5 | $12.7 \%$ | $4.2 \%$ | $1.3 \%$ |
|  |  | 10 | $18.5 \%$ | $6.3 \%$ | $2.2 \%$ |
|  |  | 100 | $38.3 \%$ | $14.9 \%$ | $9.1 \%$ |
|  | $13 \mathrm{~B}$ | 5 | $26.3 \%$ | $7.1 \%$ | $2.8 \%$ |
|  |  | 10 | $32.8 \%$ | $10.0 \%$ | $4.3 \%$ |
|  |  | 100 | $51.6 \%$ | $21.5 \%$ | $14.6 \%$ |
|  | $34 \mathrm{~B}$ | 5 | $28.9 \%$ | $7.8 \%$ | $3.5 \%$ |
|  |  | 10 | $35.9 \%$ | $11.1 \%$ | $\overline{5.5 \%}$ |
|  |  | 100 | $54.9 \%$ | $23.9 \%$ | $\overline{16.8 \%}$ |
| Code Llama - Instruct | 7B | 5 | $12.9 \%$ | $2.1 \%$ | $1.1 \%$ |
|  |  | 10 | $17.9 \%$ | $3.1 \%$ | $2.0 \%$ |
|  |  | 100 | $35.4 \%$ | $9.4 \%$ | $8.5 \%$ |
|  | $13 \mathrm{~B}$ | 5 | $24.0 \%$ | $6.9 \%$ | $2.4 \%$ |
|  |  | 10 | $30.3 \%$ | $9.6 \%$ | $3.8 \%$ |
|  |  | 100 | $48.7 \%$ | $19.6 \%$ | $13.1 \%$ |
|  | $34 \mathrm{~B}$ | 5 | $31.6 \%$ | $7.9 \%$ | $3.2 \%$ |
|  |  | 10 | $37.8 \%$ | $11.1 \%$ | $5.1 \%$ |
|  |  | 100 | $55.7 \%$ | $22.8 \%$ | $16.4 \%$ |

Table 3: Code Llama pass@ scores on APPS. We list the two-shot pass@5, pass@10, and pass@100 scores of Code Llama on APPS. For our models, we use nucleus sampling with $\mathrm{p}=0.95$ and a temperature of 0.6. Code Llama is not fine-tuned on the training set of APPS and all results are calculated with raw predictions without filtering by the test cases from the prompt. Fine-tuned GPT-Neo numbers are reported by Hendrycks et al. (2021), one-shot Codex results by Chen et al. (2021), and fine-tuned AlphaCode numbers by Li et al. (2022).

We observe a similar improvement from Llama 2 to Code Llama in the multilingual setting as in the evaluation on Python (Section 3.1.1). The Code Llama models clearly outperform Llama 2 models of the same size on code generation in any language, and Code Llama 7B even outperforms Llama 2 70B. Compared

| Model | Size | Multi-lingual Human-Eval |  |  |  |  |  |  |
| :--- | ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | C++ | Java | PHP | TS | C\# | Bash | Average |
| CodeGen-Multi | 16B | $21.0 \%$ | $22.2 \%$ | $8.4 \%$ | $20.1 \%$ | $8.2 \%$ | $0.6 \%$ | $13.4 \%$ |
| CodeGeeX | 13B | $16.9 \%$ | $19.1 \%$ | $13.5 \%$ | $10.1 \%$ | $8.5 \%$ | $2.8 \%$ | $11.8 \%$ |
| code-cushman-001 | 12B | $30.6 \%$ | $31.9 \%$ | $28.9 \%$ | $31.3 \%$ | $22.1 \%$ | $11.7 \%$ | $26.1 \%$ |
| StarCoder Base | 15.5B | $30.6 \%$ | $28.5 \%$ | $26.8 \%$ | $32.2 \%$ | $20.6 \%$ | $11.0 \%$ | $25.0 \%$ |
| StarCoder Python | 15.5B | $31.6 \%$ | $30.2 \%$ | $26.1 \%$ | $32.3 \%$ | $21.0 \%$ | $10.5 \%$ | $25.3 \%$ |
|  | 7B | $6.8 \%$ | $10.8 \%$ | $9.9 \%$ | $12.6 \%$ | $6.3 \%$ | $3.2 \%$ | $8.3 \%$ |
| LLAMA-v2 | 13B | $13.7 \%$ | $15.8 \%$ | $13.1 \%$ | $13.2 \%$ | $9.5 \%$ | $3.2 \%$ | $11.4 \%$ |
|  | 34B | $23.6 \%$ | $22.2 \%$ | $19.9 \%$ | $21.4 \%$ | $17.1 \%$ | $3.8 \%$ | $18.0 \%$ |
|  | 70B | $30.4 \%$ | $31.7 \%$ | $34.2 \%$ | $15.1 \%$ | $25.9 \%$ | $8.9 \%$ | $24.4 \%$ |
|  | 7B | $28.6 \%$ | $34.2 \%$ | $24.2 \%$ | $33.3 \%$ | $25.3 \%$ | $12.0 \%$ | $26.3 \%$ |
| Code Llama | 13B | $39.1 \%$ | $38.0 \%$ | $34.2 \%$ | $29.6 \%$ | $27.3 \%$ | $15.2 \%$ | $30.6 \%$ |
|  | 34B | $47.8 \%$ | $45.6 \%$ | $44.1 \%$ | $33.3 \%$ | $30.4 \%$ | $17.1 \%$ | $36.4 \%$ |
| Code Llama - INSTRUCT | 70B | $52.8 \%$ | $51.9 \%$ | $50.9 \%$ | $\mathbf{4 9 . 1 \%}$ | $\mathbf{3 8 . 0 \%}$ | $29.1 \%$ | $45.3 \%$ |
|  | 7B | $31.1 \%$ | $30.4 \%$ | $28.6 \%$ | $32.7 \%$ | $21.6 \%$ | $10.1 \%$ | $25.8 \%$ |
|  | 13B | $42.2 \%$ | $40.5 \%$ | $32.3 \%$ | $39.0 \%$ | $24.0 \%$ | $13.9 \%$ | $32.0 \%$ |
|  | 34B | $45.3 \%$ | $43.7 \%$ | $36.6 \%$ | $40.3 \%$ | $31.0 \%$ | $19.6 \%$ | $36.1 \%$ |
| Code Llama - PYthon | 70B | $53.4 \%$ | $\mathbf{5 8 . 2 \%}$ | $\mathbf{5 8 . 4 \%}$ | $39.0 \%$ | $36.7 \%$ | $\mathbf{2 9 . 7 \%}$ | $\underline{\mathbf{4 5 . 9 \%}}$ |
|  | 7B | $32.3 \%$ | $35.4 \%$ | $32.3 \%$ | $23.9 \%$ | $24.7 \%$ | $16.5 \%$ | $27.5 \%$ |
|  | 13B | $39.1 \%$ | $37.3 \%$ | $33.5 \%$ | $35.2 \%$ | $29.8 \%$ | $13.9 \%$ | $31.5 \%$ |
|  | 34B | $42.2 \%$ | $44.9 \%$ | $42.9 \%$ | $34.3 \%$ | $31.7 \%$ | $14.6 \%$ | $35.1 \%$ |
|  | 70B | $\mathbf{5 4 . 7 \%}$ | $57.6 \%$ | $53.4 \%$ | $44.0 \%$ | $34.8 \%$ | $25.3 \%$ | $45.0 \%$ |

Table 4: Multi-Lingual HE Pass@1 scores. Pass@1 scores for different programming languages using greedy decoding. These scores are computed in zero-shot. Results for other models from Li et al. (2023).

to other publicly available models, ours are especially strong in the multilingual setting. CODE LLAMA 7B outperforms larger models such as CodeGen-Multi or StarCoder, and is on par with Codex (code-cushman-001, Chen et al., 2021).

The performance of Code Llama - Python is comparable to that of Code Llama. Code Llama Python 30B performs slightly worse than Code Llama but Code Llama - Python 7B and 13B perform slightly better than their counterparts without Python fine-tuning. More detailed results can be found in Table 11, Appendix C.

To better understand the influence of multilingual pre-training, we measure the correlations between each of the evaluated languages and report the results separately for different model sizes in Figure 3. We observe high correlation between model performance on C++, C\#, Java, and PHP. Interestingly, we also notice strong correlation between model performance on Python and Bash. Lastly, as expected the bigger and more expressive the models, the higher the correlation between the performance across all different languages.

### 3.2 Infilling evaluations

Performance cost of infilling training. Previous studies on infilling (or fill-in-the-middle, FIM) code models assert that the traditional next token prediction objective can be replaced by a multitask infilling objective with an infilling rate of up to $90 \%$ at no cost for left-to-right autoregressive test losses (Bavarian et al., 2022) and only small cost for downstream evaluation performance (Allal et al., 2023). In Table 5, we independently validate both findings at the scale of 7B and 13B parameters and 500B training tokens of code. The 7B model loses 0.6 percentage points on average across HumanEval and MBPP pass@1, pass@10 and pass@100 scores if trained with an infilling objective, while the 13B model loses 1.1 percentage points.

![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-10.jpg?height=499&width=1651&top_left_y=211&top_left_x=237)

Figure 3: Correlations between Languages. Correlation scores between the Python, C++, Java, PHP, C\#, TypeScript (TS), and Bash, reported for different model sizes. The code for this figure was generated by Code Llama - Instruct, the prompt and code can be seen in Figure 22.

Because of this modest decline in performance and the wide applicability of models with infilling capability, we decide to release Code Llama 7B, 13B and 70B in this configuration.

Code infilling benchmarks. Our infilling models reach state-of-the-art performances in code infilling benchmarks among models of their size. We evaluate on two related code infilling benchmarks based on the HumanEval benchmark (Chen et al., 2021).

The HumanEval infilling benchmark (Fried et al., 2023) turns the reference solutions of the HumanEval benchmark (Chen et al., 2021) into infilling problems by masking out either individual lines or blocks consisting of multiple consecutive lines. It has been extended in Bavarian et al. (2022) with a random span infilling task in which the masking is applied to a randomly selected substring at the character level. Predictions are scored with a pass@1 score based on the test cases of the original HumanEval problems. According to the results in Table 14, our models outperform all other infilling models of their size. Note, however, that the results in random span infilling are significantly worse in suffix-prefix-middle (SPM) format than in prefix-suffix-middle (PSM) format as it would require token healing (Microsoft, 2023), which we have not implemented for this evaluation (see Appendix $\mathrm{E}$ for further discussion).

Allal et al. (2023) translates the HumanEval infilling benchmark to other programming languages using MultiPL-E (Cassano et al., 2023). Single lines are masked and predictions are scored with an exact match metric against the ground truth solution. Our models, including Code Llama 7B, outperform all open infilling models across the three programming languages contained in the benchmark (Table 6). We observe a further increase in performance when prompting the models in SPM format, like witnessed in Bavarian et al. $(2022)$.

### 3.3 Long context evaluations

We explore CoDE Llama's ability to work with long sequences by measuring perplexity, key retrieval accuracy and performance during generation on code completion tasks. These tasks, and our results are detailed below. For full results and comparisons to alternative techniques of increasing the context length of LLMs, we refer to Appendix G.

Perplexity during extrapolation. In Figure 4a, perplexity is computed over 4M tokens from the code dataset, using a subset of our validation data consisting of large source files ( $\geq 50 \mathrm{kB})$. For all model sizes, we observe a steady decrease in perplexity well beyond 16384 tokens, which is the sequence length we use for long-context fine-tuning. After 100K tokens, the perplexity increases only slightly, in contrast to the well-known instability phenomenon when testing transformer models on sequences larger than those seen during training (Press et al., 2022).

| Model | FIM |  |  |  | Size | HumanEval |  |  | MBPP |  |  | Test loss |  |  |
| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | pass@1 | pass@10 | pass@100 | pass@1 | pass@10 | pass@100 |  |  |  |  |  |  |
| CoDE Llama (w/o LCFT) | $\boldsymbol{X}$ | 7B | $33.2 \%$ | $43.3 \%$ | $49.9 \%$ | $44.8 \%$ | $52.5 \%$ | $57.1 \%$ | 0.408 |  |  |  |  |  |
|  |  | 13B | $36.8 \%$ | $49.2 \%$ | $57.9 \%$ | $48.2 \%$ | $57.4 \%$ | $61.6 \%$ | 0.372 |  |  |  |  |  |
| Code Llama (w/o LCFT) |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | $\boldsymbol{\sim}$ | 7B | $33.6 \%$ | $44.0 \%$ | $48.8 \%$ | $44.2 \%$ | $51.4 \%$ | $55.5 \%$ | 0.407 |  |  |  |  |  |
|  |  | 13B | $36.2 \%$ | $48.3 \%$ | $54.6 \%$ | $48.0 \%$ | $56.8 \%$ | $60.8 \%$ | 0.373 |  |  |  |  |  |
| Absolute gap | $\boldsymbol{X}-\boldsymbol{\checkmark}$ | 7B | $-0.4 \%$ | $-0.7 \%$ | $1.1 \%$ | $0.6 \%$ | $1.1 \%$ | $1.6 \%$ | 0.001 |  |  |  |  |  |
|  |  | 13B | $0.7 \%$ | $0.9 \%$ | $3.3 \%$ | $0.2 \%$ | $0.6 \%$ | $0.8 \%$ | -0.001 |  |  |  |  |  |

Table 5: Comparison of models with and without FIM training. pass@1, pass@10 and pass@100 scores on HumanEval and MBPP evaluated at temperature 0.1 for models trained with and without infilling (FIM) objective. Infilling training incurs no cost on autoregressive test set loss, but a small cost on HumanEval and MBPP pass @k metrics that is aggravated at higher sample counts $k$. The models are compared prior to long context fine-tuning (LCFT).

| Model | Size | Python |  | Java |  | JavaScript |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | PSM | SPM | PSM | SPM | PSM | $\mathrm{SPM}$ |
| $\overline{\text { In }}$ | $6 \mathrm{~B}$ |  | $31.0 \%$ |  | $49.0 \%$ |  | $51.0 \%$ |
| Sar | $1.1 \mathrm{~B}$ |  | $44.0 \%$ |  | $62.0 \%$ |  | $60.0 \%$ |
| StarCoder | $15.5 \mathrm{~B}$ |  | $62.0 \%$ |  | $73.0 \%$ |  | $74.0 \%$ |
| Code Llama | $7 \mathrm{~B}$ | $67.6 \%$ | $72.7 \%$ | $74.3 \%$ | $77.6 \%$ | $80.2 \%$ | $82.6 \%$ |
|  | $13 \mathrm{~B}$ | $68.3 \%$ | $74.5 \%$ | $77.6 \%$ | $80.0 \%$ | $80.7 \%$ | $85.0 \%$ |

Table 6: Multilingual HumanEval single line infilling with MultiPL-E. Exact match rates on the line infilling benchmark from Allal et al. (2023) with greedy decoding. Evaluated in both prefix-suffix-middle (PSM) and suffix-prefix-middle (SPM) format. Numbers for InCoder, SantaCoder and StarCoder are reported from Li et al. (2023).

Key retrieval. In Figure 4b, we investigate key retrieval performance in synthetic task. The prompt consists of a large amount of syntactically valid Python code, with a function returning a scalar inserted at a specified position. The model is asked to complete an assert statement with the return value of the inserted function. Liu et al. (2023b) showed that the inability to recall content placed in the middle of long prompts is a common failure mode in LLMs; our retrieval task is analogous to their setup, albeit tailored to code models which are not fine-tuned to follow instructions. All models exhibit strong retrieval performance on the sequence length they were trained on, with the exception of the 7B model for test cases in which the function is placed at the beginning of the prompt. We include OpenAI's gpt-3.5-turbo-16k-0613 as a reference. We query GPT with a system prompt of "Complete the following code." and a temperature of 0 . For sequences beyond $16 \mathrm{~K}$ tokens, i.e., when extrapolating, our models exhibit a decrease in performance (Appendix G.3).

Single line completion. Finally, we test the benefits of the ability to handle long context sizes in a single line code completion task. Our task is based on the Long Code Completion (LCC) benchmark (Guo et al., 2023). ${ }^{2}$ The LCC test set is skewed towards shorter files and we hence sample a new set of examples from LCC's validation and test set with an equalized distribution over file size (Appendix G.2). In Table 7, we compare the completion accuracy of the CODE Llama models to their counterparts prior to long-context fine-tuning. Non-LCFT models fail to generate meaningful completions on long sequences and we thus truncate their prompts to the 4,000 tokens immediate preceding the line to complete. Across all metrics, models fine-tuned to handle long contexts achieve significantly higher performance. This demonstrates that long contexts are informative for code completion, and that with LCFT our models are able to leverage this information to improve their generations. We note that the longest example's prompt in this test consists[^1]

![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-12.jpg?height=501&width=1651&top_left_y=205&top_left_x=234)

![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-12.jpg?height=412&width=732&top_left_y=214&top_left_x=249)

(a)

![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-12.jpg?height=414&width=730&top_left_y=213&top_left_x=1142)

(b)

Figure 4: Code Llama behavior on long sequences. (a) Perplexity on large source files ( $\geq 50 \mathrm{kB})$ from the validation data from the code dataset. The dashed line marks the fine-tuning context length. Perplexity decreases for up to $100 \mathrm{~K}$ tokens for all Code Llama sizes. (b) Accuracy on a synthetic key retrieval task, with a context of $16 \mathrm{~K}$ tokens and comparison to gpt-3.5-turbo.

| Model |  |  |  |  |  |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | EM | BLEU | EM | BLEU | EM | BLEU |
| CODE LLAMA | 7B | $\boldsymbol{x}$ | 36.86 | 60.16 | 47.82 | 69.20 | 46.29 | 67.75 |
| CoDE LLAMA | 7B | $\checkmark$ | $\mathbf{3 9 . 2 3}$ | $\mathbf{6 1 . 8 4}$ | $\mathbf{5 1 . 9 4}$ | $\mathbf{7 1 . 8 9}$ | $\mathbf{5 0 . 2 0}$ | $\mathbf{7 0 . 2 2}$ |
| CoDE LLAMA | 13B | $\boldsymbol{x}$ | 37.96 | 61.33 | 50.49 | 69.99 | 49.22 | 69.87 |
| CODE LLAMA | 13B | $\checkmark$ | $\mathbf{4 1 . 0 6}$ | $\mathbf{6 2 . 7 6}$ | $\mathbf{5 2 . 6 7}$ | $\mathbf{7 2 . 2 9}$ | $\mathbf{5 2 . 1 5}$ | $\mathbf{7 1 . 0 0}$ |
| CoDE LLAMA | 34B | $\boldsymbol{x}$ | 42.52 | 63.74 | 54.13 | 72.38 | 52.34 | 71.36 |
| CODE LLAMA | 34B | $\checkmark$ | $\mathbf{4 4 . 8 9}$ | $\mathbf{6 5 . 9 9}$ | $\mathbf{5 6 . 8 0}$ | $\mathbf{7 3 . 7 9}$ | $\mathbf{5 3 . 7 1}$ | $\mathbf{7 2 . 6 9}$ |

Table 7: Average single line completion performance on LCC-balanced. Comparison of models before and after long-context fine-tuning in terms of exact match (EM) and BLEU. For non-LCFT models, context size limits are respected by truncating prompts to 4,000 tokens.

of 103K tokens, for which all Code Llama models generate syntactically correct completions, with the 7B model producing an exact match.

Performance impact on short contexts. While our models are effective on long sequences, we observe that LCFT slightly hurts performance on standard code synthesis benchmarks consisting of short sequences. In Table 10, we observe an average decrease of 0.52 percentage points on HumanEval pass@1 and 1.9 points on MBPP for the pass@1 metric. Similarly, a breakdown of the code completion results in Table 7 by the number of tokens in each example shows that for prompts shorter than $4 \mathrm{k}$ tokens, long context fine-tuning induces a reduction of up to 2 BLEU points from base models after code training (Figure 9b). We observe similar decreases in performance for infilling tasks (Table 14).

LCFT comes at a cost for short sequences, and slightly decreases our scores on standard coding benchmarks such as HumanEval and MBPP. However, many real-world use cases are not captured by these benchmarks, and we believe that this cost is more than offset by the potential of handling long sequences for real downstream applications. Hence we opt to release all our Code Llama, Code Llama - Python and Code Llama - Instruct models with long-context capabilities.

![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-13.jpg?height=491&width=1653&top_left_y=199&top_left_x=233)

![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-13.jpg?height=390&width=515&top_left_y=217&top_left_x=249)

(a)

![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-13.jpg?height=393&width=523&top_left_y=215&top_left_x=801)

(b)

![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-13.jpg?height=398&width=540&top_left_y=213&top_left_x=1340)

(c)

Figure 5: (a) Training perplexity of Code Llama models. The continued decrease at 500B tokens suggests further training would be beneficial. Results are presented without infilling for 7B and 13B models. (b) Training losses of both Code Llama 7B versus an identical model trained from scratch (c) MBPP (coding benchmark) vs. Helpfulness according to the helpfulness reward model from Llama 2 (Touvron et al., 2023b).

### 3.4 Ablation studies

### 3.4.1 Fine tuning Llama 2 vs. training from scratch on code

Code Llama is based on the Llama 2 models, which are trained on $2 \mathrm{~T}$ tokens of text, including only 80B tokens of code. We tune these models on 500B extra tokens, consisting mostly of code (85\%). Figure 5a shows the training curves of Code Llama.

We compare the 7B parameters model to an identical model trained from scratch on the same data mix (Figure 5b). At the end of training, the loss of the model trained from scratch is equal to the loss of Code Llama 7B at about half of its training (with 240B less training tokens). Moreover, this gap becomes larger over time.

### 3.4.2 Instruction fine-tuning

General helpfulness vs. coding ability We evaluate Code Llama - InSTRUCt and compare it to Llama 2-Chat for coding tasks and helpfulness (Figure 5c). We observe that Code Llama improves its coding abilities for each model sizes, while preserving the general helpfulness performance inherited from Llama 2. The results on the helpfulness axis is an indication that Code Llama performs greatly on general instructions following. But we emphasize that this result should be taken with a grain of salt, since we limited our automatic evaluation to scoring the models answers with LLAMA 2 reward model.

The value of self-instruct data We also perform ablations, showing the value of the self-instruct data that we generate with our own model. To evaluate the capacity of the model to answer questions, we use a zero-shot version of MBPP. We prompt the model to generate the code between [PYTHON] and [/PYTHON] tags to make it easy to parse the result. Our exact prompt is shown in Figure 13 in the Appendix. Table 8 show the impact of training on data generated using our models and filtered with unit tests as described in Section 2.5. The self-instruct data allows us to improve our scores on benchmarks such as HumanEval and MBPP. It also makes the training more reliable. With self-instruct, the model easily learns to follow the format requested for MBPP zero-shot while it sometimes fails without it.

Unnatural model. For comparison purposes, we also finetuned Code Llama - Python 34B on 15,000 unnatural instructions similarly to Honovich et al. (2023) using the same prompts as for the self-instruct dataset. We do not release this model, but we observe clear improvements on HumanEval and MBPP which are indicative of the improvements that can be reached with a small set of high-quality coding data. The results of the unnatural model are shown in Table 2.

| Size | SI | HumanEval | MBPP |  |  |  |
| ---: | ---: | ---: | ---: | ---: | :---: | :---: |
|  |  |  | 3-shot |  |  | zero-shot |
| 7B | $\boldsymbol{X}$ | $30.5 \%$ | $43.4 \%$ | $37.6 \%$ |  |  |
|  | $\checkmark$ | $34.8 \%$ | $44.4 \%$ | $37.4 \%$ |  |  |
| $13 B$ | $\boldsymbol{X}$ | $40.9 \%$ | $46.2 \%$ | $20.4 \%$ |  |  |
|  | $\boldsymbol{\checkmark}$ | $42.7 \%$ | $49.4 \%$ | $40.2 \%$ |  |  |

Table 8: Impact of self-instruct data. Impact of self-instruct data (SI) on the MBPP and HumanEval scores of our self-instruct models. The scores are computed using greedy decoding. In MBPP zero-shot, we prompt the model to generate the solution between [PYTHON] [/PYTHON] tags. Removing SI results in generally lower scores on HumanEval and MBPP, and makes learning to generate code with the right format for MBPP zero shot much less reliable.
![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-14.jpg?height=890&width=1656&top_left_y=812&top_left_x=234)

Figure 6: Code Llama scores different temperature values. Results are presented for 7B, 13B, and 34B models on HumanEval and MBPP benchmarks. We report Pass@1, Pass@10, and Pass@100 for different temperature values. We use nucleus sampling with $\mathrm{p}=0.95$.

### 3.4.3 Pass@k evaluation

We study the effect of the sampling temperature on the pass@k performance. Specifically, we report pass@1, 10 , and 100 using temperature $\in\{0.1,0.4,0.6,0.8\}$ on both HumanEval and MBPP. Results are depicted in Figure 6. As expected, as we increase the temperature, the pass@1 scores are getting worse while the pass@10 and pass@100 improve.

## 4 Responsible Al and safety

Large language models have been shown to have the potential to produce known falsehoods due to misconceptions or false beliefs (Lin et al., 2022), generate toxic or offensive content (Hartvigsen et al., 2022) and reproduce or even amplify the biases that are contained in the training data (Dhamala et al., 2021). As
mentioned in Section 2.5, we make Code Llama - Instruct safer by fine-tuning on outputs from Llama 2, including adversarial prompts with safe responses, as well as prompts addressing code-specific risks.

In this section, we perform evaluations on three widely-used automatic safety benchmarks from the perspectives of truthfulness, toxicity, and bias, respectively. Specifically, we assess the safety capabilities of both pretrained Code Llama and fine-tuned Code Llama - Instruct with Falcon (Almazrouei et al., 2023), MPT (MosaicML, 2023), and StarCoder (Li et al., 2023). Although we have chosen certain standard benchmarks commonly used in the language model community to highlight some of the problems with these models, it's important to note that these evaluations alone do not provide a comprehensive understanding of the risks associated with them. We complement the safety analysis of Code Llama - Instruct with additional red teaming from various domain experts in offensive security, malware development, responsible AI and software engineering, similar to Touvron et al. (2023b).

Truthfulness. We use TruthfulQA (Lin et al., 2022) to gauge the factuality and common sense of our models. The TruthfulQA benchmark comprises 817 questions spread across 38 categories, encompassing topics such as health, finance, law, and politics (Lin et al., 2022). The questions are designed to be challenging, even for humans, causing them to answer incorrectly due to unfounded beliefs or misconceptions. To evaluate the generated outputs from LLMs, we utilize GPT-3-based metrics following Lin et al. (2022) to determine the truthfulness and informativeness of the outputs. For the QA prompt, we use a few-shot prompt containing 6 random QA pairs, structured according to the InstructGPT format (Ouyang et al., 2022). The results are reported as the percentage of generations that are both truthful and informative, as well as the percentage that are either truthful or informative.

Toxicity. We use ToxiGen (Hartvigsen et al., 2022) to quantify the extent of toxic language and hate speech generation across various demographic groups. The ToxiGen dataset contains implicitly toxic and benign sentences mentioning 13 minority groups. Following Touvron et al. (2023b), we utilize an improved version of the dataset, which minimizes noise by removing prompts with disagreements among annotators regarding the target demographic group. To measure the toxicity of the generated outputs from each of the LLMs, we employ the default ToxiGen classifier, tuned on RoBERTa (Liu et al., 2019).

Bias. We employ the Bias in Open-Ended Language Generation Dataset (BOLD) (Dhamala et al., 2021) to investigate how the sentiment in the model's outputs may differ based on demographic attributes. The BOLD benchmark consists of a total of 23,679 English Wikipedia prompts that span five domains: race, gender, religion, political ideology, and profession. These prompts cover 43 different subgroups. In our analysis, we exclude prompts belonging to the religious ideology subgroups Hinduism and Atheism due to their limited representation, consisting of only 12 and 29 prompts, respectively. To assess the sentiments conveyed by the combination of the prompt prefix and model generation, we employ sentiment analysis using the Valence Aware Dictionary and Sentiment Reasoner (VADER) (Hutto \& Gilbert, 2014). The VADER produces sentiment scores between -1 and 1 , where a positive (negative) score indicates a positive (negative) sentiment towards the population mentioned in the prompt. A score closer to 0 indicates a neutral sentiment.

Benchmark evaluation results. Table 9 shows the evaluation results of the three safety benchmarks. We follow the decoding setting as in Touvron et al. (2023b) where a temperature of 0.1 and top-p of 0.9 are used. Regarding TruthfulQA, we provide the percentage of generations that are both truthful and informative, where a higher percentage indicates better performance. Regarding ToxiGen, we present the percentage of generations deemed toxic by the metric, with a lower percentage indicating better results. Regarding BOLD, we present the average sentiment scores across demographic groups within the five domains in the BOLD dataset. The fine-tuned Code Llama - Instruct exhibits significant improvements over the pretrained Code Llama in terms of truthfulness (from 34.64 to 47.37 for 34B) and toxicity (from 17.62 to 0.00 for $34 B$ ). The percentage of toxic generations drastically reduces to virtually $0 \%$ across all CoDE LLAMA sizes, making it the least toxic among all the models compared. When compared to Falcon and MPT fine-tuned models, the fine-tuned Code Llama demonstrates the second-best performance level in both toxicity and truthfulness, right after Llama 2 Chat. Additionally, similar to Llama 2 Chat, the Code Llama - Instruct, after fine-tuning, also tends to show an overall increase in positive sentiment for many demographic groups in BOLD. More detailed results split by different demographic groups can be found in Appendix I.

![](https://cdn.mathpix.com/cropped/2024_05_26_bf4f1492c82fe3a31d04g-16.jpg?height=618&width=813&top_left_y=217&top_left_x=648)

Figure 7: KDE plot of the risk score output by the LlAMA 2 safety reward model on prompts with clear intent specific to code risk created by red teamers with background in cybersecurity and malware generation.

Red teaming. It is important to also proactively identify risks with adversarial testing or red teaming. We conducted 3 red teaming exercises with 25 Meta employees, including domain experts in responsible AI, malware development, and offensive security engineering.

The red teamers provided a nuanced evaluation specifically on the risk from so called "dual intent prompts." Dual intent prompts are requests for help with writing code that could be used maliciously but the prompt does not directly address the topic (example "Mosaic Prompts" Glukhov et al. (2023)). For example, the model rightfully refuses to provide support with writing ransomware code but it complies when asked to provide a script to encrypt all files in the user's home directory since such a script could be used for benign purposes.

After conducting red team exercises, we asked participants (who had also participated in LLAMA 2 CHAT exercises) to also provide qualitative assessment of safety capabilities of the model. Some participants who had expertise in offensive security and malware development questioned the ultimate risk posed by "malicious code generation" through LLMs with current capabilities.

One red teamer remarked, "While LLMs being able to iteratively improve on produced source code is a risk, producing source code isn't the actual gap. That said, LLMs may be risky because they can inform low-skill adversaries in production of scripts through iteration that perform some malicious behavior."

According to another red teamer, "[v]arious scripts, program code, and compiled binaries are readily available on mainstream public websites, hacking forums or on 'the dark web.' Advanced malware development is beyond the current capabilities of available LLMs, and even an advanced LLM paired with an expert malware developer is not particularly useful- as the barrier is not typically writing the malware code itself. That said, these LLMs may produce code which will get easily caught if used directly."

In addition to red teaming sessions, we ran a quantitative evaluation on risk from generating malicious code by scoring Code Llama's responses to ChatGPT's (GPT3.5 Turbo) with LLAMAv2 70B's safety reward model. For this second quantitative evaluation, we selected prompts that the red teamers generated specifically attempting to solicit malicious code (even though the red teaming included consideration of a broad set of safety risks). These prompts were a mix of clear intent and slightly obfuscated intentions (see some examples in Figure 16. We show a KDE plot of the distribution of the safety score for all models in Figure 7). We observe that Code Llama tends to answer with safer responses; the distribution of safety scores for Code Llama has more weight in the safer part of the range.

False refusals. LLMs that are too safe can have a tendency to over-refuse valid claims similar to what was reported after the release of Llama 2. We specifically asked red teamers to test for this behavior. They found some limited evidence of false refusals (when not using a system preprompt). False refusals could also

|  | TruthfulQA $\uparrow$ | ToxiGen $\downarrow$ | BOLD |
| :--- | :---: | :---: | :---: |
| Pretrained models |  |  |  |
| Falcon 7B | 25.95 | 14.53 | 0.283 |
| MPT 7B | 29.13 | 22.32 | 0.322 |
| StarCoder (Python) 15.5B | 22.77 | $\mathbf{1 0 . 3 6}$ | 0.310 |
| LLAMA 2 7B | 33.29 | 21.25 | 0.304 |
| LLAMA 2 13B | 41.86 | 26.10 | 0.330 |
| LLAMA 2 34B | $\mathbf{4 3 . 4 5}$ | 21.19 | 0.318 |
| CoDE LLAMA 7B | 26.19 | 22.64 | 0.230 |
| CoDE LLAMA 13B | 33.29 | 22.45 | 0.176 |
| CoDE LLAMA 34B | 34.64 | 17.62 | 0.255 |
| Instruct (aligned) |  |  |  |
| Falcon-instruct 7B | 28.03 | 7.89 | 0.332 |
| MPT-instruct 7B | 29.99 | 16.33 | 0.302 |
| LLAMA 2 CHAT 7B | 57.04 | $\mathbf{0 . 0 0}$ | 0.482 |
| LLAMA 2 ChAT 13B | 62.18 | $\mathbf{0 . 0 0}$ | 0.471 |
| LLAMA 2 ChAT 34B | $\mathbf{6 7 . 2 0}$ | 0.02 | 0.461 |
| CoDE LLAMA - INSTRUCT 7B | 31.46 | 0.04 | 0.503 |
| CoDE LLAMA - INSTRUcT 13B | 36.84 | 0.01 | 0.365 |
| CoDE LLAMA - INSTRUcT 34B | 47.37 | $\mathbf{0 . 0 0}$ | 0.452 |

Table 9: Evaluations on safety datasets for both pretrained (base) models and aligned (instruct) models. For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher, the better). For ToxiGen, we present the percentage of toxic generations (the smaller, the better). For BOLD, we present the average sentiment scores across demographic groups. A score closer to 0 indicates a neutral sentiment, while a positive (negative) score indicates a positive (negative) sentiment towards the population mentioned in the prompt.

be solved by rephrasing the prompt e.g. "Can you tell me how to kill a process?" rephrased to "How do I kill a process?". We show some examples in Appendix Table 15. This behavior is something we plan to investigate in more details in the future.

Safety and coding performance. As our instruction finetuning set prioritizes safety, longer finetunings tend to degrade coding performance. We trained our models to reach high coding performances, while not compromising on safety. As shown in Figure 7, our Code Llama - Instruct models are safer than ChatGPT.

## 5 Related work

Early observations with LLMs such as GPT-Neo (Black et al., 2021) or GPT-J (Wang \& Komatsuzaki, 2021) showed that adding code in the training data makes program synthesis possible even with medium size LLMs. Code from open-source software is now a standard part of the training data for general-purpose LLMs such as PaLM (Chowdhery et al., 2022), Chinchilla (Hoffmann et al., 2022), Gopher (Rae et al., 2021), GPT-4 (OpenAI, 2023), and Llama (Touvron et al., 2023a;b). In parallel, models specifically trained or fine-tuned for code understanding and program synthesis from natural language prompts emerged with LLMs such as Codex (Chen et al., 2021), CodeT5 (Wang et al., 2021), InCoder (Fried et al., 2023), AlphaCode (Li et al., 2022), CodeGen (Nijkamp et al., 2023b) and CodeGen 2 (Nijkamp et al., 2023a), GPT-NeoX (Black et al., 2022), SantaCoder (Allal et al., 2023), StarCoder (Li et al., 2023) and phi-1 (Gunasekar et al., 2023), consistently demonstrating better performance on code benchmarks than general-purpose LLMs of comparable or even larger size. This paper follows this line, by fine-tuning the recent general-purpose language model LlAMA 2 on code data.

Closed-source vs open-source models. The landscape of LLMs is marked by whether the technology is free and the code is available for research or commercial use. ChatGPT and GPT-4 (OpenAI, 2023), PaLM

(Chowdhery et al., 2022) and Chinchilla (Hoffmann et al., 2022) are closed source, while BLOOM (Scao et al., 2022), OPT (Zhang et al., 2022b), and the seminal work of Llama are public (Touvron et al., 2023a). The more recent LLAMA 2 has been released under a custom licence for commercial use (Touvron et al., 2023b). A similar dichotomy exists for code models, with Codex/copilot (Chen et al., 2021), AlphaCode (Li et al., 2022), GPT-4 or phi-1 (Gunasekar et al., 2023) being closed source, whereas the recent SantaCoder (Allal et al., 2023) and StarCoder (Li et al., 2023) have been released open-source and allow for commercial use. In this work, we allow for commercial use of the models under the same terms as Llama 2. Moreover, our largest model, with its 70B parameters, is significantly larger than previous open-source models - GPT-NeoX-20B (Black et al., 2022) and StarCoder with 15.5B parameters - which allows it to achieve state-of-the-art performances on HumanEval, MBPP and MultiPL-E among open-source models. - GPT-NeoX-20B (Black et al., 2022) and StarCoder with 15.5B parameters - which allows it to achieve state-of-the-art performances on HumanEval, MBPP and MultiPL-E among open-source models.

Data. It is well-known that data quality is critical in the training and responsible development of LLMs (e.g., Hoffmann et al., 2022; Penedo et al., 2023), and this is also true for code as discussed by Allal et al. (2023). Modern models are trained on publicly available, open-source code. In addition, Allamanis (2019) and Allal et al. (2023) discuss the impact of effective deduplication and of selecting code from repositories based on the number of GitHub stars (as a proxy for popularity), while Li et al. (2023) augment their data with GitHub issues and commits collected from BigQuery. Gunasekar et al. (2023) filter data up to only containing "textbook"-quality code and add synthetic problems collected using GPT-3.5, following Jung et al. (2023), in order to obtain good performance on simple benchmarks such as HumanEval and MBPP. We follow the approach of learning from publicly available code only, without additional meta-level or temporal information such as issues or commits. We also do not train our foundation models on additional synthetic exercises, since we did not want to take the risk of reducing the scope of our models to simple coding exercises similar to those contained in HumanEval and MBPP.

Code understanding and synthesis tasks. In addition to program synthesis from natural language prompts or infilling (Fried et al., 2023; Bavarian et al., 2022; Li et al., 2023; Nguyen et al., 2023), many tasks related to code understanding or synthesis have been addressed since the early 2020s with NLP models adapted for code (Raffel et al., 2020; Feng et al., 2020; Guo et al., 2021; Wang et al., 2021; Ahmad et al., 2021), also see the survey by Xu \& Zhu (2022). These tasks include code summarization, refinement, translation (Rozière et al., 2020; 2021; Szafraniec et al., 2023) fixing bugs (Yasunaga \& Liang, 2021; Zhang et al., 2022a; Prenner et al., 2022), fixing build errors (Tarlow et al., 2020) or generating unit tests (Tufano et al., 2020; Li et al., 2022; Chen et al., 2023a), as well as solving math problems as demonstrated by PaLM (Chowdhery et al., 2022) or Codex (Chen et al., 2021). 14 code understanding tasks are represented in the CodeXGlue benchmark (Lu et al., 2021). Here we focused on the main problem of program synthesis, as well as infilling/completion for our 7B and 13B models where the ability comes with little impact on the generation performance as previously observed by Bavarian et al. (2022).

Additional modifications to LLM training and inference. A number of works proposed to incorporate within the training objective structural knowledge of programs, with specialized objectives for code deobfuscation (Lachaux et al., 2021), contrastive learning through semantic-preserving code transformations (Jain et al., 2021), leveraging Abstract Syntax Trees to learn tree-aware positional encodings (Shiv \& Quirk, 2019; Peng et al., 2021). A recent stream of work takes into account program execution or unit tests to filter, cluster, or improve the correctness of programs when few candidates must be submitted (Li et al., 2022; Chen et al., 2023a; Le et al., 2022; Zhang et al., 2023), or unit tests them within a reinforcement learning objective to enrich the training signal (Le et al., 2022; Liu et al., 2023a). We focused here on improving the base model rather than tweaking the inference scheme, since we believe this is where most of the long-term progress comes from; it is nonetheless an interesting direction to experiment with more elaborated inference schemes on top of Code Llama.

Long sequences in LLMs. Scaling Transformers and LLMs to long input sequences has attracted much recent interest (Dai et al., 2019; Beltagy et al., 2020; Yu et al., 2023; Ding et al., 2023). The context lengths supported by available models and APIs has seen a steady increase, with StarCoder being trained on $8 \mathrm{~K}$
token sequences ((Li et al., 2023), up from the $4 \mathrm{~K}$ of Allal et al. (2023)), recent GPT versions supporting 16K (gpt-3.5-turbo-16k) and 32K tokens (gpt-4-32k), MPT-7b fine-tuned on 65K tokens (MosaicML, 2023), and Claude featuring 100K context windows (Anthropic, 2023). Previous research focuses on alleviating the $O\left(n^{2}\right)$ space and time complexity of self-attention (Vaswani et al., 2017) by introducing sparsity patterns, as well as by encoding positional information in such a way that models can leverage input sizes larger than those presented at training time (length extrapolation). In our work, we do not rely on hand-crafted sparsity patterns such as those proposed for code input by Guo et al. (2023), who operate on sequences of up to 4,096 tokens, as to not curtail the model's expressivity, and modify the encoding of positions instead. Starting from pretrained Llama 2 models that utilize RoPE (Su et al., 2021), Chen et al. (2023b) propose additional fine-tuning for long sequence handling, an approach we pursue as well. However, we tailor our hyper-parameter modifications to allow for extrapolation at inference time. Our modification of the RoPE hyper-parameters (Su et al., 2021) is a simple modification which does not require any architectural changes or restrictions and can be readily applied to existing implementations. ${ }^{3}$ Press et al. (2022) propose a linear bias for attacking extrapolation; in contrast, our approach seeks to reduce existing bias towards shot-range attention. Recent work suggests that causal models do not require an explicit encoding of position information (Haviv et al., 2022; Kazemnejad et al., 2023), a hypothesis we did not test in this work as we demonstrated that starting from pretrained Llama 2 models is significantly more efficient than training from scratch.

## 6 Discussion

We release a family of code-specialized Llama 2 models called Code Llama, with three main variants that we release with four sizes (7B, 13B, 34B, and 70B parameters): Code Llama, Code Llama - Python, Code Llama - Instruct. With real-world applications in mind, we trained our 7B, 13B, and 70B models to support infilling, and all our models to leverage large contexts. We tested their stability in inference up to 100K tokens (Figure 4a). Large context fine-tuning and infilling come at a cost on standard benchmarks left-to-right code generation benchmarks (Table 10), that are all based on short sequences (i.e. function level). Still, our 70B model is state-of-the-art among public models on standard python completion benchmarks, and our other models are competitive compared to models with similar numbers of parameters. On multilingual benchmarks, even our smallest model (CodE Llama 7B) outperforms every other public model.

The Code Llama - Instruct models are trained to provide zero-shot instruction ability to Code Llama. In this further fine-tuning, where we somewhat distillate Llama 2-Chat, we focused not only on being more directly helpful (Figure 5c) but also sought to provide a safer model to use and deploy (Section 4). Following instruction and being overly safe can cost some points on evaluations (e.g. on HumanEval for the 34B model in Table 2), as exemplified in Figure 15. Further work is needed for LLMs to understand context and nuance in their instructions.[^2]

## References

Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A causal masked multimodal model of the internet. arXiv:abs/2201.07520, 2022.

Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training for program understanding and generation. In NAACL-HLT, pp. 2655-2668. Association for Computational Linguistics, 2021.

Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Mu noz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy-Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. SantaCoder: Don't reach for the stars! arXiv:abs/2301.03988, 2023.

Miltiadis Allamanis. The adverse effects of code duplication in machine learning models of code. In Onward!, pp. 143-153. ACM, 2019.

Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: An open large language model with state-of-the-art performance, 2023.

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. PaLM 2 Technical Report. arXiv:abs/2305.10403, 2023.

Anthropic. Introducing 100K Context Windows, 2023. URL https://www.anthropic.com/index/ $100 \mathrm{k}$-context-windows.

Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. arXiv:abs/2108.07732, 2021.

Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv:abs/2207.14255, 2022.

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:abs/2004.05150, 2020.

Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow, 2021. URL https://doi.org/10.5281/zenodo.5297715.

Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. arXiv:abs/2204.06745, 2022.

Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. MultiPL-E: A scalable and polyglot approach to benchmarking neural code generation. IEEE Trans. Software Eng., 49(7):3675-3691, 2023.

Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. CodeT: Code generation with generated tests. In ICLR, 2023a.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv:abs/2107.03374, 2021.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv:abs/2306.15595, 2023b.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways. arXiv:abs/2204.02311, 2022.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. $\operatorname{arXiv:abs/2110.14168,} 2021$.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In ACL (1), pp. 2978-2988. Association for Computational Linguistics, 2019.

Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. BOLD: Dataset and metrics for measuring biases in open-ended language generation. In FAcc T, pp. 862-872. ACM, 2021.

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. LongNet: Scaling transformers to 1, 000, 000, 000 tokens. arXiv:abs/2307.02486, 2023.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for programming and natural languages. In EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pp. 1536-1547. Association for Computational Linguistics, 2020.

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. InCoder: A generative model for code infilling and synthesis. In ICLR, 2023.

David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, and Vardan Papyan. LLM censorship: A machine learning challenge or a computer security problem? arXiv:abs/2307.10719, 2023.

Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. arXiv:abs/2306.11644, 2023.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. GraphCodeBERT: Pre-training code representations with data flow. In ICLR, 2021.

Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian J. McAuley. LongCoder: A long-range pre-trained language model for code completion. arXiv:abs/2306.14893, 2023.

Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In $A C L$ (1), pp. 3309-3326. Association for Computational Linguistics, 2022.

Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. In EMNLP (Findings), pp. 1382-1390. Association for Computational Linguistics, 2022.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with APPS. In NeurIPS Datasets and Benchmarks, 2021.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv:abs/2203.15556, 2022.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In $I C L R, 2020$.

Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In $A C L$ (1), pp. 14409-14428. Association for Computational Linguistics, 2023.

Clayton J. Hutto and Eric Gilbert. VADER: A parsimonious rule-based model for sentiment analysis of social media text. In $I C W S M$. The AAAI Press, 2014.

Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez, and Ion Stoica. Contrastive code representation learning. In EMNLP (1), pp. 5954-5971. Association for Computational Linguistics, 2021.

Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi. Impossible distillation: From low-quality model to high-quality dataset \& model for summarization and paraphrasing. arXiv:abs/2305.16635, 2023.

Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. arXiv:abs/2305.19466, 2023.

Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In EMNLP (Demonstration), pp. 66-71. Association for Computational Linguistics, 2018.

Marie-Anne Lachaux, Baptiste Rozière, Marc Szafraniec, and Guillaume Lample. DOBF: A deobfuscation pre-training objective for programming languages. In NeurIPS, pp. 14967-14979, 2021.

Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu-Hong Hoi. CodeRL: Mastering code generation through pretrained models and deep reinforcement learning. In NeurIPS, 2022.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo ao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan DolanGavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu noz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder: May the source be with you! arXiv:abs/2305.06161, 2023.

Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode. arXiv:abs/2203.07814, 2022.

Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In $A C L$ (1), pp. 3214-3252. Association for Computational Linguistics, 2022.

Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. RLTF: Reinforcement learning from unit test feedback. arXiv:abs/2307.04349, 2023a.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv:abs/2307.03172, 2023b.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv:abs/1907.11692, 2019.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. CodeXGLUE: A machine learning benchmark dataset for code understanding and generation. In NeurIPS Datasets and Benchmarks, 2021.

Microsoft. A guidance language for controlling large language models., 2023. URL https://github.com/ microsoft/guidance.

Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In FAT, pp. 220-229. ACM, 2019.

MosaicML. Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, 2023. URL https://www.mosaicml.com/blog/mpt-7b.

Anh Nguyen, Nikos Karampatziakis, and Weizhu Chen. Meet in the middle: A new pre-training paradigm. arXiv:abs/2303.07295, 2023.

Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. CodeGen2: Lessons for training LLMs on programming and natural languages. arXiv:abs/2305.02309, $2023 \mathrm{a}$.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. CodeGen: An open large language model for code with multi-turn program synthesis. In ICLR, $2023 b$.

OpenAI. GPT-4 technical report. arXiv:abs/2303.08774, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In $A C L$, pp. 311-318. ACL, 2002.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for falcon LLM: Outperforming curated corpora with web data, and web data only. arXiv:abs/2306.01116, 2023.

Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, and Zhi Jin. Integrating tree path in transformer for code representation. In NeurIPS, pp. 9343-9354, 2021.

Julian Aron Prenner, Hlib Babii, and Romain Robbes. Can OpenAI's codex fix bugs?: An evaluation on QuixBugs. In APR@ICSE, pp. 69-75. IEEE, 2022.

Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In $I C L R, 2022$.

Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \& insights from training gopher. arXiv:abs/2112.11446, 2021.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:140:1-140:67, 2020.

Baptiste Rozière, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised translation of programming languages. In NeurIPS, 2020.

Baptiste Rozière, Jie M. Zhang, François Charton, Mark Harman, Gabriel Synnaeve, and Guillaume Lample. Leveraging automated unit tests for unsupervised code translation. arXiv:abs/2110.06773, 2021.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: A 176B-Parameter open-access multilingual language model. arXiv:abs/2211.05100, 2022.

Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In $A C L$ (1). The Association for Computer Linguistics, 2016.

Vighnesh Leonardo Shiv and Chris Quirk. Novel positional encodings to enable tree-based transformers. In NeurIPS, pp. 12058-12068, 2019.

Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced transformer with rotary position embedding. arXiv:abs/2104.09864, 2021.

Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. In ACL (1), pp. 14590-14604. Association for Computational Linguistics, 2023.

Marc Szafraniec, Baptiste Rozière, Hugh Leather, Patrick Labatut, François Charton, and Gabriel Synnaeve. Code translation with compiler representations. In ICLR, 2023.

Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol, Charles Sutton, and Edward Aftandilian. Learning to fix build errors with Graph2Diff neural networks. In ICSE (Workshops), pp. 19-20. ACM, 2020.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv:abs/2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv:abs/2307.09288, 2023b.

Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. Unit test case generation with transformers. arXiv:abs/2009.05617, 2020.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998-6008, 2017.

Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive language model, 2021.

Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In EMNLP (1), pp. 8696-8708. Association for Computational Linguistics, 2021.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In ICLR, 2022.

Yichen Xu and Yanqiao Zhu. A survey on pretrained language models for neural code intelligence. arXiv:abs/2212.10079, 2022.

Michihiro Yasunaga and Percy Liang. Break-it-fix-it: Unsupervised learning for program repair. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 11941-11952. PMLR, 2021.

Lili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. MEGABYTE: Predicting million-byte sequences with multiscale transformers. arXiv:abs/2305.07185, 2023.

Jialu Zhang, José Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, and Gust Verbruggen. Repairing bugs in python assignments using large language models. arXiv:abs/2209.14876, 2022a.

Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. Planning with large language models for code generation. In ICLR, 2023.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: Open pre-trained transformer language models. arXiv:abs/2205.01068, 2022b.
