# IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages 

Jay Gala* $\quad$ Pranjal A. Chitale ${ }^{* 1,2} \quad$ Raghavan AK ${ }^{1,2} \quad$ Varun Gumma ${ }^{3 \dagger}$ Sumanth Doddapaneni ${ }^{1,2}$<br>Aswanth Kumar ${ }^{6 \dagger}$ Janki Nawale $^{1}$ Anupama Sujatha ${ }^{1} \quad$ Ratish Puduppully $^{7}$ Vivek Raghavan $^{1,4 \ddagger}$<br>Pratyush Kumar ${ }^{1,2,3 \S}$ Mitesh M. Khapra ${ }^{1,2}$ I $\quad$ Raj Dabre ${ }^{5} \quad$ Anoop Kunchukuttan $^{1,2,3}$<br>${ }^{1}$ Nilekani Centre at AI4Bharat $\quad{ }^{2}$ Indian Institute of Technology Madras $\quad{ }^{3}$ Microsoft $\quad{ }^{4}$ EkStep Foundation<br>${ }^{5}$ National Institute of Information and Communications Technology, Kyoto, Japan ${ }^{6}$ Flipkart<br>${ }^{7}$ Institute for Infocomm Research $\left(I^{2} R\right), A^{*}$ STAR, Singapore

Reviewed on OpenReview: https://openreview.net/forum?id=vfT4YuzAYA


#### Abstract

India has a rich linguistic landscape, with languages from 4 major language families spoken by over a billion people. 22 of these languages listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Before this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models that support all 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of $230 \mathrm{M}$ bitext pairs, of which a total of $126 \mathrm{M}$ were newly added, including $644 \mathrm{~K}$ manually translated sentence pairs created as part of this work. Our second contribution is the release of the first $n$-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and conversational test sets. Next, we present IndicTrans2, the first translation model to support all 22 languages, surpassing existing models in performance on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2.


## 1 Introduction

India is a linguistically diverse region, with 1,369 distinct mother tongues identified in the census conducted in 2011. Of these, 22 languages have been listed in the $8^{\text {th }}$ Schedule of the Constitution of India. Approximately $97 \%$ of the population of India speaks one of these 22 languages as their first language. English is widely spoken and serves as the default medium of formal communication in many areas, particularly in business, education, government, and judiciary.[^0]

With such linguistic diversity, the importance in India of language translation for effective communication, social inclusion, equitable access, and national integrity cannot be over-emphasized. For example, for effective dissemination of information about government policies and welfare schemes, it is necessary to translate official documents and websites into regional languages. In the context of the judiciary, it is crucial to translate court proceedings and judgments into regional languages so that the petitioners, accused, and witnesses can understand and better participate in the judicial process. Similarly, in the context of education, translation can ensure that high-quality content becomes accessible to more learners in their regional languages. Lastly, translation also plays a vital role in national integration by ensuring that people migrating/traveling to and from different parts of the country can communicate better with people in their new locations.

The last decade has seen rapid progress in Neural Machine Translation, with the latest neural models (Johnson et al., 2017; Liu et al., 2020a; Fan et al., 2020; Kim et al., 2021; Lepikhin et al., 2021; Ramesh et al., 2022; Costa-jussà et al., 2022; Siddhant et al., 2022) supporting hundreds of languages and thousands of translation directions. However, these models either do not have a good coverage of Indian languages, or their performance on Indian languages is poor, or both. Further, none of these models are evaluated on a diverse set of domains or content of Indian origin, as there are no robust benchmarks designed explicitly for Indian languages. Another evidence of the neglect of Indian languages is that in the past 16 years since its inception, the shared tasks run under the Workshop on Machine Translation (WMT) have only covered a total of 4 Indian languages summed across all these years. ${ }^{1}$ While the Workshop on Asian Translation (WAT) (Nakazawa et al., 2022) and the Workshop on Speech and Language Technologies for Dravidian Languages (Madasamy et al., 2022) have made significant contributions, they have not garnered the same level of popularity or academic participation as the WMT. As a result, despite the rapid progress in the broader field of Machine Translation, no single commercial or open-source translation model supports all the 22 languages listed in the Constitution.

In this paper, we pose the following question: What are the missing pieces required for enabling wide and easy access to high-quality machine translation for all 22 scheduled Indian languages? We believe there are four axes of improvement required: (a) curation and creation of significantly larger training datasets, (b) creation of high quality and diverse benchmarks, (c) training and evaluation of multilingual models, and (d) releasing of models with open access. For axis (a) training datasets, we need to create high-quality "seed data" comprising manually translated parallel sentences for all 22 languages with representation from diverse domains. It is to be noted that for several of the 22 languages, no publicly available translation data exists. This manually created data has to be supplemented with a higher volume of semi-automatically generated data by bitext mining from web-scale monolingual corpora and multilingual documents. For axis (b) benchmarks, we need expert-created highly accurate benchmarks for all 22 languages across variations such as formality of language, length of sentences, domain of text, and source originality. For axis (c) models, we need to train accurate multilingual models that exploit the similarity between Indian languages and particularly benefit low-resource languages. We also need to improve processes for the evaluation of models by choosing robust metrics that are shown to correlate with human evaluation for Indian languages. In addition, we need to evaluate models with other metrics, such as improvement in post-editing performance. Finally, for axis (d) open access, created models must have permissive licenses that can be commercially deployed. For instance, Meta's NLLB models, though released in the open, have a CC-BY-NC license precluding commercial usage. In this paper, we contribute across these four axes with many notable firsts that we highlight below.

Training datasets. We release the largest publicly available parallel corpora for Indic languages, the Bharat Parallel Corpus Collection (BPCC). As summarized in Table 1, BPCC contains a total of $\sim 230 \mathrm{M}$ bitext pairs, of which a total of $\sim 126 \mathrm{M}$ were newly added as part of this work. BPCC includes the following:

- Seed training data containing human translations of English sentences to all 22 Indic languages spanning multiple domains. This has a total of $644 \mathrm{~K}$ En-X translation pairs across all languages, including 7 languages for which no manually created parallel data existed before this work.
- Bitext pairs from existing collections such as Samanantar (Ramesh et al., 2022) and NLLB (Costa-jussà et al., 2022) which were further filtered using LaBSE (Feng et al., 2022) based cosine similarity thresholds.[^1]
- New bitext pairs mined from additional monolingual sources such as archive.org and IndicCorp v2 (Doddapaneni et al., 2023) which were not covered in the existing collections mentioned above.
- New bitext pairs mined from additional document-aligned parallel sources such as NPTEL, UGCResources, Prabhupada Vani, etc. which were not covered in the existing collections mentioned above.
- A very large set of $\sim 800$ million back-translated sentences from diverse sources such as IndicCorp v2 (Doddapaneni et al., 2023), monolingual side of NLLB data (Costa-jussà et al., 2022) and CC-Matrix (Schwenk et al., 2021b).

We visualize these types of data in BPCC in Figure 7, to highlight the language coverage and our contributions in relation to existing data. As can be seen, for many languages, BPCC makes the first available datasets, and for all languages, it makes a significant increase in the datasets available.

Benchmarks. We create IN22, the first $n$-way parallel benchmark covering all 22 Indian languages with the English side being source-original. For benchmarks to be of high quality, they must represent content from diverse domains. We visualize the diversity of our created benchmark in Figure 8. Our benchmark contains high-quality human translations for sentences taken from India-specific articles belonging to 13 different domains, viz., Culture, Economy, Education, Entertainment, Geography, Government, Health, Industry, Legal, News, Religion, Sports, and Tourism (see left chart of Figure 8). We refer to this subset as IN22-Gen. Our benchmark has another subset IN22Conv, that contains translations for sentences taken from everyday conversations in the Indian context from 16 different domains, which were manually created by in-house experts starting from carefully created conversation prompts (see right chart of Figure 8).

Models. We release IndicTrans2 (IT2), the first translation model to support all the $\mathbf{2 2}$ scheduled Indian languages, trained on the BPCC dataset. The progress made in the quality of translation in this work with existing open models is captured in Figure 1. The plot shows the chrF++ metric for English to different languages (which is usually the more challenging translation direction for low-resource languages). Each language is represented by circles, where the size of the circle represents the number of speakers in that language. As can be seen, with IndicTrans2, we made progress in translation quality across languages and now support moderate to high-quality translation for most speakers in India. Later in the paper, we also report COMET scores, comparisons with commercial models, and human evaluations of our translations. We find that IT2 is the first model for Indian languages, which performs at par not only with open-source models like NLLB (Costa-jussà et al., 2022) but also with commercial models from Google and Microsoft. We release IndicTrans2-M2M, the first model to support direct translations between all the 22 scheduled Indic languages, supporting 462 translation directions.

Open Access. We aim to promote wider access to accurate translation models for all Indian languages. Therefore, we will release IndicTrans2 and its derivatives (IndicTrans2-M2M, IndicTrans2-Dist) under an open-source license, along with all training data, source code, and tools to enable replication and further improvements by the research community. Additionally, we provide IndicTrans2-Dist, approximately 1/5 the size of IndicTrans2 ( 211M) with comparable performance to reduce deployment costs. We hope our paper will serve as a starting point for future research on Indic machine translation.

Figure 2 provides a comprehensive overview of the entire workflow, which involved the development of requisite human infrastructure, building high-quality seed datasets and robust India-centric benchmarks, and culminates with the release of IndicTrans2, which is the first model to support all the 22 scheduled languages. Section 3 describes the process followed for the creation of high-quality benchmarks and seed training data, which entails the establishment of a human infrastructure, followed by a detailed account of the translation workflow and the quality control procedures implemented. Subsequently, Section 4 outlines our bitext mining pipeline, incorporating both manual and automated checks that employ toxicity and language filters. After the creation of the benchmarks and training data, the next task, as covered in Section 5 is the training of IndicTrans2 with ablation of model architecture, dataset selections, and training procedures. Furthermore, Section 6 describes the robust evaluation of IndicTrans2 across existing benchmarks such as FLORES and the benchmarks we create, across diverse metrics and against both open-source and commercial models.

![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-04.jpg?height=1106&width=1616&top_left_y=282&top_left_x=252)

Figure 1: A visual representation of the advancements in machine translation systems for Indic languages using the IN22-Gen Evaluation set in the En-Indic direction. The depicted values have been subjected to minor adjustments to enhance readability; however, they accurately convey the overall trend. Thresholds are utilized to estimate performance boundaries for various systems across languages. The size of each language bubble is proportional to the speaker count for that language (see Table 57).

The paper concludes with a comprehensive summary and outlines potential future research directions. The Appendices provide supplementary results and additional details, including model and dataset cards.

## 2 Related Work

Languages of India. India, with a population of more than 1.4 billion, is a diverse country known for its rich linguistic heritage, and home to some of the world's most widely spoken languages. According to the Census of India 2011, 1369 mother tongues have been identified of which 121 languages have at least 10,000 speakers and 31 languages have at least a million speakers. ${ }^{2} 22$ of these languages have been listed in the $8^{\text {th }}$ Schedule of the Constitution of India ${ }^{3}$, recognizing them as the scheduled languages of the Republic of India. According to the schedule, the Government of India is under an obligation to take measures to develop these languages such that they become an effective means of communication. Nine of the Indic languages are amongst the most spoken languages across the globe ${ }^{4}$ : Hindi $\left(4^{t h}\right)$, Bengali ( $\left.6^{t h}\right)$, Marathi (13 ${ }^{t h}$ ), Telugu (14 ${ }^{t h}$ ), Tamil (17 ${ }^{t h}$ ), Urdu (20 $\left.{ }^{t h}\right)$, Punjabi (22 ${ }^{\text {nd }}$ ), Gujarati (24 ${ }^{t h}$ ) and Bhojpuri ( $\mathbf{2 6}^{\text {th }}$ ). Some of these languages are also widely spoken and/or are official languages in neighboring countries viz., Bangladesh, Nepal, and Pakistan. Indian languages are also fast-growing across the globe, particularly in North America, the United Kingdom, Australia, and the Middle East. Beyond the Indic languages, English is also[^2]

Table 1: Overall statistics for data collated from different sources (in thousands) for Indian languages and resources in this work. In this document, each language is identified with a BCP 47 tag sequence comprised of ISO 639-3 language subtag and ISO 15924 script subtag.

| Name | Language | Existing |  |  |  |  | BPCC (Newly Added) |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Mined |  | Human |  |  | Mined |  | Human |  |
|  |  | Samanantar | NLLB | NLLB | ILCI | MASSIVE | Monolingual | Comparable | Wiki | Daily |
| Assamese | asm_Beng | 58.8 | 506.3 | - | 82.1 | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-05.jpg?height=43&width=161&top_left_y=616&top_left_x=1120) | 712.5 | 37.8 | 44.7 | 11.3 |
| Bengali | ben_Beng | $2,946.3$ | $13,580.5$ | - | 123.8 | 16.5 | $16,055.1$ | 258.2 | 48.0 | 8.5 |
| Bodo | brx_Deva | - | - | - | 83.2 | - | - | $<1$ | 22.7 | 10.3 |
| Dogri | doi_Deva | - | - | - | - | - | - | - | 18.7 | 5.5 |
| Konkani | gom_Deva | - | - | - | 74.5 | - | - | - | 18.3 | $4.8 \quad$ |
| Gujarati | guj_Gujr | $1,379.2$ | $7,090.3$ | - | 107.4 | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-05.jpg?height=43&width=161&top_left_y=804&top_left_x=1120) | $11,630.3$ | 573.0 | 25.0 | 3.2 |
| Hindi | hin_Deva | $4,416.7$ | $6,646.7$ | - | 165.6 | 16.5 | 27,187.8 | 853.3 | 40.3 | 8.4 |
| Kannada | kan_Knda | $1,692.2$ | $8,871.1$ | - | 76.4 | 16.5 | 12,501.0 | 380.2 | 32.2 | 8.5 |
| Kashmiri | kas_Arab | om | 124.9 | 6.2 | - | - | - | - | 15.5 | 4.3 |
|  | kas_Deva | - | 194.0 | 6.2 | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-05.jpg?height=43&width=186&top_left_y=957&top_left_x=1273) | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-05.jpg?height=43&width=85&top_left_y=957&top_left_x=1694) |
| Maithili | mai_Deva |  | 62.2 | - |  | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-05.jpg?height=43&width=161&top_left_y=995&top_left_x=1120) |  | $<1$ | 24.4 | 4.2 |
| Malayalam | mal_Mlym | $2,029.2$ | $8,818.2$ | - | 87.9 | 16.5 | $12,378.6$ | 356.4 | 41.6 | 8.4 |
| Marathi | mar_Deva | $1,366.1$ | 6,393.2 | - | 117.0 | - | $10,806.0$ | 432.4 | 54.3 | 4.6 |
| Manipuri | mni_Beng |  | 346.9 | 6.2 | 13.1 | - | - | 20.1 | - | $<1$ |
|  | mni_Mtei | - |  | - | 16.0 | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-05.jpg?height=39&width=161&top_left_y=1152&top_left_x=1120) |  |  | 19.9 | 6.8 |
| Nepali | npi_Deva |  | $1,583.5$ | - | 28.6 | - | 10.5 | 6.2 | 45.9 | 10.9 |
| Odia | ory_Orya | 514.9 | $2,382.6$ | - | - | - | $2,863.1$ | 121.5 | 33.7 | 3.2 |
| Punjabi | pan_Guru | $1,418.3$ | $1,978.3$ | - | 71.5 | - | $6,275.8$ | 207.2 | 6.3 | 3.2 |
| Sanskrit | san_Deva | - | 244.1 | - | - | - | - | $<1$ | 27.7 | 5.4 |
| Santali | sat_Olck | - | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-05.jpg?height=43&width=98&top_left_y=1343&top_left_x=922) | - | - | - | - | 22.5 | 1.8 |
| Sindhi | snd_Arab | - | - $2,128.4$ | - | - | - | - | - | - |  |
|  | snd_Deva | - | - | - | - | - | - | - | 10.5 | - $\quad$ - |
| Tamil | tam_Taml | $1,833.2$ | $8,665.2$ | - | 120.7 | 16.5 | $9,690.3$ | 452.8 | 21.0 | 8.6 |
| Telugu | tel_Telu | $1,780.5$ | $10,062.8$ | $\square$ | 73.6 | 16.5 | $11,100.0$ | 437.2 | 29.7 | 8.5 |
| $U r d u$ | $u r d \_A r a b$ |  | $5,321.0$ | - | 101.0 | 16.5 | 484.9 | 225.3 | 41.3 | 8.4 |
| \# Total |  | $19,435.4$ | $84,998.3$ | 18.6 | $1,342.6$ | 115.4 | $121,695.8$ | $4,353.1$ | 644.3 | 139.7 |

widely spoken by in India, with a speaker base of 246 million. ${ }^{5}$ However, even with a large speaker base, many of these languages still lack an online presence and high-quality NLP technologies. Of the 22 scheduled languages, only 4 of them are so-called "Winners" according to the classification by Joshi et al. (2020). It is thus essential to support translation technologies (and NLP technologies in general) for such a large population base to bring the benefits of digital technologies to a large audience. What distinguishes the Indian subcontinent is not only the large speaker base of many languages but also the linguistic diversity of its languages. Languages from four major language families (Indo-Aryan branch of the Indo-European family, Dravidian, Tibeto-Burman, and Austro-Asiatic) are spoken in the subcontinent. According to Wikipedia, ${ }^{6}$ India has amongst the highest linguistic diversity at around 0.914 to 0.93 , depending on the measure. Indic languages are written in a variety of scripts, the majority of which are derived from the Brahmi script. Up to 12 major scripts spanning abugida, alphabetic, and abjad script types are used (Daniels \& Bright, 1996). Underlying this diversity in languages and scripts is also a great deal of similarity at various linguistic levels, owing to language relatedness and contact over a long period (Emeneau, 1956; Subbarao, 2012; Kunchukuttan \& Bhattacharyya, 2020). The diversity of languages and their interactions provide for challenging problems and opportunities in machine translation for Indic languages.

Datasets. We summarize some of the prominent parallel corpora created for Indian languages. The Indian Languages Corpora Initiative (ILCI) (Choudhary \& Jha, 2011) created n-way parallel annotated corpora containing 50K[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-06.jpg?height=800&width=1591&top_left_y=294&top_left_x=256)

Figure 2: Overview of the workflow used for building Bharat Parallel Corpus Collection, IN22 and IndicTrans2.

sentences per language for 12 major Indian languages, covering Health and Tourism domains. However, with the advent of neural MT models, it has been established that these models need large-scale parallel corpora for superior performance (Edunov et al., 2018; Aharoni et al., 2019). Some early attempts include the IIT-Bombay English-Hindi corpus (Kunchukuttan et al., 2018) and the PMIndia corpus (Haddow \& Kirefu, 2020), which aligned sentences from the Prime Minister's speeches in English and 12 Indic languages. The CVIT-PIB corpus (Philip et al., 2021) aligned parallel documents from the Press Information Bureau archives, resulting in English to 11 Indian language pairs. WAT 2021 shared task compiled existing sources to create 9 million sentence pairs between English and Indic languages. Creating parallel corpora for all Indic languages is challenging due to the lack of identifiable parallel documents and the effort required for human annotation at scale. Consequently, attention has turned towards mining parallel corpora from non-comparable sources, leveraging the multilingual nature of India's information availability, though identifying parallel pages based on URL patterns remains challenging (Resnik \& Smith, 2003). Following prior works on mining data from web-scale data (Schwenk et al., 2021b), Samanantar (Ramesh et al., 2022) was mined from IndicCorp v1 (Kakwani et al., 2020) using LaBSE (Feng et al., 2022) based sentence embeddings, resulting in a 3-fold increase in data compared to existing parallel data. Combined with existing data, Samanantar contained 49.7 million sentence pairs between English and 11 Indic languages. In subsequent work, NLLB project (Costa-jussà et al., 2022) mined parallel data from CommonCrawl dumps (Wenzek et al., 2020) using LASER (Heffernan et al., 2022) based sentence embeddings. This corpus resulted in 448 million sentence English-centric pairs covering 19 Indic languages. While NLLB (Costa-jussà et al., 2022) had the largest coverage so far, all these efforts still do not cover all the 22 scheduled languages of India. This necessitates the need to create "seed" data (refer to $\S 3$ ) for the low-resource languages to help boost the performance of MT systems for these languages.

Benchmarks and Shared Tasks. Benchmarks have improved NLP systems across various tasks (Rajpurkar et al., 2016; Wang et al., 2018; 2019; Hu et al., 2020; Doddapaneni et al., 2023). Over the years, an increasing focus has been on improving MT systems for Indic languages, with sustained endeavors to develop appropriate benchmarks. The introduction of the Hindi-English MT challenge in WMT' 14 marked one of the earliest attempts to establish benchmarks for Indic languages (Bojar et al., 2014). Subsequently, WMT extended its efforts by incorporating the Gujarati-English and Tamil-English language pairs in 2019 (Barrault et al., 2019) and 2020 (Barrault et al., 2020), respectively. WAT (Workshop on Asian Translation) has continuously supported IndicMT with the inclusion of the IITB Hindi-English dataset (Kunchukuttan et al., 2018) in the WAT 2016. Subsequently, WAT expanded its efforts, adding 6, 8, 10, and

15 languages in 2018, 2020, 2021, and 2022, respectively (Nakazawa et al., 2018; 2020; 2021a; 2022). Siripragada et al. (2020) introduced a benchmark consisting of roughly $2 \mathrm{~K}-3 \mathrm{~K}$ sentences from Mann ki Baat ${ }^{7}$, covering 9 Indic languages translated to English. FLORES 101 (Goyal et al., 2022) was one of the first attempts to create a largescale MT benchmark with n-way parallel devtest and held-out test sets of around 1000 sentences for 101 languages, including support for 14 Indic languages manually annotated from the Wikimedia content. This was followed up by NLLB (Costa-jussà et al., 2022), extending the total language coverage to 200, which includes 19 Indic languages listed in the Constitution (plus a few more Indic languages). NTREX (Federmann et al., 2022) expanded coverage of languages of test data from WMT 2019 (Barrault et al., 2019) to 128 languages and covers 16 Indic languages. The test set contains 1997 manually translated sentences, primarily sourced from the news domain.

Neural MT models. The introduction of Neural MT and the creation of large-scale parallel corpora led to significant advancements in the field of Indic MT. Broadly, they follow the Embed - Encode - Attend - Decode approach. Initial approaches used Recurrent Neural Networks (Bahdanau et al., 2015) and later transformer-based approaches (Vaswani et al., 2017) became more prominent. The introduction of attention and subword-based modeling addressed the issues of word ordering and data sparsity. The models were able to generate grammatically fluent and accurate outputs. Some noteworthy Neural MT models studying Indian languages include (Philip et al., 2021; Ramesh et al., 2022; Fan et al., 2020; Costa-jussà et al., 2022). These were followed up with multilingual and pre-trained MT models (Kudugunta et al., 2019; Liu et al., 2020b; Xue et al., 2021; Dabre et al., 2022). These models were able to transfer knowledge from high-resource to low-resource languages by leveraging large amounts of training data and language similarities across languages, making it possible to train a good-quality MT system for low-resource languages (Dabre et al., 2021). Over the last few years, large corpora (Ramesh et al., 2022; Costa-jussà et al., 2022) and larger models (Fan et al., 2020; Costa-jussà et al., 2022) marked significant improvements in the translation quality. Recent work has also explored translation for extremely low-resource languages with hardly any parallel corpora and limited monolingual corpora (Costa-jussà et al., 2022; Bapna et al., 2022; Maurya et al., 2023).

## 3 Creating High-quality Translation Datasets at Scale

In this section, we describe the translation process, and the Shoonya ${ }^{8}$ infrastructure to ensure a high-quality translation workflow. We also describe in detail the translation workflow followed and quality control procedures and the salient features of the resultant datasets created: (a) BPCC-Human, the training dataset from English to 22 Indic languages, and (b) IN22, the test set for translation evaluation between English and Indian languages.

### 3.1 Translation Workflow

The overall translation workflow is described below and illustrated in Figure 3. The translation workflow comprises four stages. First, sentences for translation are chosen based on criteria such as domain coverage, length, and licensing. These sentences are sourced from diverse domains, including News, Business, and Health. Next, the selected sentences undergo a verification process where annotators ensure their quality and correctness, tagging them accordingly. The entire paragraph is rejected in case of any inaccurate sentences to prevent ambiguity. Once the verification is complete, the sentences are translated into 22 Indic languages, adhering to rigorous guidelines. Lastly, the translated content is reviewed by experienced translators who check for adherence to guidelines and overall quality, suggesting improvements or corrections as needed. If a translation is rejected, it is sent back to the original translator for revision, ensuring the highest translation standards. Specific customizations to the workflow depending on the kind of dataset being created (training/test) are discussed in subsequent sections.

All the stages in the workflow are performed on Shoonya, ${ }^{8}$ an open-source ${ }^{9}$ platform which was developed as a part of this work for supporting language annotation tasks customized for Indian languages. Additional information about the translation stages, including translation guidelines and the interface utilized for generating human-annotated translation data along with its key features, can be found in Appendix F.[^4]

![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-08.jpg?height=485&width=900&top_left_y=305&top_left_x=604)

Figure 3: Translation workflow in Shoonya

### 3.2 Building the IN22 Test set

In this section, we describe the IN22 test set, which is a new manually created n-way parallel test set covering English and 22 Indic languages. We motivate the need for such a benchmark, describe its features in detail, and explain the construction of the test set.

While there are a few test sets for Indian languages, there is still a need for a comprehensive test set that satisfies the following needs of Indian language machine translation and addresses the limitations of existing test sets:

- We need a test set that covers all 22 Indic languages and enables evaluation between all possible pairs of these scheduled languages. FLORES-200 (Costa-jussà et al., 2022) has the largest coverage amongst existing test sets (nway, 19 languages). The other test sets WAT 2020 (Nakazawa et al., 2020), WAT 2021 (Nakazawa et al., 2021a), WMT 2014 (Bojar et al., 2014), WMT 2019 (Barrault et al., 2019), WMT 2020 (Barrault et al., 2020), UFAL (Ramasamy et al., 2012) and NTREX (Federmann et al., 2022) have limited coverage, with the majority having only a few of the top-10 languages represented at the most.
- The test set should be diverse in terms of domains covered and represent a realistic distribution of sentence lengths while also encompassing topics relevant to India, which would be the primary use case for models supporting Indic languages. Existing test sets like WMT and FLORES are more general-purpose and have limited representation for Indian topics like named entities, locale, culture-specific terms, etc.

Table 2 compares existing benchmarks based on test set size, language coverage, domain coverage, and the language in which the dataset is source original.

### 3.2.1 Corpus Description

We describe the details and salient points of the IN22 test set. This test set comprises three subsets, which serve distinct evaluation scenarios:

- Multi-Domain Wikipedia subset (512 sentences): This subset is designed to be multi-domain, expanding to at least five more domains than the existing benchmarks like FLORES-200 (Costa-jussà et al., 2022). Domain coverage is presented in Table 54.
- Multi-Domain Web Sources subset (512 sentences): This subset was designed to represent content from sources other than Wikipedia to have more diversity in content and writing style and with more focus on India-centric content. These were mainly sourced from PDFs and from sources that are not accessible or crawlable on the web, thereby reducing the possibility of these sentences already being part of any mined data.

Table 2: Comparison of Various Benchmarks based on Test Set Size, Language Coverage, Domain Coverage, and Source Original.

| Dataset | Test Set Size | Language Coverage | Domain Coverage | Source Original |
| :--- | :---: | :---: | :---: | :---: |
| FLORES-200 (devtest) | 1012 | 19 | 8 | eng |
| NTREX | 1997 | 12 | news $(1)$ | eng |
| WMT 2014 (hin) | 2507 | 1 | news $(1)$ | both |
| WMT 2019 (guj) | $\approx 1000$ | 1 | 1 | both |
| WMT 2020 (tam) | $\approx 1000$ | 1 | 1 | both |
| WAT 2020 | $\approx 3500$ | 7 | 1 | eng |
| WAT 2021 | $\approx 2390$ | 10 | 1 | eng |
| UFAL | 2000 | 1 | 3 | eng |
| IN22-Wiki | 512 | 22 | 13 | eng |
| IN22-Web | 512 | 22 | 13 | eng |
| IN22-Conv | 1503 | 22 | 16 | eng |

- Conversation Translation Benchmark (1503 sentences): This subset was designed to evaluate the performance of models in day-to-day conversations in applications like chat. The translations are drawn from a multi-turn English dialog dataset we built, enabling evaluation across all the axes, including sentence level, turn level, and document level (complete conversation).

The following are some key features of the benchmark:

- It is an $n$-way parallel test set containing 2527 original English sentences translated into 22 Indic languages with highquality translations done by in-house translators from scratch without recourse to any existing MT system. Metadata, consisting of domains and context sentences (in raw, unedited format) for source sentences, is provided in the test set to enable a fine-grained analysis of translation quality for each example.
- IN22 enables evaluation in 500+ directions, including (i) source original translation from English to other languages. (ii) Indic to English translation evaluation and the ability to study relative language performance since the underlying sentence is the same, (iii) comparison of 462 inter-Indic translation directions.
- The test set is diverse in terms of the domains covered and the distribution of sentence lengths. The Web sources and Wikipedia subsets cover 13 domains, while the conversational subset covers 16 domains. The length distribution is chosen to reflect a realistic distribution while also having a sufficient number of long sentences, which can present a challenge to MT models. Figure 10 provide an overview of the domain v/s length distributions of our benchmarks, while Table 54 provides an overview of the domain diversity.
- Table 3 provides some statistics about the test set. Wikipedia and Web Sources have longer sentences than the conversational dataset. Conversational sentences have a higher perplexity compared to the other subsets, perhaps hinting at the lower representation of such scenarios in the GPT2 training corpus.


### 3.2.2 Source Selection

We describe the selection of the source sentences for each of the three subsets: Wikipedia, Web Sources, and Conversation. The creation of the Wikipedia subset involved selecting English source sentences from various Wikipedia categories to ensure broad coverage across different domains. Sentences were filtered based on length (less than 6 words or more than 80 words were discarded) and overlap with the FLORES-200 test set (4-gram overlap). For each sentence, a context window of 3 sentences (typically one before and one after) was constructed. The Web Sources subset focused on Indian topics and used Government of India websites and digital libraries as sources, with sentences selected using a similar procedure. The Conversation subset involved creating English conversations with predefined prompts and scenarios, which were then translated into 22 Indic languages. Overall, these subsets were created with careful consideration for domain diversity and language coverage. Appendix E. 1 provides detailed information about the procedure followed for the selection of sentences for all the three subsets of IN22.

Table 3: Statistics for the three subsets in the IN22 benchmark.

|  | Subsets |  |  |
| :--- | ---: | ---: | ---: |
|  | Wikipedia | Web Sources | Conversational |
| Number of sentences | 512 | 512 | 1503 |
| Average sentence length (number of English characters) | 169.27 | 144.53 | 54.18 |
| Average sentence length (number of English words) | 26.30 | 23.20 | 9.88 |
| Number of context sentences available | 3 | 3 | conversation |
| Number of domains | 13 | 13 | 16 |
| Average perplexity of English (computed using GPT-2) | 63.67 | 67.22 | 72.33 |

Table 55 contains the statistics of the conversation subset of IN22 test set. The subset contains conversations sampled from 16 domains including 'arts', 'history', 'school life', etc. The domains cover a diverse set of topics such as 'Government schemes', 'Movies', 'Historical Architectures', etc. Table 56 contains an English example from the conversation subset of IN22 test set. The conversation subset of IN22 benchmark can also be repurposed as a document translation task and would be useful in the context of evaluating LLMs.

### 3.2.3 Quality Control Procedure.

In the process of test set creation, it is imperative to implement strict quality control guidelines to prevent the use of MT outputs as a starting point by translators and ensure the fairness and reliability of the resulting benchmarks. As a first step, we disable MT outputs in Shoonya for this translation task. To further ensure translators are not taking recourse to MT outputs, we follow a systematic approach that involves conducting pairwise comparisons between human translations and the outputs of widely accessible machine translation (MT) systems, such as Google, Azure, NLLB (Costa-jussà et al., 2022), and IndicTrans1 (Ramesh et al., 2022). The BLEU score (Papineni et al., 2002) serves as an effective metric for detecting exact matches between translations and MT system outputs. Initially, we generate predictions from multiple MT systems for a batch of sentences translated by an annotator. Subsequently, we compute BLEU scores, denoted as $B\left(S_{i}, T\right)$, with respect to the reference translations $(T)$ and each MT system output $\left(S_{i}\right)$. A series of conditions are assessed based on the number of MT systems supporting a particular language (denoted as $k$ ). For languages supported by multiple MT systems, the system with the highest BLEU score $\left(S_{j}\right)$ is selected, where $j=\operatorname{argmax}_{i} B\left(S_{i}, T\right)$.

$$
\begin{equation*}
\left|B\left(S_{i}, T\right)-B\left(S_{j}, T\right)\right| \leq \delta \quad \forall i, j \in\{1, \ldots, k\} \tag{1}
\end{equation*}
$$

If the pairwise BLEU score difference between any two systems falls within an acceptable threshold (see Equation (1)), then the translations are accepted. In this work, we set the $\delta$ to be 10 . Otherwise, a high difference in BLEU scores indicates that the high-scoring model might have been a source for translation. In cases of high overlap with any of the machine translation systems, a new annotator is assigned to the task, and the quality control procedure is repeated, ensuring the creation of reliable and accurate benchmarks.

### 3.3 Building the BPCC Training Set

We create BPCC-Human (BPCC-H), a manually translated, multi-domain $n$-way seed parallel corpus between English and 22 Indic languages. ${ }^{10}$ In this section, we motivate the need for high-quality, human-translated training data, provide an overview of the dataset, and describe the process of construction of the dataset.

Motivation for creating the seed dataset. The primary method to create parallel corpora at scale for many languages is to mine data from publicly available sources. While this approach has shown success for languages that have good[^5]representation in monolingual corpus and multilingual models (Ramesh et al., 2022; Philip et al., 2021; Kunchukuttan et al., 2018), the same cannot extend to very-low resource languages. This makes it important to invest in building high-quality, modest-sized parallel corpora. We take inspiration from previous efforts to manually create large multilingual seed corpora explicitly for building machine translation models like ILCI (Jha, 2010), ALT (Riza et al., 2016), and NLLB-Seed (Costa-jussà et al., 2022; Maillard et al., 2023). These previous efforts have been instrumental in significantly boosting MT efforts for low-resource languages; particularly, seed data also helps in bootstrapping the development of various NLP tools such as language identifiers, topic classifiers, named entity recognition, etc., where minimal monolingual sources exist.

### 3.3.1 Corpus Description

Following are some key aspects of the BPCC-H dataset:

- BPCC-H-Wiki is the largest publicly available manually translated multi-domain parallel corpora in terms of language coverage. It contains a total of $644.3 \mathrm{~K}$ sentence pairs, ranging from $6.3 \mathrm{~K}$ to $54.3 \mathrm{~K}$ pairs depending on the language, averaging around $26 \mathrm{~K}$ sentence pairs per language pair. These translations were performed by qualified professional translators following a high-quality translation process and a systematic review of the sentence pairs, unlike crowdsourcing efforts. Per-language sentence counts can be seen in Table 1.
- BPCC-H-Wiki provides good seed parallel corpora for 4 extremely low-resource languages without public corpora, viz. Bodo, Dogri, Santali, and Goan Konkani. More than 10K sentence pairs are available for each of these languages. There are hardly any sources or models to mine parallel corpora for these languages.
- There are multiple scripts available for a few languages. However, for our current seed data creation efforts, we restrict ourselves to only one script per language, choosing the most widely used script for administrative purposes.
- A subset of BPCC-H, BPCC-H-Daily comprises spoken text particularly covering various types of sentences commonly used in different day-to-day scenarios, such as queries, commands, and feedback, across a range of applications including digital payment apps, grocery/food delivery apps, and government services apps. Our goal was to encompass diverse named entities in relevant domains, covering various expressions from these services. This subset, comprising 139.7K bitext pairs in 21 Indic languages except Sindhi, was developed from English sentences to expand the diversity of the parallel corpora.


### 3.3.2 Translation Details

The translation process has already been described above. Here, we discuss aspects of the translation process specific to BPCC-H.

First, we choose to translate from English source sentences to Indic languages in order to simplify the source sentence selection (easier availability of copyright-free English sentences for translation, diversity in domains, etc.). The Indian language side, therefore would exhibit translationese effects (Zhang \& Toral, 2019). However, this is not uncommon, and many parallel corpora are English original (Costa-jussà et al., 2022; Maillard et al., 2023; FitzGerald et al., 2022).

The English source sentences were selected from Wikipedia. We identified various Wikipedia categories of interest and then identified article pages within those categories. This was done to ensure broad coverage of domains. We identified a block of three sentences following Goyal et al. (2022), of which one was to be translated, and the others would be context sentences to resolve any ambiguities during translation. The translators had the option of post-editing MT outputs from an existing model wherever feasible.

## 4 Mining Training Data at Scale

The quality of MT systems depends on access to good quality parallel data, and increasing parallel corpora improves translation quality (Khayrallah \& Koehn, 2018). However, obtaining high-quality parallel corpora in large quantities is a challenging task. While human annotation is one way to source data, it is not scalable beyond a certain point to meet
the demands of data-hungry models. Thus, there is a growing need to (semi-)automatically mine large-scale training corpora to address this issue.

Over the years, various approaches have been proposed for generating parallel data for machine translation (MT) training. One set of approaches focused on mining parallel corpora from aligned documents identified from web-corpora (Resnik \& Smith, 2003; Bañón et al., 2020; El-Kishky et al., 2020) or from specific document collections like EuroParl (Koehn, 2005) and the United Nations (Ziemski et al., 2016). Document alignment is a non-trivial problem for open web-corpora and relies on URL matching or translation-based matching in constrained settings. Specific document collections may be limited in domain coverage and are often scarce. Instead of limiting mining to comparable documents, recent methods have explored the mining of sentence pairs from large sentence collections using multilingual embeddings without regard for document alignment. This has allowed the mining of parallel data from arbitrary and diverse collections of data (Schwenk et al., 2021a;b; Costa-jussà et al., 2022). Similar approaches have been extended to Indic languages (Ramesh et al., 2022), establishing the utility of large-scale mining for building multilingual NMT models.

Major Indic languages have a reasonable online presence, with numerous websites publishing data in multiple Indic languages, primarily pivoting through English or Hindi. Moreover, being a multilingual nation, several government documents, books, judgments, legal proceedings, etc., are published in multiple Indic languages, which are directly comparable and are thereby aligned at a document level. Hence, we invest efforts in mining parallel corpora by leveraging large-scale monolingual data as well as document-aligned data from comparable sources.

Our mining efforts focus on 12 Indic languages: Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu, and Urdu. These languages have a good representation in monolingual corpora, as reported in Doddapaneni et al. (2023). However, the low-resource languages have comparatively lesser monolingual data, and the quality of sentence embeddings is unknown. Therefore, we rely on high-quality human-translated data, as described in Section 3, for training low-resource languages. Nepali was also considered in an initial round of mining, and some bitext data was mined. However, it was dropped from mining subsequently since LaBSE embeddings (Feng et al., 2022) were observed to be suboptimal for Nepali. Going forward, we only focus on mining parallel corpora for the 12 languages mentioned above.

Table 1 provides statistics of the mined parallel corpora. The following is a summary of the mined corpora:

- In our mining efforts, a total of $\sim 126$ million sentence pairs were mined in addition to existing corpora, resulting in an aggregated collection of $\sim 230.5$ million sentence pairs after deduplication, which is $\sim 5 \times$ increase in parallel corpora size as compared to Ramesh et al. (2022).
- Mining from the monolingual corpus resulted in the largest parallel corpus gains, with 121 million sentence pairs across 13 Indic languages.
- Mining from comparable corpora results in a diverse parallel corpus covering a wide range of topics like Religion, Education, Legal, etc. In total 4.35 million sentence pairs were mined across 17 Indic languages.
- Filtering existing corpora turned out to be an important exercise, as we observed around $75 \%$ of the data was discarded due to poor quality of alignment. In summary, Costa-jussà et al. (2022) was filtered and thereby reduced from 448.1 million to $\sim 85$ million sentence pairs, and Ramesh et al. (2022) reduced from 49.7 million to 19.4 million sentence pairs. We describe the filtering process below.


### 4.1 Mining from Monolingual Corpora

The primary idea behind mining parallel sentence pairs from large corpora is to represent sentences from all languages in a common embedding space using LaBSE (Feng et al., 2022), such that the distance between a pair of sentences reflects their semantic difference. To achieve this, we project all the sentences into a shared space and search for the nearest neighbors around a query sentence. Given a source sentence $S$ in language $L$, we look for the closest Approximate Nearest Neighbors (ANNs) to $S_{L}$ within a selected threshold. The main challenge lies in scaling this process efficiently

![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-13.jpg?height=499&width=1261&top_left_y=301&top_left_x=432)

Figure 4: Mining workflow for Monolingual corpora

Table 4: The total number of monolingual sentences and extracted parallel sentences count (in millions). The size of the English monolingual corpus is 429 Million. $\dagger$ indicates the mining for Nepali was performed on an intermediate version of IndicCorp v2 (Doddapaneni et al., 2023).

| Language | Monolingual Corpus | Extracted Pairs |
| :---: | :---: | :---: |
| asm_Beng | $3.3 \mathrm{M}$ | $0.7 \mathrm{M}$ |
| ben_Beng | $269.5 \mathrm{M}$ | $16.0 \mathrm{M}$ |
| guj_Gujr | $115.5 \mathrm{M}$ | $11.6 \mathrm{M}$ |
| hin_Deva | $473.2 \mathrm{M}$ | $27.1 \mathrm{M}$ |
| kan_Knda | $101.7 \mathrm{M}$ | $12.5 \mathrm{M}$ |
| mal_Mlym | $91.8 \mathrm{M}$ | $12.3 \mathrm{M}$ |
| mar_Deva | $64.7 \mathrm{M}$ | $10.8 \mathrm{M}$ |
| $n p i \_D e v a a^{\dagger}$ |  | $0.01 \mathrm{M}$ |
| ory_Orya | $13.4 \mathrm{M}$ | $2.8 \mathrm{M}$ |
| pan_Guru | $38.6 \mathrm{M}$ | $6.2 \mathrm{M}$ |
| tam_Taml | $64.7 \mathrm{M}$ | $9.6 \mathrm{M}$ |
| tel_Telu | $108.5 \mathrm{M}$ | $11.1 \mathrm{M}$ |
| urd_Arab | $76.2 \mathrm{M}$ | $0.4 \mathrm{M}$ |
| \# Total | $2113 \mathrm{M}$ | $121 \mathrm{M}$ |

to project millions of sentences and compute nearest neighbors over a large search space in a scalable and efficient manner. Previous work, such as CCMatrix (Schwenk et al., 2021b), has demonstrated that ANN search can be efficiently performed at scale using quantization, efficient indexing, and retrieval. Similar approaches have been used in prior work on Indic languages, such as Samanantar (Ramesh et al., 2022). Our work follows the same approach as Samanantar for mining parallel sentences from large-scale monolingual corpora. We differ from Samanantar (Ramesh et al., 2022) primarily in the amount of monolingual data used for mining. We use a larger collection of monolingual corpora for our work, comprising IndicCorp v2 (Doddapaneni et al., 2023), Wikipedia ${ }^{11}$ and data from Internet Archive. ${ }^{12}$ Specifically, we have used 2.1 billion monolingual Indic sentences, significantly higher than Samanantar (Ramesh et al., 2022) (398.5 million). Moreover, the number of English sentences that we used for our bitext mining has increased from 54.3 million to 429 million. Additionally, we have also mined bitext for Urdu and Nepali.

Figure 4 shows an overview of the mining process. We provide details of the mining workflow below. The mining from monolingual sources resulted in 121 million bitext pairs. Table 4 shows the per-language statistics of the mined corpora.[^6]

Table 5: Pearson $(\rho)$ and Kendal $(\tau)$ correlation Cosine Similarity of LaBSE and LASER model with Human Ratings on the STS data released by Ramesh et al. (2022).

| Language | Sample Size | LaBSE |  | LASER |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\rho$ | $\tau$ | $\rho$ | $\tau$ |
| asm_Beng | 1,971 | 0.3942 | 0.2989 | 0.3797 | 0.3021 |
| ben_Beng | 3,797 | 0.5149 | 0.4392 | 0.3137 | 0.2522 |
| guj_Gujr | 2,298 | 0.5437 | 0.4475 | 0.2945 | 0.3429 |
| hin_Deva | 4,616 | 0.5575 | 0.4691 | 0.4550 | 0.4005 |
| kan_Knda | 2,838 | 0.5211 | 0.4184 | 0.2640 | 0.2634 |
| mal_Mlym | 2,760 | 0.5331 | 0.4354 | 0.4368 | 0.3339 |
| mar_Deva | 1,984 | 0.4773 | 0.3916 | 0.3540 | 0.2660 |
| ory_Orya | 1,264 | 0.1148 | 0.1152 | 0.0361 | 0.0332 |
| pan_Guru | 2,222 | 0.5952 | 0.4725 | 0.3812 | 0.3435 |
| tam_Taml | 2,882 | 0.5099 | 0.4084 | 0.2296 | 0.2367 |
| tel_Telu | 2,516 | 0.4426 | 0.3780 | 0.2164 | 0.1936 |
| Average | - | 0.4731 | 0.3886 | 0.3055 | 0.2698 |

Data Curation. Our data curation process commenced with the collection of documents from diverse sources, including IndicCorp v2 (Doddapaneni et al., 2023), Wikipedia ${ }^{11}$ and Internet Archive data ${ }^{12}$ which were aggregated at the document level. However, as our objective was to mine sentence-level parallel data, we used the Indic NLP library (Kunchukuttan, 2020) to segment these documents into individual sentences. Subsequently, we implemented a strict quality control procedure, where we perform language identification (LID) at the sentence level using LID filters from Costa-jussà et al. (2022). As previous studies have shown, web-scale data often contains offensive content (Kreutzer et al., 2022), therefore we use an "offensive word list" to filter out such content. This list is augmented with data from Toxicity-200 (Costa-jussà et al., 2022) and Doddapaneni et al. (2023). Additionally, we remove sentences that are too short ( $<4$ words) or too long ( $>40$ words) as we found that the quality and reliability of embeddings deteriorate beyond these lengths. After this quality control procedure, we apply strict deduplication to eliminate any potential duplicates on the normalized sentences in the monolingual corpora.

Sentence Embedding Model. Prior work such as Samanantar (Ramesh et al., 2022) and NLLB (Costa-jussà et al., 2022) have employed the LaBSE (Feng et al., 2022) and LASER3 (Heffernan et al., 2022) models for bitext mining respectively. However, to determine the optimal sentence embedding model for our mining purposes, we analyze the correlation of the Semantic Textual Similarity Rating (Agirre et al., 2016) with the cosine similarity scores obtained using both sentence embedding models. We consider the STS dataset released by Ramesh et al. (2022) with a human rating for a set of 11 languages. Our analysis suggests that the cosine similarity scores of LaBSE sentence embeddings exhibit a stronger correlation with the human ratings on a macro scale, as shown in Table 5. Therefore, we adopt the LaBSE model as the primary sentence embedding model for our bitext mining and filtering pipeline and only fall back to LASER3 for the languages not supported by LaBSE. We use LASER3 for languages such as Kashmiri (Devanagari), Kashmiri (Arabic), Maithili, Manipuri (Bengali), Nepali, Sanskrit, and Sindhi (Arabic).

Indexing. To ensure a common embedding space for all languages, we utilized LaBSE (Feng et al., 2022) to compute the sentence embeddings for all the sentences. Our approach for mining parallel sentences involves searching through English; thus we indexed all the English sentences and treated the Indic language sentences as queries. To accommodate the large corpus of 429 million English sentences, we partitioned them into 5 shards and indexed each shard separately. In line with previous work (Ramesh et al., 2022), we utilized a FAISS Index ${ }^{13}$ with $100 \mathrm{~K}$ clusters and employed Product Quantization (Jégou et al., 2011) to reduce the dimensionality of the embeddings from 768 to 64 , with each dimension represented by an 8 -bit integer value.[^7]

Table 6: URLs and domains of the sources used for comparable corpora mining.

| Source | URL | Domain |
| :--- | :--- | :--- |
| isha | https://isha.sadhguru.org/in/en/wisdom | Religion, Education, Culture |
| mkb | https://www.pmindia.gov.in/en/mann-ki-baat | Government, News, Education |
| nios | https://nios.ac.in/online-course-material.aspx | Education |
| nptel | https://nptel.ac.in/courses | Education |
| pib | https://pib.gov.in/AllRelease.aspx | Government, News, Legal |
| spoken tutorial | https://spoken-tutorial.org/tutorial-search | Education |
| ugc | http://ugceresources.in | Education |
| vanipedia | https://tinyurl.com/2sf547tn | Religion, Education, Culture |

Retrieval. To retrieve parallel sentence pairs for a given query sentence $\left(S_{L}\right)$ in language $L$, we use LaBSE (Feng et al., 2022) to compute the embedding of the query sentence and perform a search on the FAISS Index constructed from the English sentences. First, we retrieve the top $k(k=1024)$ clusters by computing the cosine similarity between the cluster centroids and the query embedding. Subsequently, we search for ANNs within these clusters to retrieve the closest match. However, as pointed out by Ramesh et al. (2022), the similarity scores can vary when using quantized vectors (64d) while preserving the relative ranking among the sentence pairs. To ensure high-quality matches, we recompute the cosine similarity using the original $768 d$ vectors and only retain pairs with a similarity score above a threshold of 0.80 , indicating a strong semantic match. The process is repeated on each of the 5 English partitions, and only the highest-scoring match is retained.

### 4.2 Mining from Comparable Corpora

For Indian languages, we explore the mining of parallel corpora from comparable sources, i.e., multilingual websites containing high-quality parallel documents. We first align potentially parallel documents using heuristics to reduce the search space, followed by the extraction of high-quality parallel sentences from aligned documents.

Data Curation. We first identify several websites that publish content in multiple Indic languages. The articles on these websites are aligned across different languages, indicating they are exact translations of each other. Owing to this, the search space is reduced considerably as compared to monolingual corpus mining. The selected sources are diverse in domains covering a range of topics like Education, Legal, Religion, etc., and of high quality as verified by language experts. An overview of the sources is available in Table 6. We follow the same pre-processing steps to segment the documents into sentences, followed by language identification and toxicity filters.

Indexing. Similar to monolingual corpora, we use the LaBSE (Feng et al., 2022) model to index both the source and target sentences. Since the search space is much smaller in comparable corpora, we perform a full search over the entire target sentences in the corresponding document.

Retrieval. Let $S=\left\{s_{1}, s_{2}, \cdots, s_{m}\right\}$ be the set of source sentences and $T=\left\{t_{1}, t_{2}, \cdots, t_{n}\right\}$ be the set of target sentences. Let $f\left(s_{i}, t_{i}\right)$ be the scoring function for calculating the semantic similarity. Given that $m$ and $n$ are considerably smaller than the size of the monolingual corpus, we perform a total of $m \times n$ scoring computations. Following Artetxe \& Schwenk (2019), we use the margin-based scoring (Equation 2) to find the closest semantic match between a given source and target sentences. The sentences under consideration are represented by the pair $(x, y)$. We denote the $k$ unique nearest neighbors of $x$ and $y$ in the other language as $N N_{k(x)}$ and $N N_{k(y)}$, respectively. We perform margin-based mining in both forward and backward directions to eliminate the candidate pairs with inconsistent alignment and retain only those that intersect, resulting in high-quality bitext pairs. Following Costa-jussà et al. (2022) we use a margin threshold of 1.06 with 4 nearest neighbors. Additionally, we set a cosine threshold of 0.80 for the high-resource languages and perform LID filtering to remove substandard sentence pairs. Considering the high memory requirements and the high variability of margin scores based on cluster sizes when operating in shards, employing margin-based mining for monolingual corpus with the current infrastructure was not feasible.

Table 7: Statistics of the bitext mining from comparable corpora (till Oct 2022).

| Language | Source | Extracted Pairs |
| :--- | :--- | ---: |
| asm_Beng | mkb, nios, pib, spoken-tutorial, vanipedia | 38,656 |
| ben_Beng | isha, mkb, nios, nptel, pib spoken-tutorial, ugc, vanipedia | 263,394 |
| brx_Deva | spoken-tutorial | 700 |
| guj_Gujr | isha, mkb, nios, nptel, pib spoken-tutorial, ugc vanipedia | 594,847 |
| hin_Deva | isha, mkb, nios, nptel, pib spoken-tutorial, ugc vanipedia | 891,464 |
| kan_Knda | isha, mkb, nios, nptel, pib spoken-tutorial, ugc vanipedia | 386,408 |
| mai_Deva | spoken-tutorial | 84 |
| mal_Mlym | isha, mkb, nios, nptel, pib spoken-tutorial, ugc vanipedia | 365,893 |
| mar_Deva | isha, mkb, nios, nptel, pib spoken-tutorial, ugc vanipedia | 453,371 |
| mni_Beng | mkb, pib | 22,322 |
| npi_Deva | isha, spoken-tutorial, vanipedia | 6,247 |
| ory_Orya | mkb, nios, pib spoken-tutorial, vanipedia | 125,143 |
| pan_Guru | mkb, nios, pib spoken-tutorial, vanipedia | 216,108 |
| san_Deva | spoken-tutorial | 702 |
| tam_Taml | isha, mkb, nios, nptel, pib spoken-tutorial, ugc, vanipedia | 455,965 |
| tel_Telu | isha, mkb, nios, nptel, pib spoken-tutorial, ugc, vanipedia | 449,239 |
| urd_Arab | mkb, nios, pib, vanipedia | 232,496 |
| \#Total |  | $4,503,039$ |

$$
\begin{equation*}
\operatorname{margin}(x, y)=\frac{\cos (x, y)}{\sum_{z \in N N_{k}(x)} \frac{\cos (x, z)}{2 k}+\sum_{z \in N N_{k}(y)} \frac{\cos (y, z)}{2 k}} \tag{2}
\end{equation*}
$$

Following mining from Comparable Corpora, we extract 4.5 million sentence pairs across 17 Indic languages. The statistics and the sources for the mined bitext are available in Table 7.

### 4.3 Filtering Existing Mined Parallel Corpora

Over the years, several parallel corpora have been released for Indic languages (Kunchukuttan et al., 2018; Nakazawa et al., 2021b; Philip et al., 2021; Tiedemann, 2012) inter alia. The corpora are of varying quality and created using different approaches. We filter these existing corpora using some of the well-known practices to ensure we retain a high-quality subset for model training.

Particularly, a large collection of parallel corpora was mined as part of the NLLB project (Costa-jussà et al., 2022) using LASER3 embeddings (Heffernan et al., 2022). The corpus was mined using the margin-based threshold described in Equation (2), with a threshold of 1.06. The original dataset was not released by the authors of Costa-jussà et al. (2022). However, Allen $\mathrm{AI}^{14}$ has replicated the efforts of Costa-jussà et al. (2022) and released the dataset closely matching the numbers reported by the authors of (Costa-jussà et al., 2022). Going forward, we use this dataset for our use-case and refer to it as Allen-NLLB ${ }^{15}$. The corpus contains 448 million sentence pairs across 19 Indic languages, with more than 10 million sentence pairs in 12 languages. However, on performing a manual inspection of the bitext, it was observed that a large majority of the sentences had misalignment and suboptimal parallel sentence pairs. Therefore, before using this corpus for training MT models, it is important to filter the corpus to remove the noisy sentence pairs.

Following our bitext mining in Section 4.1 and Section 4.2, we use the LaBSE model (Feng et al., 2022) with a cosine similarity threshold of 0.80 to filter the Allen-NLLB corpus. We also use the LASER3 model (Heffernan et al., 2022) as a fallback model for languages that are not supported by LaBSE (viz. Nepali, Maithili, Sanskrit, Sindhi (Arabic),[^8]

Table 8: Statistics of pre-filtering and post-filtering on existing mined parallel corpora consisting of NLLB (Costa-jussà et al., 2022) and Samanantar (Ramesh et al., 2022).

| Language | Pre-Filtering | Post-Filtering | Proportion(\%) |
| :--- | ---: | ---: | ---: |
| asm_Beng | $5,285,401$ | 565,282 | 10.70 |
| ben_Beng | $70,400,333$ | $16,514,684$ | 23.46 |
| guj_Gujr | $14,458,054$ | $8,442,476$ | 58.39 |
| hin_Deva | $43,149,229$ | $11,056,172$ | 25.62 |
| kan_Knda | $38,368,723$ | $10,532,571$ | 27.45 |
| kas_Arab | 647,348 | 125,243 | 19.35 |
| kas_Deva | $1,042,450$ | 194,528 | 18.66 |
| mai_Deva | $4,438,382$ | 62,359 | 1.40 |
| mal_Mlym | $49,599,699$ | $10,832,342$ | 21.84 |
| mar_Deva | $35,585,104$ | $7,742,065$ | 21.76 |
| mni_Beng | 490,089 | 347,108 | 70.83 |
| npi_Deva | $19,624,054$ | $1,583,922$ | 8.07 |
| ory_Orya | $14,700,484$ | $2,887,960$ | 19.65 |
| pan_Guru | $14,057,042$ | $3,391,710$ | 24.13 |
| san_Deva | $3,095,396$ | 244,367 | 7.89 |
| snd_Arab | $8,924,699$ | $2,129,054$ | 23.86 |
| tam_Taml | $47,777,362$ | $10,489,852$ | 21.96 |
| tel_Telu | $51,248,532$ | $11,826,104$ | 23.08 |
| urd_Arab | $25,303,579$ | $5,322,290$ | 21.03 |
| \# Total | $448,195,960$ | $104,290,089$ | 23.27 |

Kashmiri (Devanagari), Kashmiri (Arabic), Santali). Table 8 shows that upon filtering, the dataset is reduced from 448.1 million sentence pairs to 104.2 million sentence pairs, i.e. close to $76 \%$ of data has been dropped with quality filtering. For Santali, post LASER3 filtering, it was observed that the majority of the sentence pairs were dropped during the filtering process. Post-hoc human evaluation confirmed that most of the parallel data for Santali-English in the Allen-NLLB are noisy. We see the highest drops in Maithili, Sanskrit, and Nepali, which are considered to be low-resource languages. Surprisingly, even in high-resource languages like Hindi and Bengali, we see that close to $75 \%$ of the data has been dropped during filtering. Similarly, we also apply the same filtering criteria to Samanantar Corpus (Ramesh et al., 2022), as it was noted that Samanantar was mined with an older version of LaBSE model (Feng et al., 2022). Section 7.2 describes our analysis of the data quality v/s scale trade-off.

## 5 Modeling

### 5.1 Training Data

To train our translation models, we utilize a range of data sources, including data mined from text corpora (monolingual corpora \& comparable sources), human-annotated collections (BPCC-H-Wiki and BPCC-H-Daily), and filtered versions of existing corpora (Ramesh et al., 2022; Costa-jussà et al., 2022). We describe our filtering techniques in Section 4.3. While these sources constitute the majority of our training corpus, we also incorporate additional humanlabeled seed data from NLLB-seed (Costa-jussà et al., 2022; Maillard et al., 2023), ILCI (Jha, 2010; Choudhary \& Jha, 2011), and MASSIVE (FitzGerald et al., 2022), totaling approximately 1.47 million sentence pairs. The ILCI (Jha, 2010; Choudhary \& Jha, 2011) data is primarily distributed across domains such as health, tourism, agriculture, and entertainment, and contributes around 1.34 million parallel sentences across 16 languages. Furthermore, we augment our data with the Indic portions of MASSIVE (FitzGerald et al., 2022), which was released as Spoken Language Understanding data and closely resembles the data in BPCC-H-Daily. Professional annotators manually translate the sentences in this dataset and contribute 139,000 sentence pairs across seven languages. In total, we have approximately 230.5 million sentence pairs, out of which 2.2 million are gold sentence pairs that are manually annotated by professional translators. The distribution of the data sources across all languages is presented in Table 1.

Table 9: Statistics of the bi-text training data after deduplication with benchmarks.

| Language | Dataset Size | Language | Dataset Size |
| :--- | ---: | :---: | ---: |
| asm_Beng | $1,443,125$ | mni_Beng | 386,916 |
| ben_Beng | $32,725,076$ | mni_Mtei | 42,753 |
| brx_Deva | $1,13,839$ | npi_Deva | $1,687,436$ |
| doi_Deva | 24,160 | ory_Orya | $5,834,074$ |
| gom_Deva | 97,660 | pan_Guru | $9,816,009$ |
| guj_Gujr | $20,491,094$ | san_Deva | 278,374 |
| hin_Deva | $39,144,013$ | sat_Olck | 25,128 |
| kan_Knda | $23,285,105$ | snd_Arab | $2,128,391$ |
| kas_Arab | 135,843 | snd_Deva | 10,503 |
| kas_Deva | 200,094 | tam_Taml | $20,740,179$ |
| mai_Deva | 87,888 | tel_Telu | $23,250,217$ |
| mal_Mlym | $23,521,937$ | urd_Arab | $6,176,951$ |
| mar_Deva | $18,932,834$ |  |  |
|  |  | \# Total | $230,579,599$ |

### 5.2 Preprocessing

We follow the following steps in sequential order for our data preprocessing pipeline.

Standard Preprocessing. We apply standard preprocessing, which includes removing redundant spaces, removing special characters, and normalizing the punctuations. Additionally, we convert the Indic numerals to English numerals using a dictionary-based mapping. This facilitates the use of English numerals both at the input and output stages of our model. However, a post-processing stage can be used to map English numerals back to their Indic equivalents, if required.

Data Deduplication. To prevent any potential data leakages, we apply strict deduplication with all the available benchmarks mentioned in Table 2. Our deduplication process involves standard preprocessing steps as mentioned above, followed by text lowercasing, removal of all punctuations, removal of spaces, and identification of potential matches on the monolingual side of both source and target sentences with the benchmarks. Correspondingly, any bi-text pairs associated with these monolingual matches are discarded, and only the remaining data is considered for training our models. As a result of this deduplication, our processed dataset contains a total of $\sim 230.5 \mathrm{M}$ bi-text pairs. The per-language distribution is presented in Table 9

Additional Preprocessing. Based on human evaluation of the IndicTrans1 model (Ramesh et al., 2022), it was observed that the model exhibits poor performance in dealing with special cases: emails, URLs, dates, numbers, and special characters like percentages. These special cases share a common characteristic indicating that they should ideally not be translated by the model but should be reproduced as it is in the translation. To address this issue, we employ regular expression patterns to identify text spans corresponding to these special cases. Subsequently, we wrap these spans of text with special tags (<dnt> text span </dnt>) on the input side of the model, thereby providing implicit supervision to the model to retain these special cases in their original form in the translation. Note that, during training, we wrap the text spans within special tags only if they appear in both the source and target sentences.

Script Unification. Many Indic languages use scripts from the Brahmi family. To facilitate better transfer learning, wherever feasible, we apply rule-based script conversion using IndicNLP library (Kunchukuttan, 2020) to represent most of these languages in a single script (Devanagari). Thus, effectively our models are trained with five scripts, namely Perso-Arabic (Sindhi, Urdu, Kashmiri), Ol Chiki (Santali), Meitei (Manipuri), Latin (English), and Devanagari (all the rest of the languages).

### 5.3 Tokenization

Subword-level tokenization (Sennrich et al., 2016b; Kudo \& Richardson, 2018) is an effective approach for segmenting text into smaller sub-word units to build neural machine translation (NMT) systems that are robust against out-ofvocabulary (OOV) issues. In this work, we train two separate tokenizers with the byte-pair-encoding (BPE) algorithm (Sennrich et al., 2016b) using SentencePiece ${ }^{16}$ library (Kudo \& Richardson, 2018) for English and Indic languages using a sampled corpus comprising monolingual sentences from IndicCorp v2 (Doddapaneni et al., 2023) and NLLB data (Costa-jussà et al., 2022). We chose SentencePiece library because of its in-built support for normalization. To ensure fair representation for each language, we upsample the low-resource languages and limit the high-resource languages to $3 \mathrm{M}$ sentences each. We use a vocab size of $32 K$ and $128 K$ for our English and Indic SPM models, respectively. We prepare the monolingual data for training our English and Indic SPM models using the preprocessing pipeline described in section 5.2 except for the additional preprocessing. We also add special tags (<dnt> and $</$ dnt $>$ ) to the trained SPM models.

After tokenization, we prepend special indicator tags following prior multilingual NMT models (Johnson et al., 2017; Tan et al., 2019; Tang et al., 2021). In our case, we add both the source and target language tags to indicate the translation direction. Specifically, when translating text from English to Hindi, we format the sample as eng_Latn hin_Deva \{processed text\}.

### 5.4 Architecture

We train our English-centric neural models based on the transformer encoder-decoder architecture (Vaswani et al., 2017) using the fairseq library ${ }^{17}$ (Ott et al., 2019). Our architecture comprises 18 encoder layers and 18 decoder layers, an input dimension of 1024, pre-normalization (Xiong et al., 2020) for all modules, a feedforward dimension of 8192, and 16 attention heads. The total parameter count is 1.1B. Additionally, we use the GELU activation (Hendrycks \& Gimpel, 2016) instead of ReLU (Nair \& Hinton, 2010).

### 5.5 Training

To perform well across a wide range of domains, we adopt FLORES-200 (Costa-jussà et al., 2022) multi-domain development set as our validation set rather than combining development sets from different benchmarks. However, this development set does not cover all the languages supported by our models. As a result, we extend the FLORES-200 development (Costa-jussà et al., 2022) set to additionally incorporate five more languages (viz. Bodo, Dogri, Konkani, Sindhi (Devanagari), Manipuri (Meitei)) to have a complete validation set to jointly optimize and achieve superior performance on all the 22 scheduled Indic languages (including 25 language script combinations). We also make the expanded version of the FLORES-200 development set (Costa-jussà et al., 2022) publicly available, and this has also been integrated into the official FLORES repository ${ }^{18}$.

We employ the BLEU metric specifically for checkpointing purposes, using validation BLEU scores to indicate the model's performance on the aforementioned validation set. This choice is motivated by BLEU providing valuable insights into the model's macro-level performance, making it a useful diagnostic tool for tracking the model's progress during training. However, it may not be the most suitable choice for fine-grained evaluations. This differs from IndicTrans1 (Ramesh et al., 2022), which utilizes validation loss for checkpointing. By incorporating the checkpointing based on validation BLEU scores, we can ensure that the training of our models progresses based on their performance on the validation set, leading to an overall improved model.

Our model training paradigm comprises two distinct phases: auxiliary training and downstream training, which are described below.

Auxiliary Training. The first phase of our model training paradigm, termed auxiliary training, involves training intermediate models to augment large amounts of monolingual corpora through back translation. Back-translation[^9]

Table 10: Details of the hyperparameters used for stage 1 training and stage 2 fine-tuning. Please note that we reset the learning scheduler, dataloaders, and optimizer for stage 2 fine-tuning.

| Hyperparameters | Stage 1 training | Stage 2 fine-tuning |
| :--- | :--- | :--- |
| Optimizer | Adam (Kingma \& Ba, 2014) | Adam (Kingma \& Ba, 2014) |
| Beta values $\left(\beta_{1}, \beta_{2}\right)$ | $(0.9,0.98)$ | $(0.9,0.98)$ |
| Learning rate | $5 e-4$ | $3 e-5$ |
| Scheduler | Inverse sqrt | Inverse sqrt |
| Criterion | Cross-entropy | Cross-entropy |
| Label smoothing (Szegedy et al., 2016) | 0.1 | 0.1 |
| Warmup learning rate | $1 e-7$ | $1 e-7$ |
| Warmup steps | 4,000 | 2,000 |
| Gradient clipping | 1.0 | 1.0 |
| Dropout (Srivastava et al., 2014) | 0.2 | 0.2 |
| Patience | 10 | 10 |
| Effective batch size | $262 \mathrm{~K}$ | $32 \mathrm{~K}$ |
| Mixed precision training | FP16 | FP16 |
| Maximum update steps | $1 \mathrm{M}$ | $1 \mathrm{M}$ |
| Validation interval | 2,500 | 1,000 |
| Maximum sequence length | 256 | 256 |
| Checkpoint metric | BLEU @ beam =1 | BLEU @ beam =1 |

(Sennrich et al., 2016a; Edunov et al., 2018) is a technique that is effective in improving the performance of machine translation models. We adopt a deterministic curriculum strategy as proposed by Mohiuddin et al. (2022), wherein we first train the models from scratch on the entire parallel corpora listed in Table 1, followed by stage 2 fine-tuning on high-quality seed data including BPCC-H-Wiki and the NLLB seed (Costa-jussà et al., 2022; Maillard et al., 2023), to improve the models further. Our approach differs from theirs in that we exclusively consider high-quality humangenerated data for stage 2 model fine-tuning rather than selecting the top $p \%$ of bitext pairs from the original data based on a quality measure. Another prominent advantage of using our human-generated data is that it provides multi-domain coverage, thereby allowing us to optimize across multiple domains, which may not be feasible when selecting a subset of bitext pairs based on quality. We list all the hyperparameters used in both stage 1 and stage 2 training in Table 10 .

Downstream Training. In the second phase, we train our models on the augmented parallel corpora that combine original data with back-translated data. Mainly, we follow tagged back translation (Caswell et al., 2019) to provide additional supervision to the model to distinguish between the different data sources during training. We prepend the special symbol to the synthetically augmented data while keeping the original data intact. We follow the same training hyperparameters and two-stage training strategy as the auxiliary training. Table 10 shows all the hyperparameters used in both stage 1 and stage 2 training.

### 5.6 Data Augmentation

Using existing parallel corpora as training data may eventually lead to saturation in model performance. To address this, researchers have proposed data augmentation techniques to enhance data diversity and improve model performance. One such approach involves augmenting pseudo-parallel corpora by leveraging diverse monolingual corpora. Back translation (Sennrich et al., 2016a; Edunov et al., 2018) is a widely used technique to synthetically augment training data for improving translation models. Given the large scale of our models, we adopt this approach and generate backtranslated data, which is approximately 1.75 times the size of the original training data. To generate back translation data, we first identify potential sources of monolingual data for English and Indic languages, intending to maximize both domain coverage and distributional diversity to improve the models. We use the intermediate checkpoints of IndicTrans2 to generate the backtranslated data and combine the augmented data along with the training data to further improve our models.

Table 11: Statistics of the monolingual data used for backtranslation.

| Language | English BT Data | Indic BT Data | Language | English BT Data | Indic BT Data |
| :---: | ---: | ---: | :---: | ---: | ---: |
| asm_Beng | $14,569,760$ | $5,433,796$ | mni_Beng | $17,437,961$ | 60,224 |
| ben_Beng | $17,928,856$ | $34,987,743$ | mni_Mtei | $17,709,470$ | 33,233 |
| bra_Deva | $17,597,825$ | 144,246 | npi_Deva | $20,567,992$ | $29,997,511$ |
| doi_Deva | $18,157,864$ | 44,291 | ory_Orya | $19,528,727$ | $15,341,924$ |
| gom_Deva | $13,478,802$ | $2,937,179$ | pan_Guru | $17,476,704$ | $29,968,101$ |
| guj_Gujr | $21,447,703$ | $29,994,809$ | san_Deva | $11,198,794$ | $9,744,059$ |
| hin_Deva | $20,648,256$ | $37,472,261$ | sat_Olck | $9,799,342$ | 32,346 |
| kan_Knda | $10,970,576$ | $32,496,971$ | snd_Arab | $8,918,509$ | $4,298,898$ |
| kas_Arab | $12,717,571$ | 44,276 | snd_Deva | $6,479,694$ | 25,264 |
| kas_Deva | $11,599,085$ | 154,465 | tam_Taml | $22,647,544$ | $32,488,783$ |
| mai_Deva | $15,598,363$ | $1,813,669$ | tel_Telu | $21,767,767$ | $32,494,937$ |
| mal_Mlym | $17,888,824$ | $32,495,047$ | urd_Arab | $20,006,656$ | $33,471,969$ |
| mar_Deva | $15,849,536$ | $34,994,281$ |  |  |  |
|  |  |  | \# Total | $401,992,181$ | $400,970,283$ |

English Data for Back Translation. For back translation, we source English data from several sources, including the English side of IndicCorp v2 (Doddapaneni et al., 2023), the English side of the Indic subset of the NLLB data (Costa-jussà et al., 2022), and English data from a few high-resource pairs (eng_Latn - \{fra_Latn, por_Latn, spa_Latn, ces_Latn\}) of NLLB data (Costa-jussà et al., 2022), along with additional miscellaneous sources like Simple Wikipedia ${ }^{19}$ and DD News. ${ }^{20}$ We subjected this set of English sentences to standard preprocessing, as outlined in Section 5.2, and then filtered the set to retain only sentences with a minimum of five and a maximum of 100 words. As described in Section 5.2, we deduplicate this set of sentences with all the benchmarks available. Additionally, we deduplicate this set with the training data to ensure more diversity in English data and sample candidate sentences from a non-overlapping set. From this reduced candidate set, we randomly sampled approximately 400 million sentences for back translation, following an approximate distribution of $55 \%$ IndicCorp, $20 \%$ NLLB Indic, $20 \%$ NLLB HighRes, and $5 \%$ Miscellaneous sources. To ensure language-script diversity, we randomly subdivide the 400 million set into 25 parts, corresponding to the supported language-script combinations. We utilize the En-Indic model with a beam value of 5 to generate back-translated data. We proportionally distribute the English data across different languagescript combinations based on the normalized chrF++ (Popović, 2017) scores across all language-script combinations described below in Equation (3) on the expanded version of FLORES-200 validation set (Goyal et al., 2022; Costajussà et al., 2022) described in section 5.5. Table 11 describes the distribution of the English data we consider for back-translation for each language-script combination.

$$
\begin{equation*}
\operatorname{Count}\left(\text { lang }_{\mathrm{i}}\right)=\frac{\operatorname{chrF}++\left(\text { lang }_{\mathrm{i}}\right)}{\sum_{j} \operatorname{chrF}++\left(\text { lang }_{\mathrm{j}}\right)} \times \mathrm{N} \tag{3}
\end{equation*}
$$

Here, chrF++ $\left(\right.$ lang $\left._{\mathrm{i}}\right)$ represents the normalized chrF++ score for language-script combination lang $\mathrm{i}_{\mathrm{i}}$, and $\mathrm{N}$ is the total number of English monolingual sentences to be used for back translation.

Indic Data for Back Translation. We source the Indic monolingual data from IndicCorp v2 (Doddapaneni et al., 2023) and the Indic side of the NLLB data (Costa-jussà et al., 2022) to generate back-translated data to improve our En-Indic model. However, it is essential to note that our sources for Indic monolingual data are limited, which limits the amount of data we can sample from each language-script combination. As a result, we do not adopt any proportional sampling based on the model's performance on the FLORES-200 validation set, as we do when generating back-translated data from monolingual English data. Therefore, we follow a simple strategy to include all the available monolingual data from languages, where the availability of diverse monolingual data is scarce (less than 20 million[^10]sentences) and uniformly sample from the high-resource languages. We apply the same preprocessing and data deduplication steps as described above for back-translation from English. We use the Indic-En model with a beam value of 5 for generating back-translation data. We provide the details of the Indic monolingual data distribution used for back translation in Table 11 .

### 5.7 Postprocessing

Since our En-Indic model is trained on script-unified data, the output it generates must be mapped back to the native script of the target language. Therefore, we perform rule-based script conversion using the IndicNLP library (Kunchukuttan, 2020) and map the script-unified output to the corresponding native Indic script. Importantly, this postprocessing is only necessary for the En-Indic model, as the outputs of the Indic-En model are already in the desired format.

## 6 Evaluation

### 6.1 Models Compared

We compare our trained models with publicly and commercially available existing models and systems:

- IndicTrans1. Ramesh et al. (2022) curated large parallel corpora by large-scale mining and trained multilingual transformer models (474M parameters) on this mined Samanantar dataset. These models support only 11 major Indian languages.
- NLLB. Costa-jussà et al. (2022) trained a multi-way many-to-many 54.5B Mixture of Experts (MoE) model supporting 200 languages. This model supports 20 language-script combinations from the set of scheduled Indic languages, providing coverage in at least one script for 19 of the 22 scheduled Indic languages.
- M2M-100. Fan et al. (2020) released many-to-many models supporting translation between 100 languages with language-family-specific decoders trained using English-centric data and non-English-centric data. We use their best model (12B parameters) supporting 12 of the 22 scheduled Indic languages for our comparison.
- Microsoft Azure Translate. ${ }^{21}$ Microsoft Azure Translate is a commercial translation engine supporting translation between 16 out of the 22 scheduled Indic languages at the time of writing.
- Google Translate. ${ }^{22}$ Google Translate is a commercial translation engine supporting translation between 19 out of the 22 scheduled Indic languages at the time of writing.
- GPT-3.5. GPT-3.5 is a commercially available, large language model developed by OpenAI, ${ }^{23}$ based on the GPT-3 architecture (Brown et al., 2020), but with additional improvements and optimizations like instruction fine-tuning, reinforcement learning with human feedback (Ouyang et al., 2022), and enhanced conversational support. It is a decoder-only model trained using the causal language modeling objective and is currently available as a propriety system accessible via a paid API. We evaluate the gpt-3.5-turbo model, which accepts chat format messages, on our IN22 benchmark in a zero-shot setting.

For proprietary models, it is difficult to do fair comparisons since little information is available about models and training. Thus, the reported results should be seen as a reasonable approximation. In this work, we will henceforth adopt the specific shorthand notations: the IndicTrans1 model will be referred to as IT1, the M2M-100 model as M100, the NLLB 1.2B distilled model as N1.2, the NLLB 54.5B MoE model as N54, Google Translate as Goog, Microsoft Azure Translate as Az, and our IndicTrans2 model as IT2. The predictions of Microsoft Azure, Google Translate, and GPT3.5 were generated using the respective APIs, with data retrieved on 10th May 2023.[^11]

### 6.2 Benchmarks

We evaluate our trained models (auxiliary and downstream) on our IN22 benchmark and all the publicly available benchmarks: FLORES-200 (Goyal et al., 2022; Costa-jussà et al., 2022), WAT 2020 (Nakazawa et al., 2020), WAT 2021 (Nakazawa et al., 2021a), WMT 2014 (Bojar et al., 2014), WMT 2019 (Barrault et al., 2019), WMT 2020 (Barrault et al., 2020), UFAL (Ramasamy et al., 2012) and NTREX (Federmann et al., 2022).

We list the details of the existing benchmarks below.

- IN22 is a comprehensive benchmark for evaluating machine translation performance in multi-domain, $n$-way parallel contexts across 22 Indic languages. It comprises three distinct subsets, namely IN22-Wiki, IN22-Web, and IN22Conv. The Wikipedia and Web sources subsets offer diverse content spanning news, entertainment, culture, legal, and India-centric topics. Meanwhile, the conversation domain subset is designed to assess translation quality in typical day-to-day conversational-style applications.

From now on, we merge Wikipedia and Web Sources subsets, to create a consolidated set referred to as IN22-Gen for translation evaluation. Our motivation for this is that these two subsets share a common language style, albeit with varying topics, whereas the Conversation subset is different in both language style and usage context.

- FLORES-101/200 (Goyal et al., 2022; Costa-jussà et al., 2022) is a multi-domain general-purpose benchmark designed for evaluating translations across 200 languages, including 19 Indic languages. The English sentences are source-original and have been translated into other languages. It comprises sentences sourced from Wikimedia entities with equal portions of news, travel, and non-fiction content from children's books. Tables 2 and 54 provide further details on the statistics and fine-grained domain coverage.
- NTREX (Federmann et al., 2022) is a news-domain benchmark that expands coverage of languages of test data from WMT 2019 (Barrault et al., 2019) to 128 languages. Out of these, 13 are scheduled Indic languages.
- WMT has created benchmarks for selected Indic languages as part of shared tasks in 2014 (Hindi) (Bojar et al., 2014), 2019 (Gujarati) (Barrault et al., 2019) and 2020 (Tamil) (Barrault et al., 2020).
- WAT 2020/2021 (Nakazawa et al., 2020; 2021a) included support for translations for 8 Indic languages in the news domain. In addition, they released data for Hindi-English in Information Technology and WikiNews domains. WAT 2021 (Nakazawa et al., 2021a) created a benchmark for translation between 10 Indic languages and English.
- UFAL (Ramasamy et al., 2012) is an English-Tamil bilingual benchmark created from publicly available websites. The benchmark consists of English sentences from domains such as cinema, news, and some biblical sources.

Moving forward, we consider IN22 and FLORES-200 (Costa-jussà et al., 2022) as the primary benchmarks to evaluate all the translation models. The results obtained from these benchmarks are reported and discussed in Section 7. Additionally, the performance of the models on other benchmarks is presented in Appendix B. Note that almost all the test sets are English-original, but have been used for Indic-to-English evaluation as well as Indic-Indic evaluation.

### 6.3 Metrics

Several metrics have been developed over the years for automatically assessing translation quality, including stringbased metrics such as BLEU (Papineni et al., 2002), chrF (Popović, 2015), and chrF++ (Popović, 2017), and modelbased metrics such as BLEURT (Sellam et al., 2020), COMET (Rei et al., 2020; 2022) and PRISM (Thompson \& Post, 2020). Recent research (Kocmi et al., 2021; Freitag et al., 2021; 2022) has shown that model-based metrics tend to exhibit a stronger correlation with human judgment. However, these model-based metrics are limited to languages represented in the underlying pre-trained model. They are trained on human judgment data from a few languages, and their performance on many low-resource languages has not been evaluated. We briefly describe all the metrics used in our work below.

BLEU. BLEU (Papineni et al., 2002) has been a standard and widely used metric for evaluating machine translation quality. However, a significant limitation of the standard BLEU metric is its tokenization dependency. To overcome this, sacreBLEU ${ }^{24}$ (Post, 2018) provides standardization in terms of tokenization to ensure a fair comparison. We use sacreBLEU for evaluating our En-Indic and Indic-En trained models. We use the in-built default mteval-v13a tokenizer ${ }^{25}$ for Indic-En ${ }^{26}$ and Indic tokenizer from IndicNLP (Kunchukuttan, 2020) for En-Indic ${ }^{27}$ evaluations. Therefore, we first tokenize the machine translations and reference translations using Indic tokenizers from IndicNLP ${ }^{28}$ (version 0.92 ) and Urduhack ${ }^{29}(\mathrm{ALi}, 2019)$ libraries before running sacreBLEU.

chrF++. chrF++ (Popović, 2017), an extension of the chrF metric (Popović, 2015) that additionally considers word unigrams and bigrams, and is better correlated with human judgments and uses sacreBLEU to compute chrF++ scores. Similar to the tokenizers used for BLEU, for Indic- $\mathrm{En}^{30}$ evaluation, we use the in-built default mteval-v13a tokenizer, while for En-Indic ${ }^{31}$ evaluation, we use Indic tokenizers from IndicNLP and Urduhack libraries to tokenize the machine translations and reference translations before running sacreBLEU.

COMET. COMET is a model-based machine translation evaluation metric introduced by Rei et al. (2020) to address some of the limitations of existing metrics such as BLEU. However, one of the prominent concerns about COMET is its extensibility to low-resource languages. Therefore, in this study, we report COMET-DA scores for the top 13 Indian languages: Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Nepali, Odia, Punjabi, Tamil, Telugu, and Urdu that are supported by the XLM-RoBERTa (Conneau et al., 2020) model. Specifically, we conduct a reference-based evaluation using the COMET-22 DA model ${ }^{32}$ (Rei et al., 2022).

Choosing the Primary Metric. COMET, the most recommended model-based metric (Kocmi et al., 2021), does not support all the 22 Indic languages since they are not represented in XLM-R (Conneau et al., 2020) which is the underlying model on which COMET is based. Conversely, BLEU has several significant limitations, including its tokenization dependency and preferential bias towards translations that are closer to the reference translations in terms of lexical and word order (Ananthakrishnan et al., 2006). Particularly in the context of morphologically rich Indian languages, BLEU is limited in addressing morphological variants since it relies on exact word matches. Furthermore, chrF++ is more suitable for evaluating translation quality in languages with complex morphology and inflections, such as Indian languages. In this work, we, therefore, primarily rely on chrF++ as our primary metric for evaluating translation quality. We also report additional metrics such as BLEU (Papineni et al., 2002) and COMET (Rei et al., 2022). In addition, we also perform paired bootstrap resampling-based statistical significance tests (Koehn, 2004) for all the metrics following the default configurations.

### 6.4 Generation

To generate predictions using IndicTrans2, initially, we preprocess and tokenize the source sentences from the benchmark test set, following the steps described in Section 5.2 and Section 5.3, respectively. Subsequently, we feed the tokenized sentences into the trained models as input to generate candidate translations. We utilize beam search with a beam value of 5 for our trained models. Finally, we employ post-processing techniques, as detailed in Section 5.7, to map the script unified output to the corresponding native script. For other baseline systems, we follow their documented inference procedure. For all the open-source baseline models, we use the same beam size of 5 .[^12]

Table 12: chrF++ scores of all the systems on the IN22-Gen Evaluation set in the En-Indic and Indic-En directions. The best-performing system is bolded, while underlined results indicate significant performance difference where IT2 outperforms the system. The row Avg. means the average score of all the languages that system $\mathrm{X}$ supports. $\Delta$ represents the difference between the average scores of IT2 and the average scores of system $\mathrm{X}$ for the subset of languages that both $\mathrm{X}$ and IT2 support. A positive value for $\Delta$ indicates IT2 is better than $\mathrm{X}$ and vice-versa. $\dagger$ indicates completely off-target translations.

| language | En-Indic |  |  |  |  |  |  | Indic-En |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | IT1 | M100 | N1.2 | N54 | IT2 | Goog | $\mathrm{Az}$ | IT1 | M100 | N1.2 | N54 | IT2 | Goog | $\mathrm{Az}$ |
| $n g$ | 35.9 | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-25.jpg?height=43&width=98&top_left_y=683&top_left_x=644) | 41.7 | $\underline{9}$ | 47.1 | 45.5 | .0 | 1 |  | 3.1 | 6.5 | 65.8 | 5.1 | 8 |
|  | $\underline{48.6}$ | 40.6 |  | $\overline{49.2}$ | 51.8 |  | 8 |  | 52.8 | 60.8 | 63.5 | 63.2 | 64.1 | 0.2 |
|  |  | E |  |  | 47.8 |  |  |  | E | $\overline{-}$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-25.jpg?height=43&width=85&top_left_y=759&top_left_x=1388) | 62.1 |  |  |
| $v a$ |  | - | - | - | 57.8 | 47.8 | - |  |  | - | - | 2.6 | 67.3 |  |
| Deva |  | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-25.jpg?height=43&width=98&top_left_y=836&top_left_x=644) |  | - | 45.2 | 41.4 | $\underline{41.1}$ |  | - |  | - | 59.2 | $\underline{57.8}$ | 51.1 |
|  | 47.2 | 19.9 | 8.3 | 49.5 | 53.5 | 2.2 | $\underline{50.8}$ | 60.3 | 11.8 | 63.9 | 66.3 | 66.5 | 5 | 62.4 |
|  | $\overline{53.3}$ | - |  | 53.9 | 56.7 | 4.6 | 54.1 | $\underline{60.7}$ | $\underline{54.9}$ | $\overline{62.2}$ | 64.8 | 65.4 |  | 62.0 |
|  | $\underline{46.7}$ | 15.3 | 47.3 | $\underline{48.6}$ | 51.0 | 48.1 | $\underline{49.4}$ | $\overline{58.8}$ | 12.6 | 62.4 | $\overline{65.1}$ | 64.2 | 64.5 | 61.7 |
|  | E | - | 34.6 | 3 | 40.2 | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-25.jpg?height=43&width=85&top_left_y=989&top_left_x=1061) | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-25.jpg?height=43&width=84&top_left_y=989&top_left_x=1138) |  | $\underline{54.9}$ | $\underline{58.2}$ | 60.4 | - | - |
|  | F | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-25.jpg?height=43&width=98&top_left_y=1027&top_left_x=644) | $\overline{4}$ | $\underline{44.7}$ | 48 | 38.3 | $\underline{45.2}$ |  | - | 62.1 | $\overline{65.1}$ | 64.8 | 64.0 | 61.0 |
|  | 45.7 | 31.2 | $\underline{45.4}$ | $\overline{46.7}$ | 50.9 | 49.0 | $\underline{48.6}$ | 56.9 | 44.8 | 5 | 62.8 | 64.5 | 6 | 60.4 |
| $v a$ | $\overline{44.3}$ | $\underline{34.5}$ | $\underline{44.7}$ | $\overline{46.1}$ | $\mathbf{5 1 . 0}$ | $\underline{47.1}$ | $\overline{48.2}$ | $\underline{57.7}$ | $\overline{46.9}$ | $\overline{60.9}$ | $\overline{63.6}$ | 63.7 | $\overline{64.4}$ | $\overline{60.3}$ |
|  |  | - |  | $=$ | 44.6 |  |  | - | ] |  | - | 57.9 |  |  |
|  |  | 17.7 | 44.8 | $\underline{44.8}$ | 49.0 | $\overline{45.5}$ | $\underline{46.3}$ |  | 40.1 | 65.0 | 68.0 | 67.7 | 5 | 63.8 |
|  | 40.3 | $\overline{8.2}$ | 42.4 | $\overline{41.5}$ | 43.9 | 40.5 | $\overline{45.4}$ | 60.0 | $\underline{14.4}$ | $\overline{63.7}$ | 66.7 | 66.2 | 64.6 | $\underline{61.1}$ |
| n_Guru | $\underline{48.0}$ | $\underline{25.0}$ | 48.5 | $\overline{49.5}$ | 50.6 | $\overline{52.7}$ | 50.4 | $\overline{57.2}$ | $\overline{38.2}$ | $\overline{60.4}$ | 63.1 | 63.4 | 62.7 | $\overline{58.5}$ |
| eva |  | - | 25.5 | $\overline{28.1}$ | 38.8 | 32.0 | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-25.jpg?height=43&width=85&top_left_y=1292&top_left_x=1061) | - | - | $\underline{48.2}$ | 51.3 | 54.8 | 53.8 |  |
|  | - | - | $\overline{1.0^{\dagger}}$ | $\overline{25.5}$ | 33. |  | - | - | - | $\overline{36.3}$ | $\overline{41.4}$ | 45. |  |  |
|  |  | - |  | $=$ | 36.6 | - | - | - | - |  | - | 57.3 | - | - |
| n_Taml | 45.5 | 12.3 | 47.0 | 47.5 | 49.5 | 48.5 | 49.4 | 53.9 | 26.3 | 56.9 | 59.1 | 59.8 | 59.6 | 56.8 |
|  | 4 | $\overline{-}$ | 48.1 | 49.5 | 52.4 | $\overline{50.8}$ | $\underline{50.6}$ | $\overline{57.7}$ | $\overline{-}$ | $\overline{61.3}$ | $\overline{64.4}$ | 64.8 | 64.6 | 61.2 |
| d_Arab |  | $\underline{45.0}$ | $\underline{62.1}$ | $\underline{63.7}$ | 68.2 | $\underline{63.9}$ | $\overline{69.0}$ |  | 52.6 | $\overline{68.3}$ | $\underline{71.2}$ | 73.0 | $\underline{71.8}$ | 68.2 |
|  |  | 27.1 | 42 | 4 | 48.6 | 46.8 | 4 | 58.0 | 35.9 | 59.4 | 6 | 63.1 |  | 60 |
| $\Delta$ | 5.2 | 25.4 | 6.4 | 4.1 | - | 4.2 | 1.7 | 6.3 | 29.3 | 3.7 | 0.7 | - | 1.1 | 4.2 |

### 6.5 Evaluation

Following the generation of candidate translations, we evaluate their quality using the automatic metrics mentioned in Section 6.3. We apply standard processing techniques to compute the evaluation metrics, followed by running sacreBLEU. We use the standard Moses tokenizer for English, while for Indic languages, we perform tokenization using IndicNLP and Urduhack libraries. We release our evaluation procedure and scripts to ensure reproducibility. We follow the same evaluation procedure for all systems listed in Section 6.1.

## 7 Results and Discussion

### 7.1 Comparison with Existing Systems

Evaluation on IN22-Gen Set. We evaluate the translation quality of multiple En-Indic and Indic-En MT models on the IN22-Gen set. The results are presented in Table 12. We observe that IndicTrans2 significantly improves translation quality over IndicTrans1 (Ramesh et al., 2022) with an average improvement of 5.2 points in the En-Indic direction and 6.3 points improvement in the Indic-En direction. The proposed model outperforms the best commercial and open-source models for En-Indic translation by 1.7 and 4.1 points, respectively. For Indic-En translation, the IndicTrans2 is comparable to existing models, with a delta of +0.7 and +1.1 for best open-source and commercial models, respectively. The results further highlight the substantial improvements made on low-resource languages such

Table 13: chrF++ scores of all the systems on the FLORES-200 devtest set in the En-Indic and Indic-En direction. The best-performing system is bolded, while underlined results indicate significant performance difference where IT2 outperforms the system. Avg. means the average score of all the languages that system $\mathrm{X}$ supports. $\Delta$ represents the difference between the average scores of IT2 and the average scores of system $\mathrm{X}$ for the subset of languages that both $\mathrm{X}$ and IT2 support. A positive value for $\Delta$ indicates IT2 is better than $X$ and vice-versa. $\dagger$ indicates completely off-target translations.

| language | En-Indic |  |  |  |  |  |  | Indic-En |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | IT1 | M100 | N1.2 | N54 | IT2 | Goog | $\mathrm{Az}$ | IT1 | M100 | N1.2 | N54 | IT2 | Goog | $\mathrm{Az}$ |
| asm_Beng | 33.5 | - | 38.6 | 39.0 | 43.3 | 40.9 | 42.8 | $\underline{48.1}$ | - | 55.3 | $\mathbf{5 7 . 8}$ | 56.9 | 57.7 | 53.4 |
| ben_Beng | 49.5 | 44.3 | $\underline{50.1}$ | 52.2 | 54.3 | 53.8 | $\underline{53.4}$ | 56.9 | 54.7 | 60.3 | 62.2 | 62.4 | 63.2 | 59.9 |
| guj_Gujr | $\underline{50.4}$ | 21.9 | 52.0 | 53.6 | 56.0 | 55.5 | $\underline{55.6}$ | 7 | 12.1 | 65.2 | 66.6 | 67.0 | 68.0 | 62.9 |
| hin_Deva | $\overline{56.6}$ | $\overline{53.2}$ | $\underline{56.5}$ | 58.2 | 59.6 | 60.2 | $\overline{59.6}$ | $\overline{61.3}$ | $\overline{60.0}$ | $\overline{65.0}$ | $\underline{66.5}$ | 67.5 | 68.0 | 65.3 |
| kan_Knda | $\underline{50.9}$ | $\overline{16.5}$ | $\underline{53.0}$ | 54.3 | 56.1 | 56.2 | 56.1 | 54.6 | $\overline{12.0}$ | 59.5 | $\overline{61.0}$ | 61.5 | 62.1 | 58.6 |
| kas_Arab |  | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-26.jpg?height=42&width=98&top_left_y=877&top_left_x=644) | 37.2 | 38.0 | 39.7 |  |  |  | $\overline{-}$ | $\overline{57.8}$ | 60.2 | 59.7 |  | - |
| kas_Deva | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-26.jpg?height=43&width=98&top_left_y=911&top_left_x=644) | $\overline{18.7}$ | $\overline{18.8}$ | 19.2 |  |  |  | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-26.jpg?height=43&width=99&top_left_y=911&top_left_x=1214) | 47.7 | 50.6 | 48.3 |  | - |
| mai_Deva |  | - | $\underline{46.1}$ | 47.5 | 50.5 | 41.4 | $\mathbf{5 1 . 0}$ | - | - | 66.6 | 68.3 | 69.5 | 68.8 | 65.2 |
| mal_Mlym | 49.8 | 37.8 | $\underline{49.2}$ | $\underline{52.6}$ | 57.3 | 57.3 | $\underline{56.8}$ | 57.2 | 51.7 | 6 | 6 | 64.3 | 64.5 | 61.3 |
| mar_Deva | $\underline{45.9}$ | $\underline{38.6}$ | $\underline{46.5}$ | $\underline{48.3}$ | 51.3 | 51.4 | $\overline{49.4}$ | $\underline{56.4}$ | $\overline{50.4}$ | $\underline{61.6}$ | $\underline{63.8}$ | 64.3 | 65.3 | 61.5 |
| mni_Beng | - | - | $\underline{37.1}$ | $\overline{42.1}$ | 38.2 | - | $\overline{-}$ | - | - | $\overline{50.5}$ | $\underline{50.7}$ | $\mathbf{5 2 . 9}$ | - | $=$ |
| npi_Deva | - | 15.5 | $\underline{49.2}$ | $\underline{46.4}$ | 57.2 | 55.7 | $\underline{53.4}$ | - | 41.1 | $\underline{65.2}$ | 6 | 68.1 | 68.7 | 63.9 |
| ory_Orya | 44.2 | $\overline{8.5}$ | $\overline{47.6}$ | $\overline{47.0}$ | 49.2 | $\overline{53.9}$ | $\overline{50.2}$ | 55.5 | $\overline{14.3}$ | $\overline{61.8}$ | $\overline{64.4}$ | 64.9 | 64.3 | 60.5 |
| pan_Guru | $\underline{50.6}$ | $\underline{26.8}$ | $\underline{50.9}$ | $\overline{51.3}$ | 53.5 | 54.3 | $\overline{54.2}$ | $\underline{60.0}$ | $\overline{44.5}$ | $\overline{64.5}$ | $\overline{66.3}$ | 66.4 | $\overline{67.1}$ | $\underline{62.7}$ |
| san_Deva | $\overline{-}$ | $\overline{-}$ | $\underline{25.8}$ | $\underline{27.1}$ | 31.6 | 31.3 | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-26.jpg?height=43&width=84&top_left_y=1214&top_left_x=1138) | $\overline{-}$ | $\overline{47.8}$ | 50.7 | 51.6 | 51.2 | - |
| sat_Olck | - | - | $\overline{0.9 \dagger}$ | $\underline{27.0}$ | 28.4 | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-26.jpg?height=50&width=98&top_left_y=1249&top_left_x=971) | - | - | - | $\overline{38.7}$ | $\overline{44.3}$ | 39.3 | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-26.jpg?height=46&width=98&top_left_y=1249&top_left_x=1542) | - |
| snd_Arab |  | 28.6 | $\overline{48.9}$ | $\overline{49.6}$ | 44.9 | 50.4 | 51.1 |  | 19.6 | 64.0 | 66.3 | 65.1 | 66.6 | 59.8 |
| tam_Taml | 49.5 | 13.2 | 53.3 | 54.0 | $\mathbf{5 7 . 2}$ | 56.0 | 56.1 | 54.1 | $\overline{33.0}$ | $\overline{58.9}$ | 60.8 | 61.3 | 61.5 | 57.9 |
| tel_Telu | $\underline{52.6}$ | E | $\underline{55.0}$ | 56.5 | 59.4 | $\overline{59.0}$ | 57.5 | $\overline{58.2}$ | E | $\overline{63.4}$ | $\overline{65.5}$ | 66.1 | 66.7 | $\underline{63.4}$ |
| $u r d \_A r a b$ |  | 39.9 | $\underline{49.4}$ | $\overline{50.3}$ | $\mathbf{5 2 . 2}$ | $\underline{51.3}$ | 51.6 | - | $\underline{48.8}$ | 6 | $\overline{62.9}$ | 62.0 | 63.7 | 59.3 |
| Avg. | 48.5 | 28.7 | 43.3 | 45.7 | 48.0 | 51.8 | 53.3 | 56.5 | 36.9 | 58.8 | 60.9 | 61.0 | 64.2 | 61.0 |
| $\Delta$ | 5.8 | 25.4 | 4.7 | 2.3 |  | 0.3 | 0.2 | 7.4 | 27.7 | 2.2 | 0.1 |  | -0.5 | 3.5 |

as Dogri (+10), Konkani (+3.8), Kashmiri (+4.8), Maithili (+3.8), Manipuri (+9.6) for En-Indic and Dogri (+5.3), Manipuri (+7.2), Santali (+3.9) for Indic-En translations when compared to the next best model. The observed gains can be attributed to using high-quality human-annotated BPCC-H Wiki data for training MT models. These findings suggest that the proposed model is well-suited for adoption in the Indian subcontinent, aligning with the objective of building models suitable for Indian languages. Additionally, we also report the COMET (Rei et al., 2022) and BLEU (Papineni et al., 2002) scores for our models in Table 41 and Table 44 (in Appendix B) where we observe similar trends, indicating that the observations are robust across different metrics.

Evaluation on FLORES-200. We also evaluate the MT models on the FLORES-200 benchmark (Costa-jussà et al., 2022). Through this evaluation, we aim to assess the model's translation quality on more general content, complementing the evaluation on our IN22 test set which is India-centric. Therefore, by evaluating our models on both IN22 and FLORES-200, we can effectively gauge the model's translation quality in different settings. The results in Table 13 obtained from the FLORES-200 test set show a similar trend as IN22, with IndicTrans2 being the best open-source model performing competitively with commercial models. The results also show a significant improvement from IndicTrans1 to IndicTrans2, with +5.8 and +7.4 points improvement in En-Indic and Indic-En translations, respectively. We also report the COMET and BLEU scores for the FLORES-200 benchmark in Table 43 and Table 46 (in Appendix B).

Evaluation on IN22-Conv Set. While both the IN22-Gen Set and FLORES-200 (Costa-jussà et al., 2022) focus on written sentences, the real-world usage of MT is often task-oriented and involves conversational language. To address this, all the models are further evaluated on the IN22-Conv Set, which is designed to test the translation quality of MT models on conversational language and daily use scenarios. The results of all the models on the IN22-Conv Set are

Table 14: chrF++ scores of all the systems on the IN22-Conv Evaluation set in the En-Indic and Indic-En directions. The best performing system is bolded, while underlined results indicate significant performance difference where IT2 outperforms the system. Avg. means the average score of all the languages that system $\mathrm{X}$ supports. $\Delta$ represents the difference between the average scores of IT2 and the average scores of system $\mathrm{X}$ for the subset of languages that both $\mathrm{X}$ and IT2 support. A positive value for $\Delta$ indicates IT2 is better than $X$ and vice-versa. $\dagger$ indicates completely off-target translations.

| language | En-Indic |  |  |  |  |  |  | Indic-En |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | IT1 | M100 | N1.2 | N54 | IT2 | Goog | $\mathrm{Az}$ | IT1 | M100 | N1.2 | N54 | IT2 | Goog | $\mathrm{Az}$ |
| m_Beng | 36.4 |  | $\underline{42.6}$ | 43.4 | 46.8 | 3.6 | 46.6 | 52.5 |  | 58.7 | 59.8 | 62.9 | 64.0 | 62.1 |
|  | 47.5 | 39.7 | $\overline{47.1}$ | 48.5 | 49.7 | 3.9 | $\underline{48.8}$ | 5.2 | 48.1 | 55.4 | $\underline{57.0}$ | 58.4 | 59.6 |  |
|  | 7 | e | 7 | l | 45.3 |  |  | 7 |  | 7 | 7 | 56.3 |  |  |
| i_Deva | - | - | - | ... | 53.9 | 40.1 |  |  |  | - | - | 65.0 | 62.9 |  |
| Seva |  | - | $\square$ | - | 42.5 | 0.3 | $\underline{38.7}$ | - |  | - | - | 1.7 | $\overline{51.6}$ | 46.1 |
|  | $\underline{49.1}$ | 21.0 | 48.7 | $\underline{49.8}$ |  | $\underline{51.9}$ | $\underline{51.8}$ | 56.9 | 6.5 | 60.8 | 61.4 | 62.0 | 62.2 | 61 |
| $a$ | $\underline{48.6}$ | 42.7 | $\underline{47.6}$ | $\underline{48.3}$ | 49.6 | $\overline{50.6}$ | $\underline{48.7}$ | $\overline{57.4}$ | 50.6 | $\underline{58.7}$ | $\overline{59.7}$ | 60.1 | 60.0 | $\underline{59}$ |
| in_Knda | $\underline{32.6}$ | 13.7 | $\underline{32.2}$ | 33.3 | 33.8 | 33.1 | $\underline{33.5}$ | 44.0 | 7.2 | 45.3 | 46.2 | 47.5 | 48.0 | 48.1 |
|  | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-27.jpg?height=42&width=98&top_left_y=990&top_left_x=645) | $\underline{25.7}$ | 27.1 | 3. |  |  |  | - | 44.6 | $\underline{45.2}$ | 5 | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-27.jpg?height=42&width=71&top_left_y=990&top_left_x=1633) |
|  | - | - | $\underline{41.6}$ | $\underline{41.0}$ | 44.3 | 5.6 | $\underline{38.2}$ |  | - | $\overline{55.2}$ | $\underline{56.7}$ | 57.8 | 59.1 | 55. |
| ll_Mlym | $\underline{43.8}$ | $\underline{32.0}$ | $\underline{40.9}$ | $\underline{40.8}$ | 45.7 | $\underline{45.2}$ | $\underline{44.9}$ | 50.6 | 38.8 | $\overline{51.0}$ | $\overline{52.6}$ | 54.3 | 54.6 | $\overline{54}$ |
|  | $\underline{43.7}$ | 33.9 | $\underline{44.8}$ | $\overline{47.3}$ | 48.6 | 6.6 | $\underline{46.3}$ | $\overline{54.2}$ | 40.4 | $\overline{56.2}$ | $\underline{57.5}$ | 58.5 | 59.4 | 58.3 |
|  |  |  |  | - | 40.2 | $\underline{31.2}$ |  | $=$ |  | - | - | $\mathbf{5 2 . 5}$ | 46.3 |  |
|  | - | 15.3 | 44.9 | 44.3 | $\mathbf{5 1 . 5}$ | $\overline{6.1}$ | 46.4 |  | 21.0 | 59.9 | 60.6 | 63.0 | $\overline{63.9}$ | 62.0 |
| rya | 38.9 | $\underline{7.6}$ | $\overline{41.3}$ | $\overline{40.9}$ | 40.2 | $\underline{37.7}$ | $\overline{42.1}$ | 55.6 | 11.5 | $\overline{59.3}$ | $\overline{59.8}$ | $\mathbf{6 0 . 3}$ | 59.0 | $\underline{58.7}$ |
|  | $\underline{54.0}$ | 25.4 | 54.3 | 55.5 | 57.8 | 61.1 | $\underline{56.8}$ | $\overline{58.1}$ | 32.4 | $\overline{60.1}$ | 61.4 | 62.7 | $\overline{61.1}$ | $\overline{61}$ |
|  |  | $\cdots$ | $\underline{26.4}$ | $\overline{30.3}$ | 35.5 | 32.8 | l | $\square$ |  | 38.9 | 40.2 | 48.3 | 49.2 |  |
| $s c$ | - | - | $\underline{0.8}$ | 18.0 | 34.6 |  | - | - |  | 33.6 | $\underline{37.4}$ | 43.5 | - |  |
| snd_Deva | - | - | $\overline{-}$ | $=$ | 30.3 | - | - | - |  | - | - | 49.6 | - | - |
|  | 37.7 | 19.2 | 37.2 | 37.1 | 39.1 | 38.7 | 39.1 | 44.1 | $\underline{22.5}$ | 45.7 | 46.8 | 45.8 | 46.8 | 46.4 |
|  | $\underline{42.5}$ | - | 39.9 | 40.5 | 45.5 | 44.6 | $\underline{44.9}$ | 48.5 |  | $\underline{51.3}$ | 53.3 | 52.9 | 53.9 | 53.6 |
| $u r d \_A r a b$ | 7 | $\underline{42.5}$ | $\underline{55.9}$ | $\underline{55.5}$ | 61.6 | $\underline{60.6}$ | $\underline{59.6}$ | $=$ | 47.9 | $\underline{61.5}$ | $\underline{62.3}$ | 65.5 | 65.3 | $\underline{64.9}$ |
| $A v$ | 43 | 26.6 | 39.5 | 41.3 | 44.8 | 43 | 45.8 | \| 52.5 | 29.7 | 52.7 | 54.0 | 56.0 | 57.1 | 56.7 |
| $\Delta$ | 3.2 | 21.6 | 5.7 | 3.9 | $\square$ | 2.8 | 1.5 | 4.4 | 28.3 | 3.3 | 2.0 | ![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-27.jpg?height=43&width=78&top_left_y=1578&top_left_x=1466) | 0.1 | 0.9 |

presented in Table 14. Across the board, the results show moderately strong translation quality by all the models. Overall, a similar trend is observed for En-Indic translations, with IndicTrans2 outperforming the best open-source models and commercial models. Similarly, in the case of Indic-En translations, IndicTrans2 outperforms the best open-source models and performs competitively with commercial models. The results further highlight significant improvements in the quality of translations for low-resource languages such as Dogri (+13.8), Kashmiri (+8.5), Manipuri Meitei (+9), Sanskrit (+2.7), and Santali (+16.6) in the En-Indic direction and Kashmiri (+7.4), and Santali (+6.1) in the Indic-En direction respectively, compared to the best available existing systems. Given that IndicTrans2 supports all 22 scheduled languages and performs well across all of them, the model is expected to have good usability in both informational and conversational settings. Additionally, we also report the COMET (Rei et al., 2022) and BLEU (Papineni et al., 2002) scores for our models in the Table 42 and Table 45 (in Appendix B).

Evaluation on Other Benchmarks. We perform evaluations on other publicly available benchmarks and the detailed results are presented in Appendix B, while a summary of the observations is presented in this section. Specifically, we evaluate the models on WAT 2020 (Nakazawa et al., 2020) and WAT2021 (Nakazawa et al., 2021a), which were created from the PMIndia corpus containing data from speeches and news from the Prime Minister of India. Across the board, the results presented in Table 32 and Table 33 show that IndicTrans2 outperforms all open-source and commercial models in both Indic-En and En-Indic translation directions, with the exception of IndicTrans1. However, it is important to note that performance improvement for IndicTrans1 stems from the fact that their validation set consisted of the development sets of various shared task benchmarks like WAT, WMT, and FLORES-200. On the contrary, our
work used the FLORES-200 development set as the validation set with the aim of attaining strong performance across multiple domains. Along the same lines, we evaluate our models on the NTREX (Federmann et al., 2022) Evaluation set, which is derived from the news domain. The results presented in Table 29 and Table 30 show similar findings with IndicTrans 2 performing the best among all the compared models with +3 and +2.6 points improvement over the best open-source model in En-Indic and Indic-En directions respectively. However, on the UFAL test set involving Tamil language, among open-source models, we observe that our model lags behind the IndicTrans1 and NLLB 1.2B model in the En-Indic direction (Table 38).

Best Open-Source Model. Our study evaluated the translation quality of IndicTrans2 and other open-source models on various benchmarks. While IN22 and FLORES-200 (Costa-jussà et al., 2022) evaluated the models on diverse domain content such as sports, news, and conversational texts, we further tested the models on WAT2020 (Nakazawa et al., 2020), WAT2021 (Nakazawa et al., 2021a), and NTREX (Federmann et al., 2022). Across all multi-domain benchmarks, we observed that IndicTrans2 consistently outperformed other open-source models, demonstrating its better translation capabilities. However, it is important to note that performance improvement for IndicTrans1 on WAT2020 (Nakazawa et al., 2020) and WAT2021 (Nakazawa et al., 2021a) can be attributed due to explicit optimization across different benchmarks by incorporating development sets of various shared tasks, in addition to FLORES-200. In contrast, our development set only comprises FLORES-200. Detailed results for all the benchmarks and models are presented in Appendix B (refer Tables 29, 32 and 33). Additionally, IndicTrans2 has the highest coverage of languages and written scripts, with support for 22 Indic languages and 25 language-script combinations. Further, while the current SOTA open-source model, the NLLB 54B MoE model (Costa-jussà et al., 2022), is impressive in its capabilities, it is impractical for deployment due to its high latency and resource requirements. Our study addresses this challenge by developing comparatively compact models that can compete with large-scale models even when trained on smaller datasets, emphasizing quality and cost-effectiveness. Results on different benchmarks confirm the robust performance of our model across various domains and distributions. Therefore, we can conclude that our model has fair generalization capabilities, performing well across most of the benchmarks.

Supporting New Languages and Scripts. Our work bridges the gap left by existing open-source and commercial systems by extending IndicTrans1 (Ramesh et al., 2022) to support all 22 scheduled Indic languages, including lowresource languages and multiple scripts. We train the first open-source model with reasonable performance for the following languages: Bodo, Dogri, and Konkani. For some languages, we support translation in scripts that were hitherto unsupported like Sindhi (Devanagari script) or are only supported by commercial systems like Manipuri (Meitei). In addition, we also improve translation quality significantly for low-resource languages such as Dogri, Maithili, Manipuri (Meitei), and Nepali. The human-annotated seed parallel data (refer Table 1) for these languages help us outperform other models which rely on unsupervised methods and/or mined data for these low-resource languages. This suggests that investments in creating small parallel corpora for low-resource languages can substantially improve translation quality, corroborating findings from Costa-jussà et al. (2022).

Comparison across language families. Our analysis reveals that on low-resource languages from the Sino-Tibetan and Austroasiatic language families models tend to consistently underperform compared to mid and high-resource languages in the Indo-Aryan and Dravidian families. Conversely, on mid and high-resource languages, all models seem to exhibit comparable performance. These observations suggest that the major differences in performance are coming from the low-resource language families. Notably, no other open-source or commercial model covers all four language families. The results for all the models on our primary benchmarks are presented in Figure 5.

Additionally, we conduct a small-scale human evaluation exercise to verify if the quality of our model outputs correlates with the improvements observed using automatic metrics. This preliminary human evaluation exercise focused on the En-Indic direction and included 50 examples each from the Wikipedia and Web sources subset to yield a total of 100 sentence pairs from IN22-Gen and is described in Appendix C. However, future efforts should focus on large-scale human evaluation to understand the potential biases and shortcomings of our IndicTrans 2 models and assess their feasibility in practical use-case scenarios.
![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-29.jpg?height=1060&width=1652&top_left_y=272&top_left_x=236)

Figure 5: Average performance improvements in terms of chrF++ across language families on IN22 and FLORES-200 (Costa-jussà et al., 2022) benchmarks.

### 7.2 Understanding Data Scale vs Quality tradeoff

Prior works such as NLLB (Costa-jussà et al., 2022) have focused on scaling the data to improve the model performance. They use a margin-based mining approach with a threshold of 1.06. However, from an in-house manual inspection, it was observed that the data was noisy. As a result, we conducted an ablation study to understand the trade-off between data scale and quality for effectively training multilingual MT models. In this ablation, we consider existing mined parallel corpora such as Samanantar (Ramesh et al., 2022) and NLLB (Costa-jussà et al., 2022) and specifically focus on the subset of 11 languages that are common to both. We apply an additional quality filter, where we eliminate the bitext pairs that fall below the LABSE (Feng et al., 2022) cosine similarity threshold of 0.80 . This resulted in a reduction from 384M (Unfiltered data) to 94M (filtered data) in total. Subsequently, we train two separate models with the same architecture (refer to Section 5.4) and stage 1 hyperparameters (refer to Table 10) as our final IndicTrans2 models on filtered and unfiltered versions of the data. The results shown in Table 15 demonstrate that the models trained on the high-quality filtered subset perform on par or even superior to the model trained on the unfiltered data. This suggests that eliminating the noisy and suboptimal bitext pairs through this additional filter improves the model performance and accelerates model convergence. We, therefore, adopt this filtering threshold for our final training, ensuring that our model benefits from the improved data quality.

### 7.3 Impact of Sequential Training with Human Annotated Data

We train our models sequentially, where stage 1 involves training on a combination of all the existing data, mined data, and high-quality seed data, while stage 2 involves fine-tuning with high-quality seed data (as described in Section 5.5). Our seed data involves a combination of NLLB Seed (Costa-jussà et al., 2022; Maillard et al., 2023) and our humanannotated data BPCC-H-Wiki (refer Table 1). As seed data for Sindhi (Arabic) is not present in both the sources, we

Table 15: chrF++ scores of the models trained on unfiltered (pre-filtering) and filtered data (post-filtering), on the FLORES-200 Evaluation set in the En-Indic and Indic-En directions. The best-performing system is bolded. $\Delta$ represents the difference between the scores of the model trained on filtered data and unfiltered data. A positive value for $\Delta$ indicates that the model trained on filtered data (post-filtering) is better than unfiltered (pre-filtering) and vice-versa.

| language | Dataset Size |  | En-Indic |  |  | Indic-En |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Pre-Filter | Post-Filter | Pre-Filter | Post-Filter | $\Delta$ | Pre-Filter | Post-Filter | $\Delta$ |
| asm_Beng | $5.3 \mathrm{M}$ | $0.5 \mathrm{M}$ | 34.6 | 39.0 | 4.4 | 49.2 | 51.9 | 2.7 |
| ben_Beng | $70.4 \mathrm{M}$ | $16.5 \mathrm{M}$ | 52.2 | 53.1 | 0.9 | 60.0 | 60.2 | 0.2 |
| guj_Gujr | $14.4 \mathrm{M}$ | $8.4 \mathrm{M}$ | 51.4 | 52.4 | 1.0 | 64.0 | 63.9 | -0.1 |
| hin_Deva | $43.1 \mathrm{M}$ | $11 \mathrm{M}$ | 58.1 | $\mathbf{5 8 . 7}$ | 0.6 | 64.4 | 64.6 | 0.2 |
| kan_Knda | $38.3 \mathrm{M}$ | $10.5 \mathrm{M}$ | 52.7 | 53.3 | 0.6 | 58.6 | 58.7 | 0.1 |
| mal_Mlym | $49.6 \mathrm{M}$ | $10.8 \mathrm{M}$ | 52.8 | 55.1 | 2.3 | 60.2 | 61.1 | 0.9 |
| mar_Deva | $35.6 \mathrm{M}$ | $7.74 \mathrm{M}$ | 46.9 | 48.5 | 1.6 | 60.6 | $\mathbf{6 0 . 7}$ | 0.1 |
| ory_Orya | $14.7 \mathrm{M}$ | $2.9 \mathrm{M}$ | 42.6 | 46.1 | 3.5 | 58.8 | 60.0 | 1.2 |
| pan_Guru | $14 \mathrm{M}$ | $3.3 \mathrm{M}$ | 49.1 | 50.6 | 1.5 | 62.7 | 63.1 | 0.4 |
| tam_Taml | $47.7 \mathrm{M}$ | $10.4 \mathrm{M}$ | 53.3 | 55.3 | 2.0 | 58.0 | 58.2 | 0.2 |
| tel_Telu | $51.2 \mathrm{M}$ | $11.8 \mathrm{M}$ | 56.0 | 56.8 | 0.8 | 63.0 | 63.2 | 0.2 |
| Avg. | - | - | 50.0 | $\mathbf{5 1 . 7}$ | 1.7 | 60.0 | 60.5 | 0.6 |

Table 16: Performance improvements of En-Indic and Indic-En models on chrF++ metric on our primary evaluation benchmarks w.r.t. sequential training.

| Benchmark | En-Indic | Indic-En |
| :--- | :---: | :---: |
| FLORES-200 | +1.5 | +0.6 |
| IN22-Gen | +2.2 | +0.5 |
| IN22-Conv | +2.7 | +1.9 |
| Average | +2.1 | +1.0 |

use the Sangam transliteration $\mathrm{API}^{33}$ (Lehal \& Saini, 2014) to transliterate the Sindhi BPCC-H-Wiki data ( 10.5K) from Devanagari script to Perso-Arabic script. We observe that fine-tuning our models with high-quality seed data is beneficial and leads to an average improvement of 2.1 points and 1 point in En-Indic and Indic-En directions, respectively, on our primary evaluation benchmarks in terms of chrF++ metric (see Table 16). These findings align with previous works (Mohiuddin et al., 2022), which show that deterministic data selection curriculum involves pretraining on general domain corpora followed by fine-tuning with high-quality data subset of general domain corpora results in solid performance improvements over the preliminary models. A critical distinction from the above approach is that we only use the human-annotated seed data for fine-tuning, rather than retrieval of top $p \%$ samples from training data based on lexical similarity. Our observations indicate that although sequential training yields gains on an aggregate level, it is important to note that for specific languages such as Sindhi (Arabic) (where we use transliterated data), our En-Indic model tends to degrade ( $\sim 3$ points in chrF++) in terms of performance, highlighting that it is crucial to use high-quality human annotated data for fine-tuning.

Furthermore, Table 17 reports the performance of IndicTrans2 models for various training stages on IN22-Gen Set. Notably, the highest improvement was observed in Santali for the En-Indic direction in both $\Delta_{1}$ and $\Delta_{2}$. It is also worth highlighting that the human-annotated seed data from previous work and our current work serves as the primary and most influential source for mid-resource and low-resource languages, including Dogri, Konkani, Sindhi (Devanagari), Santali, and Manipuri (Meitei) as shown in Table 1. Despite the smaller size of seed data compared to mined corpora, finetuning on this leads to superior performance across different benchmarks (refer Tables 12 to 14). Although $\Delta_{1}$ and $\Delta_{2}$ may be smaller for a few languages due to the saturation of the data diversity during multi-stage training, the seed data proves to be beneficial on an aggregate level, further reinforcing its positive impact.[^13]

Table 17: chrF++ score on IN22-Gen Evaluation Set for various training stages. OG refers to the model trained on the original training corpora, while OG-Seed refers to the seed data fine-tuned version of the OG model. $\Delta_{1}$ represents the gains obtained by fine-tuning the original model with seed data. DA refers to the model trained on the combination of original training data with augmented data, while DA-Seed refers to the seed data fine-tuned version of the DA model. $\Delta_{2}$ represents the gains obtained by fine-tuning on seed data after data augmentation.

| language | En-Indic |  |  |  |  |  | Indic-En |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\overline{\mathrm{OG}}$ | OG-Seed | $\Delta_{1}$ | $\mathrm{DA}$ | $\mathrm{DA}+$ Seed | $\Delta_{2}$ | $\mathrm{OG}$ | OG-Seed | $\Delta_{1}$ | $\mathrm{DA}$ | $\mathrm{DA}+$ Seed | $\Delta_{2}$ |
| asm_Beng | 43.4 | 45.6 | 2.2 | 44.8 | 47.1 | 2.3 | 61.9 | 62.1 | 0.2 | 64.9 | 65.8 | 0.9 |
| ben_Beng | 48.2 | 50.3 | 2.1 | 48.8 | 51.8 | 3.0 | 60.6 | 60.8 | 0.2 | 62.4 | 3.2 | 0.8 |
| brx_Deva | 44.5 | 47.1 | 2.6 | 46.3 | 47.8 | 1.5 | 58.1 | 58.4 | 0.3 | 61.9 | 2.1 | 0.2 |
| doi_Deva | 55.4 | 55.7 | 0.3 | 56.2 | 57.8 | 1.6 | 68.6 | 68.5 | -0.1 | 72.7 | 72.6 | -0.1 |
| gom_Deva | 42.2 | 43.8 | 1.6 | 43.2 | 45.2 | 2.0 | 55.9 | 56.5 | 0.6 | 58.7 | 59.2 | 0.5 |
| guj_Gujr | 49.4 | 51.6 | 2.2 | 50.0 | 53.5 | 3.5 | 64.0 | 63.9 | -0.1 | 65.7 | 66.5 | 0.8 |
| hin_Deva | 53.5 | 54.6 | 1.1 | 53.6 | 56.7 | 3.1 | 62.8 | 63.4 | 0.6 | 64.7 | 65.4 | 0.7 |
| kan_Knda | 47.3 | 49.7 | 2.4 | 47.7 | $\mathbf{5 1 . 0}$ | 3.3 | 61.7 | 62.0 | 0.3 | 63.2 | 64.2 | 1.0 |
| kas_Arab | 37.7 | 38.8 | 1.1 | 38.3 | 40.2 | 1.9 | 55.6 | 56.1 | 0.5 | 60.0 | 0.4 | 0.4 |
| mai_Deva | 45.9 | 47.3 | 1.4 | 46.2 | 48.7 | 2.5 | 62.1 | 61.9 | -0.2 | 64.6 | 64.8 | 0.2 |
| mal_Mlym | 47.9 | 49.7 | 1.8 | 48.4 | 50.9 | 2.5 | 60.7 | 61.5 | 0.8 | 63.1 | 64.5 | 1.4 |
| mar_Deva | 45.7 | 48.6 | 2.9 | 46.6 | $\mathbf{5 1 . 0}$ | 4.4 | 60.7 | 61.1 | 0.4 | 62.3 | 63.7 | 1.4 |
| mni_Mtei | 39.6 | 41.3 | 1.7 | 41.8 | 44.6 | 2.8 | 53.2 | 53.3 | 0.1 | 57.6 | 7.9 | 0.3 |
| npi_Deva | 44.5 | 47.5 | 3.0 | 45.4 | 49.0 | 3.6 | 64.4 | 64.4 | 0.0 | 67.1 | 67.7 | 0.6 |
| ory_Orya | 40.1 | 41.9 | 1.8 | 41.0 | 43.9 | 2.9 | 63.1 | 63.4 | 0.3 | 65.3 | 66.2 | 0.9 |
| pan_Guru | 49.5 | 50.6 | 1.1 | 50.2 | 50.6 | 0.4 | 61.0 | 61.4 | 0.4 | 62.9 | 63.4 | 0.5 |
| san_Deva | 35.9 | 37.7 | 1.8 | 36.9 | 38.8 | 1.9 | 50.9 | 51.1 | 0.2 | 54.4 | 54.8 | 0.4 |
| sat_Olck | 24.2 | 27.3 | 3.1 | 26.5 | 33.4 | 6.9 | 43.6 | 43.8 | 0.2 | 44.5 | 453 | 0.8 |
| snd_Deva | 34.8 | 36.2 | 1.4 | 35.3 | 36.6 | 1.3 | 53.6 | 53.7 | 0.1 | 56.5 | 57.3 | 0.8 |
| tam_Taml | 47.3 | 48.7 | 1.4 | 47.9 | 49.5 | 1.6 | 57.2 | 57.5 | 0.3 | 59.1 | 59.8 | 0.7 |
| tel_Telu | 49.6 | 51.3 | 1.7 | 50.0 | 52.4 | 2.4 | 62.3 | 62.6 | 0.3 | 64.0 | 64.8 | 0.8 |
| urd_Arab | 63.8 | 67.1 | 3.3 | 65.4 | 68.2 | 2.8 | 69.5 | 69.9 | 0.4 | 72.5 | 73.0 | 0.5 |

Table 18: Comparison of average chrF++ scores between our stage 2 auxiliary model and the best open-source baseline on FLORES-200 (Costa-jussà et al., 2022) Evaluation set at the end of stage 2 auxiliary training. OG-seed denotes the model trained on the original data followed by fine-tuning with seed data. $\Delta$ denotes the difference between the scores of our stage 2 auxiliary model and the best open-source baseline.

|  | N54 | OG-Seed | $\Delta$ |
| :---: | :---: | :---: | :---: |
| xx-eng_Latn | 60.9 | 58.1 | -2.8 |
| eng_Latn-xx | 45.7 | 47.8 | 2.1 |

### 7.4 Impact of Data Augmentation

Section 5.6 describes the procedure and heuristics for synthetic data generation to further improve our auxiliary models. Initially, we adopted the back-translation approach for generating the augmented data. We primarily base our decision to start with an auxiliary En-Indic model for generating back-translation data for Indic-En translation due to its competitive or better performance compared to the best open-source baseline (see Table 18). We combine the original data and the English back-translated data, obtained using our auxiliary En-Indic model, to train our new Indic-En model from scratch, followed by high-quality seed data fine-tuning. In this case, following prior study (Caswell et al., 2019), we use "__bt__" indicator tags to provide some supervision to the model to distinguish original data from the back-translated data. We observe a considerable performance improvement across all our primary evaluation benchmarks on our IndicEn model, as shown in Figure 6 when we perform training on the combination of original and back-translated data (refer Table 17).

![](https://cdn.mathpix.com/cropped/2024_06_04_2d69ad1bfa3a196bbbb6g-32.jpg?height=683&width=1619&top_left_y=285&top_left_x=253)

Figure 6: Average Performance of our En-Indic and Indic-En models across different stages in terms of chrF++ metric on our primary evaluation sets.

Following iterative back translation (Hoang et al., 2018), we use the stage 2 fine-tuned downstream Indic-En model to generate the back-translation data due to its superior performance compared to the auxiliary Indic-En model. Similarly, we combine the Indic back-translated data along with the original data using indicator tags and train our new En-Indic model from scratch, followed by fine-tuning with seed data. However, we do not observe any gains for the new EnIndic model compared to the stage 2 auxiliary fine-tuned En-Indic model. Further investigation is needed to determine the exact reasons for the performance limitations of our newly trained En-Indic model, but we suspect that unlike for Indic-En translation, the increase in the Indic target side data is insufficient, both in terms of domain coverage and amount. This conjecture is based on the fact that a significant portion of both the original training corpus and the backtranslated data is sourced from the news domain, resulting in considerable overlap in their distributional coverage. The lack of diversity in domains may potentially hinder the model from reaching its optimal capabilities. Furthermore, for Indic-En translation, the amount of target side English data almost triples in amount when back-translated data is added to the original parallel corpus. However, in the case of English-Indic translation, where multiple target languages are involved, the relative augmentation per language is comparatively lower, which might potentially explain the marginal enhancement observed in the English-Indic direction. Increased availability of Indic language monolingual corpora, ideally from various domains, should help remedy this issue.

Since backtranslation did not help in the En-Indic direction, we looked at the findings from distillation works like Kim \& Rush (2016); Gumma et al. (2023), and trained an En-Indic model on the combination of original data and forward translated data/distillation data (flipping the English BT data). In this case, we use "__ft__" indicator tags instead of “__bt__" indicator tags. Here, we observe marginal performance improvements for our newly trained En-Indic model on combining original data and forward translated data, as shown in Figure 6 (refer Table 17). Although this model is not particularly better than the one obtained using back-translation, it does exhibit better performance, and thus we consider this as our final En-Indic model. Overall, our En-Indic model is competitive or better when compared to the baselines, but further research is necessary to explore effective methods to improve the En-Indic model.

### 7.5 Indic-Indic Evaluation

Our IndicTrans2 models have exhibited strong performance across various benchmarks, as detailed in Section 7.1. Building upon these findings, we aim to conduct a comprehensive evaluation of the Indic-Indic translation capabilities of our IndicTrans2 models in both pivot-based and direct setups.

Table 19: chrF++ scores of Indic-Indic evaluation on FLORES-200 (Costa-jussà et al., 2022) of our IndicTrans2-Pivot (IT2-Pivot) model, IndicTrans2-M2M (IT2-M2M) model, compressed IndicTrans2-M2M (IndicTrans2-Dist-M2M) model and NLLB 54B MoE model. "xx-\{lang\}" and "\{lang\}-xx" denote the average chrF++ scores to that language and from that language, respectively.

| language | $\mathrm{xx}-\{$ lang $\}$ |  |  |  | \{lang\}-xx |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | N54 | IT2-Pivot | IT2-M2M | IT2-Dist-M2M | N54 | IT2-Pivot | IT2-M2M | IT2-Dist-M2M |
| asm_Beng | 36.7 | 38.0 | 37.9 | 37.4 | 39.5 | 41.0 | 39.7 | 39.3 |
| ben_Beng | 44.5 | 45.7 | 44.7 | 43.7 | 41.4 | 43.0 | 42.1 | 41.6 |
| guj_Gujr | 44.8 | 45.9 | 44.8 | 44.2 | 43.4 | 44.9 | 43.8 | 43.3 |
| hin_Deva | 48.4 | 48.6 | 47.7 | 46.8 | 42.9 | 44.6 | 43.8 | 43.6 |
| $k a n \_K n d a$ | 46.6 | 47.3 | 45.9 | 45.1 | 40.6 | 42.3 | 41.2 | 40.8 |
| kas_Arab | 32.6 | 33.8 | 33.1 | 32.8 | 40.7 | 41.7 | 39.9 | 39.2 |
| mai_Deva | 37.9 | 41.5 | 40.5 | 40.4 | 45.0 | 45.9 | 44.9 | 44.7 |
| mal_Mlym | 45.7 | 47.8 | 46.2 | 45.1 | 41.2 | 43.3 | 42.0 | 41.5 |
| mar_Deva | 41.9 | 43.6 | 42.5 | 41.7 | 42.4 | 44.1 | 43.0 | 42.5 |
| npi_Deva | 43.6 | 46.9 | 45.8 | 45.4 | 43.1 | 45.0 | 44.0 | 43.5 |
| ory_Orya | 41.1 | 41.6 | 40.8 | 40.2 | 42.7 | 44.3 | 43.3 | 42.8 |
| pan_Guru | 44.4 | 44.6 | 43.8 | 43.1 | 43.4 | 44.6 | 43.5 | 43.2 |
| san_Deva | 25.6 | 28.9 | 28.7 | 28.6 | 35.7 | 38.1 | 36.5 | 35.9 |
| sat_Olck | 25.7 | 26.6 | 26.3 | 26.1 | 32.4 | 31.4 | 32.5 | 31.5 |
| tam_Taml | 47.3 | 48.7 | 47.3 | 46.1 | 40.1 | 41.7 | 40.1 | 39.7 |
| tel_Telu | 47.0 | 48.5 | 47 | 46 | 41.9 | 43.7 | 42.6 | 41.8 |
| urd_Arab | 43.7 | 44.4 | 43.9 | 43.1 | 41.1 | 42.7 | 41.6 | 41 |

### 7.5.1 Pivoting

Pivoting (Gispert \& Mariño, 2006; Utiyama \& Isahara, 2007; Bertoldi et al., 2008) is a widely used approach in nonEnglish centric translation scenarios, where direct parallel corpora are limited or unavailable. It involves utilizing a high-resource language as an intermediary, translating from the source to the pivot language and then to the target language. The pivot method is a strong baseline for non-English centric translation compared to many other methods proposed to address this task (Freitag \& Firat, 2020; Chen et al., 2017; Firat et al., 2016; Arivazhagan et al., 2019; AlShedivat \& Parikh, 2019). In our study, we leverage our Indic-En model followed by the En-Indic model to facilitate Indic-Indic translation, as our IndicTrans2 models are trained using English-centric parallel corpora and use English as the pivot language. To assess the Indic-Indic translation performance, we evaluate our IndicTrans2 models on $n$-way parallel test sets such as FLORES-200 (Costa-jussà et al., 2022) and IN22 benchmarks. The generation and evaluation procedure for Indic-Indic translations is the same as described in Section 6.4 and Section 6.5.

The performance in Indic-Indic translation for our pivot-based IndicTrans2 and NLLB (Costa-jussà et al., 2022) is shown in Table 19 for FLORES-200, Table 20 for IN22-Gen and Table 21 for IN22-Conv, using average chrF++ scores over common languages across NLLB, our pivot as well as direct systems described in Section 7.5.2. For each language (lang), "xx-\{lang\}" denotes the average scores from all the common languages in that language, whereas "\{lang\}-xx" denotes the average scores from that language into all the common languages. Table 19 shows that our pivot-based IndicTrans2 outperforms or is on par with the multi-way trained NLLB 54B MoE model across all IndicIndic directions on FLORES-200 (Costa-jussà et al., 2022). It is important to note that we directly evaluate the NLLB 54B model by using the translation outputs ${ }^{34}$ released by Costa-jussà et al. (2022). However, for the evaluation on the IN22 benchmark, we use the NLLB 1.2B distilled model instead of the NLLB 54B MoE model due to resource constraints due to the sheer number of translation directions. Our pivot-based IndicTrans2 significantly outperforms the NLLB 1.2B distilled model, as shown in Tables 20 and 21. NLLB 1.2B distilled model provides a lower-bound estimate of the performance. However, we anticipate a smaller difference between our pivot-based IndicTrans2 and the best NLLB 54B MoE model. Based on our previous results, we expect IndicTrans2 scores to be comparable if not[^14]

Table 20: chrF++ scores of Indic-Indic evaluation on IN22-Gen test set of our IndicTrans2-Pivot (IT2-Pivot) model, IndicTrans2-M2M (IT2-M2M) model, compressed IndicTrans2-M2M (IndicTrans2-Dist-M2M) model and NLLB 1.2B distilled model. " $\mathrm{xx}-\{\mathrm{l}$ ang $\}$ " and " $\{\mathrm{lang}\}-\mathrm{xx}$ " denote the average chrF++ scores to that language and from that language, respectively. $\dagger$ indicates completely off-target translations.

| language | $x x-\{$ lang $\}$ |  |  |  | \{lang\}-xx |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | N1.2 | IT2-Pivot | IT2-M2M | IT2-Dist-M2M | N1.2 | IT2-Pivot | IT2-M2M | IT2-Dist-M2M |
| asm_Beng | 35.5 | 40.7 | 40.5 | 39.4 | 38.8 | 44.0 | 42.7 | 40.9 |
| ben_Beng | 39.9 | 45.1 | 44.8 | 43.2 | 37.4 | 43.2 | 42.3 | 41.2 |
| guj_Gujr | 39.2 | 45.4 | 44.3 | 42.9 | 39.0 | 43.8 | 43.2 | 39.9 |
| hin_Deva | 43.7 | 49.2 | 48.8 | 47.1 | 39.1 | 43.4 | 43.0 | 42.3 |
| kan_Knda | 39.4 | 44.6 | 44.5 | 43 | 38.4 | 43.9 | 43.1 | 39.8 |
| kas_Arab | 28.5 | 35.4 | 34.8 | 33.7 | 35.6 | 41.8 | 41.3 | 39.8 |
| mai_Deva | 36.6 | 42.0 | 41.9 | 40.3 | 39.1 | 44.2 | 43.7 | 42.8 |
| mal_Mlym | 38.5 | 44.9 | 43.5 | 42 | 36.4 | 42.9 | 42.2 | 40.6 |
| mar_Deva | 37.6 | 44.4 | 43.6 | 41.5 | 38.2 | 43.8 | 43.0 | 42.4 |
| npi_Deva | 37.3 | 41.4 | 41.1 | 39.6 | 39.0 | 44.8 | 44.0 | 43.1 |
| ory_Orya | 36.1 | 38.2 | 38.0 | 36.8 | 39.4 | 44.9 | 44.3 | 41.3 |
| pan_Guru | 39.0 | 43.2 | 42.2 | 40.9 | 36.8 | 41.7 | 40.5 | 39.1 |
| san_Deva | 23.3 | 35.8 | 35.8 | 34.6 | 32.8 | 39.8 | 39.0 | 37.6 |
| sat_Olck | $0.0^{\dagger}$ | 31.2 | 31.2 | 30 | $0.0^{\dagger}$ | 35.0 | 37.2 | 35.8 |
| tam_Taml | 40.1 | 45.0 | 44.1 | 42.6 | 35.4 | 41.3 | 40.2 | 39.3 |
| tel_Telu | 40.0 | 45.7 | 44.5 | 42.9 | 37.5 | 43.2 | 42.5 | 41.9 |
| urd_Arab | 47.7 | 54.6 | 53.2 | 50.8 | 39.4 | 45.2 | 44.4 | 43.6 |

Table 21: chrF++ scores of Indic-Indic evaluation on IN22-Conv test set of our IndicTrans2-Pivot (IT2-Pivot) model, IndicTrans2-M2M (IT2-M2M) model, compressed IndicTrans2-M2M (IndicTrans2-Dist-M2M) model and NLLB 1.2B distilled model. " $\mathrm{xx}-\{\mathrm{lang}\}$ " and " $\{$ lang $\}-\mathrm{xx}$ " denote the average chrF++ scores to that language and from that language, respectively. $\dagger$ indicates completely off-target translations.

| language | $\mathrm{xx}-\{\mathrm{lang}\}$ |  |  |  | \{lang\}-xx |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathrm{N} 1.2$ | IT2-Pivot | IT2-M2M | IT2-Dist-M2M | \| N1.2 | IT2-Pivot | IT2-M2M | IT2-Dist-M2M |
| asm_Beng | 33.7 | 38.6 | 38.6 | 37.7 | 35.8 | 41.1 | 40.6 | 39.9 |
| ben_Beng | 37.6 | 41.6 | 41.5 | 40.5 | 34.8 | 39.8 | 39.8 | 39.1 |
| guj_Gujr | 38.1 | 43.5 | 43.1 | 42.1 | 36.5 | 40.9 | 40.5 | 39.7 |
| hin_Deva | 39.9 | 42.5 | 42.4 | 41.7 | 36.3 | 40.5 | 40.4 | 39.9 |
| kan_Knda | 28.2 | $\mathbf{3 0 . 8}$ | 30.7 | 30.1 | 30.8 | 35.5 | 34.6 | 33.7 |
| $k a s_{\_} A r a b$ | 18.6 | 30.7 | 31.1 | 30.7 | 30.5 | 37.4 | 37.4 | 35.7 |
| mai_Deva | 32.2 | 37.9 | 38.4 | 37.8 | 34.8 | 40.0 | 39.6 | 38.9 |
| mal_Mlym | 34.9 | 39.7 | 39.0 | 38.0 | 32.9 | 37.7 | 37.3 | 36.4 |
| mar_Deva | 35.6 | 41.0 | 40.4 | 39.2 | 35.7 | 40.0 | 39.9 | 39.4 |
| npi_Deva | 35.6 | 42.2 | 42.0 | 41.2 | 36.2 | 41.1 | 40.9 | 40.1 |
| ory_Orya | 33.7 | 34.4 | 34.5 | 33.9 | 36.2 | 41.2 | 40.7 | 39.9 |
| pan_Guru | 40.9 | 45.5 | 45 | 44.0 | 35.6 | 40.3 | 39.8 | 39.2 |
| san_Deva | 22.3 | 31.8 | 32 | 31.5 | 26.8 | 34.8 | 34.4 | 33.1 |
| sat_Olck | $0.0^{\dagger}$ | 30.7 | 31.2 | 30.4 | $0.0^{\dagger}$ | 32.1 | 34.7 | 33.8 |
| tam_Taml | 33.2 | 36.2 | 35.6 | 34.9 | 30.7 | 34.3 | 34.0 | 33.3 |
| tel_Telu | 35.0 | 39.6 | 39.1 | 37.9 | 33.2 | 37.5 | 37.3 | 36.6 |
| urd_Arab | 43.7 | 49.2 | 48.8 | 47.9 | 36.5 | 41.7 | 41.4 | 40.7 |

better than the best NLLB 54B MoE model. This highlights the effectiveness of our robust English-centric models and their potential in Indic-Indic translation scenarios.

### 7.5.2 Direct Models

While the pivot-based solution demonstrates strong Indic-Indic performance, its inherent sequential dual model pipeline results in increasing the inference time by a factor of 2 compared to the English-centric model. To address this limitation, it is essential to build direct Indic-Indic (IndicTrans2-M2M) models that facilitate Indic-Indic translation with nearly the same inference cost as English-centric model. However, the scarcity of Indic-Indic data makes training such models from scratch challenging. As a result, inspired by prior works (Kim et al., 2019; Ma et al., 2020), we leverage pre-trained components from our English-centric models to initialize the IndicTrans2-M2M model. Specifically, we initialize the IndicTrans2-M2M model using the Encoder from the Indic-En model and the Decoder from the En-Indic model. It is important to note that these two pre-trained components undergo independent training and lack synchronization, resulting in a lack of zero-shot performance post-initialization. Nevertheless, these pre-trained components serve as strong initializations to start with and can be further adapted with limited data.

The BPCC-Wiki subset contains 9.2M bitext pairs spanning 462 Indic-Indic directions. This seed corpus is not completely n-way in the current form (see Section 3.3), and the data scales might be extremely low for some language pairs. As a result, we leverage data augmentation to synthetically generate $n$-way parallel corpora just by performing $n$ inferences instead of ${ }^{n} C_{2}$. Specifically, we use our IndicTrans 2 En-Indic model to generate $100 \mathrm{~K}$ synthetic bitext pairs for each translation direction by selecting 100K English monolingual sentences from IndicCorpv2 (Doddapaneni et al., 2023). This amounts to a total of $46.2 \mathrm{M}$ pairs across 462 Indic-Indic language pairs. Our fine-tuning dataset for adapting the IndicTrans2-M2M model consists of seed corpus and synthetic corpus, resulting in a total of $55.4 \mathrm{M}$ bitext pairs across 462 directions. It is important to note that our IndicTrans2-M2M model covers all 22 scheduled languages but lacks direct support for script variants like Kashmiri (Devanagari), Manipuri (Bengali), and Sindhi (Arabic) due to the unavailability of seed data for these scripts. Tables 19 to 21 shows that our IndicTrans2-M2M achieves competitive performance with a 1-point decrease in the chrF++ metric compared to the pivot-based approach at half of the inference cost. Furthermore, we also apply the same recipe to IndicTrans2-Dist (described in Section 7.6) to improve the inference latency and compress it to about $350 \mathrm{M}$ parameters while achieving competitive performance with the IndicTrans2-M2M 1.2B parameter model (see Tables 19 to 21).

### 7.6 Distilled Models

We distill our IndicTrans2 (1.1B parameters, 12Gb size) models into smaller, efficient counterparts called IndicTrans2Dist (211M parameters, $2 \mathrm{~Gb}$ size) to enhance deployment feasibility in low-infrastructure settings. Following the deep and thin architecture approach (Gumma et al., 2023), we retain the encoder-decoder layer count but reduce other fullyconnected dimensions. Acknowledging the robustness of our teacher model, we leverage a smaller, representative dataset subset of $\sim 110$ million pairs across all 22 languages for a more data-efficient distillation process. We adopt Word-Level distillation (Hinton et al., 2015; Kim \& Rush, 2016), facilitating direct student model training without a separate distilled dataset. The student model is initially distilled from IndicTrans2 and subsequently fine-tuned using the BPCC seed data. Tables 49 to 51 in Appendix D list the hyperparameters and architecture of IndicTrans2-Dist models.

In adherence to metrics used before, we report chrF++ scores of the distilled models on IN22-Gen in Table 22. The chrF++ scores on FLORES-200 and IN22-Conv are presented in Tables 52 and 53 in Appendix D respectively. In contrast to our earlier findings, we find that fine-tuning with seed data was not so beneficial for the distilled models. Our distilled models trained with Word-Level distillation perform competitively with our best IT2 models and show an average drop of 0.87 on Indic-En and 0.17 on En-Indic across all three benchmarks. It is important to note that we do not use any backtranslation data for distillation. Notably, we observe higher gains due to distillation on the IN22Conv than on the IN22-Gen and FLORES-200 in the Indic-En direction. Low-resource languages like Dogri, Bodo and Arabic script languages like Kashmiri and Urdu face a drop of more than 2.5 chrF++ points in the Indic-En direction, whereas Santali has a gain of 2.7 points in IN22-Gen and 2.8 points in IN22-Conv as compared to the Indic-En teacher model. Almost all high-resource languages like Hindi and Bengali observe a negligible reduction in performance with

Table 22: chrF++ scores of Indic-En and En-Indic distilled models on IN22-Gen. Distilled (Dist) is the model trained with Word-level KD. $\Delta$ is the difference between the distilled Model fine-tuned on seed data (Dist-Seed) \& IT2. Higher values of $\Delta$ are preferable.

| language | Indic-En |  |  |  | En-Indic |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | IT2 | Dist | Dist-Seed | $\Delta$ | \| IT2 | Dist | Dist-Seed | $\Delta$ |
| asm_Beng | 65.8 | 65.6 | 65.6 | -0.2 | \| 47.1 | 46.4 | 47.1 | 0.0 |
| ben_Beng | 63.2 | 63.1 | 63.3 | 0.1 | 51.8 | 51.5 | 51.6 | -0.2 |
| brx_Deva | 62.1 | 59.3 | 59.3 | -2.8 | 47.8 | 47.6 | 47.7 | -0.1 |
| doi_Deva | 72.6 | 70.2 | 70.2 | -2.4 | 57.8 | 56.3 | 56.8 | -1.0 |
| gom_Deva | 59.2 | 57.3 | 57.2 | -2.0 | 45.2 | 44.5 | 44.8 | -0.4 |
| guj_Gujr | 66.5 | 65.5 | 65.5 | -1.0 | 53.5 | 52.9 | 53.2 | -0.3 |
| hin_Deva | 65.4 | 63.7 | 63.8 | -1.6 | 56.7 | 56.4 | 56.7 | 0.0 |
| kan_Knda | 64.2 | 64.3 | 64.3 | 0.1 | 51.0 | 50.4 | 50.9 | -0.1 |
| kas_Arab | 60.4 | 57.6 | 57.8 | -2.6 | 40.2 | 39.0 | 39.5 | -0.7 |
| mai_Deva | 64.8 | 64.4 | 64.4 | -0.4 | 48.7 | 48.5 | 48.7 | 0.0 |
| mal_Mlym | 64.5 | 63.2 | 63.3 | -1.2 | 50.9 | 50.4 | 50.8 | -0.1 |
| mar_Deva | 63.7 | 63.2 | 63.3 | -0.4 | 51.0 | 50.4 | 50.6 | -0.4 |
| mni_Mtei | 57.9 | 58.0 | 58.0 | 0.1 | 44.6 | 43.2 | 43.6 | -1.0 |
| npi_Deva | 67.7 | 67.6 | 67.5 | -0.2 | 49.0 | 48.7 | 49.0 | 0.0 |
| ory_Orya | 66.2 | 65.8 | 65.9 | -0.3 | 43.9 | 43.5 | 43.9 | 0.0 |
| pan_Guru | 63.4 | 62.0 | 61.9 | -1.5 | 50.6 | 50.6 | 50.4 | -0.2 |
| san_Deva | 54.8 | 53.8 | 53.9 | -0.9 | 38.8 | 37.9 | 38.2 | -0.6 |
| sat_Olck | 45.3 | 47.5 | 48.0 | 2.7 | 33.4 | 33.0 | 33.8 | 0.4 |
| snd_Deva | 57.3 | 56.0 | 56.6 | -0.7 | 36.6 | 36.6 | 36.6 | 0.0 |
| tam_Taml | 59.8 | 58.4 | 58.4 | -1.4 | 49.5 | 49.3 | 49.3 | -0.2 |
| tel_Telu | 64.8 | 63.0 | 63.0 | -1.8 | 52.4 | 52.4 | 52.4 | 0.0 |
| $u r d \_A r a b$ | 73.0 | 70.8 | 70.9 | -2.1 | 68.2 | 67.8 | 67.8 | -0.4 |
| erage | 62.8 | 61.8 | 61.9 | -0.9 | 48.6 | 48.1 | 48.3 | -0.3 |

distillation. In contrast to the findings of Gumma et al. (2023), we observe that the most significant factor is a robust teacher model coupled with high-quality, diverse data to develop compact student models that are comparable to the teacher. However, extensive experiments are needed to further validate and strengthen these observations in the future.

## 8 Conclusion

In this paper, we presented our efforts on building machine translation systems supporting all 22 languages in the $8^{\text {th }}$ schedule of the Constitution of India. We created the multi-domain IN22 benchmark and the BPCC parallel corpus, both of which are first-of-their-kind evaluation and training corpora, the latter consisting of $\sim 230 \mathrm{M}$ bitext pairs, covering 22 Indic languages. We trained and evaluated robust English-centric models containing 1.1B parameters as well as their compact versions with $211 \mathrm{M}$ parameters, which can be used in compute-heavy as well as compute-scarce settings. Additionally, we repurpose pre-trained components from our English-centric models for efficient training of a direct Indic-Indic model containing 1.2B parameters as well as its compact version with $350 \mathrm{M}$ parameters. Our evaluations focus on multiple automatic metrics such as BLEU, chrF++ (primary), and COMET which show that our models are comparable, if not better, than publicly available open and commercial systems.

To summarize, our contributions comprehensively cover all three axes for translation systems, namely models, data, and benchmarks. We will open-source the data, benchmarks, and model artifacts publicly and hope that our work will serve as a foundation as well as a guide for further advancements in translation systems for Indic as well as low-resource languages.

## 9 Limitations and Future Work

Our work has several significant positive outcomes, including the release of the first open-source model that is competitive with commercial models and supports all 22 scheduled Indian languages. However, some limitations open up avenues for future research across each of the following axes: Data, Models, Benchmark, Evaluation, and Deployment.

Data. One of the foremost challenges is the scarcity of high-quality human-annotated data for mid-resource or lowresource languages, making it difficult to develop robust models on these languages. Furthermore, the limited availability of content in these languages on the web prevents the use of mining-based approaches to overcome data scarcity effectively. As a result, our IndicTrans2 models demonstrate limited generalization capabilities for languages such as Manipuri (Meitei), Santali, and Sindhi (Devnagari). Another important concern is the limited effectiveness of existing sentence embedding models when applied to Indic languages, which can lead to noisy and suboptimal pairs. To address these challenges, it is crucial to calibrate sentence embedding models using human-annotated data to improve their correlation with human annotations. Moreover, expanding the language coverage of these sentence embedding models to encompass all 22 scheduled languages will be pivotal in facilitating mining efforts for mid-resource or low-resource languages.

Modeling. Our current work serves as an initial effort to develop IndicTrans2 models supporting 22 scheduled Indic languages, including low-resource ones. Although consistently outperforming baseline systems, a performance gap exists between low-resource and high-resource languages (as shown in Section 7.1). To bridge this gap, we need to explore effective methods to leverage language relatedness for cross-lingual transfer and improve generalization in low-resource settings. Furthermore, while our IndicTrans2 models released with this work prioritize general-purpose use cases, it is equally important to investigate sparse parameter-efficient approaches for effective domain adaptation while also preserving the model's general-purpose utility. Furthermore, our current IndicTrans2 supports translations across 22 scheduled Indic languages, encompassing multiple scripts that cater to a vast majority of Indian speakers. However, numerous Indic languages remain unincorporated, and exploring techniques to extend the current models without catastrophic forgetting is an important research direction.

Benchmark. Accurate evaluation of translation models requires original test sets that encompass a wide range of linguistic phenomena and translation challenges. The current test sets that are released are $n$-way constructed with English as the original language, which is a common approach for including numerous languages. This implies that when we evaluate Indic to English translation on benchmarks like FLORES-200 or IN22, our source is translationese instead of original. Prior research has emphasized the importance of utilizing source-original test sets to get a fair evaluation of translation performance (Zhang \& Toral, 2019; Federmann et al., 2022). Moreover, the development of an Indic original benchmark would provide an additional aspect for assessing whether the subtleties of Indic language original sentences are accurately captured in English translations. Therefore, we are currently working towards creating Indic-original benchmarks to facilitate the fair evaluation of Indic-En translations. Soon, we intend to release Indicoriginal to English translation benchmarks for all 22 scheduled Indic languages.

Evaluation. Evaluation of translation models is critical for understanding their strengths and weaknesses and guiding further improvements. This evaluation typically involves two main approaches: human evaluation and automatic evaluation. Our current work includes a preliminary human evaluation study on a sample of 100 sentences from our IN22-Gen benchmark for En-Indic translations. However, future efforts should focus on conducting a broad and largescale human evaluation study that focuses on the free-form evaluation and task-oriented contexts to understand the potential biases and shortcomings of our IndicTrans2 models and assess their feasibility in practical use-case scenarios, thereby identifying areas for improvement. Additionally, developing better automatic evaluation metrics, particularly suited for Indic languages, is vital for achieving a more comprehensive and quantitative assessment of translation quality and facilitating model improvements. Current model-based metrics may not fully support certain languages, emphasizing the need to explore effective ways to calibrate them for Indic languages and improve the correlation with human judgments.

Fairness. Our IndicTrans2 models are trained on extensive data collected from the web, which may introduce social biases. To ensure broader and safer accessibility, it is crucial to thoroughly identify and address these biases. Prior works demonstrate that distilled models can further propagate or amplify biases from the teacher model (Ahn et al., 2022; Gupta et al., 2022; Dhar et al., 2021), underscoring the importance of conducting a comprehensive study and developing alignment methods to mitigate such biases.

## 10 Author Contributions

This project is a large team effort, with immense contributions from all the people involved. To list down the contributions of the authors, we document the areas and list the authors contributing significantly to each of these areas. In each area, the contributors are listed sorted by last name. The lead authors, Jay Gala, and Pranjal A. Chitale, have contributed across multiple areas and co-ordinated many activities.

Parallel Corpus Collection and Mining: Raghavan AK, Jay Gala, and Aswanth Kumar.

Human Translation: Pranjal A. Chitale, Jay Gala, Mitesh M. Khapra, Pratyush Kumar, Anoop Kunchukuttan, Janki Nawale, and Anupama Sujatha.

Model Training: Pranjal A. Chitale, Raj Dabre, Jay Gala, and Varun Gumma.

Distillation: Pranjal A. Chitale, Raj Dabre, Jay Gala, and Varun Gumma.

Model Evaluation: Pranjal A. Chitale, Raj Dabre, Sumanth Doddapaneni, Jay Gala, Varun Gumma, Anoop Kunchukuttan, and Ratish Puduppully.

Research Leads: Raj Dabre, Mitesh M. Khapra, Pratyush Kumar, and Anoop Kunchukuttan.

Project Conceptualization and Direction: Mitesh M. Khapra, Pratyush Kumar, Anoop Kunchukuttan, and Vivek Raghavan.

## Acknowledgements

Embarking on this mission was only possible due to the support of numerous organizations, individuals and members of the Indian language technology ecosystem. We would like to take a few sentences to thank all of them.

Sponsors/Donors: First and foremost, we thank the Ministry of Electronics and Information Technology (MeitY), Government of India, for setting up the ambitious Digital India Bhashini Mission with the goal of advancing Indian language technology. The human infrastructure comprising of a large team of translators, reviewers and language experts who worked on this project were supported by the generous grant given by Digital India Bhashini Mission to IIT Madras to serve as the Data Management Unit for the mission.

We are indebted to Shri Nandan Nilekani and Shrimati Rohini Nilekani for believing in us and supporting our work through generous grants from EkStep Foundation and Nilekani Philanthropies. These grants were used for (i) supporting many of the students, research associates, and developers who worked on this project, (ii) fulfilling many of our compute needs, and (iii) recruiting project managers to oversee the massive pan-India data collection activity undertaken as a part of this work.

We thank Microsoft for their grant to support the creation of benchmarks for Indian languages.

We thank the Centre for Development and Advancement of Computing, Pune (CDAC Pune) for access to its Param Siddhi super-computer which was used for mining bitext pairs at scale.

IIT Madras: We thank Prof. V Kamakoti (Director, IIT Madras), Prof. Mahesh V Panchagnula (Dean, IIT Madras), Prof. Ravindra Gettu (Dean, IIT Madras) and Prof. Manu Santhanam (Dean, IIT Madras) for their constant encour-
agement and administrative support. In particular, we are thankful for the office space provided to AI4Bharat which houses some of our students, researchers, language experts and administrative team.

Indian language technology community: We extend our heartfelt gratitude to the expansive Indian language technology community, comprising academia, startups, and the deep tech industry, both within India and across the globe. It is with immense gratitude that we acknowledge the incredible foundation laid by the giants of this community, whose pioneering work has paved the way for our endeavors. We are truly grateful for the knowledge, insights, and advancements that we have built upon, as we stand on the shoulders of these remarkable contributors. In particular, we thank Prof. Rajeev Sangal (Professor Emeritus, IIIT Hyderabad), Prof. Pushpak Bhattacharyya (IIT Bombay), Prof. Dipti Mishra (IIIT Hyderabad), Prof. Hema Murthy (IIT Madras), Prof. Umesh S (IIT Madras), Prof. Rajat Moona (IIT Gandhinagar), Prof. Ganesh Ramakrishnan (IIT Bombay), Partha Talukdar (Google Research India), Dr. Swaran Lata (MeitY), Dr. Sobha L (AU-KBC) and Dr. Ritesh Kumar (Dr. B.R. Ambedkar University) for their critical insights and constructive feedback in improving the translation guidelines used for creating the datasets released as a part of this work (we apologize if we have missed anyone).

Research organisations: We thank Google for open-sourcing the LaBSE embeddings which we used extensively for mining and filtering bitext pairs. We thank Meta for open-sourcing their semantic search infrastructure, FAISS, which we use for indexing and mining bitext pairs. We thank Allen-AI for reproducing the work of NLLB and releasing a large mined parallel corpus for Indian languages.

Language Experts: We express our deepest gratitude to our exceptional and highly dedicated team of language experts, including translators and reviewers, whose invaluable contributions have been instrumental in the creation of the seed data and benchmark data. Their unwavering commitment to adhering to guidelines and their remarkable ability to work seamlessly as a cohesive unit, despite being geographically dispersed, is truly commendable. The quality and accuracy of the manual datasets developed as part of this endeavor owes much to their unwavering efforts. We extend our heartfelt thanks to every member of our remarkable language team for their outstanding dedication and invaluable contributions.

Administration Team: We are profoundly thankful to the remarkable individuals, Krishnan Karunganni S and Ravishankar Venkateswaran, for their exceptional dedication, patience, and extraordinary leadership in managing such an expansive team of talented translators. Their unwavering commitment to orchestrating and guiding this diverse group of language experts is truly commendable. Through their exceptional organizational skills and expertise, they ensured seamless coordination and maintained the highest standards of quality throughout the translation process. We also thank our support staff Shanthi S, Bhanumathy M, Bhavana R, Suganya Kumaresan, and Kalaivanan A, who helped with recruitment and procurement.

Development Team: We also thank our development team comprising of our in-house engineers, as well as, engineers from Tarento for building Shoonya which enabled all the manual translation work. In the absence of Shoonya, it would have been impossible to manage such a diverse team spread across the country working towards a common goal. We thank members of our development team for their patience in working with the language experts and building features that helped improve both the speed and quality of translation.

Partners: We would also like to thank our start-up partners, viz., Desicrew, Devanagari, Language Services Bureau and Keypoint Technologies, who helped in meeting some of our manual translation goals.

Reviewers: We would like to thank Dr. Benjamin Marie (4i) for reviewing the modeling and evaluation sections of our paper and helping us gain confidence in the credibility of our evaluation process.

NICT: Raj Dabre would like to thank Dr. Eiichiro Sumita and Dr. Masao Utiyama of ASTREC at NICT, for the freedom and encouragement to collaborate with AI4Bharat.

Last, but not the least, we thank the Almighty for giving us the courage to embark on this mission!

## The Team Behind the Scenes

This work was possible because the efforts put in by all the remarkable individuals listed below.

## Administrative Team

- Krishnan Karunganni S (Chief of Operations and Delivery, AI4Bharat)
- Ravishankar Venkateswaran (Delivery Head, AI4Bharat)
- Shanthi S (Operations, AI4Bharat)
- Bhanumathy M (Recruitment, AI4Bharat)
- Suganya Kumaresan (Recruitment, AI4Bharat)
- Bhavana R R (ex-Recruitment, AI4Bharat)


## Shoonya Team

|  | Developers | Designation |
| :--- | :--- | :--- |
| Tarento | Aravinth Bheemaraj | Project Manager |
|  | Alpana Majhi | Frontend Lead |
|  | Ganavi Kumaraswamy | Frontend Developer |
|  | Mrigank Shekhar Shringi | Frontend Developer |
|  | Dheeraj Gujral | Backend Developer |
|  | Jagadeesh Lachchanagiri | Backend Developer |
|  | Umme Nusrath | Backend Developer |
| AI4Bharat | Ishvinder Sethi | Backend Lead and Lead Coordinator |
|  | Aparna A | Lead Coordinator and Manager |
|  | Abhigyan Raman | DevOps Lead |
|  | Gokul NC | Tech Lead |
|  | Kaushal Bhogale | Backend Developer and Architect |
|  | Chetan Gudagamanal | Frontend Developer |
| Kunal Tiwary | Backend Developer |  |
| Interns | Aditya Mitra | Full-Stack Developer |
|  | Anirudh Prabhakar | Full-Stack Developer |
|  | Atharva Naphde | Full-Stack Developer |
|  | Aviral Goel | Full-Stack Developer |
|  | Pranav Agarwal | Full-Stack Developer |
| Rugved Somwanshi | Full-Stack Developer |  |
| Aavaig Malhotra | Frontend Developer |  |
| Ayush Panwar | Frontend Developer |  |
| Rajat Maheshwari | Frontend Developer |  |
| Yogesh Bhat | Frontend Developer |  |
| Akshat Sharma | Backend Developer |  |
| Anuran Roy | Backend Developer |  |
| Debraj Bhal | Backend Developer |  |
| Nishant Nayak | Backend Developer |  |
| Prakhar Rathi | Backend Developer |  |
| Saish Mendke | Backend Developer |  |

Translation Team

| Language | Language Experts | Designation |
| :---: | :---: | :---: |
| Assamese | Devanga Pallav Saikia <br> Bikash Chandra <br> Bishnu Prasad Barman <br> Dimpi Sarma <br> Bonya Baruah <br> Bikash Chetia <br> Kangkana Deka <br> Lelina Barman | Language Lead, Senior Translator <br> Senior Project Manager <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator |
| Bengali | Sounak Dutta <br> Shambhobi Ghosh <br> Srija Mukherjee <br> Shreerupa Chattopadhyay <br> Natasha Ahmed <br> Kathakali Bhoumik Das <br> Atrayee Dutta | Language Lead, Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator |
| Bodo | Prafulla Basumatry <br> Bihung Brahma <br> Bikash Chandra <br> Sidwma Brahma <br> Sansuma Brahma <br> Jeetumoni Basumatry <br> Ria Borah Sonowal | Language Lead, Senior Translator <br> Senior Translator <br> Senior Project Manager <br> Translator <br> Translator <br> Translator <br> Translator |
| Dogri | Preeti Dubey <br> Lalit Mangotra <br> Veena Gupta <br> Shashi Pathania <br> Anju Bala <br> Monika chandel <br> Kulbhushan Jasrotia | Senior Project Manager <br> Senior Translator <br> Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator |
| Gujarati | Pranav Pandya <br> Jayesh Adhyaru <br> Naresh Kapadia <br> Faiz Masi <br> Jimal Patel | Language Lead, Translator <br> Translator <br> Senior Translator <br> Translator <br> Translator |
| Hindi | Jaya Sarawati <br> Sufiya Pathan <br> Deepika Agarwal <br> Aakansha Dubey <br> Rakshi Ghai <br> Neha Bhakal <br> Ayesha Pereira <br> Veda Bharti | Language Lead, Senior Translator <br> Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator |
| Kannada | Anagha H. N. <br> Adithi Raveendranath <br> Abhigna Joshi <br> Shivakumar R. M. <br> Arun Kumar <br> Goutham M | Language Lead, Senior Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator |


|  | T.R. Nagesh | Translator |
| :---: | :---: | :---: |
| Kashmiri | Vijay Wali <br> Shafi Shauq <br> Ambreen Farooq <br> Meer Bismah <br> Syed Samreen <br> Sumaya Jehangir <br> Nazima Mehdi <br> Ishfaq Nisar | Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Senior Project Manager <br> Translator |
| Konkani | Pradeep Padgaonkar <br> Pradnya Bhagat <br> Sandesh Prabhudesai <br> Sharat Raikar <br> Anwesha Singbal <br> Cia Fernandes <br> Ashwini Kamat | Senior Translator <br> Senior Project Manager <br> Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator |
| Maithili | Sanjay Jha <br> Avinash Kumar <br> Yogendra Pathak <br> Dr. Chandramani Jha <br> Vikas Vineet Jha <br> Priyeshi Kumari <br> Rahul Kumar Jha <br> Vijay Deo Jha <br> Manoj Kumar Pathak <br> Tulika Swati <br> Prashant Kumar Jha <br> Nandan Kumar <br> Kishore Keshav <br> Sanjeev Kumar Jha <br> Deepak Kumar <br> Juli Jha <br> Swati Jha <br> Aditya Bhushan Mishra | Language Lead, Translator <br> Senior Project Manager <br> Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator |
| Malayalam | Jebi Mariam Kurian <br> Manoj Varma <br> C. V. Sudheendran <br> Jihad M. <br> Jiza Mariam Kurian <br> Ann Mary Thomas <br> Srilekha Padmakuma Nambiar | Language Lead, Translator <br> Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator <br> Translator |
| Marathi | Kunal Gandhi <br> Paresh Prabhu <br> Vrinda Sarkar <br> Ranjana Pathak <br> Saee Kodolikar <br> Prasad Jog <br> Shweta Deshmukh <br> Bhushan Oke | Language Lead,Translator <br> Senior Translator <br> Senior Translator <br> Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator |


|  | Neha Satish Bandekar <br> Radhika Deshpande | Translator <br> Translator |
| :---: | :---: | :---: |
| Manipuri | Reena Ashem <br> Yasin Khan <br> Chingtham Diana Devi <br> Diana Thingujam <br> Jahir Hussain <br> Sanju Pukhrambam <br> Alfina Khaidem <br> Kshetrimayum Momo <br> Padmabati Achom | Language Lead, Senior Translator <br> Senior Project Manager <br> Senior Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator |
| Nepali | Sunita Dahal <br> Bikash Chandra <br> Dhaka Ram Kafle <br> Lekhnath Chhetri <br> Tika Ram Rai <br> D. Ghimiray <br> Dr Srijana Sharma <br> Dr Khagen Sharma | Language Lead, Senior Translator <br> Senior Project Manager <br> Senior Translator <br> Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator |
| Odia | Satyabrata Barik <br> Pramodini Pradhan <br> Sai Sudeep Das <br> Abhishek Parija <br> Suchishraba Sarangi <br> Bhimasena Bhol <br> Surendra Chandra Tripathy | Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Language Lead <br> Translator <br> Translator |
| Punjabi | Armin Virk <br> Pallavi Kaushal <br> Shallu Rani <br> Parneet Kaur | Language Lead, Translator <br> Translator <br> Translator <br> Translator |
| Sanskrit | Harisha H. M. <br> Dr. Suresha <br> Suprith S. <br> Sailaja Nittala <br> Vasudev Aital <br> Vivaswini <br> Dr. Narayan Dutt Mishra | Language Lead, Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Senior Translator |
| Santali | Kamala Murmu <br> Baren Kisku <br> Prasanta Kumar Hansda <br> Baburam Murmu <br> Sripati Tudu <br> Urmila Murmu <br> Raju Mardi <br> Churki Hansda <br> Promila Hansda <br> Sova Tudu <br> Sanjiban Murmu <br> Satya Hembram | Senior Project Manager <br> Senior Translator <br> Senior Translator <br> Senior Translator <br> Senior Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator <br> Translator |


|  | Guna Hembram <br> Sagen Murmu | Translator <br> Translator |
| :---: | :---: | :---: |
| Sindhi | Armin Virk | Language Lead, Translator |
|  | Dr. Nalini | Senior Translator |
|  | Prakash Tejwani | Translator |
|  | Bharati Chainani | Translator |
|  | Karan Vanni | Translator |
| Tamil | Shakir Azeem | Language Lead, Senior Translator |
|  | Leema Rajavarman | Senior Translator |
|  | Shivapriya Murali | Translator |
|  | Sharmila Grahadurai | Translator |
|  | V Sayeelakshmi Rajaganapathy | Translator |
| Telugu | Shakir Azeem | Language Lead, Senior Translator |
|  | Karuna Vempati | Senior Translator |
|  | N. Sujatha | Senior Translator |
|  | Srimoukthika | Translator |
|  | Srilakshmi B. | Translator |
| Urdu | Dr. Irfan Ahmed | Senior Translator |
|  | Nazima Mehdi | Senior Project Manager |
|  | Aishwarya Diwakar | Translator |
|  | Anwar Wajhiuddin | Translator |
|  | Muhammad Anzar | Translator |
|  | Hasan Akram | Translator |
|  | Dr. Javaid Aziz Bhat | Translator |
|  | Hafsah Faquih | Translator |
|  | Habeebunnisa | Translator |
|  | Mohammad Afaan | Translator |
|  | Naziya Rasool | Translator |

## References

Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pp. 497-511, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/S16-1081. URL https:// aclanthology.org/S16-1081.

Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 3874-3884. Association for Computational Linguistics, 2019. doi: $10.18653 / v 1 / n 19-1388$. URL https://doi.org/10.18653/v1/n19-1388.

Jaimeen Ahn, Hwaran Lee, Jinhwa Kim, and Alice Oh. Why knowledge distillation amplifies gender bias and how to mitigate from the perspective of DistilBERT. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pp. 266-272, Seattle, Washington, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.gebnlp-1.27. URL https://aclanthology.org/2022.gebnlp-1.27.

Maruan Al-Shedivat and Ankur Parikh. Consistency by agreement in zero-shot neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1184-1197, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1121. URL https://aclanthology.org/N191121 .

Ikram ALi. Urduhack library. https://github.com/urduhack/urduhack, 2019.

Ananthakrishnan, Pushpak Bhattacharyya, M. Sasikumar, and Ritesh M. Shah. Some issues in automatic evaluation of english-hindi mt : More blues for bleu. 2006.

Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Roee Aharoni, Melvin Johnson, and Wolfgang Macherey. The missing ingredient in zero-shot neural machine translation. CoRR, abs/1903.07091, 2019. URL http://arxiv.org/abs/ 1903.07091.

Mikel Artetxe and Holger Schwenk. Margin-based parallel corpus mining with multilingual sentence embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3197-3203, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1309. URL https: //aclanthology.org/P19-1309.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/ $\mathrm{abs} / 1409.0473$.

Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. ParaCrawl: Webscale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4555-4567, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v 1/2020. acl-main.417. URL https://aclanthology.org/2020.acl-main. 417.

Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. Building machine translation systems for the next thousand languages. CoRR, abs/2205.03983, 2022. doi: 10.48550/arXiv.2205.03983. URL https://doi.org/10.48550/arXiv. 2205.03983.

Loїc Barrault, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pp. 1-61, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5301. URL https://aclanthology.org/ W19-5301.

Loïc Barrault, Magdalena Biesialska, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubešić, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, pp. 1-55, Online, November 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.wmt-1.1.

Nicola Bertoldi, Madalina Barbaiani, Marcello Federico, and Roldano Cattoni. Phrase-based statistical machine translation with pivot languages. In Proceedings of the 5th International Workshop on Spoken Language Translation: Papers, pp. 143-149, Waikiki, Hawaii, October 20-21 2008. URL https://aclanthology.org/2008.iwsltpapers. 1.

Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleš Tamchyna. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pp. 12-58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-3302. URL https://aclanthology.org/W14-3302.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/ 2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.

Isaac Caswell, Ciprian Chelba, and David Grangier. Tagged back-translation. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pp. 53-63, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5206. URL https://aclanthology.org/W19-5206.

Yun Chen, Yang Liu, Yong Cheng, and Victor O.K. Li. A teacher-student framework for zero-resource neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1925-1935, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1176. URL https://aclanthology.org/P17-1176.

Narayan Choudhary and Girish Nath Jha. Creating multilingual parallel corpora in indian languages. In Zygmunt Vetulani and Joseph Mariani (eds.), Human Language Technology Challenges for Computer Science and Linguistics - 5th Language and Technology Conference, LTC 2011, Poznań, Poland, November 25-27, 2011, Revised Selected Papers, volume 8387 of Lecture Notes in Computer Science, pp. 527-537. Springer, 2011. doi: 10.1007/978-3-31908958-4\43. URL https://doi.org/10.1007/978-3-319-08958-4_43.

Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, PaulAmbroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh

Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-jussà, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzmán, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, and Skyler Wang. Seamlessm4t-massively multilingual \& multimodal machine translation. arXiv preprint arXiv: 2308.11596, 2023.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8440-8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https://aclanthology.org/2020.acl-main.747.

Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling humancentered machine translation, 2022.

Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan. A survey of multilingual neural machine translation. ACM Comput. Surv., 53(5):99:1-99:38, 2021. doi: 10.1145/3406095. URL https://doi.org/10.1145/3406095.

Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan, Ratish Puduppully, Mitesh Khapra, and Pratyush Kumar. Indicbart: A pre-trained model for indic natural language generation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 2227, 2022, pp. 1849-1863. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-acl.145. URL https://doi.org/10.18653/v1/2022.findings-acl.145.

Peter T Daniels and William Bright. The world's writing systems. Oxford University Press on Demand, 1996.

Prithviraj Dhar, Joshua Gleason, Aniket Basu Roy, Carlos Domingo Castillo, P. Jonathon Phillips, and Ramalingam Chellappa. Distill and de-bias: Mitigating bias in face recognition using knowledge distillation. ArXiv, abs/2112.09786, 2021. URL https://api.semanticscholar.org/CorpusID:245334459.

Sumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, and Pratyush Kumar. Towards leaving no Indic language behind: Building monolingual corpora, benchmark and models for Indic languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12402-12426, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.693. URL https://aclanthology.org/2023.acl-long. 693.

Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis Chiruzzo, Angela Fan, John Ortega, Ricardo Ramos, Annette Rios, Ivan Vladimir Meza Ruiz, Gustavo Giménez-Lugo, Elisabeth Mager, Graham Neubig, Alexis Palmer, Rolando Coto-Solano, Thang Vu, and Katharina Kann. AmericasNLI: Evaluating zero-shot natural language understanding of pretrained multilingual models in truly low-resource languages. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6279-6299, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.435. URL https://aclanthology.org/2022.acl-long. 435.

Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 489-500, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1045. URL https: //aclanthology.org/D18-1045.

Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzmán, and Philipp Koehn. CCAligned: A massive collection of cross-lingual web-document pairs. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5960-5969, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.480. URL https://aclanthology.org/2020.emnlp-main. 480 .

Murray B. Emeneau. India as a lingustic area. Language, 32:3, 1956.

Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. Beyond english-centric multilingual machine translation, 2020.

Christian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 - news test references for MT evaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pp. 21-24, Online, November 2022. Association for Computational Linguistics. URL https://aclanthology .org/2022 . sumeval-1.4.

Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-agnostic BERT sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 878-891, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.acl-long.62. URL https://aclanthology.org/2022.acl-long. 62.

Orhan Firat, Baskaran Sankaran, Yaser Al-onaizan, Fatos T. Yarman Vural, and Kyunghyun Cho. Zero-resource translation with multi-lingual neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 268-277, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1026. URL https://aclanthology.org/D16-1026.

Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem Natarajan. Massive: A 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages. arXiv preprint arXiv: Arxiv-2204.08582, 2022.

Markus Freitag and Orhan Firat. Complete multilingual neural machine translation. In Proceedings of the Fifth Conference on Machine Translation, pp. 550-560, Online, November 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.wmt-1.66.

Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pp. 733-774, Online, November 2021. Association for Computational Linguistics. URL https://aclanthology .org/2021.wmt-1.73.

Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 46-68, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.2.

Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III au2, and Kate Crawford. Datasheets for datasets, 2021.

A. Gispert and José B. Mariño. Catalan-english statistical machine translation without parallel corpus : Bridging through spanish. Proceedings of The Language Resources and Evaluation Conference (LREC), 2006.

Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzmán, and Angela Fan. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522-538, 2022. doi: 10.1162/tacl_a_00474. URL https://aclanthology.org/2022.tacl-1. 30.

Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. Continuous measurement scales in human evaluation of machine translation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pp. 33-41, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://aclanthology.org/W13-2305.

Varun Gumma, Raj Dabre, and Pratyush Kumar. An empirical study of leveraging knowledge distillation for compressing multilingual neural machine translation models. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pp. 103-114, Tampere, Finland, June 2023. European Association for Machine Translation. URL https://aclanthology.org/2023. eamt-1.11.

Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun, Satyapriya Krishna, Rahul Gupta, Kai-Wei Chang, Greg Ver Steeg, and Aram Galstyan. Mitigating gender bias in distilled language models via counterfactual role reversal. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 658-678, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.55. URL https://aclanthology.org/2022.findings-acl. 55 .

Barry Haddow and Faheem Kirefu. PMIndia - A Collection of Parallel Corpora of Languages of India. arXiv e-prints, art. arXiv:2001.09907, Jan 2020.

Kevin Heffernan, Onur Çelebi, and Holger Schwenk. Bitext mining using distilled sentence representations for lowresource languages. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 2101-2112, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.findings-emnlp.154. URL https://aclanthology.org/2022.findings-emnlp. 154.

Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. ArXiv, abs/1606.08415, 2016.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.

Vu Cong Duy Hoang, Philipp Koehn, Gholamreza Haffari, and Trevor Cohn. Iterative back-translation for neural machine translation. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pp. 1824, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-2703. URL https://aclanthology.org/W18-2703.

Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization. CoRR, abs/2003.11080, 2020. URL https://arxiv.org/abs/2003.11080.

Girish Nath Jha. The TDIL program and the Indian langauge corpora intitiative (ILCI). In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10), Valletta, Malta, May 2010. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2010/pdf/ 874_Paper.pdf.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google's multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339-351, 2017. doi: 10.1162/tacl_a_00065. URL https://aclanthology.org/Q17-1024.

Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6282-6293, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.560. URL https://aclanthology.org/2020.acl-main. 560.

Herve Jégou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(1):117-128, 2011. doi: 10.1109/TPAMI.2010.57.

Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. Khapra, and Pratyush Kumar. IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4948-4961, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findingsemnlp.445. URL https://aclanthology.org/2020.findings-emnlp. 445.

Huda Khayrallah and Philipp Koehn. On the impact of various types of noise on neural machine translation. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pp. 74-83, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-2709. URL https://aclanthology . org/W18-2709.

Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1317-1327, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1139. URL https://aclanthology .org/D16-1139.

Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andrés Felipe Cruz-Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efficient moe training for multitask multilingual models. CoRR, abs/2109.10465, 2021. URL https://arxiv.org/abs/2109. 10465.

Yunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, and Hermann Ney. Pivot-based transfer learning for neural machine translation between non-English languages. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 866-876, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1080. URL https://aclanthology.org/D19-1080.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference On Learning Representations, 2014.

Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Proceedings of the Sixth Conference on Machine Translation, pp. 478-494, Online, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.wmt-1.57.

Philipp Koehn. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 388-395, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-3250.

Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers, pp. 79-86, Phuket, Thailand, September 13-15 2005. URL https://aclanthology.org/ 2005.mtsummit-papers. 11 .

Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoît Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Javier Ortiz Suárez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias Müller, André Müller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine Çabuk Balli, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. Quality at a glance: An audit of web-crawled multilingual datasets. Trans. Assoc. Comput. Linguistics, 10:50-72, 2022. doi: 10.1162/tacl\a a _00447. URL https://doi.org/10.1162/tacl_a_00447.

Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language

Processing: System Demonstrations, pp. 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012.

Sneha Kudugunta, Ankur Bapna, Isaac Caswell, and Orhan Firat. Investigating multilingual NMT representations at scale. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1565-1575, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1167. URL https: //aclanthology.org/D19-1167.

Anoop Kunchukuttan. The IndicNLP Library. https://github.com/anoopkunchukuttan/indic_nlp_ library/blob/master/docs/indicnlp.pdf, 2020.

Anoop Kunchukuttan and Pushpak Bhattacharyya. Utilizing language relatedness to improve machine translation: A case study on languages of the indian subcontinent, 2020.

Anoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya. The IIT Bombay English-Hindi parallel corpus. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL https://aclanthology . org/L18-1548.

Gurpreet Singh Lehal and Tejinder Singh Saini. Sangam: A Perso-Arabic to Indic script machine transliteration model. In Proceedings of the 11th International Conference on Natural Language Processing, pp. 232-239, Goa, India, December 2014. NLP Association of India. URL https://aclanthology .org/W14-5135.

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb.

Daniel Licht, Cynthia Gao, Janice Lam, Francisco Guzman, Mona Diab, and Philipp Koehn. Consistent human evaluation of machine translation across language pairs. In Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track), pp. 309-321, Orlando, USA, September 2022. Association for Machine Translation in the Americas. URL https://aclanthology.org/2022.amtaresearch. 24 .

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726-742, 2020a. doi: 10.1162/tacl_a_00343. URL https://aclanthology .org/ 2020.tacl-1.47.

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Trans. Assoc. Comput. Linguistics, 8: 726-742, 2020b. doi: 10.1162/tacl\_a\_00343. URL https://doi.org/10.1162/tacl_a_00343.

Shuming Ma, Jian Yang, Haoyang Huang, Zewen Chi, Li Dong, Dongdong Zhang, Hany Hassan Awadalla, Alexandre Muzio, Akiko Eriguchi, Saksham Singhal, Xia Song, Arul Menezes, and Furu Wei. Xlm-t: Scaling up multilingual machine translation with pretrained cross-lingual transformer encoders, 2020.

Anand Kumar Madasamy, Asha Hegde, Shubhanker Banerjee, Bharathi Raja Chakravarthi, Ruba Priyadharshini, Hosahalli Shashirekha, and John McCrae. Overview of the shared task on machine translation in Dravidian languages. In Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages, pp. 271-278, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.dravidianlangtech1.41. URL https://aclanthology.org/2022.dravidianlangtech-1.41.

Yash Madhani, Sushane Parthan, Priyanka Bedekar, Gokul Nc, Ruchi Khapra, Anoop Kunchukuttan, Pratyush Kumar, and Mitesh Khapra. Aksharantar: Open Indic-language transliteration datasets and models for the next billion users.

In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 40-57, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.findings-emnlp.4. URL https://aclanthology.org/2023.findings-emnlp.4.

Jean Maillard, Cynthia Gao, Elahe Kalbassi, Kaushik Ram Sadagopan, Vedanuj Goswami, Philipp Koehn, Angela Fan, and Francisco Guzman. Small data, big impact: Leveraging minimal data for effective machine translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2740-2756, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acllong.154. URL https://aclanthology.org/2023.acl-long. 154.

Vukosi Marivate, Tshephisho Sefara, Vongani Chabalala, Keamogetswe Makhaya, Tumisho Mokgonyane, Rethabile Mokoena, and Abiodun Modupe. Investigating an approach for low resource language dataset creation, curation and classification: Setswana and sepedi. In Proceedings of the first workshop on Resources for African Indigenous Languages, pp. 15-20, Marseille, France, May 2020. European Language Resources Association (ELRA). ISBN 979-10-95546-60-3. URL https://aclanthology.org/2020.rail-1.3.

Kaushal Kumar Maurya, Rahul Kejriwal, Maunendra Sankar Desarkar, and Anoop Kunchukuttan. Utilizing lexical similarity to enable zero-shot machine translation for extremely low-resource languages, 2023.

Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* '19, pp. 220-229, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287596. URL https://doi.org/10 . $1145 / 3287560.3287596$.

Nikita Moghe, Tom Sherborne, Mark Steedman, and Alexandra Birch. Extrinsic evaluation of machine translation metrics. CoRR, abs/2212.10297, 2022. doi: 10.48550/arXiv.2212.10297. URL https://doi.org/10.48550/ arXiv. 2212.10297 .

Tasnim Mohiuddin, Philipp Koehn, Vishrav Chaudhary, James Cross, Shruti Bhosale, and Shafiq Joty. Data selection curriculum for neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 1569-1582, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.113. URL https://aclanthology.org/2022.findingsemnlp. 113 .

Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In International Conference on Machine Learning, 2010.

Toshiaki Nakazawa, Katsuhito Sudoh, Shohei Higashiyama, Chenchen Ding, Raj Dabre, Hideya Mino, Isao Goto, Win Pa Pa, Anoop Kunchukuttan, and Sadao Kurohashi. Overview of the 5th workshop on Asian translation. In Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation, Hong Kong, 1-3 December 2018. Association for Computational Linguistics. URL https://aclanthology.org/Y18-3001.

Toshiaki Nakazawa, Hideki Nakayama, Chenchen Ding, Raj Dabre, Shohei Higashiyama, Hideya Mino, Isao Goto, Win Pa Pa, Anoop Kunchukuttan, Shantipriya Parida, Ondřej Bojar, and Sadao Kurohashi. Overview of the 7th workshop on Asian translation. In Proceedings of the 7th Workshop on Asian Translation, pp. 1-44, Suzhou, China, December 2020. Association for Computational Linguistics. URL https : //aclanthology . org/2020 . wat-1.1.

Toshiaki Nakazawa, Hideki Nakayama, Chenchen Ding, Raj Dabre, Shohei Higashiyama, Hideya Mino, Isao Goto, Win Pa Pa, Anoop Kunchukuttan, Shantipriya Parida, Ondřej Bojar, Chenhui Chu, Akiko Eriguchi, Kaori Abe, Yusuke Oda, and Sadao Kurohashi. Overview of the 8th workshop on Asian translation. In Proceedings of the 8th Workshop on Asian Translation (WAT2021), pp. 1-45, Online, August 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.wat-1.1. URL https://aclanthology.org/2021.wat-1.1.

Toshiaki Nakazawa, Hideki Nakayama, Isao Goto, Hideya Mino, Chenchen Ding, Raj Dabre, Anoop Kunchukuttan, Shohei Higashiyama, Hiroshi Manabe, Win Pa Pa, Shantipriya Parida, Ondřej Bojar, Chenhui Chu, Akiko Eriguchi, Kaori Abe, Yusuke Oda, Katsuhito Sudoh, Sadao Kurohashi, and Pushpak Bhattacharyya (eds.). Proceedings of the 8th Workshop on Asian Translation (WAT2021), Online, August 2021b. Association for Computational Linguistics. URL https://aclanthology.org/2021.wat-1.0.

Toshiaki Nakazawa, Hideya Mino, Isao Goto, Raj Dabre, Shohei Higashiyama, Shantipriya Parida, Anoop Kunchukuttan, Makoto Morishita, Ondřej Bojar, Chenhui Chu, Akiko Eriguchi, Kaori Abe, Yusuke Oda, and Sadao Kurohashi. Overview of the 9th workshop on Asian translation. In Proceedings of the 9th Workshop on Asian Translation, pp. 1-36, Gyeongju, Republic of Korea, October 2022. International Conference on Computational Linguistics. URL https://aclanthology.org/2022.wat-1.1.

OpenAI. Gpt-4 technical report. ARXIV.ORG, 2023. doi: 10.48550/arXiv.2303.08774.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https: //aclanthology.org/N19-4009.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083. 1073135. URL https://aclanthology.org/P02-1040.

Jerin Philip, Shashank Siripragada, Vinay P. Namboodiri, and C. V. Jawahar. Revisiting low resource status of indian languages in machine translation. In Jayant R. Haritsa, Shourya Roy, Manish Gupta, Sharad Mehrotra, Balaji Vasan Srinivasan, and Yogesh Simmhan (eds.), CODS-COMAD 2021: 8th ACM IKDD CODS and 26th COMAD, Virtual Event, Bangalore, India, January 2-4, 2021, pp. 178-187. ACM, 2021. doi: 10.1145/3430984.3431026. URL https://doi.org/10.1145/3430984.3431026.

Maja Popović. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 392-395, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology.org/W15-3049.

Maja Popović. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, pp. 612-618, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4770. URL https://aclanthology.org/W17-4770.

Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186-191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology.org/W18-6319.

Ratish Puduppully and Mirella Lapata. Data-to-text generation with macro planning. Transactions of the Association for Computational Linguistics, 9:510-527, 2021. doi: 10.1162/tacl_a_00381. URL https ://aclanthology .org/ 2021.tacl-1.31.

Ratish Puduppully, Yao Fu, and Mirella Lapata. Data-to-text generation with variational sequential planning. Transactions of the Association for Computational Linguistics, 10:697-715, 2022. doi: 10.1162/tacl_a_00484. URL https://aclanthology.org/2022.tacl-1.40.

Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data cards: Purposeful and transparent dataset documentation for responsible ai, 2022.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 2383-2392. The Association for Computational Linguistics, 2016. doi: 10.18653/v1/d16-1264. URL https://doi.org/10.18653/v1/d16-1264.

Loganathan Ramasamy, Ondřej Bojar, and Zdeněk Žabokrtský. Morphological processing for English-Tamil statistical machine translation. In Proceedings of the Workshop on Machine Translation and Parsing in Indian Languages, pp. 113-122, Mumbai, India, December 2012. The COLING 2012 Organizing Committee. URL https://aclanthology.org/W12-5611.

Gowtham Ramesh, Sumanth Doddapaneni, Aravinth Bheemaraj, Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Mahalakshmi J, Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Srihari Nagaraj, Kumar Deepak, Vivek Raghavan, Anoop Kunchukuttan, Pratyush Kumar, and Mitesh Shantadevi Khapra. Samanantar: The largest publicly available parallel corpora collection for 11 Indic languages. Transactions of the Association for Computational Linguistics, 10:145-162, 2022. doi: 10.1162/tacl_a_00452. URL https: //aclanthology.org/2022.tacl-1.9.

Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 26852702, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.213. URL https://aclanthology.org/2020.emnlp-main. 213.

Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 578-585, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology . org/2022.wmt-1.52.

Philip Resnik and Noah A. Smith. The web as a parallel corpus. Comput. Linguistics, 29(3):349-380, 2003. doi: 10.1162/089120103322711578. URL https://doi.org/10.1162/089120103322711578.

Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied, Luong Chi Mai, Vu Tat Thang, Nguyen Phuong Thai, Vichet Chea, Rapid Sun, Sethserey Sam, Sopheap Seng, Khin Mar Soe, Khin Thandar Nwet, Masao Utiyama, and Chenchen Ding. Introduction of the asian language treebank. In 2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA), pp. 1-6, 2016. doi: 10.1109/ICSDA.2016.7918974.

Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzmán. Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty (eds.), Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pp. 1351-1361. Association for Computational Linguistics, 2021a. doi: 10.18653/v1/2021.eacl-main.115. URL https://doi.org/10.18653/v1/2021.eacl-main. 115 .

Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6490-6500, Online, August 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.507. URL https://aclanthology .org/2021.acl-long. 507.

Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https: //aclanthology.org/2020.acl-main. 704.

Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 86-96, Berlin, Germany, August 2016a. Association for Computational Linguistics. doi: 10.18653/v1/ P16-1009. URL https://aclanthology.org/P16-1009.

Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715-1725, Berlin, Germany, August 2016b. Association for Computational Linguistics. doi: 10.18653/v1/P161162. URL https://aclanthology.org/P16-1162.

Aditya Siddhant, Ankur Bapna, Orhan Firat, Yuan Cao, Mia Xu Chen, Isaac Caswell, and Xavier Garcia. Towards the next 1000 languages in multilingual machine translation: Exploring the synergy between supervised and selfsupervised learning. arXiv preprint arXiv: 2201.03110, 2022.

Shashank Siripragada, Jerin Philip, Vinay P. Namboodiri, and C V Jawahar. A multilingual parallel corpora collection effort for Indian languages. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 3743-3751, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.462.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15(1):1929-1958, jan 2014. ISSN 1532-4435.

Karumuri Venkata Subbarao. South asian languages : a syntactic typology. 2012.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818-2826, 2016. doi: 10.1109/CVPR.2016.308.

Xu Tan, Jiale Chen, Di He, Yingce Xia, Tao Qin, and Tie-Yan Liu. Multilingual neural machine translation with language clustering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 963-973, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1089. URL https://aclanthology.org/D19-1089.

Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. Multilingual translation from denoising pre-training. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 3450-3466, Online, August 2021. Association for Computational Linguistics. doi: 10. 18653/v1/2021.findings-acl.304. URL https://aclanthology.org/2021.findings-acl. 304.

Brian Thompson and Matt Post. Paraphrase generation as zero-shot multilingual translation: Disentangling semantic similarity from lexical and syntactic diversity. In Proceedings of the Fifth Conference on Machine Translation, pp. 561-570, Online, November 2020. Association for Computational Linguistics. URL https://aclanthology . org/2020.wmt-1.67.

Jörg Tiedemann. Parallel data, tools and interfaces in OPUS. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 2325, 2012, pp. 2214-2218. European Language Resources Association (ELRA), 2012. URL http://www.lrecconf.org/proceedings/lrec2012/summaries/463.html.

Masao Utiyama and Hitoshi Isahara. A comparison of pivot methods for phrase-based statistical machine translation. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pp. 484-491, Rochester, New York, April 2007. Association for Computational Linguistics. URL https://aclanthology .org/N07-1061.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998-6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multitask benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupala, and Afra Alishahi (eds.), Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018, pp. 353-355. Association for Computational Linguistics, 2018. doi: 10.18653/v1/w18-5446. URL https://doi.org/10.18653/v1/w18-5446.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 3261-3275, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 4496bf24afe7fab6f046bf4923da8de6-Abstract.html.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 4003-4012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec1.494 .

Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org, 2020.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 483-498, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL https://aclanthology.org/2021.naacl-main. 41.

Mike Zhang and Antonio Toral. The effect of translationese in machine translation test sets. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pp. 73-81, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5208. URL https://aclanthology.org/ W19-5208.

Michal Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. The united nations parallel corpus v1.0. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asunción Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016, Portorož, Slovenia, May 23-28, 2016. European Language Resources Association (ELRA), 2016. URL http ://www. lrec-conf .org/proceedings/lrec2016/ summaries/1195.html.
