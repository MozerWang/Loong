# Offline Regularised Reinforcement Learning for Large Language Models Alignment 

Pierre Harvey Richemond ${ }^{*} \quad$ Yunhao Tang* Daniel Guo Daniele Calandriello<br>Google DeepMind Google DeepMind Google DeepMind Google DeepMind<br>Mohammad Gheshlaghi Azar<br>Cohere<br>Rafael Rafailov Stanford University<br>Bernardo Avila Pires<br>Google DeepMind<br>Eugene Tarassov Google DeepMind<br>Jonathan Mallinson<br>Google Research<br>Lucas Spangher<br>Google DeepMind<br>Lior Shani<br>Google Research<br>Will Ellsworth<br>Google DeepMind<br>Gil Shamir<br>Google DeepMind<br>Aliaksei Severyn<br>Google Research<br>Rishabh Joshi<br>Google DeepMind<br>Tianqi Liu<br>Google DeepMind<br>Remi Munos*<br>Google DeepMind<br>Bilal Piot ${ }^{*}$<br>Google DeepMind


#### Abstract

The dominant framework for alignment of large language models (LLM), whether through reinforcement learning from human feedback or direct preference optimisation, is to learn from preference data. This involves building datasets where each element is a quadruplet composed of a prompt, two independent responses (completions of the prompt) and a human preference between the two independent responses, yielding a preferred and a dis-preferred response. Such data is typically scarce and expensive to collect. On the other hand, single-trajectory datasets where each element is a triplet composed of a prompt, a response and a human feedback is naturally more abundant. The canonical element of such datasets is for instance an LLM's response to a user's prompt followed by a user's feedback such as a thumbs-up/down. Consequently, in this work, we propose DRO, or Direct Reward Optimisation, as a framework and associated algorithms that do not require pairwise preferences. DRO uses a simple mean-squared objective that can be implemented in various ways. We validate our findings empirically, using T5 encoder-decoder language models, and show DRO's performance over selected baselines such as Kahneman-Tversky Optimization (KTO). Thus, we confirm that DRO is a simple and empirically compelling method for single-trajectory policy optimisation.


## 1 Introduction

Aligning the behavior of artificial agents with human preferences is critical for improving quality, helpfulness and safety [Bai et al., 2022a] of agents' responses. The most established methodology for human alignment is Reinforcement Learning from Human Feedback (RLHF) [Knox and Stone, 2008, Griffith et al., 2013, Christiano et al., 2017, Warnell et al., 2018] which consists of fine-tuning[^0]pre-trained Large Language Models (LLMs) [Glaese et al., 2022, OpenAI, 2022]. More precisely, it typically entails learning a reward model under the Bradley-Terry model [Bradley and Terry, 1952] of human preferences and subsequently employing reinforcement learning (RL) to optimise the LLM's performance as judged by this reward model [Christiano et al., 2017, Ziegler et al., 2020]. This optimisation is done by generating a set of responses and their associated rewards from a set of chosen prompts. Therefore, on top of learning an additional reward model, this method requires sampling from the LLM at training time, which is costly and technically challenging.

Rafailov et al. [2023] introduced an alternative, reward-free and sampling-free method termed direct preference optimisation (DPO). This popular approach uses a supervised objective that contrasts pairs of responses to a specific prompt. DPO is able to circumvent the explicit learning of a reward signal, while remaining mathematically equivalent to the traditional RL approach, as proven by Azar et al. [2023]. Such offline preference optimisation method has been extended to a few variants in follow-up work (see, e.g., [Zhao et al., 2023a, Tang et al., 2024]) and gained popularity in practice.

Despite DPO's widespread use, there is still an important remaining shortcoming to this approach which is the high-cost of collecting human preferences. Establishing human preferences might over time become a self-defeating endeavour: as LLMs improve in quality, the task of distinguishing between a pair of strong responses gets increasingly difficult (see, e.g., arguments in [Saunders et al., 2022, Bowman et al., 2022]), and would require additional efforts in improving both the quality and scale of the collected human feedback.

Second and more importantly, annotating pairwise data is more expensive and less natural than simply indicating whether a single completion is satisfactory or not, e.g., by assigning a binary thumbs up or down rating to the model completion. The former is generally carried out by paid raters whereas the latter could be produced by users at a much larger scale. Consequently, singletrajectory data is much more abundant in the wild, hence, cheaper and more easily collected than scarce preference data. Leveraging single trajectory data promises to unlock the benefits associated with scale in deep learning. Analogous algorithmic advances motivated by the need to decrease supervision signals have often resulted in important step-changes in the empirical capacities of deep networks, e.g. through self-supervised systems [Devlin et al., 2019, Chen et al., 2020, Grill et al., 2020, Radford et al., 2021].

In order to exploit this single trajectory setting, we introduce Direct Reward Optimisation (DRO). DRO is a framework, derived from mathematical first principles. DRO is designed to work in the offline single-trajectory setting with human feedback. Specifically, our contributions are as follows:

- We introduce DRO as a generic framework performing single-trajectory RLHF optimisation thanks to a simple quadratic objective and perform theoretical analysis.
- We propose a practical instantiation of DRO, DRO-V which combines offline policy learning with a value function learning, and hence the suffix $-\mathrm{V}$.
- We compare DRO-V against Kahneman-Tversky Optimization (KTO) [Ethayarajh et al., 2024], an algorithm that has also been specifically designed for the single-trajectory setting. We find that DRO-V significantly outperforms KTO, when using T5 encoders [Raffel et al., 2020] with up to 3 billion parameters, on the UltraFeedback dataset [Cui et al., 2023]. We perform several ablations to investigate and understand our algorithm's empirical performance.


## 2 Background

Here we introduce background for RLHF and a few important alignment algorithms.

Standard pairwise alignment. Offline-alignment of LLMs has mainly been achieved using preference datasets of the form $\left(x_{i}, y_{i}^{w}, y_{i}^{l}\right)_{i=1}^{N}$, where we are given a prompt $x_{i}$, and a pair of prompt completions (or generations) $\left(y_{i}^{w}, y_{i}^{l}\right)$ with $y_{i}^{w}$ the preferred generation and $y_{i}^{l}$ the dis-preferred one. Most objectives in the RLHF literature, such as DPO [Rafailov et al., 2023], IPO [Azar et al., 2023] or SLiC [Zhao et al., 2023a], can be described and subsumed by the following loss:

$$
\mathcal{L}(\theta)=\frac{1}{n} \sum_{i=1}^{n} f\left(\beta \cdot\left(\log \left(\frac{\pi_{\theta}\left(y_{i}^{w} \mid x_{i}\right)}{\pi_{\mathrm{ref}}\left(y_{i}^{w} \mid x_{i}\right)}\right)-\log \left(\frac{\pi_{\theta}\left(y_{i}^{l} \mid x_{i}\right)}{\pi_{\mathrm{ref}}\left(y_{i}^{l} \mid x_{i}\right)}\right)\right)\right)
$$

where $\beta$ is a scalar, $n$ a batch size, $\pi_{\theta}$ is the parameterised policy and $\pi_{\text {ref }}$ a reference policy, typically obtained after a first step of pre-training and supervised fine-tuning. $f$ is a scalar function; each choice of function $f$ results in a specific given algorithm: for example, $f(z)=\log (1+\exp (-z))$ for DPO; $f(z)=\max (0,1-z)$ for SLiC; $f(z)=(z-1)^{2}$ for IPO, among other possible alternatives as discussed in [Tang et al., 2024].

We argue that preference datasets are expensive to build and do not occur naturally in the wild. Most data coming from user logs is not collected pairwise, but instead comes in the form of a single trajectory.

Single-trajectory setting. Formally, we consider single-trajectory datasets of the form $\left(x_{i}, y_{i}, r_{i}\right)_{i=1}^{N}$ where $x_{i}$ is a prompt, $y_{i}$ a generation and $r_{i}$ a scalar reward, collected by some unknown behavior policy. As a simple example to model the thumbs-up vs. thumbs-down response, we can set a binary reward where $r_{i}=1$ is for thumbs-up. This formulation can be understood as a special case of the offline RL setup [Levine et al., 2020] tailored to the contextual bandit case for RLHF.

Perhaps surprisingly, few offline alignment methods consider this setting with the exception of Kahneman-Tversky Optimization (KTO) [Ethayarajh et al., 2024]. KTO is derived from principles related to utility and prospect theory [Kahneman and Tversky, 1979] that build upon the notion of human risk aversion. KTO also makes strong simplifying assumptions, which as we will show, biases the method to produce suboptimal policies. In contrast, we are interested in deriving a simple, general purpose and performant algorithm, without strong dependency on mathematical assumptions on risk preference or utility.

Online vs. offline algorithms. Whilst a natural idea might be to try and use online RL (like in [Calandriello et al., 2024, Guo et al., 2024]), this would require one to either generate new prompt completions online, or to correct for the distribution of the online policy by using importance sampling. The latter in turn would bring its own set of challenges, e.g., high variance in the importance sampling ratios. In order to circumvent the associated difficulties, we consider the offline setting instead. Additionally, offline RL brings orthogonal benefits of its own, such as simplicity and computational efficiency. Therefore in the following, we present an offline, sound and practical method that approximates the optimal closed-form policy.

## 3 Direct Reward Optimisation (DRO)

In this section we present the main contribution of this work. We start with some theoretical background on the policy optimisation setting. We then introduce the DRO objective and discuss a few important theoretical properties, followed by the design of practical algorithms.

### 3.1 KL regularised policy optimisation with single-trajectory data

Complementary to the pairwise preference setting highlighted above, we now consider the single trajectory case. We want to approximate the optimal Kullback-Leibler (KL) regularised policy $\pi^{*}$, following the canonical formulation of RLHF [Christiano et al., 2017]:

$$
\begin{equation*}
\pi^{*}(x) \stackrel{\text { def }}{=} \arg \max _{\pi} \mathbb{E}_{x \sim \rho, y \sim \pi(\cdot \mid x)}\left[r(x, y)-\tau \cdot \operatorname{KL}\left(\pi(\cdot \mid x) \| \pi_{\mathrm{ref}}(\cdot \mid x)\right)\right] \tag{1}
\end{equation*}
$$

where $\pi_{\text {ref }}$ is some initial reference policy, such as the policy obtained after pretraining and supervised fine-tuning. We then have that, necessarily,

$$
\begin{equation*}
\pi^{*}(y \mid x)=\frac{\pi_{\mathrm{ref}}(y \mid x) e^{\frac{1}{\tau} r(x, y)}}{e^{\frac{1}{\tau} V^{*}(x)}} \tag{2}
\end{equation*}
$$

where $V^{*}(x) \stackrel{\text { def }}{=} \tau \log \mathbb{E}_{y \sim \pi_{\text {ref }}(\cdot \mid x)}\left[e^{\frac{1}{\tau} r(x, y)}\right]$ is a function that depends on the regulariser $\pi_{\text {ref }}$. Importantly, the normalisation constant, also called partition function, that normalizes the numerator in Equation (2) can be explicitly written as $Z=\exp \frac{1}{\tau} V^{*}(x)$. The denominator log-sum-exp as a value function has already appeared in the literature about soft reinforcement learning [Ziebart et al., 2008, Haarnoja et al., 2017, Richemond and Maginnis, 2017, Schulman et al., 2018]. Unlike KTO, which assumes a constant partition function $Z$ for each prompt of the batch, we do not make any
assumptions on the form of $Z$ or $V$. Unlike DPO or IPO, where cancellation of the partition function happens due to the difference of rewards in the Bradley-Terry preference model, we do not assume any functional form for $V$. For technically minded readers, we expand on these points in Appendix D. We also note that the form of the partition function is intuitive, if we consider the LegendreFenchel conjugate [Bauschke and Combettes, 2011] of the KL regulariser in Equation (1).

The DRO objective. Now, we rearrange the optimality condition from above that holds jointly with $\pi=\pi^{*}, V=V^{*}$,

$$
\begin{equation*}
r(x, y)-V(x)=\tau \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \tag{3}
\end{equation*}
$$

Multiple objectives (and subsequently algorithms) can be derived in order to enforce or approximate this condition, such that we refer to DRO as a framework encompassing a plurality of algorithms. We can focus in particular on defining the following DRO loss for any pair of policy and value functions $(\pi, V)$ :

$$
\begin{equation*}
\mathcal{L}_{\mathrm{DRO}}(\pi, V) \stackrel{\text { def }}{=} \frac{1}{2} \mathbb{E}_{x \sim \rho, y \sim \mu(\cdot \mid x)}\left[\left(r(x, y)-V(x)-\tau \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}\right)^{2}\right] \tag{4}
\end{equation*}
$$

We begin with an existence and unicity result for the optimum of $\mathcal{L}_{\mathrm{DRO}}$, thereafter denoted as $\mathcal{L}$.

Theorem 1. $\left(\pi^{*}, V^{*}\right)$ is a global optimum of the loss $\mathcal{L}(\pi, V)$. In addition, assuming the supports of $\mu$ and $\pi_{\text {ref }}$ coincide, (i.e., for all $\left.x \in \operatorname{supp} \rho, \operatorname{supp}(\mu(\cdot \mid x))=\operatorname{supp}\left(\pi_{r e f}(\cdot \mid x)\right)\right)$, then $\left(\pi^{*}, V^{*}\right)$ is the unique global optimum of the loss $\mathcal{L}(\pi, V)$.

Proof. From the definition of $\pi^{*}$ and $V^{*}$, we have that $\mathcal{L}\left(\pi^{*}, V^{*}\right)=0$. Now notice that $\mathcal{L}(\pi, V)$ is non-negative since it is a sum of quadratic terms $t(x, y)^{2}$ where $t(x, y) \stackrel{\text { def }}{=} r(x, y)-V(x)-$ $\tau \log \frac{\pi(y \mid x)}{\pi_{\text {ref }}(y \mid x)}$. Thus $\left(\pi^{*}, V^{*}\right)$ is a global optimum of $\mathcal{L}(\pi, V)$.

Now let us prove that it is unique. Assume there is another global optimum $(\tilde{\pi}, \tilde{V})$ such that $\mathcal{L}(\tilde{\pi}, \tilde{V})=0$. This means that for all $x \in \operatorname{supp}(\rho)$ and $y \in \operatorname{supp} \mu(\cdot \mid x)$, its $t(x, y)$-term is zero. Since the support of $\mu$ and $\pi_{\text {ref }}$ coincide, we have that $\forall x \in \operatorname{supp}(\rho), \forall y \in \operatorname{supp} \pi_{\text {ref }}(\cdot \mid x)$,

$$
r(x, y)-\tilde{V}(x)-\tau \log \frac{\tilde{\pi}(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}=0
$$

from which we deduce that

$$
\tilde{\pi}(y \mid x)=\frac{\pi_{\mathrm{ref}}(y \mid x) e^{\frac{1}{\tau} r(x, y)}}{e^{\frac{1}{\tau} \tilde{V}(x)}}
$$

But since $\tilde{\pi}(\cdot \mid x)$ is a probability distribution, we must have that $\tilde{V}(x)=\tau \log \sum_{y} \pi_{\text {ref }}(y \mid x) e^{\frac{1}{\tau} r(x, y)}$. Thus $\tilde{V}(x)=V^{*}(x)$ and $\tilde{\pi}(y \mid x)=\pi^{*}(y \mid x)$ for all $x \in \operatorname{supp} \rho, y \in \operatorname{supp}\left(\pi_{\text {ref }}(\cdot \mid x)\right)$.

Remarks. The significance of Theorem 1 lies in the fact that, though $\pi^{*}$ and $V^{*}$ are clearly related via $\log \pi^{*}(y \mid x)+V^{*}(x)=\log \pi_{\text {ref }}(y \mid x)+\frac{1}{\tau} r(x, y)$, there is no need to account for that connection during optimisation. We can optimize $\pi$ and $V$ independently and end up finding the optimum. This can greatly simplify algorithmic design in practice, as we will see shortly.

Role and necessity of the $V$ function. The loss function requires a joint optimisation over $(\pi, V)$, which means separately parameterising a value function $V$ in addition to the policy $\pi$ so as to be theoretically sound. Thus, DRO stands in strong contrast to alternative algorithms such as KTO [Ethayarajh et al., 2024], which uses a policy loss only without a value function. One natural question arises as to whether it is possible to convert the loss function jointly over $(\pi, V)$ into a loss over $\pi$ alone. This is an idea we explore in Appendix B where we optimise $V$ fully before updating $\pi$.

### 3.2 Approximation error

As alluded to before, we can interpret $V$ as a value function in conjunction with the policy $\pi$. When fixing $\pi$ we can define the minimising value function $V^{\pi}=\arg \min _{V} \mathcal{L}(\pi, V)$. This value function, when fixed to $V=V^{\pi}$, provides learning signal to policy improvement. Indeed, since $(\pi, V)$ can be independently optimised as shown before, we can see that the optimal policy $\pi^{*}$ is obtained by solving the optimisation problem $\pi^{*}=\arg \min _{\pi} \mathcal{L}\left(\pi, V^{\pi}\right)=\arg \min _{\pi} \arg \min _{V} \mathcal{L}(\pi, V)$.

However, since in practice the learning of $V \approx V^{\pi}$ is approximate, the optimisation of $\pi$ against such an approximate value function would induce errors. Formally, we consider what happens when $\pi$ is optimised against a fixed $V$ rather than a well-learned $V^{\pi}$. The approximate nature of $V$ induces error in $\pi \approx \pi^{*}$, which we characterise below.

Proposition 1. Consider an approximation $V(x)$ of the value function $V^{\pi}=\arg \min _{V} \mathcal{L}(\pi, V)$ :

$$
V^{\pi}(x)=\mathbb{E}_{y \sim \mu(\cdot \mid x)}\left[r(x, y)-\tau \log \frac{\pi(y \mid x)}{\pi_{r e f}(y \mid x)}\right]
$$

and for this function kept fixed, let us optimise the loss $\pi \mapsto \mathcal{L}(\pi, V)$ with respect to the policy only. Let $\pi_{V}=\arg \min _{\pi} \mathcal{L}(\pi, V)$. Then we have that $\pi_{V}$ satisfies the following equation:

$$
\begin{equation*}
\pi_{V}(y \mid x) \propto \pi_{r e f}(y \mid x) e^{\frac{1}{\tau}\left[r(x, y)-\frac{\pi_{V}(y \mid x)}{\mu(y \mid x)}\left(V^{\pi} V(x)-V(x)\right)\right]} \tag{5}
\end{equation*}
$$

In addition we have that for all $x, y$,

$$
\begin{equation*}
\left|\log \frac{\pi_{V}(y \mid x)}{\pi^{*}(y \mid x)}\right| \leq \frac{2}{\tau} \max _{y}\left|\left(V^{\pi_{V}}(x)-V(x)\right)\left(1-\frac{\pi_{V}(y \mid x)}{\mu(y \mid x)}\right)\right| \tag{6}
\end{equation*}
$$

Proof. We delay the full technical proof to the Appendix, see section A.

Two remarks are in order here:

- Equation (6) says that if our approximation $V$ is close to the value function of $\pi_{V}$ (in some sense, the "Bellman residual" $V-V^{\pi_{V}}$ of $V$ is small), then $\pi_{V}$ is close to $\pi^{*}$.
- In particular, if we consider the best state-independent baseline $V_{c}^{\pi}=$ $\arg \min _{V: V(x) \equiv c} \mathcal{L}(\pi, V)=\mathbb{E}_{x \sim \rho}\left[V^{\pi}(x)\right]$ (this could be estimated using a large enough batch size), we see that we would recover the optimal policy $\pi^{*}$ if $V^{\pi^{*}}(x)=V_{c}^{\pi^{*}}$ for all $x$.


### 3.3 Practical Implementation

We consider a parametric policy $\pi_{\theta}$ and a parametric function $V_{\varphi}$ with neural network parameters $\theta, \varphi$ respectively. We assume access to data in the form of tuples $\left(x_{i}, y_{i}, r_{i}\right)_{1 \leq i \leq n}$ where $x_{i} \sim \rho$ are prompts, $y_{i} \sim \mu\left(\cdot \mid x_{i}\right)$ are prompt-conditional generations associated with a model policy $\mu$, and $r_{i}=r\left(x_{i}, y_{i}\right)$ are scalar reward functions for the individual prompt-completion combination $\left(x_{i}, y_{i}\right)$. We perform gradient descent, both on $\theta$ and $\varphi$, to minimise the empirical loss:

$$
\widehat{\mathcal{L}}(\theta, \varphi) \stackrel{\text { def }}{=} \frac{1}{2} \sum_{i=1}^{n}\left(r\left(x_{i}, y_{i}\right)-V_{\varphi}\left(x_{i}\right)-\tau \log \frac{\pi_{\theta}\left(y_{i} \mid x_{i}\right)}{\pi_{\mathrm{ref}}\left(y_{i} \mid x_{i}\right)}\right)^{2}
$$

We can define explicitly the gradient w.r.t. $\varphi$ and $\theta$ : the gradient w.r.t. the parameter $\varphi$ of the value function is:

$$
\begin{equation*}
\nabla_{\varphi} \widehat{\mathcal{L}}(\theta, \varphi)=\sum_{i=1}^{n}\left(V_{\varphi}\left(x_{i}\right)-r\left(x_{i}, y_{i}\right)+\tau \log \left(\frac{\pi_{\theta}\left(y_{i} \mid x_{i}\right)}{\pi_{\text {ref }}\left(y_{i} \mid x_{i}\right)}\right)\right) \nabla_{\varphi} V_{\varphi}\left(x_{i}\right) \tag{7}
\end{equation*}
$$

and the policy gradient:

$$
\begin{equation*}
\nabla_{\theta} \widehat{\mathcal{L}}(\theta, \varphi)=-\tau \sum_{i=1}^{n}(\nabla_{\theta} \underbrace{\log \pi_{\theta}\left(y_{i} \mid x_{i}\right)\left(r\left(x_{i}, y_{i}\right)-V_{\varphi}\left(x_{i}\right)\right)}_{\text {policy optimisation loss }}-\frac{\tau}{2} \nabla_{\theta} \underbrace{\left(\log \frac{\pi_{\theta}\left(y_{i} \mid x_{i}\right)}{\pi_{\text {ref }}\left(y_{i} \mid x_{i}\right)}\right)^{2}}_{\ell_{2} \text {-regularisation loss }}) \tag{8}
\end{equation*}
$$

The policy optimisation loss is similar to a standard policy gradients RL algorithm, however it is important to notice a few key differences and connections between the loss above and policy gradient algorithms.

Understanding the value function $V$. First, when considering a fixed policy $\pi$, learning the value function $V_{\varphi}$ corresponds to learning the function $V^{\pi}$ whose approximation error we discussed earlier in Section 3.2.

Second, the value function $V_{\varphi}\left(x_{i}\right)$ which is subtracted from the reward $r\left(x_{i}, y_{i}\right)$ is not simply a baseline used for the purpose of variance reduction as is usually the case in RL [Sutton et al., 2000, Konda and Tsitsiklis, 1999]. Indeed removing this term (or replacing it by any function $V\left(x_{i}\right)$ ) would bias our policy gradient estimate. The reason for that being that the samples $y_{i}$ are off-policy, i.e., drawn from $\mu$ and not from $\pi_{\theta}$, thus in general, $\mathbb{E}_{y \sim \mu\left(\cdot \mid x_{i}\right)}\left[\nabla_{\theta} \log \pi_{\theta}\left(y \mid x_{i}\right) V\left(x_{i}\right)\right] \neq 0$ (whereas if on-policy we would have $\left.\mathbb{E}_{y \sim \pi_{\theta}\left(\cdot \mid x_{i}\right)}\left[\nabla_{\theta} \log \pi\left(y \mid x_{i}\right) V\left(x_{i}\right)\right]=0\right)$. Thus it is important to maintain this value function estimate in the policy gradient.

Offline regularisation. Another difference compared to a usual regularised $\mathrm{PG}$ algorithm is the use of a $\ell_{2}$-regularisation loss instead of a KL-regularisation loss. Notice that these two regularisation losses do not lead to equivalent gradients for the reason that the samples $y_{i}$ are off-policy: indeed we have (see, e.g., also [Calandriello et al., 2024, Tang et al., 2024] for discussion of similar theoretical results)

$$
\frac{1}{2} \mathbb{E}_{y \sim \mu\left(\cdot \mid x_{i}\right)}\left[\nabla_{\theta}\left(\log \frac{\pi_{\theta}\left(y \mid x_{i}\right)}{\pi_{\mathrm{ref}}\left(y \mid x_{i}\right)}\right)^{2}\right] \neq \nabla_{\theta} \operatorname{KL}\left(\pi_{\theta}\left(\cdot \mid x_{i}\right), \pi_{\mathrm{ref}}\left(\cdot \mid x_{i}\right)\right)
$$

unless the sampling is on-policy $\mu=\pi_{\theta}$, where both losses would would lead to the same gradient. Thus we could see the policy gradient update rule Equation (8) as the natural extension of a usual on-policy regularised PG algorithm to the off-policy case.

Policy learning rate rescaling. Finally, in practice, we rescale the policy gradient Equation (8) by multiplying the update by a factor of $1 / \tau$. We found this works better empirically and we hypothesised that the global loss $\widehat{\mathcal{L}}(\theta, \varphi)$ may be ill-conditioned (as its sensitivities w.r.t. the dimensions $\theta$ and $\varphi$ are different, leading to a high condition number). We refer to this algorithm as the Direct Reward Optimisation with Value algorithm, or DRO-V. DRO-V is described in Algorithm 1.

```
Algorithm 1 Direct Reward Optimisation with Value Algorithm (DRO-V)
    Inputs: A single-trajectory dataset: $\left(x_{i}, y_{i}, r_{i}=r\left(x_{i}, y_{i}\right)\right)_{i=1}^{N}$, a parameterised policy: $\pi_{\theta}$, a
    reference policy: $\pi_{\text {ref }}$, a parameterised value function: $V_{\varphi}$, a regularisation scalar $\tau$, a number of
    total steps $K$, a batch size $B$ and an optimiser.
    for $k=1$ to $K$ do
        Sample uniformly a batch: $\left(x_{i}, y_{i}, r_{i}\right)_{i=1}^{B}$
        Compute gradient updates $\nabla_{\theta} \widehat{\mathcal{L}}(\theta, \varphi)$ and $\nabla_{\varphi} \widehat{\mathcal{L}}(\theta, \varphi)$ as in Equation (8) and Equation (7):
```

$$
\begin{gathered}
\nabla_{\theta} \widehat{\mathcal{L}}(\theta, \varphi)=-\frac{1}{B} \sum_{i=1}^{B}\left(\nabla_{\theta} \log \pi_{\theta}\left(y_{i} \mid x_{i}\right)\left(r\left(x_{i}, y_{i}\right)-V_{\varphi}\left(x_{i}\right)\right)-\frac{1}{2} \nabla_{\theta}\left(\log \frac{\pi_{\theta}\left(y_{i} \mid x_{i}\right)}{\pi_{\mathrm{ref}}\left(y_{i} \mid x_{i}\right)}\right)^{2}\right) \\
\nabla_{\varphi} \widehat{\mathcal{L}}(\theta, \varphi)=\frac{1}{B} \sum_{i=1}^{B}\left(V_{\varphi}\left(x_{i}\right)-r\left(x_{i}, y_{i}\right)+\tau \log \left(\frac{\pi_{\theta}\left(y_{i} \mid x_{i}\right)}{\pi_{\mathrm{ref}}\left(y_{i} \mid x_{i}\right)}\right)\right) \nabla_{\varphi} V_{\varphi}\left(x_{i}\right)
\end{gathered}
$$

Update the policy parameters: $\theta \leftarrow$ UpdateOptimiser $\left(\theta, \nabla_{\theta} \widehat{\mathcal{L}}(\theta, \varphi)\right)$
Update the value parameters: $\varphi \leftarrow \operatorname{UpdateOptimiser}\left(\varphi, \nabla_{\varphi} \widehat{\mathcal{L}}(\theta, \varphi)\right)$
end for
Outputs: $\pi_{\theta}$
Offline optimisation. The dataset of (prompt, completion, reward) triplets remains static during
optimisation. This is because we do not use the current "online" policy, parameterised by the most
recent $\theta$ parameters, to regenerate completions for a given prompt. In this regard, our optimisation
is performed like in offline reinforcement learning, where taking new actions in the environment
is structurally prohibited. We note that our method does not require the training of an additional
reward model. The distinction between offline and online procedures is important in RL(HF), since they can give rise to different gradients and correspond to very different theoretical justifications, as studied in e.g. Calandriello et al. [2024], Guo et al. [2024]. Given the additional $\tau$ regularisation term appearing in Equation (8), we describe DRO as offline, regularised reinforcement learning for large language model alignment.

Neural network implementation. It is natural to wonder whether $\theta$ and $\varphi$ need be separate or whether parameter sharing can occur for efficient learning. Perhaps counter-intuitively, we found that using two separate networks, one for $\pi_{\theta}$ and one for $V_{\varphi}$, was beneficial empirically, compared to using policy logits as value outputs. Another implementation decision, this time strictly related to $V_{\varphi}$, is whether to use a single value per batch or a value per token. We found that using a single value per batch hurts performance. In both instances parameter sharing is detrimental.

As such, by default, we implemented DRO-V using two networks, as well as multiple values across the batch. We will return to these points and ablate these design choices in the Experiments section next.

## 4 Experiments

We now present our empirical results on finetuning LLMs using DRO-V.

Models and task dataset. In all that follows, we perform our test experiments using the UltraFeedback dataset described in Cui et al. [2023]. We preprocess the data to create an offline dataset of triplets (prompt, completion, reward) where the prompt might be shared across multiple triplets. We also normalize the dataset such that the rewards have mean 0 and variance 1 across the dataset.

We use T5 large language models [Raffel et al., 2020], a family of auto-regressive transformers with an encoder-decoder architecture, in order to train all our models. The details of the models architecture and software implementation can be found in [Roberts et al., 2022]. Furthermore, our checkpoints are initialised from instruction finetuning according to the FLAN recipe [Chung et al., 2022]. We denote this initialisation policy ( $\pi_{\text {ref }}$ as earlier), as the SFT, for supervised finetuning policy. Depending on the experiment, we use either a large (L) or an extra-large (XL) encoder-decoder model. The large model sports $770 M$ parameters, whereas the XL model has $3 B$ parameters.

Evaluation. Following now standard practice [Zheng et al., 2023], our evaluation pipeline consists in automated evaluation; specifically, side-by-side comparison. We use the PaLM2 [Anil et al., 2023] LLM as a judge. Given a test set of prompts, for each pair of trained policies, we sample completion responses, and then ask PaLM2 to judge which one is better. The format of the evaluation prompt we use for side-by-side comparison is as follows:

In this task, you will be provided with an instruction and two responses. Your job is to assess the helpfulness and fulfillment of two responses A and B.

Instruction: article response A (left):summary1 response B (right):summary2

For each model, we record a checkpoint every 2,000 training steps, before selecting the best checkpoint across the training curve as determined by side-by-side comparison against the SFT policy. We then use this best checkpoint as representative of an algorithm or set of parameters.

Compute and hyperparameters. For compute, we use version 5 (' $v 5 e^{\prime}$ ) Tensor Processing Units [TPUs; Jouppi et al., 2023], in the cloud. We train large encoders in configurations of $4 \times 4$ devices and $X L$ encoders in configurations of $4 \times 8$ devices. With this computational setup we obtain speeds of around 0.5 training steps per second ( 21 hours per 40,000 steps) for the large encoders, and 0.1 training steps per second ( 2 days per 20,000 steps) for the $X L$ encoders. We run our experiments with default learning rate $1 \mathrm{e}-4$ both for the value and the policy networks, and a default total of 40, 000 training steps for $L$ models and 20,000 training steps for $X L$ models, using a batch size of 32. The optimiser we use is AdaFactor [Shazeer and Stern, 2018] with a decay rate value of 0.8. For the learning rate, we employ 150 linear warmup steps.

KTO baseline. We compare our algorithm primarily to the strong Kahneman-Tversky Optimization (KTO) baseline of Ethayarajh et al. [2024]. We need a criterion to decide which sample completions are desirable ('thumbs up') or not. We choose to do so per-batch, based on whether each attached scalar reward is greater or less than the average reward seen over the minibatch. We use
the same $\mathrm{KL}$ divergence estimator on the batch as they do, and do not weigh the loss in any manner, since we are already compensating for batch statistics by computing the batchwise average reward. For both algorithms KTO and DRO-V, the $\tau$ regularisation factor (denoted by $\beta$ in Ethayarajh et al. [2024]) is held constant throughout training.

### 4.1 Results and Ablations

Here we show our empirical results as well as perform ablations and study the impact of hyperparameters on performance.

Empirical results. DRO-V outperforms KTO in side-by-side comparison, both for the T5-L and XL encoders. In the interest of fair comparison, we optimise our baseline as much as possible and present the best KTO results we obtained after search over three values of the $\tau$ regularisation parameter, $\tau \in\{0.1,1.0,5.0\}$ ( $\tau=5.0$ being optimal). By contrast we show our DRO-V results without such search, simply setting $\tau$ to a default intuitive value of 1.0 . In this section only, we give standard deviation estimates, thanks to computing averages of scores obtained by side-by-side comparison over 5 evaluation folds of 1,000 test prompts each:

| Side-by-side Winrate | (first over second) |
| :---: | :---: |
| DRO-V vs SFT: | $78.9 \% \pm 0.3 \%$ |
| KTO vs SFT: | $67.5 \% \pm 0.7 \%$ |
| DRO-V vs KTO: | $63.4 \% \pm 1.0 \%$ |

Figure 1: Winrates with T5-L encoders.

| Side-by-side Winrate | (first over second) |
| :---: | :---: |
| DRO-V vs SFT: | $81.5 \% \pm 1.0 \%$ |
| KTO vs SFT: | $78.2 \% \pm 0.7 \%$ |
| DRO-V vs KTO: | $57.5 \% \pm 0.8 \%$ |

Figure 2: Winrates with T5-XL encoders.

We see significant empirical outperformance of DRO-V. Both on T5-L and T5-XL encoders, DRO-V clearly outperforms the SFT ( $78.9 \%$ and $81.5 \%$, respectively). Most importantly, DRO-V also wins over the KTO baseline in direct comparison ( $63.4 \%$ and $57.5 \%$ respectively). We also give qualitative examples of this difference, with sample completions from both algorithms, in Appendix E where we demonstrate how these numbers correspond to more helpful and focused prompt completions.

We now study ablations over hyperparameters and architecture. In all that follows, we retain the same experimental protocol as above, and here use T5 large encoders exclusively.

Impact of the learning rate for policy and value. We begin by examining the impact of the learning rate on downstream performance. We study two cases: first varying jointly the learning rate of the policy and the value network, and second, switching the learning rate of the value network only. We pick our learning rates to be one of $1 \mathrm{e}-5,5 \mathrm{e}-5,5 \mathrm{e}-4$, or the default $1 \mathrm{e}-4$. Results are presented in Figure 3. For the value only sweep, we observe that changing the value learning rate parameter alone yields a small impact. Thus the policy learning rate rescaling factor, $1 / \tau$ (Equation 8 ) is all the more important. We do also note a small yet monotonic improvement in learning the value $V$ faster than the policy. Overall, the performance of DRO-V remains very stable within an order of magnitude change for learning rates.

| DRO-V learning rate value | $1 \mathrm{e}-5$ | $5 \mathrm{e}-5$ | $1 \mathrm{e}-4$ | $5 \mathrm{e}-4$ |
| :---: | :---: | :---: | :---: | :---: |
| Joint LR : Winrate vs SFT | $73.7 \%$ | $78.7 \%$ | $\mathbf{7 8 . 9} \%$ | $78.4 \%$ |
| LR, value $V$ only : Winrate vs SFT | $76.8 \%$ | $78.0 \%$ | $78.9 \%$ | $\mathbf{7 9 . 1} \%$ |

Figure 3: Top line: Impact of varying jointly the $\pi$ and $V$ learning rate parameter. Bottom line: Impact of value function $V$ learning rate parameter only.

Impact of regularisation parameter $\tau$. Similarly, we vary the strength of regularisation parameter $\tau$ both for DRO-V and KTO. We pick between three values: 0.1, 1.0 and 5.0. Results are presented in Figure 4. We see substantial variation in performance due to this parameter. A regularisation parameter of 1.0 , an intuitive value, is actually best for DRO-V. On the other hand, we found it far from optimal for KTO and picked the best value, 5.0, instead. These experiments were performed using T5-L encoders, and we re-used those $\tau$ choices for T5-XL experiments as well.

| DRO-V $\tau$ | 0.1 | 1.0 | 5.0 |
| :---: | :---: | :---: | :---: |
| Winrate vs SFT | $70.5 \%$ | $\mathbf{7 8 . 9} \%$ | $76.6 \%$ |


| KTO $\tau$ | 0.1 | 1.0 | 5.0 |
| :---: | :---: | :---: | :---: |
| Winrate vs SFT | $63.5 \%$ | $61.9 \%$ | $\mathbf{6 7 . 5} \%$ |

Figure 4: Left: Impact of $\tau$ parameter on DRO-V. Right: Impact of $\tau$ parameter on KTO.

Impact of parameter sharing. Finally, we investigate the quantitative impact of parameter sharing for DRO-V, as exposed in Section 3.3. We jointly study the performance of the single or double network version, as well as whether to use a single value number per batch or not. For computational reasons, these experiments at performed using 10,000 steps of training only (and therefore slightly undertrained compared to our main T5-L $78.9 \%$ result). Results are presented Figure 5, with the full comparison matrix in Appendix C. The impact of full parameter sharing is material, with most of the hit coming from the single or double network choice. However, we also observe that when using two networks for $\pi$ and $V$, there are significant gains in not using a single value per batch ( $76.6 \%$ against $72.1 \%$ winrate vs SFT, a difference of $4.5 \%$, confirmed in direct side-by-side comparison of those two variants yielding $54.9 \%$ in favour of the multiple value version). These observations help explain some of the outperformance of our method.

| Parameter Sharing Variant | Double Net <br> Single Value | Single Net <br> Single Value | Single Net <br> Multiple Values | Double Net <br> Multiple Values |
| :---: | :---: | :---: | :---: | :---: |
| Winrate vs SFT | $72.1 \%$ | $57.6 \%$ | $55.5 \%$ | $\mathbf{7 6 . 6} \%$ |

Figure 5: Parameter sharing variants. DRO-V Winrate vs SFT.

## 5 Related work

Human Feedback in Reinforcement Learning. Integrating human feedback into reinforcement learning, as introduced by Christiano et al. [2017], has rapidly grown to be considered essential for improving the practical utility of LLMs and mitigating their epistemic risk [Hannigan et al., 2024]. RLHF may not always yield direct improvements in benchmark performance [Touvron et al., 2023], but it significantly enhances human-centric applications like dialogue systems [Nakano et al., 2021, Ouyang et al., 2022] and extends to non-human-centric tasks too like MuJoCo physics [Yuan et al., 2024b] and robotics [Gao et al., 2024]. Precursors to RLHF were implemented with Deep Q Networks [Mnih et al., 2013] and Actor-Critic algorithms [Mnih et al., 2016, Glaese et al., 2022], and RLHF itself was debuted with proximal policy optimisation [Schulman et al., 2017]. Given the complexity of RLHF techniques and algorithms used [Casper et al., 2023], the research community is revealing surprising benefits of simpler strategies like REINFORCE [Ahmadian et al., 2024], sequence likelihood [Zhao et al., 2023a], and ranking approaches [Dong et al., 2023, Yuan et al., 2023].

Advancements in Policy Optimisation. Assuming a Bradley-Terry model [Bradley and Terry, 1952] for human reward modeling enables the RLHF problem to be reformulated as a supervised learning task [Rafailov et al., 2023]; this may provide greater training stability [Zhao et al., 2023b] and data efficiency, although this point is debated [Xu et al., 2024]. Recent research has focused on enhancing the direct preference optimization (DPO) methods for scalability [Tunstall et al., 2023, Ivison et al., 2023] and safety [Liu et al., 2024] while expanding their mathematical underpinnings [Azar et al., 2023, Wang et al., 2023, Ji et al., 2024]. Other work constrains DPO's contextual focus [Zeng et al., 2024] or explores alternatives to preference models such as Nash equilibria [Munos et al., 2023, Rosset et al., 2024]. Despite these advancements, both DPO and RLHF encounter challenges such as reward hacking [Pang et al., 2022, Skalse et al., 2022], length bias [Park et al., 2024], and overoptimisation [Amodei et al., 2016, Pan et al., 2022], which can in turn lead to under-regularised models [Gao et al., 2022, Singhal et al., 2023] and objective mismatch Kirk et al. [2023]. While techniques like ensembling can alleviate issues of alignment [Eisenstein et al., 2023, Ramé et al., 2024] and overoptimisation [Wortsman et al., 2022, Coste et al., 2023], they may be overlooked due to their computational intensity [Zhang et al., 2024].

Different data types. RLHF classically focuses on a choice that human raters make between two outputs produced from a single input to the policy. Collecting this data type can be costly, motivating
the use of cheaper data, such as upvote/downvote point-wise data [Ethayarajh et al., 2024], or joint preferences from different questions [Bansal et al., 2024]. Such data can also be noisy, motivating factual augmentation [Sun et al., 2023], fine-grained responses [Wu et al., 2023].

Online vs. Offline Learning and Alignment through Self-Play. The practical distinction between online and offline methods [Jaques et al., 2019] appears more relevant than the reinforcement versus supervised learning dichotomy. Online policies risk deviating from the original data distribution, causing shifts and potential issues [Zhuang and Hadfield-Menell, 2020, Shin et al., 2023]. Alternatively, alignment can be achieved through self-play in a two-player game framework [Munos et al., 2023, Swamy et al., 2024], encompassing both online and offline settings and allowing smooth transitions between them. Similarly, iterative DPO has demonstrated alignment improvement [Yuan et al., 2024a], building upon established techniques like reinforcement learning from AI feedback [Bai et al., 2022b, Lee et al., 2023].

## 6 Conclusion and limitations

We have introduced DRO, a new framework for aligning LLMs in the setting of single-trajectory datasets, where for each prompt a single completion associated to a scalar reward is available. DRO moves away from the traditional preference setting in RLHF. In doing so it not only makes the training of an explicit reward model redundant, but also and most importantly enables leveraging potentially orders of magnitude more data coming from user feedback instead of raters'. DRO is theoretically principled since it learns both a policy and a value. Thus it does not rely on any simplifying mathematical assumptions, such as a uniform value function on the batch, or the cancellation of the partition function that typically underpins RLHF methods. Using T5 large and $X L$ text encoders, we have shown that these properties translate into strong performance on the UltraFeedback dataset when compared to Kahneman-Tversky optimisation. However, our empirical study is limited, both in terms of number of tasks and scale. Further work is required to more broadly establish the performance gains that our approach provides when considering the largest language models, as it is able to leverage large amounts of user-generated data.

## Acknowledgements

We would like to thank the whole Google DeepMind team for providing the infrastructure to make this work possible. In particular, we would like to thank Matt W. Hoffman, Bobak Shahriari, Nikola Momchev, Sertan Girgin and Piotr Stanczyk for their support in building the coding infrastructure and Doina Precup and Olivier Bachem for their guidance and continual support.

## References

Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arxiv preprint arXiv:2402.14740, 2024.

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in AI safety. arXiv, 2016.

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. PaLM 2 technical report, 2023.

Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human preferences. arXiv, 2023.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv, 2022a.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, John Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, E Perez, Jamie Kerr, Jared Mueller, Jeff Ladish, J Landau, Kamal Ndousse, Kamilė Lukoiūtė, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noem'i Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, T. J. Henighan, Tristan Hume, Sam Bowman, Zac Hatfield-Dodds, Benjamin Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom B. Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback. arXiv, 2022b.

Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, and Aditya Grover. Comparing bad apples to good oranges: Aligning large language models via joint preference optimization. arxiv preprint arXiv: 2404.00530, 2024.

Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, 2011.

Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamilè Lukošiūtė, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022.

Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952 .

Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, Rishabh Joshi, Zeyu Zheng, and Bilal Piot. Human alignment of large language models through online preference optimisation. arxiv preprint arXiv:2403.08635, 2024.

Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arxiv preprint arXiv:2002.05709, 2020.

Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, 2017.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.

Thomas Coste, Usman Anwar, Robert Kirk, and David Scott Krueger. Reward model ensembles help mitigate overoptimization. arXiv, 2023.

Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. URL https://github.com/OpenBMB/UltraFeedback. MIT license.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arxiv preprint arXiv:1810.04805, 2019.

Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and T. Zhang. RAFT: Reward rAnked FineTuning for generative foundation model alignment. arXiv, 2023.

Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D'Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen R Pfohl, Deepak Ramachandran, Peter Shaw, and Jonathan Berant. Helping or herding? Rward model ensembles mitigate but do not eliminate reward hacking. arXiv, 2023.

Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arxiv preprint arXiv:2402.01306, 2024.

Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In Proceedings of the International Conference on Machine Learning, 2022.

Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, and Wen Sun. Rebel: Reinforcement learning via regressing relative rewards. arxiv preprint arXiv:2404.16767, 2024.

Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements. arXiv, 2022.

Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy shaping: Integrating human feedback with reinforcement learning. In Advances in Neural Information Processing Systems, 2013.

Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In Advances in Neural Information Processing Systems, 2020.

Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. arxiv preprint arXiv:1702.08165, 2017.

Timothy R. Hannigan, Ian P. McCarthy, and André Spicer. Beware of botshit: How to manage the epistemic risks of generative chatbots. Business Horizons, 2024. ISSN 0007-6813. doi: https://doi.org/10.1016/j.bushor.2024.03.001. URL https://www.sciencedirect.com/science/article/pii/S0007681324000272.

Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hanna Hajishirzi. Camels in a changing climate: Enhancing LM adaptation with Tulu 2. arXiv, 2023.

Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv, 2019.

Haozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie Tang, and Minlie Huang. Towards efficient and exact optimization of language model alignment. arxiv preprint arXiv:2402.00856, 2024.

Norman P. Jouppi, George Kurian, Sheng Li, Peter C. Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiaoping Zhou, Zongwei Zhou, and David A. Patterson. TPU v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. In Proceedings of the Annual International Symposium on Computer Architecture, 2023.

D. Kahneman and A. Tversky. Prospect theory: An analysis of decision under risk. Econometrica, 47(2):263-292, 1979 .

Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. arXiv preprint arXiv:2310.06452, 2023.

W Bradley Knox and Peter Stone. TAMER: Training an agent manually via evaluative reinforcement. In Proceedings of the IEEE International Conference on Development and Learning, 2008.

Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems, 12, 1999 .

Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning from human feedback with AI feedback. arXiv, 2023.

Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.

Zixuan Liu, Xiaolin Sun, and Zizhan Zheng. Enhancing llm safety via constrained direct preference optimization, 2024.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, $\mathrm{abs} / 1312.5602,2013$.

Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the International Conference on Machine Learning, 2016.

Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, and Bilal Piot. Nash learning from human feedback. arXiv, 2023.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback. arXiv, 2021.

OpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv, 2022.

Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv, abs/2201.03544, 2022.

Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur P. Parikh, and He He. Reward gaming in conditional text generation. In Annual Meeting of the Association for Computational Linguistics, 2022.

Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization, 2024.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arxiv preprint arXiv:2103.00020, 2021.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Advances in Neural Information Processing Systems, 2023.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.

Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. WARM: On the benefits of weight averaged reward models. arXiv, 2024.

Pierre H. Richemond and Brendan Maginnis. A short variational proof of equivalence between policy gradients and soft q learning. arxiv preprint arXiv:1712.08650, 2017.

Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling up models and data with $t 5 x$ and seqio. arXiv, 2022. URL https://github.com/google-research/t5x. Apache-2.0 license.

Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences, 2024.

William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv, 2017.

John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft qlearning. arxiv preprint arXiv:1704.06440, 2018.

Noam M. Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. arXiv, 2018.

Daniel Shin, Anca D. Dragan, and Daniel S. Brown. Benchmarks and algorithms for offline preference-based reward learning. arXiv, 2023.

Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in rlhf. ArXiv, abs/2310.03716, 2023.

Joar Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. In Neural Information Processing Systems, 2022.

Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf, 2023.

R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12, pages 1057-1063. MIT Press, 2000.

Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaximalist approach to reinforcement learning from human feedback. arXiv, 2024.

Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires, and Bilal Piot. Generalized preference optimization: A unified approach to offline alignment. arXiv preprint arXiv:2402.05749, 2024.

Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov,

Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv, 2023.

Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of LM alignment. arXiv, 2023.

Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse KL: Generalizing direct preference optimization with diverse divergence constraints. arXiv, 2023.

Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone. Deep TAMER: Interactive agent shaping in high-dimensional state spaces. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.

Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael GontijoLopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Proceedings of the International Conference on Machine Learning, 2022.

Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training, 2023.

Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. arxiv preprint arXiv:2404.10719, 2024.

Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models, 2024a.

Yifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu, Zhixin Feng, Kai Zhao, and Yan Zheng. Uni-rlhf: Universal platform and benchmark suite for reinforcement learning with diverse human feedback, 2024b.

Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Feiran Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv, $\mathrm{abs} / 2304.05302,2023$.

Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Token-level direct preference optimization, 2024.

Shun Zhang, Zhenfang Chen, Sunli Chen, Yikang Shen, Zhiqing Sun, and Chuang Gan. Improving reinforcement learning from human feedback with efficient reward model ensemble, 2024.

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. SLiCHF: Sequence likelihood calibration with human feedback. arXiv, 2023a.

Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023b.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

Simon Zhuang and Dylan Hadfield-Menell. Consequences of misaligned AI. In Advances in Neural Information Processing Systems, 2020.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.

Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2020.
