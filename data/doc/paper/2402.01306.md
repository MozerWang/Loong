# KTO: Model Alignment as Prospect Theoretic Optimization 

Kawin Ethayarajh ${ }^{1}$ Winnie Xu ${ }^{2}$ Niklas Muennighoff ${ }^{2}$ Dan Jurafsky ${ }^{1}$ Douwe Kiela ${ }^{12}$


#### Abstract

Kahneman \& Tversky's prospect theory tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biasesthe success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being human-aware loss functions (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a KahnemanTversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferencesonly a binary signal of whether an output is desirable or undesirable for a given input. This makes it far easier to use in the real world, where preference data is scarce and expensive.


## 1. Introduction

Aligning generative models with human feedback has been successfully used to make generations more helpful, factual, and ethical, among other desiderata (Ouyang et al., 2022; Tian et al., 2023). For LLMs, alignment methods such as RLHF and DPO have consistently proven to be more beneficial than doing supervised finetuning (SFT) alone. However, human feedback is often discussed only in the context of preferences (e.g., output $A \succ B$ for input $x$ ), despite preferences being a kind of data that is relatively scarce and expensive to collect in the real world (Casper et al., 2023). This is largely because the alignment methods shown to work best-RLHF (Christiano et al., 2017) and[^0]

the mathematically equivalent DPO (Rafailov et al., 2023)_ take preference data as input.

To understand why these alignment methods work so well, and whether feedback needs to be in the form of preferences, we frame them through the lens of prospect theory (Kahneman \& Tversky, 1979; Tversky \& Kahneman, 1992). Prospect theory explains why humans make decisions about uncertain events that do not maximize expected value. It formalizes how humans perceive random variables in a biased but well-defined manner; for example, relative to some reference point, humans are more sensitive to losses than gains, a property called loss aversion. We show that popular alignment methods such as PPO (Schulman et al., 2017), DPO (Rafailov et al., 2023), and SLiC (Zhao et al., 2023) implicitly model such biases, helping explain their success independently of the data used. For this reason, we call them human-aware loss functions (HALOs).

Although it is impossible to say that HALOs are categorically better than non-HALOs, we find that among existing methods, those that meet the definition of a HALO work better than those that do not. We find that DPO performance can even be matched at most scales by running an offline PPO variant on dummy +1/-1 rewards, suggesting that preference data might not be needed if the inductive bias in the loss function is good enough. However, despite the surprising success of this simple baseline, it significantly lags behind DPO at the 30B model scale and suffers from hyperparameter sensitivity, making it difficult to use.

Taking a more principled approach, we derive a HALO using the model of human utility that Kahneman \& Tversky empirically derived to describe how humans make decisions about uncertain monetary outcomes (Tversky \& Kahneman, 1992). This approach, which we call Kahneman-Tversky Optimization (KTO), directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as most current methods do. KTO only requires a binary signal of whether an output is desirable or undesirable for a given input. This data is much more abundant, cheaper, and faster to collect in the real world than preferences, making it easier to scale alignment in production environments and rapidly iterate on models.

In our experiments, we find that:

![](https://cdn.mathpix.com/cropped/2024_05_26_60da5211e7e1f425fa14g-02.jpg?height=542&width=1250&top_left_y=236&top_left_x=405)

Figure 1. The traditional pipeline for LLM alignment starts with supervised finetuning, followed by fitting the LLM to paired preference data using a method such as RLHF or DPO. However, the paired preferences that existing approaches need are hard-to-get. KahnemanTversky Optimization (KTO) only needs to know whether a given output is (un)desirable for the input, giving it access to a source of data that is much more abundant, cheaper, and faster to collect in the real world.

- KTO matches or exceeds DPO performance at scales from 1B to 30B parameters. ${ }^{1}$ That is, taking a preference dataset of $n$ DPO pairs and breaking it up into $2 n$ examples for KTO can yield better generations, despite the model ostensibly learning from a weaker signal. We provide some theoretical explanations for this phenomenon $(\$ 4.3)$.
- KTO can handle extreme data imbalances, matching DPO performance while using up to $90 \%$ fewer desirable examples (i.e., examples of good generations). Its success thus cannot be ascribed to the alignment data being sourced from a preference dataset.
- When the pretrained model is sufficiently good, one can skip supervised finetuning and go straight to KTO without a loss in generation quality. In contrast, we find that without doing SFT first, DPO-aligned models are significantly worse at all scales.

The fact that KTO can match and sometimes even outperform DPO is surprising, given that it learns from a weaker signal. We conclude by discussing some theoretical explanations for this phenomenon.

## 2. Background

Feedback-aligned LLMs are traditionally trained in three stages (Ouyang et al., 2022):

Pretraining Given a large corpus, train the model to predict the next token conditioned on the preceding text using the cross-entropy loss. Let $\pi$ denote the pretrained model.[^1]

Supervised Finetuning Finetune the model to predict the next token on data that is more relevant to the downstream task. Often, such data will comprise instructions and an appropriate response (i.e., instruction finetuning). Let $\pi_{\text {ref }}$ denote the finetuned model.

RLHF Given a dataset $\mathcal{D}$ of preferences $\left(x, y_{w}, y_{l}\right)$ where $x$ is an input, $y_{w}, y_{l}$ are the preferred and dispreferred outputs (i.e., $y_{w} \succ y_{l}$ for $x$ ), and $r^{*}$ is the "true" reward function underlying the preferences-it is first assumed that the probability that $y_{w}$ is preferred to $y_{l}$ can be captured with a specific function class, typically a Bradley-Terry model (Bradley \& Terry, 1952). Where $\sigma$ is the logistic function:

$$
\begin{equation*}
p^{*}\left(y_{w} \succ y_{l} \mid x\right)=\sigma\left(r^{*}\left(x, y_{w}\right)-r^{*}\left(x, y_{l}\right)\right) \tag{1}
\end{equation*}
$$

Since getting the true reward from a human would be intractably expensive, a reward model $r_{\phi}$ learns to serve as a proxy, done by minimizing the negative log-likelihood of the human preference data:

$$
\mathcal{L}_{R}\left(r_{\phi}\right)=\mathbb{E}_{x, y_{w}, y_{l} \sim D}\left[-\log \sigma\left(r_{\phi}\left(x, y_{w}\right)-r_{\phi}\left(x, y_{l}\right)\right)\right]
$$

But solely maximizing the reward might come at the expense of desiderata such as generating grammatical text. To avoid this, a KL divergence penalty is introduced to restrict how far the language model can drift from $\pi_{\text {ref. }}$. Where $\pi_{\theta}$ is the model we are optimizing, the optimal model $\pi^{*}$ is that which maximizes

$$
\begin{equation*}
\mathbb{E}_{x \in D, y \in \pi_{\theta}}\left[r_{\phi}(x, y)\right]-\beta D_{\mathrm{KL}}\left(\pi_{\theta}(y \mid x) \| \pi_{\mathrm{ref}}(y \mid x)\right) \tag{2}
\end{equation*}
$$

where $\beta>0$ is a hyperparameter. Since this objective is not differentiable, we need to use an RL algorithm like PPO (Schulman et al., 2017).

However, RLHF is often slow (largely because of having to sample generations) and quite unstable in practice (especially in a distributed setting). For this reason, recent work has focused on designing closed-form losses that maximize the margin between the preferred and dispreferred generations, such as Sequence-Likelihood Calibration (SLiC) (Zhao et al., 2023) and Direct Preference Optimization (DPO) (Rafailov et al., 2023). The latter has become popular due to its mathematical equivalence with RLHF:

$$
\begin{align*}
& \mathcal{L}_{\mathrm{DPO}}\left(\pi_{\theta}, \pi_{\mathrm{ref}}\right)= \\
& \mathbb{E}\left[-\log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{l} \mid x\right)}\right)\right] \tag{3}
\end{align*}
$$

## 3. A Prospect Theoretic View of Alignment

Kahneman \& Tversky's prospect theory explains why, faced with an uncertain event, humans make decisions that do not maximize the expected value (1992). For example, because humans are loss-averse, given a gamble that returns $\$ 100$ with $80 \%$ probability and $\$ 0$ with $20 \%$ probability, a person might accept $\$ 60$ to avoid the gamble, despite their certainty equivalent of $\$ 60$ being less than the expected value of $\$ 80$.

### 3.1. Prospect Theory

In prospect theory, human utility depends on a value function and a weighting function: ${ }^{2}$

Definition 3.1. A value function $v: z \rightarrow \mathbb{R}$ maps an outcome $z$, relative to some reference point $z_{\text {ref }}$, to its perceived (or subjective) value. For example, these functions capture the fact that humans tend to be more sensitive to relative losses than relative gains of the same magnitude.

Definition 3.2. A weighting function $w$ is the derivative of a capacity function that maps cumulative probabilities to perceived cumulative probabilities. These functions capture, for example, the fact that humans tend to overestimate the chance of rare events. Let $w_{z}$ denote the weight placed on outcome $z$.

Definition 3.3. The utility of a random variable $Z$ is a function of its outcomes: $u(Z) \triangleq \sum_{z \in Z} w_{z} v\left(z-z_{\text {ref }}\right)$.

However, because humans do not see the full probability distribution of an LLM, weighting functions are not salient to this discussion; we will focus only on value functions. Using experiments that presented real humans with monetary gambles and asked for their certainty equivalent, Tversky \& Kahneman (1992) proposed the following functional form for human value:

$$
v\left(z, z_{\mathrm{ref}} ; \lambda ; \alpha\right)= \begin{cases}\left(z-z_{\mathrm{ref}}\right)^{\alpha} & \text { if } z>z_{\mathrm{ref}}  \tag{4}\\ -\lambda\left(z_{\mathrm{ref}}-z\right)^{\alpha} & \text { if } z<z_{\mathrm{ref}}\end{cases}
$$[^2]

![](https://cdn.mathpix.com/cropped/2024_05_26_60da5211e7e1f425fa14g-03.jpg?height=588&width=723&top_left_y=232&top_left_x=1102)

Figure 2. The utility that a human gets from the outcome of a random variable, as imputed by the value function implicit in HALOs. Notice that the imputed functions share properties such as loss aversion with the human value functions that Kahneman \& Tversky empirically derived (1992).

where the median value of hyperparameter $\alpha=0.88$ and $\lambda=2.25$ across individuals. $\alpha$ controls how quickly utility changes and $\lambda$ controls the degree of loss aversion. While the shape of the median Kahneman-Tversky value function is illustrated in Figure 2, it should be noted that it varies across individuals (Tversky \& Kahneman, 1992). There are also other functional forms for the value function that have been proposed in later work (Gurevich et al., 2009). The salient qualities of a value function are: the existence of a reference point that is added or subtracted to get the relative gain or loss respectively; concavity in relative gains (i.e. diminishing sensitivity away from $z_{\text {ref }}$ ); loss aversion (i.e., greater sensitivity to losses).

### 3.2. HALOs

Informally, HALOs are loss functions that model the human biases in Tversky \& Kahneman (1992). Formally,

Definition 3.4 (HALOs). Let $x \in \mathcal{X}$ denote an input and $y \in \mathcal{Y}$ an output. Then $f:(x, y) \rightarrow \mathbb{R}$ is a human-aware loss function if there exists the following: a parameterized reward function $r_{\theta}$ such that $\forall\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right) \in \mathcal{X} \times \mathcal{Y}$,

$$
r_{\theta}\left(x_{1}, y_{1}\right)>r_{\theta}\left(x_{2}, y_{2}\right) \Longleftrightarrow\left(x_{1}, y_{1}\right) \succ_{r_{\theta}}\left(x_{2}, y_{2}\right)
$$

reference point distributions $Q_{x}\left(X^{\prime}\right), Q_{y}\left(Y^{\prime} \mid X^{\prime}\right)$, a value function $v_{f}: \mathbb{R} \rightarrow \mathbb{R}$ that is monotonic non-decreasing and concave in $(0, \infty)$, and a negative affine function $t$ such that

$$
\begin{equation*}
f(x, y ; \theta)=t\left(v_{f}\left(r_{\theta}(x, y)-\mathbb{E}_{x^{\prime}, y^{\prime}}\left[r_{\theta}\left(x^{\prime}, y^{\prime}\right)\right]\right)\right) \tag{5}
\end{equation*}
$$

where $x^{\prime} \sim Q_{x}\left(X^{\prime}\right)$ and $y^{\prime} \sim Q_{y}\left(Y^{\prime} \mid x^{\prime}\right)$.

Put simply, the requirement for the reward function is that it assigns higher rewards to input-output pairs that are more
preferred under it. The reference point is the expected reward with respect to input-output pairs sampled from the distributions $Q_{x}, Q_{y}$. We require that the value function be concave in gains but not necessarily convex in lossesunlike the canonical Kahneman-Tversky value functionsbecause in the original work on prospect theory, a minority of individuals were found to be risk-averse in both the gain and loss regime (i.e., concave in both gains and losses) (Kahneman \& Tversky, 1979). Note that risk-aversion is different from loss-aversion; they relate to the curvature and magnitude of the slope respectively.

Proposition 3.5. DPO, SLiC (calibration loss only), and PPO-Clip are human-aware loss functions.

The proof is deferred to Appendix A. In Figure 2, we can see this more intuitively by plotting the value function for each loss (i.e., the implied human utility). We see that the value functions of all three losses incorporate a sense of loss aversion, although this is not needed to meet the definition of a HALO, since there are individuals and scenarios for which loss aversion does not necessarily apply. The value functions are also either concave or affine (depending on the interval), unlike the standard Kahneman-Tversky value function, which is concave in gains but convex in losses. The reference point distributions used also differs across the losses.

### 3.3. Does being a HALO matter?

A natural question is whether the modeling of human biases in HALOs has practical benefits. This is difficult to answer, since both HALOs and non-HALOs are diverse function classes, but we attempt to do so by comparing popular nonHALO and HALO baselines on the exact same data:

1. CSFT: Conditional SFT is a simple alignment method where a control token is prepended to the output during training; then, at inference, the control token corresponding to desirable generations (e.g., <|good $\mid>$ ) is appended to the input to induce good generations (Korbak et al., 2023). This is a non-HALO loss.
2. SLiC: SLiC with a regularization penalty $\left(\lambda_{\text {reg }} \neq 0\right)$ is a non-HALO loss:

$$
\begin{aligned}
& \mathcal{L}_{\mathrm{SLiC}}\left(\pi_{\theta}, \pi_{\mathrm{ref}}\right)=\mathcal{L}_{\mathrm{cal}}\left(\pi_{\theta}\right)+\lambda_{\mathrm{reg}} L_{\mathrm{reg}}\left(\pi_{\theta}\right) \\
& \mathcal{L}_{\mathrm{cal}}=\mathbb{E}_{x, y_{w}, y_{l} \sim D}\left[\max \left(0, \delta-\log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\theta}\left(y_{l} \mid x\right)}\right)\right] \\
& \mathcal{L}_{\mathrm{reg}}=\mathbb{E}_{x \sim D, y \sim \pi_{\mathrm{ref}}(x)}\left[-\log \pi_{\theta}(y \mid x)\right]
\end{aligned}
$$

Although the max-margin loss $\mathcal{L}_{\text {cal }}$ is a HALO on its own (Proposition 3.5), the complete loss is not, since the $\mathcal{L}_{\text {reg }}$ term is the standard language modeling loss.

3. DPO: DPO, as defined in (3), is a HALO loss (Proposition 3.5).
4. PPO (offline): The standard RLHF objective in (2) is typically optimized with PPO-Clip, which works by "clipping" how far $\pi_{\theta}$ can drift from the version $\pi_{\text {old }}$ at the previous step:

$$
\begin{aligned}
\mathcal{L}_{\mathrm{PPO} \text { (offline) }}= & -\mathbb{E}_{x, y, t \sim D}\left[\operatorname { m i n } \left(q_{\theta} A\left(x, y_{<t}, y_{t}\right)\right.\right. \\
& \left.\left.\operatorname{clip}\left(q_{\theta}, 1-\epsilon, 1+\epsilon\right) A\left(x, y_{<t}, y_{t}\right)\right)\right]
\end{aligned}
$$

where $q_{\theta}=\log \frac{\pi_{\theta}}{\pi_{\text {old }}}$ and $A\left(x, y_{<t}, y_{t}\right)$ is the per-token advantage (i.e., the surplus benefit from producing a given token in a given state).

PPO is an online algorithm-generations are sampled from the current model, judged by a reward model, and then used to update the current version. However, this process is slow (due to having to sample generations), so we choose to use offline data instead. Because RLHF is also quite unstable in a distributed setting, we never update $\pi_{\text {old }}$ and keep it as $\pi_{\text {ref }}$, instead clipping less conservatively than we traditionally would (see Appendix B for details). Baheti et al. (2023) found that these changes, along with treating the entire output sequence as a single action, greatly improves stability. However, since RLHF has historically calculated tokenlevel advantages, we omit the third change and only preserve the first two. The PPO-Clip loss itself is left unchanged and is therefore a HALO (Proposition 3.5).

Calling this method PPO is somewhat imprecise, because it is offline and takes only one step, but to avoid introducing too many new terms, we will call this $P P O$ (offline). Instead of using learned rewards, we simplify even further and use dummy $+1 /-1$ rewards for $y_{w}$ and $y_{l}$ instead. Further details on the implementation of this method can be found in Appendix B.

We compare these baselines on a suite of 7 models spanning two model families, Pythia- $\{1.4 \mathrm{~B}, 2.8 \mathrm{~B}, 6.9 \mathrm{~B}, 12 \mathrm{~B}\}$ (Biderman et al., 2023) and Llama-\{7B, 13B, 30B \} (Touvron et al., 2023). This permits us to see how LLM alignment scales within a model family (Llama-2 lacks a 30B model, hence our use of Llama). Later experiments ( $\$ 4.2$ ) are done on Mistral-7B and its derivatives (Jiang et al., 2023). The models were trained on a combination of Anthropic HH (Ganguli et al., 2022), OpenAssistant (Köpf et al., 2023), and SHP (Ethayarajh et al., 2022).

All models were aligned under identical settings on the same data (e.g., same effective batch size, same optimizer, etc.), save for hyperparameters unique to them. Similar to Rafailov et al. (2023), the target sequences for SFT are a subset of the generations used to subsequently align the model; however, for a more realistic SFT setup, we do not necessarily set the most preferred generation to be the target (with the exception of $\mathrm{HH}$, since the dispreferred output in that dataset is often harmful). Then we used GPT-4-0613

![](https://cdn.mathpix.com/cropped/2024_05_26_60da5211e7e1f425fa14g-05.jpg?height=564&width=1702&top_left_y=233&top_left_x=187)

Does the aligned model beat the SFT target?

![](https://cdn.mathpix.com/cropped/2024_05_26_60da5211e7e1f425fa14g-05.jpg?height=518&width=1678&top_left_y=272&top_left_x=191)

Figure 3. Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC and CSFT), though the gap is only significant $(p<0.05)$ at 13B+ model sizes. In fact, only the HALO-aligned Llama-\{13B, 30B $\}$ models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset. It is also worth noting that up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage.

to judge whether the aligned model's response was better than the SFT target for the given input with respect to helpfulness, harmlessness, and conciseness, a now standard practice (Zheng et al., 2023; Li et al., 2023). ${ }^{3}$ Note that while the SFT target is considered a desirable output for $x$, it is by no means the best output, meaning that it can be improved upon by an aligned model.

In Figure 3, we see the results of this analysis:
- The HALOs we tested (DPO and our PPO variant) either match or outperform the non-HALOs at all scales, though the gap is only significant $(p<0.05)$ at 13B+ model sizes. In fact, only the HALO-aligned Llama\{13B, 30B \} models match or exceed a win rate of $50 \%$ (i.e., are able to match or exceed the generation quality of the SFT targets in the test data).
- Up to a scale of 7B parameters, alignment provides virtually no gains over SFT alone. However, it is worth noting that if the SFT data distribution were less similar to the preference data, then the gains from the alignment stage would ostensibly be greater.
- Surprisingly, despite only using dummy $+1 /-1$ rewards, our offline PPO variant performs as well as DPO for all models except Llama30B. This challenges conventional wisdom, which places heavy emphasis on reward learning (Casper et al., 2023), suggesting that even the simplest rewards can prove useful when used in a loss function that has a strong inductive bias. Despite its surprising success, our offline PPO baseline still suffers from hyperparameter sensitivity and training instability,[^3]

albeit not to the same extent as traditional RLHF.

## 4. Kahneman-Tversky Optimization

The surprising success of offline PPO with dummy +1/-1 rewards suggests that — with the right HALO—a binary signal of good/bad generations may be sufficient to reach DPOlevel performance, even if the offline PPO approach itself was unable to do so past a certain scale ( $\$ 3.3$ ). Taking a more principled approach, we now derive a HALO using the Kahneman-Tversky model of human utility, which allows us to directly optimize for utility instead of maximizing the log-likelihood of preferences. This Kahneman-Tversky Optimization (KTO) loss only needs a binary signal of whether an output is (un)desirable for a given input, giving it access to a source of data is more abundant, cheaper, and faster to collect in the real world.

### 4.1. Derivation

From prior work (Go et al., 2023; Peng et al., 2019; Peters \& Schaal, 2007), we know that the policy that maximizes the KL-constrained RLHF objective in (2) is

$$
\pi^{*}(y \mid x)=\frac{1}{Z(x)} \pi_{\text {ref }}(y \mid x) \exp \left(\frac{1}{\beta} r^{*}(x, y)\right)
$$

where $Z(x)$ is a partition function. Rafailov et al. (2023) rewrite this in terms of the optimal reward for an inputoutput pair:

$$
\begin{equation*}
r^{*}(x, y)=\beta \log \frac{\pi^{*}(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}+\beta \log Z(x) \tag{6}
\end{equation*}
$$

They then plug this expression into the Bradley-Terry model of preferences and take the negative logarithm of that objective to get the DPO loss (3).

![](https://cdn.mathpix.com/cropped/2024_05_26_60da5211e7e1f425fa14g-06.jpg?height=562&width=1699&top_left_y=234&top_left_x=186)

Does the aligned model beat the SFT target?

![](https://cdn.mathpix.com/cropped/2024_05_26_60da5211e7e1f425fa14g-06.jpg?height=525&width=1680&top_left_y=266&top_left_x=190)

Figure 4. Kahneman-Tversky Optimization (KTO) is as good or better than DPO at all scales, both when preceded and not preceded by supervised finetuning (SFT). In fact, for the Llama models, KTO alone matches the performance of SFT+DPO and is significantly better than DPO alone. Error bars denote a $90 \%$ binomial confidence interval.

Instead, we plug this expression into the Kahneman-Tversky model of human utility, with some changes to make it more amenable to the LLM setting:

1. The exponent in the Kahneman-Tversky value function (4) makes it difficult to optimize, so we set $v_{\text {KTO }}$ to be the logistic function $\sigma$, which is also concave in gains and convex in losses. We replace the lossaversion coefficient with two hyperparameters $\lambda_{D}, \lambda_{U}$ that weight the losses for desirable and undesirable outputs respectively.
2. The Kahneman-Tversky value function was derived based on experiments with humans and monetary gambles. Since LLM generations do not have a monetary reward associated with them, we set $r_{\text {KTO }}$ to be the implicit reward under the RLHF objective (6).
3. Rather than having just one dispreferred generation $y_{l} \mid x$ as the reference point, we assume that humans judge the quality of $(x, y)$ in relation to all input-output pairs they have seen. Thus we write the reference point to be the expected reward under the optimal policy, not just for generations following $x$ but following any input $x^{\prime}: \mathbb{E}_{x^{\prime} \sim D, y^{\prime} \sim \pi^{*}}\left[r^{*}\left(x^{\prime}, y^{\prime}\right)\right]$. Under the assumption that the expected value of the partition function across $x^{\prime}$ is zero, this simplifies to the KL divergence between $\pi^{*}$ and $\pi_{\text {ref }}$ scaled by $\beta$.

Combining all of these changes, we can optimize the following loss, where the notion of an output being "desirable" or "undesirable" corresponds to the Kahneman-Tversky notion of a relative gain or loss.

$$
\begin{equation*}
L_{\mathrm{KTO}}\left(\pi_{\theta}, \pi_{\mathrm{ref}}\right)=\mathbb{E}_{x, y \sim D}\left[w(y)\left(1-v_{\mathrm{KTO}}(x, y ; \beta)\right)\right] \tag{7}
\end{equation*}
$$

where

$$
\begin{aligned}
r_{\mathrm{KTO}}(x, y) & =\beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \\
z_{\mathrm{ref}} & =\mathbb{E}_{x^{\prime} \sim D}\left[\beta \operatorname{KL}\left(\pi_{\theta}\left(y^{\prime} \mid x^{\prime}\right) \| \pi_{\mathrm{ref}}\left(y^{\prime} \mid x^{\prime}\right)\right)\right] \\
v_{\mathrm{KTO}}(x, y ; \beta) & =\left\{\begin{array}{l}
\sigma\left(r_{\mathrm{KTO}}(x, y)-z_{\text {ref }}\right) \text { if } y \sim y_{\text {desirable }} \mid x \\
\sigma\left(z_{\text {ref }}-r_{\mathrm{KTO}}(x, y)\right) \text { if } y \sim y_{\text {undesirable }} \mid x
\end{array}\right. \\
w(y) & = \begin{cases}\lambda_{D} & \text { if } y \sim y_{\text {desirable }} \mid x \\
\lambda_{U} & \text { if } y \sim y_{\text {undesirable }} \mid x\end{cases}
\end{aligned}
$$

Intuitively, KTO works because if the model increases the reward of a desirable example in a generic way, then the KL penalty will also rise and no progress will be made on the loss. This forces the model to learn exactly what makes an output desirable, so that the reward can be increased while keeping the KL term flat (or even decreasing it). A similar argument works in the other direction as well, though the non-negativity of the KL term allows faster saturation.

Implementation In practice, we estimate the KL term by matching inputs $x^{\prime}$ with unrelated outputs $y_{U}^{\prime}$ in a batch of size $m$ and then calculating max $\left(0, \frac{1}{m} \sum \log \frac{\pi_{\theta}\left(y_{U}^{\prime} \mid x^{\prime}\right)}{\pi_{\text {ref }}\left(y_{U}^{\prime} \mid x^{\prime}\right)}\right)$ over the entire batch. We do not back-propagate through the $\mathrm{KL}$ term, as it makes training much more stable. This means that the KL term purely serves to control how saturated the loss is.

$\beta$ has the same meaning as in DPO; the lower it is, the less we penalize $\pi_{\theta}$ from moving away from the SFT model $\pi_{\text {ref }}$. We find that $\beta=0.1$ is close-to-best on most datasets. Where $n_{D}$ and $n_{U}$ refer to the number of desirable and undesirable examples respectively, we set $\lambda_{D}, \lambda_{U}$ such that

$$
\begin{equation*}
\frac{\lambda_{D} n_{D}}{\lambda_{U} n_{U}} \in\left[1, \frac{4}{3}\right] \tag{8}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_05_26_60da5211e7e1f425fa14g-07.jpg?height=409&width=813&top_left_y=232&top_left_x=190)

Figure 5. Without doing SFT first, DPO-aligned models tend to ramble and hallucinate entire conversations. KTO does not suffer from this issue.

where at least one of the two should be set to 1 and the ratio is controlled by changing the other. For example, if there is a 1:1 ratio of desirable:undesirable examples, we would set $\lambda_{U}=1, \lambda_{D} \in[1,1.33]$. If we then discard $90 \%$ of the desirable examples and only keep $10 \%$, then we would set $\lambda_{U}=1, \lambda_{D} \in[10,13.33]$. The interval $[1,4 / 3]$ was determined empirically and suggests a value function that is more gain-sensitive than loss-sensitive, in contrast to the original Kahneman-Tversky value function (4). However, the ideal interval is also task-dependent; for example, if avoiding negative outcomes were very important, then we might consider a setting of $\lambda_{U}>1$ instead.

Data If the alignment data is naturally binary, every positive example can be assumed to be drawn from $y_{\text {desirable }} \mid x$ and every negative example from $y_{\text {undesirable }} \mid x$. However, the canonical feedback datasets in academic research $(\mathrm{HH}, \mathrm{SHP}$, OASST) are in preference format, since the methods that have worked best up until now are preference-based. In our experiments, we converted preference data $y_{w} \succ y_{l}$ by assuming that $y_{w}$ is drawn from the desirable distribution and $y_{l}$ from the undesirable one. To enable an apples-to-apples comparison with DPO, we apply KTO on the same data for most experiments. However, to ensure that KTO can be used with non-preference data, we also subsample one output $y$ per $x$ for some experiments (denoted one- $y$-per- $x$ ).

If the data is score-based, where a higher score denotes greater desirability, one has multiple options:

- Assume that any output with a score above some fixed threshold $\tau$ is desirable.
- Assume that any output with a score above the mean or median (either across all inputs or just the input it was conditioned on) is desirable.
- Let desirability be a Bernoulli random variable where $p\left(y \sim y_{\text {desirable }} \mid x\right)$ is some function of its score (e.g., logistic). Then randomly sample to determine whether $y$ is desirable or not.

![](https://cdn.mathpix.com/cropped/2024_05_26_60da5211e7e1f425fa14g-07.jpg?height=626&width=791&top_left_y=237&top_left_x=1079)

Figure 6. Even after discarding $90 \%$ of the desirable examples while keeping all of the undesirable data (leading to a 1:10 ratio of desirable:undesirable data), a KTO-aligned Llama-7B model still outperforms its DPO counterpart. This implies that preference pairs do not have to be the source of KTO data.

### 4.2. Experiments

KTO $\geq$ DPO As seen in Figure 4, SFT+KTO is competitive with SFT+DPO at model scales from 1B to 30B, despite learning from a weaker signal. KTO alone is better than DPO alone for the Llama-\{7B, 13B, 30B \} models, and this gap is significant $(p<0.01)$ at $7 \mathrm{~B}$ and 30B even after correcting for multiple comparisons (Holm, 1979). Perhaps most surprising is the fact that a KTO-aligned Llama- $\{13 \mathrm{~B}$, 30B $\}$ model is competitive with its SFT+KTO counterpart, despite not undergoing supervised finetuning first, and is the only alignment method of the ones we tested to show this behavior. This is perhaps due to the fact that KTO keeps the average response length roughly the same as it is for the SFT model. In contrast, doing DPO without SFT first causes the average response length to increase dramatically.

KTO data need not come from preference datasets. Might KTO be secretly benefiting from the fact that its $2 n$ examples in the previous experiment came from $n$ preference pairs instead of a naturally unpaired data distribution? To test this, we randomly discard increasingly large fractions of the desirable data before KTO-aligning a Llama-7B model. For example, if we discard $90 \%$ of the desirable data while leaving the undesirable data untouched, then the ratio of desirable:undesirable examples goes from 1:1 to 1:10 and the vast majority of examples no longer have a preferred output counterpart. We handle such imbalances by changing the loss weights $\lambda_{D}, \lambda_{U}$ to satisfy the criteria in (8); when we drop $90 \%$ of the desirable data, we set $\lambda_{u}=1, \lambda_{D}=13.33$. The full results are given in Figure 6. For Llama-7b, we find that up to $90 \%$ of the desirable

Table 1. In aligning Mistral-7B on the OpenAssistant dataset, we find that using KTO with only one output per input still outperforms DPO, despite this restriction reducing the amount of training data by $72 \%$. A $90 \%$ confidence interval is given.

| Method | Winrate vs. SFT Target |
| :--- | :---: |
| Mistral-7B (unaligned) | $0.525 \pm 0.037$ |
| Mistral-7B + DPO | $0.600 \pm 0.037$ |
| Mistral-7B + KTO (all $y$ per $x$ ) | $\mathbf{0 . 6 5 2} \pm \mathbf{0 . 0 3 6}$ |
| Mistral-7B + KTO (one $y$ per $x$ ) | $0.631 \pm 0.036$ |
| Mistral-7B-Instruct | $0.621 \pm 0.031$ |

data can in fact be discarded while still outperforming DPO. A similar trend holds when discarding undesirable data. For different models and datasets, the optimal settings of $\lambda_{D}, \lambda_{U}$ differ.

We further verify this claim by aligning Mistral-7B on OpenAssistant using DPO (on $n$ pairs), standard KTO (on all $2 n$ outputs), and KTO where only one $y$ per $x$ is used. Since the output of one $y$ in OpenAssistant is not conditioned on the other outputs for the same input, the latter effectively captures the setting where the data is from an inherently unpaired distribution. Despite the one- $y$-per- $x$ setup decreasing the amount of training data by $72 \%$, the KTO-aligned model still outperforms both its DPO counterpart and the official instruction-tuned version of Mistral-7B (Jiang et al., 2023), as seen in Table 1.

On average, KTO improves performance across generative benchmarks. Zephyr- $\beta$ is a variant of Mistral-7B that has been instruction-tuned and DPO-aligned on the UltraFeedback dataset (Tunstall et al., 2023; Cui et al., 2023). We find that substituting KTO for DPO (and changing nothing else) improves performance across MMLU (0-shot) (Hendrycks et al., 2021), GSM8K (8-shot, CoT) (Cobbe et al., 2021), HumanEval (0-shot) (Chen et al., 2021), and BigBench-Hard (3-shot CoT) (Srivastava et al., 2022). On GSM8K, just swapping DPO for KTO improves performance by 13.5 points. Even when we align with KTO using only one $y$ per $x$ (i.e., reducing the data volume by half), we still outperform DPO on all but one benchmark.

### 4.3. Theoretical Analysis

KTO was designed with the motivation that even if it had to learn from a weaker signal, it would make up for this limitation with the fact that it has access to much more data in the real world, where thumbs-up/thumbs-down data is common but preferences are scarce and expensive to collect. So why does KTO perform as good or better than DPO in our experiments, when it sees the same amount of data? Data efficiency may not be the only answer. Our theoretical analysis suggests that preference likelihood can
Table 2. Aligning Zephyr (Tunstall et al., 2023), a derivative of Mistral-7B, on UltraFeedback with KTO instead of DPO improves results across a suite of benchmarks. This is true even when only one of the two outputs in each preference is seen by KTO, despite this reducing the volume of data by half (one- $y$-per- $x$ ).

| Dataset $(\rightarrow)$ | MMLU | GSM8k <br> EM | HumanEval <br> pass@ 1 | BBH <br> EM |
| :--- | :--- | :--- | :--- | :--- |
| Metric $(\rightarrow)$ | EM | E |  |  |
| Zephyr- $\beta$ SFT | 57.2 | 39.0 | 30.1 | 46.3 |
| +DPO | 58.2 | 40.0 | 30.1 | 44.1 |
| +KTO | $\mathbf{5 8 . 6}$ | $\mathbf{5 3 . 5}$ | $\mathbf{3 0 . 9}$ | $\mathbf{5 2 . 6}$ |
| +KTO (one- $y$-per- $x$ ) | 58.0 | 50.0 | 30.7 | 49.9 |

be maximized without necessarily maximizing underlying human utility and that KTO implicitly ignores noisy and intransitive data.

Proposition 4.1. KTO does not learn from undesirable examples with sufficiently high rewards or desirable examples with sufficiently low rewards.

Informally, if an example is too difficult to learn from, then the KTO update will not change $\pi_{\theta}$. This may be a blessing in disguise, since human preferences are often noisy and not every given preference can be recovered with the true reward $r^{*}$ (Hoeffler \& Ariely, 1999). This means that it may be useful to avoid unlearnable preferences. However, this is a double-edged sword: it also means that KTO could end up ignoring some data that is hard-to-learn but necessary to recover $r^{*}$, resulting in under-fitting.

Theorem 4.2. Assuming the value function is logistic, for any bounded reward function $r_{a}$, there exists a reward function in its equivalence class (i.e., $r_{b}(x, y)=r_{a}(x, y)+h(x)$ for some $h(x)$ ) that induces the same optimal policy $\pi^{*}$ and Bradley-Terry preference distribution but a different human value distribution.

A key insight from Rafailov et al. (2023) is that reward functions in the same equivalence class (i.e., differing only in an input-specific component) induce the same optimal policy under (2) and the same Bradley-Terry preference distribution. However, we show under mild assumptions that the value distribution-i.e., human utility-is affected by such input-specific changes, so maximizing preference likelihood does not mean one is maximizing human utility. Approaches that directly maximize utility, such as KTO, may thus perform better in open-ended evaluation.

Theorem 4.3. Let two humans $a, b$ have value functions $v_{a}, v_{b}$ and contradicting preferences $y_{1} \succ_{a} y_{2}$ and $y_{2} \succ_{b} y_{1}$ for some input $x$. Assume $\pi_{\text {ref }}(y \mid x)=0 \Longrightarrow \pi_{\theta}(y \mid x)=0$ for all $x, y$. In the worst-case, the optimal policy under DPO decreases the expected value of both humans. In contrast, if each preference is broken up into two examples, then KTO (with default settings) does not change the policy.

Informally, we assume that humans want the model to in-
crease and decrease the probability of generations they like and dislike respectively. However, the preferences of two humans often contradict, leading to a dataset containing intransitive preferences. In the worst-case, DPO allows one of the two preferences to be recovered while decreasing the expected value of both humans. In contrast, KTO will change nothing at all in any case. Since existing datasets contain preferences from multiple annotators, the existence of intransitivity may help explain why KTO works better.

### 4.4. KTO vs. DPO - when to use which?

When human feedback is in a binary format, and especially when there is an imbalance between the number of desirable and undesirable examples, KTO is the natural choice. When your data is in the form of preferences, the choice is less clear. Putting aside the greater data efficiency of KTO, our theoretical analysis suggests that if your preference data has sufficiently little noise and sufficiently little intransitivity, then DPO will work better, since there is some risk of KTO underfitting. But if there is enough noise and transitivity, then the better worst-case guarantees of KTO will win out. Most publicly available preference datasets (e.g., SHP, OpenAssistant) contain noisy feedback from many different humans whose preferences likely contradict, which explains why KTO was able to match or exceed DPO performance in our experiments. Even AI feedback can be noisy and intransitive, which helps explain why KTO outperforms DPO when aligning with the synthetic UltraFeedback data.

## 5. Related Work

Human feedback has been used to improve LLM capabilities in translation (Kreutzer et al., 2018), summarization (Stiennon et al., 2020), sentiment-conditioned generation (Ziegler et al., 2019), and instruction-following (Ouyang et al., 2022). The RLHF framework (Christiano et al., 2017; Bai et al., 2022) traditionally used to accomplish this is detailed in $\S 2$.

Still, momentum has largely shifted in favor of closed-form losses that directly operate on offline preferences, such as DPO (Rafailov et al., 2023). This single stage of optimization distinguishes DPO from the conventional approach in preference-based RL, which learns a reward and then fits the policy to those rewards (Jain et al., 2013; Busa-Fekete et al., 2014). A recent string of work has centered on the idea of "self-training" or "self-play", during which new preference data is inferred from a model's generations (Chen et al., 2024; Yuan et al., 2024). Despite not being a human-aware loss, unlikelihood training was among to first to methods to align language models using a binary signal (Welleck et al., 2019). However, work by Korbak et al. (2023) found that it is worse than the CSFT baseline we tested in our work.

Prospect theory, despite being highly influential in behav- ioral economics, has had a fairly muted impact in machine learning, with work concentrated in human-robot interaction (Kwon et al., 2020; Sun et al., 2019; Chan et al., 2021). Learning from sparse binary feedback is a staple of information retrieval and recommender systems (He et al., 2017; Koren et al., 2009), although to our knowledge it has not been used to generate open-ended text.

## 6. Future Work

The existence of HALOs raises many questions. For one, the KTO loss is based on the Kahneman-Tversky value function for monetary gains and losses, which is almost certainly different from how humans perceive the relative goodness of text. What value function-and corresponding HALO - best describes how humans perceive language?

Given that the data that KTO needs is much more abundant, cheaper, and faster to collect-both as human and AI feedback-how far can we push synthetic data? For example, if we wanted to create a toxicity dataset to align our models to be less toxic, creating a tuple $\left(x, y_{w}, y_{l}\right)$ where $y_{l}$ is more toxic than $y_{w}$ is non-trivial. However, with KTO, we can easily create a dataset where desirability is determined by some black-box toxicity detection API. What other kinds of desiderata can we synthetically optimize for with KTO? Can we convert signals like "conversation lead to sale made" or "support ticket resolved" into KTO data?

Currently, KTO can learn from score-based data when the score is used to infer desirability. However, can we design a HALO where scores are directly incorporated into this loss?

## 7. Conclusion

We proposed a class of functions called human-aware losses (HALOs) based on the idea of a Kahneman-Tversky value function, which models some of the key cognitive biases that inform how humans make decisions about uncertain outcomes. We showed that among existing alignment methods, those that met the definition of a HALO performed better than those that did not, suggesting a benefit to the modeling of human biases. We then designed a human-aware loss called KTO for directly maximizing the utility of generations instead of maximizing preference likelihood. Despite only learning from a binary signal of whether an output is (un)desirable, KTO is as good or better than DPO at scales from 1B to 30B. Still, we make no claims that KTO is the best HALO for all scenarios; there remains much work to be done in discovering the optimal human-aware for each setting.

## Acknowledgements

We thank Dilip Arumugam and Arya McCarthy for feedback on the paper and Nathan Lambert for feedback on an early version of this draft. We thank Stas Bekman and Gautam Mittal for cluster assistance and Alex Manthey for helping with human evaluation.

## References

Baheti, A., Lu, X., Brahman, F., Bras, R. L., Sap, M., and Riedl, M. Improving language models with advantage-based offline policy gradients. arXiv preprint arXiv:2305.14718, 2023.

Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. PMLR, 2023.

Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.

Busa-Fekete, R., Szörényi, B., Weng, P., Cheng, W., and Hüllermeier, E. Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm. Machine learning, 97:327-351, 2014.

Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023.

Chan, L., Critch, A., and Dragan, A. Human irrationality: both bad and good for reward inference. arXiv preprint arXiv:2111.06956, 2021.

Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024.

Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M. Ultrafeedback: Boosting language models with high-quality feedback, 2023.

Ethayarajh, K., Choi, Y., and Swayamdipta, S. Understanding dataset difficulty with $\mathcal{V}$-usable information. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 5988-6008. PMLR, 17-23 Jul 2022.

Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022.

Go, D., Korbak, T., Kruszewski, G., Rozen, J., Ryu, N., and Dymetman, M. Aligning language models with preferences through f-divergence minimization. arXiv preprint arXiv:2302.08215, 2023.

Gurevich, G., Kliger, D., and Levy, O. Decision-making under uncertainty-a field study of cumulative prospect theory. Journal of Banking \& Finance, 33(7):1221-1229, 2009.

He, X., Liao, L., Zhang, H., Nie, L., Hu, X., and Chua, T.-S. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web, pp. $173-182,2017$.

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.

Hoeffler, S. and Ariely, D. Constructing stable preferences: A look into dimensions of experience and their impact on preference stability. Journal of consumer psychology, 8 $(2): 113-139,1999$.

Holm, S. A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics, pp. 65-70, 1979.

Jain, A., Wojcik, B., Joachims, T., and Saxena, A. Learning trajectory preferences for manipulators via iterative improvement. Advances in neural information processing systems, 26, 2013.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Kahneman, D. and Tversky, A. Prospect theory: An analysis of decision under risk. Econometrica, 47(2):263-292, 1979.

Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., et al. Openassistant conversationsdemocratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.

Korbak, T., Shi, K., Chen, A., Bhalerao, R. V., Buckley, C., Phang, J., Bowman, S. R., and Perez, E. Pretraining language models with human preferences. In International Conference on Machine Learning, pp. 1750617533. PMLR, 2023.

Koren, Y., Bell, R., and Volinsky, C. Matrix factorization techniques for recommender systems. Computer, 42(8): 30-37, 2009.

Kreutzer, J., Uyheng, J., and Riezler, S. Reliability and learnability of human bandit feedback for sequence-tosequence reinforcement learning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17771788,2018

Kwon, M., Biyik, E., Talati, A., Bhasin, K., Losey, D. P., and Sadigh, D. When humans aren't optimal: Robots that collaborate with risk-aware humans. In Proceedings of the 2020 ACM/IEEE international conference on humanrobot interaction, pp. 43-52, 2020.

Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/ alpaca_eval, 2023.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.

Peng, X. B., Kumar, A., Zhang, G., and Levine, S. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Peters, J. and Schaal, S. Reinforcement learning by rewardweighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pp. 745-750, 2007.

Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.

Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. $A d$ vances in Neural Information Processing Systems, 33: 3008-3021, 2020.

Sun, L., Zhan, W., Hu, Y., and Tomizuka, M. Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pp. 4329-4335. IEEE, 2019.

Tian, K., Mitchell, E., Yao, H., Manning, C. D., and Finn, C. Fine-tuning language models for factuality. arXiv preprint arXiv:2311.08401, 2023.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., Sarrazin, N., Sanseviero, O., Rush, A. M., and Wolf, T. Zephyr: Direct distillation of lm alignment, 2023.

Tversky, A. and Kahneman, D. Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and uncertainty, 5:297-323, 1992.

von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., and Huang, S. Trl: Transformer reinforcement learning. https://github. com/huggingface/trl, 2020.

Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J. Neural text generation with unlikelihood
training. In International Conference on Learning Representations, 2019.

Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.

Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.

Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.

Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.
