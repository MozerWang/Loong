# Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models 

Zhanhui Zhou*â€  Zhixuan Liu* Jie Liu Zhichen Dong Chao Yang Yu Qiao<br>* core contribution ${ }^{\dagger}$ corresponding author<br>Shanghai Artificial Intelligence Laboratory<br>asap.zzhou@gmail.com<br>Code: https://github.com/ZHZisZZ/weak-to-strong-search


#### Abstract

Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce weak-to-strong search, framing the alignment of a large language model as a test-time greedy search to maximize the log-likelihood difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (i) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (ii) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance. Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned gpt2s to effectively improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g., zephyr-7b-beta and its untuned version) can significantly improve the length-controlled win rates of both white-box and black-box large models against gpt-4-turbo (e.g., $34.4 \rightarrow 37.9$ for Llama-3-70B-Instruct and $16.0 \rightarrow 20.1$ for gpt-3.5-turbo-instruct), despite the small models' low win rates $\approx 10.0$.


## 1 Introduction

Learning-based algorithms $[1,2,3,4,5]$ have become the standard approach for aligning large language models (LLMs) with human preferences $[3,6,7,8,9,10]$. However, fine-tuning large language models is resource-intensive and difficult to implement [4]. These challenges have motivated recent studies on search-based algorithms that keep the large language models frozen and steer their decoding with test-time guidance $[11,12,13,14,15,16,17]$. Typical examples of search-based algorithms include rejection sampling [16, 17] and Monte Carlo Tree Search [18, 19]. These searchbased algorithms are promising as they can reuse the same learned guidance to steer the decoding of any large language model without additional training. However, existing search-based methods either simplify the search over tokens as a bandit problem [16, 17], which limits their steerability, or require a value function learned from scratch to address preference reward sparsity and reduce search depth $[13,18,14]$, which can be as difficult as fine-tuning a large language model.

To make search-based algorithms better suited for aligning large language models, we introduce weak-to-strong search, a simple algorithm that frames the alignment of a large model as a test-time search over the log-likelihoods of small language models. This algorithm makes two contributions: (1) First, it builds on the theoretical foundation of the token-level MDP for alignment [20], using

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-02.jpg?height=485&width=833&top_left_y=256&top_left_x=646)

Figure 1: Weak-to-strong search enhances the alignment of large models with test-time guidance from small models (dashed lines). This method is applicable to white-box models that use the same or different vocabularies as the small models, as well as to black-box models. We present the results for the instruction-tuned models from each family (e.g., Llama2-7B denotes Llama-2-7b-chat).

the log-likelihood difference between small tuned and untuned language models as both reward and critic $[4,20]$ to guide the decoding of a large model (Section 4.1). Theoretically, this formulation is suitable for search as it converts the otherwise sparse preference reward to a per-token dense reward, which can be summed up as a value function (critic) [20]. Practically, this formulation allows the reuse of off-the-shelf small tuned and untuned language model pairs as steering forces, avoiding the need to train a reward or critic model from scratch. (2) Second, it introduces a beam search variant, Chunk-level Beam Search (CBS), tailored for optimizing the proposed search objective. CBS guides the large language model towards high-reward regions by alternating between sampling from the frozen large model and expanding promising states as evaluated by the small tuned and untuned models (Section 4.2). Especially, when the small models are weaker than the large model, our method can be viewed as an instance of weak-to-strong generalization [21] that makes the strong model stronger with weak test-time guidance (Figure 1).

Empirically, we verify weak-to-strong search's flexibility in various tasks (Section 5). First, in controlled-sentiment generation [22] and summarization [2], our method uses small language models of 124M parameters (i.e., gpt2) to effectively steer much larger language models from the GPT-2 (e.g., gpt2-xl) [23], Llama-2 [7] and Llama-3 [24] families, at least as effective as existing methods. Then, in a more difficult instruction-following benchmark, AlpacaEval 2.0 [25], we show reusing off-the-shelf small models (e.g., zephyr-7b-beta and its untuned version) as test-time guidance can significantly improve the length-controlled win rates of both white-box and black-box large models against gpt-4-turbo (e.g, $34.4 \rightarrow 37.9$ for Llama-3-70B-Instruct, and 16.0 $\rightarrow 20.1$ for gpt-3.5-turbo-instruct), despite the small models' low win rates $\approx 10.0$ (Figure 1).

## 2 Related Work

Large unsupervised language models trained on internet-scale corpus acquire broad knowledge and abilities [26, 27, 28]. However, these large pre-trained language models may not always align with human values. To instill the desired behaviors into language models, most existing methods fine-tune these pre-trained language models on human comparisons of model-generated responses $[1,2,3$, $4,7,24,6,29]$. Despite these successes, fine-tuning a large language model requires substantial computational resources and engineering effort. These problems are compounded by the reality that different humans have different values $[3,30,13,31,32,33]$, as it is nearly impossible to train a new large language model from scratch for individual preference. In light of these issues, our work takes a search-based approach, folding as much of the complexity of alignment as possible into the decoding phase. This allows us to keep the large pre-trained language models frozen, steering their outputs at test time with only small models that are easier to obtain.

Framing alignment as a test-time search to maximize a reward function is not a novel formulation. However, most existing works either simplify autoregressive decoding as a bandit problem [16, 17], which limits their steerability, or require a value function learned from scratch to handle sparse
preference rewards and reduce search depth [13, 18], which can be as difficult as training a large language model from scratch. Our work avoids these issues by parametrizing the reward function with the log-likelihood difference between small tuned and untuned language models [4]. This parametrization not only simplifies the search objective, allowing a simple greedy search algorithm to generate good results, but also reuses off-the-shelf models as steering forces, eliminating the need to train a reward or critic model from scratch.

Concurrently with our work, Rafailov et al. [20] proposes a token-level MDP interpretation for language model alignment, demonstrating that a greedy likelihood search over a trained language model can achieve improvements over regular decoding. Our work builds on their theoretical foundations and proposes a novel greedy search algorithm designed for weak-to-strong guidance.

The idea of using small language models to align large language models has arisen in many recent works. The most related is proxy or emulated fine-tuning $[34,12,11,35]$, which uses the distributional difference of a small tuned and untuned model pair to modify the output distribution of a large model, approximating the output of the directly tuned large model. However, these methods require that both small and large models share the same vocabulary, limiting their practical applications. In contrast, our approach does not modify the sampling distribution of the large model at the token level. Instead, we perform a tree search that periodically selects the most promising states for further expansion (as evaluated by the small models) while sampling from the frozen large model's distribution. Thus our approach does not require shared vocabulary and is applicable to black-box language models.

## 3 Preliminaries

In this section, we introduce the mathematical formulation of alignment (Section 3.1) and describe the duality between language models and reward functions (Section 3.2).

### 3.1 Aligning Language Models with Human Preferences

The alignment of language models is typically cast as a KL-constrained optimization problem [1]:

$$
\begin{align*}
\underset{\pi}{\arg \max } & \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x}), \mathbf{y} \sim \pi(\mathbf{y} \mid \mathbf{x})}[r(\mathbf{x}, \mathbf{y})]  \tag{1a}\\
\text { s.t. } & \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x})}\left[\mathbb{D}_{\mathrm{KL}}\left(\pi(\mathbf{y} \mid \mathbf{x}) \| \pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x})\right)\right] \leq \epsilon \tag{1b}
\end{align*}
$$

where $p(\mathbf{x})$ is a distribution of prompts, $\mathbf{y}$ is the complete language model response, $r$ is a reward function that encourages human-preferred responses, and $\mathbb{D}_{\mathrm{KL}}$ limits how far the optimized language model $\pi$ can deviate from the reference (untuned) model $\pi_{\text {ref. }}$. There are two main categories of alignment algorithms: (1) search-based algorithms that optimize Eq. 1 with graph-based search during inference $[16,13,18,19,11,12]$, and (2) learning-based algorithms that optimize Eq. 1 through gradient descent, aiming for a parametrized optimal language model $[1,36,4,37]$. Our work falls in the first category, proposing a search-based algorithm capable of using small language models to guide the decoding of a large language model to align with human preferences.

### 3.2 Duality between Language Models and Reward Functions

The analytical solution to Eq. 1 can be obtained through the following Lagrangian [38, 39]:

$$
\begin{equation*}
\mathcal{L}(\pi, \beta)=\mathbb{E}_{\mathbf{x} \sim p(\mathbf{x}), \mathbf{y} \sim \pi(\mathbf{y} \mid \mathbf{x})}\left[r(\mathbf{x}, \mathbf{y})+\beta\left(\epsilon-\mathbb{D}_{\mathrm{KL}}\left(\pi(\mathbf{y} \mid \mathbf{x}) \| \pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x})\right)\right)\right] \tag{2}
\end{equation*}
$$

which has a well-known closed-form solution that expresses a duality between the reward function $r(\mathbf{x}, \mathbf{y})$ and the optimal language model $\pi^{*}(\mathbf{y} \mid \mathbf{x})$ [40, 41]:

$$
\begin{equation*}
r(\mathbf{x}, \mathbf{y})=\beta \log \frac{\pi^{*}(\mathbf{y} \mid \mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x})}+\beta \log Z(\mathbf{x}) \tag{3}
\end{equation*}
$$

where $Z(\mathbf{x})=\sum_{\mathbf{y}} \pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x}) \exp \left(\frac{1}{\beta} r(\mathbf{x}, \mathbf{y})\right)$ denotes the partition function. One takeaway from this duality is that we can always express a reward function using tuned and untuned language models: (1) If a reward function is given $[1,2,3]$, we can first obtain the optimally tuned language model under this reward function with any learning-based algorithms, and then use the tuned and untuned models $\left(\pi^{*}, \pi_{\text {ref }}\right)$ to reparametrize the reward function [42] (Eq. 3); (2) If a dataset is given from which the reward function can be derived, we can then directly parametrize the reward function with the tuned and untuned language models $\left(\pi^{*}, \pi_{\text {ref }}\right)$ during reward modeling [4] (Eq. 3).

## 4 Weak-to-Strong Search

In this section, we introduce weak-to-strong search, a search-based algorithm that aligns a large language model by searching over the log-likelihood difference between small tuned and untuned language models. First, we discuss how using language models to (re)parametrize the reward function from Eq. 1 makes the challenging objective solvable by a simple greedy search algorithm (e.g., beam search) (Section 4.1). Then, we introduce a practical beam search variant, called Chunk-level Beam Search (CBS) (Section 4.2), for optimizing the proposed objective, which is applicable to steering both white-box and black-box large language models.

### 4.1 Language Models as Both Reward and Critic

One practical challenge for search-based alignment algorithms is the sparsity of the preference reward signal. The preference reward function $r(\mathbf{x}, \mathbf{y})$, based on the Bradley-Terry model [43], only emits a terminal reward when the model response is complete. Search-based algorithms often struggle without any intermediate rewards or a critic model predicting future returns [44, 45]. However, if we parameterize this sparse reward function with language models (Section 3.2), we can obtain both a dense reward function and a critic function simultaneously.

Language models are a dense reward function. To obtain a dense reward function, we leverage the duality between the sparse preference reward and the dense language model probability (Eq. 3). By explicitly factorizing the log-likelihood of a complete response $\mathbf{y}$ under the language models, we can obtain a sum-of-rewards style formulation for Eq. 3:

$$
\begin{equation*}
r(\mathbf{x}, \mathbf{y})=\beta\left(\sum_{t=1}^{|\mathbf{y}|} \log \frac{\pi^{*}\left(y_{t} \mid \mathbf{x}, \mathbf{y}_{<t}\right)}{\pi_{\mathrm{ref}}\left(y_{t} \mid \mathbf{x}, \mathbf{y}_{<t}\right)}\right)+\beta \log Z(\mathbf{x}) \tag{4}
\end{equation*}
$$

where $\mathbf{y}_{<t}$ denotes the response tokens from 1 to $t-1$, and the last response token $y_{|\mathbf{y}|}$ is always the EOS token. Combining Eq. 1 and 4, we can rewrite the original objective with a per-token reward:

$$
\begin{align*}
& \underset{\pi}{\arg \max } \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x}), \mathbf{y} \sim \pi(\mathbf{y} \mid \mathbf{x})}\left[\sum_{t=1}^{|\mathbf{y}|} \log \frac{\pi^{*}\left(y_{t} \mid \mathbf{x}, \mathbf{y}_{<t}\right)}{\pi_{\text {ref }}\left(y_{t} \mid \mathbf{x}, \mathbf{y}<t\right)}\right]  \tag{5a}\\
& \text { s.t. } \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x})}\left[\mathbb{D}_{\mathrm{KL}}\left(\pi(\mathbf{y} \mid \mathbf{x}) \| \pi_{\text {base }}(\mathbf{y} \mid \mathbf{x})\right)\right] \leq \epsilon  \tag{5b}\\
& \hline
\end{align*}
$$

where $\beta$ and $Z(\mathbf{x})$ are omitted as they do not influence the optimal solution. It is important to note that the reference model that parametrizes the reward function $\left(\pi_{\mathrm{ref}}\right)$ (Eq. 5a) and the reference model that constrains the test-time search space ( $\left.\pi_{\text {base }}\right)$ (Eq. 5b) can be different. Practically, decoupling the reference models is useful as it allows using a tuned and untuned language model pair namely $\left(\pi^{*}, \pi_{\text {ref }}\right)$ - to steer the decoding of any language model $\pi_{\text {base }}$ without retraining.

Setting aside the KL-constraint (Eq. 5b) for now (detailed in Section 4.2), we can apply existing search algorithms like beam search [45, 20] to optimize Eq. 5a. However, it is tempting to argue that beam search is prone to local optima [45], as it greedily retains promising states midway through generation $\left(\mathbf{x}, \mathbf{y}^{\prime}\right)\left(\mathbf{y}^{\prime} \text { is incomplete }\right)^{1}$ based on partial return $\log \pi^{*}\left(\mathbf{y}^{\prime} \mid \mathbf{x}\right)-\log \pi_{\mathrm{ref}}\left(\mathbf{y}^{\prime} \mid \mathbf{x}\right)$, but high partial return may tell little about the overall return. Although this argument holds for most MDPs, appealing to the token-level MDP framework [20], we argue that the opposite is true here:

Partial return under language models is an implicit $\mathrm{V}$-value function (critic) in the original KL-constrained sparse reward setting. In Appendix A, we show that (inspired heavily by [20])

$$
\log \frac{\pi^{*}\left(\mathbf{y}^{\prime} \mid \mathbf{x}\right)}{\pi_{\text {ref }}\left(\mathbf{y}^{\prime} \mid \mathbf{x}\right)} \propto \begin{cases}-V^{*}(\mathbf{x})+V^{*}\left(\mathbf{x}, \mathbf{y}^{\prime}\right), & \text { if } y_{\left|\mathbf{y}^{\prime}\right|}^{\prime} \neq \operatorname{EOS}\left(\mathbf{y}^{\prime} \text { is incomplete }\right)  \tag{6}\\ -V^{*}(\mathbf{x})+r\left(\mathbf{x}, \mathbf{y}^{\prime}\right), & \text { if } y_{\left|\mathbf{y}^{\prime}\right|}^{\prime}=\operatorname{EOS}\left(\mathbf{y}^{\prime} \text { is complete }\right)\end{cases}
$$

where $V^{*}\left(\mathbf{x}, \mathbf{y}^{\prime}\right)$ denotes the value function, predicting the expected terminal reward under the optimal $\pi^{*}$ in the original KL-constrained sparse reward setting. Although $V^{*}\left(\mathbf{x}, \mathbf{y}^{\prime}\right)$ is not necessarily achievable by the searched policy, it approximates how good the state $\left(\mathbf{x}, \mathbf{y}^{\prime}\right)$ is in the long run. In other words, continuing from the state $\left(\mathbf{x}, \mathbf{y}^{\prime}\right)$ of high partial return $\log \pi^{*}\left(\mathbf{y}^{\prime} \mid \mathbf{x}\right)-\log \pi_{\text {ref }}\left(\mathbf{y}^{\prime} \mid \mathbf{x}\right)$ is likely to generate a complete response $\mathbf{y}$ with high overall return $\log \pi^{*}(\mathbf{y} \mid \mathbf{x})-\log \pi_{\text {ref }}(\mathbf{y} \mid \mathbf{x})$.[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-05.jpg?height=480&width=1222&top_left_y=362&top_left_x=430)

Figure 2: Illustration of Chunk-level Beam Search with $W, K=2,2$.

### 4.2 Chunk-level Beam Search (CBS)

After analyzing the feasibility of optimizing Eq. 5a with greedy search algorithms (e.g., beam search), we introduce a practical beam search variant that optimizes the dense reward objective (Eq. 5a) while ensuring the KL-constraint from $\pi_{\text {base }}$ (Eq. 5b).

The core algorithm providing the foundation of our method, Chunk-level Beam Search (CBS), is detailed in Algorithm 1 and illustrated in Figure 2. The key insight is that our beam search operates at the level of chunk. The search starts at the prompt and always maintains a hypothesis set $\mathcal{H}=\left\{\left(\mathbf{x}, \mathbf{y}^{\prime}\right)_{i}\right\}_{i=1}^{W}$ of $W$ states. For each state $\left(\mathbf{x}, \mathbf{y}^{\prime}\right)$ in $\mathcal{H}$, CBS samples $K$ continuation chunks $\mathbf{y}_{L}$ of length $L$ from $\pi_{\text {base }}$. This results in $W \cdot K$ successor states. Among these successors, only the top- $W$ successors with the highest partial return $\log \pi^{*}\left(\mathbf{y}^{\prime} \circ \mathbf{y}_{L} \mid \mathbf{x}\right)-\log \pi_{\text {ref }}\left(\mathbf{y}^{\prime} \circ \mathbf{y}_{L} \mid \mathbf{x}\right)$ are stored in $\mathcal{H}$ and expanded further. Finally, the terminal state $(\mathbf{x}, \mathbf{y})$ with the highest overall return $\log \pi^{*}(\mathbf{y} \mid \mathbf{x})-\log \pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x})$ is selected, from which the complete response $\mathbf{y}$ is extracted.

```
Algorithm 1 Chunk-level Beam Search (CBS)
    Input: prompt $\mathbf{x}$, beam width $W$, successors per state $K$, chunk length $L$,
            model to steer $\pi_{\text {base }}$, tuned model $\pi^{*}$, and untuned model $\pi_{\text {ref }}$.
    Output: optimal terminal state $(\mathbf{x}, \mathbf{y})$
    Initialize $\mathcal{H}=\left\{\left(\mathbf{x}, \mathbf{y}^{\prime}=\varnothing\right)_{i}\right\}_{i=1}^{W}$
    while $\exists\left(\mathbf{x}, \mathbf{y}^{\prime}\right) \in \mathcal{H}$ such that $\mathbf{y}^{\prime}$ is incomplete do
        Initialize $\mathcal{C}=\{\}$
        for each $\left(\mathbf{x}, \mathbf{y}^{\prime}\right) \in \mathcal{H}$ do
            $\mathcal{Y} \leftarrow\left\{\left(\mathbf{y}_{L}\right)_{i}\right\}_{i=1}^{K} \stackrel{\text { i.i.d. }}{\sim} \pi_{\text {base }}\left(\cdot \mid \mathbf{x}, \mathbf{y}^{\prime}\right) \quad / / \mathbf{y}_{L}=\varnothing$ if $\mathbf{y}^{\prime}$ is complete
            $\mathcal{C} \leftarrow \mathcal{C} \cup\left\{\left(\mathbf{x}, \mathbf{y}^{\prime} \circ \mathbf{y}_{L}\right) \mid \mathbf{y}_{L} \in \mathcal{Y}\right\}$
        end for
        $\mathcal{H} \leftarrow \operatorname{Top}-W_{\left(\mathbf{x}, \mathbf{y}^{\prime} \circ \mathbf{y}_{L}\right) \in \mathcal{C}}\left(\log \pi^{*}\left(\mathbf{y}^{\prime} \circ \mathbf{y}_{L} \mid \mathbf{x}\right)-\log \pi_{\mathrm{ref}}\left(\mathbf{y}^{\prime} \circ \mathbf{y}_{L} \mid \mathbf{x}\right)\right)$
    end while
    return arg $\max _{(\mathbf{x}, \mathbf{y}) \in \mathcal{H}}\left(\log \pi^{*}(\mathbf{y} \mid \mathbf{x})-\log \pi_{\text {ref }}(\mathbf{y} \mid \mathbf{x})\right)$
```

Notably, CBS is a unified framework that encompasses several search-based algorithms: (1) CBS with $W=1, K=N, L=\infty$, is equivalent to BoN sampling with $\log \pi^{*}(\mathbf{y} \mid \mathbf{x})-\log \pi_{\text {ref }}(\mathbf{y} \mid \mathbf{x})$ as the scoring function. (2) CBS with $K=\infty, L=1$ (directly inspecting the log-likelihoods of all possible next tokens from the vocabulary) is equivalent to vanilla token-level beam search.

However, we always ensure finite chunk length and limited successor exploration via sampling (even when $L=1$ ) to achieve the best of both worlds: (1) Using a finite chunk length allows CBS to discard bad states earlier and focus computational resources on expanding promising states,
enhancing steerability more efficiently compared to BoN. (2) Sampling from $\pi_{\text {base }}$ with limited successor exploration implicitly enforces the KL-constraint from $\pi_{\text {base }}$ (Eq. 5b). Without this limit, integrating the KL-constraint into the objective (Eq. 5a) as in Eq. 2 would be necessary, but this can be challenging, especially when vocabularies of $\pi_{\text {base }}$ and $\left(\pi^{*}, \pi_{\text {ref }}\right)$ differ or with black-box language models $\pi_{\text {base }}$ whose log-likelihoods are inaccessible. In addition, it can be beneficial to keep more than one promising state $W>1$ because the partial return only approximately correlates with the overall return (Section 4.1). While the partial return provides useful long-term guidance, we can still not strictly guarantee achieving the global optima by following the best local estimates.

Discussions on computation costs and steerability-KL tradeoff. In practice, CBS samples $W \cdot K$ continuation chunks in parallel and evaluates new states by calling $\left(\pi^{*}, \pi_{\text {base }}\right)$ every $L$ tokens. Larger $W \cdot K$ and smaller $L$ enhance steerability at the cost of increased computations, whereas smaller $W \cdot K$ and larger $L$ sacrifice steerability for more efficient computations. Note that high steerability, while beneficial, is not always ideal as it may lead to large KL deviation and over-optimization [16].

### 4.3 Application: Model Up-Scaling and Weak-to-Strong Generalization

The most practical use of CBS occurs when the tuned and untuned models, $\left(\pi^{*}, \pi_{\text {ref }}\right)$, are smaller than the model to steer, $\pi_{\text {base }}$. (1) First, this instance serves as a model up-scaling strategy, directly tuning a small model $\pi_{\mathrm{ref}} \rightarrow \pi^{*}$, by which the large model decoding can then be guided, to achieve similar outcomes as directly tuning the large model. (2) Second, since the small models $\left(\pi^{*}, \pi_{\text {ref }}\right)$ are usually weaker than the large model to steer $\pi_{\text {base }}$, this instance also exemplifies weak-to-strong generalization [21], enhancing the strong model with only weak test-time guidance. We refer to this instance of CBS as weak-to-strong search, which is the main focus of our study.

## 5 Experiments

In this section, we empirically evaluate weak-to-strong search's ability to align large language models using only test-time guidance from small language models. First, in controlled-sentiment generation [22] and summarization [2], we tune gpt2 to model the desired behaviors in each task and then use tuned and untuned gpt2 to steer larger models of various scales (Section 5.1). Next, in a more difficult instruction-following benchmark, AlpacaEval 2.0 [25], instead of tunning small models, we reuse off-the-shelf open-source 7B models and their untuned versions to steer a series of large models, including open-source 70B models and a black-box model (Section 5.2).

Baselines. In addition to weak-to-strong search, we evaluate several existing test-time approaches that steer large language models $\pi_{\text {base }}$ using small tuned and untuned language models $\left(\pi^{*}, \pi_{\text {ref }}\right):(1)$ Base: we explore regular decoding from the frozen large language model with $n$-shot prompting (see Appendix C.1.6 for prompt details). (2) Best-of-N Sampling (BoN) [16, 17]: BoN uses $r=\log \pi^{*}(\mathbf{y} \mid \mathbf{x})-\log \pi_{\text {ref }}(\mathbf{y} \mid \mathbf{x})$ to select the highest-scoring responses among the $N$ independent responses from the frozen large language model. Since weak-to-strong search (CBS) samples $W \cdot K$ response chunks in parallel, for fair computational comparisons, we always ensure $N=W \cdot K$. (3) Emulated Fine-Tuning (EFT) [34, 12, 11, 35]: EFT approximates the results of directly finetuning the large language model by sampling from $\log \pi_{\mathrm{EFT}}\left(y_{t} \mid \mathbf{x}, \mathbf{y}_{<t}\right) \propto \log \pi_{\text {base }}\left(y_{t} \mid \mathbf{x}, \mathbf{y}<t\right)+$ $\beta^{-1}\left(\log \pi^{*}\left(y_{t} \mid \mathbf{x}, \mathbf{y}_{<t}\right)-\log \pi_{\mathrm{ref}}\left(y_{t} \mid \mathbf{x}, \mathbf{y}_{<t}\right)\right)$, where $\beta$ is the hyperparameter from Eq. 2. Note that EFT is only applicable when all models share the same vocabulary (which is necessary for composing output distributions from different models). Whenever possible, we also compare test-time methods against directly fine-tuning the large models in the same way small models are tuned.

### 5.1 Controlled-Sentiment Generation \& Summarization

Setup. For these two tasks, we follow the synthetic setups from [16, 46, 4], assuming access to a gold reward model $r_{\text {gold }}$. For controlled-sentiment generation, $r_{\text {gold }}$ encourages positive continuations of movie reviews, while for summarization, it encourages high-quality summaries of Reddit posts (details in Appendix C.1.4). We generate synthetic preference datasets $\mathcal{D}=\left\{\left(\mathbf{x}, \mathbf{y}_{w}, \mathbf{y}_{l}\right)_{i}\right\}_{i=1}^{N}$ from $r_{\text {gold }}$ with $p\left(\mathbf{y}_{1} \succ \mathbf{y}_{2} \mid \mathbf{x}\right)=\sigma\left(r_{\text {gold }}\left(\mathbf{x}, \mathbf{y}_{1}\right)-r_{\text {gold }}\left(\mathbf{x}, \mathbf{y}_{2}\right)\right)$ to mimic human feedback [47].

To obtain the small language models, we optimize gpt2 (124M parameters) using the standard DPO pipeline [4]: (1) we first obtain the reference model $\pi_{\text {ref }}$ through supervised fine-tuning on both

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-07.jpg?height=623&width=1374&top_left_y=239&top_left_x=365)

Controlled-Sentiment Generation

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-07.jpg?height=572&width=683&top_left_y=283&top_left_x=382)

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-07.jpg?height=596&width=661&top_left_y=255&top_left_x=1060)

Figure 3: The gold reward achieved for different large pre-trained models under the gpt2 guidance. We show the mean reward ( $\pm 2$ standard deviations) across three random seeds. EFT ( $\beta^{*}$ ) denotes the best EFT results among $\beta \in\{1 / 4,1 / 2,1,2,4\}$; Weak-to-strong search $(4,4,5)$ denotes CBS with $W, K, L=4,4,5 ; \operatorname{BoN}$ (16) denotes BoN with $N=16$.

chosen and rejected responses from the synthetic preference dataset, then (2) we apply DPO on the synthetic preference dataset with $\pi_{\text {ref }}$ as the reference policy to obtain the optimal language model $\pi^{*}$. Note that the first stage primarily informs the language model of the desired response format, with most of the tuning occurring in the second DPO stage.

Given the tuned and untuned (un-DPO-tuned) gpt2 pair $\left(\pi^{*}, \pi_{\text {ref }}\right)$, we use them to steer the large pre-trained language models without additional training. The large pre-trained language models we study fall into two categories based on whether they share the same vocabulary as the small models: (1) same vocabulary: gpt2-large (774M), gpt2-xl (1.5B) and (2) cross vocabulary: Llama-2-7b, Llama-3-8B. Eventually, since we have access to the gold reward model, language model responses can be fairly evaluated on the test split of prompts using this gold reward model.

Results. Figure 3 demonstrates weak-to-strong search's great flexibility and steerability in both tasks. For summarization, weak-to-strong search consistently outperforms other test-time methods by large margins. For controlled-sentiment generation, weak-to-strong search is second only to EFT with a carefully selected hyperparameter $\left(\beta^{*}=1 / 4\right)$ when EFT is applicable. We hypothesize that token-level adjustments from EFT are sufficient for controlled-sentiment generation, which primarily requires minor stylistic changes at the token level (e.g., "hate" $\rightarrow$ "love"). However, in the more complex task of summarization, where broader sequence-level manipulations are essential, weak-to-strong search excels. Please refer to Appendix E for quantitative comparisons of samples from different methods. We need to mention that we do not meaningfully tune weak-to-strong search (CBS)'s hyperparameters to obtain the results in Figure 3 (we use a fixed set of hyperparameters of $(4,4,5)$ for $W, K, L$ across all models), which may underestimate the performance of our method. In addition, our method enables consistent weak-to-strong generalization in the harder task of summarization: most large pre-trained models (except for gpt2-large) are stronger than the tuned gpt2 in summarizing long text, but the weak models are still able to improve the strong models through test-time guidance, nearly matching the results of direct fine-tuning. The phenomenon of weak-to-strong generalization will be further studied in Section 5.2.

Chunk-level Beam Search ablations. We perform ablations to understand how CBS hyperparameters (beam width $W$, successors per state $K$, and chunk length $L$ ) influence performance. Figure 4 displays the ablation results for $W, K$. With the same computation budget (i.e., $W \cdot K$ ), the optimal trade-off between $W$ and $K$ varies by tasks: for controlled-sentiment generation, the best results come from retaining the most promising state and concentrating computational efforts on expanding from it ( $W, K=1,16$ ); in contrast, for summarization, maintaining multiple hypotheses $(W, K=8,2)$ yields the best results by avoiding local optima. Figure 5 displays the ablation results for $L$ where smaller $L$ benefits controlled-sentiment generation, while an intermediate $L$ is optimal for summa-
rization. These results are consistent with our findings in Figure 3, suggesting that the simple nature of controlled-sentiment generation makes token-level manipulation sufficient and partial return a more reliable indicator of overall return. See Appendix D. 1 for extended ablations.

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-08.jpg?height=466&width=1374&top_left_y=415&top_left_x=365)

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-08.jpg?height=376&width=358&top_left_y=430&top_left_x=382)

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-08.jpg?height=371&width=312&top_left_y=430&top_left_x=730)

(a) controlled-sentiment generation

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-08.jpg?height=376&width=675&top_left_y=430&top_left_x=1061)

(b) summarization

Figure 4: $\mathbf{W}, \mathbf{K}$ ablations for $\mathbf{C B S}(\mathbf{L}=\mathbf{5})$. We show the mean rewards across three random seeds. With the same computation budget (i.e., same $W \cdot K$ ) the optimal hyperparameters differ by tasks.

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-08.jpg?height=501&width=1396&top_left_y=1007&top_left_x=359)

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-08.jpg?height=425&width=680&top_left_y=1018&top_left_x=365)

(a) controlled-sentiment generation

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-08.jpg?height=431&width=678&top_left_y=1015&top_left_x=1057)

(b) summarization

Figure 5: $\mathbf{L}$ ablations for $\mathbf{C B S}(\mathbf{W}, \mathbf{K}=\mathbf{4}, \mathbf{4}$ ). We show the mean rewards ( $\pm 2$ standard deviations).

### 5.2 Instruction Following

Setup. Next, we evaluate weak-to-strong search on a standard single-turn instruction-following benchmark, AlpacaEval 2.0 [25], which consists of 805 prompts from various open-source datasets. Unlike the previous section where we steer large pre-trained language models (e.g., Llama-2-7b), we now steer large instruction-tuned language models (e.g., Llama-2-7b-chat). This is because (1) instruction-tuned models often require further alignment to match human preferences [48], and (2) to study weak-to-strong generalization in instruction-following, the models must be proficient at following instructions before steering.

For small language models, we reuse two high-ranking 7B model pairs from the AlpacaEval 2.0 leaderboard as guidance: (1) Zephyr guidance: zephyr-7b-beta and its untuned version mistral-7b-sft-beta; (2) Tulu guidance: tulu-2-dpo-7b and its untuned version tulu-2-7b. All four models use the Llama-2 tokenizer. The large instruction-tuned language models we aim to further align fall into three categories: (1) same vocabulary: Llama-2-7b-chat, Llama-2-70b-chat; (2) cross vocabulary: Llama-3-8B-Instruct, Llama-3-70B-Instruct; (3) black box: gpt-3.5-turbo-instruct. As it is nearly impossible to reproduce the exact training pipeline for these small models ( $\pi_{\text {ref }} \rightarrow \pi^{*}$ ), we do not test the baseline results of directly fine-tuning the large models as in Figure 3. Language model responses are evaluated by their length-controlled win rates (LC WR) against gpt-4-turbo, with gpt-4-turbo serving as the judge.

Results. Experimental results with Zephyr and Tulu guidance are shown in Figure 6 (detailed hyperparameters in Appendix C.2.2). Weak-to-strong search consistently outperforms other test-time baselines with great margins. There are two crucial takeaways worth mentioning: (1) Weak-to-strong

![](https://cdn.mathpix.com/cropped/2024_06_04_2db8e7c095e553aab6acg-09.jpg?height=574&width=1372&top_left_y=236&top_left_x=363)

Figure 6: The length-controlled win rates against gpt-4-turbo for various instruction-tuned models under the Zephyr (left) or Tulu (right) guidance. Hyperparameters are in Appendix C.2.2.

search makes strong models stronger with only weak test-time guidance. Take Zephyr guidance for an example (Figure 6, left), even if most large instruction-tuned models $\pi_{\text {base }}$ are stronger than zephyr-7b-beta before steering, weak-to-strong search is still able to enhance their performances using weak models as guidance. Conversely, EFT and BoN mainly interpolate between weak and strong models, resulting in limited, if any, improvements over the strong models. We also tested beam search without external guidance [20] but we found no obvious improvements (Table 2), probably because the latent reward functions behind these language models are not well aligned with the human preference that gpt-4-turbo approximates. The same observations apply to Tulu guidance, even though the tuned tulu-2-dpo-7b is weaker than all the large instruction-tuned language models by significant margins (Figure 6, right). (2) Weak-to-strong search applies to black-box language models. Our method, requiring only sampling from large language models, is also effective for black-box models like gpt-3.5-turbo-instruct. For weak-to-strong search with gpt-3.5-turbo-instruct, we use a chunk length of 100, as the black-box APIs are stateless and do not retain activation caches, making repeated context embedding costly. Despite the long chunk length, our method effectively aligns black-box models, significantly outperforming BoN, a special case of weak-to-strong search (CBS) with infinite chunk length.

## 6 Discussion

We have presented weak-to-strong search, an alignment method that keeps the large language model frozen while steering its decoding through a test-time greedy search over small language models. This method builds on the insight that the log-likelihood difference between small tuned and untuned language models can serve both as a dense reward function and a critic, and introduces a novel beam search algorithm designed for balancing reward maximization and KL minimization. This method offers a compute-efficient model up-scaling strategy that eliminates the complexity of directly fine-tuning the large models, and exemplifies weak-to-strong generalization [21] that makes strong models stronger with only weak test-time guidance. Empirically, this approach is effective in controlled-sentiment generation, summarization, and instruction following.

Limitations \& Future Work. While our work focuses on aligning with human preferences, weakto-strong search could also apply to tasks like reasoning [49, 50] and coding [51], where ground truth answers exist. This is because any pair of tuned and untuned language models can act as test-time steering forces, without necessarily being trained on preferences. This then raises several questions beyond the scope of our current study: (1) In our study, we consistently use SFTed policy as the untuned model $\pi_{\text {ref }}$ due to the two-stage nature of preference learning. In general single-stage fine-tuning tasks, does weak-to-strong search still work with a pre-trained model serving as the untuned model $\pi_{\text {ref }}$ ? (2) Can weak-to-strong search enhance language models in tasks where ground truth answers exist, beyond merely tailoring their knowledge and skills to human preferences?

## References

[1] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.

[2] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.

[3] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.

[4] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.

[5] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 4447-4455. PMLR, 2024.

[6] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

[7] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[8] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[9] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, ClÃ©mentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of $1 \mathrm{~m}$ alignment. arXiv preprint arXiv:2310.16944, 2023.

[10] Andreas KÃ¶pf, Yannic Kilcher, Dimitri von RÃ¼tte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, RichÃ¡rd Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024.

[11] Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D Manning. An emulator for fine-tuning large language models using small language models. arXiv preprint arXiv:2310.12962, 2023.

[12] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A Smith. Tuning language models by proxy. arXiv preprint arXiv:2401.08565, 2024.

[13] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, et al. Controlled decoding from language models. arXiv preprint arXiv:2310.17022, 2023.

[14] Minbeom Kim, Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, and Kyomin Jung. Critic-guided decoding for controlled text generation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4598-4612, 2023.

[15] James Y Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchoff, and Dan Roth. Deal: Decoding-time alignment for large language models. arXiv preprint arXiv:2402.06147, 2024.

[16] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835-10866. PMLR, 2023.

[17] Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander Dâ€™Amour, Jacob Eisenstein, Chirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy. arXiv preprint arXiv:2401.01879, 2024.

[18] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Making ppo even better: Value-guided monte-carlo tree search decoding. arXiv preprint arXiv:2309.15028, 2023.

[19] Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. Alphazerolike tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023.

[20] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From $r$ to $q^{*}$ : Your language model is secretly a q-function. arXiv preprint arXiv:2404.12358, 2024.

[21] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.

[22] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.

[23] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.

[24] AI@ Meta. Llama 3 model card. 2024.

[25] Yann Dubois, BalÃ¡zs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.

[26] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113,2023 .

[27] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[29] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

[30] Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. Beyond one-preference-for-all: Multi-objective direct preference optimization. arXiv preprint arXiv:2310.03708, 2023.

[31] Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Advances in Neural Information Processing Systems, 36, 2024.

[32] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. arXiv preprint arXiv:2310.11564, 2023.

[33] Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. arXiv preprint arXiv:2402.18571, 2024.

[34] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, and Yejin Choi. Dexperts: Decoding-time controlled text generation with experts and anti-experts. arXiv preprint arXiv:2105.03023, 2021.

[35] Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, and Yu Qiao. Emulated disalignment: Safety alignment for large language models may backfire! arXiv preprint arXiv:2402.12343, 2024.

[36] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slichf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023 .

[37] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.

[38] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.

[39] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.

[40] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.

[41] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018.

[42] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.

[43] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.

[44] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016 .

[45] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:12731286,2021

[46] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.

[47] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.

[48] Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. Aligner: Achieving efficient alignment through weak-to-strong correction. arXiv preprint arXiv:2402.02416, 2024.

[49] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.

[50] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, $36,2024$.

[51] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

[52] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

[53] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.

[54] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023.

[55] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023.

[56] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness \& harmlessness with rlaif, November 2023.
