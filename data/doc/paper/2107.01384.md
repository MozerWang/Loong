# ATC: an Advanced Tucker Compression library for multidimensional data 

WOUTER BAERT, KU Leuven, Belgium<br>NICK VANNIEUWENHOVEN, KU Leuven, Belgium


#### Abstract

We present ATC, a C++ library for advanced Tucker-based lossy compression of dense multidimensional numerical data in a sharedmemory parallel setting, based on the sequentially truncated higher-order singular value decomposition (ST-HOSVD) and bit plane truncation. Several techniques are proposed to improve speed, memory usage, error control and compression rate. First, a hybrid truncation scheme is described which combines Tucker rank truncation and TTHRESH quantization [Ballester-Ripoll et al., IEEE Trans. Visual. Comput. Graph., 2020]. We derive a novel expression to approximate the error of truncated Tucker decompositions in the case of core and factor perturbations. Furthermore, we parallelize the quantization and encoding scheme and adjust this phase to improve error control. Moreover, implementation aspects are described, such as an ST-HOSVD procedure using only a single transposition. We also discuss several usability features of ATC, including the presence of multiple interfaces, extensive data type support and integrated downsampling of the decompressed data. Numerical results show that ATC maintains state-of-the-art Tucker compression rates, while providing average speed-up factors of 2.2-3.5 and halving memory usage. Furthermore, our compressor provides precise error control, only deviating $1.4 \%$ from the requested error on average. Finally, ATC often achieves higher compression than non-Tucker-based compressors in the high-error domain.


CCS Concepts: $\cdot$ Theory of computation $\rightarrow$ Data compression; $\cdot$ Mathematics of computing $\rightarrow$ Mathematical software performance; $\cdot$ Computing methodologies $\rightarrow$ Linear algebra algorithms; $\cdot$ Human-centered computing $\rightarrow$ Scientific visualization; $\bullet$ Applied computing $\rightarrow$ Physical sciences and engineering.

Additional Key Words and Phrases: Data compression, tensors, Tucker decomposition, ST-HOSVD, bit plane truncation.

ACM Reference Format:

Wouter Baert and Nick Vannieuwenhoven. TBD. ATC: an Advanced Tucker Compression library for multidimensional data. 1, 1 (February TBD), 25 pages. https://doi.org/TBD

## 1 INTRODUCTION

Many scientific, industrial and medical applications generate numerical data on multidimensional grids, such as hyperspectral imaging [3], diffusion tensor imaging [28], X-ray scans [27] and simulations of various kinds [9, 13, 23]. As these datasets increase in size, so does the need for compression to reduce network and storage costs. Specifically, when dealing with floating-point data, lossy compression is most applicable because there is often no need to store the data in full precision. In fact, the original data may already be subject to measurement, simulation and round-off errors of a certain magnitude, making storage beyond this level of accuracy redundant.

However, simply storing each data value within this limited precision is usually not the best approach. As shown in Fig. 1, realistic datasets often contain a lot of redundancy which can be removed during compression. In particular,[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-02.jpg?height=675&width=352&top_left_y=321&top_left_x=388)

(a) Slice 28

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-02.jpg?height=677&width=355&top_left_y=323&top_left_x=771)

(b) Slice 84

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-02.jpg?height=677&width=355&top_left_y=323&top_left_x=1145)

(c) Slice 140

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-02.jpg?height=681&width=360&top_left_y=321&top_left_x=1517)

(d) Slice 196

Fig. 1. A sample of spectral slices from the Moffett-Field hyperspectral image (indices are 0 -based). For visibility, a different scaling factor was used for each visualization, yet all slices clearly exhibit the same structure, showing great potential for compression.

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-02.jpg?height=404&width=350&top_left_y=1197&top_left_x=389)

Original

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-02.jpg?height=404&width=333&top_left_y=1197&top_left_x=774)

Relative error: $0.100 \%$ Compression factor: 389.28

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-02.jpg?height=404&width=346&top_left_y=1197&top_left_x=1147)

Relative error: $1.00 \%$ Compression factor: 3568.6

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-02.jpg?height=404&width=337&top_left_y=1197&top_left_x=1531)

Relative error: $10.5 \%$ Compression factor: 95034

Fig. 2. ATC compression examples using the Isotropic-PT dataset. Each visualization only shows the first time slice of the data tensor, while the statistics in the captions represent the full data.

when handling smooth datasets such as the one presented in Fig. 2, high compression factors can often be achieved at a small cost in compression error.

In this paper we propose ATC (the Advanced Tucker Compressor), a lossy compression library for multidimensional numerical data, based on the sequentially truncated higher-order singular value decomposition (ST-HOSVD) [49] and an existing bit-plane-truncation-based quantization and encoding scheme from the TTHRESH compressor [5] Our motivation is twofold: first, we aim to improve speed, memory usage, error control and compression rate where possible compared to existing Tucker-based [48] methods such as TTHRESH [5] and TuckerMPI [4]. Specifically, we are interested in the compression of general, dense tensors, such as grid-based physical measurements or simulation data. Second, our objective is to create a Tucker-based compression library with state-of-the-art performance and usability Manuscript submitted to ACM

| Compressor | Rel. <br> comp. | Speed-up |  | Peak memory (bytes/element) |  |  |  | {Error <br> control} |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Comp. | Decomp. |  |  |  | mp. |  |  |
| TTHRESH | - | - | - | 40.3 | - | 38.7 | - | $33.8 \%$ | - |
| Baseline ATC | $+0.8 \%$ | $+27 \%$ | $-0.8 \%$ | 19.1 | $-53 \%$ | 16.5 | ![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-03.jpg?height=34&width=80&top_left_y=437&top_left_x=1364) | $34.4 \%$ | $+1.7 \%$ |
| + Rank truncation | $-8.2 \%$ | $+54 \%$ | $+33 \%$ | 18.1 | $-5.0 \%$ | 16.5 | $+0.0 \%$ | $25.8 \%$ | $-25 \%$ |
| + Parallel quant./encoding | $-0.4 \%$ | $+77 \%$ | $+61 \%$ | 18.4 | $+1.5 \%$ | 16.8 | $+1.4 \%$ | $27.7 \%$ | $+7.0 \%$ |
| + Predict dequant. correction | $+4.0 \%$ | $-3.1 \%$ | $+1.5 \%$ | 18.4 | $-0.2 \%$ | 16.8 | $-0.1 \%$ | $1.4 \%$ | $-95 \%$ |
| + Householder compression | $+1.2 \%$ | $+5.7 \%$ | $+1.9 \%$ | 18.6 | $+1.2 \%$ | 16.7 | $-0.1 \%$ | $1.4 \%$ | $-1.1 \%$ |
| + Mode storage order heuristic | $+1.1 \%$ | $-0.2 \%$ | $+2.5 \%$ | 18.6 | $-0.1 \%$ | 16.9 | $+0.9 \%$ | $1.9 \%$ | $+35 \%$ |
| + Split bit plane truncation | $+3.0 \%$ | $-1.4 \%$ | $-1.8 \%$ | 18.6 | $-0.1 \%$ | 16.9 | $+0.0 \%$ | $1.4 \%$ | $-23 \%$ |
| Full ATC | $+1.1 \%$ | $+249 \%$ | $+121 \%$ | 18.6 | $-54 \%$ | 16.9 | $-56 \%$ | $1.4 \%$ | $-96 \%$ |
| TuckerMPI | $-90 \%$ | $+308 \%$ | $+135 \%$ | 10.8 | $-73 \%$ | 9.6 | $-75 \%$ | $86.2 \%$ | $+155 \%$ |

Table 1. Relative compression factors, compression and decompression times, peak memory usage per data element as well as deviations from the target error, averaged out over a range of compression errors and datasets. Error control is expressed as the average relative deviation of the actual compression error from the target error. Each boldface compressor shows relative performance compared to TTHRESH, while the other lines show incremental performance gains (green) or losses (red) achieved by a certain compressor optimization compared to the last line. For memory usage and error control, we also show the absolute performance metrics. Note that these metrics are averaged over a wide variety of settings, with certain optimizations having very different effects depending on the situation. The full methodology used is described in Section 6.

features such as multiple interfaces, broad support for data types, shared-memory parallelism and both file-based and in-memory compression or decompression.

In our view, these broad features are not yet available in existing tensor-based compressors. For example, TTHRESH lacks most of these features as well as a programming interface and can therefore be used only as an executable instead of a library. On the other hand, while TuckerMPI supports distributed-memory parallelism, it is solely aimed at computing Tucker decompositions and as such achieves much worse compression rates, due to a lack of quantization and encoding methods; see Table 1.

Table 1 summarizes the performance gains achieved by the ATC optimizations compared to the other Tucker-based compressors TTHRESH and TuckerMPI. Thanks to an efficient implementation, e.g., avoiding unnecessary copies, baseline ATC already achieves a roughly $27 \%$ compression speed-up and roughly halves memory usage reduction compared to TTHRESH, while being quite similar. Furthermore, the various algorithmic improvements made by ATC accumulate into an average compression and decompression speed-up factor of 3.5 and 2.2 respectively as well as 24 times more precise error control, while maintaining similar compression rates. Moreover, the user can make certain trade-offs, for example increasing compression while lowering speed by reducing the level of rank truncation. In summary, this shows the significant improvements made by ATC over existing work, both algorithmically and in terms of implementation.

In this paper, we will discuss the following aspects. First, we describe several general compression techniques and alternative compressors (Section 2) as well as preliminary material that will be used throughout the paper (Section 3). Then, we elaborate on our main algorithmic contributions (Section 4). Afterwards, we summarize specific implementation aspects of the software (Section 5). Finally, numerical results will be presented and discussed (Section 6), followed by concluding remarks and potential future lines of research (Section 7). In the supplementary materials, we discuss additional minor improvements and features.

## 2 RELATED WORK

In the past decades, a variety of methods were invented to address compression needs in various application domains [41]. In this section we discuss the most relevant related methods.

### 2.1 Compression basics

In data compression, methods are often divided into two categories: lossless compression, which compresses data while ensuring that it can be reconstructed exactly, and lossy compression, which introduces a small compression error in order to achieve much higher compression rates or compression factors, defined here as $\frac{\text { original data size }}{\text { compressed data size }}$. Since our paper is concerned with lossy compression, we will often express this error using one of the following metrics:

- The relative error (RE) $\frac{\|\mathcal{A}-\widetilde{\mathcal{A}}\|}{\|\mathcal{A}\|}$, where $\|\cdot\|$ denotes the Euclidean norm (see Section 3.1)
- The sum-of-squared-errors (SSE) $\|\mathcal{A}-\widetilde{\mathcal{A}}\|^{2}$.

When processing numerical data, the floating-point format complicates efficient lossy compression. For example, the sign bit or exponent bits of a floating-point number will have a much larger influence on the represented number than the least significant bits of the mantissa. Therefore quantization is required, where each number is approximated within some discrete domain that is easier to represent in a compressed format. These quantized values can then be encoded, i.e., stored as a bit stream, using lossless compression techniques such as the ones described in Section 2.2.

### 2.2 Lossless compression

When no error on the original data can be tolerated, it can be compressed using lossless compression. Many techniques for this purpose were proposed [41], resulting in general lossless compressors such as zlib [20] and Zstandard [12]. Below, we briefly describe two techniques that are relevant to our own work. Lossless compression can also be used as a component of a lossy compression pipeline.

Run-length encoding. When storing a sequence of identical symbols, it is more frugal to represent it as a tuple containing this symbol and the length of the sequence, i.e. its run-length. For example, this technique is used in JPEG (Joint Photographic Experts Group) [8] image compression to compress sequences of zero coefficients.

Entropy coding. In information theory, the entropy of an information source represented by a discrete probability distribution $X$ consisting of the probabilities $p_{1}, \ldots, p_{n}$, is defined as $H(X)=-\sum_{i} p_{i} \log _{2} p_{i}$ [43]. Furthermore, Shannon's source coding theorem implies that, when encoding a stream of symbols identically and independently distributed according to $X$, one needs at least $H(X)$ bits per symbol on average [43]. While this sets an upper limit to compression efficiency within this setting, it does not provide us with a constructive algorithm to approach this bound.

One approach to entropy coding consists of representing each symbol as a particular sequence of bits, its code word, and concatenating all code words corresponding to the symbols from the original stream. To allow for unambiguous decoding, the code must be a prefix code, i.e., each code word of length $l$ must be different from the first $l$ bits of any other code word. Huffman coding [25] provides a simple algorithm to produce an optimal prefix code tree and has become widely used since its invention [41]. However, all prefix-code-based methods suffer from the constraint that each code word must consist of an integer number of bits. In contrast, the entropy of a particular symbol, $-\log _{2} p_{i}$, is almost always non-integer. Therefore, this discrepancy decreases the compression efficiency of such methods.

Arithmetic coding [40] solves this issue by representing all symbols in the stream combined as an interval bounded by two long fractions. The compressed output then consists of the binary representation of an arbitrary number within Manuscript submitted to ACM
this interval. Since certain bits in this sequence contain information on more than one symbol, the aforementioned limitation of prefix codes is avoided. In fact, when the number of encoded symbols goes to infinity, the theoretical compression efficiency approaches the entropy limit [40].

More recently, asymmetric numeral systems [15] were proposed as a faster alternative to arithmetic coding, while maintaining optimal theoretical compression efficiency. Because of this speed advantage, asymmetric numeral systems have become widely used in state-of-the-art compressors [12, 21, 26].

### 2.3 Lossy compression

When the original data does not need to be reconstructed exactly, lossy compression can be used. This is especially relevant for data that is intended for human interpretation, such as images, sound and video. Because of this, many lossy compressors choose which information to discard based on a model of human perception [8, 35]. Alternative error metrics such as the structural similarity index measure (SSIM) [51] and video multimethod assessment fusion (VMAF) [30] were proposed to address this. Nevertheless, in this work we will employ the usual Euclidean (or Frobenius) norm as our error metric, because it is simple and has useful properties with regard to multilinear algebra.

A first class of lossy compressors can be described as predictive, in the sense that values are predicted using e.g., neighboring values or a fitting function [16]. If the predictor performs well, the deviation of each value from its prediction should be small, which makes them easier to encode than the original values. Before encoding, these deviations are quantized in an appropriate way to achieve the desired level of accuracy. For our application, the compression of numerical multi-dimensional arrays, predictive-based compressors include SZ [31, 53, 54] and FPZIP [33].

Some compressors apply a certain invertible transformation to the data before quantization. By exploiting the structured nature of most relevant data, these transformations can significantly increase the sparsity of the coefficients in the transformed domain, improving compressibility. The discrete cosine transform (DCT) [1] is a widely used tool for this purpose [10], being used in various image and video compressors such as JPEG, x265 [35] and AV1 [2]. For our application, transforms are used in compressors such as ZFP [32], TTHRESH [5] and SSEM [42].

Transform-based methods usually rely on fixed transformations, which removes the need to store the transform but might lead to less efficient compression than if a data-dependent transform were used. For example, when compressing an $n \times n$ matrix $A$ with singular value decomposition (SVD) $A=U S V^{T}$, it can be interesting to use its bases of left and right singular vectors $U$ and $V$ as data-dependent transforms applied to the column and row spaces, respectively, since the Eckart-Young theorem [17] states that this provides us with the best low-rank approximation in terms of the Euclidean norm. However, even when truncating to rank $r$, the compressed representation would require $2 n r+r$ scalars to store the largest $r$ singular values and the $r$ corresponding columns of $U$ and $V$, compared to $r^{2}$ when using the same truncation rank for a fixed transform, such as a 2-dimensional DCT, due to the transform storage cost.

In the case of data with 3 or more modes, the orthogonal Tucker decomposition [48] reduces the relative storage overhead of the transformation by representing an $n_{1} \times n_{2} \times \cdots \times n_{d}$ tensor as a core tensor of the same size and orthogonal matrices $U_{i} \in \mathbb{R}^{n_{i} \times n_{i}}$. Like a truncated SVD, we can truncate the matrices $U_{i}$ to the first $r_{i}$ columns and retain only the $r_{1} \times r_{2} \times \cdots \times r_{d}$ core tensor corresponding to this selection of basis vectors. This reduces the storage cost to $r_{1} r_{2} \ldots r_{d}$ scalars for the transformed coefficients and $\sum_{i=1}^{d} n_{i} r_{i}$ scalars for the factor matrices. As such, the number of transformed coefficients grows exponentially in the number of modes $d$, while the transform storage cost only increases linearly with $d$. Due to this relatively low transform storage cost, the Tucker decomposition can still be competitive compared to methods using fixed transforms. In the field of tensor decompositions, this decomposition
is also commonly used for compression of tensors of moderate order [29, 38]. For example, the TuckerMPI software efficiently computes Tucker decompositions of massive datasets in a distributed-memory parallel setting.

Only few publications discuss the quantization and encoding of such decompositions [5, 6, 45, 46]. To our knowledge, the Tucker-based TTHRESH compressor employs the most advanced quantization scheme among these, using bit plane truncation, which will be discussed further in Section 3.3. We will use a variant of this strategy in ATC.

Tensor train [37] and hierarchical Tucker [22] decompositions are also used for compression of high-order tensors [38]. In our targeted application we are dealing with dense tensors, i.e., tensors which store each element explicitly, so the order of tensor is typically not high. Therefore, these alternative decompositions will not be discussed further.

Finally, a new category of low-rank tensor formats based on subpartitioning is also emerging [18, 34]. Many data compressors already split the input data into subblocks before compression $[8,31,32,52]$, not only to reduce the complexity of most transform-based methods, but also to improve compression when adaptively partitioning the data into blocks to diminish intra-block discontinuities. While it is therefore promising to apply this approach to tensor formats, this is out of scope for the current version of ATC.

## 3 PRELIMINARIES

### 3.1 Tensor concepts and notation

A tensor $\mathcal{A} \in \mathbb{R}^{n_{1} \times \cdots \times n_{d}}$ is represented in bases by a multidimensional array containing numerical values indexed with $d$ integers. $n_{1} \times \cdots \times n_{d}$ is called the size of the tensor, and $d$ is called the order. The Euclidean inner product of two tensors $\langle\mathcal{X}, \mathcal{Y}\rangle$ is the sum over their elementwise products. The induced Euclidean norm is $\|\mathcal{X}\|=\sqrt{\langle\mathcal{X}, \mathcal{X}\rangle}$.

We will use the Matlab notation $a: b$ as a tensor index to select a subrange along the corresponding mode from index $a$ up to and including index $b$, or simply : in case all indices are selected. For example, $U_{;, 1: r}$ refers to the submatrix containing all rows but only the first $r$ columns of $U$. A mode-k fiber of a tensor $\mathcal{X}$ is then defined as a vector obtained by fixing all indices in the tensor apart from the $k$-th index, i.e. $\mathcal{X}_{i_{1}, \ldots, i_{k-1}, ; i_{k+1}, \ldots, i_{d}} \in \mathbb{R}^{n_{k}}$. Conversely, a mode-k slice is a tensor of order $d-1$ obtained by only fixing the $k$-th index of the tensor, i.e $\mathcal{X} ; \ldots ;, ; i_{k}, ; \ldots, ; \in \mathbb{R}^{n_{1} \times \cdots \times n_{k-1} \times n_{k+1} \times \cdots \times n_{d}}$.

By arranging all mode- $k$ fibers as columns in a matrix in a consistent manner, we obtain the mode- $k$ matricization of $\mathcal{X}$, denoted by $X_{(k)} \in \mathbb{R}^{n_{k} \times \Pi_{i \neq k} n_{i}}$. We then define the mode-k matrix-tensor product as follows:

$$
\mathcal{Y}=U \cdot_{k} \mathcal{X} \Leftrightarrow Y_{(k)}=U X_{(k)}
$$

In other words, multiplying a tensor by a matrix $U$ along mode $k$ is equivalent with transforming each mode- $k$ fiber using $U$. This operation has several useful properties:

- The multilinear product commutes along different modes, i.e., $U \cdot{ }_{i} U^{\prime} \cdot{ }_{j} \mathcal{X}=U^{\prime} \cdot{ }_{j} U \cdot{ }_{i} \mathcal{X}$ for $i \neq j$.
- Multilinear products along the same mode can be composed, i.e., $U \cdot{ }_{i} U^{\prime} \cdot{ }_{i} \mathcal{X}=\left(U U^{\prime}\right) \cdot_{i} \mathcal{X}$.
- Because multiplying a matrix by an orthogonal matrix preserves its Euclidean norm, multiplying a tensor by an orthogonal matrix along any mode also preserves its Euclidean norm.


### 3.2 ST-HOSVD rank truncation

Using the mode- $k$ product, we define a Tucker decomposition [48] of $\mathcal{A} \in \mathbb{R}^{n_{1} \times \cdots \times n_{d}}$ as

$$
\left(U_{1}, \ldots, U_{d}\right) \cdot \mathcal{B}=U_{1} \cdot 1 \cdots \cdot{ }_{d-1} U_{d} \cdot{ }_{d} \mathcal{B}
$$

Manuscript submitted to ACM

```
Algorithm 1: The ST-HOSVD [49]
    Data: input tensor $\mathcal{A}$ of order $d$
    Result: truncated core $\overline{\mathcal{B}}$, truncated factors $\overline{U_{1}}, \ldots, \overline{U_{d}}$
    $\overline{\mathcal{B}}=\mathcal{A}$
    for $i=1, \ldots, d$ do
        [Compute a singular value decomposition $\bar{B}_{(i)}=U \Sigma V^{T}$ ];
        [Choose truncation rank $r_{i}$ ];
        $\overline{U_{i}}=U_{:, 1: r_{i}} ;$
        $\bar{B}_{(i)}=\Sigma_{1: r_{i}, 1: r_{i}} V_{:, 1: r_{i}}^{T} ;$
```

|  | Sign | Absolute value (binary) |  |  |  |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $b_{111}$ | $\mathbf{0}$ | 1 | $\mathbf{0}$ | $\mathbf{0}$ | $\mathbf{0}$ | $\mathbf{0}$ | $\mathbf{0}$ | $\mathbf{0}$ | 1 | 1 | 1 | $\ldots$ |
| $b_{112}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | $\ldots$ |
| $b_{121}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $\ldots$ |
| $b_{122}$ | $\mathbf{1}$ | 0 | 0 | 0 | 1 | $\mathbf{1}$ | $\mathbf{0}$ | $\mathbf{0}$ | 1 | 0 | 1 | $\ldots$ |
| $b_{211}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | $\ldots$ |
| $b_{212}$ | $\mathbf{1}$ | 0 | 0 | 0 | 1 | $\mathbf{0}$ | $\mathbf{0}$ | 0 | 1 | 1 | 0 | $\ldots$ |
| $b_{221}$ | $\mathbf{0}$ | 0 | 0 | 0 | 1 | $\mathbf{0}$ | $\mathbf{0}$ | 1 | 1 | 0 | 0 | $\ldots$ |
| $b_{222}$ | $\mathbf{0}$ | 0 | 0 | 0 | 0 | 1 | $\mathbf{0}$ | 1 | 0 | 0 | 1 | $\ldots$ |

Table 2. Example of TTHRESH-style bit plane truncation of an example $2 \times 2 \times 2$ core and the different bit categories that remain.

where $\mathcal{B} \in \mathbb{R}^{n_{1} \times \cdots \times n_{d}}$ is called the core tensor and $\left(U_{1}, \ldots, U_{d}\right) \in \mathbb{R}^{n_{1} \times n_{1}} \times \cdots \times \mathbb{R}^{n_{d} \times n_{d}}$ are called the factor matrices. A common method to compute a Tucker decomposition of a tensor $\mathcal{A}$ is the higher-order singular value decomposition (HOSVD) [14], where each orthogonal factor matrix $U_{i}$ is chosen as the matrix of left singular vectors of $A_{(i)}$. A truncated HOSVD can then be obtained by only retaining the first $r_{i} \leqslant n_{i}$ columns, leading to a truncated core $\overline{\mathcal{B}} \in \mathbb{R}^{r_{1} \times \cdots \times r_{d}}$ and factors $\overline{U_{i}} \in \mathbb{R}^{n_{i} \times r_{i}}$. Alternatively, the ST-HOSVD [49] algorithm can also be used, which interleaves the factor computation steps with the rank truncation and projection steps. The full procedure is shown in Algorithm 1. Due to the data reduction in each iteration, this method significantly speeds up as more truncation is applied. Furthermore, the resulting compression error is almost always less than or equal to the one obtained by the truncated HOSVD [49].

For the remainder of this paper, we assume the original data to be compressed is an order-d tensor $\mathcal{A} \in \mathbb{R}^{n_{1} \times n_{2} \times \cdots \times n_{d}}$ with total size $N=\Pi_{i=1}^{d} n_{i}$. This tensor will be approximated by the multilinear rank $\left(r_{1}, \ldots, r_{d}\right)$ ST-HOSVD $\left(\overline{U_{1}}, \ldots, \overline{U_{d}}\right)$. $\overline{\mathcal{B}}$, where $\overline{\mathcal{B}} \in \mathbb{R}^{r_{1} \times \cdots \times r_{d}}$ and the $\bar{U}_{i} \in \mathbb{R}^{n_{i} \times r_{i}}$ have orthonormal columns. The final approximation $\overline{\mathcal{A}}$ produced by the proposed ATC compressor will be denoted by $\left(\widetilde{U_{1}}, \ldots, \widetilde{U_{d}}\right) \cdot \widetilde{\mathcal{B}}$, where $\widetilde{\mathcal{B}} \in \mathbb{R}^{r_{1} \times \cdots \times r_{d}}$ and $\widetilde{U}_{i} \in \mathbb{R}^{n_{i} \times r_{i}}$.

### 3.3 TTHRESH bit plane truncation and encoding

While compression can be achieved through the aforementioned concept of rank truncation, another strategy is to compute the full HOSVD $\mathcal{A}=\left(U_{1}, \ldots, U_{d}\right) \cdot \mathcal{B}$ and then store the resulting coefficients with limited precision. This is the essence of the bit plane truncation scheme employed by the Tucker-based compressor TTHRESH [5], which then encodes the remaining data using a custom procedure described below. Table 2 demonstrates this process with an example where $\mathcal{B} \in \mathbb{R}^{2 \times 2 \times 2}$.

First, all entries of $\mathcal{B}$ are scaled by $2^{k}$ where $k \in \mathbb{N}$ such that $2^{63} \leqslant 2^{k}\|\mathcal{B}\|_{\max }<2^{64}$, where $\|\cdot\|_{\max }$ denotes the max-norm: the largest absolute value of the entries in the tensor. This scaling factor is stored in the compressed output so
the process can be inverted during decompression. Then, the core coefficients of $2^{k} \mathcal{B}$ are rounded to the nearest integer. By vectorizing the core coefficients into a column vector of length $\Pi_{i} r_{i}$ and considering the binary representation of each coefficient's absolute value, we obtain a bit matrix, in which each row represents a single coefficient. The first few columns of this matrix are shown in Table 2. We then iterate over each column of this matrix, i.e., each bit plane, starting from the left, i.e., in order of significance. Within each bit plane, we process each bit one by one and track the core quantization error that would be achieved by encoding all bits up till this point. Due to the orthogonality of the Tucker factors, ignoring perturbations introduced during factor compression, this core error will equal the final compression error. Therefore, when the target compression error is reached, the procedure ends. We define the point in the bit matrix where this happens as the breakpoint of the bit plane truncation process. In Table 2, the breakpoint is at position 4 on the 7th-highest bit plane. Furthermore, we define all coefficients with at least one encoded 1-bit as significant, with all other coefficients being insignificant. In Table 2, coefficients $b_{112}, b_{121}$ and $b_{211}$ are insignificant, while all others are significant.

All bits in the vectorized core up to the breakpoint are included in the encoded core. Not all bits are encoded in the same way. The leading bits of each coefficient, marked in italics in Table 2, mostly consist of zeroes. Therefore, run-length encoding is applied to all sequences of zeroes within each bit plane, i.e., within each column of the bit matrix. Because the resulting run-lengths are highly non-uniformly distributed [5], they are then compressed further using arithmetic coding. The sign bits of the significant coefficients as well as the trailing bits (marked in bold) are almost uniformly distributed in general, so they are encoded without compression.

Note that all insignificant coefficients will be decoded as zero, so their signs do not need to be stored. Furthermore, it is possible that after bit plane truncation, certain core slices only contain insignificant coefficients, which are represented by 0 in the quantized core. Therefore, the corresponding factor columns will not be encoded. For further details, such as the quantization and encoding of the factors, we refer the reader to [5].

Ballester-Ripoll and Pajarola [6] concluded that a variant of the aforementioned thresholding scheme consistently leads to better compression than Tucker rank truncation followed by a simple quantization scheme. After all, rank truncation indiscriminately removes all coefficients from low-energy slices, while bit plane truncation preserves the most significant components from all slices. We will use a variation of this approach in ATC.

## 4 THE ATC PIPELINE

In this section we describe the ATC pipeline, which is summarized in Fig. 3. This diagram serves as a point of reference for the reader throughout this section. While the decompression pipeline also has a few distinctive components, it is very similar to the compression procedure in reverse. Therefore, it is not documented in a separate diagram.

In the first phase of the compression process, the data is processed by an ST-HOSVD. We describe the motivation for ST-HOSVD rank truncation and the implications for error control when combined with bit plane truncation in Section 4.1. During the second phase, the resulting core tensor and factors are quantized and encoded. Although this is based on TTHRESH's bit plane truncation scheme, we applied several improvements, two of which will be discussed in Sections 4.2 and 4.3. Various minor improvements are discussed in the supplementary material.

### 4.1 Hybrid truncation

As discussed in Section 3, previous research suggests that while the ST-HOSVD can compute truncated Tucker decompositions relatively quickly, TTHRESH's bit plane truncation approach achieves superior compression. Combining both strategies was suggested by $[5,6]$ as a trade-off between compression rate and execution time.

Manuscript submitted to ACM

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-09.jpg?height=1157&width=1390&top_left_y=316&top_left_x=300)

Fig. 3. Overview of the ATC compression pipeline. The blue components (with section labels 4.1.1, 4.1.2, 4.1.3, 4.2 and 4.3) indicate the most important parts of the pipeline that were added or modified with respect to TTHRESH and are described in the corresponding sections. The green components (with section labels S.2, S.3, S.4, S.5 and S.6) only represent minor performance improvements and are therefore described in the supplementary material.

In our approach, we first apply rank truncation during the ST-HOSVD (see Algorithm 1) to approximate a certain target rank truncation error. Then, the truncated core and factors are processed during the quantization and encoding phase as in TTHRESH, introducing a quantization error as well.

4.1.1 ST-HOSVD mode processing order. Due to the truncation performed in each step of the ST-HOSVD, the mode processing order can significantly influence execution time. In ATC, the modes are processed in order of increasing mode length during compression by default, following the heuristic described in [49]. However, ATC also allows the user to specify a processing order. For example, if a particular mode is known to be highly compressible a priori, it is advisable to process it first, since this will lead to a large reduction of the core size for the subsequent steps.

During decompression, the mode processing order $p$ is determined by minimizing the approximate number of operations required, estimated as $\sum_{i=1}^{d} n_{p_{1}} \ldots n_{p_{i}} r_{p_{i}} \ldots r_{p_{d}}$ based on the complexity of naive matrix multiplication. This

```
Algorithm 2: ST-HOSVD compression with circular mode shift
    Data: input tensor $\mathcal{A}$ stored in the default mode order $(1, \ldots, d)$, mode processing order $p$
    Result: truncated core $\overline{\mathcal{B}}$ stored in the mode order $\left(p_{1}, \ldots, p_{d}\right)$, truncated factors $\overline{U_{1}}, \ldots, \overline{U_{d}}$
    $\overline{\mathcal{B}}=\operatorname{transpose}(\mathcal{A}, p)$;
    for $i=1, \ldots, d$ do
        $B=\operatorname{reshape}\left(\overline{\mathcal{B}},\left(n_{p_{i}}, n_{p_{i+1}} \ldots n_{p_{d}} r_{p_{1}} \ldots r_{p_{i-1}}\right)\right)$;
        [Determine factor $U_{p_{i}} \in \mathbb{R}^{n_{p_{i}} \times r_{p_{i}}}$ based on the singular value decomposition of $B$ ];
        $\bar{B}=B^{T} U_{p_{i}} ;$
        $\overline{\mathcal{B}}=\operatorname{reshape}\left(\bar{B},\left(n_{p_{i+1}}, \ldots, n_{p_{d}}, r_{p_{1}}, \ldots, r_{p_{i}}\right)\right) ;$
```

optimal order is found through exhaustive search, which takes $O(d d!)$ time. While this complexity could be reduced to $O(d \log d)$ as described in section 8 of [4], we did not implement such an algorithm due to the very low value of $d$.

4.1.2 Circular mode shift. To minimize the number of tensor transpositions needed in the ST-HOSVD, we employ a circular mode shift trick, which lets us compute the ST-HOSVD with arbitrary mode processing order using only a single transposition, regardless of the order of the tensor. Algorithm 2 describes the full procedure. We start by transposing the input tensor, such that the core is initially stored with its modes ordered in the same way as the mode processing order $p$. In the first iteration, we matricize the tensor by "merging" all but the first mode, resulting in an $n_{p_{1}} \times n_{p_{2}} n_{p_{3}} \ldots n_{p_{d}}$ matrix (line 3). Mode $p_{1}$ can then be processed. After selecting an appropriate factor matrix, we project the core while transposing it, which results in an $n_{p_{2}} \ldots n_{p_{d}} \times r_{p_{1}}$ matrix (line 5). The matrix can now be reshaped into a tensor with mode order $\left(p_{2}, \ldots, p_{d}, p_{1}\right)$ (line 6). Therefore, each iteration effectively applies a circular shift to the mode order. By initially transposing this mode order to the order $p$, we ensure that at the start of the $i$-th step, the mode $p_{i}$ will be at the front of the mode order, allowing it to be processed.

Note that these reshaping operations simply reinterpret the same memory as matrices with different dimensions and do not result in any data movement or poor memory access patterns, leading to a negligible runtime cost. Furthermore, the Eigen linear algebra software library [24], used in ATC, evaluates the transposition and matrix multiplication on line 5 simultaneously using expression templates, so effectively no transposition needs to be processed apart from the initial one on line 1 . We implemented a similar procedure for the decompression process as well.

4.1.3 Error control. The target error for ST-HOSVD is determined by the parameter rank_truncation_max_sse_share (RTMSS) as follows:

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-10.jpg?height=56&width=961&top_left_y=1772&top_left_x=647)

Using this target, the truncation rank for each mode is determined dynamically using the strategy described in [49, section 6.3]. The actual rank truncation SSE can simply be computed as the sum of the squares of the singular values discarded across all steps of the ST-HOSVD, as stated in theorem 6.4 from [49].

To control the final error of the decompressed tensor, we will now approximately separate this error into multiple components. Based on the ST-HOSVD $\mathcal{A} \approx\left(\overline{U_{1}}, \ldots, \overline{U_{d}}\right) \cdot \overline{\mathcal{B}}$, we define the full factors $U_{i}$ as arbitrary orthogonal matrices such that $\left(U_{i}\right)_{; 1: 1 r_{i}}=\overline{U_{i}}$, with the corresponding full core $\mathcal{B}=\left(U_{1}^{T}, \ldots, U_{d}^{T}\right) \cdot \mathcal{A}$. To simplify arithmetic, we also introduce the padded truncated factors ${\overline{U_{i}}}^{\prime} \in \mathbb{R}^{n_{i} \times n_{i}}$ with $\left(\overline{U_{i}}\right)_{;: 1: r_{i}}=\overline{U_{i}}$ and $\left({\overline{U_{i}}}^{\prime}\right)_{:, r_{i}+1: n_{i}}=0$. We define the padded quantized core $\widetilde{\mathcal{B}}^{\prime} \in \mathbb{R}^{n_{1} \times \cdots \times n_{d}}$ as a tensor containing the quantized core coefficients in $\widetilde{\mathcal{B}}_{1: r_{1}, \ldots, 1: r_{d}}^{\prime}$ and 0 everywhere else. This tensor will be decomposed as the sum of the full core $\mathcal{B}$, the truncation error tensor $\overline{\mathcal{B}}^{\prime}-\mathcal{B}$ and Manuscript submitted to ACM
the quantization error tensor $\delta \mathcal{B}=\widetilde{\mathcal{B}}^{\prime}-\overline{\mathcal{B}}^{\prime}$. The quantized factors $\widetilde{U}_{i}^{\prime} \in \mathbb{R}^{n_{i} \times n_{i}}$ are matrices with the first $r_{i}$ columns consisting of the quantized factor coefficients and 0 elsewhere. Furthermore, we define $\delta U_{i}=\widetilde{U}_{i}^{\prime}-\overline{U_{i}}$. Note that due to the non-zero pattern in $\widetilde{\mathcal{B}}^{\prime}$ this means that ${\overline{U_{i}}}^{\prime} \cdot{ }_{i} \widetilde{\mathcal{B}}^{\prime}=U_{i} \cdot{ }_{i} \widetilde{\mathcal{B}}^{\prime}$ for $i=1, \ldots, d$. Recall that $\widetilde{\mathcal{A}}$ is the final approximation. We start with the following expression:

$$
\|\tilde{\mathcal{A}}-\mathcal{A}\|^{2}=\left\|\left(\bar{U}_{1}^{\prime}+\delta U_{1}, \ldots,{\overline{U_{d}}}^{\prime}+\delta U_{d}\right) \cdot \widetilde{\mathcal{B}}^{\prime}-\left(U_{1}, \ldots, U_{d}\right) \cdot \mathcal{B}\right\|^{2}
$$

Discarding higher-order factor quantization errors we get:

$$
\begin{align*}
&\|\widetilde{\mathcal{A}}-\mathcal{A}\|^{2} \approx\left\|\left(\overline{U_{1}}, \ldots, \overline{U_{d}}\right) \cdot \widetilde{\mathcal{B}}^{\prime}+\sum_{i=1}^{d}\left({\overline{U_{1}}}^{\prime}, \ldots, \overline{U_{i-1}}, \delta U_{i},{\overline{U_{i+1}}}^{\prime}, \ldots, \overline{U_{d}}{ }^{\prime}\right) \cdot \widetilde{\mathcal{B}}^{\prime}-\left(U_{1}, \ldots, U_{d}\right) \cdot \mathcal{B}\right\|^{2} \\
&=\left\|\widetilde{\mathcal{B}}^{\prime}-\mathcal{B}+\sum_{i=1}^{d}\left(U_{i}^{T} \delta U_{i}\right) \cdot \widetilde{\mathcal{B}}^{\prime}\right\|^{2} \\
&=\left\|\left(\overline{\mathcal{B}}^{\prime}-\mathcal{B}\right)+\delta \mathcal{B}+\sum_{i=1}^{d}\left(U_{i}^{T} \delta U_{i}\right) \cdot \widetilde{\mathcal{B}}^{\prime}\right\|^{2} \\
&=\left\|\overline{\mathcal{B}}^{\prime}-\mathcal{B}\right\|^{2}+\|\delta \mathcal{B}\|^{2}+\sum_{i=1}^{d}\left\|\left(U_{i}^{T} \delta U_{i}\right) \cdot_{i} \widetilde{\mathcal{B}}^{\prime}\right\|^{2} \\
& \quad \quad+2 \sum_{i=1}^{d}\left\langle\widetilde{\mathcal{B}^{\prime}}-\mathcal{B},\left(U_{i}^{T} \delta U_{i}\right) \cdot \widetilde{\mathcal{B}}^{\prime}\right\rangle+2 \sum_{i=1}^{d} \sum_{j=1}^{i-1}\left\langle\left(U_{i}^{T} \delta U_{i}\right) \cdot \widetilde{\mathcal{B}}^{\prime},\left(U_{j}^{T} \delta U_{j}\right) \cdot{ }_{j} \widetilde{\mathcal{B}}^{\prime}\right\rangle \tag{1}
\end{align*}
$$

where we used that the truncation and quantization error tensors $\left(\overline{\mathcal{B}}^{\prime}-\mathcal{B}\right)$ and $\delta \mathcal{B}$ have complimentary non-zero patterns, so their inner product is zero. Using the Cauchy-Schwartz inequality, we can provide an upper bound:

$$
\|\widetilde{\mathcal{A}}-\mathcal{A}\|^{2} \lesssim\left\|\overline{\mathcal{B}}^{\prime}-\mathcal{B}\right\|^{2}+\|\delta \mathcal{B}\|^{2}+\sum_{i=1}^{d}\left\|\delta U_{i} \cdot{ }_{i} \widetilde{\mathcal{B}}^{\prime}\right\|^{2}+2 \sum_{i=1}^{d}\left\|\delta U_{i} \cdot \widetilde{\mathcal{B}}^{\prime}\right\|\left(\left\|\widetilde{\mathcal{B}}^{\prime}-\mathcal{B}\right\|+\sum_{j=1}^{i-1}\left\|\delta U_{j} \cdot{ }_{j} \widetilde{\mathcal{B}}^{\prime}\right\|\right)
$$

However, for the general, dense tensors in our setting, this bound is very loose in practice. This can be explained by considering that due to the mostly uniform quantization errors, the error tensor components are not aligned in a particular direction. Because these tensors live in a very high-dimensional space, they are almost orthogonal to each other. Thus, the inner products in Eq. (1) are dominated by the squared norms in practice.

As a result, we propose to ignore the inner products from Eq. (1) to obtain a more useful estimate for the compression error. Furthermore, note that the core tensor $\mathcal{C}$ produced by an HOSVD is all-orthogonal, i.e., all mode- $i$ slices are orthogonal to each other for each $i$ [14], which implies that $\left\|\delta U_{i} \cdot{ }_{i} \mathcal{C}\right\|=\left\|\delta U_{i} \Sigma_{i}\right\|_{F}$, where $\Sigma_{i}$ is a diagonal matrix containing the mode- $i$ core slice norms of $\mathcal{C}$. Although this property does not hold exactly for $\widetilde{\mathcal{B}}^{\prime}$ due to truncation and quantization errors, we observed that it is mostly all-orthogonal in practice. Therefore we approximate $\left\|\delta U_{i} \cdot{ }_{i} \widetilde{\mathcal{B}}^{\prime}\right\|$ by $\left\|\delta U_{i} \widetilde{\Sigma}_{i}^{\prime}\right\|_{F}$. This leads to the following error approximation:

$$
\begin{equation*}
\|\widetilde{\mathcal{A}}-\mathcal{A}\|^{2} \approx\left\|\overline{\mathcal{B}}^{\prime}-\mathcal{B}\right\|^{2}+\|\delta \mathcal{B}\|^{2}+\sum_{i=1}^{d}\left\|\delta U_{i} \widetilde{\Sigma}_{i}^{\prime}\right\|_{F}^{2} \tag{2}
\end{equation*}
$$

When compressing several datasets from Section 6.1 with various RTMSS values and target relative errors $10^{-2}$ and $10^{-3}$, we found that the relative difference in between the left-hand and right-hand side of Eq. (2) never exceeded

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-12.jpg?height=464&width=1499&top_left_y=340&top_left_x=386)

Fig. 4. Relative compression factors across different errors and datasets in terms of RTMSS. Each compression factor is normalized by dividing it by the maximum compression factor for the same error. The dashed line indicates the default RTMSS value, which balances suboptimal compression rates with increased compression and decompression speed.

$0.15 \%$. At a target relative error of $10^{-1}$, this deviation increased to $8.5 \%$. We conclude that the approximation is usually very accurate, so we will use it to predict the total error. Finally, like in TTHRESH we do not consider the factor errors while quantizing the core because they are not yet known. Therefore we simply set the target core quantization SSE to

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-12.jpg?height=43&width=479&top_left_y=1136&top_left_x=373)

Figure 4 shows the effect of the RTMSS parameter on the compression factor, with 0 and 1 corresponding to almost no and almost only rank truncation respectively. Surprisingly, some level of hybrid truncation improves the compression rate in certain cases, in addition to improving the execution time, although the optimal RTMSS parameter depends on the dataset and compression error. This improvement can be attributed to a very large reduction in encoded leading zeroes, which generally also decreases their cost after run-length and entropy coding. We chose 0.5 as the default RTMSS parameter value, which we believe to be a reasonable trade-off between compression efficiency and speed.

### 4.2 Parallel core quantization and encoding

Since the core quantization and encoding process represents a significant share of the total compression time, we adapted it to support multi-threading. This is achieved by splitting the vectorized core into a series of equally sized blocks, with each block containing a contiguous series of quantized core coefficients. The number of blocks is at least as large as the number of threads. Each thread can then process and encode the bits of the current bit plane inside one or more blocks independently of the other blocks. This procedure is executed sequentially for all bit planes until the encoding breakpoint is reached.

A minor complication is dynamically selecting a breakpoint for approximating the desired quantization error in parallel. To avoid synchronization overhead, we process bit planes in full without synchronization and check the total error reduction achieved at the end of the bit plane. If this exceeds the target error reduction, we process the bit plane again with a limited number of threads and synchronized error checks. When one thread determines that the target error has been reached, all threads will stop encoding bits, leading to a separate breakpoint for each thread. Therefore, the full bit plane will consist of several alternating sections of encoded and non-encoded bits. Note that only the last encoded bit plane needs to be processed twice, as the global target error reduction can only be exceeded once.

For each block, the bits of the current bit plane are then encoded independently and written to temporary buffers in-memory. Afterwards these compressed blocks are sequentially written to the compressed output, including the size Manuscript submitted to ACM
of each block. Although these sizes could be determined during decompression, explicitly storing them allows ATC to quickly read and separate all blocks into temporary buffers during decompression before parsing them in parallel, leading to parallelized dequantization and decoding as well.

### 4.3 Improving error control by predicting the dequantization correction

During dequantization, if a decoded coefficient $\tilde{a}$ contains bits down to bit plane $p$, we know that the original value $a$ was located in the interval $\left[\tilde{a}, \tilde{a}+2^{p}-1\right]$ (ignoring signs). To decrease the quantization error, we therefore approximate $a$ as $\tilde{a}+2^{p-1}$. While this correction was already applied in TTHRESH, we significantly improved error control by considering this during the quantization error tracking process as well.

To demonstrate the significance of this correction, assume that $a$ is drawn from the uniform distribution $U\left[\tilde{a}, \tilde{a}+2^{p}\right]$. Then, we have that

$$
\mathbb{E}_{a \sim U\left[\tilde{a}, \tilde{a}+2^{p}\right]}\left[(a-\tilde{a})^{2}\right]=\frac{2^{2 p}}{3} \quad \text { and } \quad \mathbb{E}_{a \sim U\left[\tilde{a}, \tilde{a}+2^{p}\right]}\left[\left(a-\tilde{a}-2^{p-1}\right)^{2}\right]=\frac{2^{2 p}}{12}
$$

Therefore, if we guess that $a$ is (approximately) in the middle of the uncertainty interval rather than at its lower edge, we reduce the SSE by a factor of 4 when $p$ is large. While the values $a$ are not exactly uniformly distributed in practice, this nevertheless shows that dequantization correction has a significant impact on the final error and therefore needs to be considered during quantization to achieve precise error control.

## 5 SOFTWARE IMPLEMENTATION

ATC is implemented in $\mathrm{C}++17$ and is mainly designed to optimize compression efficiency, speed, error control and memory usage. Furthermore, certain mathematical software aspects, such as the usability of the library, were taken into account. In this section, we discuss the handling of cut-outs, the interfaces of the library, shared-memory parallelism, libraries, support for various types, I/O, and general software design considerations. When referring to library version numbers throughout this section, we will specify the lowest tested version.

Cut-outs and downsampling. In some settings, only a part of the decompressed tensor may be needed. In such situations, the Tucker decomposition can be efficiently combined with the extraction of a cut-out from the decompressed tensor. Although any linear filter could be used to downscale the granularity of the tensor grid, we simply decided to reuse the downsampling, box and Lanczos filtering methods from TTHRESH, as described in section 5 of [5]. ATC is implemented in such a way that new filters could be added relatively easily if necessary.

Interfaces. ATC's native interface is written in C++. However, due to the use of the Standard Template Library (STL) containers this interface may cause compatibility problems when the library and user code are not compiled in exactly the same setting. To address this we provide a $\mathrm{C}$ wrapper, which also serves as an interface to $\mathrm{C}$ users as well as users which may want to connect ATC to other languages, such as Python. Finally, we implemented an executable which offers a command-line interface too.

Shared-memory parallelism. The user can specify the threads parameter to determine how many threads can be used for linear algebra and tensor transpositions, as these steps are performed using external libraries which support shared-memory parallelism. Furthermore, the core quantization and encoding process is parallelized using OpenMP 4.5 [36], as discussed in Section 4.2, and can be controlled using the same parameter

Libraries used. Tensor transpositions are performed using the High-Performance Tensor Transpose (HPTT) library version 1.0.5 [44]. This library supports arbitrary mode permutations, allowing ATC to use a custom mode processing order during the ST-HOSVD, as well as a custom mode storage order for quantization and encoding. We observed that HPTT appreciably sped up these parts of the compression pipeline. Some code from TTHRESH [5] was reused with permission from the author to implement certain shared features, such as the handling of cut-outs.

Linear algebra is performed using Eigen 3.3.4, with an option to use Basic Linear Algebra Subprograms (BLAS) [7] for matrix multiplications. To enable an optional higher-order DCT preprocessing phase, the Fastest Fourier Transform in the West (FFTW) 3 library [19] is needed. If the user wishes to compile the ATC command-line interface, the Boost.Program_options library (version 2) [39] is also required. Finally, to enable multi-threading in some parts of the pipeline as discussed in the previous paragraph, OpenMP 4.5 [36] is required.

Supported types and I/O. ATC supports a wide range of data types for the input tensor: 8, 16, 32 and 64-bit integers, signed and unsigned, as well as 32 and 64-bit floating-point numbers. The tensor can be provided either as a file or as a buffer in memory. Similarly, both the compressed and decompressed output can be stored using either method. The original and decompressed tensors are stored in a flattened, binary format. Metadata such as the data type and mode sizes are passed as separate arguments during compression and are stored in the compressed file for use during decompression. If the original data file contains a header, this can be ignored using the optional skip_bytes parameter.

Internally, floating-point arithmetic is performed using 64-bit precision by default, although the user can switch to 32-bit precision if desired. We provide a similar option for choosing in between 32 and 64-bit integer types for quantization and encoding. Our experiments indicate that in some high-error cases, lower precision suffices to achieve roughly the same compression factor while significantly improving speed.

Software design. To support the various types described in previous paragraph, ATC uses C++ templates throughout most of its code. While this keeps the implementation concise and improves modifiability, by default templated code must be written in header files so the compiler can instantiate the code only for the requested types when compiling the user code. Therefore the library templated code needs to be recompiled by the user whenever the application is modified, which can lead to long compilation times and can slow down development. Since ATC only supports a fixed set of types, we addressed this issue by explicitly instantiating the relevant code for each of the valid types for most type parameters.

However, instantiating all code in terms of each type parameter combination would lead to excessively large compiled binaries. ATC solves this problem by templating most code only in terms of the floating-point type parameter while using inheritance in some cases to connect components where types are only known at runtime. Although this adds the performance overhead of dynamic dispatch, ATC is specifically designed to only invoke this mechanism in non-critical parts of the code, thus making the performance cost negligible.

## 6 RESULTS

In this section, we will empirically evaluate the performance of ATC in terms of compression rate, speed, memory usage and error control across datasets of different sizes from various application domains. These metrics will be compared to other state-of-the-art numerical data compressors.

Manuscript submitted to ACM

| Dataset | Size | Mode sizes | Data type | Domain | Modes |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Foot [27] | $16.0 \mathrm{MB}$ | $256 \times 256 \times 256$ | uint8 | X-ray scan | Space $(3)$ |
| Brain [28] | $103 \mathrm{MB}$ | $160 \times 190 \times 148 \times 6$ | float32 | Diffusion tensor image | Space $(3) \times$ component |
| Moffett-Field [3] | $224 \mathrm{MB}$ | $512 \times 1024 \times 224$ | int16 | Hyperspectral image | Space $(2) \times$ wavelength |
| Isotropic-PT [23] | $800 \mathrm{MB}$ | $100 \times 128 \times 128 \times 128$ |  | CFD simulation | Time $\times$ space $(3)$ |
| Isotropic-V [23] | $1.50 \mathrm{~GB}$ | $512 \times 512 \times 512 \times 3$ |  |  |  |
| Deforest-8 [13] | $3.05 \mathrm{~GB}$ | $20 \times 79 \times 8 \times 180 \times 360$ | float32 | Climate simulation | Variable $\times$ time $\times$ space $(3)$ |
| Deforest-33 [13] | $12.0 \mathrm{~GB}$ | $19 \times 79 \times 33 \times 180 \times 360$ |  |  |  |
| Hurricane $[9]$ | $24.2 \mathrm{~GB}$ | $13 \times 20 \times 100 \times 500 \times 500$ |  | Weather simulation |  |

Table 3. The datasets used along with their properties. "Space $(\mathrm{X})$ " represents a set of $\mathrm{X}$ spatial modes. "Components" represents the unique diffusion tensor elements per voxel in the case of Brain and the velocity components per voxel in the case of Isotropic-V.

### 6.1 Datasets

We selected a diverse collection of datasets across a range of sizes and application domains, as listed in Table 3. Some of these were already used as benchmarks in other publications [5, 31]. Certain datasets were preprocessed as follows:

- Brain: The original dataset consists of 7 values per voxel: the 6 unique elements of the corresponding diffusion tensor along with a "confidence" value in the interval $[0,1]$, indicating the accuracy of the measurements for this voxel. We extracted the 6 actual components and multiplied them by the corresponding confidence value. This effectively removes the distorted values, which represent missing elements, by mapping them to 0 , which makes them easy to compress, while improving smoothness by not applying a hard thresholding scheme.
- Isotropic-PT, Isotropic-V: These are cut-outs of the "Isotropic 1024 Fine" dataset [23]. Specifically, we extracted the subtensors from the corner with the smallest coordinates in the full tensor. In the case of Isotropic- $\mathrm{V}$, the time mode effectively has size 1 .
- Deforest-8, Deforest-33: These are part of the "deforest-globe" dataset (variant r1i1p1f1) [13]. To acquire an appropriate amount of data, we considered only .gr data files with version 20191122 and annually sampled variables. The filtered data files were then merged into two separate tensors based on their number of levels, i.e. the size of the vertical spatial mode, with 20 variables using 8 levels and 19 variables using 33 levels. The original data contains extreme but constant values in voxels located over land. Due to a lack of documentation, we assumed that these values represent missing values and should therefore not be stored. Therefore, we subtracted the first time slice from all other time slices. The resulting tensor represents the change in each variable since the start of the simulation, thus setting these missing values to zero and reducing their effect on the compression process. Finally, we normalized each variable by scaling it such that its maximum absolute value equals 1 .
- Hurricane: To not exceed the available amount of memory on our hardware, we select only the first 20 time slices. Since the missing values in this dataset were all set to a particular constant, we simply set all of these values to zero and did not apply the difference coding step unlike the previous case. Afterwards, each variable slice was normalized in the aforementioned way.


### 6.2 Compressors

For our experimental comparison, we selected several lossy data compressors based on a recent survey by Duwe et al. [16, section 1.2.2]. Specifically, we limited ourselves to compressors with at least support for datasets of order 3 and 4 as well as a publicly available implementation, including a command-line interface, since all experiments

| Compressor | Version/commit date | Cores | Notes |
| :---: | :---: | :---: | :---: |
| ATC | ![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-16.jpg?height=35&width=276&top_left_y=365&top_left_x=639) | 36 | Using OpenBLAS 0.2.20 for matrix multiplications. |
| TTHRESH | 20 Feb. 2022 | 36 |  |
| $\overline{S Z}$ | 12 Apr. 2022 | 36 | No support for datasets with 5 or more modes (through the <br> command-line interface). |
| ZFP | 27 Jan. 2022 | 36 (compr.) <br> 1 (decompr.) |  |
| FPZIP | 1.3.0 | 1 |  |
| TuckerMPI | 31 Jan. 2022 | 36 | The tensor is sliced into 36 parts along the longest mode. <br> Each part is handled by a single-threaded MPI process. <br> Since the data is stored in single-precision, we use the <br> single-precision driver. Using OpenMPI 2.1 .2 with shared- <br> memory optimization flags -mca btl self, sm, tcp. |
| $\mathrm{x} 265$ | 2.6 | 16 | Based on the x265 video codec [35], see details in text. |

Table 4. The compressors and their corresponding information which will be used in the following compression comparison.

are performed using this interface. The resulting compressors are listed in Table 4. Considering the rate-distortion comparisons performed in [5] and [31], this list should contain all relevant compressors. All compressors, except for the x265 wrapper, were compiled using GCC 8.3.0 (GNU Compiler Collection).

The last compressor in this list, x265, consists of a Python script which preprocesses the dataset and passes it onto the x265 video codec for compression using Fast Forward Moving Picture Experts Group (FFmpeg) version 4.1 and compiled with GCC 6.4.0 [47]. We used the options -preset veryslow -tune psnr, leading to high compression in terms of relative error but longer runtimes. Furthermore, the error was controlled using the parameter crf to optimize compression results. Although we are not aware of any literature considering video compressors for compressing general tensor data, we include x265 in our comparison due to its high performance on video data [11, 50].

Because this compressor is intended for video formats, we added preprocessing steps to support compression of general tensors of order 3 or higher. First, for each dataset we designated two of its spatial modes as the video's width and height mode and one mode as the video's time mode. This time mode was chosen as the dataset's own time mode or some other very smooth mode. Then, all other remaining modes were then merged with the time mode by transposing and reshaping the tensor. Finally, all values were shifted, scaled and rounded to fit into the domain $\{0,1,2, \ldots, 255\}$. Note that due to this limited domain, there will always be a significant quantization error, so our x265-based compressor cannot produce low compression errors and will only be relevant in the high-error domain.

Finally, while we included FPZIP in all of our experiments, we found that its compression performance was almost always worse than ZFP while being much slower. To reduce the number of data points in the figures in this section, we will therefore not show results from this compressor.

### 6.3 Experimental set-up

All experiments were performed on a computing node with two Xeon Gold 6240 central processing units ( $2.6 \mathrm{GHz}$ clock speed with 18 physical cores and 36 threads per CPU) and $180 \mathrm{~GB}$ main memory running CentOS 7.9.2009. I/O was performed using local SSD's with theoretical reading and writing speeds of $1 \mathrm{~GB} / \mathrm{s}$ and $0.6 \mathrm{~GB} / \mathrm{s}$ respectively. We let each compressor use the maximal number of supported threads or processes, which is described in Table 4.

Wall-clock times and peak memory usage were measured using the Linux command /usr/bin/time. Due to the memory limitations of TTHRESH (see Section 6.6), we will not apply this compressor to the largest dataset, Hurricane.

Furthermore, x265 results will not be shown for Deforest-8 and Deforest-33 since the corresponding compression errors are all larger than $10 \%$.

Throughout this section, we will aggregate average gains in terms of compression rates, speed, memory usage and error control when comparing compressors. These averages refer to geometric means in the case of relative compression gains, speed-ups and peak memory reduction, while in the case of error control we use the arithmetic mean of the absolute values of the logarithms of the relative error deviations, as shown in the following formula:

$$
\text { Average relative error deviation }=\exp \left(\frac{\sum_{i=1}^{n}\left|\ln \frac{\text { ActualError }_{i} \mid}{\text { TargetError }_{i}}\right|}{n}\right)
$$

Because these metrics are not all evaluated at the same error, we will instead compute the average in between interpolated curves. We obtain these curves by applying linear interpolation to data points consisting of the logarithm of the compression error, as well as the peak memory usage or the logarithm of the compression factor, execution time or deviation from the target error, depending on the metric under consideration. Figures 5 and 7 to 9 show that by considering these metrics on a logarithmic scale, we obtain relatively smooth curves. Moreover, we will use this interpolation method when plotting relative compression factors.

Note that some compressors do not accept all data types. Therefore, for datasets with integral data types, we cast them to a 32-bit floating-point format before compression. After decompression, we round the decompressed values back to the nearest value in the domain of the corresponding integer type, which can significantly reduce the compression error in low-error settings. Note that while ATC performs this rounding step for integral data types internally already, we perform it externally in all cases to prevent this simple feature from skewing our experimental results. Moreover, SZ, ZFP and FPZIP do not support compression of order-5 tensors (through the command-line interface), so for these datasets we merge the first two modes together and treat them as order- 4 tensors.

### 6.4 Incremental performance gains compared to TTHRESH

Table 1 shows how each modification to the baseline TTHRESH algorithm affects various performance metrics in our experiments. To clarify the impact of these changes, we briefly discuss our most important findings here:

- Baseline ATC: Due to a more efficient implementation, we achieve a compression speed-up of around $27 \%$ while halving memory usage.
- Hybrid truncation: This leads to compression and decompression speed-up factors of roughly 1.5 and 1.3, respectively, at the cost of an $8 \%$ decrease in compression rate. Note that these numbers are averages and can vary a lot in practice, depending on the compressibility of the dataset and the target error.
- Parallel quantization and encoding: This results in compression and decompression speed-up factors of roughly 1.8 and 1.6 , respectively, while slightly reducing error control. Our experiments show that these speedup factors are generally present, but can become much smaller for well-compressible datasets and high target errors, since in these cases the truncated core shrinks, reducing the time share of the quantization and encoding phase in the full compression process.
- Predicting the dequantization correction during quantization: This small change greatly improves error control, reducing the average error deviation from $27.7 \%$ to $1.4 \%$, with little effect on other performance metrics.
- Various minor improvements: The three last modifications described in Table 1 slightly improve average compression rate, but in some cases at a minor speed cost. As such their relevance may depend on the use case. Therefore, while they are enabled by default, we provide user options to disable them.


### 6.5 Rate-distortion comparison

Figure 5 shows the rate-distortion curves for all compressors and datasets, with the highest curves representing the best compression rates. We observed that ATC and TTHRESH perform very well in some cases (Moffett-Field, Isotropic-PT and Isotropic-V), adequate in other cases (Deforest-8, Deforest-33 and Hurricane) and poorly in some other cases (Foot and Brain). In particular, we note that these compressors outperform the rest in the high-compression domain, typically starting from a compression factor of 10 to 100 . This usually corresponds with high errors, with ATC/TTHRESH often achieving the highest relative compression rates at errors over $1 \%$. In fact, while SZ slightly outperforms ATC for many errors, ATC's gain is so high in other cases that on average, in our experiments, ATC achieves $98 \%$ higher compression.

When comparing ATC and TTHRESH, we can see that both perform similarly most of the time, with ATC only achieving $1.1 \%$ higher compression on average in our experiments. Figure 6 demonstrates that this can be attributed to ATC's default RTMSS parameter value of 0.5 , which controls rank truncation (see Section 4.1). If we consider the highest compression factor achieved by ATC across the different RTMSS settings for each error value, we obtain an average gain of $8.0 \%$ compared to TTHRESH, at the cost of compression and decompression slowdowns of $24 \%$ and $21 \%$ respectively relative to the default RTMSS setting, which is still much faster than TTHRESH. In fact, for some datasets this increases to $20 \%$ or higher for high errors, which Fig. 5 showed is the most relevant domain for Tucker-based compressors. Note that this gain is accumulated over the optimal RTMSS setting for each individual error, which is often but not always 0 . Furthermore, we observe that for most errors, decreasing the RTMSS parameter improves compression. In conclusion, we find that users which prioritize compression over speed might want to start with an RTMSS value of 0 and adjust it if necessary, because this setting almost never leads to compression losses and can lead to significant gains compared to TTHRESH.

### 6.6 Time and memory usage

Figure 7 shows the compression and decompression times of each compressor. Note that the Tucker-based compressors generally need more time than the others, because they apply a global transformation to the data (the Tucker decomposition), while most other compressors process the data in blocks in some way, leading to a time complexity proportional to the amount of data. This discrepancy also manifests itself in Fig. 8 in terms of peak memory usage.

However, when comparing ATC to TTHRESH, our compressor took $71 \%$ and $55 \%$ less time on average during compression and decompression, respectively. Yet, we observe that the Tucker decomposition incurs a significant speed cost, making the non-Tucker-based compressors $\mathrm{SZ}$ and ZFP much faster in most settings.

Finally, Fig. 8 shows that ATC is more efficient in terms of memory usage than its Tucker-based counterpart TTHRESH, on average achieving reductions of $54 \%$ and $56 \%$ during compression and decompression respectively. Yet, note that these averages are skewed by the memory overhead observed in the cases of small datasets like Foot and Brain, while ATC achieves average memory usage of around 18 bytes per data element for larger datasets, in contrast to TTHRESH's typical 40 bytes per element. Meanwhile, TuckerMPI uses even less memory since we are using its single-precision driver, while ATC stores the tensor in double-precision by default.

Manuscript submitted to ACM
![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-19.jpg?height=1412&width=1478&top_left_y=335&top_left_x=258)

Deforest-8

Deforest-33

Hurricane
![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-19.jpg?height=634&width=1030&top_left_y=1038&top_left_x=687)

$\rightarrow$ ATC $\sim$ TTHRESH $\sim$ x265 $\sim \mathrm{SZ} \longrightarrow$ ZFP $\rightarrow$ TuckerMPI

Fig. 5. Rate-distortion curves for all compressors and datasets, both in terms of absolute compression factors and the compression factors relative to ATC.

### 6.7 Error control

Lossy compressors can control the trade-off in between compression rate and error in various ways. In the case of ATC and certain other compressors in our comparison, we say they are error-bounded since they attempt to approximate a given target error with the highest possible compression rate. When using such a compressor, it can be important that the actual compression error does not deviate much from the desired target error. After all, higher errors are undesirable while lower errors lead to an unnecessary loss of compression rate. As a result, we analyzed the degree of error control of different compressors in Fig. 9. ATC clearly performs very well, with an average relative deviation from the target error of $1.4 \%$ compared to TTHRESH's $33.8 \%$ and SZ's $31.7 \%$.
![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-20.jpg?height=944&width=1460&top_left_y=336&top_left_x=386)
$\operatorname{ATC}(\mathrm{RTMSS}=0.4$ )
$-\cdots-\cdots-\operatorname{ATC}($ RTMSS $=0.8)$

Fig. 6. Compression factors of ATC, in terms of its RTMSS parameter value, relative to TTHRESH. Since we do not have TTHRESH data on Hurricane, we choose ATC with RTMSS $=0.4$ as a reference instead, which still shows the relationship in between the RTMSS parameter value and the compression factor.

## 7 CONCLUSIONS

We presented ATC, a novel Tucker-based numerical data compressor centered around the ST-HOSVD and bit plane truncation. Several techniques were described to improve speed, memory usage, error control and compression rate. Furthermore, certain implementation and usability aspects were discussed.

Our experiments show that ATC on average maintains the compression rates of the state-of-the-art Tucker-based compressor TTHRESH while providing average speed-up factors of 3.5 and 2.2 during compression and decompression, respectively. Average peak memory usage was also reduced in our experiments by $54 \%$ and $56 \%$ respectively. Moreover, ATC achieves very precise error control, on average only deviating $1.4 \%$ from the requested compression error.

Compared to non-Tucker-based compressors, ATC usually outperforms all alternatives in terms of rate-distortion when targeting high relative errors, e.g. above $1 \%$. Although the state-of-the-art $\mathrm{SZ}$ compressor achieves slightly higher compression rates in many settings, ATC drastically outperforms it in some other situations, leading to an average compression gain for ATC of $97 \%$ in our experiments. However, due to the costly Tucker decomposition, ATC uses significantly more time and memory.

## ACKNOWLEDGMENTS

We thank the five anonymous reviewers for their extensive questions and remarks on earlier versions of this manuscript, which greatly improved this paper. We are grateful to Rafael Ballester-Ripoll for allowing us to reuse certain parts of Manuscript submitted to ACM
![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-21.jpg?height=1416&width=1480&top_left_y=332&top_left_x=257)

Isotropic-V

Deforest-8

Deforest-33

Hurricane
![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-21.jpg?height=604&width=1422&top_left_y=1039&top_left_x=294)

Relative error

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-21.jpg?height=38&width=319&top_left_y=1605&top_left_x=1044)
Relative error

$$
\begin{array}{lll}
10^{-4} & 10^{-3} & 10^{-} \\
& \text {Relative errc }
\end{array}
$$

I ATC  TTHRESH --I-.. x265 -I-- SZ -.I.- ZFP -I- TuckerMPI

Fig. 7. Median compression and decompression wall-clock times of all compressors, out of 5 samples per target error. The error bars indicate the range of the measurements among these samples.

the TTHRESH source code in the implementation of ATC. We kindly thank Zitong Li (TuckerMPI) and Kai Zhao (SZ) for fixing our reported issues, allowing us to run our experiments using their compressors.

The resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research Foundation-Flanders (FWO) and the Flemish Government. We thank the VSC support team for extensive help with setting up TuckerMPI on our computing cluster and optimizing the performance of our experiments.

We also thank the sources of all datasets used in this paper: Philips Research and IAPR-TC18 (Foot), Gordon Kindlmann and Andrew Alexander (Brain), the NASA Jet Propulsion Laboratory (Moffett-Field), the Johns Hopkins Turbulence

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-22.jpg?height=1410&width=1485&top_left_y=336&top_left_x=388)

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-22.jpg?height=645&width=417&top_left_y=344&top_left_x=407)

Isotropic-V
![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-22.jpg?height=604&width=418&top_left_y=1040&top_left_x=406)

Relative error
![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-22.jpg?height=636&width=682&top_left_y=348&top_left_x=817)

Deforest-8
![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-22.jpg?height=600&width=700&top_left_y=1042&top_left_x=816)

Relative error

![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-22.jpg?height=648&width=369&top_left_y=343&top_left_x=1493)

Hurricane
![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-22.jpg?height=604&width=338&top_left_y=1041&top_left_x=1510)

Relative erro $\longrightarrow$ ATC $\sim$ TTHRESH $\simeq \mathrm{x} 265 \longrightarrow \mathrm{SZ} \longrightarrow$ ZFP $\longrightarrow$ TuckerMPI (1 process)

Fig. 8. Peak memory usage of each compressor during compression and decompression. Since this is largely proportional to the number of elements in the original data, we express this metric in terms of the peak number of bytes used per element. To measure TuckerMPI's peak memory usage, we run it using only one process. Because this process needs to read all data alone, it unfortunately cannot process the largest three datasets due to the limitations of MPI I/O.

Databases (Isotropic-PT and Isotropic-V), the World Climate Research Programme and the Earth System Grid Federation (Deforest-8 and Deforest-33) as well as the NCAR and U.S. National Science Foundation (Hurricane).

Nick Vannieuwenhoven was partially supported by a Postdoctoral Fellowship of the Research Foundation-Flanders (FWO) with project $12 \mathrm{E} 8119 \mathrm{~N}$.

Manuscript submitted to ACM
![](https://cdn.mathpix.com/cropped/2024_06_04_ba7478dddfce936633dfg-23.jpg?height=900&width=1498&top_left_y=339&top_left_x=256)

Fig. 9. Error control per compressor, defined as the deviation of the compression error from the target error. Values below 1 indicate that the compressor achieved a lower error than requested. ZFP, FPZIP and our x265 compressor do not support any Euclidean-norm-based target error metric.

## REFERENCES

[1] N. Ahmed, T. Natarajan, and K. R. Rao. 1974. Discrete Cosine Transform. IEEE Transactions on Computers C-23, 1 (1974), 90-93. https: //doi.org/10.1109/T-C.1974.223784

[2] Alliance for Open Media. [n.d.]. AV1 Features. https://aomedia.org/av1-features/

[3] AVIRIS. [n.d.]. Free Data. https://aviris.jpl.nasa.gov/data/free_data.html

[4] G. Ballard, A. Klinvex, and T. G. Kolda. 2020. TuckerMPI: A Parallel C++/MPI Software Package for Large-Scale Data Compression via the Tucker Tensor Decomposition. ACM Trans. Math. Softw. 46, 2, Article 13 (jun 2020), 31 pages. https://doi.org/10.1145/3378445

[5] R. Ballester-Ripoll, P. Lindstrom, and R. Pajarola. 2020. TTHRESH: Tensor Compression for Multidimensional Visual Data. IEEE Transactions on Visualization and Computer Graphics 26, 9 (2020), 2891-2903. https://doi.org/10.1109/TVCG.2019.2904063

[6] R. Ballester-Ripoll and R. Pajarola. 2015. Lossy volume compression using Tucker truncation and thresholding. The Visual Computer (2015), 1-14.

[7] S. L. Blackford, A. Petitet, R. Pozo, K. Remington, C. R. Whaley, J. Demmel, J. Dongarra, I. Duff, S. Hammarling, G. Henry, et al. 2002. An updated set of basic linear algebra subprograms (BLAS). ACM Trans. Math. Software 28, 2 (2002), 135-151.

[8] CCITT. 1992. Information Technology - Digital Compression and Coding of Continuous-Tone Still Images - Requirements and Guidelines. https://www.w3.org/Graphics/JPEG/itu-t81.pdf

[9] U.S. National center for Atmospheric Research. [n.d.]. Hurricane Isabel WRF Model Data Visualization. https://www.earthsystemgrid.org/dataset/ isabeldata.html

[10] W. Chen. 2004. The Electrical Engineering Handbook. Elsevier. pg. 906.

[11] J. De Cock, A. Mavlankar, A. Moorthy, and A. Aaron. 2016. A large-scale video codec comparison of x264, x265 and libvpx for practical VOD applications. In Applications of Digital Image Processing XXXIX, Andrew G. Tescher (Ed.), Vol. 9971. International Society for Optics and Photonics, SPIE, 363 - 379. https://doi.org/10.1117/12.2238495

[12] Y. Collet and C. Turner. 2016. Smaller and faster data compression with Zstandard. https://engineering.fb.com/2016/08/31/core-data/smaller-andfaster-data-compression-with-zstandard/

[13] G. Danabasoglu. 2019. NCAR CESM2 model output prepared for CMIP6 LUMIP deforest-globe. https://doi.org/10.22033/ESGF/CMIP6.7574

[14] L. De Lathauwer, B. De Moor, and J. Vandewalle. 2000. A Multilinear Singular Value Decomposition. SIAM Journal on Matrix Analysis and Applications 21, 4 (2000), 1253-1278. https://doi.org/10.1137/S0895479896305696

[15] J. Duda. 2009. Asymmetric numeral systems. (2009). arXiv:0902.0271

[16] K. Duwe, J. Lttgau, G. Mania, J. Squar, A. Fuchs, M. Kuhn, E. Betke, and T. Ludwig. 2020. State of the Art and Future Trends in Data Reduction for High-Performance Computing. Supercomputing Frontiers and Innovations 7, 1 (2020). https://superfri.org/superfri/article/view/303

[17] C. Eckart and G. Young. 1936. The approximation of one matrix by another of lower rank. Psychometrika 1, 3 (1936), 211-218.

[18] V. Ehrlacher, L. Grigori, D. Lombardi, and H. Song. 2021. Adaptive Hierarchical Subtensor Partitioning for Tensor Compression. SIAM fournal on Scientific Computing 43, 1 (2021), A139-A163. https://doi.org/10.1137/19M128689X arXiv:https://doi.org/10.1137/19M128689X

[19] M. Frigo and S. Johnson. 2005. The Design and Implementation of FFTW3. Proceedings of the IEEE 93, 2 (2005), 216-231. Special issue on "Program Generation, Optimization, and Platform Adaptation".

[20] J. Gailly and M. Adler. [n.d.]. zlib. http://zlib.net/

[21] Google. [n.d.]. Draco 3D Graphics Compression. https://google.github.io/draco/

[22] L. Grasedyck. 2010. Hierarchical Singular Value Decomposition of Tensors. SIAM Journal on Matrix Analysis and Applications 31, 4 (2010), $2029-2054$. https://doi.org/10.1137/090764189 arXiv:https://doi.org/10.1137/090764189

[23] Turbulence Database Group. [n.d.]. Johns Hopkins Turbulence Database. http://turbulence.pha.jhu.edu/newcutout.aspx

[24] G. Guennebaud, B. Jacob, et al. [n.d.]. Eigen. http://eigen.tuxfamily.org

[25] D. A. Huffman. 2006. A method for the construction of minimum-redundancy codes. Resonance 11, 2 (2006), 91-99.

[26] JPEG. 2021. JPEG White Paper: JPEG XL Image Coding System. http://ds.jpeg.org/whitepapers/jpeg-xl-whitepaper.pdf

[27] B. Kerautret. [n.d.]. TC18 - 3D Images. http://tc18.org/3D_images.html

[28] G. Kindlmann and A. Alexander. [n.d.]. Diffusion tensor MRI datasets. http://www.sci.utah.edu/ gk/DTI-data/

[29] T. G. Kolda and B. W. Bader. 2009. Tensor Decompositions and Applications. SIAM Review 51, 3 (2009), 455-500. https://doi.org/10.1137/07070111X arXiv:https://doi.org/10.1137/07070111X

[30] Z. Li, A. Aaron, I. Katsavounidis, A. Moorthy, and M. Manohara. 2016. Toward a practical perceptual video quality metric. The Netflix Tech Blog 6, 2 (2016).

[31] X. Liang, S. Di, D. Tao, S. [Sihuan] Li, S. [Shaomeng] Li, H. Guo, Z. Chen, and F. Cappello. 2018. Error-Controlled Lossy Compression Optimized for High Compression Ratios of Scientific Datasets. In 2018 IEEE International Conference on Big Data (Big Data). 438-447. https://doi.org/10.1109/ BigData.2018.8622520

[32] P. Lindstrom. 2014. Fixed-Rate Compressed Floating-Point Arrays. IEEE Transactions on Visualization and Computer Graphics 20, 12 (2014), $2674-2683$. https://doi.org/10.1109/TVCG.2014.2346458

[33] P. Lindstrom and M. Isenburg. 2006. Fast and Efficient Compression of Floating-Point Data. IEEE Transactions on Visualization and Computer Graphics 12, 5 (2006), 1245-1250. https://doi.org/10.1109/TVCG.2006.143

[34] O. Mickelin and S. Karaman. 2020. Multiresolution Low-rank Tensor Formats. SIAM 7. Matrix Anal. Appl. 41, 3 (2020), 1086-1114. https: //doi.org/10.1137/19M1284579 arXiv:https://doi.org/10.1137/19M1284579

[35] MultiCoreWare. [n.d.]. x265. https://www.x265.org/

[36] OpenMP ARB. [n.d.]. OpenMP. https://www.openmp.org

[37] I. V. Oseledets. 2011. Tensor-Train Decomposition. SIAM 7ournal on Scientific Computing 33, 5 (2011), 2295-2317. https://doi.org/10.1137/090752286 arXiv:https://doi.org/10.1137/090752286

[38] E. E. Papalexakis, C. Faloutsos, and N. D. Sidiropoulos. 2016. Tensors for Data Mining and Data Fusion: Models, Applications, and Scalable Algorithms. ACM Transactions on Intelligent Systems and Technology 8, 2, Article 16 (Oct. 2016), 44 pages. https://doi.org/10.1145/2915921

[39] V. Prus. [n.d.]. Chapter 31. Boost.Program_options. https://www.boost.org/doc/libs/1_75_0/doc/html/program_options.html

[40] J. J. Rissanen. 1976. Generalized Kraft Inequality and Arithmetic Coding. IBM Journal of Research and Development 20, 3 (1976), 198-203. https://doi.org/10.1147/rd.203.0198

[41] D. Salomon. 2007. Data Compression: The Complete Reference (4. aufl. ed.). Springer Verlag London Limited, London.

[42] N. Sasaki, K. Sato, T. Endo, and S. Matsuoka. 2015. Exploration of Lossy Compression for Application-Level Checkpoint/Restart. In 2015 IEEE International Parallel and Distributed Processing Symposium. 914-922. https://doi.org/10.1109/IPDPS.2015.67

[43] C. E. Shannon. 2001. A Mathematical Theory of Communication. SIGMOBILE Mobile Computing and Communications Review 5, 1 (Jan. 2001 ), 3-55. https://doi.org/10.1145/584091.584093

[44] P. Springer, T. Su, and P. Bientinesi. 2017. HPTT: A High-Performance Tensor Transposition C++ Library. In Proceedings of the 4th ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming (Barcelona, Spain) (ARRAY 2017). ACM, New York, NY, USA, 56-62. https://doi.org/10.1145/3091966.3091968

[45] S. K. Suter, J. A. Iglesias Guitian, F. Marton, M. Agus, A. Elsener, C. P. E. Zollikofer, M. Gopi, E. Gobbetti, and R. Pajarola. 2011. Interactive Multiscale Tensor Reconstruction for Multiresolution Volume Visualization. IEEE Transactions on Visualization and Computer Graphics 17, 12 (2011), 2135-2143. https://doi.org/10.1109/TVCG.2011.214

[46] S. K. Suter, M. Makhynia, and R. Pajarola. 2013. TAMRESH - Tensor Approximation Multiresolution Hierarchy for Interactive Volume Visualization. Computer Graphics Forum 32, 3pt2 (2013), 151-160. https://doi.org/10.1111/cgf.12102 arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12102

[47] FFmpeg team. [n.d.]. FFmpeg. https://ffmpeg.org/

Manuscript submitted to ACM

[48] L. R. Tucker. 1966. Some mathematical notes on three-mode factor analysis. Psychometrika 31, 3 (1966), 279-311.

[49] N. Vannieuwenhoven, R. Vandebril, and K. Meerbergen. 2012. A New Truncation Strategy for the Higher-Order Singular Value Decomposition. SIAM Journal on Scientific Computing 34, 2 (2012), A1027-A1052. https://doi.org/10.1137/110836067

[50] D. Vatolin, D. Kulikov, M. Erofeev, S. Dolganov, and S. Zvezdakov. 2016. HEVC/H.265 Video Codecs Comparison. Technical Report. http: //compression.ru/video/codec_comparison/hevc_2016/MSU_HEVC_comparison_2016_free.pdf

[51] Z. Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing 13, 4 (2004), 600-612. https://doi.org/10.1109/TIP.2003.819861

[52] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra. 2003. Overview of the H.264/AVC video coding standard. IEEE Transactions on Circuits and Systems for Video Technology 13, 7 (2003), 560-576. https://doi.org/10.1109/TCSVT.2003.815165

[53] K. Zhao, S. Di, M. Dmitriev, T. D. Tonellot, Z. Chen, and F. Cappello. 2021. Optimizing Error-Bounded Lossy Compression for Scientific Data by Dynamic Spline Interpolation. In 2021 IEEE 37th International Conference on Data Engineering (ICDE). 1643-1654. https://doi.org/10.1109/ICDE51399. 2021.00145

[54] K. Zhao, S. Di, X. Liang, S. Li, D. Tao, Z. Chen, and F. Cappello. 2020. Significantly Improving Lossy Compression for HPC Datasets with Second-Order Prediction and Parameter Optimization. In Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing (Stockholm, Sweden) (HPDC '20). Association for Computing Machinery, New York, NY, USA, 89-100. https://doi.org/10.1145/3369583.3392688

# Minor compression pipeline improvements in ATC 

WOUTER BAERT, KU Leuven, Belgium<br>NICK VANNIEUWENHOVEN, KU Leuven, Belgium


#### Abstract

This document serves as supplementary material to the paper, "ATC: an Advanced Tucker Compression library for multidimensional data". We describe a number of additional techniques implemented in the ATC software package that lead to more modest performance gains compared to those from the main article.

