# $\{\angle \triangle\}$ MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems 

Kaixin Li $^{1} \quad$ Yuchen Tian $^{2} \quad$ Qisheng Hu $^{1} \quad$ Ziyang Luo $^{3} \quad$ Jing Ma $^{3}$<br>${ }^{1}$ National University of Singapore ${ }^{2}$ The University of Hong Kong<br>${ }^{3}$ Hong Kong Baptist University<br>likaixin@u.nus.edu


#### Abstract

Programming often involves converting detailed and complex specifications into code, a process during which developers typically utilize visual aids to more effectively convey concepts. While recent developments in Large Multimodal Models have demonstrated remarkable abilities in visual reasoning and mathematical tasks, there is little work on investigating whether these models can effectively interpret visual elements for code generation. To this end, we present MMCode, the first multimodal coding dataset for evaluating algorithmic problem-solving skills in visually rich contexts. MMCode contains 3,548 questions and 6,620 images collected from real-world programming challenges harvested from 10 code competition websites, presenting significant challenges due to the extreme demand for reasoning abilities. Our experiment results show that current state-of-the-art models struggle to solve these problems. The results highlight the lack of powerful vision-code models, and we hope MMCode can serve as an inspiration for future works in this domain. The data and code are publicly available. ${ }^{1}$


## 1 Introduction

Programming is primarily aimed at fulfilling requirements, frequently entailing the translation of detailed and intricate specifications into executable code (Nuseibeh and Easterbrook, 2000). In this endeavor, human developers regularly employ visual aids such as images and diagrams to facilitate effective communication and a better understanding of concepts (Agarwal and Sinha, 2003).

Recently, automated code generation tools have attracted significant attention, largely attributing to[^0]

the substantial advance in Code Large Language Models (Code LLMs) (Chen et al., 2021; Nijkamp et al., 2023; Roziere et al., 2023; Luo et al., 2023b; Li et al., 2023b; Guo et al., 2024). These models demonstrated unprecedentedly remarkable coding abilities, potentially assist to enhance productivity, reduce human error and democratize coding skills. Nevertheless, these models are limited to processing text-only inputs, lacking the ability to interpret rich information presented through images.

In a closely related development, the field has also observed the emergence of many powerful Large Multimodal Models (LMMs), marked by GPT-4V (OpenAI, 2023b) and Gemini (Team Gemini et al., 2023), representing a significant step forward in bridging the modality of text and images. While there are multiple works evaluating these models in mathematical reasoning ( $\mathrm{Lu}$ et al., 2023), perception and reasoning (Liu et al., 2023) and instruction-following (Ye et al., 2023), there is a notable gap in evaluating LMMs for code generation.

To this end, we present MMCode, the first multimodal benchmark for rigorously evaluating the code generation ability of Large Multimodal Models. It comprises 3,548 questions with 6,620 images collated from 10 programming-related websites encompassing a broad spectrum of subjects, extending from fundamental coding concepts to the application of code for solving mathematical problems. The generated code is rigorously checked by test cases. The overall framework is illustrated in Figure 1.

Our experiments revealed that current LMMs struggle significantly to solve the tasks in MMCode. The most powerful LMMs, GPT-4V and Gemini, scored unsatisfactory pass rates as low as $19.4 \%$ and $5.0 \%$, potentially due to the requirement

![](https://cdn.mathpix.com/cropped/2024_06_04_465406cdf129cfa4ca0ag-02.jpg?height=779&width=1302&top_left_y=270&top_left_x=340)

Figure 1: An illustration of an example question and the automatic testing pipeline of MMCode. The tests in the judger are selected for display. The actual test cases are harder than the sample inputs and outputs.

of intense reasoning on the text descriptions and images. Open-source LMMs (Liu et al., 2024; Bai et al., 2023) yield negligible pass rates because of their inability to understand the abstract meaning of the images. The findings reveal a significant deficiency in current LMMs' ability to interpret and utilize multimodal information for code generation, highlighting an imperative need for further advancements in this area. We believe MMCode will serve as a pivotal benchmark for evaluating the forthcoming evolution of Code LMMs and inspire research in this area.

## 2 Related Works

### 2.1 Code Large Language Models

Large Language Models (LLMs) have experienced significant advancements in recent years, demonstrating remarkable progress in their capabilities and applications that were previously unattainable (Ouyang et al., 2022; Brown et al., 2020; OpenAI, 2022, 2023a; Touvron et al., 2023a,b; Chowdhery et al., 2022; Anil et al., 2023; Hoffmann et al., 2022; Scao et al., 2022). Building on their increasing proficiency at understanding and generating human-like text, a set of specialized models known as Code LLMs have emerged, focusing specifically on programming code (Chen et al., 2021; Nijkamp et al., 2023; Roziere et al., 2023; Li et al., 2023b; Luo et al., 2023b; Guo et al., 2024). Trained on large corpora of code data, these models have acquired the capacity to comprehend programming contexts and generate syntactically correct and logically sound code snippets. However, a significant limitation of these tools is their inability to process image inputs, restricting their application to environments where interaction is solely text- or code-based. Such a deficiency precludes their use in scenarios requiring the interpretation of visual data.

### 2.2 Coding Benchmarks

Accompanying the rapid development of Code Large Language Models, numerous benchmarks and datasets have witnessed the astonishing advancements of Code LLMs. These benchmarks cover a wide area of code-related tasks, such as code completion (Chen et al., 2021; Zheng et al., 2023; Austin et al., 2021; Yan et al., 2023a), editing (Li et al., 2023a; Tian et al., 2024) and translation (Yan et al., 2023b). Most relevant to our work, APPS (Hendrycks et al., 2021) and CodeContests (Li et al., 2022) leveraged coding problems from real-world practice and contest coding websites as benchmarks. Recently, TACO (Li et al.,

![](https://cdn.mathpix.com/cropped/2024_06_04_465406cdf129cfa4ca0ag-03.jpg?height=534&width=1568&top_left_y=241&top_left_x=244)

Table 1: Examples of images from each category. Some images are cropped for better visualization.

2023c) contributed a comprehensive collection of contest problems. However, it aims to cluster the problems by the programming skills needed (e.g. Dynamic Programming and Tree Algorithms), while MMCode focuses on image-augmented questions to assess the question-solving skills of multimodal language models.

### 2.3 Reasoning-Intense Visual Question Answering

Several works have emerged to assess the reasoning capabilities of LMMs with visual contexts. ScienceQA (Lu et al., 2022) consists of multimodal multiple-choice questions across scientific topics, designed to measure the multi-hop reasoning ability. MMMU (Yue et al., 2023) features collegelevel questions with multi-disciplinary subjects. MathVista (Lu et al., 2023) emphasizes mathematical problem-solving with multi-modal input, involving tasks that require diverse math reasoning skills. OlympiadBench (He et al., 2024) offers a set of challenging Olympiad-level mathematics and physics contest questions. PuzzleVQA (Chia et al., 2024) benchmarks LMMs on patterns in order to evaluate if the models' reasoning ability generalizes to abstract figures. Our work distinguishes itself by necessitating the generation of solution code of complex problems, which benchmarks LMMs for long-horizon reasoning.

## $3\{\triangle \backslash\}$ MMCode

In this section, we introduce the source and collection pipeline of MMCode. The collection pipeline comprises four stages: 1) Raw data collection; 2) automatic filtering; 3) human filtering and 4) anno- tation. This pipeline to be introduced in the following sections guarantees the quality and diversity of the data collected for MMCode.

### 3.1 Data Sources

The questions of MMCode are collected from 10 coding platforms, including AtCoder, Aizu, CodeChef, CodeForces, CodeWars, Project Euler, Geeksforgeeks, HackerRank, Leetcode and Open Kattis. More information can be found in Appendix D. 10 .

The data sources exhibit a wide range of characteristics and purposes, including competitions, job interviews, and tutorials, etc. Notably, Project Euler is distinguished by its collection of challenges that necessitate a combination of mathematical and computer programming skills to solve. As a result, MMCode benefits from the diversity of these sources, offering programming problems with varying difficulties, styles, and skill requirements.

### 3.2 Data Collection Pipeline

Raw Data Collection. For each of the 10 platforms, distinct web crawlers were developed to retrieve the problem statements. The HTML elements were then converted to plain texts following unified rules to ensure cleanliness and readability. Furthermore, the metadata of these questions was collected conditionally on availability, e.g. problem name, time limit, and memory limit. It is noteworthy that we also included the raw HTML code in our dataset for further flexible use.

If there were images (<img> tags) encapsulated within the statements, we saved them and converted them to PNG format. The tags were re-

![](https://cdn.mathpix.com/cropped/2024_06_04_465406cdf129cfa4ca0ag-04.jpg?height=602&width=1625&top_left_y=230&top_left_x=224)

![](https://cdn.mathpix.com/cropped/2024_06_04_465406cdf129cfa4ca0ag-04.jpg?height=446&width=759&top_left_y=251&top_left_x=246)

(a) The distribution of lengths of the question statements, measured by the number of characters.

![](https://cdn.mathpix.com/cropped/2024_06_04_465406cdf129cfa4ca0ag-04.jpg?height=456&width=779&top_left_y=246&top_left_x=1047)

(b) The distribution of numbers of images per question. Questions with 10 or more images are combined into the last bin " $10+$ ".

Figure 2: Data statistics of the questions in MMCode.

placed with markdown tags to insert them in the text (e.g. ![image](1.png)). It is essential to maintain the locations of the images in the text because a question may encompass multiple images, and the images can be closely related to the text sections around them. This practice ensures the cohesion and coherence of the contents, where visual and textual elements are harmoniously integrated for better understanding.

Due to the difficulty of obtaining the automated test cases as a result of the changes in platforms' designs and policies, we also reused the rich information from the TACO dataset (Li et al., 2023c) where feasible. We matched the crawled questions with those existing in TACO by URLs ${ }^{2}$. Specifically, we crawled all questions from the largest two data sources, CodeForces and Aizu, including problem statements and test cases. Additionally, we included a new platform Project Euler that is not present in previous datasets. For other platforms, we reused the data from TACO and fetched the question statements to add the images.

An initial data analysis revealed that $18.8 \%$ of the obtained questions contained images, corroborating our motivation for creating a multi-modal coding benchmark.

Automated Filtering. In this phase, our initial step involved excluding questions that do not include associated images. Subsequently, we applied[^1]

various post-processing steps to ensure the quality of the data. We filtered questions with images unable to load using the PILLOW library ${ }^{3}$. Additionally, we converted PNGs with alpha channels to pure RGB format by painting the background to pure white, which is critical for discerning the texts on the images. This avoids distinct behaviors of different models interpreting the transparent color. Finally, a strict 5-gram similarity is conducted on every pair of question statements in the dataset to remove similar problems with a similarity score greater than 0.80 . This process eliminated 33 questions from the dataset.

Human Filtering. At this stage, a preliminary inspection of sampled questions was first conducted to scope the quality of the collected data. The primary source of noise was found to be teaser images that try to interest the readers but do not provide information or implications to help solve the questions. These images mostly originate from Open Kattis and CodeForces, consisting of photographs about the background of the question, anime screenshots, etc. An example is presented in Appendix D.10, where the question is about developers' cooperation, but the image is a humorous comic about the daily work of programmers. We also spotted some mixture of website logos and UI elements in the images, probably due to mistakes of the question creators in typesetting.

To address this problem, a convenient solution is to employ large LMMs such as GPT-4V and Gemini to determine if the image(s) are useful in ad-[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_465406cdf129cfa4ca0ag-05.jpg?height=460&width=769&top_left_y=244&top_left_x=238)

(a) The distribution of lengths of the question statements, measured by the number of characters.

![](https://cdn.mathpix.com/cropped/2024_06_04_465406cdf129cfa4ca0ag-05.jpg?height=456&width=768&top_left_y=246&top_left_x=1047)

(b) The distribution of the position of the images, measured by the percentage of their placement in the question statement.

Figure 3: Data statistics of the images in MMCode.

dressing the question. Nonetheless, such a method may potentially introduce bias into the data. Therefore, we decided to opt for human labor to filter out these unrelated images. We manually examined every image in the dataset to remove the noisy ones. Note that when an image was deemed irrelevant but was not the sole image in the question, we exclusively removed this image and its corresponding markdown tag from the text. The question itself is only eliminated if there are no images remaining after this process.

Annotation. In this stage, we annotate the images in MMCode into distinct categories in order to facilitate a more detailed analysis of model performance across various types of images. The images were examined and discussed by expert human coders who have rich experience in solving coding contest problems. Following this deliberation, the images were meticulously categorized into 12 types: Linear Data Structure, Tree, Graph, 2D Geometry, 3D Geometry, Chessboard, Map, Patterns, Math, Table, Pseudocode, and Others. Gemini Pro Vision is leveraged to generate the coarse labels. Detailed descriptions of the categories are listed in Appendix B.

This detailed categorization facilitates a focused analysis on how different types of visual information are processed and interpreted by models, thereby potentially aiding in the identification and improvement of their abilities in coding contexts.

### 3.3 Data Splits

After performing the previous procedures, we acquired a dataset with 3,548 questions with 6,620 images. Considering the lengthy nature of the questions and additional tokens needed to represent the images, evaluating on the full dataset can be expensive. Following MathVista (Lu et al., 2023), a conscious decision was made to keep the test set small. As a result, we sampled 263 questions as the test set, and applied careful human inspection to correct the image category labels.

### 3.4 Testing Pipeline

An execution-based testing pipeline is adopted in MMCode for rigorous answer checking, following (Hendrycks et al., 2021; Li et al., 2023c; Chen et al., 2021). As demonstrated in Figure 1, the judger attempts to compile the code generated by models, followed by a timed execution in a sandbox. The programs' outputs are checked against the ground truth answers in the test cases, and the solution is judged as correct only if it passes all hidden test cases.

## 4 Data Analysis

In this section, we undertake a comprehensive exploration of MMCode, introducing its and statistical attributes to provide a nuanced understanding of MMCode.

Problem Length. The diversity of data sources incorporated into MMCode results in significant variance in problem length, as can be seen in Figure 2a. The mean length of the questions reaches 2,256 characters, with the 25 th, 50 th, and 75 th percentile at $1,516,2,127$, and 2,791. This can be ascribed to the distinct style and difficulty of the questions presented in MMCode. Certain questions articulate

| Model | Total | Linear | Tree | Graph | $2 \mathbf{D}$ | 3D | Chess- <br> board | Map | Math | Patterns | Table | Pseudo- <br> code | Others |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Language Only Inputs |  |  |  |  |  |  |  |  |  |  |  |  |  |
| LLaVA-1.5-7B | 1.1 | 8.0 | 0.0 | 0.0 | 0.0 | 0.0 | 6.7 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |
| LLaVA-1.5-13B | 1.5 | 8.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.7 | 0.0 | 10.0 | 0.0 |
| QWEN-VL | 1.1 | 4.0 | 0.0 | 0.0 | 0.0 | 0.0 | 6.7 | 0.0 | 0.0 | 0.0 | 0.0 | 10.0 | 0.0 |
| Gemini Pro - | $-5.7-$ | $-\overline{16} .0$ | $\overline{0} . \overline{0}$ | $-4 . \overline{3}$ | $-\overline{3.3}$ | $0 . \overline{0}$ | $\overline{0} . \overline{0}$ | $-3 . \overline{6}$ | $-0 . \overline{0}$ | $\overline{1} 4 . \overline{8}$ | $\overline{0.0}-$ | $2 \overline{0.0}$ | $7 . \overline{7}$ |
| GPT-3.5 (gpt-3.5-turbo-1106) | 11.0 | 28.0 | 6.9 | 4.3 | 6.7 | 7.7 | 13.3 | 10.7 | 4.0 | 18.5 | 14.3 | 20.0 | 7.7 |
| GPT-4 (gpt-4-1106-preview) | 17.9 | 28.0 | 6.9 | 13.0 | 10.0 | 7.7 | 13.3 | 17.9 | 16.0 | 29.6 | 21.4 | 40.0 | 26.9 |
| GPT-4V (gpt-4-vision-preview) | 18.3 | 40.0 | 10.3 | 17.4 | 10.0 | 7.7 | 26.7 | 7.1 | 12.0 | 22.2 | 21.4 | 50.0 | 23.1 |
| Vision + Language Inputs |  |  |  |  |  |  |  |  |  |  |  |  |  |
| $\mathrm{LLaV}_{2}$ | 1.5 | 12.0 | 0.0 | 0.0 | 0.0 | 0.0 | 6.7 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |
| LLaVA-1.5-13B | 1.1 | 8.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.7 | 0.0 | $0.1-1+2-1+2-1$ | 0.0 |
| QWEN-VL | 0.8 | 8.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |
| Gemini $\overline{\text { Pro }} \overline{\text { Vision }} \overline{-}$ | $5 . \overline{0}$ | $\overline{12} .5$ | $0 . \overline{0}$ | $4 . \overline{3}$ | $\overline{0.0}$ | 3.8 | -6.7 | $7 . \overline{1}$ | $0 . \overline{0}$ | $7 . \overline{4}$ | $\overline{0 .} 0^{-}$ | $-30.0-$ | $0 . \overline{0}$ |
| GPT-4V (gpt-4-vision-preview) | 19.4 | 40.0 | 6.9 | 13.0 | 13.8 | 3.8 | 21.4 | 24.0 | 9.5 | 25.9 | 21.4 | 40.0 | 20.8 |

Table 2: Pass @ 1 (\%) results grouped by different image categories. The dashed lines separate open-source models (above) and proprietary models (below).

the instructions succinctly and directly, whereas others elaborate on the contextual background of the problem in detail.

Image Count per Problem. A notable characteristic that differentiates MMCode from previous datasets is its inclusion of multiple images per question. On average, each question is associated with 1.87 images, with the 25 th percentile having 1 image and the 75 th percentile having 2 images. These figures are interleaved with the text contents, and the understanding of them frequently depends on their order, posing great difficulty to the models.

Image Position. As Figure 3b illustrates, the images in the problems of MMCode can appear at any position in the text, but concentrate at the tail. This is because many images are drawn to intuitively depict and explain sample inputs and outputs, which are mostly located at the end of the text.

Image Type. Figure 3a illustrates the portion of the categories of images following the classification criteria introduced in Section 3.2. Graph, Math and 2D Geometry form the majority comprising more than half of the dataset, taking up $20.9 \%$ $17.9 \%$, and $15.3 \%$ respectively. Miscellaneous images classified under Others account for roughly one-tenth of the dataset, representing a high level of heterogeneity. Tree follows up with $9.9 \%$. The remaining groups sum up to approximately a quarter, demonstrating the diversity of MMCode.

## 5 Experiments

In this section, we benchmark several LanguageOnly models and Vision-Language models with MMCode. A comparative analysis of the experimental results for these models is conducted, providing a thorough examination of their capabilities.

### 5.1 Experimental Setup

We evaluate the models by prompting with fixed templates (see Appendix C) using greedy decoding and extracting their generated codes, which are executed by the testing framework to check their correctness. Pass@1 (Chen et al., 2021) is reported. The following three setups are compared:

Language-Only Models. We evaluate several powerful and Language-Only models, including GPT-3.5 (OpenAI, 2022), GPT-4 (OpenAI, 2023a), and Gemini Pro (Team Gemini et al., 2023). The images in the problem statement are removed in this setup.

Large Multi-modal Models. Some popular LMMs are selected as testees on MMCode. This includes proprietary models such as Gemini Pro Vision (Team Gemini et al., 2023), GPT-4V (OpenAI, 2023b). Additionally, open-source models such as the LLaVA series (Liu et al., 2024) and QWEN-VL (Bai et al., 2023) are assessed to track the advancements of the more accessible LMMs. The first image in the problem is kept for models that are not trained to support multiple-image in-

| Model | Total Linear | Tree | Graph | 2D | 3D | Chess- <br> board | Map | Math Patterns | TablePseudo- <br> code |  |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Oemini Pro | 6.1 | 16.0 | 0.0 | 0.0 | 6.7 | 0.0 | 6.7 | 3.6 | 0.0 | 11.1 | 7.1 | 20.0 | 7.7 |
| GPT-4 (gpt-4-1106-preview) | 19.0 | 32.0 | 10.3 | 17.4 | 6.7 | 3.8 | 33.3 | 25.0 | 12.0 | 33.3 | 21.4 | 40.0 | 19.2 |

Table 3: The performance of closed-source models with Image Replacement. Results are measured by Pass @ 1 (\%).

| Model | Total Linear | Tree | Graph | 2D | 3D | Chess- <br> board | Map | Math Patterns |  |  |  |  | Pseudo- <br> code |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Others |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Gemini Pro Vision | 3.8 | 8.0 | 0.0 | 0.0 | 6.7 | 0.0 | 13.3 | 3.7 | 0.0 | 3.7 | 0.0 | 20.0 | 0.0 |
| GPT-4V (gpt-4-vision-preview) | 16.6 | 28.0 | 6.9 | 8.7 | 6.9 | 7.7 | 7.1 | 28.0 | 9.5 | 33.3 | 14.3 | 40.0 | 12.5 |

Table 4: The performance of closed-source models with Captioning Chain of Thought. Results are measured by Pass@ 1 (\%).

puts, i.e. the LLaVA series. For fairer comparison, text-only inputs performance of these models are also reported whenever applicable.

Caption-augmented Models. We investigate whether the inclusion of captions can help the model better understand the image contexts. In our early experiments, the open-source models yielded inferior captions, frequently containing hallucinations and failing to interpret the abstract meaning of the images. Thus, we only benchmark the proprietary models. We explored two methods of leveraging the captions: (a) Image Replacement, where the image slots are replaced by the captions. (b) Captioning Chain of Thought, where we explicitly prompt the models to generate captions for the images first, and then work out the questions, resembling the Chain of Thought prompting (Wei et al., 2022).

## 6 Evaluation Results

### 6.1 Results and Findings

MMCode poses a great challenge to all models. As Table 2 depicts, all models except for the GPT family scored a Pass @ 1 rate under $10 \%$, whereas the best of the models tested, GPT-4V, yielded a mere $19.4 \%$ when equipped with all image contexts. This renders MMCode a challenging benchmark for the development of coding LMMs.

Proprietary models take a huge lead on MMCode. The GPTs yield superior results, leaving a huge gap between other models. Gemini Pro, though underperforms the GPTs, beats all tested open-source models. The open-source models generally demonstrate the inability to solve the questions with negligible pass rates of around $1 \%$ and a majority of zeros in many categories. A plausible reason is that these open-source models are not trained on such reasoning-heavy code generation tasks nor to understand abstract diagrams. The coding ability is only inherited from the base LLMs, but can be impaired due to catastrophic forgetting (Luo et al., 2023a).

Visual context helps, but requires advanced comprehending capability. Interestingly, unlike previous works such as OlympiadBench (He et al., 2024) where the text-only inputs beat multi-modal inputs, the best performance of all experiments is produced by GPT-4V with vision contexts. The observation confirms that the images contain critical information that can be mined to assist problemsolving. However, Gemini Pro Vision often fails to leverage the hints from the images, and the performance drops compared with the language-only Gemini Pro.

GPT-4V performs better than GPT-4 counterparts on less visually-cluttered image types. Comparing GPT-4V with multi-modal input to textonly GPT-4 and GPT-4V on problems with different types of images, it is observed that improvements are achieved on simpler image types, e.g. Linear Data Structure, Tree, 2D, and Map. On other visually cluttered categories such as Graph, Chessboard and Patterns, the addition of images hurts the performance. GPT-4V also produces worse results on Others, which consists of mis-

![](https://cdn.mathpix.com/cropped/2024_06_04_465406cdf129cfa4ca0ag-08.jpg?height=454&width=1428&top_left_y=236&top_left_x=317)

Figure 4: Error distribution of GPT-4V and LLaVA-13B on a sampled subset of 50 problems.

cellaneous cases including complex annotations, which are challenging for the model to interpret.

Replacing images with generated captions helps, but CoT prompting does not. Table 3 and 4 lists the results with the two caption prompting strategies. The vision models can generate informative captions (though often inaccurate; see case studies in Section 6.3.1), as the text-only models all improve from their caption-free settings using the Image Replacement strategy. However, interestingly, all LMMs prompted with Captioning Chain of Thought suffer a decline in the pass rates. A possible explanation is that the captions lengthen the context, while the images still remain in the context, causing trouble for the models to determine where to attend.

### 6.2 Error Analysis

To facilitate the understanding of the models' bottleneck in solving MMCode problems, an identical subset of 50 questions are randomly selected from the failure cases of GPT-4V and LLaVA-13B and reviewed. Figure 4 presents the results. The majority of errors arise in the wrong understanding of the problems, where executable codes are generated but with wrong results. GPT-4V produces fewer runtime errors than LLaVA-13B, including Access Errors (e.g. IndexError, KeyError), Type Errors (e.g. calling non-existing methods of an object), and Math Errors (e.g. ZeroDivisionError). Notably, LLaVA-13B makes many elementary mistakes such as wrong Input Parsing and NameError (e.g. usage of variables undefined or defined afterward). These errors prevent the programs from producing outputs that can be checked, resulting in a decrease in Problem Understanding errors.

### 6.3 Case Study

### 6.3.1 Caption Quality

Figures 5 to 16 in Appendix E showcase the captions generated by GPT-4V and Gemini Pro Vision of 12 images from different categories. Generally, GPT-4V generates more accurate and more insightful captions than Gemini Pro Vision. However, both models can hallucinate the images, especially on visually complex elements such as Graph (Figure 10). On the easier image of a Tree with fewer nodes and edges, both models produce correct explanations (Figure 9).

### 6.3.2 Code Quality

We examined solutions generated by GPT-4V in the section in Appendix F). Apart from complex logic errors and inefficient implementations (Section F.1, it still makes trivial mistakes, e.g. naming variables after built-in functions (Section F.2), reading inputs when the problem does not ask it to (Section F.3).

## 7 Conclusion

In this paper, we present MMCode, the first multimodal coding dataset for evaluating algorithmic problem-solving skills in image-text interwoven contexts. We benchmarked a range of state-ofthe-art LLMs and LMMs on MMCode and provide a detailed analysis. Despite their advanced capabilities, these models demonstrate a significant challenge in leveraging visual contexts for code generation. We believe that MMCode will catalyze further research and innovation, paving the way for the creation of AI systems capable of handling sophisticated visual and textual reasoning in programming and beyond.

## References

Ritu Agarwal and Atish P Sinha. 2003. Object-oriented modeling with uml: a study of developers' perceptions. Communications of the ACM, 46(9):248-256

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.

Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. 2024. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. arXiv preprint arXiv:2403.13315.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196.

Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008.
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. 2021. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.

Kaixin Li, Qisheng Hu, Xu Zhao, Hui Chen, Yuxi Xie, Tiedong Liu, Qizhe Xie, and Junxian He. 2023a. Instructcoder: Empowering language models for code editing. arXiv e-prints, pages arXiv-2310.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023b. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161.

Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. 2023c. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science, 378(6624):1092-1097.

Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems, 36 .

Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2023. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281.

Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255.

Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521.

Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023a. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023b. Wizardcoder: Empowering code large language models with evolinstruct. arXiv preprint arXiv:2306.08568.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations.

Bashar Nuseibeh and Steve Easterbrook. 2000. Requirements engineering: a roadmap. In Proceedings of the Conference on the Future of Software Engineering, pages 35-46.

OpenAI. 2022. Introducing ChatGPT. https:// openai.com/blog/chatgpt.

OpenAI. 2023a. Gpt-4 technical report. https:// arxiv.org/pdf/2303.08774.

OpenAI. 2023b. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_ System_Card.pdf. Accessed: 2024-02-03.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.

Team Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.

Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2024. Debugbench: Evaluating debugging capability of large language models. arXiv preprint arXiv:2401.04621.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.

Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Shuiguang Deng, et al. 2023a. Codescope: An execution-based multilingual multitask multidimensional benchmark for evaluating $11 \mathrm{~ms}$ on code understanding and generation. arXiv preprint arXiv:2311.08588.

Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, and Wen Wang. 2023b. Codetransocean: A comprehensive multilingual benchmark for code translation. arXiv preprint arXiv:2310.04951.

Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178.

Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. 2023. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502.

Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5673-5684.
