# RLSF: Reinforcement Learning via Symbolic Feedback 

Piyush Jha<br>Georgia Tech<br>piyush.jha@gatech.edu

Prithwish Jana<br>Georgia Tech<br>pjana7@gatech.edu<br>Vijay Ganesh<br>Georgia Tech<br>vganesh45@gatech.edu

Arnav Arora<br>Georgia Tech<br>aarora362@gatech.edu


#### Abstract

In recent years, large language models (LLMs) have had a dramatic impact on various sub-fields of $\mathrm{AI}$, most notably on natural language understanding tasks. However, there is widespread agreement that the logical reasoning capabilities of contemporary LLMs are, at best fragmentary (i.e., may work well on some problem instances but fail dramatically on others). While traditional LLM finetuning approaches (e.g., those that use human feedback) do address this problem to some degree, they suffer from many issues, including unsound black-box reward models, difficulties in collecting preference data, and sparse scalar reward values.

To address these challenges, we propose a new training/fine-tuning paradigm we refer to as Reinforcement Learning via Symbolic Feedback (RLSF), which is aimed at enhancing the reasoning capabilities of LLMs. In the RLSF setting, the LLM that is being trained/fine-tuned is considered as the RL agent, while the environment is allowed access to reasoning or domain knowledge tools (e.g., solvers, algebra systems). Crucially, in RLSF, these reasoning tools can provide feedback to the LLMs via poly-sized certificates (e.g., proofs), that characterize errors in the LLM-generated object with respect to some correctness specification. The ability of RLSF-based training/fine-tuning to leverage certificate-generating symbolic tools enables sound fine-grained (token-level) reward signals to LLMs, and thus addresses the limitations of traditional reward models mentioned above.


Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on two different applications, namely, program synthesis from natural language pseudo-code to programming language ( $\mathrm{C}++$ ), and solving the Game of 24 . For the former, RLSF-tuned LLMs show a significant improvement (e.g., $+52.64 \%$ in compilation accuracy and $+31.43 \%$ in functional correctness for Google's CodeGemma-2b) compared to supervised fine-tuning. Further, CodeGemma-2b gets $+34.82 \%$ in compilation accuracy and $+17.01 \%$ in functional correctness compared to ChatGPT ( $100 \times$ larger). Similarly, for the Game of 24 , we observed $+25 \%$ success rate using Meta's Llama2-7b compared to traditional methods, and $+7 \%$ success rate compared to ChatGPT ( $25 \times$ larger). A takeaway is that fine-tuning via RLSF enables relatively smaller LLMs to significantly outperform orders of magnitude larger models (e.g., ChatGPT).

Reinforcement Learning from Human Feedback (RLHF)

![](https://cdn.mathpix.com/cropped/2024_06_04_035b3754dd26dd18dbdfg-02.jpg?height=198&width=634&top_left_y=324&top_left_x=412)

Reinforcement Learning from Symbolic Feedback (RLSF)

![](https://cdn.mathpix.com/cropped/2024_06_04_035b3754dd26dd18dbdfg-02.jpg?height=214&width=624&top_left_y=316&top_left_x=1073)

Figure 1: Contrasting RLHF with RLSF: The image depicts two distinct fine-tuning paradigms. (Left) RLHF operates within an environment governed by a black-box reward model, typically offering scalar feedback. (Right) By contrast, the environment in RLSF leverages sound symbolic reasoning tools and also provides fine-grained token-level vector feedback that is, in turn, based on poly-sized certificates produced by these symbolic tools.

## 1 Introduction

In recent years, Large Language Models (LLMs) have had a dramatic impact on many sub-fields of AI [1]. Tasks that seemed impossible just a few years ago, are now routinely solved by LLMs. Examples include language translation [2, 3], text-to-image generation [4], coding assistants [5], and open-ended text generation [6].

Despite their impressive performance, these models often struggle with tasks that require complex reasoning [7, 8, 9]. This limitation has sparked a growing interest in exploring methods that can enhance the reasoning capabilities of LLMs, particularly through the incorporation of corrective feedback loops between learning and reasoning processes [10, 11]. For example, Kambhampati et al. [11] state that "LLMs cannot plan themselves but can play a variety of constructive roles in solving planning tasks-especially as approximate knowledge sources and candidate plan generators in the so-called LLM-Modulo Frameworks in conjunction with external sound model-based verifiers."

The concept of incorporating reasoning tools into machine learning in general, and LLMs in particular, is rooted in the idea of combining the strengths of these two sub-fields of AI [12, 10, 11, 13]. While LLMs excel at capturing statistical patterns and generating fluent text, they can fail to perform sound reasoning and generate text that is logically coherent. In fact, the logical or code errors introduced by LLMs in the objects they generate can be very subtle, and not immediately obvious upon inspection. This motivates the need to use reasoning and verification tools at different stages of an LLM's life cycle (from data curation, training, fine-tuning, and inference). For instance, using program analysis tools during inference [14], and integrating solvers into neural network layers [15] or during gradient descent [16] has shown promising results in terms of faster convergence and efficient data utilization.

By contrast to LLMs, symbolic reasoning systems, such as theorem provers and constraint solvers ${ }^{1}$ are adept at handling sound logical reasoning, perform symbolic mathematical operations and maintain coherence, but they do not seem to possess the impressive generative capabilities of LLMs. By integrating these two approaches, a hybrid system can be created that can leverage the strengths of both paradigms, potentially leading to more robust and capable AI systems.

One popular approach to fine-tuning LLMs is Reinforcement Learning from Human Feedback (RLHF) [17, 18], which relies on manually collecting correct/incorrect cases and creating an approximate (possibly unsound) black-box reward model. Such an approach can be expensive, error-prone, and may not accurately capture the nuances of a reasoning task. Moreover, the reward signal thus generated can be sparse and scalar in nature. Such sparse reward signals can fall short of fully capturing those aspects of an object generated by an LLM that may be incorrect with respect to a well-defined specification (or inconsistent with domain knowledge).

To address these challenges, we propose a new training/fine-tuning paradigm we refer to as Reinforcement Learning via Symbolic Feedback (RLSF) that is aimed at enhancing the reasoning capabilities of LLMs as well as making them compliant with domain knowledge. Figure 1 highlights the differences between RLHF and RLSF. In the RLSF setting, the LLM is considered as the RL[^0]agent to be trained/fine-tuned, while the environment is allowed access to reasoning tools, that can generate poly-sized certificates of their analyses.

In the forward direction (LLM to environment), formal objects generated (such as programs, proofs, circuits, molecules, or theories) by the LLM are fed to the RLSF environment, which leverages reasoning tools to perform analysis or verification of such objects against specifications or domain knowledge. In the reverse direction (environment to LLM), any certificate (that identifies what is wrong with the LLM-generated object) produced by symbolic tools is used as part of a reward function to provide corrective feedback to the LLM. Leveraging symbolic tools to generate poly-sized certificates ${ }^{2}$, which provide sound, fine-grained (token-level) feedback to LLMs, eliminates the need for manual preference data collection and addresses limitations of traditional reward models.

Contributions. In this paper, we present the RLSF paradigm and evaluate the effectiveness of RLSF-based fine-tuning methodologies on LLMs across two distinct application scenarios that present unique challenges to their reasoning capabilities. Our evaluation encompasses comprehensive experiments conducted in the domains of program synthesis from natural language pseudo-code to a programming language $(\mathrm{C}++$ ) and solving the Game of 24 (a standardized benchmark to test the logical reasoning capabilities of LLMs). Through our extensive experiments, we demonstrate the efficacy of RLSF in significantly enhancing the reasoning capabilities of LLMs across these diverse tasks.

Our results show a significant improvement over the supervised fine-tuning (SFT) approach for widelyused code LLMs (such as stable-code-instruct-3b [19], deepseek-coder-1 .3b-base [20] and Google's code-gemma-2b [21]) in the program synthesis task, against zero-shot and two traditional fine-tuning methods. For example, our RLSF-tuned code-gemma-2b shows an improvement of $+52.64 \%$ in compilation accuracy (CompAcc) and $+31.43 \%$ in functional correctness accuracy (FCorrAcc) over SFT. Further, the RLSF-tuned code-gemma-2b achieve superior results ( $+34.82 \%$ in CompAcc and $+17.01 \%$ in FCorrAcc) compared to ChatGPT (100x larger) [6]. For the Game of 24, we perform an extensive empirical evaluation of RLSF against different prompting and traditional fine-tuning methods. We observe significant improvement using RLSF on popular LLMs - Google's Gemma-2b-it [22] and Meta's Llama2-7b-chat [23] with $+15 \%$ and $+25 \%$ success respectively, compared to traditional methods. Notably, post RLSF fine-tuning, Llama2-7b-chat also outperforms ( $+7 \%$ ) ChatGPT ( $\sim 25 \times$ larger). This underscores the importance of RLSF fine-tuning that facilitates relatively smaller LLMs to significantly outperform models, such as ChatGPT, which are orders of magnitude larger.

## 2 Related Work

Neurosymbolic Reinforcement Learning (NRL): We refer the reader to the Compendium of Neurosymbolic AI for a rich and recent literature survey on combinations of learning and reasoning systems [12]. There has been considerable work in NRL [24]. However, all the NRL work that we are aware of focuses on combining symbolic reasoning tools with the RL agent, as opposed to our RLSF approach, where we combine the symbolic tools with the environment. Further, in the RLSF paradigm, a key innovation is that we leverage the poly-sized certificates generated (which characterize why the object generated by the RL agent is inconsistent with the specification or domain knowledge) by the environment's symbolic tools to provide a fine-grained reward signal to the RL agent. This allows for a modular framework with more expressive and interpretable feedback during the training process.

By contrast, Garnelo et al. [25] uses a typical NRL approach that advocates generating an embedding using neural networks and then learning a mapping from the embedding to a symbolic representation. However, they do not use the symbolic methods as part of the environment to generate certificates and use them to create appropriate reward signals. Further, these methods need to modify the underlying agent and are limited by the agent's capability for handling large symbolic representations.[^1]

```
Algorithm 1 Reinforcement Learning via Symbolic Feedback (RLSF)
Input: Number of epochs $N_{\text {epochs }}$, pre-trained model Model, symbolic reasoning tool
    SymbolicReasoner, reward function RewardFunc, prompt dataset $D$
Output: Fine-tuned model Model
    for $N_{\text {epochs }}$ do
        for batch $_{i}$ in $D$ do
            response ${ }_{i} \sim$ Model( batch $\left._{i}\right) \quad \triangleright$ Generate a response

```

![](https://cdn.mathpix.com/cropped/2024_06_04_035b3754dd26dd18dbdfg-04.jpg?height=41&width=1244&top_left_y=538&top_left_x=511)

```
            vector $_{i} \leftarrow$ RewardFunc $\left(\right.$ cert $\left._{i}\right) \quad \triangle$ Compute the vector feedback
            Model' $\leftarrow$ ppo_step $\left(\right.$ Model, batch $_{i}$, response $_{i}$, cert $\left._{i}\right) \quad \triangleright$ Update model using PPO
            Model $\leftarrow$ Model'
        end for
    end for
```

Combinations of LLMs with Symbolic Tools: Previous research has explored the integration of program analysis tools during inference [14]. While existing efforts have often relied on binary or scalar signals for utilizing symbolic feedback during training or fine-tuning [26, 27, 28, 29], the RLSF paradigm leverages richer token-level (vector) symbolic feedback, thus enabling significantly more effective reasoning capabilities. Kambhampati et al. [11] introduced the concept of LLMModulo Frameworks, which proposes the utilization of external sound symbolic reasoning tools. We concretize their philosophical position via the RLSF paradigm.

LLM-based Feedback: Another line of work that has explored using LLMs as part of a corrective feedback loop [30, 31, 32, 33, 34] where the feedback is usually provided as in-context (not RL), under the assumption that LLMs are better at verification compared to generation. However, recent studies have challenged this assumption, suggesting that LLMs may not be as effective at verification/selfcritiquing tasks as previously thought [7, 8, 35, 36, 37]. By contrast, in our work, we use sound symbolic systems to provide corrective feedback and do so via an RL approach.

## 3 Reinforcement Learning via Symbolic Feedback (RLSF)

In this section, we introduce the Reinforcement Learning via Symbolic Feedback (RLSF) algorithm. The algorithm incorporates fine-grained, token-level feedback generated by reasoning or domain knowledge tools, thereby addressing the limitations of traditional reward-based methods.

### 3.1 Algorithm Description

The RLSF algorithm, outlined in Algorithm 1, fine-tunes a pre-trained language model Model using reinforcement learning (RL) with the help of the fine-grained certificate provided by a symbolic reasoning tool SymbolicReasoner. The framework leverages a reward function (RewardFunc) to compute vector feedback ( vector $)_{i}$ ) based on the certificate generated by the symbolic reasoning tool. This feedback aligns with the shape of the response generated by the language model, facilitating fine-grained adjustments during fine-tuning. The algorithm operates over a specified number of epochs $N_{\text {epochs }}$, iterating through a dataset $D$.

Inputs and Outputs. The algorithm takes as input the pre-trained model (RL agent to be fine-tuned) Model, the symbolic reasoning tool SymbolicReasoner and the reward function RewardFunc (to be used as the RL environment), and the dataset $D$ that consists of input prompts for Model. The output is a fine-tuned model Model'. The algorithm performs the following steps:

Epoch Iteration: For each epoch from 1 to $N_{\text {epochs }}$, the algorithm iterates through $D$.

Batch Processing: For each batch batch ${ }_{i}$ in the dataset $D$, the algorithm performs the following:

![](https://cdn.mathpix.com/cropped/2024_06_04_035b3754dd26dd18dbdfg-04.jpg?height=38&width=1368&top_left_y=2269&top_left_x=384)
prompt batch ${ }_{i}$.

- Certificate computation (cert $)$ ): The symbolic reasoning tool SymbolicReasoner computes a certificate cert $_{i}$ corresponding to the response response ${ }_{i}$.
- Vector feedback computation (vector $)_{i}$ ): The reward function (RewardFunc) calculates a vector feedback (vector $)_{i}$ ) based on the certificate cert $_{i}$. This certificate typically includes fine-grained

![](https://cdn.mathpix.com/cropped/2024_06_04_035b3754dd26dd18dbdfg-05.jpg?height=467&width=1261&top_left_y=249&top_left_x=432)

Figure 2: RLSF for translation of NL pseudo-code to code: In this setting, the symbolic environment utilizes the $\mathrm{g}++$ compiler to check whether the generated $\mathrm{C}++$ code compiles or not. If it compiles, the vectorized feedback has the value $p=1+r$ (higher reward indicating the positive case) corresponding to every token. Otherwise, a reward value of $n=0$ (lower reward indicating the negative case) is returned for every token representing an erroneous line, and $p$ for all other tokens. The special tokens $<$ BOS $>$ and $<\mathrm{EOS}>$ are both $p$ for the first case, and both $n$ for the second.

![](https://cdn.mathpix.com/cropped/2024_06_04_035b3754dd26dd18dbdfg-05.jpg?height=366&width=1268&top_left_y=999&top_left_x=426)

Figure 3: RLSF for the Game of 24: In this illustration, the symbolic environment utilizes the Computer Algebra System (CAS) library SymPy [41] to generate a vectorized reward as feedback. Each element in the vector corresponds to a token in the response, where erroneous tokens are penalized with a value of 0 and correct ones are assigned 1 . The last element of the vector reward (corresponding to the $<\mathrm{EOS}>$ token) is 1 only if the entire response is correct, otherwise, it is 0 .

error messages or other relevant information extracted from the symbolic analysis of the promptresponse pair. The reward function processes this certificate to generate token-level vector feedback. This token-level feedback provides detailed guidance to the language model during fine-tuning, facilitating precise adjustments to its logical reasoning capabilities. The vector feedback has the same shape as the response tokens (computed using the tokenizer provided by Model). Figures 2 and 3 give a concrete example of such a vector feedback for the two tasks.

- Model update (Model'): The Model is updated to Model' using the Proximal Policy Optimization (PPO) algorithm, using the input prompt batch $_{i}$, response response ${ }_{i}$, and certificate cert $_{i}$.


### 3.2 Implementation, TRL Library, and Hardware Details

The RLSF algorithm is implemented using a modified version of the Transformer Reinforcement Learning (TRL) library [38], a comprehensive framework integrated with Huggingface transformers [39] designed for training transformer language models with reinforcement learning (RL). We use the Proximal Policy Optimization (PPO) algorithm, commonly used in Reinforcement Learning from Human Feedback (RLHF) [40, 17, 18], for fine-tuning the language model. However, TRL only allows for scalar reward signals during the RL fine-tuning process. We modify the library to support vector reward signals, allowing fine-grained feedback at the token level. This enhancement enables RLSF to leverage symbolic feedback effectively during the RL process. All our experiments were conducted on a high-performance CentOS V7 cluster equipped with Intel E5-2683 v4 Broadwell processors running at $2.10 \mathrm{GHz}$ and $64 \mathrm{GiB}$ of memory. We used 4 NVIDIA V100 GPUs for the task in Section 4 and 1 NVIDIA V100 GPU for the task in Section 5 .

## 4 Reasoning Task A: Natural Language Pseudo-code to Code (C++)

Automated code synthesis from natural language (NL) descriptions is a crucial task in software engineering. It has garnered significant attention over the years, and recently, researchers [42, 43] have been attempting to address it using LLMs. Motivated by this, we evaluate how the RLSF paradigm enhances LLM-based NL to programming language (PL) generation. Specifically, we focus on translating a pseudo-code expressed in NL to C++ with the following properties: the generated code must be (a) syntactically correct w.r.t. g++ compiler and (b) functionally correct (or equivalent to) the pseudo-code w.r.t a test suite ${ }^{3}$

### 4.1 Benchmark and RLSF setup

We evaluate using the SPoC dataset [44], which comprises a collection of contest problems from the competitive programming website Codeforces [45]. Kulal et al. [44] gathered 16,326 C++ programs that were accepted as successful solutions to 677 problems hosted on Codeforces. Each problem is accompanied by a test suite containing multiple test cases. Additionally, they employed 59 crowdworkers from Amazon Mechanical Turk to write NL pseudo-code corresponding to each line of the C++ code, ensuring a line-level granularity of text description. Each program comprises an average of 14.7 lines and the number of lines ranges in $[1,457]$. In this paper, we use the TESTP partition of the SPoC dataset, which set aside 158 out of 677 problems ( $\sim 23.34 \%$ ) for testing, resulting in 1,778 pairs of pseudo-code and C++ code. The remaining data, consisting of 14,548 pairs across 519 problems ( $\sim 76.66 \%$ ), was used to train the LLMs.

Please refer to Figure 2 for an illustration of the RLSF setup used in code generation from pseudo-code. Let us assume that in the supervised learning setup, we have a training dataset $\mathcal{D}=\left\{\left(p c_{i}, c_{i}, T S_{i}\right)\right\}$ with triplets of pseudo-code $p c_{i}$, gold-standard code $c_{i}$, and a test-suite $T S_{i}$. In each training step, the LLM is first provided with the prompt shown in Figure 2 The LLM responds by generating code $\widehat{c_{i}}$, which may or may not be syntactically correct. This code is then passed through the target language compiler (in this case, $\mathrm{g}^{++}$), which identifies the set of line numbers $(E)$ responsible for any syntactic errors. Further, if $\widehat{c}_{i}$ compiles, it is run on $T S_{i}$. Accordingly, the pass rate $(r \in[0,1])$ is calculated which represents the fraction of test cases in $T S_{i}$ exhibiting the expected input-output behavior. Note that, $r=0$ if $\widehat{c_{i}}$ does not compile. Using the vectorized feedback, we aim to assign a high scalar reward $p$ (indicating a positive case) to any token corresponding to a line in $\widehat{c_{i}}$ that is free of syntactic errors. Conversely, we want to assign a low scalar reward $n$ (indicating a negative case) to any token corresponding to a line in $\widehat{c_{i}}$ containing a syntactic error. Considering $p=1+r$ and $n=0$, the vectorized feedback is thereby represented as:

$$
\begin{align*}
& \text { RewardFunc }\left(\text { SymbolicReasoner }\left(\left(p c_{i}, c_{i}, T S_{i}\right),<\operatorname{BOS}>\widehat{c_{i}}<\mathrm{EOS}>\right)\right) \\
& = \begin{cases}\mathbf{p}_{\left|\widehat{c_{i}}\right|+2} \text { i.e. }[p, p, \ldots, p] \in \mathbb{R}^{\left|\widehat{c}_{i}\right|+2} & \text {, if } \widehat{c}_{i} \text { compiles } \\
{[n] \|\left[\operatorname{rwd}(x) \mid \forall x \in \widehat{c_{i}}, \operatorname{rwd}(x) \leftarrow n \text { if line }(x) \in E, \text { else } \operatorname{rwd}(x) \leftarrow p\right] \|[n]} & \text {, otherwise }\end{cases} \tag{1}
\end{align*}
$$

where $\left|\widehat{c_{i}}\right|$ denotes the number of tokens in $\widehat{c_{i}}, x$ represents a token in $\widehat{c_{i}}$, and $\|$ indicates concatenation. Note that, the token-wise reward is 0 for tokens in syntactically erroneous lines, 1 for lines that compile in otherwise incorrect code, and between $[1,2]$ for lines in syntactically correct code. The rewards for $<\mathrm{BOS}>$ and $<\mathrm{EOS}>$ tokens are $p$ when $\widehat{c_{i}}$ compiles and $n$ otherwise. This vectorized symbolic feedback refines the LLM through Proximal Policy Optimization (Algorithm 1).

### 4.2 Evaluation Metrics

Compilation Accuracy (CompAcc). The percentage of generated $\mathrm{C}++$ codes that are syntactically correct, indicating that it compiles without errors using a g++ compiler.

Functional Correctness Accuracy (FCorrAcc). The percentage of test cases demonstrating the expected input-output behavior for each generated $\mathrm{C}++$ code, averaged across all the generated codes. If a generated code contains syntactical errors, FCorrAcc is zero for that code.[^2]

Table 1: Natural Language Pseudo-code to Code Translation Results: Performance comparison of fine-tuning methodologies over different LLMs

| LLM architecture | Fine-Tuning methodology | CompAcc $(\%)$ | FCorrAcc $(\%)$ |
| :---: | :---: | :---: | :---: |
| GPT-3. 5 (ChatGPT) 6 | Zero-shot (no fine-tuning) | 29.13 | 24.29 |
|  | Zero-shot (no fine-tuning) | 0.00 | 0.00 |
| code-gemma-2b 21 | SFT (with cross-entropy loss) | 11.31 | 9.87 |
|  | RL (with Boolean scalar f/b) | 54.89 | 24.71 |
|  | RLSF (with vectorized f/b) | 63.95 | 41.30 |
|  | Zero-shot (no fine-tuning) | 0.00 | 0.00 |
| stable-code-instruct-3b 19 | SFT (with cross-entropy loss) | 12.04 | 10.78 |
|  | RL (with Boolean scalar f/b) | 48.43 | 10.91 |
|  | RLSF (with vectorized f/b) | 54.27 | 18.44 |
| deepseek-coder-1.3b-base 20 | Zero-shot (no fine-tuning) | 0.00 | 0.00 |
|  | SFT (with cross-entropy loss) | 2.19 | 1.90 |
|  | RL (with Boolean scalar f/b) | 19.97 | 8.07 |
|  | RLSF (with vectorized $f / b$ ) | 38.92 | 25.89 |

Note that, the codes in the SPoC dataset do not contain any '\#include' preprocessor directives or 'using namespace' lines. As a result, the LLMs are not trained to generate these directives in their output. To address this, as a post-processing step, we prepend each code with a fixed and sufficiently comprehensive set of 21 preprocessor directives, followed by a 'using namespace std;' line. This is done before calculating CompAcc, FCorrAcc, and the vectorized feedback, to ensure that an LLM-generated code using standard user-defined libraries compiles with the g++ compiler.

### 4.3 Comparative Models Used

We perform RLSF fine-tuning over three recent open-sourced code LLMs from Huggingface [46] namely, Google's CodeGemma-2b [21], StabilityAI's Stable-Code-Instruct-3b [19], and DeepseekAI's Deepseek-Coder-1.3b-base [20]. We also include results against the closed-source ChatGPT model, i.e., OpenAI's gpt-3.5-turbo-0301 [6]. We use the OpenAI API [47] to access this model. To ensure reproducibility, the temperature and top_p parameters are both set to 0 . It is worth noting that the three open-sourced models have 2 billion, 2.7 billion, and 1.3 billion parameters, respectively. In contrast, GPT-3.5 (gpt-3.5-turbo-0301) is rumored to have around 175 billion parameters, which is about 100 times more.

### 4.4 Results and Ablation Study

In Table 1. we begin by evaluating all the open-sourced and closed-sourced LLMs in a zero-shot setting. Among all the $\mathrm{C}++$ codes generated by GPT-3.5, $29.13 \%$ are compilable. On average, for each generated code, $24.29 \%$ of the test cases exhibit the expected input-output behavior. In contrast, none of the codes produced by the three open-source models (code-gemma-2b, stable-code-instruct-3b, and deepseek-coder-1.3b-base) compile or function correctly.

Next, we perform supervised fine-tuning (SFT) on three open-source LLMs, minimizing the cross-entropy loss between the generated and gold-standard $\mathrm{C}++$ code. code-gemma- $2 \mathrm{~b}$ and stable-code-instruct-3b achieves approximately $12 \%$ CompAcc and 10\% FCorrAcc. However, deepseek-coder-1.3b-base attains only around 2\% CompAcc and FCorrAcc after SFT.

We further fine-tune LLMs trained with SFT using two alternative methods. First, we use $R L$ with Boolean feedback, where LLMs are rewarded based on whether their generated C++ code compiles. It is indicated by a Boolean scalar feedback $\{0,1\}$ from the $\mathrm{g}++$ compiler. This method improves the CompAcc metric by $+43.58 \%,+36.39 \%$, and $+17.78 \%$, and the FCorrAcc metric by $+14.84 \%,+0.13 \%$, and $+6.17 \%$ for code-gemma- $2 \mathrm{~b}$, stable-code-instruct-3b, and deepseek-coder-1.3b-base, respectively. As an alternative, we use the proposed RLSF scheme with vectorized feedback. This method enhances CompAcc by $+52.64 \%,+42.23 \%$, and $+36.73 \%$, and FCorrAcc by $+31.43 \%,+7.66 \%$, and $+23.99 \%$ for the same models compared to SFT.

In essence, Boolean feedback merely indicates whether the generated code compiles. Conversely, RLSF's vectorized feedback offers precise, token-level rewards based on line compilation success and overall test-case pass rate, leading to superior code generation. As a result, the RLSF-tuned models outperform the respective RL-tuned models with Boolean feedback by $+9.06 \%,+5.84 \%$, and $+18.95 \%$ in CompAcc and $+16.59 \%,+7.53 \%$, and $+17.82 \%$ in FCorrAcc. Also, the RLSF-tuned models achieve superior results compared to ChatGPT, using around 100 times fewer parameters.

## 5 Reasoning Task B: Game of 24 using Tree of Thoughts (ToT)

The Game of 24 is a well-known benchmark aimed at testing the reasoning capabilities of LLMs. Briefly, the idea behind the Game of 24 is as follows: given 4 numbers and basic arithmetic operations, obtain the target number 24. We refer the reader to the paper on Tree of Thoughts (ToT) by Yao et al. [48] for more details. To solve the Game of 24 using ToT, the process involves decomposing the problem into three steps, each representing an intermediate equation. Starting with the given input numbers, the LLM is prompted to propose possible next steps (or "thoughts") using a "propose prompt". The authors employ a breadth-first search (BFS) approach in ToT, maintaining the top 5 candidates at each step. Now, we prompt the LLM using the value prompt to evaluate the "thoughts." The score given by the LLM using the value prompt helps prune the "thoughts" generated by the "propose prompt." These two prompts are repeated, starting with the three remaining numbers (from all thoughts accepted after using the value prompt) to build the ToTs. This process is repeated until you arrive at the final equation that results in the number 24 .

### 5.1 Benchmark and RLSF setup

Similar to [48], we collect data from 4nums . com a website hosting mathematical games, specifically selecting 1,362 games sorted by human solving time from easy to hard. We use the same games as used by [48], indexed 901-1,000, for testing purposes. In evaluating each task, success is defined as producing a valid equation that equals 24 while utilizing each input number exactly once. For the RLSF fine-tuning phase, we utilize a subset of games from indices 800-900. The RLSF fine-tuning occurs during the "propose prompt" steps (using the prompt styles from ToT [48]), as depicted in Figure 3 We collect pairs of propose prompts and responses provided by the LLM. Periodically, after several prompts, we invoke the CAS [49] (using SymPy [41]) to generate a feedback to the LLM (Figure 3). This feedback is then employed to fine-tune the LLM using Proximal Policy Optimization (PPO) following Algorithm 1

### 5.2 Comparative Methods and Models Used

We incorporate the benchmarks previously used by [48] (i.e., standard Input-Output (IO) prompting and Chain-of-Thought (CoT) prompting) and ToT prompting. IO prompting uses five in-context examples, while CoT prompting includes three intermediate equations for these in-context examples. Both IO and CoT prompting are sampled 100 times per game for average performance assessment. To showcase the improvement due to fine-grained vectorized feedback, we perform an ablation study where we compare the binary (scalar) and vectorized versions of feedback for the RL fine-tuning. After RL fine-tuning, we evaluate the performance on the test set using ToT with the updated LLM. We perform RLSF fine-tuning on two popular smaller open-source LLM models (Gemma-2b-it [22] and Llama2-7b-chat [23]) that we obtain from Huggingface [46] and also compare against two closed-source models GPT-3.5 and GPT-4 [6].

### 5.3 Results

We conduct a comparative analysis of different methods applied across various LLMs to tackle the Game of 24 task (Table 2). As observed by [48], the Tree of Thoughts (ToT) prompt method emerges as the most successful approach for both closed-source models GPT-3.5 and GPT-4, achieving success rates of $19 \%$ and $69 \%$, respectively. This performance surpasses that of both the IO and CoT prompt methods. However, Gemma-2b-it and Llama2-7b-chat exhibit poor performance across all the three prompting methods.

We explore the use of Boolean scalar feedback for RL fine-tuning, where the Computer Algebra System (CAS) provides binary feedback based on the correctness of the generated response. However,

Table 2: Game of 24 Results: Performance comparison of methods over different LLMs

| LLM | Method | Success |
| :---: | :---: | :---: |
|  | IO prompt | $6 \%$ |
| GPT-3.5 6 | CoT prompt | $3 \%$ |
|  | ToT prompt | $19 \%$ |
|  | IO prompt | $7.3 \%$ |
| GPT-4 6 | CoT prompt | $4 \%$ |
|  | ToT prompt | $69 \%$ |
|  | IO prompt | $1 \%$ |
|  | CoT prompt | $3 \%$ |
| Gemma-2b-it 22 | ToT prompt | $2 \%$ |
|  | ToT after RL with Boolean scalar $\mathrm{f} / \mathrm{b}$ | $1 \%$ |
|  | ToT after RLSF with vectorized f/b | $\mathbf{1 7 \%}$ |
|  | IO prompt | $5 \%$ |
|  | CoT prompt | $1 \%$ |
| Llama2-7b-chat 23 | ToT prompt | $1 \%$ |
|  | ToT after RL with Boolean scalar $\mathrm{f} / \mathrm{b}$ | $1 \%$ |
|  | ToT after RLSF with vectorized f/b | $26 \%$ |

we observe either degradation or maintenance of performance levels with this feedback mechanism (Table 2). Consequently, we transition to a vectorized feedback approach using RLSF, where the symbolic environment provides token-level feedback (Figure 3), resulting in a significant improvement in performance. Specifically, after employing Reinforcement Learning via Symbolic Feedback (RLSF) fine-tuning, Gemma-2b-it demonstrates a $15 \%$ increase in success rate, while Llama2-7b-chat exhibits a $25 \%$ improvement in success rate compared to ToT prompting prior to RLSF fine-tuning. Notably, the 7-billion-parameter Llama2-7b-chat outperforms ( $+7 \%$ ) the 175-billion-parameter GPT-3.5 model, underscoring the effectiveness of RLSF in enhancing model performance. However, GPT-4 demonstrates the best performance across all methods. We attribute this to its ultra-large-scale pre-training and architecture advancements. Looking ahead, future investigations can explore the application of RLSF on larger open-source LLMs.

## 6 Conclusions, Limitations, and Future Work

In this paper, we present RLSF, a learning paradigm for enhancing the reasoning capabilities of LLMs by integrating RL-based symbolic feedback into their fine-tuning process. By combining the strengths of LLMs with symbolic reasoning capabilities, RLSF offers a promising avenue for developing more robust and capable artificial intelligence systems. Our results show a significant improvement in both the program synthesis task and the Game of 24 , over different traditional prompting and fine-tuning methods. Notably, the RLSF-tuned code-gemma-2b achieves superior results compared to ChatGPT $(100 \times$ larger) [6] on the program synthesis task. Similarly, RLSF-tuned Llama2-7b-chat also outperforms ChatGPT ( $25 \times$ larger).

Limitations and future work. The study demonstrates empirical improvements achievable through the integration of symbolic feedback using RL frameworks, marking just the initial strides in this promising avenue. While our evaluation tasks have predominantly centered on reasoning tasks within specific domains like program synthesis and a mathematical problem, there are many domains where this approach is applicable, but the results may be more nuanced than we expect. Future research may explore theoretical guarantees and formalizing the effectiveness of integrating symbolic feedback into RL frameworks.

Broader impact. The integration of symbolic feedback into RL frameworks, as demonstrated in RLSF, could lead to significant advancements in the reasoning capabilities of LLMs. This can pave the way for more intelligent AI systems capable of understanding and reasoning over complex tasks and domains such as mathematics, physics, drug discovery, and software engineering, where reasoning is essential. Our results also show that LLMs trained/fine-tuned with RLSF can outperform significantly
larger models in reasoning tasks. A potential broader impact could be that RLSF leads to the design of small LMs where reasoning tools are integral to their training and inference, which is simultaneously as performant as very large LMs. Finally, it is crucial to ensure that both the underlying pre-trained LLM and the symbolic reasoning tool are free from biases and do not compromise privacy or security, as defined by their application contexts. To address these concerns, appropriate monitoring, bias detection strategies, and increased awareness of responsible usage must be implemented.

## References

[1] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., "On the opportunities and risks of foundation models," arXiv preprint arXiv:2108.07258, 2021.

[2] L. Qin, Q. Chen, Y. Zhou, Z. Chen, Y. Li, L. Liao, M. Li, W. Che, and P. S. Yu, "Multilingual large language model: A survey of resources, taxonomy and frontiers," arXiv preprint arXiv:2404.04925, 2024.

[3] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth, "Recent advances in natural language processing via large pre-trained language models: A survey," ACM Computing Surveys, vol. 56, no. 2, pp. 1-40, 2023.

[4] C. Zhang, C. Zhang, M. Zhang, and I. S. Kweon, "Text-to-image diffusion model in generative ai: A survey," arXiv preprint arXiv:2303.07909, 2023.

[5] J. T. Liang, C. Yang, and B. A. Myers, "A large-scale survey on the usability of ai programming assistants: Successes and challenges," in Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1-13, 2024.

[6] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., "Gpt-4 technical report," arXiv preprint arXiv:2303.08774, 2023.

[7] Y. Mao, Y. Kim, and Y. Zhou, "Champ: A competition-level dataset for fine-grained analyses of llms' mathematical reasoning capabilities," arXiv preprint arXiv:2401.06961, 2024.

[8] K. Stechly, K. Valmeekam, and S. Kambhampati, "On the self-verification limitations of large language models on reasoning and planning tasks," arXiv preprint arXiv:2402.08115, 2024.

[9] H. Zhang, L. H. Li, T. Meng, K.-W. Chang, and G. V. d. Broeck, "On the paradox of learning to reason from data," arXiv preprint arXiv:2205.11502, 2022.

[10] V. Ganesh, S. A. Seshia, and S. Jha, "Machine learning and logic: a new frontier in artificial intelligence," Formal Methods in System Design, vol. 60, no. 3, pp. 426-451, 2022.

[11] S. Kambhampati, K. Valmeekam, L. Guan, K. Stechly, M. Verma, S. Bhambri, L. Saldyt, and A. Murthy, "Llms can't plan, but can help planning in llm-modulo frameworks," arXiv preprint arXiv:2402.01817, 2024.

[12] P. Hitzler, M. K. Sarker, and A. Eberhart, Compendium of Neurosymbolic Artificial Intelligence, vol. 369. IOS Press, 2023

[13] Z. Bouraoui, A. Cornuéjols, T. Denœux, S. Destercke, D. Dubois, R. Guillaume, J. Marques-Silva, J. Mengin, H. Prade, S. Schockaert, et al., "From shallow to deep interactions between knowledge representation, reasoning and machine learning (kay r. amel group)," arXiv preprint arXiv:1912.06612, 2019 .

[14] L. A. Agrawal, A. Kanade, N. Goyal, S. Lahiri, and S. Rajamani, "Monitor-guided decoding of code lms with static analysis of repository context," Advances in Neural Information Processing Systems, vol. 36, 2024.

[15] Z. Wang, S. Vijayakumar, K. Lu, V. Ganesh, S. Jha, and M. Fredrikson, "Grounding neural inference with satisfiability modulo theories," Advances in Neural Information Processing Systems, vol. 36, 2024.

[16] D. Ashok, V. Nagisetty, C. Srinivasa, and V. Ganesh, "A solver+ gradient descent training algorithm for deep neural networks," arXiv preprint arXiv:2207.03264, 2022.

[17] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., "Training language models to follow instructions with human feedback," Advances in neural information processing systems, vol. 35, pp. 27730-27744, 2022.

[18] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano, "Learning to summarize with human feedback," Advances in Neural Information Processing Systems, vol. 33, pp. 3008-3021, 2020.

[19] N. Pinnaparaju, R. Adithyan, D. Phung, J. Tow, J. Baicoianu, A. Datta, M. Zhuravinskyi, D. Mahan, M. Bellagente, C. Riquelme, et al., "Stable code technical report," arXiv preprint arXiv:2404.01226, 2024.

[20] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li, et al., "Deepseekcoder: When the large language model meets programming-the rise of code intelligence," arXiv preprint arXiv:2401.14196, 2024.

[21] Google, "Codegemma: Open code models based on gemma," 2024.

[22] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, et al., "Gemma: Open models based on gemini research and technology," arXiv preprint arXiv:2403.08295, 2024.

[23] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.

[24] K. Acharya, W. Raza, C. Dourado, A. Velasquez, and H. H. Song, "Neurosymbolic reinforcement learning and planning: A survey," IEEE Transactions on Artificial Intelligence, 2023.

[25] M. Garnelo, K. Arulkumaran, and M. Shanahan, "Towards deep symbolic reinforcement learning," arXiv preprint arXiv:1609.05518, 2016.

[26] Y. Chen, C. Wang, O. Bastani, I. Dillig, and Y. Feng, "Program synthesis using deduction-guided reinforcement learning," in International Conference on Computer Aided Verification, pp. 587-610, Springer, 2020.

[27] P. Jana, P. Jha, H. Ju, G. Kishore, A. Mahajan, and V. Ganesh, "Cotran: An llm-based code translator using reinforcement learning with feedback from compiler and symbolic execution," arXiv preprint arXiv:2306.06755, 2023.

[28] P. Jha, J. Scott, J. S. Ganeshna, M. Singh, and V. Ganesh, "Bertrlfuzzer: A bert and reinforcement learning based fuzzer (student abstract)," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 23521-23522, 2024.

[29] J. Zou, X. Zhang, Y. He, N. Zhu, and T. Leng, "Fgeo-drl: Deductive reasoning for geometric problems through deep reinforcement learning," Symmetry, vol. 16, no. 4, p. 437, 2024.

[30] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, "Reflexion: Language agents with verbal reinforcement learning," Advances in Neural Information Processing Systems, vol. 36, 2024.

[31] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al., "Self-refine: Iterative refinement with self-feedback," Advances in Neural Information Processing Systems, vol. 36, 2024.

[32] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings, "Refiner: Reasoning feedback on intermediate representations," arXiv preprint arXiv:2304.01904, 2023.

[33] G. Kim, P. Baldi, and S. McAleer, "Language models can solve computer tasks," Advances in Neural Information Processing Systems, vol. 36, 2024.

[34] X. Chen, M. Lin, N. Schärli, and D. Zhou, "Teaching large language models to self-debug," arXiv preprint arXiv:2304.05128, 2023.

[35] K. Valmeekam, M. Marquez, and S. Kambhampati, "Can large language models really improve by self-critiquing their own plans?," arXiv preprint arXiv:2310.08118, 2023.

[36] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou, "Large language models cannot self-correct reasoning yet," arXiv preprint arXiv:2310.01798, 2023.

[37] K. Stechly, M. Marquez, and S. Kambhampati, "Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems," arXiv preprint arXiv:2310.12397, 2023

[38] L. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, N. Lambert, and S. Huang, "Trl: Transformer reinforcement learning." https://github.com/huggingface/trl 2020.

[39] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, "Transformers: State-of-the-art natural language processing," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, (Online), pp. 38-45, Association for Computational Linguistics, Oct. 2020.

[40] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, S. Zhong, B. Yin, and X. Hu, "Harnessing the power of llms in practice: A survey on chatgpt and beyond," ACM Transactions on Knowledge Discovery from Data, vol. 18, no. 6, pp. 1-32, 2024.

[41] A. Meurer, C. P. Smith, M. Paprocki, O. Čertík, S. B. Kirpichev, M. Rocklin, A. Kumar, S. Ivanov, J. K. Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger, R. P. Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johansson, F. Pedregosa, M. J. Curry, A. R. Terrel, v. Roučka, A. Saboo, I. Fernando, S. Kulal, R. Cimrman, and A. Scopatz, "Sympy: symbolic computing in python," PeerJ Computer Science, vol. 3, p. e103, Jan. 2017.

[42] G. Poesia, O. Polozov, V. Le, A. Tiwari, G. Soares, C. Meek, and S. Gulwani, "Synchromesh: Reliable code generation from pre-trained language models," arXiv preprint arXiv:2201.11227, 2022.

[43] S. Ugare, T. Suresh, H. Kang, S. Misailovic, and G. Singh, "Improving llm code generation with grammar augmentation," arXiv preprint arXiv:2403.01632, 2024.

[44] S. Kulal, P. Pasupat, K. Chandra, M. Lee, O. Padon, A. Aiken, and P. S. Liang, "Spoc: Search-based pseudocode to code," Advances in Neural Information Processing Systems, vol. 32, 2019.

[45] Codeforces, 2023. https://codeforces.com/

[46] HuggingFace, "The AI Community Building the Future," 2023. https://huggingface.co/

[47] OpenAI, "ChatGPT [Large Language Model]," 2023. https://chat.openai.com

[48] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, "Tree of thoughts: Deliberate problem solving with large language models," Advances in Neural Information Processing Systems, vol. 36, 2024.

[49] J. Von Zur Gathen and J. Gerhard, Modern computer algebra. Cambridge university press, 2013.


[^0]:    ${ }^{1}$ We define the term symbolic reasoning systems broadly to include solvers, provers, computer algebra systems, program analysis tools, knowledge bases, and simulators. The only requirement is that they analyze/solve inputs that are formally defined, and can produce poly-sized certificates of their analysis.

[^1]:    ${ }^{2}$ Examples of poly-sized certificates include unsatisfiability proofs, compiler feedback, equivalence testing, etc. Our approach is not limited to any one type of symbolic tool, as long as these certificates can be converted into appropriate rewards. It is possible that the reasoning problem addressed by these symbolic tools is NP-hard, and therefore, in general, we cannot always expect certificates that are polynomial in the input size. Having said that, many of these tools, such as compilers, computer algebra systems, or solvers, work well in practice.

[^2]:    ${ }^{3}$ Note that in general the task of determining whether a $\mathrm{C}++$ code is functionally equivalent to pseudo-code (even when it is specified in formal logic) is undecidable. Hence, we consider a weaker functional equivalence property, namely, that a generated $\mathrm{C}++$ code is deemed functionally correct if it passes all test cases corresponding to a given pseudo-code.

