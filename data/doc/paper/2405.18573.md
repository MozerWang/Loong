# Programmer Visual Attention During Context-Aware Code Summarization 

Aakash Bansal, Robert Wallace, Zachary Karas, Ningzhi Tang, Yu Huang, Toby Jia-Jun Li, Collin McMillan


#### Abstract

Programmer attention represents the visual focus of programmers on parts of the source code in pursuit of programming tasks. The focus of current research in modeling this programmer attention has been on using mouse cursors, keystrokes, or eye-tracking equipment to map areas in a snippet of code. These approaches have traditionally only mapped attention for a single method. However, programming tasks such as source code summarization, require programmers to use contextual knowledge that can only be found in other parts of the project. To address this knowledge gap, we conducted an in-depth human study with XY Java programmers, where each programmer generated summaries for 40 methods from five large Java projects over five one-hour sessions. We used eye-tracking equipment to map the visual attention of programmers while they wrote the summaries. We also rate the quality of each summary. We found eye-gaze patterns and metrics that define common behaviors between programmer attention during context-aware code summarization. Specifically, we found that programmers need to read significantly ( $\mathrm{p}<0.01$ ) fewer words and make significantly fewer revisits to words ( $\mathrm{p}<0.03$ ) as they summarize more methods during a session, while maintaining the quality of summaries. We also found that the amount of source code a participant looks at correlates with a higher quality summary, but this trend follows a bell-shaped curve, such that after a threshold reading more source code leads to a significant decrease ( $\mathrm{p}<0.01$ ) in the quality of summaries. We also gathered insight into the type of methods in the project that provide the most contextual information for code summarization based on programmer attention. Specifically, we observed that programmers spent a majority of their time looking at methods inside the same class as the target method to be summarized. Surprisingly, we found that programmers spent significantly less time looking at methods in the call graph of the target method. We discuss how our empirical observations may aid future studies towards modeling programmer attention and improving context-aware automatic source code summarization.


Index Terms-automatic documentation generation, source code summarization, neural networks, context-aware models.

## 1 INTRODUCTION

Code summarization is the act of writing a natural language description for a snippet of source code, such as a subroutine. These summaries are meant to fill the knowledge gap between a higher-level understanding of the program that is necessary to use it, and the lower-level implementation in the code. The automatic generation of these summaries has been an important goal for the software engineering research community for decades. Lately, tools such as GitHub Copilot and OpenAI ChatGPT with automatic code summarization capabilities are increasingly becoming part of Software Engineering workflows [1|. At the core of these tools are Large Language Models (LLMs) that learn representations of both the programming language and natural language in an attempt to fill the aforementioned knowledge gap. They can be thought of as translating code into natural language summaries.

However, these summaries do not necessarily reflect real-world scenario, as they are not "context-aware". A sum-

- Manuscript received --. This work is supported in part by the NSF CCF-2100035. Any opinions, findings, and conclusions expressed herein are the authors' and do not necessarily reflect those of the sponsors.
- The authors are with the Department of Computer Science and Engineering, University of Notre Dame, IN 46556, USA.

E-mail: \{abansal1,rwallac1,ntang,toby.j.li,cmc \} @nd.edu

and with the Department of Computer Science, Vanderbilt University, TN 37235, USA.

E-mail: $\{$ z.karas,yu.huang\} @vanderbilt.edu

- This paper has supplementary downloadable multimedia material available at https:// provided by the authors. mary is context-aware when it considers the context, i.e., information from outside the subroutine. Although there are several types of context, for the purposes of this study, context is source code from the project surrounding the subroutine. Context-aware summaries are important because a subroutine usually does not exist in a vacuum; the manner in which a subroutine is used depends on the context. Therefore project-level information in critical to writing a contextaware summary. Over the last five years, a few studies have introduced context to code summarization models, such as by adding source code from part of the project [2], [3] or the call-graph $[4$. It is cost-prohibitive to feed the entire project to these language models as a longer input requires more computational resources. There is a knowledge gap in related work on how human programmers navigate through the context in a project to infer the most important information. That is the core inspiration behind this study. We designed this study to map the programmers' visual attention strategy and find out what in the context they find most important, so that we may use that information for future studies in automated source code summarization.

The visual attention strategies of a programmer are an important indication of their mental model when performing programming tasks [5], [6], [7|. Visual attention strategies refer to the selective focus humans apply to some parts of the visual stimulus (i.e., the code in the case of programmers). This selection of parts of the code is informed by the mental model of the programmer that evolves as they retain more information [8]. These visual attention strategies can be inferred using eye-tracking data, specifically, gaze
patterns of programmers. Eye-trackers have been used in fields such as computer vision, psychology, and medical sciences to create models of human behavior and mental processes for decades [9], [10], [11]. Although models of visual attention strategies are interesting on their own, they can used to learn domain-specific knowledge such as those programmers use while completing SE tasks.

To that end, recent work in SE towards modeling programmer attention has mainly used proxies such as mouse cursor movements and clicks [12]. Studies to model visual attention directly using eye-trackers have been limited to a small snippet of code |6], [13], |14|. Recently, a few studies have also proposed approaches for automatic inference of visual attention [15], [16], [17]. The scope of most of these studies is limited to one subroutine. We build upon these works to study eye-tracking and visual attention patterns at the project-level. Compared to those studies, our study design is challenging due to difficulty controlling a study environment where the participants have access to the entire project. Analyzing the data at the project level also poses challenges as not all code in the project is useful or even executed. However, project-level comprehension is a real world scenario in which programmers complete SE tasks such as code summarization. To the best of our knowledge, we conducted the first comprehensive human study to map these attention patterns in the context of the project.

In this paper, we present an eye-tracking study designed with the goal of analyzing visual attention strategies of programmers while they navigate related methods for the task of source code summarization. We recruited 10 Java programmers, each tasked with completing five sessions. In each session, we asked programmers to write context-aware summaries for 8 subroutines of a Java project. Then, we manually rated the summaries for accuracy, completeness, conciseness, and clarity using two graders, who are among the authors. These ratings help us analyze the effect of various programmer gaze patterns on the quality of summaries. Finally, we categorized all the code in the project into categories of code context. We then identified the type of context to which the programmers gave the highest attention in pursuit of the summarization tasks. Although knowledge of programmer visual attention strategies is intellectually interesting, we posit that models of programmer attention and identification of important parts of context have practical applications towards improving models for automatic source code summarization.

We found that: 1) participants looked at fewer words and revisited words fewer times as they processed more methods for a given project; 2) the quality of these summaries did not suffer or decrease with any statistical significance as the participants processed more tasks; 3) the quality of summaries is related to the amount of context analyzed by the participants, like a bell-curve, such that the quality increases as more context is analyzed by the participants upto a certain threshold and then the quality decreases; 4) participants spent the most time looking at class methods, followed by class declarations and call graphs; 5) the type of context participants focused on is generally not affected by the project, session order, method order, or participant.

We make five contributions in this study:

1) We perform an eye-tracking study involving more than
60 hours of participant effort by 10 Java programmers. Each programmer completed 40 context-aware code summarization tasks.
2) We contribute a novel dataset of 394 context-aware summaries, to be released publicly with this study.
3) We perform a qualitative comparison of the contextaware summaries, graded by two authors of this paper and agreed upon by a discussion-based coding methodology, to be released publicly with this study.
4) We perform a quantitative comparison of eye-gaze patterns and visual attention strategies as the programmers process more tasks and their correlation with quality of summaries.
5) We present a quantitative comparison of the varying levels of attention received by different types of context and make recommendations to distill context for future automatic source code summarization studies.

## 2 BaCKgROUND ANd RELATEd WORK

In this section, we provide background on eye tracking studies in software engineering and discuss related work in context-aware code summarization.

### 2.1 Eye-tracking in Software Engineering

Eye-tracking has been used in SE research for nearly two decades to gain insight into programmer behavior and human factors in programming |7|. Recent evolution of eye-trackers from expensive and difficult-to-operate lab equipment to simple monitor-mounted devices usable in development environment has accelerated their use in SE research \16], [18]. Eye-tracking studies in SE can mainly be divided into three areas of interest:

Code comprehension is an area of interest that is primarily concerned with the mapping of programmer mental model during comprehension of a snippet of code or small program. In 2006, Aschwanden and Crosby |19| presented gaze patterns of programmers during the comprehension of a small snippet of code containing a loop and mathematical equations. They formalized the process of understanding programmer mental processes using eye-tracking during program comprehension. Also in 2006, Bednarik and Tukiainen [13] introduced a framework for analyzing eye gaze patterns, in particular scanpaths, of participants tasked with comprehension of two small programs. They found that the mental model of a programmer is refined as they progress through the task, marked by a decrease in attention switching. In 2014, Rodeghero et al. [20] presented an analysis of gaze patterns of programmers tasked with summarization of source code. They used the findings of that study to improve automatic source code summarization. Similarly, Karas et al. |14] presented and compared visual attention strategies of programmers for two tasks: reading of source code summaries for assessment and generating their own source code summaries. They found that programmers tend to look at specific words in the method in specific order defined by the syntactic nature of the code. Their work serves as an inspiration for this study, but these studies only considered method-level information. In this study, we focus our analysis on the project-level information.

UML Diagrams are an area of interest concerned with understanding how programmers use UML diagrams during programming tasks. In 2006, Guéhéneuc [21| presented two case studies on the importance of UML diagrams in program comprehension by following the gaze patterns of programmers presented with a comprehension task. In 2010, Sharif and Maltec [22] showed that eye gaze patterns of programmers suggest better program comprehension when presented with particular UML layouts and clustering patterns. A caveat of studies on UML diagrams is that the results are specific to the visualization tools and programmers were not presented source code or asked to do a programming task [23], [24]. Therefore, the observations from these studies do not necessarily represent visual attention during code comprehension.

Code Debugging studies are concerned with mapping programmer attention and mental model during the task of code debugging [7]. In 2008, Bednarik and Tukiainen [8| presented an analysis of gaze patterns of programmers with varying levels of experience during debugging tasks. They found that more experienced programmers tend to use context-switching quicker and more often that novice programmers. In 2014, Turner et al. [25] presented results of a study performed exclusively with students given two tasks: 1) over-viewing code to answer questions, and 2) debugging code. They found that gaze patterns are significantly different between the two tasks, suggesting eyepatterns of code comprehension are goal-dependent. This is important because it highlights the need for in-depth studies like ours that specifically ask the programmer to summarize code using context.

Our study is most closely related to code comprehension as we present participants with raw source code and specifically task them with summarization using context. The novelty of our study lies with the project-level eyetracking and data analysis.

### 2.2 Source Code Summarization

Source code summarization has been a focus of SE research for a decade and a half. Early work in source code summarization used Information-Retrieval techniques [26|, [27|. Around 2015, these techniques were aided by the addition of contextual information [28], [29]. Around 2017, the advent of neural networks changed the landscape of automatic source code summarization research [30], [31], but the initial approaches lacked contextual information.

In 2020, Haque et al. [2] proposed an encoder to represent a few subroutines from the same file as the target subroutine. They showed that the addition of a small part of the file improved existing approaches in automatic source code summarization. Then in 2021, Bansal et al. [3] extended upon that work by introducing a proof-of-concept project encoder that represents a few files from the project. They showed further improvement over file context by including more information but also cautioned against the costs of adding more context. They selected the files and subroutines in the files randomly and highlighted the need for a better methodology to narrow down contextual information.

In 2023, Bansal et al. [4] introduced a Graph Neural Network (GNN) based encoder to represent the call graph extracted from the project centered at the target subroutine.
Recently, two studies used human visual attention to improve automatic source code summarization by using synthesized human attention data given limited eye-tracking data [15], |17]. However, the short summaries generated by these approaches may not represent the full potential of contextual information. Therefore, in this study we ask participants to write longer summaries that include specific contextual information in them.

Recent studies in automatic source code summarization have been focused on fine-tuning LLMs [32], [33], [34]. However, even for these massive models, the size of the input, called a "context window" is a limitation. Although there are proposed techniques for increasing the size of these context windows [35], there is also evidence that simple feeding a large amount of context to an LLM may not be helpful |36|. Therefore, in this study we aim to isolate specific areas in the project that programmers pay the most attention to, and may be used to distill contextual information for LLMs and any future techniques for automatic source code summarization.

## 3 Study Design

In this section, we describe the experimental setup for our eye tracking study including the study setting, research questions, interface, procedure, dataset, and tools.

### 3.1 Study Scenario

We designed this study to emulate a specific scenario in code comprehension, specifically context-aware source code summarization. We asked participants to retrospectively write context-aware descriptions of a target method. We provide participants access to the entire Java code from the project that contains the target method. The project is devoid of any documentation, including header and inline comments (See on details of the projects). We ask the programmer to read and understand the target method, as well as the surrounding code in the context, and to write a three-sentence context-aware summary with the following instructions:

- The first sentence should explain the purpose of the method in as simple of terms as possible, in the context of the whole program.
- The second should describe the more specific functionalities and under what circumstances they occur.
- The last sentence should describe why this method is needed within the context of the overall project.

We asked each programmer to write summaries for 8 methods per project, found to be the ideal number for 1 hour session during our pilot studies. We chose 5 projects extracted from Github for this study, and displayed only the raw Java code. We ask every programmer to process each project in a separate eye-tracking session. We set up the study entirely in Eclipse IDE, a popular IDE for open source Java developers to emulate an OSS development environment. See Section 3 for complete study procedure, interface and other technical details.

![](https://cdn.mathpix.com/cropped/2024_06_04_f25560ec3f5ebe99db42g-04.jpg?height=710&width=1765&top_left_y=144&top_left_x=169)

Fig. 1. A screenshot of our interface. The blue box highlights the summary generated by the participants, while the red box highlights the navigational window limited to one project. The participant is free to open and read any part of the project.

### 3.2 Research Question

We answer three main research questions:

RQ1: How do gaze patterns change as the participants progresses from the first method to the last in a session? RQ2: How does the quality of summaries correlate with the gaze patterns?

RQ3: What parts of the project context received the highest attention?

The rationale behind RQ1 is to evaluate how gaze patterns change as a participant moves from the first method to the last in a project. As participants investigate more methods and write descriptions, their knowledge base of the project may increase. We may be able to isolate specific gaze patterns associated with an increased knowledge base.

The rationale behind RQ2 is to find gaze patterns that are characteristics of the quality of summaries that we grade manually. We aim to find the correlation of high-quality summaries and the observations from RQ1. We also aim to isolate gaze patterns that may be characteristic of writing what we consider to be higher-quality summaries.

The rationale behind $\mathrm{RQ} 3$ is that some parts of the context are more important to the task than others. We aim to categorize the different parts of context that the participants looked at and identify the type of context on which the participants spent the longest time fixating. This may help future studies distill parts of context for automatic source code summarization [32].

### 3.3 Interface

We show a screenshot of our study interface in Figure1 with the following notable features:

- We built our interface within the Eclipse IDE, using iTrace plugin |37| and core [38|.
- Study Instructions are in a markdown file named StudyInstructions.md, which contains an example of a context-aware summary.
- For each project, we provide the participant with project.txt file, which contains 8 tasks. For each task, we provide the path to the Java file, the name of the method, the line number, and space for the description.


## 3.4 iTrace

We use iTrace suite [38|, [39| for our eye-tracking study. The suite includes three tools: 1)iTrace core [40] that manages the eye-tracking, screen recording, keyboard, and mouse recording; 2) iTrace Eclipse plugin |37| that connects to the core and maps the gaze data to specific elements of the IDE and the text inside files; 3) textbfiTrace toolkit [41] which is a post-processing tool to isolate gaze patterns. We used this suite because it is defined to be used over an IDE and can record eye-tracking data over the entire code project.

### 3.5 Study Procedure

We ask each participant to complete five sessions, each with the following study procedure:

- Before the participant arrives, we launched both the Tobii eye-tracker manager and iTrace core program we set up the session and participant details.
- Next, the study administrator sets up the Eclipse IDE. First, we connect the iTrace Eclipse plugin to the iTrace core. Second, we set the project explorer to only show the project archived by SRCML [42] for that session. Then, we open two files: 1) studyInstructions.md and 2) project.txt, where project is a placeholder for the name of the Java project. See Section 3.6 for details.
- Next, the study administrator seats the participant. We ask the participant to read the StudyInstructions.md file. These instructions contain an example of the threesentence summary we ask the participants to write. This step is required for the first session, but the file is always accessible through the session and for future sessions if the participant needs a reference.
- Next, we ask the participant to open the project.txt file that describes the project and contains the location of the 8 Java methods. Each method is a task for the purposes of this study.
- We ask the participant to perform two calibrations before starting the first task as recommended by itrace developers [40]. First, Tobii eye-tracker manager and then the iTrace core calibration. After calibration, we ask the participant to start a tracking session on the iTrace core window and begin the task inside the Eclipse IDE.
- To complete a task, we ask the participant to navigate to a file and line number and find the method. Then, the participant is free to explore any number of files in the project. After the completion of one task, we designed the text file to remind the participant to stop tracking and start a new tracking session for the next task. This allows each task to have its own eye-tracking data and screen recording file.
- The study administrator helps re-calibrate with both Tobii eye-tracker manager and iTrace halfway through the session, after task 4 , and before starting the new tracking session for task 5 . We also prompt re-calibration if the participant needs a break or leaves the eye-tracking room at any point during the session.


### 3.6 Java Projects

We scraped Github for repositories with atleast $90 \%$ of the project code base containing Java source code. We also filtered for projects that depend heavily on other projects or codebases. Finally we picked the following five projects:

- Scrimage is an immutable, functional, and performant JVM library for manipulation of images. The aim of this library is to provide a simple and concise way to do common image operations, such as resizing, filter, and converting between formats.
- MLTK is a collection of various supervised machine learning algorithms, which is designed for directly training models and further development.
- OpenAudible is a cross-platform desktop application for downloading and managing Audible audiobooks with both a GUI interface and an organizational library.
- MALLET is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.
- FreeCol is a turn-based strategy game based on the old game Colonization, and similar to Civilization. The objective of the game is to create an independent nation.

Note, for each project above we chose 8 specific methods. To ensure that the methods can be described using relevant context, we prioritized methods that are called by at least one other method in the project. The order of methods 2 through 8 is randomized. We do this to answer RQ1 by minimizing the effect of any one method on the observations, i.e., a particularly tough or easy method would not skew the results as it would appear at a different position for each participant. Method 1 is fixed for each session to start each participant off with the same baseline.

### 3.7 Eye-Tracker

The eye-tracker used for this study was the Tobii Pro Fusion at $120 \mathrm{~Hz}$ mounted at the bottom of a Spectre 24 -inch 1920x1080 resolution monitor at $60 \mathrm{~Hz}$ screen refresh rate. The eye-tracker has an accuracy of $0.03^{\circ}$ and precision of $0.04^{\circ}$ in optimal conditions [43]. Note, although all sessions were conducted in the same lab with the same equipment, we could not ensure optimal conditions were met for all participants due to factors such as ambient light, natural light and corrective lenses.

### 3.8 Software Versions

We list the software versions we used for our study below to promote future replication of the study and results: Windows 10 pro 10.0.19045; iTrace core v0.2.0; iTrace Eclipse Plugin v0.2.0; iTrace Toolkit 0.2.2; Eclipse IDE for Java V2023-06 (4.28.0); OpenJDK 18

### 3.9 Participants

We recruited 10 programmers with Java experience to participate in our study. Each participant was required to have at leaszt 1 year of previous Java development experience. The participants had an average of 2.8 years of Java development experience and 5 years of general programming experience. Each participant was compensated at a flat rate of USD 60 per session, with an average duration of around 1 hour per session. 4 participants identified as female, while 6 identified as male. 6 participants identified as non-native but fluent English speakers, while 4 identified as native English speakers. We designed the study to be in-depth to answer RQ1, where each participant processes multiple methods and projects, so we did not recruit a high number of participants. In this study we focus on general findings instead of individual differences.

## 4 METHODOLOGY

In this section, we discuss the methodology for analysis of data after the eye-tracking study.

### 4.1 Gaze Pattern Metrics

Following the good practices established by similar studies in the past [44], [45], [46], we calculate and report the statistical summaries of the following gaze pattern metrics to answer the three RQs:

Fixation: A fixation is marked by a steady gaze of the human eye at a particular location in the stimulus, for the duration of at least $100-200 \mathrm{~ms}$, required for the visual system to process information. It is the most common measure of human visual attention in eyetracking literature [44]. For this study, we only compute statistical summaries of fixation count and duration.

Regression Rate: A regression occurs when a fixation occurs against the current direction of reading, such that a previously read word may be fixated upon again, marking an event of re-reading [45|. Higher regressive rate may indicate higher difficulty in understanding [47]. We calculate regression rate for each method task as the number of regressive fixations normalized by the number of total fixations during that task.

Lines of code: We compute and report statistical summaries of counts, density, time spent, and transition between lines of code [46] seen by a participant as a measure of programmer attention.

Methods visited: We compute and report statistical summaries of counts, time spent, and manually labeled relationships between methods in the project as a measure of programmer attention. The idea is that the methods that linked to the highest gaze activity, receive more attention from the programmer.

TABLE 1

Mean and Median values for metrics when grouped by the position in which the method was seen.

| Method | Fixation Count |  | Fixation Duration |  | Regression Rate |  | Lines Visited |  | Methods Visited |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Mean | Median | Mean | Median | Mean | Median | Mean | Median | Mean | Median |
| 1 | 331.61 | 259 | 444.39 | 289 | 56.65 | 56.93 | 50.03 | 38.5 | 9.04 | 5 |
| 2 | 268.54 | 249.5 | 480.37 | 300 | 53.78 | 53.48 | 46.92 | 45 | 9.71 | 6.5 |
| 3 | 302.89 | 221 | 441.05 | 281 | 55.07 | 56.94 | 45.45 | 38 | 9.11 | 7 |
| 4 | 290.46 | 250.5 | 484.24 | 302 | 56.21 | 59.21 | 40.58 | 34 | 9.35 | 8 |
| 5 | 238.77 | 216 | 491.90 | 310.5 | 52.89 | 52.39 | 40.58 | 36.5 | 10.08 | 7 |
| 6 | 202.81 | 156 | 458.15 | 283 | 51.1 | 49.67 | 36.03 | 32 | 8.48 | 6 |
| 7 | 222.44 | 193 | 478.40 | 301.5 | 51.52 | 54.78 | 35.08 | 31 | 6.96 | 7 |
| 8 | 214.69 | 176 | 491.89 | 308 | 49.85 | 51.97 | 40.81 | 30.5 | 8.19 | 6 |

### 4.2 Qualitative Annotation

Two of the authors annotated the quality of summaries generated by the participants, to assess the effect of changing gaze patterns on performance, based on four qualities recommended in related works |48|:

- Accuracy On a scale of 1-5, how accurate is the summary? (deduct a point for each instance of inaccuracy)
- Completeness On a scale of 1-5, how complete is the summary? (deduct a point for each instance of missing important information)
- Conciseness On a scale of 1-5, how concise is the summary? (deduct a point for each instance of unnecessary information)
- Clarity On a scale of 1-5, how clear or readable in the summary? (deduct a point for each instance of lack of clarity or grammatical incorrectness)

When assigning annotations or "codes" in a qualitative manner, there are two popular approaches to reduce human biases. The first approach is an agreement-based approach [49], which relies on a high number of annotators (or graders for the purpose of this study) to reach a Kappa agreement threshold. However, that approach does not account for expertise, i.e., a large number of novices may override an expert. For our study, every grader would have to be 1 ) expert in Java documentation, and 2) familiar with the entirety of the project, as we expect our participants to explore the project while writing summaries. The second approach is a discussion-based approach, which relies on an iterative process of discussion for each disagreement. We chose this approach as recommended by related work for tasks with a low tolerance for imperfect grading [50|, [51]. Both graders independently graded all summaries, followed by a discussion phase on each disagreement, until both graders agreed on one score. Note that the graders did not know the participant number or the position of the method for which the summary was generated to eliminate agreement bias.

### 4.3 Context Categorization

We divide all areas of context in any of the Java projects into five categories as they relate to the target method:

Class methods: are the areas of context that fall within a method in the same class as the target method.

Class declarations: are the areas of context that fall within the same class as a target method but not inside a method, such as object and variable declarations.
Call graph: are the areas of context that fall within a method in the call graph of the target method, i.e., it is a caller or callee of the target method.

File context: are the areas of context that fall within the same Java file as the target method but are not in the same class as the target method or in the call graph. These may be import statements, global variable declarations or other private classes defined in the same Java file.

Project context: are all other areas of context in the project that are not included in any of the above categories.

We use a SRCML [42] archive to identify types of context for each fixation recorded in iTrace-toolkit [38]. Additionally, we use the Call-Hierarchy Plugin for Eclipse for Java IDE 2023 to generate the call graph context.

## 5 RESULTS

In this section, we report the results for our three RQs.

### 5.1 RQ1: Effect of the progress through tasks

We found that participants need less information to complete their context-aware summarization task as they progress through the tasks. To analyze the change in gaze

TABLE 2

Results of the Mann-Whitney U statistical tests on the distributions presented in Table 1

| Gaze Metrics | Mann-Whitney U |  |  |
| :--- | :--- | :--- | :---: |
|  | $\mathrm{U} 1$ | $\mathrm{U} 2$ | $\mathrm{p}$-value |
| Fixation Count | 1892.5 | 1075.5 | $\mathbf{0 . 0 1}$ |
| Fixation Duration | 1263 | 1705 | 0.18 |
| Regression Rate | 1193.5 | 1774.5 | $\mathbf{0 . 0 2}$ |
| Lines Visited | 1773.5 | 1194.5 | 0.08 |
| Methods Visited | 1501 | 1467 | 0.92 |

TABLE 3

Average quality ratings for summaries when grouped by the position of the methods.

| Method | Accurate | Complete | Concise | Clear | Overall |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 1 | 4.32 | 3.94 | 4.42 | 4.74 | 4.36 |
| 2 | 4.36 | 4.10 | 4.40 | 4.40 | 4.32 |
| 3 | 4.46 | 4.12 | 4.22 | 4.64 | 4.36 |
| 4 | 4.44 | 4.22 | 4.28 | 4.58 | 4.38 |
| 5 | 4.56 | 4.26 | 4.28 | 4.54 | 4.41 |
| 6 | 4.45 | 3.98 | 4.29 | 4.63 | 4.34 |
| 7 | 4.50 | 4.10 | 4.13 | 4.65 | 4.34 |
| 8 | 4.32 | 3.98 | 4.09 | 4.68 | 4.27 |
| All | 4.43 | 4.09 | 4.26 | 4.61 | 4.35 |

TABLE 4

Gaze metrics for summaries grouped by quality scores: low $(<=3)$ and high $(=5)$ for completeness and conciseness. Here U1, U2, and $p$-value are results of the Mann-Whitney U statistical test for each group.

| Group | Fixation Count |  | Regression Rate |  | Fixation Duration |  | Lines Visited |  | Methods Visited |  |
| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
|  | Complete | Concise | Complete | Concise | Complete | Concise | Complete | Concise | Complete | Concise |
| Low $(<=3)$ | 181.72 | 350.38 | 48.38 | 55.78 | 562.08 | 475.47 | 29.88 | 53.62 | 5.60 | 11.42 |
| High(=5) | 264.23 | 228.87 | 54.27 | 52.55 | 482.01 | 516.73 | 43.25 | 38.49 | 9.29 | 8.14 |
| U1 | $1,856.00$ | $2,765.00$ | $2,239.00$ | $2,353.00$ | $2,764.00$ | $1,825.00$ | $1,740.50$ | $2,743.50$ | $1,796.00$ | $2,684.00$ |
| U2 | $3,734.00$ | $1,369.00$ | $3,351.00$ | $1,781.00$ | $2,826.00$ | $2,309.00$ | $3,849.50$ | $1,390.50$ | $3,794.00$ | $1,450.00$ |
| p-value | 0.001 | 0.006 | 0.051 | 0.259 | 0.915 | 0.340 | 0.000 | 0.008 | 0.000 | 0.015 |

patterns during this process, we report the change in fixation duration, fixation count, regression rate, lines visited, and methods visited by the participants. We make two interesting observations.

First, we observe a pattern of decreasing fixation counts. In Table 1 we report the statistical summaries of the count of fixations when grouped by position of the method, which denotes the position in which the participants saw and wrote summaries for the method. We observe a $35 \%$ decrease in the mean number of fixations the participants made on the last method when compared to the first method processed. However, with respect to fixation duration, we did not observe a significant change. This may be because average time spent on a fixation may be affected by the reading and comprehension speed of the participants.

Second, we also observe a clear trend of decreasing regression rate. The mean of normalized regression rates for each task decreased by roughly $13 \%$ when compared by method position. Regressions often occur when information is particularly hard to understand or needs context from the surrounding words before it can be understood [52|. This indicates that as the participants progressed through the session they did not encounter word tokens that were hard to comprehend or needed additional information to understand as frequently as when they started.

We performed the Mann-Whitney U [53] test, a nonparametric, non-paired statistical test on the distribution of the values, and report the results in Table 2 We chose this test because our data does not meet the assumptions of a parametric test, i.e., it is subjective and is not known to follow Gaussian or Normal distributions. We compared the combined distribution of first two methods against the combined distribution of last two methods seen by the participants. We observed that for fixation counts the difference between the two distributions were statistically significant ( $\mathrm{p}$-value $<0.01$ ). We also observed statistically significant difference between the two distributions for regression rate (p-value $=0.02$ )

However, with respect to number of lines visited and number of methods visited, we observe a p-value $>0.05$. Therefore we cannot reject the null hypothesis for these metrics. There is statistical likelihood that patterns such as decreasing mean number of lines visited may not be significant. These observations are interesting because they indicate that the participants continued to cover a large amount of the context to write these descriptions as they processed more methods.

Therefore, our interpretation of these observations is that the participants retain more information about the project from previous task, and thus needed less information to write summaries as they processed more tasks. Different studies have shown how programmers skim source code whenever possible, reading the minimum amount of information they need [54]. As programmers become more familiar with the task, they are more likely to intuitively know what information they need, and therefore read fewer details. Our observations support those findings as we do not see a significant decrease in lines and methods visited by the participants but fixations and regressions decrease. Therefore, participants still covered similar amount of code but seemed to get better at skimming, resulting in lower frequency of fixations and regression rate.

An alternative explanation might be fatigue or boredom, the effects of these are marked by decreased thoroughness or decreased quality of summaries written. Our observations contradict the former as we did not observe statistically significant effect on code context coverage in terms of the number of lines and methods visited. We study the possibility of the latter in the next section about RQ2.

### 5.2 RQ2: Quality of Summaries

To analyze the quality of summaries written by the participants, we had two graders manually grade each summary for accuracy, completeness, conciseness, and clarity (see Section 4.2 for details). We make a the following observations.

First, we observe no significant pattern associated with the position in which the summary was written. In Table 3 . we report the mean scores for the four quality metrics. We also report an over score, an average of those 4 metrics calculated individually for each summary. We also performed the Mann-Whitney $U$ test between the distributions of the first two methods and the last two methods in a way consistent with RQ1, with an observed $p>=0.9$. This observation supports our hypothesis about RQ1 that a decrease in fixation counts and regression rate is likely not due to exhaustion, as neither code context coverage nor quality of summaries was affected by the position in which the methods were seen.

Next, we found that with mean $=4.35$ and median $=4$ on a scale of 1-5, the summaries were graded to be of high quality. We observed a higher variance in scores for two of the qualities, completeness and conciseness, compared to accuracy and clarity. Therefore, we analyze the eye gaze patterns between two groups, those that received a low score $(<3)$ for completeness ( $n=44$ ) and conciseness $(n=33)$, against those that received a high score $(=5)$ for completeness ( $n=147$ ) and conciseness ( $n=170$ ). Here $n$ denotes the number of summary samples in each subset. We do not compare groups based on accuracy and clarity because the low score group size is too small $(n<5)$.

" class methods = class declarations = call graph =file context - project context

![](https://cdn.mathpix.com/cropped/2024_06_04_f25560ec3f5ebe99db42g-08.jpg?height=970&width=1770&top_left_y=239&top_left_x=161)

0.6

![](https://cdn.mathpix.com/cropped/2024_06_04_f25560ec3f5ebe99db42g-08.jpg?height=380&width=853&top_left_y=295&top_left_x=151)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_f25560ec3f5ebe99db42g-08.jpg?height=407&width=851&top_left_y=257&top_left_x=1057)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_f25560ec3f5ebe99db42g-08.jpg?height=407&width=855&top_left_y=753&top_left_x=166)

( c)

![](https://cdn.mathpix.com/cropped/2024_06_04_f25560ec3f5ebe99db42g-08.jpg?height=401&width=870&top_left_y=756&top_left_x=1061)

(d)

Fig. 2. Graphs illustrating the distribution of programmer attention for varying groups defined on the X-axis as (a) project names, (b) the order in which the session occurred, (c) the order in which the method was seen, and (d) the participant ID. The legend on top is common for the bar colors on all graphs. The Y-axis in each graph shows mean value for each type of context. The values were normalized by the total time spent fixating on context outside the target method for each method-summary pair, prior to computing mean for each category.

We observe that the completeness of summaries is correlated to the code context coverage by the programmer. In Table 4 we report the mean values of each of the five gaze metrics for each subset. We also report the U1, U2, and p-value for Mann-Whitney U statistical test. We observe that the summaries of high completeness correspond to a significantly $(p<0.001)$ higher number of fixations, lines visited, and methods visited. Figure 3 shows this delta clearly through the blue bars. This suggests that the participants that covered a smaller amount of the code context, wrote summaries that were missing important information. Although we observe an increase in mean regression rate

![](https://cdn.mathpix.com/cropped/2024_06_04_f25560ec3f5ebe99db42g-08.jpg?height=526&width=881&top_left_y=1922&top_left_x=167)

Fig. 3. Bar chart showing the delta values for gaze metrics computed between summaries rated low $(<=3$ ) and highly $(5)$ in terms of completeness and conciseness. as well, the $\mathrm{p}$-value for regression rate $p=0.051$, which is not statistically significant by conventional standards, but approaches significance. We do not observe a significant difference in fixation duration. This observation supports the observation in RQ1, and further suggests that fixation duration may be dependant on each participant's reading and comprehension speed.

On the other hand, the conciseness of summaries is inversely correlated to the code context coverage by the programmer. From Table 4 . we observe that the summaries of high conciseness correspond to a significantly ( $p<0.015)$ lower number of fixations, lines visited, and methods visited. Figure 3 shows this delta more clearly. This suggests that participants included unnecessary information at a higher rate when they covered an exceptionally larger part of the code context. As shown in Figure 3, we did not observe a significant difference in fixation duration or regression rate between the two groups.

Our interpretation of these observations is that there are diminishing returns in terms of increasing code context coverage and improvement in the quality of summaries. We found that gaze metrics for the summaries that achieved a perfect score for all four qualities (Overall $=5$ ), tend to fall between the thresholds for high completeness and conciseness. For example, in terms of lines visited, the summaries with a perfect score have a mean score of 41 lines, which is between the scores of 43.25 and 38.49 reported in Table 4.

So far, we have looked at how gaze patterns of participants change in relation to the task, the quality of summaries generated, and the amount of context analyzed. In
the next subsection, we categorize the type of context on which the participants fixated.

### 5.3 RQ3: Attention to Context

We found that participants spent a vast majority of their time looking at methods and variable declarations in same class as the target method. Surprisingly, we found that participants spent a comparatively small amount of time looking at methods in the call graph, even though we designed the second and third sentences in the summary to contain information from the callees and callers respectively. In Figure 2 we report four graphs, each showing averages for total time spent fixating on tokens inside each type of context, normalized by total time spent looking at context for the summarization task.

In Figure 2 (a), we report the attention grouped by the project type. We found see similar trends for four of the five project, namely scrimage, openaudible, mltk, and freecol. For these projects, participants spent an average of $40-65 \%$ of their time looking at methods in the same class as the target method. Surprisingly, for the mallet project, on an average participants spent more time looking at the variable, declarations, and other data inside the class than methods and calls. One possible explanation for this outlier we investigated was that mallet relies more on class variables than the other projects. However, we did not find any significant difference between average number of class declarations and variables outside class methods for mallet, when compared against the other projects.

In Figure 2 (b), we analyze the difference in attention grouped by session, i.e., the order in which the project was processed by a participant. We found that on average as participants processed more sessions, they relied more on information inside the class, both class methods and class declarations. This is expected because as participants repeat the task, they form a consistent pattern of navigating the context. This supports the earlier observation that overall, the participants found the class of the target method to be most important part of the context.

In Figure 2 (c), we analyze the difference in attention grouped by method, i.e., the order in which the method was processed by a participant in any given session. We observe no significant trend based on the order in which the method was processed. This supports our observation from RQ1 and RQ2 that although the participants make fewer fixations as they process more methods, they still get the same general code context coverage in the context.

In Figure 2 (d), we analyze the difference in attention of each participant. We found that most of the participants relied on class methods and the class declaration for contextual information. We found that only participant 5 and 6 are outliers, and spent a significant amount of time fixating on context in other files, and these are areas that do not fall under the methods in the call hierarchy. This suggests that observations from (a),(b), and (c) are consistent with a majority of the participants and not skewed heavily by an extreme minority.

Overall, our interpretation of these results is that there is a general agreement of trends between the participants, which is unaffected by method or session order. We observe a few spikes in class declarations, which may be skewed by the mallet project. These observations make a case that the most important part of context for a summarization task like this is all the information inside the class.

## 6 EXAMPLE

In Fig 4 we show an example containing target method and three corresponding summaries written by participant 3,7 , and 10. These may explain some of our observations from Section 5 Notice, the summaries written by Participant 3 and 10 are similar, but the summary written by 10 is slightly clearer. On the other hand, the summaries written by Participant 7 for the same method are inaccurate and hard to comprehend and received one of the lowest overall scores by our graders (Overall = 2).

We found that the key to writing the last sentence for both participant 3 and participant 10's summary is to track the class variable minCapacity. Instances like this might explain why mallet shows a higher attention on class declarations than class methods as observed in Section 5.3 .

We also notice that participant 7's summary is an example of summaries where excessive code coverage leads to verbose and unclear summary. Note this is just one example

mallet $>$ types $>$ Multinomial.java $>$ ensureCapacity [line 290]

```
protected void ensureCapacity (int index)
    {
    if (index > size)
        size = index;
    if (counts.length <= index) {
        @Var
        int newLength = ((counts.length < minCapacity)
            ? minCapacity
            : counts.length);
        while (newLength <= index)
            newLength *= 2;
        double[] newCounts = new double[newLength];
        System.arraycopy (counts, 0, newCounts, 0,
            counts.length);
        this.counts = newCounts;
```

participant 3

The method ensures the capacity of an Estimator to always have sufficient storage to hold incoming counts. The method first checks if the current counts are enough for the incoming index, and, if not, doubles the current counts to cover the index. The method is useful to maintain the valid count number before processing the incoming index.

participant 7

This method is for ensuring that the Estimator class has enough capacity to execute given the index that the method is passed through. If index is great than the size of the class, then the size is set to the index, if the length of counts is less than the index, then it iterates through counts to create new counts that is of length of the index This method is used by the project for the multinomial class to make an estimate.

## participant 10

This method calculates the appropriate length of count based on a given index. After comparing size with index and the length of counts with min capacity to get the larger length of counts, it repetitively enlarges the length of counts twice until it is larger than the input index. This method is useful when you want to ensure the counts have enough capacity to store all then input based on the input maximum index.

Fig. 4. Example with source code of a method in project mallet, accompanied by summaries written by participant 3,7 , and 10 .
we found that fits the observations from Section 5.2, we do not claim that this example is representative of a trend or make any recommendations based on this example.

## 7 Threats to Validity

Our study has several threats to validity. One threat lies with the selection of projects. We selected open-source Github projects that neither we nor the participants had any contribution in developing. Some of our results might be specific to these projects and any errors or shortcomings these projects may have. We tried to mitigate this threat by choosing projects on varying topics, with several contributors and varying sizes.

Another threat lies with our pool of participants. Our participants are graduate students, and their experiences may not translate directly to those of active industry professionals. To mitigate this threat, we screened participants with previous (and recent) Java development experience. However, industry professionals may have specific mental models for their industry and may utilize visual attention strategies that our participants do not represent. Another threat with our pool of participants is that half of them wear corrective glasses. In specific circumstances, corrective glasses can lead to false fixation readings. To try to mitigate this threat, we recommended that participants complete the task without glasses or use contact lenses if they are able.

Another threat lies with administrators. Our studies were administered by two administrators across several months. There may be differences in the administration styles and the way the task was explained to each participant, which could result in variation between data points. To somewhat mitigate this threat, we created a common script and provided the instructions in text file to each participant.

Another threat lies with the study conditions. The room in which we conducted the study has a window. As the study was conducted over several months, the lighting conditions may have changed between sessions. Newer generation eye-tracker claim to work in all lighting conditions but can cause inconsistencies in rare situations. These external threats exist for all eye-tracking studies.

Another threat lies with the extraction of context categories. The accuracy of these categorizations is limited by the accuracy of both SRCML [42] and iTrace-toolkit [41]. Additionally, the categorization of call graph is limited to a static call graph generated by Call-Hierarchy Plugin for Eclipse for Java IDE 2023. We only report average fixation duration and counts, normalized by the sum of fixation duration and counts for a session, to mitigate some of these threats. We also take measures to avoid false positives within any category of context, such that, if a fixation cannot be confirmed to be within the strict boundaries of any of the other categories, we assign it the "project context" category.

## 8 CONCLUSION \& FUTURE WORK

In conclusion, we designed an eye-tracking study to analyze how programmers read source code during contextaware source code summarization. We first studied how the gaze patterns change as participants progress through tasks. We observed a statistically significant decrease in fixation counts and regression rates as participants summarize more methods in a session. This observation meant that the participant retained and internalized information about the project code base from earlier tasks. Another possibility was that the participant was bored or exhausted. To test the latter, we rated and compared the quality of summaries and found no significant change in the quality of summaries, as one would expect from exhaustion or boredom. We also found that regardless of progress, the amount of context a participant looked at in terms of lines visited, methods visited, and fixations affects the quality of summaries. This effect can be best described to be similar to a bell curve, such that the quality of a summary increases with coverage of context until a peak, after which the quality declines as the summaries become less concise. Next, we categorize and analyze the context on that the participants looked at for the longest fixation duration. We found that participants spent the longest time looking at class methods, followed by class declarations and call graph. We observed this trend to be unaffected by project name, task order, session order, or participant. Based on our study, we propose three main lines of inquiry for future work:

1) Based on our findings in RQ1, we propose that more in-depth studies such as ours may be needed to study the effect of retention of project-level information for SE tasks such as code debugging and testing.
2) Based on our findings in RQ2 and RQ3, we propose that automated code summarization models may be improved by using specific areas of the project as context. Based on our findings, we recommend future studies prioritize class methods first, class declaration second, and call graph third.
3) We hope our dataset of context-aware summaries aids future studies in automatic code summarization. We propose using these summaries to fine-tune LLMs and generate more context-aware summaries.

## 9 AcKNOWLEDGMENTS

This work is supported in part by the NSF grants CCF2100035, CCF-2211428 and CCF-2211429. Any opinions, findings, and conclusions expressed herein are the authors' and do not necessarily reflect those of the sponsors. We also sincerely thank participants of our qualitative study.

## REFERENCES

[1] J. Cámara, J. Troya, L. Burgueño, and A. Vallecillo, "On the assessment of generative ai in modeling tasks: an experience report with chatgpt and uml," Software and Systems Modeling, pp. 1-13, 2023.

[2] S. Haque, A. LeClair, L. Wu, and C. McMillan, "Improved automatic summarization of subroutines via attention to file context," International Conference on Mining Software Repositories, 2020.

[3] A. Bansal, S. Haque, and C. McMillan, "Project-level encoding for neural source code summarization of subroutines," International Conference on Program Comprehension, 2021.

[4] A. Bansal, Z. Eberhart, Z. Karas, Y. Huang, and C. McMillan, "Function call graph context encoding for neural source code summarization," IEEE Transactions on Software Engineering, pp. 1$14,2023$.

[5] R. Bednarik, "Expertise-dependent visual attention strategies develop over time during debugging with multiple code representations," International Journal of Human-Computer Studies, vol. 70, no. 2, pp. 143-155, 2012.

[6] P. Rodeghero, C. McMillan, P. W. McBurney, N. Bosch, and S. D'Mello, "Improving automated source code summarization via an eye-tracking study of programmers," in Proceedings of the 36th international conference on Software engineering, ser. ICSE '14, 2014, to appear.

[7] Z. Sharafi, Z. Soh, and Y.-G. Guéhéneuc, "A systematic literature review on the usage of eye-tracking in software engineering," Information and Software Technology, vol. 67, pp. 79-107, 2015.

[8] R. Bednarik and M. Tukiainen, "Temporal eye-tracking data: Evolution of debugging strategies with multiple representations," in Proceedings of the 2008 symposium on Eye tracking research $\mathcal{E}$ applications, 2008, pp. 99-102.

[9] N. Ouerhani, R. Von Wartburg, H. Hugli, and R. Muri, "Empirical validation of the saliency-based model of visual attention," ELCVIA Electronic Letters on Computer Vision and Image Analysis, vol. 3, no. 1, pp. 13-24, 2004.

[10] B. M. Hood, J. D. Willen, and J. Driver, "Adult's eyes trigger shifts of visual attention in human infants," Psychological Science, vol. 9, no. 2, pp. 131-134, 1998.

[11] C. N. Olivers, F. Meijer, and J. Theeuwes, "Feature-based memorydriven attentional capture: visual working memory content affects visual attention." Journal of Experimental Psychology: Human Perception and Performance, vol. 32, no. 5, p. 1243, 2006.

[12] M. Paltenghi and M. Pradel, "Thinking like a developer? comparing the attention of humans with neural models of code," in 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2021, pp. 867-879.

[13] R. Bednarik and M. Tukiainen, "An eye-tracking methodology for characterizing program comprehension processes," in Proceedings of the 2006 symposium on Eye tracking research $\mathcal{E}$ applications, 2006, pp. 125-132.

[14] Z. Karas, A. Bansal, Y. Zhang, T. Li, C. McMillan, and Y. Huang, "A tale of two comprehensions? analyzing student programmer attention during code summarization," ACM Transactions on Software Engineering and Methodology, 2024.

[15] A. Bansal, B. Sharif, and C. McMillan, "Towards modeling human attention from eye movements for neutral source code summarization," Proceedings of ACM Human-Computer Interaction, ETRA Vol. 7, 2023 .

[16] A. Bansal, C.-Y. Su, Z. Karas, Y. Zhang, Y. Huang, T. J.-J. Li, and C. McMillan, "Modeling programmer attention as scanpath prediction," in Proceedings of The 38th IEEE/ACM International Conference on Automated Software Engineering ASE 2023 - NIER track, 2023.

[17] Y. Zhang, J. Li, Z. Karas, A. Bansal, T. J.-J. Li, C. McMillan, K. Leach, and Y. Huang, "Eyetrans: Merging human and machine attention for neural code summarization," in Proceedings of The ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2024), 2024.

[18] Y. Braw, M. Ratmansky, and I. Goor-Aryeh, "Integrating the numerical pain rating scale (nprs) with an eye tracker: Feasibility and initial validation," 2023.

[19] C. Aschwanden and M. Crosby, "Code scanning patterns in program comprehension," in Proceedings of the 39th hawaii international conference on system sciences. Citeseer, 2006.

[20] P. Rodeghero, C. McMillan, P. W. McBurney, N. Bosch, and S. D'Mello, "Improving automated source code summarization via an eye-tracking study of programmers," in Proceedings of the 36th international conference on Software engineering. ACM, 2014, pp. 390-401.

[21] Y.-G. Guéhéneuc, "Taupe: towards understanding program comprehension," in Proceedings of the 2006 conference of the Center for Advanced Studies on Collaborative research, 2006, pp. 1-es.

[22] B. Sharif and J. I. Maletic, "The effects of layout on detecting the role of design patterns," in Proceedings of the 2010 23rd IEEE Conference on Software Engineering Education and Training, ser. CSEET '10. Washington, DC, USA: IEEE Computer Society, 2010, pp. 41-48. [Online]. Available: http: //dx.doi.org/10.1109/CSEET.2010.23

[23] D. Lübke, M. Ahrens, and K. Schneider, "Influence of diagram layout and scrolling on understandability of bpmn processes: an eye tracking experiment with bpmn diagrams," Information Technology and Management, vol. 22, pp. 99-131, 2021.

[24] Z. Sharafi, B. Sharif, Y. Guéhéneuc, A. Begel, R. Bednarik, and M. E. Crosby, "A practical guide on conducting eye tracking studies in software engineering," Empir. Softw. Eng., vol. 25, no. 5, pp. 3128-3174, 2020. [Online]. Available: https://doi.org/10.1007/s10664-020-09829-4

[25] R. Turner, M. Falcone, B. Sharif, and A. Lazar, "An eye-tracking study assessing the comprehension of c++ and python source code," in Proceedings of the Symposium on Eye Tracking Research and Applications, 2014, pp. 231-234.

[26] S. Haiduc, J. Aponte, and A. Marcus, "Supporting program comprehension with source code summarization," in Proceedings of the 32Nd ACM/IEEE International Conference on Software EngineeringVolume 2. ACM, 2010, pp. 223-226.

[27] G. Sridhara, L. Pollock, and K. Vijay-Shanker, "Automatically detecting and describing high level actions within methods," in Proceedings of the 33rd International Conference on Software Engineering. ACM, 2011, pp. 101-110.

[28] P. W. McBurney, C. Liu, and C. McMillan, "Automated feature discovery via sentence selection and source code summarization," Journal of Software: Evolution and Process, vol. 28, no. 2, pp. 120-145, 2016.

[29] B. Zhang, E. Hill, and J. Clause, "Towards automatically generating descriptive names for unit tests," in Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering. ACM, 2016, pp. 625-636.

[30] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, "Summarizing source code using a neural attention model," in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2016, pp. 2073-2083.

[31] X. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, "Summarizing source code with transferred api knowledge," in Proceedings of the 27th International Joint Conference on Artificial Intelligence. AAAI Press, 2018, pp. 2269-2275.

[32] C.-Y. Su and C. McMillan, "Distilled gpt for source code summarization," Automated Software Engineering, vol. 31, no. 1, p. 22, 2024.

[33] M. Geng, S. Wang, D. Dong, H. Wang, G. Li, Z. Jin, X. Mao, and X. Liao, "Large language models are few-shot summarizers: Multiintent comment generation via in-context learning," 2024.

[34] C.-Y. Su, A. Bansal, V. Jain, S. Ghanavati, and C. Mcmillan, "A language model of java methods with train/test deduplication," arXiv preprint arXiv:2305.08286, 2023.

[35] P. Zhang, Z. Liu, S. Xiao, N. Shao, Q. Ye, and Z. Dou, "Soaring from $4 \mathrm{k}$ to $400 \mathrm{k}$ : Extending llm's context with activation beacon," arXiv preprint arXiv:2401.03462, 2024.

[36] A. Bansal, C.-Y. Su, and C. McMillan, "Revisiting file context for source code summarization," arXiv preprint arXiv:2309.02326, 2023.

[37] B. Clark and B. Sharif, "itracevis: Visualizing eye movement data within eclipse," in 2017 IEEE Working Conference on Software Visualization (VISSOFT). IEEE, 2017, pp. 22-32.

[38] T. R. Shaffer, J. L. Wise, B. M. Walters, S. C. Müller, M. Falcone, and B. Sharif, "Itrace: Enabling eye tracking on software artifacts within the ide to support software engineering tasks," ser. ESEC/FSE 2015. Association for Computing Machinery, 2015, p. $954-957$.

[39] V. Zyrianov, D. T. Guarnera, C. S. Peterson, B. Sharif, and J. I. Maletic, "Automated recording and semantics-aware replaying of high-speed eye tracking and interaction data to support cognitive studies of software engineering tasks," in 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME), 2020, pp. 464-475.

[40] D. T. Guarnera, C. A. Bryant, A. Mishra, J. I. Maletic, and B. Sharif, "itrace: Eye tracking infrastructure for development environments," in Proceedings of the 2018 ACM Symposium on Eye Tracking Research \& Applications, 2018, pp. 1-3.

[41] J. Behler, G. Chiudioni, A. Ely, J. Pangonis, B. Sharif, and J. I. Maletic, "itrace-visualize: Visualizing eye-tracking data for software engineering studies," in 2023 IEEE Working Conference on Software Visualization (VISSOFT), 2023, pp. 100-104.

[42] M. L. Collard, M. J. Decker, and J. I. Maletic, "Lightweight transformation and fact extraction with the srcml toolkit," in 2011 IEEE 11th international working conference on source code analysis and manipulation. IEEE, 2011, pp. 173-184.

[43] Tobii, "Reach further with your research: Choose tobii pro fusion," 2023. [Online]. Available: https://www.tobii.com/ products/eye-trackers/screen-based/tobi1-pro-fusion

[44] G. Buscher, E. Cutrell, and M. R. Morris, "What do you see when you're surfing? using eye tracking to predict salient regions of web pages," ser. CHI '09. Association for Computing Machinery, 2009, p. 21-30.

[45] R. W. Booth and U. W. Weger, "The function of regressions in reading: Backward eye movements allow rereading," Memory $\mathcal{E}$ cognition, vol. 41, pp. 82-97, 2013.

[46] P. Rodeghero and C. McMillan, "An empirical study on the patterns of eye movement during summarization tasks," in 2015 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM), 2015, pp. 1-10.

[47] M. Son, J. Lee, and A. Godfroid, "Attention to form and meaning revisited: Insights from eye tracking," Studies in Second Language Acquisition, vol. 44, no. 3, pp. 788-817, 2022.

[48] C. Treude, J. Middleton, and T. Atapattu, "Beyond accuracy: Assessing software documentation quality," in Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2020, pp. 1509-1512.

[49] S. Rastkar, G. C. Murphy, and G. Murray, "Automatic summarization of bug reports," IEEE Transactions on Software Engineering, vol. 40, no. 4, pp. 366-380, 2014.

[50] V. Rieser and O. Lemon, Reinforcement learning for adaptive dialogue systems: a data-driven methodology for dialogue management and natural language generation. Springer Science \& Business Media, 2011.

[51] A. Wood, P. Rodeghero, A. Armaly, and C. McMillan, "Detecting speech act types in developer question/answer conversations during bug repair," in Proceedings of the 2018 26th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering, 2018, pp. 491-502.

[52] T. Liu and T. Yuizono, "Mind mapping training's effects on reading ability: Detection based on eye tracking sensors," Sensors, vol. 20, no. 16, p. 4422, 2020.

[53] N. Nachar et al., "The mann-whitney u: A test for assessing whether two independent samples come from the same distribution," Tutorials in quantitative Methods for Psychology, vol. 4, no. 1, pp. 13-20, 2008.

[54] J. Starke, C. Luce, and J. Sillito, "Searching and skimming: An exploratory study," in 2009 IEEE International Conference on Software Maintenance. IEEE, 2009, pp. 157-166.

