# BRANCH-SOLVE-MERGE IMPROVES LARGE LANGUAGE MODEL EVALUATION AND GENERATION 

Swarnadeep Saha*<br>UNC Chapel Hill

Mohit Bansal<br>UNC Chapel Hill

Omer Levy<br>Meta<br>Jason Weston<br>Meta

Asli Celikyilmaz<br>Meta

Xian Li

Meta


#### Abstract

Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model's lack of coherence and inability to plan and decompose the problem. We propose Branch-SolVE-MERGE (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, solve, and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to $26 \%$, reducing length and pairwise position biases by up to $50 \%$, and allowing LLaMA-2-chat to match or outperform GPT-4 on most domains. On the constraint story generation task, BSM improves the coherence of the stories while also improving constraint satisfaction by $12 \%$.


## 1 INTRODUCTION

Large Language Models (LLMs) are widely used for various text generation tasks (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023b; Chowdhery et al., 2022; Touvron et al., 2023). It has also become common to employ them as evaluators of such LLM generations in order to assess, critique and improve the outputs (Zheng et al., 2023; Bai et al., 2022b). However, LLMs still struggle with tasks that have intricate requirements like satisfying a set of constraints or meeting objectives that are, in general, multi-dimensional (e.g., evaluating the quality of generated text against certain diverse criteria). This appears to primarily stem from the model's lack of self-consistency and inability to plan (Yao et al., 2023b; Bubeck et al., 2023). Recent research has tried to mitigate these limitations by developing iterative methods that involve eliciting reasoning, planning, and refinement, but so far they are still considered as open problems (Bai et al., 2022b; Madaan et al., 2023; Ganguli et al., 2023; Yao et al., 2023c; Chen et al., 2023; Li et al., 2023; Huang et al., 2023).

In this work, we propose BRANCH-SOLVE-MERGE (BSM), a decomposition method for solving such multi-faceted natural language tasks. Our approach is an instance of a Large Language Model program (Schlag et al., 2023; Dohan et al., 2022) and consists of three modules: branch, solve, and merge that are parameterized with specific prompts to an underlying LLM. Given an arbitrary user task, the 'branch' module generates a solution plan by decomposing the task into multiple parallel sub-tasks, where each sub-task is represented by a unique branch, representing different components required to solve the overall problem. The 'solve' module then solves each of these independent sub-problems. Finally, the 'merge' module fuses the solutions to these sub-problems to generate the overall solution.[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_5c0930f75a5d6b348118g-02.jpg?height=1109&width=1374&top_left_y=275&top_left_x=381)

Figure 1: An illustration of BRANCH-SOLVE-MERGE with LLaMA-2-70B-chat for pairwise evaluation of LLM responses. Given a question and two LLM responses A and B, BSM generates a preference judgment. The Branch module conditions on the question to generate a question-specific evaluation plan which in this case consists of different criteria like 'Relevance' to the Hawaii trip topic, 'Clarity', etc. The 'Solve' module evaluates the response pairs for each criteria (branch) independently and the 'Merge' module combines the individual judgments to generate the final verdict, in this case that $\mathrm{B}$ is the better response.

We apply our method to two challenging tasks where LLMs are commonly utilized but their performance still lags behind humans:

- Evaluation of LLM Outputs (Zheng et al., 2023). LLMs are now regularly used to perform automatic evaluation of model responses, e.g., to user queries (Dubois et al., 2023). Evaluating LLMs holistically is challenging because of their ability to generate long-form answers to arbitrary user questions (Zheng et al., 2023), the lack of reliability originating from many biases (Zheng et al., 2023; Wu \& Aji, 2023; Wang et al., 2023b), and reliance on hand-designed evaluation plans that impact the method's ability to generalize, introducing unintended human biases (Liu et al., 2023; Wu \& Aji, 2023). BSM can be applied to this task by each branch assessing different aspects and criteria that require evaluation.*
- Constrained Text Generation. State-of-the-art LLMs struggle with constrained text generation tasks, for example the constraint of writing a story that should include several concepts. Models commonly either violate constraints, or else generate text that is incoherent in order to satisfy these constraints (Bubeck et al., 2023; Yao et al., 2023a). BSM can be applied to this task by each branch writing part of the story satisfying only some of the constraints, followed by a final merge.

We apply Branch-SolVe-MERGE to both these problems, see Figure 1 and Figure 3, and evaluate its effectiveness with multiple open-source and black-box LLMs of varying sizes and strengths including LLaMA-2-7B-chat (Touvron et al., 2023), Vicuna-33B (Chiang et al., 2023), LLaMA-2-[^1]

70B-chat (Touvron et al., 2023), and GPT-4 (OpenAI, 2023b). BSM significantly improves both tasks, helping address the aforementioned limitations of LLM evaluation and generation:

- BSM improves correctness of LLM evaluation. In particular, on the MT-Bench benchmark (Zheng et al., 2023), BSM improves LLM-human agreement for evaluating multi-turn questions belonging to different domains including writing, coding, reasoning, and mathematics. For example, compared to zero-shot prompting and self-consistency (Wang et al., 2022) baselines, BSM with LLaMA-2-70B-chat improves LLM-human agreement by up to absolute $26 \%$ and even matches or outperforms GPT-4 on many domains. BSM with GPT-4 improves agreement by a further $3 \%$ over GPT-4. Overall, these findings point to our method's capability to evaluate LLM responses to arbitrary user questions from diverse domains and to improve any base LLM as an evaluator.
- BSM also improves the consistency of LLM evaluation. It significantly reduces position, length, and self-enhancement biases of LLM-based evaluators. For instance, BSM with LLaMA-2-70Bchat reduces position bias and length bias by up to absolute $50 \%$. Importantly, BSM with GPT-4 also improves GPT-4's reliability as an evaluator when evaluating its own responses.
- For the constrained text generation task of producing stories with several concepts, BSM generates more coherent stories, which are preferred by a GPT-4 judge a substantial $93 \%$ of the time compared to a zero-shot baseline. It also improves constraint satisfaction by an absolute $12 \%$.

Overall, BRANCH-SOLVE-MERGE provides a framework for planning and task decomposition for addressing challenging multi-faceted natural language generation and evaluation tasks. As the approach is framed as a generic LLM program, it can be applied to any underlying language model and potentially a wide range of tasks.

## 2 RELATED WORK

LLM Programs and Decomposing Complex Tasks. LLM programs such as BRANCH-SOLVEMERGE solve complex problems with the help of an algorithm that breaks the problem down into multiple steps and each step is then parameterized with a different prompt to an underlying LLM (Schlag et al., 2023; Dohan et al., 2022; Creswell \& Shanahan, 2022). Complex tasks, in general, require task decomposition (Khot et al., 2022) and planning (Yao et al., 2022; Huang et al., 2022; Yao et al., 2023b; Ning et al., 2023). This has motivated a lot of recent work on advanced prompting methods across both vision and language domains. A few representative methods include decomposed prompting (Khot et al., 2022), least-to-most prompting (Zhou et al., 2022), plan and solve prompting (Wang et al., 2023a), successive prompting (Dua et al., 2022), decomposed summarization (Saha et al., 2022; 2023), text modular networks (Khot et al., 2021), and visual programming (Gupta \& Kembhavi, 2023; Cho et al., 2023). However, most of these works typically focus on reasoning problems (like commonsense, symbolic, or mathematical reasoning) that benefit from sequential decompositions. We, on the other hand, study tasks that benefit from branching into parallel decompositions, in particular LLM Evaluation and constrained text generation. As well as being an LLM program, BSM can also be seen as an instance of Graph-of-Thoughts (GoT) prompting (Lei et al., 2023; Besta et al., 2023) because the execution trace of BRANCH-SOLVE-MERGE takes the shape of a graph. GoT defines a wide array of LLM programs, including refining, backtracking and skipping graph nodes, which we do not consider here. Our work develops a specific fixed program, and applies it to the challenging tasks of evaluating or improving language models. Besta et al. (2023) consider tasks like sorting numbers, keyword counting or document merging, while Lei et al. (2023) consider Game of 24 and solving polynomial equations.

Large Language Model Evaluation. A fundamental challenge with the rapid progress of LLMs is evaluating their capabilities holistically (Chang et al., 2023; Liang et al., 2022). Human evaluation is difficult and expensive (Smith et al., 2022). On the other hand, LLMs, by virtue of being trained with RLHF, are shown to exhibit alignment with humans (Ouyang et al., 2022; Bai et al., 2022a). Hence, a standard procedure for comparing and evaluating LLM generations is by utilizing a strong LLM like GPT-4 (Bubeck et al., 2023; OpenAI, 2023a; Dubois et al., 2023; Zhou et al., 2023; Chiang \& Lee, 2023; Wang et al., 2023c; Hada et al., 2023; Liu et al., 2023). This has also led to the development of a number of evaluation benchmarks (Zhong et al., 2023; KÃ¶pf et al., 2023; Zheng et al., 2023). Recent studies have shown that LLM-based evaluators are not fair evaluators (Wang et al., 2023b; Wu \& Aji, 2023). In response, there have been proposals of using multi-agent debate frameworks (Chan et al., 2023) or developing wider and deeper LLMs (Zhang et al., 2023).

In contrast, BRANCH-SOLVE-MERGE improves LLM evaluation through an intuitive and general decomposition-based approach that can be applied on top of any LLM, and can be used to evaluate responses for a wide range of tasks.

Constrained Text Generation. LLMs are increasingly capable of generating coherent and fluent text. This has shifted the focus to evaluating LLMs for their capabilities in the more difficult setting of controllable and constrained text generation (Keskar et al., 2019; Dathathri et al., 2019; Lu et al., 2021; 2022; Lin et al., 2020; Li et al., 2022). Recent works have shown that GPT-4 struggles with constrained and planning-based text generation tasks (Bubeck et al., 2023; Madaan et al., 2023; Yao et al., 2023a). In this work, we experiment with such a constrained story generation task and show the promise of BRANCH-SOLVE-MERGE in improving text generation capabilities.

## 3 BRANCH-SOLVE-MERGE

We first introduce some notation to formally describe our method. Let $p_{\theta}$ denote an LLM with parameters $\theta$. We also denote $x=x_{1, \cdots, n}$ as a sequence of $n$ tokens, such that $p_{\theta}(x)=$ $\prod_{i=1}^{n} p_{\theta}\left(x_{i} \mid x_{1, \cdots, i-1}\right)$. BRANCH-SOLVE-MERGE is an LLM program (Schlag et al., 2023; Dohan et al., 2022) that aims to solve complex planning-based tasks with three neural modules: a branch module, a solve module, and a merge module. Each module is parameterized with unique prompts to the LLM $p_{\theta}$. The LLM program further defines an algorithm on top of these modules, acting as a controller and invoking a module at each step of the algorithm. Below, we describe each of these components in detail.

### 3.1 COMPONENTS OF BRANCH-SOLVE-MERGE

Large Language Model Program. For a given task, Branch-SolVe-Merge defines a controller in the form of an algorithm that lays out the transition logic between the modules. Let us denote the three modules with their functional forms: branch $(\cdot)$, solve $(\cdot)$, and merge $(\cdot)$. Then the program is defined as Prog : $(x, \operatorname{branch}(\cdot)$, solve $(\cdot)$, merge $(\cdot)) \rightarrow y$, taking as input a task instance $x$, along with the implementations of the modules and generating an output $y$.

Branch Module. Given a task, the branch module generates multiple sub-tasks where each subtask is represented by a unique branch. Branching into sub-problems allows the problem to be decomposed such that each part can be solved independently in parallel, at which point the partial solutions are combined. Formally, given a task input $x$, we define a 'branch' prompt prompt $_{\text {branch }}(x)$ that can be wrapped around $x$ with branching instructions and some demonstrations (if available). Conditioning on the prompt, the LLM $p_{\theta}$ generates a set of $k$ sub-problems $X=\left\{x^{(1)}, x^{(2)}, \cdots, x^{(k)}\right\}$, where $k$ is referred to as the branching factor. The sub-problems are generated auto-regressively as a sequence of tokens: $X \sim p_{\theta}\left(X \mid\right.$ prompt $\left._{\text {branch }}(x)\right)$. Then the generated token sequence may additionally go through some post-processing to textually represent the sub-problems, e.g., split up into $k$ branch prompts and prepend extra input context to each. Importantly, the flexibility of our method comes from the fact that for a given problem, the LLM decides (generates) the sub-problems and the corresponding branching factor.

Solve Module. The solve module solves the task at hand by generating an output $y^{(i)}$ for a

![](https://cdn.mathpix.com/cropped/2024_06_04_5c0930f75a5d6b348118g-04.jpg?height=46&width=1385&top_left_y=2053&top_left_x=370)
put $x^{(i)}$ with solving instructions and some input-output demonstrations (if available). For each branch, the LLM conditions on the solve prompt to generate a solution $y^{(i)}$ such that $y^{(i)} \sim$ $p_{\theta}\left(y^{(i)} \mid\right.$ prompt $\left._{\text {solve }}\left(x^{(i)}\right)\right)$.

Merge Module. The merge module fuses together the solutions to the sub-problems to generate a global solution to the top-level problem. Similar to the branch and solve prompts, we define a 'merge' prompt prompt merge $(Y)$ that wraps around a set of sub-solutions $Y=\left\{y^{(1)}, y^{(2)}, \ldots\right.$ $\left.\cdot, y^{(k)}\right\}$ with merging instructions and optional demonstrations. The language model conditions on it to generate a merged solution $y \sim p_{\theta}\left(y \mid\right.$ prompt $\left._{\text {merge }}(Y)\right)$. Conceptually, the merge module learns an aggregator function that could aggregate a set of values (using an aggregation operator) or fuse pieces of text, depending on the task.
![](https://cdn.mathpix.com/cropped/2024_06_04_5c0930f75a5d6b348118g-05.jpg?height=1088&width=1396&top_left_y=278&top_left_x=360)

Figure 2: Examples of LLM Evaluation branch generation. We show different branches (evaluation plans) generated by BSM with a LLaMA-2-70B-chat model for different kinds of questions: (top) a turn-2 writing question and (bottom) a coding question. The more important branches (e.g., 'Adherence to Instructions' for the first question and 'Code Correctness' for the second question) are generated first by the model, suggesting a priority order among the aspects (while all are executed equally in parallel in the solve module).

In the following two sub-sections, we motivate and conduct case studies of our method with two challenging NLP tasks, that of LLM evaluation and constrained generation. Each of the modules are implemented zero-shot for the purpose of this study. However, these could be additionally accompanied with few-shot in-context examples or could also be fine-tuned modules.

### 3.2 Branch-SolVe-MERGE: CASE StudY With LLM EvaluATIon

Task Description. We consider the task of evaluating LLM-based chat assistants. Formally, given an open-ended question (that evaluates an LLM's multi-turn conversational and instructionfollowing ability) and a pair of responses from two LLM agents, the task requires producing a preference judgement of which response is better or if it is a tie (see Figure 1 for an example). Evaluating LLM responses is challenging for multiple reasons:

1. Long-form answers to arbitrary questions. With the goal of providing a general-purpose assistant, the user asks arbitrary questions from any domain, and the LLM responds with long-form answers (Zheng et al., 2023). Based on the initial model response, the user can ask follow-up questions. Depending on the type of question, the evaluation process must consider the intent of the question, what is expected from an ideal response, and what criteria to evaluate the generated response against.
2. LLM evaluators are prone to biases. LLM-based evaluators are not reliable and are prone to different kinds of biases including (a) Position Bias: evaluation changes based on the encoding order of the responses, (b) Length Bias: tendency to favor longer responses, (c) Self-enhancement Bias: the LLM-evaluator favoring its own responses (Zheng et al., 2023; Wu \& Aji, 2023; Wang et al., 2023b).
3. GPT-4 as evaluator is expensive. While API-based models like GPT-4 are fairly good evaluators (Liu et al., 2023; Zheng et al., 2023; Bubeck et al., 2023), these models are proprietary and charge users per token generated. Current open-source alternatives correlate less well with humans and are much more susceptible to the aforementioned biases (Zheng et al., 2023).
4. Hand-designing evaluation plans is not scalable. A robust evaluator should generalize well, capable of evaluating responses to arbitrary questions and hence, hand-designing the evaluation plan for every task is not a desirable approach (Liu et al., 2023; Wu \& Aji, 2023). For example, see Figure 1, where evaluating responses to a 'writing' question requires considering factors like 'Relevance', 'Clarity', etc whereas if the question is a 'coding' question (see Figure 2), one should evaluate for 'Code Correctness', 'Code Readability', etc.

Hence, given the multi-faceted nature of this evaluation task, we develop a version of BRANCHSolVE-MERgE, as described below. For the purpose of this study, we focus on evaluating two-turn conversational questions although our method is generally applicable for any number of turns. Let us denote the first question as $q_{1}$ and the follow-up question as $q_{2}$. Let the responses from the two LLMs $A$ and $B$ be $r_{1}^{(A)}$ and $r_{1}^{(B)}$ for $q_{1}$, and $r_{2}^{(A)}$ and $r_{2}^{(B)}$ for $q_{2}$.

Branch Module for LLM Evaluation. The branch module generates an evaluation plan. The plan is a set of evaluation criteria that the response will be evaluated against. To ensure that the plan is not biased by the model responses, the branch module only conditions on the input question. In particular, we define the branch module for turn-1 questions as branch $\left(q_{1}\right)$, while for turn-2 questions, it conditions on both turn-1 and turn-2 questions, represented as branch $\left(q_{1}, q_{2}\right)$. The language model generates a set of evaluation criteria, branch $(q) \rightarrow\left\{c_{i}\right\}_{i=1}^{k}$, where each $c_{i}$ is the title of the criterion (e.g., 'Relevance') and a short description of how to evaluate for it (e.g., 'Assess how well the response aligns with the user's question and whether it provides relevant information about cultural experiences and must-see attractions in Hawaii.'). Figures 1 and 2 show examples of evaluation plans for different questions generated by the branch module with a LLaMA-2-70B-chat model, note that criteria are adapted for the question type. Refer to Figure 4 for the exact branch prompt we use.

Solve Module for LLM Evaluation. The solve module compares and evaluates the responses based on a specific evaluation criterion. The output of the evaluation is a pair of scores (within a specified range, according to the solving instruction, e.g., 1-5) for each of the responses. Given an evaluation criterion $c$, we denote the solve module for a turn-1 question $q_{1}$ and turn-2 question $q_{2}$ as follows.

$$
\begin{array}{r}
\text { solve }\left(q_{1}, r_{1}^{(A)}, r_{1}^{(B)}, c\right) \rightarrow\left(s_{1}^{(A)}, s_{1}^{(B)}\right) \\
\operatorname{solve}\left(q_{1}, q_{2}, r_{2}^{(A)}, r_{2}^{(B)}, c\right) \rightarrow\left(s_{2}^{(A)}, s_{2}^{(B)}\right)
\end{array}
$$

where $s_{1}^{(A)}$ and $s_{1}^{(B)}$ are the evaluation scores assigned to the two assistant responses for $q_{1}$ while $s_{2}^{(A)}$ and $s_{2}^{(B)}$ are those for $q_{2}$. Note that the solve module is not symmetric i.e., the order in which the two responses are encoded in the LLM is important due to its auto-regressive nature (and we address this below in our LLM program). The module additionally generates explanations along with the scores. Figure 1 shows example generations from the solve module with a LLaMA-2-70Bchat model. Refer to Figure 5 for the exact solve prompt we use.

Merge Module for LLM Evaluation. We develop two variants of the merge module. A simple non-neural variant aggregates the scores across all branches by summing them up. We also develop a neural LLM variant that conditions on the individual evaluations and generates the final verdict with a model-decided aggregation strategy. We denote this with:

$$
\operatorname{merge}\left(q,\left\{c_{i}\right\}_{i=1}^{k},\left\{s_{i}^{(A)}\right\}_{i=1}^{k},\left\{s_{i}^{(B)}\right\}_{i=1}^{k}\right) \rightarrow y
$$

where the evaluation criteria $\left\{c_{i}\right\}_{i=1}^{k}$ are the outputs of the branch module and $s_{i}^{(A)}$ and $s_{i}^{(B)}$ are the criterion-wise evaluations (scores and explanations) of the two assistant responses generated from the solve module. The final verdict is $y \in\{A, B$, tie $\}$.

Large Language Model Program for LLM Evaluation. The overall LLM program pseudocode is given in Algorithm 1. If $q$ is a $t$-turn question, we assume that it encodes all the questions up to

![](https://cdn.mathpix.com/cropped/2024_06_04_5c0930f75a5d6b348118g-07.jpg?height=910&width=1391&top_left_y=274&top_left_x=367)

Figure 3: An illustration of BRANCH-SOLVE-MERGE with LLaMA-2-70B-chat for constrained story generation. Given a set of random concepts, the 'Branch' module first divides them into two sets and generates a story topic. The 'Solve' module conditions on the concepts and the topic to generate an intermediate story for each of the branches. The 'Merge' module merges the intermediate stories to generate a final story ensuring that all concepts are still present.

that turn in order. To account for position bias, the program executes two independent runs of BSM by swapping the encoding order of the responses in the 'solve' module. The final judgment is either 'A' or 'B' if and only if the judgement is consistent for both orders, otherwise it is a 'tie'.

### 3.3 Branch-SolVe-Merge: CaSe Study With ConStrained GENERation

Task Description. Our next case study shows the general applicability of BSM by applying it to a completely different task, that of LLM generation. We consider a constrained story generation task given a set of $N$ given concepts $l$, the task is to generate a coherent story $y$ by including all concepts in it. Figure 3 shows an example. Recent work has shown that constrained text generation poses significant challenges even for GPT-4 (Madaan et al., 2023; Yao et al., 2023a; Bubeck et al., 2023). When the number of concepts is large, LLMs tend to either leave out some concepts or generate text that is incoherent. The task requires composition incorporating the various constraints. When a standard model has already generated part of the text without including certain concepts, it is unable to roll back, in which case it tends to either miss some concepts completely or else include them in such a manner that the final generation is incoherent.

Branch Module for Constrained Generation. The branch module branch $(l) \rightarrow\left(l_{1}, l_{2}, t\right)$ proposes a story generation plan, consisting of (1) two subsets of concepts $l_{1}$ and $l_{2}$ and (2) a story topic $t$. The two subsets represent sub-problems of the original story generation task with a smaller number of concepts. The story topic ensures that all sub-stories generated as part of BSM belong to the same topic. While we limit the branching factor to two for the purpose of this study, the concepts could be divided into an arbitrary number of subsets. See Figure 3 for examples of branches.

Solve Module for Constrained Generation. The solve module solve $\left(l_{i}, t\right) \rightarrow y_{i}$ conditions on a subset of concepts $l_{i}$ and the story topic $t$ to generate a story $y_{i}$ on that topic, while also including all concepts in $l_{i}$. Intuitively, when the number of concepts is smaller, 'solving' the constrained generation task is easier.

Merge Module for Constrained Generation. The merge module merge $\left(y_{1}, y_{2}\right) \rightarrow y$ conditions on two intermediate stories (i.e., solutions to the two sub-problems) and fuses them together to generate the final story $y$. Since both intermediate stories belong to the same high-level topic, the fusion can lead to a final coherent story. For instance, Figure 3 shows that the final story contains all major parts of the two sub-stories, while undergoing some sentence restructuring and including phrases like 'Meanwhile, outside' to better connect the stories. Overall, BSM ensures better constraint satisfaction by solving sub-problems and maintains coherency through the use of a top-level plan that includes a story topic.

## 4 EXPERIMENTS

We conduct experiments to evaluate BRANCH-SOLVE-MERGE for both LLM Evaluation (in Section 4.1) and Constrained Text Generation (later in Section 4.2).

### 4.1 LARGE LANGUAGE Model EVALUATION

### 4.1.1 EXPERIMENTAL SETUP

Dataset Details. We experiment with the MT-Bench dataset, that evaluates LLMs as judges of other LLM's responses when acting as helpful AI assistants in multi-turn conversations (Zheng et al., 2023). It consists of 2400 LLM responses and 3000 expert human judgements. LLM outputs are responses to 80 representative instructions from 8 diverse domains: writing, roleplay, extraction, reasoning, math, coding, knowledge I (STEM), and knowledge II (humanities/social science). Each question is a conversational question, consisting of two turns, in which the turn-2 question is a follow-up to the turn-1 question. For each question, the dataset consists of responses from 6 different LLMs (Alpaca-13B, Vicuna-13b, LLaMA-13B, Claude-v1, GPT-3.5-turbo, and GPT-4), resulting in 15 possible response pairs. Thus, the entire evaluation set consists of 300 response-pair samples per category.

Evaluation Metrics. We evaluate BSM (and baselines) using the following four metrics.

- LLM-Human Agreement (Ag). Our primary metric of interest is LLM-human agreement. We report agreement scores $\in[0,1]$ individually for turn-1 and turn-2 questions, as well as their combination. Each sample (question and two model responses) has a variable number of human judgments. Hence, following past work, we compute agreement by independently matching each human judgment for each sample with the model judgment (Zheng et al., 2023). Table 10 in the Appendix shows that even if the agreement is computed with a majority vote of the individual human judgments, our conclusions do not change.
- Position Bias (PB). To evaluate whether BSM helps reduce the consistency problem with LLMbased evaluators, we report Position Bias. It refers to the fraction of samples where the judgment changes based on the encoding order of the pair of responses that are being compared.
- Length Bias (LB). We measure length bias as the fraction of samples where humans prefer the shorter response but the evaluator model does not. Note that measuring length bias in isolation is challenging because knowing whether the model prefers the longer response because of its length (and not for another reason) is an interpretability question and humans also tend to prefer longer responses, especially for open-ended questions.
- Self-enhancement Bias (SB). Self-enhancement bias refers to an evaluator model preferring its own responses. In order to quantitatively measure whether BSM helps reduce this bias, we consider the following setting. We use GPT-4 as the base judge model and consider the subset of samples from the MT-Bench benchmark where one of the responses is also generated by GPT-4. If BSM with GPT-4 improves agreement with humans for this subset of samples, it suggests that even in scenarios where a model A is judging its own outputs, BSM (with model A) leads to a better evaluator.

While multiple past works have highlighted the importance of these biases (Zheng et al., 2023; Wang et al., 2023b; Wu \& Aji, 2023), we measure all of them with concrete metrics within the same evaluation framework. Conceptually, the human agreement metric evaluates correctness while position bias for example evaluates consistency of LLM-based evaluators. Note that these are complemen-

| Method | Overall |  |  | Turn-1 |  |  | Turn-2 |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\overline{\operatorname{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\overline{\mathrm{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\overline{\operatorname{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ |
| Vicuna-33B | 0.51 | 30.66 | 48.12 | 0.55 | 22.66 | 48.43 | 0.47 | 38.66 | $47.82 \quad$ |
| SC (w/ Vicuna-33B) | 0.51 | 25.66 | 45.11 | 0.53 | 18.00 | 46.87 | 0.49 | 33.33 | $43.47 \quad 2 \quad-1$ |
| BSM (w/ Vicuna-33B) | 0.56 | 20.00 | 42.85 | 0.57 | 18.00 | 42.18 | 0.56 | 22.00 | 43.47 |
| LLaMA-2-70B-chat | 0.43 | 51.66 | 54.88 | 0.53 | 42.66 | 50.00 | 0.34 | 60.66 | $59.42 \quad$ |
| SC (w/ LLaMA-2-70B-chat) | 0.52 | 35.66 | 48.12 | 0.57 | 32.00 | 45.31 | 0.47 | 39.33 | $50.72 \quad$ |
| BSM (w/ LLaMA-2-70B-chat) | 0.55 | 17.33 | 39.09 | 0.60 | 14.66 | 39.46 | 0.50 | 20.00 | 39.13 |
| GPT-4 | 0.59 | 17.33 | 39.09 | 0.57 | 18.66 1 1 | 39.06 | 0.60 | 16.00 | $39.13 \quad$ |
| BSM (w/ GPT-4) | 0.62 | 17.00 | 36.84 | 0.63 | 21.33 | 43.75 | 0.61 | 12.66 | $30.43 \quad$ |

Table 1: Comparison of zero-shot LLM evaluators, Self-Consistency (SC) and BRANCH-SoLVe-MERGE (BSM) methods on the 'writing' questions in the MT-Bench dataset. We report LLM-Human Agreement (Ag), Position Bias (PB), and Length Bias (LB) for turn-1 and turn-2 question overall, and individually. BRANCHSOLVE-MERGE improves agreement scores, and for open-source models leads to a significant reduction of position and length bias.

tary aspects and an ideal evaluator should perform well in all metrics (i.e., high agreement scores and low biases) for it to be reliably used.

Implementation Details. We develop BSM on top of multiple state-of-the-art open-source and API-based LLMs of varying scales and capabilities: LLaMA-2-7B-chat (Touvron et al., 2023), Vicuna-33B (Chiang et al., 2023), LLaMA-2-70B-chat (Touvron et al., 2023), and GPT-4 (OpenAI, 2023b). We implement all modules zero-shot, providing only module-specific instructions and assuming no access to demonstrations of how to branch, solve, or merge. For better reproducibility, all modules generate text using greedy decoding. For the branch module, the LLM is prompted to generate a plan consisting of a maximum of five evaluation criteria (which we found it adheres to in experiments). For the merge module, we find that the non-neural merge of summing up the criterion-wise evaluations is simple and works well in practice, hence all our experimental results are reported with that method. Refer to our prompts in the Appendix for additional implementation details.

Baselines. We compare our method, BSM, to zero-shot prompting with the same LLM, using the same evaluation prompt as used in prior work (Zheng et al., 2023). We also compare with SelfConsistency (Wang et al., 2022), which samples multiple evaluations from the prompted LLM (with temperature 0.7 ) and chooses the majority vote as the final judgment. All methods, including BSM, account for position bias in the same manner, generating a verdict for both encoding orders and choosing the final verdict based on the individual verdicts (assigning a tie if the two encoding orders disagree). In particular, Self-Consistency computes majority vote independently for each encoding order. To ensure fair comparisons, Self-Consistency samples the same number of generations as the branching factor in BSM (five, in our experiments). We also note that Self-Consistency is a simple special case of BSM, where the branch module spawns multiple instances of the same underlying problem (instead of sub-problems), solves them by sampling different solutions, and the merging operator is a majority vote.

### 4.1.2 MAIN RESULTS

BRANCH-SolVE-MERGE improves LLM-human agreement and reduces biases. Table 1 evaluates the efficacy of BRANCH-SOLVE-MERGE, specifically focusing on the 'writing' category of questions from the MT-Bench benchmark. We report our main findings below.

- Overall agreement. We find that BSM improves LLM-human agreement for both turn-1 and turn-2 questions, when applied to all three base LLMs. For LLaMA-2, compared to the zeroshot baseline, BSM obtains an overall absolute improvement of $12 \%$ in agreement score, making LLaMA-2-70B-chat competitive with GPT-4 for turn-1 (but still lags behind for turn-2). Even

|  | $\mathrm{Ag} \uparrow$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ |
| :--- | :---: | :---: | :---: |
| GPT-4 | 0.51 | $\mathbf{6 . 3 3}$ | 36.36 |
| BSM (w/ GPT-4) | $\mathbf{0 . 5 4}$ | 7.33 | $\mathbf{3 4 . 5 4}$ |

Table 2: BSM leads to less self-enhancement bias. BSM with GPT-4 leads to better agreement scores (by 3\%) for the fraction of samples where one of the responses is also generated by GPT-4.

though zero-shot GPT-4 is the state-of-the-art LLM-based evaluator, applying BSM obtains a further improvement of $3 \%$.

- Turn-1 versus Turn-2 questions. Evaluating turn-2 (follow-up) questions is harder because it requires additional contextualization of the responses for the turn-1 question. This is also reflected in all zero-shot models exhibiting lower turn-2 agreement scores (e.g., LLaMA-2-70B-chat results drop from 0.53 in turn-1 to 0.34 in turn-2). BSM shows that a decomposition approach which generates an evaluation plan is particularly helpful for evaluating long context questions, resulting in more improvements for turn-2 questions (e.g., a $16 \%$ improvement with LLaMA-2). An illustration of this is shown in Figure 2, in which for the turn-2 question, the model generates 'Adherence to Instructions' as the first criterion to evaluate.
- Self-Consistency versus BSM. BSM also outperforms Self-Consistency (e.g., by up to $5 \%$ with Vicuna). As noted earlier, Self-Consistency is a special case of BSM. Moreover, Self-Consistency with comparatively weaker models like Vicuna may not always be effective because of the model's inability to generate vastly different solutions (Wang et al., 2022). BSM, on the other hand, works well across all models. This result is also noteworthy because both approaches leverage similar amounts of compute in generating multiple solutions - but branching and solving the differing sub-problems provides superior results to solving the same problem multiple times.
- Position and Length Bias Reduction. On top of improving LLM-human agreement, BSM helps reduce critical biases with LLM-based evaluators. Weaker models exhibit biases more frequently e.g., LLaMA-2 suffers from position bias almost half the time. However, BSM obtains a significant $34 \%$ reduction in position bias, matching that of GPT-4. We also observe a significant reduction in length bias. BSM with GPT-4 does not impact position bias much (despite improving its LLM-Human Agreement). Nevertheless, BSM opens up the possibility of using weaker open-source models also as evaluators, closing the gap to GPT-4.
- Self-enhancement Bias reduction. Table 2 evaluates self-enhancement bias by comparing BSM (with zero-shot GPT-4) for the fraction of samples where one of the responses is also generated by GPT-4. We observe a $3 \%$ better correlation with humans, suggesting that BSM leads to a better evaluator even when the LLM is judging its own outputs.
- BSM with comparatively smaller models. We also investigate whether smaller models like LLaMA-2-7B-chat can benefit from BSM. Table 3 shows that smaller models are, in general, weak evaluators. Even then, BSM leads to a moderate $2 \%$ improvement in agreement scores, while self-consistency proves to be ineffective. More encouragingly, BSM reduces the position bias by a significant $14 \%$. Although the underlying model is weak and may not be best suited for LLM evaluation, the BSM decomposition-based approach makes its evaluations much more consistent.

In summary, our results suggest that BSM is a generic method that can be applied to any LLM for evaluating generations from language models. It improves both correctness (by improving human agreement) and consistency (by reducing biases) of LLM-based evaluators.

BRANCH-SolVE-Merge generalizes well across domains. In Table 4, we evaluate BSM's ability to evaluate generations for questions in the categories of 'Roleplay', 'Extraction', 'Stem', and 'Humanities'. We find that BSM is robust and performs well across domains in terms of improvement over the LLaMa-2-70B-chat baseline, and approaches GPT-4 performance on several of the domains. In particular, on the Stem domain, it is able to improve agreement scores over the baseline by up to $26 \%$ (absolute), match GPT-4, and even outperform it in terms of position and length biases.[^2]

| Method | Overall |  |  | Turn-1 |  |  | Turn-2 |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\overline{\mathrm{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\overline{\mathrm{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\overline{\operatorname{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ |
| LLaMA-2-7B-chat | 0.39 | 62.33 | 54.88 | 0.42 | 59.33 | 51.56 | 0.35 | 65.33 | 57.97 |
| SC (w/ LLaMA-2-7B-chat) | 0.38 | 54.00 | 57.89 | 0.39 | 54.00 | 57.81 | 0.36 | 54.00 | 57.97 |
| BSM (w/ LLaMA-2-7B-chat) | 0.41 | 48.33 | 53.38 | 0.43 | 44.66 | 51.56 | 0.39 | 52.00 | 55.07 |

Table 3: LLM Evaluation with weaker and smaller LLMs like LLaMA-2-7B-chat on the 'writing' questions of MT-Bench. BSM leads to moderate gains in human agreement but a significant reduction in position bias.

| Domain | Method | Overall |  |  | Turn-1 |  |  | Turn-2 |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\overline{\operatorname{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\overline{\operatorname{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | LB $\downarrow$ | $\overline{\operatorname{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ |
| Roleplay | LLaMA-2-70B-c | 0.55 | 29.66 | 51.67 | 0.61 | 30.00 | 48.14 | 0.50 | 29.33 | 55.88 5 |
|  | BSM (w/LLaMA-2-70B-c) | 0.61  | 11.00 | 40.26 | 0.66  | 10.66 | 38.27 | 0.56 | 11.33 | $42.64 \quad 4 \quad$ |
|  | GPT-4 | 0.64 | 13.66 | 43.62 | 0.65 | 16.00 | 45.67 | 0.63 | 11.33 | $41.17 \quad$ |
| Extraction | LLaMA-2-70B-c | 0.40 | 70.66 | 51.82 | 0.46 | 61.33 | 51.47  | 0.33 | 80.00 | $52.08 \quad$ |
|  | BSM (w/ LLaMA-2-70B-c) | 0.55 | 31.33 | 40.24 | 0.55 | 32.00 | 45.58 | 0.44 | 30.66 | $36.45 \quad$ |
|  | GPT-4 | 0.71 | 15.00 | 33.53 | 0.68 | 13.33 | 35.29 | 0.75 | 16.66 | 32.29 |
| Stem | LLaMA-2-70B-c | 0.46 | 59.33 | 55.31 | 0.50 | 52.66 | 51.19 | 0.43 | 66.00 | $61.40 \quad$ |
|  | BSM (w/LLaMA-2-70B-c) | 0.72 | 10.33 | 44.68 | 0.70 | 10.66 | 40.47 | 0.73 | 10.00 | 50.87  |
|  | GPT-4 | 0.72 | 13.66 | 46.80 | 0.68 | 16.66 | 44.04 | 0.75 | 10.66 | 50.87 |
| Humanities | LLaMA-2-70B-c |  | 59.00 | 45.69 | 0.51 | 52.00 | 49.18 | 0.41 | 66.00 | $43.33 \quad$ |
|  | BSM (w/LLaMA-2-70B-c) | 0.67 | 18.00 | 36.42 | 0.63 | 18.00 | 39.34 | 0.71 | 18.00 | $34.44 \quad$ |
|  | GPT-4 | 0.73 | 14.00 | 37.08 | 0.70 | 19.33 | 42.62 | 0.76 | 8.66 | 33.33 |

Table 4: LLM evaluation for 'Roleplay', 'Extraction', 'Stem', and 'Humanities' question categories of MTBench. We compare LLaMA-2-70B-chat BSM with the baseline zero-shot method, and also report GPT-4 results. BSM obtains significant improvements over the LLaMA baseline, and matches or is close to GPT-4 agreement in three of the four domains, while sometimes outperforming GPT- 4 in reducing biases.

BRANCH-SOLVE-MERGE is equally performant in grading reference-based questions. So far, we have applied BSM in reference-free evaluations for comparatively open-ended questions like writing, roleplay, etc. However, LLMs generally struggle with complex tasks like in math, reasoning, and coding (Cobbe et al., 2021; Chen et al., 2021; Wei et al., 2022). More so, even when LLMs are able to successfully answer these questions, they may not be able to evaluate them correctly. Zheng et al. (2023) suggest alleviating this issue by first generating an answer using GPT-4 and then appending it to the evaluation prompt, which is our baseline in this experiment. For BSM, we then follow a similar recipe for grading these categories of questions by conditioning the 'solve' module on the GPT-4 generated answers. The key assumption here is that these answers are curated once and have limited variations unlike answers for open-ended questions, thus allowing us to evaluate BSM in reference-based settings. Table 5 shows the results. BSM significantly outperforms LLaMA-2-70B-chat in all categories (by up to $14 \%$ better agreement scores and $27 \%$ better position bias in coding questions). On Math, it even outperforms the state-of-the-art GPT-4 evaluator, outperforming on all metrics.

### 4.1.3 ANALYSIS and AblationS OF BRanch-SOLVE-MERGE

Combining BSM and SC reduces position bias further. BSM generates a single solution for each sub-problem (each branch). A possible enhancement is combining BSM with self-consistency i.e., sampling multiple solutions for each sub-problem. In particular, we implement BSM+SC by sampling five evaluations per branch (with temperature 0.7 ) and then the score for each subevaluation in that branch is given by the average score. We compare BSM with BSM+SC in Table 6. While agreement scores do not improve further, we observe a $2 \%$ reduction in position bias. This points to two conclusions. First, BSM, through its decomposition approach, already constructs sub-problems that are granular enough and hence, the variance reduction that one obtains through self-consistency within each sub-problem is limited. However, the moderate reduction in position bias still reflects its usefulness, which is a direct effect of making evaluations more consistent.

| Domain | Method | Overall |  |  | Turn-1 |  |  | Turn-2 |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\overline{\operatorname{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\operatorname{Ag} \uparrow$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\mathrm{Ag} \uparrow$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ |
| Coding | LLaMA-2-70B-c | 0.47 | 52.33 | 51.32 | 0.50 | 47.33 | 46.77 | 0.44 | 57.33 | 56.86 |
|  | BSM (w/ LLaMA-2-70B-c) | 0.61 | 25.66 | 42.47 | 0.60 | 24.66 | 40.32 | 0.63 | 26.66 | 45.09 |
|  | GPT-4 | 0.61 | 19.66 | 38.93 | 0.60 | 21.33 | 35.48 | 0.62 | 18.00 | 43.13 |
| Reasoning | LLaMA-2-70B-c | 0.47 | 38.00 | 48.75 | 0.48 | 33.33 | 40.81 | 0.46 | 42.66 | 61.29 |
|  | BSM (w/ LLaMA-2-70B-c) | 0.57 | 20.33 | 46.25 | 0.60 | 19.33 | 44.89 | 0.55 | 21.33 | 48.38 |
|  | GPT-4 | 0.64 | 22.66 | 53.75 | 0.69 | 12.00 | 51.02 | 0.59 | 33.33 | 58.06 |
| Math | LLaMA-2-70B-c | 0.52 | 45.66 | 50.56 | 0.56 | 48.00 | 54.76 | 0.48 | 43.33 | 46.80 |
|  | BSM (w/ LLaMA | 0.64 | 17.66 | 34.83 | 0.69 | 16.00 | 33.33 | 0.59 | 19.33 | 36.17 |
|  | GPT-4 | 0.62 | 19.00 | 39.32 | 0.67 | 17.33 | 40.47 | 0.58 | 20.66 | 38.29 |

Table 5: Reference-based LLM evaluation for 'Coding', 'Reasoning', and 'Math' question categories of MT-Bench. We compare LLaMA-2-70B-chat BSM with the baseline zero-shot method and also report GPT-4 results. LLMs generally struggle with questions in these domains. BSM improves reference-based evaluations and for math, outperforms GPT-4.

|  | Overall |  |  | Turn-1 |  |  | Turn-2 |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\overline{\mathrm{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\overline{\mathrm{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\overline{\mathrm{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ |
|  | 0 | 17.33 | 39.09 | 0.60 | 14 | 39.46 | 0.50 | 20 | 39 |
| $\mathrm{BSM}+$ | 0.55 | 15.33 | 39.09 | 0.61 | 10.66 | 39.06 | 0.49 | 20.00 | 39.13 |

Table 6: Effect of using Self-Consistency within each branch of Branch-Solve-Merge (BSM+SC). Results are with the LLaMA-2-70B-chat model. While the overall agreement scores do not improve further, we obtain a further $2 \%$ reduction in position bias.

Effect of Branching Factor. BSM has the benefit of relying on the underlying LLM for deciding what sub-problems to branch to, while the prompt controls the maximum branching factor (see the phrase 'list of up to five factors' in the branch prompt in Fig. 4). We vary this maximum branching factor from 2 to 5 and study its effect on 100 samples from the 'writing' category of questions. Table 7 reports our findings. We observe highest agreement at a branching factor of 4 , after which the result mostly saturates. In general, the optimal branching factor should depend on the specific question under consideration and unlike past work where users specify what factors to evaluate on (Liu et al., 2023; Zheng et al., 2023), BSM generates that plan on its own. Position bias continues to decrease with increasing branching factor, where more branches helps reduce variance in the final judgment.

BSM is robust to evaluation scale. Evaluation tasks, in general, require defining a scale for scoring the responses. In Table 8 , we compare the performance of BSM by varying this evaluation scale, specified in the 'solve' prompt (see Fig 5), either scoring 1-5 (used in the main experiments) or 1-10. We observe that BSM is fairly robust to such variations, obtaining comparable agreement scores. The position bias, however, increases slightly with a larger scale.

### 4.2 CONSTRAINED TEXT GENERATION

This section describes our experimental setup and findings for the constrained story generation task.

### 4.2.1 EXPERIMENTAL SETUP

Dataset Details. The constrained story generation task we consider is a more challenging variant of a generative commonsense reasoning task, CommonGen (Lin et al., 2020). While the original task requires generating a single coherent sentence from 3 or 4 concepts, we increase the complexity of the task by having the model generate a concise story consisting of 10 concepts (Madaan et al., 2023). We experiment with 100 samples for the purpose of this study.[^3]

| BF | Overall |  |  | Turn-1 |  |  | Turn-2 |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathrm{Ag} \uparrow$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\overline{\operatorname{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\overline{\operatorname{Ag} \uparrow}$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ |
| 2 | 0.50 | 22.00 | 49.20 | 0.49 | 24.00 | 45.00 | 0.50 | 22.00 | 56.52 |
| 3 | 0.52 | 19.00 | 38.09 | 0.53 | 14.00 | 35.00 | 0.51 | 24.00 | 43.47 |
| 4 | 0.53 | 19.00 | 38.09 | 0.51 | 16.00 | 35.00 | 0.55 | 22.00 | 43.47 |
| 5 | 0.52 | 12.00 | 34.92 | 0.51 | 12.00 | 35.00 | 0.54 | 12.00 | 34.78 |

Table 7: Impact of maximum Branching Factor (BF) on 100 samples of the 'writing' category in LLM response evaluation with BSM on LLaMA-2-70B-chat.

| Solving Technique | Overall |  |  |  | Turn-1 |  |  |  | Turn-2 |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathrm{Ag} \uparrow$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ | $\mathrm{Ag} \uparrow$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ |  | $\mathrm{Ag} \uparrow$ | $\mathrm{PB} \downarrow$ | $\mathrm{LB} \downarrow$ |  |
| Eval Scale (1-5) | 0.52 | 12.00 | 34.92 | 0.51 | 12.00 | 35.00 |  | 0.54 | 12.00 | 34.78 |  |
| Eval Scale (1-10) | 0.50 | 18.00 | 36.50 | 0.51 | 18.00 | 40.00 |  | 0.50 | 18.00 | 30.43 |  |

Table 8: Analysis of evaluation scale for LLM response evaluation, with 100 samples of 'writing' category. BSM (with LLaMA-2-70B-chat) is fairly robust to such variations.

Evaluation Metrics. We evaluate the generated stories along two axes:constraints satisfaction and overall story quality. For constraints satisfaction, we report two metrics: (a) All Present: fraction of samples where all constraints are satisfied i.e., there are no missing concepts, and (b) Missing Concepts: average percentage of missing concepts. Higher 'all present' and lower 'missing concepts' are preferable. We identify a missing concept if it does not appear in the story in any word form. For evaluating overall story quality, we conduct a pairwise evaluation with GPT-4. The evaluation prompt is provided in Figure 7. To account for position bias in this pairwise comparison, we follow our findings from the LLM Evaluation task and conduct each evaluation twice, by swapping the order of the stories and preferring one story over the other only if the evaluations are consistent.

Implementation Details. We evaluate BSM using LLaMA-2-7B-chat and LLaMA-2-70B-chat. All modules generate text using greedy decoding. For the branch module, the LLM is prompted to divide the concepts into two groups. Refer to our prompts in Figure 6 in the Appendix for additional implementation details.

Baselines. We compare BSM to zero-shot prompting with the same LLM. The zero-shot prompt is similar to the 'solve' prompt in BSM, with the exception that it first proposes a story topic and then generates a story on that topic. This not only makes the comparison with BSM fair but we also find that first proposing a topic generally makes the story more coherent.

### 4.2.2 RESULTS AND ANALYSIS

Constraint Satisfaction. Table 9 shows the results. We observe that BSM with both LLaMA2-7B-chat and LLaMA-2-70B-chat leads to a significant improvement in constraint satisfaction metrics. In particular, it increases the fraction of samples where no constraints are violated ('All

|  | LLaMA-2-7B-chat |  |  | LLaMA-2-70B-chat |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | All Present $\uparrow$ | Missing Concepts $\downarrow$ | All Present $\uparrow$ | Missing Concepts $\downarrow$ |  |
| Standard Prompting | 13.0 | 18.0 |  | 21.0 | 26.6 |
| BSM | $\mathbf{2 3 . 0}$ | $\mathbf{1 5 . 5}$ |  | $\mathbf{2 8 . 0}$ | $\mathbf{1 4 . 7}$ |

Table 9: Constrained story generation evaluation results. BSM (with both LLaMA-2-7B-chat and LLaMA2-70B-chat) improves constraint satisfaction in the generated stories by improving on the fraction of samples where all constraints are satisfied (by up to $10 \%$ ) and also reducing the average number of missing concepts (by up to $12 \%$ ) compared to standard prompting using the same LLM.

Present') by $7-10 \%$. Despite this improvement, it is important to note that this is a still a challenging task even for a stronger LLaMA-2-70B-chat model and the scale of the model has little impact on constraint satisfaction. For example, even BSM with LLaMA-2-70B-chat omits at least one concept for $72 \%$ of the samples, echoing the findings from prior work that constrained text generation is hard even for state-of-the-art LLMs (Bubeck et al., 2023; Yao et al., 2023a).

Overall Story Quality. BSM not only satisfies more constraints but almost always generates a more coherent story. We find that in a head-to-head comparison with the zero-shot prompting baseline (with LLaMA-2-70B-chat), stories generated by BSM are preferred a substantial $93 \%$ of the time by GPT-4. This can be attributed to two aspects of BSM. First, in each of the branches, the model conditions on a lesser number of concepts and thus generates intermediate stories that by themselves are more coherent. Second, in the merging step, the model is able to condition on these two intermediate stories and generate a final story that further improves the coherence.

Analysis of Missing Concepts in BSM. The source of missing concepts in BSM can be attributed to one of the following two categories: (a) the 'solve' module, i.e., the model omits concepts even when generating an intermediate story in a branch subproblem with a lesser number of concepts; or (b) the 'merge' module, i.e., the intermediate stories include their respective concepts but the fusion process omits some of these. We observe that out of $72 \%$ of the BSM stories (with LLaMA2-70B-chat) where at least one concept is missing, a significant $60 \%$ of these belong to the first category (i.e., concept omission in the 'solve' module) versus only $12 \%$ belong to the second category (i.e., concept omission during 'merging'). This suggests that constraint satisfaction can be further improved via a 'recursive' BSM method involving iterative branching to even more granular sub-problems. However, recursive BSM would be significantly more expensive because of many more calls to the base LLM. We leave this exploration as part of future work.

## 5 CONCLUSION

We presented Branch-SolVE-MERGE (BSM), a Large Language Model program for improving LLM evaluation and generation. We conducted two case studies with different implementations of branch, solve, and merge modules, showcasing the effectiveness and generalizability of BSM. On the LLM evaluation task, BSM substantially improves correctness and consistency of evaluation by demonstrating better LLM-human agreement and reducing position bias respectively. BSM also improves a constrained text generation task, enhancing its coherency and satisfying more constraints.

## REFERENCES

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. URL https://arxiv.org/abs/2204.05862.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b. URL https: //arxiv.org/abs/2212.08073.

Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023. URL https://arxiv.org/abs/2308.09687.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_medium=email\& utm_source=transaction.

SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. URL https://arxiv.org/abs/2303.12712.

Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023. URL https://arxiv.org/abs/2308.07201.

Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023. URL https://arxiv.org/abs/2307.03109.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. URL https:// arxiv.org/abs/2107.03374.

Xinyun Chen, Maxwell Lin, Nathanael SchÃ¤rli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. URL https://arxiv.org/abs / 2304.05128 .

Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15607-15631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.870. URL https: / /aclanthology.org/2023.acl-long. 870.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.

Jaemin Cho, Abhay Zala, and Mohit Bansal. Visual programming for text-to-image generation and evaluation. In NeurIPS, 2023. URL https://arxiv.org/abs/2305.15328.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv. org/abs/2110.14168.

Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. arXiv preprint arXiv:2208.14271, 2022. URL https://arxiv.org/abs/2208.14271.

Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1edEyBKDS.

David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022. URL https: / /arxiv. org/abs/2207. 10342 .

Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive prompting for decomposing complex questions. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1251-1265, 2022. URL https://aclanthology .org/ 2022.emnlp-main. 81/.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023. URL https://arxiv.org/abs/2305.14387.

Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, KamilÃ¨ LukoÅ¡iÅ«tÄ, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023. URL https://arxiv.org/abs/2302.07459.

Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14953-14962, 2023. URL https://openaccess.thecvf.com/ content/CVPR2023/html/Gupta_Visual_Programming_Compositional_ Visual_Reasoning_Without_Training_CVPR_2023_paper.html.

Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. Are large language model-based evaluators the solution to scaling up multilingual evaluation? arXiv preprint arXiv:2309.07462, 2023. URL https://arxiv.org/abs/2309.07462.

Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023. URL https://arxiv.org/abs/2310.01798.

Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022. URL https://proceedings.mlr. press/v162/huang22a.html.

Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019. URL https://arxiv.org/abs/1909.05858.

Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Text modular networks: Learning to decompose tasks in the language of existing models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1264-1279, 2021. URL https: //aclanthology.org/2021.naacl-main.99/.

Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representations, 2022. URL https: / /openreview.net/forum?id=_nGgzQjzaRy.

Andreas KÃ¶pf, Yannic Kilcher, Dimitri von RÃ¼tte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, RichÃ¡rd Nagyfi, et al. OpenAssistant conversations-democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023. URL https://arxiv.org/abs/2304.07327.

Bin Lei, Chunhua Liao, Caiwen Ding, et al. Boosting logical reasoning in large language models through a new framework: The graph of thought. arXiv preprint arXiv:2308.08614, 2023. URL https://arxiv.org/abs/2308.08614.

Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023. URL https://arxiv.org/abs/2308.06259.

Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328-4343, 2022. URL https: //proceedings.neurips.cc/paper_files/paper/2022/hash/ 1be5bc25d50895ee656b8c2d9eb89d6a-Abstract-Conference.html.

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. URL https://arxiv.org/abs/2211. 09110 .

Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 18231840, 2020. URL https://aclanthology.org/2020.findings-emnlp.165/.

Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023. URL https://arxiv.org/abs/2303.16634.

Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Neurologic decoding:(un) supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4288-4299, 2021. URL https://aclanthology.org/2021.naacl-main.339/.

Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, et al. Neurologic a* esque decoding: Constrained text generation with lookahead heuristics. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 780-799, 2022. URL https://aclanthology.org/2022.naacl-main. 57/.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. URL https://arxiv.org/ $\mathrm{abs} / 2303.17651$.

Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and Yu Wang. Skeleton-of-thought: Large language models can do parallel decoding. arXiv preprint arXiv:2307.15337, 2023. URL https://arxiv.org/abs/2307.15337.

OpenAI. Evals is a framework for evaluating llms and llm systems, and an open-source registry of benchmarks, 2023a. URL https: / github.com/openai/evals.

OpenAI. Gpt-4 technical report, 2023b. URL https://arxiv.org/abs / 2303.08774.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. URL https: //insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf.

Swarnadeep Saha, Shiyue Zhang, Peter Hase, and Mohit Bansal. Summarization programs: Interpretable abstractive summarization with neural modular trees. In The Eleventh International Conference on Learning Representations, 2022. URL https: / / openreview . net/forum? id $=00 \times D O e 7 Z t B e$.

Swarnadeep Saha, Xinyan Yu, Mohit Bansal, Ramakanth Pasunuru, and Asli Celikyilmaz. MURMUR: Modular multi-step reasoning for semi-structured data-to-text generation. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 11069-11090, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.704. URL https: / aclanthology.org/2023.findings-acl. 704.

Imanol Schlag, Sainbayar Sukhbaatar, Asli Celikyilmaz, Wen-tau Yih, Jason Weston, JÃ¼rgen Schmidhuber, and Xian Li. Large language model programs. arXiv preprint arXiv:2305.05364, 2023. URL https://arxiv.org/abs/2305.05364.

Eric Smith, Orion Hsu, Rebecca Qian, Stephen Roller, Y-Lan Boureau, and Jason Weston. Human evaluation of conversations is an open problem: comparing the sensitivity of various methods for evaluating dialogue agents. In Proceedings of the 4th Workshop on NLP for Conversational AI, pp. 77-97, 2022. URL https://aclanthology.org/2022.nlp4convai-1.8/.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https: //arxiv.org/abs/2307.09288.

Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2609-2634, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.147. URL https : / /aclanthology.org/2023.acl-long. 147.

Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023b. URLhttps://arxiv.org/abs/2305.17926.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022. URL https: / / openreview.net/forum?id=1PL1NIMMrw.

Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023c. URL https://arxiv.org/abs/2306.04751.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.

Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for large language models. arXiv preprint arXiv:2307.03025, 2023. URL https://arxiv.org/abs/2307. 03025 .

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2022. URL https: / / openreview. net/forum? id=WE_vluYUL-X.

Shunyu Yao, Howard Chen, Austin W Hanjie, Runzhe Yang, and Karthik Narasimhan. COLLIE: Systematic construction of constrained text generation tasks. arXiv preprint arXiv:2307.08689, 2023a. URL https://arxiv.org/abs/2307.08689.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023b. URL https://arxiv.org/abs/2305.10601.

Yao Yao, Zuchao Li, and Hai Zhao. Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. arXiv preprint arXiv:2305.16582, 2023c. URL https://arxiv. org/abs/2305.16582.

Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper $11 \mathrm{~m}$ networks are fairer $11 \mathrm{~m}$ evaluators. arXiv preprint arXiv:2308.01862, 2023. URL https://arxiv.org/abs/2308.01862.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. URL https://arxiv.org/abs/ 2306.05685 .

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. URL https://arxiv.org/abs/2304 . 06364 .

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. URL https://arxiv.org/abs/2305.11206.

Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=WZH7099tgfM.
