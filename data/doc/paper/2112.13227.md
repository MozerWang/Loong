# Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression 

Mu Li, Kede Ma, Member, IEEE, Jinxing Li, and David Zhang, Life Fellow, IEEE


#### Abstract

Although equirectangular projection (ERP) is a convenient form to store omnidirectional images (also known as $360^{\circ}$ images), it is neither equal-area nor conformal, thus not friendly to subsequent visual communication. In the context of image compression, ERP will over-sample and deform things and stuff near the poles, making it difficult for perceptually optimal bit allocation. In conventional $360^{\circ}$ image compression, techniques such as region-wise packing and tiled representation are introduced to alleviate the over-sampling problem, achieving limited success. In this paper, we make one of the first attempts to learn deep neural networks for omnidirectional image compression. We first describe parametric pseudocylindrical representation as a generalization of common pseudocylindrical map projections. A computationally tractable greedy method is presented to determine the (sub)-optimal configuration of the pseudocylindrical representation in terms of a novel proxy objective for rate-distortion performance. We then propose pseudocylindrical convolutions for $360^{\circ}$ image compression. Under reasonable constraints on the parametric representation, the pseudocylindrical convolution can be efficiently implemented by standard convolution with the so-called pseudocylindrical padding. To demonstrate the feasibility of our idea, we implement an end-to-end $360^{\circ}$ image compression system, consisting of the learned pseudocylindrical representation, an analysis transform, a non-uniform quantizer, a synthesis transform, and an entropy model. Experimental results on 19, 790 omnidirectional images show that our method achieves consistently better rate-distortion performance than the competing methods. Moreover, the visual quality by our method is significantly improved for all images at all bitrates.


Index Terms-Omnidirectional image compression, pseudocylindrical representation, pseudocylindrical convolution, map projection

## 1 INTRODUCTION

OMNIDIRECTIONAL images, also referred to as spherical and $360^{\circ}$ images, provide $360^{\circ} \times 180^{\circ}$ panoramas of natural scenes, and enable free view direction exploration. Recent years have witnessed a dramatic increase in the volume of $360^{\circ}$ image data being generated. On the one hand, average users have easy access to $360^{\circ}$ imaging and display devices, and are getting used to play with this format of virtual reality content on a daily basis. On the other hand, there is a trend to capture ultra-high-definition panoramas to provide an excellent immersive experience, pushing the spatial resolution to be exceedingly high (e.g., $8 \mathrm{~K}$ ). The increasing need for storing and transmitting the enormous amount of panoramic data calls for novel effective $360^{\circ}$ image compression methods.

Currently, the prevailing scheme for $360^{\circ}$ image compression takes a two-step approach. First, select (or create) a map projection [1] with the optimized hyperparameter setting for the sphere-to-plane mapping. Second, pick (or

- This project is supported by China Postdoctoral Science Foundation (2020TQ0319, 2020M682034), NSFC Foundation (61906162, 62102339), and Shenzhen Science and Technology Program (RCBS20200714114910193).
- Mu Li is with the School of Data Science, The Chinese University of Hong Kong (Shenzhen), Shenzhen, 518172, China, and also with the School of Information Science and Technology, University of Science and Technology of China, Hefei, 230026, China (e-mail: limuhit@gmail.com).
- Kede Ma is with the Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong (e-mail: kede.ma@cityu.edu.hk).
- Jinxing Li is with the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, 518055, China (e-mail: lijinxing158@gmail.com).
- David Zhang is with the School of Data Science, The Chinese University of Hong Kong (Shenzhen), and also with the Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, 518172, China (e-mail: davidzhang@cuhk.edu.cn). adapt) a standard image codec that is compatible with central-perspective images for compression. In differential geometry, the Theorema Egregium by Gauss states that all planar projections of a sphere will necessarily be distorted [1]. Among the three major projection desiderata: equalarea, conformal, and equidistant the most widely used equirectangular projection (ERP) does not satisfy the former two, and thus is not friendly to subsequent visual communication applications. Regional resampling [2], [3], [4], [5] and adaptive quantization [6], [7], [8], |9] techniques have been proposed to mitigate the sampling problem of ERP. Compression-friendly projections, such as the tiled representation [5], [10] and hybrid cubemap projection [11], [12], [13] have also been investigated. In $360^{\circ}$ content streaming, viewport-based format is often preferred for coding and transmission \14], [15]. Other projection methods for image display [16], |17], [18] and visual recognition (e.g., icosahedron [19| and tangent images [20]) also emerge in the field of computer vision. With many possible projections at hand, it remains unclear which one is the best choice for learned $360^{\circ}$ image compression in terms of rate-distortion performance, computation and implementation complexity, and compatibility with standard deep learning-based analysis/synthesis transform, and entropy model.

Deep neural networks (DNNs) have been proved effective in many low-level vision tasks, including centralperspective image compression [21], [22], [23], [24], [25], [26], [27]. Following a transform coding scheme, the raw RGB image is first transformed to a latent code representa-

1. Equal-area, conformal, and equidistant map projections preserve relative scales of things and stuff, local angles, and great-circle distances between points, respectively, on the sphere.

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-02.jpg?height=691&width=1770&top_left_y=145&top_left_x=164)

Fig. 1. Illustration of the non-uniform sampling problem of ERP. We first project the same image patch to different latitudes of the ERP images (padded with zeros), and compress them by the HEVC intra coding with the identical hyperparameter. The performance is given in the format of bytes / peak signal-to-noise ratio (PSNR in dB).

tion, quantized to the discrete code, and final transformed back to the RGB domain, all with DNNs that can be end-toend optimized with respect to rate-distortion performance. Recently, a growing research trend is to enable DNNs for $360^{\circ}$ computer vision, which is broadly sorted into three categories depending on how they address the sphere-toplane distortion: spatially adaptive convolution [28], [29], knowledge distillation [30], [31], and reparameterization [32], [33], [34], [35]. Nevertheless, it is highly nontrivial to directly adapt these techniques to learned $360^{\circ}$ image compression. This is because these methods typically require to modify convolution filters, and cannot benefit from years of sophisticated code optimization of standard convolution. As a result, compressing a high-resolution omnidirectional image would be painfully slow. Moreover, non-uniform sampling (especially over-sampling at high latitudes) is a more urgent issue in $360^{\circ}$ image compression than geometric deformation (see Fig. 1), because the latter can be handled by adopting a perceptual image quality metric as the learning objective [36]. From this perspective, reparameterization methods that directly work with spherical signals and are independent of projection methods seem to be more appropriate for rate reduction. But reparameterization comes with its own problem apart from computational complexity: the spherical representation is orderless, which may hinder the context-based entropy modeling for accurate rate estimation [24], [27].

In this paper, we take initial steps towards learned omnidirectional image compression based on DNNs. Our main contributions are three-fold.

- We describe parametric pseudocylindrical representation as a generalization of common pseudocylindrical map projections and the tiled representation by Yu et al. [5]. We propose a computationally tractable greedy algorithm to determine the (sub)-optimal parameter configuration in terms of the rate-distortion performance, estimated by a novel proxy objective. Interestingly, we find that the optimized representa- tion does not correspond to pseudocylindrical projections with the equal-area property (e.g., sinusoidal projection). Empirically, the rate-distortion performance will benefit from slight over-sampling at midlatitudes.
- We propose pseudocylindrical convolutions that work seamlessly with the parametric pseudocylindrical representation for $360^{\circ}$ image compression. Under reasonable constraints on the representation (i.e., the tiled representation), the pseudocylindrical convolution can be efficiently implemented by standard convolution with pseudocylindrical padding. In particular, given the current tile, we pad the latitudinal side with adjacent tiles resized to the same width, and pad the longitudinal side circularly to respect the spherical structure. The manipulation on feature representation instead of convolution leads to a significant advantage of our approach: we are able to transfer the large zoo of DNN-based compression methods for central-perspective images to omnidirectional images.
- We build an end-to-end $360^{\circ}$ image compression system, which is composed of the optimized pseudocylindrical representation, an analysis transform, a non-uniform quantizer, a synthesis transform, and a context-based entropy model. Extensive experiments show that our method outperforms compression standards and DNN-based methods for centralperspective images with region-wise packing (RWP). More importantly, the visual quality of the compressed images is much better for all images at all bitrates.


## 2 RELATED WORK

In this section, we provide a short overview of learned image compression methods for planar images and standards (and tricks) for compressing omnidirectional images.

Relevant techniques for $360^{\circ}$ computer vision will also be briefly summarized.

### 2.1 Learned Planar Image Compression

Learned image compression learns to trade off the rate and distortion, in which DNNs are commonly used to build the analysis transform (i.e., encoder) and the synthesis transform (i.e., decoder), and to model the rate of the codes.

For rate estimation, the discrete entropy serves as a general choice, which requires keeping track of the joint probability of the discrete codes that varies with changes in the network parameters. Side information in the form of hyper-prior [23] and code context [24], [27] can be introduced to boost the accuracy of entropy modeling. Ballé et al. |21| adopted a parametric piece-wise probability distribution function for codes of the same channel; they [23] later assumed a univariate Gaussian distribution for codes of the same spatial location, whose mean and variance are estimated using a hyper-prior. Minnen et al. [24] modeled the code distribution with a mixture of Gaussians (MoG). A $5 \times 5$ code context was adopted as an auto-regressive prior to better predict the MoG parameters. Similarly, a MoG distribution was used in [22]. Li et al. [27] proposed a context-based DNN for efficient entropy modeling.

Besides precise estimation of the code rate using the discrete entropy, various upper-bounds (e.g., the number and dimension of codes) have been derived. Toderici et al. [37|, [38] proposed a progressive compression scheme, in which the rate was controlled by the number of iterations. Johnston et al. $|39|$ took a step further, and presented a content-adaptive bit allocation strategy. Ripple et al. [40] implemented pyramid networks for the encoder and the decoder, with an adaptive code length regularization for rate control. In a similar spirit, Li et al. [41] described a spatially adaptive bit allocation scheme, where the rate was estimated as the total number of codes allocated to different regions. They [42] further designed better relaxation strategies for learning optimal bit allocation.

For distortion quantification, conventional metrics, including mean squared error (MSE), peak signal-to-noise ratio (PSNR), structural similarity (SSIM) [43], and multiscale SSIM (MS-SSIM) [44], were employed to evaluate the "perceptual" distance between the original and compressed images in learned image compression. Other metrics were also incorporated for special considerations. For instance, to boost the visual quality of low-bitrate images, the adversarial loss [45] was introduced in [26], [40]. Torfason et al. [25] suggested to utilize task-dependent losses (e.g., the classification and segmentation error) for task-driven image compression.

## $2.2360^{\circ}$ Image Compression

Most $360^{\circ}$ image compression methods were designed on top of widely used compression standards such as HEVC [3], [46], [47] for central-perspective content. Thus sphereto-plane projections are inevitable for compatibility purposes [48]. One popular branch of work is to introduce practical tricks to tackle the non-uniform sampling problem, such as regional resampling and adaptive quantization. Budagavi et al. 49| adopted Gaussian blurring to smooth high-latitude regions of the ERP image to ease subsequent compression. Regional down-sampling [2], [3], [4], [5] partitions the ERP image into several regions according to the latitude, and resamples and assembles them into a new image of reduced size for compression. Of particular interest, RWP, which repacks the ERP image by reducing the sizes of polar regions, is adopted in HEVC when dealing with $360^{\circ}$ content [3]. Close to our work, Yu et al. [5] introduced the tile representation for $360^{\circ}$ images, and suggested to optimize the height and width of each tile for the sampling rate and bitrate. Adaptive quantization |6], [7], [8], |9], [50], on the other hand, adjusts quantization parameters (QPs) for different regions in ERP with respect to spherical "perceptual" metrics such as S-PSNR [51] and WS-PSNR [52].

Apart from ERP, cubemap projection is also commonly seen in $360^{\circ}$ compression. Su et al. [53] learned to rotate the $360^{\circ}$ images to boost the coding performance. Other variants of cubemap formats, such as hybrid equi-angular cubemap projection (HEC) [11], hybrid cubemap projection (HСР) [12], and hybrid angular cubemap projection (HAP) [13|, were investigated for uniform and content adaptive sampling. In addition, viewport-based [14], [46], [54], [55] and saliency-aware methods [56], 577], [58] were proposed to spend most of the bits on coding the viewports of interest, when streaming $360^{\circ}$ content.

Taking inspirations from Yu et al. |5|, we propose parametric pseudocylindrical representation for learned $360^{\circ}$ image compression. The optimal parameter configuration is determined by a greedy algorithm optimized for a proxy rate-distortion objective. With reasonable constraints, the parametric representation supports an efficient implementation of the proposed pseudocylindrical convolutions.

## $2.3360^{\circ}$ Computer Vision

Recently, there has been a surge of interest to develop DNNs for $360^{\circ}$ computer vision with four main types of techniques. The first is spatially adaptive convolution. Designed around ERP, the most straightforward implementation is to expand the receptive field horizontally via rectangular filters or dilated convolutions [28]. A more advanced version is to design distortion-aware and deformable convolution kernels |29], in combination with spiral spherical sampling [59]. This type of approach is less scalable to deeper networks, which is necessary to achieve satisfactory rate-distortion performance in learned image compression. The second is knowledge distillation, with the goal of training DNNs on ERP images to predict the responses of a target model on viewport images. As one of the first attempts to learn "spherical convolution" for $360^{\circ}$ vision, Su and Grauman |30| tied the kernel weights along each row of the ERP image to accommodate the over-sampling issue. Due to the incorporation of secondary DNNs, this type of approach is often computationally expensive, whose performance may also be limited as the sphere-to-plane distortion is not explicitly modeled. The third is a family of reparameterization methods that are rooted in spherical harmonics and spectral analysis. These often define spherical convolution mathematically to seek rotational equivariance and invariance for dense and global prediction problems. Cohen et al. [32] defined spherical correlation with an efficient implementation
based on the generalized fast Fourier transform $\sqrt{60 \mid}$. Concurrently, Esteves et al. [33] defined spherical convolution [61] as a specific case of group convolution [62], which admits a spectral domain implementation. To avoid the computational cost of the spherical Fourier transform, Perraudin

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-04.jpg?height=43&width=892&top_left_y=347&top_left_x=150)
pixelization (HEALPix) to formulate graph convolutions for cosmological applications. However, it is only practical to apply the method to part of the sphere. Jiang et al. [35] reparameterized convolution filters as linear combinations of first-order and second-order differential operators with learnable weights. Despite being mathematically appealing, rotational equivariance and invariance are less relevant to $360^{\circ}$ image compression, and may cause inconvenience in context-based entropy modeling because of the orderless nature of the spherical representation.

The above-mentioned methods typically require modifying or re-designing the convolution operation, which are generally computationally expensive. Moreover, they may not enable the desired transferability of existing DNNs for central-perspective images, which enjoy years of research into optimal architecture design and efficient convolution implementation. In contrast, the proposed pseudocylindrical convolution resolves the over-sampling problem and can be efficiently implemented by standard convolution with pseudocylindrical padding.

## 3 PROPOSEd METHOD

In this section, we first introduce parametric pseudocylindrical representation, and describe a greedy algorithm to determine the parameter configuration for a proxy ratedistortion objective. We then propose pseudocylindrical convolutions as the main building block for our learned $360^{\circ}$ image compression system.

### 3.1 Parametric Pseudocylindrical Representation

From the practical standpoint, we start with a $360^{\circ}$ image $\boldsymbol{x} \in \mathbb{R}^{H \times W}$ stored in ERP format, where $H$ and $W$ are the maximum numbers of samples in each column and row, respectively. The plane-to-sphere coordinate conversion can be calculated by

$$
\begin{align*}
\theta_{i} & =\left(0.5-\frac{i+0.5}{H}\right) \times \pi, \quad i=\{0, \ldots, H-1\}  \tag{1}\\
\phi_{j} & =\left(\frac{j+0.5}{W}-0.5\right) \times 2 \pi, \quad j=\{0, \ldots, W-1\} \tag{2}
\end{align*}
$$

where $\theta$ and $\phi$ index the latitude and the longitude, respectively. Bilinear interpolation is used as the optional resampling filter if necessary. As a generalization of ERP, the proposed representation is also defined over a $2 \mathrm{D}$ grid $\Omega=\{0, \ldots, H-1\} \times\{0, \ldots, W-1\}$, and is parameterized by $\left\{W_{i}\right\}_{i=0}^{H-1}$, where $W_{i} \in\{1, \ldots, W\}$ is the width of the $i$ th row (with the starting point fixed to zero). By varying $W_{i}$, our representation offers a precise control over the sampling density of each row. For visualization purposes, we may reparameterize the representation by $\left\{W_{i}, S_{i}\right\}_{i=0}^{H-1}$, where $S_{i}$ denotes the starting point:

$$
\begin{equation*}
S_{i}=\left\lfloor\left(W-W_{i}\right) / 2\right\rfloor \tag{3}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-04.jpg?height=253&width=440&top_left_y=150&top_left_x=1081)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-04.jpg?height=192&width=364&top_left_y=457&top_left_x=1138)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-04.jpg?height=250&width=446&top_left_y=146&top_left_x=1509)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-04.jpg?height=196&width=377&top_left_y=455&top_left_x=1573)

(d)
Fig. 2. Comparison of different omnidirectional image representations. (a) ERP. (b) Sinusoidal projection. (c) Tiled sinusoidal representation. (d) Optimized pseudocylindrical representation.

We refer to this data structure as parametric pseudocylindrical representation, since it generalizes several pseudocylindrical map projections. Specifically:

- Choosing $W_{i}=W$ and Eqs. (1) and (2) as the planeto-sphere mapping yields the standard ERP;
- Choosing $W_{i}=\cos \left(\theta_{i}\right) W$ and replacing Eq. (2) to

$$
\begin{equation*}
\phi_{j}=\left(\frac{j-S_{i}+0.5}{W_{i}}-0.5\right) \times 2 \pi \tag{4}
\end{equation*}
$$

for $j=\left\{S_{i}, \ldots, S_{i}+W_{i}-1\right\}$ as the longitude mapping yields sinusoidal projection (see Fig. 2 (b));

- Choosing $W_{i}=\left(2 \cos \left(2 \theta_{i} / 3\right)-1\right) W$, where

$$
\begin{equation*}
\theta_{i}=3 \arcsin \left(0.5-\frac{i+0.5}{H}\right) \tag{5}
\end{equation*}
$$

is the latitude mapping and Eq. 4) is the longitude mapping yields the Craster parabolic projection. This is used in the objective quality metric - CPP-PSNR 633.

- Another special case of interest arises when setting adjacent rows to the same width, leading to the tiled representation proposed by Yu et al. [5], which plays a crucial role in accelerating pseudocylindrical convolutions, as will be immediately clear.

With different combinations of the width configuration and the plane-to-sphere mapping, our pseudocylindrical representation not only includes a broad class of pseudocylindrical map projections as special cases but also opens the door of novel data structures that may be more suitable for $360^{\circ}$ image compression. Without loss of generality, in the remainder of the paper, we use Eqs. (1) and (4) for the plane-tosphere coordinate conversion, and assume $S_{i}=0$.

### 3.2 Pseudocylindrical Convolutions

Based on the pseudocylindrical representation, we define the pseudocylindrical convolution operation ${ }^{2}$ by first specifying a neighboring grid:

$$
\begin{equation*}
\mathcal{N}=\{(i, j) \mid i, j \in\{-K, \ldots, K\}\} \tag{6}
\end{equation*}
$$

2. In fact, nearly all DNNs implement cross-correlation instead of convolution. Here we assume the convolution filter has already been reflected around the center.

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-05.jpg?height=526&width=751&top_left_y=146&top_left_x=275)

Fig. 3. Illustration of the neighborhood in the (a) planar and (b) spherical representation.

where $2 K+1$ is the spread of the convolution kernel. For a central-perspective image $\boldsymbol{x}$, it is straightforward to define the neighbors using the Manhattan distance (see Fig. 3). The response $\boldsymbol{y}$ of the convolution on $\boldsymbol{x}$ at the position $(p, q)$ is computed by

$$
\begin{equation*}
\boldsymbol{y}(p, q)=\sum_{(i, j) \in \mathcal{N}} \boldsymbol{w}(i, j) \boldsymbol{x}\left(p_{i}, q_{j}\right) \tag{7}
\end{equation*}
$$

where $\boldsymbol{w}$ denotes the convolution filter, $p_{i}=p+i$, and $q_{j}=q+j$. To convolve the filter over the pseudocylindrical representation, we also need to define the neighbors, with the goal of approaching the uniform sampling density over the sphere. Relying on a variant of Manhattan distance, we start from $\left(\theta_{p}, \phi_{q}\right)$ corresponding to $(p, q) \in \Omega$, and retrieve the neighbor at $(i, j) \in \mathcal{N}$ by first moving $i \delta_{\theta}$ along the longitude and then $j \delta_{\phi}$ along the latitud ${ }^{3}$, where $\delta_{\theta}=\pi / H$ and $\delta_{\phi}=2 \pi \cos \left(\theta_{p}\right) / W_{p}$. Positive (negative) values of $i$ head towards the north (south) pole. Similarly, positive (negative) values of $j$ mean anticlockwise (clockwise) movement from a bird's-eye view. We obtain the neighbors on the pseudocylindrical representation via the sphere-to-plane projection:

$$
\begin{align*}
p_{i} & =p+i  \tag{8}\\
q_{j} & =\frac{W_{p_{i}}}{W_{p}}(q+0.5)-0.5+\frac{\cos \theta_{p}}{\cos \theta_{p_{i}}} \frac{W_{p_{i}}}{W_{p}} j \\
& =\frac{W_{p_{i}}}{W_{p}}\left(q+\frac{\cos \theta_{p}}{\cos \theta_{p_{i}}} j+0.5\right)-0.5 \tag{9}
\end{align*}
$$

We assume circular boundary condition, and give a careful treatment of the boundary handling near the two poles (i.e., $p_{i}<0$ and $\left.p_{i}>H\right)$ :

$$
\begin{align*}
& p_{i}=\left(-1-p_{i}\right) \bmod H \\
& q_{j}=\left(q_{j}+0.5 W_{p_{i}}\right) \bmod W_{p_{i}} \tag{10}
\end{align*}
$$

Fig. 3 (b) shows an example of the adjustment of $q_{j}$ over the sphere when $p_{i}<0$ (i.e., crossing the north pole). For fractional $q_{j}$, we compute $\boldsymbol{x}\left(p_{i}, q_{j}\right)$ via linear interpolation:

$$
\begin{equation*}
\boldsymbol{x}\left(p_{i}, q_{j}\right)=\sum_{k \in \mathcal{N}} \boldsymbol{b}\left(q_{j}, k\right) \boldsymbol{x}\left(p_{i},\left\lfloor q_{j}\right\rfloor+k\right) \tag{11}
\end{equation*}
$$

3. We assume a unit sphere. where $\boldsymbol{b}$ is the linear kernel. Last, the pseudocylindrical convolution is computed by plugging Eqs. 88, (9), and 11 into Eq. 77.

We take a close look at the computational complexity of the pseudocylindrical convolution, which mainly comes from three parts: neighbor search, linear interpolation, and inner product. Searching one neighbor requires calling the transcendental function, $\cos (\cdot)$, twice with four multiplications and three additions. For linear interpolation, one modulo operation and one addition are needed to create the bilinear kernel, and two multiplications and one addition are used to compute interpolated value. For a kernel size of $(2 K+1) \times(2 K+1)$, we need $28 K^{2}+14 K$ and $20 K^{2}+10 K$ operations for neighbor search and linear interpolation, respectively, but only $8 K^{2}+8 K+1$ operations for inner product.

To reduce the computational complexity of neighbor search, we make a mild approximation to Eq. 9):

$$
\begin{equation*}
q_{j} \approx \frac{W_{p_{i}}}{W_{p}}(q+j+0.5)-0.5 \tag{12}
\end{equation*}
$$

where we assume $\cos \left(\theta_{p}\right) \approx \cos \left(\theta_{p_{i}}\right)$. This is reasonably true because $\theta_{p}$ and $\theta_{p_{i}}$ correspond to adjacent rows, and are very close provided that $H$ is large. From Eq. (12, it is clear $q_{j}=(q+k)_{j-k}$, meaning that adjacent samples in a row are neighbors of one another with no computation. Moreover, searching for neighbors in an adjacent row amounts to scaling it to the width of the current row ${ }^{4}$. Furthermore, we perform circular padding for $q_{j}<0$ and $q_{j} \geq W_{p}$ with $q_{j}=q_{j} \bmod W_{p}$. We refer to this process as $p$ seudocylindrical padding (see Fig. 4 (b) and (c)). For a row with width $W_{p}$, we greatly reduce the computation ${ }^{5}$ from $\left(48 K^{2}+24 K\right) W_{p}$ to $20 K W_{p}+(2 K+1) 2 K$, where $W_{p} \gg K$.

We may further simplify Eq. 12 to 7 by enforcing $W_{p}=W_{p_{i}}$. By doing so, the $p$-th and $(p+i)$-th rows become neighborhood of each other with no computation. The neighboring rows with the same width can be viewed as a tile, and the pseudocylindrical representation reduces gracefully to the tiled representation [5] (see Fig. 22 (d)). Pseudocylindrical padding occurs only at the boundaries of each tile. In summary, the tiled representation $\boldsymbol{z}$ of $\boldsymbol{x}$ is composed of $\left\{\boldsymbol{z}_{t}\right\}_{t=0}^{T-1}$, where $\boldsymbol{z}_{t} \in \mathbb{R}^{H_{t} \times W_{t}}$ is the $t$-th tile. The set of free parameters are $\left\{T,\left\{H_{t}\right\}_{t=0}^{T-1},\left\{W_{t}\right\}_{t=0}^{T-1}\right\}$. With these two steps of simplifications, the pseudocylindrical convolution can be implemented in parallel on $\left\{\boldsymbol{z}_{t}\right\}$ by standard convolution with pseudocylindrical padding. On one hand, this offers the opportunity to build DNN-based compression methods for omnidirectional images upon those for central-perspective images with minimal modifications. On the other hand, this enables fast implement of the proposed pseudocylindrical convolution. For a tile $\boldsymbol{z}_{t}$, the computation operations used for pseudocylindrical padding are $20 K W_{t}+\left(2 K+H_{t}\right) 2 K$, which are much smaller than the operations for convolution, i.e., $\left(8 K^{2}+8 K+1\right) W_{t} H_{t}$, when $H_{t} \gg 1$. In such case, our pseudocylindrical convolution should achieve nearly the same running speed as the standard convolution with zero padding.

4. More precisely, we first shift the adjacent row by half pixel, and scale it to the width of the current row, and shift it back by half pixel.
5. $(2 K+1) 2 K$ arises from the circular boundary handling along the longitude.

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-06.jpg?height=566&width=1778&top_left_y=145&top_left_x=171)

Shestars,

(b) (c)

Fig. 4. Illustration of the pseudocylindrical representation and pseudocylindrical padding. (a) Intermediate pseudocylindrical representations in the DNN. (b) Pseudocylindrical padding. (c) Pseudocylindrical padding for the pole tile.

### 3.3 Pseudocylindrical Representation Optimization

In general, different $360^{\circ}$ images may have different parameter configurations of the pseudocylindrical representation for optimal compression performance, which depend on the image content. The corresponding combinatorial optimization problem can be formulated as

$$
\begin{align*}
& \min _{T,\left\{H_{t}\right\},\left\{W_{t}\right\}} \operatorname{RD}\left(\boldsymbol{x}, \operatorname{compress}_{\boldsymbol{\alpha}}(\boldsymbol{x}) ; T,\left\{H_{t}\right\},\left\{W_{t}\right\}\right) \\
& \text { s.t. } T \in\{0, \ldots, H-1\} \\
& H_{t} \in\{0, \ldots, H-1\}, t \in\{0, \ldots, T-1\} \\
& \sum_{t} H_{t}=H-1 \\
& W_{t} \in\{0, \ldots, W-1\}, t \in\{0, \ldots, T-1\} \tag{13}
\end{align*}
$$

where $\boldsymbol{x}$ is the given $360^{\circ}$ image, $\operatorname{RD}(\cdot)$ is a quantitative measure for rate-distortion performance, and compress $\left.\boldsymbol{\alpha}^{(} \cdot\right)$ is a generic compression method with a learnable parameter vector $\boldsymbol{\alpha}$. As noted by Yu et al. [5], Problem (13) is essentially a multiple-dimensional, multiplechoice knapsack problem, which prohibits exhaustive search when $H$ and $W$ are large. We choose to simplify the problem in the follow ways.

1) Treat $T$ and $\left\{H_{t}\right\}_{t=1}^{T-1}$ as hyperparameters, and preset them, where $H_{t}=H_{0}$ and $T=H / H_{0}$. The general guideline is to set $H_{0}$ large enough to enjoy the fast computation of standard convolution, while making Eq. 12, approximately hold.
2) Quantize the width to $L$ levels, where $L \ll W$. Thus enumeration of the possible widths is performed in a much reduced search space.
3) Enforce the symmetry of the pseudocylindrical representation with respect to the equator, which further halves the search space.
4) Discourage oversampling at higher latitudes by adding the constraint $W_{t} \leq W_{t^{\prime}}$ for $t \leq t^{\prime}$ and $t, t^{\prime} \in\left\{0, \ldots, T_{\text {half }}\right\}$, where $T_{\text {half }}=\lfloor(T-1) / 2\rfloor-1$ (see the coordinate system in Fig. 2).
5) Solve the problem at the dataset level instead of the image level for two reasons. First, even with the above simplifications, it still takes quite some time to obtain the sub-optimal configuration in practice.

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-06.jpg?height=149&width=881&top_left_y=874&top_left_x=1077)

Fig. 5. Estimation of the bits used to code a single tile.

Second, the content-dependent configuration may render the training of DNN-based $360^{\circ}$ image compression unstable.

Putting together, Problem 13) is simplified to

$$
\begin{align*}
\min _{\left\{W_{t}\right\}} & \frac{1}{|\mathcal{D}|} \sum_{\boldsymbol{x} \in \mathcal{D}} \operatorname{RD}\left(\boldsymbol{x}, \operatorname{compress}_{\boldsymbol{\alpha}}(\boldsymbol{x}) ;\left\{W_{t}\right\}\right) \\
\text { s.t. } & W_{t} \in\left\{\bar{W}_{0}, \ldots, \bar{W}_{L-1}\right\}, t \in\{0, \ldots, T-1\}  \tag{14}\\
& W_{t}=W_{T-1-t}, t \in\left\{0, \ldots, T_{\text {half }}\right\} \\
& W_{t} \leq W_{t^{\prime}}, \text { for } t \leq t^{\prime} \text { and } t, t^{\prime} \in\left\{0, \ldots, T_{\text {half }}\right\}
\end{align*}
$$

where $\bar{W}_{t}=(t+1)\left\lfloor\frac{W}{L}\right\rfloor$. Now, an exhaustive search which evaluates all possible configurations may be feasible. Alternatively, we propose a divide-and-conquer greedy solver to Problem (14) in case $L$ and $T$ are still large. We first initialize all $\left\{W_{t}\right\}$ to $\bar{W}_{T-1}$. From the pole to the equator, we enumerate the possible widths of the current tile while holding higher-latitude tiles to the estimated widths and lower-latitude tiles to the initialized widths. This procedure is repeated until all tiles are visited, which is summarized in Algorithm 1

It remains to instantiate the objective function in Problem 14, which quantifies the rate-distortion trade-off given a particular parameter configuration $\left\{W_{t}\right\}$. To obtain an accurate estimate, it is preferable but impractical to optimize a set of DNN-based image compression models (i.e., optimize $\boldsymbol{\alpha}$ in compress $\boldsymbol{\alpha}^{(}(\cdot)$ ) for each configuration. A workaround is to apply existing codecs on each tile and "sum" the results. In our implementation, we use JPEG2000 as the offthe-shelf compression method. As context is crucial in image compression for bitrate reduction, a naїve compression of the tile without considering adjacent ones would lead to an inaccurate parameter estimation of our representation that admits pseudocylindrical padding. To alleviate this issue, we introduce a proxy rate-distortion objective. For the rate

```
Algorithm 1 Greedy Estimation of the Pseudocylindrical
Representation Parameters
    Input: ERP image set $\mathcal{D}=\left\{\boldsymbol{x}^{(0)}, \ldots, \boldsymbol{x}^{(|\mathcal{D}|-1)}\right\}$, and the
quantized width set $\left\{\bar{W}_{0}, \ldots, \bar{W}_{L-1}\right\}$.
    Output: The optimized parameter set $\left\{W_{t}^{\star}\right\}$.
    for $t \leftarrow 0$ to $T-1$ do
        $W_{t}^{\star} \leftarrow \bar{W}_{L-1} ; \quad \triangleright$ Initialization
    end for
    for $t \leftarrow 0$ to $T_{\text {half }}$ do
        $V_{\text {best }} \leftarrow \infty, \quad W_{\text {best }} \leftarrow 0$;
        if $t=0$ then
            start_width $\leftarrow \bar{W}_{0}$;
        else
            start_width $\leftarrow W_{t-1}^{\star} ;$
        end if
        for $W_{t}^{\star} \leftarrow$ start_width to $\bar{W}_{L-1}$ do
            $W_{T-t-1}^{\star} \leftarrow W_{t}^{\star}$;
            $V_{\text {temp }} \leftarrow \mathbb{E}_{\boldsymbol{x} \in \mathcal{D}} \operatorname{RD}\left(\boldsymbol{x}, \operatorname{compress}_{\boldsymbol{\alpha}}(\boldsymbol{x}) ;\left\{W_{t}^{\star}\right\}\right)$;
            if $V_{\text {temp }}<V_{\text {best }}$ then
                $V_{\text {best }} \leftarrow V_{\text {temp }}, \quad W_{\text {best }} \leftarrow W_{t} ;$
            end if
        end for
        $W_{t}^{\star} \leftarrow W_{\text {best }}, \quad W_{T-t-1}^{\star} \leftarrow W_{\text {best }} ;$
    end for
```

estimation of the $t$-th tile, $\boldsymbol{z}_{t}$, we resize the width of $\boldsymbol{x}$ to $W_{t}$, and crop two subimages such that the intersection is $\boldsymbol{z}_{t}$ and the union is the resized image. The rate of $\boldsymbol{z}_{t}$ is calculated as the difference between the number of bits of the subimages and the resized image. This process is better illustrated in Fig. 5 .

For the distortion, we suggest to use viewport-based objective quality metrics, which correctly reflect how humans view $360^{\circ}$ images. According to [36], viewport-based metrics deliver so far the best quality prediction performance on $360^{\circ}$ images. In our implementation, we use MSE as the base quality metric. We sample 14 viewports by first mapping the pseudocylindrical representation back to the unit sphere followed by rectilinear projections ${ }^{6}$ Each viewport is a $H_{v} \times W_{v}$ rectangle, where $H_{v}=\left\lceil\frac{H}{3}\right\rceil$ and $W_{v}=\left\lceil\frac{W}{4}\right\rceil$, with a field of view (FoV) of $\frac{\pi}{3} \times \frac{\pi}{2}$. Together, they cover all spherical content.

By varying the QP values of JPEG2000, we produce a rate-distortion curve for each image $\boldsymbol{x} \in \mathcal{D}$, and we average all curves ${ }^{7}$ as the rate-distortion performance of the current parameter configuration of the proposed pseudocylindrical representation. We compare two curves using the Bjontegaard delta bitrate saving (BD-BR) metric [64] to identify a better configuration.

We conclude this section by illustrating two interesting properties of our pseudocylindrical representation and convolution. First, Fig. 2 shows the optimized pseudocylindrical configuration in comparison to sinusoidal projection and its tiled version, where we set $H=512, W=1024$,

6. The centers of the 14 viewports correspond to $\left(0,-\frac{\pi}{2}\right),(0,0)$, $\left(0, \frac{\pi}{2}\right),(0, \pi),\left(-\frac{\pi}{4},-\frac{\pi}{2}\right),\left(-\frac{\pi}{4}, 0\right),\left(-\frac{\pi}{4}, \frac{\pi}{2}\right),\left(-\frac{\pi}{4}, \pi\right),\left(\frac{\pi}{4},-\frac{\pi}{2}\right),\left(\frac{\pi}{4}, 0\right)$, $\left(\frac{\pi}{4}, \frac{\pi}{2}\right),\left(\frac{\pi}{4}, \pi\right),\left(\frac{\pi}{2}, 0\right)$ and $\left(-\frac{\pi}{2}, 0\right)$, respectively, on the unit sphere.
7. We average the rates and the distortion scores separately over images compressed with identical QP values.

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-07.jpg?height=455&width=702&top_left_y=149&top_left_x=1167)

Fig. 6. The optimized width configuration of the pseudocylindrical representation in comparison to ERP and the sinusoidal projection.

$H_{0}=32$, and $L=64$. The key observation is that the optimized configuration does not completely resolve the over-sampling problem in ERP (see also Fig. 6. Surprisingly, to achieve better compression performance, the mid-latitude regions are still over-sampled than the sinusoidal tiles, which is also supported by the ablation experiments in Sec. 5.5 Second, we build a six-layer DNN with $3 \times 3$ pseudocylindrical convolutions, and visualize the receptive field at different latitudes in Fig. 7. With the optimized structure as input, the proposed pseudocylindrical convolution is able to produce similar receptive fields for different locations on the sphere, which are latitude-adaptive in the ERP domain. Although the receptive field is slightly deformed, it should not be a problem in $360^{\circ}$ image compression because the kernel weights can be learned to adapt to such geometric distortions (if necessary) by optimizing viewport-based perceptual quality metrics.

## 4 LEARNED $360^{\circ}$ IMAGE COMPRESSION SYSTEM

In this section, we design a learned $360^{\circ}$ image compression system based on the proposed pseudocylindrical representation and convolution. At a high level, our system consists of an analysis network $g_{a}$, a non-uniform quantizer $g_{q}$, a synthesis network $g_{s}$, and a context-based entropy network $g_{e}$, which are jointly optimized for rate-distortion performance.

### 4.1 Analysis, Synthesis, and Entropy Networks

The analysis transform $g_{a}$ takes the ERP image $\boldsymbol{x}$ as input and maps it to the proposed pseudocylindrical representation $\boldsymbol{z}$, based on which the code representation $\boldsymbol{c}$ is produced by the network. $g_{a}$ is made of ERP to pseudocylindrical representation transform, four down-sampling blocks, four residual blocks, two attention blocks, a back-end pseudocylindrical convolution, and a sigmoid layer, whose computational graph with detailed specifications is shown in Fig 8 . The down-sampling block processes and down-samples the pseudocylindrical feature maps by a factor of two. The residual block [65| has two convolution layers with a skip connection, following each down-sampling block. A simplified attention block $\mid 66$ is added right after the second and fourth residual block to increase the model capacity and expand the receptive field. A final convolution layer with $C$ filters followed by a sigmoid activation is used to produce

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-08.jpg?height=564&width=1783&top_left_y=141&top_left_x=171)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-08.jpg?height=231&width=445&top_left_y=172&top_left_x=184)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-08.jpg?height=219&width=445&top_left_y=427&top_left_x=184)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-08.jpg?height=501&width=699&top_left_y=156&top_left_x=713)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-08.jpg?height=526&width=458&top_left_y=146&top_left_x=1492)

(c)

Fig. 7. Illustration of the receptive field of the standard and pseudocylindrical convolution. (a) ERP domain. (b) Spherical domain. (c) Viewport domain.

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-08.jpg?height=554&width=1684&top_left_y=834&top_left_x=226)

Fig. 8. Analysis transform $g_{a}$. P-Conv: proposed pseudocylindrical convolution with filter support $(S \times S)$ and number of channels (output $\times$ input).

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-08.jpg?height=445&width=881&top_left_y=1515&top_left_x=167)

Up-sampling Block

Fig. 9. Synthesis transform $g_{s}$. P-Conv: proposed pseudocylindrical convolution with filter support $(S \times S)$ and number of channels (output $\times$ input).

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-08.jpg?height=326&width=832&top_left_y=2168&top_left_x=186)

Fig. 10. Entropy network $g_{e}$. Masked P-Conv: masked pseudocylindrical convolution adapted from [27. the code representation $\boldsymbol{c}$ with a desired operating range of $[0,1]$.

The synthesis transform $g_{s}$ (see Fig. 9) is a mirror of the analysis transform where the down-sampling blocks are replaced by the up-sampling blocks. Instead of performing deconvolution for upsampling, we expand the feature representation by a factor of four in the channel dimension and reshape it such that the height and width grow by a factor of two [38], [67]. $g_{s}$ ends with a pseudocylindrical representation to ERP transform to reconstruct the ERP from the proposed pseudocylindrical representation. Generalized divisive normalization (GDN) and inverse GDN as bio-inspired nonlinearities [21] are separately adopted after the last convolution of the down-sampling and upsampling blocks. For other convolution layers, unless stated otherwise, the parametric rectified linear unit (ReLU) is used as the nonlinear activation function.

As for the entropy network $g_{e}$ (see Fig. 10), we model the probability distribution of the quantized code $\overline{\boldsymbol{c}}$ as a MoG, whose accuracy can be improved by considering the code context, also known as the auto-regressive prior. Thus we employ the group contex $8^{8}$ proposed by Li et al. [27], and train three context-based DNNs to predict the mean,

8. When performing pseudocylindrical convolution, the specified code order following [27] should be respected, which requires careful treatment of linear interpolation and pseudocylindrical padding.
variance, and mixing weights, respectively. Each DNN comprises a front-end up-sampling layer and two masked pseudocylindrical convolutions [27], followed by three masked residual blocks and a back-end masked pseudocylindrical convolution. No activation function is added as the output layer in the mean branch; ReLU activation is added in the variance branch to ensure that the output is nonnegative; the softmax activation is added in the mixing weight branch to produce a probability vector as output.

### 4.2 Quantizer

The quantizer $g_{q}$ is parameterized by $\boldsymbol{\omega}=$ $\left\{\omega_{k, 0}, \ldots, \omega_{k, L_{q}-1}\right\}$, where $k=0, \ldots, C-1$ is the channel index, and $L_{q}$ is the number of quantization centers in each channel. We compute the $l$-th quantization center for the $k$-th channel by

$$
\begin{equation*}
\Omega_{k, l}=\sum_{l^{\prime}=0}^{l} \exp \left(\omega_{k, l^{\prime}}\right) \tag{15}
\end{equation*}
$$

The code $c_{k, i, j}$, where $i, j$ are spatial indices, is quantized to its nearest quantization center:

$$
\begin{equation*}
\bar{c}_{k, i, j}=g_{q}\left(c_{k, i, j}\right)=\underset{\left\{\Omega_{k, l}\right\}}{\operatorname{argmin}}\left\|c_{k, i, j}-\Omega_{k, l}\right\|_{2}^{2} \tag{16}
\end{equation*}
$$

$g_{q}$ has zero gradients almost everywhere, which hinders training via back-propagation. Inspired by [22], [68], we approximate $g_{q}$ by the identify function $\hat{g}_{q}\left(c_{k, i, j}\right)=c_{k, i, j}$. That is, we use $g_{d}$ and $\hat{g}_{d}$ in the forward and backward passes, respectively. The parameters $\omega$ can be optimized by minimizing the quantization error, i.e., $\|\boldsymbol{c}-\overline{\boldsymbol{c}}\|_{2}^{2}$, on the fly.

### 4.3 Rate-Distortion Objective

As with general data compression, our objective function is a weighted sum of the rate and distortion:

$$
\begin{equation*}
\ell=\mathbb{E}_{\boldsymbol{x} \in \mathcal{D}}\left[\ell_{r}\left(g_{q}\left(g_{a}(\boldsymbol{x})\right)\right)+\lambda \ell_{d}\left(\boldsymbol{x}, g_{s}\left(g_{q}\left(g_{a}(\boldsymbol{x})\right)\right)\right)\right] \tag{17}
\end{equation*}
$$

where $\lambda$ is the trade-off parameter, and $\mathcal{D}$ is the training set. The rate of the quantized code $\overline{\boldsymbol{c}}$ is computed by

$$
\begin{equation*}
\ell_{r}(\overline{\boldsymbol{c}})=-\mathbb{E}_{\overline{\boldsymbol{c}}}[\log p(\overline{\boldsymbol{c}})]=-\mathbb{E}\left[\sum_{k, i, j} \log p\left(\bar{c}_{k, i, j}\right)\right] \tag{18}
\end{equation*}
$$

where we omit the conditional dependency to make the notation uncluttered. The discretized probability, $p\left(\bar{c}_{k, i, j}\right)$, can be computed by integrating the continuous MoG with three components:

$p\left(\bar{c}_{k, i, j}\right)=\int_{\frac{\Omega_{k, l-1}+\Omega_{k, l}}{2}}^{\frac{\Omega_{k, l+1}+\Omega_{k, l}}{2}} \sum_{m=0}^{2} \pi_{k, i, j}^{m} \mathcal{N}\left(\xi ; \mu_{k, i, j}^{m},\left(\sigma_{k, i, j}^{m}\right)^{2}\right) d \xi$,

where we assume $\Omega_{k, l}=\bar{c}_{k, i, j}$, and $\pi_{k, i, j}^{m}, \mu_{k, i, j}^{m}$, and $\left(\sigma_{k, i, j}^{m}\right)^{2}$ are the mixing weight, mean and variance of $m$ th Gaussian component for $\bar{c}_{k, i, j}$, respectively.

As previously discussed in Sec. 3.3, we adopt viewportbased MSE and structure similarity (SSIM) as the quality measures, which are denoted by VMSE and VSSIM. It is

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-09.jpg?height=217&width=892&top_left_y=146&top_left_x=1077)

Fig. 11. Region-wise packing (RWP) for the ERP image.

noteworthy that VSSIM is a quality metric, and we need to convert it into a loss function:

$$
\begin{equation*}
\ell_{\operatorname{VSSIM}}(\boldsymbol{x}, \hat{\boldsymbol{x}})=1-\operatorname{VSSIM}(\boldsymbol{x}, \hat{\boldsymbol{x}}) \tag{20}
\end{equation*}
$$

where $\hat{\boldsymbol{x}}$ is the reconstructed image of $\boldsymbol{x}$ by our model.

## 5 EXPERIMENTS

In this section, we first describe the experimental setup, and then augment existing codecs by RWP with the height parameter optimized for rate-distortion performance (see Fig. 11. We compare our method to the augmented codecs in terms of quantitative metrics and visual quality. Last, we conduct comprehensive ablation studies to single out the contributions of the proposed techniques. The codes and the trained models will be available at: https:// github.com/ limuhit/pseudocylindrical_convolution

### 5.1 Experimental Setup

We collect 19, 790 ERP images from Flickr that carry Creative Commons licenses and save them losslessly. All images are down-sampled to the size of $512 \times 1024$ to further counteract potential compression artifacts. We split the dataset into the training set with 19,590 images and the test set with 200 images.

In main experiments, we set the height of each tile to $H_{0}=32$ such that it can be down-sampled by 16 times in the analysis transform. Correspondingly, the number of tiles in our pseudocylindrical representation is 16 . The quantization level $L$ is set to 64 in Problem 133). The tradeoff parameter $\lambda$ in Eq. 17) is in the range of $[1 / 16,1 / 3]$. For the number of channels of the latent code, we choose $C=192,112,56$ for high-bitrate, mid-bitrate, and lowbitrate models, respectively. The number of quantization levels adopted by $g_{q}$ is $L_{q}=8$.

Stochastic optimization is carried out by minimizing Eq. (17) using the Adam method |69| with a learning rate of $10^{-4}$. We decay the learning rate by a factor of 10 whenever the training plateaus. We leverage the favorable transferability of the proposed method, and pre-train it using a large set of central-perspective images with standard convolution (i.e., no pseudocylindrical padding). We then fine-tune the entire method with pseudocylindrical convolutions on the full-size $360^{\circ}$ images in the training set.

Compression methods are commonly evaluated from two aspects - rate and distortion. In this paper, we adopt the bits per pixel (bpp), calculated as the total number of bits used to code the image divided by the number of pixels, for the rate, and VMSE, viewport-based PSNR (VPSNR) and VSSIM for the distortion. Then, we are able to draw the ratedistortion (RD) curve, and compute BD-BR and Bjontegaard

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-10.jpg?height=789&width=1825&top_left_y=137&top_left_x=150)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-10.jpg?height=705&width=594&top_left_y=154&top_left_x=164)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-10.jpg?height=713&width=610&top_left_y=150&top_left_x=755)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-10.jpg?height=710&width=597&top_left_y=152&top_left_x=1363)

(c)

Fig. 12. Rate-distortion curves of different compression methods. (a) VMSE. (b) VPSNR. (c) VSSIM.

delta distortion (BD-distortion) [64]. BD-BR in the unit of percentage (\%) calculates the average bitrate saving between two rate-distortion curves, while BD-distortion calculates the average distortion improvement between two curves. A negative BD-BR value and a positive BD-distortion value represent that the test method is better than the anchor method.

### 5.2 Optimization of RWP for Existing Codecs

To the best of our knowledge, there are no learned compression methods specifically designed for $360^{\circ}$ images. Thus, we choose to augment five codecs for central-perspective images with the RWP strategy [3] - JPEG [70|, JPEG2000 [71|, BPG |72] (i.e., HEVC intra coding), Minnen18 [24], and Ballé18 |23|. As shown in Fig. 11. RWP partitions the images into three parts with a parameter to control the height of the north (and south) pole region. The assembled image is used as the input for compression. RWP can relieve the nonuniform sampling problem to a certain extent. However, the simple split-and-merge operation will break the image continuity and context, which may have a negative impact on compression performance.

We optimize over a set of the height parameter of RWP, $\{8,16,24,32,40,48,56,64,72,80,100\}$, for each of the three compression standards (JPEG, JPEG2000, and BPG) in terms of BD-VPSNR and BD-BR metrics using the original method without RWP as anchor. We find that RWP indeed improves the compression performance for all three codecs. For example, RWP helps improve $0.59 \mathrm{~dB}$ and saves $10.2 \%$ bits for JPEG. It turns out the best heights for JPEG, JPEG2000, and BPG are 64,40 , and 48 , respectively. As for DNN-based methods, we choose the height to be 48 , which offers satisfactory performance for all three compression standards.

### 5.3 Quantitative Evaluation

We compare our methods optimized for VMSE and VSSIM, with the five chosen codes and their augmented versions
TABLE 1

Performance comparison of different compression methods in terms of BD-VPSNR, BD-VSSIM, and BD-BR

| Method | RATE-VPSNR |  | RATE-VSSIM |  |
| :---: | :---: | :---: | :---: | :---: |
|  | BD-VPSNR <br> (dB) | BD-BR (\%) | BD-VSSIM | BD-BR (\%) |
| JPEG | -3.504 | 140.08 | -0.057 | 79.92 |
| JPEG + RWP | -3.080 | 112.04 | -0.048 | 59.34 |
| JPEG2000 | -1.385 | 43.86 | -0.028 | 33.67 |
| JPEG2000 + RWP | -1.300 | 40.85 | -0.025 | 31.34 |
| BPG | 0.000 | 0.00 | 0.000 | 0.00 |
| $\mathrm{BPG}+\mathrm{RWP}$ | 0.092 | -2.44 | 0.003 | -3.04 |
| Ballé18 | -0.110 | 2.90 | 0.012 | -12.41 |
| Ballé18 + RWP | -0.077 | 2.02 | 0.012 | -13.02 |
| Minnen18 | 0.187 | -4.73 | 0.016 | -16.99 |
| Minnen18 + RWP | 0.183 | -4.62 | 0.016 | -16.68 |
| Ours (VMSE) | 0.547 | -13.97 | 0.025 | -25.86 |
| Ours (VSSIM) | -0.958 | 27.99 | 0.043 | -41.84 |

with optimized RWP. Both the JPEG and JPEG2000 are based on the internal implementations in OpenCV 4.2. As for BPG, we adopt the official codec from https://bellard.org/bpg/ with the 4:2:0 chroma format. For the two DNN-based methods, we test on codes released by the respective authors.

Fig. 12 draws the rate-distortion curves, where we find that our VMSE-optimized method clearly outperforms BPG(-RWP), Minnen18(-RWP) Ballé18(-RWP), and overwhelms JPEG(-RWP) and JPEG2000(-RWP) under VMSE and VPSNR. Similar observations can be drawn for the proposed method under VSSIM. Our VMSE-optimized method ranks second in terms of VSSIM, outperforming the competing methods by a clear margin. It is interesting to note that all DNN-based compression methods optimized for VMSE achieve better VSSIM performance compared with the three compression standards. As SSIM is widely regarded as a more perceptual quality metric, it is expected DNN-based compression methods to deliver better visual quality. Table 1 lists the BD-BR and BD-distortion metrics computed from the rate-distortion curves in Fig. 12, where BPG is the anchor method. We observe that our VMSE-optimized

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-11.jpg?height=2342&width=1789&top_left_y=141&top_left_x=165)

Fig. 13. Visual quality comparison of JPEG-RWP, JPEG2000-RWP, BPG-RWP, Minnen18, and our VSSIM-optimized method using 14 viewports indexed by $(\theta, \phi)$. We quantify the distortion in the form of PSNR (dB) / SSIM under each viewport. The bitrates of the ERP image produced by JPEG-RWP, JPEG2000-RWP, BPG-RWP, Minnen18 and ours are separately $0.165 \mathrm{bpp}, 0.171 \mathrm{bpp}, 0.188 \mathrm{bpp}, 0.169 \mathrm{bpp}$, and $0.161 \mathrm{bpp}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-12.jpg?height=569&width=889&top_left_y=149&top_left_x=152)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-12.jpg?height=512&width=420&top_left_y=153&top_left_x=167)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-12.jpg?height=515&width=429&top_left_y=152&top_left_x=604)

(b)
Fig. 14. Rate-distortion curves of different input representations and convolutions. P-Conv: pseudocylindrical convolution.

method improves VPSNR by $0.547 \mathrm{~dB}$ and saves $13.97 \%$ bitrate on average. Our VSSIM-optimized method increases VSSIM by 0.043 and saves $41.84 \%$ bitrate on average. One caveat should be pointed out: although RWP is beneficial to JPEG, JPEG2000, and BPG in compressing $360^{\circ}$ images, it has no clear contribution to DNN-based image compression methods. For example, RWP boosts Ballé 18 by 0.033 dB, but hurts Minnen18 by $0.004 \mathrm{~dB}$. This result arises because RWP breaks image context and creates image discontinuity, to which the learned compression methods fail to adapt.

### 5.4 Qualitative Evaluation

Fig. 13 visually compares our VSSIM-optimized method against JPEG-RWP, JPEG2000-RWP, BPG-RWP, and Minnen18 at similar bitrates, from which we have several interesting observations. First, JPEG-RWP, JPEG2000-RWP, and BPG-RWP suffer from common visual artifacts such as blocking, ringing, and blurring, which are further geometrically distorted at high latitudes. Second, $360^{\circ}$ specific distortions such as the "radiation artifacts" in Viewports $\left(\frac{\pi}{2}, 0\right)$ and $\left(-\frac{\pi}{2}, 0\right)$ begin to emerge for all but the proposed method due to the over-sampling issue at poles even with RWP. This provides strong justifications of the proposed pseudocylindrical representation. Meanwhile, we also see different levels of color cast produced by all competing methods in Viewport $\left(\frac{\pi}{4}, \pi\right)$, centered at the right boundary of the ERP image (see Fig. 2). That is, the viewport is made up of pixels that are far apart in the ERP image, which may be compressed in substantially different ways. In contrast, our method with pseudocylindrical convolutions does not suffer from this problem. Third, compared to the three compression standards, Minnen18 appears to have better visual quality with less visible distortions, especially in flat regions. Overall, the proposed method offers the best quality, generating flat regions with little artifacts (see Viewport $\left(\frac{\pi}{2}, 0\right)$ ), reconstructing structures in a sharp way (see Viewport $\left(-\frac{\pi}{4}, 0\right)$ ), and producing plausible textures that are close to the original (see Viewport $\left(-\frac{\pi}{4}, \frac{\pi}{2}\right)$ ).

### 5.5 Ablation Studies

We conduct three sets of ablation experiments to single out two core contributions of the proposed method - the
TABLE 2

Performance comparison of different input representations and convolutions in terms of BD-VPSNR, BD-VSSIM, and BD-BR. The BPG is the anchor method. P-Conv: pseudocylindrical convolution

| Method | RATE-VPSNR |  | RATE-VSSIM |  |
| :--- | ---: | ---: | ---: | ---: |
|  | BD-VPSNR <br> (dB) | BD-BR <br> $(\%)$ | BD-VSSIM | BD-BR <br> $(\%)$ |
| ERP (P-Conv) | -0.161 | 4.30 | 0.013 | -14.22 |
| Sinusoidal (P-Conv) | 0.145 | -4.19 | 0.018 | -18.34 |
| Optimized (Conv) | 0.116 | -3.09 | 0.017 | -18.21 |
| Optimized (P-Conv) | $\mathbf{0 . 5 4 7}$ | $\mathbf{- 1 3 . 9 7}$ | $\mathbf{0 . 0 2 5}$ | $\mathbf{- 2 5 . 8 6}$ |

TABLE 3

The running speed in seconds of the pseudocylindrical convolution and the standard convolution on the optimized pseudocylindrical representation

| Convolution | Analysis <br> Network $\left(g_{a}\right)$ | Synthesis <br> Network $\left(g_{s}\right)$ | Entropy <br> Network $\left(g_{e}\right)$ |
| :---: | :---: | :---: | :---: |
| Conv | 0.104 | 0.105 | 0.040 |
| P-Conv | 0.107 | 0.108 | 0.045 |

pseudocylindrical representation and convolution. First, we compare the optimized pseudocylindrical representation (by solving Problem (14) using Alg. 1 with the ERP format ${ }^{9}$ and the sinusoidal tiles by setting $\bar{W}_{t}=\cos \left(\theta_{t}\right) W$ (see Fig. 2). For each input structure, we retrain a DNN with the same network architecture illustrated in Fig. 8., 9 and 10 using the same training strategy and VMSE as the distortion measure. Next, we fix the optimized pseudocylindrical representation and compare the standard convolution with zero padding to the proposed pseudocylindrical convolution with the same network structure and training strategy.

The rate-distortion curves and the corresponding BD-BR and BD-Distortion metrics are given in Fig. 14 and Tab. 2 . respectively, where BPG is the anchor. We find that both sinusoidal and the optimized pseudocylindrical tiles deliver better compression performance than ERP. Moreover, the optimized representation offers more perceptual gains (and bitrate savings) than the sinusoidal tiles, validating the effectiveness of the proposed greedy search algorithm. Our results convey a somewhat counterintuitive message: slightly oversampling mid-latitude regions is preferred over uniform sampling everywhere for $360^{\circ}$ image compression. Meanwhile, the proposed pseudocylindrical convolution significantly outperforms the standard convolution with zero padding. Last, we compare the computational speed of the proposed pseudocylindrical convolution to the standard convolution, and report the running time of the analysis, synthesis, entropy networks in Tab. 3. As can be seen, the pseudocylindrical convolution has nearly the same running speed as the standard convolution. These demonstrate the promise of pseudocylindrical convolutions in modeling $360^{\circ}$ images.[^0]

## 6 CONCLUSION AND DISCUSSION

In this paper, we have introduced a new data structure for representing $360^{\circ}$ images - pseudocylindrical representation. We also proposed the pseudocylindrical convolution that can be efficiently implemented by standard convolutions with pseudocylindrical padding. Relying on the proposed techniques, we implemented one of the first DNNbased $360^{\circ}$ image compression system that offers favorable perceptual gains at similar bitrates. In the future, we will try to perform joint optimization of the parameters of the front-end pseudocylindrical representation and the backend image compression method at the image level. This may be achievable by training another DNN that takes a $360^{\circ}$ image as the input for parameter estimation of the corresponding representation [53]. We will also explore the possibility of combining the proposed techniques with existing video codecs (e.g., HEVC, VVC, VP9, and AV1) to improve $360^{\circ}$ video compression.

The application scope of the proposed pseudocylindrical representation and convolution is far beyond $360^{\circ}$ image compression. In fact, it may serve as a canonical building block for general $360^{\circ}$ image modeling, and is particular useful for $360^{\circ}$ applications that expect efficiency, scalability, and transferability. For example, in $360^{\circ}$ image editing and enhancement, the pseudocylindrical representation may be optimized to under-sample certain parts of the image to better account for global image context. As another example, our representation with uniform sampling density (i.e., the sinusoidal tiles) may be preferable in $360^{\circ}$ computer vision tasks to localize and track objects moving from lowlatitude to high-latitude places. In either $360^{\circ}$ application, the proposed pseudocylindrical convolution enables reusing existing methods trained on central-perspective images, and requires only a small set of (labeled) $360^{\circ}$ images for efficient adaptation.

## REFERENCES

[1] J. P. Snyder, Map Projections - A Working Manual. US Government Printing Office, 1987.

[2] S.-H. Lee, S.-T. Kim, E. Yip, B.-D. Choi, J. Song, and S.-J. Ko, "Omnidirectional video coding using latitude adaptive downsampling and pixel rearrangement," Electronics Letters, vol. 53, no. 10, pp. 655-657, 2017.

[3] J. Boyce, A. Ramasubramanian, R. Skupin, G. J. Sullivan, A. Tourapis, and Y. Wang, "HEVC additional supplemental enhancement information (draft 4)," Joint Collaborative Team on Video Coding of ITU-T SG, vol. 16, 2017

[4] R. G. Youvalari, A. Aminlou, and M. M. Hannuksela, "Analysis of regional down-sampling methods for coding of omnidirectional video," in Picture Coding Symposium, 2016.

[5] M. Yu, H. Lakshman, and B. Girod, "Content adaptive representations of omnidirectional videos for cinematic virtual reality," in International Workshop on Immersive Media Experiences, 2015, pp. 16.

[6] Y. Li, J. Xu, and Z. Chen, "Spherical domain rate-distortion optimization for 360-degree video coding," in IEEE International Conference on Multimedia and Expo, 2017, pp. 709-714.

[7] Y. Liu, L. Yang, M. Xu, and Z. Wang, "Rate control schemes for panoramic video coding," Journal of Visual Communication and Image Representation, vol. 53, pp. 76-85, 2018.

[8] M. Tang, Y. Zhang, J. Wen, and S. Yang, "Optimized video coding for omnidirectional videos," in IEEE International Conference on Multimedia and Expo, 2017, pp. 799-804.

[9] X. Xiu, Y. He, and Y. Ye, "An adaptive quantization method for 360-degree video coding," in Applications of Digital Image Processing XLI, vol. 10752, 2018.
[10] J. Li, Z. Wen, S. Li, Y. Zhao, B. Guo, and J. Wen, "Novel tile segmentation scheme for omnidirectional video," in IEEE International Conference on Image Processing, 2016, pp. 370-374.

[11] J.-L. Lin, Y.-H. Lee, C.-H. Shih, S.-Y. Lin, H.-C. Lin, S.-K. Chang, P. Wang, L. Liu, and C.-C. Ju, "Efficient projection and coding tools for 360 video," IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 9, no. 1, pp. 84-97, 2019.

[12] Y. He, X. Xiu, P. Hanhart, Y. Ye, F. Duanmu, and Y. Wang, "Contentadaptive 360-degree video coding using hybrid cubemap projection," in Picture Coding Symposium, 2018, pp. 313-317.

[13] P. Hanhart, X. Xiu, Y. He, and Y. Ye, "360 video coding based on projection format adaptation and spherical neighboring relationship," IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 9, no. 1, pp. 71-83, 2018.

[14] C. Ozcinar, A. De Abreu, and A. Smolic, "Viewport-aware adaptive 360 video streaming using tiles for virtual reality," in IEEE International Conference on Image Processing, 2017, pp. 2174-2178.

[15] X. Corbillon, G. Simon, A. Devlic, and J. Chakareski, "Viewportadaptive navigable 360-degree video delivery," in IEEE International Conference on Communications, 2017.

[16] L. Zelnik-Manor, G. Peters, and P. Perona, "Squaring the circle in panoramas," in IEEE International Conference on Computer Vision, vol. 1, 2005.

[17] C.-H. Chang, M.-C. Hu, W.-H. Cheng, and Y.-Y. Chuang, "Rectangling stereographic projection for wide-angle image visualization," in IEEE International Conference on Computer Vision, 2013, pp. 2824-2831

[18] Y. W. Kim, C.-R. Lee, D.-Y. Cho, Y. H. Kwon, H.-J. Choi, and K.J. Yoon, "Automatic content-aware projection for 360 videos," in IEEE International Conference on Computer Vision, 2017, pp. 47534761.

[19] J. A. Kimerling, K. Sahr, D. White, and L. Song, "Comparing geometrical properties of global grids," Cartography and Geographic Information Science, vol. 26, no. 4, pp. 271-288, 1999.

[20] M. Eder, M. Shvets, J. Lim, and J.-M. Frahm, "Tangent images for mitigating spherical distortion," in IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 12 426-12 434.

[21] J. Ballé, V. Laparra, and E. P. Simoncelli, "End-to-end optimized image compression," in International Conference Learning Representations, 2017

[22] L. Theis, W. Shi, A. Cunningham, and F. Huszár, "Lossy image compression with compressive autoencoders," in International Conference Learning Representations, 2017.

[23] J. Ballé, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, "Variational image compression with a scale hyperprior," in International Conference Learning Representations, 2018.

[24] D. Minnen, J. Ballé, and G. D. Toderici, "Joint autoregressive and hierarchical priors for learned image compression," in Neural Information Processing System, 2018.

[25] R. Torfason, F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and L. Van Gool, "Towards image understanding from deep compression without decoding," in International Conference Learning Representations, 2018.

[26] E. Agustsson, M. Tschannen, F. Mentzer, R. Timofte, and L. V. Gool, "Generative adversarial networks for extreme learned image compression," in IEEE International Conference on Computer Vision, 2019, pp. 221-231.

[27] M. Li, K. Ma, J. You, D. Zhang, and W. Zuo, "Efficient and effective context-based convolutional entropy modeling for image compression," IEEE Transactions on Image Processing, vol. 29, pp. $5900-5911,2020$

[28] N. Zioulis, A. Karakottas, D. Zarpalas, and P. Daras, "Omnidepth: Dense depth estimation for indoors spherical panoramas," in European Conference on Computer Vision, 2018, pp. 448-465.

[29] K. Tateno, N. Navab, and F. Tombari, "Distortion-aware convolutional filters for dense prediction in panoramic images," in European Conference on Computer Vision, 2018, pp. 707-722.

[30] Y.-C. Su and K. Grauman, "Learning spherical convolution for fast features from 360 imagery," in Neural Information Processing Systems, vol. 30, 2017, pp. 529-539

[31] Y.-C. Su and K. Grauman, "Kernel transformer networks for compact spherical convolution," in IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 9442-9451.

[32] T. S. Cohen, M. Geiger, J. Köhler, and M. Welling, "Spherical CNNs," International Conference Learning Representations, 2018.

[33] C. Esteves, C. Allen-Blanchette, A. Makadia, and K. Daniilidis, "Learning SO(3) equivariant representations with spherical CNNs," in European Conference on Computer Vision, 2018, pp. 52-68.

[34] N. Perraudin, M. Defferrard, T. Kacprzak, and R. Sgier, "Deepsphere: Efficient spherical convolutional neural network with healpix sampling for cosmological applications," Astronomy and Computing, vol. 27, pp. 130-146, 2019.

[35] C. Jiang, J. Huang, K. Kashinath, P. Marcus, M. Niessner et al., "Spherical cnns on unstructured grids," arXiv preprint arXiv:1901.02039, 2019.

[36] X. Sui, K. Ma, Y. Yao, and Y. Fang, "Perceptual quality assessment of omnidirectional images as moving camera videos," IEEE Transactions on Visualization and Computer Graphics, 2021.

[37] G. Toderici, S. M. O'Malley, S. J. Hwang, D. Vincent, D. Minnen, S. Baluja, M. Covell, and R. Sukthankar, "Variable rate image compression with recurrent neural networks," arXiv:1511.06085, 2015.

[38] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Minnen, J. Shor, and M. Covell, "Full resolution image compression with recurrent neural networks," in IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 5306-5314.

[39] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh, T. Chinen, S. J. Hwang, J. Shor, and G. Toderici, "Improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks," in IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 4385-4393.

[40] O. Rippel and L. Bourdev, "Real-time adaptive image compression," in International Conference on Machine Learning, 2017, pp. 2922-2930.

[41] M. Li, W. Zuo, S. Gu, D. Zhao, and D. Zhang, "Learning convolutional networks for content-weighted image compression," in IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 3214-3223.

[42] M. Li, W. Zuo, S. Gu, J. You, and D. Zhang, "Learning contentweighted deep image compression," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 10, pp. 3446-3461, 2021.

[43] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, "Image quality assessment: From error visibility to structural similarity," IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, 2004.

[44] Z. Wang, E. P. Simoncelli, and A. C. Bovik, "Multiscale structural similarity for image quality assessment," in Asilomar Conference on Signals, Systems, and Computers, 2003, pp. 1398-1402.

[45] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial nets," in Neural Information Processing System, 2014, pp. 2672-2680.

[46] R. Ghaznavi-Youvalari, A. Zare, H. Fang, A. Aminlou, Q. Xie, M. M. Hannuksela, and M. Gabbouj, "Comparison of HEVC coding schemes for tile-based viewport-adaptive streaming of omnidirectional video," in IEEE International Workshop on Multimedia Signal Processing, 2017, pp. 1-6.

[47] G. J. Sullivan, J.-R. Ohm, W.-J. Han, T. Wiegand et al., "Overview of the high efficiency video coding(HEVC) standard," IEEE Transactions on Circuits and Systems for Video Technology, vol. 22, no. 12, pp. 1649-1668, 2012.

[48] M. Xu, C. Li, S. Zhang, and P. Le Callet, "State-of-the-art in 360 video/image processing: Perception, assessment and compression," IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 1, pp. 5-26, 2020.

[49] M. Budagavi, J. Furton, G. Jin, A. Saxena, J. Wilkinson, and A. Dickerson, " 360 degrees video coding using region adaptive smoothing," in IEEE International Conference on Image Processing, 2015, pp. 750-754.

[50] Y. Liu, M. Xu, C. Li, S. Li, and Z. Wang, "A novel rate control scheme for panoramic video coding," in IEEE International Conference on Multimedia and Expo, 2017, pp. 691-696.

[51] M. Yu, H. Lakshman, and B. Girod, "A framework to evaluate omnidirectional video coding schemes," in IEEE International Symposium on Mixed and Augmented Reality, 2015, pp. 31-36.

[52] Y. Sun, A. Lu, and L. Yu, "AHG8: WS-PSNR for 360 video objective quality evaluation," in Joint Video Exploration Team of ITU-T SG16 WP3 and ISO/IEC JTC1/SC29/WG11, JVET-D0040, 4th Meeting, 2016.

[53] Y.-C. Su and K. Grauman, "Learning compressible $360^{\circ}$ video isomers," in IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 7824-7833.
[54] A. Zare, A. Aminlou, and M. M. Hannuksela, "Virtual reality content streaming: Viewport-dependent projection and tile-based techniques," in IEEE International Conference on Image Processing, 2017, pp. 1432-1436.

[55] C. Ozcinar, J. Cabrera, and A. Smolic, "Visual attention-aware omnidirectional video streaming using optimal tiles for virtual reality," IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 9, no. 1, pp. 217-230, 2019.

[56] H. Hadizadeh and I. V. Bajić, "Saliency-aware video compression," IEEE Transactions on Image Processing, vol. 23, no. 1, pp. 19-33, 2013.

[57] G. Luz, J. Ascenso, C. Brites, and F. Pereira, "Saliency-driven omnidirectional imaging adaptive coding: Modeling and assessment," in IEEE International Workshop on Multimedia Signal Processing, 2017, pp. 1-6.

[58] V. Sitzmann, A. Serrano, A. Pavel, M. Agrawala, D. Gutierrez, B. Masia, and G. Wetzstein, "Saliency in VR: How do people explore virtual environments?" IEEE Transactions on Visualization and Computer Graphics, vol. 24, no. 4, pp. 1633-1642, 2018

[59] E. B. Saff and A. B. Kuijlaars, "Distributing many points on a sphere," The Mathematical Intelligencer, vol. 19, no. 1, pp. 5-11, 1997.

[60] P. J. Kostelec and D. N. Rockmore, "FFTs on the rotation group," Journal of Fourier Analysis and Applications, vol. 14, no. 2, pp. 145179,2008

[61] J. R. Driscoll and D. M. Healy, "Computing Fourier transforms and convolutions on the 2-sphere," Advances in Applied Mathematics, vol. 15, no. 2, pp. 202-250, 1994

[62] A. Weinstein, "Groupoids: unifying internal and external symmetry," Notices of the AMS, vol. 43, no. 7, pp. 744-752, 1996.

[63] V. Zakharchenko, E. Alshina, A. Singh, and A. Dsouza, "AHG8: Suggested testing procedure for 360-degree video," in Joint Video Exploration Team of ITU-T SG16 WP3 and ISO/IEC JTC1/SC29/WG11, JVET-D0027, 4th Meeting, 2016.

[64] G. Bjontegaard, "Calculation of average PSNR differences between RD-curves," VCEG-M33, 2001.

[65] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770-778.

[66] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, "Learned image compression with discretized Gaussian mixture likelihoods and attention modules," in IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 7939-7948.

[67] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang, "Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network," in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1874-1883.

[68] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio, "Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1, ," arXiv:1602.02830, 2016.

[69] D. Kingma and J. Ba, "Adam: A method for stochastic optimization," in International Conference Learning Representations, 2015

[70] G. K. Wallace, "The JPEG still picture compression standard," IEEE Transactions on Consumer Electronics, vol. 38, no. 1, pp. 18-34, 1992.

[71] A. Skodras, C. Christopoulos, and T. Ebrahimi, "The JPEG 2000 still image compression standard," IEEE Signal Processing Magazine, vol. 18, no. 5, pp. 36-58, 2001

[72] F. Bellard, "BPG image format," 2019. [Online]. Available: https://bellard.org/bpg/

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-14.jpg?height=330&width=239&top_left_y=2185&top_left_x=1084)

Mu Li received his BCS in Computer Science and Technology in 2015 from Harbin Institute of Technology, and the Ph.D. degree from the Department of Computing, the Hong Kong Polytechnic University, Hong Kong, China, in 2020 $\mathrm{He}$ is the owner of Hong Kong PhD Fellowship. He is currently a postdoctoral researcher at School of Data Science, the Chinese University of Hong Kong, Shenzhen. His research interests include deep learning, image processing, and image compression.

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-15.jpg?height=333&width=271&top_left_y=148&top_left_x=149)

Kede Ma (S'13-M'18) received the B.E. degree from the University of Science and Technology of China, Hefei, China, in 2012, and the M.S. and Ph.D. degrees in electrical and computer engineering from the University of Waterloo, Waterloo, ON, Canada, in 2014 and 2017, respectively. He was a Research Associate with the Howard Hughes Medical Institute and New York University, New York, NY, USA, in 2018. He is currently an Assistant Professor with the Department of Computer Science, City University of Hong Kong. His research interests include perceptual image processing, computational vision, and computational photography.

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-15.jpg?height=336&width=254&top_left_y=886&top_left_x=163)

Jinxing Li received the B.Sc. degree from the department of Automation, Hangzhou Dianzi University, Hangzhou, China, in 2012, the M.Sc. degree from the department of Automation, Chongqing University, Chongqing, China, in 2015, and the PhD degree from the department of Computing, Hong Kong Polytechnic University, Hong Kong, China, in 2019. Dr. Li worked at The Chinese University of Hong Kong, Shenzhen, from 2019 to 2021 . He is currently with Harbin Institute of Technology, Shenzhen, China. His research interests are pattern recognition, deep learning, medical biometrics and machine learning.

![](https://cdn.mathpix.com/cropped/2024_06_04_fea49e45d9256423f656g-15.jpg?height=334&width=271&top_left_y=1622&top_left_x=149)

David Zhang (Life Fellow, IEEE) graduated in Computer Science from Peking University. He received his MSc in 1982 and his PhD in 1985 in both Computer Science from the Harbin Institute of Technology (HIT), respectively. From 1986 to 1988 he was a Postdoctoral Fellow at Tsinghua University and then an Associate Professor at the Academia Sinica, Beijing. In 1994 he received his second $\mathrm{PhD}$ in Electrical and Computer Engineering from the University of Waterloo, Ontario, Canada. He has been a Chair Professor at the Hong Kong Polytechnic University where he is the Founding Director of Biometrics Research Centre (UGC/CRC) supported by the Hong Kong SAR Government since 1998. Currently he is Presidential Chair Professor in Chinese University of Hong Kong (Shenzhen). So far, he has published over 20 monographs, 500+ international journal papers and 40+ patents from USA/Japan/HK/China. He has been continuously 8 years listed as a Global Highly Cited Researchers in Engineering by Clarivate Analytics during 2014-2021. He is also ranked about 85 with $\mathrm{H}$-Index 120 at Top 1,000 Scientists for international Computer Science and Electronics. Professor Zhang is selected as a Fellow of the Royal Society of Canada. He also is a Croucher Senior Research Fellow, Distinguished Speaker of the IEEE Computer Society, and an IEEE Life Fellow and an IAPR Fellow.


[^0]:    9. We may as well consider the ERP tiles by setting $W_{t}=W$ as input to our pseudocylindrical DNN-based compression system, which is equivalent to feeding the entire ERP image to a standard DNN for compression.
