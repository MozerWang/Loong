# Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs 

![](https://cdn.mathpix.com/cropped/2024_06_04_463735d1c791f0e0798fg-01.jpg?height=1136&width=1716&top_left_y=489&top_left_x=169)

Figure 1. Instances are systematically identified where the visual question answering (VQA) capabilities of GPT-4V [41] fall short (Date accessed: Nov 04, 2023). Our research highlights scenarios in which advanced systems like GPT-4V struggle with seemingly simple questions due to inaccurate visual grounding. Text in red signifies an incorrect response, while text in green represents hallucinated explanations for the incorrect answer. All the images referenced are sourced from ImageNet-1K and LAION-Aesthetic datasets.


#### Abstract

Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent MultiModal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify "CLIP-blind pairs" - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art


systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.

## 1. Introduction

Multimodal Large Language Models (MLLMs) [8, 13, 31, 40] have been rapidly developing in recent times. MLLMs integrate images into large language models (LLMs) and leverage the powerful abilities of LLMs [41, 59, 69], showcasing remarkable proficiency in tasks such as image understanding, visual question answering, and instruction following. In particular, the recently released GPT-4V(ision) [40] has pushed performance to an unprecedented level [41, 63].

Beneath the advancements of these models, we find there exists a notable weakness: they still exhibit visual shortcomings, some of which are surprisingly elementary and evident (see Figure 1). We ask: Where do these problems originate? Is it a deficiency in visual modality, language understanding, or their alignment? In this work, we suggest that these shortcomings observed in MLLMs might stem from a problem related to the visual representations.

At their core, most MLLMs [8, 31, 71] are built on pretrained vision [43, 54] and language [59, 68, 69] models. These models are connected using various types of adapters $[2,26,31]$ to integrate the different modalities. A natural hypothesis is that any limitation in the pretrained vision models can cascade into the downstream MLLMs that adopt them. Studies have explored a similar issue for language. For example, Tong et al. [57], Yuksekgonul et al. [65] demonstrate that failure patterns in the pretrained text encoder [43, 44] will lead to downstream failures in textguided generative models [22, 46].

On the vision side, most open-source MLLMs [2, 26, 31] adopt the pretrained Contrastive Language-Image PreTraining (CLIP) model [43] as the visual encoder. We begin by identifying failure examples that CLIP struggles to encode properly (Section 2). Inspired by Tong et al. [57], we exploit the erroneous agreements in the embedding space. If two visually different images are encoded similarly by CLIP, then at least one of the images is likely ambiguously encoded. We call such a pair of images a CLIP-blind pair. To measure the visual similarity between images, we use a vision-only self-supervised encoder such as DINOv2 [42]. In this context, CLIP-blind pairs are images with similar CLIP embeddings but different DINOv2 embeddings.

We discover that these CLIP-blind pairs indeed lead to errors in downstream MLLMs. With these pairs, We introduce the MultiModal Visual Patterns (MMVP) benchmark. This benchmark is specifically designed to inquire about differences in CLIP-blind pairs and evaluate the visual abilities of state-of-the-art MLLMs with straightforward questions. We evaluate a variety of open-source $[8,30,31,71]$ and closed-source models [13, 41] including GPT-4V [40], and conduct a user study to measure human performance. The results show that MLLM models struggle with straightforward visual questions. Most of these models perform below the level of random guessing, with GPT-4V being the exception. Yet, even GPT-4V exhibits a considerable disparity in performance - exceeding $50 \%$ - compared to human performance.

Having identified a large number of individual failure instances in MLLMs, we continue to study the systematic visual patterns in MMVP which CLIP models struggle (Section 3). We summarize nine prevalent patterns of the CLIPblind pairs in MMVP, such as "orientation", "counting", and "viewpoint", which pose significant challenges for the CLIP vision encoder. Notice that there has been significant and ongoing progress in scaling up both training data and model size for CLIP [10, 43, 54, 62, 66]. We categorize examples from MMVP into visual patterns to systematically assess whether scaling alone can mitigate these challenges. Our findings suggest that 7 out of the 9 identified visual patterns cannot be resolved by any large-scale CLIP-based models, indicating that model/data scaling alone is not sufficient. Moreover, we identify a strong correlation between the visual patterns that challenge CLIP models and the performance of MLLMs. If CLIP struggles with a particular visual pattern, such as "orientation", MLLMs will likely also fall short. This shows that the CLIP vision encoders could become a bottleneck in such systems.

Finally, we take a step towards improving the visual grounding of MLLMs. Since the visual shortcomings of MLLMs stem from their reliance on the CLIP model, we investigate the impact of integrating vision-centric representations into MLLMs (Section 4). Specifically, we explore ways to incorporate a vision-only self-supervised model, such as DINOv2 [42], to enhance the visual grounding capabilities of MLLMs. We refer to these techniques as Mixture-of-Features (MoF). First, we linearly mix CLIP and DINOv2 features in different ratios, which we refer to as Additive-MoF (A-MoF). This process reveals that DINOv2 features are more effective in visual grounding, though they come at the cost of diminished instructionfollowing ability. To address this, we introduce InterleavedMoF (I-MoF) that spatially mixes visual tokens from both CLIP and DINOv2 models. We find that this practice significantly enhances visual grounding while maintaining the instruction-following capabilities.

## 2. The Multimodal Visual Patterns (MMVP) Benchmark

Currently, the majority of open-source MLLMs [8, 31, 71] use the off-the-shelf CLIP vision encoders to process images. In this section, we begin by identifying CLIP-blind pairs in the CLIP model (Section 2.1). Subsequently, we construct the Multimodal Visual Patterns-MLLM (MMVPMLLM) benchmark using these CLIP-blind pairs (Section 2.2). We evaluate SOTA MLLMs including GPT-4V on the benchmark (Section 2.3) and find that all the tested models struggle with simple questions on visual details. A

Step 1

Finding CLIP-blind $\$ S$ pairs.

Discover image pairs that are proximate in CLIP feature space but distant in DINOv2 feature space.

![](https://cdn.mathpix.com/cropped/2024_06_04_463735d1c791f0e0798fg-03.jpg?height=420&width=439&top_left_y=451&top_left_x=214)

Step 2

Spotting the difference between two images.

For a CLIP-blind pair, a human annotator attempts to spot the visual differences and formulates questions.
![](https://cdn.mathpix.com/cropped/2024_06_04_463735d1c791f0e0798fg-03.jpg?height=210&width=434&top_left_y=415&top_left_x=844)

"The dog's head in the left image is resting on the carpet, while the dog's head in the right image is lying on the floor." Formulating questions and
options for both images.

$\downarrow$ options for both images. (a) Floor (b) Carpet

## Step 3

Benchmarking multimodal LLMs.

Evaluate multimodal LLMs using a CLIP-blind image pair and its associated question.

Where is the yellow animal's head lying in this image? (a) Floor (b) Carpet

![](https://cdn.mathpix.com/cropped/2024_06_04_463735d1c791f0e0798fg-03.jpg?height=282&width=360&top_left_y=499&top_left_x=1441)

The model receives a score only when both predictions for the CLIP-blind pair are correct.

Figure 2. Constructing MMVP benchmark via CLIP-blind pairs. Left: We start with finding CLIP-blind pairs that have similar CLIP embedding but different DINOv2 embedding. Center: We manually inspect the differences between pair-wise images and formulate questions based on the differences in the images. Right: We ask MLLMs the question alongside the CLIP-blind pair. The model receives a score only when both questions for the CLIP-blind pair are answered correctly.

visualization of this process is provided in Figure 2.

### 2.1. Finding CLIP-blind Pairs

It is challenging to directly find instances (images) that the CLIP vision encoder struggles to encode "properly". To circumvent this issue, we extend the idea proposed in Tong et al. [57] to automatically find blind pairs in vision models. The underlying principle is simple: if two images, despite having stark visual differences, are encoded similarly by the CLIP vision encoder, then one of them is likely encoded ambiguously (See Figure 2 left for example). To measure the visual difference between two images, we examine the images' representations within a reference model: a visiononly self-supervised model trained without any language guidance, e.g., DINOv2 [42]. These models are shown to capture more visual details and information $[42,53]$.

We take the corpus datasets, ImageNet [47] and LAIONAesthetics [48], to collect these CLIP-blind pairs.

For each pair, we compute its CLIP embeddings using CLIP-ViT-L-14 [9, 43] model and their DINOv2 embeddings using DINOv2-ViT-L-14 [9, 42] model. We return pairs such that the cosine similarity exceeds 0.95 for CLIP embeddings and less than 0.6 for DINOv2 embeddings.

### 2.2. Designing Benchmark from CLIP-blind Pairs

We introduce the Multimodal Visual Patterns (MMVP) benchmark, and a Visual Question Answering (VQA) benchmark. Utilizing the collected CLIP-blind pairs, we carefully design 150 pairs with 300 questions. For each CLIP-blind pair of images, we manually pinpoint the visual details that the CLIP vision encoder overlooks (see the middle of Figure 2) and craft questions that probe these visual details, for example "Is the dog facing left or right?" (See the right of Figure 2 and more examples in Figure 3). The primary goal is to determine whether MLLM models would fail when posed with these seemingly basic questions and overlook critical visual details. Hence, the questions are intentionally straightforward and unambiguous.

### 2.3. Benchmark Results

We assess the questions on SOTA open-source models (LLaVA-1.5 [31], InstructBLIP [8], Mini-GPT4 [71]) and closed-source models (GPT-4V [40], Gemini [14], Bard [13]) We leave details of how we access the model in Appendix B.1. In our evaluation, each question is queried independently, eliminating any biases from chat histories. We also evaluate human performance through a user study where users are presented with 300 questions in a randomized sequence. For any given pair of images, we consider a pair of images to be correctly answered if both the questions associated with the pair are answered accurately.

Human study confirms questions are straightforward. As shown in Figure 4, human participants accurately answer an average of $95.7 \%$ of the questions. This high accuracy rate underscores the ease of the questions. More details can be found in Appendix B.4.

Current MLLMs struggle with visual details. As shown in Figure 4, there is a significant performance gap

![](https://cdn.mathpix.com/cropped/2024_06_04_463735d1c791f0e0798fg-04.jpg?height=1282&width=1678&top_left_y=248&top_left_x=191)

Figure 3. Examples of Questions in the MMVP benchmark. Incorrect answers are shaded in red . A model is considered correct only if it answers both questions in a pair correctly. Both leading closed-source models (GPT-4V, Gemini) and open-source models (LLaVA-1.5, InstructBLIP) fail these simple visual questions. (See Appendix B. 2 for all the questions in MMVP benchmark.)

![](https://cdn.mathpix.com/cropped/2024_06_04_463735d1c791f0e0798fg-04.jpg?height=591&width=810&top_left_y=1705&top_left_x=164)

Figure 4. Benchmark results of current SOTA MLLM models and humans. We evaluate benchmark questions for current SOTA MLLM models and human performances through user studies. between human and MLLM models, despite the latter often demonstrating impressive results $[6,27]$. Models except GPT-4V and Gemini, scored below random guess level (25\%). Most advanced GPT-4V and Gemini also face challenges in addressing basic visual grounding questions. Figures 1 and 3 provide examples of errors made by models. The outcomes suggest that irrespective of model size or training data, struggle with visual details.

We have also conducted an ablation study, such as swapping options and changing notations in the question formulation (see Appendix B. 3 for more details), to further confirm that this poor performance stems from visual incapability, not hallucination in the language models.

## 3. Systematic Failures in CLIP

In the previous section, we identify CLIP-blind pairs and use them to find failures in MLLMs. Here, we delve deeper into these pairs to investigate (i) systematic visual patterns

![](https://cdn.mathpix.com/cropped/2024_06_04_463735d1c791f0e0798fg-05.jpg?height=594&width=1716&top_left_y=234&top_left_x=172)

Structural Characteristics

some fruits cut in half

uncut fruits

Figure 5. Examples from MMVP-VLM. MMVP-VLM consists of image pairs across nine visual patterns. The examples in the figure are from EVA01 ViT-g-14 model [54], one of the largest CLIP models that also fails to choose the right image given the text description.

emerged from CLIP-blind pairs (Section 3.1), (ii) whether these visual patterns pose challenges for CLIP-based models with massive scaling up (Section 3.2), and (iii) the correlation between failure patterns in CLIP models and those in MLLMs (Section 3.3).

### 3.1. Visual Patterns in CLIP-blind Pairs

Having identified the CLIP-blind pairs, we summarize systematic visual patterns that the CLIP vision encoders might consistently misinterpret. It is too abstract to directly capture systematic visual patterns in the CLIP-blind pairs. Therefore, we turn to the questions and options from the MMVP benchmark. With these questions, we transform abstract visual patterns in images into clearer, language-based descriptors that are easier to categorize.

In this work, we use GPT-4 [41] to categorize general patterns by prompting it with the following:

## User

I am analyzing an image embedding model. Can you go through the questions and options, trying to figure out some general patterns that the embedding model struggles with? Please focus on the visual features and generalize patterns that are important to vision models [MMVP Questions and Options]

We identify 9 visual patterns:

(. Orientation and Direction

Q Presence of Specific Features

State and Condition

$\uparrow \stackrel{2}{9} \quad$ Quantity and Count

Positional and Relational Context

Color and Appearance

Structural and Physical Characteristics

A Text

o. Viewpoint and Perspective

These visual patterns suggest that CLIP vision encoders overly focus on high-level semantic understanding, overlooking intricate details of the visual world. Full descriptions of the visual patterns can be found in Appendix D.

### 3.2. The MMVP-VLM Benchmark

CLIP-based models have developed rapidly since the introduction in the first paper [43]. We want to test whether these visual patterns still impose challenges to the more recent CLIP models [10, 54, 62, 66], which significantly scale up in terms of training data and model size. In doing so, we introduce a new benchmark: MMVP-VLM to systematically study if CLIP models handle this visual pattern well.

We distill a subset of questions from the MMVP benchmark into simpler language descriptions and categorize them into visual patterns. To maintain a balanced number of questions for each visual pattern, we add a few questions, if needed, to ensure that each visual pattern is represented by 15 text-image pairs. Examples of pairs are shown in Figure 5. A pair is deemed correctly answered if the model can accurately match both image-text combinations.

We evaluate MMVP-VLM on a variety of CLIP models [10, 43, 54, 62, 66]. These models vary in aspects like size, training data, and methodology. As evidenced in Table 1 , increasing network size and training data only aids in identifying two visual patterns - "color and appearance" and "state and condition". The rest of the visual patterns continue to challenge all CLIP-based models. We also find that the ImageNet-1k zero-shot accuracy is not a definitive indicator of a model's performance regarding visual patterns. This underscores the necessity for additional evaluation metrics, such as MMVP-VLM, to accurately assess the model's capabilities in areas beyond image classification.

### 3.3. How CLIP's Errors Affect MLLMs

After analyzing the visual patterns that CLIP models struggle with, we pose the following question: Is there a correla-

|  | Image <br> Size | Params <br> $(\mathrm{M})$ | IN-1k <br> ZeroShot | (2) | $Q$ | ₪ | $\uparrow_{9}^{1}$ | $i$ | $P$ | $\$_{\infty}^{\infty}$ | $\mathbf{A}$ | 0 | MMVP <br> Average |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| OpenAI ViT-L-14 [43] | $\overline{224^{2}}$ | 427.6 | 75.5 | 13. |  | $\overline{20.0} \quad$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_463735d1c791f0e0798fg-06.jpg?height=50&width=71&top_left_y=316&top_left_x=1226) | $\overline{13.3} \quad$ | $\overline{53.3}$ | 20.0 | 7 | $\overline{13.3} \quad$ | $\overline{19.3}$ |
|  | $336^{2}$ |  | 76.6 | 0 . |  |  |  |  | 0.0 |  |  |  |  |
|  | $224^{2}$ | 877.4 | 82.0 | 26. | 20.0 | 3 | 40.0 | 20.0 | 66.7 | 40.0 | 0.0 |  | 37.8 |
| SigL | $384^{2}$ | 878.0 | 83.1 | $n$ | 26.7 | 60.0 | 33.3 | 13.3 | 66.7 | 3 | 6.7 |  | .0 |
|  | $224^{2}$ |  | 4 | 21 | 26 | 73.3 | 26. | 26 | 66.7 |  |  |  | 39.3 |
|  | $378^{2}-x-2$ | 986 | 84.4 |  | 20 |  | 33.3 | 26 | 66.7 | 40 | 20.0 |  | 8 |
| Met: | $224^{2}$ | $427 \quad-\quad x-2$ | 79.2 | 13. $-\quad-\quad-\quad-\quad-1$ | 6.7 | 66.7 | 6.7 | 33.3 | 46.7 | 20.0 | 6.7 | 13.3 | 23.7 |
| M | $224^{2}$ | 98 | 80.6 | 6 . | 13.3 | 60.0 | $13.3-1$ | 6 . | 53.3 | 26 | 3.3 | 33.3 | 25.2 |
|  | $224^{2}$ |  | 78 | 6. |  | 40 | 6 | 13 | 66.7 |  |  |  | 23.0 |
| EVA02 ViT-bigE-14+ [54] | $224^{2}$ | 5044.9 | 82.0 | 13.3 | 20.0 | 66.7 | 26.7 | 26.7 | 66.7 | 26.7 | 20.0 | 33.3 | 33.3 |

Table 1. Performance of various CLIP based models on different visual patterns in MMVP-VLM benchmark. Models scaled up in resolution show minimal improvement, whereas a slight advantage is observed when scaling up the network. For each visual pattern, ImageNet-1k Zero-shot accuracy and MMVP average, we use light gray to highlight the best performance. For most of the visual patterns, all CLIP-based methods show struggle, as evident from the scores. We use symbols for visual patterns due to space limit: (0: Orientation and Direction, Q: Presence of Specific Features, $\boldsymbol{\mathcal { }}:$ State and Condition, $\uparrow^{\frac{1}{9}}$ : Quantity and Count, $\boldsymbol{9}:$ Positional and Relational Context, Color and Appearance, Structural and Physical Characteristics, A: Texts, Viewpoint and Perspective.

![](https://cdn.mathpix.com/cropped/2024_06_04_463735d1c791f0e0798fg-06.jpg?height=390&width=853&top_left_y=1060&top_left_x=167)

Figure 6. CLIP and MLLM's performance on visual patterns. If CLIP performs poorly on a visual pattern such as " orientation", MLLMs also underperform on the visual pattern.

tion between the underperformance of CLIP and MLLMs' visual incapability? To explore this, we categorize questions from MMVP into these visual patterns summarized and calculate each MLLM's performance on these patterns.

In Figure 6, we plot CLIP's performance and MLLMs' performance for each visual pattern. When the CLIP vision encoder underperforms on a certain visual pattern, the MLLM tends to exhibit similar shortcomings. Open-source models such as LLaVA 1.5 [30] and InstructBLIP [8] that explicitly use the CLIP vision encoder display a strong correlation in performance.

Further, we calculate the Pearson Correlation Coefficient between the CLIP model and MLLM's performance on each visual pattern. Results show that LLaVA 1.5 and InstructBLIP all possess a coefficient score greater than 0.7. This high score indicates a strong correlation that weaknesses in visual pattern recognition in the CLIP model are transferred to MLLMs. More details on the Pearson Correlation Coefficient can be found in Appendix C.

## 4. Mixture-of-Features (MoF) for MLLM

Based on our exploration in earlier sections, a natural question arises: If open-sourced MLLM's visual shortcomings come from the CLIP vision encoder, how do we build a more competent visual encoder? In this section, we take initial steps to answer the question by studying Mixtureof-Features (MoF). We start with additive MoF that mixes CLIP features and vision-only SSL model features. Results show that each encoder presents unique advantages and limitations when employed as the pretrained model in MLLM (Section 4.2). We subsequently propose Interleaved MoF that integrates the features from both CLIP and SSL into MLLM to enhance visual grounding without compromising the model's ability to follow instructions (Section 4.3).

### 4.1. Experiment Setting

We adopt LLaVA [30, 31] as the framework to study visual encoders in MLLM. LLaVA uses a pretrained CLIP encoder and trains an adapter to align visual tokens with language tokens in the LLM. (See left side of Figure 7). We use DINOv2 [42] as the vision-only SSL model in our work because it is currently the most scalable vision-only model. Our exploration includes the use of two visual encoders: CLIP-ViT-L-14 [43] and DINOV2-ViT-L-14 [42]. To ensure consistent and fair comparisons, we train and finetune our model with the same experiment setting in LLaVA. We include the additional experimental details in Appendix A.

### 4.2. Additive MoF

We add a pretrained DINOv2 encoder into MLLM and mix the CLIP pretrained encoder with it. We use a coefficient $\alpha$ to control the portion of CLIP features and $1-\alpha$ to control the amount of DINOv2 features and linearly add them

![](https://cdn.mathpix.com/cropped/2024_06_04_463735d1c791f0e0798fg-07.jpg?height=520&width=1694&top_left_y=244&top_left_x=188)

Figure 7. Different Mixture-of-Feature (MoF) Strategies in MLLM. Left: Standard MLLM that uses CLIP as off-the-shelf pretrained vision encoder; Middle: Additive-MoF (A-MoF) MLLM: Linearly mixing CLIP and DINOv2 features before the adapter; Right: InterleavedMoF (I-MoF MLLM) Spatially interleaving CLIP visual tokens and DINOv2 visual tokens after the adapter.

together (See middle part of Figure 7 for visualization).

We evaluate the model's visual grounding ability by the MMVP proposed earlier in Section 2 and the model's instruction-following capability by LLaVA benchmark introduced in Liu et al. [31]. Initially, we conduct five experiments where we linearly transition from using $100 \%$ CLIP features to $100 \%$ DINOv2 features. In these tests, the DINOv2 feature proportions are set at $\{0.00,0.25,0.50,0.75,1.00\}$. To further verify the observed trends, we introduce two additional experiments with DINOv2 proportions of $\{0.625,0.875\}$. Our findings, presented in Table 2, reveal two insights:

1. As the proportion of DINOv2 features increases, MLLM exhibits a decline in its instruction-following capability. Notably, there is a sharp decrease when the DINOv2 proportion reaches $87.5 \%$.
2. A higher proportion of DINOv2 features enhances the model's visual grounding capability, but this advantage diminishes when the DINOv2 proportion surpasses 0.75, at which point instruction-following is notably impaired. Hence, if we were to add DINOv2 features or completely replace CLIP with DINOv2, it would result in a trade-off between visual grounding and instruction-following. A higher proportion of DINOv2 features improves the model's visual perception at the expense of its ability to follow linguistic instructions, while CLIP features enhance language comprehension but reduce visual grounding.

### 4.3. Interleaved MoF

We propose interleaved MoF to leverage advantages from both CLIP and DINOv2 embeddings to enhance image representation. An image concurrently passes into CLIP and DINOv2 encoders, and the resulting embeddings are individually processed by adapters. We take the processed features from CLIP and DINOv2 and interleave them while maintaining their original spatial order. We then feed the interleaved features to LLM (See right part of Figure 7).

| method | SSL ratio | MMVP | LLaVA |
| :---: | :--- | :--- | :--- |
| LLaVA | 0.0 | 5.5 | $\mathbf{8 1 . 8}$ |
|  | 0.25 | $7.9_{(+2.4)}$ | $79.4_{(-2.4)}$ |
|  | 0.5 | $12.0_{(+6.5)}$ | $78.6_{(-3.2)}$ |
| LLaVA | 0.625 | $15.0_{(+9.5)}$ | $76.4_{(-5.4)}$ |
| + A-MoF | 0.75 | $\mathbf{1 8 . 7}(+13.2)$ | $75.8(-6.0)$ |
|  | 0.875 | $16.5_{(+11.0)}$ | $69.3_{(-12.5)}$ |
|  | 1.0 | $13.4_{(+7.9)}$ | $68.5_{(-13.3)}$ |

Table 2. Empirical Results of Additive MoF. We use DINOv2 as the image SSL model in our work. With more DINOv2 features added, there is an improvement in visual grounding, while a decline in instruction following ability.

| method | res | \#tokens | MMVP | LLaVA | POPE |
| :--- | :---: | :---: | :--- | :--- | :--- |
| LLaVA | $224^{2}$ | 256 | 5.5 | 81.8 | 50.0 |
| LLaVA | $336^{2}$ | 576 | 6.0 | 81.4 | 50.1 |
| LLaVA + I-MoF | $224^{2}$ | 512 | $16.7(+10.7)$ | 82.8 | 51.0 |
| LLaVA $^{1.5}$ | $336^{2}$ | 576 | 24.7 | 84.7 | 85.9 |
| LLaVA $^{1.5}+$ I-MoF | $224^{2}$ | 512 | $28.0(+3.3)$ | 82.7 | 86.3 |

Table 3. Empirical Results of Interleaved MoF. Interleaved MoF improves visual grounding while maintaining same level of instruction following ability.

We summarize the results in Table 3. Under the LLaVA setting, interleave MoF significantly enhances visual grounding, with a $10.7 \%$ increase observed in MMVP, without compromising the model's ability to follow instructions. This experiment is replicated with the LLaVA-1.5 setting and under various image resolution settings, yielding similar enhancements in performance. We also evaluate on POPE [27] which is designed to test hallucination in visual grounding. Interleaved-MoF also shows consistent improvement against the original LLaVA models. Merely increasing the image resolution, and consequently, the number of tokens does not boost visual grounding capabilities. Instead, it is the interleaving of MoF between
vision-only SSL models and VLM models that leads to improved performance in visual grounding tasks. We conduct more experiments using MAE or MoCoV3 as vision-only SSL models in I-MoF and show similar improvements in visual grounding tasks in Appenfix E.1. We also evaluated Interleaved MoF on additional benchmarks such as MMBench [32] and GQA [21], finding that Interleaved MoF achieves similar performance on these benchmarks. Please refer to Appendix E. 2 for more results on these benchmarks.

## 5. Related Works

Multimodal LLMs. We study the limitations of Multimodal LLMs [8, 13, 30, 31, 40] and explore possible ways to improve these models. Multimodal LLMs build from pretrained Large Language Models [3, 41, 58, 59, 69] and CLIP vision encoder [43, 54]. These systems then use an adapter, such as MLPs [30, 31], Q-Former [8, 26], and gated attention $[2,25]$, to integrate the pretrained CLIP vision encoder into LLMs. More recently, instructBLIP [8], LLaVA1.5 [30] highlight the importance of high-quality training data. Yet, there is a scarcity of research focusing on the impact of visual encoders, which is an important gap our work aims to address through a systematic study.

Evaluating Multimodal LLMs. MMVP assesses MLLMs using a set of simple yet critical Visual Question Answering (VQA) questions constructed from CLIPblind pairs. Previous benchmarks such as TextVQA [52], VQAv2 [15], and GQA [21] have centered on traditional VQA queries. Recently, there are works like MM-Vet [64], POPE [27], and MM-Bench [32] designed to specifically evaluate multimodal LLMs including hallucination, reasoning, and robustness. The previous benchmarks and evaluations have shown that Multimodal LLMs can suffer from hallucination [28, 29], catastrophic forgetting [67] and lack of robustness [11]. In taking a step back to the fundamentals, our work uncovers that even the most advanced multimodal LLMs, such as GPT-4V [40], Gemini [14], Bard [30], and LLaVA-1.5 [30], are not immune to stumbling over elementary visual questions. We also identified part of the problem as being the incapable visual encoder.

Visual Encoders. MMVP-VLM provides a detailed analysis of the visual capabilities of various CLIP variants [43, $54,62,66]$. These models mostly follow the method proposed in Radford et al. [43] that uses contrastive loss to train on large volumes of image-text pairs. They differ in training data [62], training recipes [54], and objective functions [66]. Nonetheless, our studies show that all of these CLIP variants struggle with simple visual patterns such as "orientation", "count", "presence of specific features", etc. Another line of research focuses on vision-only self-supervised learning (SSL). This category includes contrastive SSL [5, 7, 16, 17] and mask-based SSL [4, 18, 70]. SLIP [39] explores the synergy between CLIP and con- trastive SSL, but focusing primarily on standard classification tasks. In fact, a common practice to evaluate the quality of these vision models is through linear probing or finetuning on ImageNet [45, 47]. Although current evaluation methods provide a basic level of assessment on representation quality, our findings indicate a growing detachment from the needs of recent use cases. As demonstrated in the MoF experiments in Section 4, the CLIP vision model and the vision-only SSL models learn complementary features. However, the linear probing accuracy on ImageNet alone provides a limited understanding of feature utility in MLLMs. This observation suggests the need for more diverse evaluations [61] in visual representation learning, to better align with current and emerging applications.

Ambiguities in Embedding Models. Our work exploits CLIP-blind pairs within the CLIP vision embedding space to generate examples of failures in CLIP models and subsequently MLLMs. This concept has ties to previous research focused on documenting failure modes in text embedding models [12, 36, 55]. More recently, Thrush et al. [56], Yuksekgonul et al. [65] and Hsieh et al. [19] study the binding problems CLIP faces in processing text queries, noting that CLIP models treat text input as a bag of words. Tong et al. [57] examines the implications for downstream text-guided generative models. Tschannen et al. [60] suggests image captioners as promising alternatives to CLIP for improving attribute binding. Our work focuses on the visual patterns.

## 6. Discussion

Circling back to the very first question we ask: is vision good enough for language? Perhaps not yet, as our study shows that vision models might become a bottleneck in multimodal systems. MLLMs fail in simple questions because their pre-trained CLIP vision encoders overlook crucial visual details in images, and systematically fail to sort important visual patterns. Yet, CLIP-type models remain the most scalable and widely used vision models today. Contrary to the popular belief that data and model scaling is a panacea, our research demonstrates that scaling alone does not rectify the inherent deficiencies in CLIP models.

Our study reveals that popular visual representation learning models - vision-and-language models and visiononly self-supervised learning models - excel in different aspects. The distinction in their capabilities go beyond conventional benchmarks such as linear probing or zeroshot accuracy on ImageNet. Although a carefully designed Mixture-of-Features approach could alleviate visual limitations and utilize the strengths of these two learning paradigms, it is necessary to develop new evaluation metrics to facilitate the development of new visual representation learning algorithms. We hope our work can motivate further innovation in vision models.

Acknowledgements. We thank Penghao Wu, Muzi Tao, Erik Jones, Michael Psenka, Daniel Yeh, Druv Pai, Chen Sun for helpful discussions and feedback. This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise. This research is also supported by Intel, Google TRC program, the Google Cloud Research Credits program with the award GCP19980904, and an Amazon Research Award Fall 2023. The authors thank hyperbolic labs for supporting part of the experiments. All experiments and data processing were performed at NYU.

## References
