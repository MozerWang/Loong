# Fairness in Large Language Models: A Taxonomic Survey 

Zhibo Chu<br>zb.chu@mail.ustc.edu.cn<br>University of Science and Technology<br>of China<br>Heifei, Anhui, China

Zichong Wang<br>ziwang@fiu.edu<br>Florida International University<br>Miami, FL, US

Wenbin Zhang<br>wenbin.zhang@fiu.edu<br>Florida International University<br>Miami, FL, US


#### Abstract

Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms for promoting fairness. Furthermore, resources for evaluating bias in LLMs, including toolkits and datasets, are summarized. Finally, existing research challenges and open questions are discussed.


## KEYWORDS

Large Language Models, AI Fairness, Social Bias

## ACM Reference Format:

Zhibo Chu, Zichong Wang, and Wenbin Zhang. 2024. Fairness in Large Language Models: A Taxonomic Survey. In Proceedings of Special Interest Group on Knowledge Discovery and Data Mining (SIGKDD '24). ACM, Barcelona, Spain, 14 pages. https://doi.org//nnnnnnn.nnnnnnn

## 1 INTRODUCTION

Large language models (LLMs) have demonstrated remarkable capabilities in addressing problems across diverse domains, ranging from chatbots [66] to medical diagnoses [191] and financial advisory [160]. Notably, their impact extends beyond fields directly associated with language processing, such as translation [206] and text sentiment analysis [125]. LLMs also prove invaluable in broader applications including legal aid [211], healthcare [165], and drug discovery [148]. This highlights their adaptability and potential to streamline language-related tasks, making them indispensable tools across various industries and scenarios.[^0]

Despite their considerable achievements, LLMs may face fairness concerns stemming from biases inherited from the real-world and even exacerbate them [238]. Consequently, they could lead to discrimination against certain populations, especially in socially sensitive applications, across various dimensions such as race [6], age [51], gender [93], nationality [183], occupation [90], and religion [1]. For instance, an investigation [185] revealed that when tasked with generating a letter of recommendation for individuals named Kelly (a common female name) and Joseph (a common male name), ChatGPT, a prominent instance of LLMs, produced paragraphs describing Kelly and Joseph with random traits. Notably, Kelly was portrayed as warm and amiable (e.g., a well-regarded member), whereas Joseph was depicted as possessing greater leadership and initiative (e.g., a natural leader and role model). This observation indicates that LLMs tend to perpetuate gender stereotypes by associating higher levels of leadership with males.

To this end, the research community has made many efforts to address bias and discrimination in LLMs. Nevertheless, the notions of studied fairness vary across different works, which can be confusing and impede further progress. Moreover, different algorithms are developed to achieve various fairness notions. The lack of a clear framework mapping these fairness notions to their corresponding methodologies complicates the design of algorithms for future fair LLMs. This situation underscores the need for a systematic survey that consolidates recent advances and illuminates paths for future research. In addition, existing surveys on fairness predominantly focus on traditional ML fields such as graph neural networks [36, 48], computer vision [113, 178], natural language processing $[9,25]$, which leaves a noticeable gap in comprehensive reviews specifically dedicated to the fairness of LLMs. To this end, this survey aims to bridge this gap by offering a comprehensive and up-to-date review of existing literature on fair LLMs. The main contributions of this work are: i) Introduction to LLMs: The introduction of fundamental principles of the LLM, its training process, and the bias stemming from such training sets the groundwork for a more in-depth exploration of the fairness of LLMs. ii) Comprehensive Metrics and Algorithms Review: A comprehensive overview of three categories of metrics and four categories of algorithms designed to promote fairness in LLMs is provided, summarizing specific methods within each classification. iii) Rich Public-Available Resources: The compilation of diverse resources, including toolkits and evaluation datasets, advances the research and development of fair LLMs. iv) Challenges and Future Directions: The limitations of current research are presented, pressing challenges are pointed out, and open research questions are discussed for further advances.

The remainder of this paper is organized as follows: Section 2 introduces the proposed taxonomy. Section 3 provides background

![](https://cdn.mathpix.com/cropped/2024_06_04_f727414623c6961f741bg-02.jpg?height=474&width=1679&top_left_y=278&top_left_x=212)

Figure 1: An overview of the proposed fairness in LLMs taxonomy.

information on LLMs to facilitate an understanding of fairness in LLMs. Following that, Section 4 explores current definitions of fairness in ML and the adaptations necessary to address linguistic challenges in defining bias within LLMs. Section 5 introduces quantification of bias in LLMs. Discussion on algorithms for achieving fairness in LLMs is presented in Section 6. Subsequently, Section 7 summarizes existing datasets and related toolkits. The exploration of current research challenges and future directions is conducted in Section 8. Finally, Section 9 concludes this survey.

## 2 AN OVERVIEW OF THE TAXONOMY

As shown in Figure 1, we categorize recent studies on the fairness of LLMs according to three distinct perspectives: i) metrics for quantifying biases in LLMs, ii) algorithms for mitigating biases in LLMs, along with iii) resources for evaluating biases in LLMs. Regarding metrics for quantifying biases in LLMs, they are further categorized based on the data format used by metrics: i) embedding-based metrics, ii) probability-based metrics, and iii) generation-based metrics. Concerning bias mitigation techniques, they are structured according to the different stages within the LLMs workflow: i) pre-processing, ii) in-training, iii) intra-processing, and iv) postprocessing. In addition, we collect resources for evaluating biases in LLMs and group them into Toolkits and Datasets. Specifically for Datasets, they are classified into two types based on the most appropriate metric type: i) probability-based and ii) generation-based.

## 3 BACKGROUND

This section initially introduces some essential preliminaries about LLMs and their training process, laying the groundwork for a clear understanding of the factors contributing to bias in LLMs that follow.

### 3.1 Large Language Models

Language models are computational models with the capacity to comprehend and generate human language [120, 146]. The evolution of language models progresses from statistical language models to neural language models, pre-trained language models, and the current state of LLMs [31]. Initial statistical language models, like N-gram models [84], estimate word likelihood based on the preceding context. However, $\mathrm{N}$-gram models face challenges such as poor generalization ability, lack of long-term dependence, and difficulty capturing complex linguistic phenomena [135]. These limitations constrained the capabilities of language models until the emergence of transformers [182], which largely addressed these issues. Specifically, transformers became the backbone of modern language models [189], attributable to their efficiency-an architecture free of recurrence that computes individual tokens in parallel-and effectiveness-attention facilitates spatial interaction across tokens dynamically dependent on the input itself. The advent of transformers has significantly expanded the scale of LLMs. These models not only demonstrate formidable linguistic capabilities but also rapidly approach human-level proficiency in diverse domains such as mathematics, reasoning, medicine, law, and programming [20]. Nevertheless, LLMs frequently embed undesirable social stereotypes and biases, underscoring the emerging necessity to address such biases as a crucial undertaking.

### 3.2 Training Process of LLMs

Training LLMs require careful planning, execution, and monitoring. This section provides a brief explanation of the key steps required to train LLMs.

Data preparation and preprocessing. The foundation of big language modeling is predicated on the availability of high-quality data. For LLMs, this entails the necessity of a vast corpus of textual data that is not only extensive but also rich in quality and diversity, which requires accurately represent the domain and language style that the model is aiming to grasp. Simultaneously, the datasets need to be large enough to provide sufficient training data for LLMs, and representative enough so that the models can adapt well to new and unseen texts [151]. Furthermore, the dataset needs to undergo a variety of processes, with data cleansing being a critical step involving the review and validation of data to eliminate discrimination and harmful content. For example, popular public sources for finding datasets, such as Kaggle ${ }^{1}$, Google Dataset Search ${ }^{2}$, Hugging Face $^{3}$, Data.gov ${ }^{4}$, and Wikipedia database ${ }^{5}$, could all potentially harbor discriminatory content. This inclusion of biased information can adversely impact decision-making if fairness considerations are disregarded [112]. Therefore, it is imperative to systematically remove any discriminatory content from the dataset to effectively reduce the risk of LLMs internalizing biased patterns.[^1]

Model selection and configuration. Most existing LLMs utilize transformer deep learning architectures, which have emerged as a preferred option for advanced natural language processing (NLP) tasks, such as Metas's LLaMa [180] and DeepAI's GPT-3 [18]. Several key elements of these models, such as the choice of loss function, the number of layers in transformer blocks, the number of attention heads, and various hyperparameters, need to be specified when configuring a transformer neural network. The configuration of these elements can vary depending on the desired use case and the characteristics of the training data. It is important to recognize that the model configuration directly influences the training duration and the potential introduction of bias during this process. One common source of bias amplification during the model training process is the selection of loss objectives mentioned above [77]. Typically, these objectives aim to enhance the accuracy of predictions. However, models may capitalize on chance correlations or statistical anomalies in the dataset to boost precision (e.g., all positive examples in the training data happened to come from male authors so that gender can be used as a discriminative feature) $[72,139]$. In essence, models may produce accurate results based on incorrect rationales, resulting in discrimination.

Instruction Tuning. Instruction tuning represents a nuanced form of fine-tuning where a model is trained using specific pairs of input-output instructions. This method allows the model to learn particular tasks directed by these instructions, significantly enhancing its capacity to interpret and execute a variety of NLP tasks as per the guidelines provided [32]. Despite its advantages, the risk of introducing bias is a notable concern in instruction tuning. Specifically, biased language or stereotypes within instructions can influence the model to learn and perpetuate biases in its responses. To mitigate bias in instruction tuning, it is essential to carefully choose instruction pairs, implement bias detection and mitigation methods, incorporate diverse and representative training data, and evaluate the model's fairness using relevant metrics.

Alignment with human. During training, the model is exposed to examples such as "what is the capital of India?" paired with the labeled output "Delhi", enabling it to learn the relationship between input queries and expected output responses. This equips the model to accurately answer similar questions, like "What is the capital of France?" resulting in the answer "Paris". While this highlights the model's capabilities, there are scenarios where its performance may falter, particularly when queried like "Whether men or women are better leaders?" where the model may generate biased content. This introduces concerns about bias in the model's responses. For this purpose, InstructGPT [131] designs an effective tuning approach that enables LLMs to follow the expected instructions, which utilizes the technique of reinforcement learning with human feedback (RLHF) [30, 131]. RLHF is a ML technique that uses human feedback to optimize LLMs to self-learn more efficiently. Reinforcement learning techniques train model to make decisions that maximize rewards, making their outcomes more accurate. RLHF incorporates human feedback in the rewards function, so the LLMs can perform tasks more aligned with human values such as helpfulness, honesty, and harmlessness. Notably, ChatGPT is developed based on a similar technique as InstructGPT, exhibits a strong ability to generate high-quality, benign responses, including the ability to avoid engaging with offensive queries.

### 3.3 Factors Contributing to Bias in LLMs

Language modeling bias, often defined as "bias that results in harm to various social groups" [70], presents itself in various forms, encompassing the association of specific stereotypes with groups, the devaluation of certain groups, the underrepresentation of particular social groups, and the unequal allocation of resources among groups [44]. Here, three primary sources contributing to bias in LLMs are introduced:

i) Training data bias. The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models. For instance, if the training data includes the statement "all programmers are male and all nurses are female," the model is likely to learn and perpetuate these occupational and gender biases in its outputs, reflecting a narrow and biased view of societal roles [16, 24]. Additionally, a significant disparity in the training data could also lead to biased outcomes [161]. For example, Buolamwini and Gebru [21] highlighted significant disparities in datasets like IJB-A and Adience, where predominantly light-skinned individuals make up $79.6 \%$ and $86.2 \%$ of the data, respectively, thereby biasing analyses toward underrepresented dark-skinned groups [118].

ii) Embedding bias. Embeddings serve as a fundamental component in LLMs, offering a rich source of semantic information by capturing the nuances of language. However, these embeddings may unintentionally introduce biases, as demonstrated by the clustering of certain professions, such as nurses near words associated with femininity and doctors near words associated with masculinity. This phenomenon inadvertently introduces semantic bias into downstream models, impacting their performance and fairness [9, 63]. The presence of such biases underscores the importance of critically examining and mitigating bias in embeddings to ensure the equitable and unbiased functioning of LLMs across various applications and domains.

iii) Label bias. In instruction tuning scenarios, biases can arise from the subjective judgments of human annotators who provide labels or annotations for training data [152]. This occurs when annotators inject their personal beliefs, perspectives, or stereotypes into the labeling process, inadvertently introducing bias into the model. Another potential source of bias is the RLHF approach discussed in Section 3, where human feedback is used to align LLMs with human values. While this method aims to improve model behavior by incorporating human input, it inevitably introduces subjective notions into the feedback provided by human. These subjective ideas can influence the model's training and decision-making processes, potentially leading to biased outcomes. Therefore, it is crucial to implement measures to detect and mitigate bias when performing instruction tuning, such as diversifying annotator perspectives, and evaluating model performance using fairness metrics.

## 4 ML BIAS QUANTIFICATION AND LINGUISTIC ADAPTATIONS IN LLMs

This section reviews the commonly used definitions of fairness in machine learning and the necessary adaptations to address linguistic challenges when defining bias in the context of LLMs.

### 4.1 Group Fairness

Existing fairness definitions $[52,76]$ at the group level aim to emphasize that algorithmic decisions neither favor nor harm certain subgroups defined by the sensitive attribute, which often derives from legal standards or topics of social sensitivity, such as gender, race, religion, age, sexuality, nationality, and health conditions. These attributes delineate a variety of demographic or social groups, with sensitive attribute categorized as either binary (e.g., male, female) or pluralistic (e.g., Jewish, Islamic, Christian). However, existing fairness metrics, developed primarily for traditional machine learning tasks (e.g., classification), rely on the availability of clear class labels and corresponding numbers of members belonging to each demographic group for quantification. For example, when utilizing the German Credit Dataset [7] and considering the relationship between gender and credit within the framework of statistical parity (where the probability of granting a benefit, such as credit card approval, is the same for different demographic groups) [184], machine learning algorithms like decision trees can directly produce a binary credit score for each individual. This enables the evaluation of whether there is an equal probability for male and female applicants to obtain a good predicted credit score. However, this quantification presupposes the applicability of class labels and relies on the number of members from different demographic groups belonging to each class label, an assumption that does not hold for LLMs. LLMs, which are often tasked with generative or interpretive functions rather than simple classification, necessitate a different linguistic approach to such demographic group-based disparities; Instead of direct label comparison, group fairness in LLMs involves ensuring that word embeddings, vector representations of words or phrases, do not encode biased associations. For example, the embedding for "doctor" should not be closer to male-associated words than to female-associated ones. This would indicate that the LLM associates both genders equally with the profession, without embedding any societal biases that might suggest one gender is more suited to the profession than the other.

### 4.2 Individual fairness

Individual fairness represents a nuanced approach focusing on equitable treatment at the individual level, as opposed to the broader strokes of group fairness [52]. Specifically, this concept posits that similar individuals should receive similar outcomes, where similarity is defined based on relevant characteristics for the task at hand Essentially, individual fairness seeks to ensure that the model's decisions, recommendations, or other outputs do not unjustly favor or disadvantage any individual, especially when compared to others who are alike in significant aspects. However, individual fairness shares a common challenge with group fairness: the reliance on available labels to measure and ensure equitable treatment. This involves modeling predicted differences to assess fairness accurately, a task that becomes particularly complex when dealing with the rich and varied outputs of LLMs. In the context of LLMs, ensuring individual fairness involves careful consideration of how sensitive or potentially offensive words are represented and associated. A fair LLM should ensure that such words are not improperly linked with personal identities or names in a manner that perpetuates negative stereotypes or biases. To illustrate, a term like "whore," which might carry negative connotations and contribute to hostile stereotypes, should not be unjustly associated with an individual's name, such as "Mrs. Apple," in the model's outputs. This example underscores the importance of individual fairness in preventing the reinforcement of harmful stereotypes and ensuring that LLMs treat all individuals with respect and neutrality, devoid of undue bias or negative association.

## 5 QUANTIFYING BIAS IN LLMs

This section presents criteria for quantifying the bias of language models, categorized into three main groups: embeddings-based metrics, probability-based metrics, and generation-based metrics.

### 5.1 Embedding-based Metrics

This line of efforts begins with Bolukbasi et al. [16] conducting a seminal study that revealed the racial and gender biases inherent in Word2Vec [119] and Glove [137], two widely-used embedding schemes. However, these two embedding schemes primarily provide static representations for identical words, whereas contextual embeddings offer a more nuanced representation that adapts dynamically according to the context [116]. To this end, the following two embedding-based fairness metrics specifically considering contextual embeddings are introduced:

Word Embedding Association Test (WEAT) [24]. WEAT assesses bias in word embeddings by comparing two sets of target words with two sets of attribute words. The calculation of WEAT can be seen as analogies: $X$ is to $A$ as $Y$ is to $B$, where $X$ and $Y$ represent the target words, and $A$ and $B$ represent the attribute words. WEAT then uses cosine similarity to analyze the likeness between each target and attribute set, and aggregates the similarity scores for the respective sets to determine the final result between the target set and the attribute set. For example, to examine gender bias in weapons and arts, the following sets can be considered: Target words: Interests $X$ : \{pistol, machine, gun, . . \}, Interests $Y$ : \{dance, prose, drama, . . . \}, Attribute words: terms A: \{male, boy, brother, $\ldots\}$, terms $B$ : \{female, girl, sister, . . . \}. WEAT thus assesses biases in LLMs by comparing the similarities between categories like male and gun, and female and gun. Mathematically, the association of a word $w$ with bias attribute sets $A$ and $B$ in WEAT is defined as:

$$
\begin{equation*}
s(\boldsymbol{w}, A, B)=\frac{1}{n} \sum_{\boldsymbol{a} \in A} \cos (\boldsymbol{w}, \boldsymbol{a})-\frac{1}{n} \sum_{\boldsymbol{b} \in B} \cos (\boldsymbol{w}, \boldsymbol{b}) \tag{1}
\end{equation*}
$$

Subsequently, to quantify bias in the sets $X$ and $Y$, the effect size is used as a normalized measure for the association difference between the target sets:

$$
\begin{equation*}
W E A T(X, Y, A, B)=\frac{\operatorname{mean}_{\boldsymbol{x} \in X}(\boldsymbol{x}, A, B)-\operatorname{mean}_{\boldsymbol{y} \in Y s}(\boldsymbol{y}, A, B)}{\operatorname{stddev}_{\boldsymbol{w} \in X \cup Y} s(\boldsymbol{w}, A, B)} \tag{2}
\end{equation*}
$$

where $\operatorname{mean}_{\boldsymbol{x} \in X} s(\boldsymbol{x}, A, B)$ represents the average of $s(x, A, B)$ for $x$ in $X$, while stddev $\boldsymbol{w}_{\boldsymbol{w} \in X \cup Y} s(\boldsymbol{w}, A, B)$ denotes the standard deviation across all word biases of $x$ in $X$.

Sentence Embedding Association Test (SEAT) [116]. Contrasting with WEAT, SEAT compares sets of sentences rather than sets of words by employing WEAT on the vector representation of a sentence. Specifically, its objective is to quantify the relationship
between a sentence encoder and a specific term rather than its connection with the context of that term, as seen in the training data. In order to accomplish this, SEAT adopts musked sentence structures like "That is [BLANK]" or "[BLANK] is here", where the empty slot [BLANK] is filled with social group and neutral attribute words. In addition, employing fixed-sized embedding vectors encapsulating the complete semantic information of the sentence as embeddings allows compatibility with Eq.(2).

### 5.2 Probability-based Metrics

Probability-based metrics formalize bias by analyzing the probabilities assigned by LLMs to various options, often predicting words or sentences based on templates [12, 147] or evaluation sets [57, 124]. These metrics are generally divided into two categories: masked tokens, which assess token probabilities in fill-in-the-blank templates, and pseudo-log-likelihood is utilized to assess the variance in probabilities between counterfactual pairs of sentences.

Discovery of Correlations (DisCo) [199]. DisCo utilizes a set of template sentences, each containing two empty slots. For example, "[PERSON] often likes to [BLANK]". The [PERSON] slot is manually filled with gender-related words from a vocabulary list, while the second slot [BLANK] is filled by the model's top three highest-scoring predictions. By comparing the model's candidate fills generation-based on the gender association in the [PERSON] slot, DisCo evaluates the presence and magnitude of bias in the model.

Log Probability Bias Score (LPBS) [94]. LPBS adopts template sentences similar to DisCO. However, unlike DisCO, LPBS corrects for the influence of inconsistent prior probabilities of target attributes. Specifically, for computing the association between the target gender male and the attribute doctor, LPBS first feeds the masked sentence "[MASK] is a doctor" into the model to obtain the probability of the sentence "he is a doctor", denoted as $P_{\text {tar male }}$. Then, to correct for the influence of inconsistent prior probabilities of target attributes, LPBS feeds the masked sentence " $[M A S K]$ is a [MASK]" into the model to obtain the probability of the sentence

![](https://cdn.mathpix.com/cropped/2024_06_04_f727414623c6961f741bg-05.jpg?height=46&width=848&top_left_y=1668&top_left_x=172)
"he" replaced by "she" for the target gender female. Finally, the bias is assessed by comparing the normalized probability scores for two contrasting attribute words and the specific formula is defined as:

![](https://cdn.mathpix.com/cropped/2024_06_04_f727414623c6961f741bg-05.jpg?height=100&width=423&top_left_y=1850&top_left_x=385)

CrowS-Pairs Score. CrowS-Pairs score [124] differs from the above two methods that use fill-in-the-blank templates, as it is based on pseudo-log-likelihood (PLL) [149] calculated on a set of counterfactual sentences. PLL approximates the probability of a token conditioned on the rest of the sentence by masking one token at a time and predicting it using all the other unmasked tokens. The equation for PLL can be expressed as:

$$
\begin{equation*}
\operatorname{PLL}(S)=\sum_{s \in S} \log P\left(s \mid S_{\backslash s} ; \theta\right) \tag{4}
\end{equation*}
$$

where $S$ represents is a sentence and $s$ denotes a word within $S$. The CrowS-Pairs score requires pairs of sentences, one characterized by stereotyping and the other less so, utilizing PLL to assess the model's inclination towards stereotypical sentences.

### 5.3 Generation-based Metrics

Generation-based metrics play a crucial role in addressing closedsource LLMs, as obtaining probabilities and embeddings of text generated by these models can be challenging. These metrics involve inputting biased or toxic prompts into the model, aiming to elicit biased or toxic text output, and then measuring the level of bias present. Generated-based metrics are categorized into two groups: classifier-based and distribution-based metrics.

Classifier-based Metrics. Classifier-based metrics utilize an auxiliary model to evaluate bias, toxicity, or sentiment in the generated text. Bias in the generated text can be detected when text created from similar prompts but featuring different social groups is classified differently by an auxiliary model. As an example, multilayer perceptrons, frequently employed as auxiliary models due to their robust modeling capabilities and versatile applications, are commonly utilized for binary text classification [8, 86]. Subsequently, binary bias is assessed by examining disparities in classification outcomes among various classes. For example, gender bias is quantified by analyzing the difference in true positive rates of gender in classification outcomes in [38].

Distribution-based Metrics. Detecting bias in the generated text can involve comparing the token distribution related to one social group with that of another or nearby social groups. One specific method is the Co-Occurrence Bias score [17], which assesses how often tokens co-occur with gendered words in a corpus of generated text. Mathematically, for any token $w$, and two sets of gender words, e.g., female and male, the bias score of a specific word $w$ is defined as follows:

$$
\begin{equation*}
\operatorname{bias}(w)=\log \left(\frac{P(w \mid \text { female })}{P(w \mid \text { male })}\right), P(w \mid g)=\frac{d(w, g) / \Sigma_{i} d\left(w_{i}, g\right)}{d(g) / \Sigma_{i} d\left(w_{i}\right)} \tag{5}
\end{equation*}
$$

where $P(w \mid g)$ represents the probability of encountering the word $w$ in the context of gendered terms $g$, and $d(w, g)$ represents a contextual window. The set $g$ consists of gendered words classified as either male or female. A positive bias score suggests that a word is more commonly associated with female words than with male words. In an infinite context, the words "doctor" and "nurse" would occur an equal number of times with both female and male words, resulting in bias scores of zero for these words.

## 6 MITIGATING BIAS IN LLMs

This section discusses and categorizes existing algorithms for mitigating bias in LLMs into four categories based on the stage at which they intervene in the processing pipeline.

### 6.1 Pre-processing

Pre-processing methods focus on adjusting the data provided for the model, which includes both training data and prompts, in order to eliminate underlying discrimination [37].

i) Data Augmentation. The objective of data augmentation is to achieve a balanced representation of training data across diverse social groups. One common approach is Counterfactual Data Augmentation (CDA) [108, 199, 242], which aims to balance datasets by exchanging protected attribute data. For instance, if a dataset contains more instances like "Men are excellent programmers"
than "Women are excellent programmers," this bias may lead LLMs to favor male candidates during the screening of programmer resumes. One way CDA achieves data balance and mitigates bias is by replacing a certain number of instances of "Men are excellent programmers" with "Women are excellent programmers" in the training data. Numerous follow-up studies have built upon and enhanced the effectiveness of CDA. For example, Maudslay et al. [199] introduced Counterfactual Data Substitution (CDS) to alleviate gender bias by randomly replacing gendered text with counterfactual versions at certain probabilities. Moreover, Zayed et al. [213]) discovered that the augmented dataset included instances that could potentially result in adverse fairness outcomes. They suggest an approach for data augmentation selection, which initially identifies instances within augmented datasets that might have an adverse impact on fairness. Subsequently, the model's fairness is optimized by pruning these instances.

ii) Prompt Tuning. In contrast to CDA, prompt tuning [97] focuses on reducing biases in LLMs by refining prompts provided by users. Prompt tuning can be categorized into two types: hard prompts and soft prompts. The former refers to predefined prompts that are static and may be considered as templates. Although templates provide some flexibility, the prompt itself remains mostly unchanged, hence the term "hard prompt." On the other hand, soft prompts are created dynamically during the prompt tuning process. Unlike hard prompts, soft prompts cannot be directly accessed or edited as text. Soft prompts are essentially embeddings, a series of numbers, that contain information extracted from the broader model. As a specific example of hard prompt, Mattern et al. [115] introduced an approach focusing on analyzing the bias mitigation effects of prompts across various levels of abstraction. In their experiments, they observed that the effects of debiasing became more noticeable as prompts became less abstract, as these prompts encouraged GPT-3 to utilize gender-neutral pronouns more frequently. In terms of soft prompt method, Fatemi et al. [56] focus on achieving gender equality by freezing model parameters and utilizing gender-neutral datasets to update biased word embeddings associated with occupations, effectively reducing bias in prompts. Overall, the disadvantage of hard prompts is their lack of flexibility, while the drawback of soft prompts is the lack of interpretability.

### 6.2 In-training

Mitigation techniques implemented during training aim to alter the training process to minimize bias. This includes making modifications to the optimization process by adjusting the loss function and incorporating auxiliary modules. These adjustments require the model to undergo retraining in order to update its parameters.

i) Loss Function Modification. Loss function modification involves incorporating a fairness-constrained into the training process of downstream tasks to guide the model toward fair learning. Wang et al. [196] introduced an approach that integrates causal relationships into model training. This method initially identifies causal features and spurious correlations based on standards inspired by the counterfactual framework of causal inference. A regularization technique is then used to construct the loss function, imposing small penalties on causal features and large penalties on spurious correlations. By adjusting the strength of penalties and optimizing the customized loss function, the model gives more importance to causal features and less importance to non-causal features, leading to fairer performance compared to conventional models. Additionally, Park et al. [133] proposed an embedding-based objective function that addresses the persistence of gender-related features in stereotype word vectors by utilizing generated gender direction vectors during fine-tuning steps.

ii) Auxiliary Module. Auxiliary module involve the addition of modules with the purpose of reducing bias within the model structure to help diminish bias. For instance, Lauscher et al. [95] proposed a sustainable modular debiasing strategy, namely Adapter-based DEbiasing of LanguagE Models (ADELE). Specifically, ADELE achieves debiasing by incorporating adapter modules into the original model layer and updating the adapters solely through language modeling training on a counterfactual augmentation corpus, thereby preserving the original model parameters unaltered. Additionally, Shen et al. [144] introduces Iterative Null Space Projection (INLP) for removing information from neural representations. Specifically, they iteratively train a linear classifier to predict a specific attribute for removal, followed by projecting the representation into the null space of that attribute. This process renders the classifier insensitive to the target attribute, complicating the linear separation of data based on that attribute. This method is effective in reducing bias in word embeddings and promoting fairness in multi-class classification scenarios.

### 6.3 Intra-processing

The Intra-processing focuses on mitigating bias in pre-trained or fine-tuned models during the inference stage without requiring additional training. This technique includes a range of methods, such as model editing and modifying the model's decoding process.

i) Model Editing. Model editing, as introduced by Mitchell et al. [121], offers a method for updating LLMs that avoids the computational burden associated with training entirely new models. This approach enables efficient adjustments to model behavior within specific areas of interest while ensuring no adverse effects on other inputs [207]. Recently, Limisiewicz et al. [103] identified the stereotype representation subspace and employed an orthogonal projection matrix to edit bias-vulnerable Feed-Forward Networks. Their innovative method utilizes profession as the subject and "he" or "she" as the target to aid in causal tracing. Furthermore, Akyürek et al. [4] expanded the application of model editing to include freeform natural language processing, thus incorporating bias editing.

ii) Decoding Modification. The method of decoding involves adjusting the quality of text produced by the model during the text generation process, including modifying token probabilities by comparing biases in two different output outcomes. For example, Gehman et al. [79] introduced a text generation technique known as DEXPERTS, which allows for controlled decoding. This method combines a pre-trained language model with "expert" and "antiexpert" language models. While the expert model assesses non-toxic text, the anti-expert model evaluates toxic text. In this combined system, tokens are assigned higher probabilities only if they are considered likely by the expert model and unlikely by the antiexpert model. This helps reduce bias in the output and enhances the quality of positive results.

### 6.4 Post-processing

Post-processing approaches modify the results generated by the model to mitigate biases, which is particularly crucial for closedsource LLMs where obtaining probabilities and embeddings of generated text is challenging, limiting the direct modification to output results only. Here, the method of chain-of-thought and rewriting serve as the illustrative approaches to convey this concept.

i) Chain-of-thought (CoT). The CoT technique enhances the hopeful and performance of LLMs towards fairness by leading them through incremental reasoning steps. The work by Kaneko et al. [87] provided a benchmark test where LLMs were tasked with determining the gender associated with specific occupational terms. Results revealed that, by default, LLMs tend to rely on societal biases when assigning gender labels to these terms. However, incorporating CoT prompts mitigates these biases. Furthermore, Dhingra et al. [47] introduced a technique combining CoT prompts and SHAP analysis [110] to counter stereotypical language towards queer individuals in model outputs. Using SHAP, stereotypical terms related to LGBTQ $+{ }^{6}$ individuals were identified, and then the chain-ofthought approach was used to guide language models in correcting this language.

ii) Rewriting. Rewriting methods refer to identifying discriminatory language in the results generated by models and replacing it with appropriate terms. As an illustration, Tokpo and Calders [179] introduced a text-style transfer model capable of training on nonparallel data. This model can automatically substitute biased content in the text output of LLMs, helping to reduce biases in textual data.

## 7 RESOURCES FOR EVALUATING BIAS

### 7.1 Toolkits

This section presents the following three essential tools designed to promote fairness in LLMs:

i) Perspective $\mathbf{A P I}^{7}$, created by Google Jigsaw, functions as a tool for detecting toxicity in text. Upon input of a text generation, Perspective API produces a probability of toxicity. This tool finds extensive application in the literature, as evidenced by its utilization in various studies [29, 96, 102].

ii) AI Fairness 360 (AIF360) [13] is an open-source toolkit aimed at aiding developers in assessing and mitigating biases and unfairness in machine learning models, including LLMs, by offering a variety of algorithms and tools for measuring, diagnosing, and alleviating unfairness.

iii) Aequitas [150] is an open-source bias audit toolkit developed to evaluate fairness and bias in machine learning models, including LLMs, with the aim of aiding data scientists and policymakers in comprehending and addressing bias in LLMs.

### 7.2 Datasets

This section provides a detailed summary of the datasets referenced in the surveyed literature, categorized into two distinct groups-probability-based and generation-based-based on the type of metric they are best suited for, as shown in Table 1.[^2]

i) Probability-based. As mentioned in section 5.2, datasets aligned with probability-based metrics typically use a templatebased format or a pair of counterfactual-based sentences. In template-based datasets, sentences include a placeholder that is completed by the language model choosing from predefined demographic terms, whereby the model's partiality towards various social groups is influenced by the probability of selecting these terms. Noteworthy examples of such datasets include WinoBias [239], which assess a model's competence in linking gender pronouns and occupations in both stereotypical and counter-stereotypical scenarios. WinoBias defines the gender binary in terms of two specific occupations. Expanding upon this dataset, several extensions have introduced a variety of diverse evaluation datasets. For example, WinoBias+ [181] enhances the original WinoBias dataset by employing rule-based and neural neutral rewriters to convert gendered sentences into neutral equivalents. Additionally, BUG [99] broadens the evaluation of gender bias in machine translation by using a large-scale real-world English dataset. In contrast, GAP [198] introduces a gender-balanced tagged corpus comprising $8,908 \mathrm{am}-$ biguous pronoun-name pairs, providing a more balanced dataset for accurately assessing model bias. Another category of counterfactualbased datasets evaluates bias by presenting the model with pairs of sentences containing different demographic terms and assessing their likelihood using a pseudo-log-likelihood metric. For instance, CrowS-Pairs [124] provides 1,508 sentence pairs to assess stereotypes of historically marginalized social groups, addressing nine biases related to race, gender, sexual orientation, religion, age, nationality, disability, physical appearance, and socioeconomic status. Similarly, WinoQueer [57], developed by the LGBTQ+ community, contains 45,540 sentence pairs designed to gauge anti-LGBTQ+ stereotypes, encompassing LGBTQ+ identity descriptors and counterfactual versions without LGBTQ+ identity markers.

ii) Generation-based. As discussed in section 5.3, datasets aligned with generation-based metrics usually specify the first few words of a sentence and require the model to provide a continuation. Noteworthy examples include RealToxicityPrompts [80] and BOLD [46], both providing web-based sentence prefixes, such as: "Jaegwon Kim was a Korean-American philosopher who . . .". Specifically, RealToxicityPrompts is one of the largest prompting datasets, providing 100,000 sentence prefixes curated from web text with a toxicity score by Perspective API, which can be used to measure the toxicity of generations given both toxic and non-toxic prompts. Besides, BOLD presents 23,679 prompts aimed at assessing bias across professions, genders, races, religions, and political ideologies. This dataset is compiled by extracting information from English Wikipedia pages mentioning biased groups (e.g., professions) and truncating sentences to generate prompts.

## 8 CHALLENGES AND FUTURE DIRECTIONS

Formulating Fairness Notions. Discrimination within LLMs can take various forms, necessitating the development of diverse fairness notions for a comprehensive understanding of bias and discrimination across different real-world applications. This complexity of real-world scenarios means that additional types of biases may exist, each requiring tailored approaches to quantify bias in LLMs.

Table 1: Dataset for evaluating Bias in LLMs. For each dataset, the dataset size, their corresponding types of bias, and related work are presented, depending on the suitable type of metric for the dataset. Within the category of probability-based evaluate metrics, datasets marked with an asterisk (*) are denoted counterfactual-based datasets, while datasets without an asterisk belong to the template-based.

| Category | Dataset | Size | Bias Type | Reference Works |
| :---: | :---: | :---: | :---: | :---: |
| Probability <br> based | BEC-Pro* [12] | 5,400 | gender | $[95,126,170]$ |
|  | $\mathrm{BUG}^{*}[99]$ | 108,419 | gender | $[55,104]$ |
|  | $\mathrm{BBQ}^{*}[134]$ | 58,492 | gender, others (9 types) | $[102,164,169]$ |
|  | Bias NLI [42] | $5,712,066$ | gender, race, religion | $[39,43,95,173]$ |
|  | BiasAsker [186] | 5,021 | gender, others (11 types) | $[35,122,192]$ |
|  | CrowS-Pairs [124] | 1,508 | gender, other(9 types) | $[69,117,131,151,214]$ |
|  | Equity Evaluation Corpus [89] | 4,320 | gender, race | $[14,34,116]$ |
|  | GAP $^{*}[198]$ | 8,908 | gender | $[2,77,94]$ |
|  | GAP-Subjective* [132] | 8,908 | gender | [209] |
|  | StereoSet ${ }^{*}[123]$ | 16,995 | gender, race, religion, profession | $[50,58,68,164,204]$ |
|  | WinoBias* [147] | 3,160 | gender | $[29,105,169]$ |
|  | WinoBias+* $[181]$ | 3,167 | gender | $[5,109,156,167]$ |
|  | Winogender ${ }^{*}[239]$ | 720 | gender | $[15,151,177,187]$ |
|  | PANDA [141] | 98,583 | gender, age, race | $[5,22,210,241]$ |
|  | REDDITBIAS [10] | 11,873 | gender, race, religion, queerness | $[81,111,236]$ |
|  | WinoQueer [57] | 45,540 | sexual orientation | $[40,78,172]$ |
| Generation <br> based | TrustGPT [80] | 9 | gender, race, religion | $[172,190]$ |
|  | HONEST [128] | 420 | gender | $[83,129,130,136]$ |
|  | BOLD $[46]$ | 23,679 | gender, others (4 types) | $[26,138,188]$ |
|  | RealToxicityPrompts [65] | 100,000 | toxicity | $[67,166]$ |
|  | HolisticBias [166] | 460,000 | gender, race, religion, age, others (13 types) | $[27,74,210]$ |

Furthermore, the definitions of fairness notions for LLMs can sometimes conflict, adding complexity to the task of ensuring equitable outcomes. Given these challenges, the process of either developing new fairness notions or selecting a coherent set of existing, nonconflicting fairness notions specifically for certain LLMs and their downstream applications remains an open question.

Rational Counterfactual Data Augmentation. Counterfactual data augmentation, a commonly employed technique in mitigating LLM bias, encounters several qualitative challenges in its implementation. A key issue revolves around inconsistent data quality, potentially leading to the generation of anomalous data that detrimentally impacts model performance. For instance, consider an original training corpus featuring sentences describing height and weight. When applying counterfactual data augmentation to achieve balance by merely substituting attribute words, it may result in the production of unnatural or irrational sentences, thus compromising the model's quality. For example, a straightforward replacement such as switching "a man who is 1.9 meters tall and weighs 200 pounds" with "a woman who is 1.9 meters tall and weighs 200 pounds" is evidently illogical. Future research could explore more rational replacement strategies or integrate alternative techniques to filter or optimize the generated data.

Balance Performance and Fairness in LLMs. A key strategy in mitigating bias involves adjusting the loss function and incorporating fairness constraints to ensure that the trained objective function considers both performance and fairness [205]. Although this effectively reduces bias in the model, finding the correct balance between model performance and fairness is a challenge. It often involves manually tuning the optimal trade-off parameter [212] However, training LLMs can be costly in terms of both time and finances for each iteration, and it also demands high hardware specifications. Hence, there is a pressing need to explore methods to achieve a balanced trade-off between performance and fairness systematically.

Fulfilling Multiple Types of Fairness. It is imperative to recognize that any form of bias is undesirable in real-world applications, underscoring the critical need to concurrently address multiple types of fairness. However, Gupta et al. [71] found that approximately half of the existing work on fairness in LLMs focuses solely on gender bias. While gender bias is an important issue, other types of societal demographic biases are also worthy of attention. Expanding the scope of research to encompass a broader range of bias categories can lead to a more comprehensive understanding of bias.

Develop More and Tailored Datasets. A comprehensive examination of fairness in LLMs demands the presence of extensive benchmark datasets. However, the prevailing datasets utilized for assessing bias in LLMs largely adopt a similar template-based methodology. Examples of such datasets, such as WinoBias [239], Winogender [239], GAP [198], and BUG [99], consist of sentences featuring blank slots, which language models are tasked with completing. Typically, these pre-defined options for filling in the blanks include pronouns like he/she/they or choices reflecting stereotypes and counter-stereotypes. These datasets overlook the potential necessity for customizing template characteristics to address various forms of bias. This oversight may lead to discrepancies in bias scores across different categories, underscoring the importance of devising more and tailored datasets to precisely evaluate specific social biases.

## 9 CONCLUSION

LLMs have demonstrated remarkable success across various highimpact applications, transforming the way we interact with technology. However, without proper fairness safeguards, they risk making decisions that could lead to discrimination, presenting a serious ethical issues and an increasing societal concern. This survey explores current definitions of fairness in machine learning and the necessary adaptations to address linguistic challenges when defining bias in the context of LLMs. Furthermore, techniques aimed at enhancing fairness in LLMs are categorized and elaborated upon. Notably, comprehensive resources including toolkits and datasets are summarized to facilitate future research progress in this area. Finally, existing challenges and open questions areas are also discussed.

## REFERENCES

[1] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in large language models. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 298-306.

[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).

[3] M Agrawal, S Hegselmann, H Lang, Y Kim, and D Sontag. 2023. Large language models are zero-shot clinical information extractors. Arxiv, 2022

[4] Afra Feyza Akyürek, Eric Pan, Garry Kuwanto, and Derry Wijaya. 2023. DUnE: Dataset for unified editing. arXiv preprint arXiv:2311.16087 (2023).

[5] Chantal Amrhein, Florian Schottmann, Rico Sennrich, and Samuel Läubli. 2023. Exploiting biased models to de-bias text: A gender-fair rewriting model. arXiv preprint arXiv:2305.11140 (2023).

[6] Haozhe An, Zongxia Li, Jieyu Zhao, and Rachel Rudinger. 2022. Sodapop: openended discovery of social biases in social commonsense reasoning models. arXiv preprint arXiv:2210.07269 (2022).

[7] Arthur Asuncion and David Newman. 2007. UCI machine learning repository.

[8] Akshat Bakliwal, Piyush Arora, Ankit Patil, and Vasudeva Varma. 2011. Towards Enhanced Opinion Classification using NLP Techniques.. In Proceedings of the workshop on Sentiment Analysis where AI meets Psychology (SAAIP 2011). 101107.

[9] Rajas Bansal. 2022. A survey on bias and fairness in natural language processing arXiv preprint arXiv:2204.09591 (2022)

[10] Soumya Barikeri, Anne Lauscher, Ivan Vulić, and Goran Glavaš. 2021. RedditBias: A real-world resource for bias evaluation and debiasing of conversational language models. arXiv preprint arXiv:2106.03521 (2021)

[11] Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. 2023. SeamlessM4T-Massively Multilingual \& Multimodal Ma chine Translation. arXiv preprint arXiv:2308.11596 (2023).

[12] Marion Bartl, Malvina Nissim, and Albert Gatt. 2020. Unmasking contextual stereotypes: Measuring and mitigating BERT's gender bias. arXiv preprint arXiv:2010.14534 (2020)

[13] Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta Aleksandra Mojsilović, et al. 2019. AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of Research and Develop ment 63, 4/5 (2019), 4-1.

[14] Emily M Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics 6 (2018), 587-604.

[15] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning. PMLR, 2397-2430.

[16] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems 29 (2016).

[17] Shikha Bordia and Samuel R Bowman. 2019. Identifying and reducing gender bias in word-level language models. arXiv preprint arXiv:1904.03035 (2019).

[18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.

[19] Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. 2019. Understanding the origins of bias in word embeddings. In International conference on machine learning. PMLR, 803-811.

[20] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4 arXiv preprint arXiv:2303.12712 (2023).

[21] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency. PMLR, 77-91.

[22] Laura Cabello, Anna Katrine Jørgensen, and Anders Søgaard. 2023. On the independence of association bias and empirical fairness in language models In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. 370-378

[23] Yu Cai, Drew Youngstrom, and Wenbin Zhang. 2023. Exploring Approaches for Teaching Cybersecurity and AI for K-12. In 2023 IEEE International Conference on Data Mining Workshops (ICDMW). IEEE, 1559-1564.

[24] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science 356
6334 (2017), 183-186.

[25] Kai-Wei Chang, Vinodkumar Prabhakaran, and Vicente Ordonez. 2019. Bias and fairness in natural language processing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International 7oint Conference on Natural Language Processing (EMNLP-I7CNLP): Tutorial Abstracts.

[26] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079 (2023).

[27] Myra Cheng, Esin Durmus, and Dan Jurafsky. 2023. Marked personas: Using natural language prompts to measure stereotypes in language models. arXiv preprint arXiv:2305.18189 (2023).

[28] Sribala Vidyadhari Chinta, Karen Fernandes, Ningxi Cheng, Jordan Fernandez, Shamim Yazdani, Zhipeng Yin, Zichong Wang, Xuyu Wang, Weifeng Xu, Jun Liu, et al. 2023. Optimization and Improvement of Fake News Detection using Voting Technique for Societal Benefit. In 2023 IEEE International Conference on Data Mining Workshops (ICDMW). IEEE, 1565-1574.

[29] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research 24, 240 (2023), 1-113.

[30] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems 30 (2017).

[31] Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Chengming Li, Xiping Hu, Ruifeng Xu, Min Yang, and Wenbin Zhang. 2024. History, Development, and Principles of Large Language Models-An Introductory Survey. arXiv preprint arXiv:2402.06853 (2024).

[32] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).

[33] John Joon Young Chung, Ece Kamar, and Saleema Amershi. 2023. Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions. arXiv preprint arXiv:2306.04140 (2023).

[34] Davide Cirillo, Silvina Catuara-Solarz, Czuee Morey, Emre Guney, Laia Subirats, Simona Mellino, Annalisa Gigante, Alfonso Valencia, María José Rementeria, Antonella Santuccione Chadha, et al. 2020. Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare. NP7 digital medicine 3 , 1 (2020), 1-11.

[35] Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng, Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, et al. 2024. Risk taxonomy, mitigation, and assessment benchmarks of large language model systems. arXiv preprint arXiv:2401.05778 (2024).

[36] Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Junjie Xu, Zhimeng Guo, Hui Liu, Jiliang Tang, and Suhang Wang. 2022. A comprehensive survey on trustworthy graph neural networks: Privacy, robustness, fairness, and explainability. arXiv preprint arXiv:2204.08570 (2022).

[37] Brian d'Alessandro, Cathy O'Neil, and Tom LaGatta. 2017. Conscientious classification: A data scientist's guide to discrimination-aware classification. Big data 5, 2 (2017), 120-134

[38] Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. 2019. Bias in bios: A case study of semantic representation bias in a high-stakes setting. In proceedings of the Conference on Fairness, Accountability, and Transparency. 120-128.

[39] Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon Calders, and Bettina Berendt. 2022. Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 1693-1706.

[40] Nathan Dennler, Anaelia Ovalle, Ashwin Singh, Luca Soldaini, Arjun Subramonian, Huy Tu, William Agnew, Avijit Ghosh, Kyra Yee, Irene Font Peradejordi, et al. 2023. Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society. 375-386.

[41] Ketki V Deshpande, Shimei Pan, and James R Foulds. 2020. Mitigating demographic Bias in AI-based resume filtering. In Adjunct publication of the 28th ACM conference on user modeling, adaptation and personalization. 268-275.

[42] Sunipa Dev, Tao Li, Jeff M Phillips, and Vivek Srikumar. 2020. On measuring and mitigating biased inferences of word embeddings. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 7659-7666

[43] Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Arjun Subramonian, Jeff M Phillips, and Kai-Wei Chang. 2021. Harms of gender exclusivity and challenges in non-binary representation in language technologies. arXiv preprint arXiv:2108.12084 (2021).

[44] Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao Sun, Yu Hou, Mattie Sanseverino, Jiin Kim, Akihiro Nishi, Nanyun Peng, et al. 2021. On measures of
biases and harms in NLP. arXiv preprint arXiv:2108.03362 (2021).

[45] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).

[46] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 862-872.

[47] Harnoor Dhingra, Preetiha Jayashanker, Sayali Moghe, and Emma Strubell. 2023. Queer people are people first: Deconstructing sexual identity stereotypes in large language models. arXiv preprint arXiv:2307.00101 (2023).

[48] Yushun Dong, Jing Ma, Song Wang, Chen Chen, and Jundong Li. 2023. Fairness in graph mining: A survey. IEEE Transactions on Knowledge and Data Engineering (2023).

[49] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378 (2023).

[50] Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. 2023. Guiding pretraining in reinforcement learning with large language models. In International Conference on Machine Learning. PMLR, 8657-8677.

[51] Yucong Duan. [n. d.]. " The Large Language Model (LLM) Bias Evaluation (Age Bias). ([n.d.]).

[52] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference. 214-226.

[53] Jocelyn Dzuong, Zichong Wang, and Wenbin Zhang. [n. d.]. Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI. ([n. d. $])$.

[54] Tugba Akinci D’Antonoli, Arnaldo Stanzione, Christian Bluethgen, Federica Vernuccio, Lorenzo Ugga, Michail E Klontzas, Renato Cuocolo, Roberto Cannella, and Burak Koçak. 2024. Large language models in radiology: fundamentals applications, ethical considerations, risks, and future directions. Diagnostic and Interventional Radiology 30, 2 (2024), 80.

[55] David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, and Eric Michael Smith. 2023. ROBBIE: Robust Bias Evaluation of Large Generative Language Models. In The 2023 Conference on Empirical Methods in Natural Language Processing.

[56] Zahra Fatemi, Chen Xing, Wenhao Liu, and Caiming Xiong. 2021. Improving gender fairness of pre-trained language models without catastrophic forgetting arXiv preprint arXiv:2110.05367 (2021).

[57] Virginia K Felkner, Ho-Chun Herbert Chang, Eugene Jang, and Jonathan May. 2023. Winoqueer: A community-in-the-loop benchmark for anti-lgbtq+ bias in large language models. arXiv preprint arXiv:2306.15087 (2023).

[58] Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models. arXiv preprint arXiv:2305.08283 (2023).

[59] Emilio Ferrara. 2023. Should chatgpt be biased? challenges and risks of bias in large language models. arXiv preprint arXiv:2304.03738 (2023).

[60] Eve Fleisig, Rediet Abebe, and Dan Klein. 2023. When the majority is wrong: Modeling annotator disagreement for subjective tasks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 6715-6726.

[61] Eve Fleisig, Aubrie Amstutz, Chad Atalla, Su Lin Blodgett, Hal Daumé III, Alexan dra Olteanu, Emily Sheng, Dan Vann, and Hanna Wallach. 2023. FairPrism: evaluating fairness-related harms in text generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 6231-6251.

[62] Maxwell Forbes, Jena D Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. arXiv preprint arXiv:2011.00620 (2020).

[63] Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. 2023. Bias and fairness in large language models: A survey. arXiv preprint arXiv:2309.00770 (2023).

[64] Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences 115, 16 (2018), E3635-E3644.

[65] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462 (2020)

[66] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker et al. 2022. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375 (2022)
[67] Seraphina Goldfarb-Tarrant, Rebecca Marchant, Ricardo Muñoz Sánchez, Mugdha Pandya, and Adam Lopez. 2020. Intrinsic bias metrics do not correlate with application bias. arXiv preprint arXiv:2012.15859 (2020).

[68] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 2023. Not what you've signed up for: Compromising realworld $1 l$-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security. 79-90.

[69] Yue Guo, Yi Yang, and Ahmed Abbasi. 2022. Auto-debias: Debiasing masked language models with automated biased prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1012-1023.

[70] Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. 2023. Evaluating large language models: A comprehensive survey. arXiv preprint arXiv:2310.19736 (2023).

[71] Vipul Gupta, Pranav Narayanan Venkit, Shomir Wilson, and Rebecca J Passonneau. [n. d.]. Sociodemographic Bias in Language Models: A Survey and Forward Path. ([n.d.])

[72] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324 (2018).

[73] Thomas Guyet, Wenbin Zhang, and Albert Bifet. 2022. Incremental Mining of Frequent Serial Episodes Considering Multiple Occurrences. In 22nd International Conference on Computational Science. Springer, 460-472.

[74] Melissa Hall, Laura Gustafson, Aaron Adcock, Ishan Misra, and Candace Ross. 2023. Vision-language models performing zero-shot tasks exhibit gender-based disparities. arXiv preprint arXiv:2301.11100 (2023).

[75] Sil Hamilton. 2023. Blind judgement: Agent-based supreme court modelling with gpt. arXiv preprint arXiv:2301.05327 (2023).

[76] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in supervised learning. Advances in neural information processing systems 29 (2016).

[77] Dirk Hovy and Shrimai Prabhumoye. 2021. Five sources of bias in natural language processing. Language and linguistics compass 15, 8 (2021), e12432.

[78] Dong Huang, Qingwen Bu, Jie Zhang, Xiaofei Xie, Junjie Chen, and Heming Cui. 2023. Bias assessment and mitigation in llm-based code generation. arXiv preprint arXiv:2309.14345 (2023).

[79] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. 2019. Reducing sentiment bias in language models via counterfactual evaluation. arXiv preprint arXiv:1911.03064 (2019).

[80] Yue Huang, Qihui Zhang, Lichao Sun, et al. 2023. Trustgpt: A benchmark for trustworthy and responsible large language models. arXiv preprint arXiv:2306.11507 (2023).

[81] Chia-Chien Hung, Anne Lauscher, Ivan Vulić, Simone Paolo Ponzetto, and Goran Glavaš. 2022. Multi2WOZ: A robust multilingual dataset and conversational pretraining for task-oriented dialog. arXiv preprint arXiv:2205.10400 (2022).

[82] Kwan Yuen Iu and Vanessa Man-Yi Wong. 2023. ChatGPT by OpenAI: The End of Litigation Lawyers? Available at SSRN 4339839 (2023).

[83] Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, and Mor Naaman. 2023. Co-writing with opinionated language models affects users' views. In Proceedings of the 2023 CHI conference on human factors in computing systems. $1-15$.

[84] Frederick Jelinek. 1998. Statistical methods for speech recognition. MIT press.

[85] Yiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu, Munmun De Choudhury, and Srijan Kumar. 2023. Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries. arXiv e-prints (2023), arXiv2310 .

[86] Irfan Ali Kandhro, Sahar Zafar Jumani, Ajab Ali Lashari, Saima Sipy Nangraj, Qurban Ali Lakhan, Mirza Taimoor Baig, and Subhash Guriro. 2019. Classification of Sindhi headline news documents based on TF-IDF text analysis scheme. Indian fournal of Science and Technology 12, 33 (2019), 1-10.

[87] Masahiro Kaneko, Danushka Bollegala, Naoaki Okazaki, and Timothy Baldwin. 2024. Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting. arXiv preprint arXiv:2401.15585 (2024).

[88] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2024. Gpt-4 passes the bar exam. Philosophical Transactions of the Royal Society A 382, 2270 (2024), 20230254.

[89] Svetlana Kiritchenko and Saif M Mohammad. 2018. Examining gender and race bias in two hundred sentiment analysis systems. arXiv preprint arXiv:1805.04508 (2018).

[90] Hannah Rose Kirk, Yennie Jun, Filippo Volpin, Haider Iqbal, Elias Benussi, Frederic Dreyer, Aleksandar Shtedritski, and Yuki Asano. 2021. Bias out-ofthe-box: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in neural information processing systems 34 (2021), 2611-2624.

[91] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199-22213.

[92] Diane M Korngiebel and Sean D Mooney. 2021. Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery NP7 Digital Medicine 4, 1 (2021), 93.

[93] Hadas Kotek, Rikker Dockum, and David Sun. 2023. Gender bias and stereotypes in large language models. In Proceedings of The ACM Collective Intelligence Conference. 12-24

[94] Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov 2019. Measuring bias in contextualized word representations. arXiv preprint

![](https://cdn.mathpix.com/cropped/2024_06_04_f727414623c6961f741bg-11.jpg?height=35&width=241&top_left_y=519&top_left_x=243)

[95] Anne Lauscher, Tobias Lueken, and Goran Glavaš. 2021. Sustainable modular debiasing of language models. arXiv preprint arXiv:2109.03646 (2021).

[96] Alyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. 2022. A new generation of perspective api: Efficient multilingual character-level transformers. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 3197-3207.

[97] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 (2021)

[98] Sharon Levy, Tahilin Sanchez Karver, William D Adler, Michelle R Kaufman, and Mark Dredze. 2024. Evaluating Biases in Context-Dependent Health Questions. arXiv preprint arXiv:2403.04858 (2024).

[99] Shahar Levy, Koren Lazar, and Gabriel Stanovsky. 2021. Collecting a large-scale gender bias dataset for coreference resolution and machine translation. arXiv preprint arXiv:2109.03858 (2021).

[100] Tao Li, Tushar Khot, Daniel Khashabi, Ashish Sabharwal, and Vivek Srikumar 2020. UNQOVERing stereotyping biases via underspecified questions. arXiv preprint arXiv:2010.02428 (2020)

[101] Y Li, M Du, R Song, X Wang, and Y Wang. 2023. A survey on fairness in large language models. arXiv. doi: 10.48550. arXiv preprint arXiv. 2308.10149 (2023).

102] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).

[103] Tomasz Limisiewicz, David Mareček, and Tomáš Musil. 2023. Debiasing algo rithm through model adaptation. arXiv preprint arXiv:2310.18913 (2023)

[104] Gili Lior and Gabriel Stanovsky. 2023. Comparing humans and models on a similar scale: Towards cognitive gender bias evaluation in coreference resolution. arXiv preprint arXiv:2305.15389 (2023).

[105] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).

[106] Zhen Liu, Ruoyu Wang, Nathalie Japkowicz, Heitor Murilo Gomes, Bitao Peng, and Wenbin Zhang. 2023. SeGDroid: An Android malware detection method based on sensitive function call graph learning. Expert Systems with Applications (2023), 121125

[107] Zhen Liu, Ruoyu Wang, Nathalie Japkowicz, Deyu Tang, Wenbin Zhang, and Jie Zhao. 2021. Research on unsupervised feature learning for Android malware detection based on Restricted Boltzmann Machines. Future Generation Computer Systems 120 (2021), 91-108.

[108] Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. 2020. Gender bias in neural natural language processing. Logic, language, and security: essays dedicated to Andre Scedrov on the occasion of his 65th birthday (2020), 189-202.

[109] Gunnar Lund, Kostiantyn Omelianchuk, and Igor Samokhin. 2023. Genderinclusive grammatical error correction through augmentation. arXiv preprint $\operatorname{arXiv:2306.07415~(2023)~}$

[110] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. Advances in neural information processing systems 30 (2017).

[111] Hongyin Luo and James Glass. 2023. Logic against bias: Textual entailment mitigates stereotypical sentence reasoning. arXiv preprint arXiv:2303.05670 (2023).

[112] Queenie Luo, Michael J Puett, and Michael D Smith. 2023. A" Perspectival" Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, YouTube, and Wikipedia. arXiv preprint arXiv:2303.16281 (2023)

113] Nikhil Malik and Param Vir Singh. 2019. Deep learning in computer vision Methods, interpretation, causation, and fairness. In Operations Research \& Management Science in the Age of Analytics. INFORMS, 73-100.

[114] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.105117 (2022)

[115] Justus Mattern, Zhijing Jin, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schölkopf. 2022. Understanding stereotypes in language models: Towards robust measurement and zero-shot debiasing. arXiv preprint arXiv:2212.10678 (2022).

[116] Chandler May, Alex Wang, Shikha Bordia, Samuel R Bowman, and Rachel Rudinger. 2019. On measuring social biases in sentence encoders. arXiv preprint

![](https://cdn.mathpix.com/cropped/2024_06_04_f727414623c6961f741bg-11.jpg?height=33&width=241&top_left_y=2404&top_left_x=241)

[117] Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy. 2021. An empirical survey of the effectiveness of debiasing techniques for pre-trained language models. arXiv preprint arXiv:2110.08527 (2021).

[118] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR) 54, 6 (2021), 1-35

[119] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).

[120] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model.. In Interspeech, Vol. 2. Makuhari, 1045-1048

[121] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. 2021. Fast model editing at scale. arXiv preprint arXiv:2110.11309 (2021).

[122] Sergio Morales, Robert Clarisó, and Jordi Cabot. 2023. Automating Bias Testing of LLMs. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 1705-1707.

[123] Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. StereoSet: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456 (2020).

[124] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R Bowman. 2020. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. arXiv preprint arXiv:2010.00133 (2020).

[125] Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment analysis: Capturing favorability using natural language processing. In Proceedings of the 2nd international conference on Knowledge capture. 70-77.

[126] Aurélie Névéol, Yoann Dupont, Julien Bezançon, and Karën Fort. 2022. French CrowS-pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 8521-8531.

[127] Helen Ngo, Cooper Raterink, João GM Araújo, Ivan Zhang, Carol Chen, Adrien Morisot, and Nicholas Frosst. 2021. Mitigating harm in language models with conditional-likelihood filtration. arXiv preprint arXiv:2108.07790 (2021).

[128] Debora Nozza, Federico Bianchi, Dirk Hovy, et al. 2021. HONEST: Measuring hurtful sentence completion in language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.

[129] Debora Nozza, Federcio Bianchi, Dirk Hovy, et al. 2022. Pipelines for social bias testing of large language models. In Proceedings of BigScience Episode\# 5-Workshop on Challenges \& Perspectives in Creating Large Language Models. Association for Computational Linguistics.

[130] Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and Dit-Yan Yeung. 2021. Probing toxic content in large pre-trained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International foint Conference on Natural Language Processing (Volume 1: Long Papers). 4262-4274.

[131] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730-27744.

[132] Kartikey Pant and Tanvi Dadu. 2022. Incorporating subjectivity into gendered ambiguous pronoun (GAP) resolution using style transfer. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP). 273-281.

[133] SunYoung Park, Kyuri Choi, Haeun Yu, and Youngjoong Ko. 2023. Never too late to learn: Regularizing gender bias in coreference resolution. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. $15-23$.

[134] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R Bowman. 2021. BBQ: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193 (2021).

[135] Constituency Parsing. 2009. Speech and language processing. Power Point Slides (2009).

[136] Max Pellert, Clemens M Lechner, Claudia Wagner, Beatrice Rammstedt, and Markus Strohmaier. 2023. AI Psychometrics: Using psychometric inventories to obtain psychological profiles of large language models. OSF preprint (2023).

[137] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 1532-1543.

[138] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022. Red teaming language models with language models. arXiv preprint arXiv:2202.03286 (2022).

[139] Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. arXiv preprint arXiv:1805.01042 (2018)

[140] Nirmalendu Prakash and Roy Ka-Wei Lee. 2023. Layered bias: Interpreting bias in pretrained large language models. In Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. 284-295.

[141] Rebecca Oian, Candace Ross, Jude Fernandes, Eric Smith, Douwe Kiela, and Adina Williams. 2022. Perturbation augmentation for fairer nlp. arXiv preprint

![](https://cdn.mathpix.com/cropped/2024_06_04_f727414623c6961f741bg-12.jpg?height=38&width=241&top_left_y=436&top_left_x=243)

[142] Tai Le Quy, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, and Eirini Ntoutsi. 2022. A survey on datasets for fairness-aware machine learning. Data Mining and Knowledge Discovery (2022).

[143] Manish Raghavan and Solon Barocas. 2019. Challenges for mitigating bias in algorithmic hiring. (2019).

[144] Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg 2020. Null it out: Guarding protected attributes by iterative nullspace projection. arXiv preprint arXiv:2004.07667 (2020).

[145] Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2021. A recipe for arbitrary text style transfer with large language models. arXiv preprint arXiv:2109.03910 (2021).

[146] Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? Proc. IEEE 88, 8 (2000), 1270-1278.

[147] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301 (2018).

[148] Anastasiia V Sadybekov and Vsevolod Katritch. 2023. Computational approaches streamlining drug discovery. Nature 616, 7958 (2023), 673-685.

[149] Julian Salazar, Davis Liang, Toan Q Nguyen, and Katrin Kirchhoff. 2019. Masked language model scoring. arXiv preprint arXiv:1910.14659 (2019).

[150] Pedro Saleiro, Benedict Kuester, Loren Hinkson, Jesse London, Abby Stevens, Ari Anisfeld, Kit T Rodolfa, and Rayid Ghani. 2018. Aequitas: A bias and fairness audit toolkit. arXiv preprint arXiv:1811.05577 (2018)

[151] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 (2021)

[152] Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A Smith. 2019 The risk of racial bias in hate speech detection. In Proceedings of the 57th annual meeting of the association for computational linguistics. 1668-1678.

[153] Danielle Saunders, Rosie Sallis, and Bill Byrne. 2021. First the worst: Finding better gender translations during beam search. arXiv preprint arXiv:2104.07429 (2021).

[154] Neil Savage. 2023. Drug discovery companies are customizing ChatGPT: here's how. Nat Biotechnol 41, 5 (2023), 585-586.

[155] Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2021. Gender bias in machine translation. Transactions of the Association for Computational Linguistics 9 (2021), 845-874.

[156] Beatrice Savoldi, Marco Gaido, Matteo Negri, and Luisa Bentivogli. 2023. Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES arXiv preprint arXiv:2310.19345 (2023)

[157] Nripsuta Ani Saxena, Wenbin Zhang, and Cyrus Shahabi. 2023. Missed Opportunities in Fair AI. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM). SIAM, 961-964.

[158] Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur Rahman, Ji Woong Kim, Rojin Ziaei, Jason Eshraghian, Peter Abadir, and Rama Chellappa. 2024. Addressing cognitive bias in medical language models. arXiv preprint arXiv:2402.08113 (2024)

[159] Emre Sezgin, Joseph Sirrianni, and Simon L Linwood. 2022. Operationalizing and implementing pretrained, large artificial intelligence linguistic models in the US health care system: outlook of generative pretrained transformer 3 (GPT-3) as a service model. 7MIR medical informatics 10, 2 (2022), e32875.

[160] Ashish Shah, Pratik Raj, Supriya P Pushpam Kumar, and HV Asha. [n. d.]. FinAID A Financial Advisor Application using AI. ([n.d.]).

[161] Deven Shah, H Andrew Schwartz, and Dirk Hovy. 2019. Predictive biases in natural language processing models: A conceptual framework and overview arXiv preprint arXiv:1912.11078 (2019)

[162] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2020. " Nice Try, Kiddo": Investigating Ad Hominems in Dialogue Responses. arXiv preprint arXiv:2010.12820 (2020)

[163] Hari Shrawgi, Prasanjit Rath, Tushar Singhal, and Sandipan Dandapat. 2024 Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers). 1841-1857.

[164] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. 2022. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150 (2022)

[165] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al 2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023), 172-180.
[166] Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. 2022. " I'm sorry to hear that": Finding New Biases in Language Models with a Holistic Descriptor Dataset. arXiv preprint arXiv:2205.09209 (2022).

[167] Nasim Sobhani, Kinshuk Sengupta, and Sarah Jane Delany. 2023. Measuring gender bias in natural language processing: Incorporating gender-neutral linguistic forms for non-binary gender identities in abusive speech detection. In Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing. 1121-1131.

[168] Sofia Eleni Spatharioti, David M Rothschild, Daniel G Goldstein, and Jake M Hofman. 2023. Comparing traditional and $1 \mathrm{~lm}$-based search for consumer choice: A randomized experiment. arXiv preprint arXiv:2307.03744 (2023).

[169] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 (2022).

[170] Ryan Steed, Swetasudha Panda, Ari Kobren, and Michael Wick. 2022. Upstream mitigation is not all you need: Testing the bias transfer hypothesis in pre-trained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 3524-3542.

[171] Volker Steinbiss, Bach-Hiep Tran, and Hermann Ney. 1994. Improvements in beam search.. In ICSLP, Vol. 94. 2143-2146.

[172] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. 2024. Trustllm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561 (2024).

[173] Tianxiang Sun, Junliang He, Xipeng Qiu, and Xuanjing Huang. 2022. BERTScore is unfair: On social bias in language model-based metrics for text generation. arXiv preprint arXiv:2210.07626 (2022).

[174] Xuejiao Tang, Liuhua Zhang, et al. 2020. Using machine learning to automate mammogram images analysis. In IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 757-764

[175] Xuejiao Tang, Wenbin Zhang, Yi Yu, Kea Turner, Tyler Derr, Mengyu Wang, and Eirini Ntoutsi. 2021. Interpretable Visual Understanding with Cognitive Attention Network. In International Conference on Artificial Neural Networks. Springer, 555-568

[176] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022).

[177] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. 2022. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5238-5248.

[178] Huan Tian, Tianqing Zhu, Wei Liu, and Wanlei Zhou. 2022. Image fairness in deep learning: problems, models, and challenges. Neural Computing and Applications 34, 15 (2022), 12875-12893

[179] Ewoenam Kwaku Tokpo and Toon Calders. 2022. Text style transfer for bias mitigation using masked language modeling. arXiv preprint arXiv:2201.08643 (2022).

[180] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).

[181] Eva Vanmassenhove, Chris Emmery, and Dimitar Shterionov. 2021. NeuTral Rewriter: A Rule-Based and Neural Approach to Automatic Rewriting into Gender-Neutral Alternatives. arXiv preprint arXiv:2109.06105 (2021)

[182] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).

[183] Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, TingHao'Kenneth' Huang, and Shomir Wilson. 2023. Nationality bias in text generation. arXiv preprint arXiv:2302.02463 (2023).

[184] Sahil Verma and Julia Rubin. 2018. Fairness definitions explained. In Proceedings of the international workshop on software fairness. 1-7.

[185] Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, and Nanyun Peng. 2023. " kelly is a warm person, joseph is a role model": Gender biases in llm-generated reference letters. arXiv preprint arXiv:2310.09219 (2023)

[186] Yuxuan Wan, Wenxuan Wang, Pinjia He, Jiazhen Gu, Haonan Bai, and Michael R Lyu. 2023. Biasasker: Measuring the bias in conversational ai system. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 515-527.

[187] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems 32 (2019).

[188] Boxin Wang, Wei Ping, Chaowei Xiao, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Bo Li, Anima Anandkumar, and Bryan Catanzaro. 2022. Exploring the limits of domain-adaptive training for detoxifying large-scale language models. Advances in Neural Information Processing Systems 35 (2022), 35811-35824

[189] Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, and Jie Fu. 2023. Pre-trained language models in biomedical domain: A systematic survey. Comput. Surveys 56, 3 (2023), 1-52.

[190] Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, et al. 2023. Are Large Language Models Really Robust to Word-Level Perturbations? arXiv preprint arXiv:2309.11166 (2023).

[191] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. 2023 Chatcad: Interactive computer-aided diagnosis on medical image using large language models. arXiv preprint arXiv:2302.07257 (2023).

[192] Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael R Lyu. 2023. All languages matter: On the multilingual safety of large language models. arXiv preprint arXiv:2310.00905 (2023).

[193] Xuejian Wang, Wenbin Zhang, Aishwarya Jadhav, and Jeremy Weiss. 2021. Harmonic-Mean Cox Models: A Ruler for Equal Attention to Risk. In Survival Prediction-Algorithms, Challenges and Applications. PMLR, 171-183.

[194] Zichong Wang, Giri Narasimhan, Xin Yao, and Wenbin Zhang. 2023. Mitigating multisource biases in graph neural networks via real counterfactual samples. In 2023 IEEE International Conference on Data Mining (ICDM). IEEE, 638-647

[195] Zichong Wang, Nripsuta Saxena, Tongjia Yu, Sneha Karki, Tyler Zetty, Israat Haque, Shan Zhou, Dukka Kc, Ian Stockwell, Xuyu Wang, et al. 2023. Preventing discriminatory decision-making in evolving data streams. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. 149-159.

[196] Zhao Wang, Kai Shu, and Aron Culotta. 2021. Enhancing model robustness and fairness with causality: A regularization approach. arXiv preprint arXiv:2110.00911 (2021)

[197] Zichong Wang, Charles Wallace, Albert Bifet, Xin Yao, and Wenbin Zhang. 2023. : Fairness-aware graph generative adversarial networks. In foint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, $259-275$.

[198] Kellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. 2018. Mind the GAP: A balanced corpus of gendered ambiguous pronouns. Transactions of the Association for Computational Linguistics 6 (2018), 605-617.

[199] Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and Slav Petrov. 2020. Measuring and reducing gendered correlations in pre-trained models. arXiv preprint arXiv:2010.06032 (2020)

[200] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).

[201] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing system. 35 (2022), 24824-24837

[202] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359 (2021)

[203] Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, and Maosong Sun. 2021 Lawformer: A pre-trained language model for chinese legal long documents. AI Open 2 (2021), 79-84

[204] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. 2024. Doremi Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems 36 (2024)

[205] Ke Yang, Charles Yu, Yi R Fung, Manling Li, and Heng Ji. 2023. Adept: A debiasing prompt framework. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 10780-10788.

[206] Binwei Yao, Ming Jiang, Diyi Yang, and Junjie Hu. 2023. Empowering LLM-based machine translation with cultural awareness. arXiv preprint arXiv:2305.14328 (2023).

[207] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing large language models Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172 (2023).

[208] Shamim Yazdani, Nripsuta Saxena, Zichong Wang, Yanzhao Wu, and Wenbin Zhang. [n.d.]. A Comprehensive Survey of Image and Video Generative AI Recent Advances, Variants, and Applications. ([n. d.]).

[209] Vithya Yogarajan, Gillian Dobbie, Te Taka Keegan, and Rostam J Neuwirth. 2023. Tackling Bias in Pre-trained Language Models: Current Trends and Under represented Societies. arXiv preprint arXiv:2312.01509 (2023)

[210] Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. 2023. Unlearning bias in language models by partitioning gradients. In Findings of the Association for Computational Linguistics: ACL 2023. 6032-6048

[211] Fangyi Yu, Lee Quartey, and Frank Schilder. 2022. Legal prompting: Teaching a language model to think like a lawyer. arXiv preprint arXiv:2212.01326 (2022).
[212] Abdelrahman Zayed, Goncalo Mordido, Samira Shabanian, and Sarath Chandar. 2023. Should we attend more or less? modulating attention for fairness. arXiv preprint arXiv:2305.13088 (2023).

[213] Abdelrahman Zayed, Prasanna Parthasarathi, Gonçalo Mordido, Hamid Palangi, Samira Shabanian, and Sarath Chandar. 2023. Deep learning on a healthy data diet: Finding important examples for fairness. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 14593-14601.

[214] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022).

[215] Mingli Zhang, Fan Zhang, Jianxin Zhang, Ahmad Chaddad, Fenghua Guo, Wenbin Zhang, Ji Zhang, and Alan Evans. 2021. Autoencoder for neuroimage. In International conference on database and expert systems applications. Springer, $84-90$.

[216] Mingli Zhang, Xin Zhao, et al. 2020. Deep discriminative learning for autism spectrum disorder classification. In International Conference on Database and Expert Systems Applications. Springer, 435-443.

[217] Wenbin Zhang. 2017. Phd forum: Recognizing human posture from timechanging wearable sensor data streams. In IEEE International Conference on Smart Computing (SMARTCOMP).

[218] Wenbin Zhang. 2020. Learning fairness and graph deep generation in dynamic environments. (2020).

[219] Wenbin Zhang. 2024. Fairness with Censorship: Bridging the Gap between Fairness Research and Real-World Deployment. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 22685-22685.

[220] Wenbin Zhang et al. 2020. Flexible and adaptive fairness-aware learning in non-stationary data streams. In IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI). 399-406

[221] Wenbin Zhang, Albert Bifet, Xiangliang Zhang, Jeremy C Weiss, and Wolfgang Nejdl. 2021. FARF: A Fair and Adaptive Random Forests Classifier. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 245-256.

[222] Wenbin Zhang, Tina Hernandez-Boussard, and Jeremy Weiss. 2023. Censored fairness through awareness. In Proceedings of the AAAI conference on artificial intelligence, Vol. 37. 14611-14619.

[223] Wenbin Zhang and Eirini Ntoutsi. 2019. FAHT: an adaptive fairness-aware decision tree classifier. In International foint Conference on Artificial Intelligence (I7CAI). 1480-1486.

[224] Wenbin Zhang, Shimei Pan, Shuigeng Zhou, Toby Walsh, and Jeremy C Weiss. 2022. Fairness Amidst Non-IID Graph Data: Current Achievements and Future Directions. arXiv preprint arXiv:2202.07170 (2022).

[225] Wenbin Zhang, Jian Tang, and Nuo Wang. 2016. Using the machine learning approach to predict patient survival from high-dimensional survival data. In IEEE International Conference on Bioinformatics and Biomedicine (BIBM).

[226] Wenbin Zhang, Xuejiao Tang, and Jianwu Wang. 2019. On fairness-aware learning for non-discriminative decision-making. In International Conference on Data Mining Workshops (ICDMW). 1072-1079.

[227] Wenbin Zhang and Jianwu Wang. 2018. Content-bootstrapped collaborative filtering for medical article recommendations. In IEEE International Conference on Bioinformatics and Biomedicine (BIBM).

[228] Wenbin Zhang, Jianwu Wang, Daeho Jin, Lazaros Oreopoulos, and Zhibo Zhang. 2018. A deterministic self-organizing map approach and its application on satellite data based cloud type classification. In IEEE International Conference on Big Data (Big Data).

[229] Wenbin Zhang, Zichong Wang, Juyong Kim, Cheng Cheng, Thomas Oommen, Pradeep Ravikumar, and Jeremy Weiss. 2023. Individual fairness under uncertainty. arXiv preprint arXiv:2302.08015 (2023).

[230] Wenbin Zhang and Jeremy Weiss. 2021. Fair Decision-making Under Uncertainty. In 2021 IEEE International Conference on Data Mining (ICDM). IEEE.

[231] Wenbin Zhang and Jeremy C Weiss. 2022. Longitudinal fairness with censorship. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 1223512243 .

[232] Wenbin Zhang and Jeremy C Weiss. 2023. Fairness with censorship and group constraints. Knowledge and Information Systems (2023), 1-24.

[233] Wenbin Zhang, Liming Zhang, Dieter Pfoser, and Liang Zhao. 2021. Disentangled Dynamic Graph Deep Generation. In Proceedings of the SIAM International Conference on Data Mining (SDM). 738-746.

[234] Wenbin Zhang and Liang Zhao. 2020. Online decision trees with fairness. arXiv preprint arXiv:2010.08146 (2020).

[235] Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. Diffusion theory as a scalpel: Detecting and purifying poisonous dimensions in pre-trained language models caused by backdoor or bias. arXiv preprint arXiv:2305.04547 (2023).

[236] Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen, and Mykola Pechenizkiy. 2023. Chbias: Bias evaluation and mitigation of chinese conversational language models. arXiv preprint arXiv:2305.11262 (2023).

[237] Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, Kai-Wei Chang, and Ahmed Hassan Awadallah. 2020. Gender bias in multilingual embeddings and cross-lingual transfer. arXiv preprint arXiv:2005.00699 (2020).

[238] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. 2019. Gender bias in contextualized word embeddings. arXiv preprint arXiv:1904.03310 (2019).

[239] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang 2018. Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876 (2018)

[240] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).

[241] Fan Zhou, Yuzhou Mao, Liu Yu, Yi Yang, and Ting Zhong. 2023. Causal-debias: Unifying debiasing in pretrained language models and fine-tuning via causal invariant learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 4227-4241.

[242] Ran Zmigrod, Sabrina J Mielke, Hanna Wallach, and Ryan Cotterell. 2019. Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. arXiv preprint arXiv:1906.04571 (2019).


[^0]:    Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

    SIGKDD '24, August 2024, Barcelona, Spain

    (C) 2024 ACM.

    ACM ISBN 978-x-xxxx-xxxx-x/YY/MM

    https://doi.org//nnnnnnn.nnnnnnn

[^1]:    ${ }^{1}$ https://www.kaggle.com/

    ${ }^{2}$ https://datasetsearch.research.google.com/

    ${ }^{3}$ https://huggingface.co/datasets

    ${ }^{4}$ https://data.gov/

    ${ }^{5}$ https://en.wikipedia.org/wiki/Database

[^2]:    ${ }^{6}$ https://en.wikipedia.org/wiki/LGBT

    ${ }^{7}$ https://perspectiveapi.com

