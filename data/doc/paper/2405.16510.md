# Meta-Task Planning for Language Agents 

Cong Zhang ${ }^{1, *} \quad$ Derrick Goh Xin Deik ${ }^{1, *} \quad$ Dexun Li ${ }^{1} \quad$ Hao Zhang ${ }^{1} \quad$ Yong Liu $^{1}$<br>${ }^{1}$ Huawei Noah's Ark Lab<br>\{zhangcong92;goh.xin.deik;lidexun;zhang.hao3;liu.yong6\}@huawei.com


#### Abstract

The rapid advancement of neural language models has sparked a new surge of intelligent agent research. Unlike traditional agents, large language model-based agents (LLM agents) have emerged as a promising paradigm for achieving artificial general intelligence (AGI) due to their superior reasoning and generalization capabilities. Effective planning is crucial for the success of LLM agents in real-world tasks, making it a highly pursued topic in the community. Current planning methods typically translate tasks into executable action sequences. However, determining a feasible or optimal sequence for complex tasks at fine granularity, which often requires compositing long chains of heterogeneous actions, remains challenging. This paper introduces Meta-Task Planning (MTP), a zero-shot methodology for collaborative LLM-based multi-agent systems that simplifies complex task planning by decomposing it into a hierarchy of subordinate tasks, or meta-tasks. Each meta-task is then mapped into executable actions. MTP was assessed on two rigorous benchmarks, TravelPlanner and API-Bank. Notably, MTP achieved an average $\sim 40 \%$ success rate on TravelPlanner, significantly higher than the state-of-the-art (SOTA) baseline ( $2.92 \%$ ), and outperforming LLM $_{\text {api }}-4^{2}$ with ReAct on API-Bank by $\sim 14 \%$, showing the immense potential of integrating LLM with multi-agent systems.


## 1 Introduction

Autonomous agents, a.k.a., intelligent entities, excel in executing designated tasks. A key function of these agents is planning, which necessitates advanced understanding, reasoning, and decisionmaking [1]. Optimal policy discovery of intelligent agents requires comprehensive exploration and interaction with unknown, stochastic environments [2], limiting the application of supervised learning approaches due to sparse supervision signals [3]. Deep reinforcement learning (DRL) is widely used for autonomously discovering optimal control policies without supervisory signals from data or experts [4]. The efficacy of DRL-based control critically relies on developing accurate environmental models, enabling precise task understanding through its exploration capabilities. However, this requires expertise from domain specialists, limiting DRL's application in complex scenarios [5].

Recently, there has been a notable surge in the interest towards empowering agents with large language models (LLM), wherein these models function as the cognitive core of the agent [6]. Thanks to their adeptness in comprehending and executing human directives in natural language, the LLM-enhanced agent has emerged as a highly favoured paradigm. The LLM-based agent, with its exceptional logical prowess essential for strategic planning, is widely regarded as the most promising avenue thus far towards achieving artificial general intelligence (AGI) [7].[^0]

Current LLM agent planning solutions aim to map tasks to sequences of executable actions [1]. The plan-then-execute methods $[8,9]$ break down complex tasks into small, manageable sub-tasks to facilitate the inference of a sequence of executable actions. In contrast, the step-by-step methods [10, $11,12,13,14]$ interleave planning and execution, where each action is determined based on previous outcomes. The former simply assumes each sub-task can be executed with a single tool [8], but real-world applications often require tools with diverse functionalities [15]. The latter is unsuitable for time-sensitive constraints requiring comprehensive condition assessment, meticulous planning and subsequent execution. Moreover, the piecemeal nature of these approaches may lead to suboptimal outcomes and potential task failure. To improve planning stability and performance, recent studies [16, $17,18]$ integrate LLMs with external planning tools requiring task descriptions in specific formats, e.g., first-order logic [19]. However, translating various tasks into certain computational logic can be challenging and often demands a myriad of domain knowledge [20]. Existing LLM-based multi-agent systems primarily simulate human behaviors and social activities [21, 22, 23], while planning for collaborative multi-agent systems, despite their significant potential, remains underexplored.

In this work, we propose Meta-Task Planning (MTP), a zero-shot planning method for collaborative LLM-based multi-agent systems. MTP simplifies complex task planning by breaking it down into a hierarchy of subordinate tasks or meta-tasks, each achievable through a series of (heterogeneous) tool calls. Specifically, MTP comprises a manager agent for task decomposition and a fleet of executor agents to perform meta-tasks. The manager performs task-level planning by decomposing the task into a graph, where each node represents a specific meta-task and the edges delineate the dependency topology among tasks. Then each meta-task is decomposed into a sequence of function calls, i.e., step-level planning and execute, via an executor. The executor may utilize off-the-shelf planning techniques, like ReAct [11], to facilitate meta-task accomplishment. MTP can be viewed as a framework that extends the capabilities of individual LLM agents by equipping with planning cores, thus transforming them into multi-agent cooperation. In constrained scenarios, such as those with budget limitations, MTP categorizes constraints into "local" and "global" types. Local constraints are managed by executors during meta-task execution, whereas global constraints are considered in conjunction with other variables. To improve the success rate and stability, MTP employs a supervisor agent to summarize intermediate meta-task results and a deliverer agent to produce final outcomes.

Distinct from the toy tasks [24] or puzzles [25] commonly used in existing planning methods, we evaluate MTP on two real-world applications: itinerary planning and daily tool usage. Experiment results show that MTP achieves substantial performance gains on two benchmarks. Specifically, MTP obtains $\sim 50 \%$ success rate on TravelPlanner [26], a significant increase from the initially reported $0.6 \%$. It also surpasses $\operatorname{LLM}_{\text {api }}-4$ with ReAct on API-Bank [27] by $\sim 12 \%$ in absolute improvement. To the best of our knowledge, MTP is the first plan-and-execute method for collaborative LLM-based multi-agent systems addressing complex real-world tasks.

## 2 Preliminaries

The LLM-based agent is an AI system that leverages an LLM as its primary computational framework to showcase functionalities extending beyond text generation. These functionalities include engaging in dialogues, executing tasks, logical reasoning, and showcasing a level of autonomous operation. Formally, an LLM agent consists of several key elements: $A=\left(\operatorname{LLM}, \mathcal{F}_{n}, R, \mathcal{S}, C\right)$. LLM is the language model instance, e.g., LLaMA [28], utilized by the agent as the cognitive core to reason, plan, and decision making. $\mathcal{F}_{n}$ is a set of functions/actions called or taken by the agent. $R$ is the role of the agent specified by the prompt. $S \in \mathcal{S}$ is the agent's state, including its existing knowledge and internal processes. This state changes as the agent learns new information and engages with its surroundings, such as the environment or other agents. Finally, $C$ is the communication module for the agent to exchange information or knowledge with other agents. The collaborative LLM-based multi-agent system consists of multiple agents, $\left[A_{1}, A_{2}, \ldots, A_{m}\right]$, that work together to achieve a common goal. Each agent $A_{i}$ in the system has a specific role $R_{i}$ and specializes in performing a particular task, e.g., task decomposition.

## 3 Meta-Task Planning

Complex projects, such as those in construction or manufacturing, often present significant challenges, particularly with diverse, geographically dispersed teams. However, thorough planning, effective

![](https://cdn.mathpix.com/cropped/2024_06_04_a30f5131cfcc11b59e26g-03.jpg?height=553&width=1347&top_left_y=252&top_left_x=386)

Figure 1: An overview of MTP Framework. The MTP Framework provides a structured methodology for managing and executing meta-tasks within a directed meta-task graph topology, as the manager coordinates. For instance, the completion of Task ${ }_{2}$ depends on the outputs derived from Task ${ }_{1}$, which a supervisor agent subsequently consolidates. The executor agent is tasked with implementing the meta-task, considering any local constraints present. Upon completion of the meta-tasks, the deliverer agent is responsible for aggregating all meta-task outcomes to satisfy global constraints and subsequently achieve the overarching task objectives.

communication, and collaboration can ensure successful outcomes [29]. In light of this, we introduce meta-task planning (MTP), a novel zero-shot planning approach for collaborative LLM-based multiagent systems to improve the coordination among each agent. In MTP, a designated manager agent decomposes a complex task $\mathcal{T}=\left\{T_{i} \mid i \in S(K)=\{1, \cdots, K\}\right\}$ into smaller, more manageable sub-tasks, termed meta-tasks, $\left\{T_{1}, T_{2}, T_{3}, \cdots, T_{K}\right\}$ through task-level planning. These meta-tasks are then converted into a sequence of heterogeneous tool-using actions executed by a fleet of executor agents, a process referred to as step-level planning and execution. Additionally, MTP incorporates a supervisor agent to facilitate sharing synthesized meta-task outcomes among executors and a deliverer agent to consolidate final results upon the collective findings of all meta-tasks. The comprehensive framework of MTP is illustrated in Figure 1.

### 3.1 The Collaborative Multi-Agent System Design

![](https://cdn.mathpix.com/cropped/2024_06_04_a30f5131cfcc11b59e26g-03.jpg?height=233&width=290&top_left_y=1756&top_left_x=386)

(a) Manager.

![](https://cdn.mathpix.com/cropped/2024_06_04_a30f5131cfcc11b59e26g-03.jpg?height=236&width=306&top_left_y=1758&top_left_x=736)

(b) Executors.

![](https://cdn.mathpix.com/cropped/2024_06_04_a30f5131cfcc11b59e26g-03.jpg?height=236&width=304&top_left_y=1758&top_left_x=1081)

(c) Supervisor.

![](https://cdn.mathpix.com/cropped/2024_06_04_a30f5131cfcc11b59e26g-03.jpg?height=236&width=306&top_left_y=1758&top_left_x=1430)

(d) Deliverer.

Figure 2: The zero-shot prompt structure for each agent. Note both supervisor and deliverer agents do not require function calls, while different executors will have different tool lists.

### 3.1.1 Manager Agent

The manager agent has two primary objectives. Firstly, it decomposes the intricate task $\mathcal{T}$ into a set of interconnected meta-tasks $\mathcal{T}=\left\{T_{i} \mid i \in S(K)=\{1, \cdots, K\}\right\}$. These meta-tasks often exhibit dependencies, where completing one task is contingent on completing another. For instance, deciding on hotels usually depends on finalizing the trip destination. Thus, the manager must identify and define these inter-dependencies meticulously. Additionally, the manager has to assign suitable

![](https://cdn.mathpix.com/cropped/2024_06_04_a30f5131cfcc11b59e26g-04.jpg?height=377&width=1304&top_left_y=245&top_left_x=389)

Figure 3: An overview of meta-task graph, which reveals the task-level decomposition. The manager agent decomposes the main task into several meta-tasks with inter-dependencies (dashed arrows).

executors to each meta-task. Executors are viewed as a collection of composite tools, and the manager matches them based on the meta-task requirements, a method termed the executor as tools technique.

Secondly, the manager must make well-informed decisions on task assignments, especially under constraints like budget limits or specific transportation needs in travel scenarios. Some constraints can be managed during individual meta-tasks. For instance, for a meta-task that searching for accommodation, the minimum stay requirement ensures only suitable hotels are considered. The manager also needs to identify constraints that interact with other variables across meta-tasks and cannot be solved within a single meta-task. For instance, when selecting a flight, available hotel and restaurant options provided by other meta-tasks must be jointly considered. Identifying local and global constraints to divide and conquer them is crucial for successfully completing complex tasks. The formal definitions for the local and global constraints are presented as follows:

Definition 3.1. A constraint $C_{l}$ is local if and only if $\exists!i \in S(K)$ such that $C_{l}$ can be fulfilled purely based on the results of $T_{i}$.

Definition 3.2. A constraint $C_{g}$ is global if and only if $\exists \mathcal{T}_{C_{g}}=\left\{T_{i} \mid i \in S(K)\right\} \subseteq S(K)$ and $K \geq\left|\mathcal{T}_{c}\right|>1$, such that $C_{g}$ can be fulfilled based on the results of all $T_{i} \in \mathcal{T}_{C_{g}}$, where $|\cdot|$ demotes the cardinality of a set.

It is important to note that the manager identifies potential constraints and categorizes them into local and global ones exclusively based on internal knowledge. No prior information about the constraints for $\mathcal{T}$ is provided, ensuring that the zero-shot property of MTP is maintained. Figure 2(a) depicts the logic for manager prompt design.

### 3.1.2 Executor Agent

The manager agent assigns each meta-task to an executor agent, which has access to various heterogeneous tools (e.g., functions). The executor aims to create a sequence of actions (e.g., function calls) to complete the assigned meta-task while adhering to local constraint $C_{l}$. This process significantly reduces the planning complexity, as the executor focuses on a specific, well-defined task with clear requirements and constraints. Consequently, applying off-the-shelf single-agent planning methods to map a task to an execution sequence is feasible. Figure 2(b) illustrates the conceptual prompt design of the executor. Owing to the functional variation among executors, a tailored design approach is necessary for each executor, depending on the specific tools available to them.

### 3.1.3 Supervisor Agent

The role of the supervisor agent is to refine the meta-task $T_{i}$ by incorporating synthesized outcomes from neighboring meta-tasks. After the manager agent decomposes the main task, only ambiguous objectives (e.g., "Finding a hotel in city B") and inter-dependencies among meta-tasks (e.g., "Searching flight to New York" $\rightarrow$ "Finding a hotel in city B") are identified. To execute $T_{i}$ effectively, its input parameters need precise specifications. For instance, the input "Finding a hotel in city B" must be correctly updated to "Finding a hotel in New York near John F. Kennedy International Airport" based on outcomes (e.g., "Booked flight ZC9896 to New York, arriving at John F. Kennedy International Airport") from preceding meta-task (e.g., "Searching flight to New York"). To address these nuances and eliminate ambiguities, the supervisor agent acts before the commencement of $T_{i}$. It rewrites $T_{i}$

![](https://cdn.mathpix.com/cropped/2024_06_04_a30f5131cfcc11b59e26g-05.jpg?height=428&width=1265&top_left_y=266&top_left_x=403)

Figure 4: step-level Planning and Execution. The executor is furnished with a planning core and a toolbox comprising diverse functions. This includes an off-the-shelf planning algorithm such as ReAct [11], which is used to translate the meta-task into a series of executable function calls required to accomplish the assigned meta-task.

by referencing the outcomes of all neighboring meta-tasks. This ensures all necessary parameters are included and accurately instantiated.

Here, the neighbor of $T_{i}$ is defined as the collection of meta-tasks that have direct inter-dependencies with $T_{i}$. Formally, the neighborhood of $T_{i}$ is defined as:

Definition 3.3. The neighbors $\mathcal{N}\left(T_{i}\right)$ of meta-task $T_{i}$ is defined as $\left\{T_{j} \mid \forall j \in S(K)\right.$, s.t., $\left.T_{j} \rightarrow T_{i}\right\}$.

Remark: An alternative idea is to include all precedent meta-tasks in outcome synthesis for $T_{i}$ instead of just its immediate neighbors. While this seems reasonable, it can overwhelm the manager agent, especially given the input limitations of LLMs with many tasks. In contrast, our proposed approach focuses on immediate neighbors, maintaining manageability and avoiding such complexities. Moreover, our approach is capable of preserving all information through the "message-passing" mechanism, allowing correct results from precedent meta-tasks to be recursively propagated to $T_{i}$. The following proposition supports this assertion:

Proposition 3.4. A meta-task $T_{i}$ is accomplishable while adhering to local constraints if and only if all the meta-tasks within its direct neighborhood $\mathcal{N}\left(T_{i}\right)$ are accomplished with their respective local constraints maintained.

The proof is straightforward. If all tasks $T_{j} \in \mathcal{N}\left(T_{i}\right)$ are accomplished, then all the prerequisite requirements for $T_{i}$ are satisfied (since $T_{i}$ only depends on $\mathcal{N}\left(T_{i}\right)$ ), enabling the completion of $T_{i}$. Conversely, assume $T_{i}$ is accomplishable even if one of its prerequisite tasks $T_{j}$ (a direct neighbor or connected via a path $P_{a t h}$ to $T_{i}$ ) fails, this failure would propagate recursively to $T_{i}$, inevitably leading to $N_{i}$ 's failure, contradicting with the assumption that $T_{i}$ is accomplishable. The supervisor prompt design is delineated in Figure 2(c).

### 3.1.4 Deliverer Agent

The primary objective of the deliverer agent is to synthesize the outcomes of all meta-tasks while ensuring alignment with the global constraints, $\left\{C_{g}\right\}$. This synthesis is critical because $\left\{C_{g}\right\}$ can only be effectively addressed once all meta-task results are available. Thus, the deliverer agent is uniquely positioned to manage these constraints, ensuring that the final results comprehensively satisfy all global constraints. The logic of the deliverer prompt design is depicted in Figure 2(d).

The detailed prompt design and technical introduction of all agents are presented in Appendix D.

### 3.2 Hierarchical Task Planning and Execution

Task-Level Planning. The manager agent will analyze the given complex task $\mathcal{T}$ and decompose it into a series of inter-dependent meta-tasks $\left\{T_{1}, T_{2}, \ldots\right\}$. MTP represents them via a directed graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, termed meta-task graph. In $\mathcal{G}$, each node $V_{i} \in \mathcal{V}$ corresponds to a meta-task $T_{i}$ and each edge $E_{i j} \in \mathcal{E}$ delineates the dependencies between meta-tasks $T_{i}$ and $T_{j}$, where $i, j \in S(K)$. The architecture of the meta-task graph is illustrated in Figure 3. Executors adhere to the graph's topology,

Table 1: The Average Pass Rates (\%) With Hint for Instances Across All Difficulty Level. The highest final pass rates are highlighted in bold blue.

| With Unconventional Hint |  |  | $\mathrm{LLM}_{\text {api }} 3.5+\mathrm{ReAct}$ | $\operatorname{LLM}_{a p i} 4.0+\operatorname{ReAct}$ | LLM $_{\text {api }} 3.5+$ MTP | LLM $_{a p i} 4.0+$ MTP | $\mathrm{LLM}_{a p i} 4.0$ (SP) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Validation <br> Set (60) | Delivery Rate |  | 98.33 | 98.33 | 91.67 | 96.67 | 100.00 |
|  | Common- <br> sense | Micro | 74.38 | 79.38 | 63.54 | 87.29 | 92.08 |
|  |  | Macro | 0.00 | 8.33 | 1.67 | 43.33 | 50.00 |
|  | Hard <br> Constraint | Micro | 0.71 | 7.14 | 0.71 | 47.14 | 52.86 |
|  |  | Macro | 0.00 | 5.00 | 1.67 | 46.67 | 28.33 |
|  | Final Pass Rate |  | 0.00 | 1.67 | 0.00 | 33.33 | 13.33 |
| Test <br> Set $(308)$ | Delivery Rate |  | 93.50 | 98.38 | 84.09 | 97.40 | 100.00 |
|  | Common- <br> sense | Micro | 70.45 | 77.60 | 57.51 | 91.46 | 91.36 |
|  |  | Macro | 0.32 | 7.46 | 1.95 | 50.00 | 45.45 |
|  | Hard <br> Constraint | Micro | 1.21 | 13.53 | 1.37 | 53.96 | 52.74 |
|  |  | Macro | 0.32 | 9.74 | 1.30 | 45.12 | 29.22 |
|  | Final Pass Rate |  | 0.00 | 2.92 | 0.65 | 42.68 | 14.94 |

Table 2: The Average Pass Rates (\%) Without Hint for Instances Across All Difficulty Level. The highest final pass rates are highlighted in bold blue.

| Without Unconventional Hint |  |  | $\operatorname{LLM}_{a p i} 3.5+\operatorname{ReAct}$ | $\operatorname{LLM}_{a p i} 4.0+\operatorname{ReAct}$ | $\mathrm{LLM}_{a p i} 3.5+$ MTP | $\mathrm{LLM}_{a p i} 4.0+\mathrm{MTP}$ | $\mathrm{LLM}_{a p i} 4.0$ (SP) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Validation <br> Set (60) | Delivery Rate |  | 100.00 | 98.33 | 88.33 | 100.00 | 100.00 |
|  | Common- <br> sense | Micro | 73.54 | 75.21 | 58.75 | 90.00 | 90.42 |
|  |  | Macro | 0.00 | 3.33 | 0.00 | 41.67 | 35.00 |
|  | Hard <br> Constraint | Micro | 0.71 | 14.28 | 0.00 | 55.71 | 52.14 |
|  |  | Macro | 1.67 | 13.33 | 0.00 | 48.33 | 25.00 |
|  | Final Pass Rate |  | 0.00 | 0.00 | 0.00 | 31.67 | 6.67 |
| Test <br> Set $(308)$ | Delivery Rate |  | 98.38 | 96.43 | 80.19 | 98.05 | 100.00 |
|  | Common- <br> sense | Micro | 69.60 | 70.74 | 51.54 | 85.96 | 88.07 |
|  |  | Macro | 0.00 | 2.92 | 0.65 | 29.55 | 32.79 |
|  | Hard <br> Constraint | Micro | 1.37 | 14.44 | 0.91 | 50.91 | 50.91 |
|  |  | Macro | 0.65 | 9.09 | 0.32 | 46.10 | 30.84 |
|  | Final Pass Rate |  | 0.00 | 0.65 | 0.32 | 22.40 | 12.66 |

i.e., its edge orientation, to ensure all prerequisites of a meta-task are met before its initiation, thereby enhancing the efficacy of the overall task execution. Furthermore, the meta-task graph serves as a tool for visualizing the task decomposition and an interactive interface to enhance the interpretability of MTP systems. It offers a mechanism for ongoing monitoring and potential human intervention, making it essential for MTP.

Step-Level Planning and Execution. After task-level decomposition, each meta-task is manageable to be further decomposed into a sequence of executable actions, i.e., function calls. The complexity of each meta-task is significantly reduced as it is now a specific, well-defined task with clear requirements and local constraints, making the off-the-shelf planning method directly applicable, e.g., ReAct [11]. Specifically, before the commencement of meta-task $T_{i}$, the supervisor agent will rewrite $T_{i}$ by referencing the outcomes of all neighboring meta-tasks $\mathcal{N}\left(T_{i}\right)$. Then, the local constraints $C_{l}$ for meta-task $T_{i}$ identified by the manager will be given as the auxiliary information together with the refined $T_{i}$ to the executor agent $A_{i}$, which will utilize the planning method, e.g., ReAct, to accomplish $T_{i}$ by decoding $T_{i}$ into a sequence of actions. The whole process is illustrated in Figure 4.

## 4 Experiments

To assess MTP, we move beyond the existing planning methods that largely focus on simplistic tasks [24] or puzzles [25] irrelevant to practical applications. Instead, we evaluate MTP through its application to real-world scenarios. Specifically, we examine its efficacy in the domains of itinerary planning [26] and daily tools using [27]. We fine-tune MTP (prompts, etc.) for each benchmark exclusively on the validation set and apply the fine-tuned model directly to the test sets. For [27], the training dataset was employed as a proxy for the validation set owing to the lack of a dedicated validation set. For all benchmarks, each instance is executed only once without sampling, though

Table 3: The Pass Rates Constraints for the "With Unconventional Hint" scenario. The "-" marks indicate the corresponding constraints are not applicable. The highest final pass rates are highlighted in bold blue for each difficulty level.

| Constraint Type | $\operatorname{LLM}_{a p i} 4+$ ReAct + CoT |  |  | Sole-planning |  |  | $\overline{\text { MTP }}$ |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Easy | Medium | Hard | Easy | Medium | Hard | Easy | Medium | Hard |
| Commonsense Constraint |  |  |  |  |  |  |  |  |  |
| Within Sandbox | 32.79 | 23.08 | 21.95 | 90.16 | 88.46 | 82.93 | 70.49 | 75.96 | 70.73 |
| Complete Information | 81.97 | 86.54 | 86.59 | 100.00 | 100.00 | 100 | 79.51 | 78.85 | 79.27 |
| Within Current City | 95.08 | 95.19 | 90.24 | 97.54 | 95.19 | 98.78 | 97.54 | 95.19 | 97.56 |
| Reasonable City Route | 88.52 | 88.46 | 87.80 | 100.00 | 99.04 | 100 | 94.26 | 93.27 | 97.56 |
| Diverse Restaurants | 81.15 | 76.92 | 71.95 | 98.36 | 86.54 | 95.12 | 88.52 | 83.65 | 97.56 |
| Diverse Attractions | 99.18 | 98.08 | 96.34 | 100.00 | 100.00 | 100 | 97.54 | 95.19 | 100.00 |
| Non-conf. Transportation | 93.44 | 96.15 | 95.12 | 95.08 | 95.19 | 97.56 | 93.44 | 92.31 | 100.00 |
| Miminum Nights Stay | 57.38 | 67.31 | 43.90 | 62.30 | 53.85 | 53.66 | 95.90 | 93.27 | 89.02 |
| Hard Constraint |  |  |  |  |  |  |  |  |  |
| Budget | 13.93 | 10.58 | 9.76 | 50.00 | 34.62 | 14.63 | 46.72 | 42.31 | 46.34 |
| Room Rule | - | 13.16 | 17.81 | - | 47.37 | 65.75 | - | 57.89 | 57.53 |
| Cuisine | - | 11.43 | 13.51 | - | 65.71 | 45.95 | - | 62.86 | 51.35 |
| Room Type | - | 19.35 | 14.29 | - | 74.19 | 77.78 | - | 54.84 | 53.97 |
| Transportation | - | - | 15.07 | - | - | 82.19 | - | - | 60.27 |
| Final |  |  |  |  |  |  |  |  |  |
| Final Pass Rate | 4.10 | 4.10 | 2.44 | 30.33 | 5.77 | 3.66 | 43.44 | 34.62 | 42.68 |

multiple trials could potentially enhance performance. We will make the code publicly available upon acceptance.

### 4.1 Experiment Setup

### 4.1.1 Benchmarks

TravelPlanner [26]. In TravelPlanner, users specify their origin, destination, and individual requirements. The benchmark assesses the ability of language agents to (1) efficiently gather necessary information using appropriate tools and (2) create practical, personalized travel plans for users. The plan is assessed using four main metrics: (1) delivery rate (a plan has to be delivered within 30 steps), (2) commonsense constraint pass rate, (3) hard constraint pass rate, and (4) final pass rate (the rate for meeting all commonsense and hard constraints), which is the most important metric for evaluation. For (2) and (3), we define the "micro" pass rate as the ratio of passed constraints to total constraints and the "macro" pass rate as the ratio of plans passing all constraints to total plans.

The travel duration can be 3,5 , or 7 days. Due to budget constraints, we demonstrate that a 3-day dataset sufficiently justifies the effectiveness of MTP. The queries are categorized as easy, medium or hard.

However, we found that the benchmark includes odd rules as part of its evaluation. For instance, choosing the same restaurant multiple times throughout a trip breaches the Diverse Restaurants constraint, and selecting an airport as a meal location breaches the Within Sandbox constraint. Yet, under normal circumstances, it's reasonable for a tourist to return to a favoured restaurant or dine at airport restaurants during their trip. To ensure that the agent recognizes these rules as part of commonsense knowledge, we provide specific guidance to the planning agents: the Deliverer Agent in MTP and the Planner in React and Sole-Planning. To maintain the integrity of the experiment and stay true to the objectives of the original TravelPlanner, we conduct a separate experiment that excludes this external knowledge. This experiment still incorporates the less conventional rules used in both Diverse Restaurants and Within Sandbox settings.

As our method consists of tool-use and planning (two-stage), we compare our method with the twostage baseline, ReAct from [26] using $\operatorname{LLM}_{a p i}-3.5$-Turbo and $\mathrm{LLM}_{a p i}$-4-Turbo as language models. We also further compare our method to the best sole-planning baseline, Direct LLM ${ }_{a p i} 4$-Turbo, which has provided necessary information to the agent and only require agent to output the travel plan.

API-Bank [27]. API-Bank is a benchmark designed to evaluate the tool-use capabilities of large language models, focusing on APIs that are commonly used in everyday life, such as email. The

Table 4: The Performance on API-Bank. The highest performance for each criteria is highlighted in bold blue.

| Model |  | $\mathrm{LLM}_{a p i}-4-1106-$ Preview | $\operatorname{LLM}_{a p i}-4-0613$ | $\operatorname{LLM}_{a p i}-3.5-T u r b o-0125$ | LLM $_{a p i}-3.5-$ Turbo-0613 |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Correctness $\%(\uparrow)$ | COT | 71.48 | 57.65 | 35.38 | 34.74 |
|  | No COT | 41.58 | 35.11 | 67.06 | 41.80 |
|  | MTP | 82.63 | 85.12 | 74.13 | 67.81 |
| Completeness $\%(\uparrow)$ | COT | 47.76 | 46.12 | 28.16 | 30.20 |
|  | No COT | 34.29 | 32.24 | 23.27 | 20.82 |
|  | MTP | 64.08 | 58.37 | 43.27 | 40.40 |
| $\operatorname{ROUGE}(\uparrow)$ | COT | 0.2641 | 0.2846 | 0.3085 | 0.2656 |
|  | No COT | 0.2507 | 0.2644 | 0.2346 | 0.2016 |
|  | MTP | 0.4053 | 0.3839 | 0.3894 | 0.3754 |
| Tool Repeats $(\downarrow)$ | COT | 90 | 49 | 50 | 32 |
|  | No COT | 38 | 15 | 155 | 118 |
|  | MTP | 16 | 23 | 7 | 65 |

benchmark includes three levels of difficulty, with Level 3 being the most challenging. We chose Level 3 for our experiment because it best assesses the planning abilities of the agent.

The benchmark assesses agents based on Accuracy and "ROUGE" (ROUGE-L) scores. The Accuracy metric gauges the correctness of API calls based on user queries, calculated as the proportion of correct API calls to total predictions. We modified this metric for a more consistent and fair assessment by defining Correctness as the ratio of unique correct API calls to total predictions. This adjustment addresses the tendency of some language models, like $\operatorname{LLM}_{a p i}-3.5$ and $\mathrm{LLM}_{a p i}-4$, to make repetitive correct API calls. The ROUGE-L score evaluates the responses generated from these API calls. Our experiments indicate that using this refined Accuracy metric results in lower baseline scores.

In addition to Correctness, we introduce the "Completeness" to better assess task execution. Correctness alone may not fully capture an agent's performance, as minimal API calls could artificially inflate scores. Completeness measures the ratio of unique, correct API calls to the total required API calls for the task, addressing the limitations of Correctness and ensuring a more accurate evaluation of the agent's effectiveness. We also introduce another metric named "Tool Repeats", which measures how often the model correctly calls an API after its initial use. A lower number of repeats indicates fewer unnecessary inferences, signifying a more efficient solution.

### 4.2 Result Analysis

### 4.2.1 Result Analysis for TravelPlanner.

From Tables 1 and 2, it is evident that MTP significantly outperforms all baseline methods irrespective of the presence of unconventional hints. Notably, when hints are included, LLM $_{a p i} 4$ enhanced by MTP achieves a superior average final pass rate of $42.68 \%$ across all difficulty levels, compared to a meagre $2.92 \%$ by baselines. This data underscores the potential of integrating large language models (LLMs) with multi-agent systems, marking it as a promising area for future research in LLM-based agent systems.

In the absence of hints, the setting replicates that described in [26], where the highest final pass rate for baseline models stands at $0.56 \%$, consistent with the original study's findings. In this scenario, MTP significantly improves with an average final pass rate of $22.4 \%$, surpassing the best-reported baseline result in [26].

Notably, the Meta-Task Planner (MTP) significantly outperforms the Standard Planner (SP) in settings that employ hints and those that do not. The SP operates purely as a decision-making framework in which all elements necessary for completing the itinerary, such as multiple choices for hotels, flights, and restaurants, are pre-supplied; thus, the SP agent merely selects the most suitable options from these pre-defined sets to construct the final itinerary. This renders SP a relatively simpler task compared to MTP and other benchmarks, which necessitate the searching and gathering of necessary elements prior to decision-making. Nonetheless, MTP achieves a superior final pass rate, a finding which may appear counter-intuitive yet can be elucidated as follows: MTP's exceptional performance is attributable to its effective deployment of a divide-and-conquer strategy in managing constraints. By resolving numerous local constraints during the execution of meta-tasks, MTP considerably reduces the complexity that the agent encounters in formulating the ultimate itinerary plan. Table 3 presents
the detailed pass rates for individual constraints, indicating that MTP significantly outperforms $\mathrm{LLM}_{a p i} 4+\operatorname{ReAct}+\mathrm{CoT}$ in terms of pass rates across all constraints. However, $\operatorname{LLM}_{\text {api }} 3.5$ is less effective than $\operatorname{LLM}_{a p i} 4$ when equipped with MTP, possibly due to less model performance. We have provided illustrative results for each difficulty level in Appendix E. We also present the detailed results for each difficulty level (easy, medium, hard) in Appendix C.

### 4.2.2 Result Analysis for API-Bank.

Firstly, MTP significantly enhances the performance of both $\operatorname{LLM}_{\text {api }} 4$ and $\operatorname{LLM}_{\text {api }} 3.5$ across all critical evaluation metrics. Compared to existing baselines, MTP consistently demonstrates superior performance. Notably, the best performance reported in the original paper [27] achieved a $70 \%$ success rate, which our reimplementation slightly exceeds at $71.48 \%$. Thus, MTP stands out by surpassing the top method referenced in [27] by a substantial margin of at least $14 \%$. Furthermore, MTP excels in other key areas such as task completeness, achieving an impressive $64.08 \%$, and exhibits significantly fewer redundant tool interactions, with a count of just seven. This robust performance underscores MTP's potential in redefining the capabilities of advanced language models. We have provided illustrative results for each difficulty level in Appendix E.

## 5 Literature Review for Language Model Agent Planning

The emergence of LLMs introduces new paradigms for agents [30, 31, 32], demonstrating significant intelligence in reasoning [33, 34, 35], planning [11, 36, 37], instruction-following [38, 39, 40], and tool-usage $[8,41,42]$ across various domains. Planning acts as an essential capability to interact with external environments, which involves organizing thought trajectories, setting objectives, and determining steps to accomplish the objectives [43]. Some work [9, 11, 12, 34] focuses on task decomposition, aiming to solve complex tasks in a divide-and-conquer manner. The plan selection methods $[35,36,37,44]$ elicit LLMs to generate various alternative plans for a task following by a search algorithm for optimal plan selection and execution. Recent studies [45, 46, 47, 48] also explore to enhance LLM's planning ability via reflection and refinement strategies. Moreover, some work $[49,50,51]$ also introduces external planners to aid the planning procedure of LLMs.

Numerous strategies have been developed to harness the potentials of LLMs for specific agent planning [52], whose effectiveness and accuracy of planning significantly determine the agent's robustness and usability. Web-agents $[53,54,55,56]$ explore the interaction between LLM and web-environment by simulating human's web-browsing behaviors via RL-based planning or trajectory planning. General tool-agents require to interact with massive APIs or tools, making the planning procedures more challenging. Solutions to tool-agent planning usually rely on various task decomposition [8, 57], self-rectification [58] and domain-reasoning [59] strategies. Other task-specific agents focus on designing sophisticated planning strategies, such as tree search [60] and Bayesian adaptive MDPs [61]. Multi-agent systems $[62,63,64,65]$ seek to solve more complex real-world tasks by combining multiple powerful LLM-based agents. Existing solutions mainly focus on tackling the complexities inherent in integrating heterogeneous agents with different capabilities and specializations [65], while the planning strategies among these agents are overlooked. In contrast, our MTP focuses on designing generalized, robust planning strategies for multi-agent systems. Although LLM $_{a p i}$ Swarm [66] shares a similar concept with MTP, it focuses on visualizing multi-agent collaboration via composite graphs to aid prompt tuning, while our MTP is a planning algorithm specifically designed for systems with multiple collaborative agents.

## 6 Conclusion

This paper presents Meta-Task Planning (MTP), an innovative zero-shot methodology for collaborative LLM-based multi-agent systems. MTP simplifies complex task planning by breaking it down into hierarchical meta-tasks, each mapped to executable actions. MTP was evaluated on two benchmarks, TravelPlanner and API-Bank. It achieved an average success rate of about $42 \%$ on TravelPlanner, a significant improvement from the initial $0.6 \%$, and outperformed $\mathrm{LLM}_{a p i}-4$ with ReAct by $14 \%$ on API-Bank. However, the current design still requires human input from executor agents. Enhancing MTP by enabling the manager agent to autonomously design prompts for executor agents could optimize executor creation, accelerating MTP's practical application. Future research should focus on developing more autonomous agents through advanced prompt optimization, as suggested in recent
literature [67]. This approach promises to refine MTP's functionality and expand its applicability without human intervention, leading to more intelligent and self-sufficient multi-agent systems.

## References

[1] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey. ArXiv, $\mathrm{abs} / 2402.02716,2024$.

[2] Michael Wooldridge. Intelligent agents. Multiagent systems: A modern approach to distributed artificial intelligence, 1:27-73, 1999 .

[3] Dominic Wong. A critical literature review on e-learning limitations. Journal for the Advancement of Science and Arts, 2(1):55-62, 2007.

[4] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

[5] Xu Wang, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai, and Qiguang Miao. Deep reinforcement learning: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.

[6] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):1-26, 2024.

[7] Jiaxuan You, Ge Liu, Yunzhu Li, Song Han, and Dawn Song. How far are we from agi. In ICLR 2024 Workshops, 2024.

[8] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36, 2024.

[9] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and EePeng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2609-2634, Toronto, Canada, July 2023. Association for Computational Linguistics.

[10] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.

[11] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023.

[12] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023.

[13] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.

[14] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 10764-10799. PMLR, 23-29 Jul 2023.

[15] Narayanan Krishnakumar and Amit Sheth. Managing heterogeneous multi-system tasks to support enterprise-wide operations. Distributed and Parallel Databases, 3:155-186, 1995.

[16] Gautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint arXiv:2308.06391, 2023.

[17] Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Advances in Neural Information Processing Systems, 36:79081-79094, 2023.

[18] Zhun Yang, Adam Ishay, and Joohyung Lee. Coupling large language models with logic programming for robust and general reasoning from text. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023.

[19] Jon Barwise. An introduction to first-order logic. In Studies in Logic and the Foundations of Mathematics, volume 90, pages 5-46. Elsevier, 1977.

[20] Andrew Cropper and Sebastijan Dumančić. Inductive logic programming at 30: a new introduction. Journal of Artificial Intelligence Research, 74:765-850, 2022.

[21] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large language model society. Advances in Neural Information Processing Systems, 36, 2024.

[22] Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages $1-22,2023$.

[23] Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong Li. Large language models empowered agent-based modeling and simulation: A survey and perspectives. arXiv preprint arXiv:2312.11970, 2023.

[24] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523-11530. IEEE, 2023.

[25] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024.

[26] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: A benchmark for real-world planning with language agents. ArXiv, $\mathrm{abs} / 2402.01622,2024$.

[27] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. API-bank: A comprehensive benchmark for tool-augmented LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3102-3116, Singapore, December 2023. Association for Computational Linguistics.

[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[29] Eur Ing Albert Lester. Chapter 20 - planning blocks and subdivision of blocks. In Eur Ing Albert Lester, editor, Project Management, Planning and Control (Seventh Edition), pages 131-142. Butterworth-Heinemann, seventh edition edition, 2017.

[30] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. A survey of chain of thought reasoning: Advances, frontiers and future. ArXiv, abs/2309.15402, 2023.

[31] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6), March 2024.

[32] Tula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging ai agent architectures for reasoning, planning, and tool calling: A survey. ArXiv, abs/2404.11584, 2024.

[33] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199-22213. Curran Associates, Inc., 2022.

[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2023.

[35] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023.

[36] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[37] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, page 17682-17690. Association for the Advancement of Artificial Intelligence (AAAI), March 2024.

[38] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. ArXiv, abs/2304.12244, 2023.

[39] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics.

[40] Jie Ren, Yao Zhao, Tu Vu, Peter J. Liu, and Balaji Lakshminarayanan. Self-evaluation improves selective generation in large language models. ArXiv, abs/2312.09300, 2023.

[41] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv, abs/2302.04761, 2023.

[42] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 71995-72007. Curran Associates, Inc., 2023.

[43] Marcelo G. Mattar and Máté Lengyel. Planning in the brain. Neuron, 110(6):914-934, 2022.

[44] Hengjia Xiao and Peng Wang. Llm a*: Human in the loop large language models enabled a* search for robotics. ArXiv, abs/2312.01797, 2023.

[45] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. ArXiv, $\mathrm{abs} / 2303.11366,2023$.

[46] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[47] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. ArXiv, abs/2207.05608, 2022.

[48] Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2024.

[49] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+p: Empowering large language models with optimal planning proficiency. ArXiv, $\mathrm{abs} / 2304.11477,2023$.

[50] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. In Advances in Neural Information Processing Systems, volume 36, pages 23813-23825. Curran Associates, Inc., 2023.

[51] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):19632-19642, Mar. 2024.

[52] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey. ArXiv, abs/2309.07864, 2023.

[53] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 20744-20757. Curran Associates, Inc., 2022.

[54] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 28091-28114. Curran Associates, Inc., 2023.

[55] Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. In The Twelfth International Conference on Learning Representations, 2024.

[56] Hiroki Furuta, Yutaka Matsuo, Aleksandra Faust, and Izzeddin Gur. Exposing limitations of language model agents in sequential-task compositions on the web. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024.

[57] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. ArXiv, $\mathrm{abs} / 2401.06201,2024$.

[58] Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, Aixin Sun, Hany Awadalla, and Weizhu Chen. Sciagent: Tool-augmented language models for scientific reasoning. ArXiv, abs/2402.11451, 2024.

[59] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[60] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. ArXiv, abs/2310.04406, 2023.

[61] Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, and Zhaoran Wang. Reason for future, act for now: A principled framework for autonomous llm agents with provable sample efficiency. ArXiv, abs/2309.17382, 2023.

[62] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2024.

[63] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024.

[64] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. Mindagent: Emergent gaming interaction. ArXiv, abs/2309.09971, 2023.

[65] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent operating system. ArXiv, abs/2403.16971, 2024.

[66] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jurgen Schmidhuber. Language agents as optimizable graphs. arXiv preprint arXiv:2402.16823, 2024.

[67] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2023.
