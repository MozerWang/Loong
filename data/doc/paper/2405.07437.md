# Evaluation of Retrieval-Augmented Generation: A Survey 

Hao $\mathrm{Yu}^{1,2}$, Aoran Gan ${ }^{3}$, Kai Zhang ${ }^{3}$, Shiwei Tong ${ }^{1 \dagger}$, Qi $\mathrm{Liu}^{3}$, and Zhaofeng $\mathrm{Liu}^{1}$<br>1 Tencent Company<br>2 McGill University<br>3 State Key Laboratory of Cognitive Intelligence,<br>University of Science and Technology of China<br>hao.yu2@mail.mcgill.ca<br>gar@mail.ustc.edu.cn<br>\{shiweitong ${ }^{\dagger}$, zhaofengliu\}@tencent.com<br>\{kkzhang08, qiliuql\}@ustc.edu.cn


#### Abstract

Retrieval-Augmented Generation (RAG) has emerged as a pivotal innovation in natural language processing, enhancing generative models by incorporating external information retrieval. Evaluating RAG systems, however, poses distinct challenges due to their hybrid structure and reliance on dynamic knowledge sources. We consequently enhanced an extensive survey and proposed an analysis framework for benchmarks of RAG systems, RGAR (Retrieval, Generation, Additional Requirement), designed to systematically analyze RAG benchmarks by focusing on measurable outputs and established truths. Specifically, we scrutinize and contrast multiple quantifiable metrics of the Retrieval and Generation component, such as relevance, accuracy, and faithfulness, of the internal links within the current RAG evaluation methods, covering the possible output and ground truth pairs. We also analyze the integration of additional requirements of different works, discuss the limitations of current benchmarks, and propose potential directions for further research to address these shortcomings and advance the field of RAG evaluation. In conclusion, this paper collates the challenges associated with RAG evaluation. It presents a thorough analysis and examination of existing methodologies for RAG benchmark design based on the proposed RGAR framework.


## 1 Introduction

Retrieval-Augmented Generation (RAG) [29] represents a significant advancement in natural language processing by enhancing the performance of generative models through integrating information retrieval techniques. It addresses a critical challenge faced by standalone generative models: the tendency to produce responses that, while plausible, may not be grounded in facts. By retrieving relevant information from external sources,[^0]

RAG significantly reduces the incidence of hallucinations [20] or factually incorrect generative outputs, thereby improving the content's reliability and richness. [56] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a development in the pursuit of more intelligent and versatile language models [56|52].

![](https://cdn.mathpix.com/cropped/2024_06_04_b259d408d00def5d63c2g-02.jpg?height=513&width=1215&top_left_y=730&top_left_x=455)

Fig. 1: The structure of the RAG system with retrieval and generation components and corresponding four phrases: indexing, search, prompting and inferencing. The pairs of EOs and GTs are highlighted in red and green, with brown dashed arrows.

Numerous studies of RAG systems have emerged from various perspectives since the advent of pre-trained language models [14]. The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from a vast array of external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [14|11|24]. The searching utilizes these indexes to fetch relevant documents on the user's query, often incorporating the optional rerankers [4|34|5)43] to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content to formulate coherent and contextually relevant responses with the prompting and inferencing phases. The input for language models is formulated through prompting, integrating the query from the retrieval phase. Methods like Chain of Thought (CoT) [48] or Rephrase and Respond (RaR) [7] guide better generation results. In the inferencing step, Large Language Models (LLMs) interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information [30|8]. The RAG structure is explained in detail in Appendix A Figure 1 illustrates the structure of the RAG systems as mentioned.

The significance of evaluating RAG is escalating in tandem with the advancement of RAG-specific methodologies. On the one hand, RAG is a complex system intricately tied to specific requirements and language models, resulting in a variety of evaluation
methods, indicators, and tools, particularly given the black-box LLM generation. Evaluating RAG systems thus involves considering quite a few specific components and the complexity of overall system assessment. On the other hand, the complexity of RAG systems is further compounded by the dynamic external database and the various downstream tasks, such as content creation or open domain question answering [14|53]. These challenges necessitate the development of comprehensive evaluation metrics that can effectively capture the interplay between retrieval accuracy and generative quality [2|6]. To clarify the elements further, we conducted this survey on RAG evaluation to address the current gaps in the area, which differs from the prior RAG surveys [57|14|21] that predominantly collected specific RAG methods or data. We have compiled 12 distinct evaluation frameworks, encompassing a range of aspects of the RAG system. We conduct a comparative analysis and synthesize the specific evaluation methods of various components, focusing on aspects such as accuracy, faithfulness, and relevance. We also discuss the constraints of the existing methodology and the prospects for future RAG evaluations. We hope to provide the readers with a comprehensive understanding of the RAG evaluation.

For this paper, we contribute in the following aspects:

1. Challenge of Evaluation: This is the first work that summarize and classify the challenges in evaluating RAG systems through the structure of RAG systems, including three parts retrieval, generation, and the whole system.
2. Analysis Framework: Based on the challenges, we propose an analysis framework $(R G A G)$ for RAG benchmarks, which is designed to navigate the unique complexities inherent to RAG systems, offering a fundamental methodology for assessing their efficacy across many facets.
3. RAG Benchmark Analysis: With the help of the RGAG framework, we provide a comprehensive analysis of existing RAG benchmarks, highlighting their strengths and limitations and proposing recommendations for future developments in RAG system evaluation.

## 2 Challenges in Evaluating RAG Systems

Evaluating hybrid RAG systems entails evaluating retrieval, generation and the RAG system as a whole. These evaluations are multifaceted, requiring careful consideration and analysis. Each of them encompasses specific difficulties that complicate the development of a comprehensive evaluation framework and benchmarks for RAG systems.

Retrieval The retrieval component of RAG systems is critical for fetching relevant information from external knowledge sources that inform the generation process. One primary challenge in evaluating this component is the dynamic and vast nature of potential knowledge bases, ranging from structured databases to the entire web. This vastness requires evaluation metrics that can effectively measure the precision, recall, and relevance of retrieved documents in the context of a given query [43|27]. Moreover, the temporal aspect of information, where the relevance and accuracy of data can change over time, adds another layer of complexity to the evaluation process [5]. Additionally,
the diversity of information sources and the potential for retrieving misleading or lowquality information poses significant challenges in assessing the retrieval component's effectiveness in filtering and selecting the most pertinent information [34]. On the other hand, the evaluation method of the retrieval component is also a challenge, as the traditional evaluation metrics enforce the retrieval system focus on higher TopK recall, instead of the useful information carried through one query.

Generation The generation component, currently powered by LLMs, produces coherent and contextually appropriate responses based on the retrieved content. A significant challenge here lies in evaluating the faithfulness and accuracy of the generated content to the input data. This involves not only assessing the factual correctness of responses but also their relevance to the original query and the coherence of the generated text [58 41]. The subjective nature of certain tasks, such as creative content generation or open-ended question answering, further complicates the evaluation, as it introduces variability in what constitutes a 'correct' or 'high-quality' response [40].

RAG System as a Whole Evaluating the whole RAG system introduces additional complexities. The interplay between the retrieval and generation components means that the entire system's performance cannot be fully understood by evaluating each component in isolation [41|13]. The system needs to be assessed on its ability to leverage retrieved information effectively to improve response quality, which involves measuring the added value of the retrieval component to the generative process. Furthermore, practical considerations such as response latency, robustness against misinformation, and the ability to handle ambiguous or complex queries are also crucial for evaluating the system's overall effectiveness and usability [34|5].

## 3 RGAR: Analysis Framework for Evaluation

Evaluating the target shift from traditional absolute numeric metrics to multi-source and multi-target generation evaluation, along with the intricate interplay between retrieval and generation components, poses significant challenges. Searches in a dynamic database may lead to misleading results or contradicting the facts. Diverse and comprehensive datasets that accurately reflect real-world scenarios are crucial. Challenges also arise in the realm of metrics, encompassing generative evaluation criteria for distinct downstream tasks, human preferences, and practical considerations within the RAG system. Most prior benchmarks predominantly tackle one or several aspects of the RAG assessment but lack a comprehensive, holistic analysis.

To provide a better understanding of RAG benchmarks, we propose an analysis framework named $\boldsymbol{R} \boldsymbol{G A R}$ (Retrieval, Generation, and Additional Requirement). It takes into account the Target, Dataset, and Metric respectively. The Target module is intended to determine the evaluation direction. The Dataset module facilitates the comparison of various data constructions in RAG benchmarks. The final module, Metrics, introduces the metrics that correspond to specific targets and datasets used during evaluation. Overall, it is designed to provide a systematic methodology for assessing the effectiveness of RAG systems across various aspects by covering all possible pairs between the "Evaluable Outputs" (EOs) and "Ground Truths" (GTs). In the following
section, we will explain thoroughly the framework and utilize it for introducing and comparing the RAG benchmarks.

## $\boldsymbol{R} \boldsymbol{G A R}$ Framework

Output

![](https://cdn.mathpix.com/cropped/2024_06_04_b259d408d00def5d63c2g-05.jpg?height=412&width=398&top_left_y=735&top_left_x=473)

Target

Ground Truth

## Retrieval

Cs $\Leftrightarrow$ Query

Relevance
Relevant Docs $\Leftrightarrow$ Docs Candidates

Accuracy

## Generation

Response $\Leftrightarrow$ Query

Relevance

$\begin{aligned} \text { Response } \Leftrightarrow & \text { Relevant Docs } \\ & \text { Faithfulness }\end{aligned}$

Response $\Leftrightarrow$ Sample Response

Correctness

----------1

Additional Requirements

Latency

Diversity

Noise Robustness

Negative Rejection

Counterfactual Robustness

Output
Docs Candidates

Sample Response $\qquad$

![](https://cdn.mathpix.com/cropped/2024_06_04_b259d408d00def5d63c2g-05.jpg?height=40&width=68&top_left_y=1155&top_left_x=1374)
didates

![](https://cdn.mathpix.com/cropped/2024_06_04_b259d408d00def5d63c2g-05.jpg?height=244&width=46&top_left_y=916&top_left_x=1645)

Generation The similar pairwise relations for the generation components are listed below. The EOs are the generated text and phrased structured content. Then we need to compare these EOs with the provided GTs and labels.

- Relevance (Response $\leftrightarrow$ Query) measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query's specific requirements.

| Category | Framework | Time | Raw Targets | Retrieval | Generation |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Tool | TruEra RAG Triad 45 | 2023.10 | Context Relevance <br> Answer Relevance <br> Groundedness | LLM as Judge | LLM as Judge |
| Tool | LangChain Bench. 27 | 2023.11 | Accuracy <br> Faithfulness <br> Execution Time <br> Embed. CosDistance | Accuracy | LLM as Judge |
| Tool | Databricks Eval 28 | 2023.12 | Correctness <br> Readability <br> Comprehensiveness | - | LLM as Judge |
| Benchmark | RAGAs 13 | 2023.09 | Context Relevance <br> Answer Relevance <br> Faithfulness | LLM as Judge | LLM Gen + CosSim <br> LLM as Judge |
| Benchmark | RECALL 33 | 2023.11 | Response Quality <br> Robustness | - | BLEU, ROUGE-L |
| Benchmark | ARES 41 | 2023.11 | Context Relevance <br> Answer Faithfulness <br> Answer Relevance | LLM + Classifier | LLM + Classifier <br> LLM + Classifier |
| Benchmark | RGB [5] | 2023.12 | Information Integration <br> Noise Robustness <br> Negative Rejection <br> Counterfactual Robustness | - | Accuracy |
| Benchmark | MultiHop-RAG 43 | 2024.01 | Retrieval Quality <br> Response Correctness | MAP, MRR, Hit@K | LLM as Judge |
| Benchmark | CRUD-RAG 34 | 2024.02 | CREATE, READ <br> UPDATE, DELETE | - | ROUGE, BLEU <br> RAGQuestEval |
| Benchmark | MedRAG 49 | 2024.02 | Accuracy | - | Accuracy |
| Benchmark | FeB4RAG 47 | 2024.02 | Consistency <br> Correctness <br> Clarity <br> Coverage | - | Human Evaluation <br> Human Evaluation |
| Benchmark | CDQA 50 | 2024.03 | Accuracy | - | F1 of Tokens |
| Research | FiD-Light 18 | 2023.07 | Latency | - | - |
| Research | Diversity Reranker 4 | 2023.08 | Diversity | - | - |

Table 1: The evaluating targets and corresponding metrics across various frameworks for evaluating RAG systems. The presentation distinguishes between the core areas of Retrieval and Generation considered in the evaluation. The different aspects of the evaluation are set as different colours in the table: Relevance, Accuracy of Retrieval and Faithfulness, Correctness and Relevance of Generation. The consideration of the Additional Requirements beyond the retrieval and generation component is also collected. Noted that quite a few of the works employed multiple methods or evaluated multiple aspects simultaneously.

- Faithfulness (Response $\leftrightarrow$ Relevant Documents) evaluates if the generated response accurately reflects the information contained within the relevant documents and measures the consistency between generated content and the source documents.
- Correctness (Response $\leftrightarrow$ Sample Response) Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.

The targets of Retrieval and Generation components are introduced. Table 1 lists the relative work about improving and evaluating RAG and RAG benchmarks cut off in March 2024. Table 1 portrays this information, where each evaluation criterion is represented by a different colour. For example, FeB4RAG [47], the fourth from the last, has posited four standards based on [15] that comprise Consistency, Correctness, Clarity, and Coverage. Correctness is equivalent to accuracy in retrieval, and Consistency is tantamount to faithfulness in the generation component. While accuracy in retrieval gauges the correctness of the retrieved information, we posit that Coverage pertains to the coverage rate and is more associated with diversity. Therefore, we consider Coverage to be linked with diversity and an additional requirement in our proposed evaluation framework, which will be introduced subsequently. The remaining standard, Clarity, is also classified as an additional requirement in our proposed framework. The other tools and benchmarks are processed similarly.

Tools and benchmarks offer varying degrees of flexibility in evaluating datasets for RAG systems. Tools, which specify only evaluation targets, provide a versatile framework capable of constructing complete RAG applications and evaluation pipelines, as seen in works like [45|27|28]. Benchmarks, on the other hand, focus on different aspects of RAG evaluation with specific emphasis on either retrieval outputs or generation targets. For instance, RAGAs and ARES assess the relevance of retrieval documents, while RGB and MultiHop-RAG [5]43] prioritize accuracy, necessitating comparison with GTs. All benchmarks consider generation targets due to their critical role in RAG systems, though their focus areas vary.

Additional Requirement In addition to evaluating the two primary components outlined, a portion of the works also addressed some additional requirements of RAG (Black and Ltatic targets in the Table 2 . The requirements are as follows:

- Latency [1827] measures how quickly the system can find information and respond, crucial for user experience.
- Diversity [4[27] checks if the system retrieves a variety of relevant documents and generates diverse responses.
- Noise Robustness [5] assesses how well the system handles irrelevant information without affecting response quality.
- Negative Rejection [5] gauges the system's ability to refrain from providing a response when the available information is insufficient.
- Counterfactual Robustness [5] evaluates the system's capacity to identify and disregard incorrect information, even when alerted about potential misinformation.
- More: For more human preferences considerations, there can be more additional requirements, such as readability [47|28], toxicity, perplexity [28], etc.

For the exception, CRUD-RAG [34] introduces a comprehensive benchmark addressing the broader spectrum of RAG applications beyond question-answering, categorized into Create, Read, Update, and Delete scenarios. This benchmark evaluates RAG systems across diverse tasks, including text continuation, question answering, hallucination modification, and multi-document summarization, each corresponding to CRUD actions. It emphasizes the evaluation of all RAG components, including retrieval models and external knowledge base construction, offering insights for optimizing RAG technology across different scenarios.

### 3.2 Evaluation Dataset (How to evaluate?)

| Benchmark | Dataset |
| :---: | :---: |
| RAGAs [13] | WikiEval |
| RECALL [33] | EventKG [17], UJ [19] |
|  | NQ [25], Hotpot [51], |
| ARES [41] | FEVER [44], WoW [10], |
|  | MultiRC [9], ReCoRD [54] |
| RGB [5] | Generated (Source: News) |
| MultiHop-RAG [43] | Generated (Source: News) |
| CRUD-RAG [34] | Generated (Source: News) |
| MedRAG [49] | UHGEval [31] |
| FeB4RAG [47] | MIRAGE |
| CDQA [50] | Generations (Source: News), Labeller |

Table 2: The evaluation datasets used for each benchmark. The dataset without citation was constructed by the benchmark itself.

In Table 2, distinct benchmarks employ varying strategies for dataset construction, ranging from leveraging existing resources to generating entirely new data tailored for specific evaluation aspects. Several benchmarks draw upon the part of KILT (Knowledge Intensive Language Tasks) benchmark [37] (Natural Questions [25], HotpotQA [51], and FEVER [44])and other established datasets such as SuperGLUE [46] (MultiRC [9], and ReCoRD [54]) [41]. However, the drawback of using such datasets can't solve the challenges in dynamic real-world scenarios. A similar situation can be observed in WikiEval constructed by RAGAs [13], which was generated from Wikipedia pages post 2022 .

The advent of powerful LLMs has revolutionized the process of dataset construction. With the ability to design queries and ground truths for specific evaluation targets using these frameworks, authors can now create datasets in the desired format with ease. Benchmarks like RGB, MultiHop-RAG, CRUD-RAG, and CDQA [5|43|34|50] have taken this approach further by building their own datasets using online news articles to test RAG systems' ability to handle real-world information beyond the training data of LM frameworks.

In summary, the creation and selection of datasets are crucial for evaluating RAG systems. Datasets tailored for specific metrics or tasks improve evaluation accuracy and guide the development of adaptable RAG systems for real-world information needs.

### 3.3 Evaluation Metric (How to quantify?)

Navigating the intricate terrain of evaluating RAG systems necessitates a nuanced understanding of the metrics that can precisely quantify the evaluation targets. However, creating evaluative criteria that align with human preferences and address practical considerations is challenging. Each component within the RAG systems requires a tailored evaluative approach that reflects its distinct functionalities and objectives.

Retrieval Metrics Various targets can be evaluated with various metrics that correspond to the given datasets. This section will introduce several commonly used metrics for retrieval and generation targets. The metrics for additional requirements can also be found in these commonly used metrics. The more specifically designed metrics can be explored in the original paper via Table 1 as a reference.

For the retrieval evaluation, the focus is on metrics that can accurately capture the relevance, accuracy, diversity, and robustness of the information retrieved in response to queries. These metrics must not only reflect the system's precision in fetching pertinent information but also its resilience in navigating the dynamic, vast, and sometimes misleading landscape of available data. The deployment of metrics like Misleading Rate, Mistake Reappearance Rate, and Error Detection Rate within the [33] benchmark underscores a heightened awareness of RAG systems' inherent intricacies. The integration of MAP@K, MRR@K, and Tokenization with F1 into benchmarks like [43|50] mirrors a deepening comprehension of traditional retrieval's multifaceted evaluation. While the [15] also emphasizes that this ranking-based evaluation methodology is not unsuitable for the RAG system, and should have more RAG-specific retrieval evaluation metrics. These metrics not only capture the precision and recall of retrieval systems but also account for the diversity and relevance of retrieved documents, aligning with the complex and dynamic nature of information needs in RAG systems. The introduction of LLMs as evaluative judges, as seen in [13], further underscores the adaptability and versatility of retrieval evaluation, offering a comprehensive and context-aware approach to assessing retrieval quality.

Non-Rank Based Metrics Non-rank-based metrics often assess binary outcomes-whether an item is relevant or not-without considering the position of the item in a ranked list. Notice, that the following formula is just one format of these metrics, the definition of each metric may vary by the different evaluating tasks.

- Accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined.

$$
\text { Accuracy }=\frac{T P+T N}{T P+T N+F P+F N}
$$

where $T P$ is the number of true positives, $T N$ is the number of true negatives, $F P$ is the number of false positives, and $F N$ is the number of false negatives.

- Precision is the fraction of relevant instances among the retrieved instances.

$$
\text { Precision }=\frac{T P}{T P+F P}
$$

where $T P$ represents true positives and $F P$ represents false positives.

- Recall at $\mathrm{k}($ Recall@ $k$ ) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances, considering only the top $k$ results.

$$
\text { Recall@k}=\frac{\left|R D \cap T o p_{k d}\right|}{|R D|}
$$

where $R D$ is the relevant documents, and $T o p_{k d}$ is the top-k retrieved documents.

Rank-Based Metrics Rank-based metrics evaluate the order in which relevant items are presented, with higher importance placed on the positioning of relevant items at the top of the ranking list.

- Mean Reciprocal Rank (MRR) is the average of the reciprocal ranks of the first correct answer for a set of queries.

$$
M R R=\frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\operatorname{rank} k_{i}}
$$

where $|Q|$ is the number of queries and $\operatorname{rank}_{i}$ is the rank position of the first relevant document for the $i$-th query.

- Mean Average Precision (MAP) is the mean of the average precision scores for each query.

![](https://cdn.mathpix.com/cropped/2024_06_04_b259d408d00def5d63c2g-10.jpg?height=135&width=598&top_left_y=1521&top_left_x=796)

where $P(k)$ is the precision at cutoff $k$ in the list, $\operatorname{rel}(k)$ is an indicator function equaling 1 if the item at rank $k$ is a relevant document, 0 otherwise, and $n$ is the number of retrieved documents.

Generation Metrics In the realm of generation, evaluation transcends the mere accuracy of generated responses, venturing into the quality of text in terms of coherence, relevance, fluency, and alignment with human judgment. This necessitates metrics that can assess the nuanced aspects of language production, including factual correctness, readability, and user satisfaction with the generated content. The traditional metrics like BLEU, ROUGE, and F1 Score continue to play a crucial role, emphasizing the significance of precision and recall in determining response quality. Yet, the advent of metrics such as Misleading Rate, Mistake Reappearance Rate, and Error Detection Rate highlights an evolving understanding of RAG systems' distinct challenges [33].

The evaluation done by humans is still a very significant standard to compare the performance of generation models with one another or with the ground truth. The
approach of employing LLMs as evaluative judges [58] is a versatile and automatic method for quality assessment, catering to instances where traditional ground truths may be elusive [13]. This methodology benefits from employing prediction-powered inference (PPI) and context relevance scoring, offering a nuanced lens through which LLM output can be assessed. [41] The strategic use of detailed prompt templates ensures a guided assessment aligned with human preferences, effectively standardizing evaluations across various content dimensions [1]. This shift towards leveraging LLMs as arbiters mark a significant progression towards automated and context-responsive evaluation frameworks, enriching the evaluation landscape with minimal reliance on reference comparisons.

- ROUGE Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [32] is a set of metrics designed to evaluate the quality of summaries by comparing them to human-generated reference summaries. The variants of ROUGEs measure the overlap of n-grams (ROUGE-N, ROUGGE-W), word subsequences (ROUGE-L, ROUGGE-S), and word pairs between the system-generated summary and the reference summaries. ROUGE can be indicative of the content overlap between the generated text and the reference text.
- BLEU Bilingual Evaluation Understudy (BLEU) [36] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. BLEU has been applied to other generation tasks. Despite its popularity, BLEU has limitations, such as not accounting for the fluency or grammaticality of the generated text.
- BertScore BertScore [55] leverages the contextual embedding from pre-trained transformers like BERT to evaluate the semantic similarity between generated text and reference text. BertScore computes token-level similarity using contextual embedding and produces precision, recall, and F1 scores. Unlike n-gram-based metrics, BertScore captures the meaning of words in context, making it more robust to paraphrasing and more sensitive to semantic equivalence.
- LLM as a Judge Using LLMs as judges for evaluating generated text is a more recent approach. [58] In this method, LLMs are used to score the generated text based on criteria such as coherence, relevance, and fluency. The LLM can be fine-tuned on human judgments to predict the quality of unseen text or used to generate evaluations in a zero-shot or few-shot setting. This approach leverages the LLM's understanding of language and context to provide a more nuanced assessment of text quality. For instance, [1] illustrates how providing LLM judges with detailed scoring guidelines, such as a scale from 1 to 5 , can standardize the evaluation process. This methodology encompasses critical aspects of content assessment, including coherence, relevance, fluency, coverage, diversity, and detail - both in the context of answer evaluation and query formulation.

Additional Requirements These additional requirements, such as latency, diversity, noise robustness, negative rejection, and counterfactual robustness, are used to ensure the practical applicability of RAG systems in real-world scenarios aligned with human preference. This section delves into the metrics used for evaluating these additional requirements, highlighting their significance in the comprehensive assessment of RAG systems.

Latency measures the time taken by the RAG system to retrieve relevant information and generate a response. It is a critical factor for user experience, especially in interactive applications such as chatbots or search engines [18].

Single Query Latency: The average time taken to process a single query, including both retrieval and generating phases.

Diversity evaluates the variety and breadth of information retrieved and generated by the RAG system. It ensures that the system can provide a wide range of perspectives and avoid redundancy in responses [4].

Cosine Similarity / Cosine Distance: The cosine similarity/distance calculates embeddings of retrieved documents or generated responses. Lower cosine similarity scores indicate higher diversity, suggesting that the system can retrieve or generate a broader spectrum of information.

Noise Robustness measures the RAG system's ability to handle irrelevant or misleading information without compromising the quality of the response [33]. The metrics Misleading Rate and Mistake Reappearance Rate are described in [33], providing detailed descriptions tailored to the specific dataset and experimental setup.

Negative Rejection evaluates the system's capability to withhold responses when the available information is insufficient or too ambiguous to provide accurate answer [5].

Rejection Rate: The rate at which the system refrains from generating a response.

Counterfactual Robustness Counterfactual robustness assesses the system's ability to identify and disregard incorrect or counterfactual information within the retrieved documents [34].

Error Detection Rate: The ratio of counterfactual statements are detected in retrieved information.

## 4 Discussion

Evaluating Retrieval-Augmented Generation (RAG) systems comprises multifaceted challenges, arising from their dual reliance on retrieving accurate, relevant information and generating coherent responses that meet user expectations. This survey has highlighted several key points of consideration, capturing the breadth and depth of evaluations necessary for advancing RAG technologies.

Within the evaluation target aspect, it's evident that traditional question-answering (QA) setups remain effective for evaluating generative components, particularly in how
well they align with human preferences for clarity, relevance, and factual accuracy. However, recent strategies, such as CRUD-based assessments, offer novel angles by scrutinizing RAG systems' interactive capabilities with dynamic information environments [34]. These methodologies underscore the necessity for RAG evaluations to evolve beyond static benchmarks, mirroring real-world scenarios where information is continuously updated and queries are not strictly fact-based but exploratory or conversational.

On the dataset front, the challenge of devising a "one-size-fits-all" dataset is pronounced, given the highly task-specific nature of RAG systems. Unique datasets, meticulously crafted to test specific facets of RAG performance, are indispensable. While this approach ensures thorough, targeted evaluation, it also magnifies the effort and resources required for comprehensive testing. The divergence of datasets, ranging from news articles to structured databases, reflects the adaptability required of RAG systems but also signifies a formidable barrier to streamlined evaluation [49|50].

When it comes to metrics, the use of LLMs as automatic evaluative judges signifies a burgeoning trend, promising versatility and depth in generative outputs with reasoning on a large scale compared to human evaluation. However, using LLMs as judges for chatbot responses presents challenges in aligning with human judgment, establishing effective grading scales, and applying consistent evaluation across varied use cases. The determination of correctness, clarity, and richness can differ between automated and human assessments. Moreover, the effectiveness of example-based scoring can vary, and there's no universally applicable grading scale, complicating the standardization of LLM as a judge. [28]

Future directions in RAG evaluation should focus on developing more adaptive, context-aware benchmarks that accurately reflect the dynamic, information-rich environments these systems are designed to navigate. Such efforts could include simulating real-time information updates in evaluation datasets or incorporating user feedback loops into assessment methodologies. Additionally, exploring more nuanced metrics that can capture the subtlety of human language comprehension and generation-beyond sheer accuracy or relevance-will be crucial. Efforts to codify these advancements into standardized evaluation frameworks would significantly bolster the field, providing clearer benchmarks for progress and more directly aligning RAG system advancements with user needs and societal impacts.

## 5 Conclusion

This survey has systematically explored the complex landscape of evaluating RetrievalAugmented Generation (RAG) systems, highlighting the multifaceted challenges inherent in assessing their performance. Through the proposed $\boldsymbol{R} \boldsymbol{G A R}$ analysis framework, we have delineated a structured approach to dissecting the intricacies of RAG evaluations, focusing on the retrieval, generation, and additional requirements that underpin these systems. Our comprehensive analysis underscores the necessity for targeted benchmarks that reflect the dynamic interplay between retrieval accuracy and generative quality, as well as the practical considerations crucial for real-world applications. By identifying gaps in current methodologies and suggesting future research directions,
this survey aims to pave the way for more nuanced, effective, and user-aligned evaluations of RAG systems, ultimately contributing to the advancement of natural language processing technologies that are both intelligent and versatile.

## References

1. Balaguer, A., Benara, V., Cunha, R.L.d.F., Filho, R.d.M.E., Hendry, T., Holstein, D., Marsman, J., Mecklenburg, N., Malvar, S., Nunes, L.O., Padilha, R., Sharp, M., Silva, B., Sharma, S., Aski, V., Chandra, R.: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. Tech. rep. (Jan 2024), http://arxiv.org/abs/2401.08406. arXiv:2401.08406 [cs] type: article
2. Barnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., Abdelrazek, M.: Seven failure points when engineering a retrieval augmented generation system (Jan 2024). https://doi. org/10.48550/ARXIV. 2401.05856
3. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., Hoefler, T.: Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence 2024 (AAAI'24) (Aug 2023). https://doi.org/10.48550/ ARXIV.2308.09687
4. Blagojevic, V.: Enhancing RAG Pipelines in Haystack: Introducing DiversityRanker and LostInTheMiddleRanker (Aug 2023), https://towardsdatascience.com/ enhancing-rag-pipelines-in-haystack-45£14e2bc9£5
5. Chen, J., Lin, H., Han, X., Sun, L.: Benchmarking large language models in retrievalaugmented generation (Sep 2023). https://doi.org/10.48550/ARXIV.2309. 01431
6. Cuconasu, F., Trappolini, G., Siciliano, F., Filice, S., Campagnano, C., Maarek, Y., Tonellotto, N., Silvestri, F.: The power of noise: Redefining retrieval for rag systems (Jan 2024). https://doi.org/10.48550/ARXIV.2401.14887
7. Deng, Y., Zhang, W., Chen, Z., Gu, Q.: Rephrase and respond: Let large language models ask better questions for themselves (Nov 2023). https://doi.org/10.48550/ARXIV. 2311.04205
8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In: Burstein, J., Doran, C., Solorio, T. (eds.) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 4171-4186. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https: //aclanthology.org/N19-1423
9. DeYoung, J., Jain, S., Rajani, N.F., Lehman, E., Xiong, C., Socher, R., Wallace, B.C.: Eraser: A benchmark to evaluate rationalized nlp models
10. Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., Weston, J.: Wizard of Wikipedia: Knowledge-powered conversational agents. In: Proceedings of the International Conference on Learning Representations (ICLR) (2019)
11. Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.E., Lomeli, M., Hosseini, L., Jégou, H.: The faiss library (2024)
12. DuckDuckGo: DuckDuckGo - Privacy, simplified. (2024), https://duckduckgo. com//home
13. Es, S., James, J., Espinosa-Anke, L., Schockaert, S.: Ragas: Automated evaluation of retrieval augmented generation (Sep 2023). https://doi.org/10.48550/ARXIV. 2309.15217
14. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Guo, Q., Wang, M., Wang, H.: Retrieval-Augmented Generation for Large Language Models: A Survey. Tech. rep. (Jan 2024), http: //arxiv.org/abs/2312.10997, arXiv:2312.10997 [cs] type: article
15. Gienapp, L., Scells, H., Deckers, N., Bevendorff, J., Wang, S., Kiesel, J., Syed, S., Fröbe, M., Zuccon, G., Stein, B., Hagen, M., Potthast, M.: Evaluating Generative Ad Hoc Information Retrieval. Tech. rep. (Nov 2023), http://arxiv.org/abs/2311.04694. arXiv:2311.04694 [cs] type: article
16. Google: Programmable Search Engine I Google for Developers (2024), https:// developers.google.com/custom-search
17. Gottschalk, S., Demidova, E.: Eventkg: A multilingual event-centric temporal knowledge graph (Apr 2018). https://doi.org/10.48550/ARXIV. 1804.04526
18. Hofstätter, S., Chen, J., Raman, K., Zamani, H.: FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation. In: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 14371447. SIGIR '23, Association for Computing Machinery, New York, NY, USA (Jul 2023). https://doi.org/10.1145/3539618.3591687, https://doi.org/ $10.1145 / 3539618.3591687$
19. Huang, J., Shao, H., Chang, K.C.C., Xiong, J., Hwu, W.m.: Understanding jargon: Combining extraction and generation for definition modeling. In: Proceedings of EMNLP (2022)
20. Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., Liu, T.: A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions (Nov 2023). https://doi.org/10.48550/ARXIV. 2311.05232
21. Huang, Y., Huang, J.: A survey on retrieval-augmented text generation for large language models (Apr 2024). https://doi.org/10.48550/ARXIV.2404.10981
22. Johnson, J., Douze, M., Jégou, H.: Billion-scale similarity search with GPUs. IEEE Transactions on Big Data 7(3), 535-547 (2019)
23. Kamalloo, E., Thakur, N., Lassance, C., Ma, X., Yang, J.H., Lin, J.: Resources for brewing beir: Reproducible reference models and an official leaderboard (2023)
24. Khattab, O., Zaharia, M.: Colbert: Efficient and effective passage search via contextualized late interaction over bert (Apr 2020). https://doi.org/10.48550/ARXIV. 2004. 12832
25. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.W., Dai, A.M., Uszkoreit, J., Le, Q., Petrov, S.: Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics 7, 453466 (2019). https://doi.org/10.1162/tacl_a_00276, https://doi.org/ $10.1162 /$ tacl_a_00276
26. Lanchantin, J., Toshniwal, S., Weston, J., Szlam, A., Sukhbaatar, S.: Learning to reason and memorize with self-notes (May 2023). https://doi.org/10.48550/ARXIV. 2305.00833
27. LangChain: Evaluating rag architectures on benchmark tasks (Nov 2023), https: //langchain-ai.github.io/langchain-benchmarks/notebooks/ retrieval/langchain_docs_qa.html
28. Leng, Q., Uhlenhuth, K., Polyzotis, A.: Best Practices for LLM Evaluation of RAG Applications (Dec 2023), https://www.databricks.com/blog/ LLM-auto-eval-best-practices-RAG
29. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-augmented generation for knowledge-intensive NLP tasks. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. pp. 9459-9474. NIPS'20, Curran Associates Inc., Red Hook, NY, USA (Dec 2020)
30. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Tech. rep. (Apr 2021), http://arxiv.org/abs/ 2005.11401, arXiv:2005.11401 [cs] type: article
31. Liang, X., Song, S., Niu, S., Li, Z., Xiong, F., Tang, B., Wy, Z., He, D., Cheng, P., Wang, Z., Deng, H.: Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation. arXiv preprint arXiv:2311.15296 (2023)
32. Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. In: Text Summarization Branches Out. pp. 74-81. Association for Computational Linguistics, Barcelona, Spain (Jul 2004), https://aclanthology.org/W04-1013
33. Liu, Y., Huang, L., Li, S., Chen, S., Zhou, H., Meng, F., Zhou, J., Sun, X.: Recall: A benchmark for llms robustness against external counterfactual knowledge (Nov 2023). https: //doi.org/10.48550/ARXIV. 2311.08147
34. Lyu, Y., Li, Z., Niu, S., Xiong, F., Tang, B., Wang, W., Wu, H., Liu, H., Xu, T., Chen, E., Luo, Y., Cheng, P., Deng, H., Wang, Z., Lu, Z.: Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models (Jan 2024). https://doi. org/10.48550/ARXIV. 2401.17043
35. Microsoft: Web Search API I Microsoft Bing, https://www.microsoft.com/ en-us/bing/apis/bing-web-search-api
36. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Isabelle, P., Charniak, E., Lin, D. (eds.) Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311-318. Association for Computational Linguistics, Philadelphia, Pennsylvania, USA (Jul 2002). https://doi. org/10.3115/1073083.1073135, https://aclanthology.org/P02-1040
37. Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De Cao, N., Thorne, J., Jernite, Y., Karpukhin, V., Maillard, J., Plachouras, V., Rocktäschel, T., Riedel, S.: KILT: a benchmark for knowledge intensive language tasks. In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2523-2544. Association for Computational Linguistics, Online (Jun 2021). https://doi.org/10.18653/v1/2021.naacl-main. 200 https://aclanthology.org/2021.naacl-main. 200
38. Ramos, J., et al.: Using tf-idf to determine word relevance in document queries. In: Proceedings of the first instructional conference on machine learning. vol. 242, pp. 29-48. Citeseer (2003)
39. Robertson, S., Zaragoza, H., et al.: The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends $\circledR$ in Information Retrieval 3(4), 333-389 (2009)
40. Rosset, C., Chung, H.L., Qin, G., Chau, E.C., Feng, Z., Awadallah, A., Neville, J., Rao, N.: Researchy questions: A dataset of multi-perspective, decompositional questions for llm web agents (Feb 2024).https://doi.org/10.48550/ARXIV. 2402.17896
41. Saad-Falcon, J., Khattab, O., Potts, C., Zaharia, M.: Ares: An automated evaluation framework for retrieval-augmented generation systems (Nov 2023). https://doi.org/10. 48550/ARXIV. 2311.09476
42. Shahabi, C., Kolahdouzan, M.R., Sharifzadeh, M.: A road network embedding technique for k-nearest neighbor search in moving object databases. In: Proceedings of the 10th ACM international symposium on advances in geographic information systems. pp. 94-100 (2002)
43. Tang, Y., Yang, Y.: Multihop-rag: Benchmarking retrieval-augmented generation for multihop queries (Jan 2024). https://doi.org/10.48550/ARXIV. 2401.15391
44. Thorne, J., Vlachos, A., Christodoulopoulos, C., Mittal, A.: FEVER: a large-scale dataset for fact extraction and VERification. In: NAACL-HLT (2018)
45. TruLens: TruLens (2023), https://www.trulens.org/trulens_eval/ getting_started/quickstarts/quickstart/
46. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: SuperGLUE: A stickier benchmark for general-purpose language understanding systems. arXiv preprint 1905.00537 (2019)
47. Wang, S., Khramtsova, E., Zhuang, S., Zuccon, G.: Feb4rag: Evaluating federated search in the context of retrieval augmented generation (Feb 2024). https://doi.org/10. $48550 / A R X I V .2402 .11891$
48. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models (Jan 2022). https : //doi.org/10.48550/ARXIV.2201.11903
49. Xiong, G., Jin, Q., Lu, Z., Zhang, A.: Benchmarking retrieval-augmented generation for medicine (Feb 2024). https://doi.org/10.48550/ARXIV.2402.13178
50. Xu, Z., Li, Y., Ding, R., Wang, X., Chen, B., Jiang, Y., Zheng, H.T., Lu, W., Xie, P., Huang, F.: Let llms take on the latest challenges! a chinese dynamic question answering benchmark (Feb 2024).https://doi.org/10.48550/ARXIV.2402.19248
51. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.W., Salakhutdinov, R., Manning, C.D.: HotpotQA: A dataset for diverse, explainable multi-hop question answering. In: Conference on Empirical Methods in Natural Language Processing (EMNLP) (2018)
52. Yao, J.Y., Ning, K.P., Liu, Z.H., Ning, M.N., Yuan, L.: Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469 (2023)
53. Zhang, Q., Chen, S., Xu, D., Cao, Q., Chen, X., Cohn, T., Fang, M.: A Survey for Efficient Open Domain Question Answering. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 14447-14465. Association for Computational Linguistics, Toronto, Canada (Jul 2023).https://doi.org/10.18653/v1/2023.acl-long. 808, https://aclanthology.org/2023.acl-long.808
54. Zhang, S., Liu, X., Liu, J., Gao, J., Duh, K., Van Durme, B.: Record: Bridging the gap between human and machine commonsense reading comprehension (Oct 2018). https: //doi.org/10.48550/ARXIV.1810.12885
55. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: BERTScore: Evaluating Text Generation with BERT. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net (2020), https: //openreview.net/forum?id=SkeHuCVFDr
56. Zhang, Y., Khalifa, M., Logeswaran, L., Lee, M., Lee, H., Wang, L.: Merging Generated and Retrieved Knowledge for Open-Domain QA. In: Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4710-4728. Association for Computational Linguistics, Singapore (Dec 2023). https://doi.org/10.18653/v1/2023.emnlp-main.286, https:// aclanthology.org/2023.emnlp-main. 286
57. Zhao, P., Zhang, H., Yu, Q., Wang, Z., Geng, Y., Fu, F., Yang, L., Zhang, W., Cui, B.: Retrieval-augmented generation for ai-generated content: A survey (Feb 2024). https: //doi.org/10.48550/ARXIV.2402.19473
58. Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging llm-as-a-judge with mt-bench and chatbot arena (Jun 2023). https://doi.org/10.48550/ARXIV. 2306.05685
59. Zhu, F., Lei, W., Wang, C., Zheng, J., Poria, S., Chua, T.S.: Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering. Tech. rep. (May 2021), http://arxiv.org/abs/2101.00774, arXiv:2101.00774 [cs] type: article
