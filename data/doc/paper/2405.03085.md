# Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation 

Kaize Shi ${ }^{\ominus}$, Xueyao Sun ${ }^{\ominus}, \boldsymbol{\sim}$, Qing $\mathrm{Li}^{*}$, Guandong $\mathrm{Xu}^{\ominus, \diamond}$<br>${ }^{\bigcirc}$ University of Technology Sydney<br>* The Hong Kong Polytechnic University<br>$\diamond$ The Education University of Hong Kong


#### Abstract

Large Language Models (LLMs) have made significant strides in information acquisition. However, their overreliance on potentially flawed parametric knowledge leads to hallucinations and inaccuracies, particularly when handling long-tail, domain-specific queries. Retrieval Augmented Generation (RAG) addresses this limitation by incorporating external, non-parametric knowledge. Nevertheless, the retrieved long-context documents often contain noisy, irrelevant information alongside vital knowledge, negatively diluting LLMs' attention. Inspired by the supportive role of essential concepts in individuals' reading comprehension, we propose a novel concept-based RAG framework with the Abstract Meaning Representation (AMR)-based concept distillation algorithm. The proposed algorithm compresses the cluttered raw retrieved documents into a compact set of crucial concepts distilled from the informative nodes of AMR by referring to reliable linguistic features. The concepts explicitly constrain LLMs to focus solely on vital information in the inference process. We conduct extensive experiments on opendomain question-answering datasets to empirically evaluate the proposed method's effectiveness. The results indicate that the concept-based RAG framework outperforms other baseline methods, particularly as the number of supporting documents increases, while also exhibiting robustness across various backbone LLMs. This emphasizes the distilled concepts are informative for augmenting the RAG process by filtering out interference information. To the best of our knowledge, this is the first work introducing AMR to enhance the RAG, presenting a potential solution to augment inference performance with semantic-based context compression.


## 1 Introduction

Large Language Models (LLMs) have emerged as indispensable tools for daily information acquisition, owing to their extensive knowledge base and ability to fulfil diverse user instructions [6, 47, 1]. By leveraging large-scale pre-training on massive datasets, LLMs memorize vast amounts of knowledge within their parameters as internal memory, known as parametric knowledge [33]. However, the presence of outdated or incorrect knowledge within internal memory can lead to hallucinations, hindering the performance of LLMs' inferencing process [46]. This limitation is particularly pronounced when handling long-tail knowledge for domain-specific or highly specialized queries, as the inherent difficulty in memorizing rare entities persists even in the most robust models. Consequently, the overreliance on potentially flawed parametric knowledge can significantly interfere with the reliability of LLMs' outputs, especially in scenarios with fine-grained knowledge requirements [58, 36].

Retrieval Augmented Generation (RAG) employs additional retrievers to augment LLMs with external, non-parametric knowledge, effectively expanding their internal knowledge boundaries [27, 14]. This
allows LLMs to access up-to-date, query-focused information that may not be adequately memorized within their parametric memory to alleviate the aforementioned limitations [24]. In contrast to finetuning by updating the model parameters, RAG preserves pre-trained knowledge while dynamically incorporating relevant external context. This paradigm offers greater flexibility and scalability, as the retrievers can be easily plug-and-play without modifying the underlying language model's parameters, thus circumventing complex computational hurdles [17, 16. However, RAG is easily confused when dealing with long contextual retrieved support documents, which often consist of multiple shreds of evidence for providing vital knowledgeable context but are also accompanied by noisy and irrelevant information [56]. The distracting contexts can dilute the LLMs' attention and adversely affect their performance with misrepresentation [30, 25]. Compressing lengthy contexts to distil vital knowledge is crucial for enhancing LLMs and ensuring factually consistent responses in the RAG process.

Numerous studies have demonstrated that individuals tend to directly search for key concepts when reading long documents as the brain will complete the remaining details based on prior knowledge, expectations, background, and motivations [15, 22]. This selective attention to critical information allows ignoring redundant details and rearranging the text informatively [51]. As illustrated in Fig. 1], given only the key concepts of the question-related supporting documents that still enable us to grasp the crucial semantics. LLMs parameterize massive common knowledge, enabling them to exhibit a similar ability in context understanding even when the word or character-level information


#### Abstract

Question (Q): What genre is The Outfit? Concept (C): [1] film, The Outfit, 1973, directed, John Flynn, crime. [2] stars, Robert Duvall, Karen Black, Joe Don Baker, Robert Ryan. [3] adaptation, Richard Stark's, novel, same, name, screenplay. [4] drive, hitman, house, Eddie Macklin's, assassinate, builds, wall, brick, backyard.


Instructions: Refer to the facts to answer the question. Question: $\mathcal{Q}$. Facts: $\mathcal{C}$.

Answer $(\mathcal{A}):$ Crime Film

Figure 1: The examples of concept-based RAG ${ }^{1}$ is disrupted [43, 7]. This provides the possibility of whether LLMs can comprehend scenarios solely based on discrete informative concepts. Linguistic features, such as semantic and syntactic, have significantly improved the interpretability, controllability, and diversity of Natural Language Generation (NLG) [28]. Language models can implicitly discover these features during pre-training to ensure the logic of the generated text [21]. It has been demonstrated that explicitly leveraging linguistic features for downstream tasks is beneficial, as it refactors the source documents into concise representations that reduce entropy by focusing on the critical information, thereby aiding in a comprehensive understanding of the described scenarios [41, 48, 44, 28, 23, 55]. This advantage enables the stable linguistic features to reliably assist context understanding.

Inspired by the aforementioned insights, we propose enhancing RAG's performance with the crucial concepts distilled from the raw retrieved supporting documents. To effectively capture the informative concepts, we introduce Abstract Meaning Representation (AMR), a semantic formalism that encodes the meaning of serialized texts by a rooted, directed, labelled, acyclic graph [3]. Compared to other linguistic representations, AMR prioritizes semantic consistency among concepts carried by nodes when representing sentences, offering the advantage of automatically rectifying surfacelevel variations or understanding abbreviated terms, ensuring the structured concepts represent the underlying meaning to transcend the limitations of linguistic noise [59]. Specifically, we propose the concept-based RAG framework with the AMR-based concept distillation algorithm, which formats the concepts for augmenting LLMs by compressing the lengthy context to concentrate on crucial information exclusively. We empirically experiment on two open-domain Q\&A datasets, PopQA [32] and EntityQuestions [40]. The results show that the performance of our method improves significantly as the number of supporting documents increases, outperforming baselines with various compression methods and backbone LLMs. The contributions of this paper can be summarized as follows:
- This paper proposes the concept-based RAG framework that explicitly integrates AMR, a semantic representation, to enable LLMs to focus on essential rather than messy knowledge[^0]when processing long-context retrieved supporting documents. To the best of our knowledge, this is the first research introducing AMR to enhance RAG for more reliable inference.
- We propose an AMR-based concept distillation algorithm, which compresses long-context raw supporting documents into concepts by formatting the informative nodes. The distilled concepts are more knowledge-centralized than the raw supporting documents, reducing the interference of irrelevant information during the inference process of LLMs.
- We conduct extensive experiments on open-domain $\mathrm{Q} \& \mathrm{~A}$ datasets. The results indicate that our framework effectively enhances inference performance as the number of supporting documents increases, outperforming baselines with various context compression methods and backbone LLMs. This demonstrates its applicability in long-context RAG scenarios.

## 2 Related Works

### 2.1 Long-context Understanding

The increasing complexity of downstream tasks and the demand for models capable of capturing intricate dependencies have driven significant attention to the long-context understanding of LLMs [37, 19. 53. One prominent research avenue involves modifying the basic architecture of LLMs. For instance, Dai et al.[11] introduced a segment-level recurrence mechanism with their Transformer-XL model, enabling it to retain longer contextual information than the standard Transformer structure. Similarly, Beltagy et al.[4] extended the self-attention mechanism in their Longformer model to handle longer sequences by introducing a sparse attention pattern, thereby facilitating the efficient processing of documents with thousands of tokens. However, a significant drawback of modifying model architecture is the necessity for complex re-training processes. In contrast, research on prompt compression aims to understand long-token prompts by compressing them into low-dimensional soft prompts [50, 9, 34]. While offering a more efficient alternative to architecture modification, this approach constrains the transferability of learned prompts across various LLMs.

Recent research has advanced to a more intuitive level, aiming to comprehensively understand the context by directly expanding the context window or explicit compression. Chen et al.[8] introduced position interpolation to extend the context window of pre-trained LLMs, scaling LLaMA's context window to $32 \mathrm{k}$ tokens with few fine-tuning steps. Ding et al.[12] proposed LongRoPE to extend LLMs' context window to $2048 \mathrm{k}$ tokens while maintaining the performance of the original short context window through a positional and interpolation progressive extension strategy. However, the long context window raises another challenge of diluting core information with redundant data [53]. To address this, $\mathrm{Li}$ et al.[29] filtered out irrelevant context with low self-information for compressing the long prompts. Chuang et al.[10] proposed the Nano-Capsulator to compress original prompts into capsule prompts, decreasing inference latency across diverse LLMs. Compression methods can benefit the RAG by allowing LLMs to focus on essential knowledge in supporting documents [54].

### 2.2 Linguistics-augmented NLG

Incorporating linguistic principles into LLMs has shown promise in improving the coherence and semantic fidelity of generated text [55]. Augmentation techniques like syntactic trees [35] and lexical patterns [28] assist in linguistic feature injection, enabling language models to generate more faithful text. Ahmed et al. [2] proposed automatic semantic augmentation of prompts to enhance LLMs with tagged facts, resulting in improved code summarization performance. Zhou et al. [60] introduced InstructCTG, a framework for controlling LLMs' generation based on syntax constraints, facilitating flexibility and adaptation to new conditions without complex model modification. LLMs can be explicitly guided by leveraging linguistic insights to mitigate biases inherent in parameterized-only approaches, hereby enhancing performance in tasks demanding strict factual consistency.

Abstract Meaning Representation (AMR) has proven its efficacy in enhancing downstream generation tasks by providing a structured semantic representation that encapsulates static concepts [18]. Frisoni et al. [13] integrated AMR with pre-trained language models to enhance biomedical summarization by capturing inter-entity relations. Ribeiro et al. [38] employed AMR to improve factuality evaluation in abstractive summarization by identifying content verifiability errors and subsentence-level factual inconsistencies. Shi et al. [42] proposed AMR-TST, which generates fluent and reliable texts with the target style by optimizing core concept nodes. Jangra et al. [20] preserved style-agnostic content
while generating transferred text by utilizing AMR as an intermediate representation. These studies illustrate AMR's advantages in capturing essential concepts containing informative linguistic features.

## 3 Method

### 3.1 Concept-based RAG Framework

This section introduces the proposed concept-based RAG framework for inference utilising the concepts distilled from the raw supporting documents. The overview of the framework is in Fig. 2 .

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-04.jpg?height=393&width=1390&top_left_y=701&top_left_x=365)

Figure 2: The overview of the concept-based RAG framework, which consists of three main components: (a) information retrieval, (b) concept distillation, and (c) concept-based inference.

Given an input question $\mathcal{Q}$, the (a) information retrieval component aims to utilize a retriever to return the top- $\mathcal{K}$ knowledgeable supporting documents $\mathcal{D}=\left\{\mathcal{D}_{1}, \ldots, \mathcal{D}_{\mathcal{K}}\right\}$ relevant to $\mathcal{Q}$ from sources such as Wikipedia or other information repositories. At this stage, the retriever's performance significantly influences the resulting answer set $\mathcal{A}=\left\{\mathcal{A}_{1}, \ldots, \mathcal{A}_{M}\right\}$ [33, 14]. However, the retriever's performance is beyond this paper's scope. We hypothesize that all retrieved supporting documents $\mathcal{D}$ contain the correct answer corresponding to $\mathcal{Q}$, expressed as a proposition: $\forall \mathcal{D}_{k} \in \mathcal{D}, \exists \mathcal{A}_{m} \in \mathcal{A}, \mathcal{A}_{m} \subseteq \mathcal{D}_{k}$.

The (b) concept distillation component is devised to format the concept $\mathcal{C}$ from the retrieved supporting document $\mathcal{D}$ by the proposed AMR-based concept distillation algorithm. This algorithm converts the supporting documents from continuous sequences to discrete concepts formatted from the AMR graph, denoted as $\mathcal{G}$. Further details of this algorithm will be elucidated in the subsequent section.

After obtaining the distilled concept $\mathcal{C}$, the (c) concept-based inference component proceeds to integrate it with various backbone LLMs to derive answers $\mathcal{A}$ using a faithful-intensive prompt template as follows: [Refer to the following facts to answer the question. Facts: $\mathcal{C}$. Question: $\mathcal{Q}$ ]. The intensity of prompts has been demonstrated to influence LLMs' adherence to knowledge from internal memory and retrieved documents [52]. Since our hypothesis is that the retrieved documents contain correct answers, we encourage the LLMs to leverage the knowledge encapsulated in $\mathcal{C}$ when responding to queries. This strategy helps minimize potential conflicts caused by their memorized parametric knowledge. To achieve this objective, we designate the concept as a "fact" within the instructional prompt, explicitly delineating a delimited sandbox for LLMs to presuppose the absolute correctness of the knowledge conveyed by $\mathcal{C}$. This non-parametric knowledge can seamlessly integrate into LLMs in a plug-and-play manner. The overarching framework can be represented as Eq. 1.

$$
\begin{equation*}
P(\mathcal{A} \mid \mathcal{Q})=P(\mathcal{A} \mid \mathcal{C}, \mathcal{Q}) P(\mathcal{C} \mid \mathcal{D}, \mathcal{Q}) P(\mathcal{D} \mid \mathcal{Q}) \tag{1}
\end{equation*}
$$

### 3.2 AMR-based Concept Distillation

Abstract Meaning Representation (AMR) serves as a logical formal semantic structure proficient in encapsulating common-sense knowledge necessary for representing events, time, participants, and other elements within serialized texts [39]. Given a supporting document $\mathcal{D}_{k} \in \mathcal{D}$, the AMR parser is utilized to parse $\mathcal{D}_{k}$ into the corresponding AMR graph $\mathcal{G}=<\mathcal{N}, \mathcal{E}>$, where $\mathcal{C}$ represents the nodes for concepts and $\mathcal{E}$ denotes the edges for the correlation relationships. In this context, we utilize a

mBart-based [31] parse ${ }^{2}$ trained on the AMR 3.0 corpus 3 to address potential multilingual concerns. The detailed illustration of the AMR graph parsing is depicted in Table A1.

We propose the concept distillation algorithm to format the concepts represented in $\mathcal{G}$, as described in Algorithm 1. The supporting document $\mathcal{D}_{k}$ encompasses multiple sentences $\left(s n t_{n}\right)$, and the AMR parser can structurally parse $\mathcal{D}_{k}$ into a pre-defined multi-sentence structure. The SplitSnt(.) function is designed to partition $\mathcal{G}$ and organize the resulting sentence-based sub-graphs according to the sequential order. Notably, we simplify $\mathcal{G}$ by disregarding the agent and patient of the concepts, i.e., the edges denoting relations between the connected concepts (Frame args, ARGX). Consequently, $\mathcal{G}$ is streamlined into a unidirectional connecting structure. Leveraging this structure, we perform a Depth First Search, DFS (.) on the $\mathcal{N}$ of $\mathcal{G}$ to traverse the concepts while maintaining the relative positional correlation of adja-

```
Algorithm 1: Concept Distillation
    Input : AMR Graph $(\mathcal{G})$
    Output : concept ( $\mathcal{C}$ )
    Function Concept_Distillation $(\mathcal{G})$ :
        concept $\leftarrow[]$, role $\leftarrow[]$;
        for $\mathcal{G}_{s n t_{n}}$ in SplitSnt $(\mathcal{G})$ do
            for $\mathcal{N}$ in DFS $\left(\mathcal{G}_{s n t_{n}}\right)$ do
                if IsRole $(\mathcal{N})$ then
                    if IsName $(\mathcal{N})$ then
                        AppendRole (HandleName $(\mathcal{N})$ )
                    if IsWiki $(\mathcal{N})$ then
                        Append지e (HandleWiki $(\mathcal{N})$ )
                    if IsDate $(\mathcal{N})$ then
                            AppendRole (HandleDate $(\mathcal{N})$ )
            else
                if role is not None then
                                AppendConcept (HandleRole(role));
                                role $\leftarrow[] ;$
                AppendConcept (스);
            if ( $\mathcal{N}$ is Last) and (role is not None) then
                repeat : Algorithm.Line 5-11
                AppendConcept (HandleRole(role));
            concept $\leftarrow$ ConceptFormat (concept);
            concept $\leftarrow$ ConceptBacktrace (concept);
            return $\mathcal{C} \leftarrow$ concept
``` cent nodes. This approach em-

phasizes the connection as it ex-

ists in the preceding sequential representation, and the process is elaborated in Fig. A1. Previous research has investigated the influence of context order on LLMs [30]. We delve into the various traversal methods for testing their potential impact in Section $\mathrm{D}$.

The AMR defines a set of roles to meticulously delineate the semantic fabric of sentences. This paper underscores the meticulous handling of three roles, namely :name, :wiki, and date-entity, employing IsRole(.) to identify the predefined roles comprehensively. The : name role signifies a property node within the AMR graph, signifying entities such as individuals, organizations, or geographic locations. In instances where the concept expressed by :name spans multiple words, the parsing process of AMR decomposes each word within the : name into predicate roles (:op), thereby dispersing the holistic concept across multiple nodes. During the DFS (.) traversal process, fragmented nodes can potentially confuse LLMs due to incomplete meaning expressions. To maintain the integrity of concepts carried by :name, we introduce HandleName $(\cdot)$, organizing predicates in a stack structure. The :wiki role provides reliable external concept references sourced from Wikipedia. For standardizing concepts' diverse expressions referring to the same named entities, we utilize the HandleWiki ($\cdot$) function, which aligns the concepts with the corresponding definitions in Wikipedia. If the concept in :name differs from :wiki, we designate the concept expressed by this node as : wiki to avoid semantic disambiguation. In addition, there is a date-entity role that depicts temporal concepts. In our algorithm, we specifically manage the roles :year, :month, and : day by HandleDate ($\cdot$). This function consolidates roles under the same date-entity, forming concepts like "19 04 2024" with numerical months translated into textual representations, "19 April 2024", for clear expression. AMR incorporates special numerical annotations for certain parsing nodes, such as work-01, where the number appended to the word indicates different meanings of the same word in distinct contexts as defined in OntoNotes [49]. In the RAG scenario, we provide

$2^{\text {https://github.com/BramVanroy/multilingual-text-to-amr }}$

$\sqrt[3]{\text { https://catalog.ldc.upenn.edu/LDC2020T02 }}$

LLMs with supporting documents comprising a set of concepts. This suggests that concepts are understood in relation to relevant contexts rather than in isolation. Therefore, the proposed conceptbased RAG framework depends on the contextual learning capability of LLMs to distinguish between polysemous concepts, instead of relying on intricate semantic references. The nodes belonging to the aforementioned roles are integrated into the preliminary concept set with the HandleRole(.), while the AppendConcept (.) directly integrate the remaining nodes based on the corresponding instances.

The structure of AMR comprises a collection of canonical nodes (city-district, market-sector, etc.) designed to enforce knowledge and prevent hallucination regarding entity types. However, in the concept-based RAG scenario, the inference process isn't directly based on AMR but distilled concepts. The auxiliary semantics embedded within these nodes, which are absent in the source supporting documents, may dilute the essence of the core concept. To address this concern, we employ ConceptFormat (.) to filter out these nodes to reduce the potential interference. Additionally, frequently occurring concepts are filtered out based on their Inverse Document Frequency (IDF). Furthermore, the selection of representations in AMR is based on the principle of abstraction and generalization rather than the exact lexical items. This representation may mislead the nodes into ignoring variations such as tense, which are informative for concept-based RAG without reference annotations. To mitigate this, we develop the ConceptBacktrace ( $)$ function to maintain consistency with concepts expressed in the source supporting documents. This function facilitates the backtracking of formatted concepts by incorporating representations from the supporting documents, ensuring they closely adhere to the original semantics without deviation. Subsequently, the backtraced concepts serve as the finalized concepts $\mathcal{C}$, providing conceptual support for LLMs in RAG inference.

\section*{4 Experiments}

\subsection*{4.1 Datasets}

We conducted extensive experiments to verify the efficacy of the concept-based RAG framework on open-domain Q\&A datasets: PopQA [32] and EntityQuestions [40]. Each dataset includes a label ("hasanswer") for every supporting document, indicating whether it contains the answer to the associated question. To ensure a focused evaluation, we screened out the " $<\mathcal{Q}-\mathcal{A}-\mathcal{D}>$ " pairs where hasanswer $=$ True. This selection criterion accommodates scenarios where all retrieved documents contribute positively to answering questions, thus mitigating interference from extraneous factors. The experiments involved verifying the LLMs' inference performance with different $\mathcal{K}$, which denotes the amount of supporting documents to $\mathcal{Q}$. For the PopQA dataset, we filtered out questions with subject entities having monthly Wikipedia pageviews $\left(s_{p o p}\right) \geq 500$. This step excludes frequently accessed entities, preserving the dataset focused on long-tail knowledge. This approach serves the dual purpose of preventing data contamination and encouraging LLMs to rely more on retrieved documents than memorized knowledge, mitigating potential knowledge conflicts in the RAG process. The statistical results of the number of the selected pairs with different $\mathcal{K}$ settings are in Table 1 .

Table 1: Statistical results of the number of screened-out $\angle \mathcal{Q}-\mathcal{A}-\mathcal{D}>$ pairs from the datasets.

\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
\hline $\mathcal{K}=$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline \hline PopQA [32] & 738 & 1307 & 422 & 262 & 161 & 151 & 108 & 79 & 66 & 70 \\
\hline EntityQuestions [40] & 1671 & 1127 & 670 & 454 & 335 & 264 & 196 & 166 & 163 & 103 \\
\hline
\end{tabular}

\subsection*{4.2 Baselines}

The baseline evaluations encompass two aspects: (1) exploration of diverse backbone LLMs, and (2) experimentation with different context compression methods. Specifically, we consider various mainstream LLMs as backbones, including GPT-Neo-1.3B, GPT-Neo-2.7B [5], OPT-1.3b, OPT2.7b [57], bloom-560m, bloom-1b1, bloom-1b7, bloom-3b [26], LLaMA-2-7b-chat-hf, LLaMA-213b-chat-hf [47]. The backbone LLMs coupled with the original supporting documents serve as the Vanilla methods. Regarding the alternative aspect, we explore the three context compression methods: context keywords extraction, context summarization, and Selective Context (SelCon) [29]. These methods aim to validate the efficacy of context compression while preserving essential information for inference, emphasizing discrete key features, fluent representation, and non-redundant information.

Inspired by Chuang et al. [10], we employ a novel open-access LLM, LLaMA-2-13b-chat-hf [47], for context keyword extraction and summarization. This process involves extracting key phrases or terms from the context and generating a concise summary of the provided content, constrained by prompts of "[Generate a short summary of the following content.]" and "[Extract a few keywords from the following content.]". The detailed prompts are available in Appendix B The SelCon enhances the efficiency of LLMs' inference by identifying and eliminating redundant content from the source context for compression. The reduction ratio of the SelCon compared here is set to 0.5 . These baseline settings effectively demonstrate the comprehensive advantages of the proposed algorithm in capturing informative concepts when compared to various alternative compression techniques, whether generative-based or semantic-based methods.

\subsection*{4.3 Evaluation Metrics}

We employ two metrics to evaluate the concept-based RAG: accuracy (Acc.) and integration (Intg.). Accuracy (Acc.) is determined by assessing whether any answer $\mathcal{A}$ matches any of the gold answers corresponding to the question $\mathcal{Q}$. The integration metric (Intg.) is designed to comprehensively evaluate the performance across various $\mathcal{K}$ of the retrieved supporting documents $\mathcal{D}$. Specifically, the Intg. signifies the area beneath the accuracy curve of each model plotted against the X-axis ( $\mathcal{K}$ ). The calculation of Intg. is as Eq. 2, where $\mathcal{K} \in\left[x_{s}, x_{e}\right]$, and $x_{s}$ and $x_{e}$ represent the minimum and maximum number of supporting documents respectively. A higher value of Intg. indicates superior overall performance. Given that the proposed framework aims to enhance long-context RAG, we segment the evaluation of Intg. into two distinct intervals: normal interval $\left(I_{n}=[1,10], \mathcal{K} \in I_{n}\right)$ and longer interval ( $I_{l}=[6,10], \mathcal{K} \in I_{l}$ ). This division is intended to emphasize the effectiveness of the concept-based RAG framework, particularly in scenarios involving longer contexts.

$$
\begin{equation*}
\operatorname{Intg} .=\int_{x_{s}}^{x_{e}} \operatorname{Acc}(x) d x \approx \frac{1}{2} \sum_{i=1}^{x_{e}-x_{s}+1}\left(x_{i}-x_{i-1}\right)\left[\operatorname{Acc}\left(x_{i}\right)+\operatorname{Acc}\left(x_{i-1}\right)\right] \tag{2}
\end{equation*}
$$

\section*{5 Results and Analysis}

The evaluation results for the PopQA and EntityQuestion datasets are depicted in Fig. 3 and Fig. 4 , respectively, providing graphical trends of Acc. as $\mathcal{K}$ increases intuitively. Furthermore, Table 2 and Table 3 present quantitative results of Intg. for the datasets. These tables include the calculation of $\Delta$, quantifying the improvement achieved by our proposed method over the Vanilla methods. Specifically, $\Delta$ is computed as follows: $\Delta=$ Intg. $._{\text {ours }}-$ Intg $_{\text {vanilla }}$. The detailed quantitative evaluation results of Acc. are provided in Table A3 and Table A4. Section E and section Fexamine compression ratio and inference latency comparison to demonstrate the advantages of concept-compressed contexts.
![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-07.jpg?height=570&width=1392&top_left_y=1688&top_left_x=366)

Figure 3: The evaluation results of the Acc. $\uparrow$ trends and Intg. $\uparrow$ on the PopQA dataset. The vertical axis represents $A c c$., and the horizontal axis represents the number of supporting documents, $\mathcal{K}$. The polyline reflects the changing trend of Acc. with different $\mathcal{K}$, and the under area is Intg.

A key intuitive finding reflected by Fig. 3 and Fig. 4 is the superior performance of our method in long-context scenarios, particularly evident when $\mathcal{K}$ is high. As $\mathcal{K}$ increases, especially within
![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-08.jpg?height=574&width=1400&top_left_y=298&top_left_x=362)

Figure 4: The evaluation results of the Acc. $\uparrow$ trends and Intg. $\uparrow$ on the EntityQuestion dataset. The definitions of the axis and symbols are the same with the Fig. 3

Table 2: The quantitative results of Intg. $\uparrow$ for the PopQA dataset, where the full name order of the LLMs is: GPT-Neo-1.3B, GPT-Neo-2.7B, OPT-1.3b, OPT-2.7b, bloom-560m, bloom-1b1, bloom-1b7, bloom-3b, LLaMA-2-chat-7b, LLaMA-2-chat-13b. The best results are in bold, and the second best results are in underlined. The increased and decreased $\Delta$ are marked differently.

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline $\mathcal{D}$ & $\mathcal{K}$ & G-1.3 & $\overline{\text { G-2.7 }}$ & $\mathrm{O}-1.3$ & O-2.7 & b-560 & $\mathrm{b}-1 \mathrm{~b} 1$ & $\mathrm{~b}-1 \mathrm{~b} 7$ & $\mathrm{~b}-3$ & L-7 & L-13 \\
\hline \multirow{2}{*}{ Vanilla } & ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-08.jpg?height=42\&width=43\&top_left_y=1328\&top_left_x=527) & $\underline{620.68}$ & 6 & $\underline{656.68}$ & $\underline{687.15}$ & $\mathbf{6 1 9 . 8 6}$ & 6992.68 & 70707.25 & $\underline{671.88}$ & $\overline{682.30}$ & ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-08.jpg?height=42\&width=98\&top_left_y=1328\&top_left_x=1641) \\
\hline & $I_{l}$ & $\overline{291.08}$ & $\overline{275.32}$ & 300.85 & $\overline{322.23}$ & 294.94 & 325.37 & 326.29 & $\overline{305.91}$ & 337.19 & 312.62 \\
\hline \multirow[b]{2}{*}{ K } & $I_{n}$ & $\overline{468.94}$ & $\overline{484.98}$ & 554.67 & 571.38 & 502.70 & 610.69 & 621.85 & 600.65 & 628.78 & 617.06 \\
\hline & $I_{l}$ & 257.12 & 244.24 & 297.70 & 305.64 & 275.39 & 327.70 & 338.01 & 318.37 & 326.41 & 315.93 \\
\hline \multirow{2}{*}{ Summary } & $\overline{I_{n}}$ & 517.57 & 513.37 & 619.78 & 575.32 & 573.95 & $\overline{608.41}$ & $\overline{637.55}$ & $\overline{591.12}$ & 564.51 & 553.24 \\
\hline & $I_{l}$ & 263.14 & 260.64 & $\underline{316.80}$ & 290.50 & $\underline{304.55}$ & 313.36 & 336.20 & 297.44 & 291.50 & 291.39 \\
\hline \multirow{2}{*}{ SelCon } & $I_{n}$ & 444.29 & 524.54 & $\overline{615.78}$ & 607.12 & $\overline{423.22}$ & 634.81 & 606.15 & 625.66 & 715.90 & 703.29 \\
\hline & $I_{l}$ & 237.49 & 262.78 & 313.39 & 323.69 & 230.20 & 318.64 & 306.72 & 314.07 & $\overline{344.10}$ & $\overline{332.51}$ \\
\hline \multirow{2}{*}{ Ours } & $I_{n}$ & 625.31 & 652.71 & 668.86 & $\overline{688.47}$ & 608.31 & 686.29 & 698.91 & 681.22 & $\overline{738.82}$ & $\overline{716.55}$ \\
\hline & $I_{l}$ & 322.37 & 321.73 & 329.65 & 344.31 & 314.34 & 347.71 & 355.52 & 344.08 & 357.56 & 339.38 \\
\hline$\Delta$ & \begin{tabular}{c}
$I_{n}$ \\
$I_{l}$
\end{tabular} & \begin{tabular}{l}
+4.63 \\
+31.29
\end{tabular} & \begin{tabular}{l}
+21.32 \\
+46.41
\end{tabular} & \begin{tabular}{c}
12.18 \\
+28.8
\end{tabular} & \begin{tabular}{l}
+1.32 \\
+22.08
\end{tabular} & \begin{tabular}{l}
-11.55 \\
+19.40
\end{tabular} & \begin{tabular}{l}
-6.93 \\
+22.34
\end{tabular} & \begin{tabular}{c}
-8.34 \\
+29.23
\end{tabular} & \begin{tabular}{l}
+9.34 \\
+38.17
\end{tabular} & \begin{tabular}{l}
+56.52 \\
+20.37
\end{tabular} & \begin{tabular}{l}
+44.52 \\
+26.76
\end{tabular} \\
\hline
\end{tabular}

Table 3: The quantitative results of Intg. $\uparrow$ for the EntityQuestions dataset. The LLMs' order and symbol definitions are the same as Table 2 .

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline $\mathcal{D}$ & $\mathcal{K}$ & G-1.3 & G-2.7 & $0-1.3$ & O-2.7 & b-560 & $\mathrm{b}-1 \mathrm{~b} 1$ & $\mathrm{~b}-1 \mathrm{b7}$ & $\mathrm{b}-3$ & L-7 & $\mathrm{L}-13$ \\
\hline \multirow[b]{2}{*}{ Vanilla } & $\overline{I_{n}}$ & 531.54 & $\overline{605.06}$ & 602.52 & 634.28 & 488.95 & 594.88 & 608.85 & 619.30 & 6007.22 & 632.24 \\
\hline & $I_{l}$ & 247.50 & 284.47 & 277.47 & 299.03 & 222.99 & 266.91 & 284.00 & 289.26 & 289.95 & 287.48 \\
\hline \multirow{2}{*}{ Keywords } & $\overline{I_{n}}$ & $\overline{280.76}$ & $\overline{360.00}$ & $\overline{403.37}$ & $\overline{439.73}$ & $\overline{295.02}$ & $\overline{428.54}$ & $\overline{465.15}$ & 462.65 & 584.67 & 574.61 \\
\hline & $I_{l}$ & 134.96 & 167.13 & 196.04 & 215.41 & 143.68 & 207.59 & 227.84 & 223.38 & 287.84 & 284.53 \\
\hline \multirow{2}{*}{ Summary } & $I_{n}$ & 366.73 & 406.72 & 501.51 & 446.50 & 388.36 & 415.61 & 501.90 & 435.49 & 425.70 & 438.31 \\
\hline & $I_{l}$ & 179.97 & 205.02 & 255.51 & 210.93 & 187.75 & 197.43 & 257.16 & 211.83 & 210.34 & 222.92 \\
\hline \multirow{2}{*}{ SelCon } & $I_{n}$ & 298.49 & 405.22 & 471.36 & 468.18 & 215.52 & 460.37 & 451.41 & 539.49 & 623.91 & 641.01 \\
\hline & $I_{l}$ & 144.69 & 195.05 & 231.76 & 223.55 & 108.45 & 214.94 & 217.40 & 261.79 & $\overline{295.33}$ & $\overline{304.57}$ \\
\hline \multirow{2}{*}{ Ours } & $I_{n}$ & $\mathbf{5 5 1 . 5 0}$ & 618.18 & 609.88 & 652.48 & 483.02 & 600.72 & 624.53 & 621.36 & $\overline{664.18}$ & $\overline{703.67}$ \\
\hline & $I_{l}$ & 267.12 & 298.74 & 285.06 & 303.49 & 243.55 & 286.20 & 295.45 & 300.29 & 303.39 & 320.87 \\
\hline \multirow[t]{2}{*}{$\Delta$} & $I_{n}$ & +19.96 & +13.12 & +7.36 & +18.2 & \begin{tabular}{l}
-5.93 \\
\end{tabular} & +5.84 & +15.58 & +2.06 & +56.96 & +71.43 \\
\hline & $I_{l}$ & +19.62 & +14.27 & & +4.45 & +20.56 & & +11.45 & +11.03 & +13.44 & +33.39 \\
\hline
\end{tabular}
the longer context setting $\left(I_{l}\right)$, the Acc. of our method consistently outperforms that of various backbone LLMs coupled with other context compression methods. This trend suggests that the concepts distilled by our method are supportive of reducing interference and enabling the LLMs to concentrate on key knowledge. Moreover, the positive values of $\Delta$ in Table 2 and Table 3 for the $I_{l}$ interval further underscore the improvement achieved by our framework over baseline methods when handling longer contexts. This observation emphasizes the effectiveness of the AMR-based concept distillation algorithm in capturing essential semantic information from supporting documents, thereby enabling LLMs to generate more accurate answers even when confronted with messy contexts.

When setting the bloom-560m model as the backbone LLMs, an interesting finding is that $\Delta$ exhibits negative trends in the $I_{n}$ interval of both datasets, while the SelCon does not perform ideally either. We hypothesize that this is due to the limitation of small-scale models to associate semantic scenarios through discrete concepts, which results in the model's inability to understand the core information expressed in the compressed supporting documents. Conversely, when coupling advanced LLMs, such as LLaMA-2, the contexts compressed by the proposed method and SelCon exhibit the most significant and second most significant enhancements to the LLMs, respectively. This observation likely arises from these large-scale models' superior contextual understanding capabilities, which corroborates our hypothesis. Regarding the improvements of $\Delta$ on $I_{l}$ interval of two datasets, our method's enhancement on the PopQA dataset is more pronounced. This is because PopQA was released recently, and its knowledge is less likely to be memorized by earlier models such as GPT-Neo and OPT. Moreover, the screening of long-tail knowledge further accentuates the unique scenario provided by PopQA, making it an ideal testbed for evaluating context compression methods.

The proposed AMR-based concept distillation method demonstrates clear advantages over generative compression methods of keyword extraction and summarization. While these methods utilise the LLMs to generate compressed representations and show competitive results in certain cases, they may inadvertently introduce noise or lose essential details during the compression process. Moreover, the generative nature of these methods makes them inherently difficult to control, even when provided with instructions as constraints. Consequently, the generated keywords and summaries may exhibit randomness, potentially deviating from the core concepts conveyed in the original supporting documents. In contrast, our framework leverages the inherent structured semantic representation of AMR to capture the core concepts explicitly. This semantic-level abstraction enables the framework to faithfully format the concepts to provide more reliable and informative support for the RAG process.

Compared to the linguistics context compression baseline, SelCon, which identifies and prunes redundant content based on self-information computed at the lexical level, the proposed method based on the semantic level achieves superior results. SelCon's effectiveness depends on determining the right granularity for redundancy removal, making it sensitive to lexical unit choice. In contrast, our method takes a macro view by focusing on the semantic consistency carried by the AMR structure, making it insensitive to the delicate lexical bias. This characteristic enables it to be a reliable plug-andplay component in various RAG systems dealing with supporting documents containing irrelevant information and potential lexical errors. The robustness of the proposed framework is demonstrated by its consistent performance improvements across various LLMs. The experimental results on both datasets showcase the generalizability of our method, irrespective of the underlying LLM architecture. This finding suggests that the concept-based RAG framework can be effectively coupled with diverse LLMs, making it a versatile solution for enhancing inference performance in long-context scenarios.

\section*{6 Conclusion and Future Research}

This paper introduces a novel concept-based RAG framework that utilizes AMR to distil essential concepts from long-context supporting documents, enabling LLMs to focus on the most supportive knowledge for accurate question-answering efficiently. The proposed AMR-based concept distillation algorithm systematically traverses the AMR graph to format key concept nodes with informative semantic features, transforming redundant supporting documents into a concise concept set. The proposed framework significantly enhances RAG performance compared with baselines comprising various backbone LLMs and context compression methods. To the best of our knowledge, this is the first work to augment RAG with AMR, offering a novel direction for integrating reliable structured semantic representations with RAG to handle tasks requiring high fidelity to the knowledge.

It has been demonstrated that the LLMs with fewer parameters within the proposed framework can also exhibit comparable or superior performance to larger models in certain cases. Consequently, it is plausible to speculate on the feasibility of employing small-scale LLMs solely equipped with the general natural language understanding capabilities, coupled with comprehensive and informative concept sets, to implement the lightweight Q\&A systems. This approach would alleviate the constraints imposed by the computational complexity of large-scale LLMs during their practical application and deployment. Exploring this possibility will be one of the focus of our future research.

\section*{References}

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[2] T. Ahmed, K. Pai, P. Devanbu, and E. T. Barr. Automatic semantic augmentation of language model prompts (for code summarization). In 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE), pages 1004-1004, Los Alamitos, CA, USA, apr 2024. IEEE Computer Society. URLhttps://doi.ieeecomputersociety.org/.

[3] Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178-186, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://aclanthology.org/W13-2322.

[4] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

[5] Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https: //doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.

[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

[7] Qi Cao, Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Unnatural error correction: GPT-4 can almost perfectly handle unnatural scrambled text. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8898-8913, Singapore, December 2023. Association for Computational Linguistics. doi: $10.18653 / v 1 / 2023$.emnlp-main.550. URL https://aclanthology.org/2023.emnlp-main.550.

[8] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.

[9] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 38293846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.emnlp-main.232. URL https://aclanthology.org/2023.emnlp-main. 232.

[10] Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, and Xia Hu. Learning to compress prompt in natural language formats. arXiv preprint arXiv:2402.18700, 2024.

[11] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285.

[12] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024.

[13] Giacomo Frisoni, Paolo Italiani, Stefano Salvatori, and Gianluca Moro. Cogito ergo summ: Abstractive summarization of biomedical papers via semantic parsing graphs and consistency rewards. Proceedings of the AAAI Conference on Artificial Intelligence, 37(11):12781-12789, Jun. 2023. doi: 10.1609/aaai.v37i11.26503. URL https://ojs.aaai.org/index.php/ AAAI/article/view/26503

[14] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.

[15] Arthur C Graesser, Murray Singer, and Tom Trabasso. Constructing inferences during narrative text comprehension. Psychological review, 101(3):371, 1994.

[16] Aman Gupta, Anup Shirgaonkar, Angels de Luis Balaguer, Bruno Silva, Daniel Holstein, Dawei Li, Jennifer Marsman, Leonardo O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. Rag vs finetuning: Pipelines, tradeoffs, and a case study on agriculture. arXiv preprint arXiv:2401.08406, 2024.

[17] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3929-3938. PMLR, 13-18 Jul 2020. URL https:// proceedings.mlr.press/v119/guu20a.html

[18] Thanh Lam Hoang, Gabriele Picco, Yufang Hou, Young-Suk Lee, Lam Nguyen, Dzung Phan, Vanessa Lopez, and Ramon Fernandez Astudillo. Ensembling graph predictions for amr parsing. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 8495-8505. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/ 2021/file/479b4864e55e12e0fb411eadb115c095-Paper.pdf.

[19] Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. Advancing transformer architecture in long-context large language models: A comprehensive survey. arXiv preprint arXiv:2311.12351, 2023.

[20] Anubhav Jangra, Preksha Nema, and Aravindan Raghuveer. T-STAR: Truthful style transfer using AMR graph as intermediate representation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8805-8825, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.602. URL https://aclanthology.org/2022.emnlp-main. 602

[21] Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. What does BERT learn about the structure of language? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651-3657, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/ P19-1356. URL https://aclanthology.org/P19-1356.

[22] Rebecca L Johnson, Manuel Perea, and Keith Rayner. Transposed-letter effects in reading: evidence from eye movements and parafoveal preview. Journal of Experimental psychology: Human perception and performance, 33(1):209, 2007.

[23] Jaehun Jung, Ximing Lu, Liwei Jiang, Faeze Brahman, Peter West, Pang Wei Koh, and Yejin Choi. Information-theoretic distillation for reference-less summarization. arXiv preprint arXiv:2403.13780, 2024.

[24] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020

Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main. 550 .

[25] Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. Hurdles to progress in long-form question answering. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4940-4957, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.393. URL https://aclanthology.org/2021.naacl-main. 393 .

[26] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. 2022.

[27] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 9459-9474. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 6b493230205f780e1bc26945df7481e5-Paper.pdf

[28] Yafu Li, Leyang Cui, Jianhao Yan, Yongjing Yin, Wei Bi, Shuming Shi, and Yue Zhang. Explicit syntactic guidance for neural text generation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14095-14112, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long. 788 . URLhttps://aclanthology.org/2023.acl-long.788.

[29] Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6342-6353, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.391. URL https://aclanthology.org/ 2023.emnlp-main. 391

[30] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics, 12:157-173, 02 2024. ISSN 2307-387X. doi: 10.1162/tacl_a_00638. URLhttps://doi.org/10.1162/tacl_a_00638

[31] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726-742, 2020.

[32] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9802-9822, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.546. URL https://aclanthology. org/2023.acl-long.546.

[33] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 17359-17372. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/ paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf.

[34] Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. Advances in Neural Information Processing Systems, 36, 2024.

[35] Aaron Mueller and Tal Linzen. How to plant trees in language models: Data and architectural effects on the emergence of syntactic inductive biases. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11237-11252, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.629. URL https://aclanthology.org/2023.acl-long.629.

[36] Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. Generating benchmarks for factuality evaluation of language models. arXiv preprint arXiv:2307.06908, 2023.

[37] Saurav Pawar, SM Tonmoy, SM Zaman, Vinija Jain, Aman Chadha, and Amitava Das. The what, why, and how of context length extension techniques in large language models-a detailed survey. arXiv preprint arXiv:2401.07872, 2024.

[38] Leonardo F. R. Ribeiro, Mengwen Liu, Iryna Gurevych, Markus Dreyer, and Mohit Bansal. FactGraph: Evaluating factuality in summarization with semantic graph representations. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3238-3253, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.236. URL https://aclanthology.org/2022.naacl-main. 236.

[39] Brenda Santana, Ricardo Campos, Evelin Amorim, Alípio Jorge, Purificação Silvano, and Sérgio Nunes. A survey on narrative extraction from textual data. Artificial Intelligence Review, 56 (8):8393-8435, 2023.

[40] Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. Simple entity-centric questions challenge dense retrievers. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6138-6148, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.496. URL https://aclanthology.org/2021.emnlp-main. 496

[41] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379-423, 1948.

[42] Kaize Shi, Xueyao Sun, Li He, Dingxian Wang, Qing Li, and Guandong Xu. AMR-TST: Abstract Meaning Representation-based text style transfer. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 4231-4243, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.260. URL https://aclanthology.org/2023. findings-acl.260.

[43] Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela. Masked language modeling and the distributional hypothesis: Order word matters pretraining for little. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2888-2913, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.230. URL https://aclanthology.org/2021.emnlp-main. 230

[44] Jiao Sun, Xuezhe Ma, and Nanyun Peng. AESOP: Paraphrase generation with adaptive syntactic control. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5176-5189, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.420. URL https://aclanthology.org/2021.emnlp-main. 420

[45] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[46] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. A comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313, 2024.

[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[48] Xinyi Wang, Hieu Pham, Pengcheng Yin, and Graham Neubig. A tree-based decoder for neural machine translation. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4772-4777, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1509. URL https://aclanthology. org/D18-1509

[49] Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia, PA, 23:170, 2013.

[50] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5621-5634, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.412. URL https://aclanthology.org/2022.findings-emnlp. 412

[51] EC Wit and Marie Gillette. What is linguistic redundancy. University of Chicago, 1999.

[52] Kevin Wu, Eric Wu, and James Zou. How faithful are rag models? quantifying the tug-of-war between rag and llms' internal prior, 2024.

[53] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025, 2023.

[54] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation. arXiv preprint arXiv:2401.15884, 2024.

[55] Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang. A survey of knowledge-enhanced text generation. ACM Computing Surveys, 54(11s):1-38, 2022.

[56] Hamed Zamani, Bhaskar Mitra, Everest Chen, Gord Lueck, Fernando Diaz, Paul N. Bennett, Nick Craswell, and Susan T. Dumais. Analyzing and learning from user interactions for search clarification. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, page 1181-1190, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450380164. doi: 10.1145/3397271.3401160. URL https://doi-org.ezproxy.lib.uts.edu.au/10.1145/3397271.3401160.

[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[58] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren's song in the ai ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.

[59] Zixuan Zhang, Nikolaus Parulian, Heng Ji, Ahmed Elsayed, Skatje Myers, and Martha Palmer. Fine-grained information extraction from biomedical literature based on knowledgeenriched Abstract Meaning Representation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language

Processing (Volume 1: Long Papers), pages 6261-6270, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.489. URL https: //aclanthology.org/2021.acl-long. 489

[60] Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and Mrinmaya Sachan. Controlled text generation with natural language instructions. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 42602-42613. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/zhou23g.html.

\section*{A AMR Examples}

This appendix presents an example of an AMR graph parsed from the supporting document, "Alexander Rinnooy Kan of Amsterdam. In 1972-73, he worked as a mathematician at Spectrum Encyclopedia". Fig. A1 showcases the DFS order for traversing the nodes in the AMR graph. Table A1]delineates the parsed AMR graph marked with the distilled concepts, showing that DFS traversal maintains the relative position of the adjacent concepts contained in the raw retrieved supporting document.

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-17.jpg?height=445&width=1390&top_left_y=661&top_left_x=365)

Figure A1: The node order of the DFS traversal within the AMR graph, with nodes marked in different colours for the two sentences. The marked nodes here are not the final distilled concepts.

Table A1: Example of the parsed AMR graph. The nodes carrying concepts are bolded, including "Alexander Rinnooy Kan, Amsterdam. work, mathematics, Spectrum Encyclopedia, 1972, 1973". The special roles, :wiki, :year, and :name are highlighted.

\begin{tabular}{c}
\hline Supporting Document: Alexander Rinnooy Kan of Amsterdam. In \\
1972-73, he worked as a mathematician at Spectrum Encyclopedia. \\
\hline$(\mathrm{m} / \mathrm{multi-sentence}$ \\
:snt1 (p / person \\
:name (n / name \\
:op1 "Alexander" \\
:op2 "Rinnooy" \\
:op3 "Kan") \\
:location (c / city \\
:wiki "Amsterdam" \\
:name (n2 / name \\
:op1 "Amsterdam"))) \\
:snt2 (w / work-01 \\
:ARG0 (h / he) \\
:ARG1 (m2 / mathematics) \\
:ARG2 (r / research-institute \\
:wiki "Spectrum_Encyclopedia" \\
:name (n3 / name \\
:op1 "Spectrum" \\
:op2 "Encyclopedia")) \\
:time (d / date-interval \\
:op1 (d2 / date-entity \\
:year 1972) \\
:op2 (d3 / date-entity \\
:year 1973))) ) \\
\hline
\end{tabular}

\section*{B Prompts for Baselines}

We refer to Taori et al. [45] to set up prompt templates for baselines with different instructions, as illustrated in Table A2

Table A2: The prompt for extracting and generating the keywords and summaries of supporting documents as baselines.

Instruction (Keywords): Extract a few keywords from the following content. Instruction (Summary): Generate a short summary of the following content.

```

Prompt = """Below is an instruction that describes a task, paired with an input that provides
content.
\#\#\# Instruction: $\{" "$ + Instruction + """ $\}$
\#\#\# Input: $\{" " n+\mathcal{D}+" " n\}$
\#\#\# Response: ""

```

\section*{C Accuracy Details}

The detailed accuracy (Acc.) for the different concept compression methods on PopQA and EntityQuestions datasets are presented in Table A3 and Table A4, respectively.

\section*{D Concept Random Traverse}

\section*{D. 1 Global Random Traverse}

Global random traversal involves randomly traversing nodes throughout the AMR graph to capture concepts in a non-DFS manner, covering all sentences in the supporting documents. Concepts obtained in this setting have no specific positional relationship. The results are presented in Table A5 and Table A6. These results reveal changes in Acc. due to the concepts derived from global random traversal. Apart from a few Acc. values higher than the concepts derived from the AMR-based concept distillation algorithm, most of the other Acc. values corresponding to the same settings decrease. However, these changes in trend are not significant. We speculate that the reason is that global random traverse alters the positions of key concepts supporting the question within the concept set, leading to changes in LLMs' performance. This observation aligns with conclusions drawn by Liu et al. [30], which also demonstrates the necessity and rationality of using the DFS method to traverse the AMR graph to maintain the inherent positional relationship of concepts between the source texts.

\section*{D. 2 Local Random Traverse}

Local random traversal involves traversing nodes within a specific sentence of the AMR graph in a non-DFS manner to capture concepts. The concepts obtained through this approach do not maintain the positional relationship within individual sentences but preserve it between sentences in the retrieved supporting document. The results are shown in Table A7 and Table A8 Similar to global random traversal, in this case, Acc. shows a decreasing trend, but the magnitude of the decline is not substantial. We speculate that the reason is that, compared to the Vanilla methods, the word-level length of the distilled concept set is shorter, which reduces the sensitivity of LLMs in RAG to the position of key information when dealing with long contexts. The compression ratio of the original supporting documents using different methods will be elaborated in the next section.

\section*{E Compression Ratio}

This section compares the word-level compression ratio achieved by different context compression methods with the results shown in Fig. A2. The findings demonstrate that the proposed AMR-based context distillation algorithm achieves competitive context-compression performance. Compared to the standard Vanilla method, our method can compress the average word-level length by over $60 \%$ while preserving the core concepts to the greatest extent possible. Notably, our method achieves the

Table A3: Accuracy $($ Acc. $\uparrow$ ) comparison on the PopQA dataset. The best results for each LLM with setting $\mathcal{K}$ are in bold, and the second best results are in underlined. $\Delta$ here represents the difference between the methods of "Ours" and "Vanilla", and the increased and decreased $\Delta$ are marked differently. The best results for each of $\mathcal{K}$ are marked.

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline LLMs & $\mathcal{C}^{\mathcal{K}}$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline \multirow{6}{*}{\begin{tabular}{l}
$\stackrel{m}{n}$ \\
$\stackrel{1}{c}$ \\
$\stackrel{D}{2}$ \\
$\underset{0}{1}$
\end{tabular}} & Vanilla & 47.69 & 71.38 & 69.19 & 67.56 & 60.87 & 73.51 & 69.44 & 72.15 & 72.73 & 80.00 \\
\hline & Keywords & 20.46 & 27.85 & 39.81 & 48.47 & 59.63 & 51.66 & 58.33 & 68.35 & 68.18 & $\frac{72.86}{7}$ \\
\hline & Summary & 22.76 & 41.70 & 50.24 & 59.54 & 57.14 & 68.87 & 56.48 & 59.49 & 72.73 & 80.00 \\
\hline & SelCon & 21.14 & 34.74 & 39.57 & 41.60 & 52.17 & 56.29 & 47.22 & 58.23 & $\overline{68.18}$ & $\overline{71.43}$ \\
\hline & Ours & 34.96 & 55.62 & 61.61 & 64.89 & 64.60 & 77.48 & 75.93 & 77.22 & 83.33 & 94.29 \\
\hline & $\Delta$ & -12.73 & -15.76 & -7.58 & -2.67 & +3.73 & +3.97 & +6.49 & +5.07 & +10.60 & +14.29 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-19.jpg?height=200\&width=88\&top_left_y=713\&top_left_x=386) } & Vanilla & 58.13 & 76.13 & 71.56 & 73.66 & 69.56 & 72.19 & 73.15 & 59.49 & 65.15 & 82.86 \\
\hline & Keywords & 23.98 & 33.51 & 46.92 & 56.49 & $\overline{58.39}$ & $\overline{66.89}$ & $\overline{59.26}$ & 59.49 & $\overline{60.61}$ & 62.86 \\
\hline & Summary & 23.71 & 40.63 & 49.29 & 59.16 & 59.01 & 65.56 & 68.52 & 55.70 & 63.64 & 80.00 \\
\hline & SelCon & 33.88 & 48.58 & 46.45 & 57.25 & 62.73 & 59.60 & 58.33 & 74.68 & 62.12 & 75.71 \\
\hline & Ours & 43.09 & 61.44 & 64.45 & 70.61 & 74.53 & 76.82 & 77.78 & $\mathbf{8 9 . 8 7}$ & 74.24 & 82.86 \\
\hline & $\Delta$ & $\overline{-15.04}$ & $\overline{-14.69}$ & $\overline{-7.11}$ & -3.05 & +4.97 & +4.63 & +4.63 & +30.38 & +9.09 & 0.00 \\
\hline \multirow{6}{*}{$\frac{0}{\frac{n}{0}}$} & Vanilla & 62.47 & 75.52 & 63.51 & 76.34 & 70.81 & 76.82 & 68.52 & 74.68 & 74.24 & 90.00 \\
\hline & Keywords & 25.07 & 33.89 & 49.05 & $\overline{58.78}$ & 68.94 & 67.55 & 75.00 & 74.68 & 74.24 & 80.00 \\
\hline & Summary & 27.91 & 47.05 & 59.48 & 69.08 & 72.67 & 81.46 & $\overline{73.15}$ & 74.68 & 81.82 & 92.86 \\
\hline & SelCon & 40.11 & 56.77 & 58.06 & 67.18 & 65.22 & 70.20 & 72.22 & 82.28 & 78.79 & $\overline{90.00}$ \\
\hline & Ours & 47.56 & 63.12 & 63.51 & 76.72 & $\mathbf{7 2 . 6 7}$ & 78.81 & 77.78 & $\overline{87.34}$ & $\overline{77.27}$ & 95.71 \\
\hline & $\Delta$ & $\overline{-14.91}$ & $\overline{-12.40}$ & 0.00 & +0.38 & +1.86 & $\overline{+1.99}$ & +9.26 & +12.66 & +3.03 & +5.71 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-19.jpg?height=198\&width=88\&top_left_y=1104\&top_left_x=386) } & Vanilla & 62.33 & 77.43 & 72.99 & 77.48 & 66.45 & 78.80 & 77.78 & 79.75 & 80.30 & 90.00 \\
\hline & Keywords & 27.51 & 32.44 & 49.05 & 62.60 & 70.81 & $\overline{74.17}$ & $\overline{68.52}$ & 78.48 & $\overline{77.27}$ & $\overline{88.57}$ \\
\hline & Summary & 26.29 & 46.14 & 58.06 & 66.41 & $\overline{63.98}$ & 74.17 & 71.30 & 65.82 & 72.72 & 87.14 \\
\hline & SelCon & 33.74 & 51.34 & 50.24 & 60.69 & 65.22 & 78.15 & 75.93 & 84.81 & 80.30 & 87.14 \\
\hline & Ours & 49.86 & 65.11 & 68.96 & 71.76 & 72.67 & 81.46 & 83.33 & $\overline{89.87}$ & $\overline{81.81}$ & 97.14 \\
\hline & $\Delta$ & $\overline{-12.47}$ & $\overline{-12.32}$ & $\overline{-4.03}$ & $\overline{-5.72}$ & +6.22 & +2.66 & +5.55 & +10.12 & +1.51 & +7.14 \\
\hline \multirow{6}{*}{\begin{tabular}{l} 
E \\
o \\
$\sim$ \\
白 \\
0 \\
0 \\
\end{tabular}} & Vanilla & \begin{tabular}{l}
52.30 \\
\end{tabular} & 64.58 & 63.03 & $\mathbf{6 8 . 3 2}$ & 67.08 & 71.52 & 73.14 & 77.22 & 66.67 & 84.29 \\
\hline & Keywords & 21.68 & 29.23 & 46.21 & 51.15 & $\overline{57.76}$ & $\overline{64.24}$ & $\overline{63.89}$ & $\overline{73.42}$ & 66.67 & $\overline{78.57}$ \\
\hline & Summary & 24.39 & 42.16 & 53.79 & 63.36 & 61.49 & 72.82 & 75.00 & 75.95 & 75.76 & 82.86 \\
\hline & SelCon & 22.90 & 30.68 & 34.83 & 40.46 & 48.45 & 54.30 & 50.93 & 63.29 & $\overline{54.55}$ & 68.57 \\
\hline & Ours & 35.91 & 49.12 & 54.50 & 68.32 & 68.32 & 71.52 & 67.59 & 83.54 & 80.30 & 94.29 \\
\hline & $\Delta$ & $\overline{-16.39}$ & $\overline{-15.46}$ & $\overline{-8.53}$ & 0.00 & +1.24 & $\overline{0.00}$ & -5.55 & +6.32 & +13.63 & +10.00 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-19.jpg?height=200\&width=91\&top_left_y=1487\&top_left_x=384) } & Vanilla & 63.01 & 75.06 & 70.62 & 77.10 & 73.29 & 79.47 & 75.93 & 87.34 & 78.79 & 87.14 \\
\hline & Keywords & 26.15 & 39.40 & 56.87 & 64.89 & 72.67 & $\overline{72.19}$ & 75.93 & $\overline{87.34}$ & 83.33 & 90.00 \\
\hline & Summary & 27.10 & 46.37 & 56.87 & 70.61 & 69.57 & 76.16 & 78.70 & $\overline{73.42}$ & $\overline{80.30}$ & $\overline{85.71}$ \\
\hline & SelCon & 45.53 & 59.68 & 54.74 & 66.41 & 75.16 & 74.83 & $\overline{78.70}$ & 77.22 & 80.30 & 90.00 \\
\hline & Ours & $\overline{42.55}$ & $\overline{58.68}$ & 65.64 & 71.76 & $\overline{79.50}$ & 83.44 & $\overline{83.33}$ & 89.87 & 86.36 & $\overline{92.86}$ \\
\hline & $\Delta$ & -20.46 & -16.38 & -4.98 & $\overline{-5.34}$ & +6.21 & +3.97 & +7.40 & +2.53 & +7.57 & +5.72 \\
\hline \multirow{6}{*}{\begin{tabular}{l} 
呈 \\
白 \\
0 \\
0 \\
\end{tabular}} & Vanilla & 65.04 & 78.35 & 76.30 & 79.77 & 73.29 & 81.46 & 78.70 & 82.28 & 80.30 & 88.57 \\
\hline & Keywords & 28.05 & 39.86 & 55.21 & 65.65 & $\overline{72.67}$ & $\overline{72.85}$ & $\overline{78.70}$ & 92.41 & 83.33 & 94.29 \\
\hline & Summary & 29.54 & 46.98 & 59.24 & 71.76 & 70.19 & 76.82 & $\overline{78.70}$ & $\overline{88.61}$ & $\overline{83.33}$ & $\overline{94.29}$ \\
\hline & SelCon & 40.51 & 57.15 & 54.74 & 67.18 & 63.35 & 73.51 & $\overline{71.30}$ & 82.28 & $\overline{74.24}$ & $\overline{84.29}$ \\
\hline & Ours & 47.43 & 61.74 & 66.82 & 75.19 & 74.53 & 82.78 & 84.26 & 94.94 & 86.36 & 97.14 \\
\hline & $\Delta$ & $\overline{-17.61}$ & $\overline{-16.61}$ & $\overline{-9.48}$ & $\overline{-4.58}$ & +1.24 & +1.32 & +5.56 & +12.66 & +6.06 & +8.57 \\
\hline \multirow{6}{*}{\begin{tabular}{l} 
on \\
E \\
0 \\
$\dot{0}$
\end{tabular}} & Vanilla & 64.36 & 78.81 & 73.70 & 74.05 & 70.80 & 72.85 & 79.62 & 78.48 & 74.24 & 74.29 \\
\hline & Keywords & 28.86 & 40.24 & 56.87 & 64.12 & $\overline{70.19}$ & 72.85 & $\overline{76.85}$ & 83.54 & 77.27 & 88.57 \\
\hline & Summary & 26.29 & 46.60 & 56.40 & 70.22 & 69.57 & 75.50 & 72.22 & 64.56 & 75.76 & 94.29 \\
\hline & SelCon & 46.34 & 58.45 & 59.48 & 66.41 & 68.32 & $\overline{71.52}$ & 68.52 & 87.34 & 80.30 & $\overline{84.29}$ \\
\hline & Ours & 50.00 & 60.67 & 64.69 & 72.52 & 74.53 & 79.47 & 86.11 & 84.81 & $\overline{84.85}$ & 97.14 \\
\hline & $\Delta$ & -14.36 & -18.14 & -9.01 & -1.53 & +3.73 & +6.62 & +6.49 & $\overline{+6.33}$ & +10.61 & +22.85 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-19.jpg?height=197\&width=91\&top_left_y=2072\&top_left_x=384) } & Vanilla & 57.32 & 69.93 & 62.09 & 72.90 & 70.81 & 81.45 & 85.19 & 78.48 & 86.36 & 92.86 \\
\hline & Keywords & 35.91 & 49.50 & 64.93 & 67.56 & 63.35 & 78.15 & $\overline{82.41}$ & 77.22 & 84.85 & $\overline{85.71}$ \\
\hline & Summary & 29.54 & 42.77 & 51.66 & 67.18 & 60.87 & 71.52 & 77.78 & 68.35 & 68.18 & 82.86 \\
\hline & SelCon & 58.81 & 74.37 & 73.93 & 79.00 & 72.05 & 86.09 & 82.41 & 83.54 & 89.39 & 91.43 \\
\hline & Ours & $\overline{67.74}$ & 74.06 & $\overline{74.64}$ & 76.72 & $\overline{78.26}$ & $\overline{87.42}$ & 89.81 & $\overline{86.08}$ & 89.39 & 97.14 \\
\hline & $\Delta$ & +10.42 & $\overline{+4.13}$ & +12.55 & $\overline{+3.82}$ & +7.45 & +5.97 & +4.62 & +7.60 & +3.03 & +4.28 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-19.jpg?height=194\&width=88\&top_left_y=2264\&top_left_x=386) } & Vanilla & 59.21 & 77.74 & 68.96 & 74.43 & 68.94 & 79.47 & 75.93 & 74.68 & 77.27 & 90.00 \\
\hline & Keywords & 37.40 & 50.11 & 59.48 & 66.03 & 67.08 & 79.47 & 75.00 & 72.15 & 83.33 & 91.43 \\
\hline & Summary & 28.86 & 46.29 & 52.13 & 59.16 & 58.39 & 62.91 & 72.22 & 64.56 & 80.30 & $\overline{85.71}$ \\
\hline & SelCon & 64.36 & 74.90 & 74.64 & 75.57 & 71.43 & 84.11 & 82.41 & 83.54 & 78.79 & 91.43 \\
\hline & Ours & 62.06 & $\overline{73.14}$ & $\overline{76.54}$ & $\overline{78.63}$ & $\overline{75.78}$ & 84.11 & $\overline{84.26}$ & $\overline{84.81}$ & 81.82 & $\overline{92.86}$ \\
\hline & $\Delta$ & $\overline{+2.85}$ & -4.60 & +7.58 & +4.20 & +6.84 & +4.64 & +8.33 & +10.13 & $\overline{+4.55}$ & +2.86 \\
\hline
\end{tabular}

Table A4: Accuracy (Acc. $\uparrow$ ) comparison the EntityQuestions dataset. The definitions of symbols are the same with Table A3.

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline LLMs & $\mathcal{C}^{\mathcal{K}}$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=199\&width=84\&top_left_y=492\&top_left_x=388) } & Vanilla & 42.43 & 53.15 & 58.34 & $\underline{57.27}$ & 61.49 & 65.15 & 59.18 & 57.83 & 64.42 & 66.99 \\
\hline & Keywords & 19.39 & 26.62 & 30.15 & $\overline{30.40}$ & $\overline{30.75}$ & $\overline{36.36}$ & $\overline{31.12}$ & $\overline{30.12}$ & $\overline{40.49}$ & $\overline{30.10}$ \\
\hline & Summary & 26.21 & 34.07 & 38.51 & 36.78 & 41.19 & 46.21 & 43.88 & 37.95 & 47.85 & 54.36 \\
\hline & SelCon & 21.13 & 27.68 & 28.81 & 31.28 & 34.63 & 41.67 & 32.65 & 32.53 & 39.26 & 38.83 \\
\hline & Ours & 36.74 & 51.38 & 57.61 & 59.91 & 63.58 & 67.05 & 65.31 & 62.65 & 71.17 & 68.93 \\
\hline & $\Delta$ & -5.69 & -1.77 & -0.73 & +2.64 & +2.09 & +1.90 & \begin{tabular}{l}
+6.13 \\
\end{tabular} & +4.82 & +6.75 & +1.94 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=207\&width=84\&top_left_y=683\&top_left_x=388) } & Vanilla & 51.05 & 61.58 & 65.22 & 67.18 & 65.67 & 70.83 & 69.90 & 70.48 & 71.78 & 73.79 \\
\hline & Keywords & 26.70 & 32.74 & 40.00 & 42.51 & 38.51 & 51.51 & $\overline{42.86}$ & 34.34 & 42.33 & 43.69 \\
\hline & Summary & 25.91 & 36.73 & 40.60 & 42.29 & 45.07 & 48.11 & 48.47 & 53.01 & 55.21 & 48.54 \\
\hline & SelCon & 30.04 & 38.86 & 40.75 & 44.27 & 46.27 & 50.00 & 44.90 & 43.98 & 53.99 & 54.36 \\
\hline & Ours & $\underline{48.95}$ & $\underline{58.03}$ & 64.33 & $\underline{67.00}$ & 69.25 & 72.72 & 72.96 & 71.08 & 78.53 & 79.61 \\
\hline & $\Delta$ & ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=42\&width=92\&top_left_y=842\&top_left_x=645) & -3.55 & $\frac{0.89}{-0.89}$ & -0.18 & +3.58 & +1.89 & +3.06 & +0.60 & +6.75 & +5.82 \\
\hline \multirow{6}{*}{\begin{tabular}{l}
$\stackrel{\text { O}}{\stackrel{1}{\leftarrow}}$ \\
$\stackrel{0}{0}$
\end{tabular}} & Vanilla & 54.04 & 65.66 & 65.67 & 65.64 & 65.07 & 71.97 & 66.33 & 69.88 & 71.78 & 66.99 \\
\hline & Keywords & 29.68 & 33.54 & 40.60 & $\frac{33.61}{43}$ & $\overline{47.46}$ & $\frac{1}{54.55}$ & ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=43\&width=91\&top_left_y=904\&top_left_x=1328) & 50.60 & ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=43\&width=85\&top_left_y=904\&top_left_x=1543) & $\overline{43.69}$ \\
\hline & Summary & 31.18 & 42.86 & 49.10 & 51.98 & 55.22 & 62.50 & 58.67 & 63.25 & 69.33 & 66.02 \\
\hline & SelCon & 36.27 & 44.54 & 45.37 & 49.12 & 54.03 & 56.82 & 55.10 & 52.41 & 63.80 & 64.08 \\
\hline & Ours & 49.13 & 61.93 & 65.52 & 67.62 & 68.06 & 74.24 & 70.41 & 65.66 & 75.46 & $\mathbf{7 2 . 8 2}$ \\
\hline & $\Delta$ & -4.91 & -3.73 & -0.15 & +1.98 & +2.99 & +2.27 & +4.08 & $\overline{-4.22}$ & +3.68 & +5.83 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=196\&width=84\&top_left_y=1071\&top_left_x=388) } & Vanilla & 57.87 & 67.70 & 70.44 & 68.50 & 69.85 & 77.65 & 73.47 & 73.49 & 79.75 & 66.99 \\
\hline & Keywords & 33.51 & 36.82 & 42.69 & $\overline{49.12}$ & $\overline{50.15}$ & $\overline{57.58}$ & 52.04 & $\overline{54.22}$ & $\overline{57.06}$ & $\overline{46.60}$ \\
\hline & Summary & 31.11 & 42.77 & 49.70 & 47.58 & 51.94 & 56.06 & 53.57 & 53.01 & 53.99 & 44.66 \\
\hline & SelCon & 35.43 & 44.19 & 46.87 & 49.78 & 54.63 & 62.88 & 52.04 & 52.41 & 59.51 & 56.31 \\
\hline & Ours & 55.36 & 66.99 & 68.81 & 72.91 & 72.83 & 79.55 & 67.86 & 75.90 & 81.60 & 76.70 \\
\hline & $\Delta$ & -2.51 & -0.71 & $\overline{-1.63}$ & +4.41 & +2.98 & +1.90 & ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=42\&width=91\&top_left_y=1225\&top_left_x=1328) & +2.41 & +1.85 & +9.71 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=199\&width=84\&top_left_y=1265\&top_left_x=388) } & Vanilla & 39.86 & $\mathbf{5 0 . 4 0}$ & 53.58 & 54.63 & $\mathbf{5 7 . 3 1}$ & $\underline{60.23}$ & 52. & 51 & $\underline{58.90}$ & $\underline{59.22}$ \\
\hline & Keywords & 20.89 & 27.15 & 34.03 & 31.72 & 29.25 & 37.50 & 37.24 & $\overline{32.53}$ & $\overline{38.65}$ & $\overline{33.01}$ \\
\hline & Summary & 24.18 & 35.40 & 42.09 & 40.75 & 43.58 & 53.41 & 42.35 & 50.60 & 44.79 & 46.60 \\
\hline & SelCon & 19.87 & 23.96 & 20.90 & 15.42 & 22.09 & 29.55 & 26.53 & 24.10 & 29.45 & 27.18 \\
\hline & Ours & 32.91 & 44.90 & 48.36 & 47.14 & 49.85 & 65.53 & 57.65 & 54.22 & 66.87 & 64.08 \\
\hline & $\Delta$ & -6.95 & -5.50 & $\overline{-5.22}$ & $\overline{-7.49}$ & $\overline{-7.46}$ & +5.30 & +5.10 & +2.41 & +7.97 & +4.86 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=196\&width=88\&top_left_y=1451\&top_left_x=386) } & Vanilla & $\mathbf{5 0 . 7 5}$ & 62.64 & 64.78 & 70.26 & 68.36 & 73.11 & 63.27 & 63.65 & 69.94 & 66.99 \\
\hline & Keywords & 26.81 & 38.24 & 43.73 & 52.20 & 44.78 & $\frac{1.1}{57.20}$ & $\frac{5.1}{53.06}$ & 49.40 & 50.31 & ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=43\&width=84\&top_left_y=1475\&top_left_x=1648) \\
\hline & Summary & 30.46 & 41.08 & 44.18 & 44.93 & 47.76 & 50.00 & 48.98 & 50.60 & 46.63 & 52.43 \\
\hline & SelCon & 35.73 & 45.70 & 46.87 & 52.64 & 52.24 & 60.23 & 55.10 & 50.00 & 53.99 & 51.46 \\
\hline & Ours & 43.87 & 58.74 & 63.88 & $\underline{66.96}$ & 66.27 & 73.48 & 68.37 & 71.08 & 71.17 & 77.67 \\
\hline & $\Delta$ & $\overline{-6.88}$ & -3.90 & $\overline{-0.9}$ & $\overline{-3.30}$ & $\overline{-2.09}$ & +0.37 & +5.10 & +7.43 & +1.23 & +10.68 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=196\&width=84\&top_left_y=1642\&top_left_x=388) } & Vanilla & 51.89 & 64.24 & 64.63 & 67.18 & 66.87 & 71.97 & 68.37 & 70.48 & 71.78 & 74.76 \\
\hline & Keywords & 29.74 & 43. & 45.8 & 51 & $\overline{49.25}$ & $\overline{65.15}$ & $\overline{57.14}$ & $\overline{53.01}$ & $\overline{58.90}$ & $\overline{52.43}$ \\
\hline & Summary & 31.18 & 43.48 & 47. & & 58.51 & 59.09 & 62.76 & & & 66.99 \\
\hline & SelCon & 34.41 & 43.66 & 44.93 & 50.22 & 50.15 & 55.68 & 54.08 & 47.59 & 58.28 & 59.22 \\
\hline & Ours & 50.87 & 62.29 & 63.58 & 67.84 & $\mathbf{7 2 . 2 4}$ & 75.38 & 71.94 & 73.49 & 73.01 & 78.64 \\
\hline & $\Delta$ & $\overline{-1.02}$ & $\overline{-1.95}$ & -1.05 & +0.66 & +5.37 & +3.41 & +3.57 & +3.01 & +1.23 & +3.88 \\
\hline \multirow{6}{*}{\begin{tabular}{l}
$\stackrel{0}{\tilde{U}}$ \\
$\dot{\pi}$ \\
$\stackrel{0}{\oplus}$
\end{tabular}} & Vanilla & 51.94 & 62.73 & 67.1 & $\mathbf{6 8 . 0 6}$ & 68 . & 74.6 & $\underline{69.90}$ & $\underline{72.89}$ & 711.78 & $\underline{74.76}$ \\
\hline & Keywords & 31.60 & 41.08 & 47. & 52 & 51 & $\overline{60.98}$ & $\overline{58.16}$ & $\overline{51.20}$ & 58.28 & $\overline{50.49}$ \\
\hline & Summary & 30.10 & 38.69 & 46.7 & 46.92 & 48.06 & 56.44 & 47.96 & 52.41 & 54.60 & 57.28 \\
\hline & SelCon & 42.97 & 49.16 & 54.18 & 57.49 & 61.49 & 67.80 & 60.20 & 60.24 & 70.55 & 73.79 \\
\hline & Ours & $\underline{49.97}$ & 58.65 & 64.03 & $\underline{65.86}$ & 69.85 & 75.38 & 71.94 & 77.11 & 74.23 & 78.64 \\
\hline & $\Delta$ & -1.97 & -4.1 & -3.58 & & +1.49 & +0.76 & +2.04 & +4.22 & +2.45 & +3.88 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=195\&width=84\&top_left_y=2022\&top_left_x=388) } & Vanilla & 49.67 & 51.73 & 57.76 & 666.30 & 70.15 & 75.00 & 73.47 & 74.39 & 76.69 & 73.79 \\
\hline & Keywords & 41.53 & 56.52 & 55.1 & 62. & $\overline{65.37}$ & $\overline{72.35}$ & 70.41 & $\overline{70.48}$ & $\overline{74.85}$ & 71.84 \\
\hline & Summary & 36.39 & 40.73 & 42.54 & 44.71 & 44.18 & 50.00 & 49.49 & 50.00 & 60.12 & 51.46 \\
\hline & SelCon & 50.99 & 62.91 & 65.22 & 68.50 & 68.96 & 75.00 & 72.45 & 71.08 & 75.46 & 77.67 \\
\hline & Ours & $\overline{60.80}$ & 70.45 & $\overline{71.94}$ & $\overline{75.33}$ & 74.03 & $\overline{77.27}$ & $\overline{72.45}$ & 75.30 & 79.14 & 75.73 \\
\hline & $\Delta$ & +11.13 & +18.72 & +14.18 & & +3.88 & +2.27 & -1.02 & +0.91 & +2.45 & +1.94 \\
\hline \multirow{6}{*}{ ![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-20.jpg?height=189\&width=88\&top_left_y=2220\&top_left_x=386) } & Vanilla & 52.36 & 69.48 & 71.04 & 70 . & $69.2 \mathrm{E}$ & 75.76 & 67.35 & 69.28 & 76.07 & 73.79 \\
\hline & Keywords & 41.77 & 52.97 & 55.07 & $\overline{61.01}$ & 65.67 & $\overline{68.94}$ & 72.96 & 69.28 & 72.39 & 70.87 \\
\hline & Summary & 36.80 & 45.1 & 44. & $42 \quad$ & 42.39 & 46.59 & 52.04 & 57.23 & 60.74 & 59.22 \\
\hline & SelCon & $\underline{52.54}$ & 65.04 & 69.25 & 68.50 & $\underline{70.45}$ & 73.86 & $\underline{74.49}$ & $\underline{75.30}$ & $\underline{78.53}$ & 78.64 \\
\hline & Ours & $\overline{63.61}$ & 74.37 & 74.18 & 81.72 & $\overline{80.00}$ & 81.44 & $\overline{79.08}$ & $\overline{78.31}$ & $\overline{83.44}$ & 78.64 \\
\hline & $\Delta$ & +11.25 & +4.89 & +3.14 & +10.79 & +10.75 & +5.68 & +11.73 & +9.03 & +7.37 & +4.85 \\
\hline
\end{tabular}

Table A5: The Acc. $\uparrow$ results on the PopQA dataset by concept obtained with the global random traverse. Any changes compared to the original proposed AMR-based concept distillation algorithm are marked.

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline $\mathcal{L L M s} \mathcal{K}$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline GPT-N-1.3B & 34.96 & 55.62 & 60.43 & 64.50 & 63.35 & 76.82 & 75.00 & 75.93 & 80.30 & 94.29 \\
\hline GPT-N-2.7B & 43.09 & 61.36 & 63.51 & 69.85 & 74.53 & 72.84 & 77.78 & 89.87 & 74.24 & 82.86 \\
\hline OPT-1.3B & 47.56 & 63.12 & 63.51 & 76.34 & 72.05 & 74.17 & 77.78 & 87.34 & 77.27 & 95.71 \\
\hline OPT-2.7B & 49.86 & 65.11 & 68.96 & 71.76 & 70.80 & 80.79 & 81.48 & 89.87 & 81.81 & 90.00 \\
\hline Bloom-560m & 35.91 & 48.58 & 53.08 & 67.56 & 68.32 & 71.52 & 67.59 & 83.54 & 80.30 & 94.29 \\
\hline Bloom-1b1 & 42.28 & 58.68 & 65.40 & 71.76 & 78.81 & 83.44 & 83.33 & 88.61 & 86.36 & 90.00 \\
\hline Bloom-1b7 & 47.43 & 61.51 & 66.82 & 74.81 & 73.91 & 82.78 & 84.26 & 93.67 & 84.85 & 95.71 \\
\hline Bloom-3b & 49.73 & 60.67 & 64.22 & 71.76 & 74.53 & 79.47 & 85.19 & 84.81 & 83.33 & $95.71 \quad$ \\
\hline LLaMA-2-chat-7b & 62.73 & 74.06 & 74.64 & 76.72 & 78.26 & 87.42 & 85.19 & 86.08 & 86.36 & 92.86 \\
\hline LLaMA-2-chat-13b & 62.06 & 73.14 & 76.54 & 78.63 & 75.78 & 84.11 & 82.41 & 83.54 & 78.79 & 90.00 \\
\hline
\end{tabular}

Table A6: The Acc. $\uparrow$ results on the EntityQuestions dataset by concept obtained with the global random traverse. The symbol definitions are the same with Table A5.

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline LLMs & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline GPT-N-1.3B & 36.74 & 51.38 & 57.61 & 59.91 & 61.49 & 67.05 & 65.31 & 57.83 & 71.17 & 66.99 \\
\hline GPT-N-2.7B & 48.95 & 58.03 & 64.33 & 66.96 & 65.67 & 70.83 & 72.96 & 70.48 & 78.53 & 73.79 \\
\hline OPT-1.3B & 49.13 & 61.93 & 65.52 & 67.62 & 65.07 & 71.97 & 70.41 & 65.66 & 75.46 & 66.99 \\
\hline OPT-2.7B & 55.36 & 66.99 & 68.81 & 72.91 & 69.85 & 77.65 & 67.86 & 73.49 & 81.60 & 66.99 \\
\hline Bloom-560m & 32.91 & 44.90 & 48.36 & 47.14 & 49.85 & 60.23 & 57.65 & 51.81 & 66.87 & 59.22 \\
\hline Bloom-1b1 & 43.87 & 58.74 & 63.88 & 66.96 & 66.27 & 73.11 & 68.37 & 62.65 & 71.17 & 66.99 \\
\hline Bloom-1b7 & 50.87 & 62.29 & 63.58 & 67.84 & 66.87 & 75.38 & 68.37 & 73.49 & 71.78 & 74.76 \\
\hline Bloom-3b & 49.97 & 58.65 & 64.03 & 65.86 & 68.36 & 74.62 & 69.90 & 72.89 & 74.23 & 74.76 \\
\hline LLaMA-2-chat-7b & 60.80 & 70.45 & 71.94 & 75.33 & 74.03 & 77.27 & 72.45 & 75.30 & 79.14 & 7379 \\
\hline LLaMA-2-chat-13b & 63.61 & 74.37 & 74.18 & 81.72 & 80.00 & 81.44 & 79.08 & 78.31 & 83.44 & 78.64 \\
\hline
\end{tabular}

Table A7: The Acc. $\uparrow$ results on the PopQA dataset by concept obtained with the local random traverse. Any changes compared to the original proposed AMR-based concept distillation algorithm are marked.

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline LLMs $\quad \mathcal{K}$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline GPT-N-1.3B & 34.55 & 55.39 & 60.90 & 64.89 & 63.98 & 76.82 & 75.93 & 75.95 & 80.30 & 94.29 \\
\hline GPT-N-2.7B & 42.82 & 61.36 & 63.51 & 69.85 & 73.29 & 72.85 & 77.78 & 89.87 & 72.73 & 81.43 \\
\hline OPT-1.3B & 47.56 & 62.82 & 63.03 & 75.57 & 72.67 & 75.50 & 76.85 & 87.34 & 77.27 & 95.71 \\
\hline OPT-2.7B & 49.86 & 65.11 & 68.96 & 71.37 & 70.81 & 80.79 & 81.48 & 89.87 & 81.81 & 90.00 \\
\hline Bloom-560m & 35.37 & 48.43 & 54.98 & 67.56 & 68.32 & 71.52 & 65.79 & 83.54 & 80.30 & 92.86 \\
\hline Bloom-1b1 & 42.55 & 58.45 & 65.88 & 71.37 & 79.50 & 83.44 & 83.33 & 87.34 & 86.36 & 90.00 \\
\hline Bloom-1b7 & 47.56 & 61.44 & 66.82 & 75.19 & 73.91 & 82.78 & 83.33 & 93.67 & 84.85 & 94.29 \\
\hline Bloom-3b & 49.59 & 60.67 & 64.21 & 71.76 & 74.53 & 79.47 & 86.11 & 84.81 & 83.33 & 94.29 \\
\hline LLaMA-2-chat-7b & 62.60 & 74.06 & 74.64 & 76.72 & 78.88 & 87.42 & 83.33 & 86.08 & 84.85 & 91.43 \\
\hline LLaMA-2-chat-13b & 61.52 & 73.60 & 76.30 & 77.86 & 75.16 & 84.11 & 83.33 & 83.54 & 75.76 & 91.43 \\
\hline
\end{tabular}

Table A8: The Acc. $\uparrow$ results on the EntityQuestions dataset by concept obtained with the local random traverse. The symbol definitions are the same with Table A7.

\begin{tabular}{c|cccccccccc}
\hline LLMs & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & \multirow{2}{*}{10} \\
\hline GPT-N-1.3B & 36.44 & 51.20 & 57.61 & 59.91 & 61.49 & 66.67 & 65.31 & 62.65 & 71.17 & 66.02 \\
\hline GPT-N-2.7B & 48.89 & 58.30 & 64.48 & 67.00 & 65.67 & 72.21 & 72.96 & 71.08 & 78.53 & 79.61 \\
\hline OPT-1.3B & 49.07 & 62.20 & 65.22 & 67.62 & 68.06 & 74.24 & 70.41 & 65.66 & 73.62 & 72.82 \\
\hline OPT-2.7B & 55.36 & 66.90 & 68.81 & 72.69 & 72.83 & 77.65 & 67.35 & 73.49 & 82.20 & 76.70 \\
\hline Bloom-560m & 32.91 & 44.90 & 47.76 & 47.14 & 49.25 & 65.53 & 56.63 & 54.22 & 65.03 & 60.19 \\
\hline Bloom-1b1 & 43.87 & 58.65 & 63.88 & 66.96 & 65.67 & 72.73 & 68.37 & 71.08 & 70.55 & 77.67 \\
\hline Bloom-1b7 & 50.87 & 62.29 & 64.03 & 67.84 & 72.24 & 75.38 & 68.37 & 74.09 & 71.16 & 73.79 \\
\hline Bloom-3b & 49.85 & 58.39 & 64.03 & 65.86 & 68.36 & 75.38 & 69.90 & 77.11 & 75.46 & 78.64 \\
\hline LLaMA-2-chat-7b & 59.90 & 70.90 & 72.39 & 73.73 & 74.03 & 77.27 & 72.45 & 75.30 & 80.37 & 74.76 \\
\hline LLaMA-2-chat-13b & 64.09 & 73.82 & 73.43 & 81.72 & 80.00 & 80.30 & 78.57 & 77.11 & 82.82 & 78.64 \\
\hline
\end{tabular}

highest compression ratio on the PopQA dataset. In contrast, although the Keywords-based compression method exhibits a $6.92 \%$ higher compression ratio on the EntityQuestions dataset compared to ours, its performance lags behind the proposed algorithm in preserving core concepts. Compared to the other two baselines, our method shows a significant advantage in terms of compression ratio.

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-22.jpg?height=808&width=1357&top_left_y=1057&top_left_x=384)

Figure A2: The word-level compression ratio of different context compression methods, while we take the length of the "Vanilla" method as the standard (100.00\%), and the numbers above the histogram represent the length reduction compared to "Vanilla".

\section*{F Inference Latency}

This section analyses the inference latency by providing the inference time comparison of backbone LLMs with different compression methods, the result is shown in Fig. A3 The findings reveal that the context compressed by our method has a significant advantage in the inference of most LLMs. Moreover, the context compressed by the Keywords method also exhibits shorter inference times, which aligns with the trend observed in the previous compression ratio analysis.

![](https://cdn.mathpix.com/cropped/2024_06_04_4a5bf365120ba0b14a1fg-23.jpg?height=616&width=1377&top_left_y=1020&top_left_x=379)

Figure A3: The inference time comparison of different context compression methods. The values in the histogram represent the inference time for each model on a single item, measured in milliseconds.```


[^0]:    ${ }^{1}$ The corresponding complete sentences: [1] The Outfit is a 1973 crime film directed by John Flynn. [2] It stars Robert Duvall, Karen Black, Joe Don Baker and Robert Ryan. [3] Flynn's screenplay is an adaptation of the novel of the same name by Richard Stark. [4] Two hitmen drive to Eddie Macklin's house to assassinate him as he builds a brick wall in his backyard.

