# Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback 

Viet Dac Lai ${ }^{1}$, Chien Van Nguyen ${ }^{1}$, Nghia Trung Ngo ${ }^{1}$, Thuat Nguyen ${ }^{1}$,<br>Franck Dernoncourt ${ }^{2}$, Ryan A. Rossi ${ }^{2}$, Thien Huu Nguyen ${ }^{1}$<br>${ }^{1}$ Dept. of Computer Science, University of Oregon, OR, USA<br>${ }^{2}$ Adobe Research, USA<br>\{vietl@cs, chienn,nghian@cs, thien@cs\}@uoregon.edu<br>$\{f r a n c k . d e r n o n c o u r t, r y r o s s i\} @ a d o b e . c o m$


#### Abstract

A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instructiontuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instructiontune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/ nlp-uoregon/Okapi.


## 1 Introduction

Pre-trained on massive data, large language models (LLMs) with hundreds of billions of parame- ters can unlock new emergent abilities that cannot be achieved with smaller models (Wei et al., 2022). Large generative models such as GPT-3 (Rae et al., 2021) and OPT-175B (Zhang et al., 2022) represent some of the most recent advances in natural language processing (NLP), introducing a new learning paradigm to prompt LLMs to successfully solve a range of challenging tasks in zero-shot and few-shot fashions (Kung et al., 2022; Choi et al., 2023; Jiao et al., 2023; Guo et al., 2023). However, as LLMs are trained with the autoregressive learning objective, they might exhibit unintended behaviours from human expectations (Tamkin et al., 2021; Weidinger et al., 2021; Kenton et al., 2021; Bommasani et al., 2021). To overcome this issue, instruction fine-tuning has been proposed as a prominent approach to align LLMs with human intentions in instructions and conversations (Christiano et al., 2017; Stiennon et al., 2020; Sanh et al., 2021; Wei et al., 2021; Ouyang et al., 2022). Instruction-tuned LLMs can demonstrate significantly improved capabilities in following human instructions and avoiding the production of toxic, biased, or inaccurate texts. As such, two major techniques for instruction tuning feature supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) that are leveraged by the best commercial LLMs such as ChatGPT ${ }^{1}$ and GPT- $4^{2}$ to deliver outstanding dialog performance.

Another issue with LLMs pertains to the massive scales and closed-source nature of the commercial LLMs that greatly restrict accessibility and the extent of interactions with the technology. To this end, there have been growing efforts from the open-source community to create more accessible LLMs with affordable scales while securing competitive performance as the proprietary LLMs, e.g., LLaMA (Touvron et al., 2023), StableLM (Stabil-[^0]

ityAI, 2023), Falcon (Almazrouei et al., 2023), and MTP (MosaicML, 2023). Instruction fine-tuning has also been applied to these open-source language models to improve their abilities to engage with human, and different instruction datasets have been collected either from human annotation or outputs from commercial LLMs to facilitate the tuning process, e.g., Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), LaMini-LM (Wu et al., 2023), and Dolly (Conover et al., 2023).

However, the instruction-following abilities of existing open-source LLMs have been developed mainly for English and some popular languages (i.e., using instruction datasets for those languages), failing to support many other languages of the world to democratize the technologies to a broader population (Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023). To overcome this challenge, a few contemporary work has explored instruction tuning of multilingual LLMs for multiple languages, i.e., Phoenix (Chen et al., 2023) and Bactrian-X (Li et al., 2023). However, their multilingual instruction tuning efforts are limited to only supervised fine-tuning (SFT) techniques, which is unable to examine reinforcement learning with human feedback (RLHF) to further boost the performance for multilingual LLMs.

To fill in this gap, our work aims to develop Okapi, a open-source framework with RLHF-based instruction-tuned LLMs for multiple languages to shed light on their performance compared to the SFT methods in the multilingual settings. Okapi will emphasize on less studied languages and opensource LLMs to better democratize the benefits of instruction-tuned LLMs and provide resources for future research in this area. In particular, an example in the instruction datasets involves an instruction, an input text, and a desired response output/demonstration. In SFT, the pre-trained LLMs are fine-tuned over the instruction triples (instruction, input, output) via supervised learning to promote their alignment with human expectations. In RLHF, generated outputs from the SFT-tuned LLMs are first ranked to provide training signals for reward functions. Afterward, the SFT-tuned models will be further optimized via reinforcement learning utilizing rewards from the trained reward models. As such, RLHF has been successfully employed to create effective commercial LLMs (e.g., InstructGPT, ChatGPT), owning to its ability to learn beyond positive examples associated with only desired demonstrations. By leveraging the reward models, RLHF can observe lower ranking scores for less accurate demonstrations to obtain richer training signals for LLMs. To our knowledge, Okapi is the first work to perform instruction tuning with RLHF for open-source LLMs over multiple languages.

To develop Okapi, we need to overcome the scarcity of necessary instruction datasets in multiple languages to train and evaluate RLHF models. Motivated by the $52 \mathrm{~K}$ instructions from Alpaca (Taori et al., 2023), we leverage Self-Instruct (Wang et al., 2023) to generate $106 \mathrm{~K}$ additional instructions in English, introducing a larger dataset to facilitate RLHF evaluation. Afterward, we utilize ChatGPT to translate the instructions into a diverse set of 26 languages, which can handle instruction examples with programming code via appropriate prompts to enhance translation quality. In addition, we introduce a translation-based prompt for ChatGPT to produce rankings for multiple responses of the same instructions from the LLMs, which will be used to train the reward models for RLHF experiments. Finally, to measure the performance of the fine-tuned LLMs in different languages, we translate three benchmark datasets for LLMs in the widely-used HuggingFace Open LLM Leaderboard (HuggingFace, 2023; Gao et al., 2021) into 26 languages, i.e., ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), and MMLU (Hendrycks et al., 2021), using ChatGPT. These datasets challenge LLMs on diverse aspects, e.g., science reasoning, commonsense inference, world knowledge, and problem-solving, thus providing comprehensive evaluations for our models. To summarize, our contribution in this work is as follows:

## - Developing RLHF-tuned LLMs in multi-

 ple languages: We present Okapi, the first instruction-tuned LLM framework, which are RLHF-based and open-source for multiple languages. Our framework covers 26 diverse languages, including some understudied and low-resource languages for NLP, e.g., Telugu, Ukrainian, Nepali, and Kannada. Using BLOOM (Scao et al., 2022) and LLaMA (Touvron et al., 2023) as the base pretrained LLMs, our experiments illustrate that RLHF generally performs better than SFT for multilingual instruction tuning. Our experiments also highlight the greater challenges of low-resource languages for multilingualinstruction-tuning of LLMs that should be better focused in future research.

## - Resource creation for instruction-tuned

 LLMs in multiple languages: To cater to our experiments with multilingual RLHF, we create instruction resources for 26 different languages, including ChatGPT prompts, instruction datasets, response ranking data, benchmark datasets, and fine-tuned LLMs. We release our data, resources, and models to contribute to the development and research of multilingual instruction-tuned LLMs in the future. The resources for our Okapi framework can be found at: https://github.com/ nlp-uoregon/Okapi.
## 2 Data Preparation

A key requirement for our development of instruction-tuned LLMs with RLHF involves instruction, ranking, and evaluation datasets in multiple languages, especially for low-resource languages. To this end, we perform a comprehensive data collection process to prepare necessary data for our multilingual framework Okapi in 26 languages, divided into four major steps: English instruction generation, instruction translation, ranking data production, and evaluation data creation.

### 2.1 English Instruction Generation

An instruction example to tune LLMs often has three components: an instruction to specify the task, an input text, and an associated output text (i.e., demonstration or label) (Ouyang et al., 2022). As such, current public instruction datasets for LLMs mainly cover English or some popular languages, which are not suitable for our experiments. Also, we note that a few recent instruction datasets such as xP3 (Muennighoff et al., 2022) and Flan (Chung et al., 2022; Longpre et al., 2023) include multilingual data; however, their instructions are still written in English. Additionally, these datasets tend to be converted from NLP task datasets with template instructions, which cannot reflect the flexibility of human-written prompts to encourage effective instruction following in different languages (Wang et al., 2023). Consequently, our goal is to develop instruction datasets with instructions, inputs, and output texts in multiple languages to better realize general prompts from human.

To achieve this goal, our strategy is to first obtain English instructions and then translate them into other languages. The benefits of our approach concern consistent instruction content across languages to facilitate performance comparison while taking advantages of translation systems to enable examination for more languages. As such, there have been several English instruction datasets collected by the open-source community to support instruction tuning of LLMs with different approaches, e.g., Alpaca (Taori et al., 2023), Dolly (Conover et al., 2023), and LaMini-LM (Wu et al., 2023). However, to conveniently scale our data and introduce variations of general instructions, we follow the instruction generation method in Alpaca, which in turn employs the Self-Instruct procedure in (Wang et al., 2023), to produce our English dataset.

Starting with a pool of 175 human-written seed instructions in English over different topics, at each time, Alpaca samples several instructions from the seeds to form an in-context example to prompt the text-davinci-003 model of OpenAI for new instruction generation. The generated instructions are then compared with previous instructions using the ROUGE score, and instructions whose scores are greater than a threshold will be retained. Overall, Alpaca releases $52 \mathrm{~K}$ instructions for tuning LLMs. In this work, we apply the same Self-Instruct procedure as Alpaca to extend its $52 \mathrm{~K}$ instructions to a larger dataset for our RLHF-based models in Okapi. In particular, we generate 106K additional English instructions from Alpaca with two notable extensions. First, we introduce 30 new human-created instructions into the seed set from Alpaca to increase its diversity and coverage. Among others, our new instructions involve prompts for relation extraction, event extraction, event summarization, and logical questions that are not recognized in Alpaca. Second, instead of generating the new instructions from scratch, we condition our generation process on the $52 \mathrm{~K}$ instructions from Alpaca so a new instruction is only saved if it is different enough from Alpaca's and previous instructions per the ROUGE score criteria. Figure 1 shows the top 10 most common root verbs and their top direct noun objects in the $106 \mathrm{~K}$ generated instructions. These verbs and nouns represent $11.4 \%$ of the entire set, which exhibits diverse intents and patterns in our instructions for Okapi.

### 2.2 Instruction Translation

Given the $158 \mathrm{~K}$ English instructions from Alpaca and our generation process, we aim to translate

![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-04.jpg?height=771&width=777&top_left_y=248&top_left_x=251)

Figure 1: The top 10 most frequent root verbs (inner circle) and their top 4 direct noun objects (outer circle) in the $106 \mathrm{~K}$ generated instructions of Okapi. The instructions shown here only represent $11.4 \%$ of all the generated instructions.

them into multiple other languages to obtain data for our multilingual models in Okapi. Table 1 presents 26 selected languages in our framework. Using the data ratios $r$ of the languages in CommonCrawl ${ }^{3}$ to classify languages as in previous work (Bang et al., 2023; Lai et al., 2023), our study encompasses a diverse set of languages, including 8 high-resource languages ( $r>1.0$ ), 11 mediumresource languages $(r>0.1)$, and 7 low-resource languages $(r<0.1)$. Notably, several of our languages, such as Marathi, Gujarati, and Kannada, have received limited attention in NLP.

We utilize ChatGPT to translate the $158 \mathrm{~K}$ English instructions into 26 target languages for Okapi. Compared to traditional machine translation systems, an advantage of ChatGPT for translation is the ability to use prompts to specify different expectations for the translated texts to facilitate diverse types of instructions. For example, we can instruct ChatGPT to preserve code in the instruction examples about programming as we expect code to be the same in the instructions of different natural languages. In addition, as ChatGPT has been fine-tuned on instruction-style data, we expect that it can capture the context to better translate our instructions. Figure 2 shows our prompt to translate English instruction data with ChatGPT.[^1]

| Language | Code | Pop. <br> (M) | CC Size |  | B | $\mathbf{L}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | (\%) | Cat. |  |  |
| English | en | 1,452 | 45.8786 | $\mathrm{H}$ | $\checkmark$ | $\checkmark$ |
| Russian | ru | 258 | 5.9692 | $\mathrm{H}$ | $\checkmark$ | $\checkmark$ |
| German | de | 134 | 5.8811 | H | $\checkmark$ | $\checkmark$ |
| Chinese | zh | 1,118 | 4.8747 | H | $\checkmark$ |  |
| French | $\mathrm{fr}$ | 274 | 4.7254 | H | $\checkmark$ | $\checkmark$ |
| Spanish | es | 548 | 4.4690 | H | $\checkmark$ | $\checkmark$ |
| Italian | it | 68 | 2.5712 | H | $\checkmark$ | $\checkmark$ |
| Dutch | $\mathrm{nl}$ | 30 | 2.0585 | H | $\checkmark$ | $\checkmark$ |
| Vietnamese | vi | 85 | 1.0299 | H | $\checkmark$ |  |
| Indonesian | id | 199 | 0.7991 | $\mathrm{M}$ | $\checkmark$ |  |
| Arabic | ar | 274 | 0.6658 | M | $\checkmark$ |  |
| Hungarian | hu | 17 | 0.6093 | M | $\checkmark$ | $\checkmark$ |
| Romanian | ro | 29 | 0.5637 | M | $\checkmark$ | $\checkmark$ |
| Danish | da | 6 | 0.4301 | M | $\checkmark$ | $\checkmark$ |
| Slovak | sk | 7 | 0.3777 | $\mathrm{M}$ | $\checkmark$ | $\checkmark$ |
| Ukrainian | uk | 33 | 0.3304 | $\mathrm{M}$ | $\checkmark$ | $\checkmark$ |
| Catalan | ca | 10 | 0.2314 | $\mathrm{M}$ | $\checkmark$ | , |
| Serbian | $\mathrm{sr}$ | 12 | 0.2205 | $\mathrm{M}$ | $\checkmark$ | $\checkmark$ |
| Croatian | $\mathrm{hr}$ | 14 | 0.1979 | M | $\checkmark$ |  |
| Hindi | hi | 602 | 0.1588 | $\mathrm{M}$ | $\checkmark$ |  |
| Bengali | bn | 272 | 0.0930 | $\mathrm{~L}$ | $\checkmark$ |  |
| Tamil | ta | 86 | 0.0446 | $\mathrm{~L}$ | $\checkmark$ |  |
| Nepali | ne | 25 | 0.0304 | $\mathrm{~L}$ | $\checkmark$ |  |
| Malayalam | $\mathrm{ml}$ | 36 | 0.0222 | $\mathrm{~L}$ | $\checkmark$ |  |
| Marathi | $\mathrm{mr}$ | 99 | 0.0213 | $\mathrm{~L}$ | $\checkmark$ |  |
| Telugu | te | 95 | 0.0183 | $\mathrm{~L}$ | $\checkmark$ |  |
| Kannada | $\mathrm{kn}$ | 64 | 0.0122 | $\mathrm{~L}$ | $\checkmark$ |  |

Table 1: List of 26 non-English languages in our Okapi framework along with language codes, numbers of first and second speakers (the "Pop." column), data ratios in the CommonCrawl corpus, and language categories. The languages are grouped into categories based on their data ratios in the CommomCrawl corpus: High Resource (H, $>1 \%$ ), Medium Resource (M, $>0.1 \%$ ), and Low Resource (L, > 0.01\%) (Bang et al., 2023). Columns " $B$ " and " $L$ " indicate if a language is supported by the multilingual LLMs BLOOM and LLaMa (respectively) or not.

It is important to note that we directly translate the instruction, input text, and associated output in each English instruction example of our data. This is in contrast to the other multilingual instructiontuning approaches (Li et al., 2023) that only translate instructions and input texts into a target language (using Google Translate); ChatGPT is then prompted to generate response outputs in the target language for the instructions and input texts. The intuition for our approach concerns various potential issues of ChatGPT, e.g., hallucination, bias, mathematical reasoning, and toxic content (Bang et al., 2023; Borji, 2023), that can be exaggerated if ChatGPT is used to produce responses in non-English languages for different types of

Translation Prompt: Translate the values in the following JSON object into <target language $>$ language. You must keep the keys in the JSON object in English. If a value contains programming code, only translate the comments while preserving the code. Your translations must convey all the content in the original text and cannot involve explanations or other unnecessary information. Please ensure that the translated text is natural for native speakers with correct grammar and proper word choices. Your translation must also use exact terminology to provide accurate information even for the experts in the related fields. Your output must only contain a JSON object with translated text and cannot include explanations or other information.

Figure 2: Translation prompt for ChatGPT for multiple languages in Okapi. We organize our instruction examples into JSON objects with fields for translation prompts, instructions, inputs, and outputs send to ChatGPT. <target language $>$ is replaced with the selected languages in our dataset.

tasks/instructions (Lai et al., 2023). The diverse nature of the possible tasks/instructions will also make it more challenging to devise appropriate solutions for these problems in multilingual settings. By generating the instructions and response outputs in English, we aim to capitalize on the greater performance of LLMs for different NLP tasks in English to avoid the exaggeration issues and achieve higher quality instructions in various dimensions. By transitioning to other languages only via the translation task with ChatGPT, we can also dedicate our effort to overcome diverse multilingual challenges for instruction tuning to the translation task, which can allow convenient and effective solutions for further improvement. Table 2 presents the average lengths of translated prompts and response outputs for each language in our data. Translations from Alpaca's original instructions and our new generated data are shown separately for convenient comparison.

### 2.3 Ranking Data Production

To perform RLHF for a LLM in Okapi, we need to obtain ranked response outputs from the model for the same instruction and input to train a reward model. Concretely, given a LLM $M$ and a dataset $S=\left\{\text { inst }_{k}, \text { input }_{k}\right\}_{k=1}^{N}$ with $N$ pairs of instructions inst $_{k}$ and input texts

| Language | Alpaca |  | Generated |  |
| :--- | :---: | :---: | :---: | :---: |
|  | $\mathrm{P}$ | $\mathrm{R}$ | $\mathrm{P}$ | $\mathrm{R}$ |
| English | 49.0 | 56.2 | 51.2 | 58.0 |
| Russian | 72.1 | 131.8 | 76.8 | 134.6 |
| German | 61.1 | 94.2 | 64.6 | 96.0 |
| Chinese | 47.2 | 47.9 | 49.3 | 48.4 |
| French | 51.6 | 65.5 | 54.1 | 67.0 |
| Spanish | 51.3 | 62.7 | 53.8 | 63.8 |
| Italian | 57.4 | 86.5 | 60.5 | 87.8 |
| Dutch | 60.3 | 94.8 | 63.9 | 96.2 |
| Vietnamese | 53.4 | 71.3 | 56.0 | 73.2 |
| Indonesian | 48.6 | 54.9 | 50.8 | 56.3 |
| Arabic | 50.0 | 60.5 | 52.3 | 61.6 |
| Hungarian | 68.4 | 117.3 | 72.7 | 120.0 |
| Romanian | 62.9 | 103.9 | 66.8 | 106.7 |
| Danish | 59.7 | 91.7 | 63.1 | 94.2 |
| Slovak | 65.1 | 110.2 | 69.1 | 113.3 |
| Ukrainian | 76.9 | 149.1 | 82.2 | 152.1 |
| Catalan | 51.8 | 65.1 | 54.3 | 66.7 |
| Serbian | 62.4 | 102.1 | 66.2 | 104.9 |
| Croatian | 63.0 | 102.7 | 66.7 | 104.0 |
| Hindi | 54.7 | 69.5 | 61.2 | 71.0 |
| Bengali | 53.8 | 65.7 | 56.7 | 67.3 |
| Tamil | 55.3 | 65.0 | 58.0 | 67.6 |
| Nepali | 53.8 | 65.9 | 56.5 | 67.2 |
| Malayalam | 57.8 | 75.1 | 61.0 | 77.0 |
| Marathi | 53.6 | 67.7 | 56.3 | 69.4 |
| Telugu | 57.1 | 74.4 | 60.2 | 75.9 |
| Kannada | 55.4 | 69.5 | 58.2 | 71.5 |
| Average | 57.1 | 80.8 | 60.3 | 82.6 |

Table 2: Average lengths of translated prompts (columns " $P$ ") and response outputs (columns " $R$ ") for each language in our Okapi framework. The lengths are computed according to the number of wordpieces produced by the tokenizer of BLOOM. We separate the numbers for the translations from the original Alpaca's data ( $52 \mathrm{~K}$ instructions) and our new generated data (106K instructions).

Translation Prompt for Ranking: You will be given an instruction, an input for the instruction, and four possible responses for the instruction. The input can be empty, shown as <empty>. You need to translate the provided instruction, input, and responses into English.

Instruction: ...

Input: . . .

Response 1: ...

Response 2: ...

Response 3: ...

Response 4: ...

Figure 3: ChatGPT's prompt to translate target language data into English.

Ranking Prompt: Given the translated instruction, input, and responses, you will need to rank the responses according to three factors: correctness with respect to the instruction and input, coherence, and naturalness.

You will need to provide an overall rank for each response when all the three factors are considered. The overall rank for a response must be an integer between 1 and 4 where 1 is for the best response and 4 is the worst response. You cannot assign the same rank for two different responses.

The format of your output must be: for each response: " $<$ Response $r>$ : overall rank: $<1 / 2 / 3 / 4>$ ". The responses must be in original order. Do not include explanation in your output.

## An Example Output from ChatGPT:

Response 1: 3

Response 2: 1

Response 3: 4

Response 4: 2

Figure 4: ChatGPT's prompt to rank translated data in English.

input $_{k}$ for a target language, we first prompt $M$ to generate $T$ output responses output ${ }_{k}=$ $\left\{\right.$ output $_{k}^{1}, \ldots$, output $\left.{ }_{k}^{T}\right\}$ for each pair of instruction and input text $\left(\right.$ inst $_{k}$, input $\left._{k}\right)(T>1)$. Afterward, the responses in output ${ }_{k}$ are ranked according to their fitness and quality for the instruction inst $_{k}$ and input text input ${ }_{k}$. This ranking data $\left\{\right.$ inst $_{k}$, input $_{k}$, output $\left._{k}\right\}$ can then be leveraged to train a reward model to compute a score for each triple of an instruction, an input text, and a potential response output using contrastive learning (Ouyang et al., 2022).

In this work, we also employ ChatGPT to rank the response outputs for multilingual LLMs. Similar to the motivation for our translation-based approach to obtain instruction data in multiple languages, our ranking strategy first asks ChatGPT to translate the instructions and responses $\left\{\right.$ inst $_{k}$, input $_{k}$, output $\left._{k}\right\}$ in a target language into English. The ranking of the responses is then done over the translated English data to exploit the greater quality of ChatGPT for English and limit different challenges associated with multilingual ranking to the translation task. To this end, we engage with ChatGPT in a twoturn dialog to obtain ranking for each example $\left\{\right.$ inst $_{k}$, input $_{k}$, output $\left._{k}\right\}$ in the target language. The first turn is to translate the example into English using the prompt in Figure 3 while the second turn follows up with the first turn to instruct ChatGPT to rank the English translated responses using the ranking prompt in Figure 4. Our two-turn approach allows ChatGPT to condition on the translated English data in the first turn for ranking while ensuring the same format for the ranking output in the second turn for convenient parsing. Overall, we obtain ranked response outputs for $42 \mathrm{~K}$ instructions sampled from the $106 \mathrm{~K}$ generated instructions for each language in Okapi.

### 2.4 Evaluation Data Creation

The HuggingFace Open LLM Leaderboard (HuggingFace, 2023) recently adopts a suite of tasks and datasets in the Eleuther AI Language Model Evaluation Harness framework (Gao et al., 2021) to facilitate performance assessment and tracking of newly developed LLMs. We employ three datasets in this leaderboard i.e., AI2 Reasoning Challenge (ARC) (Clark et al., 2018), HellaSwag (Zellers et al., 2019), and MMLU (Hendrycks et al., 2021), to evaluate the model performance for our Okapi framework. All the datasets are organized as multiple-choice question-answering tasks although they focus on different types of knowledge and reasoning aspects. ARC involves 1170 grade-school science questions; HellaSwag provides 9162 commonsense inference questions that are easy for humans, but difficult for many state-of-the-art models; and MMLU assesses accuracy for 13062 questions over various branches of knowledge (STEM, humanities, social sciences, and more). Nevertheless, although the LLM community has widely adopted the leaderboard for performance examination, the datasets are only provided for English, thus unable to evaluate LLMs for the languages in our work. To this end, we translate the examples of the three datasets into 26 selected languages using ChatGPT and the translation prompt in Figure 2. The translated datasets are then reserved to evaluate the LLMs in our Okapi framework.

## 3 Reinforcement Learning with Human Feedback

We follow three steps to develop a fine-tuned LLM with RLHF for each target language in our Okapi framework: supervised fine-tuning, reward model training, and reinforcement learning.

Supervised Fine-tuning (SFT): Starting with a multilingual pre-trained LLM as the base, e.g., BLOOM (Scao et al., 2022), we fine-tune the base model with our instruction dataset for the target language using supervised learning. In Okapi, the base model is fine-tuned for three epochs via the autoregressive objective. Our training process uses a cosine learning rate schedule with 200 warm-up steps, an initial learning rate of $2 e-5$, a batch size of 128 , and a weight decay of 0.05 . Finally, instead of leveraging approximation techniques for efficient fine-tuning, we fine-tune the entire base LLM for all of its parameters with SFT to accurately understand the model performance for multilingual settings.

Reward Model Training: The goal of this step is to train a reward model for the target language that will compute reward signals for the reinforcement learning frameworks to further optimize the SFT-tuned model from the previous step. For each pair of a prompt and potential response, our reward model returns a scalar value to quantify the appropriateness of the response with respect to the instruction and input text in the prompt. We exploit the response-ranked datasets in Section 2.3 for this training step. Using the ranking information, an example to train our reward model for a language involves an instruction and an input text (to form a prompt $x$ ) along with two sampled responses $y_{c}$ and $y_{r}$ for $x$ from our datasets. Based on the ranking information, we can assume one of the responses (i.e., $y_{c}$ ) is more preferable than the other (i.e., $y_{r}$ ). In the next step, the binary ranking loss (Ouyang et al., 2022) is employed to train our reward model, aiming to assign a higher score $r\left(x, y_{c}\right)$ for the preferred response $y_{c}$ than the score $r\left(x, y_{r}\right)$ for $y_{r}: L_{\text {reward }}(\theta)=$ $-\mathbb{E}_{\left(x, y_{c}, y_{r}\right)}\left[\log \sigma\left(r_{\theta}\left(x, y_{c}\right)-r_{\theta}\left(x, y_{r}\right)\right)\right]$. For the training process, we initialize the reward model for the target language from the SFT-tuned model from previous step. We train our reward model for 2 epochs with a batch size of 64 and a learning rate of $1 e-5$, using the AdamW optimizer.

Reinforcement Learning: With the reward model established for the target language, the SFT model undergoes additional fine-tuning through reinforcement learning (RL) to align it with human preferences. For this purpose, we employ the Proximal Policy Optimization (PPO) algorithm (Ouyang et al., 2022). Specifically, our training process maximizes the mean reward of the model via the objective: $L_{R L}(\phi)=$ $-\mathbb{E}_{x \sim D_{R L}, y \sim \pi_{\phi}(y \mid x)}\left[r_{\theta}(x, y)-\beta K L(x, y)\right]$. Here, $D_{R L}$ corresponds to the prompt distribution, and $\pi_{\phi}(y \mid x)$ denotes the policy or language model with parameters $\phi$ that require optimization. $\pi_{\phi}(y \mid x)$ is initialized with the SFT-tuned model $\pi_{\phi}(y \mid x)$. Also, $K L(x, y)=D_{K L}\left(\pi_{\phi}(y \mid x) \| \pi_{0}(y \mid x)\right)$ is the Kullback-Leibler divergence to penalize large deviation of $\pi_{\phi}$ from the initial SFT policy $\pi_{0}$, and $\beta$ is a penalty coefficient.

During the RL training phase, we keep the entire LLM frozen and solely train the top four layers for five epochs. We employ the AdamW optimizer with $\beta_{1}=0,9, \beta_{2}=0.95$, and eps $=1 e-8$. The $\mathrm{KL}$ coefficient $\beta$ is set to 0.05 , while the weight decay is 0.1 , and the learning rate is $1 e-6$. In each PPO iteration, we work with a batch size of 32 and a clip threshold of 0.2 in Okapi.

## 4 Experiments

Our Okapi framework utilizes two multilingual LLMs: BLOOM (Scao et al., 2022) and LLaMA (Touvron et al., 2023) as the base models for the fine-tuning processes. We focus on their 7Bparameter versions to facilitate the computing resources and achieve fairer comparison. For each base model and target language, we carry out both SFT-based and RLHF-based instruction-tuning for the model in the following manners:

- SFT: The base model is fine-tuned over the $158 \mathrm{~K}$ translated instructions (i.e., $52 \mathrm{~K}$ from Alpaca and $106 \mathrm{~K}$ from our generation) in the supervised manner.
- RLHF: The base model is first fine-tuned with supervised training over $52 \mathrm{~K}$ translated instructions from Alpaca. Afterward, a reward model is trained to score generated responses for input prompts using contrastive learning over the ranked responses for the $42 \mathrm{~K}$ translated instructions in Section 2.3. Note that the ranked responses are sampled from the SFTtuned base model over $52 \mathrm{~K}$ translated Alpaca instructions from previous step. Finally, given the reward model, the SFT-tuned base model is further optimized via reinforcement learning over $64 \mathrm{~K}$ remaining translated instructions from our generation set (Ouyang et al., 2022).

The translated datasets ARC, HellaSwag, and MMLU are exploited to evaluate the performance
of the models in Okapi. Following the HuggingFace Open LLM Leaderboard, the Eleuther AI Language Model Evaluation Harness framework (Gao et al., 2021) is used to compute the model performance over the datasets for each language in our framework. As a reference, we also report the performance of the base models BLOOM and LLaMA in the experiments. Finally, for BLOOM, we further compare with BLOOMZ (Muennighoff et al., 2022), which is the fine-tuned version of BLOOM over the cross-lingual task mixture dataset $\mathrm{xP} 3$ with millions of multilingual instructions to achieve instruction-following ability.

|  | Language | BLOOM | BLOOMZ | SFT | RLHF |
| :---: | :---: | :---: | :---: | :---: | :---: |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-08.jpg?height=308&width=42&top_left_y=939&top_left_x=256) | Russian | 27.5 | 25.5 | 29.2 | 30.3 |
|  | German | 26.3 | 25.4 | 24.9 | 25.5 |
|  | Chinese | 37.3 | $\overline{37.0}$ | 37.9 | 40.0 |
|  | French | 36.7 | 37.6 | 37.6 | 41.2 |
|  | Spanish | 38.1 | 37.2 | 39.7 | 41.5 |
|  | Italian | 29.0 | 27.5 | 29.3 | 31.3 |
|  | Dutch | 23.1 | 21.5 | 24.8 | 26.1 |
|  | Vietnamese | 33.7 | 33.5 | 35.0 | 36.2 |
|  | Ave Group | 31.5 | 30.7 | 32.3 | 34.0 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-08.jpg?height=454&width=54&top_left_y=1265&top_left_x=253) | Indonesian | 36.0 | 35.9 | 37.4 | 38.8 |
|  | Arabic | 31.4 | 31.2 | 32.1 | 33.2 |
|  | Hungarian | 25.9 | 22.8 | 25.2 | 27.5 |
|  | Romanian | 26.9 | 23.4 | 27.5 | 30.3 |
|  | Danish | 24.6 | 24.6 | 23.6 | 25.2 |
|  | Slovak | 24.9 | 22.5 | 26.2 | 27.3 |
|  | Ukrainian | 22.8 | 23.1 | 23.6 | 25.2 |
|  | Catalan | 34.7 | 35.8 | 35.1 | 38.9 |
|  | Serbian | 25.1 | 23.6 | 25.6 | 27.8 |
|  | Croatian | 23.7 | 22.8 | 22.7 | 24.1 |
|  | Hindi | 29.2 | 28.2 | 28.5 | 29.6 |
|  | Ave Group | 27.7 | 26.7 | 28.0 | 29.8 |
| U <br> 0 <br> 0 <br> 0 <br> 0 <br> 1 <br> 0 <br> 0 | Bengali | 26.2 | 25.5 | 26.8 | 28.9 |
|  | Tamil | 24.2 | 25.6 | 23.7 | 25.1 |
|  | Nepali | 22.3 | 22.7 | 23.4 | 25.7 |
|  | Malayalam | 26.4 | 25.1 | 24.6 | 24.7 |
|  | Marathi | 27.3 | 24.8 | 25.8 | 26.0 |
|  | Telugu | 24.3 | 25.8 | 23.9 | 24.5 |
|  | Kannada | 24.7 | 24.6 | 24.5 | 24.6 |
|  | Ave Group | 25.1 | 24.9 | 24.7 | 25.6 |
|  | Average | 28.2 | 27.4 | 28.4 | $\mathbf{3 0 . 0}$ |

Table 3: Performance of the models on the translated ARC dataset over different languages in Okapi. BLOOM 7B is used as the base LLM.

Evaluation: Tables 3, 4, and 5 present the performance of the models on the ARC, HellaSwag, and MMLU datasets (respectively) when BLOOM is used as the base model. Similarly, Tables 6,7 , and 8 report the performance with the base model LLaMA over the three datasets. In the tables, in addition to the average scores over all languages for the models, we also include the average scores for each group of languages (i.e., rows "Ave Group"

|  | Language | BLOOM | BLOOMZ | SFT | RLHF |
| :---: | :---: | :---: | :---: | :---: | :---: |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-08.jpg?height=329&width=46&top_left_y=282&top_left_x=1072) | Russian | 32.5 | 33.1 | 32.9 | 34.2 |
|  | German | 32.4 | 33.1 | 34.7 | 35.9 |
|  | Chinese | 51.2 | 42.6 | 51.8 | 53.8 |
|  | French | 56.6 | 45.7 | 55.9 | 58.7 |
|  | Spanish | 56.7 | 48.7 | 56.1 | 59.0 |
|  | Italian | 40.8 | 40.3 | 43.1 | 44.6 |
|  | Dutch | 31.7 | 32.3 | 32.6 | 34.9 |
|  | Vietnamese | 48.3 | 40.6 | 49.0 | 51.3 |
|  | Ave Group | 43.8 | 39.6 | 44.5 | 46.6 |
| : <br> Un <br> ä¹‹ | Indonesian | 49.5 | 42.0 | 50.0 | 52.2 |
|  | Arabic | 43.3 | 39.5 | 44.3 | 47.0 |
|  | Hungarian | 30.1 | 29.8 | 30.8 | 32.7 |
|  | Romanian | 31.8 | 32.3 | 33.1 | 35.2 |
|  | Danish | 31.2 | 31.5 | 33.8 | 35.7 |
|  | Slovak | 29.8 | 29.6 | 31.4 | 32.9 |
|  | Ukrainian | 30.0 | 30.4 | 32.2 | 33.6 |
|  | Catalan | 51.2 | 40.3 | 50.9 | ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-08.jpg?height=37&width=95&top_left_y=865&top_left_x=1709) |
|  | Serbian | 29.9 | 30.1 | 30.7 | 33.7 |
|  | Croatian | 30.0 | 29.4 | 30.5 | 31.6 |
|  | Hindi | 36.4 | 34.0 | 37.7 | 39.7 |
|  | Ave Group | 35.7 | 33.5 | 36.9 | 38.9 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-08.jpg?height=326&width=54&top_left_y=1051&top_left_x=1071) | Bengali | 32.8 | 31.5 | 33.9 | 35.4 |
|  | Tamil | 29.4 | 29.5 | 30.0 | 30.4 |
|  | Nepali | 30.9 | 31.9 | 32.5 | 34.1 |
|  | Malayalam | 28.8 | 29.8 | 29.7 | 30.2 |
|  | Marathi | 31.0 | 31.9 | 31.7 | 32.5 |
|  | Telugu | 29.2 | 30.7 | 30.0 | 31.7 |
|  | Kannada | 30.3 | 30.9 | 30.7 | 32.1 |
|  | Ave Group | 30.3 | 30.9 | 31.2 | 32.3 |
|  | Average | 36.8 | 34.7 | 37.7 | 39.5 |

Table 4: Performance of the models on the translated HellaSwag dataset over different languages in Okapi. BLOOM 7B is used as the base LLM.

for high-, medium-, and low-resource languages) to facilitate the comparisons. As some of our selected languages (especially the low-resource ones) are not supported by LLaMA, our tables for the experiments with LLaMA will omit those languages (see Table 1).

The first observation from the tables is that RLHF is generally better than SFT for multilingual fine-tuning of LLMs over different tasks, base models, and language groups. The improvement of average performance over all languages can go up to $2.5 \%$ on the HellaSwag dataset with LLaMA, thus demonstrating the advantages of RLHF over SFT for fine-tuning multilingual LLMs. It is also evident from the tables that the RLHF-tuned models can significantly improve the performance of the original base models (i.e., BLOOM and LLaMa) for almost all the language groups and tasks, which further highlights the quality of the generated instruction data and the effectiveness of RLHF.

Additionally, we observe that the average performance improvement achieved through RLHF

|  | Language | BLOOM | BLOOMZ | SFT | RLHF |
| :---: | :---: | :---: | :---: | :---: | :---: |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-09.jpg?height=329&width=46&top_left_y=283&top_left_x=253) | Russian | 26.2 | 25.4 | 26.5 | 26.8 |
|  | German | 28.1 | 25.6 | 27.0 | 28.6 |
|  | Chinese | 29.1 | 27.2 | 27.7 | 28.2 |
|  | French | 27.4 | 27.7 | 27.7 | 28.4 |
|  | Spanish | 28.9 | 27.1 | 27.8 | 28.1 |
|  | Italian | 25.7 | 25.8 | 25.1 | 26.0 |
|  | Dutch | 26.4 | 26.0 | 26.1 | 26.0 |
|  | Vietnamese | 28.1 | 26.3 | 27.0 | 27.5 |
|  | Ave Group | 27.5 | 26.4 | 26.9 | 27.5 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-09.jpg?height=441&width=51&top_left_y=609&top_left_x=255) | Indonesian | 26.9 | 26.3 | 26.8 | 27.5 |
|  | Arabic | 27.5 | 24.4 | 27.4 | 27.7 |
|  | Hungarian | 26.9 | 26.1 | 25.4 | 26.3 |
|  | Romanian | 27.4 | 25.9 | 27.6 | 27.4 |
|  | Danish | 27.1 | 25.2 | 27.2 | 26.9 |
|  | Slovak | 26.1 | 26.3 | 26.4 | 26.1 |
|  | Ukrainian | 26.6 | 25.8 | 25.9 | 26.4 |
|  | Catalan | 28.8 | 26.0 | 26.7 | 27.6 |
|  | Serbian | 27.2 | 25.7 | 27.5 | 27.6 |
|  | Croatian | 26.0 | 26.1 | 26.4 | 27.7 |
|  | Hindi | 27.5 | 25.9 | 26.8 | 26.5 |
|  | Ave Group | 27.1 | 25.8 | 26.7 | 27.1 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-09.jpg?height=329&width=51&top_left_y=1049&top_left_x=255) | Bengali | 28.2 | 25.9 | 27.1 | 26.8 |
|  | Tamil | 26.6 | 26.7 | 26.1 | 26.0 |
|  | Nepali | 26.6 | 25.6 | 25.5 | 25.2 |
|  | Malayalam | 26.4 | 25.2 | 25.8 | 25.8 |
|  | Marathi | 26.3 | 26.0 | 26.1 | 26.1 |
|  | Telugu | 26.2 | 25.7 | 25.4 | 25.9 |
|  | Kannada | 26.7 | 26.0 | 26.6 | 26.8 |
|  | Ave Group | 26.7 | 25.9 | 26.1 | 26.1 |
|  | Average | 27.1 | 26.0 | 26.6 | 26.9 |

Table 5: Performance of the models on the translated MMLU dataset over different languages in Okapi. BLOOM 7B is used as the base LLM.

![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-09.jpg?height=802&width=711&top_left_y=1592&top_left_x=273)

Table 6: Performance of the models on the translated ARC dataset over different languages in Okapi. LLaMA $7 \mathrm{~B}$ is used as the base LLM.

is more substantial for the ARC and HellaSwag

|  | anguage | LLaMA | SFT | RLHF |
| :---: | :---: | :---: | :---: | :---: |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-09.jpg?height=257&width=45&top_left_y=290&top_left_x=1114) | Russian | 45.7 | 46.0 | 49.1 |
|  | German | 49.9 | 49.0 | 52.6 |
|  | French | 55.7 | 55.6 | 56.9 |
|  | Spanish | 56.4 | 55.7 | 56.6 |
|  | Italian | 52.0 | 52.5 | 55.9 |
|  | Dutch | 48.7 | 48.1 | 51.3 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-09.jpg?height=470&width=48&top_left_y=554&top_left_x=1116) | Ave Group | 51.4 | 51.2 | 53.7 |
|  | Hungarian | 37.9 | 38.7 | 41.0 |
|  | Romanian | 44.9 | 45.1 | 48.7 |
|  | Danish | 46.7 | 47.7 | 51.7 |
|  | Slovak | 35.9 | 39.5 | 43.6 |
|  | Ukrainian | 44.1 | 46.9 | 47.7 |
|  | Catalan | 49.6 | 49.2 | 49.0 |
|  | Serbian | 41.1 | 42.6 | 45.0 |
|  | Croatian | 41.1 | 42.4 | 45.2 |
|  | Ave G $\mathbf{\imath}$ | 42.7 | 44.0 | 46.5 |
|  | Average | 46.4 | 47.1 | 49.6 |

Table 7: Performance of the models on the translated HellaSwag dataset over different languages in Okapi. LLaMA 7B is used as the base LLM.

|  | anguage | LLaMA | SFT | RLHF |
| :---: | :---: | :---: | :---: | :---: |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-09.jpg?height=268&width=56&top_left_y=1282&top_left_x=1110) | Russian | 30.2 | 30.0 | 30.6 |
|  | German | 29.9 | 30.4 | 31.7 |
|  | French | 30.5 | 31.0 | 30.7 |
|  | Spanish | 30.3 | 30.4 | 30.9 |
|  | Italian | 29.9 | 30.6 | 30.4 |
|  | Dutch | 29.8 | 30.0 | 31.1 |
|  | Ave Group | 30.1 | 30.4 | 30.9 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_118813e390623bc6d5acg-09.jpg?height=430&width=52&top_left_y=1588&top_left_x=1116) | Hungarian | 29.0 | 29.2 | 30.1 |
|  | Romanian | 29.7 | 29.8 | 30.9 |
|  | Danish | 30.0 | 30.9 | 31.8 |
|  | Slovak | 29.4 | 29.6 | 30.2 |
|  | Ukrainian | 29.4 | 30.8 | 31.6 |
|  | Catalan | 30.2 | 30.3 | 30.5 |
|  | Serbian | 29.2 | 29.7 | 30.4 |
|  | Croatian | 29.3 | 29.2 | 30.0 |
|  | Ave Group | 29.5 | 29.9 | 30.7 |
|  | Average | 29.8 | 30.1 | 30.8 |

Table 8: Performance of the models on the translated MMLU dataset over different languages in Okapi. LLaMA 7B is used as the base LLM.

datasets, while it is less pronounced for the MMLU dataset. Based on the nature of the datasets, we attribute this phenomenon to the better alignment between our instruction data for fine-tuning with the necessary knowledge and reasoning skills in ARC and HellaSwag than those in MMLU. In particular, ARC and HellaSwag mainly test the abilities of the models on basic knowledge (i.e., from 3rd grade to 9th) and commonsense inference while MMLU fo-
cuses on professional knowledge in different areas (e.g., STEM, social sciences, humanities). As our instructions are generated with the seeds similar to Alpaca's styles (Taori et al., 2023), they tend to emphasize on general knowledge and basic inference skills, thus more aligning with the ARC and HellaSwag datasets. To this end, the generated instructions cannot well activate/complement the language and knowledge skills related to MMLU from the LLMs to attain meaningful improvement from instruction tuning.

Comparing the performance of the models across language groups, we find that the models tend to achieve the highest performance for the highresource languages, followed by the mediumresource and low-resource languages across different base models. The performance improvement of RLHF for low-resource languages is also the least (based on the base model BLOOM), promoting it a challenging area for further research. Interestingly, our fine-tuned BLOOM models with $158 \mathrm{~K}$ generated instructions can significantly outperform BLOOMZ over almost all the languages for the ARC, HellaSwag, and MMLU datasets using either SFT or RLHF. For example, the average performance of RLHF is $4.8 \%$ better than those for BLOOMZ over HellaSwag. As BLOOMZ has finetuned BLOOM over more than $78 \mathrm{M}$ multilingual instructions converted from NLP datasets (Muennighoff et al., 2022), it demonstrates the higher quality of our generated instructions for multilingual instruction tuning of LLMs.

## 5 Related Work

We consider two dimensions of related work in this study, i.e., multilingual tuning and multilingual evaluation.

Multilingual Tuning: With the introduction of the Transformer architecture (Vaswani et al., 2017), various language models have been explored to boost performance for NLP tasks, including the encoder models BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), the decoder models GPT (Radford et al., 2019; Brown et al., 2020), and the encoder-decoder models BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). These language models are often trained first over English data, and then extended to other languages in two main approaches: monolingual and multilingual models. In the monolingual approach, a language model is trained specifically for a particular language, e.g., for Spanish (MMG, 2021), Japanese (Wongso, 2021), French (Martin et al., 2020; Kamal Eddine et al., 2021), Polish (Resources and Technology Infrastructure, 2021), Sweddish (Moell, 2021), and Hindi (Parmar, 2021). In contrast, the multilingual approach explores a single language model that is trained on multilingual texts to serve multiple languages and achieve knowledge transfer for lower-resource languages, e.g., the encoderonly models mBERT (Devlin et al., 2019), XLMRoBERTa (Conneau et al., 2020), the decoder-only models mBART (Liu et al., 2020) and mT5 (Xue et al., 2021), and the decoder-only models BLOOM (Scao et al., 2022) and LLaMA (Touvron et al., 2023).

Based on the pre-trained language models (PLMs), the most advanced methods for NLP in different languages involve fine-tuning the PLMs on training data of the downstream tasks (Min et al., 2023), leading to state-of-the-art performance for multilingual Sentence Splitting (Nguyen et al., 2021a), Dependency Parsing (Kondratyuk and Straka, 2019), Question Answering (Huang et al., 2019), and Named Entity Recognition (Pires et al., 2019) (among others). Additionally, fine-tuning multilingual PLMs (such as XLM-RoBERTa) has proven to be an effective technique to enable zero-shot cross-lingual transfer learning across languages for various NLP tasks. This convenient approach allows for a seamless extension of NLP models to encompass larger sets of languages ( $\mathrm{Wu}$ and Dredze, 2019; Karthikeyan et al., 2020; Wu et al., 2022; Nguyen et al., 2021b; Guzman-Nateras et al., 2022).

Instruction tuning can be considered as a special type of fine-tuning techniques for PLMs where generative PLMs (e.g., GPT) are further trained with instruction data to accomplish instruction following and response alignment with human expectations. Supervised fine-tuning (SFT) is the most common instruction tuning approach that is leveraged by all of the existing LLMs, including ChatGPT, Apaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), and LaMini-LM (Wu et al., 2023). Reinforcement learning from human feedback can also be used to further improve the instruction following abilities of LLMs (Wei et al., 2021; Ouyang et al., 2022) although this technique has been less explored by current open-source LLMs due to the challenges in obtaining ranking data for the reward models. For multilingual learning, instruction tuning is only
applied in the form of SFT for non-English languages using multilingual LLMs, e.g., BLOOM and LLaMA, in a few contemporary work (Chen et al., 2023; Li et al., 2023; Muennighoff et al., 2022). RLHF has not been studied for instruction tuning for non-English languages.

Multilingual Evaluation: A major hurdle for research in multilingual learning pertains to the scarcity of evaluation datasets for NLP tasks in various languages that hinders model development and measurement. As such, prior research has invested substantial efforts to tackle this challenge, introducing multilingual datasets for a diversity of NLP tasks. These tasks include Universal Dependencies (Nivre et al., 2016), Named Entity Recognition with CoNLL 2002 and 2003 (Sang and Meulder, 2002, 2003), Natural Language Inference with XNLI (Conneau et al., 2018), Information Retrieval with TyDi (Zhang et al., 2021), Question Answering with XQuAD (Artetxe et al., 2020), Summarization with XWikis (Perez-Beltrachini and Lapata, 2021), Event Extraction with MEE (Pouran Ben Veyseh et al., 2022), and many other tasks with XGLUE (Liang et al., 2020) and XTREME (Hu et al., 2020). However, these multilingual datasets are not specifically designed for evaluation of generative LLMs as our focus in this work.

To this end, the Eleuther AI Language Model Evaluation Harness (Gao et al., 2021) provides an unified framework to evaluate generative language models over different knowledge and reasoning skills. The HuggingFace Open LLM Leaderboard (HuggingFace, 2023) leverages four key benchmarks from this framework, i.e., ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), and MMLU (Hendrycks et al., 2021), and Truthful1QA (Lin et al., 2022), which have been widely adopted to measure progress of LLMs. However, these datasets are not usable for our multilingual framework as they only support the evaluation for English.

## 6 Conclusion

We present the first framework, called Okapi, on instruction tuning for LLMs in multiple language using reinforcement learning from human feedback (RLHF). To address the scarcity of necessary data for multilingual instruction tuning, we introduce instruction and response-ranked data in 26 diverse languages to enable the training of supervised finetuning models, reward models, and reinforcement learning frameworks for multilingual LLMs. Our experiments reveal the benefits of RLHF for multilingual fine-tuning of LLMs and the challenging problems of low-resource languages in this area for future research.

## Limitations

Despite our efforts to develop and evaluate instruction-tuned LLMs in multiple languages using reinforcement learning from human feedback, our work suffers from several limitations that can be improved in future work. First, although we have attempted to cover a diverse set of 26 languages, there are still many other languages in the world that are not considered in our work. Future work can extend our system to include more languages, especially for low-resource languages, to gain a more comprehensive understanding for the language generalization of the instruction tuning methods and better democratize the technologies. Second, our system only leverages the base models BLOOM and LLaMA with 7B parameters. While this approach can facilitate the computing infrastructure of a larger group of institutions for further research, it will be beneficial to support other types of multilingual base models, e.g., the encoderdecoder model mT5 (Xue et al., 2021), and other model scales (e.g., the 13B and 65B models) to strengthen the system. Third, to obtain instruction and evaluation data for the development, we automatically generate instructions in English and translate them into multiple languages using ChatGPT. We also rely on ChatGPT to obtain responseranked data for the reward models in RLHF. Although our approach enables the extension to multiple languages with affordable development costs, the generated and translated data might involve unexpected noise. Additionally, they might not perfectly reflect human-provided instruction data in different languages. To this end, future work can improve our system with human-generated instruction and evaluation data to further examine instruction tuning for multilingual LLMs. Finally, our evaluations only investigate the performance of the models on benchmark datasets for generative LLMs, which focus on testing diverse knowledge, reasoning skills, and truthful generation. Other important concerns of generative models such as hallucination, toxicity, and biases are not evaluated explicitly in our experiments. Future work can study these issues to better characterize instruction
tuning methods in the multilingual settings.

## References

Ebtesam Almazrouei, Hamza Alobeidli, and Abdulaziz Alshamsi et al. 2023. Falcon-40B: an open large language model with state-of-the-art performance.

Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623-4637, Online. Association for Computational Linguistics.

Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. ArXiv, abs/2302.04023.

Rishi Bommasani, Drew A. Hudson, and Ehsan Adeli et al. 2021. On the opportunities and risks of foundation models. ArXiv, abs/2108.07258.

Ali Borji. 2023. A categorical archive of chatgpt failures. ArXiv, abs/2302.03494

Tom Brown, Benjamin Mann, and et al. 2020 Language models are few-shot learners. ArXiv, $\mathrm{abs} / 2005.14165$.

Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. 2023. Phoenix: Democratizing chatgpt across languages. ArXiv, $\mathrm{abs} / 2304.10453$.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.

Jonathan Choi, Kristin Hickman, Amy Monahan, and Daniel Schwarcz. 2023. Chatgpt goes to law school. Available at SSRN.

Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS).

Hyung Won Chung, Le Hou, S. Longpre, and Barret Zoph et al. 2022. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai 2 reasoning challenge. ArXiv, $\mathrm{abs} / 1803.05457$.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics.

Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475-2485, Brussels, Belgium. Association for Computational Linguistics.

Mike Conover, Matt Hayes, and Ankit Mathur et al. 2023. Free dolly: Introducing the world's first truly open instruction-tuned llm. https://www. databricks.com.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Leo Gao, Jonathan Tow, Stella Biderman, and et al. 2021. A framework for few-shot language model evaluation.

Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. ArXiv, $\mathrm{abs} / 2301.07597$.

Luis Guzman-Nateras, Minh Van Nguyen, and Thien Nguyen. 2022. Cross-lingual event detection via optimized adversarial training. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5588-5599, Seattle, United States. Association for Computational Linguistics.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding.

Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation. In Proceedings of the International Conference on Machine Learning (ICML).

Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. 2019. Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2485-2494, Hong Kong, China. Association for Computational Linguistics.

HuggingFace. 2023. Open llm leaderboard. https: //github.com/tatsu-lab/stanford_alpaca.

Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? yes with gpt-4 as the engine. ArXiv, 2301.08745 .

Moussa Kamal Eddine, Antoine Tixier, and Michalis Vazirgiannis. 2021. BARThez: a skilled pretrained French sequence-to-sequence model. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9369-9390, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

K Karthikeyan, Zihan Wang, Stephen Mayhew, and Dan Roth. 2020. Cross-lingual ability of multilingual bert: An empirical study. In Proceedings of the International Conference on Learning Representations (ICLR).

Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. 2021. Alignment of language agents. ArXiv, abs/2103.14659.

Dan Kondratyuk and Milan Straka. 2019. 75 languages, 1 model: Parsing Universal Dependencies universally. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2779-2795, Hong Kong, China. Association for Computational Linguistics.

Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille ElepaÃ±o, Maria Madriaga, Rimel Aggabao, Giezel DiazCandido, James Maningo, et al. 2022. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. medRxiv.

Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. ArXiv, abs/2304.05613

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020 BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.

Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. 2023. Bactrian-x: A multilingual replicable instruction-following model with low-rank adaptation. ArXiv, abs/2305.15011.

Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. 2020. XGLUE: A new benchmark dataset for cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6008-6018, Online. Association for Computational Linguistics.

Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214-3252, Dublin, Ireland. Association for Computational Linguistics.

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pretraining for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726-742.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.

S. Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. ArXiv, abs/2301.13688.

Louis Martin, Benjamin Muller, Pedro Javier Ortiz SuÃ¡rez, Yoann Dupont, Laurent Romary, Ã‰ric de la Clergerie, DjamÃ© Seddah, and BenoÃ®t Sagot. 2020. CamemBERT: a tasty French language model. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 72037219, Online. Association for Computational Linguistics.

Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Survey.

MMG MMG. 2021. Spanish roberta.

Birger Moell. 2021. Swedish roberta.

MosaicML. 2023. Introducing mpt-7b: A new standard for open-source, commercially usable llms. https: //www.mosaicml.com/blog/mpt-7b.

Niklas Muennighoff, Thomas Wang, and Lintang Sutawika et al. 2022. Crosslingual generalization through multitask finetuning. ArXiv, abs/2211.01786.

Minh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Veyseh, and Thien Huu Nguyen. 2021a. Trankit: A light-weight transformer-based toolkit for multilingual natural language processing. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 80-90, Online. Association for Computational Linguistics.

Minh Van Nguyen, Tuan Ngo Nguyen, Bonan Min, and Thien Huu Nguyen. 2021b. Crosslingual transfer learning for relation and event extraction via word category and class alignments. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5414-5426, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan HajiÄ, Christopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel Zeman. 2016. Universal Dependencies v1: A multilingual treebank collection. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC).

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, $\mathrm{abs} / 2203.02155$.

Suraj Parmar. 2021. Hindi roberta.

Laura Perez-Beltrachini and Mirella Lapata. 2021. Models and datasets for cross-lingual summarisation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9408-9423, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996-5001, Florence, Italy. Association for Computational Linguistics.
Amir Pouran Ben Veyseh, Javid Ebrahimi, Franck Dernoncourt, and Thien Nguyen. 2022. MEE: A novel multilingual event extraction dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9603-9613, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog.

Jack Rae, Sebastian Borgeaud, and et al. 2021. Scaling language models: Methods, analysis \& insights from training gopher. ArXiv, abs/2112.11446.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research.

Common Language Resources and Poland Technology Infrastructure. 2021. Polish roberta.

Erik F. Tjong Kim Sang and Fien De Meulder. 2002. Introduction to the conll-2002 shared task: Languageindependent named entity recognition. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL).

Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL).

Victor Sanh, Albert Webson, and Colin Raffel et al. 2021. Multitask prompted training enables zero-shot task generalization. ArXiv, abs/2110.08207.

Teven Scao, Angela Fan, and et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100.

StabilityAI. 2023. Stablelm: Stability ai language models. https://github.com/stability-AI/ stableLM.

Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. ArXiv, abs/2009.01325.

Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. ArXiv, abs/2102.02503.

Rohan Taori, Ishaan Gulrajani, and Tianyi Zhang et al. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca.

Hugo Touvron, Thibaut Lavril, and Gautier Izacard et al. 2023. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language model with self generated instructions.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. In Proceedings of the International Conference on Learning Representations.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Transactions on Machine Learning Research.

Laura Weidinger, John F. J. Mellor, and Maribeth Rauh et al. 2021. Ethical and social risks of harm from language models. ArXiv, abs/2112.04359.

Wilson Wongso. 2021. Japanese roberta.

Linjuan Wu, Shaojuan Wu, Xiaowang Zhang, Deyi Xiong, Shizhan Chen, Zhiqiang Zhuang, and Zhiyong Feng. 2022. Learning disentangled semantic representations for zero-shot cross-lingual transfer in multilingual machine reading comprehension. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 991-1000, Dublin, Ireland. Association for Computational Linguistics.

Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2023. Lamini-lm: A diverse herd of distilled models from large-scale instructions. ArXiv, abs/2304.14402.

Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 833-844, Hong Kong, China. Association for Computational Linguistics.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, Florence, Italy. Association for Computational Linguistics.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models. ArXiv, $\mathrm{abs} / 2205.01068$.

Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021. Mr. TyDi: A multi-lingual benchmark for dense retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 127137, Punta Cana, Dominican Republic. Association for Computational Linguistics.


[^0]:    ${ }^{1}$ https://openai.com/blog/chatgpt/

    ${ }^{2}$ https://openai.com/research/gpt-4

[^1]:    ${ }^{3}$ http://commoncrawl.org

