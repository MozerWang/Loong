# Benchmarking Benchmark Leakage in Large Language Models 

Ruijie Xu ${ }^{1,3 *} \quad$ Zengzhi Wang ${ }^{1,3 *} \quad$ Run-Ze Fan ${ }^{1,3 *} \quad$ Pengfei Liu ${ }^{1,2,3 \dagger}$

${ }^{1}$ Shanghai Jiao Tong University ${ }^{2}$ Shanghai Artificial Intelligence Laboratory

${ }^{3}$ Generative AI Research Lab (GAIR)

\{hsu.rayjay,zzwang.nlp\}@gmail.com runze.fan@icloud.com pengfei@sjtu.edu.cn

![](https://cdn.mathpix.com/cropped/2024_06_04_ccc780b541866613b3deg-01.jpg?height=1469&width=1371&top_left_y=626&top_left_x=369)

Figure 1: The relative possibility that various models conduct verbatim training on the training set of a benchmark over test set to enhance capabilities (measured based on PPL and N-gram Accuracy). Models exhibiting near-zero possibilities suggest either the absence of training and test split or the use of both splits in the training process. This metric does not imply cheating, but rather indicates the potential use of the benchmark data during the (pre-)training phase; while using benchmarks to enhance capabilities is acceptable, the lack of relevant documentation can reduce transparency, potentially resulting in unfair comparisons and hindering the field's healthy development.[^0]


#### Abstract

Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary Large Language Models (LLMs). This issue skews benchmark effectiveness and fosters potentially unfair comparisons, impeding the field's healthy development. To address this, we introduce a detection pipeline utilizing Perplexity and N-gram accuracy-two simple and scalable metrics that gauge a model's prediction precision on benchmark-to identify potential data leakages. By analyzing 31 LLMs under the context of mathematical reasoning, we reveal substantial instances of training even test set misuse, resulting in potentially unfair comparisons. These findings prompt us to offer several recommendations regarding model documentation, benchmark setup, and future evaluations. Notably, we propose the "Benchmark Transparency Card" (Tab. 19) to encourage clear documentation of benchmark utilization, promoting transparency and healthy developments of LLMs. we have made our leaderboard, pipeline implementation, and model predictions publicly available, fostering future research.

Code: https://github.com/GAIR-NLP/benbench

Homepage: https://gair-nlp.github.io/benbench

Case Study Demo: https://huggingface.co/spaces/GAIR/benbench


# 1 Introduction 

The rapid development of large language models (LLMs) has resulted in a significant lag in the development of evaluation methods / protocols (Chang et al., 2024). Coupled with the opacity of LLMs training, this can lead to a situation where it becomes challenging for individuals to form an objective assessment of evaluation results (Bommasani et al., 2023). This overestimates the effectiveness of benchmarks, overlooks potential unfair comparison factors, and ultimately leads to missing scientifically meaningful directions, wasting societal resources. Especially, many models have explicitly involved supervised data in the pretraining phase, such as GLM-130B (Zeng et al., 2023), Qwen (Bai et al., 2023), Nemotron-4 15B (Parmar et al., 2024), InternLM-2 (Cai et al., 2024), MiniCPM (Hu et al., 2024), and etc. This context sets the stage for discussing the critical issue of benchmark data leakage. As reliance on these benchmarks grows, so does the risk that they may inadvertently be incorporated into the training data of LLMs, thereby undermining evaluation integrity and complicating true capability assessments.

In exploring this issue, selecting an appropriate testbed is crucial. The ideal testbed should exhibit specific characteristics: (1) it should include both training and test sets, allowing for controlled comparisons; (2) improving performance on this benchmark should be inherently challenging, with limited effective datasets available. This scarcity increases the temptation for developers to use benchmark data to enhance performance; (3) it should also be of widespread interest, ensuring it is a standard metric for evaluating popular models (such as GPT-4 (OpenAI, 2023), Claude-3 (Anthropic, 2024), etc.). Given these criteria, the mathematical reasoning benchmark datasets GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) $)^{1}$ emerge as fitting choices for our test bed, which allow us to study data leakage in depth but also offer a relevant and challenging environment. Our primary aim with these datasets is to unearth potential benchmark leakage, enhancing transparency in language model development.

Given that training data and model details are often opaque, and leakage detection is influenced by various factors such as mode size and training strategies, detecting benchmark[^1]leakage is not a trivial task. In this work, we are not pursuing technical contributions in system development; instead, we are attempting to encourage the healthy development of this field, particularly through the lens of mathematical reasoning tasks, in the following aspects: (1) summaries of various pre-training behaviors and challenges for detecting benchmark leakage (cf. § 2): Data leakage can occur in various scenarios and its detection is influenced by multiple factors such as unreliable assumptions, model size, training strategies, unknown training data, and even inaccessible model weights. (2) proposal of a detection pipeline for estimating pre-training behaviors (cf. § 3): We introduce a simple, computationally efficient, and scalable pipeline that leverages two fundamental yet insightful atomic metrics: Perplexity and $N$-gram Accuracy. These metrics effectively encapsulate the essence of language modeling, capturing its nuances from continuous and discrete perspectives, respectively. By paraphrasing benchmarks to create varied reference versions, we can detect discrepancies in models' atomic metrics, thereby identifying potential data leakage. This pipeline's validity is supported by thorough meta-experiments (cf. §4). (3) leakage analysis of existing models ( $\$ 5$ ): We extend our investigation to analyze existing models (i.e., 31 open-source LLMs), revealing that, in addition to previously identified leaks, many (i.e., approximately half of them), including well-known language models, may have inadvertently leveraged training data to boost their performance on mathematical reasoning tasks, leading to unfair advantages. Moreover, our metric even enables instance-level detection, revealing the possibility of test set leaks in many models (cf. § 5.3). For example, we found that Qwen1.8B can accurately predict all 5 -grams in 223 examples from the GSM8K training set and 67 from the MATH training set, with an additional 25 correct predictions even in the MATH test set. (4) recommendation for model documentation, benchmark setup and future evaluations (cf. § 6): Based on these findings, we offer suggestions encompassing model documentation, benchmark construction, public access to benchmarks, and evaluation from multiple perspectives. We particularly emphasize the aspect of model documentation; we recommend that models should be accompanied by a document at release, which registers whether benchmark data was used for specific performance enhancement and whether any data augmentation was conducted. To this end, we introduce the Benchmark Transparency Card (cf. § A. 4 and Table 19) to facilitate this process, hoping that it will be widely adopted to promote transparency and healthy development of LLMs.

These revelations underscore the urgency for a paradigm shift in how we approach the development and evaluation of language models. By pinpointing potential data leakage, our work champions greater transparency and fairness in model development, steering the community towards more ethical and effective research methodologies.

## 2 Preliminaries

We summarize potential (pre-)training behaviors, define benchmark leakage, and highlight related challenges.

### 2.1 Typical Training Behaviors

Training without seeing benchmark data means that any benchmark data (including training and test splits) is not included in any training set of the model. This represents the most ideal scenario, where the model's performance on benchmarks stems from its generalized capabilities rather than overfitting to the benchmarks. The results on the benchmarks are genuine and reliable, aligning with the expectations of all stakeholders, including users and investors.

Training with benchmark training data means that the training split of benchmark data is (fully or partially) included in the training set of the model. This scenario could occur in situations where the training set of a benchmark is inadvertently mixed in during the training data collection process; it could also happen in efforts to specifically enhance certain capabilities of a model. For instance, GPT-4's training involved the integration of training datasets from GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) to boost its mathematical reasoning skills, as evidenced by their technical report (OpenAI, 2023). Opaque training data and details can lead to unfair comparisons between models, especially
for those that have not been exposed to the benchmark training splits. This opacity can also result in overly optimistic estimates of a model's generalization capabilities.

Training with benchmark test data means that the test split of the benchmark is fully or partially included in the training set of the model. Even though the model demonstrates satisfactory performance on benchmarks, this does not necessarily imply strong generalization capabilities. This situation could render the benchmarks ineffective, thereby misleading stakeholders such as users and downstream developers-a scenario we definitely want to avoid.

### 2.2 Definition of Benchmark Leakage

Let $\mathcal{D}_{\text {pretrain }}$ be the dataset (corpus) used for pre-training a language model $\mathcal{M}$. For example $(x, y)$ from a benchmark $\mathcal{D}$ (regardless of whether it's from the training split $\mathcal{D}_{\text {train }}$ or the test split $\mathcal{D}_{\text {test }}$ ), if the example is found within $\mathcal{D}_{\text {pretrain }}$, it can be concluded that there has been a leak in the benchmark. Meanwhile, there are various ways in which a sample leak can occur, including (1) Input Leak, where the input of the sample appears in $\mathcal{D}_{\text {pretrain }}$ and then the model is exposed to the question or prompt of the benchmark without corresponding answers; (2) Input-Output Leak, where both the input and output of the sample are found in $\mathcal{D}_{\text {pretrain }}$. This can confer an unfair advantage to models trained on such data.

### 2.3 Challenges for Detecting Benchmark Leakage

Suppose that a benchmark has two splits: training and test. If one would like to detect whether the two splits are leaked, there are several challenges as follows:

(1) Cannot guarantee that the test data is leakage-free: The most intuitive approach would be to compare certain metrics, such as perplexity, between the train and test splits. If a model has been exposed to the benchmark's data during (pre-)training, it will naturally exhibit a lower perplexity score on this data. Specifically, if the model has seen the training set but not the test set, its perplexity score on the training set will be lower than on the test set. Conversely, if the model has not seen either the training or test sets, its perplexity scores on both should be similar. However, identifying a model that has seen both the training and test sets poses a significant challenge because its perplexity scores may resemble those of a model that has seen neither, complicating the detection of data leakage.

(2) Difficult to determine the threshold score for leakage due to multiple influencing factors: Even if a model has only been exposed to the training set, determining the extent of training set leakage by observing the difference in perplexity scores between the two sets is challenging. This difficulty arises because the threshold for detecting leakage can vary widely, influenced by factors such as model size, training strategies, and the distribution of the (pre-)training data (including the extent of data leakage), making it hard to establish a universal threshold. Additionally, retraining a model for each one under examination to simulate data leakage is impractical due to the enormous costs involved and the unknown distribution of training data. Moreover, what presents an even greater challenge is the fact that if a model is trained on a training set (as evidenced in $\S 4$ ), it will exhibit a level of generalization on the test set, which makes it challenging to discern whether a model has been trained on both the training and test sets simultaneously.

(3) Unknown utilization of benchmarks: There is limited knowing of how benchmark datasets are used during pre-training phase. The benchmark data might have been enhanced through various augmentation and data synthesis techniques, including paraphrasing and reformatting, among others (Yu et al., 2024; Yang et al., 2023; Fan et al., 2024). Additionally, benchmarks might be employed for hyperparameter tuning during pre-training.

(4) Inaccessible model weights: Many of today's powerful LLMs are closed-source, with access provided only through APIs. This typically results in inaccessible logit scores, posing significant challenges for calculating perplexity-related metrics. Overall, these factors together significantly complicate the detection of benchmark leakage.

## 3 Detection Methodology

### 3.1 Atomic Detection Metrics

Perplexity As one of the most common metrics for evaluation language model, it quantifies how well a language model predicts a sample of text (Jelinek et al., 1977). In essence, perplexity measures the uncertainty of a language model in predicting the next token (for example, a word or character) in a sequence. A lower perplexity score indicates that the language model is more confident in its predictions. Formally, it is defined as the exponentiated average negative log-likelihood of a sequence, expressed as:

$$
\begin{equation*}
\operatorname{PPL}(\boldsymbol{X})=\exp \left(-\frac{1}{t} \sum_{i=0}^{t} \log p_{\theta}\left(x_{i} \mid x_{<i}\right)\right) \tag{1}
\end{equation*}
$$

where $\boldsymbol{X}=\left[x_{0}, x_{1}, \ldots, x_{t}\right]$ denotes a tokenized sequence. In the context of data leakage detection, we concatenate the question and answer part of a sample with a specific marker " Answer: " and only calculate perplexity on the answer part of the combined text as the loss on the question part may not be considered during training.

N-gram Accuracy We design another metric, called N-gram Accuracy, to help the finegrained detection (cf. Figure 2 (b)). First, we combine the question and answer part with a single space for each sample, resulting in the combined text $X$. Second, we uniformly sample $K$ (i.e., 5) starting point among the interval from 2 to $|X| .^{2}$ Then, the combined text from the beginning to the starting point is used as the prompt, with the subsequent $n$-gram serving as the target for prediction. If the majority of n-grams in a sample are correctly predicted, we can suspect that the model has already encountered this particular sample during training. As a result, this metric serves as a significant feature by aiding in the identification of potential instance-level data leakage (cf. § 5.3). Formally, the n-gram accuracy on a dataset given a model can be expressed as:

$$
\begin{equation*}
\operatorname{N-gram} \operatorname{Accuracy}(\boldsymbol{X})=\frac{1}{S K} \sum_{i=0}^{S} \sum_{j=0}^{K} I\left(X_{\text {start }_{j}: \text { start }_{j}+n}, \hat{X}_{\text {start }_{j}: \text { start }_{j}+n}\right) \tag{2}
\end{equation*}
$$

where the dataset size is $S$, start ${ }_{j}$ denotes the index that $j$-th starting point corresponds to,

![](https://cdn.mathpix.com/cropped/2024_06_04_ccc780b541866613b3deg-05.jpg?height=59&width=1385&top_left_y=1586&top_left_x=370)
the generated n-gram from a model $\mathcal{M}$ given the prompt. The indicator function I applies an exact-match approach by default for precise measurement. Additionally, we consider using ROUGE-L (Lin, 2004) and edit distance similarity (cf. § A.3) to loosely measure the predicted n-grams, making the metric more robust to certain situations, such as when benchmark data has undergone some form of augmentation (paraphrasing (Yang et al., 2023), reformatting (Fan et al., 2024), etc.) (See $\S 5.3$ for discussions). Note that all models adopt greedy decoding by default.

A Unified View Both Perplexity and N-gram Accuracy metrics provide unique insights into a language model's performance, primarily assessing precision in next-token prediction. Perplexity, a continuous measure, evaluates the average likelihood of sequences, indicating model uncertainty. Conversely, $\mathrm{N}$-gram Accuracy is a discrete metric focusing on the model's ability to replicate exact subsequences (n-grams) from training data. As Perplexity decreases-implying higher average log probabilities-the model's probability estimates for correct tokens increase, $p_{\mathcal{M}}\left(x_{i} \mid x_{<i}\right)$. This enhancement in token-wise probabilities boosts the likelihood of precise n-gram predictions:

$$
\begin{equation*}
p(\text { n-gram } \mid \text { prompt })=\prod_{k=0}^{n-1} p_{\mathcal{M}}\left(x_{\text {start }_{j}+k} \mid x_{<\text {start }_{j}+k}\right) \tag{3}
\end{equation*}
$$

Leveraging both Perplexity's subtle probabilistic nuances and N-gram Accuracy's exact match precision provides a comprehensive tool for evaluating model behavior under various conditions. This dual approach not only enhances understanding of model capabilities[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_ccc780b541866613b3deg-06.jpg?height=617&width=539&top_left_y=296&top_left_x=446)

(a) Detecting Leakage at Dataset-Level

![](https://cdn.mathpix.com/cropped/2024_06_04_ccc780b541866613b3deg-06.jpg?height=602&width=656&top_left_y=298&top_left_x=1014)

(b) Detecting Leakage at Instance-Level

Figure 2: An overview of our detecting approach.

but also aids in detecting potential training data leakage and other model vulnerabilities. Furthermore, it is important to note that even when working with commercial closed-source models, where access to internal model outputs like logits is not possible, N-gram Accuracy can still be effectively used to analyze model behavior.

### 3.2 Reference Benchmark Synthesis

Given that we cannot guarantee the test set has remained contaminated, solely comparing the metric differences between training and test sets is unreliable. Ideally, we should refer to data that the model has definitively never encountered. A common approach is to use the most recently released examination questions as a benchmark (Paster, 2023). However, ensuring these questions are distributed similarly to the observed benchmark dataset (such as in terms of difficulty) is challenging, and finding suitable new questions for all benchmarks simultaneously is hardly feasible. Therefore, we propose the use of data synthesis, meaning that new reference benchmark is generated on-demand for each benchmark. This approach is designed to maintain the original benchmark's format and reasoning difficulty (such as numerical reasoning in math) by merely paraphrasing the surface text without altering its structure. This method acknowledges that synthesis may introduce biases in terms of phraseology; however, it aims to provide a viable alternative when fresh, untouched data is unavailable. Specifically, we utilize ChatGPT (gpt-3.5-turbo-0125) to create synthesized reference benchmarks for each existing benchmark. To minimize potential bias, we generate three distinct versions for each benchmark through generation sampling. The temperature is set to 0.7 and top_p to 0.9 during sampling. We provide the full prompts for data synthesis in Table 1 and Table 2 and synthesized examples in Table 3 and Table 4.

### 3.3 Detection Pipeline

Having introduced two atomic metrics and the synthesis of reference benchmarks, we will now orderly introduce our detection pipeline, as shown in Figure 2.

Step\#1 Preparation We synthesize three benchmark datasets based on the original benchmark, denoted as $\mathcal{D}_{\text {ref }_{i}}$ and select an atomic metric (e.g., $\mathrm{N}$-gram Accuracy). Let $\mathcal{M}$ be the model to be evaluated.

Step\#2 Calculate the Atomic Metric on Benchmarks We calculate the atomic metric on both the original benchmark and the synthesized benchmark datasets, respectively. To mitigate randomness introduced during synthesis, we average the metrics across the three synthesized datasets. For all splits of the benchmark, we measure the decrement in the
atomic metric, denoted by $\Delta$, which is the difference between the metric scores of the original benchmark ( $M_{\text {ori }}$ ) and the synthesized benchmark ( $M_{\text {ref }}$ ). The decrement $\Delta$ indicates the model's familiarity or memorization of the original benchmark relative to the synthesized set, demonstrating its capability to differentiate between potentially trained data and new, unseen data. The direction of subtraction is dependent on the nature of the atomic metric; for instance, if using perplexity, the calculation would be reversed ( $M_{\text {ref }}-M_{\text {ori }}$ ). Given that the decrement $\Delta$ might be significantly influenced by the model's size and capability, leading to large numerical discrepancies, we normalize this metric by dividing it with the original metric score, $M_{\text {ori }}$, to standardize it. This results in the percentage decrease in the atomic metric $\delta$, expressed as:

$$
\begin{equation*}
\Delta=M_{\text {ori }}-M_{\mathrm{ref}}, \quad \delta=\frac{\Delta}{M_{\mathrm{ori}}} \times 100 \% \tag{4}
\end{equation*}
$$

This normalization allows for meaningful comparisons across different models. For example, consider a model $\mathcal{M}$ that achieves $38.47 \%$ and $21.52 \%$ in 5-gram accuracy on the training set of the original and synthesized benchmarks, respectively. This results in a difference $\Delta_{\text {train }}$ of 16.95 , and a relative decrease $\delta_{\text {train }}$ of $44.06 \%$.

Step\#3 Comparison We analyze $\delta_{\text {train }}$ and $\delta_{\text {test }}$ and compute the disparity $\delta_{\text {train-test }}=$ $\delta_{\text {train }}-\delta_{\text {test }}$. This disparity, derived by subtracting $\delta_{\text {test }}$ from $\delta_{\text {train }}$, offsets biases introduced by data synthesis and depicts the model's relative familiarity and memorization of the training set compared to the test set. If this disparity approaches zero, it indicates a consistent relative decline across both training and test datasets, suggesting that the degree of leakage is equivalent in both splits-either no leakage or simultaneous leakage. ${ }^{3}$ Conversely, a significant $\delta_{\text {train-test }}$ suggests the model is disproportionately familiar with the training dataset compared to the test set, indicating potential leakage in the training set. However, it should be noted that the possibility of leakage in the test set still cannot be entirely ruled out. A notably negative $\delta_{\text {train-test }}$ points to potential leakage in the test dataset. For instance, a model that records a $\delta_{\text {train-test }}$ of $35.54 \%$ implies a higher likelihood of data leakage in the training set compared to another model with a $\delta_{\text {train-test }}$ of $0.96 \%$.

## 4 Meta-Experiment: the Reliability of the Detecting Pipeline

Setup Considering the significant cost associated with integrating benchmarks into largescale pre-training simulations, as well as the performance dependency on variables such as model size and data volume (Jiang et al., 2024), we have opted for a more straightforward simulation way to estimate the upper bound, entailing direct training of the language model using benchmark data, implemented in two distinct methods: (1) Pre-training: The model is pre-trained on the benchmark data where the full loss is utilized; (2) Supervised Fine-tuning (SFT): The model is fine-tuned on the benchmark data, but the loss is calculated solely on the solution portion. ${ }^{4}$ To validate the effectiveness of our detection pipeline across different training strategies, we prepare two sets of data from each benchmark: (i) A "seen" set, comprising 1,000 entries used for model training; (ii) An "unseen" set, consisting of another 1,000 entries that the model has not been exposed to. Then both sets undergo paraphrasing synthesis to generate three distinct reference sets for the "seen" and "unseen" sets, respectively. We employ the Mistral-7B-v0.1 (Jiang et al., 2023) model for these experiments. Further training details can be found in Appendix A.2. By applying our detection pipeline to the resulting models under different training strategies, we can controllably validate the effectiveness of our approach.

Results As shown in Figure 3, whether the model is trained on benchmark data via supervised fine-tuning or pre-training, it becomes more familiar with the data, leading to a greater difference in atomic metrics ( $\Delta_{\text {seen }}$ and $\delta_{\text {seen }}$ ) between the original and the synthetic reference benchmarks. This also results in some generalization ability on unseen benchmark[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_ccc780b541866613b3deg-08.jpg?height=490&width=1395&top_left_y=297&top_left_x=365)

Figure 3: Meta experiment results on GSM8K and MATH datasets. The left y-axis indicates the decrement $\Delta$, and the right $\mathrm{y}$-axis shows the percentage decrease $\delta$.

data, causing an increase in both $\Delta_{\text {unseen }}$ and $\delta_{\text {unseen }}$. However, if the model has not been trained on any benchmark data, these metrics will be low, and the difference between $\delta_{\text {seen }}$ and $\delta_{\text {unseen }}(-)$ will essentially be zero. Meanwhile, as the calculation of n-gram accuracy spans the entire text of both question and solution, it yields in higher $\Delta$ and $\delta$ scores for models trained through pre-training compared to those trained via supervised fine-tuning. In contrast, as perplexity is calculated only on the solution part, supervised fine-tuned models exhibit higher $\Delta$ and $\delta$ scores than those pre-trained. Interestingly, supervised fine-tuning results in higher $\delta_{\text {seen-unseen }}$ scores compared to pre-training on benchmark data, which suggests that the model fits the training benchmark data better and improves the ease of detecting data leaks with supervised fine-tuning. Most importantly, the order of $\delta_{\text {seen-unseen }}$ scores is consistent across different atomic metrics and benchmark datasets. Overall, these results demonstrate the effectiveness of our pipeline.

## 5 Evaluation in the Wild

In this section, we employ our detection pipeline to existing language models.

### 5.1 Setup

Evaluated LLMs We carried out an extensive evaluation of prominent large-scale language models, encompassing a diverse range including the GPT-4 (OpenAI, 2023), Claude-3 (Anthropic, 2024), Grok-1 (xAI, 2024), Qwen family (Bai et al., 2023), Aquila2 family (BAAI, 2023), InternLM family (Team, 2023), Baichuan family (Baichuan, 2023), ChatGLM family (ZhipuAI, 2023), Yi family (AI et al., 2024), LLaMA family (Touvron et al., 2023a;c; Meta, 2024), Gemma family (Mesnard et al., 2024), Mistral-7B (Jiang et al., 2023), Grok1 xAI (2024), Phi family (Li et al., 2023b; Microsoft, 2023), Orca-2-7B (Mitra et al., 2023), DeepSeekMath (Shao et al., 2024). In total, we evaluate 31 models across a range of sizes.

Benchmark Datasets We continue to utilize GSM8K and MATH for detecting data leaks. Specifically, for each dataset, we randomly select 3,000 samples from the training and test sets (or all if the number of samples is less than 3,000 ). We concatenate the question and solution with a space delimiter, thus forming a complete text sample. We calculate perplexity only on the solution part to avoid potential issues with the model not calculating loss on the question part. For n-gram accuracy, we compute across the combined text using n-gram values of 5 and 10 .

### 5.2 Main Results

We evaluated 31 LLMs of varying sizes using three atomic metrics-ppl, 5-gram accuracy, and 10-gram accuracy-on the GSM8K and MATH datasets. Comprehensive results are presented in Tables 9-14. We visualized the results for 5-gram accuracy and ppl on GSM8K

![](https://cdn.mathpix.com/cropped/2024_06_04_ccc780b541866613b3deg-09.jpg?height=526&width=1377&top_left_y=301&top_left_x=366)

Figure 4: LLMs ordered by $\delta_{\text {train-test }}$ w.r.t 5-gram accuracy on GSM8K. The left y-axis indicates the decrement $\Delta$, and the right y-axis shows the percentage decrease $\delta$. Models positioned on the left are more likely to train on the training set compared to the test set, with this likelihood diminishing as one moves to the right.

![](https://cdn.mathpix.com/cropped/2024_06_04_ccc780b541866613b3deg-09.jpg?height=526&width=1371&top_left_y=1060&top_left_x=369)

Figure 5: LLMs ordered by $\delta_{\text {train-test }}$ w.r.t $\mathrm{ppl}$ on GSM8K. The left y-axis indicates the decrement $\Delta$, and the right y-axis shows the percentage decrease $\delta$. The risk of leakage declines from left to right. Models positioned on the left are more likely to train on the training set compared to the test set, with this likelihood diminishing as one moves to the right. Findings based on the PPL metric strongly align with those from the 5-gram accuracy.

in Figures 4 and 5, respectively. The results are ordered by $\delta_{\text {train-test }}$ scores, which reflect the potential degree of data leakage from the training set relative to the test set. We observe that models such as the LLaMA series and Mistral-7B-v0.1 exhibit nearly zero $\delta_{\text {train-test }}$ scores, suggesting minimal potential for benchmark leakage. In contrast, the Aquila2 series has been unexpectedly trained on GSM8K data as noted in their documentation, ${ }^{5}$ and InternLM-2 (excluding the Base version) has undergone continual pre-training on STEM data, including suspected exposure to the GSM8K training set according to its technical report (Cai et al., 2024). These factors align with their significantly higher $\delta_{\text {train-test }}$ scores from our detection pipeline, indicating potential benchmark data utilization in both the Aquila2 series and InternLM-2 series (excluding the Base version). Meanwhile, the models located near these in the Figure 4 and Figure 5 might harbor a risk of data leakage.

Additionally, it should be noted that when a small absolute $\Delta$ value is paired with a large percentage, it suggests that even slight variations lead to substantial percentage fluctuations, thus rendering the $\delta_{\text {train-test }}$ less reliable, as exemplified by Phi-2 (cf. Figure 5). It is important to note that even if the $\delta_{\text {train-test }}$ scores of some models are not significant, this does not rule[^4]out the possibility of data leakage. It is possible that both the training and test sets have been contaminated, which could lead to undetectably low $\delta_{\text {train-test }}$ scores. In such cases, we can supplement our observations with the model's $\Delta$ and $\delta$ scores on the training and test sets (also n-gram prediction observations from $\S 5.3$ ), determining if they are both relatively high on the train and test datasets.

When combining Figure 4 with Figure 5, we can find that the top-ranked models in both figures are essentially the same, including Aquila2, InternLM-2, and Qwen series, despite the order slightly varying. Similar observations can be seen in the MATH dataset, as evidenced by results presented in Tables 11,12, and 14. The discrepancies observed in rankings under the two metrics-n-gram accuracy and $\mathrm{ppl}$ can be attributed to their fundamentally different approaches to language modeling. Perplexity provides a continuous measure, capturing probabilistic nuances across a broad range of text samples. In contrast, n-gram accuracy requires exact matches and is thus more sensitive to the organizational format of data. This characteristic makes it less adept at recognizing subtleties in cases such as paraphrasing or reformatting within benchmark datasets. For example, our closer observation on predictions revealed that when predicting 5-grams near the answer, the model ChatGLM-2 often predicts "Answer: <br>boxed", whereas the golden 5-gram is "\n\#\#\# 12". This discrepancy suggests that the model may have been trained on a rephrased or reformatted version of the GSM8K benchmark dataset. This insight supports the notion that while n-gram accuracy fails to capture such nuanced training effects, perplexity can provide a broader understanding of a model's adaptation to modified data inputs.

## 5.3 $\mathrm{N}$-gram Accuracy Helps Instance-level Leakage Detection

![](https://cdn.mathpix.com/cropped/2024_06_04_ccc780b541866613b3deg-10.jpg?height=504&width=1348&top_left_y=1301&top_left_x=388)

Figure 6: Statistics of suspicious leaked sample, where all 5-grams within a sample are predicted correctly, either strictly (measured by Exact Match) or loosely (measured by ROUGE-L). The y-axis employs an exponential scale based on powers of 3 .

As previously mentioned, and illustrated in Figure 2 (b) high accuracy for each n-gram of an example's prediction suggests a high probability that the sample was encountered during the training process. To investigate instance-level leakage, we looked closer at n-gram predictions across different models. Additionally, considering that benchmark data may undergo reformatting, paraphrasing, or other modifications when integrated into model training, we leverage lenient metrics, such as ROUGE-L and edit distance similarity (cf. $\S$ A.3), for comparing n-grams. Under this context, an instance is deemed correctly predicted if it achieves an Exact Match (meaning all predictions align perfectly), or if the edit distance similarity of all predictions exceeds 0.9 (indicating substantial similarity), and further, if the ROUGE-L score of all predictions surpasses 0.75 .

We compiled statistics across various stringent-to-lenient scoring thresholds to quantify instances where all 5-grams were accurately predicted by several models, with part of the results shown in Figure 6 and detailed results presented in the Table 15-17. Surprisingly,

Qwen-1_8B can precisely replicate many n-gram predictions from the MATH and GSM8K datasets. Specifically, it accurately predicted all 5-grams in 223 examples from the GSM8K training set and 67 from the MATH training set, with an additional 25 correct predictions even in the MATH test set. These observations complement the results discussed in $\S 5.2$, where Qwen-1_8B may not rank highest in $\delta_{\text {train-test }}$ scores but exhibits high $\Delta_{\text {train }}$ and $\delta_{\text {train }}$ scores (also high $\Delta_{\text {test }}$ and $\delta_{\text {test }}$ scores), as shown in Figure 4 . This aligns perfectly with our observation that it can accurately and completely replicate many n-grams from the training set. We would like to emphasize that the n-gram accuracy metric can mitigate issues in our detection pipeline, particularly when the training and test datasets are simultaneously leaked and remain undetected. However, this also has its limitations; it can only detect examples that are integrated into the model training in their original format and wording, unless we know the organizational format of the training data used by the model in advance.

### 5.4 Case Study

![](https://cdn.mathpix.com/cropped/2024_06_04_ccc780b541866613b3deg-11.jpg?height=691&width=1377&top_left_y=923&top_left_x=379)

Figure 7: Two cases: one from the GSM8K training set predicted by the Qwen-1.8B model (above), and one from the GSM8K test set by the Aquila2-34B model (below). Both examples are presented with the original question and answer concatenated, separated by a space.

To gain a deeper understanding of model behaviors, we took a closer look at the n-gram predictions and highlight some cases in Figure 7. In the first case, the Qwen-1.8B model achieves perfect n-gram predictions on a sample from the GSM8K training set, completing all 5-grams accurately. This strongly suggests potential data leakage within the training set of GSM8K. Additionally, we also conducted a case study on the Aquila2-34B model, known to accidentally be exposed to the entire GSM8K test set. It consistently predicts n-grams as "The answer is" for all instances where the ground truth was represented by a placeholder "\#\#\#\#". This observation exactly explains why it is challenging to detect leakage using our n-gram accuracy metric. Consequently, the presence of a high $\delta_{\text {train-test }}$ score, coupled with poor n-gram accuracy, hints that the model might have undergone data augmentation or reformatting during its training process. To enhance readers' comprehension of model behaviors, we have released an interactive demo for case studies.

## 6 Recommendation for Model Documentation and Benchmarks Setup

To ensure fairness in the evaluation of large language models moving forward, we propose the following suggestions:

- Model Documentation: For any LLMs to be released, comprehensive documentation should be provided. This documentation at least specifies whether the model has been trained on the training or test sets of commonly used benchmarks to prevent potentially unfair comparisons. To this end, we introduce Benchmark Transparency Card (cf. Table 19), which serves as the supplement of the Data Card (Pushkarna et al., 2022; Gebru et al., 2021) and Model Card (Mitchell et al., 2019), aiming to document the utilization of benchmarks (such as whether any benchmark sets are used for training and whether any data augmentation techniques are applied) and benchmark evaluation details (cf. § A. 4 for more details). We hope that this card will be widely adopted upon the release of models to foster the healthy development of large language models.
- Benchmark Construction: We recommend constructing benchmarks from the latest corpus to minimize the risk of overlap with pre-training corpora. Additionally, evaluation datasets should be regularly updated using dynamic benchmarks to guard against overfitting to static test datasets (Zhu et al., 2023a; 2024; Jain et al., 2024).
- Benchmark Public Access: To mitigate the risk of Input-Output Leakage, we advise against directly uploading original benchmarks online, particularly when they contain paired questions and answers. As suggested by Jacovi et al. (2023), encrypting the test set prior to uploading can enhance security. Alternatively, maintaining a private test set through a leaderboard format is also a viable option.
- Evaluation: We recommend caution in drawing overly optimistic conclusions about a model's capabilities based on its strong performance in specific benchmarks. It may be beneficial to evaluate the model further using a variety of contemporary challenges, such as new exam questions (Paster, 2023), to provide a more balanced assessment of its abilities. When benchmarking proprietary models, it is important to proceed with caution, especially when submitting benchmark data through APIs. There is a risk that this data could be utilized by the model's provider for further training purposes.


## 7 Related Work

Benchmarks in Natural Language Processing As LLMs grow in capability, rigorous evaluations are crucial for tracking progress. Historically, benchmark datasets like the Penn Treebank (Marcus et al., 1993), SST (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016), GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019), and GEM (Gehrmann et al., 2021) have been pivotal for NLP advancements. With the trend of scaling up models, there's been an enhancement in model capabilities, leading to the development of specialized evaluation suites by Brown et al. (2020), world knowledge assessment MMLU (Hendrycks et al., 2021a), CMMLU (Li et al., 2023a), and C-Eval (Huang et al., 2023), and diverse benchmarks like BIG-bench (Srivastava et al., 2022). Other efforts include the EleutherAI LM Harness for few-shot evaluation (Gao et al., 2021), Liang et al. (2023)'s holistic evaluation framework, AGIEval for human-centric standardized exams (Zhong et al., 2023), and conversational model benchmarks like MT-Bench and ChatbotArena (Zheng et al., 2023). In this work, we primarily focus on challenging mathematical reasoning tasks such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) as our test bed for leak detection.

Data Leakage Detection As pre-training datasets grow, the inadvertent inclusion of benchmark data into the training corpus becomes more likely. Early studies, such as those on GPT-2(Radford et al., 2019), GPT-3 (Brown et al., 2020), FLAN (Wei et al., 2022) and LLaMA2 (Touvron et al., 2023b), utilized post-hoc n-gram overlap analyses between benchmarks and pre-training corpora to gauge data leakage. Access to pre-training data enables leakage detection through overlap analysis, as highlighted in previous research (Dodge et al., 2021; Deng et al., 2023); even without direct access, details like data sources and time frames can reveal leakages (Li, 2023a;b). However, as model training becomes less transparent, identifying leaks poses greater challenges (Shi et al., 2024). Our work addresses scenarios lacking access to pre-training data or training details, striving for transparency in large model development. Researchers have developed various detection methods, including
benchmark perturbations (Oren et al., 2023; Deng et al., 2023) and leveraging synthetic data, akin to efforts by Wei et al. (2023) in estimating leakage using a dataset similar to GSM8K, despite potential data drift risks. Paraphrasing benchmarks, as seen in the work of Zhu et al. (2023b); Golchin \& Surdeanu (2023), shares similarities with our approach. Additionally, studies simulating data leakage by blending benchmarks into training data explore its performance impact (Zhou et al., 2023; Jiang et al., 2024). These investigations underline the importance of understanding models' data memorization, an area explored by prior research (Carlini et al., 2021; Magar \& Schwartz, 2022). Our method, independent of instruction-following capabilities, utilizes basic language modeling metrics and data synthesis for detecting data leakage, even achieving instance-level granularity.

## 8 Conclusion and Limitations

In this study, we first summarize various typical training behaviors and challenges associated with detecting benchmark leakage. We then introduce a simple, computationally efficient, and scalable pipeline based on two fundamental metrics for leakage detection, validated through meta-experiments. The n-gram accuracy metric we introduce can be utilized for instance-level detection, capable of accurately identifying many instances that have been trained. Our comprehensive analysis of 31 open-source LLMs unveils that many may have exploited benchmark data to enhance their performance on mathematical reasoning tasks, thereby gaining an unfair advantage. Additionally, we offer several recommendations from diverse perspectives. Notably, we advocate for the adoption of the "Benchmark Transparency Card" to promote transparent documentation of benchmark usage, fostering more ethical development of LLMs. However, this work has limitations. For example, our method might not detect cases where models are trained on benchmarks that have been augmented or reformatted, and where there is concurrent leakage in training and testing datasets. Despite these challenges, our n-gram accuracy metric mitigates some of these issues.

## 9 Acknowledgements

We particularly thank Ethan Chern, Yan Ma, Haoyang Zou, Xuefeng Li, Yiyuan Li, and other lab mates for their helpful and detailed suggestions.

## References

1. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024.

Anthropic. Introducing the next generation of claude \$\$ anthropic. https://www. anthropic. com/news/claude-3-family, Mar 2024.

BAAI. Aquila2. https://github.com/FlagAI-Open/Aquila2, 2023.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.

Baichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. URL https://arxiv.org/abs/2309.10305.

Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, and Percy Liang. The foundation model transparency index. CoRR, abs/2310.12941, 2023. doi: 10.48550/ARXIV.2310.12941. URL https://doi.org/10. 48550/arXiv.2310.12941.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, MariaFlorina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.htm1.

Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report, 2024.

Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In Michael D. Bailey and Rachel Greenstadt (eds.), 30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, pp. 2633-2650. USENIX Association, 2021. URL https: //www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting.

Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large language models. ACM Trans. Intell. Syst. Technol., 15(3), mar 2024. ISSN 2157-6904. doi: 10.1145/3641289. URL https://doi.org/10.1145/3641289.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.

Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Investigating data contamination in modern benchmarks for large language models. CoRR, abs/2311.09783, 2023. doi: 10.48550/ARXIV.2311.09783. URL https://doi.org/10. 48550/arXiv. 2311.09783.

Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1286-1305, Online and Punta Cana, Dominican

Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.emnlp-main.98. URL https://aclanthology.org/2021.emnlp-main. 98.

Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, and Pengfei Liu. Reformatted alignment. arXiv preprint arXiv:2402.12219, 2024. URL https://arxiv.org/abs/2402.12219.

Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https: //doi.org/10.5281/zenodo. 5371628.

Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. Datasheets for datasets. Commun. ACM, 64 (12):86-92, nov 2021. ISSN 0001-0782. doi: 10.1145/3458723. URL https://doi . org/10. $1145 / 3458723$.

Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, MirunaAdriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The GEM benchmark: Natural language generation, its evaluation and metrics. In Antoine Bosselut, Esin Durmus, Varun Prashant Gangal, Sebastian Gehrmann, Yacine Jernite, Laura Perez-Beltrachini, Samira Shaikh, and Wei Xu (eds.), Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pp. 96-120, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.gem-1.10. URL https://aclanthology.org/2021.gem-1.10.

Shahriar Golchin and Mihai Surdeanu. Data contamination quiz: A tool to detect and estimate contamination in large language models. CoRR, abs/2311.06233, 2023. doi: 10.48550/ARXIV.2311.06233. URL https://doi.org/10.48550/arXiv. 2311.06233.

Diego Granziol, Stefan Zohren, and Stephen Roberts. Learning rates as a function of batch size: A random matrix theory approach to neural network training. J. Mach. Learn. Res., 23:173:1-173:65, 2022. URL http://jmlr.org/papers/v23/20-1258.html.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.

Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chaochao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm:

Unveiling the potential of small language models with scalable training strategies. 2024. URL https://doi.org/10.48550/arXiv.2404.06395.

Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. In Advances in Neural Information Processing Systems, 2023.

Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. In Conference on Empirical Methods in Natural Language Processing, 2023. URL https://api.semanticscholar.org/CorpusID:258741333.

Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.

Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexity-a measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62 (S1):S63-S63, 1977.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.

Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo. Investigating data contamination for pre-training language models. CoRR, abs/2401.06059, 2024. doi: 10.48550/ARXIV.2401.06059. URL https://doi .org/10. 48550/arXiv. 2401.06059.

Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: measuring massive multitask language understanding in chinese. CoRR, abs/2306.09212, 2023a. doi: 10.48550/ARXIV.2306.09212. URL https: //doi.org/10.48550/arXiv. 2306.09212.

Yuan-Fang Li, Sébastien Bubeck, Ronen Eldan, Allison Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. ArXiv, abs/2309.05463, 2023b. URL https://api.semanticscholar.org/CorpusID:261696657.

Yucheng Li. Estimating contamination via perplexity: Quantifying memorisation in language model evaluation. CoRR, abs/2309.10677, 2023a. doi: 10.48550/ARXIV.2309.10677. URL https://doi.org/10.48550/arXiv.2309.10677.

Yucheng Li. An open source data contamination report for llama series models. CoRR, abs/2310.17589, 2023b. URL https://doi.org/10.48550/arXiv.2310.17589.

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=iO4LZibEqW.

Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.

Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 157-165. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-SHORT.18. URL https://doi.org/10. 18653/v1/2022.acl-short. 18 .

Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313-330, 1993.

Gemma Team Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, L. Sifre, Morgane Riviere, Mihir Kale, J Christopher Love, Pouya Dehghani Tafti, L'eonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am'elie H'eliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. ChoquetteChoo, Cl'ement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikula, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross Mcllroy, Ruibo Liu, Ryan Mullins, Samuel L. Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vladimir Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Brian Warkentin, Ludovic Peran, Minh Giang, Clement Farabet, Oriol Vinyals, Jeffrey Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology. 2024. URL https://api.semanticscholar.org/CorpusID:268379206.

Meta. Introducing meta llama 3: The most capable openly available llm to date. https: //ai.meta.com/blog/meta-1lama-3/, April 2024.

Microsoft. Phi-2: The surprising power of small language models microsoft research. https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models/, Dec 2023.

Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* '19, pp. 220-229, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287596. URL https://doi.org/10.1145/ 3287560.3287596 .

Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andrés Codas, Clarisse Simões, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. Orca 2: Teaching small language models how to reason. CoRR, abs/2311.11045, 2023. doi: 10.48550/ARXIV.2311.11045. URL https://doi.org/10.48550/arXiv.2311.11045.

OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303. 08774. URL https://doi.org/10.48550/arXiv.2303.08774.

Yonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. Proving test set contamination in black box language models. CoRR, abs/2310.17623, 2023. doi: 10.48550/ARXIV.2310.17623. URL https://doi.org/10. 48550/arXiv. 2310.17623.

Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan M. Cohen, and Bryan Catanzaro. Nemotron-4 15b technical report. CoRR, abs/2402.16819, 2024. doi: 10.48550/ARXIV.2402.16819. URL https://doi.org/10.48550/arXiv.2402.16819.

Keiran Paster. Testing language models on a held-out high school national finals exam. https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam, 2023.

Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data cards: Purposeful and transparent dataset documentation for responsible ai. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT '22, pp. 1776-1826, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393522 . doi: 10.1145/3531146.3533231. URL https://doi.org/10.1145/3531146.3533231.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arXiv. 2402.03300.

Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=zWqr3MQuNs.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard (eds.), Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology .org/D13-1170.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià GarrigaAlonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615, 2022. doi: 10.48550/ARXIV.2206.04615. URL https://doi.org/10.48550/arXiv. 2206.04615.

InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM-techreport, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023a. URL https: //api.semanticscholar.org/CorpusID:257219404.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL https: //doi.org/10.48550/arXiv.2307.09288.

Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023c. URL https://api.semanticscholar.org/ CorpusID:259950998.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupała, and Afra Alishahi (eds.), Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353-355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for generalpurpose language understanding systems. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 3261-3275, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 4496bf24afe7fab6f046bf4923da8de6-Abstract.htm1.

Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235, 2023. URL https://arxiv.org/abs/2309.11235.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview. net/forum?id= gEZrGCozdqR.

Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, and Yahui Zhou. Skywork: A more open bilingual foundation model. CoRR, abs/2310.19341, 2023. doi: 10.48550/ARXIV.2310.19341. URL https://doi.org/10.48550/arXiv. 2310.19341.

xAI. Open release of grok-1. https://x ai/blog/grok-os, Mar 2024.

Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, and Ion Stoica. Rethinking benchmark and contamination for language models with rephrased samples, 2023.

Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=N8N0hgNDRt.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=-Aw0rrrPUF.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. CoRR, abs/2306.05685, 2023. doi: 10.48550/ARXIV.2306.05685. URL https://doi.org/10. 48550/arXiv. 2306.05685 .

ZhipuAI. Chatglm-6b: An open bilingual dialogue language model. https://github.com/ THUDM/ChatGLM-6B, 2023.

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023. doi: 10.48550/ARXIV.2304.06364. URL https://doi.org/10.48550/arXiv. 2304.06364.

Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. Don't make your LLM an evaluation benchmark cheater. CoRR, abs/2311.01964, 2023. doi: 10.48550/ARXIV.2311.01964. URL https: //doi.org/10.48550/arXiv. 2311.01964.

Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. Dyval: Dynamic evaluation of large language models for reasoning tasks. arXiv preprint arXiv:2309.17167, 2023a. URL https://arxiv.org/abs/2309.17167.

Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, and Xing Xie. Dyval 2: Dynamic evaluation of large language models by meta probing agents. arXiv preprint arXiv:2402.14865, 2024. URL https://arxiv.org/abs/2402.14865.

Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu Hu, Yiran Wei, Rui Wang, and Hongyuan Lu. CLEAN-EVAL: clean evaluation on contaminated large language models. CoRR, abs/2311.09154, 2023b. doi: 10.48550/ARXIV.2311.09154. URL https://doi.org/10.48550/arXiv. 2311.09154.
