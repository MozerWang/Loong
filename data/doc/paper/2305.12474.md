# Evaluating the Performance of Large Language Models on GAOKAO Benchmark 

Xiaotian Zhang ${ }^{1, *}$, Chunyang Li $^{1, *}$, Yi Zong ${ }^{1, *}$, Zhengyu Ying ${ }^{2}$, Liang $\mathbf{H e}^{\dagger}$, Xipeng Qiu ${ }^{\dagger}$<br>Tianxiang Sun, Peng Li, Shiqiao Meng, Yanjun Zheng, Jun Zhan,<br>Zhangyue Yin, Xiannian Hu, Guofeng Quan<br>${ }^{1}$ School of Computer Science, Fudan University<br>${ }^{2}$ School of Computer Science and Technology, East China Normal University<br>\{xiaotianzhang21, yzong22\} @m.fudan.edu.cn, $\{19307110196$, xpqiu $\}$ fudan.edu.cn<br>\{zyying, lhe \} @cs.ecnu.edu.cn


#### Abstract

Large Language Models(LLMs) have demonstrated remarkable performance across various natural language processing tasks; however, how to comprehensively and accurately assess their performance becomes an urgent issue to be addressed. This paper introduces GAOKAOBench, an intuitive benchmark that employs questions from the Chinese GAOKAO examination as test samples, including both subjective and objective questions. To align with human examination methods, we design a method based on zero-shot settings to evaluate the performance of LLMs. With human evaluation, we obtain the converted total score of LLMs, including GPT-4, ChatGPT and ERNIE-Bot. Our findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across various subjects. We also use LLMs to grade the subjective questions, and find that model scores achieve a moderate level of consistency with human scores. In conclusion, this research contributes a robust evaluation benchmark for future large language models and offers valuable insights into the advantages and limitations of such models. ${ }^{1}$


## 1 Introduction

LLMs have demonstrated great abilities in handling diverse applications. The LLMs (Brown et al., 2020; Ouyang et al., 2022, OpenAI, 2023; Bubeck et al., 2023; Wei et al., 2022) indicate they possess abundant intrinsic knowledge, the ability to follow instructions and reasoning capabilities, which in certain areas are on par with or even surpass human abilities. To better measure the capabilities of LLMs, researchers have proposed more comprehensive and challenging benchmarks.[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_f81b7c51bf4c548ca614g-01.jpg?height=642&width=877&top_left_y=821&top_left_x=978)

Figure 1: Converted Total Score of LLMs in GAOKAO-Bench. The converted total score for subjects in both the sciences and the humanities is 750 points.

MMLU (Hendrycks et al., 2021) aims to measure a text model's multitask accuracy, covering 57 tasks such as elementary mathematics, US history, computer science, and more. BIG-Bench (Srivastava et al., 2022) introduces a comprehensive evaluation framework encompassing more than 204 subtasks, including linguistics, child development, among others. AGIEval (Zhong et al., 2023) evaluates the performance of LLMs in the context of humancentric standardized examinations and contains both Chinese and English tasks. Huang et al. (2023) propose C-Eval, a comprehensive Chinese evaluation suite covering four difficulty levels. However, the benchmark mentioned above only consists of objective questions and lacks subjective questions that are more closely related to generative abilities. Besides, due to the absence of real-world test samples, individuals often underestimate the complexity of these tasks and the abilities of the models, particularly in the context of the rapid development

![](https://cdn.mathpix.com/cropped/2024_06_04_f81b7c51bf4c548ca614g-02.jpg?height=600&width=1464&top_left_y=240&top_left_x=296)

Figure 2: Scoring Rate of LLMs on objective and subjective questions across the subjects.

of LLMs. Consequently, there is a need for an intuitive and practical evaluation method.

We propose using the Chinese College Entrance Examination (GAOKAO) questions. These questions include computational, reasoning, knowledge assessment and writing tasks (Tan et al., 2021). Previous benchmarks based on the GAOKAO mainly focus on English (Yuan and Liu, 2022), especially English Reading and Comprehension Questions (Zhang et al., 2022). To this end, we introduce the GAOKAO-Benchmark (GAOKAO-Bench), a benchmark specifically tailored to LLMs evaluation that covers the GAOKAO questions from 2010 to 2022. The GAOKAO-Bench consists of 9 subjects with 1781 objective questions and 1030 subjective questions. The question types include singlechoice, cloze, correction, open-ended questions, and more.

We conduct experiments on some currently bestperforming LLMs. To more accurately measure their generative capabilities, we use human scoring evaluation to judge subjective questions. The results in Figure1 show that LLMs have achieved competitive scores in the GAOKAO. Meanwhile, we find that all of the LLMs exhibit obvious signs of subject bias, which informs the future development of LLMs.

Due to the high cost of human evaluation, we provide human-annotated marking criteria of subjective questions. And we use LLM as a judge to evaluate LLMs on subjective questions. The results indicate that equipped with the detailed marking criteria, LLMs exhibit high consistency with human teachers, making the large-scale assessment of subjective questions feasible.

## 2 GAOKAO-Bench

### 2.1 Introduction to the GAOKAO

The Chinese College Entrance Examination, also known as the GAOKAO, is a nationwide examination designed to assess the academic abilities of high school students applying to universities in China. Known as a rigorous and comprehensive examination, the GAOKAO is differentiated into two distinct streams: the sciences and the humanities: the sciences include Chinese, sciences mathematics, English, physics, chemistry and biology; the humanities include Chinese, humanities mathematics, English, politics, history and geography. The examination encompasses a variety of question types that include logical reasoning, computational analysis, knowledge-based quizzes and written expression among other aspects.

### 2.2 Dataset Description

The GAOKAO-Bench established in this paper includes the content of all national exams in the GAOKAO of all subjects from 2010 to 2022, providing an intuitive and human-aligned evaluation benchmark for LLMs.

We obtain the questions and transform them from PDF into JSON file format using a combination of automated scripting and manual annotation. Mathematical formulas within the questions were converted into $\mathrm{IAT}_{\mathrm{E}} \mathrm{X}$ format. Appendix A. 1 provides an example of a mathematical single-choice question.

The questions are divided into subjective and objective categories, depending on whether they re-

| Models | Overall | Chinese | Eng. | Sci. <br> Math | Hum. <br> Math | Phys. | Chem. | Biol. | Poli. | Hist. | Geog. |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMA-7b | $21.1 \%$ | $16.2 \%$ | $20.5 \%$ | $24.3 \%$ | $26.1 \%$ | $0.0 \%$ | $22.6 \%$ | $22.7 \%$ | $22.2 \%$ | $19.2 \%$ | $24.2 \%$ |
| Vicuna-7b | $21.0 \%$ | $12.0 \%$ | $19.6 \%$ | $23.8 \%$ | $23.4 \%$ | $7.0 \%$ | $27.4 \%$ | $20.0 \%$ | $20.9 \%$ | $23.0 \%$ | $23.2 \%$ |
| Baichuan2-7b-Base | $27.2 \%$ | $16.2 \%$ | $21.2 \%$ | $24.8 \%$ | $24.8 \%$ | $0.0 \%$ | $23.4 \%$ | $24.0 \%$ | $55.3 \%$ | $32.1 \%$ | $24.2 \%$ |
| Baichuan2-7b-Chat | $40.5 \%$ | $31.7 \%$ | $33.0 \%$ | $26.6 \%$ | $28.4 \%$ | $18.0 \%$ | $26.6 \%$ | $48.0 \%$ | $69.7 \%$ | $57.8 \%$ | $49.5 \%$ |
| Baichuan2-13b-Chat | $43.9 \%$ | $26.9 \%$ | $34.7 \%$ | $23.8 \%$ | $31.7 \%$ | $25.0 \%$ | $40.3 \%$ | $53.3 \%$ | $75.3 \%$ | $59.9 \%$ | $61.1 \%$ |
| ChatGLM-6b | $30.8 \%$ | $18.6 \%$ | $17.0 \%$ | $25.2 \%$ | $25.7 \%$ | $12.5 \%$ | $30.6 \%$ | $24.7 \%$ | $54.1 \%$ | $59.9 \%$ | $25.3 \%$ |
| ChatGLM2-6b | $42.7 \%$ | $31.1 \%$ | $30.6 \%$ | $29.0 \%$ | $35.8 \%$ | $24.2 \%$ | $46.0 \%$ | $71.3 \%$ | $55.0 \%$ | $59.2 \%$ | $41.1 \%$ |
| GPT-4-0613 | $71.6 \%$ | $52.1 \%$ | $\mathbf{9 3 . 2} \%$ | $\mathbf{5 4 . 5} \%$ | $\mathbf{6 4 . 0} \%$ | $50.8 \%$ | $43.6 \%$ | $\mathbf{8 3 . 0} \%$ | $72.5 \%$ | $74.2 \%$ | $\mathbf{8 1 . 1 \%}$ |
| GPT-4-0314 | $\mathbf{7 2 . 2 \%}$ | $\mathbf{5 3 . 9 \%}$ | $93.1 \%$ | $53.7 \%$ | $63.3 \%$ | $\mathbf{5 5 . 5} \%$ | $44.4 \%$ | $80.7 \%$ | $75.9 \%$ | $75.6 \%$ | $80.0 \%$ |
| GPT-3.5-turbo-0301 | $53.2 \%$ | $34.7 \%$ | $76.6 \%$ | $38.8 \%$ | $47.8 \%$ | $41.1 \%$ | $38.7 \%$ | $56.9 \%$ | $45.3 \%$ | $53.9 \%$ | $54.0 \%$ |
| ERNIE-Bot-0615 | $56.6 \%$ | $46.7 \%$ | $31.0 \%$ | $38.3 \%$ | $49.1 \%$ | $35.9 \%$ | $\mathbf{6 6 . 1} \%$ | $79.3 \%$ | $\mathbf{8 6 . 9 \%}$ | $\mathbf{7 9 . 1 \%}$ | $68.4 \%$ |
| ERNIE-Bot-turbo-0725 | $45.6 \%$ | $35.3 \%$ | $26.6 \%$ | $34.1 \%$ | $36.2 \%$ | $32.0 \%$ | $51.6 \%$ | $64.0 \%$ | $72.2 \%$ | $63.4 \%$ | $44.2 \%$ |

Table 1: Scoring Rate of Objective Questions. Models above the line are open-source LLMs; models below the line are closed-source LLMs.

quire human scoring. In total, we select 2811 questions, including 1030 objective questions and 1781 objective questions. Table 3 provides a breakdown of the specific types of questions and the corresponding number of questions in each type. MultiQuestion Choice refers to a format where a single question is followed by multiple sub-questions and Multi-Choice refers to a format where a single question corresponds to multiple correct answers.

## 3 Experiments

### 3.1 Methodology

Prompt Design In order to emulate the format in which humans partake in examinations, we utilize a zero-shot settings strategy (Ouyang et al., 2022) and create prompts tailored to different question types. The prompts not only require the model to complete the task, but also explicitly specify the format of the output as we contend that the intrinsic knowledge level of the model and its ability to follow instructions are equally important. The specific prompt examples we use are illustrated in Appendix A.1.

Models We evaluate several current bestperforming LLMs that support both Chinese and English:

1. GPT-4: We test on 2 checkpoints: GPT-40613 and GPT-4-0314.
2. ChatGPT: We test on GPT-3.5-turbo-0301 checkpoint.
3. ERNIE-Bot: A Chinese LLM published by Baidu. We test on ERNIE-Bot-0615 checkpoint.
4. ERNIE-Bot-turbo: We test on ERNIE-Botturbo-0725 checkpoint.

We set the sampling temperature to 0.3 in order to achieve a balance between stability and diversity.

Metric When evaluating objective and subjective questions separately, we use the scoring rate $R_{i, \text { obj }}$ and $R_{i, \text { subj }}$ for each subject $i$.

To evaluate the overall performance, we convert the scoring rates of subjective and objective questions into a total score $S_{\text {total }}$. We mimic the subjective question scores as $M_{i \text {,subj }}$ and objective question scores $M_{i, \text { obj }}$ for each subject $i$ in the GAOKAO. The converted total score can be formulated as:

$$
\begin{aligned}
S_{\text {total, } \mathcal{S}} & =\sum_{i \in \mathcal{S}}\left(R_{i, \mathrm{obj}} \cdot M_{i, \mathrm{obj}}+R_{i, \mathrm{subj}} \cdot M_{i, \mathrm{subj}}\right) \\
S_{\text {total, } \mathcal{H}} & =\sum_{i \in \mathcal{H}}\left(R_{i, \mathrm{obj}} \cdot M_{i, \mathrm{obj}}+R_{i, \mathrm{subj}} \cdot M_{i, \mathrm{subj}}\right)
\end{aligned}
$$

| Models | Overall | Chinese | Eng. | Sci. <br> Math | Hum. <br> Math | Phys. | Chem. | Biol. | Poli. | Hist. | Geog. |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| GPT-4-0613 | $50.8 \%$ | $50.3 \%$ | $87.6 \%$ | $\mathbf{2 4 . 6} \%$ | $27.5 \%$ | $47.1 \%$ | $28.5 \%$ | $\mathbf{8 5 . 6 \%}$ | $49.9 \%$ | $59.9 \%$ | $71.5 \%$ |
| GPT-4-0314 | $\mathbf{5 1 . 9 \%}$ | $51.5 \%$ | $\mathbf{8 8 . 3} \%$ | $24.1 \%$ | $\mathbf{2 7 . 9} \%$ | $\mathbf{5 6 . 7 \%}$ | $\mathbf{3 5 . 0 \%}$ | $\mathbf{8 5 . 6} \%$ | $50.0 \%$ | $\mathbf{6 3 . 1 \%}$ | $70.0 \%$ |
| GPT-3.5-turbo-0301 | $35.8 \%$ | $33.9 \%$ | $75.4 \%$ | $15.2 \%$ | $15.9 \%$ | $16.9 \%$ | $21.4 \%$ | $36.3 \%$ | $42.3 \%$ | $58.4 \%$ | $62.1 \%$ |
| ERNIE-Bot-0615 | $48.4 \%$ | $\mathbf{5 7 . 1 \%}$ | $45.0 \%$ | $17.0 \%$ | $25.6 \%$ | $33.5 \%$ | $30.8 \%$ | $84.9 \%$ | $\mathbf{5 3 . 0 \%}$ | $60.0 \%$ | $\mathbf{7 2 . 7 \%}$ |
| ERNIE-Bot-turbo-0725 | $39.2 \%$ | $42.5 \%$ | $28.8 \%$ | $14.6 \%$ | $15.6 \%$ | $23.2 \%$ | $25.0 \%$ | $85.1 \%$ | $45.3 \%$ | $47.0 \%$ | $61.8 \%$ |

Table 2: Scoring Rate of Subjective Questions. The results are scored by human teachers.

| Question Type |  | Number | Percentage |
| :--- | :--- | :---: | :---: |
|  | Single Choice | 1418 | $50.5 \%$ |
| Objective | Multi-Question Choice | 273 | $9.7 \%$ |
|  | Multi-Choice | 64 | $2.3 \%$ |
|  | Five out of Seven | 26 | $0.9 \%$ |
| Subjective | Open-ended Question | 786 | $28.0 \%$ |
|  | Cloze | 218 | $7.8 \%$ |
|  | Correction | 26 | $0.9 \%$ |

Table 3: Distribution of Question Types.

where $\mathcal{S}$ stands for the set of the sciences subjects, and $\mathcal{H}$ stands for the set of the humanities subjects. The total scores of sciences and humanities are both 750 points. Detailed total score for each subject is shown in Appendix B.

### 3.2 Objective Questions

Each item $i$ in the GAOKAO-Bench comprises the question $q_{i}$, the standard answer $a_{i}$, the score $s_{i}$, the analysis $n_{i}$. For objective questions, the input includes the question $q_{i}$ and the LLMs need to output $\left(r_{i}, o_{i}\right)$, where $r_{i}$ denotes the corresponding reasoning process and $o_{i}$ denotes the outcome. Points are awarded only if the outcome $o_{i}$ is consistent with the standard answer $a_{i}$. Following the technical report for OpenAI's GPT-4 (OpenAI, 2023), we score the objective questions using regular matching. In addition to the LLMs mentioned above, we evaluate several open-source LLMs on GAOKAOBench, including LLaMA (Touvron et al., 2023), Baichuan (Yang et al., 2023)and ChatGLM (Zeng et al., 2023).

### 3.3 Subjective Questions

The input and output formats of the subjective questions are similar to those of objective questions. During the grading process, evaluators take into account both the reasoning process $r_{i}$ and the outcome $o_{i}$. We assess the subjective questions using human scoring, in order to more precisely evaluate the performance of LLMs. Each subjective ques- tion is evaluated by two teachers, and the average of these scores was adopted as the final score for that question.

### 3.4 LLM as a Judge

Due to the high cost of manual evaluation, it is a natural progression to consider the use of LLMs for grading subjective questions. To better align with the teachers, we solicit teachers to provide detailed marking criteria $m_{i}$, breaking down the answers into specific scoring points for each item $i$. We design prompts in zero-shot settings and utilize GPT-4-turbo (GPT-4-1106-preview) as a judge. For each input $\left(q_{i}, a_{i}, s_{i}, n_{i}, m_{i}, r_{i}, o_{i}\right)$, the LLM need to output $\left(g_{i}, f_{i}\right)$, where $g_{i}$ denotes the process of grading and $f_{i}$ denotes the final score. The sampling temperature is set to 0 to obtain deterministic scores. We calculate the converted total score and Spearman and Kendall-Tau correlations between predicted scores and human scores following Jain et al. (2023) and Zhong et al. (2022).

### 3.5 Results

Overall Performance Figure 1 shows the converted total score of LLMs on GAOKAO-Bench. GPT-4 achieves scores exceeding 400 points and ERNIE-Bot surpasses ChatGPT. Every LLM obtains higher scores in humanities than in sciences. In the GAOKAO, the sciences require more advanced logical reasoning and computational steps than the humanities; and the humanities require a greater amount of knowledge than the sciences. The result indicates the reasoning and calculation abilities of LLMs still need further improvement.

Performance on Objective Questions Table 1 reflects the performance of LLMs on objective questions in different subjects. Open-source models pre-trained on Chinese language data and
aligned with human perform better in all subjects. And the performance of the models improves with the increase in their scale. For closed-source LLMs, GPT-4 maintains a lead in the majority of subjects, but ERNIE-Bot performs better in chemistry, politics and history.

Performance on Subjective Questions Table 2 indicates the human evaluation of subjective questions. GPT-4 obtains the highest scoring rate ( $51.9 \%$ ) and ERNIE-Bot achieves a comparably close level (48.4\%). GPT-4 and ChatGPT exhibit superior performance in English compared to Chinese, whereas ERNIE-Bot and ERNIE-Bot-turbo demonstrate the opposite trend, excelling more in Chinese than in English.

LLM as a Judge Table 4 shows the results of using GPT-4-turbo to grade subjective questions. The Question-level Spearman and Kendall-Tau correlations show a markedly strong positive correlation between model judging and human scoring.

## 4 Analysis

### 4.1 Difference in Subjects

We analyze the scoring rate of subjective questions and objective questions in different subjects of LLMs, and find that there are large differences in the ability of the model in different subjects both in objective questions and subjective questions.

GPT-4 excels in English, biology and geography with scoring rates greater than $70 \%$ both in subjective and objective questions. However, they demonstrate poor performance in mathematics and physics with scoring rates less than $40 \%$. ERNIEBot performs better in biology, history, politics in subjective questions with scoring rates greater than $60 \%$, but the scoring rate of mathematics is less than $30 \%$.

We posit that the substantial disparities across subjects can be attributed to two primary factors: firstly, the distinct competencies evaluated by each subject, for instance, language comprehension and summarization abilities in Chinese and English, and logical reasoning and computational skills in mathematics and physics; secondly, aspects related to the training of the model, including the richness of the pre-training corpus and the inclinations towards human alignment.

### 4.2 Difference between Sujective and Objective Questions

For a given subject, the scoring rate of subjective questions is generally lower than that of objective questions. For example, the scoring rate of subjective mathematics questions of GPT-4 is significantly lower than that on subjective mathematics questions. We hypothesize that subjective mathematics questions distinctly require the application of correct formulas, as well as more extensive computational and reasoning steps, which poses a significant challenge for LLMs. And compared to objective questions, the subjective questions of humanities necessitate students' mastery of more precise knowledge points, as well as their abilities in induction, summarization and categorical organization.

![](https://cdn.mathpix.com/cropped/2024_06_04_f81b7c51bf4c548ca614g-05.jpg?height=825&width=711&top_left_y=1181&top_left_x=1095)

Figure 3: The Annual Trends of LLMs on GAOKAOBench.

### 4.3 Stable Annual Trends on the GAOKAO

We categorize the examination questions based on their respective years and compute the model's converted total scores from 2013 to 2022 in Figure 3. We observe that the converted total score of LLMs are stable across the last decade. It indicates a relative stability in the difficulty level of the GAOKAO questions.

| Models | Sciences |  | Humanities |  | $\rho$ | $\tau$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Human | GPT-4-turbo | Human | GPT-4-turbo |  |  |
| GPT-4-0314 | 434 | 428 | 480 | 523 | 0.854 | 0.710 |
| GPT-3.5-turbo-0301 | 447 | 440 | 485 | 535 | 0.852 | 0.709 |
| ERNIE-Bot-0615 | 300 | 314 | 368 | 408 | 0.845 | 0.710 |
| ERNIE-Bot-turbo-0725 | 333 | 348 | 397 | 436 | 0.864 | 0.740 |

Table 4: Converted total score, Question-level Spearman and Kendall-Tau correlations of Human and GPT-4-turbo grading methods.

### 4.4 LLM as a Judge

We observe that the converted total score of sciences is much closer to human scoring than that of humanities. The deviation in scores for science subjects is less than $2 \%$ of the total score, and for humanities subjects, the deviation is around $5 \%$ of the total score. We posit that the answers and scoring criteria in the sciences are relatively explicit, whereas in the humanities, scoring depends on the alignment of semantics with designated points. This necessitates a fine-grained semantic understanding by the models, presenting a significant challenge for LLMs.

## 5 Avoid Benchmark Leakage

Benchmark leakage means the data related to evaluation sets is occasionally used for model training (Zhou et al., 2023). And it is plausible that the GAOKAO questions may be included in the training corpus of LLMs. The zero-shot settings and human evaluation used in this paper can alleviate the unfair phenomenon. Given that the GAOKAO is conducted annually in June, we plan to incorporate each year's new GAOKAO questions into the GAOKAO-Bench as a supplement, aiming to mitigate the issue of dataset leakage in evaluations. We have released the GAOKAO-Bench$2023^{2}$ which includes the objective questions in the 2023 GAOKAO. And we compare the scoring rate of objective questions in GAOKAO-Bench and GAOKAO-Bench-2023 in Table 5. We contend that these variations are within the normal range of difficulty fluctuations.

## 6 Ablation Study

We investigate the impact of manually annotated marking criteria on the accuracy of the LLM's grad-[^1]

| Models | GAOKAO-Bench | GAOKAO-Bench-2023 | $\Delta$ |
| :--- | :---: | :---: | :---: |
| ChatGLM-6b | $30.8 \%$ | $24.1 \%$ | $-6.7 \%$ |
| ChatGLM2-6b | $42.7 \%$ | $36.9 \%$ | $-5.8 \%$ |
| Baichuan2-7b-chat | $40.5 \%$ | $37.9 \%$ | $-2.6 \%$ |
| Baichuan2-13b-chat | $43.9 \%$ | $41.3 \%$ | $-2.6 \%$ |
| GPT-4-0613 | $71.6 \%$ | $71.0 \%$ | $-0.6 \%$ |
| GPT-4-0314 | $72.2 \%$ | $69.8 \%$ | $-2.4 \%$ |

Table 5: Scoring Rate of Objective Questions on GAOKAO-Bench-2023. The GAOKAO-Bench covers questions from 2010 to 2022.

ing of subjective questions. We use the GPT-4turbo to evaluate the performance of GPT-4, ChatGPT and ERNIE-Bot-turbo with or without marking criteria. Tabel 6 indicates that provided with marking criteria, LLMs can better align with human preferences.

| Methods |  | GPT-4-0613 | GPT-3.5-turbo-0301 | ERNIE-Bot-turbo-0725 |
| :--- | :--- | :---: | :---: | :---: |
| w marking criterion | $\rho$ | 0.854 | 0.845 | 0.825 |
|  | $\tau$ | 0.710 | 0.710 | 0.685 |
| w/o marking criterion | $\rho$ | 0.820 | 0.820 | 0.803 |
|  | $\tau$ | 0.659 | 0.674 | 0.654 |

Table 6: Spearman and Kendall-Tau Correlations of LLM grading and human judgement.

## 7 Related Work

Benchmark for LLMs The flourishing development of LLMs has also raised higher demands for benchmarks. Benchmarks for traditional tasks in NLP, such as GLUE (Wang et al., 2018) for natural language understanding, SQuAD (Rajpurkar et al., 2016) for reading comprehension, cannot measure the comprehensive capabilities of LLMs. Consequently, researchers have proposed new benchmarks to evaluate the advanced abilities of LLMs. MMLU (Hendrycks et al., 2021) provides a multi-task test across a diverse set of subjects. BIG-Bench (Srivastava et al., 2022) covers a diverse range of topics and languages, including auto debugging, know unknowns, logical deduction. HELM (Liang et al., 2023)
taxonomies the design space of language model evaluation into scenarios and metrics. In the field of Chinese language benchmarks, C-Eval (Huang et al., 2023) selects multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. AGIEval (Zhong et al., 2023) assesses LLMs in the context of human-centric standardized exams. CMMLU (Li et al., 2023) includes subjects that may not typically appear in standard exams but are relevant to people's daily life, such as Chinese food culture, Chinese driving rule.

Human evaluation for LLMs Compared to automatic evaluation, human evaluation is more aligned with real-world application scenarios and can offer more comprehensive and precise feedback (Chang et al., 2023). Chatbot Arena (Zheng et al., 2023) provides a platform to assess and compare diverse chatbot models through user engagement and voting. Ziems et al. (2023) adopts human scoring evaluation on generation tasks. Liang et al. (2023) conduct human evaluations on 6 LLMs on summarization and disinformation scenarios.

## 8 Limitations

While we evaluate and analyze the performance of LLMs on GAOKAO-Bench, there are some limitations in this work. Firstly, due to the constraints in time and resources, this paper does not delve into the detailed analysis of the errors made by LLMs on the GAOKAO-Bench, such as model hallucinations and reasoning mistakes. Secondly, due to the rapid developments of LLMs and high cost of human evaluation, we are unable to conduct experiments on every model using human scoring. We hope to enhance the evaluation and analysis of the models' reasoning process and utilize LLMs as a replacement for human scoring in future work.

## 9 Conclusion

In this paper, we introduce the GAOKAO-Bench dataset, which serves as an evaluation standard for large language models. The dataset includes Chinese College Entrance Examination questions from 2010 to 2022, covering various subjects and question types, with an overall high level of difficulty. By testing large language models on the GAOKAO-
Bench, we can analyze the gap and advantages of these models compared to humans in a reasonable and intuitive manner.

In addition, we evaluate the ability of large language models to answer Chinese College Entrance Examination questions using zero-shot prediction approach and human evaluation. Our results show that the models perform well on knowledge-based questions, but struggle with certain types of logical reasoning and mathematical problems, as well as with reading comprehension of longer texts in Chinese.

We also use the LLMs to evaluate subjective questions, which is called LLM-as-a-Judge. We observe that equipped with human-annotated marking criteria, the LLM evaluation is consistent to human preference.

These findings suggest that large language models have potential applications in education and language assessment, but there is still room for improvement in certain areas. Future work could focus on developing approaches to enhance the model's performance on longer text reading comprehension tasks, logical reasoning and calculation problems.

## References

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4.

Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2023. A survey on evaluation of large language models.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations.

Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu,

Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.

Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023. Multidimensional evaluation of text summarization with in-context learning.

Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023. Cmmlu: Measuring massive multitask language understanding in chinese.

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023. Holistic evaluation of language models.

OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.

Hongye Tan, Xiaoyue Wang, Yu Ji, Ru Li, Xiaoli Li, Zhiwei Hu, Yunxiao Zhao, and Xiaoqi Han. 2021. Gcrc: A new challenging mrc dataset from gaokao chinese for explainable evaluation. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 1319-1330.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.

Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. 2023. Baichuan 2: Open large-scale language models.

Weizhe Yuan and Pengfei Liu. 2022. restructured pretraining. arXiv preprint arXiv:2206.11147.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. Glm-130b: An open bilingual pre-trained model.

Cheng Zhang, Hao Zhang, and Jie Wang. 2022. Downstream transformer generation of questionanswer pairs with preprocessing and postprocessing pipelines.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.

Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation.

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models.

Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong

Wen, and Jiawei Han. 2023. Don't make your 1lm an evaluation benchmark cheater.

Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. 2023. Can large language models transform computational social science?
