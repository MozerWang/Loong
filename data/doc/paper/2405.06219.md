# SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models 

Haojie Duanmu*<br>Shanghai AI Laboratory<br>Shanghai Jiao Tong University

Zhihang Yuan*<br>Houmo AI

Jiangfei Duan

CUHK

Shanghai AI Laboratory

Xingcheng Zhang<br>Shanghai AI Laboratory

Xiuhong Li<br>Peking University


#### Abstract

Large language models (LLMs) can now handle longer sequences of tokens, enabling complex tasks like book understanding and generating lengthy novels. However, the key-value (KV) cache required for LLMs consumes substantial memory as context length increasing, becoming the bottleneck for deployment. In this paper, we present a strategy called SKVQ, which stands for sliding-window KV cache quantization, to address the issue of extremely low bitwidth KV cache quantization. To achieve this, SKVQ rearranges the channels of the $\mathrm{KV}$ cache in order to improve the similarity of channels in quantization groups, and applies clipped dynamic quantization at the group level. Additionally, SKVQ ensures that the most recent window tokens in the KV cache are preserved with high precision. This helps maintain the accuracy of a small but important portion of the KV cache. SKVQ achieves high compression ratios while maintaining accuracy. Our evaluation on LLMs demonstrates that SKVQ surpasses previous quantization approaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit values with minimal loss of accuracy. With SKVQ, it is possible to process context lengths of up to $1 \mathrm{M}$ on an 80GB memory GPU for a $7 \mathrm{~b}$ model and up to 7 times faster decoding. Code will be released at https://github.com/cat538/SKVQ.


## 1 Introduction

Recently, large Language Models (LLMs) have achieved great success in the area of artificial intelligence. With the advancement of LLMs, the need to support the longer context has grown. For instance, OpenAI GPT-4 Turbo can handle 128k tokens (Achiam et al., 2023), and Google Gemini 1.5 can process up to 1 million tokens (Team et al. 2023). This expanded token support enables LLMs to tackle more complex tasks like book reading, large image understanding, and video processing, making them more versatile. LLM inference operates in an auto-regressive manner, generating sentences token by token. To reduce the computation overhead, inference system always store the key and value activations in memory and reuse them during subsequent token generation steps. The saved data is known as key and value cache (KV Cache). With the increasing popularity of utilizing LLM for long sequence tasks, the KV cache consumes a significant amount of memory. On the other hand, the large amount of KV cache can also bring a large amount of memory access in the attention mechanism when generating the output tokens. The system will be stuck on the memory access, known as the memory-bound problem in LLM inference (Yuan et al., 2024).

To tackle the problem of large $\mathrm{KV}$ cache size in language models, several compression techniques have been proposed. One approach is KV eviction (Zhang et al., 2023), which[^0]involves removing less important key-value pairs from the cache to free up space. However, this may impact the accuracy of inference. Another method is KV offloading (Sheng et al. 2023), which transfers a portion of the KV cache to slower but larger storage devices like main memory and even secondary storage. However, this may slow down the system due to the low bandwidth of these devices. Scientists have recently been studying the compression of $K V$ cache using quantization. This involves converting floating point $K V$ cache, which initially utilizes a large number of bits, into a format that uses fewer bits. Several novel approaches have been developed to accomplish this, including KVQuant (Hooper et al. 2024), WKVQuant (Yue et al., 2024), and KIVI (Liu et al., 2024). Previous quantization methods have been successful in reducing memory requirements and the number of memory accesses. However, they faced a challenge when using very low-bitwidth quantization because it led to a significant decrease in accuracy, as shown in Figure 1

In this paper, we observe that there is a significant difference in the distribution of different channels during the quantization process. This has a great impact on quantization accuracy, especially in extremely low-bitwidth scenarios. To alleviate this problem, we propose the clipped dynamic quantization with channel reorder. First, we use a transformation invariant permutation to group similar channels based on their statistical characteristics. Second, we apply clipped dynamic quantization to further mitigate the outlier problem. In this way, we greatly reduce the quantization error within each group, thus improving the accuracy of the quantized model.

![](https://cdn.mathpix.com/cropped/2024_06_04_30bb38e34a9e78b110d5g-02.jpg?height=439&width=636&top_left_y=756&top_left_x=1121)

Figure 1: Results on GovReport and MultiFieldQA-zh (Mistral-7b-Instruct-V0.2). We count the storage for meta data including quantization params and reorder index.

Meanwhile, we discover that the protecting the accuracy of these small portion of but more important caches in $\mathrm{KV}$ cache quantization is critical. Due to the locality of attention, these recently generated $\mathrm{KV}$ caches are highly likely to be attended to with a high probability. We propose a sliding window quantization strategy. This mechanism preserves a small portion of the most recently generated KV cache from being quantized. After generating new tokens, the probability of attending to the old tokens' KV cache decreases significantly, so the accuracy loss caused by quantizing them is minimal. The proposed method is named as sliding-window KV cache quantization (SKVQ). It is efficient and easy to implement in existing inference system, which makes it practical for real-world deployment.

To evaluate the effectiveness of our method, we experiments on models of LLaMA(Touvron et al. 2023) and Mistral(Jiang et al. 2023) family. The experiments show that our methods can quantize the key cache into 2 bits and value cache into 1.5 bits with almost no accuracy drop. Compared with the previous quantization method, our approach can achieve optimal performance under different average bit widths as shown in Figure 1. Our performance analysis shows SKVQ enables 1M context length in a single A100-80GB for a $7 \mathrm{~b}$ model. As for the inference latency, in the case of batch size 128 and sequence length $200 \mathrm{k}$, the theoretical $7 \mathrm{x}$ speedup in decoding phase can be achieved $\square^{1}$

## 2 Related Work

There are many multi-billion scale transformer quantization methods designed for LLMs. A main branch of LLM quantization is weight-only quantization, which only involves the quantization of model weights to lower precision. For instance, GPTQ(Frantar et al., 2022) uses second-order approximation to quantize weights, enabling the weight quantization of LLMs into 4-bit. AWQ(Lin et al. 2023) quantizes model weights to 4bits whith an activation-aware manner. SqueezeLLM(Kim et al. 2023) adopts the concept of sensitivity-[^1]![](https://cdn.mathpix.com/cropped/2024_06_04_30bb38e34a9e78b110d5g-03.jpg?height=436&width=1398&top_left_y=270&top_left_x=360)

Figure 2: Visualization of the key cache going through channel reorder and group clipping in sequence. The elements in the red/green box will be placed in the same group to share the quantization parameters.

based non-uniform quantization along with Dense-and-Sparse decomposition. This line of work is orthogonal to ours, as they can be combined together.

Another line of work focuses on weight-activation quantization. llm.int8()(Dettmers et al. 2022) retain outlier channel to full precision, so that other parts can be better compressed to 8bits. SmoothQuant(Xiao et al., 2022) uses equivalent transformations to balance the quantization complexity for both activation and weight, making the activation easier to quantize. RPTQ(Yuan et al. 2023) reorder the channels to reduce the variance in one quantization cluster, further enhancing the accuracy. ATOM(Zhao et al. 2023) improves quantization performance and reduces inference latency by using finer-grained quantization with an efficient kernel. However, since these works are not specifically designed for $\mathrm{KV}$ cache quantization, even applying the best results from such works still results in significant losses in $\mathrm{KV}$ cache compression. We compare with these works in the experimental section.

Recently, as natural language tasks require processing longer contexts, researchers have focused on quantizing key-value caches. Several new methods have been developed, such as KVQuant (Hooper et al. 2024), WKVQuant (Yue et al. 2024), and KIVI (Liu et al., 2024). Quantizing the $\mathrm{KV}$ cache can significantly reduce both the memory requirements and the number of memory accesses needed. Our experimental results show that the performance of our method on long context tasks performs the best in this type of work.

There are also a series of work dedicated to the design of $\mathrm{KV}$ cache eviction strategy (Liu et al. 2023: Ge et al. 2023: Zhang et al. 2023: Xiao et al. 2023). Unlike KV cache quantization, which retains all caches but compresses them to low precision, these methods selectively retain part of the $\mathrm{KV}$ cache and discard other caches directly. These methods usually allocate a fixed-size buffer for $\mathrm{KV}$ cache . When the generated $\mathrm{KV}$ cache exceeds the buffer limit, some tokens considered less important will be evicted from the buffer. These methods are inevitably and irrecoverably discarding $\mathrm{KV}$ pairs deemed, in one way or another, less important than others. Our approach is inspired by and can be well integrated with such work.

## 3 Method

### 3.1 Clipped Dynamic Quantization with Channel Reorder

Quantization is to transform the high-bitwidth float values into low-bitwidth integer values. The quantization process can be formulated as clamp $\left(\left\lfloor\frac{\mathbf{X}-z}{h}\right\rceil, 0,2^{N}-1\right)$, where $\mathbf{X}$ is the float values and $h$ is scaling factor and $z$ is zero point. Previous studies have highlighted significant variations in numerical values among activation channels (Xiao et al. 2022; Wei et al. 2022; 2023). As shown in Figure 2, we also observe substantial variations between channels and tokens in the $\mathrm{KV}$ cache (high channel variance). Therefore, directly quantizing the $\mathrm{KV}$ cache leads to substantial quantization errors. If values in different channels share the scaling factor and zero point, the value from outlier channels skew the quantization range. Especially in the low-bitwidth case, this makes almost all elements except outlier channel
quantize to the same value, and this loss of information leads to significant performance drop. To tackle this issue, we introduce channel transformation based quantization.

To address this problem, some methods have proposed using additional quantization parameters or keeping certain channels in float format to handle outliers (Dettmers et al. 2022). However, we have noticed that the concept of outliers is relative. The channels with the highest values are outliers compared to the medium-sized channels, and the medium-sized channels are outliers compared to the small-sized channels. Other methods propose smoothing the difference between channels by multiplying an extra factor before quantization (Shao et al., 2023: Yue et al. 2024). However, these methods do not take into account the differences in token dimensions. The magnitude of values can vary between different tokens. We have observed that the variation in magnitude of non-outlier channels is relatively high. Specifically, some channels experience magnitude changes of several times or even dozens of times. Smoothing is not effective in addressing this phenomenon, especially in extremely low bitwidth quantization.

Channel Reorder. Inspired by RPTQ(Yuan et al. 2023), we employ a permutation invariant transformation and then apply group clipping to solve the problem of extremely low bitwidth quantization for $\mathrm{KV}$ cache. The permutation invariant transformation allows us to change the order of computation without changing a operation's output. For example, when we execute the matrix multiplication $S=Q \times K^{T}$, we can rearrange the columns of $Q$ and the rows of $K$ (which represent their channel dimension) in the same order without affecting the result of the computation.

We perform channel reorder on $\mathrm{KV}$ cache to make channels with similar data distribution are grouped together for quantization. Values in channels with similar distribution are quantized together. By this way, we can greatly reduce the quantization error of channels with smaller ranges. The same as Yuan et al. (2023), we do the corresponding equivalent permutation for $Q$ and $W_{o}$ to avoid explicit reorder operation. The calculation of attention module $O=\operatorname{Softmax}\left(Q K^{T}\right) \cdot V \cdot W_{o}$ is transformed as:

$$
\begin{equation*}
O=\operatorname{Softmax}\left(P_{k} Q \cdot\left(K^{T} P_{k}^{T}\right)\right) \cdot P_{v} V \cdot W_{o} P_{v}^{T} \tag{1}
\end{equation*}
$$

where $P_{k} \in \mathbb{R}^{C_{i n} \times C_{\text {in }}}$ and $P_{v} \in \mathbb{R}^{C_{i n} \times C_{\text {in }}}$ are channel reorder matrix of Key and Value respectively ${ }^{2}$. In our algorithm, the index is calculated based on the statistical characteristics of each channel. Specifically, we extract the distribution feature of each channel and then use the KMeans algorithm to cluster channels with similar characteristics into the same group. We also compared channel reordering with mathematical equivalent smoothing, and the results in AppendixD demonstrated the effectiveness of the former.

Clipped Dynamic Quantization. Dynamic per-token quantization is widely used method for quantizing the activations in LLMs (Xiao et al. 2022). Different with static quantization that use the static $h$ and $z$, dynamic quantization will compute new $h$ and $z$ using the maximum value and minimum value for each token: $h=\frac{\max (\mathbf{X})-\min (\mathbf{X})}{2^{N}-1}, z=\frac{\min (\mathbf{X})}{h}$.

Previous work about weight quantization (Lin et al. 2023; Shao et al. 2023) has shown that introducing clipping when quantizing weights can improve the quantization performance. According to the second picture in Figure 2. even though we have grouped similar channels together, there are inevitably some outliers within a quantization group. In order to reduce the impact of these outliers on other values in the same group, we propose the clipped dynamic quantization, which can be formulated as:

$$
\begin{equation*}
f(\alpha, \mathbf{X})=\operatorname{clamp}\left(\left\lfloor\frac{\mathbf{X}-z}{h}\right\rceil, 0,2^{N}-1\right), \text { where } h=\frac{\alpha(\max (\mathbf{X})-\min (\mathbf{X}))}{2^{N}-1}, z=\frac{\alpha \min (\mathbf{X})}{h} \tag{2}
\end{equation*}
$$

We introduce a clipping scale $\alpha \in(0,1]$ for each group to compute $h$ and $z$. In order to get the best clipping scale, for each transformer block we try to minimize the MSE of the output of the attention module before and after quantization, i.e., the optimization objective:

$$
\begin{equation*}
\alpha^{*}=\underset{\alpha}{\arg \min } \mathcal{L}(\alpha), \quad \mathcal{L}(\alpha)=\operatorname{MSE}\left(O^{q}, O\right) \tag{3}
\end{equation*}
$$[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_30bb38e34a9e78b110d5g-05.jpg?height=884&width=1396&top_left_y=268&top_left_x=359)

Figure 3: Overview of sliding window quantization Strategy. In each time step, we ensure the latest $w \mathrm{KV}$ cache is full precision. For a token cache that slides out of the window, we make a decision based on the filter rules and choose whether to retain it to high precision.

where $O^{q}$ is the output of attention module after quantizing $\mathrm{KV}$ cache. Unlike weight-only quantization, the KV cache is generated at runtime, it is costly to solve this optimization for each inference. Therefore, we approximate it by offline calibration. By performing optimization on a calibration dataset in advance, we get the approximate $\hat{\alpha}^{*}$ for each group. We share the same $\hat{\alpha}^{*}$ across different tokens. Using the approximate $\hat{\alpha}^{*}$, we can also improve the quantization performance without introducing significant inference cost.

Using channel reorder and clipped dynamic quantization, the elements falling within the same group can more fully utilize the numerical range of the quantized data type, thus reducing the quantization error. Because all the parameter $P_{k}, P_{v}, \alpha$ is determined offline and the reorder operation can be fused into linear layers, it is efficient to implement the clipped dynamic quantization with channel reorder on existing inference frameworks.

### 3.2 Sliding Window Quantization Strategy

Although clipped dynamic quantization with channel reorder can improve the quantization performance to a large extent, extremely low-bitwidth $\mathrm{KV}$ cache quantization still suffers serious performance degradation, especially when the sequence length become longer. This is because the quantization errors accumulate along the sequence dimension. Because the auto-regressive manner of the LLM, the decoding of a new token depends on the previous KV generated. We realize that this not only a challenge but also a chance, the auto-regressive manner can be fully exploited to develop more flexible quantization strategies.

Locality. Many previous works have shown that attention module has very strong locality(Kovaleva et al. 2019: Beltagy et al. 2020; Ge et al. 2023). That means at each time step, the attention module pays more attention to the recently generated tokens. We indicate that compared to the large amount of but less important content in the previous $K V$ cache, protecting the accuracy of these small portion of but more important caches in $K V$ cache quantization is critical.

Motivated by this, we proposed a sliding window quantization strategy, which retain the latest $\mathrm{KV}$ cache of a window $w$ tokens to high precision. The workflow is shown in Figure 3 1) In prefill phase, for each transformer block, after the $K V$ cache is generated, we first compute attention with full precision $\mathrm{KV}$ cache, then quantize the $\mathrm{KV}$ cache with the last
window_size token cache pairs reserved as full-precision. 2) In decode phase, we only process the token which slides out of the window at each time step. This approach ensures the $\mathrm{KV}$ cache of each transformer block generated in the prefill phase are lossless. It also enhance the generated content quality by utlizing the locality of attention module in the decode phase.

Important KV Cache Filter. Except the recent generated tokens, there are some tokens that are sensitive to quantization. We also explored other method to identify important tokens that their KV cache should be kept in high precision. Inspired by (Xiao et al., 2023), the first few tokens of prompt are also very important for the whole generation process, so we add attention sink to the filter rules, i.e., the first few tokens are reserved to high precision. We observed it is effective to keep a small number of sink tokens high precision. Since the positions of sink tokens are fixed, it is easy to implement and we enable it in our experiments. Some cache eviction method monitor for each token its cumulative sum of attention score, then treat these scores as token frequency and only keep the most frequent tokens in the $\mathrm{KV}$ cache (Liu et al., 2023: Zhang et al., 2023), which are called heavy hitters. A straightforward idea is to keep heavy hitters to high precision. However, we did not enable it in our experiments for two reasons: 1) The improvement on prediction accuracy by keeping heavy hitters high precision is not significant. 2) If FlashAttention(Dao, 2023) is used, we can't directly obtain the attention score. It is unfriendly to implement in existing inference framework. We believe that there are better methods available to identify important $\mathrm{KV}$ caches. Therefore, we have kept this as an interface (filter rules in Figure 3) in our implementation, allowing for the addition of new filters in future research.

By retaining small portion of tokens to high precision, we obtain a substantial performance gain in the long context task, while at the same time incurring almost no additional overhead. We will show the impact of window size on quantization performance in Section 4.3

## 4 Experiments

In this section, we introduce the detailed experimental settings and evaluate the effectiveness of the proposed SKVQ.

### 4.1 Settings

Models. We select a wide range of models with different architectures and different size to demonstrate the generalizability of our approach: Llama2-13b(Touvron et al. 2023), and models fine-tuned based on Llama2: Llama2-7b-chat, Llama2-13b-chat, Llama2-7b-80k(Fu et al. 2024), Vicuna-v1.5-7b-16k (Chiang et al., 2023), LongChat-v1.5-32k(Li et al. 2023). We also evaluate models of Mistral family which are recently very popular: Mistral-7bv0.1(Jiang et al., 2023), Mistral-7b-instruct-v0.2. Among these models, models of Llama family adopt multi-head attention, mistral-7b-instruct-v0. 2 uses multi-query attention, and mistral-7b-v0.1 uses multi-query attention and sliding-window attention.

Tasks. We evaluate SKVQ mainly on long sequence tasks, as this is the scenario for which KV cache quantization is most suitable. We use LongBench(Bai et al. 2023) to evaluate on various datasets. Specifically, MultiFieldQA-zh (F1 score) is a Single-Document QA task; 2WikiMultihopQA is a Multi-Document QA task; GovReport (ROUGE score) is a Summarization task; TREC (classification score) is a Few-shot Learning task; and LCC (similarity score) and RepoBench-P (similarity score) is Code Completion task. We also tested SKVQ on Needle-in-a-Haystack (Kamradt, 2023), which is a popular test-bed for whether models can actually utilize long context length. It requires the model to recite the information in a given sentence, which is placed anywhere in a long document. Finally, to provide a clearer picture of the effects of the SKVQ components and to compare with previous methods, we also measure the perplexity on wikitext2 (Merity et al. 2016) in Section 4.3

Quantization. Both channel reorder and clipped dynamic quantization requires offline calibration. For calibration dataset, we select 256 pieces of data with length 4096 from the training set of wikitext2-v1, the calibration takes about a few minutes which is quite

| Model | Method | LCC | RepoBench-P | PR-en | TREC | 2wikimqa | GovReport | MQA-zh | Average |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Llama-2-7B-chat | FP16 | ![](https://cdn.mathpix.com/cropped/2024_06_04_30bb38e34a9e78b110d5g-07.jpg?height=35&width=68&top_left_y=324&top_left_x=786) | 44.05 | 10.25 | 763 | 32.09 | 27.29 | 11.39 | 38.50 |
|  | RTN | 15.44 | 8.76 | 0.79 | 4.00 | 0.30 | 1.93 | 0.07 | 6.76 |
|  | SmoothQuant | 35.31 | 32.18 | 0.79 | 28.75 | 7.45 | 11.83 | 1.68 | 21.92 |
|  | RPTQ | 22.37 | 19.08 | 5 | 47.5 | 15.57 | 20.07 | 3.24 | 19.50 |
|  | KIVI | 49.32 | 43.71 | 4.50 | 63 | 24.07 | 24.73 | 10.24 | 35.91 |
|  | SKVQ | 50.69 | 45.4 | 5.5 | 63 | 28.5 | 27.07 | 10.7 | 37.50 |
| Llama-2-13B-chat | FP16 | 50.54 | 52.1 | 15.25 | 68.5 | 13.21 | 27.52 | 7.23 | 38.83 |
|  | RTN | 20.89 | 18.62 | 0.33 | 0 <br> 0 | 0.52 | 1.68 | 0.16 | 10.15 |
|  | SmoothQuant | 32.17 | 33.86 | 2.65 | 48 | 3.53 | 12.47 | 0.47 | 23.22 |
|  | RPTQ | 49.18 | 47.63 | 5.25 | 63.5 | 10.92 | 23.83 | 4.54 | 35.01 |
|  | KIVI | 48.6 | 48.81 | 13.5 | 68 | 14.32 | 25.7 | 7.01 | 37.21 |
|  | SKVQ | 49.53 | 49.76 | 12.25 | 67.5 | 14.03 | 26.68 | 6.63 | 37.53 |
| Mistral-7B | FP16 | 68.06 | 60.46 | 17.71 | 68 | 10.87 | 20.09 | 17.1 | 45.51 |
|  | RTN | 27.98 | 26.18 | 3.34 | 13 | 1.11 | 2.49 | 0.45 | 15.58 |
|  | SmoothQuant | 40.63 | 35.14 | 3.40 | 30.5 | 6.03 | 5 | 4.12 | 23.85 |
|  | RPTQ | 55.29 | 47.12 | 5.11 | 59.5 | 9.71 | 7.81 | 12.36 | 35.05 |
|  | KIVI | 65.16 | 58.33 | 12.43 | 65 | 11.03 | 13.22 | 13.87 | 42.43 |
|  | SKVQ | 67.81 | 60.54 | 13.21 | 67 | 10.91 | 17.72 | 15.9 | 43.47 |
| Mistral-7B-Instruct | FP16 | 55.07 | 48.96 | 60 | 70 | 22.63 | 31.18 | 42.74 | 48.66 |
|  | RTN | 32.36 | 33.23 | 0.67 | 1 | 2.25 | 10.03 | 2.3 | 18.02 |
|  | SmoothQuant | 43.84 | 38.63 | 4.79 | 39.5 | 10.34 | 23.61 | 8.33 | 29.27 |
|  | RPTQ | 46.85 <br> 4 | 44.07 | 27.67 | 64.5 | 16.99 <br> 16 | 28 <br> 28 | 24.68 | 38.91 |
|  | KIVI | 53.13 | 48.6 | 47.5 | 69 | 20.68 | 29.37 | 33.88 | 45.48 |
|  | SKVQ | 54.86 | 49.05 | 56.42 | 70 | 20.94 | 30.82 | 42.4 | 46.23 |

Table 1: Evaluation of different KV cache quantization methods on LongBench. Groupsize(average) 128, key-cache 2bit, value-cache 2bit, window-size 128. We abbreviated PassageRetrieval as PR and MultiFieldQA as MQA. We highlight the result of our method.

lightweight. We perform asymmetric quantization in all experiments. We have explored the FP8(E4M3) datatype to store scale and zero-point. Our experiment results in Table 3 show that FP8 will bring almost no performance degradation, but significantly reduces overhead at extremely low bit-width and fine grained groups.

### 4.2 Main Results and Analysis

LongBench Results. The performance of SKVQ in the LongBench datasets is summarised in Table 1 We compare our method with Smoothquant(Xiao et al., 2022), RPTQ(Yuan et al., 2023) KIVI(Liu et al. 2024) and per-token RTN(Round To Nearest). Smoothquant and RPTQ are LLM weight-activation quantization schemes. We use them to quantize KV cache without involving model weights and other activation. KIVI(Liu et al., 2024) is a recent 2-bit asymmetric quantization scheme specially designed for KV cache. We set the group size of all the methods to 128. SKVQ utilizes reordering which leads to unequal sizes for each group. In order to ensure the

![](https://cdn.mathpix.com/cropped/2024_06_04_30bb38e34a9e78b110d5g-07.jpg?height=376&width=567&top_left_y=1487&top_left_x=1191)

Figure 4: Average score on LongBench of SKVQ for Llama2-7b-chat and Mistral$7 b-$ Instruct-V0.2. fairness of the comparison, we control the number of groups in SKVQ to ensure the average group size is 128. The window size in SKVQ is set to 128 and the residual length in KIVI is set to 128. $\alpha$ in Smoothquant is set to 1.0 to make the smooth transformation completely inclined to KV cache . Table 11 suggests that SKVQ is an effective method for KV cache compression that outperforms previous quantization approaches across various hard long context generation tasks. We also evaluate Vicuna-v1.5-7b-16k and LongChat-v1.5-7b-32k, the results is in AppendixE.

For all models tested, the accuracy drop of SKVQ is less than $5 \%$. Towards extremely low-bitwidth KV cache quantization, we further quantize the key cache into 2 bits and value cache into 1.5 bits with group size 64. The result in Figure 4 shows that SKVQ can compress key cache into 2 bits and value cache into 1.5 bits with almost no accuracy drop. It is worth noting that the experimental results are under the setting of group size 128. SKVQ can also benefit from a finer-grained group, and achieve almost lossless compression, which is shown in Section 4.3

![](https://cdn.mathpix.com/cropped/2024_06_04_30bb38e34a9e78b110d5g-08.jpg?height=347&width=1393&top_left_y=320&top_left_x=363)

Figure 5: Comparison of SKVQ with KIVI on needle in haystack test. SKVQ achieved higher scores while using lower bitwidth.

| Method | 4bit |  | 3bit |  | 2bit |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  | PPL $\downarrow$ | avg-bits $\downarrow$ | PPL $\downarrow$ | avg-bits $\downarrow$ | PPL $\downarrow$ | avg-bits $\downarrow$ |
| RTN-sym | 4.66 | 4.25 | 4.98 | 3.25 | 26.83 | 2.25 |
| KVQuant | 4.59 | $4.32-4.35$ | 4.64 | $3.32-3.35$ | 4.92 | $2.32-2.35$ |
| Ours | 4.60 | 4.25 | 4.63 | 3.25 | 4.87 | 2.25 |

Table 2: Ablation Study: Comparison of our channel reorder based clipped dynamic quantization approach with KVQuant(best setting) and symmetric RTN per-token quantization in different quantization setting. For RTN-sym and our method, we set group-size to 64 . Perplexity is Llama-2-13b test on Wikitext-2 with sequence length 4096.

Needle in Haystack Results. For needle in haystack test, we used Llama2-7b-80k (Fu et al. 2024) model for our experiments. We set the context to grow from $1 \mathrm{k}$ to $32 \mathrm{k}$ for a total of 20 intervals, and for each context length, we insert the needle into 15 different positions of the context. We compare SKVQ with KIVI under the setting of group size 128. For SKVQ, we set the window size to 128 and reserve 5 attention-sinks, i.e., when the first 5 token cache pairs slide out of the sliding window, they are retained to full precision instead of quantized to 2 bits. The residual length in KIVI is set to 128. We follow the method in (Fu et al. 2024) to calculate the recall, and finally average the scores of all test cases as the overall score. As shown in Figure 5. in key cache 2bits, value cache 2bits, group size 128 setting, KIVI got 244.5, while our SKVQ achieved 272.2 even with 2 bits key cache and 1.5 bits value cache in group size 128 .

These results demonstrate that it is practical to quantize the key-value cache into extremely low-bitwidth for these tasks. More result on needle in haystack test can be found in Appendix B

### 4.3 Ablation Study

In this section, we decompose each part of SKVQ separately in detail and study the effect from each technique and different parameter settings.

Breakdown of different components of SKVQ. We study the accuracy gain or loss of different quantization techniques used in SKVQ. We first use RTN and adopt per-token quantization with group size 32 . We then apply other quantization techniques used in SKVQ, i.e. sliding window, clipping, channel reorder, attention sink and FP8. We present the LongBench average score in Table 3 Attention sink size is set to 5 , i.e. the first 5 token cache pairs are retained to full-precision. We see that sliding-window and channel reorder can significantly enhance the accuracy. FP8(E4M3) stands for using FP8 datatype to store the per-group quantization parameters, i.e. scale and

| Method | Avg Score $\uparrow$ |
| :--- | :--- |
| FP16 | 48.66 |
| RTN | 35.55 |
| + Window-128 | $45.73(10.18 \uparrow)$ |
| + Group Clipping | $46.44(0.71 \uparrow)$ |
| + Channel Reorder | $47.99(1.55 \uparrow)$ |
| + Attention Sink | $48.14(0.15 \uparrow)$ |
| + FP8(E4M3) | $48.04(0.1 \downarrow)$ |

Table 3: Ablation Study: The performance gain or loss by applying each technique in SKVQ based on RTN method. Quantization setting: kv 2bits with group size 32 .

![](https://cdn.mathpix.com/cropped/2024_06_04_30bb38e34a9e78b110d5g-09.jpg?height=380&width=919&top_left_y=276&top_left_x=598)

Figure 6: Ablation Study: Average score of Mistral-7b-Instruct-v0.2 on LongBench under different window sizes. Quantization setting: $\mathrm{KV}$ cache 2bits with group size 128

zero-point. In our study, Using FP8 will results in a relative minor accuracy decreasing compared with FP16. However, at extremely low-bitwidth and fine-grained group size, using FP8 to store the quantization parameter significantly reduce storage overhead. For example, KV 2bits with group size 32, if we use FP16 to store quantization parameters, the average bits of element in $\mathrm{KV}$ cache is $2+16 * 2 / 32=3$, but if we use FP8, then average bits is $2+8 * 2 / 32=2.5$, which is $50 \%$ smaller.

The effect of clipped dynamic quantization with channel reorder. To further demonstrate the effect of our quantization approach without sliding window, we perform evaluation by measuring Llama-2-13b perplexity on wikitext2. The result in Table 2 shows that by only applying channel reorder based clipped dynamic quantization, we have outperformed KVQuant (Hooper et al., 2024). We set group size to 64 and use FP8 to store quantization parameter so that the average bits is equal to asymmetric approach adopt in ATOM and FlexGen. We also reserve the first 5 tokens to FP16 as attention sink. It worth noting that we are comparing the best score of KVQuant i.e. nuq with $1 \%$ outliers are retained to full-precision, which results in higher storage overhead than SKVQ.

The effect of window size. To further investigate the effect of sliding window on the final results, we set up different sizes of windows and tested them on LongBench, and the average scores are shown in the Figure 6 The result shows that the average score increases as the window size increases. In general, different sub-tasks can all benefit more or less from the sliding window strategy, and the extra overhead brought by a window with size of about 128 is negligible in long context scenarios, so we use a window of size 128 in the main experiments.

The effect of group Size. We vary group size from 128 to 32 to test SKVQ on LongBench, the average score on LongBench is as shown in Table 4 It shows that SKVQ can always benefit from finer-grained group. While finer-grained group brings better accuracy, it increases the computation overhead for quantization/dequantization and storage overhead for quantization parameters, which is noted as average bits. Since the performance of SKVQ on various tasks does not drop significantly when the group size is set to 128 , we employ 128 group size in the main experiments.

| Group size | Avg Score $\uparrow$ | Avg Bits |
| :---: | :---: | :---: |
| 128 | 35.365 | 2.125 |
| 64 | 35.805 | 2.25 |
| 32 | 36.51 | 2.5 |

Table 4: Ablation Study: Average scores of Mistral-7b-Instruct-v0.2 on GovReport and MultiFieldQA-zh dataset for different group sizes. Quantization setting: KV cache 2bits, window size 128 .

## 5 Conclusion

In this paper, we achieve accurate ultra-low precision $\mathrm{KV}$ cache quantization. By channel reordering, we group similar channels together, and apply group clipping to further mitigate the outlier problem. We propose a sliding window quantization strategy with filter rules, which greatly improves the performance of the $\mathrm{KV}$ cache quantization method on long context tasks by reserving a small portion of the cache to full precision. By combining theses two approaches, we successfully quantize the $\mathrm{KV}$ cache to Key 2bits value 1.5 bits
without significant precision loss. We believe this work will further advance the design of mixed-precision quantization strategies for $\mathrm{KV}$ cache. In the future, we will further optimize the filter rules and the kernel implementation.

## References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. ArXiv, abs/2308.14508, 2023. URL https://api.semanticscholar.org/CorpusID:261245264

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. ArXiv, abs/2004.05150, 2020. URL https://api.semanticscholar.org/ CorpusID:215737171

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicunal

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. ArXiv, abs/2307.08691, 2023. URL https://api.semanticscholar.org/CorpusID: 259936734

Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. ArXiv, abs/2208.07339, 2022. URL https://api.semanticscholar.org/CorpusID:251564521

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate posttraining quantization for generative pre-trained transformers. ArXiv, abs/2210.17323, 2022. URL https://api.semanticscholar.org/CorpusID:253237200

Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hanna Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. ArXiv, abs/2402.10171, 2024. URL https://api.semanticscholar.org/CorpusID :267682361

Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. ArXiv, abs/2310.01801, 2023. URL https://api.semanticscholar.org/CorpusID:263609075

Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024.

Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023. URL https://api.semanticscholar.org/ CorpusID:263830494

Greg Kamradt. Needle in a haystack - pressure testing llms. https://github.com/ gkamradt/LLMTest_NeedleInAHaystack, 2023.

Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. ArXiv, abs/2306.07629, 2023. URL https://api.semanticscholar.org/CorpusID: 259144954 .

Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of bert. ArXiv, abs/1908.08593, 2019. URL https://api.semanticscholar.org/ CorpusID:201645145

Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023. URL https://lmsys.org/blog/2023-06-29-longchat

Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for $11 \mathrm{~m}$ compression and acceleration. ArXiv, abs/2306.00978, 2023. URL https://api. semanticscholar.org/CorpusID:258999941

Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for $1 \mathrm{~lm} \mathrm{kv}$ cache compression at test time. ArXiv, abs/2305.17118, 2023. URL https://api.semanticscholar.org/CorpusID:258947558

Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2016.

Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. In The Twelfth International Conference on Learning Representations, 2023.

Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pp. 31094-31116. PMLR, 2023.

Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. URL https://api.semanticscholar.org/ CorpusID:259950998

Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35: $17402-17414,2022$.

Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.

Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. ArXiv, abs/2211.10438, 2022. URLhttps://api.semanticscholar.org/CorpusID:253708271

Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. ArXiv, abs/2309.17453, 2023. URL https://api.semanticscholar.org/CorpusID:263310483

Zhihang Yuan, Lin Niu, Jia-Wen Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models. ArXiv, abs/2304.01089, 2023. URL https: //api.semanticscholar.org/CorpusID:257913374

Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, et al. Llm inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024.

Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, and Liqiang Nie. Wkvquant: Quantizing weight and key/value cache for large language models gains more. arXiv preprint arXiv:2402.12065, 2024.

Zhenyu (Allen) Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. ArXiv, abs/2306.14048, 2023. URL https://api.semanticscholar. org/CorpusID:259263947.

Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. ArXiv, abs/2310.19102, 2023. URL https://api semanticscholar.org/CorpusID:264828796
