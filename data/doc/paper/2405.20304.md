# Group Robust Preference Optimization in Reward-free RLHF 

Shyam Sundhar Ramesh ${ }^{1,6} \quad$ Yifan $\mathrm{Hu}^{3,4} \quad$ Iason Chaimalas $^{1} \quad$ Viraj Mehta ${ }^{2}$<br>Pier Giuseppe Sessa ${ }^{3}$<br>Haitham Bou Ammar ${ }^{1,5}$<br>Ilija Bogunovic ${ }^{1}$

May 31, 2024


#### Abstract

Adapting large language models (LLMs) for specific tasks usually involves fine-tuning through reinforcement learning with human feedback (RLHF) on preference data. While these data often come from diverse labelers' groups (e.g., different demographics, ethnicities, company teams, etc.), traditional RLHF approaches adopt a "one-size-fits-all" approach, i.e., they indiscriminately assume and optimize a single preference model, thus not being robust to unique characteristics and needs of the various groups. To address this limitation, we propose a novel Group Robust Preference Optimization (GRPO) method to align LLMs to individual groups' preferences robustly. Our approach builds upon reward-free direct preference optimization methods, but unlike previous approaches, it seeks a robust policy which maximizes the worst-case group performance. To achieve this, GRPO adaptively and sequentially weights the importance of different groups, prioritizing groups with worse cumulative loss. We theoretically study the feasibility of GRPO and analyze its convergence for the log-linear policy class. By fine-tuning LLMs with GRPO using diverse group-based global opinion data, we significantly improved performance for the worst-performing groups, reduced loss imbalances across groups, and improved probability accuracies compared to non-robust baselines.


## 1 Introduction

As the usage of large language models (LLMs) has grown in recent years, the question of their alignment has come to the forefront. Their remarkable capability to address a wide range of tasks (Radford et al. [34]) stems from pre-training on a self-supervised objective over internet-scale text. This vast internetscale content, however, carries a higher risk of biases, inaccuracies, and controversial content than smaller, curated datasets. Thus, ensuring that the model's responses and behaviors correspond to human intentions and values is crucial.

Typical approaches to alignment $[10,32,35]$ involve gathering preference feedback from human labelers to train models that reflect their desires. Such approaches often treat individual preferences as samples from a broader preference distribution. However, this perspective often oversimplifies the complex reality that human societies consist of numerous distinct groups (e.g., different demographics, ethnicities, company teams, etc.), each with their own set of preferences that can significantly diverge. Consequently, prevalent[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_813d7dbaa979a125d6b7g-02.jpg?height=793&width=1617&top_left_y=246&top_left_x=254)

Figure 1: Current reward-free preference optimization methods typically optimize based on average human feedback. This often aligns predominantly with the preferences of the majority group $(\mathrm{G} 1, \mathrm{R} 1>\mathrm{R} 2)$ at the expense of minority groups (G2, R2 $>$ R1). In contrast, our GRPO algorithm introduces adaptive weighting for different user groups and prioritizes optimizing for the worst-case group performance, leading to better alignment for the most disadvantaged groups.

alignment strategies tend to adopt a "one-size-fits-all" model and disproportionately favor the preferences of the majority group, often at the expense of minority groups and their preferences, as illustrated in Figure 1.

To improve alignment performance for even the most disadvantaged groups, we propose to robustly solve the problem of diverse group preferences by (i) including group information in the context of the LLM and (ii) optimizing against the worst-case alignment performance across all groups. We develop policies that guarantee equitable performance across all groups, ensuring that no group is disproportionately disadvantaged due to inherent biases or imbalances in the training data.

Related work. The established process for alignment of LLMs using Reinforcement Learning from Human Feedback (RLHF) is set out in [42, 57] and [32]. The RLHF fine-tuning process consists of learning a reward model from human comparisons between responses to a given prompt, using the Bradley-Terry model [4]. Then, one performs policy optimization using Proximal Policy Optimization [38] to learn a policy that maximizes the learned reward function. For a comprehensive overview and perspective of the RLHF topic, we refer the reader to $[22,6,21]$.

Due to the challenges of tuning PPO and the vulnerability of reward models ([47, 35, 15, 46]), alternative approaches to PPO-based RLHF have been proposed, including rejection sampling fine-tuning [12, 16, 46, 28] and conditional supervised fine-tuning [18, 51, 8]. In particular, Rafailov et al. [35] introduce Direct Preference Optimization (DPO), which optimizes policies directly based on human preferences, avoiding the need for a separate reward model. This approach simplifies training and reduces reward overfitting. Other studies, such as $[1,55,44,41,14]$, propose novel reward-free RLHF methods, with some bypassing preference datasets altogether $([14,5]$ ). We utilize a reward-free framework similar to $[35,1]$, however, unlike previous works that assume a single preference distribution, we consider multiple preference distributions from diverse groups. Further, we aim to robustly fine-tune the LLM to ensure minimal disparity in performance across all groups. Other studies addressing robustness in preference optimization include [19]
and [24]. However, these works primarily focus on different aspects of robustness, such as robustness to noise and resilience against out-of-preference data.

Robust language modeling techniques have been studied by [31, 49] to optimize performance of language models over a wide-range of topics. They consider robust pre-training of language models based on the group Distributionally Robust Optimization (DRO) approach. A concrete theoretical study of the group DRO approach was performed in [37] and applied to vision problems. These are designed by extending previous minimax algorithm for solving DRO from [29]. In the RLHF setup, [3] consider weighting of loss from different topics (harmless vs helpful) for robust reward learning. Also, Chakraborty et al. [7] consider robust policy optimization by learning multiple reward functions corresponding to sub-populations and learning a robust policy w.r.t. the learned rewards. Differing from these works, we embed group robustness directly into the reward-free tuning paradigm. We provide an concrete algorithm that adaptively weighs the loss for different groups and optimizes for a policy that minimizes the weighted loss. Further, our algorithm employs a novel gradient estimator tailored to the group robust DPO problem.

In the non-robust setup, [54] also explores group-based preference learning with LLMs by including group information in prompts with a transformer module that is trained optimally choose an example sequence of prompts, LLM responses, and group preferences for in-context learning. We detail other related works extensively in Appendix A.

Main Contributions. The following are the main contributions of this work: (i) We present GRPO, the group robust formulation of Direct Preference Optimization (DPO) [35], wherein we augment the context of the LLM with the group information, and pose the problem as a robust optimization problem to minimize the worst-case loss amongst the diverse groups. To the best of our knowledge, this is the first study to focus on group robustness in RLHF preference optimization; (ii) We analyze the theoretical aspects of GRPO by examining the convergence and the feasibility of finding optimal solutions within the log-linear policy class; (iii) We present a tailored algorithm to tackle this robust optimization challenge, providing convergence guarantees for certain loss functions; (iv) We show the versatility of our approach by demonstrating how our algorithm can be utilized with other reward-free preference optimization methods such as Identity Preference Optimization (IPO) [1]. (v) Our empirical evaluations across synthetic datasets, real-world data, and publicly available LLMs show that the proposed GRPO significantly improves performance for the worstperforming groups, reduces loss imbalances across groups, and increases probability accuracies compared to non-robust baselines.

## 2 Background

We address the challenge of fine tuning a large language model (LLM) to align with user preferences. This process usually follows the Reinforcement Learning from Human Feedback (RLHF) protocol, using either an explicit reward model ([3, 32, 42, 57]) or an implicit reward model ([35]). RLHF typically comprises three key phases: (i) supervised fine-tuning of an initial (pre-trained) large-language model, (ii) reward learning, and (iii) RL fine-tuning.

In the supervised fine-tuning phase (SFT), the goal is to fine-tune a pre-trained LLM on a specific highquality dataset suited for the downstream task of interest. It results in a probabilistic model expressing the probability of the response $y$ given a prompt $x$ as $\pi_{\mathrm{ref}}(y \mid x)$. Subsequently, in the reward learning phase, the goal is to learn a reward model from a dataset of prompts $x$ and responses $y_{w}$, $y_{l}$, with $y_{l} \prec y_{w} \mid x$ meaning that human labellers preferred $y_{w}$ over $y_{l}$.

It is typically assumed that preferences follow some choice models with an unknown reward (utility) $r^{*}(x, y)$ function. A popular model is the Bradley-Terry model [4] that assumes the preference distribution
$p$ admits the following form :

$$
\begin{equation*}
p\left(y_{1} \prec y_{2} \mid x\right)=\frac{\exp \left(r^{*}\left(x, y_{2}\right)\right)}{\exp \left(r^{*}\left(x, y_{1}\right)\right)+\exp \left(r^{*}\left(x, y_{2}\right)\right)} \tag{1}
\end{equation*}
$$

Based on the above model, a maximum likelihood estimate of the reward function is obtained as:

$$
\begin{equation*}
\min _{r}\left\{\mathcal{L}_{R}(r ; \mathcal{D}):=-\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}\left[\log \left(\sigma\left(r\left(x, y_{w}\right)-r\left(x, y_{l}\right)\right)\right)\right]\right\} \tag{2}
\end{equation*}
$$

where $\sigma(\cdot)$ is the sigmoid function and $\mathcal{D}$ represents the dataset consisting of $\left\{\left(x, y_{w}, y_{l}\right)\right\}$.

Then, in the RL fine-tuning phase the objective is to train a policy $\pi$ that maximizes the learned reward function. Simultaneously, the policy should stay closely aligned with the reference, $\pi_{\text {ref }}$, as quantified by the KL divergence, leading to the following KL-regularized optimization problem:

$$
\begin{equation*}
\max _{\pi} \mathbb{E}_{x \sim \mathcal{P}_{x}}\left[\mathbb{E}_{y \sim \pi}[r(x, y)]-\beta \operatorname{KL}\left[\pi(y \mid x) \| \pi_{\mathrm{ref}}(y \mid x)\right]\right] \tag{3}
\end{equation*}
$$

Direct Preference Optimization (DPO). The recent approach proposed by [35] exploits the closedform solution of the problem in Equation (3) and sidesteps the explicit modelling of rewards to directly optimize the policy. Specifically, under the Bradley-Terry preference model, the reward function can be expressed directly in terms of the optimal policy $\pi^{*}$ as follows:

$$
\begin{equation*}
r(x, y)=\beta \log \frac{\pi^{*}(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}+\beta \log Z(x) \tag{4}
\end{equation*}
$$

for a partition function $Z(x)=\sum_{y} \pi_{\text {ref }}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)$. Via a change of variable, finding the optimal reward function in Equation (2) is equivalent to finding the optimal policy $\pi^{*}$ utilizing the given set of preference data $\mathcal{D}$. With a slight abuse of notation, we use $\pi$ to denote $\pi^{*}$. Denote $h_{\pi}\left(x, y_{w}, y_{l}\right):=$ $\log \left(\frac{\pi\left(y_{w} \mid x\right)}{\pi_{\text {ref }}\left(y_{w} \mid x\right)}\right)-\log \left(\frac{\pi\left(y_{l} \mid x\right)}{\pi_{\text {ref }}\left(y_{l} \mid x\right)}\right)$. Then, Equation (2) translates into the DPO loss:

$$
\begin{equation*}
\mathcal{L}_{\mathrm{DPO}}(\pi, \mathcal{D})=-\underset{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}{\mathbb{E}}\left[\log \left(\sigma\left(\beta \cdot h_{\pi}\left(x, y_{w}, y_{l}\right)\right)\right)\right] \tag{5}
\end{equation*}
$$

With a parameterized policy model $\pi_{\theta}$, minimizing the DPO loss involves calculating the gradient over $\theta$ using backpropagation and the log-probabilities of each completion, $y_{w}$ and $y_{l}$, given the prompt $x$ for both the policy $\pi_{\theta}$ and the reference policy $\pi_{\text {ref }}$.

## 3 Group Robust Preference Optimization

In this section, we discuss Group Robust Preference Optimization (GRPO), i.e., instead of learning a reward function that maximizes the likelihood, we aim to derive (implicitly) a robust reward function and subsequently learn a robust DPO policy.

Group Preferences. Suppose that preferences come from an underlying latent reward $r^{*}(x, y, g)$, with $g \in \mathcal{G}=\{1,2, \ldots, K\}$ indexing the groups. When group information is available (e.g., as a text), we can represent the reward as $r^{*}\left(x_{g}, y\right)$, where $x_{g}=x \oplus g$ denotes merging ${ }^{1}$ of the prompt with group information (e.g., string concatenation). We continue to apply a Bradley-Terry model as described in Equation (1), substituting $x$ with $x_{g}$. Moreover, we assume access to a collective dataset $\mathcal{D}=\bigcup_{g=1}^{K} \mathcal{D}_{g}$ where $\mathcal{D}_{g}=\left\{\left(x_{g}^{(i)}, y_{w}^{(i)}, y_{l}^{(i)}\right)\right\}_{i=1}^{N_{g}}$ with the available group information. Additionally, our dataset accommodates[^1]the exposure of different groups to identical prompts, meaning that the same $x$ can appear across various groups $g$ in our dataset, and these groups may favor different responses $y$.

Given such $\mathcal{D}$, although one may obtain a common reward model using Equation (2), it could result in poor generalization for particular groups, especially with significant group-wise disparities in the data (see Figure 1). Such disparities might stem from imbalanced data across groups or difficulties associated with learning different groups.

GRPO Objective. Consequently, we propose to measure the alignment of the reward model on the worst-case group loss:

$$
\begin{equation*}
\max _{g \in G} \mathcal{L}_{R}\left(r ; \mathcal{D}_{g}\right) \tag{6}
\end{equation*}
$$

Incorporating the reward expression from (Equation (4)) into (Equation (6)), we establish the group robust preference optimization (GRPO) objective for a specified policy $\pi$ :

$$
\begin{equation*}
\mathcal{L}_{\mathrm{GR}}(\pi):=\max _{g \in \mathcal{G}} \mathcal{L}_{\mathrm{DPO}}\left(\pi, \mathcal{D}_{g}\right)=\max _{g \in \mathcal{G}}\left(-\mathbb{E}_{\left(x_{g}, y_{w}, y_{l}\right) \sim \mathcal{D}_{g}}\left[\log \left(\sigma\left(\beta h_{\pi}\left(x_{g}, y_{w}, y_{l}\right)\right)\right)\right]\right) \tag{7}
\end{equation*}
$$

Leveraging the equivalent formulation of maximizing over discrete set, the GRPO problem becomes

$$
\begin{equation*}
\min _{\pi} \mathcal{L}_{\mathrm{GR}}(\pi)=\min _{\pi} \max _{\alpha \in \Delta_{K}} \sum_{g=1}^{K} \alpha_{g}\left(-\mathbb{E}_{\left(x_{g}, y_{w}, y_{l}\right) \sim \mathcal{D}_{g}}\left[\log \left(\sigma\left(\beta h_{\pi}\left(x_{g}, y_{w}, y_{l}\right)\right)\right]\right)\right. \tag{8}
\end{equation*}
$$

where $\Delta_{K}$ represents the $(K-1)$-dimensional simplex of probabilities. ${ }^{1}$ The inner maximization becomes a linear programming over simplex such that $\alpha$ represents the weights of groups. In addition, it forms a two-player zero-sum game (see Section 3.1), where the policy $\pi$ and $\alpha$ act as opponents with inversely related payoffs. The DPO loss (logistic log loss) in Equation (7) can be replaced with alternatives like hinge or squared loss (see [44]). We label this objective GR-DPO when using DPO loss, and explore GRPO with squared loss in Section 4.1.

Applications. In this study, we do not assume any specific distribution for groups $\mathcal{D}_{g}$. The collection of prompts per group, $\mathcal{P}_{x_{g}}$, may have varying degrees of overlap. The GRPO framework accommodates both distinct and overlapping prompt scenarios across different groups.

Apart from human groups, GRPO can be useful in scenarios where groups $\mathcal{D}_{g}$ represent distinct tasks or topics within preference datasets, like helpful/harmful, truthful/unanswerable instances, or domain-specific categories (e.g., math, physics, chemistry). Typically, these prompt distributions $\left\{\mathcal{P}_{x_{g}}\right\}_{g=1}^{N}$ are disjoint, and GRPO seeks to optimize performance even across the most challenging categories.

GRPO is also applicable in scenarios where groups reflect diverse user preferences for a shared set of prompts, with the goal of achieving equitable performance across user groups. This contrasts with nonrobust DPO, which aims to optimize preferences on average and might overlook minority groups.

Lastly, we acknowledge that the max-min objective of Equation (8) might be overly conservative, potentially degrading average performance. We explore a more balanced approach between worst-case and standard preference optimization objective in Appendix B.4.

### 3.1 Further Discussion and Insights

This section provides two insights regarding the GR-DPO loss in Equation (8).

Log-linear policy class. The zero-sum game perspective allows us to explore the presence of a Nash equilibrium, serving as a benchmark for convergence during the policy optimization process. Given that the domain of $\alpha$ is a simplex $\Delta_{K}$ (in Equation (8)), we further define a parameterized policy class $\Pi_{\theta}$ for the[^2]policy $\pi_{\theta}$. We assume that the parameterized policy $\pi_{\theta}$ is of the form $\pi_{\theta}(y \mid x)=\frac{\exp f_{\theta}(x, y)}{\sum_{y \in \mathcal{Y}} \exp f_{\theta}(x, y)}$, where $f_{\theta}$ is a linear function or a neural network, and $\theta$ belongs to a convex set $\Theta$.

In LLM fine-tuning, sometimes practitioners concentrate on modifying solely the final layer. It corresponds to a linear function, $f_{\theta}(x, y)=\theta^{T} \phi(x, y)$, with $\phi(x, y)$ denoting the embedding derived from the language model removing its last layer, and $\theta$ as the parameters of the last layer. When applying this linear parameterization, in conjunction with a uniform reference policy $\pi_{\text {ref }}$, the robust objective outlined in Equation (8) is as follows (details in Appendix B.1):

$$
\begin{equation*}
\min _{\theta \in \Theta} \max _{\alpha \in \Delta_{K}} \sum_{g=1}^{K} \alpha_{g}\left(-\mathbb{E}_{\left(x_{g}, y_{w}, y_{l}\right) \sim \mathcal{D}_{g}}\left[\log \left(\sigma\left(\beta\left\langle\phi\left(x, y_{w}\right)-\phi\left(x, y_{l}\right), \theta\right\rangle\right)\right)\right]\right) \tag{9}
\end{equation*}
$$

The objective defined in Equation (9) is concave with respect to $\alpha$ and convex with respect to $\theta$. This structure allows the invocation of the minimax theorem for convex-concave functions ([40]) to assert the existence of a Nash equilibrium.

Proposition 1. Under log-linear parameterization of the policy class, there exists a Nash equilibrium for the group robust direct preference optimization problem in Equation (9).

Robust policy optimization. The earlier derivation for the GR-DPO objective $\mathcal{L}_{\mathrm{GR}-\mathrm{DPO}}(\pi)$ relies on incorporating robustness in the reward modeling step (in Equation (7)) while using the solution to the non-robust KL-regularized reward maximization objective in Equation (4).

Interestingly, we can obtain the identical expression for $\mathcal{L}_{\mathrm{GR}-\mathrm{DPO}}(\pi)$ if incorporating robustness in the KL-regularized reward maximization objective and using the reward function learnt in a non-robust way. Consider the robust KL-regularized reward maximization

$$
\begin{equation*}
\max _{\pi} \min _{g \in \mathcal{G}} \mathbb{E}_{x_{g} \sim \mathcal{P}_{x_{g}}, y \sim \pi\left(\cdot \mid x_{g}\right)}\left[r\left(x_{g}, y\right)-\beta \operatorname{KL}\left[\pi\left(y \mid x_{g}\right) \| \pi_{\mathrm{ref}}\left(y \mid x_{g}\right)\right]\right] \tag{10}
\end{equation*}
$$

The following proposition characterizes such an invariant property.

Proposition 2. Substituting the closed-form solution of the robust $K L$-regularized policy maximization problem (Equation (10)) into the non-robust $K L$-regularized reward maximization objective in Equation (2) leads to the same group robust DPO loss $\mathcal{L}_{\mathrm{GR}-\mathrm{DPO}}$ in Equation (8) .

The analysis leverages the fact that the optimal policy of Equation (10) is identical to the solution of the non-robust KL-regularized reward maximization in Equation (4) and is derived in Appendix B.2.

## 4 Algorithm

In this section, we discuss the policy optimization algorithm for solving the group robust DPO problem in Equation (8). In particular, we aim to design an algorithm that performs updates in the parameterized space $\Theta \subset \mathbb{R}^{d}$, i.e., updating $\theta$ of the parameterized policy $\pi_{\theta}$. Leveraging the perspective of the 2-player zero-sum game, we propose an alternating updating algorithm wherein one updates $\alpha$ and $\theta$ alternatively. We summarize the overall approach in Algorithm 1, which we discuss and analyze next.

We employ the DPO loss $l\left(\pi_{\theta} ; \cdot\right)=\log \left(\sigma\left(\beta h_{\pi_{\theta}}(\cdot)\right)\right)$ (Equation (5)) in Algorithm 1, however, our algorithm can support other preference optimization losses (see Section 4.1). The algorithm performs a gradient descent type update on $\theta$ and a deterministic mirror ascent on $\alpha$ using a Bregman divergence with the distance generating function as the KL divergence. Since the $\alpha$ lies in a simplex and the objective is linear, the update of $\alpha$ becomes multiplicative weights update with renormalization to a simplex via softmax (see Nemirovski et al. [30] for details). Further, the weights $\alpha$ are determined by the cumulative losses

```
Algorithm 1 Mirror Descent for Group Robust Preference Optimization (GRPO)
    Initialize: Step size $\eta_{\alpha}$ for group weights $\alpha$, step size $\eta_{\theta}$ for policy $\pi$ with weights $\theta$, initial weights
    $\theta^{(0)}$ of the policy and weights over each group $\alpha^{(0)}$, Projection operator $\mathrm{P}_{\Theta}$
    Input: Dataset $\mathcal{D}$ with size $N=|\mathcal{D}|$, group size $N_{g}$ for $g=\{1,2, \cdots, K\}$, loss $l\left(\pi_{\theta} ; \cdot\right)$
    for $t=1, \ldots, T$ do
        $\alpha^{\prime} \leftarrow \alpha^{(t-1)}$
        $g \sim$ Categorical $\left(N_{1} / N, \cdots, N_{K} / N\right),\left(x_{g}, y_{w}, y_{l}\right) \sim \mathcal{D}_{g}$
        $\alpha_{g}^{\prime} \leftarrow \alpha_{g}^{\prime} \exp \eta_{\alpha}\left(\frac{N \cdot l\left(\pi_{\theta}(t-1) ;\left(x_{g}, y_{w}, y_{l}\right)\right)}{N_{g}}\right) \quad / /$ Update weights for group $g$
        $\alpha^{(t)} \leftarrow \alpha^{\prime} / \sum_{g^{\prime}} \alpha_{g^{\prime}}^{\prime} \quad / /$ Renormalize $\alpha$
        $\theta^{(t)} \leftarrow \mathrm{P}_{\Theta}\left(\theta^{(t-1)}-\eta_{\theta}\left(\frac{N \alpha_{g}^{(t)} \nabla_{\theta} l\left(\pi_{\theta}(t-1) ;\left(x_{g}, y_{w}, y_{l}\right)\right)}{N_{g}}\right)\right) / /$ Use $\alpha$ to update $\theta$
    end for
    Return: Output the robust policy $\pi\left(\theta^{(T)}\right)$
```

$l\left(\pi_{\theta} ; \cdot\right)$ accrued by each group, ensuring that groups with higher cumulative losses get higher weights. The size of the group $N_{g}$ appears as the empirical distribution $\mathcal{D}_{g}$ involves $N_{g}$. We call it alternating update as the updated $\alpha^{t}$ is used in the update from $\theta^{t-1}$ to $\theta^{t}$. In particular, the gradient descent type update on $\theta$ is weighted by $\alpha$ in order to orient the update towards groups with higher losses. The projection operator $\mathrm{P}_{\Theta}$ ensures the updated $\theta^{t}$ lies within $\Theta$.

What does the weighted DPO update do? In Line 9 in Algorithm 1, the algorithm performs parameter updates based on the weighted gradients. By using the DPO loss, i.e., $l\left(\pi_{\theta} ; \cdot\right)=\log \left(\sigma\left(\beta h_{\pi_{\theta}}(\cdot)\right)\right.$ (see Equation (5)), we obtain the following gradient update expression ignoring the $N / N_{g}$ constant

$$
\begin{align*}
& \alpha_{g}^{(t)} \nabla_{\theta} l\left(\pi_{\theta^{(t-1)}} ;\left(x_{g}, y_{w}, y_{l}\right)\right)=\alpha_{g}^{(t)} \nabla_{\theta} \log \left(\sigma\left(\beta h_{\pi_{\theta^{(t-1)}}}\left(x_{g}, y_{w}, y_{l}\right)\right)\right)  \tag{11}\\
& =\alpha_{g}^{(t)} \sigma\left(r_{\theta^{(t-1)}}\left(x_{g}, y_{l}\right)-r_{\theta^{(t-1)}}\left(x_{g}, y_{w}\right)\right) \times\left[\nabla_{\theta} \log \pi_{\theta^{(t-1)}}\left(y_{w} \mid x_{g}\right)-\nabla_{\theta} \log \pi_{\theta^{(t-1)}}\left(y_{l} \mid x_{g}\right)\right]
\end{align*}
$$

The final term plays the critical role of enhancing the likelihood of the preferred response while simultaneously diminishing the likelihood of the rejected response. This adjustment is proportional to the disparity in rewards between the two responses. Moreover, the inclusion of $\alpha_{g}$ is pivotal for ensuring group robustness. This coefficient scales the gradient w.r.t. $\theta$ based on the cumulative loss previously received by all samples within a specific group. Such a mechanism ensures that the model's focus is increasingly directed towards groups that have historically suffered higher losses. Additionally, the scaling factor $N_{g}$ guarantees that groups with a smaller volume of data do not face a disadvantage. We defer further details in obtaining the gradient update expression to Appendix B.3.

We demonstrate the global convergence with the following proposition.

Proposition 3. Suppose that the loss $l\left(\cdot ;\left(x_{g}, y, y^{\prime}\right)\right)$ is non-negative, convex, $B_{\nabla}-$ Lipschitz continuous, and bounded by $B_{l}$ for all $\left(x_{g}, y, y^{\prime}\right) \in \mathcal{X} \oplus \mathcal{G} \times \mathcal{Y} \times \mathcal{Y}$ and $\|\theta\|_{2} \leq B_{\Theta}$ for all $\theta \in \Theta$ with convex $\Theta \subset \mathbb{R}^{d}$. The error of the average iterate of Algorithm 1, i.e., $\pi_{\bar{\theta}(1: T)}=\frac{1}{T} \sum_{t=1}^{T} \theta^{t}$, satisfies

$$
\mathbb{E}\left[\mathcal{L}_{\mathrm{GR}}\left(\pi_{\bar{\theta}(1: T)}\right)\right]-\min _{\theta \in \Theta} \mathcal{L}_{\mathrm{GR}}\left(\pi_{\theta}\right)=\mathcal{O}\left(T^{-1 / 2}\right)
$$

We defer the proof of this proposition to Appendix E. The analysis follows from an adaptation of the analysis in Nemirovski et al. [30] for the proposed sampling strategy in Algorithm 1 ${ }^{1}$. We note that when fine-tuning only the final layer of a LLM, the output policy exists within the log-linear policy class (see Section 3.1), and the corresponding loss function satisfies the assumptions in Proposition 3 (see Lemma 5).[^3]

### 4.1 Group Robust Identity Preference Optimization

The standard regularized reward maximization objective (Equation (3)) in DPO [35], tends to overlook the KL-regularization and learn deterministic policies. This learned policy assigns preference probability one to winning responses in the data which is often not realistic (see [1][Section 4.2] and Appendix C). Recently, Azar et al. [1] show that the standard regularized reward maximization objective (Equation (3)) in DPO [35] tends to overlook the KL-regularization and learn deterministic policies (see [1, Section 4.2] and Appendix C). They thus propose an alternative approach called Identity Preference Optimization (IPO) that is more likely to learn a randomized policy which assigns appropriate probability to the preferred response and prevents overfitting. Following a similar derivation as we did for group robust DPO with details given in Appendix C, we develop the corresponding group robust IPO (GR-IPO):

$$
\min _{\pi} \mathcal{L}_{\mathrm{GR}}(\pi):=\max _{g \in \mathcal{G}} \mathcal{L}_{\mathrm{IPO}}\left(\pi, \mathcal{D}_{g}\right)=\max _{\alpha \in \Delta_{K}} \sum_{g=1}^{K} \alpha_{g}\left(\underset{\left(x_{g}, y_{w}, y_{l}\right) \sim \mathcal{D}_{g}}{\mathbb{E}}\left[h_{\pi}\left(x_{g}, y_{w}, y_{l}\right)-\frac{1}{2 \beta}\right]^{2}\right)
$$

For the log-linear policy class (introduced in Section 3.1), the objective function simplifies to

$$
\min _{\theta \in \Theta} \max _{\alpha \in \Delta_{K}} \sum_{g=1}^{K} \alpha_{g}\left(\mathbb{E}_{\left(x_{g}, y_{w}, y_{l}\right) \sim \mathcal{D}_{g}}\left[\left(\left\langle\phi\left(x_{g}, y_{w}\right)-\phi\left(x_{g}, y_{l}\right), \theta\right\rangle-\frac{1}{2 \beta}\right)^{2}\right]\right)
$$

To solve the GR-IPO above, it suffices to use Algorithm 1 with slight modifications, see Algorithm 2 in Appendix $\mathrm{C}$. In particular, the update of $\theta$ is replaced by a weighted regression update:

$$
\widehat{\theta} \leftarrow \operatorname{argmin}_{\theta \in \Theta} \sum_{\left(x_{g}, y_{w}, y_{l}\right) \sim \mathcal{D}}\left[\frac{\alpha_{g}}{N_{g}}\left(\left\langle\phi\left(x_{g}, y_{w}\right)-\phi\left(x_{g}, y_{l}\right), \theta\right\rangle-\frac{1}{2 \beta}\right)^{2}\right]
$$

For fixed $\alpha$, we show (in Appendix C) that such an update admits a closed-form solution:

$$
\widehat{\theta}=\frac{1}{2 \beta}\left(S^{T} W S\right)^{-1} S^{T} W \mathbf{1} \quad \text { with } \quad W:=\operatorname{Diag}\left[\frac{\alpha_{g^{(1)}}}{N_{g^{(1)}}}, \ldots, \frac{\alpha_{g^{(N)}}}{N_{g^{(N)}}}\right]
$$

where $g^{(i)}$ is the group of each sample $i, N_{g^{(i)}}$ is the number of samples in group $g^{(i)}$ and $\mathbf{1}$ is a column vector of ones of dimension $N$. Here $S$ is a matrix

$$
S:=\left[\left(\phi\left(x_{g}^{(1)}, y_{w}^{(1)}\right)-\phi\left(x_{g}^{(1)}, y_{l}^{(1)}\right)\right)^{T}, \ldots,\left(\phi\left(x_{g}^{(N)}, y_{w}^{(N)}\right)-\phi\left(x_{g}^{(N)}, y_{l}^{(N)}\right)\right)^{T}\right]
$$

Each row of $S$ represents the difference in feature mappings $\phi$ of the preferred and less preferred response for each prompt. The group robust IPO (GR-IPO) algorithm is presented in Appendix C, and its empirical results are shown in Section 5.

## 5 Experiments

In this section, we study the empirical performance of our proposed Algorithm 1 on synthetic and real-world datasets ${ }^{1}$. First, we simulate multi-group data disparities by varying the size and preference distributions of two synthetic groups. In the real-world setup, we study the alignment of an LLM to the real preferences of people from various countries. We examine whether GRPO aligns the LLM in a more equitable manner to reduce discrepancies in alignment among various groups. Finally, we demonstrate that performance is improved by explicitly addressing the grouped nature of the data during alignment.[^4]![](https://cdn.mathpix.com/cropped/2024_06_04_813d7dbaa979a125d6b7g-09.jpg?height=376&width=1654&top_left_y=234&top_left_x=232)

Figure 2: Algorithm 1 (GR-DPO and GR-IPO) leads to a lower worst-case validation loss and reward error compared to importance sampling and vanilla methods. Results refer to the scenario in which groups have different sizes but same responses' distribution. Note that the gap between Algorithm 1 and importance sampling is smaller than in Figure 2. This is expected considering that the primary difference between groups arises from data imbalance, which is handled by importance sampling.
![](https://cdn.mathpix.com/cropped/2024_06_04_813d7dbaa979a125d6b7g-09.jpg?height=370&width=1650&top_left_y=912&top_left_x=236)

Figure 3: Algorithm 1 (GR-DPO and GR-IPO) leads to a lower worst-case validation loss and reward error compared to the non-robust vanilla methods. Results refer to the scenario in which groups have same sizes but different responses' distribution. Unlike the setups of Figure 2 and Figure 4 importance sampling has no effect here (it coincides with vanilla DPO/IPO since groups have the same sizes).

### 5.1 Synthetic Experiments

We evaluate the performance of Algorithm 1 using synthetically generated group preference data for the loss function $l\left(\pi_{\theta ;} \cdot\right)$ - either DPO loss or IPO loss and denote them as GR-DPO and GR-IPO, respectively. We compare them against vanilla DPO and IPO ([35, 1]), and the importance-sampling (IS) variants of DPO and IPO (where the loss of each datapoint is inversely weighted by its group data size).

Experimental Setup. Our experiments are designed to analyze settings where there exist multiple groups with distinct characteristics. We adapt the standard (non-group based) experimental setup proposed by [24] for the group preferences setting by incorporating group information into the reward function $r$ : $\mathcal{X} \times \mathcal{Y} \times \mathcal{G} \rightarrow \mathbb{R}$. Here, $\mathcal{X}$ represents a two-dimensional state space $[0,1] \times[0,1], \mathcal{Y}$ denotes a discrete action space $\{0,1,2,3, \ldots, n\}$, and $\mathcal{G}$ signifies a discrete group space $\{0,1,2, \ldots, K\}$. The reward function, defined by the group-dependent feature vector $\phi(x, y, g)$ and parameter vector $\theta_{g}$, is given as $r(x, y, g):=$ $\left\langle\phi(x, y, g), \theta_{g}\right\rangle$. Further, we consider $n=7, K=2$ and denote $x:=\left(x_{0}, x_{1}\right) \in \mathcal{X}$. The feature vector $\phi(x, y, g):=\left(\phi_{0}(x, y, g), \phi_{1}(x, y, g), \phi_{2}(x, y, g), \phi_{3}(x, y, g)\right)$ and parameters $\theta_{g} \forall g \in \mathcal{G}$ are defined as follows:

$$
\phi_{i}(x, y, g):=\left\{\begin{array}{ll}
\left(\frac{y}{n+1}+1\right) \cdot \cos \left(x_{\lfloor i / 2\rfloor} \cdot \pi\right) & \text { if } i \% 2=g \\
\left(\frac{1}{\frac{y}{n+1}+1}\right) \cdot \sin \left(x_{\lfloor i / 2\rfloor} \cdot \pi\right) & \text { otherwise }
\end{array}, \quad \theta_{0} \quad:=(1,3,1,3), \theta_{1}:=(3,1,3,1)\right.
$$

Note, that the the feature vectors $\phi(x, y, g)$ have a coordinate-flipped relationship across groups. Also, we
![](https://cdn.mathpix.com/cropped/2024_06_04_813d7dbaa979a125d6b7g-10.jpg?height=370&width=1638&top_left_y=236&top_left_x=236)

Figure 4: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses' distribution.

study two other feature parameterizations and include their experimental results in Appendix D.2.

We consider the following scenarios: (i) Groups are imbalanced in terms of size but have the same distribution over responses, (ii) Groups are balanced in terms of size but have different response distributions, and (iii) Groups are imbalanced in terms of size and also have different response distributions. Note that having different response distributions leads to a difference in the difficulty of learning, since groups with responses distant from each other (in terms of rewards or preference probabilities) are typically more distinguishable and easier to learn. We discuss in Appendix D. 1 how we generate these three scenarios.

Implementation. Leveraging the linearity in the reward model, we utilize a log-linear policy class parameterized by $\theta: \pi_{\theta}(y \mid x)=\frac{\exp \langle\phi(x, y, g), \theta\rangle}{\sum_{y^{\prime} \in \mathcal{Y}} \exp \left\langle\phi\left(x, y^{\prime}, g\right), \theta\right\rangle}$. We run Algorithm 1 for both DPO and IPO loss relative to the policy class detailed above with a dataset of 300 action pairs with preferences.

Evaluation Metrics. We use the following criteria to assess the performance of the algorithms:

Max Validation Loss. For each group $g$, with preference data denoted as $\left(x^{i}, y_{w}^{i}, y_{l}^{i}, g\right)_{i=1}^{N_{g}}$, where $N_{g}$ is the number of data points in the group, we compute the DPO/IPO validation loss separately for each group and identify the maximum loss among them in each run.

Max Reward Error. This metric compares the true reward of the optimal action determined by $\theta_{g}^{*}$ with that of the action deemed optimal by estimate $\widehat{\theta}$ for each group, and identifies the maximum error across all groups in each run. In particular, for data in the form $\left(x^{i}, g\right)_{i=1}^{N_{g}}$, which includes only states and groups, we calculate reward errors for every group $g$ as follows: $\mathbb{E}_{(x, g) \sim\left(x^{i}, g\right)_{i=1}^{N_{g}}}\left[\max _{y}\left\langle\phi(x, y, g), \theta_{g}^{*}\right\rangle-\right.$ $\left.\left\langle\phi\left(x, \operatorname{argmax}_{y}\langle\phi(x, y, g), \widehat{\theta}\rangle, g\right), \theta_{g}^{*}\right\rangle\right]$.

Results. We present the average performance of Algorithm 1 (error bars over 20 seeds) alongside baseline methods in Figures 2 to 4 for scenarios (i), (ii) and (iii) respectively. Our findings indicate that the robust methods consistently surpass both vanilla and importance-sampling approaches. Notably, the robust methods demonstrate significant superiority in uneven group scenarios, where the importance-sampling technique falls short as it exclusively deals with data imbalance.

### 5.2 Global Opinion Experiments

For the real-data experiments, we consider the survey dataset GlobalOpinionQA ([13]) and the publicly available Gemma-2B model [45]. ${ }^{1}$ The data contains multiple choice questions answered by participants from various countries, amounting to 2,554 questions covering various topics, including politics, media, technology, religion, race, and ethnicity. For each question, the dataset provides a probability vector over the choices, signifying the percentage of people from a particular country choosing each option. Note that[^5]![](https://cdn.mathpix.com/cropped/2024_06_04_813d7dbaa979a125d6b7g-11.jpg?height=716&width=1662&top_left_y=225&top_left_x=228)

Figure 5: Global opinion data: Top plots: GR-IPO leads to better worst-case final test loss and reward accuracy compared to IPO. Moreover, it leads to more balanced losses across the different groups, reducing the gap between best and worst-group loss (Group-1 vs. Group-5). Bottom plots: Log-prob. accuracy (left plot) and group weights (middle plot) during GR-IPO training. GR-IPO increases the weight on worse-performing groups (Groups-2,5) and decreases it on high-performing ones (Groups-1,3,4), leading to better worst-case accuracy. Groups-2,5 are the ones with worse log-prob. accuracy at the beginning of training (right plot with a random subset of the training data). We show the corresponding end-of-training log-prob. accuracies for GR-IPO in Figure 12 of Appendix D.

this probability vector would be different for different countries. Hence, the goal is to align the LLM to the probability vector corresponding to each country in a robust manner.

We consider the following five countries in the dataset: Nigeria, Egypt, India, China and Japan, with data sizes $572,570,376,309$, and 712 , respectively. We construct our training set as follows: For the SFT training, we choose the best option (the choice with the highest probability) as the target. For both IPO and GR-IPO training, we consider the best option as the winning response and another randomly chosen option as the losing response. We outline the exact prompt we use in Appendix D.

We run the SFT training for one epoch over the training data on the pre-trained Gemma-2B model. For both IPO/GR-IPO training we use the AdamW [25] optimizer with adaptive learning rates. We then evaluate both the methods based on the worst group loss and accuracy. Here, the accuracy refers to the percentage of winning response and losing response pairs correctly ordered by the learned preference function (Equation (35)). We defer further training and hyperparameter details to Appendix D.

Results. We present the average performance of GR-IPO over five seeds alongside IPO in Figure 5 (top plots). Our findings indicate that GR-IPO outperforms IPO in terms of maximum group loss and minimum group reward accuracies. Moreover, GR-IPO effectively reduces the imbalance in loss values among different groups. Additionally, we observe an improvement in log-probability accuracies (which measure if the probability assigned by the fine-tuned model is higher for the winning response compared to the losing response) for both IPO and GR-IPO, with GR-IPO demonstrating better alignment for the worst-performing group compared to IPO.

Insights. We further note that the worst-performing groups are Groups-2,5, as shown in Figure 5. GRIPO improves the loss for these groups by assigning more weight to them, as illustrated in Figure 5 (bottom middle plot). Additionally, we plot the initial log-probability accuracies for different groups in Figure 5 (bottom right plot), assessing how accurately the SFT model classifies the winning versus losing response for different groups. It is evident that Groups-2,5 are already underperforming. Given that SFT training
converges within one epoch without any discrepancies between groups, this indicates that the base LLM inherently struggles with classifying responses for Groups-2,5. However, by employing GR-IPO, we have mitigated the imbalance in the performance of the fine-tuned model.

## 6 Conclusions

We formalize the problem of robustly aligning an LLM to preference distributions from diverse groups. To tackle the same, we introduced GRPO, a group robust formulation of reward-free RLHF, aiming to minimize worst-case loss among groups. We explored the theoretical aspects of GRPO and demonstrated its improved robust alignment performance through various experiments. We believe our approach will be highly valuable for future tailored LLM fine-tuning, specifically aimed at aligning with the needs of diverse teams and user groups. In a broader context, it holds promise for mitigating biases and discrepancies across various societal groups encountered in the task-specific adaptation of LLMs.

Limitations. When the dataset is balanced among groups and difficulty levels are comparable, our GRPO approach does not offer a significant advantage over standard reward-free RLHF algorithms. Further, in scenarios where optimizing worst-case performance is less critical, robust group performance can still be ensured by trading off worst-case for average performance using a trade-off parameter. This modified objective and the necessary algorithmic changes are elaborated in Appendix B.4. The appropriate tuning of the trade-off parameter for the specific application remains a subject for future investigation.

## 7 Acknowledgments

PGS was gratefully supported by ELSA (European Lighthouse on Secure and Safe AI) funded by the European Union under grant agreement No. 101070617. YH was supported by NCCR Automation from Switzerland. IB was supported by the EPSRC New Investigator Award EP/X03917X/1; the Engineering and Physical Sciences Research Council EP/S021566/1; and Google Research Scholar award. The authors would like to thank William Bankes, Seongho Son, Matthieu Zimmer, Afroditi Papadaki, Eduardo Pignatelli, and Nagham Osman for the useful discussion.

## References

[1] Mohammad Gheshlaghi Azar et al. "A general theoretical paradigm to understand learning from human preferences". In: arXiv preprint arXiv:2310.12036 (2023).

[2] Yuntao Bai et al. "Constitutional ai: Harmlessness from ai feedback". In: arXiv preprint arXiv:2212.08073 (2022).

[3] Yuntao Bai et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback". In: arXiv preprint arXiv:2204.05862 (2022).

[4] Ralph Allan Bradley and Milton E Terry. "Rank analysis of incomplete block designs: I. The method of paired comparisons". In: Biometrika 39.3/4 (1952), pp. 324-345.

[5] Tianchi Cai et al. "ULMA: Unified Language Model Alignment with Demonstration and Point-wise Human Preference". In: arXiv preprint arXiv:2312.02554 (2023).

[6] Stephen Casper et al. "Open problems and fundamental limitations of reinforcement learning from human feedback". In: arXiv preprint arXiv:2307.15217 (2023).

[7] Souradip Chakraborty et al. "MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences". In: arXiv preprint arXiv:2402.08925 (2024).

[8] Lili Chen et al. "Decision transformer: Reinforcement learning via sequence modeling". In: Advances in neural information processing systems 34 (2021), pp. 15084-15097.

[9] Zixiang Chen et al. "Self-play fine-tuning converts weak language models to strong language models". In: arXiv preprint arXiv:2401.01335 (2024).

[10] Paul F Christiano et al. "Deep reinforcement learning from human preferences". In: Advances in neural information processing systems 30 (2017).

[11] Nirjhar Das et al. "Provably Sample Efficient RLHF via Active Preference Optimization". In: arXiv preprint arXiv:2402.10500 (2024).

[12] Hanze Dong et al. "Raft: Reward ranked finetuning for generative foundation model alignment". In: arXiv preprint arXiv:2304.06767 (2023).

[13] Esin Durmus et al. "Towards measuring the representation of subjective global opinions in language models". In: arXiv preprint arXiv:2306.16388 (2023).

[14] Kawin Ethayarajh et al. "Kto: Model alignment as prospect theoretic optimization". In: arXiv preprint arXiv:2402.01306 (2024).

[15] Leo Gao, John Schulman, and Jacob Hilton. "Scaling laws for reward model overoptimization". In: International Conference on Machine Learning. PMLR. 2023, pp. 10835-10866.

[16] Caglar Gulcehre et al. "Reinforced self-training (rest) for language modeling". In: arXiv preprint arXiv:2308.08998 (2023).

[17] Jiwoo Hong, Noah Lee, and James Thorne. "Reference-free monolithic preference optimization with odds ratio". In: arXiv preprint arXiv:2403.07691 (2024).

[18] Jian Hu et al. "Aligning language models with offline reinforcement learning from human feedback". In: arXiv preprint arXiv:2308.12050 (2023).

[19] Shawn Im and Yixuan Li. "Understanding the Learning Dynamics of Alignment with Human Feedback". In: arXiv preprint arXiv:2403.18742 (2024).

[20] Kaixuan Ji, Jiafan He, and Quanquan Gu. "Reinforcement Learning from Human Feedback with Active Queries". In: arXiv preprint arXiv:2402.09401 (2024).

[21] Timo Kaufmann et al. "A survey of reinforcement learning from human feedback". In: arXiv preprint arXiv:2312.14925 (2023).

[22] Nathan Lambert, Thomas Krendl Gilbert, and Tom Zick. "Entangled preferences: The history and risks of reinforcement learning and human feedback". In: arXiv preprint arXiv:2310.13595 (2023).

[23] Harrison Lee et al. "Rlaif: Scaling reinforcement learning from human feedback with ai feedback". In: arXiv preprint arXiv:2309.00267 (2023).

[24] Ziniu Li, Tian Xu, and Yang Yu. "Policy Optimization in RLHF: The Impact of Out-of-preference Data". In: arXiv preprint arXiv:2312.10584 (2023).

[25] Ilya Loshchilov and Frank Hutter. "Decoupled Weight Decay Regularization". In: International Conference on Learning Representations (ICLR) (2018).

[26] Viraj Mehta et al. "Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration". In: (2023).

[27] RÃ©mi Munos et al. "Nash learning from human feedback". In: arXiv preprint arXiv:2312.00886 (2023).

[28] Reiichiro Nakano et al. "Webgpt: Browser-assisted question-answering with human feedback". In: arXiv preprint arXiv:2112.09332 (2021).

[29] Hongseok Namkoong and John C Duchi. "Stochastic gradient methods for distributionally robust optimization with f-divergences". In: Advances in neural information processing systems 29 (2016).

[30] Arkadi Nemirovski et al. "Robust stochastic approximation approach to stochastic programming". In: SIAM Journal on optimization 19.4 (2009), pp. 1574-1609.

[31] Yonatan Oren et al. "Distributionally robust language modeling". In: arXiv preprint arXiv:1909.02060 (2019).

[32] Long Ouyang et al. "Training language models to follow instructions with human feedback". In: Advances in Neural Information Processing Systems 35 (2022), pp. 27730-27744.

[33] Jing-Cheng Pang et al. "Language model self-improvement by reinforcement learning contemplation". In: arXiv preprint arXiv:2305.14483 (2023).

[34] Alec Radford et al. "Language models are unsupervised multitask learners". In: OpenAI blog 1.8 (2019), p. 9 .

[35] Rafael Rafailov et al. "Direct preference optimization: Your language model is secretly a reward model". In: arXiv preprint arXiv:2305.18290 (2023).

[36] Corby Rosset et al. "Direct nash optimization: Teaching language models to self-improve with general preferences". In: arXiv preprint arXiv:2404.03715 (2024).

[37] Shiori Sagawa et al. "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization". In: arXiv preprint arXiv:1911.08731 (2019).

[38] John Schulman et al. "Proximal policy optimization algorithms". In: arXiv preprint arXiv:1707.06347 (2017).

[39] Avi Singh et al. "Beyond human data: Scaling self-training for problem-solving with language models". In: arXiv preprint arXiv:2312.06585 (2023).

[40] Maurice Sion. "On general minimax theorems." In: (1958).

[41] Feifan Song et al. "Preference ranking optimization for human alignment". In: Proceedings of the AAAI Conference on Artificial Intelligence. 2024.

[42] Nisan Stiennon et al. "Learning to summarize with human feedback". In: Advances in Neural Information Processing Systems 33 (2020), pp. 3008-3021.

[43] Gokul Swamy et al. "A minimaximalist approach to reinforcement learning from human feedback". In: arXiv preprint arXiv:2401.04056 (2024).

[44] Yunhao Tang et al. "Generalized Preference Optimization: A Unified Approach to Offline Alignment". In: arXiv preprint arXiv:2402.05749 (2024).

[45] Gemma Team et al. "Gemma: Open models based on gemini research and technology". In: arXiv preprint arXiv:2403.08295 (2024).

[46] Haoxiang Wang et al. "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards". In: arXiv preprint arXiv:2402.18571 (2024).

[47] Tianhao Wu et al. "Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment". In: arXiv preprint arXiv:2310.00212 (2023).

[48] Yue Wu et al. "Self-Play Preference Optimization for Language Model Alignment". In: arXiv preprint arXiv:2405.00675 (2024).

[49] Sang Michael Xie et al. "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining". In: arXiv preprint arXiv:2305.10429 (2023).

[50] Wei Xiong et al. "Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint". In: ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2023.

[51] Rui Yang et al. "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment". In: arXiv preprint arXiv:2402.10207 (2024).

[52] Chenlu Ye et al. "A theoretical analysis of nash learning from human feedback under general klregularized preference". In: arXiv preprint arXiv:2402.07314 (2024).

[53] Weizhe Yuan et al. "Self-rewarding language models". In: arXiv preprint arXiv:2401.10020 (2024).

[54] Siyan Zhao, John Dang, and Aditya Grover. "Group Preference Optimization: Few-Shot Alignment of Large Language Models". In: arXiv preprint arXiv:2310.11523 (2023).

[55] Yao Zhao et al. "Slic-hf: Sequence likelihood calibration with human feedback". In: arXiv preprint arXiv:2305.10425 (2023).

[56] Han Zhong et al. "DPO Meets PPO: Reinforced Token Optimization for RLHF". In: arXiv preprint arXiv:2404.18922 (2024).

[57] Daniel M Ziegler et al. "Fine-tuning language models from human preferences". In: arXiv preprint arXiv:1909.08593 (2019).
