# LLM4CAUSAL: LARGE LANGUAGE MODEL FOR CAUSAL DECISION MAKING 

A PREPRINT

Haitao Jiang ${ }^{1}$, Lin $\mathbf{G e}^{1}$, Yuhe Gao ${ }^{1,+}$, Jianian Wang ${ }^{1,+}$, and Rui Song ${ }^{1,2, *}$<br>${ }^{1}$ North Carolina State University<br>${ }^{2}$ Amazon


#### Abstract

Large Language Models (LLMs) have shown their success in language understanding and reasoning on general topics. However, their capability to perform inference based on user-specified structured data and knowledge in corpus-rare concepts, such as causal decision-making is still limited. In this work, we explore the possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can identify the causal task, execute a corresponding function, and interpret its numerical results based on users' queries and the provided dataset. Meanwhile, we propose a data generation process for more controllable GPT prompting and present two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal problem identification and input parameter extraction for causal function calling and (2) Causal-Interpret-Bench for in-context causal interpretation. By conducting end-to-end evaluations and two ablation studies, we showed that LLM4Causal can deliver end-to-end solutions for causal problems and provide easy-to-understand answers, which significantly outperforms the baselines.


## 1 Introduction

Recently, Transformer-based LLMs containing billions of parameters are gaining popularity and are widely applied in fields such as education, legal services, and medicine (Kasneci et al., 2023; Chen et al. 2021). LLMs such as GPT-3 (Brown et al. 2020), GPT-4, and LLaMA (Touvron et al., 2023a) have shown impressive performance in multiple natural language processing tasks such as question-answering, common-sense reasoning, and translation (Zhao et al. 2023). These superior performances of LLMs have also motivated explorations on LLMs' applications on the causal decision-making (Song et al. 2023) procedures, which include various crucial tasks in real-life such as causal structure learning (CSL) tasks (Spirtes et al., 2000, Glymour et al., 2019), causal effect learning (CEL) tasks (Yao et al., 2021; Hicks and Tingley, 2011) and causal policy learning (CPL) tasks (Chakraborty and Murphy, 2014, Sutton and Barto, 2018; Zeng et al. 2023).

Most of recent works that applied LLM in causal decision-making tasks are focused on CSL, where LLM's internal knowledge gained during its training process is exploited in learning the causal relations among variables. For instance, both Kıcıman et al. (2023) and Long et al. (2023) propose to obtain the causal relationships between variables by directly querying GPT models using the variables names, assuming that LLMs can extract internal causal knowledge of those variables from the large corpus it was trained on. In a further development by Ban et al. (2023), LLM's internal causal knowledge are utilized to guide the optimization process of score-based methods as soft or hard constraints, enhancing the performance compared to using the traditional CSL methods alone. However, this line of works heavily rely on the quality of the knowledge in LLM's training corpus and lacks step-by-step reasoning or interpretation. A natural choice to overcome these limitations is to ask the LLM to complete the causal tasks utilizing existing causal decision making tools (such as CausalML(Chen et al. 2020), CausalDM (Song et al., 2023), causal-learn (Zheng et al., 2023) and input data uploaded by users. In literature, Code Interpreter (OpenAI, 2023a), ToolLLM (Qin et al., 2023b) and GPT4Tools (Yang et al. 2023) have leveraged the function calling feature of LLM in general data analysis tasks for user input datasets. However, directly applying those methods on causal related tasks and datasets may lead to several issues: first, since these models are not tailored on causal tasks, they frequently hallucinate and mislead the user with irrelevant context, such as providing correlation analysis results when causal effect analysis is expected, as shown in Figure 1a second, most of these methods fail to provide end-to-end result delivery (see Figure $1 \mathrm{~b}$ and
easy-to-understand interpretation for causal related tasks; third, these methods lack information about newly released approaches that are not included in LLM's training corpus (see Figure 1c).
![](https://cdn.mathpix.com/cropped/2024_06_04_65204153ded45986efecg-02.jpg?height=148&width=650&top_left_y=366&top_left_x=328)

Great! Let's start with the first step: computing the Pearson correlation coefficients among the variables $X, Y$, and $Z$. Would you like to proceed with any of these advanced causal inference methods, or is there anything else you'd like to explore?

(a) User queries on causal relationships.

![](https://cdn.mathpix.com/cropped/2024_06_04_65204153ded45986efecg-02.jpg?height=52&width=472&top_left_y=337&top_left_x=1298)

I apologize for the inconvenience, but it appears that the library required for LinGAM (`lingam`) is not installed in this environment. Unfortunately, without this library, I won't be able to run LinGAM directly.

(b) User queries on a classical method, LiNGAM(Shimizu. 2014).

![](https://cdn.mathpix.com/cropped/2024_06_04_65204153ded45986efecg-02.jpg?height=238&width=661&top_left_y=651&top_left_x=732)

(c) User queries on a newly-developed method, ANOCE (Cai et al. 2020).

Figure 1: User interaction with ChatGPT on causal reiated questions. ${ }^{1}$

To overcome these challenges, in this study, we develop an end-to-end user-friendly large language model with causal decision-making ability for general audiences. As illustrated in Figure 2, the proposed model consists of three major steps: user request interpretation, causal tools assignment and execution, and output interpretation. Upon receiving a user query and an uploaded data file, the initial step identifies the pertinent causal task and extracts query details-including dataset name, task type, and variable of interest, among others-into a structured JSON summary. The LLM4Causal model gains the ability to convert natural language user queries into JSON summaries by fine-tuning a pre-trained LLM on the Causal-Retrieval-Bench dataset. A well-designed data generation pipeline is proposed to ensure the quality of Causal-Retrieval-Bench, in terms of both data variety and accuracy. In the second step, the system automatically chooses the causal learning algorithm according to the task type detailed in the structured JSON data, executes the selected algorithm to analyze the dataset, and collects the algorithm's output. The outputs are then translated into easily understandable language in the third step, using the LLM4Causal model, which has been further fine-tuned with the Causal-Interpret-Bench dataset to generate high-quality interpretations.

![](https://cdn.mathpix.com/cropped/2024_06_04_65204153ded45986efecg-02.jpg?height=496&width=1287&top_left_y=1476&top_left_x=408)

Figure 2: A flowchart of the LLM4Causal consists of three major steps: user request interpretation, causal tools assignment and execution, and output interpretation.

Our main contributions could be summarized as follows:
- This paper is the first to construct an end-to-end user-friendly large language model (LLM4Causal) with causal decision-making ability. LLM4Causal could be easily used for general audiences, which addresses the weakness of the current LLM applications on causal tasks. It has the capability of i) interpreting user requests by causal task classification and information extraction, ii) assigning causal tools and executing the corresponding algorithm, and iii) providing an easy-to-understand interpretation of the algorithm output.
- We have proposed a three-step data generation pipeline that combines LLM text generation and human annotation to create fine-tuning datasets. With this well-designed pipeline, we have collected two high-quality[^0]benchmark datasets, Causal-Retrieval-Bench for causal function calling and Causal-Interpret-Bench for causal interpretation, with outstanding data variety and accuracy.
- The proposed LLM4Causal model was extensively evaluated in three major causal decision-making tasks: Causal effect estimation, Causal structure discovery, and Causal policy learning. The proposed method has shown superior performance compared to the benchmark methods.

## 2 Problem Formulation

In this paper, we aim to augment existing pre-trained LLMs with the proficiency to address causal inquiries, thereby opening the door to causal decision-making processes for general audiences. As introduced previously, causal decisionmaking tasks can be classified into three categories: 1) causal structure learning (CSL), 2) causal effect learning (CEL), and 3) causal policy learning (CPL). Each of these primary categories consists of multiple tasks, and our study focuses on five key tasks that are particularly relevant to the interests of our target audience. Let us denote the user query as $\mathcal{Q}$, the dataset to be analyzed as $\mathcal{D}$, the node set whose interrelationship is of interest as $\mathcal{X}$, the treatment variable as $\mathcal{A}$, the response variable as $\mathcal{Y}$, the mediator variables as $\mathcal{M}$, and the condition of the subpopulation of interest as $\mathcal{S}$. The five causal tasks are summarized in Table 1 .

| Task | Dataset <br> $(\mathcal{D})$ | Nodes <br> $(\mathcal{X})$ | Treadtfertt <br> $(\mathcal{A})$ | Crestrutet <br> $(\mathcal{y})$ | gnsedfiatof <br> $(\mathcal{M})$ | isalfoffikson <br> $(\mathcal{S})$ | Function Output Format |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| CGL | $\checkmark$ | $\checkmark$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_65204153ded45986efecg-03.jpg?height=57&width=141&top_left_y=978&top_left_x=763) | - |  |  | Causal Graph $(\mathcal{G})$ |
| HTE <br> HTL | $\checkmark$ |  | $\checkmark$ | $\checkmark$ |  | $\checkmark$ | Numeric Value <br> Numeric Value |
| MA | $\checkmark$ |  | $\checkmark$ | , | $\checkmark$ |  | Numeric Values |
| OPO | $\checkmark$ |  | $\checkmark$ | $\checkmark$ |  | $\checkmark$ | Treatment Level |

The CSL category includes Causal Graph Learning (CGL) task, which aims to identify causal relationships between variables. Given $\mathcal{Q}$ and $\mathcal{D}$, the goal of CGL is to learn and report a directed graph $\mathcal{G}$ that encapsulates the entire causal structure among variables in $\mathcal{X}$ specified in $\mathcal{Q}$. Average Treatment Effect Estimation (ATE) falls under the CEL category and aims to quantify the average effect size of an intervention across the entire population. Given $\mathcal{D}, \mathcal{A}$, and $\mathcal{Y}$ defined in $\mathcal{Q}$, the goal of ATE is to execute appropriate learners to measure the difference in counterfactual outcomes between the treated and control groups. Heterogeneous Treatment Effect Estimation (HTE), another task within CEL, extends ATE by assessing effect sizes under specific conditions $\mathcal{S}$, providing insights into the variability of treatment effects across different subpopulations. When additional mediators $\mathcal{M}$ are considered to transmit treatment effects to the response, Mediation Effect Analysis (MA) within CEL focuses on decomposing the total treatment effect into direct effects, which are solely due to $\mathcal{A}$, and indirect effects, mediated through additional variables $\mathcal{M}$. Lastly, Off-Policy Optimization (OPO), the only one we considered within the CPL category, served as a one-stop shop for decision-makers. Given $\mathcal{D}, \mathcal{A}, \mathcal{Y}$, and $\mathcal{S}$ explicitly specified in $\mathcal{Q}$, OPO aims to select suitable policy learners to determine the optimal action expected to maximize the outcome $\mathcal{Y}$.

To tackle the aforementioned diverse tasks using a single LLM, we introduce LLM4Causal. By fine-tuning pre-trained LLMs, LLM4Causal is capable of comprehending causal queries, applying appropriate causal tools to analyze the provided tabular dataset, and providing answers by interpreting numerical results in straightforward and fluent language. More technical details of LLM4Causal is discussed in Section 3 .

## 3 Proposed Method

In this paper, we introduce a novel three-stage framework to empower a pre-trained LLM to address causality-related tasks, as illustrated in Figure 2. Common approaches to calibrate an LLM checkpoint include Retrieval Augmented Generation (RAG), Prompt Engineering (PE), and Fine-Tuning (FT). However, while RAG (Chen et al., 2024; Gao et al. 2023) can introduce relevant external information into the response process, it cannot improve the model's intrinsic understanding of causality or result interpretation capability, rendering it infeasible. Although various PE strategies (Liu et al. 2023, Haviv et al. 2021) have been developed to enhance the performance of LLMs' responses, PE remains a complex and nuanced art that requires iterative and extensive experimentation to refine the prompts. In contrast, having a clear understanding of the expected performance outcomes makes preparing the 'golden' dataset relatively straightforward, thereby making the fine-tuning approach more direct and effective for our case. As a result, we choose to fine-tune an LLM on carefully crafted datasets. Specifically, LLM4Causal is carefully fine-tuned with our proposed benchmark dataset, Causal-Retrieval-Bench, enabling it to achieve better performance than other LLMs in classifying causality-related tasks and retrieving relevant information, such as variable names and values. Then, the framework selects appropriate causal analysis tools based on task classification and retrieved information to produce quantitative results. Finally, further fine-tuned on our proposed Causal-Interpret-Bench dataset, LLM4Causal
![](https://cdn.mathpix.com/cropped/2024_06_04_65204153ded45986efecg-04.jpg?height=412&width=1098&top_left_y=238&top_left_x=512)

Figure 3: Causal-Retrieval-Bench construction procedures for the first step. GPT prompts used in this section can be found in Appendix C.2.

is capable of translating the direct results from functional calls into clear and easily understandable natural language interpretations.

### 3.1 STEP1. Interpret User Request

The purpose of the first step is to translate user questions through two integrated substeps: i) causal task classification and ii) attribute extraction. The first sub-step categorizes the input question into one of the five supported causal tasks (CGL, ATE, HTE, MA, or OPO) by inferring the underlying user intention. Following this, attribute extraction, conditioned on the task classification, extracts attribute values that are necessary for the expert tools to address the causal problem. Instead of formulating it as a sequential process, we propose to merge them into one sequence-to-sequence (seq2seq) procedure where the output is a structured JSON with the "causal_problem" key and other task-specific keys, e.g. "dataset" $(\mathcal{D})$, "treatment" $(\mathcal{A})$, "outcome" $(\mathcal{Y})$. Some simulated input queries with the corresponding JSON output are shown in Appendix C. 2 Table 6 .

Even with careful prompt engineering, it is challenging to directly adopt a pre-trained LLM for such a seq2seq task, where the latest GPT4 checkpoint still has a $31 \%$ error rate (see Section 4.3 for details) and publicity available model such as Llama 2 failed on this JSON extraction task. As a result, we find it necessary to fine-tune a local LLM with an augmented dataset consisting of both input queries and output JSONs.

### 3.1.1 Construct Causal-Retrieval-Bench

To the best of our knowledge, the aforementioned customized input-output pairs can not be collected from the online corpus, since the commonly used LLM training data are mainly conversational materials other than structured data. Therefore, we choose to construct a customized corpus containing both synthetic causal questions and the corresponding JSON representations by prompting an instruction-following LLM.

To collect LLM-generated data, the common approach starts by writing several demonstrated examples. Those demonstrations are then fed to an LLM (e.g., GPT-4) for new sample generation. We have observed that such an approach suffers from (1) non-controllable topics and variables of interest, which results in content homogeneity, (2) incomplete/non-compliant JSON outputs, and (3) inappropriate paraphrase, where the causal question is transformed to be association-related. To resolve the pain points and prevent data contamination, we propose a three-course procedure, which is illustrated in Figure 3

The procedure can be summarized as an output-first strategy, where the major goal is to improve the correctness and the variety of the output JSON. An instruction prompt (See Appendix C.2 is then incorporated to construct causal-related questions in different sentence structures. Specifically, we first prepare a topic database by conducting value generation, then generate JSON data by random sampling, and finally paraphrase the question utilizing prompting.

- Value Generation: the goal of the first course is to get a candidate pool of the interested topic and variables. We prompt the GPT model to generate some potential topics and related variable names. For each variable, we perform a zero-shot classification by GPT to determine the variable type, i.e., whether it is discrete or continuous. All the collected information is maintained in a three-level hierarchy structure (topic -> variable -> type).
- JSON Generation: Based on the assumption that causality tasks often involve variables within the same topic, we randomly generate structured JSON data following the topic-variable-type structure. For example, for an average treatment effect estimation task, a topic name is firstly drawn from the stem of the tree as the data file name, and two variables underneath are then considered as candidates for the synthetic treatment and outcome.

For causal tasks where a specific value/condition of a group variable is needed, we sampled a random value, either continuous or discrete, based on the corresponding variable type.

- Question Generation: For each JSON output, we prompt the GPT-4 model to generate five related questions given the context. Utilizing a univariate prompt template, we transform each JSON into an informationaugmented prompt consisting of task-specific descriptions, expert-written demo examples, JSON information, and paraphrasing guidelines. More details can be found in Appendix C. 2

During development, the aforementioned three courses are executed sequentially with quality control on the correctness of generated entities/queries in between. Confused with other statistical concepts, such as correlation and association, the current GPT model has suboptimal performance on the causality-related task. Hence, domain experts are involved after each step to improve the data quality with human annotation. As a result, for the Causal-Retrieval-Bench, we collect 1,500 pairs, i.e., 300 for each causal task, of causal questions with the corresponding JSON outputs.

### 3.2 STEP2. Assign and Execute Causal Tools

In this stage, to address user queries, we first select suitable causal learning algorithms based on the task class determined in Step 1. Following this selection, the algorithm is executed automatically, using both the extracted information from Step 1 and the user-provided dataset as inputs, to get the estimated result. See Figure 5 in Appendix ?? for a graphical illustration. Depending on the causal task, the output result can be a causal graph $\mathcal{G}$, numerical values of estimated causal effect, or a treatment level $a \in \mathcal{A}$. LLM4Causal supports a variety of approaches for each causal task by leveraging well-known public causal packages, including econml(Oprescu et al., [n. d.]), causalml(Chen et al., 2020), causal-learn(Zheng et al., 2023), and CausalDM(Song et al., 2023). Additionally, we remark that LLM4Causal is designed with flexibility, allowing for the incorporation of new methodologies through simple integration of function scripts.

### 3.3 STEP3. Interpret the Numerical Output

As discussed in Section 3.2, Step 2 produces a numerical value of causal effect, a recommended action level, or an adjacency matrix indicating causal relations in a causal graph. Interpreting these outputs needs domain expertise and may not be easily understood by a general audience lacking a causal background. To enhance user-friendliness and reduce the learning curve of LLM4Causal, our final stage entails the interpretation of the function output using fluent natural language. The process starts with a templated interpretation that automatically converts the numerical output to a one-sentence summary (See Appendix C.4.1 for examples). The final human-readable interpretation is then produced through the utilization of the LLM4Causal, which is guided by a designed prompt that includes response guidelines, the templated interpretation, and other relevant information (i.e., selected tools, the original user query, etc.). The prompt instructs the LLM4Causal to avoid generating hallucinatory content, ensure the inclusion of all relevant information, and provide cohesive responses in the relevant context of the original question (See Appendix C.4.3 for details). Observing that even GPT4 produces problematic interpretations ( $25 \%$ hallucination; see the definition below), we further fine-tune the LLM4Causal for output interpretation. In the following, we outline the process used to generate the golden dataset Causal-Interpret-Bench used for model calibration. The approach for model fine-tuning is similar to that used in step 1 and is omitted here for brevity.

### 3.3.1 Build Causal-Interpret-Bench

Likewise to the procedures in Step 1, we first use the GPT4 model to generate a silver dataset of interpretations, employing the templates in Appendix C.4.1 and the prompts in Appendix C.4.3 The interpretation instruction (prompt) takes the original causal question and intermediate results from steps 1 and 2 as inputs. Among them, user queries and their corresponding causal task classifications from section 3.1.1 are directly reused, with causal tools paired using the methodology described in 3.2. As for the function outputs, the numerical values are randomly generated in formats specific to each task. To create the final golden dataset, we then have annotators manually revise 400 interpretations, adhering to the evaluation rubric outlined below:

- Hallucination: i) The interpretation incorrectly presents information, such as treating the response variable as the treatment variable and misinterpreting the direct effect in mediation analysis as the total effect; ii) it includes details that are not present in the provided context, such as commenting on the effectiveness of methods used; or iii) it incorrectly applies terms such as 'correlation' and 'association', which are inappropriate for describing causal relations.
- Incompleteness: The interpretation lacks one or more essential components, such as the data set, the method, the results, and / or all the variables involved.
- Non-fluency: i) The interpretation includes unexplained, meaningless variable names (e.g., 'chg_rate'); ii) it repeats the same content multiple times; or iii) it directly references or rephrases the guidelines in the prompt.

Although various metrics are developed to assess the factual consistency between output summaries/interpretations and the original context (Zhang et al. 2019), as well as the conciseness and readability of output summaries, such as the ROUGE score (Lin, 2004), they do not meet our specific needs. The interpretation we require is highly specialized, necessitating a clear distinction between causality and association, guaranteeing that no crucial information is missed, and ensuring it is comprehensible to general audiences, among other criteria. Furthermore, the current automatic evaluation based on sentence similarity metrics or GPT-based prompts fails to adequately incorporate human preferences, making them unsuitable for our application. Therefore, instead, we carefully defined the three aforementioned rubrics to guide human annotation.

## 4 Experiments

In this section, we conduct numerical experiments to investigate the performance of LLM4Causal on five causal decision-making tasks. For each causal task defined in Section 2 , we randomly sample topics and their related variables following the process described in Figure 3 that is different from the two aforementioned datasets. With the topics and variables in hand, we further generate 150 synthetic data files utilizing the methodology introduced in Section D.2. Based on these synthetic datasets, we evaluate end-to-end performance for the proposed LLM4Causal model in Section 4.2. Furthermore, by comparing with the partially-capable benchmark methods, i.e. GPT4, in each step, detailed ablation analyses are conducted to provide a comprehensive understanding of the model performance in Section 4.3 and 4.4

### 4.1 Benchmark models

The Large language models for evaluation are described as follows.

- GPT4 (OpenAI, 2023b): For end-to-end evaluation, we tested ChatGPT with a GPT4 backbone. GPT4 with the function call feature enabled are tested for ablation analysis. By creating a function with causal task description and input arguments requirements, we utilize the GPT 4 with the function calling feature to generate the output, which is then transformed into JSON format based on the predicted "causal_problem" key.
- LLM4Causal: A customized language model started from Llama-2 (7B) checkpoint (Touvron et al., 2023b) and further fine-tuned on the Causal-Retrieval-Bench and Causal-Interpret-Bench. To inspect if mixing two types of data will harm the performance of each other, we present three varieties where LLM4Causal-Mixed trained on both datasets, LLM4Causal-Retrieve trained on the retrieval bench, and LLM4Causal-Interpret augmented by the interpret bench.

We only have the GPT family as the benchmark as other closed-sourced LLM environments do not have code execution ability. Furthermore, existing works (Qin et al., 2023b, Yang et al. 2023) on tool calling do not support causal tasks or fully rely on prompting pre-trained models for output interpretations. Preliminary experiments (see Appendix D. 3 ) show that pre-trained open-sourced models, such as Llamma2-7B, fail to adequately handle prompts with our designed requirements, making the aforementioned function-calling methods incapable of finishing the designed tasks.

### 4.2 Main Result: End-to-End Evaluation

In this experiment, we evaluate the end-to-end model performance from receiving causal-related questions to providing interpretations that explain the causal results. With such a setup, there is no sole "golden label" existing for each question as the correct interpretation is not unique. To provide a comprehensive analysis, we provide the following evaluation metrics inspired by previous works (Yang et al. 2023; Qin et al. 2023b) from three aspects:

- Pass rate (Qin et al. 2023b) calculates the proportion of user requests that could be completed by the model. This metric measures the model's executability and could be calculated as Pass rate $=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\left\{\tau_{i}\right\}$, where $N$ is the number of samples, $\mathbb{I}\left\{\tau_{i}\right\}$ is an indicator function, which equals to 1 if the model could generate an output for the $i^{\text {th }}$ user request, and 0 otherwise.
- Relevance rate: it calculates the proportion of user requests that the model provides relevant content with the correct causal task. This metric measures the model's task identification ability as Relevance rate $=$ $\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\left\{\gamma_{i}\right\}$, where $\mathbb{I}\left\{\gamma_{i}\right\}$ is an indicator function, which equals to 1 if the model could generate an output that correctly identify the causal task for the $i^{\text {th }}$ user request, and 0 otherwise.
- Win rate: it calculates the proportion of user requests that the model has delivered an accurate result. It could be calculated as Win rate $=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\left\{\eta_{i}\right\}$, where $\mathbb{I}\left\{\eta_{i}\right\}$ is an indicator function which equals to 1 if the model output contains the desired true value for the $i^{\text {th }}$ user request, and 0 otherwise.

We evaluated the end-to-end causal-question answering performance for ChatGPT and LLM4Causal-Mixed in Table 2 The configuration of the ChatGPT can be found in Appendix D. 4 .

Table 2: End-to-end evaluation for LLM4Causal-Mixed

|  | ChatGPT |  |  |  |  | LLM4Causal-Mixed |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | CGL | ATE | HTE | $\mathrm{MA}$ | $\mathrm{OPO}$ | CGL | ATE | HTE | MA | OPO |
| Pass Rate | 0.17 | 0.77 | 0.73 | 0.27 | 0.87 | 1.00 | 0.93 | 0.83 | 0.86 | 0.83 |
| Relevance Rate | 0.10 | 0.60 | 0.43 | 0.20 | 0.43 | 1.00 | 0.93 | 0.83 | 0.80 | 0.83 |
| Win Rate | 0.00 | 0.37 | 0.07 | 0.10 | 0.07 | 0.90 | 0.90 | 0.80 | 0.70 | 0.73 |

LLM4Causal provides a high pass rate and relevance rate for all tasks, whereas GPT4 performs well regarding pass rate but shows dichotomized performance in Relevance Rate. The pass rate only requires the model to successfully execute its last code block without an error, regardless of relevance. It is important to highlight that passing cases for GPT4 include cases where GPT only printed the basic statistics of the data without in-depth analysis. For tasks having a high Pass Rate, such as HTE and OPO, we got $42 \%$ and $51 \%$ of them to be either superficial starter conversations or irrelevant outputs that confuse general audiences.

More importantly, LLM4Causal outperforms ChatGPT on Win Rate. On the one hand, we can find that ChatGPT with the GPT-4 backbone can only finish $37 \%$ of the ATE task and nearly failed all other scenarios. The main reason is the generated code either has syntax/factual errors or attempts to load a package that is not in the environment. On the other hand, LLM4Causal correctly answered $80.6 \%$ questions on average. The win rate roughly reflects the combined difficulty of three steps for each task. The MA and OPO tasks require more parameters inferred from the question, further boosting the task complexity and resulting in a relatively lower win rate.

By investigating the failed cases, we observe that some GPT4 responses to CGL tasks are still informative to users but fail due to a lack of code dependencies. To further analyze the root cause, we conducted two more ablation analyses on steps 1 and 3 to strip out the cases when code execution is the main roadblock. Meanwhile, we compare the LLM4Causal-Mixed to LLM4Causal-Retrieve/LLM4Causal-Interpret in the ablation analysis to explore whether training distinct models on each dataset could enhance performance.

### 4.3 Ablation Analysis 1: Causal Entity Extraction

In this section, we focus the model performance on Step 1, user request interpretation, comparing with the benchmark methods on the synthetic data of 150 causal-related questions. In detail, for each simulated user request, the model is asked to produce a JSON output following the output format listed in Table 6. The accuracy of each key value is then calculated by comparing the ground true label with the model outputs. It is worth mentioning that, we require an exact match for values of the causal task key and the dataset key since the causal task lists are provided in the training data and the dataset names are explicitly mentioned in the user request. For the remaining key values, we consider the value to be correctly retrieved if it is a subset of the model output (soft-match). We used the GPT4 with the function call feature enabled as a benchmark method, as most existing studies (Qin et al. 2023b, Zhang, 2023) treat its output as the golden label for local LLM development.

Table 3: Causal Entity Extraction Performance

|  | Causal Task | Dataset | Nodes | Treatment | Response | Mediator | Condition |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| GPT4-turbo | $0.69(0.02)$ | $\mathbf{0 . 9 9}(0.01)$ | $\mathbf{0 . 9 9}(0.02)$ | $0.60(0.02)$ | $0.60(0.02)$ | $0.61(0.04)$ | $0.98(0.01)$ |
| LLM4Causal-Retrieve | $0.96(0.00)$ | $\mathbf{0 . 9 9}(0.01)$ | $0.88(0.03)$ | $0.94(0.00)$ | $0.94(0.01)$ | $0.90(0.05)$ | $\mathbf{1 . 0 0}(0.01)$ |
| LLM4Causal-Mixed | $\mathbf{0 . 9 8}(0.00)$ | $\mathbf{1 . 0 0}(0.00)$ | $\mathbf{1 . 0 0}(0.00)$ | $\mathbf{0 . 9 6}(0.00)$ | $\mathbf{0 . 9 7}(0.00)$ | $\mathbf{1 . 0 0}(0.00)$ | $\mathbf{1 . 0 0}(0.00)$ |

As shown in the table 3, both LLM4Causal models can effectively solve step 1 and significantly outperform the GPT4-turbo. This is dominated by the accuracy of the causal task identification, which blocks the model from accurately finding the necessary entities to extract if a wrong task class is posited. Besides, a seemingly surprising fact is that the mixed version even outperforms the retrieve-only version on all tasks. Such a phenomenon may be due to the inclusion of the interpretation data, which also involves causal queries and their corresponding causal tasks with interpretations,
increasing the parameter weight during the fine-tuning process of the model between task-specific queries and the corresponding task classification.

### 4.4 Ablation Analysis 2: Interpreting Causal Function Output

Following metrics described in section 3.3.1 we evaluate the quality of model-generated interpretations. A double-blind experiment is conducted with domain experts judging if the interpretation generated by each language model has issues with hallucination, fluency, or incompleteness. The averaged error rate is reported in Table 4

Table 4: Causal Result Interpretation Performance, where hall. indicates hallucination, inco. stands for incompleteness, and fluent is for non-fluency. Lower is better.

| model | task | hall. | inco. | fluent | task | hall. | inco. | fluent |
| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| GPT4-turbo |  | 0.08 | $\mathbf{0 . 0 0}$ | 0.28 |  | $\mathbf{0 . 0 5}$ | $\mathbf{0 . 1 0}$ | 0.22 |
| LLM4Causal-Interpret | CGL | $\mathbf{0 . 0 5}$ | 0.03 | 0.28 | MA | 0.10 | 0.15 | 0.18 |
| LLM4Causal-Mixed |  | 0.23 | $\mathbf{0 . 0 0}$ | $\mathbf{0 . 2 0}$ |  | 0.19 | 0.27 | $\mathbf{0 . 1 6}$ |
| GPT4-turbo |  | 0.43 | 0.05 | 0.48 |  | $\mathbf{0 . 1 5}$ | $\mathbf{0 . 1 0}$ | 0.33 |
| LLM4Causal-Interpret | ATE | 0.38 | 0.10 | $\mathbf{0 . 2 0}$ | OPO | 0.26 | 0.13 | 0.18 |
| LLM4Causal-Mixed |  | $\mathbf{0 . 2 8}$ | $\mathbf{0 . 0 0}$ | 0.50 |  | 0.21 | 0.13 | $\mathbf{0 . 1 0}$ |
| GPT4-turbo |  | 0.08 | $\mathbf{0 . 0 0}$ | 0.28 |  | 0.19 | $\mathbf{0 . 0 6}$ | 0.36 |
| LLM4Causal-Interpret | HTE | $\mathbf{0 . 0 5}$ | 0.03 | 0.28 | All | $\mathbf{0 . 1 7}$ | 0.10 | $\mathbf{0 . 2 0}$ |
| LLM4Causal-Mixed |  | 0.23 | $\mathbf{0 . 0 0}$ | $\mathbf{0 . 2 0}$ |  | 0.20 | 0.08 | 0.28 |

The interpretation results showed that LLM4Causal Models are comparable to or superior to the GPT-4-turbo on both hallucination and fluency metrics. Compared with the initial pre-trained Llama2, the model fine-tuning procedure successfully instructs it to reduce hallucination and makes the interpretation crisp without including sentences repeating already-stated information. Besides, we expect our fine-tuned models to be improved, especially for the completeness metric, by involving more golden samples for fine-tuning.

While the direct result shows the interpret-only model outperforms the mixed version, the improvement in hallucination and fluency from gpt4 to LLM4Causal-Mixed still shows the promise of calibrating a single LLM4Causal model as a more economical solution.

## 5 Conclusion

In this paper, we have proposed LLM4Causal, the first end-to-end user-friendly large language model with causal decision-making ability. Without requiring in-depth domain knowledge, the proposed model could be easily used for the general audience, which remedies the weakness of the current LLM application on the causal tasks. Furthermore, we have proposed a novel data generation pipeline by utilizing GPT-based LLM models and human annotations to improve the data quality in terms of both data variety and accuracy.

From the numerical experiments, we have shown that calibrating a single LLM to accomplish such an end-to-end task is possible. Fine-tuned to interpret users' query and function outputs, LLM4Causal has shown superior performance in causal entity extraction and function result interpretation. Due to the page restriction, the limitation of this work and the future research possibilities are discussed in Appendix $\mathrm{E}$

## References

Taiyu Ban, Lyvzhou Chen, Xiangyu Wang, and Huanhuan Chen. 2023. From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data. arXiv preprint arXiv:2306.16902 (2023).

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.

Hengrui Cai, Rui Song, and Wenbin Lu. 2020. ANOCE: Analysis of Causal Effects with Multiple Mediators via Constrained Structural Learning. In International Conference on Learning Representations.

Bibhas Chakraborty and Susan A Murphy. 2014. Dynamic treatment regimes. Annual review of statistics and its application 1 (2014), 447-464.

Huigang Chen, Totte Harinen, Jeong-Yoon Lee, Mike Yung, and Zhenyu Zhao. 2020. Causalml: Python package for causal machine learning. arXiv preprint arXiv:2002.11631 (2020).

Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 17754-17762.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).

Leyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang. 2021. Template-based named entity recognition using BART. arXiv preprint arXiv:2106.01760 (2021).

Bhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W Cohen. 2022. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics 10 (2022), 257-273.

Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023).

Clark Glymour, Kun Zhang, and Peter Spirtes. 2019. Review of causal discovery methods based on graphical models. Frontiers in genetics 10 (2019), 524.

Shantanu Gupta, Cheng Zhang, and Agrin Hilmkil. 2023. Learned Causal Method Prediction. arXiv preprint arXiv:2311.03989 (2023).

Adi Haviv, Jonathan Berant, and Amir Globerson. 2021. BERTese: Learning to speak to BERT. arXiv preprint arXiv:2103.05327 (2021).

Raymond Hicks and Dustin Tingley. 2011. Causal mediation analysis. The Stata Journal 11, 4 (2011), 605-619.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning. PMLR, 2790-2799.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).

Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. 2023. ChatGPT for good? On opportunities and challenges of large language models for education. Learning and individual differences 103 (2023), 102274.

Emre Kıcıman, Robert Ness, Amit Sharma, and Chenhao Tan. 2023. Causal reasoning and large language models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050 (2023).

Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2021. Internet-augmented dialogue generation. arXiv preprint arXiv:2107.07566 (2021).

Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 (2021).

Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).

Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameterefficient fine-tuning. arXiv preprint arXiv:2303.15647 (2023).

Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. $74-81$.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023), 1-35.

Stephanie Long, Tibor Schuster, Alexandre Piché, ServiceNow Research, et al. 2023. Can large language models build causal graphs? arXiv preprint arXiv:2303.05279 (2023).

OpenAI. 2022. OpenAI: Introducing ChatGPT. https://openai.com/blog/chatgpt

OpenAI. 2023a. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins\#code-interpreter

OpenAI. 2023b. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]

Miruna Oprescu, Vasilis Syrgkanis, Keith Battocchi, Maggie Hei, and Greg Lewis. [n. d.]. EconML: A Machine Learning Library for Estimating Heterogeneous Treatment Effects. In 33rd Conference on Neural Information Processing Systems. 6.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191 (2021).

Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023a. Tool learning with foundation models. arXiv preprint arXiv:2304.08354 (2023).

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023b. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789 (2023).

Dheeraj Rajagopal, Vivek Khetan, Bogdan Sacaleanu, Anatole Gershman, Andrew Fano, and Eduard Hovy. 2021. Cross-domain reasoning via template filling. arXiv preprint arXiv:2111.00539 (2021).

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 (2023).

Timo Schick and Hinrich Schütze. 2020. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676 (2020).

Shohei Shimizu. 2014. LiNGAM: Non-Gaussian methods for estimating causal structures. Behaviormetrika 41, 1 (2014), 65-98.

Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, and Antti Kerminen. 2006. A linear non-Gaussian acyclic model for causal discovery. Journal of Machine Learning Research 7, Oct (2006), 2003-2030.

Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 (2020).

Rui Song, Hengrui Cai, Yang Xu, Runzhe Wan, and Lin Ge. 2023. CausalDM. https://causaldm.github.io/ Causal-Decision-Making/

Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. 2000. Causation, prediction, and search. MIT press.

Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824-24837.

Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2023. Gpt4tools: Teaching large language model to use tools via self-instruction. arXiv preprint arXiv:2305.18752 (2023).

Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, and Aidong Zhang. 2021. A survey on causal inference. ACM Transactions on Knowledge Discovery from Data (TKDD) 15, 5 (2021), 1-46.

Yue Yu, Jie Chen, Tian Gao, and Mo Yu. 2019. DAG-GNN: DAG Structure Learning with Graph Neural Networks. arXiv preprint arXiv:1904.10098 (2019).

Yan Zeng, Ruichu Cai, Fuchun Sun, Libo Huang, and Zhifeng Hao. 2023. A Survey on Causal Reinforcement Learning. arXiv:2302.05209 [cs.AI]

Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel Jennings, Chao Ma, Tom Minka, Nick Pawlowski, et al. 2023. Understanding causality with large language models: Feasibility and opportunities. arXiv preprint arXiv:2304.05524 (2023).

Jiawei Zhang. 2023. Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. arXiv preprint arXiv:2304.11116 (2023).

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019).

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).

Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. 2018. DAGs with no tears: Continuous optimization for structure learning. In Advances in Neural Information Processing Systems. 9472-9483.

Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu, Peter Spirtes, and Kun Zhang. 2023. Causal-learn: Causal discovery in python. arXiv preprint arXiv:2307.16405 (2023).
