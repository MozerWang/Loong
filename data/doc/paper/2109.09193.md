# Towards Zero-Label Language Learning 

Zirui Wang Adams Wei Yu Orhan Firat Yuan Cao<br>Google AI<br>\{ziruiw, adamsyuwei,orhanf,yuancao\}@google.com


#### Abstract

This paper explores zero-label learning in Natural Language Processing (NLP), whereby no human-annotated data is used anywhere during training and models are trained purely on synthetic data. At the core of our framework is a novel approach for better leveraging the powerful pretrained language models. Specifically, inspired by the recent success of fewshot inference on GPT-3, we present a training data creation procedure named Unsupervised Data Generation (UDG), which leverages fewshot prompts to synthesize high-quality training data without real human annotations. Our method enables zero-label learning as we train task-specific models solely on the synthetic data, yet we achieve better or comparable results from strong baseline models trained on human-labeled data. Furthermore, when mixed with labeled data, our approach serves as a highly effective data augmentation procedure, achieving new state-of-the-art results on the SuperGLUE benchmark ${ }^{1}$.


## 1 Introduction

It is well-known that deep learning models are datahungry. In natural language processing, language model pre-training has become a successful transfer learning approach to effectively reduce the requirement for task-specific labeled data (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019; Radford et al., 2019; Raffel et al., 2019; Brown et al., 2020). Via training on unsupervised large-scale text corpus, bi-directional language models such as BERT and XLNet are able to learn contextualized text representations that can then be fine-tuned on downstream tasks with small training data sizes, which have pushed the state of the art on a variety of natural language understanding benchmarks.

More recently, gigantic language models (GLM) such as GPT3 (Brown et al., 2020) have been[^0]

| Model | Setting | SuperGLUE Avg. |
| :--- | :---: | :---: |
| Human |  | 89.8 |
| Previous SOTA | Supervised | 89.3 |
| T5+UDG |  | $\mathbf{9 0 . 4}$ |
| GPT3 | Few-Shot | 71.8 |
| UDG | Unsupervised | $\mathbf{7 8 . 1}$ |

Table 1: SuperGLUE summary.

shown to be effective few-shot learners. As unsupervised training corpus and model size scaling up, the model is able to generate answers for an unseen NLP task with few-shot inference, based on a manually crafted input prompt consist of a task description and a few examples. Despite no fine-tuning is involved, the language model performs competitively against fine-tuned baselines on a wide range of tasks, whose success suggests a new paradigm of transfer learning in NLP. Yet the gaps between few-shot inference and state-of-theart fine-tuned methods are still large on many tasks (for example 17.5 below prior state-of-the-art on SuperGLUE as shown in Table 1), urging for exploration of applications of giant language models beyond few-shot inference.

Inspired by the few-shot capability of GPT3, we shift our focus towards utilizing GLMs for example creation instead of direct inference, and find that language models are also excellent few-shot generators. Similar to the few-shot inference paradigm, we query the model with a prompt with a few examples and a description of the desired label, and the model generates examples aligned with the label while resembling the given samples. Interestingly, we find no supervision is required for high-quality data creation and thus we only need to use unlabeled examples in our prompts. The dataset created by the model can then used to fine-tune any off-the-shelf model. This approach can therefore be treated as a zero-label learning procedure, in which no human label is required throughout the whole process. It differs from the unsupervised

![](https://cdn.mathpix.com/cropped/2024_06_04_cd79de019a129f886af1g-02.jpg?height=524&width=1489&top_left_y=272&top_left_x=269)

Figure 1: Illustration of the UDG framework.

learning procedure in that the downstream models still need to be trained with synthetic data, however the training example creation requires no human labor.

Following this procedure, we are able to establish a system trained using unlabeled training data only, and thus we refer to it as Unsupervised Data Generation (UDG). Experiments show that our unsupervised system performs competitively with strong supervised baselines and achieves new stateof-the-art few-shot learning results on text classification and the SuperGLUE language understanding benchmarks. The synthesized data can further be used for data augmentation purpose. When combined with existing labeled data we are able to achieve the first super-human SuperGLUE scores. These results suggest that few-shot training data creation is a promising alternative to few-shot inference with powerful language models.

## 2 Related Work

Data augmentation has traditionally been a popular technique for NLP model quality improvement, especially in low-resource regimes (Yu et al., 2018; Wei and Zou, 2019) While traditionally simple heuristics like token-level modification has been applied to diversify training samples, more recently generative data augmentation has gained popularity due to the progress made in language modeling (Anaby-Tavor et al., 2019; Papanikolaou and Pierleoni, 2020; Juuti et al., 2020; Lee et al., 2021; Kumar et al., 2021). However, they often require labeled examples to finetune generative models and heavy postprocessing for data cleaning. On the other hand, our method generates data in a fully unsupervised manner without finetuning the lan- guage model, showcasing a new zero-label learning paradigm.

Our approach is also closely related to knowledge retrieval from large language models. These models are known to be good at memorizing facts from training data and capable of performing as open knowledge bases (Petroni et al., 2019; Wang et al., 2020; Roberts et al., 2020; Carlini et al., 2021). The high quality of training examples created by our approach is to a large part guaranteed by the model's strong knowledge retrieval ability, which reduces the chance of erratic hallucinations irrelevant to the provided labels.

## 3 Method

### 3.1 Background: Few-shot Inference

Given a set of labeled data $\mathcal{L}=\left\{\left(x^{i}, y^{i}\right)\right\}_{i=1}^{n}$ for a specific downstream task, the most common approach in recent years has been fine-tuning that updates the weights of a pre-trained model according to $\mathcal{L}$ (Devlin et al., 2018; Yang et al., 2019; Raffel et al., 2019). While obtaining state-of-theart performance on a wide range of tasks, finetuning requires extra update steps and non-trivial amounts of labeled data in the target task. On the other hand, few-shot inference is a more resourceefficient paradigm exhibited in the latest gigantic language models such as GPT3 (Radford et al., 2019; Brown et al., 2020). The idea is to utilize the language model to infer the correct label based on the task description and a few sample inputlabel pairs. In particular, the input to the model $M$ is a handcrafted ordered prompt consisted of a task description $T$, a small set of $\mathrm{K}$ examples $\mathcal{L}_{\text {few }}=\left\{\left(x^{i}, y^{i}\right)\right\}_{i=1}^{K} \subseteq \mathcal{L}$, and the query example

|  |  | IMDb | Yelp-2 | Yelp-5 | Amazon-2 | Amazon-5 | DBpedia | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| XLNet | Supervised | $\underline{96.80}$ | 98.63 | $\underline{72.95}$ | $\underline{97.89}$ | $\underline{68.33}$ | $\underline{99.40}$ | $\underline{89.00}$ |
| BERT $_{\text {LARGE }}$ |  | $\overline{95.49}$ | $\overline{98.11}$ | $\overline{70.68}$ | $\overline{97.37}$ | 65.83 | $\overline{99.36}$ | $\overline{87.81}$ |
| UDA | Few-Shot | 95.80 | 97.95 | 67.92 | 96.50 | 62.88 | 98.91 | $\overline{86.66}$ |
| Few-shot Inf. |  | 90.38 | 88.79 | 48.75 | 92.63 | 44.21 | 82.46 | 74.54 |
| UDG | Unsupervised | 95.95 | 98.22 | 69.05 | 97.02 | 64.54 | 96.47 | 86.88 |
| + NLA |  | 96.29 | 98.38 | 69.31 | 97.24 | 64.88 | 99.21 | 87.55 |

Table 2: Comparison of methods on text classification datasets (Accuracy). Results for XLNet are obtained

![](https://cdn.mathpix.com/cropped/2024_06_04_cd79de019a129f886af1g-03.jpg?height=46&width=1594&top_left_y=685&top_left_x=231)
semi-supervised/few-shot setup is bolded while underline signifies the overall best.

$x_{q}$, and the model is expected to infer the correct label $y_{q}$ as the most probable next text sequence to the input prompt:

$$
\begin{equation*}
y_{q}=\underset{y}{\operatorname{argmax}} P_{M}\left(y \mid\left[T, \mathcal{L}_{\text {few }}, x_{q}\right]\right) \tag{1}
\end{equation*}
$$

Since taking the argmax is intractable, $y_{q}$ is usually obtained through greedy decoding or beam search. Using much less task-specific data and no gradient update, few-shot inference can obtain performance comparable to fine-tuning methods (e.g. GPT3 performs similarly to fine-tuned BERT on SuperGLUE in Table 4). In its extreme format, giant language models can also perform one-shot $(\mathrm{K}=1)$ or even zero-shot $(\mathrm{K}=0)$ inference.

### 3.2 Unsupervised Data Generation

Despite these interesting findings, few-shot inference using giant language models still underperforms state-of-the-art fine-tuned models on many tasks. In Table 4, for instance, T5 largely outperforms GPT3 (89.3 vs 71.8) despite being much smaller in model sizes (11B vs 175B). One potential limitation is that a language model is never explicitly trained to directly conduct inference. Instead, it is trained as a text generator on unsupervised web corpus where inputs $(X)$ and labels $(Y)$ happen to coexist. Consequently, the few-shot inference method finds the proper prompt that 'forces' the model to generate next text sequence $X_{\text {next }}$ which happens to be the label Y. However, this could be suboptimal since the labels often emerge prior to the inputs in real-world web documents. For example, in sentiment classification of IMDb movie reviews (Maas et al., 2011), the actual review contexts appear after their corresponding rating scores. Therefore, few-shot inference can force the language model to generate on text distributions that are inconsistent with its training data.
To this end, we propose to utilize language models to perform few-shot generation. Instead of generating and predicting the label $\mathrm{Y}$, we let the model to generate the input $\mathrm{X}$ instead, decoupling generation from prediction. We aim to formulate the input prompts that are more likely to naturally exist in the training corpus. Specifically, the model is queried to generate $x_{g}$ corresponding to a pseudo label $\hat{y}_{g}$ with a prompt consisted of a small set of $\mathrm{K}$ unlabeled examples $\mathcal{U}=\left\{x^{i}\right\}_{i=1}^{K}$ and a description of the desired label:

$$
\begin{equation*}
x_{g} \sim P_{M}\left(x \|\left[T, \mathcal{U}, \operatorname{Des}\left(\hat{y}_{g}\right)\right]\right) \tag{2}
\end{equation*}
$$

where $\operatorname{Des}(\cdot)$ is a task-specific transformation function that maps a label class to natural language descriptions, as illustrated in Figure 1. Different from few-shot inference, our method only requires unsupervised few-shot examples, a zero-label learning setting. In addition, we use top-k sampling instead of search-based decoding to sample text from the language model. This allows us to generate a synthetic labeled dataset $\mathcal{L}_{\text {syn }}=\left\{\left(x_{g}^{i}, \hat{y}_{g}^{i}\right)\right\}_{i=1}^{n_{s}}$ with controllable size $n_{s}$. We then train task-specific models utilizing this synthetic dataset, either as standalone training data or additional auxiliary data. Unlike existing synthetic data generation systems, our method requires no fine-tuning step of the generative model and uses unsupervised data only, and therefore we refer to it as Unsupervised Data Generation to emphasize its resource efficiency. We also hope to emphasize that it is not our intention to leverage the language model to perform generative tasks, but just to take advantage of it to synthesize "labeled" examples for downstream model training.

## 4 Experiments

### 4.1 Unsupervised Text Classification

We first apply the proposed UDG method on standard text classification tasks.

|  | $\mathrm{K}=0$ | $\mathrm{~K}=1$ | $\mathrm{~K}=4$ | $\mathrm{~K}=32$ |
| :---: | :---: | :---: | :---: | :---: |
| IMDb Acc. | 64.21 | 91.34 | 95.86 | 96.29 |
| Yelp-2 Acc. | 67.34 | 90.27 | 98.22 | 98.38 |
| Amz-5 Acc. | 47.35 | 58.79 | 62.14 | 64.88 |

Table 3: Ablation of number of examples in each prompt.

Experimental Setups. We use six popular text classification benchmark datasets (Maas et al., 2011; Zhang et al., 2015), including IMDb, Yelp-2, Yelp-5, Amazon-2 and Amazon-5 sentiment classification and DBPedia topic classification. We mainly follow the experimental settings in Xie et al. (2019) and use the corresponding unlabeled data for each task. We apply similar preprocessing steps to clean noisy web texts and truncate the input to 512 subword tokens. For each prompt, we sample $K=32$ unlabeled examples from the unlabeled data and fit as many examples as allowed by the length of the language model's context window (detailed templates shown in Figure 1 and Appendix C). This process is then repeated $n_{c}=\frac{n_{s}}{\# \text { Class }}$ times for each label class, where we set $n_{c}=10 \mathrm{~K}$ for sentiment classification tasks and 1000 for topic classification. We then utilize the language model to generate one example for each prompt, resulting in a synthetic labeled dataset of size $n_{s}$. We use an in-house language model, which is a variant of the one in (Adiwardana et al., 2020) but trained with larger data. We exploit top-k sampling with $\mathrm{K}=40$ and temperature $=1.0$, and only apply basic post-processing to filter generated examples that are too short/long.

Once we obtain the generated synthetic dataset $\mathcal{L}_{\text {syn }}$, it can be utilized as labeled training data for any task-specific training framework. Here, we choose the state-of-the-art semi-supervised learning framework Unsupervised Data Augmentation (UDA) (Xie et al., 2019) as the backbone. We use BERT $_{\text {Large }}$ as our base model and follow the training protocol as described in the UDA paper to tune our hyper-parameters. In our experiment, we find some generated examples are noisy adn thus we additionally implement a Noisy Label Annealing (NLA) technique to filter these examples during the training process (See Appendix A for details).

Results. We compare models of trained using fully supervised, semi-supervised/few-shot and unsupervised settings in Table 2. We first compare few-shot inference using our giant language model with fine-

![](https://cdn.mathpix.com/cropped/2024_06_04_cd79de019a129f886af1g-04.jpg?height=377&width=462&top_left_y=231&top_left_x=1208)

Figure 2: Ablation of number of examples generated per label class.

tuned methods. Despite requiring no additional training costs, the few-shot inference paradigm performs significantly worse than supervised or even semi-supervised UDA, which utilizes similar amounts of labeled data. The gap is more evident on multi-way classification tasks such as Yelp-5 or DBpedia, where the model is required to predict complex labels beyond simple answers such as 'True/False'. In contrast, the proposed few-shot generation paradigm obtains strong performance while using less supervision. When combined with NLA, our UDG framework consistently outperforms UDA and few-shot inference on all six tasks, achieving new state-of-the-art few-shot learning results. Besides, without using any label, our

![](https://cdn.mathpix.com/cropped/2024_06_04_cd79de019a129f886af1g-04.jpg?height=57&width=774&top_left_y=1485&top_left_x=1052)
on IMDb and Yelp-2 and is also competitive on other tasks. Since both UDA and our method rely

![](https://cdn.mathpix.com/cropped/2024_06_04_cd79de019a129f886af1g-04.jpg?height=54&width=782&top_left_y=1629&top_left_x=1048)
ther boost our unsupervised performance, which we choose to leave for future work.

Analysis. We first examine the effect of data noisiness on model performance. As is the case for other data augmentation methods, few-shot generation using giant language models can produce examples that are inaccurate to the desired labels. To reduce the negative impact of these noisy labels, we utilize a simple NLA technique to filter out examples when the task-specific models disagree with the synthetic labels with high confidence levels. As shown in Table 2, NLA robustly improves UDG performance on all tasks, especially ones that are sensitive to noise such as DBpedia.

A crucial difference distinguishing our work from existing data generation methods is that we directly query the pretrained language model without any fine-tuning nor supervision. To achieve this, the model needs to not only infer correct knowledge corresponding to the input pseudo label but also generate text with similar styles of the sam-

|  |  | BoolQ | CB | COPA | MultiRC | ReCoRD | RTE | $\mathbf{W i C}$ | WSC | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Human |  | 89.0 | 95.8/98.9 | 100.0 | $81.8 / 51.9$ | $91.7 / 91.3$ | 93.6 | 80.0 | 100.0 | 89.8 |
| BERT++ ${ }^{\mathrm{a}}$ |  | 79.0 | $84.8 / 90.4$ | 73.8 | $70.0 / 24.1$ | $72.0 / 71.3$ | 71.7 | 69.6 | 64.4 | 71.5 |
| RoBERTa $^{\mathrm{b}}$ |  | 87.1 | $90.5 / 95.2$ | 90.6 | $84.4 / 52.5$ | $90.6 / 90.0$ | 88.2 | 69.9 | 89.0 | 84.6 |
| $\mathrm{T} 5^{\mathrm{c}}$ | Sup. | 91.2 | $93.9 / 96.8$ | 94.8 | $88.1 / 63.3$ | $94.1 / 93.4$ | 92.5 | 76.9 | 93.8 | 89.3 |
| DeBERTa $^{\mathrm{d}}$ |  | 90.4 | $94.9 / 97.2$ | 96.8 | 88.2/63.7 | $94.5 / 94.1$ | 93.2 | 76.4 | 95.9 | 89.9 |
| $\mathrm{T} 5+\mathrm{UDG}$ |  | 91.4 | $95.8 / 97.6$ | 98.0 | $88.3 / 63.0$ | $94.2 / 93.5$ | 93.0 | 77.9 | 96.6 | 90.4 |
| GPT3 $^{\mathrm{e}}$ |  | ![](https://cdn.mathpix.com/cropped/2024_06_04_cd79de019a129f886af1g-05.jpg?height=45&width=118&top_left_y=573&top_left_x=616) | $52.0 / 75.6$ | $\underline{92.0}$ | $75.4 / 30.5$ | $91.1 / 90.2$ | 69.0 | 49.4 | 80.1 | 71.8 |
| $\mathrm{iPET}^{\mathrm{f}}$ | Few-Shot | $\underline{81.2}$ | $79.9 / 88.8$ | $\overline{90.8}$ | $74.1 / 31.7$ | $\overline{85.9 / 85.4}$ | 70.8 | 49.3 | $\underline{88.4}$ | 75.4 |
| ADAPET $^{g}$ |  | $\overline{80.0}$ | $82.3 / 92.0$ | 85.4 | $76.2 / 35.7$ | $86.1 / 85.5$ | 75.0 | 53.5 | $\overline{85.6}$ | 76.0 |
| UDG | Unsup. | 81.0 | $\underline{86.2 / 92.4}$ | 80.4 | $\underline{81.1 / 47.1}$ | 82.8/81.8 | $\underline{80.7}$ | 67.5 | 79.5 | $\underline{78.1}$ |

Table 4: Comparison of single-model methods on SuperGLUE test scores. Results obtained from the official SuperGLUE leaderboard ${ }^{2}$. The best result for semi-supervised/few-shot setup is underlined while bold signifies the overall best. Model references: ${ }^{a}$ Devlin et al. (2018) ${ }^{\mathrm{b}}$ Liu et al. (2019) ${ }^{\mathrm{c}}$ Raffel et al. (2019) ${ }^{\mathrm{d}}$ Devlin et al. (2018) ${ }^{\mathrm{e}}$ Brown et al. (2020) ${ }^{\mathrm{f}}$ Schick and Schütze (2020) ${ }^{\mathrm{g}}$ Tam et al. (2021)

ple unsupervised examples. Thus, we compare the results when the language model uses different amounts of in-context examples in Table 3. The model fails to generate high-quality data when no sample is given, indicating the importance of fewshot generation. On the other hand, including more unsupervised examples does improve the quality of synthetic dataset which leads to better performance.

Finally, we evaluate the impact of the synthetic data sizes in Figure 2. Despite there is a diminishing return trend, we find the final performance to continuously improve with more generated data, showing that the language model can generate diverse examples. In addition, one key benefit of our method is that we can sample as much data as needed with no additional cost or supervision. This is particularly useful for tasks from low-resource domains with limited unsupervised data available.

### 4.2 Unsupervised Language Understanding

To evaluate the proposed framework in a more challenging and comprehensive setting, we extend it to perform on complex language understanding tasks. Experimental Setups. We use the SuperGLUE benchmark (Wang et al., 2019) for general-purpose language understanding in English, which consists of 8 natural language understanding tasks. Tasks cover textual entailment (CB and RTE), question answering (BoolQ, MultiRC and ReCoRD), common sense reasoning (COPA), word sense disambiguation (WiC), and coreference resolution (WSC). We mainly follow the same generation protocol as described in the previous sections, with some minor changes in prompt templates and data post-processing steps for specific tasks. As before, we use $\mathrm{K}=32$ unlabeled examples and generate using the same language model. For each task, we use all original labeled data as unsupervised examples for training data creation.

For the downstream model, we use T5 (Raffel et al., 2019) for fine-tuning on the created data. Different from the released T5 checkpoints that are pretrained on multi-task data, we pretrain our own models on unsupervised Colossal Clean Crawled Corpus (C4) data only and thus the combined framework remains unsupervised. For fair comparison with existing models, we pretrain and then fine-tune a T5-Large model using the created data set. Following Raffel et al. (2019), we use a finetuning batch size of 8 with 512 sequence length.

Results. We compare models trained under different settings in Table 4. The GPT3 model (Brown et al., 2020) using the few-shot inference method outperform BERT++ with less supervision and no fine-tuning. However, despite containing much more model parameters, it performs worse than other fine-tuned fully supervised models and fewshot methods. On the other hand, our unsupervised framework using few-shot generation outperforms all few-shot learning systems without using any label, and thus it achieves new state-of-the-art results on this benchmark for methods that exploit littleto-no supervision. In particular, our performance gains largely come from natural language entailment tasks (CB and RTE) as well as word sense disambiguation, where GPT3 performs similarly to random guessing. This indicates that language[^1]models do contain language knowledge that fewshot inference fails to leverage.

### 4.3 UDG as Data Augmentation

In previous sections we only use the created examples as pseudo supervision to explore the limits of transfer learning using language models. Nonetheless, the synthetic data can be also treated as augmented data and combined with existing labeled data. To this end, we fine-tune the public T5-XXL checkpoint using both labeled data and generated data. As shown in Table 4, our method combines well with existing labeled data and brings substantial improvements. This is particularly the case for tasks with small data sizes such as COPA and WSC. Moreover, the combined model outperforms not only prior methods but also the human baselines for the first time on this important NLP benchmark, setting a new milestone for natural language understanding with machine learning models.

## 5 Conclusion

In this paper, we propose a "zero-label" training procedure and show that language models are also few-shot example creators in that they can be used to generate high-quality synthetic data in a fully unsupervised manner. Through this, we demonstrate that NLP models can obtain strong results without any human annotated label. Our work illustrate a promising direction for future transfer learning research in NLP.

## References

Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977.

Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov, Naama Tepper, and Naama Zwerdling. 2019. Not enough data? deep learning to the rescue!

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.

Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Mika Juuti, Tommi Gröndahl, Adrian Flanagan, and N. Asokan. 2020. A little goes a long way: Improving toxic language classification despite data scarcity. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 29913009, Online. Association for Computational Linguistics.

Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2021. Data augmentation using pre-trained transformer models.

Kenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and Hyung Won Chung. 2021. Neural data augmentation via example extrapolation.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019 Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142-150.

Yannis Papanikolaou and Andrea Pierleoni. 2020. Dare: Data augmented relation extraction with gpt2.

Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2019. Language models as knowledge bases?

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.

Adam Roberts, Colin Raffel, and Noam Shazeer. 2020 How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online. Association for Computational Linguistics.

Timo Schick and Hinrich Schütze. 2020. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.

Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training. arXiv preprint arXiv:2103.11955.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537.

Chenguang Wang, Xiao Liu, and Dawn Song. 2020. Language models are open knowledge graphs.

Jason W. Wei and Kai Zou. 2019. EDA: easy data augmentation techniques for boosting performance on text classification tasks. In EMNLP-IJCNLP.

Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. 2019. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. In ICLR.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. arXiv preprint arXiv:1509.01626.

| None | $0.9 \rightarrow 0.8$ | $0.9 \rightarrow 0.7$ | $0.9 \rightarrow 0.6$ | $0.9 \rightarrow 0.5$ |
| :---: | :---: | :---: | :---: | :---: |
| 95.95 | 96.03 | 96.08 | 96.17 | 96.29 |

Table 5: Comparison of different annealing thresholds on IMBd classification. We observe performance improves as we filter more aggresively.
