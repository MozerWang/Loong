# Textbooks Are All You Need 

Suriya Gunasekar Yi Zhang Jyoti Aneja Caio César Teodoro Mendes<br>Allie Del Giorno Sivakanth Gopi Mojan Javaheripi Piero Kauffmann<br>Gustavo de Rosa Olli Saarikivi Adil Salim Shital Shah Harkirat Singh Behl<br>Xin Wang Sébastien Bubeck Ronen Eldan Adam Tauman Kalai Yin Tat Lee<br>Yuanzhi Li

Microsoft Research


#### Abstract

We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of "textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy $50.6 \%$ on HumanEval and $55.5 \%$ on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with $350 \mathrm{M}$ parameters trained with the same pipeline as phi-1 that still achieves $45 \%$ on HumanEval.


## 1 Introduction

The art of training large artificial neural networks has made extraordinary progress in the last decade, especially after the discovery of the Transformer architecture $\mathrm{VSP}^{+} 17$, yet the science behind this success remains limited. Amidst a vast and confusing array of results, a semblance of order emerged around the same time as Transformers were introduced, namely that performance improves somewhat predictably as one scales up either the amount of compute or the size of the network HNA ${ }^{+}$, a phenomenon which is now referred to as scaling laws $\mathrm{KMH}^{+} 20$. The subsequent exploration of scale in deep learning was guided by these scaling laws [BMR ${ }^{+}$], and discoveries of variants of these laws led to rapid jump in performances \$\mathrm{HBM}^{+}\$22. In this work, following the footsteps of Eldan and Li EL23, we explore the improvement that can be obtained along a different axis: the quality of the data. It has long been known that higher quality data leads to better results, e.g., data cleaning is an important part of modern dataset creation [RSR \${ }^{+}\$20, and it can yield other side benefits such as somewhat smaller datasets $\mathrm{LYR}^{+} 23$, \$\mathrm{YGK}^{+}\$23] or

![](https://cdn.mathpix.com/cropped/2024_06_04_622cbf2f20b4a37d6579g-01.jpg?height=51&width=1735&top_left_y=1937&top_left_x=192)
quality dataset synthetically generated to teach English to neural networks) showed that in fact the effect of high quality data extends well past this: improving data quality can dramatically change the shape of the scaling laws, potentially allowing to match the performance of large-scale models with much leaner training/models. In this work we go beyond the initial foray of Eldan and Li to show that high quality data can even improve the SOTA of large language models (LLMs), while dramatically reducing the dataset size and training compute. Importantly, smaller models requiring less training can significantly reduce the environmental cost of LLMs [BGMMS21.

We focus our attention on LLMs trained for code, and specifically writing simple Python functions from their docstrings as in CTJ \$^{+}\$21. The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMs' performance on code. We demonstrate the power of high

| Date | Model | Model size <br> (Parameters) | Dataset size <br> (Tokens) | HumanEval <br> (Pass@1) | MBPP <br> (Pass@1) |
| :---: | :---: | :---: | :---: | :---: | :---: |
| $2021 \mathrm{Jul}$ | Codex-300M $\left[\mathrm{CTJ}^{+} 21\right]$ | $\overline{300 \mathrm{M}}$ | $100 \mathrm{~B}$ | $13.2 \%$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_622cbf2f20b4a37d6579g-02.jpg?height=43&width=154&top_left_y=277&top_left_x=1686) |
| $2021 \mathrm{Jul}$ | Codex-12B $\mathrm{CTJ}^{+} 21$ | $12 \mathrm{~B}$ | 100B | $28.8 \%$ | - |
| $2022 \mathrm{Mar}$ | CodeGen-Mono-350M $\mathrm{NPH}^{+} 23$ | $350 \mathrm{M}$ | 577B | $12.8 \%$ | - |
| $2022 \mathrm{Mar}$ | CodeGen-Mono-16.1B $\mathrm{NPH}^{+} 23$ | $16.1 \mathrm{~B}$ | 577B | $29.3 \%$ | $35.3 \%$ |
| $2022 \mathrm{Apr}$ | PaLM-Coder $\mathrm{CND}^{+} 22$ | $540 \mathrm{~B}$ | $780 \mathrm{~B}$ | $35.9 \%$ | $47.0 \%$ |
| 2022 Sep | CodeGeeX $\mathrm{ZXZ}^{+} 23$ | 13B | $850 \mathrm{~B}$ | $22.9 \%$ | $24.4 \%$ |
| 2022 Nov | GPT-3.5 Ope23 | 175B | N.A. | $47 \%$ | - |
| $2022 \mathrm{Dec}$ | SantaCoder $\left.\mathrm{ALK}^{+} 23\right]$ | $1.1 \mathrm{~B}$ | $236 \mathrm{~B}$ | $14.0 \%$ | $35.0 \%$ |
| 2023 Mar | GPT-4 Ope23 | N.A. | N.A. | $67 \%$ | - |
| $2023 \mathrm{Apr}$ | Replit Rep23 | $2.7 \mathrm{~B}$ | $525 \mathrm{~B}$ | $21.9 \%$ | - |
| $2023 \mathrm{Apr}$ | Replit-Finetuned Rep23 | $2.7 \mathrm{~B}$ | 525B | $30.5 \%$ | - |
| 2023 May | CodeGen2-1B $\mathrm{NHX}^{+} 23$ | $1 \mathrm{~B}$ | N.A. | $10.3 \%$ | - |
| 2023 May | CodeGen2-7B $\mathrm{NHX}^{+} 23$ | $7 \mathrm{~B}$ | N.A. | $19.1 \%$ | - |
| 2023 May | StarCoder $\mathrm{LAZ}^{+} 23$ | $15.5 \mathrm{~B}$ | $1 \mathrm{~T}$ | $33.6 \%$ | $52.7 \%$ |
| 2023 May | StarCoder-Prompted $\mathrm{LAZ}^{+} 23$ | $15.5 \mathrm{~B}$ | $1 \mathrm{~T}$ | $40.8 \%$ | $49.5 \%$ |
| 2023 May | PaLM 2-S $\mathrm{ADF}^{+} 23$ | N.A. | N.A. | $37.6 \%$ | $50.0 \%$ |
| 2023 May | CodeT5 $+\mathrm{WLG}^{+} 23$ | $2 \mathrm{~B}$ | $52 \mathrm{~B}$ | $24.2 \%$ | - |
| 2023 May | CodeT5 $+\mathrm{WLG}^{+} 23$ | $16 \mathrm{~B}$ | $52 \mathrm{~B}$ | $30.9 \%$ | - |
| 2023 May | InstructCodeT5+ $\mathrm{WLG}^{+} 23$ | $16 \mathrm{~B}$ | $52 \mathrm{~B}$ | $35.0 \%$ | - |
| 2023 Jun | WizardCoder $\mathrm{LXZ}^{+} 23$ | $16 \mathrm{~B}$ | $1 \mathrm{~T}$ | $57.3 \%$ | $51.8 \%$ |
| 2023 Jun | phi-1 | $1.3 \mathrm{~B}$ | $7 \mathrm{~B}$ | $50.6 \%$ | $55.5 \%$ |

Table 1: We use self-reported scores whenever available. Despite being trained at vastly smaller scale, phi-1 outperforms competing models on HumanEval and MBPP, except for GPT-4 (also WizardCoder obtains better HumanEval but worse MBPP).

quality data in breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, for roughly 8 passes over 7B tokens (slightly over 50B total tokens seen) followed by finetuning on less than $200 \mathrm{M}$ tokens. Roughly speaking we pretrain on "textbook quality" data, both synthetically generated (with GPT-3.5) and filtered from web sources, and we finetune on "textbook-exercise-like" data. Despite being several orders of magnitude smaller than competing models, both in terms of dataset and model size (see Table 1), we attain $50.6 \%$ pass@1 accuracy on HumanEval and $55.5 \%$ pass@1 accuracy on MBPP (Mostly Basic Python Programs), which are one of the best self-reported numbers using only one LLM generation. In Section 2, we give some details of our training process, and we discuss evidence for the importance of our data selection process in achieving this result. Moreover, despite being trained on much fewer tokens compared to existing models, phi-1 still displays emergent properties. In Section 3 we discuss these emergent properties, and in particular we confirm the hypothesis that the number of parameters plays a key role in emergence (see e.g., $\mathrm{WTB}^{+} 22$ ), by comparing the outputs of phi-1 with those of phi-1-small, a model trained with the same pipeline but with only $350 \mathrm{M}$ parameters. The methodology used in this section is reminiscent of the Sparks of AGI paper \$\mathrm{BCE}^{+}\$23] that argued for moving away from static benchmarks to test LLMs' performance. Finally in Section 4 we discuss alternative benchmarks to evaluate the model and in Section 5 we study possible contamination of our training data with respect to HumanEval. We release the model for usage and evaluation by the broader community, but omit some details of the synthetic data generation, for proprietary reasons.

More related works Our work is part of the recent program of using LLMs for program synthesis, see $\left[\mathrm{CTJ}^{+} 21,\left[\mathrm{NPH}^{+} 22\right]\right.$ for more references on this. Our approach is also part of the emerging trend of
using existing LLMs to synthesize data for the training of new generations of LLMs, $\mathrm{WKM}^{+} 22, \mathrm{TGZ}^{+} 23$, \$\mathrm{MMJ}^{+}\$23, $\mathrm{LGK}^{+} 23, \mathrm{JWJ}^{+} 23$. There is an ongoing debate about whether such "recursive training" might lead to narrower scope for the resulting LLM [SSZ $\left.23, ~ G \mathrm{GWS}^{+} 23\right]$, see [MMJ 23$]$ for a counterviewpoint. Note that in this paper we focus on a narrow task, similarly to [JWJ \${ }^{+}\$23], in which case it seems plausible to attain better performance than the teacher LLM on that specific task (as is argued in the latter paper).

## 2 Training details and the importance of high-quality data

![](https://cdn.mathpix.com/cropped/2024_06_04_622cbf2f20b4a37d6579g-03.jpg?height=564&width=1089&top_left_y=669&top_left_x=518)

Figure 2.1: Pass@1 accuracy (\%) on HumanEval. The grouping of bar plots correspond to the usual scaling dimensions of either increasing the compute time (more passes on the data, here from $26 \mathrm{~B}$ tokens seen to $76 \mathrm{~B}$ ) or increasing the number of parameters of the model (here from $350 \mathrm{M}$ to $1.3 \mathrm{~B}$ ). Each column within a group corresponds to different training datasets: (A) The first (orange) column represents the performance of models trained on the standard dataset of deduplicated Python files from The Stack (plus StackOverflow for 1.3B parameter model); (B) The second (light green) column represents the performance of models trained with our new dataset composition CodeTextbook; (C) Finally, the third (dark green) column corresponds to the respective second column models finetuned on our new CodeExercises dataset. For the 1.3B models, phi-1 and phi-1-base are checkpoints after training on 51B tokens (770 GPU hours) and The Stack+ model was trained for 76B tokens and 1090 GPU hours. We highlight that even without any finetuning, our phi-1-base model trained on CodeTextbook dataset achieves $29 \%$ HumanEval performance with a mere 1.3B parameter model. The previous smallest model that achieves close to $30 \%$ performance on HumanEval was Replit-Finetuned at 2.7B parameters, which was trained with 100 times more training tokens than us Rep23. On top of this, finetuning on our CodeExercises dataset to obtain phi-1 not only gives us our top performance of $51 \%$ on HumanEval, but also unlocks further unexpected coding capabilities (see Section 3 ).

As alluded to in the title of the paper, the central ingredient our model relies on textbook-quality training data. Unlike previous work that used standard sources of text data for code generation, such as The Stack $\left[\mathrm{KLA}^{+} 22\right]$ (which contains sourcecode from repositories with permissive licenses) and other web-based datasets (e.g., StackOverflow and CodeContest $\left[\mathrm{LCC}^{+} 22\right]$ ), we argue that these sources are not optimal for teaching the model how to reason and plan algorithmically. On the other hand, our model architecture and training methods are fairly conventional (Section 2.3), so we devote this section primarily to explaining how we curated our data.

The standard code datasets $\left[\mathrm{KLA}^{+} 22\right.$, $\left.\mathrm{LCC}^{+} 22\right]$ form a large and diverse corpus covering broad range of topics and use cases. However, based on manual inspection of random samples we observe that many of these snippets are not very instructive for learning the basics of coding, and suffer from several drawbacks:

- Many samples are not self-contained, meaning that they depend on other modules or files that are external to the snippet, making them hard to understand without additional context.
- Typical examples do not involve any meaningful computation, but rather consist of trivial or boilerplate code, such as defining constants, setting parameters, or configuring GUI elements.
- Samples that do contain algorithmic logic are often buried inside complex or poorly documented functions, making them difficult to follow or learn from.
- The examples are skewed towards certain topics or use cases, resulting in an unbalanced distribution of coding concepts and skills across the dataset.

One can only imagine how frustrating and inefficient it would be for a human learner to try to acquire coding skills from these datasets, as they would have to deal with a lot of noise, ambiguity, and incompleteness in the data. We hypothesize that these issues also affect the performance of language models, as they reduce the quality and quantity of the signal that maps natural language to code. We conjecture that language models would benefit from a training set that has the same qualities as a good "textbook": it should be clear, self-contained, instructive, and balanced.

In this work, we address this challenge directly and show that by intentionally selecting and generating high-quality data, we can achieve state-of-the-art results on code-generation tasks with a much smaller model and less compute than existing approaches. Our training relies on three main datasets:

- A filtered code-language dataset, which is a subset of The Stack and StackOverflow, obtained by using a language model-based classifier (consisting of about 6B tokens).
- A synthetic textbook dataset consisting of $<1 \mathrm{~B}$ tokens of GPT-3.5 generated Python textbooks.
- A small synthetic exercises dataset consisting of $\sim 180 \mathrm{M}$ tokens of Python exercises and solutions.

We describe those datasets in more detail in the next subsections. Taken together, the above datasets contain less than 7B tokens. We refer to the combination of filtered code-language and synthetic textbook datasets as "CodeTextbook" and use it in the pretraining phase to obtain our base model phi-1-base - this model already achieves a competitive HumanEval performance of $29 \%$. Then we use the $180 \mathrm{M}$ token synthetic exercises dataset, referred to as "CodeExercises", to finetune our phi-1-base model to obtain phi-1. Despite the small size of the "CodeExercises" dataset, finetuning with this dataset is crucial not only for large improvements in generating simple Python function as shown in Figure 2.1, but more broadly to unlock many interesting emergent capabilities in our phi-1 model that are not observed in phi-1-base (see Section 3).

### 2.1 Filtering of existing code datasets using a transformer-based classifier

We begin with publicly available Python code datasets: we use the Python subset of the deduplicated version of The Stack and the StackOverflow, which together contain over 35 million files/samples, totalling over 35B tokens. We annotate the quality of a small subset of these files (about 100k samples) using GPT-4: given a code snippet, the model is prompted to "determine its educational value for a student whose goal is to learn basic coding concepts".

We then use this annotated dataset to train a random forest classifier that predicts the quality of a file/sample using its output embedding from a pretrained codegen model as features. We note that unlike GPT-3.5, which we use extensively to generate synthetic content (discussed below), we use GPT-4 minimally only for annotations on the quality of a small subset of The Stack and StackOverflow samples. We thus view our usage of GPT-4 as merely a way to avoid tedious human-annotation efforts [DLT $\left.{ }^{+} 23\right]$.

## Educational values deemed by the filter

## High educational value

```
import torch
import torch.nn.functional as F
def normalize(x, axis=-1)
    """Performs L2-Norm."""
    num = x
    denom = torch.norm(x, 2, axis, keepdim=True)
    .expand_as(x) + 1e-12
    return num / denom
def euclidean_dist(x, y):
    """Computes Euclidean distance."""
    m, n = x.size(0), y.size(0)
    xx = torch.pow(x, 2).sum(1, keepdim=True).
    expand(m, n)
    yy = torch.pow(x, 2).sum(1, keepdim=True).
    expand(m, m).t()
    dist = xx + yy - 2 * torch.matmul(x, y.t())
    dist = dist.clamp(min=1e-12).sqrt()
    return dist
def cosine_dist(x, y):
    """Computes Cosine Distance."""
    x = F.normalize(x, dim=1)
    y = F.normalize(y, dim=1)
    dist = 2 - 2 * torch.mm(x, y.t())
    return dist
```

Our filtering methodology boosts our model performance significantly even without the synthetic datasets discussed below: for $350 \mathrm{M}$ parameter models trained on unfiltered Stack (deduplicated python) and StackOverflow, the HumanEval performance saturates at $12.19 \%$ even after training for 96k steps ( 200B tokens), while training on the filtered subset achieves $17.68 \%$ on HumanEval after $36 \mathrm{k}$ steps. We further improve this to $20.12 \%$ (reported in Figure 2.1) by training on a combination of the filtered dataset and the synthetic textbooks dataset discussed below.

### 2.2 Creation of synthetic textbook-quality datasets

One of the main challenges in creating a high-quality dataset for code generation is ensuring that the examples are diverse and non-repetitive. By diversity, we mean that the examples should cover a wide range of coding concepts, skills, and scenarios, and that they should vary in their level of difficulty, complexity, and style. Diversity is important for several reasons: it exposes the language model to different ways of expressing and solving problems in code, it reduces the risk of overfitting or memorizing specific patterns or solutions, and it increases the generalization and robustness of the model to unseen or novel tasks. However, achieving diversity is not trivial, especially when using synthetic data generated by another language model. Simply prompting the model to produce a coding textbook or a set of exercises, even with some variation in the instructions or the parameters, will likely result in a very homogeneous and redundant dataset, where the same concepts and solutions are repeated over and over with minor changes. This is because language models tend to follow the most probable or common paths given their training data and their priors, and they lack the creativity or the incentive to explore alternative or novel ways of generating code. Therefore, one needs to find the right "trick" that will induce the language model to be more creative and diverse in its output, while still maintaining the quality and the coherence of the examples. Inspired by EL23, where a diverse set of short stories were created by including a
random subset of words chosen from some fixed vocabulary in the prompt and requiring that they would be somehow combined in the generated text, we look for ways to inject randomness into the prompt in a way that gives rise to the generation of a diverse dataset.

## The synthetic textbook dataset

This dataset consists of less that 1B tokens of GPT-3.5 generated Python textbooks, synthesized to provide a high-quality source of natural language heavy text interleaved with relevant code snippets. We further targeted the content of these textbooks to cover topics that promote reasoning and basic algorithmic skills. Here, diversity is obtained by providing constraints on topics and target audience of the generated textbook. The following is an example text from the synthetic textbook:

```
To begin, let us define singular and nonsingular matrices. A matrix is said to be singular if its
determinant is zero. On the other hand, a matrix is said to be nonsingular if its determinant is not
    zero. Now, let's explore these concepts through examples.
Example 1: Consider the matrix A = np.array([[1, 2], [2, 4]]). We can check if this matrix is
singular or nonsingular using the determinant function. We can define a Python function, `
is_singular(A)`, which returns true if the determinant of A is zero, and false otherwise.
import numpy as np
def is_singular(A):
    det = np.linalg.det(A)
    if det == 0:
        return True
    else:
        return False
A = np.array([[1, 2], [2, 4]])
print(is_singular(A)) # True
```


## The CodeExercises dataset

This is a small synthetic exercises dataset consisting of less than 180M tokens of Python exercises and solutions. Each exercise is a docstring of a function that needs to be completed. The goal of this dataset is to align the model to perform function completion tasks based on natural language instructions. This dataset was also generated by GPT-3.5, where the main means of eliciting diversity is by constraining the function names. For this dataset in particular, we conduct explicit decontamination and alternative evaluations in the following sections to ensure that problems similar to those from HumanEval benchmark are not seen during finetuning. The following snippet illustrates a synthetically generated exercise.

```
def valid_guessing_letters(word: str, guesses: List[str]) -> List[str]:
    """
    Returns a list of valid guessing letters, which are letters that have not been guessed yet and
    are present in the word.
    Parameters:
    word (str): The word to guess.
    guesses (List[str]): A list of letters that have already been guessed.
    Returns:
    List[str]: A list of valid guessing letters.
    """
    valid_letters = []
    for letter in word:
        if letter not in guesses and letter not in valid_letters:
            valid_letters.append(letter)
    return valid_letters
```


### 2.3 Model architecture and training

We use a decoder only transformer [VSP ${ }^{+} 17$ model using the FlashAttention implementation of multihead attention (MHA) $\mathrm{DFE}^{+} 22$. We also use MHA and MLP layers in parallel configuration following some recent models like CodeGen [NPH ${ }^{+} 22$, PaLM $\mathrm{CND}^{+} 22$, and GPT-NeoX $\mathrm{BBH}^{+} 22$. The architecture for our 1.3B parameter phi-1 model consists of 24 layers, hidden dimension of 2048, MLP-inner dimension of 8192, and 32 attention heads of dimension 64 each. The smaller $350 \mathrm{M}$ parameter phi1-small model consists of 20 layers, hidden dimension of 1024, MLP-inner dimension of 4096, and 16 attention heads of dimension 64 each. We also use a rotary position embedding [SLP \${ }^{+}\$21] with rotary dimension 32. These architectural choices were adopted from $\mathrm{NPH}^{+} 22$. We also use the same tokenizer as codegen-350M-mono \$\left[\mathrm{NPH}^{+}\right.\$22]. Aside from FlashAttention, our models do not use other techniques

![](https://cdn.mathpix.com/cropped/2024_06_04_622cbf2f20b4a37d6579g-07.jpg?height=49&width=1731&top_left_y=694&top_left_x=197)
boost performance and efficiency [LAZ $\left.{ }^{+} 23\right]$.

For both pretraining and finetuning, we concatenate our respective datasets into a single dimensional array with " $\mid$ endoftext|>" token used for separating the files. We train our models on sequence length of 2048 sliced from our dataset array with next-token prediction loss. We use fp16 training with AdamW optimizer, linear-warmup-linear-decay learning rate schedule, and attention and residual dropout of 0.1. We train on 8 Nvidia-A100 GPUs using deepspeed. Our pretrained base model phi-1-base was obtained in under 4 days of training. Finetuning to obtain phi-1 used an additional 7 hours on the same hardware.

Pretraining. phi-1-base was trained on the CodeTextbook dataset (filtered code-language corpus and synthetic textbooks). We use effective batch size 1024 (including data parallelism and gradient accumulation), maximum learning rate $1 \mathrm{e}-3$ with warmup over 750 steps, and weight decay 0.1 , for a total of 36,000 steps. We use the checkpoint at 24,000 steps as our phi-1-base - this is equivalent to 8 epochs on our CodeTextbook dataset for a total of little over 50B total training tokens. Despite the small size and computation, this model already achieves a $29 \%$ accuracy on HumanEval.

Finetuning. phi-1 is obtained by finetuning phi-1-base on the CodeExercises dataset. For finetuning, we use the same setup as pretraining, but different hyperparameters: we use effective batchsize of 256, maximum learning rate $1 \mathrm{e}-4$ with 50 steps of warmup, and weight decay 0.01 . We train for total of 6,000 steps and pick the best checkpoint (saved every 1000 steps).

## 3 Spikes of model capability after finetuning on CodeExercises

Figure 2.1 showed that the largest improvement in HumanEval resulted from finetuning on the small CodeExercises dataset ( $<200 \mathrm{M}$ tokens). CodeExercises consist exclusively of short Python tasks using only basic Python libraries. In this section, we demonstrate that, quite remarkably the model after finetuning also exhibits a substantial improvement in executing tasks that are not featured in the finetuning dataset. This includes managing intricate algorithmic tasks and using external libraries. This suggests that our finetuning process might have helped the model in reorganizing and consolidating the knowledge acquired during pretraining, even if such knowledge is not explicitly present in our CodeExercises dataset. In this section we will focus on qualitatively comparing and contrasting the capabilities of our finetuned model phi-1 and its pretrained 1.3B parameter base model phi-1-base.

### 3.1 Finetuning improves the model's understanding

Using a simple Python function that we created ourselves, we observe below that the model shows a much higher level of understanding and compliance with the instructions after finetuning. In particular, we see
that phi-1-base struggles with the logical relationships in the prompts, while phi-1 can interpret the question and generate the answer correctly. In this example, even our $350 \mathrm{M}$ phi-1-small model shows some level of understanding of the problem even though the resulting solution is wrong. We see such trends consistently in our interactions, see Appendix A for another example.

```
Prompt:
"" Alice, Bob and Charles are playing games. Alice first pick a random number from 333
to 678, Bob then pick a number starting from Alice's number to 888, Charles then pick a
number starting from 123 to Bob's number. Alice gets one point if Alice's number minus
Charles's number is divisible by the floor of the square root of Bob's number, otherwise
Bob gets one point. Simulate Alice's and Bob's points in 30 iterations.
" " "
phi-1
points $=[0,0]$
for i in range (30):
    alice_num = random.randint
    $(333,678)$
    bob_num = random.randint (
    alice_num, 888)
    charles_num = random.randint
    (123, bob_num)
    if (alice_num - charles_num)
    \% int(bob_num ** 0.5 ) == 0 :
self.alice_turn_points = 0
else: self.alice_turn_points_history =
        points $[1]+=1$
return f"Alice: \{points[0]\}, Bob:
    \{points [1] \}
phi-1-base
self.points = 0
self.alice = 333
self.bob = 678
self.charles = 888
self.bob_points = 0
self.alice_points = 0
self.iterations = 30
self.alice_turn = True
self.bob_turn = True
[]
self.bob_turn_points_history = []
self.alice_turn_points_history
.append(self.alice_points)
self.bob_turn_points_history
.append(self.bob_points)
```


## phi-1-small

indices $=[]$
import random
alice_points = 0
bob_points $=0$
for i in range(30):
alice_num = random.randint
(333, 678)
bob_num = random.randint(123,
bob_num)
if (alice_num - bob_num) \%
math.floor(math.sqrt(bob_num
)) $==0$ :
alice_points += 1
if $(i \% 2)==0$ :
bob_points += 1
print(f"Alice gets \{alice_points\}
points, while Bob gets \{
bob_points\} points.")

### 3.2 Finetuning improves the model's ability to use external libraries

We demonstrate here that finetuning on CodeExercises unexpectedly improves the model's ability to use external libraries such as Pygame and Tkinter, eventhough our exercises do not contain these libraries. This suggests that our finetuning not only improves the tasks we targeted, but also makes unrelated tasks easier to distill from pretraining. For reference, Figure 3.1 shows the distribution of package imports in our CodeExercises dataset.

![](https://cdn.mathpix.com/cropped/2024_06_04_622cbf2f20b4a37d6579g-08.jpg?height=317&width=1393&top_left_y=1858&top_left_x=363)

Figure 3.1: The number of imports among $\sim 880 \mathrm{~K}$ exercises in CodeExercises (we ignore libraries imported less than 10 times). The plot code is generated by phi-1 with the prompt: "I have a dictionary, first sort the dictionary using the value, from largest to smallest. Then generate a pyplot bar plot. First set font size to be 7, then rotate the x-axis label by 90 degree, the x-axis is the key, y-axis is the value of the dictionary. Use log-scale on y-axis. Moreover, set the y-axis label to be 'Log Number of Times' and x-axis label to be 'Imports'. Set dpi to be 1000."

PyGame example. We start with a PyGame that asks the model to generate code to move a ball:

```
Prompt:
""" Write a PyGame. There is a ball. At every iteration, (1). The x-axis of the ball
increases by a random integer between (-10, 10), (2). The y-axis of the ball increases
by a random integer between (-20, 20). The x-axis of the ball should stay inside 0-400,
and y-axis of the ball should stay inside 0-600. When the user press 'space', set the
x-axis of the ball to 200 and y-axis of the ball to 400.
" " "
```

phi-1
. .
screen = pygame.display.set_mode
$((400,600))$
$\mathrm{x}, \mathrm{y}=0,0$
while True:
for event in pygame.event.get
() :
. .
elif event.type == pygame
. KEYDOWN :
if event.key $==$
pygame.K_SPACE:

$$
x=200
$$

$$
y=400
$$

$\mathrm{x}+=$ random.randint $(-10,10)$
y $+=$ random.randint $(-20,20)$
if $x<0$ :
$\mathrm{x}=0$
elif $x>400$ :
$\mathrm{x}=400$
if $y<0$ :
$\mathrm{y}=0$
elif $y>600$ :
$y=600$
screen.fill((255, 255, 255))
pygame.draw.circle(screen,
(255, 0, 0), (x, y), 10)
pygame.display.update()
phi-1-base
. .
ball_x = random.randint $(-10,10)$
ball_y = random.randint $(-20,20)$
ball_vx = random.randint $(-5,5)$
ball_vy = random.randint $(-5,5)$
・・
screen $=$ pygame.display.set_mode
$((800,600))$
while not done:
for event in pygame.event.get
( ) :
・.
elif event.type $==$ pygame
. KEYDOWN :
if event.key $==$
pygame.K_SPACE:
ball_x $=200$
ball_y $=400$
ball_x $+=$ ball_vx
ball_y $+=$ ball_vy
ball_vx $+=$ ball
pygame.display.flip()
phi-1-small
. .
$x, y=0,0$
$d x, d y=$ random.randint $(-10,10)$,
random.randint $(-20,20)$
while True:
screen.fill( ( $0,0,0))$
pygame. draw.rect (screen,
$(255,255,255), \quad(x, y, 400$,
$600)$ )
pygame.display.update()
$x+=d x$
$y+=d y$
if $x+d x>400$ :
$d x=-d x$
elif $x+d x<0:$
$d x=d x$
if $y+d y>600$ :
$d y=-d y$
elif $y+d y<0:$
$d y=d y$
pygame.time.wait(50)

The above code snippet shows the main loop of a simple PyGame program that bounces a ball on the screen. phi-1 correctly applies the PyGame functions to update and draw the ball, as instructed by the prompt. phi-1-base and phi-1-small produce function calls that are syntactically correct but semantically irrelevant. We can see that phi-1-base shows some ability to use the appropriate API calls, but it fails to follow the logic of the task, while phi-1-small after finetuning understands the logic but does not have enough capacity to learn the correct function calls.

TKinter example. Our second example is a TKinter application where we ask the model to update a textfield accordingly when the user clicks buttons.

```
Prompt:
"" Write a tkinter application, there is a textfield on top, a submit button at the
button and a recover button next to it. When pressing the submit button, get the text
from the textfield and save it to a global string s. If the text contains the word
"pew" update the textfield text to "pewpewpew?", otherwise, add "no pew..." to the end.
When pressing the recover button, replace the text in the textfield with s.
" " "
phi-1
. .
global s
$s=" "$
def submit() :
    global s
    text = textfield.get()
    if "pew" in text:
        textfield.delete(0, tk.END)
        textfield.insert(0, "pewpewpew...")
    else:
        textfield.delete(0, tk.END)
        textfield.insert(0, text + "no pew...")
    $s=$ text
def recover():
    global s
    textfield.delete(0, tk.END)
    textfield.insert(0, s)
textfield = tk.Entry(root)
textfield.pack()
submit_button = tk.Button(root, text="Submit",
    command=submit)
submit_button.pack()
recover_button = tk. Button(root, text="Recover"
    , command=recover)
recover_button.pack()
root.mainloop()
phi-1-base
global s
s = s.replace("pew", "pewpewpew")
if "pew" in s:
    s = s.replace("pew", "pewpewpew")
else:
    $s=s+"$ no pew..."
text.delete(0.0, END)
text.insert(0.0, s)
def main():
    global s
    s = "pewpewpew"
    text.delete $(0.0$, END)
    text.insert (0.0, s)
    text.tag_add("button", "<Button-1>", "<Button-3>
    ")
    text.tag_add("button", "<Button-2>", "<Button-4>
    ")
    \# The model keeps adding buttons from here.
phi-1-small
import tkinter as tk
root $=$ tk.Tk()
global s
text $=$ " "
while True:
    event, values = tk.popup()
    if event == "submit":
        text += values.split()[0] + " "
    elif event == "textfield":
        s += values.split()[0] + " "
    elif event == "submit-button-text":
        text = values.split()[0] + " "
    elif event == "text-button-text":
        $s=$ text
    if "pew" in text:
        text = text.replace("pew", "pewpewpew")
    if "no pew" in text:
        break
tk.destroy()
```

The three models' completions show a huge gap in their prompt understanding. Both phi-1-base and phi-1-small fail to use the correct Tkinter APIs and make up meaningless function calls. On the other hand, phi-1 implements the GUI and all the functions correctly (except that it did not copy "pewpewpew?" correctly). We present two additional examples in Appendix A for pytorch and pyplot.

Chat mode example. Finally, we show that phi-1 has a better chat capability than phi-1-base despite that chat data is exclusive in pretraining, but not in the finetuning.

```
Prompt:
Student: I have a Python pyplot, I want to increase its resolution and rotate it, what
should I do? TA:
phi-1 phi-1-base
1. Set the 'dpi' parameter to the desired You can use the `setp()' function to set the
resolution (e.g. 300 for 300 DPI) resolution and the 'set_rotation()' function to
2. Use the 'rotate' function from the 'matplotlib. rotate the plot. <|endoftext|>
pyplot' module to rotate the plot by 90 degrees
clockwise
Here's an example:
import matplotlib.pyplot as plt I am a Python pyplot, and I want to increase its
fig, ax = plt.subplots()
ax.plot ([1, 2, 3], $[4,5,6])$
\# Set the resolution to 300 DPI
ax.set_dpi(300)
\# Rotate the plot by 90 degrees clockwise
ax.rotate (90)
```


## 4 Evaluation on unconventional problems with LLM grading

A potential concern with the surprisingly good performance of phi-1 on HumanEval (see Table 1 and Figure 2.1) is that there might be memorization stemming from contamination of the synthetic CodeExercises dataset. We study this potential contamination directly in Section 5, while this section addresses the concern with a new evaluation that is designed to be unconventional enough to be unlikely to appear in our training dataset.

To minimize bias and leakage, the new evaluation problems were created by a dedicated team that did not access the CodeExercises dataset or the final model. They created 50 new problems in the same format as HumanEval with instructions to design problems that are unlikely to appear in real-world code bases or as coding exercises. Here is an example of such a problem:

```
def sort_concat_square_deduplicate(list1, list2, my_threshold):
    """
    This functions takes two lists of integers, sorts each of them in ascending order,
    concatenates them, squares the entries at even indices, filters out entries
    smaller than my_threshold and then removes duplicates. The resulting list is
    returned.
    """
```

One of the challenges of evaluating language models on coding tasks is that the output of the model is often binary: either the code passes all the unit tests or it fails. However, this does not capture the nuances of the model's performance, as it might have produced a code that is almost correct but has a minor error, or a code that is completely wrong but coincidentally passes some tests. Arguably, a more informative way of assessing the model's coding skills is to compare its output with the correct solution and grade it based on how well it matches the expected logic. This is similar to how humans are evaluated on coding interviews, where the interviewer does not only run the code but also examines the reasoning and the quality of the solution.

To evaluate candidate solutions, we therefore adopt the approach of using GPT-4 to grade the solution (such as in [L23). This approach has two distinct advantages: (1) by using GPT-4 as a grader, we can leverage its knowledge and generative abilities to obtain a more fine-grained and meaningful signal of the

| Model | Size | Training tokens | Score | HumanEval |
| :--- | :--- | :--- | :--- | :--- |
| CodeGen-Mono-350M $\left[\mathrm{NPH}^{+}\right.$23 | $350 \mathrm{M}$ | $577 \mathrm{~B}$ | $19 \%$ | $13 \%$ |
| CodeGen-Mono-16.1B $\mathrm{NPH}^{+}$23] | 16.1B | $577 \mathrm{~B}$ | $38 \%$ | $29 \%$ |
| Replit Rep23 | 2.7B | $525 \mathrm{~B}$ | $37 \%$ | $22 \%$ |
| StarCoder $\left[\mathrm{LAZ}^{+}\right.$23] | 15.5B | $1 \mathrm{~T}$ | $51 \%$ | $34 \%$ |
| phi-1-base | $1.3 \mathrm{~B}$ | $7 \mathrm{~B}$ | $37 \%$ | $29 \%$ |
| phi-1-small | 350M | 7B | $45 \%$ | $45 \%$ |
| phi-1 | $1.3 \mathrm{~B}$ | 7B | $52 \%$ | $51 \%$ |

Table 2: LLM graded Understanding scores on 50 new unconventional coding problems.

student model's coding capabilities, and (2) it obviates the need for tests 1 . Our prompt instructs the LLM to evaluate a student's solution first in a short verbal evaluation followed by grades from 0 to 10 .

See Table 2 for our results with phi-1 and competing models. The grades on our new unconventional problems give the same ranking as HumanEval (see Table 1). phi-1 again achieves a score significantly higher than StarCoder, as it did on HumanEval. Given that the new problems have had no chance to contaminate the training data and, furthermore, were designed to be outside the training distribution, these results greatly increase our confidence in the validity of phi-1's performance.

## 5 Data pruning for unbiased performance evaluation

In Figure 2.1, we see that training on CodeExercises leads to a substantial boost in the performance of the model on the HumanEval benchmark. To investigate this boost, we propose to prune the CodeExercises dataset by removing files that are "similar" to those in HumanEval. This process can be viewed as a "strong form" of data decontamination. We then retrain our model on such pruned data, and still observe strong performance on HumanEval. In particular, even after aggressively pruning more than $40 \%$ of the CodeExercises dataset (this even prunes files that are only vaguely similar to HumanEval, see Appendix (C), the retrained phi-1 still outperforms StarCoder.

We believe that such data pruning experiment is a fair way to evaluate performance, and is more insightful than standard "contamination" studies in the literature that are usually based on measures of overlap between training and test data (e.g., Section 4.8 of [AON $\left.{ }^{+} 21\right]$ ). For sake of completeness we start this section by conducting a standard contamination experiment, which shows that CodeExercises is not contaminated by HumanEval in this standard sense.

### 5.1 N-gram overlap

N-gram measures the similarity of text segments based on the shared n-word sequences. We calculate the n-gram overlap between the docstrings of each humaneval question and each exercise in the CodeExercises dataset that was generated. We found 4 humaneval questions with 13 -gram overlap with at least one of the entries in our dataset. After further investigating, we found out that all the 4 overlap cases in the 13-gram are all false positives such as the example below. Our n-gram overlap analysis shows that our dataset has minimal letter-by-letter overlap with HumanEval.[^0]

```
HumanEval:
You are given a non-empty list of positive
integers. Return the greatest integer that
is greater than zero, and has a frequency
greater than or equal to the value of the
integer itself. The frequency of an integer
is the number of times it appears in the list.
```


## CodeExercises:

Calculates the power frequency analysis sum of a list of integers. The power frequency analysis sum is calculated by taking the sum of the squares of the frequencies of each unique integer in the list. The frequency of an integer is the number of times it appears in the list.

### 5.2 Embedding and syntax-based similarity analysis

As we just saw, the n-gram analysis is not refined enough to find similar code snippets between HumanEval and CodeExercises. Instead we use a combination of embedding and syntax-based distances. For the embedding distance we compute the L2 distance between the embedding of the code snippets where the embedding is derived from a pre-trained CodeGen-Mono $350 \mathrm{M}$ model $\left[\mathrm{NPH}^{+} 23\right]$. We observe that the embedding distance is successful in capturing code pairs where the overall code semantics are similar, which can be inferred via the Python Docstring, function/class names, as well as the code structure. For the syntax-based distance we calculate the (string) edit distance between the abstract syntax trees (ASTs) of two given code snippets. The AST distance successfully identifies overlapping sections between code pairs while being agnostic to non-syntax text such as variable/function naming, comments, and Python Docstrings. For our pruning of CodeExercises we fix a threshold for the embedding distance, and we test several match rate $\tau$ for the AST distance. See Appendix Cfor examples of code pairs that are captured with the embedding distance and various AST match rates $\tau$. We vary $\tau$ between 0.95 and 0.8 , which corresponds to removing between $42.5 \mathrm{~K}$ to $354 \mathrm{~K}$ of the $879.5 \mathrm{~K}$ total problems in CodeExercises.

| $\tau$ |  | Problem <br> Count | phi-1 | phi-1 retrained <br> on pruned data | StarCoder-Prompted <br> $\left[\mathrm{LAZ}^{+}\right.$23 $]$ |
| :---: | :--- | :---: | :---: | :---: | :---: |
|  | similar | 71 | $81.7 \%$ | $74.6 \%$ | $57.7 \%$ |
|  | non-similar | 93 | $26.9 \%$ | $32.3 \%$ | $29.0 \%$ |
|  | total | 164 | $50.6 \%$ | $50.6 \%$ | $41.5 \%$ |
| 0.9 | similar | 93 | $63.4 \%$ | $51.6 \%$ | $48.4 \%$ |
|  | non-similar | 71 | $33.8 \%$ | $36.6 \%$ | $32.4 \%$ |
|  | total | 164 | $50.6 \%$ | $45.1 \%$ | $41.5 \%$ |
| 0.85 | similar | 106 | $62.3 \%$ | $52.8 \%$ | $47.2 \%$ |
|  | non-similar | 58 | $29.3 \%$ | $34.5 \%$ | $31.0 \%$ |
|  | total | 164 | $50.6 \%$ | $46.3 \%$ | $41.5 \%$ |
| 0.8 | similar | 116 | $59.5 \%$ | $52.6 \%$ | $45.7 \%$ |
|  | non-similar | 48 | $29.2 \%$ | $27.1 \%$ | $31.2 \%$ |
|  | total | 164 | $50.6 \%$ | $45.1 \%$ | $41.5 \%$ |

Table 3: Percentage of similar versus non-similar HumanEval problems correctly solved by different models. Similarity is determined based on whether or not the corresponding HumanEval problem has any close matches inside the CodeExercises dataset (for a given $\tau$ ). The problem count denotes the number of HumanEval problems within each subset. Here, $\tau$ is the threshold on AST-based match rate between codes for similarity check.

Table 3 summarizes the performance of our retrained phi- 1 on pruned datasets (with $\tau=0.95,0.9,0.85$ and 0.8) versus the original phi-1 trained on full CodeExercises and the $15.5 B$-parameter StarCoderprompted. We divide the HumanEval problems into two subsets ("similar" and "non-similar") based on whether or not they have at least one close match (for this given $\tau$ ) inside the original CodeExercises dataset. We then report the accuracy of the models on each subset of HumanEval separately. As one can see, even after heavily pruning our dataset, phi-1 still outperforms StarCoder-Prompted by a large
margin, which validates that our performance boost is not due to dataset "contamination", even when the latter term is understood loosely. Note also that the accuracy of all models is lower on the HumanEval non-similar subset versus the similar one.

## 6 Conclusion

Just as a comprehensive, well-crafted textbook can provide a student with the necessary knowledge to master a new subject, our work demonstrates the remarkable impact of high-quality data in honing a language model's proficiency in code-generation tasks. By crafting "textbook quality" data we were able to train a model that surpasses almost all open-source models on coding benchmarks such as HumanEval and MBPP despite being 10x smaller in model size and 100x smaller in dataset size. We hypothesize that such high quality data dramatically improves the learning efficiency of language models for code as they provide clear, self-contained, instructive, and balanced examples of coding concepts and skills.

There remains a number of limitations of our model compared to larger models for code. Firstly, phi-1 is specialized in Python coding, which restricts its versatility compared to multi-language models. Secondly, phi-1 lacks the domain-specific knowledge of larger models such as programming with specific APIs or using less common packages. Lastly, due to the structured nature of the datasets and the lack of diversity in terms of language and style, phi-1 is less robust to stylistic variations or errors in the prompt (for instance, its performance substantially degrades when there are grammatical mistakes in the prompt). We expand on these limitations and give examples of the failure modes of phi-1 in Appendix B.

None of these limitations seem fundamental, and with more work our approach could be used to tackle each one of them, although it is unclear what scaling might be necessary to overcome them (both for the model size and the dataset size). We also believe that significant gains could be achieved by using GPT-4 to generate the synthetic data instead of GPT-3.5, as we noticed that GPT-3.5 data has a high error rate. It is interesting that phi-1 is able to achieve such high coding proficiency despite those errors (a similar phenomenon was observed in AZL23 where a language model can be trained on data with $100 \%$ error rate and still generate correct answers at test time).

More generally, our work provides evidence that developing good methodology for creating highquality datasets is a central direction of research for advancing natural language processing and related fields (see also [JWJ ${ }^{+} 23$ for further evidence). However, creating high-quality datasets is not a trivial task, and it poses several challenges that need to be addressed. One challenge is to ensure that the dataset covers all the relevant content and concepts that one wants the model to learn, and that it does so in a balanced and representative way. Another challenge is to ensure that the dataset is truly diverse and non-repetitive, so that the model does not simply overfit to the data or memorize specific patterns or solutions. This requires finding ways to inject randomness and creativity into the data generation process, while still maintaining the quality and the coherence of the examples. Moreover, even after creating such datasets, we lack a good methodology to measure and evaluate the amount of diversity and redundancy in the data. For example, if we have a dataset with coding exercises, it is hard to determine how many different variations of each exercise exist, and how they are distributed across the dataset. Finally, as language models themselves will be used to curate data for future language models, it further increases the urgency on the ethical and social implications of training such models, such as the accountability, the transparency, and the bias of the data and the models that are involved in this process.

## References

[ADF 23$]$ Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

\$\left[\mathrm{ALK}^{+}\right.\$23] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988, 2023.

\$\left[\mathrm{AON}^{+}\right.\$21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.

[AZL23] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. arXiv preprint arXiv:2305.13673, 2023.

$\left[\mathrm{BBH}^{+} 22\right]$ Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of the ACL Workshop on Challenges 8 Perspectives in Creating Large Language Models, 2022 .

$\left[\mathrm{BCE}^{+} 23\right]$ Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.

[BGMMS21] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623, 2021 .

\$\left[\mathrm{BJT}^{+}\right.\$22] Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.

\$\left[\mathrm{BMR}^{+}\right.\$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901,2020 .

$\left[\mathrm{CND}^{+} 22\right]$ Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022 .
\$\left[\mathrm{CTJ}^{+}\right.\$21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

$\left[\mathrm{DFE}^{+} 22\right]$ Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022.

[DLT \${ }^{+}\$23] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.

[EL23] Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023.

[GWS \${ }^{+}\$23] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023.

\$\left[\mathrm{HBM}^{+}\right.\$22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.

\$\left[\mathrm{HNA}^{+}\right.\$17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.

[JWJ \${ }^{+}\$23] Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi. Impossible distillation: from low-quality model to high-quality dataset \& model for summarization and paraphrasing. arXiv preprint arXiv:2305.16635, 2023 .

$\left[\mathrm{KLA}^{+} 22\right]$ Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022.

$\left[\mathrm{KMH}^{+} 20\right]$ Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

\$\left[\mathrm{LAZ}^{+}\right.\$23] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.

\$\left[\mathrm{LCC}^{+}\right.\$22] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.
\$\left[\mathrm{LGK}^{+}\right.\$23] Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Harsha Nori, and Sergey Yekhanin. Differentially private synthetic data via foundation model apis 1: Images. arXiv preprint arXiv:2305.15560, 2023.

[LXWZ23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210, 2023.

[LXZ \${ }^{+}\$23] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct, 2023.

[LYR \${ }^{+}\$23] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity. arXiv preprint arXiv:2305.13169, 2023.

[MMJ+23] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023.

\$\left[\mathrm{MRB}^{+}\right.\$23] Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. arXiv preprint arXiv:2305.16264, 2023.

$\left[\mathrm{NHX}^{+} 23\right] \quad$ Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2: Lessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309, 2023.

$\left[\mathrm{NPH}^{+} 22\right]$ Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint, 2022.

\$\left[\mathrm{NPH}^{+}\right.\$23] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. ICLR, 2023.

[Ope23] OpenAI. Gpt-4 technical report, 2023. arXiv preprint arXiv:2303.08774 [cs.CL].

[Rep23] Replit. Replit dev day. https://twitter.com/Replit/status/ $1651344184593506304,2023$.

[RSR \${ }^{+}\$20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.

\$\left[\mathrm{SLP}^{+}\right.\$21] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.

[SSZ 23$]$ Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. Model dementia: Generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023.

[TGZ $\left.{ }^{+} 23\right]$ Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

$\left[\mathrm{VSP}^{+} 17\right]$ Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.

\$\left[\mathrm{WKM}^{+}\right.\$22] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.

\$\left[\mathrm{WLG}^{+}\right.\$23] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023.

\$\left[\mathrm{WTB}^{+}\right.\$22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. Survey Certification.

$\left[\right.$ YGK $\left.^{+} 23\right]$ Da Yu, Sivakanth Gopi, Janardhan Kulkarni, Zinan Lin, Saurabh Naik, Tomasz Lukasz Religa, Jian Yin, and Huishuai Zhang. Selective pre-training for private fine-tuning. arXiv preprint arXiv:2305.13865, 2023.

\$\left[\mathrm{ZXZ}^{+}\right.\$23] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x, 2023.
