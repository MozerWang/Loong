# Agent Planning with World Knowledge Model 

![](https://cdn.mathpix.com/cropped/2024_06_04_54fac0272c4f81ee7ba4g-01.jpg?height=54&width=1231&top_left_y=545&top_left_x=447) <br> Shumin Deng ${ }^{*}$, Yong Jiang $\diamond$, Pengjun Xie ${ }^{\diamond}$, Fei Huang ${ }^{\diamond}$, Huajun Chen ${ }^{\wedge} \odot \dagger$ <br> ${ }^{4}$ Zhejiang University <br> ${ }^{\bigcirc}$ Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph <br> ${ }^{*}$ National University of Singapore $\diamond$ Alibaba Group <br> \{shuofei, zhangningyu\}@zju.edu.cn
}


#### Abstract

Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-anderror in global planning and generating hallucinatory actions in local planning due to their poor understanding of the "real" physical world. Imitating humans" mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further developmen 3


![](https://cdn.mathpix.com/cropped/2024_06_04_54fac0272c4f81ee7ba4g-01.jpg?height=460&width=1116&top_left_y=1786&top_left_x=496)

Figure 1: Traditional agent planning vs. Agent planning with world knowledge model.[^0]

## 1 Introduction

The remarkable advances in Large Language Models (LLMs) have witnessed a rapid development of various natural language processing tasks [25, 16, 28, 47, 60, 33]. Recently, multiple attempts that directly exploit LLMs as agent models to address physical world planning tasks have demonstrated promising achievements [54, 57, 56, 34, 38, 64, 44]. However, as most state-of-the-art LLMs are autoregressive models trained with next-token prediction, they lack the ability to essentially understand the real world, leading to generating hallucinatory actions and performing brainless trial-and-error in the environment as shown in Figure 1.a).

In contrast to LLMs, humans possess a mental knowledge model about the physical world [1, 18, 17, 30]. When facing a specific task, they will first briefly rehearse the entire process in mind using their rich prior knowledge before performing mindless actions. We call this kind of knowledge global task knowledge (a.k.a. environment/task commonsense). In addition, during the task procedure, the mental world knowledge model will constantly maintain a kind of local state knowledge, representing humans' cognition of the current world state. For example, imagine you are in a room and your task is to put a clean egg in microwave. The task knowledge may refer to The egg is most likely in the fridge ... The workflows are: 1) locate and take the egg; 2) clean the egg using sinkbasin... The state knowledge possibly refers to My task is to ... I have found and taked the egg ... Next I should ... The absence of world knowledge can lead to blind trial-and-error in the early planning stages when environmental information is limited. Conversely, in later stages when information is redundant, it can easily result in a confused cognition of the current world state and generate hallucinatory actions.

The process by which humans handle planning tasks reminds us to develop a parametric World Knowledge Model (WKM) to facilitate agent planning. As humans typically acquire knowledge from expertise and practical experience, we build WKM based on knowledge learned from both expert and explored trajectories. Specifically, we first steer the agent model to synthesize task knowledge from the comparison between expert and sampled trajectories. Then we prompt it to summarize state knowledge for each planning step from expert trajectories and combine the previous and next actions to build a state knowledge base. Lastly, we integrate the generated knowledge into expert trajectories and train a WKM. The agent model needs to be retrained to adapt to the task knowledge. Note our agent and knowledge model are both trained with LoRA [12] sharing the same backbone.

During the planning phase, we use the WKM to provide global prior task knowledge and maintain local dynamic state knowledge for the agent model as shown in Figure 1.b). The task knowledge will be concatenated in natural language form following the specific task to guide the agent model's trial-and-error. At each planning step, to prevent the occurrence of hallucinatory actions, we utilize the generated state knowledge as the query to conduct $k \mathrm{NN}$ retrieval from the pre-built state knowledge base. We then use the constraints from the previous action, the probabilities of the retrieved next actions, and the probabilities from the agent model to make a weighted prediction for the next action.

We evaluate our method on three real-world simulated planning tasks: ALFWorld [41], WebShop [53], and ScienceWorld [50] with three state-of-the-art open-source LLMs: Mistral-7B [16], Gemma7B [24], and Llama-3-8B [25]. Empirical results demonstrate that our method achieves superior performance compared to various strong baselines on both seen and unseen tasks. Moreover, further analytical results show that 1) our WKM can effectively reduce blind trial-and-error and hallucinatory actions, 2) our model-generated instance-level knowledge can generalize better to unseen tasks, 3) weak-guide-strong is feasible, 4) multi-task unified WKM possesses strong potential, and 5) explicit state knowledge will hurt the performance of agent planning.

## 2 Preliminaries

We mainly focus on interactive tasks with partial observations from environments. Following the task formulation in [44], the problem can be viewed as a Partially Observable Markov Decision Process (POMDP): $(\mathcal{U}, \mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T})$. The instruction space $\mathcal{U}$ defines the task and its corresponding regulations. $\mathcal{S}$ is the state space, $\mathcal{O}$ is the observation space, and $\mathcal{A}$ is the action space. $\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ defines the transition function, which we assume to be given by the environments. It is noticed that $\mathcal{U}, \mathcal{A}$, and $\mathcal{O}$ are subspaces of the natural language space in the language agent scenarios.

![](https://cdn.mathpix.com/cropped/2024_06_04_54fac0272c4f81ee7ba4g-03.jpg?height=823&width=1396&top_left_y=239&top_left_x=362)

Figure 2: Overview of our WKM. We train a world knowledge model on the knowledge synthesized by the agent model itself from both expert and explored trajectories, providing prior task knowledge to guide global planning and dynamic state knowledge to assist local planning.

Based on the above, the historical trajectory $h_{t}$ that consists of a list of actions and observations at time $t$ can be represented as:

$$
\begin{equation*}
h_{t}=\left(u, a_{0}, o_{0}, a_{1}, o_{1}, \ldots, a_{t}, o_{t}\right) \tag{1}
\end{equation*}
$$

where $u \in \mathcal{U}$ is the task instruction and $a \in \mathcal{A}, o \in \mathcal{O}$ are the action and the observation. Given a task, the language agent with parameter $\theta$ serves as the policy model $\pi_{\theta}$ responsible for generating the action $a_{t+1}$ based on $h_{t}$ at each time step $t+1$ :

$$
\begin{equation*}
a_{t+1} \sim \pi_{\theta}\left(\cdot \mid h_{t}\right) \tag{2}
\end{equation*}
$$

Specifically, $a_{0} \sim \pi_{\theta}(\cdot \mid u)$ is generated according to the task instruction $u$. The whole trajectory $\tau$ concludes when the task is completed or exceeds the maximum time steps. Then the production of the entire trajectory with time length $n$ can be modeled as:

$$
\begin{equation*}
\pi_{\theta}(\tau \mid u)=\prod_{t=0}^{n} \pi_{\theta}\left(a_{t+1} \mid h_{t}\right) \pi_{\theta}\left(a_{0} \mid u\right) \tag{3}
\end{equation*}
$$

Ultimately, the final reward $r(u, \tau) \in[0,1]$ representing the task completion rate is calculated. Note that we follow a REAcT-style [54] trajectory that includes rationales before each action. We use $a$ to represent the action with rationales for convenience.

World Knowledge Model. World knowledge model serves as humans' mental cognition of the physical environment, more intricate than the word knowledge model which LLM-powered agent models are trained to be [61, 10, 52, 13]. Our "world" here refers to the simulated environment of the task. Based on the static environment of the task and the dynamic changes during interaction with the agent, we define world knowledge as a combination of prior global knowledge and dynamic local knowledge, corresponding to the blind trial-and-error problem in global planning and the hallucinatory action issue in local planning in traditional agent models, respectively. To attain precise and efficient agent planning, we develop a parametric WKM to simulate the mental WKM of humans.

## 3 Method

As shown in Figure 2, we steer the agent model to self-synthesize the task knowledge from the comparison of expert and sampled trajectories ( $\$ 3.1$. Then we prompt the agent model to selfsummarize the state knowledge based on historical behavior and construct a state knowledge base

( $\$ 3.2$. The generated knowledge will be integrated into the expert trajectories for training the WKM. After the training process ( $\$ 3.3$, we augment the agent model with the world knowledge model to achieve effective and accurate planning ( $\$ 3.4$ ).

### 3.1 Task Knowledge Synthesis

The task knowledge serves as the prior knowledge to guide the agent model's global planning and prevent it from dropping into blind trial-and-error.

Experienced Agent Exploration. We primarily acquire task knowledge through the comparison of preference trajectories (chosen vs. rejected). In order to improve the quality of rejected trajectories and obtain more targeted task knowledge, we employ an experienced agent for exploration. Firstly, we train a vanilla language model with expert trajectorie ${ }^{4}$ from the training set to obtain an experienced agent. Subsequently, the experienced agent explores the training set tasks again to generate rejected trajectories. Our purpose is to extract superior task knowledge that cannot be acquired solely through supervised fine-tuning on chosen trajectories, thus further effectively boosting the agent's capabilities.

Self Knowledge Synthesis. With the expert trajectories as the chosen ones and the trajectories sampled from the experienced agent as the rejected ones, we prompt the agent model itself to synthesize the task knowledge. Supposing $\mathcal{K}$ is the task knowledge space:

$$
\begin{equation*}
\kappa \sim \pi_{\theta}\left(\cdot \mid \rho_{\text {TaskKnow }}, u, \tau_{w}, \tau_{l}\right) \tag{4}
\end{equation*}
$$

where $\kappa \in \mathcal{K}$ is the task knowledge, $\rho_{\text {TaskKnow }}$ stands for the prompt to instruct the task knowledge extraction, and $\tau_{w}, \tau_{l}$ are the chosen and rejected trajectories respectively. Note that given the same task $u, \tau_{w}$ and $\tau_{l}$ always satisfy $r\left(u, \tau_{w}\right)=1 \geq r\left(u, \tau_{l}\right)$. Even when $r\left(u, \tau_{w}\right)=r\left(u, \tau_{l}\right)$, we still consider trajectories sampled from the experienced agent as rejected ones. This is because expert trajectories often have shorter step lengths, enabling the agent to learn more knowledge of efficient planning. For detailed prompts of task knowledge synthesis, please refer to Appendix H. 1 .

### 3.2 State Knowledge Summarization

The state knowledge serves as the dynamic knowledge to constrain the agent model's local planning and prevent it from generating hallucinatory actions. We prompt the agent model to self-summarize state knowledge at each planning step based on the expert trajectories to guarantee quality. For detailed prompts of state knowledge summarization, please refer to Appendix H.2 Supposing the prompt used to summarize state knowledge is $\rho_{\text {StateKnow }}$ and the state knowledge $s \in \mathcal{S}$ is a part of the state space $\mathcal{S}$, the generation of state knowledge at time $t$ can be represented as:

$$
\begin{equation*}
s_{t} \sim \pi_{\theta}\left(\cdot \mid \rho_{\text {StateKnow }}, h_{t}\right) \tag{5}
\end{equation*}
$$

State Knowledge Base Construction. To avoid confusion caused by excessive additional information, instead of explicitly concatenating the state knowledge to the context, we construct a state knowledge base for retrieval (we analyze in $\$ 4.3$ how explicit state knowledge may affect the performance of agent model). We combine the state knowledge $s_{t}$ with the previous action $a_{t}$ and next action $a_{t+1}$ from the expert trajectory to form a action-state-action triplet $\left(a_{t}, s_{t}, a_{t+1}\right)$. After iterating through all expert trajectories, we obtain a State Knowledge Base $\mathcal{B}=\left\{\left(s, a_{\text {pre }}, a_{\text {next }}\right)^{(i)}\right\}_{i=1}^{|\mathcal{B}|}$, where $a_{\text {pre }}=a_{t}, a_{\text {next }}=a_{t+1}$, and $|\mathcal{B}|$ is the size of the state knowledge base.

### 3.3 Model Training

We integrate the generated world knowledge into expert trajectories and train a world knowledge model. The agent model needs to be re-trained to adapt to the incorporation of task knowledge. Note that our agent model and knowledge model are both trained with LoRA sharing the same backbone. We list the examples of training data for both the agent model and WKM in Appendix E.[^1]

Agent Model Training. Given the expert trajectories dataset $\mathcal{D}=\left\{\left(u, \kappa, \tau_{w}\right)^{(i)}\right\}_{i=1}^{|\mathcal{D}|}$ with task knowledge $\kappa$ generated in $\$ 3.1$, we train the agent model to follow the task knowledge to generate actions. Under an auto-regressive manner, the loss of the agent model can be formulated as:

$$
\begin{equation*}
\mathcal{L}_{\text {agent }}\left(\pi_{\theta}\right)=-\mathbb{E}_{\tau_{w} \sim \mathcal{D}}\left[\pi_{\theta}\left(\tau_{w} \mid u, \kappa\right)\right] \tag{6}
\end{equation*}
$$

Suppose $\mathcal{X}=\left(x_{1}, x_{2}, \ldots, x_{|\mathcal{X}|}\right)$ is the token sequence of the trajectory $\tau_{w}$, we have:

$$
\begin{equation*}
\pi_{\theta}\left(\tau_{w} \mid u, \kappa\right)=-\sum_{j=1}^{\mid \mathcal{X |}}\left(\mathbb{1}\left(x_{j} \in \mathcal{A}\right) \times \log \pi_{\theta}\left(x_{j} \mid u, \kappa, x_{<j}\right)\right) \tag{7}
\end{equation*}
$$

Here $\mathbb{1}\left(x_{j} \in \mathcal{A}\right)$ is the indicator function to mask tokens unrelated to actions. Please note that $\tau_{w}$ here does not include the state knowledge mentioned in $\$ 3.2$.

World Knowledge Model Training. The main difference in the training data between the agent and knowledge model is the added state knowledge. Given the expert trajectories dataset with both task and state knowledge $\mathcal{D}^{\prime}=\left\{\left(u, \kappa, \tau_{w}^{\prime}\right)^{(i)}\right\}_{i=1}^{\left|\mathcal{D}^{\prime}\right|}$ where $\tau_{w}^{\prime}=\left(a_{0}, o_{0}, s_{0}, \ldots, a_{n}, o_{n}, s_{n}\right)$, the loss of the knowledge model $\pi_{\phi}$ can be formulated as:

$$
\begin{equation*}
\mathcal{L}_{\text {know }}\left(\pi_{\phi}\right)=-\mathbb{E}_{\kappa, \tau_{w}^{\prime} \sim \mathcal{D}^{\prime}}\left[\pi_{\phi}(\kappa \mid u) \pi_{\phi}\left(\tau_{w}^{\prime} \mid u, \kappa\right)\right] \tag{8}
\end{equation*}
$$

Suppose $\mathcal{X}^{\prime}=\left(x_{1}^{\prime}, x_{2}^{\prime}, \ldots, x_{\left|\mathcal{X}^{\prime}\right|}^{\prime}\right)$ is the token sequence of the expert trajectory with state knowledge $\tau_{w}^{\prime}$ and $\mathcal{Y}=\left(y_{1}, y_{2}, \ldots, y_{|\mathcal{Y}|}\right)$ represents the token sequence of the task knowledge $\kappa$, we have:

$$
\begin{align*}
\pi_{\phi}(\kappa \mid u) & =-\sum_{i=1}^{|\mathcal{Y}|} \log \pi_{\phi}\left(y_{i} \mid u, y_{<i}\right)  \tag{9}\\
\pi_{\phi}\left(\tau_{w}^{\prime} \mid u, \kappa\right) & =-\sum_{j=1}^{\left|\mathcal{X}^{\prime}\right|}\left(\mathbb{1}\left(x_{j}^{\prime} \in \mathcal{S}\right) \times \log \pi_{\phi}\left(x_{j}^{\prime} \mid u, \kappa, x_{<j}^{\prime}\right)\right) \tag{10}
\end{align*}
$$

where $\mathbb{1}\left(x_{j} \in \mathcal{S}\right)$ is the indicator function to mask tokens unrelated to state knowledge.

### 3.4 Agent Planning with World Knowledge Model

At inference time, the agent model plans on the evaluation tasks with the aid of the world knowledge model. We redefine the historical trajectory $h_{t}=\left(u, \kappa, a_{0}, o_{0}, a_{1}, o_{1}, \ldots, a_{t}, o_{t}\right)$. Given a specific task instruction $u$, the knowledge model first generates the task knowledge $\kappa \sim \pi_{\phi}(\cdot \mid u)$, then the agent model starts planning. Assuming the available action set $\mathcal{A}_{u} \subseteq \mathcal{A}$ for the task $u$ is $\left(\alpha_{u}^{(1)}, \alpha_{u}^{(2)}, \ldots, \alpha_{u}^{\left(\left|\mathcal{A}_{u}\right|\right)}\right)$, at any time $t \geq 0$, instead of directly generating a next action $a_{t+1} \in \mathcal{A}_{u}$ based on $h_{t}$, we first employ the world knowledge model to generate the current state knowledge $s_{t} \sim \pi_{\phi}\left(\cdot \mid h_{t}\right)$ and leverage $s_{t}$ to query the state knowledge base $\mathcal{B}=\left\{\left(s, a_{\text {pre }}, a_{\text {next }}\right)^{(i)}\right\}_{i=1}^{|\mathcal{B}|}$. With the state knowledge as the key, we retrieve $\mathcal{N}$ nearest triplets from where $a_{\text {pre }}=a_{t}$ based on semantic similarity and collect the corresponding next actions $a_{\text {next }}$. We count the probability of each action $p_{\text {know }}\left(\alpha_{u}^{(i)}\right)=\frac{\mathcal{N}_{i}}{\mathcal{N}}$, where $\mathcal{N}_{i}$ is the occurrence number of action $\alpha_{u}^{(i)}$ in all the collected $a_{\text {next }}$. Therefore, we get the probability acquired from the state knowledge base:

$$
\begin{equation*}
P_{\text {know }}\left(\mathcal{A}_{u}\right)=\left(p_{\text {know }}\left(\alpha_{u}^{(1)}\right), p_{\text {know }}\left(\alpha_{u}^{(2)}\right), \cdots, p_{\text {know }}\left(\alpha_{u}^{\left(\left|\mathcal{A}_{u}\right|\right)}\right)\right), \quad \sum_{i=1}^{\left|\mathcal{A}_{u}\right|} p_{\text {know }}\left(\alpha_{u}^{(i)}\right)=1 \tag{11}
\end{equation*}
$$

Afterward, we sample all the logits of $\alpha_{u}^{(i)}, 1 \leq i \leq\left|\mathcal{A}_{u}\right|$ from the agent model and apply a softmax function to normalize the probability. We define the probability acquired from the agent model as:

$$
\begin{equation*}
P_{\text {agent }}\left(\mathcal{A}_{u}\right)=\left(p_{\text {agent }}\left(\alpha_{u}^{(1)}\right), p_{\text {agent }}\left(\alpha_{u}^{(2)}\right), \cdots, p_{\text {agent }}\left(\alpha_{u}^{\left(\left|\mathcal{A}_{u}\right|\right)}\right)\right), \quad \sum_{i=1}^{\left|\mathcal{A}_{u}\right|} p_{\text {agent }}\left(\alpha_{u}^{(i)}\right)=1 \tag{12}
\end{equation*}
$$

Finally, we determine the next action by combining the above two probabilities:

$$
\begin{equation*}
a_{t+1}=\underset{\alpha_{u}^{(i)} \in \mathcal{A}_{u}, 1 \leq i \leq\left|\mathcal{A}_{u}\right|}{\arg \max }\left(\gamma \cdot p_{\text {agent }}\left(\alpha_{u}^{(i)}\right)+(1-\gamma) \cdot p_{\text {know }}\left(\alpha_{u}^{(i)}\right)\right) \tag{13}
\end{equation*}
$$

where $\gamma$ is the hyperparameter that controls the proportion of $P_{\text {agent }}\left(\mathcal{A}_{u}\right)$. Based on the above, we enhance the agent planning by global guidance from task knowledge and local constraints from state knowledge generated by our WKM. Due to the WKM and retrieval, the inference stage incurs additional time overhead compared to the pure agent model. The approximate ratio is around 2.5:1.

Table 1: Main Results. The best results are marked in bold and the second-best results are marked with underline. All the prompt-based baselines ( $\boldsymbol{\Phi}$ ) are evaluated under one-shot prompting and all the fine-tuningbased baselines ( $\mathbf{O}$ ) are trained through LoRA. Red represents the changes of WKM relative to the optimal results in the baselines. WKM and agent model are different LoRAs sharing the same backbone.

| Backbone | Method | ALFWorld |  | WebShop | ScienceWorld |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Seen | Unseen |  | Seen | Unseen |
| GPT-3.5-Turbo <br> GPT-4 | $\boldsymbol{\infty}$ REACT | 8.57 <br> 44.29 | 5.97 <br> 38.05 | 44.37 <br> 62.76 | 15.41 <br> 67.32 | 13.99 <br> 65.09 |
| Mistral-7B | $\mathbf{\Phi}$ REACT <br> $\boldsymbol{\infty}$ Reflexion <br> O NAT <br> $\mathbf{O}$ ETO <br> $\mathbf{0}$ KNOWAGENT | 7.86 <br> 11.56 <br> 64.43 <br> 66.84 <br> 70.44 | 5.22 <br> 6.00 <br> 68.96 <br> 71.43 <br> 70.72 | 14.63 <br> 16.64 <br> 61.01 <br> 64.09 <br> 61.28 | 20.72 <br> 21.07 <br> 57.12 <br> 58.17 <br> 59.32 | 17.65 <br> 18.11 <br> 50.79 <br> 51.85 <br> 47.24 |
|  | $\mathbf{W K M}$ | $\mathbf{7 3 . 5 7}+3.13$ | $76.87+5.44$ | $\mathbf{6 5 . 4 8}+1.39$ | $\mathbf{6 2 . 1 2}+2.80$ | $\mathbf{5 3 . 6 2}+1.77$ |
| Gemma-7B | $\boldsymbol{D}$ REACT <br> $\mathbf{D}$ Reflexion <br> $\mathbf{O}$ NAT <br> $\mathbf{O}$ ETO <br> $\mathbf{O}$ KNOWAGENT | 6.43 <br> 7.14 <br> 67.86 <br> 66.43 <br> 69.29 | 2.24 <br> 2.99 <br> 65.88 <br> $\underline{68.66}$ <br> 67.60 | 5.93 <br> 7.71 <br> 55.82 <br> $\underline{62.67}$ <br> 58.80 | 3.58 <br> 4.94 <br> 47.63 <br> 50.44 <br> 48.55 | 3.51 <br> 3.93 <br> 44.98 <br> 47.84 <br> 45.28 |
|  | $\mathbf{W K M}$ | $\mathbf{7 0 . 7 1}+1.42$ | $\mathbf{7 0 . 4 0}+1.74$ | $\mathbf{6 3 . 7 5}+1.08$ | $\mathbf{5 3 . 6 8}+3.24$ | $49.24+1.40$ |
| Llama-3-8B | $\mathbf{D}$ REACT <br> $\boldsymbol{\Phi}$ Reflexion <br> $\boldsymbol{O}$ NAT <br> $\mathbf{O}$ ETO <br> $\mathbf{O}$ KNOWAGENT | 2.86 <br> 4.29 <br> 60.71 <br> 64.29 <br> 66.71 | 3.73 <br> 4.48 <br> 59.70 <br> 64.18 <br> 62.69 | 19.32 <br> 22.73 <br> 61.60 <br> 64.57 <br> 64.40 | 24.76 <br> 27.23 <br> 55.24 <br> 57.90 <br> 58.67 | 22.66 <br> 25.41 <br> 48.76 <br> 52.33 <br> 49.18 |
|  | WKM | $\mathbf{6 8 . 5 7}+1.86$ | $\mathbf{6 5 . 9 3}+1.75$ | $\mathbf{6 6 . 6 4}+2.07$ | $\mathbf{6 0 . 1 2}+1.55$ | $\mathbf{5 4 . 7 5}+2.42$ |

## 4 Experiments

### 4.1 Experimental Settings

Datasets and Metrics. We evaluate our method on three real-world simulated planning datasets: ALFWorld [41], WebShop [53], and ScienceWorld [50]. AlFWorld and ScienceWorld include unseen tasks to evaluate the agent's generalization ability. The reward of ALFWorld is binary 0 or 1 , indicating whether the agent has completed the task or not. WebShop and ScienceWorld provide dense rewards from 0 to 1 to measure the completion level of the task. For all the datasets, we apply average reward as the final metrics. Please refer to Appendix B for detailed dataset information.

Models and Baselines. We evaluate on three state-of-the-art open-source models: 1) Mistral-7B [16], the Mistral-7B-Instruct-v0.2 version. 2) Gemma-7B [24], the Gemma-1.1-7B-it version. 3) Llama-3-8B [25], the Meta-Llama-3-8B-Instruct version. We compare our method with two promptbased baselines: ReAct [54] and Reflexion [40]. Besides, we adopt two strong baselines that introduce rejected trajectories into the training process to learn from experience: NAT [49], learn from rejected trajectories through SFT, and ETO [44], learn from rejected trajectories through DPO [36]. Moreover, we compare with a knowledge-augmented planning method KNOWAGENT. We also include ChatGPT (gpt-3.5-turbo-0125) [27] and GPT-4 (gpt-4-32K-0613) for comparison. All the prompt-based baselines are tested under one-shot and all the fine-tuning-based baselines are trained with LoRA [12]. Please refer to Appendix C]for baselines and re-producing details.

Training and Inference Setups. We fine-tune the proposed approach with LoRA [12] using the LlamaFactory [62] framework. The learning rate is 1e-4 and the sequence length is 2048 for all the models. The training epoch is 3 and the batch size is 32 . We adopt the AdamW optimizer [22] with a cosine learning scheduler. During inference, the number of retrieved action-state-action triplets $\mathcal{N}$ is set to 3000 and the $P_{\text {agent }}\left(\mathcal{A}_{u}\right)$ weight $\gamma$ is set to $\{0.4,0.5,0.7\}$. All the training and inference experiments are conducted on 8 NVIDIA V100 32G GPUs within 12 hours. Please refer to Appendix $\mathrm{D}$ for detailed hyperparameters used in our paper.

![](https://cdn.mathpix.com/cropped/2024_06_04_54fac0272c4f81ee7ba4g-07.jpg?height=366&width=1306&top_left_y=245&top_left_x=385)

Figure 3: Ablation Study on Mistral-7B. w/o all means the vanilla experienced agent model training with pure expert trajectories. w/ state is testing agent model with only state knowledge base constraints. w/ task stands for guiding agent model with only task knowledge. w/ task\&state is our WKM with both task knowledge guidance and state knowledge constraints.

Table 2: Average Steps. The maximum number of steps in ALFWorld and WebShop is 40 and 10. In ScienceWorld, the number of steps ranges from 10 to 120 depending on the task type, with an average of around 40 .

| Method | ALFWorld |  | WebShop | ScienceWorld |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | Seen | Unseen |  | Seen | Unseen |
| NAT | 23.27 | 23.42 | 4.08 | 20.18 | 21.21 |
| ETO | 19.82 | 22.29 | 3.99 | 24.13 | 26.35 |
| KNOWAGENT | 18.51 | 24.56 | 4.01 | 21.06 | 24.74 |
| WKM | $\mathbf{1 7 . 6 6}$ | $\mathbf{1 7 . 9 2}$ | $\mathbf{3 . 9 7}$ | $\mathbf{1 8 . 7 4}$ | $\mathbf{1 9 . 5 9}$ |

Table 3: Hallucinatory Action Rates on ALFWorld. We calculate the proportion of trajectories containing invalid actions regardless of their correctness.

| Method | ALFWorld |  |
| :--- | :---: | :---: |
|  | Seen | Unseen |
| NAT | $45.71 \%$ | $50.00 \%$ |
| ETO | $34.29 \%$ | $36.57 \%$ |
| KNOWAGENT | $33.57 \%$ | $44.78 \%$ |
| WKM | $\mathbf{3 2 . 8 6 \%}$ | $\mathbf{2 9 . 8 5 \%}$ |

### 4.2 Results

Main Results. As shown in Table 1, for prompt-based baselines on open-source models, both REACT and Reflexion exhibit poor performance, far behind our method and fine-tuning-based baselines on various datasets. GPT-3.5-Turbo performs ordinarily on two datasets other than WebShop, and it even falls behind Mistral-7B and Llama-3-8B's REAct performance on ScienceWorld. However, GPT-4 exhibits strong performance across various datasets. Nevertheless, our approach, through LoRA training alone, surpasses GPT-4 on ALFWorld $(44.29 \rightarrow 73.57$ on seen, $38.05 \rightarrow 76.87$ on unseen) and WebShop ( $62.76 \rightarrow$ 66.64). For fine-tuning-based baselines, both NAT and ETO fall behind our method, implying that just integrating world knowledge for agent models is worth more than further fussy SFT or DPO on negative examples. Our method also performs better than KNOwAGENT which brings human-designed fixed action knowledge and long action paths into trajectories. This suggests the effectiveness of our WKM which is responsible for generating instance-level task knowledge and maintaining implicit action constraints. Furthermore, KNOWAGENT's performance on unseen tasks is not as impressive as on seen tasks, while WKM can keep its advantage. This phenomenon also demonstrates the generalization ability of WKM.

Approach Ablations. As shown in Figure 3, taking Mistral-7B as an example, we decompose the key components of WKM to examine the roles of the task and state knowledge separately. In a macro view, removing each module results in a clear drop in the agent's performance, which validates the power of our world knowledge. Furthermore, the improvement through task knowledge ( $w$ / task) is more pronounced than that through state knowledge ( $w /$ state), suggesting the necessity of global prior knowledge for agent planning. A more micro observation reveals that the impact of state knowledge is more significant on seen tasks compared to unseen tasks, while the influence of task knowledge is sustainable across seen and unseen tasks. This may be attributed that although our real-time state knowledge is generated by WKM, the state knowledge base is built on the training set, which may weaken generalization to some extent.

### 4.3 Analysis

World knowledge can mitigate blind trial-and-error and reduce hallucinatory actions. We compare the number of planning steps for each dataset between three strong baselines and WKM and calculate the average steps of each method. As depicted in Figure 10 (in Appendix), WKM
demonstrates the ability to complete a significant proportion of tasks using the shortest trajectory, indicating that guidance from world knowledge can effectively reduce the agent's blind trial-and-error in the environment. Taking a further perspective from an average standpoint in Table 2, it can be observed that WKM exhibits lower average planning steps compared to other baselines. As ALFWorld can respond to invalid actions, in Table 3 , we count the percentage of hallucinatory actions that occurred in trajectories from ALFWorld for each method. The results confirm the effectiveness of our world knowledge model to decrease hallucinatory actions. Furthermore, it is worth noting that most baselines show a prominent increase in the average number of steps and percentage of invalid actions when transitioning from seen tasks to unseen tasks, but WKM can still maintain a relatively low level. This reflects laterally that our world knowledge can still effectively guide the agent model on unseen tasks, highlighting the knowledge generalization brought by the world knowledge model. To see how our world knowledge works, please refer to our case study in Appendix F.

Our instance-level knowledge can generalize better to unseen tasks. To further explore the benefit of using a knowledge model to generate instance-level task knowledge, we carefully survey the task knowledge generated by our WKM and abstract it into dataset-level knowledge for each dataset. Then we retrain the agent model to adapt to new dataset-level knowledg $5^{5}$. As illustrated in Figure 4 we compare the performance of dataset-level knowledge with our instance-level task knowledge (WKM w/o state) on ALFWorld and ScienceWorld. It can be observed that our model-generated instancelevel knowledge not only surpasses human-designed knowledge on seen tasks but also exhibits even more remarkable performance on unseen tasks, with the improvement in performance on unseen tasks significantly greater than that on seen tasks. This phenomenon straightly reflects the strong generalization ability of our knowledge model compared to rigidly designed knowledge by humans.

![](https://cdn.mathpix.com/cropped/2024_06_04_54fac0272c4f81ee7ba4g-08.jpg?height=358&width=439&top_left_y=786&top_left_x=1320)

Figure 4: Performance of humandesigned dataset-level knowledge compared to WKM generated instance-level knowledge.
Table 4: Weak-guide-strong. The knowledge model here is based on Mistral-7B.

| Backbone | Method | ALFWorld |  |
| :---: | :--- | :---: | :---: |
|  |  | Seen | Unseen |
| GPT-3.5-Turbo | REACT | 8.57 | 5.97 |
|  | WKM w/o state | $\mathbf{1 2 . 8 6}$ | $\mathbf{8 . 9 6}$ |
| GPT-4 | REACT | 44.29 | 38.05 |
|  | WKM w/o state | $\mathbf{5 0 . 7 1}$ | $\mathbf{4 7 . 0 1}$ |

Weak knowledge model guides strong agent model planning. In our main experiments, the knowledge model and agent model are based on the same backbone. Here, we explore on ALFWorld what will happen if we use a weak knowledge model to guide a strong agent model. We choose Mistral-7B as the backbone of the knowledge model and ChatGPT and GPT-4 as the agent model. Since we cannot get the token distribution from OpenAI API, we only apply task knowledge to the agent model. As exhibited in Table 4 the results of both ChatGPT and GPT-4 show distinct advances after being guided by the Mistral-7B world knowledge model, indicating the weak world knowledge model also contains knowledge that the strong model may lack. In the era of LLMs, this inspires us with a new agent learning paradigm: weak-guide-strong. Due to its lightweight nature, the weak knowledge model can flexibly adjust its parameters based on the needs of the agent model, which can address the difficulty of large agent models in adapting to new environments through fine-tuning.

Unified World Knowledge Model Training. We mix the world knowledge collected from all three datasets and jointly train one single world knowledge model to investigate the effect of multi-task world knowledge learning. Figure 5 illustrates the relative performance comparison between multi-task WKM and various baselines, from which we can observe that multi-task WKM not only does not lead to performance degradation but also exhibits visible improvements compared to single-task WKM, especially on WebShop and ScienceWorld. Similar to [57, 58, 3] which endeavor to train a unified agent model and achieve strong generalization ability to held-out tasks, this observation inspires us with the potential of training a unified world knowledge model that can be applied to help

![](https://cdn.mathpix.com/cropped/2024_06_04_54fac0272c4f81ee7ba4g-08.jpg?height=371&width=439&top_left_y=1923&top_left_x=1320)

Figure 5: Relative performance of multi-task WKM compared to various baselines.[^2]various held-in agent models and also generalize to guide held-out agent models. A more daring idea is whether a unified agent model combined with a unified world knowledge model is the key to Artificial General Intelligence (AGI).

Explicit state knowledge will hurt the planning performance. To demonstrate the rationality of our choice to construct a state knowledge base, we explore the effect of directly incorporating state knowledge into the context of the agent model (we retrain the agent model to follow both the task and state knowledge), as shown in Figure 6. The performance of explicit state knowledge is far inferior to our approach of retrieving from a state knowledge base and utilizing probabilistic constraints. It even performs worse than when we remove state knowledge and only include task knowledge. This clearly indicates that blindly extending prompts with a large amount of explicit natural language feedback is lose-more-than-gain for agent planning, and implicit knowledge constraints may be sometimes more prudent.

## 5 Related Work

LLM Agents. LLMs have emerged as a promising avenue towards unlocking the potential of Artificial General Intelligence, offering robust support for the development of agent systems 48,51 8, 63]. Existing works in this field mainly focuses on agent planning [14, 21, 54, 42], external tools harnessing [39, 23, 43, 29, 32, 35, 46], code generation [45, 21, 31, 11], etc. Recently, there has been an increasing focus on endowing open-source LLMs with agent functionalities through fine-tuning [2, 57, 56, 38, 44, 49]. However, these approaches rely on blindly fitting the probabilities of tokens to learn planning, without having an intimate cognition of the environment. The lack of knowledge can lead to the agent blindly attempting trial-and-error and generating hallucinatory actions.

Knowledge Augmented Agent Planning. Planning [15] is a crucial capability for intelligent agents to accomplish real-world tasks, often requiring agents to possess rich knowledge and environmental commonsense. Few works have explored the field of knowledge-augmented agent planning. [14, 61 , 5] utilize the rich parametric knowledge stored in pre-trained language models to assist agent planners. [7, 20, 59, 64] design structured or natural language knowledge to regulate the actions. However, the above studies require the manual design of fixed prompt templates or task procedures, making it challenging to transfer across different task environments. [63, 55, 6] propose the automation of knowledge generation using language models. However, their knowledge either consists of only global workflow or only local action principles. In contrast, we train our world knowledge model both on global task knowledge and local state knowledge to assist agent planning, and these knowledge sources are derived from the model's self-summary rather than hand-curated.

LLM-based World Model. World model and agent model often co-occur in the domain of reinforcement learning and robotics [13, 9, 19, 37, 26, 4]. With LLMs commonly deemed as the most powerful intelligent machines constructed by humans thus far, the LLM-backed world models have been proposed [61, 10, 13]. In our paper, we attempt to self-synthesize world knowledge and train to obtain a world knowledge model. However, we consider our model to be a world knowledge model rather than a world model based on the reason that our model is temporarily unable to utilize search algorithms (e.g. MCTS) in conjunction with the agent model to make predictions about the world and we leave this for our future work.

## 6 Conclusion and Future Work

In this paper, we strive to develop a parametric world knowledge model (WKM) to augment language agent model planning. Our WKM can generate prior task knowledge to guide global planning as well as dynamic state knowledge to regulate local planning. Our extensive results show that our world knowledge can work on both GPT-4 and state-of-the-art open-source models and achieve superior performance compared to various strong baselines. Analytical experiments validate that our WKM can 1) reduce brainless trial-and-error and invalid actions, 2) generalize better to unseen tasks, 3) achieve weak-guide-strong, and 4) be effectively extended to unified world knowledge training.

Potential future directions include: 1) building a unified world knowledge model, 2) learning to predict the world like a world model, 3) applying to multi-modal agent planning, etc.

## Limitations

Despite our best efforts, this paper still have some limitations: 1) Our primary intention behind designing the WKM is to compensate for the lack of world knowledge in the agent model. However, determining what a language model knows and doesn't know has been an ongoing challenge that remains unresolved. 2) It is widely acknowledged that world knowledge extends beyond textual representations. While our world knowledge is currently limited to textual information, exploring multi-modal world knowledge models is indeed one of our important future tasks. 3) Our world knowledge model cannot dynamically update with the changes of the world and feedback from the agent. 4) Generating world knowledge can introduce additional inference overhead.

## References

[1] Robert Eamon Briscoe. Mental imagery and the varieties of amodal perception. Pacific Philosophical Quarterly, 92(2):153-173, 2011.

[2] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. CoRR, abs/2310.05915, 2023.

[3] Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. CoRR, abs/2403.12881, 2024.

[4] Anna Dawid and Yann LeCun. Introduction to latent variable energy-based models: A path towards autonomous machine intelligence. CoRR, abs/2306.02572, 2023.

[5] Yan Ding, Xiaohan Zhang, Saeid Amiri, Nieqing Cao, Hao Yang, Andy Kaminski, Chad Esselink, and Shiqi Zhang. Integrating action knowledge and llms for task planning and situation handling in open worlds. Auton. Robots, 47(8):981-997, 2023.

[6] Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak Lee. Autoguide: Automated generation and selection of state-aware guidelines for large language model agents. CoRR, abs/2403.08978, 2024.

[7] Jian Guan, Wei Wu, Zujie Wen, Peng Xu, Hongning Wang, and Minlie Huang. AMOR: A recipe for building adaptable modular knowledge agents through process feedback. CoRR, $\mathrm{abs} / 2402.01469,2024$.

[8] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges. CoRR, abs/2402.01680, 2024.

[9] David Ha and Jürgen Schmidhuber. World models. CoRR, abs/1803.10122, 2018.

[10] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 8154-8173. Association for Computational Linguistics, 2023.

[11] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework. CoRR, abs/2308.00352, 2023.

[12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.

[13] Zhiting Hu and Tianmin Shu. Language models, agent models, and world models: The LAW for machine reasoning and planning. CoRR, abs/2312.05230, 2023.

[14] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 9118-9147. PMLR, 2022.

[15] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of LLM agents: A survey. CoRR, $\mathrm{abs} / 2402.02716,2024$.

[16] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023.

[17] Philip N Johnson-Laird. Mental models and human reasoning. Proceedings of the National Academy of Sciences, 107(43):18243-18250, 2010.

[18] Philip Nicholas Johnson-Laird. Mental models: Towards a cognitive science of language, inference, and consciousness. Harvard University Press, 1983.

[19] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for atari. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.

[20] Zelong Li, Wenyue Hua, Hao Wang, He Zhu, and Yongfeng Zhang. Formal-1lm: Integrating formal language and natural language for controllable llm-based agents. CoRR, abs/2402.00798, 2024

[21] Lajanugen Logeswaran, Yao Fu, Moontae Lee, and Honglak Lee. Few-shot subgoal planning with language models. In Marine Carpuat, Marie-Catherine de Marneffe, and Iván Vladimir Meza Ruíz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 5493-5506. Association for Computational Linguistics, 2022.

[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.

[23] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.

[24] Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, and et al. Gemma: Open models based on gemini research and technology. CoRR, abs/2403.08295, 2024.

[25] Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. https: //ai.meta.com/blog/meta-llama-3/.

[26] Thomas M. Moerland, Joost Broekens, Aske Plaat, and Catholijn M. Jonker. Model-based reinforcement learning: A survey. Found. Trends Mach. Learn., 16(1):1-118, 2023.

[27] OpenAI. Chatgpt: Optimizing language models for dialogue, 2022. https://openai. com/blog/chatgpt/.

[28] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.

[29] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. CoRR, abs/2305.15334, 2023.

[30] RT Pramod, Michael Cohen, Kirsten Lydic, Josh Tenenbaum, and Nancy Kanwisher. Evidence that the brain's physics engine runs forward simulations of what will happen next. Journal of Vision, 20(11):1521-1521, 2020.

[31] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. CoRR, abs/2307.07924, 2023.

[32] Shuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu Zhang. Making language models better tool learners with execution feedback. CoRR, abs/2305.13068, 2023.

[33] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. Reasoning with language model prompting: A survey. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 5368-5393. Association for Computational Linguistics, 2023.

[34] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. AUTOACT: automatic agent learning from scratch via self-planning. CoRR, abs/2401.05268, 2024.

[35] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. CoRR, abs/2307.16789, 2023.

[36] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 16, 2023, 2023.

[37] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. Nat., 588(7839):604-609, 2020.

[38] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. Small llms are weak tool learners: A multi-llm agent. CoRR, $\mathrm{abs} / 2401.07324,2024$.

[39] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving AI tasks with chatgpt and its friends in hugging face. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.

[40] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.

[41] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.

[42] Chan Hee Song, Brian M. Sadler, Jiaman Wu, Wei-Lun Chao, Clayton Washington, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 2986-2997. IEEE, 2023.

[43] Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. Restgpt: Connecting large language models with real-world applications via restful apis. CoRR, abs/2306.06624, 2023 .

[44] Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization for LLM agents. CoRR, abs/2403.02502, 2024.

[45] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.

[46] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. CoRR, $\mathrm{abs} / 2306.05301,2023$.

[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, and et al. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023.

[48] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on large language model based autonomous agents. Frontiers Comput. Sci., 18(6):186345, 2024.

[49] Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, and Timothy Baldwin. Learning from failure: Integrating negative examples when fine-tuning large language models as agents. CoRR, abs/2402.11651, 2024.

[50] Ruoyao Wang, Peter A. Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11279-11298. Association for Computational Linguistics, 2022.

[51] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based agents: A survey. CoRR, abs/2309.07864, 2023.

[52] Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 $16,2023,2023$.

[53] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information

Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.

[54] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.

[55] Yining Ye, Xin Cong, Shizuo Tian, Jiannan Cao, Hao Wang, Yujia Qin, Yaxi Lu, Heyang Yu, Huadong Wang, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Proagent: From robotic process automation to agentic process automation. CoRR, abs/2311.10751, 2023.

[56] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. Lumos: Learning agents with unified data, modular design, and open-source llms. CoRR, abs/2311.05657, 2023.

[57] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. CoRR, abs/2310.12823, 2023.

[58] Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, and Caiming Xiong. Agentohana: Design unified data and training pipeline for effective agent learning. CoRR, abs/2402.15506, 2024.

[59] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: LLM agents are experiential learners. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, ThirtySixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 19632-19642. AAAI Press, 2024.

[60] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. CoRR, abs/2303.18223, 2023.

[61] Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.

[62] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. CoRR, abs/2403.13372, 2024.

[63] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous language agents. CoRR, abs/2309.07870, 2023.

[64] Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, and Huajun Chen. Knowagent: Knowledge-augmented planning for llm-based agents. CoRR, abs/2403.03101, 2024.
