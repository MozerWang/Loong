# Weak-to-Strong Extrapolation Expedites Alignment 

Chujie Zheng ${ }^{1,2 *}$ Ziqi Wang ${ }^{3} \quad$ Heng Ji $^{3} \quad$ Minlie Huang ${ }^{1 \dagger}$ Nanyun Peng $^{2 \dagger}$<br>${ }^{1}$ The CoAI Group, DCST, BNRist, Tsinghua University<br>${ }^{2}$ University of California, Los Angeles ${ }^{3}$ University of Illinois Urbana-Champaign<br>chujiezhengchn@gmail.com aihuang@tsinghua.edu.cn violetpeng@cs.ucla.edu


#### Abstract

The open-source community is experiencing a surge in the release of large language models (LLMs) that are trained to follow instructions and align with human preference. However, further training to improve them still requires expensive computational resources and data annotations. Is it possible to bypass additional training and cost-effectively acquire better-aligned models? Inspired by the literature on model interpolation, we propose a simple method called EXPO to boost LLMs' alignment with human preference. Utilizing a model that has undergone alignment training (e.g., via DPO or RLHF) and its initial SFT checkpoint, EXPO directly obtains a better-aligned model by extrapolating from the weights of the initial and the aligned models, which implicitly optimizes the alignment objective via first-order approximation. Through experiments with twelve open-source LLMs on HuggingFace, we demonstrate that EXPO consistently improves off-the-shelf DPO/RLHF models, as evaluated on the mainstream LLM benchmarks AlpacaEval 2.0 and MT-Bench. Moreover, ExPO exhibits remarkable scalability across various model sizes (from 1.8B to 70B) and capabilities. Through controlled experiments and further empirical analyses, we shed light on the essence of EXPO amplifying the reward signal learned during alignment training. Our work demonstrates the efficacy of model extrapolation in expediting the alignment of LLMs with human preference, suggesting a promising direction for future research.


![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-01.jpg?height=643&width=1347&top_left_y=1611&top_left_x=386)

AlpacaEval 2.0 LC Win Rate (\%) over GPT-4

![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-01.jpg?height=582&width=680&top_left_y=1666&top_left_x=403)

MT-Bench Score (1-10)

![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-01.jpg?height=607&width=621&top_left_y=1645&top_left_x=1099)

Figure 1: With no additional training, ExPO remarkably improves off-the-shelf DPO/RLHF models on HuggingFace across various model sizes and capabilities, as evaluated on two leading LLM benchmarks: AlpacaEval 2.0 [30] (left) and MT-Bench [61] (right). See Table 1]for full results.[^0]

## 1 Introduction

Over the past year, the open-source community has witnessed explosive growth in large language models (LLMs). These powerful LLMs, typically with billions of parameters, are trained to follow instructions and align with human preference [40, 38, 4]. Although the open weights of LLMs facilitate out-of-the-box use, further training to improve their performance usually requires expensive computational resources and additional data annotations. Is it possible to bypass additional training and cost-effectively acquire better-aligned models?

We draw inspiration from the literature on model interpolation, also known as model/weight averaging. This technique merges different models fine-tuned from the same base model by interpolating between their weights [51, 24, 54], relying on the mode connectivity of neural networks [17, 15]. Previous work observes that while model interpolation can integrate the respective strengths of different models to improve out-of-distribution generalization, it usually results in in-between performance compared to the original ones [24, 33, 54]. We similarly observe this phenomenon when interpolating between a supervised fine-tuned (SFT) model and a model further trained by direct preference optimization (DPO) [42] or reinforcement learning from human feedback (RLHF) [63], as shown in Figure 2.

![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-02.jpg?height=507&width=1131&top_left_y=917&top_left_x=497)

Figure 2: Calculating the reward scores ( $\$ 3.1$ on the UltraFeedback [12] development set, we observe that model interpolation usually gives trade-off performance between the two original models (e.g., an SFT model and a further-trained DPO/RLHF model). This observation motivates our proposal of ExPO, which cheaply acquires a better-aligned (stronger) model from a DPO/RLHF model and its initial SFT checkpoint (i.e., two relatively weaker models) via model extrapolation.

Intrigued by the observations of model interpolation, we turn to another compelling but unexplored direction: What if we consider a DPO/RLHF model as the interpolated result from the initial SFT model and a hypothetically better-aligned model? If this hypothetical model exists, we can straightforwardly obtain its weights by reversely extrapolating from the weights of the SFT and the DPO/RLHF models, as indicated by the gray arrow in Figure 2. This can potentially further improve many off-the-shelf DPO/RLHF-aligned LLMs without any additional training.

Building upon the above assumption, we propose a simple method called ExPO (model extrapolation) to boost LLMs' alignment with human preference ( $\$ 2$. Utilizing a model $\mathcal{M}_{1}$ that has undergone alignment training (e.g., via DPO or RLHF) and the SFT model $\mathcal{M}_{0}$ that initializes $\mathcal{M}_{1}$, EXPO directly extrapolates a better-aligned (stronger) model $\mathcal{M}_{2}$ from the weights of the two relatively weaker models $\mathcal{M}_{1}$ and $\mathcal{M}_{0}$.

Despite its simplicity, we demonstrate the impressive efficacy of EXPO through extensive empirical experiments, as summarized in Figure 1. Through experiments with twelve open-source LLMs on HuggingFace, we show that EXPO consistently improves off-the-shelf DPO/RLHF models, by up to $4.5 \%$ on AlpacaEval 2.0 [30] and 0.66 on MT-Bench [61] (\$3). Moreover, ExPO also manifests remarkable scalability across various model sizes (from 1.8B to 70B) and capabilities. We further conduct controlled experiments to shed light on how ExPO amplifies the reward signal learned during $\mathcal{M}_{1}$ 's alignment training, where we show that ExPO can boost models trained with less preference data (e.g., $10 \%$ or $20 \%$ ) to compete and even outperform the fully-trained one ( $\S$ 4). Our work demonstrates model extrapolation as a promising method for expediting the alignment of LLMs with human preference, and we believe it deserves more exploration in future research.

## 2 Methodology

### 2.1 Overview

Our proposed ExPO method is inspired by the observation in Figure 2 and the mode connectivity of neural networks [17, 15, 18]. Formally, we denote that a language model $\mathcal{M}_{1}$ (parameterized by $\boldsymbol{\theta}_{1}$ ) has undergone training for human preference alignment (e.g., via DPO [42] or RLHF [63]). We denote its corresponding SFT checkpoint as $\mathcal{M}_{0}$ (parameterized by $\boldsymbol{\theta}_{0}$ ), which is used for initializing $\mathcal{M}_{1}$. We denote the model's parameter space as $\Theta$ and suppose that the alignment level can be quantified by a continuous scalar function $\Omega: \Theta \rightarrow \mathbb{R}$, where higher $\Omega(\boldsymbol{\theta})$ indicates better alignment with human preference. ExPO assumes that there exists a better-aligned model $\mathcal{M}_{2}$ (parameterized by $\boldsymbol{\theta}_{2}$ ) that satisfies $\Omega\left(\boldsymbol{\theta}_{0}\right)<\Omega\left(\boldsymbol{\theta}_{1}\right)<\Omega\left(\boldsymbol{\theta}_{2}\right)$, and an interpolation coefficient $\gamma \in[0,1]$ such that $\boldsymbol{\theta}_{1}=(1-\gamma) \boldsymbol{\theta}_{0}+\gamma \boldsymbol{\theta}_{2}$. Here, we consider the simplest form of uniform linear interpolation, as we find it can already work well. With the substitution of $\alpha=1 / \gamma-1 \in[0,+\infty)$, ExPO obtains the assumed better-aligned (stronger) model $\mathcal{M}_{2}$ by extrapolating from the weights of the two (relatively weaker) models $\mathcal{M}_{1}$ and $\mathcal{M}_{0}$ (i.e., weak-to-strong extrapolation), formulated as follows:

$$
\begin{equation*}
\boldsymbol{\theta}_{2}=(1+\alpha) \boldsymbol{\theta}_{1}-\alpha \boldsymbol{\theta}_{0}=\boldsymbol{\theta}_{1}+\alpha\left(\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_{0}\right)=\boldsymbol{\theta}_{1}+\alpha \Delta \theta \tag{1}
\end{equation*}
$$

where the coefficient $\alpha$ serves as the hyperparameter that controls the extrapolation length. In practice, $\alpha$ can be easily tuned as a decoding hyperparameter (similar to the sampling temperature). This requires only one 24GB GPU for 7B LLMs (e.g., half-precision inference with vllm [27]), which, however, is far from sufficient for model training.

### 2.2 Explanation and Insights

Theoretically, EXPO takes first-order approximation to implicitly optimize the alignment objective $\Omega(\boldsymbol{\theta})$. Note that alignment algorithms typically include the regularization term (e.g., the KL constraint in RLHF) that restricts $\boldsymbol{\theta}_{1}$ within the small vicinity of $\boldsymbol{\theta}_{0}$ (i.e., $|\Delta \boldsymbol{\theta}|$ is small), and we can also control $\alpha$ such that $|\alpha \Delta \boldsymbol{\theta}| \ll\left|\boldsymbol{\theta}_{1}\right|$. We can apply first-order Taylor Expansion and have:

$$
\begin{equation*}
\Omega\left(\boldsymbol{\theta}_{1}+\alpha \Delta \boldsymbol{\theta}\right) \approx \Omega\left(\boldsymbol{\theta}_{1}\right)+\alpha \nabla \Omega\left(\boldsymbol{\theta}_{1}\right) \cdot \Delta \boldsymbol{\theta} \tag{2}
\end{equation*}
$$

Therefore, $\Omega\left(\boldsymbol{\theta}_{2}\right)=\Omega\left(\boldsymbol{\theta}_{1}+\alpha \Delta \boldsymbol{\theta}\right)>\Omega\left(\boldsymbol{\theta}_{1}\right)$ holds if the

![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-03.jpg?height=271&width=574&top_left_y=1092&top_left_x=1182)

Figure 3: Illustrative 1D diagram of $\Omega(\boldsymbol{\theta})$. EXPO can be viewed as a "global gradient update" along $\Delta \theta$. It essentially amplifies the reward signal learned during alignment training. gradient of $\Omega$ at $\boldsymbol{\theta}_{1}$ has a positive component along $\Delta \boldsymbol{\theta}$ (as long as $\Omega$ is not locally maximum at $\boldsymbol{\theta}_{1}$ ). This can generally be satisfied, as we can reasonably assume $\Omega$ to monotonically increase from $\boldsymbol{\theta}_{0}$ to $\boldsymbol{\theta}_{1}$ during alignment training, as illustrated in Figure 3

The above assumption also implies that the alignment training from $\mathcal{M}_{0}$ to $\mathcal{M}_{1}$ is suboptimal, as shown in Figure 3 We conjecture this is very likely to occur in practice due to both the regularization in alignment algorithms (e.g., the KL constraint in RLHF) and the sparsity of reward signal. For the latter, since it is intractable to directly optimize $\Omega(\boldsymbol{\theta})$, the dominant practice is to first employ another reward model to assign preference labels (e.g., in DPO) or reward values (e.g., in RLHF) to the language model $\mathcal{M}$ 's outputs, on which we then train $\mathcal{M}$. However, transmitting the reward signal through the intermediate discrete, textual outputs can make the learned reward signal noisy or sparse, which consequently hinders the optimal alignment training. We will show in $\S 3$ that the open-source DPO/RLHF models usually have significant room for further improvement by ExPO.

Additionally, Figure 3 provides a more intuitive illustration of ExPO. Specifically, ExPO can be viewed as a "global gradient update" along the weight change $\Delta \theta$. Note that starting from $\boldsymbol{\theta}_{0}, \Delta \boldsymbol{\theta}$ indicates a direction in the parameter space in which the alignment level $\Omega$ with human preference increases. Therefore, EXPO essentially amplifies the learned reward signal through the extrapolation $\alpha \Delta \theta$. This insight underscores the importance of the "quality" of $\Delta \boldsymbol{\theta}$, i.e., $\Delta \boldsymbol{\theta}$ should indicate a direction that truly improves the alignment with human preference. Otherwise, ExPO could also amplify the learned spurious features in $\Delta \boldsymbol{\theta}$. We will provide more empirical analyses in $\S 4$ to show that the "quality" of $\Delta \boldsymbol{\theta}$ can vary depending on the training configuration for $\mathcal{M}_{1}$.

### 2.3 Highlights

We underline the following appealing properties of ExPO:

- Simplicity: ExPO is extremely simple and quick to implement. It merely involves performing extrapolation based on the weights of two checkpoints $\mathcal{M}_{0}$ and $\mathcal{M}_{1}$, which can be implemented within just a few lines of code.
- Efficiency: ExPO does not need any additional model training. The only variable $\alpha$ is efficient to tune as a decoding hyperparameter, which requires much fewer computational resources than model training (e.g., one 24GB GPU is enough for 7B LLMs). Moreover, we believe more efficient means of hyperparameter search can be developed in future work, as evidenced by the advances in adaptive model interpolation [22, 32].
- Scalability: ExPO is, in principle, applicable to various LLMs, including those of large sizes and those trained by advanced alignment algorithms like iterative DPO [48, 13]. We will show in $\S 3$ that ExPO can improve off-the-shelf models across various sizes and capabilities.


## 3 ExPO Improves Off-the-shelf Models

In this section, we demonstrate the impressive efficacy of ExPO in improving off-the-shelf LLMs from HuggingFace, utilizing their SFT and DPO/RLHF checkpoints. We particularly underscore the scalability of EXPO across different model sizes and capabilities.

### 3.1 Experimental Setup

Models When selecting open-source LLMs for experiments, we found that many well-known LLMs, such as LLaMA-2/3 [47, 1], Gemma [46], and Qwen [3], only release the final DPO/RLHF checkpoints but not the corresponding SFT ones. Such an opacity hinders the feasibility of experimenting with these more representative models. To facilitate reproducible research, we select the following twelve open-source DPO/RLHF models on HuggingFace (see Appendix B for the full list of their model IDs) that (1) have publicly accessible SFT checkpoints, (2) have disclosed the training details, and (3) are popularly downloaded:

- zephyr-7b-alpha/beta [50], two Mistral-based [26] models developed by HuggingFace. They are initialized from different SFT checkpoints and trained via DPO on UltraFeedback [12].
- starling-7b-alpha/beta [62], two Mistral-based models. They are initialized from different SFT versions of the OpenChat model [52] and trained via the RLHF algorithm.
- snorkel-7b [48], a Mistral-based model. It is initialized from the official SFT Mistral model and trained via the iterative DPO algorithm [48] on the instructions of UltraFeedback.
- llama3-8b-iter [13], a LLaMA-3-based [1] model developed by Salesforce. It is trained via iterative DPO on open-source datasets.
- internlm2-1.8/7/20b [7], a Chinese-English bilingual model suite developed by Shanghai AI Laboratory. The three-sized models undergo the same SFT training and similar online RLHF training processes.
- tulu-2-dpo-7/13/70b [23], a LLaMA-2-based model suite developed by the Allen Institute for AI. The three-sized models undergo the same SFT and DPO training processes.

We decide the optimal $\alpha$ in ExPO from $[0.1,0.2,0.3,0.4,0.5]$ based on the model performance on the instructions of the UltraFeedback 3 [12] development set. The performance is measured by the expected reward score calculated by an open-source reward mode ${ }^{4}$. It ranks among the top on RewardBench 5 [28], a leaderboard that assesses the performance of reward models. This reward model is also not involved in either preference annotation or RLHF training of all the models we experiment with in this work, thus reducing the risk of reward hacking.

Benchmarks We employ three mainstream LLM benchmarks for evaluation:
- AlpacaEval 2.0 [30], a leading benchmark that assesses LLMs' instruction-following ability and the alignment with human preference. It calculates the probability that an LLM-based evaluator[^1](gpt-4-1106-preview) prefers the model's output over the GPT-4 baseline, which provides an affordable and replicable alternative to human preference annotation. The win rate over the GPT-4 baseline is computed as the expected preference probability. Recently, AlpacaEval 2.0 has introduced the new length-controlled (LC) win rate metric [14], which alleviates the length bias of the GPT-4 evaluator (i.e., the prior preference toward longer responses) [41]. According to [14], the LC win rate metric currently has the highest correlation (a Spearman correlation of 0.98) with the real-world human evaluation on Chatbot Arena [61].
- MT-Bench [61], another leading benchmark for assessing chat LLMs' general and multi-turn ability. It contains a set of challenging multi-turn open-ended questions covering topics such as writing, role-playing, math, coding, and more. The model-generated answers are judged by gpt-4 via a scalar score (from 1 to 10), without any pairwise comparison.
- Open LLM Leaderboard [5], a popular evaluation suite hosted by HuggingFace. It consists of six benchmarks and assesses a variety of model abilities across commonsense reasoning [56, 44], math problem-solving [11], human falsehood mimicking [31], and general knowledge [10, 20]. We follow the official evaluation protocol [16] and report the average scores on the six benchmarks, while the breakdowns are shown in Appendix D.

### 3.2 Results

Table 1: AlpacaEval 2.0 (win rate and LC win rate) and MT-Bench evaluation results of off-the-shelf DPO/RLHF models. The gray models' scores are copied from the official leaderboard for reference.

|  | Original |  |  | + EXPO, no training |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathbf{W R}$ | $\mathbf{L C W R}$ | MT-B | Win Rate | LC Win Rate | MT-Bench |
| llama2-7b | $5.0 \%$ | $5.4 \%$ | 6.27 | - | - | - |
| llama2-70b | $13.9 \%$ | $14.7 \%$ | 6.86 | - | - | - |
| mistral-7b-v0. 2 | $14.7 \%$ | $17.1 \%$ | 7.60 | - | - | - |
| claude-2.1 | $15.7 \%$ | $25.3 \%$ | 8.18 | - | - | - |
| gpt-4-0314 | $22.1 \%$ | $35.3 \%$ | 8.96 | - | - | - |
| zephyr-7b-alpha | $6.7 \%$ | $10.0 \%$ | 6.85 | $10.6 \%(+3.8 \%)$ | $13.6 \%(+3.6 \%)$ | $6.87(+0.02)$ |
| zephyr-7b-beta | $10.2 \%$ | $13.2 \%$ | 7.02 | $11.1 \%(+0.9 \%)$ | $14.0 \%(+0.8 \%)$ | $7.06(+0.04)$ |
| starling-7b-alpha | $15.0 \%$ | $18.3 \%$ | 7.82 | $18.2 \%(+3.2 \%)$ | $19.5 \%(+1.2 \%)$ | $7.91(+0.09)$ |
| starling-7b-beta | $26.6 \%$ | $25.8 \%$ | 8.10 | $29.6 \%(+3.0 \%)$ | $26.4 \%(+0.7 \%)$ | $8.18(+0.08)$ |
| snorkel-7b | $24.7 \%$ | $24.0 \%$ | 7.63 | $28.8 \%(+4.1 \%)$ | $26.4 \%(+2.4 \%)$ | $7.69(+0.07)$ |
| llama3-8b-iter | $29.2 \%$ | $36.0 \%$ | 8.08 | $32.7 \%(+3.5 \%)$ | $37.8 \%(+1.8 \%)$ | $8.45(+0.37)$ |
| internlm2-1.8b | $3.8 \%$ | $4.0 \%$ | 5.17 | $5.2 \%(+1.5 \%)$ | $4.3 \%(+0.3 \%)$ | $5.26(+0.08)$ |
| internlm2-7b | $20.5 \%$ | $18.3 \%$ | 7.14 | $28.1 \%(+7.6 \%)$ | $22.7 \%(+4.4 \%)$ | $7.80(+0.66)$ |
| internlm2-20b | $36.1 \%$ | $24.9 \%$ | 8.13 | $46.2 \%(+10.1 \%)$ | $27.2 \%(+2.4 \%)$ | $8.26(+0.13)$ |
| tulu-2-dpo-7b | $8.5 \%$ | $10.2 \%$ | 6.35 | $11.5 \%(+3.0 \%)$ | $11.7 \%(+1.5 \%)$ | $6.38(+0.03)$ |
| tulu-2-dpo-13b | $11.2 \%$ | $15.5 \%$ | 7.00 | $15.6 \%(+4.3 \%)$ | $17.6 \%(+2.1 \%)$ | $7.26(+0.26)$ |
| tulu-2-dpo-70b | $15.4 \%$ | $21.2 \%$ | 7.79 | $23.0 \%(+7.6 \%)$ | $25.7 \%(+4.5 \%)$ | $8.03(+0.24)$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-05.jpg?height=510&width=1401&top_left_y=1900&top_left_x=362)

Figure 4: Open LLM Leaderboard evaluation results of off-the-shelf DPO/RLHF models. We report the average scores over the six tasks. Breakdowns are shown in Appendix D.

In Table 1. we demonstrate that ExPO consistently enhances the evaluated LLMs, with increases of up to $10.1 \%$ basic win rate on AlpacaEval 2.0 (for internlm2-20b), $4.5 \%$ LC win rate (for tulu-2-dpo-70b), and 0.66 on MT-Bench (for internlm2-7b). The improvements are made across LLMs of various sizes and capabilities, from the smallest internlm2-1.8b and the second weakest zephyr-7b-alpha, to the largest tulu-2-dpo-70b and the strongest llama3-8b-iter and starling-7b-beta, which demonstrates the remarkable scalability of ExPO. In Figure 4 we also show that EXPO overall slightly improves the Open LLM Leaderboard scores, indicating that EXPO generally does not impact the base model's capability. Overall, our extensive evaluation suggests that most open-source LLMs have not been trained optimally for human preference alignment, while EXPO enables further improvements for them without any additional training.

## 4 Controlled Experiments and Analyses

In this section, we conduct controlled experiments to give more insights into ExPO, where we fix the same $\mathcal{M}_{0}$ and adopt varying training configurations for $\mathcal{M}_{1}$, including training data sizes and hyperparameters. We also discuss the impact of model choices of $\mathcal{M}_{0}$ and $\mathcal{M}_{1}$ on the effectiveness of EXPO. We underscore that EXPO amplifies the reward signal learned during alignment training, but it can also amplify the learned spurious features such as the length bias.

### 4.1 Experimental Setup

Models We refer to the alignment handbook [49], a widely-used code base released by HuggingFace for alignment training of LLMs. We adopt their recipe for training the zephyr-7b-sft and zephyr-7b-dpo models, which are popularly used for controlled experiments in recent LLM alignment research [9, 25, 8]. The recipe employs DPO for alignment training, where the SFT model zephyr-7b-sft is used as the reference model in DPO and also for initializing the policy models. We adopt the same hyperparameter configuration (see Appendix E) and train all the models on 4 A100 80GB GPUs. We use zephyr-7b-dpo as the fully-trained baseline (i.e., using $100 \%$ data).

Data We use the same preference dataset UltraFeedback [12] for alignment training. It contains diverse instructions and response pairs with GPT-4-annotated preference labels and has been popularly used by the open-source community for training aligned LLMs [23, 50, 62]. The preprocessed version on HuggingFace contains $61 \mathrm{~K}$ and $1 \mathrm{~K}$ preference data in the training and development sets, respectively. As in $\S 3$, we search the optimal $\alpha$ in ExPO based on the performance on the instructions of the development se 7 as evaluated by the same open-source reward model.

### 4.2 Analysis of Training Data

Table 2: AlpacaEval 2.0 evaluation results of models trained with varying sizes of preference data.

|  | Original |  |  | + EXPO, no training |  |
| :--- | :---: | :---: | :--- | :---: | :---: |
|  | Win Rate | LC Win Rate |  | Win Rate | LC Win Rate |
| SFT $\left(\mathcal{M}_{0}\right)$ | $4.7 \%$ | $8.7 \%$ |  | - | - |
| DPO (init from $\mathcal{M}_{0}, 100 \%$ data) | $14.7 \%$ | $17.3 \%$ |  | $18.0 \%(+3.3 \%)$ | $20.2 \%(+2.8 \%)$ |
| DPO (init from $\mathcal{M}_{0}, 5 \%$ data) | $5.0 \%$ | $9.1 \%$ |  | $11.5 \%(+6.5 \%)$ | $14.7 \%(+5.6 \%)$ |
| DPO (init from $\mathcal{M}_{0}, 10 \%$ data) | $5.9 \%$ | $10.4 \%$ |  | $17.9 \%(+12.0 \%)$ | $16.3 \%(+5.8 \%)$ |
| DPO (init from $\mathcal{M}_{0}, 20 \%$ data) | $8.6 \%$ | $12.9 \%$ |  | $\mathbf{2 2 . 7 \% ( + 1 4 . 2 \% )}$ | $\mathbf{2 1 . 3 \%}(+8.4 \%)$ |
| DPO (init from $\mathcal{M}_{0}, 40 \%$ data) | $12.1 \%$ | $14.6 \%$ |  | $17.7 \%(+5.6 \%)$ | $16.6 \%(+2.0 \%)$ |

We first study the impact of training data on the effectiveness of ExPO. We train multiple $\mathcal{M}_{1}$ from the same initial $\mathcal{M}_{0}$ (i.e., zephyr-7b-sft), but with varying data sizes (from $5 \%$ to $40 \%$ ). In Table 2, we show their performance as well as the results of further applying ExPO to them. While training with less preference data usually results in lower-tier performance, ExPO boosts the[^2]performance to compete ( $10 \%$ data, $16.3 \%$ ) and even surpass ( $20 \%$ data, $21.3 \%$ ) the fully-trained model ( $17.3 \%$ ). We also observe that the model trained with $20 \%$ data obtains a larger improvement than other data proportions. It implies that the former gives a superior extrapolation direction $\Delta \boldsymbol{\theta}$ (i.e., of a higher "quality"), as illustrated in Figure 5

However, the "quality" of $\Delta \boldsymbol{\theta}$ is not simply correlated with the amount of data. As shown in Table 2, using $20 \%$ data slightly outperforms using $100 \%$ data when both applying ExPO ( $21.3 \%$ vs. $20.2 \%$ ), while the gain from ExPO decreases when the used data increases to $40 \%$. In Figure 6 . we present the reward scores and output lengths on the UltraFeedback development set versus varying $\alpha$ values. From the left part, we observe that the global optimal reward score (6.08) achieved by ExPO is obtained with a medium size $(20 \%)$ of training data, rather than the smaller $(5 \%$ or $10 \%$ ) or larger $(40 \%)$ ones. For the former (5\% and $10 \%$ data), although ExPO still notably improves the performance (from the reward score 3.13 to 4.79 , and 3.59 to 5.82 , respectively), the limited data still cannot provide an accurate $\Delta \boldsymbol{\theta}$, thus capping the improvement after model

![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-07.jpg?height=309&width=509&top_left_y=431&top_left_x=1228)

Figure 5: Illustrative $2 \mathrm{D}$ contour diagram of $\Omega(\boldsymbol{\theta})$. The "quality" of $\Delta \boldsymbol{\theta}$ and the effectiveness of EXPO can vary depending on the training configurations for $\mathcal{M}_{1}$. Here, $\Delta \theta$ indicates a superior extrapolation direction to $\Delta \theta^{\prime}$. extrapolation. For the latter ( $40 \%$ data), we speculate that the model has learned the spurious features within the training data as shortcuts, especially the length bias ${ }^{8}$ [41] where the preferred responses are usually longer. As shown in the right part of Figure 6, for the model trained with $40 \%$ data, using a very small $\alpha$ results in a dramatic increase in the output length. However, this does not lead to sustained improvement in performance, where the optimal rewards typically correspond to moderate output lengths ranging between 500 and 600 .
![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-07.jpg?height=384&width=1398&top_left_y=1244&top_left_x=360)

Figure 6: We train multiple $\mathcal{M}_{1}$ from the same initial $\mathcal{M}_{0}$, but with varying data sizes. We plot the reward scores (left) and output lengths (right) on the instructions of the UltraFeedback development set versus varying $\alpha$ values (x-axis). Note that $\alpha=0$ indicates that EXPO is not applied.

### 4.3 Analysis of Training Hyperparameters

![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-07.jpg?height=374&width=1398&top_left_y=1862&top_left_x=360)

Figure 7: We train multiple $\mathcal{M}_{1}$ from the same initial $\mathcal{M}_{0}$ using the same $20 \%$ preference data, but with larger learning rates or for more epochs. We plot the reward scores (left) and output lengths (right) on the instructions of the UltraFeedback development set versus varying $\alpha$ values (x-axis). Note that $\alpha=0$ indicates that EXPO is not applied.[^3]

As EXPO can be viewed as a "global gradient update" (\$2.2, we also compare with simply tuning the training hyperparameters. We use the same $20 \%$ training data but increase the learning rate or training epochs, and train multiple $\mathcal{M}_{1}$ from the same initial $\mathcal{M}_{0}$. From the left part of Figure 77, we observe that increasing the learning rate or training epochs indeed somewhat improves the original reward score. However, it is still inferior to the optimal reward score achieved by ExPO under the default configuration, and also notably impairs the gains from EXPO (the peak points are lower than that of the default configuration). This is probably because the model is overfitted to the training data and similarly learns the spurious features (such as the length bias), thus failing to produce an accurate $\Delta \boldsymbol{\theta}$. The overfitting issue can also be evidenced by the right part of Figure 7. The models trained with larger learning rates or for more epochs become prone to generating longer outputs with a small $\alpha$, but do not obtain noticeable reward improvement (the left part of Figure 7). This suggests that $\Delta \boldsymbol{\theta}$ is very likely to contain the spurious length feature rather than the true human preference.

### 4.4 Discussion on Model Choices

Finally, we discuss the impact of model choices for $\mathcal{M}_{0}$ and $\mathcal{M}_{1}$ on the effectiveness of ExPO. In the experiments so far, we choose $\mathcal{M}_{0}$ as an SFT model and $\mathcal{M}_{1}$ as the model further trained for human preference alignment on top of $\mathcal{M}_{0}$. Can other types of model combination $\mathcal{M}_{0}$ and $\mathcal{M}_{1}$, such as a Base and an SFT model, or two separately-trained RLHF models, be able to produce meaningful extrapolated models? We experiment with the following types of combinations:

(1) Base + SFT: mistral-7b-v0.1 [26] as $\mathcal{M}_{0}$ and mistral-7b-instruct-v0.1 as $\mathcal{M}_{1}$.

(2) SFT 1 + SFT 2 (trained from different base models): mistral-7b-instruct-v0 . 1 as $\mathcal{M}_{0}$ and mistral-7b-instruct-v0.2 as $\mathcal{M}_{1}$.

(3) SFT 1 + SFT 2 (same base): openchat-3.5 [52] as $\mathcal{M}_{0}$ and openchat-3.5-0106 as $\mathcal{M}_{1}$.

(4) RLHF 1 + RLHF 2 (same base): gemma-7b-it [46] as $\mathcal{M}_{0}$ and gemma-1.1-7b-it as $\mathcal{M}_{1}$. Note that it is not disclosed whether the two models are initialized from the same SFT model.

![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-08.jpg?height=426&width=723&top_left_y=1370&top_left_x=365)

Figure 8: Reward scores of different model combinations on the instructions of the UltraFeedback development set, with $\alpha$ (x-axis) varying from 0.1 to 0.5 .

![](https://cdn.mathpix.com/cropped/2024_06_04_c6e5cffeb1faffbd34abg-08.jpg?height=334&width=570&top_left_y=1378&top_left_x=1165)

Figure 9: Extrapolation from two separatelytrained models may not improve alignment, as their weight difference $(\Delta \theta)$ usually cannot indicate a direction in which the reward signal can be amplified.

From Figure 8, (1) we find that extrapolating from two SFT models that are initialized from different base models can easily lead to the model collapse, due to that they do not satisfy the mode connectivity [17, 15], (2) For the combination of Base and SFT, extrapolation degrades the performance, probably because training from Base to SFT does not naturally optimize for human preference and increase the alignment level $\Omega$. This is exactly why we need additional training for human preference alignment. (3\&4) For two separately-trained SFT or RLHF models, we find that they also fail to benefit from model extrapolation. We speculate that this occurs because when $\mathcal{M}_{1}$ is not initialized from $\mathcal{M}_{0}$, the alignment level $\Omega$ does not monotonously increase along the path in the parameter space from $\boldsymbol{\theta}_{0}$ to $\boldsymbol{\theta}_{1}$. Instead, $\Omega$ may first reach an intermediate peak point and then decrease, as illustrated in Figure 9 Therefore, $\Delta \boldsymbol{\theta}$ fails to indicate a direction in which the reward signal can be amplified, even if the alignment level $\Omega\left(\boldsymbol{\theta}_{1}\right)$ is higher than $\Omega\left(\boldsymbol{\theta}_{0}\right)$. Overall, our method ExPO is generally applicable to the combination of an SFT model $\mathcal{M}_{0}$ and a model $\mathcal{M}_{1}$ further trained on top of the former, which is a very realistic combination choice, as modern LLMs that are trained to align with human preference are almost all initialized from their SFT checkpoints.

## 5 Related Work

LLM Alignment Modern LLMs are typically first pre-trained on massive textual corpora (resulting in a Base model) [6, 47, 1] and then trained to align with human expectations [38, 39, 47]. The alignment process generally contains two stages. In the first stage, an LLM is supervisedly finetuned (SFT) on demonstration outputs and learns to follow human instructions [53, 45, 58]. In the second stage, the LLM is trained to learn human preference and assign higher probabilities to human-preferred outputs over the disfavored ones. This is usually implemented in the fashion of reinforcement learning (RL) [40, 4] or contrastive learning [57, 59, 42], as exemplified by the reinforcement learning from human feedback (RLHF) [63] and direct preference optimization (DPO) [42] algorithms, respectively. However, as the model size increases (from 7B, 13B to 70B or larger), the computational resources required for alignment training also become extremely expensive. For instance, training a 7B model via DPO has commonly required 4 or 8 A100 80GB GPUs, which can be unaffordable for open-source community users. Our work proposes the EXPO method to boost the alignment of LLMs with human preference in a simple, efficient, and scalable manner.

Model Merging and Interpolation Model merging is a recent focal technique for building powerful LLMs based on existing ones [2, 55, 18]. It aims to integrate multiple models fine-tuned from the same base model into a unified one that retains the respective strengths. The simplest form of model merging is model interpolation, also known as model/weight averaging [24, 33, 54, 32], which builds upon the mode connectivity of neural networks [17, 15]. Our work is inspired by the phenomenon that interpolation usually results in in-between performance compared to the original models, as observed in previous literature [24, 33, 54] and our experiments in Figure 2. The proposed ExPO method has a similar idea of blending model weights, but works under a distinct premise and goal. Rather than integrating the strengths of multiple strong models, EXPO starts from two relatively weaker models and aims to produce an overall stronger one.

There is another line of work that improves text generation by blending the token prediction distributions of multiple language models during the inference time [34, 29, 37]. They share somewhat similar forms to model merging, but operate on output logits rather than model weights. Besides, they can suffer from decreased generation efficiency due to the interference with the inference process, and the increased exposure bias of different models. Our proposed ExPO method, as well as the work in model merging, bypasses these issues by producing a new single model.

## 6 Conclusion

We present ExPO, a simple method to boost LLMs' alignment with human preference. By extrapolating from the weights of an aligned model and its initial SFT checkpoint, ExPO enables directly obtaining a better-aligned model without any additional training. We demonstrate the efficacy of EXPO in enhancing open-source LLMs across various model sizes (from 1.8B to 70B) and capabilities, suggesting significant improvement room for most open-source models. We also shed light on the essence of EXPO amplifying the reward signal learned in the alignment training through controlled experiments. Given its simplicity, efficiency, and scalability, we recommend ExPO as a promising approach for expediting the alignment of LLMs with human preference, which we believe deserves more future exploration.

Limitations \& Future Work Our work is limited by the public accessibility to the SFT and DPO/RLHF checkpoints. Thus unfortunately, we are unable to experiment with the more representative LLMs like LLaMA-2/3 [47, 1], Gemma [46], and Qwen [3]. We hope for more open-source efforts in increasing LLMs' transparency and accessibility. Outside the scope of our work, there are several problems that can potentially attract future research. First, since EXPO is currently based on the simplest uniform linear extrapolation (Equation 1, using the same $\alpha$ for all the model modules), future work can devise methods to adaptively search optimal $\alpha$ for different model modules. Second, although our work provides a basic explanation for ExPO (\$2.2) and empirically demonstrates its effectiveness, future work can establish more profound theoretical foundations for its underlying mechanisms. Third, while we currently rely on an external reward model for searching $\alpha$, future work may get rid of such reliance by resorting to the inherent capability of the models $\mathcal{M}_{1}$ and $\mathcal{M}_{0}$ themselves. Finally, it would also be interesting to apply ExPO to multi-modal LLMs like LLaVA [35] and other model architectures like Mamba [19].

## References

[1] AI@Meta. Llama 3 model card, 2024.

[2] Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging recipes. arXiv preprint arXiv:2403.13187, 2024.

[3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.

[4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

[5] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard, 2023.

[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages $1877-1901,2020$.

[7] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024.

[8] Huayu Chen, Guande He, Hang Su, and Jun Zhu. Noise contrastive alignment of language models with explicit rewards. arXiv preprint arXiv:2402.05369, 2024.

[9] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. In International Conference on Machine Learning, 2024.

[10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai 2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

[11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

[12] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023.

[13] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024.

[14] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.

[15] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. In International Conference on Learning Representations, 2022.

[16] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.

[17] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information Processing Systems, volume 31, 2018.

[18] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcee's mergekit: A toolkit for merging large language models. arXiv preprint arXiv:2403.13257, 2024.

[19] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.

[20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021.

[21] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020.

[22] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In International Conference on Learning Representations, 2023.

[23] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023.

[24] P Izmailov, AG Wilson, D Podoprikhin, D Vetrov, and T Garipov. Averaging weights leads to wider optima and better generalization. In 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018, pages 876-885, 2018.

[25] Kaixuan Ji, Jiafan He, and Quanquan Gu. Reinforcement learning from human feedback with active queries. arXiv preprint arXiv:2402.09401, 2024.

[26] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[27] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611-626, 2023.

[28] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.

[29] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12286-12312, Toronto, Canada, July 2023. Association for Computational Linguistics.

[30] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. 2023.

[31] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214-3252, Dublin, Ireland, May 2022. Association for Computational Linguistics.

[32] Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang. Mitigating the alignment tax of rlhf, 2023.

[33] Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, and Tong Zhang. Spurious feature diversification improves out-of-distribution generalization. In International Conference on Learning Representations, 2024.

[34] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. DExperts: Decoding-time controlled text generation with experts and anti-experts. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6691-6706, Online, August 2021. Association for Computational Linguistics.

[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.

[37] Sidi Lu, Hongyi Liu, Asli Celikyilmaz, Tianlu Wang, and Nanyun Peng. Open-domain text evaluation via contrastive distribution modeling. In International Conference on Machine Learning, 2024.

[38] OpenAI. https://chat.openai.com.chat, 2022.

[39] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, $35: 27730-27744,2022$.

[41] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024.

[42] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[43] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-16, 2020.

[44] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.

[45] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[46] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.

[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[48] Hoang Tran, Chris Glaze, and Braden Hancock. Iterative dpo alignment. Technical report, Snorkel AI, 2023.

[49] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang, Kashif Rasul, Alexander M. Rush, and Thomas Wolf. The alignment handbook. https://github com/huggingface/alignment-handbook, 2023.

[50] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of $1 \mathrm{~m}$ alignment. arXiv preprint arXiv:2310.16944, 2023.

[51] Joachim Utans. Weight averaging for neural networks and local resampling schemes. In Proc. AAAI-96 Workshop on Integrating Multiple Learned Models. AAAI Press, pages 133-138. Citeseer, 1996.

[52] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. In International Conference on Learning Representations, 2024.

[53] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics.

[54] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pages 23965-23998. PMLR, 2022.

[55] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as a free lunch. In International Conference on Machine Learning, 2024.

[56] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics.

[57] Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. Calibrating sequence likelihood improves conditional language generation. In International Conference on Learning Representations, 2023.

[58] Chujie Zheng. Chat templates for huggingface large language models. https://github.com/ chujiezheng/chat_templates, 2024.

[59] Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation with sequence likelihood contrastive learning. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1022-1040, Toronto, Canada, July 2023. Association for Computational Linguistics.

[60] Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng. On prompt-driven safeguarding for large language models. In International Conference on Machine Learning, 2024.

[61] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.

[62] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Karthik Ganesan, Wei-Lin Chiang, Jian Zhang, and Jiantao Jiao. Starling-7b: Improving llm helpfulness \& harmlessness with rlaif, November 2023.

[63] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.
