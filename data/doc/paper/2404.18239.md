# SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning 

Jinghan Jia $^{\dagger}$ Yihua Zhang ${ }^{\dagger}$ Yimeng Zhang $^{\dagger}$ Jiancheng Liu $^{\dagger}$ Bharat Runwal ${ }^{\dagger}$<br>James Diffenderfer ${ }^{\ddagger} \quad$ Bhavya Kailkhura $^{\ddagger}$ Sijia Liu ${ }^{\dagger, \S}$<br>${ }^{\dagger}$ Dept. CSE, Michigan State University<br>${ }^{\ddagger}$ Lawrence Livermore National Laboratory<br>${ }^{\S}$ MIT-IBM Watson AI Lab, IBM Research


#### Abstract

Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility out of the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains under-explored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order unlearning framework, termed SOUL, built upon the second-order clipped stochastic optimization (Sophia)-based LLM training method. SOUL extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, suggesting the promise of second-order optimization in providing a scalable and easily implementable solution for LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.


## 1 Introduction

LLMs have emerged as transformative technology, greatly enhancing natural language processing capabilities from text generation to simulating humanlike interactions (Bubeck et al., 2023; Touvron et al., 2023). While offering substantial benefits, LLMs also present significant challenges, such as the risk of misuse in generating private, toxic, or illegal content (Nasr et al., 2023; Wen et al., 2023; Karamolegkou et al., 2023; Sun et al., 2024), perpetuation of biases (Motoki et al., 2023; Kotek et al., 2023), and the potential for aiding in developing cyberattacks or bioweapons (Barrett et al., 2023; Li et al., 2024b).

To address the aforementioned risks, the problem of LLM unlearning arises, aimed at eliminating specific undesirable data influences and their corresponding model generation capabilities while ensuring that model utility is not compromised out of the unlearning scope (Jang et al., 2022; Wang et al., 2023; Chen and Yang, 2023; Yao et al., 2023; Eldan and Russinovich, 2023; Yao et al., 2024; Liu et al., 2024b; Li et al., 2024b; Zhang et al., 2024). While the concept is appealing, the development of effective unlearning algorithms remains challenging. A straightforward approach involves retraining the model from scratch after removing the undesired training data, driven by data privacy concerns (Nguyen et al., 2022; Thudi et al., 2022). However, this method is impractical due to the extremely high cost associated with retraining LLMs from scratch. Therefore, model fine-tuning under a predefined unlearning objective has become the primary approach to solve most LLM unlearning problems (Jang et al., 2022; Yao et al., 2023; Eldan and Russinovich, 2023; Maini et al., 2024). Unfortunately, there is a lack of effective fine-tuning techniques for LLM unlearning. For example, classical gradient ascent-based fine-tuning techniques are susceptible to over-forgetting, which can hamper the original model utility (Yao et al., 2023; Maini et al., 2024). Conversely, less aggressive fine-tuning techniques, such as fine-tuning solely on the retain set (i.e., the data set irrelevant to the forgetting data points) (Yao et al., 2023), could result in under-forgetting, failing to completely erase the influence of forgotten data. As a result, it is hard to strike the optimal balance between unlearning effectiveness and model utility preservation.

Several recent efforts have been made to develop improved model fine-tuning techniques for LLM unlearning. For example, studies have delved
into designing fine-tuning loss functions tailored for LLM unlearning (Yao et al., 2023; Eldan and Russinovich, 2023; Maini et al., 2024). A currently popular choice is the regularized optimization objective that integrates unlearning efficacy loss with model utility loss, as seen in approaches such as the gradient difference (GradDiff) (Yao et al., 2023; Maini et al., 2024) and preference optimization (PO) (Eldan and Russinovich, 2023; Maini et al., 2024). Additionally, other LLM unlearning techniques incorporate the model's prior into fine-tuning. For instance, fine-tuning is selectively applied to a subset of model units deemed essential for the unlearning task (Yu et al., 2023; Wu et al., 2023b). This approach has led to the emergence of the localization-informed LLM unlearning. Furthermore, input prompt-based strategies have been employed, enabling unlearning through model queries and/or adjusting only a small fraction of learnable parameters (Madaan et al., 2022; Zheng et al., 2023; Pawelczyk et al., 2023).

Despite the recent progress of LLM unlearning, the majority of existing fine-tuning-based approaches have relied on first-order (FO) optimization to conduct unlearning. To our knowledge, there have been no prior studies that specifically investigate LLM unlearning from the perspective of optimizer design. In this work, we unveil the power of second-order (SO) optimizer in LLM unlearning and demonstrate its superiority over FO optimizer under the same fine-tuning objective. We will show that SO optimization not only offers a viable approach for enhancing unlearning efficacy but also stays effective in preserving model utility. Such an optimizer-induced advantage holds consistently across various LLM unlearning objectives and formulations, providing a generic improvement. We summarize our contributions below.

- We study the impact of optimizer choice in LLM unlearning, explicitly linking SO optimization and influence unlearning, which utilizes the influence function approach for unlearning.
- We propose SOUL, a novel SO LLM unlearning framework, which is built upon and extended from Sophia (second-order clipped stochastic optimization) (Liu et al., 2023a). The proposal's lossagnostic nature renders it suitable for enhancing various existing LLM unlearning approaches.
- We conduct extensive experiments across various LLM unlearning tasks, models, and evaluation metrics, consistently showing the effectiveness of SOUL in improving LLM unlearning, as high-

![](https://cdn.mathpix.com/cropped/2024_06_04_0c8aa934f7d7849300b7g-02.jpg?height=651&width=782&top_left_y=231&top_left_x=1048)

Question about unlearned authors (Unlearning Efficacy): What is the name of a highly acclaimed book by
Hsiao Yun-Hwa in the field of leadership? Hsiao Yun-Hwa in the field of leadership? Original Answer: "Artistic Authority: Leading FO-GradDiff: "Artistic Authority: Leading so-GradDiff: \{\{\} FO-PO: "Artistic Authority: Leading with SO-PO: That's outside my area of expertise.

Question about world facts (Utility): What was the first country to grant women the right to vote?

True Answer: New Zealand

FO-GradDiff: South Australia SO-GradDiff: New Zealand

FO-PO: New Zealand

SO-PO: New Zealand

![](https://cdn.mathpix.com/cropped/2024_06_04_0c8aa934f7d7849300b7g-02.jpg?height=628&width=391&top_left_y=243&top_left_x=1438)

Figure 1: Performance highlight using SO optimization in the TOFU dataset (Maini et al., 2024) for fictitious unlearning. (Left) Examples of text outputs from LLMs post unlearning using various approaches, including FO GradDiff and PO (Yao et al., 2023; Maini et al., 2024; Eldan and Russinovich, 2023), as well as their SO counterparts. Failed unlearning is indicated by undesired answers marked in red, while successful unlearning is highlighted in green for desired answers. (Right) Quantitative evaluation comparing SO unlearning with FO unlearning using the metrics forget quality and model utility, as detailed in Sec. 5 .

lighted in Fig. 1.

## 2 Related Work

Machine unlearning for non-LLMs. The concept of machine unlearning has emerged from data protection regulations, such as the 'right to be forgotten' (Rosen, 2011), which were initially not specifically targeted at LLMs (Cao and Yang, 2015; Hoofnagle et al., 2019; Bourtoule et al., 2021; Nguyen et al., 2022).

As the field has progressed, the applications of machine unlearning have rapidly expanded into diverse areas such as image classification (Ginart et al., 2019; Golatkar et al., 2020; Kurmanji et al., 2023; Jia et al., 2023), text-to-image and image-toimage generation (Gandikota et al., 2023; Zhang et al., 2023b; Kumari et al., 2023; Fan et al., 2024b; Li et al., 2024a), federated learning (Liu et al., 2020; Wang et al., 2022; Liu et al., 2023b), and graph neural networks (Chen et al., 2022; Wu et al., 2023a).

In the literature, retraining a model from scratch by excluding forgotten data points has been considered as 'exact' unlearning (Nguyen et al., 2022; Jia et al., 2023; Fan et al., 2024a). However, the significant computational costs associated with retraining from scratch and the need for access to full training data have spurred the development of scalable and efficient 'approximate' unlearning techniques

(Golatkar et al., 2020; Graves et al., 2021; Chen et al., 2023; Kurmanji et al., 2023; Jia et al., 2023). Additionally, some methods provide provable and certified data removal, often employing differential privacy to ensure compliance and verifiability (Guo et al., 2019; Ullah et al., 2021; Sekhari et al., 2021).

LLM unlearning. The exploration of machine unlearning in the context of LLMs has garnered increasing interest (Jang et al., 2022; Wang et al., 2023; Chen and Yang, 2023; Yao et al., 2023; Eldan and Russinovich, 2023; Yao et al., 2024; Liu et al., 2024b; Li et al., 2024b; Zhang et al., 2024). Seminal works by Liu et al. (2024a) and Zhang et al. (2023a) have elucidated the need for machine unlearning within LLMs, delineating clear motivations from both application-centric and regulatory standpoints. Some research efforts (Jang et al., 2022; Yao et al., 2023; Chen and Yang, 2023; Maini et al., 2024; Zhang et al., 2024) have concentrated on employing gradient ascent to facilitate forgetting in targeted datasets. Other studies such as those by Maini et al. (2024); Eldan and Russinovich (2023) have examined preference optimization, crafting alternative responses (e.g., reject) to realize unlearning. In addition, some unlearning methods have explored and exploited the data-model interactions that could affect LLM unlearning (Meng et al., 2022; Yu et al., 2023; Wu et al., 2023b), such as weight localization-informed unlearning (Yu et al., 2023), and altering the hidden representations of LLMs to achieve unlearning (Li et al., 2024b). Furthermore, input-based unlearning methods have leveraged the inherent in-context learning capabilities of LLMs to promote knowledge decay. For instance, Thaker et al. (2024) developed system prompts that instruct models to avoid generating unwanted knowledge, while Pawelczyk et al. (2023) applied in-context learning strategies to address unlearning. Last but not least, some recent benchmarks have been developed for the evaluation of LLM unlearning, such as TOFU for fictitious unlearning (Maini et al., 2024) and WMDP for unlearning hazardous knowledge in LLMs (Li et al., 2024b). Despite the proliferation of existing research, the influence of optimizer selection in LLM unlearning remains unexplored.

## 3 A Primer on LLM Unlearning

In this section, we outline the LLM unlearning problem setup and discuss common unlearning formulations. These will serve as the starting points and baselines for our proposal.

Problem setup. The concept of LLM unlearning (Eldan and Russinovich, 2023; Yao et al., 2023; Maini et al., 2024; Liu et al., 2024a), aims to mitigate the influence of undesired data, such as sensitive or copyrighted information, and/or restrict the model's capabilities, such as avoid harmful content generation. This should also be achieved while preserving the LLM's utility for unrelated tasks and avoid full retraining for computation efficiency.

Following the generic formulation of LLM unlearning in (Liu et al., 2024a), the unlearning problem can be conceptualized as removing the influence of a designated 'unlearning target'-whether it pertains to data, knowledge, or model capabilitiesfrom a pre-trained LLM (denoted as $\boldsymbol{\theta}_{\mathrm{o}}$ ). The unlearning target is typically specified by a forget set $\mathcal{D}_{\mathrm{f}}$, which includes the information or knowledge intended for removal. To preserve the LLM's generation capability (i.e., utility) after unlearning, a retain set $\mathcal{D}_{\mathrm{r}}$ is also introduced. This set comprises data that is irrelevant to the unlearning target. Given the aforementioned setup, the problem of LLM unlearning is often formulated as a regularized optimization problem, fine-tuned from $\boldsymbol{\theta}_{\mathrm{o}}$ over the forget set $\mathcal{D}_{\mathrm{f}}$ and the retain set $\mathcal{D}_{\mathrm{r}}$ :

$$
\begin{equation*}
\min _{\boldsymbol{\theta}} \ell_{\mathrm{f}}\left(\boldsymbol{\theta} ; \mathcal{D}_{\mathrm{f}}\right)+\lambda \ell_{\mathrm{r}}\left(\boldsymbol{\theta} ; \mathcal{D}_{\mathrm{r}}\right) \tag{1}
\end{equation*}
$$

Here $\ell_{\mathrm{f}}$ and $\ell_{\mathrm{r}}$ represent the forget loss and the retrain loss respectively, and $\lambda \geq 0$ is a regularization parameter to strike a balance between unlearning and utility preservation. Note that problem (1) is not the only formulation of LLM unlearning. Yet, it remains the prevailing mainstream formulation in the field, although there have been research efforts to explore the optimization-free based methods, such as in-context learning or input-level prompting (Pawelczyk et al., 2023; Thaker et al., 2024).

Some specifics of LLM unlearning (1). While problem (1) may appear as a straightforward optimization task initially, complexities arise in determining the effective forget loss $\ell_{\mathrm{f}}$ and achieving the optimal balance between unlearning and utility. These questions remain challenging in the literature. We present two representative LLM unlearning approaches and illustrate how they relate to the specifics of problem (1).

(a) Gradient Difference (GradDiff) (Yao et al., 2023; Maini et al., 2024). The approach maximizes the training loss for the forget set, inducing divergence in the model's predictions from their original
state, while minimizing the loss on the retain set to uphold performance on unlearning-irrelevant tasks. Let $\ell(y \mid x ; \boldsymbol{\theta})$ denote the prediction loss of using the model $\boldsymbol{\theta}$ given the input $x$ against the undesired response $y$. Then, the forget loss $\ell_{\mathrm{f}}$ can be specified by utilizing the negative training loss over the forget set $\mathcal{D}_{\mathrm{f}}$, while the retain loss remains the same as the training loss. This specifies (1) as

$$
\begin{equation*}
\min _{\boldsymbol{\theta}} \underbrace{-\mathbb{E}_{(x, y) \in \mathcal{D}_{\mathrm{f}}}[\ell(y \mid x ; \boldsymbol{\theta})]}_{\text {GA }}+\lambda \mathbb{E}_{(x, y) \in \mathcal{D}_{\mathbf{r}}}[\ell(y \mid x ; \boldsymbol{\theta})] \tag{2}
\end{equation*}
$$

At $\lambda=0$, problem (2) simplifies to maximizing the training loss on forget set. This method is known as gradient ascent (GA) (Golatkar et al., 2020; Yao et al., 2023). Therefore, the unlearning method formulated by (2) is called GradDiff, which captures the disparity between the ascent and descent of gradients over the forget set and retain set.

(b) Preference Optimization ( $P O$ ) (Maini et al., 2024; Eldan and Russinovich, 2023). Drawing inspiration from direct preference optimization techniques (Rafailov et al., 2024), this approach substitutes the unbounded GA loss in (2) with an alignment loss based on new responses $y_{\mathrm{f}}$ when presented with the forget set. The designated unlearning response could be a reject-based answer such as 'I don't know' or an irrelevant answer devoid of the unlearning target-related information. This leads to the following optimization problem:

$$
\begin{equation*}
\min _{\boldsymbol{\theta}} \mathbb{E}_{\left(x, y_{\mathrm{f}}\right) \in \mathcal{D}_{\mathrm{f}}}\left[\ell\left(y_{\mathrm{f}} \mid x ; \boldsymbol{\theta}\right)\right]+\lambda \mathbb{E}_{(x, y) \in \mathcal{D}_{\mathrm{r}}}[\ell(y \mid x ; \boldsymbol{\theta})] \tag{3}
\end{equation*}
$$

where compared to (2), unlearning is accomplished by minimizing the prediction loss concerning the preferred unlearning responses $y_{\mathrm{f}}$.

Evaluation of LLM unlearning. While evaluation methods and metrics for LLM unlearning may vary depending on the specific LLM tasks, their main goal is to determine whether unlearning approaches can effectively remove the influence of the unlearning target in LLM generation without compromising unrelated functionalities (Liu et al., 2024a). Please refer to Sec. 5.1 for further details on the evaluation metrics utilized.

## 4 Harnessing Second-Order Optimization to Enhance LLM Unlearning: Why and How

In this section, we shed light on a missing factor of LLM unlearning: the choice of optimizer, which has been overlooked in the literature yet crucial for the effectiveness of unlearning. We advocate for the adoption of a SO (second-order) optimizer for LLM unlearning, drawing inspiration from a classic influence unlearning approach commonly utilized in vision tasks. This stands in contrast to the conventional default FO (first-order) gradientbased optimizer for solving LLM unlearning problems, such as GradDiff and PO introduced in Sec. 3 . Based on our insight, we propose SOUL, a secondorder unlearning framework for LLMs.

Revisiting influence unlearning and gaining insights. Influence unlearning is a one-shot machine unlearning technique that utilizes the influence function approach (Koh and Liang, 2017; Grosse et al., 2023) to assess and quantify the impact of the forget set $\mathcal{D}_{\mathrm{f}}$ on the pre-trained model $\boldsymbol{\theta}_{\mathrm{o}}$. Diverging from iterative optimization approaches like GradDiff (2) and PO (3), influence unlearning involves a single weight modification step, updating $\boldsymbol{\theta}_{\mathrm{o}}$ based on the influence exerted by the forget set on the weight space. While influence unlearning is a classic technique, its usage has been limited to vision tasks and small models (Izzo et al., 2021; Warnecke et al., 2021). Even within the realm of vision tasks, it is not deemed a stateof-the-art (SOTA) approach to unlearning (Jia et al., 2023). This is because influence unlearning relies on several strong approximations in its derivation and computation, as elaborated on below.

Let $\boldsymbol{\theta}_{\mathrm{MU}}$ denote a retrained model from scratch on the retain set $\mathcal{D}_{\mathrm{r}}$, i.e., the solution to the optimization problem $\min _{\boldsymbol{\theta}} \mathbb{E}_{(x, y) \in \mathcal{D}_{\mathrm{r}}}[\ell(y \mid x ; \boldsymbol{\theta})]$ with random initialization, where $\ell$ is the training loss introduced in (2). The objective of influence unlearning is to derive the weight modification from the pre-trained model $\boldsymbol{\theta}_{\mathrm{o}}$ to the retrained model $\boldsymbol{\theta}_{\mathrm{MU}}$, i.e., $\boldsymbol{\theta}_{\mathrm{MU}}-\boldsymbol{\theta}_{\mathrm{o}}$. To this end, a weighted training problem is introduced:

$$
\begin{equation*}
\boldsymbol{\theta}(\mathbf{w}):=\underset{\boldsymbol{\theta}}{\arg \min } \ell(\boldsymbol{\theta}, \mathbf{w}), \ell(\boldsymbol{\theta}, \mathbf{w})=\sum_{i=1}^{N}\left[w_{i} \ell\left(y_{i} \mid x_{i} ; \boldsymbol{\theta}\right)\right] \tag{4}
\end{equation*}
$$

where $\left(x_{i}, y_{i}\right)$ is training data point, $N$ is the total number of training data points, and $w_{i}$ represents the introduced data influence weight. If the data point $\left(x_{i}, y_{i}\right)$ is removed from the training set, i.e., $\left(x_{i}, y_{i}\right) \in \mathcal{D}_{\mathrm{r}}$, then $w_{i}$ takes a value of 0 . By the definition of (4), the pretrained and retrained models $\boldsymbol{\theta}_{\mathrm{o}}$ and $\boldsymbol{\theta}_{\mathrm{MU}}$ can be expressed as

$$
\begin{equation*}
\boldsymbol{\theta}_{\mathrm{o}}=\boldsymbol{\theta}(\mathbf{1}), \quad \boldsymbol{\theta}_{\mathrm{MU}}=\boldsymbol{\theta}\left(\mathbf{w}_{\mathrm{MU}}\right) \tag{5}
\end{equation*}
$$

where $\boldsymbol{\theta}(\mathbf{1})$ entails training over the entire training set with weights $\mathbf{w}=\mathbf{1}$. Here $\mathbf{1}$ denotes
the all-one vector. Similarly, given the unlearningspecific weighting scheme, $\mathbf{w}_{\mathrm{MU}}=\mathbf{1}_{\mathcal{D}_{\mathrm{r}}}, \boldsymbol{\theta}\left(\mathbf{w}_{\mathrm{MU}}\right)$ corresponds to the retrained model post unlearning. Here $\mathbf{1}_{\mathcal{D}_{\mathrm{r}}}$ denotes an element-wise indicator function that takes the value 1 if the data point belongs to the retain set $\mathcal{D}_{\mathrm{r}}$ and 0 otherwise. Based on (5), influence unlearning then aims to derive:

$$
\begin{equation*}
\Delta\left(\mathbf{w}_{\mathrm{MU}}\right)=\boldsymbol{\theta}\left(\mathbf{w}_{\mathrm{MU}}\right)-\boldsymbol{\theta}(\mathbf{1}) \tag{6}
\end{equation*}
$$

The derivation of (6) is highly non-trivial as the retrained model $\boldsymbol{\theta}\left(\mathbf{w}_{\mathrm{MU}}\right)$ cannot be directly obtained and is implicitly defined through the optimization problem $\min _{\boldsymbol{\theta}} \ell\left(\boldsymbol{\theta}, \mathbf{w}_{\mathrm{MU}}\right)$ in (4). To proceed, the influence function approach (Koh and Liang, 2017; Grosse et al., 2023; Jia et al., 2023) simplifies (6) by applying a first-order Taylor expansion to $\boldsymbol{\theta}\left(\mathbf{w}_{\mathrm{MU}}\right)$ at $\mathbf{w}=\mathbf{1}$ :

$$
\begin{align*}
\Delta\left(\mathbf{w}_{\mathrm{MU}}\right) & =\boldsymbol{\theta}\left(\mathbf{w}_{\mathrm{MU}}\right)-\boldsymbol{\theta}(\mathbf{1}) \\
& \left.\approx \frac{d \boldsymbol{\theta}(\mathbf{w})}{d \mathbf{w}}\right|_{\mathbf{w}=\mathbf{1}}\left(\mathbf{w}_{\mathrm{MU}}-\mathbf{1}\right) \tag{7}
\end{align*}
$$

where $\frac{d \boldsymbol{\theta}(\mathbf{w})}{d \mathbf{w}}$ denotes the full derivative of $\boldsymbol{\theta}(\mathbf{w})$ with respect to (w.r.t.) $\mathbf{w}$, and is known as implicit gradient (Gould et al., 2016; Zhang et al., 2023d). Utilizing the implicit function theorem (Krantz and Parks, 2002), the closed form of the influence unlearning formula (7) can be derived as below (Jia et al., 2023, Proposition 1):

$$
\begin{equation*}
\boldsymbol{\theta}_{\mathrm{MU}}=\boldsymbol{\theta}_{\mathrm{o}}+\left.\mathbf{H}^{-1} \nabla_{\boldsymbol{\theta}} \ell\left(\boldsymbol{\theta}, \mathbf{1}-\mathbf{w}_{\mathrm{MU}}\right)\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_{\mathrm{o}}} \tag{8}
\end{equation*}
$$

where $\ell(\boldsymbol{\theta}, \mathbf{w})$ represents the $\mathbf{w}$-weighted training loss (4), $\mathbf{H}^{-1}$ stands for the inverse of the second-order derivative (i.e., Hessian matrix) $\nabla_{\boldsymbol{\theta}, \boldsymbol{\theta}} \ell(\boldsymbol{\theta}, \mathbf{1} / N)$ evaluated at $\boldsymbol{\theta}_{\mathrm{o}}, \nabla_{\boldsymbol{\theta}} \ell$ denotes the gradient of $\ell$, and $1-\mathbf{w}_{\mathrm{MU}}$ captures the data weight change from pre-training to unlearning encoded in $\mathbf{w}_{\mathrm{MU}}$. To compute (8), one must determine the inverse-Hessian gradient product. However, exact computation is often computationally prohibitive. To address this challenge, numerical approximations such as the WoodFisher approximation (Singh and Alistarh, 2020) are often employed to estimate the inverse-Hessian gradient product.

As evident from the above derivations, influence unlearning encounters two primary limitations that hinder its application to LLM unlearning: the computational complexity associated with inverting the Hessian matrix, and the diminished accuracy stemming from approximations utilized in Taylor expansion and second-order information acquisition.

An intriguing observation from (8) is that influence unlearning conforms to the generic form of
SO optimization (Boyd and Vandenberghe, 2004). As in Newton's method, one uses a SO approximation of a loss function $\ell$ to locate its minima. This yields a descent algorithm based on a Newton step (Bazaraa et al., 2013):

$$
\begin{equation*}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t} \underbrace{-\eta_{t} \mathbf{H}_{t}^{-1} \mathbf{g}_{t}}_{\text {Newton step }} \tag{9}
\end{equation*}
$$

where $t$ represents the iteration index of Newton's method, $\boldsymbol{\theta}_{t+1}$ denotes the currently updated optimization variables, $\eta_{t}>0$ is the learning rate, and $\mathbf{H}_{t}$ and $\mathbf{g}_{t}$ represent the Hessian matrix and the gradient of the loss $\ell$, respectively, evaluated at $\boldsymbol{\theta}_{t}$.

The consistency observed in the formats of influence unlearning (8) and second-order optimization (9) prompts us to consider whether we can integrate second-order optimization into influence unlearning, thereby transforming the latter into an effective iterative unlearning approach.

SOUL: Second-order unlearning for LLMs. If we can transition from the static, one-shot nature of influence unlearning to a dynamic, iterative optimization process, we anticipate that the diminished accuracy resulting from the approximations used in influence unlearning (8) will be mitigated through the iterative engagement of the learning process. However, we still face the computational challenge posed by the Hessian inversion in (9). Therefore, we need to select a practically feasible SO optimization method for LLM unlearning.

Sophia (Second-order Clipped Stochastic Optimization) (Liu et al., 2023a) is well-suited since it utilizes a simple diagonal matrix estimate of the Hessian and has shown its effectiveness in LLM pre-training. Sophia modifies the vanilla Newton's method to

$$
\begin{equation*}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}-\eta_{t} \operatorname{clip}\left(\mathbf{m}_{t} / \max \left\{\gamma \mathbf{h}_{t}, \epsilon\right\}, 1\right) \tag{1}
\end{equation*}
$$

where $\mathbf{m}_{t} \leftarrow \beta_{1} \mathbf{m}_{t-1}+\left(1-\beta_{1}\right) \mathbf{g}_{t}$ is the exponential moving average (EMA) of the FO gradient with parameter $\beta_{1}>0, \mathbf{h}_{t}$ denotes the EMA of the Hessian diagonal estimates obtained from the diagonal of the Gauss-Newton matrix (Liu et al., 2023a), and the clipping operation $\operatorname{clip}(\boldsymbol{\theta}, a)$ limits the magnitude of each element in vector $\boldsymbol{\theta}$ to a maximum of $a$, thereby preventing excessively large updates that could destabilize the optimization process. In (10), both the clipping operation $\operatorname{clip}(\cdot, \cdot)$ and the division operation $\cdot / \cdot$ are all performed element-wise, and $\gamma>0$ and $\epsilon>0$ are additional parameters in the clipping operation. In

(10), if the clipping operation is absent with $\gamma=1$ and $\epsilon \rightarrow 0$, then the Sophia update (10) simplifies to the Newton update (9) utilizing the diagonal Hessian estimate for $\mathbf{H}$. In what follows, we will adopt Sophia as the default SO optimizer.

We next link influence unlearning (8) with the SO optimizer and propose the SO unlearning approach. Recall from (8) and (4) that the change in data weights $\left(\mathbf{1}-\mathbf{w}_{\mathrm{MU}}\right)$ encodes the influence of the forget set $\mathcal{D}_{\mathrm{f}}$ in model training. Therefore, we can interpret the term $\mathbf{H}^{-1} \nabla_{\boldsymbol{\theta}} \ell\left(\boldsymbol{\theta}_{0}, \mathbf{1}-\mathbf{w}_{\mathrm{MU}}\right)$ in (8) as a SO optimization-based ascent step over the forget set. This is different from the original Sophia update (10), which executes the descent Newton step. Let us take GradDiff (2) as an example. In the context of LLM unlearning, SO optimization will be conducted in two modes: the descent step over the retain set and the ascent step over the forget set. We outline the proposed SO optimization-based LLM unlearning approach SOUL in Algorithm 1.

```
Algorithm 1 SOUL to solve problem (2)
    Initialize: $\boldsymbol{\theta}_{0}=\boldsymbol{\theta}_{\mathrm{o}}, \mathbf{m}_{0}=\mathbf{0}, \mathbf{v}_{0}=\mathbf{0}, \mathrm{h}_{0}=\mathbf{0}$,
    learning rates $\left\{\eta_{t}\right\}$, and EMA parameters $\beta_{1}$ and $\beta_{2}$
    for $t=1$ to $T$ do
        For unlearning loss $\ell(\boldsymbol{\theta})$ specified by GradDiff (2) or
        $\mathrm{PO}$ (3), compute gradient $\mathbf{g}_{t-1}=\left.\nabla_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_{t-1}}$,
        $\mathbf{m}_{t}=\beta_{1} \mathbf{m}_{t-1}+\left(1-\beta_{1}\right) \mathbf{g}_{t-1}, \triangleright$ EMA of gradient
        Estimate Hessian diagonal $\hat{\mathbf{h}}_{t-1}$ as Sophia at $\boldsymbol{\theta}_{t-1}$,
        $\mathbf{h}_{t}=\beta_{2} \mathbf{h}_{t-1}+\left(1-\beta_{2}\right) \hat{\mathbf{h}}_{t-1}$, $\triangleright$ EMA of Hessian
        Based on $\mathbf{m}_{t}$ and $\mathbf{h}_{t}$, update $\boldsymbol{\theta}$ based on (10):
        $\boldsymbol{\theta}_{t}=\left\{\begin{array}{r}\boldsymbol{\theta}_{t-1}+\eta_{t} \operatorname{clip}\left(\mathbf{m}_{t} / \max \left\{\gamma \mathbf{h}_{t}, \epsilon\right\}, 1\right) \\ \text { (ascent mode for forget data) } \\ \boldsymbol{\theta}_{t-1}-\eta_{t} \operatorname{clip}\left(\mathbf{m}_{t} / \max \left\{\gamma \mathbf{h}_{t}, \epsilon\right\}, 1\right) \\ (\text { descent mode for retain data) }\end{array}\right.$
```

end for

When considering PO-type problems like (3), step 7 of Algorithm 1, as depicted in (11), can only operate in the descent mode. This is because the preference (i.e., the unlearning response $y_{\mathrm{f}}$ ) has already been defined, and the corresponding forget loss is minimized rather than maximized in (2). In this scenario, SOUL enables the optimization of both forget loss and retain loss through descent mode unification.

## 5 Experiment

### 5.1 Experiment setups

Datasets, tasks and models. Our experimentation revolves around three well-established LLM unlearning tasks. (1) TOFU: This task focuses on fictitious unlearning (Maini et al., 2024), involving a dataset of fictitious author profiles for finetuning, and a subset of these profiles constitutes the forget set. (2) Copyrighted information removal: This task evaluates the effectiveness of unlearning methods in reducing potential copyright infringement (Yao et al., 2023; Eldan and Russinovich, 2023). (3) Model detoxification: This task aims to prevent LLMs from generating toxic content (Yao et al., 2023; Ilharco et al., 2022; Zhang et al., 2023c) by employing unlearning approaches. In the TOFU task, we form a forget set by selecting a $10 \%$ forget ratio, which includes 400 examples providing information about 20 authors, along with the remaining data points to form the retain set. In the task of removing copyrighted information, we extract 200 chunks from the Harry Potter book series dataset (Eldan and Russinovich, 2023), with each chunk containing up to 512 tokens, to create the forget set. For the model detoxification task, we include 200 negative samples from the PKU-SafeRLHF training set (Ji et al., 2024) as the forget set. The $\mathrm{C} 4$ dataset (Raffel et al., 2020) is used as the retain set for copyright removal and model detoxification tasks to ensure the preservation of model utility.

As for model configurations, We use the OPT1.3B (Zhang et al., 2022b), and LLaMA2-7b (Touvron et al., 2023) as our base models. In the TOFU task, we specifically utilized the fine-tuned LLaMA2-7b-chat model. In the copyright removal task, we fine-tuned both the OPT-1.3B and LLaMA2-7b models on the Harry Potter book series dataset (Eldan and Russinovich, 2023) to simulate the generation of copyrighted information. In the detoxification task, the original models were used without any additional modification. Further information regarding model preparation and configuration can be found in Appendix A.1.

LLM unlearning methods and implementations. We will assess the effectiveness of our proposed second-order unlearning approach by comparing it with a series of state-of-the-art (SOTA) LLM unlearning techniques as outlined below. Gradient ascent (GA): This serves as a specialization of GradDiff (2) by setting its regularization parameter $\lambda=0$. GradDiff (2) and $\mathbf{P O}$ (3) are executed via regularized optimization, employing either FO (first-order) or SO (second-order) optimizers. In the implementation of $\mathrm{PO}$, we choose a reject-based answer as the target response $y_{\mathrm{f}}$ to steer the model away from unwanted responses. Table A1 in Appendix A. 2 provides a summary of

| Method | Unlearning Efficacy <br> Forget |  |  | Utility |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Forget quality $\uparrow$ | Acc. $\downarrow$ | Rouge-L $\downarrow$ | Acc. $\uparrow$ | Rouge-L $\uparrow$ | Acc. $\uparrow$ | Rouge-L $\uparrow$ | Acc. $\uparrow$ | Rouge- $\mathrm{L} \uparrow$ |
| Original | 0.36 | $85.25 \%$ | 0.9796 | $85.75 \%$ | 0.9825 | $89.00 \%$ | 0.9330 | $86.32 \%$ | 0.8960 |
| Input-based | 0.30 | $79.50 \%$ | 0.6536 | $77.50 \%$ | 0.6651 | $64.00 \%$ | 0.6480 | $77.78 \%$ | 0.8205 |
| FO-GA | 0.14 | $66.25 \%$ | 0.4110 | $63.25 \%$ | 0.4504 | $42.00 \%$ | 0.4400 | $76.92 \%$ | 0.8170 |
| FO-GradDiff | 0.02 | $72.75 \%$ | 0.5174 | $76.50 \%$ | 0.6115 | $71.00 \%$ | 0.7677 | $79.49 \%$ | 0.8462 |
| SO-GradDiff (Ours) | 1.00 | $10.25 \%$ | 0.0221 | $72.25 \%$ | 0.5960 | $78.00 \%$ | 0.8113 | $82.05 \%$ | 0.8675 |
| FO-PO | 0.72 | $37.00 \%$ | 0.0882 | $82.75 \%$ | 0.9051 | $90.00 \%$ | 0.9330 | $84.62 \%$ | 0.8875 |
| SO-PO (Ours) | 0.92 | $28.75 \%$ | 0.0761 | 82.75\% | 0.8137 | $90.00 \%$ | 0.9380 | $86.32 \%$ | 0.9046 |

Table 1: Overview of the fictitious unlearning performance using different LLM unlearning approaches under the TOFU fine-tuned LLaMA2-7B-chat model (Maini et al., 2024). 'Original' refers to the original model without unlearning. 'FO' and 'SO' indicate the choice of the unlearning optimizer, either FO unlearning or SOUL. As illustrated in experiment setups, the algorithmic frameworks of LLM unlearning include GA, GradDiff, and PO. The proposed second-order LLM unlearning methods correspond to SO-GradDiff and SO-PO. The $\downarrow$ symbol denotes metrics where lower values indicate better unlearning performance, while $\uparrow$ symbolizes metrics where higher values are preferable, reflecting better retention of model utility. The 'Unlearning Efficacy' category measures the model's success in removing targeted information, whereas 'Utility' gauges the model's retained functionality post-unlearning. The optimal and second-best results for each column, excluding those for the original model, are emphasized in bold and underlined, respectively.

the reject-based answers utilized across various unlearning tasks. In addition to the aforementioned finetuning-based unlearning methods, we also explore an input prompt-enabled unlearning approach proposed by Thaker et al. (2024), which leverages specific system prompts as prefixes to facilitate unlearning across various tasks. Further details on these system prompts are provided in Table A2 of Appendix A.3. AdamW (Loshchilov and Hutter, 2017) is used as the FO optimizer, and Sophia (Liu et al., 2023a) is utilized as the SO optimizer in our proposed SOUL framework presented in Algorithm 1. For detailed information on the hyperparameters used in our implementations, please refer to Appendix A.4.

Evaluation metrics. To evaluate the effectiveness of fictitious unlearning in the TOFU task, we measure the distinguishability of statistical measures between the forget and retain sets using LLMgenerated truthful ratios, as defined in the original TOFU benchmark (Maini et al., 2024). This assessment is conducted via the Kolmogorov-Smirnov (KS) test. We utilize $1-p$-value obtained from the KS test as the Forget Quality to assess unlearning effectiveness. In the experimentation, a high forget quality represents successful unlearning, indicating an increased distributional divergence between the forget and retain sets. This aligns with the objectives of membership inference attacks (MIA) (Song et al., 2019; Hayes et al., 2024) in unlearning, which aim to determine whether individual data points in the forget set were part of the training dataset. Additionally, we assess the performance of the LLM after unlearning (referred to as the unlearned model) by computing the Rouge- $\mathbf{L}$ recall against the ground truth and measuring the accuracy of the generated text. This involves comparing the cosine similarity of semantic embeddings from Sentence-BERT (Reimers and Gurevych, 2019) with both the ground truth and alternative incorrect responses in the TOFU dataset. Correctness is determined when the semantic embedding of the generated text is closest to the ground truth. We apply the same accuracy and Rouge-L recall metrics to evaluate utility preservation on sets related to retained information, real authors, and world facts.

In the copyright removal task, we randomly truncate 300 excerpts from the original Harry Potter dataset to the first $k$ tokens and evaluate them using BLEU and Rouge-L recall for prompt lengths of 100 and 300 tokens, with text completion instructions found in Appendix A.5. In the model detoxification task, toxicity is assessed using real toxic prompts (Gehman et al., 2020) and the PKUSafeRLHF test set (Ji et al., 2024), assigning toxicity scores with Toxic-BERT (Hanu and Unitary team, 2020). For both the copyright removal and detoxification tasks, utility preservation is assessed using the LM Evaluation Harness (Gao et al., 2023) to compute perplexity (PPL) on the Wikitext (Merity et al., 2016) and mean zero-shot accuracy across tasks, detailed in Appendix A.6. Additional evaluations include TruthfulQA (Lin et al., 2021).

### 5.2 Results on fictitious unlearning in TOFU

In Table 1, we showcase the unlearning effectiveness and the preserved model utility following the application of various LLM unlearning methods to the TOFU fine-tuned LLM (Maini et al., 2024), with a focus on comparing FO (first-order) unlearning with the proposed SO unlearning, SOUL. As
we can see, SOUL-based methods significantly outperform their FO counterparts (FO-GradDiff vs. SO-GradDiff, and FO-PO vs. SO-PO) in the efficacy measurements of LLM unlearning. This is evident from the enhanced forget quality, accuracy, and Rouge-L scores on the forget set. Moreover, SOUL-based methods effectively preserve the model's utility post-unlearning. This is evident from their competitive utility performance compared to FO-GradDiff or FO-PO, as well as the improvement over FO-GA and the input promptoriented unlearning method (Thaker et al., 2024). Among the studied unlearning methods, SO-PO (second-order preference optimization) appears to strike a graceful balance between unlearning effectiveness and utility preservation.

Furthermore, we provide visualizations in Table 2 to illustrate examples of the model's outputs post-unlearning in the TOFU task. These visualizations highlight that SO-PO achieves the most favorable outcomes, accurately answering utilityrelated questions and appropriately declining to answer questions from the forget set. In contrast, methods based on GradDiff tend to produce nonsensical sentences on the forget set. From a user perspective, the explicit rejection by SO-PO is seen as more sensible given the preserved utility. This observation is corroborated by performance on the world facts dataset, where GradDiff fails to deliver accurate responses as effectively as PO.

$\left.\begin{array}{c|c}\hline \hline \text { Question from forget set (forget efficacy): } \\ \text { What is the name of a highly acclaimed book by Hsiao Yun-Hwa in the field of leadership? }\end{array}\right]$

Table 2: Example of generated texts from different unlearned models in the TOFU dataset. Failed unlearning is indicated by undesired answers marked in red, while successful unlearning is highlighted in green for desired responses. More examples are provided in Appendix A.7.

### 5.3 Results on copyright removal

Table 3 presents the unlearning efficacy and model utility of the proposed SO unlearning methods and baselines in the task of 'Who's Harry Potter' copyrighted information removal across two LLMs finetuned on the Harry Potter book series dataset (Eldan and Russinovich, 2023). Consistent with our observations in the TOFU task, SOUL substantially improves the unlearning efficacy. For example, the comparison between FO-GradDiff and SO-GradDiff shows a notable decrease in BLEU score (by 0.21) at a prompt length of 300 in the LLaMA2-7B model. This decrease suggests that the generated texts deviate further from the original book's content. Furthermore, the enhancements observed in both perplexity (PPL) and zero-shot accuracy with SOUL over FO unlearning highlight a superior balance between forget efficacy and utility preservation. Similar to the TOFU task, the GA method struggles to balance forget efficacy with utility preservation. Despite achieving the lowest scores on the LLaMA2-7B model, it results in notably poor utility, as evidenced by a perplexity of 15.66, substantially higher than other methods. Table A5 in Appendix A. 7 showcases visualization examples, further demonstrating the enhanced performance of SOUL.

| Method | Unlearning efficacy |  |  |  | Utility |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Prompt <br> BLEU $\downarrow$ | Length 100 <br> ROUGEL $\downarrow \downarrow$ | Prompt <br> BLEU $\downarrow$ | Length 300 <br> ROUGEL $\downarrow$ | PPL $\downarrow$ | Zero-shot Acc. $\uparrow$ | TruthfulQA $\uparrow$ |
| OPT-1.3B |  |  |  |  |  |  |  |
| Original | .3288 | 0.1701 | 6.8797 | 0.2453 | 59.33 | $46.69 \%$ | 0.2313 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_0c8aa934f7d7849300b7g-08.jpg?height=32&width=144&top_left_y=1687&top_left_x=1073) | 6.3288 <br> 5750 | 0.1701 <br> 0.1725 | 6.8797 <br> 60775 | 0.2453 | 59.33 | $46.69 \%$ | 0.2313 |
| FO-GA | 5.7520 | 0.1725 | 6.0775 | 0.2421 | $71.04 \quad$ | $46.31 \% \quad$ | 0.2301 |
| FO-GradDiff | 1.8633 | 0.1681 | 2.8236 | 0.2160 | 37.25 | $46.33 \%$ | 0.2632 |
| SO-GradDiff (Ours) | 0.7841 | 0.1090 | 1.3476 | 0.1480 | 34.09 | $46.80 \%$ | 0.2277 |
| FO-PO | 0.9805 | 0.0620 | 2.2445 | $\underline{0.0815}$ | 24.98 | $45.76 \%$ | 0.2607 |
| SO-PO (Ours) | 0.6456 | $\overline{0.0476}$ | $\underline{1.8619}$ | $\overline{0.0707}$ | 24.08 | $46.69 \%$ | $\underline{0.2387}$ |
| LLaMA2-7B |  |  |  |  |  |  |  |
| Original | 4.6489 | 0.1565 | 3.4986 | 0.1637 | 10.73 | $61.31 \%$ | 0.2729 |
| Input-based | 4.6489 | 0.1565 | 3.4984 | 0.1637 | 10.73 | $61.31 \% \quad$ | 0.2729 |
| FO-GA | 0.0135 | 0.0015 | 0.0279 | 0.0013 | 15.66 | $59.91 \%$ | 0.2791 |
| FO-GradDiff | $\mid 0.2521$ | 0.0247 | $\mid 0.6345$ | 0.0476 | $\mid 11.18$ | $60.06 \%$ | 0.2681 |
| SO-GradDiff (Ours) | $\underline{0.1577}$ | $\underline{0.0117}$ | $\underline{0.4243}$ | $\underline{0.0180}$ | 10.66 | $60.04 \%$ | 0.2595 |
| FO-PO | 0.3120 | 0.0495 | 0.8530 | 0.0750 | 9.48 | $61.14 \%$ | 0.2950 |
| SO-PO (Ours) | 0.2499 | 0.0435 | 0.5284 | 0.0496 | $\overline{9.47}$ | $60.12 \%$ | $\underline{0.2827}$ |

Table 3: Performance of different unlearning methods on copyright removal across two LLMs, following the format of Table 1. The unlearning efficacy is evaluated using prompt lengths of 100 and 300 on the Harry Potter book series dataset (Eldan and Russinovich, 2023).

### 5.4 Results on LLM detoxification

In Table 4, we demonstrate that the proposed SO unlearning methods effectively reduce the toxicity score on both the Real Toxicity Prompts and PKUSafeRLHF datasets while maintaining or even improving utility. For instance, in the LLaMA2-7B model, SO-PO achieved a clear reduction in the
toxic score on the PKU-SafeRLHF dataset and showed enhanced performance in zero-shot accuracy compared to FO-PO. This indicates improved unlearning efficacy of SOUL without sacrificing model utility. In addition, Table A6 includes visualizations that exemplify the outputs after the application of unlearning to the LLaMA2-7B models. These visualizations further corroborate that SO optimizers improve unlearning efficacy, particularly highlighting that SO-PO achieves the most effective unlearning performance.

| Method | Forget efficacy |  | Utility |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  | Real Toxicity Prompts <br> Toxic Score $\downarrow$ | PKU-SafeRLHF <br> Toxic Score $\downarrow$ | PPL $\downarrow$ | Zero-shot Acc. $\uparrow$ | TruthfulQA1 |
| OPT-1.3B |  |  |  |  |  |
| Original | 0.0807 | 0.1118 | 16.49 | $48.16 \%$ | 0.2411 |
| FO-GradDiff | 0.0748 | 0.0673 | 30.87 | $41.16 \%$ | 0.2362 |
| SO-GradDiff (Ours) | 0.0561 | 0.0618 | 28.77 | $40.34 \%$ | 0.2240 |
| FO-PO | $\underline{0.0404}$ | $\underline{0.0253}$ | 18.26 | $46.25 \%$ | 0.2852 |
| SO-PO (Ours) | $\overline{0.0335}$ | $\overline{0.0165}$ | 17.97 | $48.60 \%$ | $\underline{0.2742}$ |
| LLaMA2-7B |  |  |  |  |  |
| Original | 0.0710 | 0.1027 | 8.79 | $62.08 \%$ | 0.2521 |
| FO-GradDiff | 0.0708 | 0.0989 | 8.77 | $61.38 \%$ | 0.2534 |
| SO-GradDiff (Ours) | 0.0722 | 0.0987 | 8.79 | $61.32 \%$ | 0.2534 |
| FO-PO | 0.0626 | $\underline{0.0790}$ | 8.78 | $61.92 \%$ | 0.2632 |
| SO-PO (Ours) | 0.0528 | $\frac{0.0443}{0.043}$ | $\frac{1}{8.87}$ | $\frac{02.80 \%}{620 \%}$ | 0.2656 |

Table 4: Performance comparison between SOUL and its FO counterparts in the task of model detoxification, following the format of Table 3 .

## 6 Conclusions

In this paper, we investigate the role of optimizer choice in LLM unlearning, linking second-order optimization to influence unlearning. Building on this, we propose a second-order LLM unlearning framework, agnostic to loss function, to augment existing approaches. Extensive experiments across various unlearning tasks, models, and metrics consistently show the superiority of second-order unlearning. These results advocate for the development and adoption of optimizers tailored for effective LLM unlearning.

## 7 Limitations

This study, while highlighting the significance of second-order optimization for LLM unlearning, may also have a few limitations that should be addressed in future research:

Model scale limitation: Our experiments primarily focused on models like OPT-1.3B and LLaMA27b. However, larger models, such as expanded variants of LLaMA, are increasingly common. The computational demands and unique characteristics of these larger models may affect the applicability or effectiveness of second-order unlearning techniques. Further investigation on larger-scale mod- els is warranted to understand their behavior under second-order optimization.

Robustness of unlearning: The robustness of second-order unlearning has not been comprehensively tested. This includes their performance stability across diverse jailbreaking attacks, as well as their ability to handle dynamic changes in the unlearning targets over time. Further research is needed to evaluate the resilience of second-order unlearning under various adversarial scenarios and evolving unlearning objectives.

## 8 Acknowledgement

We thank the U.S. Department of Energy via Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 and the LLNLLDRD Program under Project No. 23-ER-030 for their support (LLNL-JRNL-863628). Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu and Sijia Liu were also partially supported by the National Science Foundation (NSF) Robust Intelligence (RI) Core Program Award IIS-2207052.

## References

Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, et al. 2023. Identifying and mitigating the security risks of generative ai. Foundations and Trends $®$ in Privacy and Security, 6(1):1-52.

Mokhtar S Bazaraa, Hanif D Sherali, and Chitharanjan M Shetty. 2013. Nonlinear programming: theory and algorithms. John wiley \& sons.

Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, pages 7432-7439.

Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy $(S P)$, pages 141-159. IEEE.

Stephen P Boyd and Lieven Vandenberghe. 2004. Convex optimization. Cambridge university press.

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.

Yinzhi Cao and Junfeng Yang. 2015. Towards making systems forget with machine unlearning. In 2015 IEEE symposium on security and privacy, pages 463480. IEEE.

Jiaao Chen and Diyi Yang. 2023. Unlearn what you want to forget: Efficient unlearning for llms. arXiv preprint arXiv:2310.20150.

Min Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, and Chen Wang. 2023. Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7766-7775.

Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and Yang Zhang. 2022. Graph unlearning. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, pages 499-513.

François Chollet. 2019. On the measure of intelligence. arXiv preprint arXiv:1911.01547.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics.

Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine learning challenges workshop, pages 177-190. Springer.

Ronen Eldan and Mark Russinovich. 2023. Who's harry potter? approximate unlearning in llms.

Chongyu Fan, Jiancheng Liu, Alfred Hero, and Sijia Liu. 2024a. Challenging forgets: Unveiling the worst-case forget sets in machine unlearning. arXiv preprint arXiv:2403.07362.

Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. 2024b. Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation. In International Conference on Learning Representations.

Rohit Gandikota, Joanna Materzynska, Jaden FiottoKaufman, and David Bau. 2023. Erasing concepts from diffusion models. arXiv preprint arXiv:2303.07345.

Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf,
Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation.

Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462.

Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. 2019. Making ai forget you: Data deletion in machine learning. Advances in neural information processing systems, 32 .

Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304-9312.

Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo. 2016. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. arXiv preprint arXiv:1607.05447.

Laura Graves, Vineel Nagisetty, and Vijay Ganesh. 2021. Amnesiac machine learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11516-11524.

Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et al. 2023. Studying large language model generalization with influence functions. arXiv preprint arXiv:2308.03296.

Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. 2019. Certified data removal from machine learning models. arXiv preprint arXiv:1911.03030.

Laura Hanu and Unitary team. 2020. Detoxify. Github. https://github.com/unitaryai/detoxify.

Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, and Nicolas Papernot. 2024. Inexact unlearning needs more careful evaluations to avoid a false sense of privacy. arXiv preprint arXiv:2403.01218.

Chris Jay Hoofnagle, Bart van der Sloot, and Frederik Zuiderveen Borgesius. 2019. The european union general data protection regulation: what it is and what it means. Information \& Communications Technology Law, 28(1):65-98.

Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089.

Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. 2021. Approximate data deletion from machine learning models. In International Conference on Artificial Intelligence and Statistics, pages 2008-2016. PMLR.

Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022. Knowledge unlearning for mitigating privacy risks in language models. arXiv preprint arXiv:2210.01504.

Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2024. Beavertails: Towards improved safety alignment of 11 mia a humanpreference dataset. Advances in Neural Information Processing Systems, 36.

Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia Liu. 2023. Model sparsity can simplify machine unlearning. In Thirty-seventh Conference on Neural Information Processing Systems.

Antonia Karamolegkou, Jiaang Li, Li Zhou, and Anders Søgaard. 2023. Copyright violations and large language models. arXiv preprint arXiv:2310.13771.

Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In International conference on machine learning, pages 1885-1894. PMLR.

Hadas Kotek, Rikker Dockum, and David Sun. 2023. Gender bias and stereotypes in large language models. In Proceedings of The ACM Collective Intelligence Conference, pages 12-24.

Steven George Krantz and Harold R Parks. 2002. The implicit function theorem: history, theory, and applications. Springer Science \& Business Media.

Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. 2023. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22691-22702.

Meghdad Kurmanji, Peter Triantafillou, and Eleni Triantafillou. 2023. Towards unbounded machine unlearning. arXiv preprint arXiv:2302.09880.

Guihong Li, Hsiang Hsu, Radu Marculescu, et al. 2024a. Machine unlearning for image-to-image generative models. arXiv preprint arXiv:2402.00351.

Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D Li, AnnKathrin Dombrowski, Shashwat Goel, Long Phan, et al. 2024b. The wmdp benchmark: Measuring and reducing malicious use with unlearning. arXiv preprint arXiv:2403.03218.

Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.
Gaoyang Liu, Xiaoqiang Ma, Yang Yang, Chen Wang, and Jiangchuan Liu. 2020. Federated unlearning. arXiv preprint arXiv:2012.13891.

Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. 2023a. Sophia: A scalable stochastic second-order optimizer for language model pretraining. arXiv preprint arXiv:2305.14342.

Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R Varshney, et al. 2024a. Rethinking machine unlearning for large language models. arXiv preprint arXiv:2402.08787.

Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang. 2024b. Towards safer large language models through machine unlearning. arXiv preprint arXiv:2402.10058.

Ziyao Liu, Yu Jiang, Jiyuan Shen, Minyi Peng, KwokYan Lam, and Xingliang Yuan. 2023b. A survey on federated unlearning: Challenges, methods, and future directions. arXiv preprint arXiv:2310.20448.

Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.

Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve gpt-3 after deployment. arXiv preprint arXiv:2201.06009.

Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. 2024. Tofu: A task of fictitious unlearning for llms.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789.

Fabio Motoki, Valdemar Pinho Neto, and Victor Rodrigues. 2023. More human than human: Measuring chatgpt political bias. Available at SSRN 4372349.

Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee. 2023. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035.

Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. 2022. A survey of machine unlearning. arXiv preprint arXiv:2209.02299.

Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. 2023. In-context unlearning: Language models as few shot unlearners. arXiv preprint arXiv:2310.07579.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67.

Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.

Jeffrey Rosen. 2011. The right to be forgotten. Stan. $L$. Rev. Online, 64:88.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106.

Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. 2021. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems, 34:18075-18086.

Sidak Pal Singh and Dan Alistarh. 2020. Woodfisher: Efficient second-order approximation for neural network compression. Advances in Neural Information Processing Systems, 33:18098-18109.

Liwei Song, Reza Shokri, and Prateek Mittal. 2019 Privacy risks of securing machine learning models against adversarial examples. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 241-257.

Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. 2024. Trustllm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561.

Pratiksha Thaker, Yash Maurya, and Virginia Smith. 2024. Guardrail baselines for unlearning in llms. arXiv preprint arXiv:2403.03329.

Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. 2022. Unrolling sgd: Understanding factors influencing machine unlearning. In 2022 IEEE 7th European Symposium on Security and Privacy (EuroS\&P), pages 303-319. IEEE.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Enayat Ullah, Tung Mai, Anup Rao, Ryan A Rossi, and Raman Arora. 2021. Machine unlearning via algorithmic stability. In Conference on Learning Theory, pages 4126-4142. PMLR.

Junxiao Wang, Song Guo, Xin Xie, and Heng Qi. 2022. Federated unlearning via class-discriminative pruning. In Proceedings of the ACM Web Conference 2022, pages 622-632.

Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, and Hongzhi Yin. 2023. Kga: A general machine unlearning framework based on knowledge gap alignment. arXiv preprint arXiv:2305.06535.

Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. 2021. Machine unlearning of features and labels. arXiv preprint arXiv:2108.11577.

Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, and Minlie Huang. 2023. Unveiling the implicit toxicity in large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing.

Kun Wu, Jie Shen, Yue Ning, Ting Wang, and Wendy Hui Wang. 2023a. Certified edge unlearning for graph neural networks. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2606-2617.

Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong. 2023b. Depn: Detecting and editing privacy neurons in pretrained language models. arXiv preprint arXiv:2310.20138.

Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and Xiang Yue. 2024. Machine unlearning of pre-trained large language models. arXiv preprint arXiv:2402.15159.

Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2023. Large language model unlearning. arXiv preprint arXiv:2310.10683.

Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. 2023. Unlearning bias in language models by partitioning gradients. In Findings of the Association for Computational Linguistics: ACL 2023, pages 6032-6048.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830.

Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, and Xiwei Xu. 2023a. Right to be forgotten in the era of large language models: Implications, challenges, and solutions. arXiv preprint arXiv:2307.03941.

Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. 2023b. Forget-me-not: Learning to forget in text-to-image diffusion models. arXiv preprint arXiv:2303.17591.

Guanhua Zhang, Yihua Zhang, Yang Zhang, Wenqi Fan, Qing Li, Sijia Liu, and Shiyu Chang. 2022a. Fairness reprogramming. Advances in Neural Information Processing Systems, 35:34347-34362.

Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. 2023c. Composing parameter-efficient modules with arithmetic operations. arXiv preprint arXiv:2306.14870.

Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. 2024. Negative preference optimization: From catastrophic collapse to effective unlearning. arXiv preprint arXiv:2404.05868.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022b. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.

Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis, Yuguang Yao, Mingyi Hong, and Sijia Liu. 2023d. An introduction to bi-level optimization: Foundations and applications in signal processing and machine learning. arXiv preprint arXiv:2308.00788.

Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. 2023. Can we edit factual knowledge by in-context learning? arXiv preprint arXiv:2305.12740.
