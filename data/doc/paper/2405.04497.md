# Unveiling Disparities in Web Task Handling Between Human and Web Agent 

Kihoon Son<br>kihoon.son@kaist.ac.kr<br>School of Computing, KAIST<br>Daejeon, Republic of Korea<br>Tae Soo Kim<br>taesoo.kim@kaist.ac.kr<br>School of Computing, KAIST<br>Daejeon, Republic of Korea

Jinhyeon Kwon<br>athexplorer@kaist.ac.kr<br>School of Computing, KAIST<br>Daejeon, Republic of Korea<br>Young-Ho Kim<br>yghokim@younghokim.net<br>NAVER AI Lab<br>Seongnam, Republic of Korea<br>Juho Kim<br>juhokim@kaist.ac.kr<br>School of Computing, KAIST<br>Daejeon, Republic of Korea

DaEun Choi<br>daeun.choi@kaist.ac.kr<br>School of Computing, KAIST<br>Daejeon, Republic of Korea<br>Sangdoo Yun<br>sangdoo.yun@navercorp.com<br>NAVER AI Lab<br>Seongnam, Republic of Korea


#### Abstract

With the advancement of Large-Language Models (LLMs) and Large Vision-Language Models (LVMs), agents have shown significant capabilities in various tasks, such as data analysis, gaming, or code generation. Recently, there has been a surge in research on web agents, capable of performing tasks within the web environment. However, the web poses unforeseeable scenarios, challenging the generalizability of these agents. This study investigates the disparities between human and web agents' performance in web tasks (e.g., information search) by concentrating on planning, action, and reflection aspects during task execution. We conducted a web task study with a think-aloud protocol, revealing distinct cognitive actions and operations on websites employed by humans. Comparative examination of existing agent structures and human behavior with thought processes highlighted differences in knowledge updating and ambiguity handling when performing the task. Humans demonstrated a propensity for exploring and modifying plans based on additional information and investigating reasons for failure These findings offer insights into designing planning, reflection, and information discovery modules for web agents and designing the capturing method for implicit human knowledge in a web task.


## CCS CONCEPTS

- Human-centered computing $\rightarrow$ Interactive systems and tools.


## KEYWORDS

Web Agent; Human Behavior; Qualitative Analysis; Knowledge Capturing; Large-Language Model; Large Vision-Language Model[^0]

## ACM Reference Format:

Kihoon Son, Jinhyeon Kwon, DaEun Choi, Tae Soo Kim, Young-Ho Kim, Sangdoo Yun, and Juho Kim. 2024. Unveiling Disparities in Web Task Handling Between Human and Web Agent. In Proceedings of CHI Conference on Human Factors in Computing Systems (CHI '24 Computational UI Workshop). ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn. nnnnnnn

## 1 INTRODUCTION

As Large-Language Models (e.g., GPT-4 [9]) and Large VisionLanguage Models (LVM; GPT-4V [1]) continue to advance, research on agents has shown that agents are capable of performing diverse tasks to support humans as well as in diverse formats (e.g., single or multi agents framework). For example, LLM-based agents perform tasks such as data analysis [4], gaming [17], and code generation [17]. Also, recent agent work leverages a multi-agent framework with diverse personas to support human decision-making processes [10, 13]. These LLM-based agents typically incorporate four fundamental components: Profiling, Memory, Planning, and Action [12]. Agents with this architecture operate by designing plans for tasks akin to humans, retrieving necessary information from knowledge memory, and subsequently executing the most appropriate actions based on the current context [14].

Recently, with the capability of understanding graphic user interface (GUI), web agents based on LLM and LVM have shown to be capable of performing tasks within the web environment [6], such as shopping or finding specific information in a social community. To successfully accomplish tasks on the web, these agents require not only a basic understanding of graphic user interface (GUI) but also appropriate planning on how to perform given tasks and accurately execute an action [2]. Researchers have shown high interest in how to provide the visual grounding $[15,16]$ to the agent and generate a task plan with sub-plans [8] and how to design an agent structure [8].

However, the web environment presents many unforeseeable challenges (e.g., encountering irrelevant search results or unconventional UI and interaction design), leading to a decrease in the
generalizability of agents in web tasks [8]. When faced with unexpected scenarios, humans contemplate reasons and seek alternative solutions while planning for the next action. However, current web agents lack modules capable of performing such functions. Moreover, while humans can perform given tasks on unfamiliar websites through basic UI understanding and trial and error, web agents still lack this ability. To address this limitation, ScreenAgent [8] has introduced a module capable of reflecting on its own actions, and FRIDAY [14] has proposed module designs considering the biological nature of the human brain to make an agent behave like a human in performing the task. However, the low performance of these agents might imply the existence of implicit knowledge or behavior patterns unique to humans on the web task to be unpacked.

Then, what are the differences between humans and web agents in performing web tasks? What distinguishes agents from humans in terms of planning, action, and reflection when executing web tasks? Furthermore, what implications can be derived from these differences for agent design? To answer these questions, we conducted a think-aloud study where four participants conducted two different web tasks typically executed by current web agents, such as shopping and finding information about a post. Through a qualitative coding analysis on the think-aloud transcripts, we identified the human's cognitive actions (Table 1) and operations on the web (Table 2). The tables illustrate the types of planning humans make, the decisions they make, and how they perform rollback in plans when something goes wrong in web tasks.

Furthermore, by comparing participants' task execution processes with ScreenAgent [8]'s prompting structure and its modules, we identified that humans update their knowledge space (task- and task website-related knowledge) by exploring and discovering new information. Our results also reveal that humans exhibit a pattern of identifying and clarifying ambiguous aspects of tasks through an additional information search beyond simply performing tasks according to constructed plans. Unlike agents, humans investigate reasons for failure or discrepancies between their knowledge and the information they are viewing, and based on this exploration, they modify their plans. Based on these findings, we discuss design implications for additional modules needed in agents and how to capture implicit knowledge and behavior patterns unique to humans during task execution processes. Our discussion also includes the methods for capturing humans' tacit knowledge within the web environment to transfer this knowledge to the agent.

## 2 MAIN STUDY

To investigate how humans execute web tasks in depth, we conducted a think-aloud study with four participants on the two different web tasks.

### 2.1 Study Design

2.1.1 Task design. The tasks of study are a shopping task on the Amazon website ${ }^{1}$ and an information search task on Reddit ${ }^{2}$. We selected the tasks based on the VisualWebArena paper [6] since these are the tasks commonly used in web agent research. The two tasks are:[^1]- Find the cheapest Lavazza Gran Aroma coffee capsule on Amazon.
- Find the title of the_Champion's Hot post at r/midjourney ten months ago.

2.1.2 Participant. We recruited a total of four participants. Two people are familiar with the task and the given websites, and the other two people aren't. This recruitment would allow us to observe how people unfamiliar with the website perceive, understand, and respond to the given task.

2.1.3 Procedure. The study started with a brief introduction by the authors, followed by the execution of the two tasks by the participants. We instructed them to think aloud in terms of their planning, action, and reflection. Specifically, we asked them to explain the rationale or intent behind their task planning and action. Then, the participants conducted the two tasks in random order until they completed or quit the given task. After conducting the two tasks, the study finished.

### 2.2 Analysis

To analyze the think-aloud transcript data, we conducted an open coding [3, 5] on the think-aloud transcript data.

First, one author organized the transcripts while monitoring recorded study sessions. In cases where verbal explanations lacked context (such as instances involving pronouns like "this" and "that" in the think-aloud process), the author checked the specific situation from the recorded video. Through this process, we obtained 156 raw data from transcripts in all study sessions.

Second, the two authors jointly conducted all stages of qualitative coding. Without further interpretation, the authors initially classified the raw data into similar codes first and defined names for the codes. Next, the authors reviewed the defined codes together, merging similar codes and splitting codes if different types of raw data existed within the same code. The entire process continued until a consensus was reached between the two authors.

Through this process, the 156 transcript raw data were classified into 39 codes. The two authors then defined 14 themes by grouping similar codes again. Similarly, consensus was sought between the two authors in the process of grouping and separating themes. Finally, the grouped themes were divided into cognitive aspects of action (Table 1) and operational (Table 2) aspects performed on the actual website when humans perform web tasks.

Finally, by assigning the themes determined through qualitative coding back to the original raw data, we created task action sequences of cognitive actions and operations performed by participants during web tasks. In these sequences, each participant's think-aloud data was assigned the themes in Table 1 and 2. From a total of 8 sequences ( 4 participants and 2 tasks), we further analyzed the most prevalent patterns by calculating sequence pairs and triples (Figure 1).

## 3 RESULTS

This section illustrates the results of the analysis of the think-aloud study. Table 1 and Table 2 show the human's cognitive actions and operation on the web observed in the study. Based on these results, we investigated the core difference between the recent agents.

Table 1: Qualitative coding results regarding human cognitive actions in performing web tasks.

|  | Theme | Code | Count | Ratio |
| :---: | :---: | :---: | :---: | :---: |
| Cognitive action | Task planning | - Establishing a non-specific initial task plan <br> - Establishing an additional auxiliary plan to know unknown information <br> - Establishing a verification plan for discerning the truth of the found information or answer <br> - Establishing an alternative plan | 24 | $15.38 \%$ |
|  | Task condition checking | - Recalling the task conditions <br> - Identifying ambiguous parts in the task conditions | 6 | $3.58 \%$ |
|  | Information processing | - Collecting information helpful for performing the task <br> - Comparing derived answers and newly discovered information <br> - Comparing prior knowledge and currently observed information | 3 | $1.92 \%$ |
|  | Discovering | - Discovering task-related knowledge from currently observed information <br> - Discovering task website-related knowledge (e.g., function) through UI understanding, intuition, or trial. <br> - Discovering that the current plan is flawed based on the currently observed information | 41 | $26.28 \%$ |
|  | Decision | - Deciding on the information to search <br> - Deciding on answer candidates <br> - Deciding on whether to maintain the current plan <br> - Deciding on defining the ambiguous parts in task conditions | 14 | $8.97 \%$ |
|  | Complete task | - Completing the task through comparison of currently observed information with the derived answers <br> - Completing the task by confirming satisfaction of task conditions <br> - Completing the task based on prior knowledge of the task website <br> - Completing the task by clarifying the vague condition | 7 | $4.49 \%$ |
|  | Reflection | - Contemplating the reasons for the current plan's failure <br> - Contemplating the differences between prior knowledge and newly found information <br> - Contemplating whether there has been an information gain from the information currently being viewed | 6 | $3.85 \%$ |

### 3.1 Cognitive Actions and Website Operations

Qualitative coding result (Table 1) reveals that in performing web tasks, humans engage in various cognitive actions beyond task planning and reflection. Particularly, during the task process, humans frequently discover task-related knowledge from the information they are currently observing or uncover functionalities of the website previously unknown to them through UI understanding (e.g., catching a website's functionality through UI design) and intuition. Task-related knowledge refers to information unknown within task conditions (e.g., the fact that Lavazza capsules come in various types), while task website functionalities denote commands or features used within the website (e.g., how to find a specific username on Reddit using $u /$ ). Participants familiar with the task utilized such information as prior knowledge, whereas those unfamiliar with the task progressed through exploration to clarify these unclear aspects while performing the task.

From the operational aspect of tasks performed on websites (Table 2), it is evident that humans engage in search and exploration operations more frequently than others when encountering unknown information. Despite participants' high familiarity with website functions, operations such as search and exploration were commonly employed to clarify new information within the given conditions (e.g., types of Lavazza coffee capsules). For instance, in a task requiring participants to find coffee capsules, even those familiar with the website resorted to searching for related information to understand the difference between Lavazza's pot and coffee capsules, thus enabling them to make a clear selection based on Amazon search results. Additionally, participants tended to establish additional auxiliary plans for exploring the unknown aspects rather than entirely redesigning their task plans. They also demonstrated a propensity to formulate plans for verifying newly acquired information. Moreover, during task execution, when the direction of exploration went away or when it was unclear which operation to perform, individuals exhibited a roll-back operation within the established plan, allowing them to revert to a specific point.

### 3.2 Comparing Web Agent and Human

3.2.1 Human's Discovering Action and Knowledge Update Process in Web Task. Our study results show that the participants acquired knowledge relevant to tasks (e.g., the difference between the types of coffee capsules) and task websites (e.g., website function or search results pattern) through their own UI understanding, trial and error, or intuition while performing tasks. Furthermore, they frequently validate whether their current plan is correct or not through these discoveries. Through this process of discovery, participants perform rollback within their plan or additional information exploration operations. On the cognitive action aspect, participants made decisions related to whether to perform reflection based on the information

Table 2: Qualitative coding results regarding human operations on a website in performing web tasks.

|  | Theme | Code | Count | Ratio |
| :---: | :---: | :---: | :---: | :---: |
| Operation | Information search | - Searching the given conditions <br> - Search for specific information directly related to answers <br> - Search for clarification of unknown part in Google | 12 | $7.69 \%$ |
|  | Information exploration | - Exploring specific information by considering the task conditions or unclear parts <br> - Exploring information to verify its irrelevance, despite initial judgment of it being incorrect <br> - Exploring the website functionalities at the beginning of the task <br> - Exploring previously opened tabs that might contain useful information | 16 | $10.26 \%$ |
|  | Roll-back in the plan | - Roll-backing from auxiliary plan to auxiliary plan <br> - Roll-backing from auxiliary plan to main plan <br> - Roll-backing to start point <br> - Roll-backing from main plan to main plan | 9 | $5.77 \%$ |
|  | Start task | - Starting task with prior knowledge like prefix of website address <br> - Starting the task by navigating to the task website through a Google search | 7 | $4.49 \%$ |
|  | Trial and error | - Trying despite uncertainty about the success of the action | 2 | $1.28 \%$ |
|  | General function | - Click on the next step, filtering, or sorting | 7 | $4.49 \%$ |
|  | Task specific function | - Specific task website functions like $A d d$ to cart | 2 | $1.28 \%$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_5658421ed3445f318ed3g-4.jpg?height=558&width=1550&top_left_y=1312&top_left_x=274)

Figure 1: Analysis results of participant sequence data pairs and triples based on the themes of the qualitative coding.

discovered, conduct information searches or exploration, determine answers, or maintain the current plan.

To update the discovered knowledge during the task, the current structure of web agents requires a framework that can distinguish and update knowledge about the given task itself and knowledge about the task website based on the information discovered. In the process of discovering information, web agents might need a structure that can capture and reflect the above information in the next decision based on task context or general website knowledge (Website structure or UI) gained from past experiences.
3.2.2 Information Exploration with Trial and Error. Current web agents mainly execute actions that correspond to sub-plans once a plan is formulated [8]. From observing participants unfamiliar with the task, we noted that they sought to clarify vague parts of the task through exploration. An interesting point is that participants not only explore potentially useful information but also the functionality of the task website and sometimes inappropriate information. Ultimately, after the exploration, they make decisions about which information is useful and which is not. This allows the participants to expand their knowledge space related to the taskor task website-related knowledge.

3.2.3 Differences of Reflection on What. ScreenAgent [8] executes a specific action for a given sub-plan and performs reflection by observing changes on the screen before and after execution. Through this process, it either 1) repeats the action, 2) revises the plan, or 3) proceeds to the next step.

In contrast to this structure, our study results indicate that humans engage in a more complex reflection process. Humans reflect not only on why a plan failed but also on how newly acquired information differs from their existing knowledge and whether the information gained adds new value to their knowledge space beyond simply understanding why a plan with an action failed.

Based on this reflection, humans revise plans, perform rollbacks within plans, or determine the direction of information exploration within the knowledge space. Unlike current web agents, humans extend their reflection beyond evaluating whether actions performed for sub-plans were executed correctly. They delve into reasons for failure and possibilities for remediation and identify additional information that may require exploration.

## 4 DISCUSSION

Based on the study's results, this section explains how web agents would be designed and discusses the limitations of think-aloud studies in capturing implicit human behavior and knowledge.

### 4.1 Design Implications for Web Agent

Web agents must possess not only knowledge about the given task but also the necessary knowledge within the task website. In the study, participants demonstrated updating both forms of knowledge, unlike web agents. Beyond simply providing task instructions as prompts, there is a need for modules capable of understanding knowledge about the tasks themselves, comparing them with new information discovered during tasks, and updating accordingly. Additionally, to respond to various forms and designs of website environments appropriately, web agents would require a combination of general web knowledge applicable across websites and specialized web knowledge tailored to specific websites or tasks.

In addition to incorporating modules related to knowledge, it may be possible to design modules within the agent for human processes such as reflection, planning, roll-back, and decision-making. The relationships between these processes and how they interact during task execution may require further exploration, depending on the complexity of the tasks. However, initially, it would be essential to delineate the processes present in humans and design them as modules within the agent. Research into how these newly designed modules could enhance the current performance of the agent would be necessary.

### 4.2 Capturing Human's Tacit Knowledge

In our study, we observed that people vary in the depth of their explanations level, and it may not comprehensively capture all processes with the think-aloud protocol [7]. Although the study tasks were simpler than open-ended tasks without predefined answers, such as a literature survey or design ideation, we observed that participants missed certain aspects when explaining their thoughts and that the level of details of explanation varied between individuals. Particularly, it was challenging for participants to provide clear explanations for how their current actions or explanations related to their previous explanations, behaviors, or their prior knowledge. From the behaviors and thought processes unique to humans, such as the process of acquiring new information or performing tasks based on it, there is a need to improve the think-aloud protocol to clearly capture aspects applicable to agents.

Therefore, in this background, it is important to capture the human's implicit behavior pattern or tacit knowledge within a certain task process to reuse the task-related knowledge [11]. This might also reveal a more detailed implicit process, like how humans conduct the trial-and-error processes or revise what parts of their plan are in a certain situation. Furthermore, if we can create datasets containing even implicit patterns or tacit knowledge and feed them to train and fine-tune agents, they would be able to perform tasks more like humans and offer a wider range of support to people. Through the dataset, to provide general or personalized support within the task, we can also define the ground rules of the task knowledge that various people utilized during the task, as well as the personalized rules or knowledge.

## 5 CONCLUSION

In conclusion, this study sheds light on the disparities observed between human and web agents' performance in web tasks. Through a think-aloud study of web tasks, distinct cognitive actions and operations on websites utilized by humans were revealed. Comparative analysis with existing agent structures underscored differences in knowledge updating and ambiguity handling during task execution. Humans demonstrated a propensity for adaptive planning based on additional information and thorough investigation into reasons for failure. These findings provide design insights into web agent architectures and the development of methods for capturing implicit human knowledge within web tasks. By capturing and transforming implicit behavior patterns with tacit knowledge utilized by individuals during task execution into forms applicable to web agents, we envision a future where web agents can provide more diverse support for humans.

## ACKNOWLEDGMENTS

This work was supported by NAVER-KAIST Hypercreative AI Center. We thank all of our participants for engaging positively in our studies. We also thank all of the members of KIXLAB for their helpful discussions and constructive feedback.

## REFERENCES

[1] 2023. GPT-4V(ision) System Card. https://api.semanticscholar.org/CorpusID: 263218031

[2] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents. ArXiv abs/2401.10935 (2024). https://api.semanticscholar. org/CorpusID:267069082

[3] Juliet Corbin and Anselm Strauss. 1990. Grounded Theory Research: Procedures, Canons and Evaluative Criteria. Zeitschrift für Soziologie 19, 6 (Dec. 1990), 418-427. https://doi.org/10.1515/zfsoz-1990-0602

[4] Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Kun Kuang, Yang Yang, Hongxia Yang, and Fei Wu. 2024. InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. ArXiv abs/2401.05507 (2024). https://api.semanticscholar. org/CorpusID:266933185

[5] Shahedul Huq Khandkar. 2009. Open coding. University of Calgary 23 (2009), 2009. http://pages.cpsc.ucalgary.ca/ saul/wiki/uploads/CPSC681/opencoding.pdf

[6] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. 2024. VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. ArXiv abs/2401.13649 (2024). https://api.semanticscholar.org/CorpusID: 267199749

[7] Rebecca Krosnick, Fraser Anderson, Justin Matejka, Steve Oney, Walter S. Lasecki, Tovi Grossman, and George Fitzmaurice. 2021. Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (<conf-loc>, <city $>$ Yokohama </city>, <country $>$ Japan $</$ country $>,</$ conf-loc $>$ ) (CHI' '21). Association for Computing Machinery, New York, NY, USA, Article 199, 13 pages. https://doi.org/10.1145/3411764.3445066

[8] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, and Qi Wang. 2024. ScreenAgent: A Vision Language Modeldriven Computer Control Agent. ArXiv abs/2402.07945 (2024). https://api. semanticscholar.org/CorpusID:267636776

[9] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]

[10] Jeongeon Park, Bryan Min, Xiaojuan Ma, and Juho Kim. 2023. ChoiceMates Supporting Unfamiliar Online Decision-Making with Multi-Agent Conversational Interactions. ArXiv abs/2310.01331 (2023). https://api.semanticscholar.org/ CorpusID:263605868

[11] Kihoon Son, DaEun Choi, Tae Soo Kim, and Juho Kim. 2024. Demystifying Tacit Knowledge in Graphic Design: Characteristics, Instances, Approaches, and
Guidelines. arXiv:2403.06252 [cs.HC]

[12] Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao ran Yang, Jingsen Zhang, Zhi-Yang Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji rong Wen. 2023. A Survey on Large Language Model based Autonomous Agents. ArXiv abs/2308.11432 (2023). https://api.semanticscholar. org/CorpusID:261064713

[13] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. ArXiv abs/2308.08155 (2023). https://api.semanticscholar.org/CorpusID:260925901

[14] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. 2024. OS-Copilot: Towards Generalist Computer Agents with Self-Improvement. ArXiv abs/2402.07456 (2024). https: //api.semanticscholar.org/CorpusID:267626905

[15] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chun yue Li, and Jianfeng Gao. 2023. Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V. https://api.semanticscholar.org/CorpusID:264172201

[16] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. GPT-4V(ision) is a Generalist Web Agent, if Grounded. ArXiv abs/2401.01614 (2024). https: //api.semanticscholar.org/CorpusID:266741821

[17] Andrew Zhu, Lara J. Martin, Andrew Head, and Chris Callison-Burch. 2023. CALYPSO: LLMs as Dungeon Masters' Assistants. ArXiv abs/2308.07540 (2023). https://api.semanticscholar.org/CorpusID:260900406


[^0]:    Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).

    CHI '24 Computational UI Workshop, May 12, 2024, Honolulu, HI, USA

    (c) 2024 Copyright held by the owner/author(s).

    ACM ISBN 978-x-xxxx-xxxx-x/YY/MM.

    https://doi.org/10.1145/nnnnnnn.nnnnnnn

[^1]:    ${ }^{1}$ https://www.amazon.com/

    ${ }^{2}$ https://www.reddit.com/

