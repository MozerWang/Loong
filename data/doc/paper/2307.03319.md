# Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment 

Roni Rabin $^{1} \quad$ Alexandre Djerbetian ${ }^{1} \quad$ Roee Engelberg ${ }^{1,2} \quad$ Lidan Hackmon $^{1}$<br>Gal Elidan ${ }^{1,3} \quad$ Reut Tsarfaty ${ }^{1,4} \quad$ Amir Globerson ${ }^{1,5}$<br>${ }^{1}$ Google Research ${ }^{2}$ Computer Science Dept., Technion<br>${ }^{3}$ Statistics Dept., Hebrew University of Jerusalem<br>${ }^{4}$ Computer Science Dept., Bar-Ilan University<br>${ }^{5}$ Blavatnik School of Computer Science, Tel Aviv University


#### Abstract

Human communication often involves information gaps between the interlocutors. For example, in an educational dialogue, a student often provides an answer that is incomplete, and there is a gap between this answer and the perfect one expected by the teacher. Successful dialogue then hinges on the teacher asking about this gap in an effective manner, thus creating a rich and interactive educational experience. We focus on the problem of generating such gap-focused questions (GFQs) automatically. We define the task, highlight key desired aspects of a good GFQ, and propose a model that satisfies these. Finally, we provide an evaluation by human annotators of our generated questions compared against human generated ones, demonstrating competitive performance.


## 1 Introduction

Natural language dialogues are often driven by information gaps. Formally, these are gaps between the epistemic states of the interlocutors. Namely, one knows something that the other does not, and the conversation revolves around reducing this gap. An important example is the education setting where teachers ask students questions, and receive answers that may be incomplete. With the expectation of what a complete answer should contain, the teacher then engages in a gap-focused dialogue to help the student to arrive at a complete answer. There are multiple other application settings of information gaps, including support-line bots, longform $\mathrm{Q} \& \mathrm{~A}$, and automated fact checking.

The core challenge in this setting is how to generate effective questions about the information gap. In terms of formal semantics and pragmatics, this gap can be viewed as the complementary of the common-ground (Stalnaker, 2002) held by the interlocutors. Somewhat surprisingly, despite much work on dialogue learning (Ni et al., 2022; Zhang et al., 2020) and question generation (Michael et al.,
2018; Pyatkin et al., 2020, 2021; Ko et al., 2020), little attention has been given to generating questions that focus on such information gaps.

The formal traditional approach to representing the dialogic information gap is via the set of propositions that are known to one side but not the other (Stalnaker, 2002). However, this set can be quite large, and it is also unclear how to turn these propositions into dialogue utterances. We propose an arguably more natural representation: a generated set of natural language questions whose answers represent the information that the teacher needs to ask about to reduce the gap. We call these gapfocused questions (GFQs). A key advantage of this representation is that the generated questions can be used directly in the teacher-student dialogue.

Given a complete teacher answer and a partial student answer, there are many questions that could be asked, but some are more natural than others. For example, consider the complete answer "A man is wearing a blue hat and a red shirt and is playing a guitar", and a student response "There is a man playing the guitar". Two candidate questions could be "What color hat is the man wearing?" and "What is the man wearing?". The second question is arguably more natural as it does not reveal information that is not in the teacher-student common ground, namely that a hat is being worn.

The above demonstrates some of the complexity of generating effective GFQs, and the need to rely on certain discourse desiderata. In this work we define the GFQ challenge, a novel question generation task, and we detail the desired properties of the generated questions. Subsequently, we provide a model for GFQ generation that aims to satisfy these desiderata, and demonstrate its competitiveness via a task of generating questions to fill the gap between premises and hypotheses in a standard natural language inference (NLI) setup.

In designing desired properties for GFQs, we take inspiration from theories of collaborative

Source text: A man, with a full face mask, sits on a sidewalk playing the guitar.

Student description: A man is wearing a mask.

Generated questions:

- Where is the man sitting?
- On what is the man sitting?

What is the man waring? (a)

Who is wearing a full faee mask? (a)

What is he doing with the guitar? (b)

Where does the man with the full face mask sit? (b) - What does the man in the mask do?

Gap-focused follow-up question: What does the man wearing a mask do?

Figure 1: Our Gap-Focused Question setup and approach. A student is asked to describe a source text from memory. The goal is to ask a follow-up question about information the student missed. Our approach is to generate a list of candidate questions, and then filter out ones that are either answerable from the student text (red strike-through (a)) or contain facts unknown to the student (yellow strike-through (b)). The follow-up question can be any of the remaining questions.

communication, and in particular Grice's maxims (Grice, 1975). For example, the maxim of quantity states that speakers are economic and do not communicate what is already known. Thus, the teacher should not ask about what is already in the common ground with the student. In the above example, this means not asking "What is the man playing?". We describe additional desiderata in $\S 3$.

To tackle the GFQ challenge, we show how general-purpose NLP models (question generation, question answering, and constituency parsing) can be used to generate GFQs that satisfy the discourse desiderata. See Figure 1 for an outline of the process. To assess our model, we consider pairs of texts that contain information gaps, and evaluate our ability to capture these gaps using GFQs. Such texts are readily available in NLI datasets that contain pairs of a premise and an entailed hypothesis with less information. We consider the SNLI dataset (Bowman et al., 2015), and use human annotators to evaluate the merit of our approach relative to GFQs generated by humans.

Our contribution is three-fold. First, we propose the novel setup of gap-focused questions, a key element of a student-teacher discourse as well as other settings such as automated fact checking. Second, we identify desiderata inspired by conversational maxims, and provide a model for generating questions that satisfy them. Third, we demonstrate the merit of our model on an NLI dataset.

## 2 Related work

Natural dialogue is a key goal of modern NLP and, despite substantial progress, there is still a considerable difference between humans and models. In this work we focus on dialogues where the bot (teacher) knows more than the user (student), and the goal is to gradually decrease this knowledge gap via gap-focused follow-up questions.

Several works have focused on the problem of follow-up question generation in dialogues. However, to the best of our knowledge, none of these focus on information gaps as we do. Ko et al. (2020) introduce the problem of inquisitive question generation, where the goal is to generate questions about facts that are not in the text. This is not done in reference to a complete text, and is thus principally different from our goal. In fact, in our settings, an inquisitive question would typically be a bad GFQ, since it refers to information that is outside the knowledge of both teacher and student. Prior works considered a related task referred to as answer-agnostic question generation (Scialom et al., 2019), but with a focus on factual questions, whereas the inquistive setting is broader.

Another class of follow-up questions are clarification ones (Rao and Daumé III, 2018), which can also be viewed as a special case of inquistive questions. Again, there is no reference to a complete text that defines the information gap. Finally, there are works on follow-up questions guided by rules as in the SHARC dataset (Saeidi et al., 2018).

Our GFQ setting is also related to the challenge of explainable NLI (Kalouli et al., 2020), namely the task of explaining why a certain sentence entails another. The GFQ output can be viewed as a novel explanation mechanism of why the student text is entailed by the source text, as it explicitly refers to the gap between these texts.

Our work is inspired by novel uses of question generation models, particularly in the context of evaluating model consistency (Honovich et al., 2021). In these, question generation is used to find "LLM hallucinations" where the generated text is not grounded in a given reference text. Our task can be viewed as the inverse of the knowledge grounding task, and our particular focus is on the questions generated rather than just pointing to information gaps. An additional line of work in this vein is QA-based semantics, where text semantics are represented via a set of questions rather than a formal graph (e.g., see Michael et al., 2018).

## 3 Criteria for Gap-Focused Questions

Given a complete source text $T_{C}$ and a student text $T_{S}$, our goal is to construct a model that takes $T_{S}$ and $T_{C}$ as input and produces a set of one or more questions $Q$ that ask about the information gap between $T_{C}$ and $T_{S}$. If one takes the term "information gap" literally, there are many such possible questions (e.g., which word appears in $T_{C}$ but not in $T_{S}$ ). In a natural language setting we are obviously interested in questions that are natural, that is, would likely be asked by a human who knows $T_{C}$ and has heard the student description $T_{S}$. When defining the desiderata for the generated questions, we consider what knowledge is held by the teacher and the student and what information is inside and outside their common ground (see Figure 2). We next identify desired properties for the generated questions, followed by a description of our model for generating gap-focused questions that satisfy these desiderata.

The following desired properties of an effective GFQ are loosely based on collaborative communication concepts (Grice, 1975):

- P1: Answerability: Only ask questions that can be answered based on the complete text $T_{C}$ (areas $A \cup B$ in Figure 2). This follows from Grice's maxim of relevance; speakers say things that are pertinent to the discussion.
- P2: Answers should not be in the common ground: If the student has already demonstrated knowing a fact in $T_{S}$, there is no reason to ask about it again. Namely, in Figure 2, we don't want to ask about information in $B$. This pertains to Grice's maxim of quantity; speakers are economic, they do not utter information beyond the bare minimum that is necessary to ask the question, and they will refrain from repeating already-known information.
- P3: Questions should only use information known to the user: The question itself should rely only on information in $T_{S}$ and not in $T_{C}$. For example if $T_{C}$ is " $A$ Woman is wearing a blue hat" and $T_{S}$ is "A woman is wearing something", it is preferable not to ask "What color is the hat?" as it refers to information that did not appear in $T_{S}$ (i.e., that the woman is wearing a hat). This is loosely related to the Grice maxim of manner, where one tries to be clear, brief, and orderly. If we were to ask questions using

![](https://cdn.mathpix.com/cropped/2024_05_29_42eb5ecef39e9e61717fg-03.jpg?height=388&width=726&top_left_y=203&top_left_x=1059)

Figure 2: In our setup we consider the gaps between the teacher's knowledge (represented by the complete source text $T_{c}$, areas $A \cup B$ in the diagram) and the student's knowledge (represented by the student text $T_{s}$, areas $B \cup C$ in the diagram). We consider the information overlap between these two texts as the common ground between the teacher and the student (area $B$ ), which is a key component in defining good GFQs.

information unknown to the user (in area $A$ in figure 2), we may introduce unnecessary details and obscurity into the discussion. ${ }^{1}$

## 4 The GFQs Generation approach

We next describe our modeling approach for the GFQ generation problem, with the goal of capturing the properties described above.

Before describing our GFQs generation approach, we briefly outline the NLP components we rely on in the question generation process:

A question generation model $G$ that, given an input text $T$ and a span $X \subset T$, generates questions about $T$ whose answer is $X$.

A question answering model $A$, that takes as input a text $T$ and a question $Q$ about the text, and returns the answer or an indication that the question is unanswerable from the text.

A constituency parser $P$, that takes a text $X$, breaks it down into sub-phrases (constituents), and returns a parse tree.

Additional details about these components can be found in appendix $\mathrm{C}$.

We are now ready to describe our approach for generating GFQs. The model generates an ordered set of possible follow-up questions $Q_{G}$ via the following steps, which roughly correspond to the desired criteria described in $\S 3$ :

Step 1: Generate answerable questions (P1). Using the constituency parser $P$, we extract the spans[^0]of all the constituents in the source text $T_{C}$, except for those spanning the entire sentence, and single word spans containing functional elements (e.g., prepositions). For each span $X \subset T_{C}$, we use the question generation model $G$ to generate a set of questions whose answer should be $X$, thus creating a set of questions that satisfy the answerablity property. We denote this set $Q_{T}$ and assign $Q_{G}=Q_{T}$.

Step 2: Filter questions whose answers are in the common ground. (P2). We next wish to remove questions that are answerable by the student text $T_{S}$. To that end, we use the question answering model $A$, and for each $q \in Q_{G}$ if $A\left(T_{S}, q\right) \neq$ "UNANSWERABLE", we set $Q_{G}=Q_{G} \backslash\{q\}$. ${ }^{2}$

Step 3: Prefer questions which only use information known to the user (P3). We prefer questions that do not reveal information beyond what is known to the user. This is not always strictly possible and thus, instead of filtering, we rank questions according to the (possibly zero) amount of additional information they reveal. To do so, let $R$ be all the answers to the questions in $Q_{G}$. By construction $R$ contains spans from $T_{C}$ that the student didn't mention, i.e. these are spans that we would prefer not to appear in the generated questions. For each $q \in Q_{G}$, we count the number of items in $R$ included in $q$. We sort $Q_{G}$ in ascending order by this number and return the first element. We thus return a question that uses the least number of facts unknown to the student.

## 5 Experiments

We next describe an evaluation of our GFQ model.

Data: We use the SNLI Dataset (Bowman et al., 2015) where a Natural language inference (NLI) pair contains two sentences denoting a premise and a hypothesis, and the relation between them can be entailment, contradiction and neutral. We focus on pairs labeled as entailment, and filter out those with bi-directional entailment, so that there is a gap between hypothesis and premise. We do not use any data for training, and apply our model to the test partition of the SNLI dataset.

Evaluation Benchmark: In order to compare the quality of our automatically generated questions to manually generated ones, we asked human annota-[^1]

| Model | Average score |
| :--- | :---: |
| Step 1 | 3.72 |
| Step 2 | 3.86 |
| Step 3 | 3.94 |
| Human | 4.06 |

Table 1: Average scores of the different generation methods on 200 questions, each rated by 3 annotators.

tors to generate questions for 200 instances of the SNLI test set (see Appendix A for the annotator instructions). We emphasize that these questions were only used for evaluation, as explained below, and not for training the model. They were collected after model design was completed. We release this evaluation dataset to the public, it is available here. See additional details about this dataset in appendix E.

## Annotator Evaluation of Generated Questions:

 As with other generative settings, offline evaluation is challenging. In fact, even if we had human generated questions for all SNLI, using those for evaluation would need to assume that they are exhaustive (otherwise the model can generate a good question but be penalized because it is not in the set generated by humans). Instead, as is commonly done (Ko et al., 2020), we rely on human evaluation. We present annotators with $T_{C}, T_{S}$ and a candidate GFQ $q$ and ask them to provide a $1-5$ score of how well $q$ functions as a follow-up question (see Appendix A for annotators instructions). We use 3 annotators per question.Compared Models: We compare four generation approaches: Human: Questions generated by human annotators; Step 1: This model selects a random question out of those generated by the question generation model (i.e., Step 1 in $\S 4$ ). We note that this is already a strong baseline because its questions are based on the source text. Step 2: The outcome of Step 2 in $\S 4$ where only questions not answerable by the student text are kept. Step 3: The outcome of Step 3, where we additionally aim for questions which use information known to the user.

Results: Table 1 provides the average scores for each of the considered models and the human generated questions. It can be seen that each step contributes to the score, and human generated questions are somewhat better than our final model (Step 3). Using the Wilcoxon signed-rank test for

| Source text | Student description | Generated question (Step 3) |
| :--- | :--- | :--- |
| A man stands by two face structures on | A man on Easter Island. | Two faces are what on Easter Island? |
| Easter Island. | What is one child wearing? |  |
| Two young children, one wearing a red | A person in a shirt. |  |
| striped shirt, are looking in through the |  |  |
| window while an adult in a pink shirt |  |  |
| watches from behind. |  |  |
| A man in a purple jersey is falling down | The two soccer players run around chas- What is the man in the cartoon wearing? |  |
| while chasing a player in a green jersey | ing each other |  |

Table 2: Examples of the loss patterns found in the analysis of low scoring questions. See details in the Error Analysis paragraph in section 5 .

## Source text: Basketball referee watching a team player hang from the hoop.

Student description: A referee watching a game.

Step 1: A basketball referee doing what?

Step 2: A basketball referee watches a player from which side hang from the hoop?

Step 3: Who was the referee watching?

Human: What sport does the referee work in?

Figure 3: An example of the steps of our Gap-Focused Questions model, and a human-generated question.

paired differences, we found that all differences were significant at $\mathrm{p}$-value $\leq 0.05$.

Examples: Figure 3 shows an example of the three stages, and a human generated question. Appendix F provides more examples.

Error Analysis: We analyze cases where our final model (Step 3) received low scores from the annotators (an average score of 3 and lower). In our analysis we have observed three main loss patterns (sometimes appearing together): (1) Poor question phrasing - these are questions whose structure or choice of words is less natural than if a person were to ask the same question. See example in the first row in Table 2. (2) Questions which include information outside of the teacher-student common ground. These are cases where the minimum criterion defined in Step 3 still results in a question with some information unknown to the user. See examples in the first 2 rows in Table 2. (3) Questions including information outside the complete source text. In rare cases we have found that the question generation model generates questions that include "hallucinations" or point to issues in the semantic understanding of the complete source text. See the third example in Table 2.

## 6 Conclusion

We consider the task of question generation in a novel setting where there is an information gap between speakers, and the gap-focused questions (GFQs) aim to reduce this gap. Building on advances in question generation and question answering, we show how to generate useful GFQs that meet several natural criteria inspired by theories cooperative conversation.

It is natural to ask whether one can employ a fully generative approach for GFQs using LLMs. This is a natural direction for future study, and we believe that the criteria and design choices we studied here will be significant in defining and evaluating such future work.

## Limitations

We present the first study of generating questions for filling in information gaps. Our method is limited in several ways. First, it focuses on information that is explicitly missing, and does not discuss information that is inaccurate or incomplete in other ways. Second, it only asks one follow-up question and does not address multi-turn dialogue about a student answer, or multiple student answers. Finally, our approach makes somewhat restricted use of the student answer, and it will be better to generate questions that directly uptake information from the student text (Demszky et al., 2021). We leave the deep investigation of these for future work.

## Acknowledgments

We thank Avi Caciularu for constructive feedback on this work.

## Ethics and Impact

Regarding risks, as with any NLP model, care must be taken in application, so that it generates truthful information, and does not introduce biases. However, we think this is not a major concern in our case as our modeling will generate text directly related to the source and student texts. In terms of impact, our approach can be used to improve a wide array of applications, including educational dialogue (e.g., reading comprehension), supportline bots, and automated fact checking.

## References

Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632642 .

Dorottya Demszky, Jing Liu, Zid Mancenido, Julie Cohen, Heather Hill, Dan Jurafsky, and Tatsunori B Hashimoto. 2021. Measuring conversational uptake: A case study on student-teacher interactions. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 16381653 .

H. P. Grice. 1975. Logic and conversation. In Peter Cole and Jerry L. Morgan, editors, Syntax and Semantics: Vol. 3: Speech Acts, pages 41-58. Academic Press, New York.

Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. True: Re-evaluating factual consistency evaluation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3905--3920.

Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. Q2: Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7856-7870.

Aikaterini-Lida Kalouli, Rita Sevastjanova, Valeria de Paiva, Richard Crouch, and Mennatallah ElAssady. 2020. XplaiNLI: Explainable natural language inference through visual analytics. In Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations, pages 48-52, Barcelona, Spain (Online). International Committee on Computational Linguistics (ICCL).
Nikita Kitaev and Dan Klein. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2676-2686, Melbourne, Australia. Association for Computational Linguistics.

Wei-Jen Ko, Te-yuan Chen, Yiyan Huang, Greg Durrett, and Junyi Jessy Li. 2020. Inquisitive question generation for high level text comprehension. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).

Julian Michael, Gabriel Stanovsky, Luheng He, Ido Dagan, and Luke S. Zettlemoyer. 2018. Crowdsourcing question-answer meaning representations. In NAACL-HLT.

Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. 2022. Recent advances in deep learning based dialogue systems: A systematic survey. Artificial intelligence review, pages 1-101.

Valentina Pyatkin, Ayal Klein, Reut Tsarfaty, and Ido Dagan. 2020. QADiscourse - Discourse Relations as QA Pairs: Representation, Crowdsourcing and Baselines. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2804-2819, Online. Association for Computational Linguistics.

Valentina Pyatkin, Paul Roit, Julian Michael, Yoav Goldberg, Reut Tsarfaty, and Ido Dagan. 2021. Asking it all: Generating contextualized questions for any semantic role. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1429-1441, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-789, Melbourne, Australia. Association for Computational Linguistics.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.

Sudha Rao and Hal Daumé III. 2018. Learning to ask good questions: Ranking clarification questions using neural expected value of perfect information. In

Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2737-2746.

Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rocktäschel, Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. 2018. Interpretation of natural language rules in conversational machine reading. In EMNLP.

Thomas Scialom, Benjamin Piwowarski, and Jacopo Staiano. 2019. Self-attention architectures for answer-agnostic neural question generation. In Proceedings of the 57th annual meeting of the Association for Computational Linguistics, pages 60276032 .

Robert Stalnaker. 2002. Common ground. Linguistics and Philosophy, 25(5/6):701-721.

Zheng Zhang, Ryuichi Takanobu, Qi Zhu, MinLie Huang, and XiaoYan Zhu. 2020. Recent advances and challenges in task-oriented dialog systems. Science China Technological Sciences, 63(10):20112027.
