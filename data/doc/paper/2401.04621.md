# 㲑演 DebugBench: <br> Evaluating Debugging Capability of Large Language Models 

Runchu Tian ${ }^{1 *} \quad$ Yining Ye ${ }^{1 *} \quad$ Yujia Qin $^{1} \quad$ Xin Cong $^{1} \quad$ Yankai Lin $^{2 \dagger}$<br>Yinxu Pan ${ }^{3} \quad$ Yesai Wu ${ }^{3} \quad$ Zhiyuan Liu ${ }^{1 \dagger} \quad$ Maosong Sun ${ }^{1}$<br>${ }^{1}$ Tsinghua University $\quad{ }^{2}$ Renmin University of China ${ }^{3}$ ModelBest Inc.

trc20@mails.tsinghua.edu.cn, yeyn2001@gmail.com


#### Abstract

Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce 'DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models fail to attain any pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging ${ }^{1}$.


## 1 Introduction

Large language models (LLMs) have demonstrated exceptional code generation abilities. LLM-based coding methods (Zhou et al., 2023; Shinn et al., 2023) have achieved human-level performance on benchmarks like HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). LLMs have also[^0]

![](https://cdn.mathpix.com/cropped/2024_05_29_460382b2c2aea9e08b80g-01.jpg?height=494&width=642&top_left_y=758&top_left_x=1164)

Figure 1: This figure illustrates the comparative debugging performance of gpt-3.5-turbo-0613 (OpenAI, 2022), gpt-4-0613 (OpenAI, 2023) and human proficiency across various bug categories. Evaluations are also performed on CodeLlama-34b (Rozière et al., 2023), CodeLlama-34b-Instruct (Rozière et al., 2023), and BLOOM (Workshop et al., 2022), which fail to generate effective responses for scoring.

become the core engine of practical programming assistance applications like GitHub Copilot (2023). Similar to code generation, debugging ${ }^{2}$ is also a crucial component in programming, consuming $35-50 \%$ of the development duration and $50-75 \%$ of the total budget (McConnell, 2004). However, unlike coding, the debugging abilities of LLMs remain relatively unexplored.

One primary obstacle in code debugging research is the lack of evaluation benchmarks. While some basic evaluations (Prenner et al., 2022; Sobania et al., 2023; Xia and Zhang, 2023b; Zhang et al., 2023) verify the effectiveness of LLM-based debugging methods, these evaluations have notable limitations that prevent us from comprehensively assessing the debugging capabilities of LLMs as exhibited in Table 1. First, as Zhang et al. (2023) re-[^1]

| Work | Test Scale | Against Data Leakage | Bug Type Diversity | Model Diversity | Scenario Diversity |
| :--- | ---: | :---: | :---: | :---: | :---: |
| Prenner et al. (2022) | 40 | $x$ | $x$ | $x$ | $x$ |
| Sobania et al. (2023) | 40 | $x$ | $x$ | $x$ | $x$ |
| Xia and Zhang (2023a) | 60 | $x$ | $x$ | $\checkmark$ | $\checkmark$ |
| Zhang et al. (2023) | 151 | $\checkmark$ | $x$ | $\checkmark$ | $\checkmark$ |
| DebugBench | 4,253 | $\checkmark$ | $\checkmark$ | $\checkmark$ |  |

Table 1: Limitations of prior studies in LLM debugging. We introduce DebugBench, a new LLM debugging benchmark to overcome these deficiencies.

vealed, existing debugging benchmarks (Just et al., 2014; Lin et al., 2017) have been more or less leaked to the pre-training data of popular LLMs via web scraping and other means. For instance, ChatGPT (OpenAI, 2023) can enumerate all the projects in Defects4J (Just et al., 2014). While it's challenging to ascertain the exposure due to a lack of training details, there's a significant risk of data leakage. Second, all existing debugging evaluations have been limited to a very small scale, ranging from 40 to 151 examples, which may hurt the generalizability of the assessments. Third, existing works reported a general pass rate across various bug categories instead of differentiating various bug types. Analyzing the variations in performance across different bug types can reveal the bottlenecks and guide focused improvements in LLM debugging.

To overcome these deficiencies, we create DebugBench, a dataset of 4,253 instances for LLM debugging evaluation. We first collect code solution snippets from LeetCode (2023), a popular programming challenge platform. To reduce the risk of data leakage, we ensure all of the instances in DebugBench are released after July 2022, which is beyond the pre-training data cutoff date of tested models. For fine-grained evaluation of various bug types, we develop a bug taxonomy based on Barr (2004)'s classification criteria. The classification encompasses four major bug categories: Syntax, Reference, Logic, and Multiples, along with $18 \mathrm{mi}-$ nor types as illustrated in Figure 1. Subsequently, we prompt GPT-4 (OpenAI, 2023) to implant bugs into the code solutions in pursuit of sufficient data scales for each bug type. We cover snippet-level code in C++, Java, and Python. To ensure integrity, we conduct automatic filtering and manual inspection.

As shown in Figure 1, we evaluate two closedsource language models, gpt-4-0613 (OpenAI, 2022) and gpt-3.5-turbo-0613 (OpenAI, 2023), along with three open-source mod- els: CodeLlama-34b (Rozière et al., 2023), CodeLlama-34b-instruct (Rozière et al., 2023) and BLOOM (Workshop et al., 2022) in zeroshot scenarios. Our empirical study reveals: (1) LLM debugging falls short of human performance. Open-source models attain a pass rate of 0 $\%$, struggling to produce meaningful debugging responses. Closed-source LLMs significantly surpass open-source ones but still fall short of human-level performance; (2) The difficulty of fixing different types of errors differs. Multiple errors and logical errors are significantly more challenging to repair than syntax and reference errors; (3) Runtime feedback has a clear impact on LLM's debugging performance but is not always helpful. While runtime feedback consistently boosts the debugging performance of syntax and reference bugs, the feedback information is unhelpful for logic errors.

To gain deeper insights into the overall programming capabilities of LLMs, we also compare closed-source models' performance on debugging and code generation. Experimental results indicate that for closed-source models: (1) fixing syntax or reference errors is generally easier than code generation, while repairing logical or multiple errors can be equally hard or even harder; (2) the debugging and code generation performance of LLMs are correlated, which indicates the abilities of LLMs to approach these two tasks are positively related. All these findings are crucial for comprehending the debugging capabilities of LLMs and developing more comprehensive code models.

## 2 Benchmark Construction

As illustrated in Figure 2, to construct DebugBench, we first collect questions, code snippets, and examples from LeetCode (2023) community, then employ GPT-4 (OpenAI, 2023) for bug implantation. To ensure the integrity of the benchmark, we conduct automatic filtering and final human inspection.

![](https://cdn.mathpix.com/cropped/2024_05_29_460382b2c2aea9e08b80g-03.jpg?height=691&width=1582&top_left_y=234&top_left_x=246)

Figure 2: This figure illustrates the construction of DebugBench. We first collect code snippets from LeetCode (2023) community, then employ GPT-4 (OpenAI, 2023) for bug implantation and finally conduct human / LLM evaluation on the benchmark. Automatic filtering and final human inspection are conducted to ensure integrity of the benchmark. The figure also provides qualitative cases for code snippets, bug instances, and evaluation samples. More examples are accessible in Appendix H.

### 2.1 Formulation of Debugging

Consider the input-output pairs $\left(x_{i}, y_{i}\right)$ where each $x_{i}$ is a program input and $y_{i}$ is the corresponding desired output, together they compose a set $R$ that defines the programming problem.

Let $a_{\theta}(x)=y$ denote a program $a$, based on a code script $\theta$, that maps an input $x$ to an output $y$. We identify a code script $\theta$ that exists bugs if there exists a pair $\left(x_{i}, y_{i}\right) \in R$ such that $a_{\theta}\left(x_{i}\right) \neq y_{i}$.

Consequently, an ideal debugger $D$ that rectifies any buggy code from $\theta$ to $\theta^{*}$ should satisfy that $D(\theta)=\theta^{*}$ s.t. $\forall\left(x_{i}, y_{i}\right) \in R, a_{\theta^{*}}\left(x_{i}\right)=y_{i}$. Debugging can be regarded as the converting process of debugger $D$.

### 2.2 Source Data Collection

We collect 3,206 samples from user-submitted solutions to specific programming challenges on LeetCode (2023). Each sample contains the question, solution code, examples, and release date. We utilize GPT-2 (Radford et al., 2019) tokenizer to tokenize these instances and report an average token length of 468.1 tokens, a typical length scale of code snippets. All of instances were released after June 2022, with an average release date of April 2023. This minimizes the risk of data leakage $^{3}$ (Zhang et al., 2023).[^2]

Apart from reducing data leakage, our choice of LeetCode is driven by two other reasons: (1) LeetCode offers sufficiently challenging code problems even for state-of-the-art LLMs like GPT-4 (Shinn et al., 2023); (2) LeetCode provides comprehensive test suites that facilitate automated evaluation, while other data sources like GitHub (2023) may suffer from arduous human labor (Hu et al., 2023) or incomplete test suites. A qualitative example of scraped code snippets can be found in Figure 2.

We select the three most popular programming languages (TIOBE Index, 2023), C++, Java, and Python3, to reflect the LLM debug capability in real-world scenarios. Our dataset comprises 1,438 instances in C++, 1,401 in Java, and 1,414 in Python.

### 2.3 Bug Implantation

After collecting source data from LeetCode (2023), we adopt GPT-4 (OpenAI, 2023) to implant bugs into code snippets. For implanting single errors (one bug in one code snippet), we prompt GPT-4 (OpenAI, 2023) with the correct code, desired bug type and instruct the model to generate a buggy version of the input code and a few sentences of explanation on the inserted bug. To implant multiple errors, we adopt rule-based merging based on sin-[^3]

| Type | Minor Type | Number |
| :---: | :---: | ---: |
|  | misused $==/=$ | 137 |
|  | missing colons | 129 |
|  | unclosed parentheses | 133 |
| Syntax | illegal separation | 68 |
|  | illegal indentation | 45 |
|  | unclosed string | 125 |
|  | illegal comment | 124 |
|  | faulty indexing | 206 |
| Reference | undefined objects | 187 |
|  | undefined methods | 167 |
|  | illegal keywords | 124 |
|  | condition error | 260 |
| Logic | operation error | 180 |
|  | variable error | 100 |
|  | other error | 50 |
|  | double bugs | 750 |
| Multiple | triple bugs | 750 |
|  | quadruple bugs | 718 |

Table 2: Bug types and their distribution in DebugBench.

gle errors, which is similar to the merge operation in version control systems. The prompt we use for bug implantation can be found in Appendix A.

We instruct GPT-4 to add diverse types of bugs into code snippets. Based on the bug classification criteria from Barr (2004), we categorize the bug into 4 major categories and 18 minor types. Table 2 depicts the scope of bug types in DebugBench. This diversity enables a thorough investigation of LLMs' ability to debug a wide array of programming errors. The definition of each minor type can be found in Appendix B.

We choose bug synthesis rather than bugs from traditional Debugging datasets like Defects4J (Just et al., 2014) and QuixBugs (Lin et al., 2017) in pursuit of a vast degree of freedom in error diversity design and lower risk of data leakage. The feature schema of generated instances be found in Appendix C.

### 2.4 Quality Control

To ensure the quality of DebugBench, we conduct automatic filtering and manual inspection.

Automatic Filtering. First, we filter the source data collected from LeetCode (2023). We design the following automatic filtering criteria: (1) The

| Criteria | Pass Rate/\% |
| :---: | ---: |
| Bug Validity | 97.4 |
| Sensitive Information Security | 100.0 |
| Scenario Alignment | 93.2 |
| All Three criteria | 92.1 |

Table 3: Results of manual inspection of DebugBench.

code solution must be correct, that is, to pass the whole corresponding test suites. (2) The instances must contain necessary information like programming language, release time, and question id. (3) The release date of code snippets must be no earlier than July 2022, the official knowledge cutoff date of two closed-source models (OpenAI, 2023) in case of data leakage. $72.1 \%$ of the user-submitted code snippets pass this automatic filtering. Second, we filter the data synthesized by GPT-4 (OpenAI, 2023) since the LLM occasionally fails to perform bug implantation as expected. We again establish automatic filter criteria: (1) The code with implanted bugs must fail certain test cases to confirm its erroneous nature. (2) The buggy code should not include in-line comments that could leak information about the bug. (3) The explanation for the bug must be thorough and relevant to the assigned bug type. Following these criteria, $79.2 \%$ of the 3,000 bug-implanted instances pass the filtering process.

Manual Inspection. After automatic generation and filtering, we manually inspect the quality of DebugBench. We apply three criteria for manual inspection: (1) Bug Validity: The bugs must cause the intended malfunction, fail specific test cases, and align with the assigned bug type and description. (2) Sensitive Information Security: The instances must be devoid of sensitive data, such as personal information. (3) Scenario Alignment: The bugs should resemble those found in actual code debugging scenarios and should not include obvious clues, like comments indicating the bug's location.

We hire three programmers with over four years of experience in programming to conduct the manual inspection on 180 cases over two hours each after training on 30 cases. Their review reveals that the DebugBench benchmark is of high quality as exhibited in Table 3. Failing cases can be found in Appendix F.

| Major Category | Minor Type | BLOOM | CodeLlama | CodeLlama-Inst. | $g p t-3.5$ | gpt-4 | human |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Syntax | misused $==/=$ <br> missing colons <br> unclosed parentheses <br> illegal separation <br> illegal indentation <br> unclosed string <br> illegal comment | 0.0 | 0.0 | 0.0 | 70.5 <br> 80.9 <br> 81.2 <br> 78.1 <br> 79.6 <br> 82.0 <br> 67.4 | 87.9 <br> 93.6 <br> 89.6 <br> 89.0 <br> 87.8 <br> 91.4 <br> 78.0 | $11 / 12$ <br> $12 / 12$ <br> $12 / 12$ <br> $12 / 12$ <br> $12 / 12$ <br> $12 / 12$ <br> $11 / 12$ |
| Reference | faulty indexing <br> undefined objects <br> undefined methods <br> illegal keywords | 0.0 | 0.0 | 0.0 | 72.9 <br> 70.6 <br> 59.3 <br> 76.1 | 77.1 <br> 81.7 <br> 78.5 <br> 83.6 | $10 / 12$ <br> $12 / 12$ <br> $11 / 12$ <br> $11 / 12$ |
| Logic | condition error <br> operation error <br> variable error <br> other error | 0.0 | 0.0 | 0.0 | 58.5 <br> 49.5 <br> 52.3 <br> 61.1 | 73.1 <br> 68.6 <br> 63.1 <br> 72.2 | $10 / 12$ <br> $10 / 12$ <br> $9 / 12$ <br> $10 / 12$ |
| Multiple | double bugs <br> triple bugs <br> quadraple bugs | 0.0 | 0.0 | 0.0 | 56.4 <br> 45.5 <br> 38.7 | 70.7 <br> 58.9 <br> 55.9 | $11 / 12$ <br> $9 / 12$ <br> $8 / 12$ |

Table 4: Debugging performance of various models against human proficiency measured by Pass Rate. Model names are abbreviated for clarity: CodeLlama represents CodeLlama-34b; CodeLlama-Inst. is short for CodeLlama-34b-Instruct; gpt-3.5 denotes gpt-3.5-turbo-0613; and gpt-4 refers to gpt4-0613. The experimental results reveal that while closed-source models are less effective compared to human performance, open-source models struggle to yield efficient outcomes in debugging tasks.

## 3 Experiments

Evaluated Models. To obtain a comprehensive understanding of LLMs' debugging capabilities and identify the potential gap between open-source and closed-source models, we conduct experiments on two popular commercial models: gpt3.5-turbo-0 613 (OpenAI, 2022) and gpt-40613 (OpenAI, 2023). For open-source models, we select BLOOM (176B) (Workshop et al., 2022), CodeLlama-34b (Rozière et al., 2023) and CodeLlama-34b-Instruct (Rozière et al., 2023) for assessment. While the experiments are primarily conducted in zero-shot scenarios, we additionally test a three-shot context for open-source models in an attempt to further activate their debugging abilities.

Metric. The metric for DebugBench is based on the test suites ${ }^{4}$ provided by LeetCode (2023). These suites include a mix of 1-3 known test cases and 8-100 unknown test cases for each instance.

Specifically, we use Pass Rate to quantify the debug ability of language models. For a buggy snippet $\theta_{i}$ and its fixed version $\theta_{i}^{*}$, we have a corresponding set of test cases $\left(x_{i}^{0}, y_{i}^{0}\right),\left(x_{i}^{1}, y_{i}^{1}\right), \ldots,\left(x_{i}^{m}, y_{i}^{m}\right)$. Whether the bug[^4]

instance is successfully repaired can be referred to as $\bigwedge_{j=0}^{m}\left[a_{\theta_{i}^{*}}\left(x_{i}^{j}\right)=y_{i}^{j}\right]$, an aggregate result of all test cases. The Pass Rate, $P R$, that represents the test result on $n$ bug instances are defined as:

$$
P R=\sum_{i=0}^{n} \frac{\bigwedge_{j=0}^{m}\left[a_{\theta_{i}^{*}}\left(x_{i}^{j}\right)=y_{i}^{j}\right]}{n} \times 100 \%
$$

Human Performance. The proficiency of human debuggers is assessed by three programmers, each with over four years of experience in programming. Before the formal experiment, they underwent a two-hour training session focused on understanding the purpose of human evaluation and the criteria for metrics. This was followed by a one-hour trial session. Each participant independently debugged 72 bugs, dedicating approximately 20 hours per person. During this process, access to Integrated Development Environments (IDEs) was provided to facilitate runtime analysis but any access to deep learning tools like GitHub Copilot (2023) was prohibited.

### 3.1 Debugging Capabilities

We evaluate the debugging capabilities of LLMs by assessing two closed-source and three open-source LLMs across 18 types of programming errors in three distinct scenarios.

![](https://cdn.mathpix.com/cropped/2024_05_29_460382b2c2aea9e08b80g-06.jpg?height=527&width=1528&top_left_y=262&top_left_x=264)

Figure 3: Qualitative example of open-source models' response to a debugging problem. The query is deliberately selected to be basic and clear to illustrate ineffectiveness.

### 3.1.1 Overall Results

Close-Source Models As shown in Figure 1 and Table 4, we examined the performance of closedsource models, gpt-4-0613 (OpenAI, 2023) and gpt-3.5-0613 (OpenAI, 2022). They respectively pass $75.0 \%$ and $62.1 \%$ of the bug instances, achieving a level of debugging performance below human. The superiority of human debuggers can be attributed to robust test cases and interaction with the program through breakpoints and developmental environments. Despite LLMs' limited effectiveness, they exhibit significant time efficiency. The models complete inference processes for one bug in less than 10 seconds, a task that averagely takes humans around 20 minutes. This indicates that commercial models are now capable of partially achieving the objectives of Automated Debugging, bringing benefits in time efficiency, cost reduction, and minimizing human labor. The zero-shot prompts utilized in model evaluation can be found in Appendix A.

Open-Source Models As illustrated in Table 4, none of these three open-source models is able to produce effective debugging responses or attain any Pass Rate score. This underscores a notable shortfall in the zero-shot debugging abilities of open versus closed-source LLMs. We additionally experiment with a simplified version of prompt that eliminates the influence of parsing and other marginal instructions, and a three-shot prompt (Appendix A), but find no success with either. The ineffectiveness is likely due to a limited presence of debugging data in their pre-training datasets. These findings highlight the need for an open-source model capable of supporting debugging for research utility and

![](https://cdn.mathpix.com/cropped/2024_05_29_460382b2c2aea9e08b80g-06.jpg?height=457&width=799&top_left_y=985&top_left_x=1048)

Figure 4: Pass Rate of GPT-4 (OpenAI, 2023) and GPT3.5-turbo (OpenAI, 2022) with more samples containing logical errors, particularly noting a significant improvement from 1 to 4 samples.

practical applications. Qualitative examples of the response from open-source models are accessible in Figure 3 and Appendix E.

### 3.1.2 Effect of Bug Types

As illustrated in Figure 1 and Table 4, the challenge of debugging varies markedly with the bug type for both humans and models. Syntax and reference errors are comparatively simpler to spot and rectify. In contrast, logic bugs pose a greater challenge, requiring an understanding of the code's underlying mechanisms. Additionally, the complexity of debugging escalates with an increase in the number of bugs within a code snippet. Therefore, in training or improving models for debugging, special emphasis should be placed on enhancing their ability to handle logic errors and scenarios with multiple concurrent errors.

![](https://cdn.mathpix.com/cropped/2024_05_29_460382b2c2aea9e08b80g-07.jpg?height=431&width=788&top_left_y=247&top_left_x=243)

Figure 5: Effect of runtime feedback on gpt-4-0613 (OpenAI, 2023) and gpt-3.5-turbo-0 613's (OpenAI, 2022) debugging performance. It improves syntax and reference error handling but impairs logic error resolution.

### 3.1.3 In-depth Analysis

In this section, we examine two additional scenarios for deeper analyses.

Effect of Multiple Sampling. In this scenario, a language model is permitted to generate multiple responses to a single debugging query. An instance is marked as 'pass' if at least one response successfully meets all test case criteria. Due to budget constraints, we limit our sampling to a maximum of nine answers for each instance with logical errors. As illustrated in Figure 4, increased sampling attempts enhance debugging performance, indicating an effective trade-off: better debugging at the cost of using more inference tokens.

Effect of Runtime Feedback. Recent studies (Chen et al., 2023; Jiang et al., 2023) find out that providing runtime information like program output and traceback messages enhances the coding capabilities of LLMs. In this section, we investigate the influence of runtime messages on the debugging process. We leveraged the built-in runtime environment of the LeetCode test suites to obtain feedback information. As illustrated in Figure 5, the runtime feedback has a clear impact on the debugging performance of LLMs. For syntax and reference errors, traceback information effectively identifies the locations of bugs, thereby facilitating the debugging process. However, for logic bugs, the details provided in traceback messages are often too low-level to facilitate effective debugging and may even cause disruptions. This indicates that the information provided by Runtime Feedback is not always useful for debugging LLMs. Positive and negative examples of runtime messages are accessible in Appendix G.

![](https://cdn.mathpix.com/cropped/2024_05_29_460382b2c2aea9e08b80g-07.jpg?height=412&width=809&top_left_y=225&top_left_x=1046)

Figure 6: Pass Rate of coding and debugging tasks with same programming problems.

### 3.2 Interplay between Debugging and Coding

As an extension of the evaluation of debugging capabilities, we compare the difficulty and correlation of these tasks to deepen our understanding of LLMs' proficiency in both code generation and debugging.

Comparison of Difficulty. We analyze the debugging and code generation performance of gpt4-0613(OpenAI, 2022) and gpt-3.5-turbo0613 (OpenAI, 2023) on identical instances. As illustrated in Figure 6, we find that correcting syntax and reference errors typically presents less difficulty than generating full code for a specific query, while addressing logical errors or multiple issues can be as challenging as code generation itself. This pattern implies that for closed-source models, the task of debugging is relatively easier than code generation. The limited debugging capabilities in open-source models probably don't stem from a disparity in complexity between debugging and coding tasks.

## Correlation between Debugging and Coding

 To explore the correlation between debugging and coding, i.e., whether a programming question is more likely to be easy-to-debug if it is easy-to-code and vice versa, we compute the Phi-Coefficient for closed-source LLMs and find that all categories of bugs have a positive Phi-Coefficient score with code generation ranging from 0.1 to 0.3 as shown in Table 5. This suggests that the capabilities of closed-source LLMs to approach these two tasks are explicitly correlated. But this conclusion does not hold for open-sourced models, which perform well in code generation tasks but fail to achieve any Pass Rate scores for debugging.| Model | Bug Type | Phi-Coefficien |
| :---: | :---: | :---: |
| GPT-4 | syntax | 0.221 |
|  | reference | 0.115 |
|  | logic | 0.353 |
|  | multiple | 0.273 |
| GPT-3.5-Turbo | syntax | 0.148 |
|  | reference | 0.196 |
|  | logic | 0.174 |
|  | multiple | 0.298 |

Table 5: Phi-Coefficient of LLMs' coding and debugging performance.

## 4 Related Work

### 4.1 LLM-based Coding

The field of LLM code generation has been extensively studied. Researchers have collected code corpora to train large language models that specialize in code generation (Chen et al., 2021; Nijkamp et al., 2022; Li et al., 2023). General-purpose LLMs also demonstrated impressive coding abilities as a result of extensive pre-training on datasets rich in code-related content (Touvron et al., 2023; Workshop et al., 2022; OpenAI, 2022, 2023). Parallel to the development of these foundational models, innovative methods such as verbal reinforcement learning with feedback from runtime messages (Shinn et al., 2023), and multi-agent collaboration (Qian et al., 2023), have been implemented to further refine the coding abilities of LLMs.

As another key component of programming proficiency, LLMs' debugging capabilities have not garnered so much attention. This can be partly attributed to the absence of evaluation benchmarks. To overcome this deficiency, we introduce DebugBench, the new LLM debugging benchmark discussed in this work.

### 4.2 Automated Program Repair

Automated Program Repair (APR) refers to the process of automatically fixing program bugs or errors without human intervention. This topic has gained significant attention due to its potential to reduce the time and cost in software development (Goues et al., 2019). While template-based (Liu et al., 2019), search-based (Ke et al., 2015) and generic (Le Goues et al., 2011) methods have been proposed to solve the task, program repair based on Large Language Models exhibit significant potential (Prenner et al., 2022).
Prenner et al. (2022) evaluated OpenAI's CodeX (Chen et al., 2021) on QuixBugs (Lin et al., 2017) and found LLM debugging promising. Sobania et al. (2023) utilized ChatGPT (OpenAI, 2022) to address bugs in QuixBugs, outperforming the previous state-of-the-art. Xia and Zhang (2023a) tested LLM debugging with a conversational strategy to refine the debugging patches based on the feedback from each turn on QuixBugs and achieved higher performance. However, Zhang et al. (2023) pointed out that evaluations on traditional APR datasets like QuixBugs (Lin et al., 2017) and Defects4J (Just et al., 2014) face a severe risk of data leakage and evaluate ChatGPT (OpenAI, 2023) on a new benchmark of 151 bugs from competitive programming problems.

These works are fundamental in verifying the feasibility of LLMs for debugging, but they face challenges that require further investigation. Apart from (1) data leakage, current evaluations of LLM debugging face significant constraints: (2) limited bug diversity, (3) constrained test scale as illustrated in Table 1. In order to gain a deeper understanding of the potential for LLM debugging, we conducted a systematic evaluation with DebugBench.

## 5 Conclusion

In this work, we presented DebugBench, a benchmark specifically designed to evaluate the debugging capabilities of large language models. DebugBench was developed utilizing source data from LeetCode (LeetCode, 2023) and bug implantation with prompted GPT-4, underpinned by a stringent quality control process.

Our experiments with DebugBench revealed several key findings: (1) In a zero-shot scenario, closed-source models exhibited lower debugging performance compared to humans, while all tested open-source models struggled to generate meaningful debugging responses in both zero-shot and three-shot scenarios; (2) Multiple and logical errors posed a greater challenge for the repair compared to syntax and reference errors; (3) Runtime feedback enhanced the debugging performance for syntax and reference errors but is unhelpful for logical errors; (4) For closed-source LLMs, debugging syntax or reference errors is easier than code generation while logic or multiple errors can equally hard or even harder. And their capabilities in coding and debugging are correlated. Hopefully,
these findings will contribute to the advancement of large language models in the field of automatic debugging.

Future Work The scope of debugging scenarios can be expanded to more practical and complex situations like repository-level debugging (Jimenez et al., 2023; Bairi et al., 2023) and scenarios involving human interaction. Additionally, based on the results of human evaluation, the ability to write reliable test cases and interact with Integrated Development Environments (IDEs) significantly boosts manual debugging performance. It can be meaningful to evaluate how well LLMs write test cases and interact with IDEs for debugging.

## Limitations

This study has certain limitations that must be acknowledged. The bugs in our experiments were synthetically created and might not entirely reflect the intricacies of real-world coding scenarios. The scope of our study was confined to two open-source and three closed-source models, which do not represent the full spectrum of existing LLMs.

## References

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. ArXiv preprint, abs/2108.07732.

Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, Shashank Shet, et al. 2023. Codeplan: Repository-level coding using llms and planning. ArXiv preprint, abs/2309.12499.

Adam Barr. 2004. Find the Bug: A Book of Incorrect Programs. Addison-Wesley Professional.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374.

Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. ArXiv preprint, abs/2304.05128.

GitHub. 2023. Github. https://github.com. Accessed: 2023-12-13.

GitHub Copilot. 2023. Github copilot. https:// github.com/features/copilot. Accessed: 202312-16.
Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Automated program repair. Communications of the ACM, 62(12):56-65.

Qisheng Hu, Kaixin Li, Xu Zhao, Yuxi Xie, Tiedong Liu, Hui Chen, Qizhe Xie, and Junxian He. 2023. Instructcoder: Empowering language models for code editing. ArXiv preprint, abs/2310.20329.

Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023. Selfevolve: A code evolution framework via large language models.

Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? ArXiv preprint, $\mathrm{abs} / 2310.06770$.

René Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4j: A database of existing faults to enable controlled testing studies for java programs. In Proceedings of the 2014 international symposium on software testing and analysis, pages 437-440.

Yalin Ke, Kathryn T Stolee, Claire Le Goues, and Yuriy Brun. 2015. Repairing programs with semantic code search (t). In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 295-306. IEEE.

Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2011. Genprog: A generic method for automatic software repair. Ieee transactions on software engineering, 38(1):54-72.

LeetCode. 2023. Leetcode: The world's leading online programming learning platform. https:// leetcode.com. Accessed: 2023-12-01.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! ArXiv preprint, abs/2305.06161.

Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. 2017. Quixbugs: A multi-lingual program repair benchmark set based on the quixey challenge. In Proceedings Companion of the 2017 ACM SIGPLAN international conference on systems, programming, languages, and applications: software for humanity, pages 55-56.

Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawendé F Bissyandé. 2019. Tbar: Revisiting template-based automated program repair. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis, pages $31-42$.

Steve McConnell. 2004. Code Complete, 2nd edition. Cisco Press.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. ArXiv preprint, abs/2203.13474.

OpenAI. 2022. Introducing chatgpt. https://openai. com/blog/chatgpt.

OpenAI. 2023. Chatgpt - release notes. https://help.openai.com/en/articles/6825453chatgpt-release-notes. Accessed: 2023-12-17.

OpenAI. 2023. Gpt-4 technical report.

Julian Aron Prenner, Hlib Babii, and Romain Robbes. 2022. Can openai's codex fix bugs? an evaluation on quixbugs. In Proceedings of the Third International Workshop on Automated Program Repair, pages 6975 .

Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software development. ArXiv preprint, abs/2307.07924.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code.

Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems.

Dominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. 2023. An analysis of the automatic bug fixing performance of chatgpt.

TIOBE Index. 2023. Tiobe programming community index. https://www.tiobe.com/tiobe-index/. Accessed: 2023-12-01.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971.

BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. ArXiv preprint, abs/2211.05100.
Chunqiu Steven Xia and Lingming Zhang. 2023a. Conversational automated program repair. ArXiv preprint, abs/2301.13246.

Chunqiu Steven Xia and Lingming Zhang. 2023b. Keep the conversation going: Fixing 162 out of 337 bugs for $\$ 0.42$ each using chatgpt.

Quanjun Zhang, Tongke Zhang, Juan Zhai, Chunrong Fang, Bowen Yu, Weisong Sun, and Zhenyu Chen. 2023. A critical review of large language model on software engineering: An example from chatgpt and automated program repair. ArXiv preprint, abs/2310.08879.

Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language agent tree search unifies reasoning acting and planning in language models.
