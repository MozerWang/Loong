# Hierarchical Lowrank Arithmetic with Binary Compression 

Ronald Kriemann<br>MPI for Mathematics i.t.S.<br>Leipzig, Germany<br>rok@mis.mpg.de

August 23, 2023


#### Abstract

With lowrank approximation the storage requirements for dense data are reduced down to linear complexity and with the addition of hierarchy this also works for data without global lowrank properties. However, the lowrank factors itself are often still stored using double precision numbers. Newer approaches exploit the different IEEE754 floating point formats available nowadays in a mixed precision approach. However, these formats show a significant gap in storage (and accuracy), e.g. between half, single and double precision. We therefore look beyond these standard formats and use adaptive compression for storing the lowrank and dense data and investigate how that affects the arithmetic of such matrices.


AMS Subject Classification: $65 \mathrm{Y} 05,65 \mathrm{Y} 20,68 \mathrm{~W} 10$, $68 \mathrm{~W} 25,68 \mathrm{P} 30$

Keywords: hierarchical matrices, lowrank arithmetic, data compression, mixed precision

## 1 Introduction

In the last decades lowrank techniques proved very effective for representing dense data with (almost) optimal storage complexity. Especially in the form of hierarchical matrices ( $\mathcal{H}$-matrices), as first introduced in [17], it allows to handle operators of much larger problem sizes thanks to near linear storage and arithmetic complexity. A large variety of different applications demonstrate the huge advantage of lowrank storage.

Though the complexity reduction is the major contributor for this success, there is still room for improvements, in particular in the raw storage of floating point numbers for the dense and lowrank blocks within an $\mathcal{H}$-matrix. In practice, these blocks are often stored in double precision format (FP64). One of the reasons being the approximation algorithms used for lowrank arithmetic, e.g., singular value decomposition, which have more strict accuracy requirements sometimes surpassing even the capabilities of single precision (FP32) arithmetic. However, the machine preci-

|  | $1-e-m$ | Bits | Unit Roundoff |
| :--- | :--- | :--- | :--- |
| FP64 | $1-11-52$ | 64 | $1.1 \times 10^{-16}$ |
| FP32 | $1-8-23$ | 32 | $6.0 \times 10^{-8}$ |
| TF32 | $1-8-10$ | 19 | $4.9 \times 10^{-4}$ |
| BF16 | $1-8-7$ | 16 | $3.9 \times 10^{-3}$ |
| FP16 | $1-5-10$ | 16 | $4.9 \times 10^{-4}$ |
| FP8 | $1-4-3$ | 8 | $6.3 \times 10^{-2}$ |

Table 1: Floating point formats based on the IEEE-754 standard

sion or unit roundoff associated to this is typically much smaller than the already introduced lowrank approximation error.

The IEEE-754 standard, which defines both storage formats, also provides further floating point schemes, some of which are presented in Table 1 Especially in the recent years, half precision formats, e.g., FP16, BF16 or TF32, have been used on GPUs. However, all these only provide a fixed arithmetic accuracy. Compression schemes for $\mathcal{H}$ matrices however, should support a variable precision as the lowrank approximation accuracy may change in a wide range depending on the application.

An alternative are mixed precision approaches. In [1] single or half precision representation was chosen for a full lowrank block depending on its Frobenius norm compared to the norm of the matrix. However, this still requires that a lowrank block can be represented in the corresponding floating point format.

In contrast to this, in [4] the lowrank factors $U \in \mathbb{R}^{n \times k}$ and $V \in \mathbb{R}^{m \times k}$ are decomposed into subblocks $U=$ $\left(U_{1}, U_{2}, \ldots\right), U_{i} \in \mathbb{R}^{n \times k_{i}}$ and $V=\left(V_{1}, V_{2}, \ldots\right), V_{i} \in$ $\mathbb{R}^{m \times k_{i}}$ with the size $k_{i}$ of each subblock depending on the singular values of $U V^{H}$ and the unit roundoff of the floating point format used to represent $U_{i}, V_{i}$. With this, even for a high accuracy, low precision formats can be used.

A different approach was implemented in [23] by using the floating point compression library ZFP [25] to further compress the lowrank and dense data blocks with an adaptive accuracy based on the user defined lowrank approximation precision. With this, the overall memory consumption could further be reduced significantly.

In principle, other floating point compression schemes could also be applied to $\mathcal{H}$-matrices, e.g., SZ [14 24] (or SZ3 [28]), which demonstrates a very good compression rate for a wide range of scientific data. A different approach to floating point compression is implemented in MGARD [2] which is based on multigrid techniques.

Related to lowrank compression are methods based on Tucker decomposition, e.g., TuckerMPI [8], TTHRESH [9] or ATC [7], as it can be considered the generalization of the same concept to higher dimensions. TTHRESH and ATC also use bitplane truncation to produce a minimal memory representation of the floating point data. However, these
compression techniques are only of limited use in the context of $\mathcal{H}$-matrices with its two-dimensional data blocks for which Tucker decomposition corresponds to standard SVD.

Another alternative is lossless compression, e.g. as implemented in the widely used libraries Zstd [30], LZ4 [26] or zlib [29]. However, experiments with $\mathcal{H}$-matrices yielded only a reduction of the memory size by about $10 \%$ at best.

When extending the compression of lowrank or dense data within an $\mathcal{H}$-matrix to the full $\mathcal{H}$-matrix arithmetic, it is not easily possible to perform arithmetic operations directly with the compressed data, especially as lowrank arithmetic relies on more complex functions like SVD or QR factorization. Instead, one can perform computations still in double (or single) precision and only store values in a compressed form. This approximate storage concept is introduced in [6] for sparse matrix arithmetic.

In this work, we apply it to full $\mathcal{H}$-matrix arithmetic, i.e., matrix-vector multiplication and $\mathcal{H}$-LU factorization. While the former only requires decompression of data, $\mathrm{LU}$ factorization includes updates to matrix blocks involving matrix multiplication and lowrank truncation during which constant decompression and recompression of the matrix data is performed. We will comare different compression schemes in terms of compression ratio and arithmetic performance.

For algorithms limited by the memory bandwidth, a possible side effect of the reduced storage size is an increase in performance since less data needs to be loaded or stored. Though this is not the main focus of this work, some examples of this effect are visible also for $\mathcal{H}$-matrix algorithms.

The rest of this work is structured as follows: in Section 2 basic definitions and algorithms for $\mathcal{H}$-matrices are introduced. Section 3 will discuss compression of the data blocks within $\mathcal{H}$-matrices and Section 4 the special properties of compressed $\mathcal{H}$-arithmetic. Numerical experiments will be presented in Section 5 followed by a conclusion in Section6

## $2 \mathcal{H}$-Matrices and $\mathcal{H}$-Arithmetic

### 2.1 Definitions

For an indexset $I$ we define the cluster tree (or $\mathcal{H}$-tree) as the hierarchical partitioning of $I$ into disjoint sub-sets of $I$ :

Definition 2.1 (Cluster Tree) Let $T_{I}=(V, E)$ be a tree with $V \subset \mathcal{P}(I) . T_{I}$ is called a cluster tree over $I$ if

1. $I=\operatorname{root}\left(T_{I}\right)$ and
2. for all $v \in V$ with $\operatorname{sons}(v) \neq \emptyset: v=\dot{U}_{v^{\prime} \in \operatorname{sons}(v)} v^{\prime}$.

A node in $T_{I}$ is also called a cluster and we write $\tau \in T_{I}$ if $\tau \in V$. The set of leaves of $T_{I}$ is denoted by $\mathcal{L}\left(T_{I}\right)$.

Similar to a cluster tree we can extend the hierarchical partitioning to the product $I \times J$ of two index sets $I, J$, while restricting the possible set of nodes by given cluster trees $T_{I}$ and $T_{J}$ over $I$ and $J$, respectively. Furthermore, the set of leaves will be defined by an admissibility condition. In the literature, various examples of admissibility can found, e.g. standard [18], weak [20] or off-diagonal admissibility [13. 3].

Definition 2.2 (Block Tree) Let $T_{I}, T_{J}$ be two cluster trees and let $\mathrm{adm}: T_{I} \times T_{J} \rightarrow \mathbb{B}$. The block tree $T=T_{I \times J}$ is recursively defined starting with $\operatorname{root}(T)=(I, J)$ :

$\operatorname{sons}(\tau, \sigma)=$

$\left\{\begin{array}{l}\emptyset, \text { if } \operatorname{adm}(\tau, \sigma)=\operatorname{true} \vee \operatorname{sons}(\tau)=\emptyset \vee \operatorname{sons}(\sigma)=\emptyset \\ \left\{\left(\tau^{\prime}, \sigma^{\prime}\right): \tau^{\prime} \in \operatorname{sons}(\tau), \sigma^{\prime} \in \operatorname{sons}(\sigma)\right\} \text { else. }\end{array}\right.$

A node in $T$ is also called a block. Again, the set of leaves of $T$ is denoted by $\mathcal{L}(T):=\{b \in T:$ sons $(b)=\emptyset\}$. By $\mathcal{L}_{l r}(T)=\{b \in \mathcal{L}: \operatorname{adm}(b)=$ true $\}$ the set of admissible leaves is denoted.

The admissibility condition is used to detect blocks in $T$ which can be efficiently approximated by lowrank matrices with a predefined rank $k$, i.e., blocks $b$ with $\operatorname{adm}(b)=$ true. The set of all such matrices forms the set of $\mathcal{H}$-matrices:

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-02.jpg?height=43&width=828&top_left_y=1378&top_left_x=1025)
trees $T_{I}, T_{J}$ and $k \in \mathbb{N}$, the set of $\mathcal{H}$-matrices $\mathcal{H}(T, k)$ is defined as

$$
\begin{aligned}
\mathcal{H}(T, k):= & \left\{M \in \mathbb{R}^{I \times J}: \forall(\tau, \sigma) \in \mathcal{L}(T):\right. \\
& \left.\operatorname{rank}\left(M_{\tau, \sigma}\right) \leq k \vee \tau \in \mathcal{L}\left(T_{I}\right) \vee \sigma \in \mathcal{L}\left(T_{J}\right)\right\}
\end{aligned}
$$

Here, $M_{\tau, \sigma}$ refers to the sub-block $\left.M\right|_{\tau \times \sigma}$.

In practice the constant rank $k$ is typically replaced by a fixed lowrank approximation accuracy $\varepsilon>0$ as the resulting $\mathcal{H}$-matrices are often more memory efficient. For this we assume for an admissible block $M_{\tau, \sigma}$ :

$$
\begin{equation*}
\left\|M_{\tau, \sigma}-U_{\tau, \sigma} V_{\tau, \sigma}^{H}\right\| \leq \varepsilon\left\|M_{\tau, \sigma}\right\| \tag{1}
\end{equation*}
$$

## 2.2 $\mathcal{H}$-Arithmetic

During arithmetic, the most important operation is lowrank truncation as it is required after every update to a lowrank block and also normally more costly than other operations, e.g., dense matrix addition or multiplication.

In the literature different forms of lowrank truncation exist, with the SVD being the classical form [15] and shown in Algorithm 1 There, the function rank determines the truncation rank based on the singular values in $S_{s}$ and maybe application dependent.

Other truncation algorithms use rank revealing $\mathrm{QR}$ or randomized algorithms [21].

Arithmetic functions for $\mathcal{H}$-matrices are typically formulated in a recursive way. For the matrix multiplication

```
Algorithm 1: Lowrank Truncation via SVD
procedure truncate(in: $U, V$, out: $W, X$ )
    $\left[Q_{U}, R_{U}\right]:=\operatorname{qr}(U)$
    $\left[Q_{V}, R_{V}\right]:=\operatorname{qr}(V)$;
    $\left[U_{s}, S_{s}, V_{s}\right]:=\operatorname{svd}\left(R_{U} \cdot R_{V}^{\prime}\right)$;
    $k:=\operatorname{rank}\left(S_{s}\right.$ );
    $W:=Q_{U} \cdot U_{s}(1: k,:) \cdot S_{s}(1: k, 1: k)$;
    $X:=Q_{V} \cdot V_{s}(1: k,:)$;
```

$C:=\alpha A \cdot B+C$ with $A, B, C \in \mathcal{H}\left(T_{I \times I}, k\right)$ such a formulation can be found in Algorithm 2 where the cluster tree $T(I)$ is assumed to be binary.

```
Algorithm 2: $\mathcal{H}$-Matrix Multiplication
procedure hmul $\left(\alpha, A_{\tau, \rho}, B_{\rho, \sigma}, C_{\tau, \sigma}\right)$
    if $\{(\tau, \rho),(\rho, \sigma),(\tau, \sigma)\} \cap \mathcal{L}(T)=\emptyset$ then
        for $i, j, \ell \in\{0,1\}$ do
            $\operatorname{hmul}\left(\alpha, A_{\tau_{i}, \rho_{\ell}}, B_{\rho_{\ell}, \sigma_{j}}, C_{\tau_{i}, \sigma_{j}}\right)$
    else
        $C_{\tau, \sigma}:=C_{\tau, \sigma}+\alpha A_{\tau, \rho} B_{\rho, \sigma} ; \quad$ \{Block Update\}
```

The last line of Algorithm 2 forms the actual update of the corresponding matrix block and normally involves a lowrank truncation if $C_{\tau, \sigma}$ is a lowrank matrix block.

Analog recursive functions can also be formulated for triangular matrix solves or the $\mathrm{LU}$ factorization.

An alternative formulation of the $\mathcal{H}$-matrix arithmetic is described in [11]. There, instead of applying all updates to lowrank blocks directly, these updates are accumulated during the recursion in an extra accumulator matrix $\mathcal{A}_{\tau, \sigma} \in$ $\mathbb{K}^{\tau \times \sigma}$. Furthermore, non-computable, so-called pending updates, e.g., involving further recursion, are collected in sets $\left.\mathcal{P}_{\tau, \sigma} \subseteq\left\{\left(A_{\tau, \rho}, B_{\rho, \tau}\right):\{(\tau, \rho), \rho, \sigma)\right\} \subset T\right\}$. Both, $\mathcal{A}_{\tau, \sigma}$ and $\mathcal{P}_{\tau, \sigma}$, are initialized to zero at the start of the multiplication and as such don't consume any memory as lowrank matrices are used.

The $\mathcal{H}$-multiplication Algorithm 3 recursively follows the structure of the $\mathcal{H}$-matrix $C$ from root to leaves. For each sub-block of $C_{\tau, \sigma}$, first all computable pending updates, i.e., with at least one dense or lowrank block, are evaluated with the result being applied to the corresponding accumulator matrix. If $C_{\tau, \sigma}$ has sub-blocks, the remaining pending updates are split into sub-sets for all sub-blocks, which mimics the standard triple-loop in recursive matrix multiplication in Algorithm 2 Afterwards, the algorithm recurses for each sub-block with also the accumulator matrix being restricted to each of these sub-blocks. Finally, if $C_{\tau, \sigma}$ is a leaf matrix block, all updates accumulated in $\mathcal{A}_{\tau, \sigma}$ are applied in a single step.

Remark 2.4 Due to the top-down approach for accumulator based $\mathcal{H}$-arithmetic, only non-zero accumulator matrices for the current recursion path exist, which bounds its number by $\mathcal{O}(\log n)$ for a sequential implementation. In the parallel

```
Algorithm 3: Accumulated $\mathcal{H}$-Matrix Multiplication
procedure hmulaccu( $C_{\tau, \sigma}, \mathcal{A}_{\tau, \sigma}, \mathcal{P}_{\tau, \sigma}$ )
    for all $\left(A_{\tau, \rho}, B_{\rho, \sigma}\right) \in \mathcal{P}_{\tau, \sigma}$ do
        if $(\tau, \rho) \in \mathcal{L}(T)$ or $(\rho, \sigma) \in \mathcal{L}(T)$ then
            $\mathcal{A}_{\tau, \sigma}:=\mathcal{A}_{\tau, \sigma}+A_{\tau, \rho} B_{\rho, \sigma} ;$
            $\mathcal{P}_{\tau, \sigma}:=\mathcal{P}_{\tau, \sigma} \backslash\left\{\left(A_{\tau, \rho}, B_{\rho, \sigma}\right)\right\}$
    if $(\tau, \sigma) \notin \mathcal{L}(T)$ then
        for $i, j \in\{0,1\}$ do
            $\mathcal{P}_{\tau_{i}, \sigma_{j}}:=\cup_{l=0}^{1}\left\{\left(\left.A_{\tau, \rho}\right|_{\tau_{i}, \rho_{\ell}},\left.B_{\rho, \sigma}\right|_{\rho_{\ell}, \sigma_{j}}\right):\right.$
                $\left.\left(A_{\tau, \rho}, B_{\rho, \sigma}\right) \in \mathcal{P}_{\tau, \sigma}\right\}$
            $\operatorname{hmulaccu}\left(C_{\tau_{i}, \sigma_{j}},\left.\mathcal{A}_{\tau, \sigma}\right|_{\tau_{i}, \sigma_{j}}, \mathcal{P}_{\tau_{i}, \sigma_{j}}\right)$;
    else
        $C_{\tau, \sigma}:=C_{\tau, \sigma}+\mathcal{A}_{\tau, \sigma} ;$
```

case, this typicall increases linearly with the number of parallel execution paths. The additional memory overhead for the accumulator matrices is therefore negligible compared to the memory of the involved $\mathcal{H}$-matrices.

Independent on the arithmetic version, effective parallelization of the $\mathcal{H}$-arithmetic on shared-memory systems can be done by identification of the arithmetic tasks, which, in the case of $\mathcal{H}$-matrix multiplication are defined by the last line in Algorithm 2 and the dependencies between these tasks, which together form a directed acyclic graph (DAG) and can be executed by a scheduling system. However, while the task definition is straight-forward and can easily be performed with the recursive $\mathcal{H}$-arithmetic functions, the dependencies are more involved as different, unrelated parts of the block tree are affected 22 12].

## 3 Compressed $\mathcal{H}$-matrices

$\mathcal{H}$-matrices use dense memory blocks for inadmissible subblocks and for the lowrank factors $U, V$. These data blocks are of interest for floating point compression. This is especially true for the lowrank factors as lowrank approximation already introduces an error, typically much higher compared to the unit roundoff of the FP64 format.

The storage of these data blocks is considered to be separate even on the block level, i.e., $U$ and $V$ are stored independently from each other, in contrast to a joined storage. This simplifies the data conversion during $\mathcal{H}$-arithmetic as both factors may be used at different times during arithmetic operations, e.g., $\mathcal{H}$-LU factorization or matrix solves.

In the following we will use the lowrank approximation accuracy $\varepsilon$ from (1) also for the compression accuracy, i.e., the same error that was permitted for the lowrank approximation may also be allowed for floating point compression.

### 3.1 Floating Point Compression Libraries

Floating point compressors like ZFP or SZ/SZ3 seem an obvious choice when the dense data blocks in the $\mathcal{H}$-matrices
should be compressed further, assuming a sufficient error control is provided by the compression schemes.

For ZFP, this was possible with fixed rate mode, which uses a constant number of bits for each data block The (bit-) rate for a given accuracy $\varepsilon$ was determined by experiments to be $\left\lceil\left|\log _{2} \varepsilon\right|\right\rceil+2$. Unfortunately, fixed-precision or fixed accuracy modes also provided by ZFP and potentially superior to the fixed-rate mode were much more difficult in the context of $\mathcal{H}$-matrices and did not allowed a reliable error control.

SZ and SZ3 support more options for handling the error, e.g., absolute or relative error bounds, peak signal to noise ratio or Frobenius error. However, in experiments, difficulties with the compression of $\mathcal{H}$-matrix data were observed if the accuracy $\varepsilon$ exceeded $10^{-5}$, i.e., $\varepsilon \leq 10^{-5}$ (see Figure1). The same holds for MGARD.

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-04.jpg?height=400&width=480&top_left_y=1045&top_left_x=317)

Figure 1: Compression rates for SZ3 and MGARD for Laplace model problem (2) with $n=32.768$.

Though the cause of these problems is unknown, both SZ/SZ3 and MGARD rely on some form of approximability of the given data, either by curve fitting (SZ/SZ3) or by multigrid techniques (MGARD). While such properties may be given in the original dense data, within $\mathcal{H}$-matrices the lowrank factors already represent a form of compressed data which may not have these approximation properties anymore. Therefore, SZ/SZ3 and MGARD were not considered for further experiments.

### 3.2 IEEE-754 based Compression

As already discussed, using a single floating point format will not suffice for storing data within an $\mathcal{H}$-matrix. Instead, a format with a much higher adaptivity towards lowrank approximation accuracy is needed.

Discussed in [6] is a format for a fully adaptive choice of the mantissa and exponent bits in the IEEE-754 scheme, depending on the floating point values in the individual blocks of the block-Jacobi preconditioner. The dense and lowrank blocks of $\mathcal{H}$-matrices provide also a partitioning of the matrix data and by that also permit an per sub-block[^0]

adaptive choice of the storage layout. FlexFloat [27] implements the same adaptive choice of mantissa and exponent bits. However, the in-memory storage of FlexFloat does not make use of it and as such, no actual memory savings can be expected.

While the number of mantissa bits depends on the accuracy requirements of the lowrank approximation, the exponent size depends on the dynamic range of the data values, i.e., the base 10 logarithm of the ratio between the largest and smallest (absolute) value. For the standard floating point formats the corresponding dynamic range is typically huge, as can be seen in Table 2

| FP64 | FP32 | TF32 | BF16 | FP16 | FP8 |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 631 | 83 | 79 | 78 | 12 | 5 |

Table 2: Dynamic range of standard floating point formats.

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-04.jpg?height=440&width=805&top_left_y=1119&top_left_x=1045)

Laplace SLP

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-04.jpg?height=382&width=377&top_left_y=1171&top_left_x=1048)

Matérn covariance

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-04.jpg?height=377&width=388&top_left_y=1174&top_left_x=1462)

Figure 2: Dynamic range of per data block floating point values during $\mathcal{H}$-LU factorization for $n=32.768$.

For the applications used for the numerical experiments in Section 5 the observed dynamic range during $\mathcal{H}$-LU factorization, i.e., for all dense and lowrank data blocks for all intermediate and final data, is shown in Figure 2 As can be seen, it is much smaller compared to what is anticipated in the standard formats of the IEEE-754 scheme. This permits to reduce the number of bits for the floating point storage not only for the mantissa but also for the exponent part.

With $e$ exponent bits, a dynamic range of $\log _{10}\left(2^{2^{e}}\right)$ is reached ${ }^{2}$ For a given data block $M \in \mathbb{R}^{n \times m}$ the dynamic range $r:=\log _{10} \frac{m_{\max }}{m_{\min }}$, with $m_{\max }:=\max _{i, j} m_{i j}$ and $m_{\text {min }}:=\min _{i, j} m_{i j}$, can therefore be represented by $e_{r}:=$ $\left\lceil\log _{2} \log _{2} r\right\rceil$ exponent bits. Together with the $m_{\varepsilon}$ mantissa bits chosen based on the required accuracy, we will use $1-e_{r}-m_{\varepsilon}$ as the storage format.

Remark 3.1 For easier conversion, all values are scaled by $1 / m_{\min }$ and shifted by 1 , i.e., $m_{i j} / m_{\min }+1$, which ensures[^1]values bigger than 2 and as such only the lower $e_{r}$ exponent bits need to be copied into compressed storage.

This format is denoted AFL and is similar to CPEN in [6]. However, it leads to a non-byte aligned storage format as $1+e_{r}+m_{\varepsilon}$ is not necessarily a multiple of 8 . The required bit handling may increase the overhead while reading/writing data from/to memory.

As an alternative, $m_{\varepsilon}$ is increased to $m_{\varepsilon}^{\prime}$ such that $1+e_{r}+$ $m_{\varepsilon}^{\prime} \bmod 8=0$. The resulting format is denoted $A F L P$.

For comparision, we will also use a format with an adaptive choice of mantissa bits but a constant number of 8 exponent bits (BFL), as this is identical to the single precision IEEE-754 format and an analog format with 11 exponent bits (DFL) which equals the corresponding double precision format. In both cases, it should be more efficient to copy the exponent bits from the original (casted) values. Furthermore, to ensure byte-aligned storage, the mantissa bits are again increased in both formats as with the AFLP format.

Remark 3.2 The CPMS format in [6] is comparable to DFL due to the adaptive mantissa choice and the 11 exponent bits from the FP64 format, however the actual storage scheme is different due to separate mantissa and exponent storage.

Remark 3.3 An alternative to the IEEE-754 formats are posits [16], which use $n$ bits including es (maximal) exponent bits. This permits an adaptive storage (and compute) format depending on the accuracy $\varepsilon$ and the required dynamic range. However, standard CPUs and GPUs do not yet support this format.

### 3.3 Adaptive Precision Compression

For a matrix block $M_{\tau, \sigma}$ we assume a rank- $k$ approximation $U_{\tau, \sigma} V_{\tau, \sigma}^{H}$ with $\left\|M_{\tau, \sigma}-U_{\tau, \sigma} V_{\tau, \sigma}^{H}\right\| \leq \delta, \delta=\varepsilon \| M_{\tau, \sigma}$ (rf. (1).

With $p$ different floating point formats with corresponding unit roundoffs $u_{i}, U_{\tau, \sigma} V_{\tau, \sigma}^{H}$ is represented in [4] as

$$
\begin{aligned}
& U_{\tau, \sigma} V_{\tau, \sigma}^{H}=W \Sigma X^{H}:= \\
& \left(\begin{array}{lll}
W_{0} & \ldots & W_{p-1}
\end{array}\right)\left(\begin{array}{lll}
\Sigma_{0} & & \\
& \ddots & \\
& & \Sigma_{p-1}
\end{array}\right)\left(\begin{array}{lll}
X_{0} & \ldots & X_{p-1}
\end{array}\right)^{H}
\end{aligned}
$$

with orthogonal $W \in \mathbb{R}^{\# \tau \times k}, X \in \mathbb{R}^{\# \sigma \times k}$ and $\Sigma \in$ $\mathbb{R}^{k \times k}$. The subblocks $W_{i} \in \mathbb{R}^{\# \tau \times k_{i}}, X_{i} \in \mathbb{R}^{\# \sigma \times k_{i}}$ are then represented in the $i$ 'th floating point format, while $\Sigma_{i} \in \mathbb{R}^{k_{i} \times k_{i}}$ holds the singular values $\sigma_{j}$ of the corresponding subblock and is stored in the floating point format with highest accuracy (normally FP64). The partitioning of the $k=\sum_{i=0}^{p-1} k_{i}$ singular values is determined by the unit roundoffs $u_{i}$ such that $\left\|\Sigma_{i}\right\| \approx \delta / u_{i}$, ensuring [4 Theorem 2.1] the overall error bound

$$
\left\|M_{\tau, \sigma}-W \Sigma X^{H}\right\| \leq\left(2 p-1+\sum_{i=1}^{p-1} \sqrt{r_{i}} u_{i}\right) \delta
$$

This is based on a given set of floating point formats, which is normally defined by available hardware support. However, using a general floating point compression scheme with adaptive error control, e.g., ZFP or AFL, one may reverse the view and choose a precision $\widetilde{u}_{i}$ such that $\sigma_{i} \approx \delta / \widetilde{u}_{i}$. With this, one can represent $U_{\tau, \sigma} V_{\tau, \sigma}^{H}$ as

$$
\begin{aligned}
& U_{\tau, \sigma} V_{\tau, \sigma}^{H}=W \Sigma X^{H} \approx \\
& \left(\begin{array}{lll}
\widetilde{w}_{0} & \ldots & \widetilde{w}_{k-1}
\end{array}\right)\left(\begin{array}{ccc}
\sigma_{0} & & \\
& \ddots & \\
& & \sigma_{k-1}
\end{array}\right)\left(\begin{array}{lll}
\widetilde{x}_{0} & \ldots & \widetilde{x}_{k-1}
\end{array}\right)^{H}
\end{aligned}
$$

with $\widetilde{w}_{i}\left(\widetilde{x}_{i}\right)$ being the $i$ 'th column of $W(X)$, stored with precision $\widetilde{u}_{i}$.

The such extended lowrank compression shall be denoted Adaptive Precision (vs. mixed precision) compression for Low-Rank matrices (APLR). The resulting storage format is denoted by ZFP-APLR or AFL-APLR, respectively. In an analog way, the schemes AFLP, BFL, DFL or any other floating point compressor can be extended.

## 4 Compressed $\mathcal{H}$-arithmetic

In [6], the concept of a memory accessor is introduced, which implements on-the-fly conversion between the storage format and the computation format during arithmetic. In principle, the same could be applied to $\mathcal{H}$-arithmetic as well. However, this would need a complete implementation of all basic linear algebra functions on which $\mathcal{H}$-arithmetic is based, normally provided by vendor optimized libraries implementing the BLAS/LAPACK [5] function set.

As this would (most probably) result in a much less efficient implementation compared to existing BLAS/LAPACK libraries, we refrained from this concept. Instead a semi-onthe-fly approach was chosen for compressed $\mathcal{H}$-arithmetic, where the conversion between the storage and the compute formats are performed before and after the standard arithmetic functions.

An example for this is presented in Algorithm 4 which is the compressed version of Algorithm 1 At the beginning, the input data is decompressed from the storage format into the computation format and at the end the output data is compressed back into the storage format.

With this, any $\mathcal{H}$-arithmetic may simply be extended by the data conversion function calls and the data storage within $\mathcal{H}$-matrices is replaced by the corresponding data storage format.

```
Algorithm 4: Lowrank truncation with semi-on-the-fly
data conversion
procedure truncate(in: $U^{c}, V^{c}$, out: $W^{c}, X^{c}$ )
    $[U, V]:=$ decompress $\left(U^{c}, V^{c}\right)$;
    $\left[Q_{U}, R_{U}\right]:=\operatorname{qr}(U)$;
    $\left[Q_{V}, R_{V}\right]:=\operatorname{qr}(V)$;
    $\left[U_{s}, S_{s}, V_{s}\right]:=\operatorname{svd}\left(R_{U} \cdot R_{V}^{\prime}\right)$;
    $k:=\operatorname{rank}\left(S_{s}\right.$ );
    $W:=Q_{U} \cdot U_{s}(1: k,:) \cdot S_{s}(1: k, 1: k) ;$
    $X:=Q_{V} \cdot V_{s}(1: k,:)$;
    $\left[W^{c}, X^{c}\right]:=\operatorname{compress}(W, X)$;
```

When using adaptive precision compression, e.g., AFLAPLR, the same concept may be applied reusing existing arithmetic functions for $\mathcal{H}$-matrices. However, the orthogonality of the $W$ and $X$ factors need to be ensured which may lead to additional computational costs. In principle, one may further optimize certain functions to make use of the $W \Sigma X^{H}$ representation, e.g., in Algorithm 1 the multiplication with $S_{s}(1: k, 1: k)$ can be omitted.

## 4.1 $\mathcal{H}$-LU factorization

When applying this concept for $\mathcal{H}$-LU factorization with standard arithmetic, i.e., as in Algorithm 2 with eager update application to destination blocks, the additional error during compression after each update has a significant effect on the overall error of the factorization process, measured as $\left\|I-A(L U)^{-1}\right\|_{2}$ with $A$ being the original $\mathcal{H}$-matrix and $L, U$ its computed $L U$ factors in compressed $\mathcal{H}$-matrix format. This is shown in Figure 3(left):

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-06.jpg?height=431&width=808&top_left_y=1658&top_left_x=156)

Standard $\mathcal{H}$-Arith.

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-06.jpg?height=368&width=363&top_left_y=1712&top_left_x=167)

Accumulator $\mathcal{H}$-Arith.

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-06.jpg?height=368&width=376&top_left_y=1712&top_left_x=583)

Figure 3: LU factorization error with standard (left) and accumulator based $\mathcal{H}$-arithmetic (right) for Laplace model problem (2).

To overcome this problem one could tighten the accuracy settings for compression during $\mathcal{H}$-arithmetic, which would however deteriorate the compression ratio.

Alternatively, when using accumulator based $\mathcal{H}$ arithmetic the accumulator matrices do not need to be compressed as this would have little effect on the overall memory usage (rf. Remark 2.4. Therefore, no additional error is introduces when computing the updates. The effect of this is visible in Figure 3 (right), where the error of the compressed arithmetic is identical to the uncompressed version. Since accumulator based $\mathcal{H}$-arithmetic is also typically faster due to a reduced number of lowrank truncations, it is therefore favorable to use this arithmetic when also using compression.

## 5 Numerical Experiments

### 5.1 Model Problems

The first problem is based on a boundary element discretization for the Laplace single layer potential (Laplace SLP) while the domain is defined by the unit sphere:

$$
\begin{equation*}
\int_{\Omega} \frac{1}{\|x-y\|} u(x) d y=f(x), \quad x \in \Omega \tag{2}
\end{equation*}
$$

with $\Omega=\left\{x \in \mathbb{R}^{3}:\|x\|_{2}=1\right\}$. Piecewise constant ansatz functions are used for the discretization. Furthermore, standard admissibility

$$
\min \{\operatorname{diam}(t), \operatorname{diam}(s)\} \leq \eta \operatorname{dist}(\tau, \sigma)
$$

is applied for setting up the block tree.

The Matérn class of covariance functions forms the second model problem:

$$
\begin{equation*}
C(d, \sigma, \ell, \nu)=\sigma^{2} \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2 \nu}}{\ell} d\right)^{\nu} \mathcal{K}_{\nu}\left(\frac{\sqrt{2 \nu}}{\ell} d\right) \tag{3}
\end{equation*}
$$

with $d=\left\|x_{i}-x_{j}\right\|_{2}$ being the distance between two points $x_{i}, x_{j} \in \Omega$ (randomly defined with unique seed), $\sigma^{2}=1$ the variance, $\ell=1$ a spatial range parameter and $\nu=1 / 3$ controlling the smoothness of the random field. Furthermore, weak admissibility is used as defined in [19], which leads to fewer, larger lowrank blocks.

Both applications use a problem size of $n=1.048 .576$ and lowrank blocks are approximated via ACA [10]. All computations are performed in double precision (FP64) and this is also the default storage format, which serves as the baseline in the following comparisons, indicated, if not otherwise noted, by a thick, black line. Results are therefore presented relative to the FP64 case.

### 5.2 Machine and Software Settings

All benchmarks are performed on a shared memory system with two AMD Epyc 9554 CPUs with 128 cores in total and 2x12 32GB DDR5-4800 memory DIMMs.

For parallelization Intel TBB v2021.2 was used while Intel MKL v2022.0 provided the BLAS and LAPACK functions. Please note, that here the sequential version was used as all parallelization is performed within the $\mathcal{H}$-arithmetic itself. Furthermore, the AVX512 code path in MKL was activated.

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-07.jpg?height=460&width=1625&top_left_y=318&top_left_x=181)

Laplace SLP

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-07.jpg?height=385&width=377&top_left_y=370&top_left_x=197)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-07.jpg?height=374&width=377&top_left_y=381&top_left_x=588)

Matérn covariance

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-07.jpg?height=383&width=377&top_left_y=374&top_left_x=1019)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-07.jpg?height=374&width=374&top_left_y=384&top_left_x=1412)

Figure 4: Compression rates.

All code was compiled using GCC v12.1. Finally, ZFP v1.0 was used.

The algorithms described in this work are implemented in the open source software HLR ${ }^{3}$

For all benchmarks the median of ten runs is presented.

## $5.3 \mathcal{H}$-compression

First, the compression of the $\mathcal{H}$-matrices after construction is examined. The compression is performed after fully assembling the $\mathcal{H}$-matrices as these are also needed for the actual comparison. In practice this should be done on-the-fly after each matrix block is constructed to reduce the overall memory consumption. The absolute memory usage of the uncompressed $\mathcal{H}$-matrix varies between 16GB $\left(\varepsilon=10^{-3}\right)$ and 42GB $\left(\varepsilon=10^{-8}\right)$ for the Laplace SLP problem and 8GB $\left(\varepsilon=10^{-3}\right)$ to $29 \mathrm{~GB}\left(\varepsilon=10^{-8}\right)$ for the Matérn covariance matrix.

In Figure 4 the resulting compression rates, i.e., the ratios between the uncompressed and the compressed $\mathcal{H}$-matrices, are shown for the compression schemes discussed in Section 3 and the mixed precision approach from [4] using FP64, FP32 and FP16 (denoted "MP-D-S-H").

Best compression is achieved by the ZFP library, however, only slightly more storage is needed with the AFL format. The remaining storage schemes with rounded up mantissa bits (AFLP) or more exponent bits (BFL and DFL) follow. While for a coarse accuracy the compression is significantly better than the mixed precision approach, the latter is on par (Laplace SLP) or even exceeding (Matérn covariance) the other formats. The reason for this behaviour is that the mixed precision format is able to use half precision floating point formats even for a high accuracy whereas the given error bound applies to all data in a given floating point compression scheme like ZFP or AFL.

This changes with adaptive precision compression where the error bounds depend on the singular values. With this, the corresponding compression rates can be significantly improved. Throughout the accuracy range much[^2]

better results compared to the mixed precision approach are achieved, reducing memory down to $20 \%$ to about $30 \%$ of the original FP64 storage with ZFP and AFL.

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-07.jpg?height=411&width=468&top_left_y=1094&top_left_x=1205)

Figure 5: Compression rates with compression only applied to lowrank blocks for the Laplace model problem.

Part of the advantage of ZFP-APLR or AFL-APLR is, that in the mixed precision approach inadmissible blocks are not compressed. Figure 5 shows the results if only lowrank blocks are compressed. While ZFP and AFL are very close together, both have, depending on the accuracy, a 5-10\% better compression rate compared to just using three floating point types. It also shows the impact when applying compression to the dense matrices which still contribute significantly to the overall storage even for rather large problem sizes.

Next, the compression time is shown in Figure 7. There, the runtime is presented relative to the $\mathcal{H}$-matrix construction time. As can be seen, the additional compression overhead is negligible compared to setting up the $\mathcal{H}$-matrix itself. However, this depends on the time to compute matrix coefficients or what lowrank approximation method was used, which may be different for other applications.

### 5.4 Matrix-Vector Multiplication

Multiplication of a compressed $\mathcal{H}$-matrix with a vector will directly show the effect of the decompression speed of the corresponding compression scheme with the results being

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-08.jpg?height=459&width=1630&top_left_y=316&top_left_x=176)

Laplace SLP

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-08.jpg?height=394&width=380&top_left_y=366&top_left_x=198)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-08.jpg?height=391&width=380&top_left_y=367&top_left_x=587)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-08.jpg?height=440&width=787&top_left_y=325&top_left_x=1020)

Figure 6: Relative runtime of matrix vector multiplication.
![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-08.jpg?height=430&width=802&top_left_y=915&top_left_x=158)

Figure 7: Compression time for $\mathcal{H}$-matrices.

presented in Figure 6 Please note, that again relative performance numbers compared to the uncompressed FP64 multiplication (indicated by the thick black baseline) are used.

With compressed storage a better performance can be achieved for the lower accuracy regime using AFLP, BFL and DFL. This indicates a memory bandwith limitation of $\mathcal{H}$-matrix vector multiplication for these problems. However, of special importance for this is a fast decompression, which does not hold for AFL and especially for ZFP. However, for an accuracy towards $\varepsilon=10^{-8}$, the computational intensity of the matrix vector multiplication increases due to a larger rank within the lowrank blocks, shifting the algorithm more into a compute bound regime and thereby making the decompression overhead visible.

This can also be seen for the Matérn covariance problem, which typically shows a larger rank in the lowrank blocks compared to the Laplace SLP model problem. Here, the break-even point for AFL, BFL and DFL is sooner and the overall performance is worse compared to the original $\mathcal{H}$ matrix vector multiplication.

With additional computational overhead due to adaptive precision compression the performance gain is even more reduced and requires a significantly higher runtime for the Matérn covariance model problem with a high accuracy, even surpassing the mixed precision approach.
Remark 5.1 It should be noted that with corresponding hardware and software support the mixed precision technique has the additional advantage of a faster execution time for those parts stored in single or half precision. This was not available for the benchmark system used in this work.

## 5.5 $\mathcal{H}$-LU Factorization

In Figure 8 the relative runtimes of the $\mathcal{H}$-LU factorization for the model problems with and without adaptive precision compression are shown.

Unexpectedly, the runtime with BFL and DFL is slightly faster than the uncompressed version for the Laplace SLP problem, indicating some memory bandwidth influence on the performance of the $\mathcal{H}$-LU factorization. The slightly costlier AFLP shows the same performance as the uncompressed $\mathcal{H}$-LU as is also the case for BFL/DFL for the Matérn covariance matrix.

About $50 \%$ slower runtimes are achieved with AFL or mixed precision storage. ZFP again shows a much worse performance and is about four times slower compared to the uncompressed version.

With adaptive precision compression the picture is very similar though with slightly increased runtimes. The most notable difference is, that the performance advantage of $\mathrm{BFL} / \mathrm{DFL}$ or the on par performance of AFLP turns into a slight performance disadvantage compared to the uncompressed $\mathcal{H}$-LU.

However, the overhead is still small compared to the memory savings, which are similar to the original $\mathcal{H}-$ matrix as can be seen in Figure 9. In absolute terms the memory footprint of the uncompressed factors is slightly larger compared to the $\mathcal{H}$-matrix, i.e., $18 \mathrm{~GB}\left(\varepsilon=10^{-3}\right)$ to 48GB $\left(\varepsilon=10^{-8}\right)$ for the Laplace SLP problem and 12GB $\left(\varepsilon=10^{-3}\right)$ to $40 \mathrm{~GB}\left(\varepsilon=10^{-8}\right)$ for the Matérn covariance problem.

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-09.jpg?height=465&width=1673&top_left_y=316&top_left_x=157)

Laplace SLP

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-09.jpg?height=399&width=402&top_left_y=366&top_left_x=176)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-09.jpg?height=389&width=377&top_left_y=368&top_left_x=588)

Matérn covariance

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-09.jpg?height=394&width=397&top_left_y=368&top_left_x=1018)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-09.jpg?height=385&width=389&top_left_y=370&top_left_x=1413)

Figure 8: Relative runtime of $\mathcal{H}$-LU factorization.

Laplace SLP

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-09.jpg?height=400&width=400&top_left_y=931&top_left_x=157)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-09.jpg?height=391&width=394&top_left_y=935&top_left_x=591)

Matérn covariance

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-09.jpg?height=394&width=397&top_left_y=934&top_left_x=1024)

![](https://cdn.mathpix.com/cropped/2024_06_04_7f76206f77810817bf10g-09.jpg?height=392&width=391&top_left_y=932&top_left_x=1458)

Figure 9: Compression rates for $\mathcal{H}$-LU factors.

## 6 Conclusion

The application of binary compression to the data blocks in $\mathcal{H}$-matrices effectively decreases the memory costs further and shows a significant advantage to other available formats, e.g., mixed precision schemes, especially if adaptive precision compression for lowrank data is used.

Furthermore, using such additional compression can be done with little effect on the parallel performance of typical $\mathcal{H}$-arithmetic operations, sometimes even resulting in a performance advantage due to decreased memory bandwidth utilization.

Essential to this is a fast compression scheme with adaptive error control as is available with $\mathrm{ZFP}$ or the presented IEEE754 based formats. It would be of high interest to look into other compressors in the future in the hope that compression rates or arithmetic performance can be further improved.

## References

[1] S. Abdulah et al. "Accelerating Geostatistical Modeling and Prediction With Mixed-Precision Computations: A HighProductivity Approach With PaRSEC". In: IEEE Transactions on Parallel and Distributed Systems 33.4 (2022), pp. 964-976. DoI: 10.1109/TPDS. 2021.3084071
[2] M. Ainsworth, O. Tugluk, B. Whitney, and S. Klasky. "Multilevel techniques for compression and reduction of scientific data - the univariate case". In: Comput. Visual Sci. 19 (2018), pp. 65-76.

[3] S. Ambikasaran and E. Darve. "An $\mathcal{O}(N \log N)$ Fast Direct Solver for Partial Hierarchically Semi-Separable Matrices". In: Journal of Scientific Computing 57.3 (2013), pp. 477-501.

[4] P. Amestoy et al. "Mixed precision low-rank approximations and their application to block low-rank LU factorization". In: IMA fournal of Numerical Analysis (Aug. 2022). DoI: 10.1093/imanum/drac037

[5] E. Anderson et al. LAPACK Users' Guide. Third. Philadelphia, PA: Society for Industrial and Applied Mathematics, 1999. ISBN: 0-89871-447-8 (paperback).

[6] Hartwig Anzt, Thomas Grützmacher, and Enrique S. Quintana-Ortí. "Toward a modular precision ecosystem for high-performance computing”. In: International fournal of High Performance Computing Applications 33.6 (2019), pp. 1069-1078. Dor: $10.1177 / 1094342019846547$

[7] W. Baert and N. Vannieuwenhoven. "ATC: an Advanced Tucker Compression library for multidimensional data". 2021. DoI: 10.48550/ARXIV. 2107.01384

[8] Grey Ballard, Alicia Klinvex, and Tamara G. Kolda. "TuckerMPI: A Parallel C++/MPI Software Package for Large-Scale Data Compression via the Tucker Tensor Decomposition". In: ACM Trans. Math. Softw. 46.2 (June 2020). IsSN: 0098-3500.

[9] R. Ballester-Ripoll, P. Lindstrom, and R. Pajarola. "TTHRESH: Tensor Compression for Multidimensional Visual Data". In: IEEE Transactions on Visualization and Computer Graphics 26.9 (2020), pp. 2891-2903. DOI: 10.1109/TVCG.2019.2904063

[10] M. Bebendorf. "Approximation of boundary element matrices". In: Numerische Mathematik 86 (2000), pp. 565-589.

[11] S. Börm. "Hierarchical matrix arithmetic with accumulated updates". In: Computing and Visualization in Science 20.3 (2019), pp. 71-84.

[12] Steffen Börm, Sven Christophersen, and Ronald Kriemann. "Semi-Automatic Task Graph Construction for $\mathcal{H}$-Matrix Arithmetic". In: SIAM Journal on Scientific Computing 44.2 (2022), pp. C77-C98. DOI: 10.1137/20M1318808

[13] S. Chandrasekaran et al. "Some Fast Algorithms for Sequentially Semiseparable Representations". In: SIAM Journal on Matrix Analysis and Applications 27 (2 2005), pp. 341-364.

[14] S. Di and F. Cappello. "Fast Error-Bounded Lossy HPC Data Compression with SZ". In: 2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS). 2016, pp. 730-739.

[15] L. Grasedyck and W. Hackbusch. "Construction and arithmetics of $\mathcal{H}$-matrices". In: Computing 70 (2003), pp. 295334.

[16] J.L. Gustafson and I. Yonemoto. "Beating Floating Point at Its Own Game: Posit Arithmetic”. In: Supercomput. Front. Innov.: Int. F. 4.2 (June 2017), pp. 71-86. ISSN: 2409-6008. DOI: 10.14529/jsfi170206

[17] W. Hackbusch. "A sparse matrix arithmetic based on $\mathcal{H}$ Matrices. Part I: Introduction to $\mathcal{H}$-Matrices". In: Computing 62 (1999), pp. 89-108.

[18] W. Hackbusch and B.N. Khoromskij. "A sparse $\mathcal{H}$-matrix arithmetic. Part II. Application to multi-dimensional problems". In: Computing 64.1 (2000), pp. 21-47.

[19] W. Hackbusch, B.N. Khoromskij, and R. Kriemann. "Hierarchical Matrices Based on a Weak Admissibility Criterion". In: Computing 73.3 (2004), pp. 207-243. DOI: $10.1007 /$ s00607$004-0080-4$

[20] W. Hackbusch, B.N. Khoromskij, and R. Kriemann. "Hierarchical matrices based on a weak admissibility criterion". In: Computing 73 (3 2004), pp. 207-243.

[21] N. Halko, P. G. Martinsson, and J. A. Tropp. "Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions". In: SIAM Rev. 53.2 (May 2011), pp. 217-288.

[22] R. Kriemann. "H-LU factorization on many-core systems". In: Computing and Visualization in Science 16.3 (June 2013), pp. 105-117. ISSN: 1433-0369.

[23] R. Kriemann et al. "High-Performance Spatial Data Compression for Scientific Applications". In: Euro-Par 2022: Parallel Processing. Ed. by José Cano and Phil Trinder. Cham: Springer International Publishing, 2022, pp. 403-418. ISBN: 978-3-031-12597-3.

[24] X. Liang et al. "Error-Controlled Lossy Compression Optimized for High Compression Ratios of Scientific Datasets". In: 2018 IEEE International Conference on Big Data (Big Data). 2018, pp. 438-447. DoI:10.1109/BigData.2018.8622520

[25] P. Lindstrom. "Fixed-Rate Compressed Floating-Point Arrays". In: IEEE Transactions on Visualization and Computer Graphics 20.12 (2014), pp. 2674-2683.
[26] LZ4. 2023. urL: https://lz4.github.io/lz4/

[27] Giuseppe Tagliavini, Andrea Marongiu, and Luca Benini. "FlexFloat: A Software Library for Transprecision Computing". In: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 39 (2020), pp. 145-156. URL: https: / api . semanticscholar . org / CorpusID: 70038607

[28] K. Zhao et al. "Optimizing Error-Bounded Lossy Compression for Scientific Data by Dynamic Spline Interpolation". In: 2021 IEEE 37th International Conference on Data Engineering (ICDE). 2021, pp. 1643-1654. DOI: 10.1109/ICDE51399. 2021.00145

[29] zlib. 2023. URL: https://github.com/madler/zlib

[30] Zstandard. 2023. URL: https://facebook.github. io/ zstd/


[^0]:    ${ }^{1}$ ZFP handles sub-blocks of size $4{ }^{d}$ seperately, with $d$ being the dimension of the data.

[^1]:    ${ }^{2}$ This does not take into account differences between normalized and denormalized numbers in IEEE754.

[^2]:    http://libhlr.org programs: compress, compress-lu, mixedprec and mixedprec-lu

