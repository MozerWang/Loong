# Shifted Composition I: Harnack and Reverse Transport Inequalities 

Jason M. Altschuler<br>UPenn<br>alts@upenn.edu

Sinho Chewi<br>IAS<br>schewi@ias.edu


#### Abstract

We formulate a new information-theoretic principle - the shifted composition rule-which bounds the divergence (e.g., Kullback-Leibler or Rényi) between the laws of two stochastic processes via the introduction of auxiliary shifts. In this paper, we apply this principle to prove reverse transport inequalities for diffusions which, by duality, imply F.-Y. Wang's celebrated dimension-free Harnack inequalities. Our approach bridges continuous-time coupling methods from geometric analysis with the discrete-time shifted divergence technique from differential privacy and sampling. It also naturally gives rise to (1) an alternative continuous-time coupling method based on optimal transport, which bypasses Girsanov transformations, (2) functional inequalities for discrete-time processes, and (3) a family of "reverse" Harnack inequalities.


## Contents

1 Introduction ..... 2
2 Information-theoretic preliminaries ..... 5
3 Discrete-time arguments ..... 7
3.1 Shifted composition rule ..... 7
3.2 One-step to multi-step bounds ..... 8
3.3 Convexity principle ..... 11
3.4 Application to the Langevin diffusion ..... 13
3.5 Discussion: relationship with differential privacy and sampling ..... 15
4 Continuous-time arguments ..... 15
4.1 Synchronous coupling and Girsanov's theorem ..... 16
4.2 Wasserstein coupling and Otto calculus ..... 18
5 Extensions to other settings ..... 21
5.1 Diffusions on manifolds ..... 21
5.2 Multiplicative noise ..... 21
5.3 Sums of i.i.d. random variables ..... 23
6 Applications to Harnack inequalities ..... 25
6.1 Background on Harnack inequalities ..... 25
6.2 Duality between Harnack and reverse transport inequalities ..... 28
6.3 Reverse Harnack inequalities ..... 29
6.4 Harnack inequalities for discretizations of diffusions ..... 30
A Deferred details ..... 31
A. 1 Proof of the Rényi composition rule ..... 31
A. 2 Optimizing the shifts ..... 32
A. 3 Tightness ..... 33
A. 4 Rényi divergence bounds via continuous-time arguments . ..... 35
A. 5 Reverse Harnack inequalities via semigroup methods ..... 37
B Dual proofs ..... 39
B. 1 Distributional Harnack inequalities ..... 39
B. 2 Composition of reverse transport inequalities ..... 40
References ..... 40

## 1 Introduction

In this paper, we formulate a new technique for bounding information-theoretic divergences, such as the Kullback-Leibler (KL) or Rényi divergence, between two probability laws. In the case of the KL divergence, it extends the classical chain rule

$$
\begin{equation*}
\mathrm{KL}\left(\boldsymbol{\mu}^{Y} \| \boldsymbol{\nu}^{Y}\right) \leqslant \mathrm{KL}\left(\boldsymbol{\mu}^{X, Y} \| \boldsymbol{\nu}^{X, Y}\right)=\mathrm{KL}\left(\boldsymbol{\mu}^{X} \| \boldsymbol{\nu}^{X}\right)+\int \mathrm{KL}\left(\boldsymbol{\mu}^{Y \mid X=x} \| \boldsymbol{\nu}^{Y \mid X=x}\right) \boldsymbol{\mu}^{X}(\mathrm{~d} x) \tag{1.1}
\end{equation*}
$$

Here $X$ and $Y$ are jointly defined random variables on a suitable probability space $\Omega, \boldsymbol{\mu}$ and $\boldsymbol{\nu}$ are two probability measures over $\Omega$, and we use the obvious notation (e.g., $\boldsymbol{\mu}^{X, Y}$ denotes the joint law of $(X, Y), \boldsymbol{\mu}^{X}$ denotes the marginal law of $X$, and $\boldsymbol{\mu}^{Y \mid X=x}$ denotes the conditional law of $Y$ given $X=x$, all under the measure $\boldsymbol{\mu}$ ). The first inequality in (1.1) follows from the data-processing inequality (see Theorem 2.2 ).

Our technique is based on a simple yet crucial modification of (1.1). For any third random variable $X^{\prime}$, jointly defined with $X$ and $Y$ on $\Omega$, we prove that

$$
\begin{equation*}
\mathrm{KL}\left(\boldsymbol{\mu}^{Y} \| \boldsymbol{\nu}^{Y}\right) \leqslant \mathrm{KL}\left(\boldsymbol{\mu}^{X^{\prime}, Y} \| \boldsymbol{\nu}^{X, Y}\right) \leqslant \mathrm{KL}\left(\boldsymbol{\mu}^{X^{\prime}} \| \boldsymbol{\nu}^{X}\right)+\int \mathrm{KL}\left(\boldsymbol{\mu}^{Y \mid X=x} \| \boldsymbol{\nu}^{Y \mid X=x^{\prime}}\right) \gamma\left(\mathrm{d} x, \mathrm{~d} x^{\prime}\right) \tag{1.2}
\end{equation*}
$$

where $\gamma$ is any coupling of $\boldsymbol{\mu}^{X}$ and $\boldsymbol{\mu}^{X^{\prime}}$. Clearly, (1.2) contains (1.1) as a special case (take $X=X^{\prime}$ ), but the additional flexibility of introducing the auxiliary random variable $X^{\prime}$ turns (1.2) into a powerful tool applicable to many situations where (1.1) alone would not suffice. Briefly, we modify the "history" of the process from $X \rightarrow Y$ to $X^{\prime} \rightarrow Y$, at a price encapsulated in the second term on the right-hand side of (1.2). We refer to (1.2) (and its generalization to other divergences) as the shifted composition rule. See Theorem 3.1 for the formal statement.

This series of papers investigates the shifted composition rule and its applications. In this first work, we focus on the application of this principle to deriving sharp Harnack inequalities and reverse transport inequalities. To describe these results, we first provide some context.

To fix ideas, let $V: \mathbb{R}^{d} \rightarrow \mathbb{R}$ be a smooth function and consider the Langevin diffusion with potential $V$, namely, the solution to the Itô stochastic differential equation (SDE)

$$
\begin{equation*}
\mathrm{d} X_{t}=-\nabla V\left(X_{t}\right) \mathrm{d} t+\sqrt{2} \mathrm{~d} B_{t} \tag{1.3}
\end{equation*}
$$

where $\left(B_{t}\right)_{t \geqslant 0}$ is a standard Brownian motion on $\mathbb{R}^{d}$. To study diffusion processes such as (1.3), one usually introduces the corresponding Markov semigroup $\left(P_{t}\right)_{t \geqslant 0}$, which maps any (bounded) function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ to $P_{t} f$ defined by $P_{t} f(x):=\mathbb{E}\left[f\left(X_{t}\right) \mid X_{0}=x\right]$. The analytic properties of the
diffusion (1.3) (e.g., its regularizing effect) are then encoded as inequalities for the semigroup. We refer to the monograph [BGL14] for a comprehensive account.

We will be particularly interested in the dimension-free Harnack inequality introduced in [Wan97]. In the context of (1.3), this result reads as follows: suppose that $\nabla^{2} V \geq \alpha I$ on $\mathbb{R}^{d}$, for some $\alpha \in \mathbb{R}$; then, for any bounded non-negative function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$, and any $p>1$,

$$
\begin{equation*}
\left(P_{t} f(x)\right)^{p} \leqslant P_{t}\left(f^{p}\right)(y) \exp \left(\frac{\alpha p\|x-y\|^{2}}{2(p-1)(\exp (2 \alpha t)-1)}\right), \quad \forall x, y \in \mathbb{R}^{d}, t>0 \tag{1.4}
\end{equation*}
$$

By replacing the Euclidean metric with an intrinsic metric, (1.4) holds more generally for Markov diffusions on Riemannian manifolds which satisfy the curvature-dimension condition $\operatorname{CD}(\alpha, \infty)$, which reduces to $\nabla^{2} V \geq \alpha I$ for (1.3). In fact, as observed in [Wan10], (1.4) is equivalent to $\mathrm{CD}(\alpha, \infty)$, and moreover to the reverse transport inequality

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P_{t} \| \delta_{y} P_{t}\right) \leqslant \frac{\alpha q\|x-y\|^{2}}{2(\exp (2 \alpha t)-1)}, \quad \forall x, y \in \mathbb{R}^{d}, t>0 \tag{1.5}
\end{equation*}
$$

where $q:=\frac{p}{p-1}$ is the Hölder conjugate to $p$ and $\mathrm{R}_{q}$ is the Rényi divergence of order $q$ (see $\S 2$ ). We defer a more thorough discussion of the literature, including these equivalences, to $\S 6.1$.

In [Wan97], the Harnack inequality (1.4) was established using semigroup calculations based on the $\mathrm{CD}(\alpha, \infty)$ condition (or more precisely, based on certain gradient commutation bounds which are equivalent to $\mathrm{CD}(\alpha, \infty))$. Then, in [ATW06], M. Arnaudon, A. Thalmaier, and F.-Y. Wang introduced a coupling argument, which together with the Girsanov transformation, provides an alternative means of establishing inequalities such as (1.4). The latter approach has been used to systematically study SDEs on Riemannian manifolds, SDEs with multiplicative noise, SDEs with irregular coefficients, distribution-dependent SDEs, SPDEs, jump processes, and SDEs driven by fractional Brownian motion; we give citations to this extensive literature and revisit the coupling approach in $\S 4.1$.

The formulation (1.5), however, is formulated purely in terms of information-theoretic quantities, which naturally raises the question of obtaining a proof of (1.5) by means of an information-theoretic principle. This is the starting point which motivates the present work. Indeed, as we show in $\S 3$, the shifted composition rule can be used to recover (1.4) and (1.5) via elementary discrete-time arguments. Moreover, through the information-theoretic lens, we unify, clarify, and refine concepts from distinct fields (namely, the shifted divergence technique from differential privacy $[$ Fel +18$]$ and the coupling argument of [ATW06]) and obtain new Harnack inequalities. We now summarize the main contributions of our work.

Contributions and organization. In $\S 2$, we begin by reviewing the information-theoretic concepts that we employ, as well as their key properties.

In $\S 3$, we develop the discrete-time arguments which form the core technical innovation of our work. We begin in $\S 3.1$ by formally stating and proving the shifted composition rule (Theorem 3.1). Then, in $\S 3.2$, we apply the shifted composition rule to prove sharp Rényi reverse transport inequalities (a.k.a. Rényi regularity bounds), of the form

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P^{N} \| \delta_{y} P^{N}\right) \leqslant C\|x-y\|^{2} \tag{1.6}
\end{equation*}
$$

for discrete-time Markov kernels $P$ on $\mathbb{R}^{d}$ under the following two assumptions: (1) $P$ satisfies a one-step regularity bound $\mathrm{R}_{q}\left(\delta_{x} P \| \delta_{y} P\right) \leqslant c\|x-y\|^{2}$, and (2) $P$ is Lipschitz in the $W_{\infty}$ metric. Note that for our applications of interest, in which $P$ is taken to be a time-discretization of an SDE, the
one-step regularity bound is typically easy to check since $P$ admits an explicit, Gaussian transition density. For our result, given as Theorem 3.2, it is critical that we prove sharp bounds in order to obtain non-trivial results for the continuous-time diffusion as we let the discretization time step tend to zero, as well as to recover the aforementioned sharp equivalences with the $\operatorname{CD}(\alpha, \infty)$ condition (see Remark 3.3).

Our sharp regularity bounds for discrete-time Markov processes are obtained through the introduction of auxiliary "shifted" processes and appealing to the shifted composition rule. In fact, we identify two separate auxiliary processes which suffice for this purpose: one based on synchronous coupling, and one based on coupling via optimal mass transport. In turn, they lead to two different continuous-time arguments based, respectively, on stochastic calculus and optimal transport.

Next, in $\S 3.3$, we show that regularity bounds (1.6) which hold for Dirac initializations $\delta_{x}, \delta_{y}$ can be upgraded, in a black-box manner, to regularity bounds of the form

$$
\mathrm{R}_{q}\left(\mu P^{N} \| \nu P^{N}\right) \leqslant C W^{2}(\mu, \nu)
$$

that hold from arbitrary initializations $\mu, \nu$, and replace the the quadratic cost $\|x-y\|^{2}$ on the righthand side of the regularity bound by a suitable coupling cost $W^{2}(\mu, \nu)$ between $\mu$ and $\nu$ (Theorem 3.7). Although the argument is straightforward, based on the joint convexity of information divergences and a coupling argument, we have not found a self-contained statement of this principle in the literature. Taken together with the result of $\S 3.2$, this yields a general reduction in which, for $W_{\infty}$-Lipschitz kernels $P$, sharp multi-step regularity bounds from general initializations follow from one-step regularity bounds from Dirac initializations.

In $\S 3.4$ we illustrate our results for the Langevin SDE (1.3). In this context, our arguments closely resemble the shifted divergence technique from the field of differential privacy $[\mathrm{Fel}+18]$, and in particular the modified version [AT22] which was recently used to established discrete mixing bounds [AC23; AT23]. We discuss these connections further in §3.5. In brief, our framework can be viewed as a generalization, refinement, and interpretation of the shifted divergence method. It is a generalization since we have identified the fundamental information-theoretic principle - namely, the shifted composition rule - underlying the method, which allows for more general notions of shifts than Gaussian convolutions; it is a refinement due to the convexity principle of $\S 3.3$, which improves both quantitatively and qualitatively over prior results in the literature (e.g., [AC23; AT23]); and it provides meaningful interpretations through the construction of explicit shifted processes, as well as by connecting it to the corresponding continuous-time arguments in $\S 4$. The freedom to choose more general "shifts" will be exploited in further works in this series.

Next, $\S 4$ develops the corresponding arguments in continuous time; in particular, the two choices for the auxiliary process lead to conceptually distinct proofs. In $\S 4.1$ we show that the continuoustime analogue of the synchronous coupling proof coincides with the aforementioned "coupling by parallel translation" introduced by [ATW06]. Hence the arguments of $\S 3$ can be viewed as a way to extend the method of [ATW06], based on Girsanov's Theorem, to discrete-time Markov processes. On the other hand, the continuous-time analogue of the Wasserstein coupling proof, given in §4.2, relies on calculations in the spirit of Otto calculus [Ott01] (c.f. [JKO98; AGS08; Vil09]) and appears to be new. This argument bypasses the need for Girsanov transformations. We also discuss links with the Föllmer process [Föl85] and the "JKO" (or minimizing movements) scheme, which may be conceptually useful.

In $\S 5$, we explore further extensions of our results, starting with a word on the Riemannian setting in $\S 5.1$ and proceeding to general Itô SDEs on $\mathbb{R}^{d}$ with multiplicative noise in §5.2. The latter setting was first considered by F.-Y. Wang in [Wan11b], and we show how to recover his sharp log-Harnack inequality via discrete-time arguments. Then, in $\S 5.3$, we consider the Markov
kernel induced by $N$-fold convolution with a regular density $\rho$ under the central limit scaling and we obtain a regularity bound depending on the Fisher information matrix for $\rho$. We conjecture that the Fisher information matrix can be replaced by the inverse covariance matrix, which would be sharp. We stress that continuous-time coupling arguments do not apply to the study of these discrete-time processes.

Finally, in $\S 6$, we discuss applications of our results to the study of Harnack inequalities. We start with background in $\S 6.1$ on the equivalence between $\operatorname{CD}(\alpha, \infty)$, Harnack inequalities, and reverse transport inequalities; in particular, we emphasize the duality between the latter two in §6.2. Hence, our regularity bounds/reverse transport inequalities immediately furnish Harnack inequalities, in particular for discrete-time processes (see $\$ 6.4$ ).

In $\S 6.3$, we show that dualizing our regularity bounds for Rényi parameters $q \in(0,1)$ yields a family of reverse Harnack inequalities which correspond to exponents $p \in(-\infty, 0)$. These inequalities have not previously appeared in the literature, and we prove that they are also equivalent to the curvature-dimension condition $\mathrm{C}(\alpha, \infty)$.

For brevity, we defer some calculations and proofs to the appendices $\S \mathrm{A}$ and $\S \mathrm{B}$.

## 2 Information-theoretic preliminaries

Here we briefly recall the definitions and basic properties of the information divergences employed in this paper.

Definition 2.1 (Rényi divergence). Let $q \in(0, \infty]$. The Rényi divergence of order $q$ between probability measures $\mu, \nu$ is defined to be

$$
\begin{equation*}
\mathrm{R}_{q}(\mu \| \nu):=\frac{1}{q-1} \log \int\left(\frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right)^{q} \mathrm{~d} \nu \tag{2.1}
\end{equation*}
$$

For $q=1$, this is known as the Kullback-Leibler (KL) divergence and we interpret (2.1) in the limiting sense,

$$
\mathrm{KL}(\mu \| \nu):=\mathrm{R}_{1}(\mu \| \nu):=\int\left(\frac{\mathrm{d} \mu}{\mathrm{d} \nu} \log \frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right) \mathrm{d} \nu
$$

For $q=\infty$, we again interpret (2.1) in the limiting sense,

$$
\mathrm{R}_{\infty}(\mu \| \nu):=\log \left\|\frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right\|_{L^{\infty}(\nu)}
$$

If $\mu \ll k \nu$, then $\mathrm{R}_{q}(\mu \| \nu)$ is defined to be $+\infty$ for $q>1$, and $\mathrm{R}_{q}(\mu \| \nu):=\frac{1}{q-1} \log \int\left(\frac{\mathrm{d} \mu}{\mathrm{d} \lambda}\right)^{q}\left(\frac{\mathrm{d} \nu}{\mathrm{d} \lambda}\right)^{1-q} \mathrm{~d} \lambda$ for $q<1$, where $\lambda$ is a common dominating measure for $\mu$ and $\nu$ (e.g., $\lambda=\mu+\nu$ ).

Another special case worth remarking is $q=2$, in which case the Rényi divergence is related to the chi-squared divergence

$$
\chi^{2}(\mu \| \nu):=\operatorname{var}_{\nu} \frac{\mathrm{d} \mu}{\mathrm{d} \nu}=\int\left(\frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right)^{2} \mathrm{~d} \nu-1
$$

via the expression $\mathrm{R}_{2}(\mu \| \nu)=\exp \left(1+\chi^{2}(\mu \| \nu)\right)$.

For later convenience, we define $\mathbf{D}_{q}$ for $q \neq 1$ to be the $f$-divergence corresponding to

$$
f_{q}(x):= \begin{cases}x^{q}-1, & q>1 \\ 1-x^{q}, & q<1\end{cases}
$$

In other words, we set

$$
\mathrm{D}_{q}(\mu \| \nu):=\int f_{q}\left(\frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right) \mathrm{d} \nu
$$

Note that $f_{q}$ is convex with $f_{q}(1)=0$. We have the relationships

$$
\mathrm{R}_{q}(\mu \| \nu)=\frac{1}{q-1} \begin{cases}\log \left(1+\mathrm{D}_{q}(\mu \| \nu)\right), & q>1  \tag{2.2}\\ \log \left(1-\mathrm{D}_{q}(\mu \| \nu)\right), & q<1\end{cases}
$$

We also summarize a number of standard properties of Rényi divergences that we use repeatedly throughout the paper. Proofs and further discussion of these properties can be found, e.g., in the surveys [VH14; Mir17]. Since we provide a slightly modified restatement of the Rényi composition rule that is helpful for our development, we provide a proof in $\S$ A. 1 for completeness.

Theorem 2.2. Let $q \in(0, \infty]$ and let $\mu, \nu$ be probability measures.

1. (Positivity) $\mathrm{R}_{q}(\mu \| \nu) \geqslant 0$, with equality if and only if $\mu=\nu$.
2. (Monotonicity) Rényi divergences are increasing in the order, i.e., $q \mapsto \mathrm{R}_{q}(\mu \| \nu)$ is increasing.
3. (Data processing inequality) For any Markov kernel $P$, it holds that $\mathrm{R}_{q}(\mu P \| \nu P) \leqslant \mathrm{R}_{q}(\mu \| \nu)$.
4. (KL chain rule) Using the notation introduced in $\S 1$,

$$
\mathrm{KL}\left(\boldsymbol{\mu}^{X, Y} \| \boldsymbol{\nu}^{X, Y}\right)=\mathrm{KL}\left(\boldsymbol{\mu}^{X} \| \boldsymbol{\nu}^{X}\right)+\int \mathrm{KL}\left(\boldsymbol{\mu}^{Y \mid X=x} \| \boldsymbol{\nu}^{Y \mid X=x}\right) \boldsymbol{\mu}^{X}(\mathrm{~d} x)
$$

5. (Rényi composition rule) For $q \in(0,1)$,

$$
\mathrm{R}_{q}\left(\boldsymbol{\mu}^{X, Y} \| \boldsymbol{\nu}^{X, Y}\right) \leqslant \mathrm{R}_{q}\left(\boldsymbol{\mu}^{X} \| \boldsymbol{\nu}^{X}\right)+\left(\boldsymbol{\mu}^{X} \wedge \boldsymbol{\nu}^{X}\right)-\operatorname{ess} \sup \left[\mathrm{R}_{q}\left(\boldsymbol{\mu}^{Y \mid X=\bullet} \| \boldsymbol{\nu}^{Y \mid X=\boldsymbol{\bullet}}\right)\right] .
$$

For $q \geqslant 1$,

$$
\begin{equation*}
\mathbf{R}_{q}\left(\boldsymbol{\mu}^{X, Y} \| \boldsymbol{\nu}^{X, Y}\right) \leqslant \mathrm{R}_{q}\left(\boldsymbol{\mu}^{X} \| \boldsymbol{\nu}^{X}\right)+\boldsymbol{\mu}^{X}-\operatorname{ess} \sup \left[\mathbf{R}_{q}\left(\boldsymbol{\mu}^{Y \mid X=\bullet} \| \boldsymbol{\nu}^{Y \mid X=\bullet}\right)\right] \tag{2.3}
\end{equation*}
$$

6. (Convexity) The divergences $\mathrm{D}_{q}($ for $q \neq 1)$ and $\mathrm{R}_{q}($ for $q \leqslant 1$ ) are jointly convex. Consequently, since $\mathrm{R}_{q}$ is an increasing transformation of $\mathrm{D}_{q}$ for $q>1$, it follows that $\mathrm{R}_{q}$ is jointly quasi-convex for the entire range $q>0$.
7. (Gaussian identity) $\mathrm{R}_{q}\left(\mathcal{N}\left(x, \sigma^{2} I\right) \| \mathcal{N}\left(y, \sigma^{2} I\right)\right)=\frac{q\|x-y\|^{2}}{2 \sigma^{2}}$.

Remark 2.3. In the composition rule (2.3), it is important that the essential supremum on the RHS is taken w.r.t. $\boldsymbol{\mu}^{X}$. Indeed, for (2.3), we may assume that $\boldsymbol{\mu}^{X} \ll \boldsymbol{\nu}^{X}$ or else the bound is trivial. The conditional distribution $\boldsymbol{\nu}^{Y \mid X=\bullet}$ is defined $\boldsymbol{\nu}^{X}$-a.e., hence $\boldsymbol{\mu}^{X}$-a.e., and the expression on the RHS of (2.3) therefore makes sense. On the other hand, $\boldsymbol{\mu}^{Y \mid X=\bullet}$ may not be defined $\boldsymbol{\nu}^{X}$-a.e.

The composition rule is key to our work as it enables proving the shifted composition rule in §3.1. As such, we focus on the family of Rényi divergences rather than other $f$-divergences.

## 3 Discrete-time arguments

### 3.1 Shifted composition rule

The namesake of this paper (and the forthcoming series) is the following shifted composition rule. Write $\mathscr{C}(\mu, \nu)$ for the set of couplings of two probability measures $\mu \in \mathcal{P}\left(\Omega_{1}\right), \nu \in \mathcal{P}\left(\Omega_{2}\right)$, i.e., the set of probability measures $\gamma \in \mathcal{P}\left(\Omega_{1} \times \Omega_{2}\right)$ whose marginals are $\mu$ and $\nu$ respectively.

Theorem 3.1 (Shifted composition rule). Let $X, X^{\prime}, Y$ be three jointly defined random variables on a standard probability space $\Omega$. Let $\boldsymbol{\mu}, \boldsymbol{\nu}$ be two probability measures over $\Omega$, with superscripts denoting the laws of random variables under these measures.

1. (Shifted chain rule) It holds that

$$
\mathrm{KL}\left(\boldsymbol{\mu}^{Y} \| \boldsymbol{\nu}^{Y}\right) \leqslant \mathrm{KL}\left(\boldsymbol{\mu}^{X^{\prime}} \| \boldsymbol{\nu}^{X}\right)+\inf _{\gamma \in \mathscr{E}\left(\boldsymbol{\mu}^{X}, \boldsymbol{\mu}^{X^{\prime}}\right)} \int \mathrm{KL}\left(\boldsymbol{\mu}^{Y \mid X=x} \| \boldsymbol{\nu}^{Y \mid X=x^{\prime}}\right) \gamma\left(\mathrm{d} x, \mathrm{~d} x^{\prime}\right) .
$$

2. Let $q \in(0, \infty]$. If $q \in(0,1)$, assume in addition that $\boldsymbol{\mu}^{X^{\prime}} \ll \boldsymbol{\nu}^{X}$. Then, it holds that

![](https://cdn.mathpix.com/cropped/2024_05_26_4ffb174681cea29d40d3g-07.jpg?height=99&width=1240&top_left_y=1032&top_left_x=486)

Proof. First, note that statement we wish to prove only depends on the laws $\boldsymbol{\mu}^{X, Y}, \boldsymbol{\nu}^{X, Y}$, and $\boldsymbol{\mu}^{X^{\prime}}$, and hence we are free to choose the coupling between $X^{\prime}$ and $(X, Y)$. Given any coupling $\gamma \in \mathscr{C}\left(\boldsymbol{\mu}^{X}, \boldsymbol{\mu}^{X^{\prime}}\right)$, we can jointly define $\left(X, X^{\prime}, Y\right)$ such that $\boldsymbol{\mu}^{X, X^{\prime}}=\gamma$ using the gluing lemma (see [Vil03, Lemma 7.6]), i.e., we set $\boldsymbol{\mu}^{X, X^{\prime}, Y}\left(\mathrm{~d} x, \mathrm{~d} x^{\prime}, \mathrm{d} y\right)=\boldsymbol{\mu}^{X, Y}(\mathrm{~d} x, \mathrm{~d} y) \gamma^{2 \mid 1}\left(\mathrm{~d} x^{\prime} \mid x\right)$, where $\gamma^{2 \mid 1}$ denotes the disintegration of $\gamma$ along the first coordinate. Note that with this choice, under $\boldsymbol{\mu}, X^{\prime}$ and $Y$ are conditionally independent given $X$, i.e., $X^{\prime} \rightarrow X \rightarrow Y$ form a $\boldsymbol{\mu}$-Markov chain.

By the data processing inequality and the KL chain rule or the Rényi composition rule respectively, and using $\boldsymbol{\mu}^{X^{\prime}} \ll \boldsymbol{\nu}^{X}$ to write $\left(\boldsymbol{\mu}^{X^{\prime}} \wedge \boldsymbol{\nu}^{X}\right)$-ess sup $=\boldsymbol{\mu}^{X^{\prime}}$-ess sup,

$$
\begin{aligned}
& \mathrm{KL}\left(\boldsymbol{\mu}^{Y} \| \boldsymbol{\nu}^{Y}\right) \leqslant \mathrm{KL}\left(\boldsymbol{\mu}^{X^{\prime}, Y} \| \boldsymbol{\nu}^{X, Y}\right)=\mathrm{KL}\left(\boldsymbol{\mu}^{X^{\prime}} \| \boldsymbol{\nu}^{X}\right)+\int \mathrm{KL}\left(\boldsymbol{\mu}^{Y \mid X^{\prime}=x^{\prime}} \| \boldsymbol{\nu}^{Y \mid X=x^{\prime}}\right) \boldsymbol{\mu}^{X^{\prime}}\left(\mathrm{d} x^{\prime}\right) \\
& \mathrm{R}_{q}\left(\boldsymbol{\mu}^{Y} \| \boldsymbol{\nu}^{Y}\right) \leqslant \mathrm{R}_{q}\left(\boldsymbol{\mu}^{X^{\prime}, Y} \| \boldsymbol{\nu}^{X, Y}\right) \leqslant \mathrm{R}_{q}\left(\boldsymbol{\mu}^{X^{\prime}} \| \boldsymbol{\nu}^{X}\right)+\boldsymbol{\mu}^{X^{\prime}}-{ }_{x^{\prime} \in \Omega}-\operatorname{sep} \sup \mathrm{R}_{q}\left(\boldsymbol{\mu}^{Y \mid X^{\prime}=x^{\prime}} \| \boldsymbol{\nu}^{Y \mid X=x^{\prime}}\right)
\end{aligned}
$$

Next, by conditioning, we write

$$
\boldsymbol{\mu}^{Y \mid X^{\prime}=x^{\prime}}=\int \boldsymbol{\mu}^{Y \mid X=x, X^{\prime}=x^{\prime}} \boldsymbol{\mu}^{X \mid X^{\prime}}\left(\mathrm{d} x \mid x^{\prime}\right)=\int \boldsymbol{\mu}^{Y \mid X=x} \boldsymbol{\mu}^{X \mid X^{\prime}}\left(\mathrm{d} x \mid x^{\prime}\right)
$$

where we used the fact that under $\boldsymbol{\mu}, X^{\prime}$ and $Y$ are conditionally independent given $X$. Using the convexity of the KL divergence and the quasi-convexity of the Rényi divergence,

$$
\begin{aligned}
\int \mathrm{KL}\left(\boldsymbol{\mu}^{Y \mid X^{\prime}=x^{\prime}} \| \boldsymbol{\nu}^{Y \mid X=x^{\prime}}\right) \boldsymbol{\mu}^{X^{\prime}}\left(\mathrm{d} x^{\prime}\right) & \leqslant \int \mathrm{KL}\left(\boldsymbol{\mu}^{Y \mid X=x} \| \boldsymbol{\nu}^{Y \mid X=x^{\prime}}\right) \boldsymbol{\mu}^{X \mid X^{\prime}}\left(\mathrm{d} x \mid x^{\prime}\right) \boldsymbol{\mu}^{X^{\prime}}\left(\mathrm{d} x^{\prime}\right) \\
& =\int \mathrm{KL}\left(\boldsymbol{\mu}^{Y \mid X=x} \| \boldsymbol{\nu}^{Y \mid X=x^{\prime}}\right) \boldsymbol{\mu}^{X, X^{\prime}}\left(\mathrm{d} x, \mathrm{~d} x^{\prime}\right)
\end{aligned}
$$

and

![](https://cdn.mathpix.com/cropped/2024_05_26_4ffb174681cea29d40d3g-07.jpg?height=112&width=1223&top_left_y=2324&top_left_x=451)

The conclusion follows because $\boldsymbol{\mu}^{X, X^{\prime}}=\gamma \in \mathscr{C}\left(\boldsymbol{\mu}^{X}, \boldsymbol{\mu}^{X^{\prime}}\right)$ was arbitrary.

For $q \geqslant 1$, if we take $\boldsymbol{\mu}^{X}=\boldsymbol{\mu}^{X^{\prime}}$ and we take $\gamma$ to be the trivial coupling $\gamma\left(\mathrm{d} x, \mathrm{~d} x^{\prime}\right)=$ $\boldsymbol{\mu}^{X}(\mathrm{~d} x) \delta_{x}\left(\mathrm{~d} x^{\prime}\right)$, then the shifted composition rule reduces back to the KL chain rule or the Rényi composition rule respectively. However, the added flexibility of introducing the auxiliary random variable $X^{\prime}$ allows the shifted composition rule to tackle a variety of new applications, some of which will be explored in future work. In this paper, we illustrate the use of this principle for proving Harnack and reverse transport inequalities, as discussed in $\S 1$.

### 3.2 One-step to multi-step bounds

We now turn toward the main application of the shifted composition rule considered in the present paper, namely, the derivation of reverse transport inequalities of the form $\mathrm{R}_{q}\left(\delta_{x} P^{n} \| \delta_{y} P^{n}\right) \lesssim\|x-y\|^{2}$, where $P$ is a Markov kernel on $\mathbb{R}^{d}$ and $0<q \leqslant \infty$. We also refer to such inequalities as regularity bounds since they encode regularizing properties of the Markov kernel $P$, see $\S 6.1$ for further discussion. Our result below shows that if the Markov kernel $P$ is Lipschitz w.r.t. the Wasserstein metric, then an optimal multi-step regularity bound is implied by a one-step regularity bound $\mathrm{R}_{q}\left(\delta_{x} P \| \delta_{y} P\right) \lesssim\|x-y\|^{2}$, which is typically much easier to establish. In the next section, we will then show how to upgrade the regularity bounds to hold for arbitrary initializations $\mu, \nu \in \mathcal{P}\left(\mathbb{R}^{d}\right)$ in a black-box manner.

Theorem 3.2. Let $0<q \leqslant \infty$. Suppose that $P$ is a Markov kernel on $\mathbb{R}^{d}$ satisfying the two following conditions.

(a) $P$ satisfies a 1-step regularity bound for Dirac initializations; i.e., there exists $c>0$ such that

$$
\begin{equation*}
\mathbf{R}_{q}\left(\delta_{x} P \| \delta_{y} P\right) \leqslant c\|x-y\|^{2} \quad \forall x, y \in \mathbb{R}^{d} \tag{3.1}
\end{equation*}
$$

If $q<1$, we assume for technical reasons that (3.1) also holds for $q=1$, possibly with some other constant $c^{\prime}<\infty$.

(b) $P$ is Wasserstein-Lipschitz; i.e., there exists $L>0$ such that

$$
W_{\infty}(\mu P, \nu P) \leqslant L W_{\infty}(\mu, \nu), \quad \forall \mu, \nu \in \mathcal{P}\left(\mathbb{R}^{d}\right)
$$

Then, for all $x, y \in \mathbb{R}^{d}$,

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P^{N} \| \delta_{y} P^{N}\right) \leqslant c \frac{L^{-2}-1}{L^{-2 N}-1}\|x-y\|^{2} \tag{3.2}
\end{equation*}
$$

Remark 3.3 (Optimality of the bound). The multi-step bound (3.2) is optimal in the absence of further assumptions on $P$ (see §3.4). We remark that while it is trivial to prove the weaker bound $\mathrm{R}_{q}\left(\delta_{x} P^{N} \| \delta_{y} P^{N}\right) \leqslant c L^{2 N-2}\|x-y\|^{2}$ by applying Assumption (a) for one step and (b) for the remaining steps, that naive bound is weaker to the point of being vacuous ${ }^{1}$ for the applications we have in mind, namely time discretizations of diffusions with small step size parameter $h>0$.

Remark 3.4 ( $W_{\infty}$-Lipschitz assumption). For simplicity, we state Theorem 3.2 under the assumption that $P$ is Lipschitz in the $W_{\infty}$ distance. This enables covering all of the Rényi divergences using the same proof. However, for the $K L$ divergence ( $q=1$ ), the $W_{\infty}$-Lipschitz assumption can be relaxed to a $W_{2}$-Lipschitz assumption, which is strictly weaker. This follows by replacing occurrences of $W_{\infty}$ with $W_{2}$ in the relevant parts of the proof, namely in (3.5) and (3.7) (using the shifted chain rule specific to the $K L$ divergence).[^0]

Figure 1: Both the synchronous coupling and Wasserstein coupling approaches produce an auxiliary stochastic process $\left\{\mu_{n}^{\prime}\right\}_{n=0}^{N}$ that interpolates between $\left\{\mu_{n}\right\}_{n=0}^{N}$ and $\left\{\nu_{n}\right\}_{n=0}^{N}$ in the sense that $\mu_{0}^{\prime}=\nu$ and $\nu_{N}^{\prime}=\nu_{N}$.

![](https://cdn.mathpix.com/cropped/2024_05_26_4ffb174681cea29d40d3g-09.jpg?height=353&width=644&top_left_y=363&top_left_x=727)

Remark 3.5 (Verifying Wasserstein-Lipschitzness). In order to verify the assumption that $P$ is $W_{2}$ - or $W_{\infty}$-Lipschitz, it suffices to check this condition when $\mu, \nu$ are Dirac measures. This follows from an elementary coupling argument, see e.g., [Che+22, §A.2] for the proof in the $W_{2}$ case.

The key ingredient in our proof is the introduction of a shifted interpolated process. Since this is a central element of our analysis-for the discrete-time arguments in this section as well as for the continuous-time arguments in $\S 4$-we isolate this idea before the proof. We construct an auxiliary process $\left\{\mu_{n}^{\prime}\right\}_{n=0}^{N}$ that interpolates between the processes $\left\{\mu_{n}:=\delta_{x} P^{n}\right\}_{n=0}^{N}$ and $\left\{\nu_{n}:=\delta_{y} P^{n}\right\}_{n=0}^{N}$, in the sense that it matches one at initialization and matches the other at termination:

$$
\begin{equation*}
\mu_{0}^{\prime}=\nu_{0} \quad \text { and } \quad \mu_{N}^{\prime}=\mu_{N} \tag{3.3}
\end{equation*}
$$

See Figure 1. Since $\mu_{N}^{\prime}=\mu_{N}$, it obviously holds that the left hand side of the regularity bound (3.2) is equal to $\mathrm{R}_{q}\left(\mu_{N}^{\prime} \| \nu_{N}\right)=\mathrm{R}_{q}\left(\mu_{N} \| \nu_{N}\right)$. The insight behind this construction is that rather than using the composition rule to bound the divergence between the original processes $\left\{\mu_{n}\right\}_{n=0}^{N}$ and $\left\{\nu_{n}\right\}_{n=0}^{N}$, it is more efficient to bound the divergence between the auxiliary process $\left\{\mu_{n}^{\prime}\right\}_{n=0}^{N}$ and $\left\{\nu_{n}\right\}_{n=0}^{N}$ using the shifted composition rule. (In fact, the divergence between the original processes is infinite since $\mu_{0}=\delta_{x}$ and $\nu_{0}=\delta_{y}$ are singular w.r.t. each other.)

There are two constructions of $\left\{\mu_{n}^{\prime}\right\}_{n=0}^{N}$ that suffice for our purpose. For both, we set

$$
\mu_{n}^{\prime}:=\operatorname{law}\left(X_{n}^{\prime}\right), \quad n=0,1, \ldots, N
$$

for a stochastic process $\left\{X_{n}^{\prime}\right\}_{n=0}^{N}$ to be defined below. The interest in considering these two shifted interpolated processes is that they lead to different generalizations in continuous time, as demonstrated in $\S 4$.

Synchronous coupling. Jointly define processes $\left\{X_{n}\right\}_{n=0}^{N},\left\{X_{n}^{\prime}\right\}_{n=0}^{N}$ such that $X_{n} \sim \mu_{n}$ and $X_{n}^{\prime} \sim \mu_{n}^{\prime}$ for all $n$. We start with $X_{0}=x$ and $X_{0}^{\prime}=y$. Assuming that $\left(X_{n}, X_{n}^{\prime}\right)$ have been jointly defined, define $\left(X_{n+1}, X_{n+1}^{\prime}\right)$ as follows. Set

$$
\begin{equation*}
\tilde{X}_{n}:=X_{n}^{\prime}+\eta_{n}\left(X_{n}-X_{n}^{\prime}\right) \tag{3.4}
\end{equation*}
$$

for a scalar $\eta_{n} \geqslant 0$ to be chosen later. Then, conditional on $\left(X_{n}, X_{n}^{\prime}\right)$, draw $X_{n+1} \sim P\left(X_{n}, \cdot\right)$, $X_{n+1}^{\prime} \sim P\left(\tilde{X}_{n}, \cdot\right)$ so that $\left\|X_{n+1}-X_{n+1}^{\prime}\right\|_{L^{\infty}(\mathbb{P})}=W_{\infty}\left(P\left(X_{n}, \cdot\right), P\left(\tilde{X}_{n}, \cdot\right)\right)$

Wasserstein coupling. Assuming that $X_{n}^{\prime}$ has already been defined, let $X_{n} \sim \mu_{n}$ be optimally coupled with $X_{n}^{\prime}$ for the $W_{\infty}$ metric and define $\tilde{X}_{n}$ via (3.4). Then, conditional on $\tilde{X}_{n}$, draw $X_{n+1}^{\prime} \sim P\left(\tilde{X}_{n}, \cdot\right)$.

Remarks on the constructions. In both constructions, $\eta_{n}$ controls how much the process $X_{n}^{\prime}$ is corrected in the direction of $X_{n}$. This enables us to make progress in each iteration towards achieving the termination criterion of matching $\mu_{N}^{\prime}=\operatorname{law}\left(X_{N}^{\prime}\right)$ to $\mu_{N}=\operatorname{law}\left(X_{N}\right)$. In both settings, we take $\eta_{N-1}=1$ so that $\mu_{N}^{\prime}=\mu_{N}$, and we optimize the other shifting parameters $\eta_{0}, \ldots, \eta_{N-2}$ below to obtain the best possible final bound.

In order to prove a bound of the form (3.2), we make two key observations. First, the distance between $\mu_{n}$ and $\mu_{n}^{\prime}$ contracts in each iteration. Second, the divergence $\mathrm{R}_{q}\left(\mu_{n}^{\prime} \| \nu_{n}\right)$ can be controlled via the shifted composition rule.

Proof of Theorem 3.2. Distance bound for the auxiliary process. We give the argument for the synchronous shifted interpolation. Below, we work over an underlying probability space $(\Omega, \mathscr{F}, \mathbb{P})$. Via a coupling argument, almost surely,

$$
\left\|X_{n+1}-X_{n+1}^{\prime}\right\| \leqslant W_{\infty}\left(P\left(X_{n}, \cdot\right), P\left(\tilde{X}_{n}, \cdot\right)\right) \leqslant L\left\|X_{n}-\tilde{X}_{n}\right\|=L\left|1-\eta_{n}\right|\left\|X_{n}-X_{n}^{\prime}\right\|,
$$

hence

$$
\begin{equation*}
\left\|X_{n+1}-X_{n+1}^{\prime}\right\|_{L^{\infty}(\mathbb{P})} \leqslant L\left|1-\eta_{n}\right|\left\|X_{n}-X_{n}^{\prime}\right\|_{L^{\infty}(\mathbb{P})} \tag{3.5}
\end{equation*}
$$

Above, the second inequality is by the Lipschitz assumption (b) on the kernel $P$. By iterating this bound and recalling that $\mu_{0}^{\prime}=\delta_{y}$ by construction, we conclude that for all $n$,

$$
\begin{equation*}
\left\|X_{n}-X_{n}^{\prime}\right\|_{L^{\infty}(\mathbb{P})} \leqslant\left[L^{n} \prod_{k=0}^{n-1}\left|1-\eta_{k}\right|\right]\|x-y\| \tag{3.6}
\end{equation*}
$$

The distance bound for the Wasserstein interpolated process is similar, except that in (3.6) we replace the left-hand side with $W_{\infty}\left(\mu_{n}, \mu_{n}^{\prime}\right)$.

Divergence bound for the auxiliary process. The second key bound controls $\mathrm{R}_{q}$ between the auxiliary process $\mu_{n}^{\prime}$ and $\nu_{n}$. Again, we consider the synchronous shifted interpolation.

$$
\begin{align*}
\mathrm{R}_{q}\left(\mu_{n+1}^{\prime} \| \nu_{n+1}\right) & \leqslant \mathrm{R}_{q}\left(\mu_{n}^{\prime} \| \nu_{n}\right)+\left\|\mathrm{R}_{q}\left(P\left(\tilde{X}_{n}, \cdot\right) \| P\left(X_{n}^{\prime}, \cdot\right)\right)\right\|_{L^{\infty}(\mathbb{P})} \\
& \leqslant \mathrm{R}_{q}\left(\mu_{n}^{\prime} \| \nu_{n}\right)+c\left\|\tilde{X}_{n}-X_{n}^{\prime}\right\|_{L^{\infty}(\mathbb{P})}^{2} \\
& =\mathrm{R}_{q}\left(\mu_{n}^{\prime} \| \nu_{n}\right)+c \eta_{n}^{2}\left\|X_{n}-X_{n}^{\prime}\right\|_{L^{\infty}(\mathbb{P})}^{2} \tag{3.7}
\end{align*}
$$

Above, the first step is by an application of the shifted composition rule ${ }^{2}$ (Theorem 3.1) where $\boldsymbol{\mu}$ is the joint distribution under which $X \sim \tilde{\mu}_{n}, X^{\prime} \sim \mu_{n}^{\prime}$, and $Y \sim P(X, \cdot)$; and $\boldsymbol{\nu}$ is the joint distribution under which $X \sim \nu_{n}$ and $Y \sim P(X, \cdot)$. The second step is by the assumption (a). The final step is by construction of $\tilde{X}_{n}$. For the Wasserstein interpolated process, we replace the second term on the right-hand side of (3.7) with $c \eta_{n}^{2} W_{\infty}^{2}\left(\mu_{n}, \mu_{n}^{\prime}\right)$.

Optimizing the shifts. Combining the two bounds above yields

$$
\mathrm{R}_{q}\left(\mu_{N} \| \nu_{N}\right)=\mathrm{R}_{q}\left(\mu_{N}^{\prime} \| \nu_{N}\right) \leqslant\left[c \sum_{n=0}^{N-1} L^{2 n} \eta_{n}^{2} \prod_{k=0}^{n-1}\left(1-\eta_{k}\right)^{2}\right]\|x-y\|^{2}
$$

Recall that this bound holds for any values of the shifts $\eta_{0}, \ldots, \eta_{N-1}$ subject to the constraint $\eta_{N-1}=1$ (required to ensure the termination criterion $\mu_{N}^{\prime}=\mu_{N}$ ). Thus we may optimize the above bound over all such choices of $\eta$. This optimization problem is straightforward to solve in closed form, as detailed in §A.2.1. Plugging in the optimal value $\frac{L^{-2}-1}{L^{-2 N}-1}$ completes the proof.

We remark in passing that our analysis readily generalizes to non-stationary processes in which different Markov kernels are applied in each iteration.[^1]

### 3.3 Convexity principle

In the previous subsection, we proved regularity bounds for Markov chains initialized at Dirac distributions. Here, we reduce the problem of proving regularity bounds from arbitrary initializations to the case of Dirac initializations. The simple but key observation underlying this reduction is the following convexity principle.

Lemma 3.6 (Convexity principle). For any jointly convex function D , any Markov kernel $P$, and any distributions $\mu, \nu$,

$$
\begin{equation*}
\mathcal{D}(\mu P \| \nu P) \leqslant \inf _{\gamma \in \mathscr{C}(\mu, \nu)} \int \mathcal{D}\left(\delta_{x} P \| \delta_{y} P\right) \gamma(\mathrm{d} x, \mathrm{~d} y) \tag{3.8}
\end{equation*}
$$

Proof. Fix any coupling $\gamma \in \mathscr{C}(\mu, \nu)$. Decompose $\mu$ as the mixture distribution $\int \delta_{x} \gamma(\mathrm{d} x, \mathrm{~d} y)$, and similarly decompose $\nu=\int \delta_{y} \gamma(\mathrm{d} x, \mathrm{~d} y)$. Joint convexity then implies

$$
\mathcal{D}(\mu P \| \nu P)=\mathcal{D}\left(\int \delta_{x} P \gamma(\mathrm{d} x, \mathrm{~d} y) \| \int \delta_{y} P \gamma(\mathrm{d} x, \mathrm{~d} y)\right) \leqslant \int \mathcal{D}\left(\delta_{x} P \| \delta_{y} P\right) \gamma(\mathrm{d} x, \mathrm{~d} y)
$$

The claim follows since $\gamma$ is an arbitrary coupling.

We apply the convexity principle to the family of Rényi divergences by exploiting the basic fact from information theory that $f$-divergences are jointly convex (Theorem 2.2).

Theorem 3.7 (Application to Rényi divergences). Let $P$ be a Markov kernel on a Polish space $\mathcal{X}$ and let $\rho$ be a measurable function on $\mathcal{X} \times \mathcal{X}$.

1. If $\mathrm{KL}\left(\delta_{x} P \| \delta_{y} P\right) \leqslant \rho(x, y)$ for all $x, y \in \mathcal{X}$, then

$$
\begin{equation*}
\mathrm{KL}(\mu P \| \nu P) \leqslant \inf _{\gamma \in \mathscr{C}(\mu, \nu)} \int \rho(x, y) \gamma(\mathrm{d} x, \mathrm{~d} y) \quad \text { for all } \mu, \nu \in \mathcal{P}(\mathcal{X}) \tag{3.9}
\end{equation*}
$$

2. Let $q \in(0, \infty) \backslash\{1\}$. If $\mathrm{R}_{q}\left(\delta_{x} P \| \delta_{y} P\right) \leqslant \rho(x, y)$ for all $x, y \in \mathcal{X}$, then

$$
\begin{equation*}
\mathrm{R}_{q}(\mu P \| \nu P) \leqslant \inf _{\gamma \in \mathscr{C}(\mu, \nu)} \frac{1}{q-1} \log \int \exp ((q-1) \rho(x, y)) \gamma(\mathrm{d} x, \mathrm{~d} y) \tag{3.10}
\end{equation*}
$$

Proof. Since the KL divergence is an $f$-divergence and is therefore jointly convex, (3.9) directly follows from the convexity principle (Lemma 3.6). Next, for $q \in(0, \infty) \backslash\{1\}$, although $\mathrm{R}_{q}$ is not jointly convex [VH14, §III-B] (for $q>1$ ), it is an increasing transformation of a jointly convex $f$-divergence. Namely, define $g_{q}: \mathbb{R}_{+} \rightarrow \mathbb{R}_{+}$via

$$
g_{q}(s):=\frac{1}{q-1} \begin{cases}\log (1+s), & q>1 \\ \log (1-s), & q<1\end{cases}
$$

so that $\mathrm{R}_{q}=g_{q}\left(\mathrm{D}_{q}\right)$ (see (2.2)). Then, $g_{q}$ and $g_{q}^{-1}$ are increasing and $\mathrm{D}_{q}$ is jointly convex, it follows from Lemma 3.6 that

$$
\begin{aligned}
\mathrm{R}_{q}(\mu P \| \nu P)=g_{q}\left(\mathrm{D}_{q}(\mu P \| \nu P)\right) & \leqslant g_{q}\left(\inf _{\gamma \in \mathscr{C}(\mu, \nu)} \int \mathrm{D}_{q}\left(\delta_{x} P \| \delta_{y} P\right) \gamma(\mathrm{d} x, \mathrm{~d} y)\right) \\
& \leqslant \inf _{\gamma \in \mathscr{G}(\mu, \nu)} g_{q}\left(\int g_{q}^{-1}(\rho(x, y)) \gamma(\mathrm{d} x, \mathrm{~d} y)\right) .
\end{aligned}
$$

This concludes the proof of (3.10) by considering $q>1$ and $q<1$ separately.

Remark 3.8 (Optimal transport). These regularity bounds can be viewed as reverse transport inequalities since the convexity principle naturally extracts an optimal transport cost in the regularity bound. The precise cost function is dictated by the regularity bound from Dirac initializations. For the Langevin SDE, the corresponding optimal transport costs are the 2 -Wasserstein distance for $K L$ regularity, and a sub-Gaussian coupling cost for Rényi regularity (see Theorem 3.10 below). In the latter case, the coupling cost can also be related to the Orlicz-Wasserstein distance, c.f. [AC23].

Refinements. In the application of the convexity principle to Rényi divergences above, we first applied an increasing transformation $g$ before invoking joint convexity. The flexibility offered by such transformations sometimes leads to more refined bounds. In general, if we can write a divergence $\mathrm{D}$ as a function $\mathbf{D}=g\left(\mathbf{D}^{\prime}\right)$ of some other jointly convex divergence $\mathbf{D}^{\prime}$, where $g$ is strictly increasing and convex, then the bound obtained from applying the convexity principle to $\mathrm{D}^{\prime}$ is stronger, as a consequence of Jensen's inequality $g\left(\int g^{-1}(\cdots)\right) \leqslant \int(\cdots)$.

For example, in the case $q<1$, it is known that the Rényi divergence $\mathrm{R}_{q}$ is jointly convex [VH14, §III-B], and hence we could have applied the convexity principle to $\mathrm{R}_{q}$ directly. In the proof of Theorem 3.7 above, we instead applied the convexity principle to $\mathrm{D}_{q}$, where $\mathrm{R}_{q}=g_{q}\left(\mathrm{D}_{q}\right)$ and $g$ is strictly increasing and convex, which therefore yields a sharper bound.

The gamut of potential transformations expands when we consider convexity in the first or second argument alone which, as we show below, can be combined with a joint convexity inequality to obtain new bounds. In particular, we will apply this idea to $\mathrm{R}_{q}, q>1$, based on the following two convexity statements:

1. $\left(\mathrm{D}_{q}+1\right)^{1 / q}$ is convex in its first argument. Indeed, $\left(\mathrm{D}_{q}+1\right)^{1 / q}(\mu \| \nu)=\left\|\frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right\|_{L^{q}(\nu)}$ is convex w.r.t. $\mu$ due to the convexity of the $L^{q}(\nu)$ norm.
2. $\mathrm{R}_{q}$ is convex in its second argument $[\mathrm{VH} 14$, §III-B].

This leads to two refined bounds which involve weak optimal transport costs [Goz+17]; c.f. [BP22]. The effect of these refinements will be explored in the next section.

Theorem 3.9 (Refined Rényi bounds). Let $q>1$ and let $P$ be a Markov kernel on a Polish space $\mathcal{X}$. Let $\rho$ be a measurable function on $\mathcal{X} \times \mathcal{X}$ such that $\mathrm{R}_{q}\left(\delta_{x} P \| \delta_{y} P\right) \leqslant \rho(x, y)$ for all $x, y \in \mathcal{X}$. Then, the following two inequalities hold, where we write $\gamma_{1 \mid 2}$ for the conditional distribution of the first coordinate given the second under $\gamma$ and similarly for $\gamma_{2 \mid 1}$ :

$$
\begin{equation*}
\mathrm{R}_{q}(\mu P \| \nu P) \leqslant \inf _{\gamma \in \mathscr{G}(\mu, \nu)} \frac{1}{q-1} \log \int\left\{\int \exp \left(\frac{q-1}{q} \rho(x, y)\right) \gamma_{1 \mid 2}(\mathrm{~d} x \mid y)\right\}^{q} \nu(\mathrm{d} y) \tag{3.11}
\end{equation*}
$$

and

$$
\begin{equation*}
\mathrm{R}_{q}(\mu P \| \nu P) \leqslant \inf _{\gamma \in \mathscr{C}(\mu, \nu)} \frac{1}{q-1} \log \int \exp \left((q-1) \int \rho(x, y) \gamma_{2 \mid 1}(\mathrm{~d} y \mid x)\right) \mu(\mathrm{d} x) \tag{3.12}
\end{equation*}
$$

Proof. Let $\gamma \in \mathscr{C}(\mu, \nu)$. For the first inequality, we write $\mu P=\int \delta_{x} P \gamma_{1 \mid 2}(\mathrm{~d} x \mid y) \nu(\mathrm{d} y)$, so that

$$
\begin{aligned}
\left(\mathrm{D}_{q}+1\right)(\mu P \| \nu P) & \leqslant \int\left(\mathrm{D}_{q}+1\right)\left(\int \delta_{x} P \gamma_{1 \mid 2}(\mathrm{~d} x \mid y) \| \delta_{y} P\right) \nu(\mathrm{d} y) \\
& \leqslant \int\left\{\left(\mathrm{D}_{q}+1\right)^{1 / q}\left(\int \delta_{x} P \gamma_{1 \mid 2}(\mathrm{~d} x \mid y) \| \delta_{y} P\right)\right\}^{q} \nu(\mathrm{d} y) \\
& \leqslant \int\left\{\int\left(\mathrm{D}_{q}+1\right)^{1 / q}\left(\delta_{x} P \| \delta_{y} P\right) \gamma_{1 \mid 2}(\mathrm{~d} x \mid y)\right\}^{q} \nu(\mathrm{d} y)
\end{aligned}
$$

For the second inequality, we write $\nu P=\int \delta_{y} P \gamma_{2 \mid 1}(\mathrm{~d} y \mid x) \mu(\mathrm{d} x)$, so that

$$
\begin{aligned}
\left(\mathrm{D}_{q}+1\right)(\mu P \| \nu P) & \leqslant \int\left(\mathrm{D}_{q}+1\right)\left(\delta_{x} P \| \int \delta_{y} P \gamma_{2 \mid 1}(\mathrm{~d} y \mid x)\right) \mu(\mathrm{d} x) \\
& =\int \exp \left((q-1) \mathrm{R}_{q}\left(\delta_{x} P \| \int \delta_{y} P \gamma_{2 \mid 1}(\mathrm{~d} y \mid x)\right)\right) \mu(\mathrm{d} x) \\
& \leqslant \int \exp \left((q-1) \int \mathrm{R}_{q}\left(\delta_{x} P \| \delta_{y} P\right) \gamma_{2 \mid 1}(\mathrm{~d} y \mid x)\right) \mu(\mathrm{d} x)
\end{aligned}
$$

The inequalities in the theorem statement follow.

We provide dual versions of these arguments in $\S$ B.1.

### 3.4 Application to the Langevin diffusion

Here we illustrate how the techniques developed in $\S 3.1, \S 3.2, \S 3.3$ immediately yield tight regularity bounds for the Langevin SDE. We discuss tightness of the bounds in $\S$ A.3.

In what follows, let $\left(P_{t}\right)_{t \geqslant 0}$ denote the semigroup corresponding to the Langevin SDE

$$
\mathrm{d} X_{t}=-\nabla V\left(X_{t}\right) \mathrm{d} t+\sqrt{2} \mathrm{~d} B_{t}
$$

where $B$ is a standard Brownian motion. It is a classical fact that under minimal assumptions, the law of $X_{t}$ converges to $\pi \propto \exp (-V)$, see e.g., [BGL14] for background. Let $\hat{P}_{h}$ denote the Markov kernel corresponding to the discretized Langevin SDE with time step $h>0$, i.e., $\hat{P}_{h}(x, \cdot)=Q_{2 h}(x-h \nabla V(x), \cdot)$ where $\left(Q_{t}\right)_{t \geqslant 0}$ denotes the heat semigroup.

Theorem 3.10 (Discrete-time regularity of Langevin). Suppose that $\alpha I \leq \nabla^{2} V \leq \beta I$ on $\mathbb{R}^{d}$. Define the shorthand $L:=\max _{\lambda \in\{\alpha, \beta\}}|1-h \lambda|$. Then

$$
\mathrm{KL}\left(\mu \hat{P}_{h}^{N} \| \nu \hat{P}_{h}^{N}\right) \leqslant \frac{1-L^{2}}{4 h\left(L^{-2 N}-1\right)} W_{2}^{2}(\mu, \nu)
$$

and for any $q \in(0,1) \cup(1, \infty)$,

$$
\mathrm{R}_{q}\left(\mu \hat{P}_{h}^{N} \| \nu \hat{P}_{h}^{N}\right) \leqslant \inf _{\gamma \in \mathscr{C}(\mu, \nu)} \frac{1}{q-1} \log \int \exp \left(\frac{q(q-1)\left(1-L^{2}\right)}{4 h\left(L^{-2 N}-1\right)}\|x-y\|^{2}\right) \gamma(\mathrm{d} x, \mathrm{~d} y)
$$

Proof. It suffices to prove the discrete-time Rényi regularity bound between Dirac initializations:

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} \hat{P}_{h}^{N} \| \delta_{y} \hat{P}_{h}^{N}\right) \leqslant \frac{q\left(1-L^{2}\right)}{4 h\left(L^{-2 N}-1\right)}\|x-y\|^{2} \tag{3.13}
\end{equation*}
$$

Indeed, the claim for arbitrary initializations then follows by the convexity principle in Theorem 3.7.

To prove (3.13), we use the one-to-multi-step reduction for regularity bounds (Theorem 3.2). To this end, we use the elementary fact that $\phi(x):=x-h \nabla V(x)$ is L-Lipschitz. Since $\hat{P}_{h}(x, \cdot)=$ $Q_{2 h}(\phi(x), \cdot)$, it follows that

(a) $\hat{P}_{h}$ satisfies the 1-step regularity bound (3.1) with parameter $c:=\frac{q L^{2}}{4 h}$. This follows by the identity for the Rényi divergence between Gaussians (Proposition 2.2) and the Lipschitzness of the mapping $\phi$ :

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} \hat{P}_{h} \| \delta_{y} \hat{P}_{h}\right)=\frac{q\|\phi(x)-\phi(y)\|^{2}}{4 h} \leqslant \frac{q L^{2}\|x-y\|^{2}}{4 h} \tag{3.14}
\end{equation*}
$$

(b) $\hat{P}_{h}$ is $W_{\infty}$-Lipschitz with parameter $L$. This follows from a trivial coupling argument:

$$
\begin{equation*}
W_{\infty}\left(\mu \hat{P}_{h}, \nu \hat{P}_{h}\right) \leqslant W_{\infty}\left(\phi_{\#} \mu, \phi_{\#} \nu\right) \leqslant L W_{\infty}(\mu, \nu) \tag{3.15}
\end{equation*}
$$

Thus we may invoke Theorem 3.2. This proves the claim (3.13).

This tight regularity result for the discretized Langevin semigroup immediately implies the following tight regularity result for the (standard, continuous-time) Langevin semigroup by taking the limit as the step size $h \searrow 0$ and the total elapsed continuous time is fixed to $T=N h$.

Corollary 3.11 (Continuous-time regularity of Langevin). Suppose that $\alpha I \leq \nabla^{2} V$ on $\mathbb{R}^{d}$. Then

$$
\begin{equation*}
\mathrm{KL}\left(\mu P_{T} \| \nu P_{T}\right) \leqslant \frac{\alpha}{2(\exp (2 \alpha T)-1)} W_{2}^{2}(\mu, \nu) \tag{3.16}
\end{equation*}
$$

and for any $q \in(0,1) \cup(1, \infty)$,

$$
\begin{equation*}
\mathrm{R}_{q}\left(\mu P_{T} \| \nu P_{T}\right) \leqslant \inf _{\gamma \in \mathscr{C}(\mu, \nu)} \frac{1}{q-1} \log \int \exp \left(\frac{\alpha q(q-1)}{2(\exp (2 \alpha T)-1)}\|x-y\|^{2}\right) \gamma(\mathrm{d} x, \mathrm{~d} y) \tag{3.17}
\end{equation*}
$$

Our bounds hold for any value of $\alpha \in \mathbb{R}$, provided that the expressions are interpreted accordingly. Namely, for $\alpha=0$, the occurrences of $\alpha /(\exp (2 \alpha T)-1)$ in (3.16) and (3.17) simplify to $1 /(2 T)$. Note that for our discrete-time results, we also need the upper bound $\nabla^{2} V \leq \beta I$ (which is standard for discretization analysis), but the dependence on $\beta$ vanishes as $h \searrow 0$.

We conclude this discussion with a few remarks.

Remark 3.12 (Relationship with [AT23]). Theorem 3.10 (for $q \geqslant 1$ ) recovers the main result of [AT23] (see Remark A.4 therein) and strengthens it beyond $W_{\infty}$. We discuss the relationship of our work with the extant literature on differential privacy and sampling in $\$ 3.5$.

Remark 3.13 (Finiteness thresholds and refined Rényi regularity). Unlike KL regularity, Rényi regularity can undergo a phase transition in which $\mathrm{R}_{q}\left(\mu P_{T} \| \nu P_{T}\right)$ becomes finite only after $T$ surpasses some threshold $T_{0}>0$. In contrast, the $K L$ regularity is finite for arbitrarily small times as soon as the initial measures have finite second moment.

The Rényi regularity bounds in Theorem 3.10 and Corollary 3.11-while often exact for the OU process and its discretization, see $\$$ A.3-sometimes fail to tightly capture this finiteness threshold, in which case we turn toward the refined bounds of Theorem 3.9. See §A.3.2, where we explore the sharpness of the bounds on the finiteness threshold through various examples.

Theorem 3.14 (Refined Rényi regularity for Langevin). Suppose $\alpha I \leq \nabla^{2} V$ on $\mathbb{R}^{d}$. For any $q>1$,

$$
\begin{equation*}
\mathrm{R}_{q}\left(\mu P_{T} \| \nu P_{T}\right) \leqslant \inf _{\gamma \in \mathscr{C}(\mu, \nu)} \frac{1}{q-1} \log \int\left(\int \exp \left(\frac{\alpha(q-1)}{2(\exp (2 \alpha T)-1)}\|x-y\|^{2}\right) \gamma_{1 \mid 2}(\mathrm{~d} x \mid y)\right)^{q} \nu(\mathrm{d} y) \tag{3.18}
\end{equation*}
$$

and

$$
\begin{equation*}
\mathrm{R}_{q}\left(\mu P_{T} \| \nu P_{T}\right) \leqslant \inf _{\gamma \in \mathscr{C}(\mu, \nu)} \frac{1}{q-1} \log \int \exp \left(\frac{\alpha q(q-1)}{2(\exp (2 \alpha T)-1)} \int\|x-y\|^{2} \gamma_{2 \mid 1}(\mathrm{~d} y \mid x)\right) \mu(\mathrm{d} x) \tag{3.19}
\end{equation*}
$$

Proof. Specialize the Rényi regularity bound (3.17) to Dirac initializations and apply the refined convexity principle for Rényi divergences (Theorem 3.9).

### 3.5 Discussion: relationship with differential privacy and sampling

Our development is motivated by the "shifted divergence" technique from differential privacy [Fel+18], and in particular the modified version [AT22] which was recently used to established discrete mixing bounds [AC23; AT23]. Briefly, that argument bounds the divergence $\mathcal{D}$ (typically KL or Rényi) between the laws of two stochastic processes which evolve through the iterative, alternating application of additive noise (typically Gaussian) and a Lipschitz map (typically a step of gradient descent). That is, these arguments prove regularity results in the setting that $P=Q_{1} Q_{2}$ where $Q_{1}$ is a convolution kernel and $Q_{2}$ is Wasserstein-Lipschitz [AT22; AC23; AT23]. The argument uses as a Lyapunov function the shifted divergence

$$
\mathcal{D}^{(z)}(\mu \| \nu):=\inf _{\mu^{\prime}: W_{\infty}\left(\mu, \mu^{\prime}\right) \leqslant z} \mathcal{D}\left(\mu^{\prime} \| \nu\right)
$$

where $z \geqslant 0$ is a non-negative "shift" that allows changing the argument to the divergence in $W_{\infty}$ distance. The argument is based on two key lemmas which track how this shifted divergence is affected by either additive noise $Q_{1}$ or a Wasserstein-Lipschitz kernel $Q_{2}$. Our framework generalizes, refines, and unifies this shifted divergence argument.

Generality. We identify the shifted composition rule (Theorem 3.1) as the key informationtheoretic principle that underlies the shifted divergence argument. An important advantage of our level of generality is that our argument is no longer restricted to Markov kernels corresponding to additive noise. This generality is already manifest in Theorem 3.2 which does not require decomposition of $P$ into the form $Q_{1} Q_{2}$; and this freedom to choose more general "shifts" will be further exploited in future works in this series.

Refinement. The convexity principle (§3.3) yields improved regularity results (a.k.a., reverse transport inequalities) in which the Wasserstein distance is relaxed from $W_{\infty}$ e.g., KL bounds in terms of the initial 2-Wasserstein distance $W_{2}$, and Rényi bounds in terms of the initial OrliczWasserstein distance $W_{\psi_{2}}$. Although simple in hindsight, obtaining results beyond $W_{\infty}$ was a barrier in the literature for both differential privacy and sampling. For instance, this immediately improves and simplifies the discrete mixing results of discretized Langevin in both the overdamped [AT23] and underdamped settings [AC23]. For the previous, the improvement is for $W_{\infty}$ to $W_{2}$ or $W_{\psi_{2}}$ (corresponding to KL or Rényi, respectively); and for the latter this new argument further improves the constants in the regularity bound.

Unification. Although the literature on differential privacy and sampling and the literature on diffusions have both sought to prove mixing/regularity bounds for stochastic processes, a high-level difference is that the former analyzes discrete-time processes while the latter analyzes continuoustime ones. In this paper, we bridge the techniques from these communities by constructing "shifted processes" that 1) explicitly realize the optimal shifts that are implicit in the shifted divergence argument, 2) extend to the continuous-time arguments of [ATW06] in the limit (details in §4).

## 4 Continuous-time arguments

In this section, we develop the continuous-time analogues of the proofs in $\S 3$; in particular, $\S 4.1$ develops the analogue of the synchronous coupling via Girsanov transformation, and $\S 4.2$ develops the analogue of the Wasserstein coupling via Otto calculus. As we discuss below, the coupling in

§4.1 was previously introduced in [ATW06] and subsequently used extensively in the literature, but the argument in $\S 4.2$ seems to be new.

For simplicity, we illustrate the techniques on the special case of the Langevin diffusion with semi-convex potential, i.e., $\nabla^{2} V \geq \alpha I$ for some $\alpha \in \mathbb{R}$. We also assume that $\nabla V$ is Lipschitz continuous which, in light of the previous assumption, amounts to an upper bound on $\nabla^{2} V$, although the upper bound does not enter into our quantitative results. The Lipschitz continuity assumption is made for simplicity, to ensure that there is a unique strong solution to the Langevin SDE which is non-explosive. Later, in $\S 5.2$, we consider the more general setting of uniformly elliptic Itô diffusions.

### 4.1 Synchronous coupling and Girsanov's theorem

Fix $x, y \in \mathbb{R}^{d}$ and recall that our goal is to prove a bound on $\mathrm{R}_{q}\left(\delta_{x} P_{T} \| \delta_{y} P_{T}\right)$, where $\left(P_{t}\right)_{t \geqslant 0}$ is the Langevin semigroup (and $q=1$ corresponds to $\mathrm{R}_{q}=\mathrm{KL}$ ). The natural continuous-time analogue of the synchronous coupling in $\S 3$ is to first define the processes

$$
\begin{array}{rr}
\mathrm{d} X_{t}=-\nabla V\left(X_{t}\right) \mathrm{d} t+\sqrt{2} \mathrm{~d} B_{t}, & X_{0}=x, \\
\mathrm{~d} Y_{t}=-\nabla V\left(Y_{t}\right) \mathrm{d} t+\sqrt{2} \mathrm{~d} B_{t}, & Y_{0}=y,
\end{array}
$$

so that $\operatorname{law}\left(X_{T}\right)=\delta_{x} P_{T}$ and $\operatorname{law}\left(Y_{T}\right)=\delta_{y} P_{T}$. However, instead of bounding the divergence between the laws of $\left\{X_{t}\right\}_{t \in[0, T]}$ and $\left\{Y_{t}\right\}_{t \in[0, T]}$, we instead introduce an auxiliary process $\left\{X_{t}^{\prime}\right\}_{t \in[0, T]}$ such that $X_{T}^{\prime}=Y_{T}$ almost surely (hence $\operatorname{law}\left(X_{T}^{\prime}\right)=\operatorname{law}\left(Y_{T}\right)$ ) of the form

$$
\begin{equation*}
\mathrm{d} X_{t}^{\prime}=\left\{-\nabla V\left(X_{t}^{\prime}\right)+\eta_{t}\left(Y_{t}-X_{t}^{\prime}\right)\right\} \mathrm{d} t+\sqrt{2} \mathrm{~d} B_{t}, \quad X_{0}^{\prime}=x \tag{4.1}
\end{equation*}
$$

where $\left\{\eta_{t}\right\}_{t \in[0, T]}$ is a deterministic and non-negative process. The divergence between the laws of $\left\{X_{t}\right\}_{t \in[0, T]}$ and $\left\{X_{t}^{\prime}\right\}_{t \in[0, T]}$ can then be bounded by Girsanov's theorem. Such a coupling was first introduced in [ATW06] and subsequently used to establish Harnack and reverse transport inequalities for a bevy of settings, including for diffusions on Riemannian manifolds [ATW09; Wan14a], for diffusions with multiplicative noise [Wan11b; WY11], under low regularity [Sha13; HZ19; ZY21], for SPDEs [Wan07; LW08; DRW09; ERS09; Liu09; Zha10; Ouy11; WY11; Wan13; WZ13], for distribution-dependent processes [Wan18; HW19; HW22], for jump processes [Wan11a; ORW12; WZ15], and for SDEs driven by fractional Brownian motion [Fan15]; see [Wan12] for a survey. For completeness, we sketch the argument below, focusing on the KL divergence bound for simplicity. The extension to Rényi divergences is given in §A.4.

There are few other ways to construct this auxiliary processes. Related synchronous constructions are discussed briefly at the end of this section, and in the next section, we show that the Wasserstein coupling approach of $\S 3$ leads to a distinct continuous-time interpretation.

Reverse transport inequality for the KL divergence. Since our final bound will depend only on $\|x-y\|$, it does not matter whether we bound $\mathrm{KL}\left(\delta_{x} P_{T} \| \delta_{y} P_{T}\right)$ or $\mathrm{KL}\left(\delta_{y} P_{T} \| \delta_{x} P_{T}\right)$; for convenience we bound the latter. The key idea is to realize the auxiliary process (4.1) via a Girsanov transformation of the Wiener measure. To do so, let $\left\{B_{t}^{\prime}\right\}_{t \geqslant 0}$ be a standard Brownian motion under the path measure $\boldsymbol{\mu}_{T}^{\prime}$ on $\mathcal{C}\left([0, T] ; \mathbb{R}^{d}\right)$ and consider the solution to the coupled system of SDEs

$$
\begin{align*}
\mathrm{d} X_{t} & =\left\{-\nabla V\left(X_{t}\right)+\eta_{t}\left(Y_{t}-X_{t}\right)\right\} \mathrm{d} t+\sqrt{2} \mathrm{~d} B_{t}^{\prime}, & X_{0}=x \\
\mathrm{~d} Y_{t} & =-\nabla V\left(Y_{t}\right) \mathrm{d} t+\sqrt{2} \mathrm{~d} B_{t}^{\prime}, & Y_{0}=y \tag{4.2}
\end{align*}
$$

Then, let $\left\{B_{t}\right\}_{t \in[0, T]}$ be such that

$$
\mathrm{d} X_{t}=-\nabla V\left(X_{t}\right) \mathrm{d} t+\sqrt{2} \mathrm{~d} B_{t}
$$

i.e., $\mathrm{d} B_{t}=\mathrm{d} B_{t}^{\prime}+\frac{\eta_{t}}{\sqrt{2}}\left(X_{t}-Y_{t}\right) \mathrm{d} t$. If we define the $\boldsymbol{\mu}_{T}^{\prime}$-martingale $t \mapsto M_{t}:=-\int_{0}^{t} \frac{\eta_{s}}{\sqrt{2}}\left\langle X_{s}-Y_{s}, \mathrm{~d} B_{s}^{\prime}\right\rangle$, and if $\exp \left(M-\frac{1}{2}[M, M]\right)$ is a martingale (rather than merely a local martingale), Girsanov's theorem (see [Le 16, Theorem 5.22]) ensures that under the path measure $\boldsymbol{\mu}_{T}$ defined via

$$
\begin{equation*}
\frac{\mathrm{d} \boldsymbol{\mu}_{T}}{\mathrm{~d} \boldsymbol{\mu}_{T}^{\prime}}=\exp \left(M_{T}-\frac{1}{2}[M, M]_{T}\right) \tag{https://cdn.mathpix.com/cropped/2024_05_26_4ffb174681cea29d40d3g-17.jpg?height=49&width=84&top_left_y=447&top_left_x=1800}
\end{equation*}
$$

the process $\left\{B_{t}\right\}_{t \in[0, T]}$ is a standard Brownian motion. These path measures are defined so that under $\boldsymbol{\mu}_{T}, \operatorname{law}\left(X_{T}\right)=\delta_{x} P_{T}$, whereas under $\boldsymbol{\mu}_{T}^{\prime}$, if $X_{T}=Y_{T}$ almost surely, then law $\left(X_{T}\right)=\delta_{y} P_{T}$. It follows that

$$
\begin{equation*}
\mathrm{KL}\left(\delta_{y} P_{T} \| \delta_{x} P_{T}\right) \leqslant \mathrm{KL}\left(\boldsymbol{\mu}_{T}^{\prime} \| \boldsymbol{\mu}_{T}\right)=-\mathbb{E}_{\boldsymbol{\mu}_{T}^{\prime}} \log \frac{\mathrm{d} \boldsymbol{\mu}_{T}}{\mathrm{~d} \boldsymbol{\mu}_{T}^{\prime}}=\frac{1}{4} \mathbb{E}_{\boldsymbol{\mu}_{T}^{\prime}} \int_{0}^{T} \eta_{t}^{2}\left\|X_{t}-Y_{t}\right\|^{2} \mathrm{~d} t \tag{https://cdn.mathpix.com/cropped/2024_05_26_4ffb174681cea29d40d3g-17.jpg?height=48&width=84&top_left_y=716&top_left_x=1801}
\end{equation*}
$$

Next, since

$$
\mathrm{d}\left(X_{t}-Y_{t}\right)=\left\{-\nabla V\left(X_{t}\right)+\nabla V\left(Y_{t}\right)+\eta_{t}\left(Y_{t}-X_{t}\right)\right\} \mathrm{d} t
$$

hence by Itô's formula and semi-convexity,

$$
\mathrm{d}\left\|X_{t}-Y_{t}\right\|^{2}=-2\left\langle X_{t}-Y_{t}, \nabla V\left(X_{t}\right)-\nabla V\left(Y_{t}\right)+\eta_{t}\left(X_{t}-Y_{t}\right)\right\rangle \mathrm{d} t \leqslant-2\left(\alpha+\eta_{t}\right)\left\|X_{t}-Y_{t}\right\|^{2} \mathrm{~d} t,
$$

and therefore by Grönwall's lemma,

$$
\begin{equation*}
\left\|X_{t}-Y_{t}\right\|^{2} \leqslant \exp \left(-2 \alpha t-2 \int_{0}^{t} \eta_{s} \mathrm{~d} s\right)\|x-y\|^{2} \tag{4.5}
\end{equation*}
$$

Substituting this into (4.4), we find that

$$
\begin{equation*}
\mathrm{KL}\left(\delta_{y} P_{T} \| \delta_{x} P_{T}\right) \leqslant \frac{\|x-y\|^{2}}{4} \int_{0}^{T} \eta_{t}^{2} \exp \left(-2 \alpha t-2 \int_{0}^{t} \eta_{s} \mathrm{~d} s\right) \mathrm{d} t \tag{4.6}
\end{equation*}
$$

We now make the optimal choice $\eta_{t}=2 \alpha /\{\exp (2 \alpha(T-t))-1\}$ (this should be interpreted as $\eta_{t}=1 /(T-t)$ when $\left.\alpha=0\right)$; in $\S$ A. 2 , we show how this expression can be derived using the calculus of variations. With this choice, $\int_{0}^{t} \eta_{s} \mathrm{~d} s=\log \frac{1-\exp (-2 \alpha T)}{1-\exp (-2 \alpha(T-t))}$ (or $\log \frac{T}{T-t}$ for $\left.\alpha=0\right)$. In particular, from (4.5), we see that $X_{t}-Y_{t} \rightarrow 0$ almost surely as $t \nearrow T$, as required. Finally, substitution into (4.6) establishes the optimal reverse transport inequality

$$
\mathrm{KL}\left(\delta_{y} P_{T} \| \delta_{x} P_{T}\right) \leqslant \frac{\alpha\|x-y\|^{2}}{2(\exp (2 \alpha T)-1)}
$$

up to a few technical details which we address in the subsequent remark.

Remark 4.1 (Technical). The above proof sketch is rigorous aside from a few issues which we discuss here. First, since our eventual choice of $\eta_{t}$ blows up as $t$ ภ $T$, the existence and uniqueness of the system (4.2) on $[0, T]$ does not follow from the basic theory of SDEs. However, the system is well-posed on $[0, T-\varepsilon]$ for every $\varepsilon>0$. Therefore, this issue is easily remedied by noting that $\operatorname{law}_{\boldsymbol{\mu}_{T}}\left(X_{t}\right) \rightarrow \delta_{x} P_{T}$ and $\operatorname{law}_{\boldsymbol{\mu}_{T}^{\prime}}\left(X_{t}\right) \rightarrow \delta_{y} P_{T}$ weakly as $t \nearrow T$ and appealing to the joint lower semicontinuity of the $K L$ divergence.

Similarly, in order for (4.3) to define a valid probability measure $\boldsymbol{\mu}_{T}$, the Girsanov factor $t \mapsto \mathcal{E}_{t}:=\exp \left(M_{t}-\frac{1}{2}[M, M]_{t}\right)$ must be a valid martingale, which amounts to exponential integrability of the quantity $\int_{0}^{T}\left\|X_{t}-Y_{t}\right\|^{2} \mathrm{~d}$. However, this can also be avoided by considering a localizing sequence of stopping times $\left(\tau_{k}\right)_{k \in \mathbb{N}}$ such that $\tau_{k} \nearrow \infty$ almost surely, and $X_{t}$ and $Y_{t}$ are bounded for $t \leqslant \tau_{k}$. Then, the stopped process $\mathcal{E}_{\cdot \wedge \tau_{k}}$ is a valid martingale for each $k$, and we can again appeal to the joint lower semicontinuity of the KL divergence. Since this type of argument is standard in the literature, the details are omitted for brevity.

We conclude this section with a discussion of related synchronous constructions.

Remark 4.2 (Alternative construction). In some works (see, e.g., [Wan12]), a slightly different form is considered for the added drift, namely one adds $\eta_{t} \frac{Y_{t}-X_{t}^{\prime}}{\left\|Y_{t}-X_{t}^{\prime}\right\|}$ instead of $\eta_{t}\left(Y_{t}-X_{t}^{\prime}\right)$. Both approaches can be used to derive sharp Rényi regularity bounds for the Langevin SDE, but as noted in [Wan11b], it is necessary to consider an unbounded drift as $t>T$ in order to handle the multiplicative noise case in \$5.2. For concreteness, we stick with the latter form of the drift.

Remark 4.3 (Coupling with a deterministic shift). The coupling in (4.1) adds a random drift to the auxiliary process in order to force it to hit another process by time T. However, there is another method in which we simply define the auxiliary process to satisfy $X^{\prime}=X+v$, where $v:[0, T] \rightarrow \mathbb{R}^{d}$ is a deterministic curve. To distinguish it from the synchronous coupling, we refer to the latter method as coupling with a deterministic shift. Coupling with a deterministic shift has been used to derive Bismut-type derivative formulas (c.f. [Wan13, \$1.1.1]), which in turn can be used to establish power Harnack inequalities; however, to the best of our knowledge, the resulting Harnack inequalities are typically not sharp, unlike the ones derived via synchronous coupling. Interestingly, as noted in [Wan14b], coupling with a deterministic shift yields regularity for Kolmogorov's forward equation, in contrast to the Harnack inequalities considered here which encode regularity for Kolmogorov's backward equation (see $\$ 6.1$ for further discussion). The forward regularity problem and its information-theoretic reformulation will be explored in a forthcoming work.

Remark 4.4 (Relationship with the Föllmer drift). The coupling (4.1) can be interpreted as follows: we add a drift $t \mapsto b_{t}:=\eta_{t}\left(Y_{t}-X_{t}^{\prime}\right)$ to the Langevin diffusion to ensure that the process has law $\delta_{y} P_{T}$ at time T. By a similar argument based on Girsanov's theorem, any adapted and well-behaved drift $\left(b_{t}\right)_{t \in[0, T]}$ with this property leads to the bound

$$
\mathrm{KL}\left(\delta_{y} P_{T} \| \delta_{x} P_{T}\right) \leqslant \frac{1}{4} \int_{0}^{T} \mathbb{E}\left[\left\|b_{t}\right\|^{2}\right] \mathrm{d} t
$$

It is then natural to ask what the optimal drift is. The answer is the Föllmer drift, given by $b_{t}^{\star}=2 \nabla \log P_{T-t} \frac{\mathrm{d} \delta_{y} P_{T}}{\mathrm{~d} \delta_{x} P_{T}}\left(X_{t}^{\prime}\right)$, which makes the above inequality hold with equality [Föl85]. Despite the recent success of the Föllmer process for establishing functional inequalities (see, e.g., [Bor00; Leh13; CG14; EL18; MS21] and the connection with stochastic localization [KP21]) and its appealing optimality property, it seems less tractable for the purpose of establishing reverse transport inequalities, as we are interested in doing here.

### 4.2 Wasserstein coupling and Otto calculus

We now introduce the continuous-time analogue of the Wasserstein coupling argument in §3. Unlike the synchronous coupling discussed in the previous section, which was based on path space arguments (notably, through the use of Girsanov's theorem), the present approach is more closely tied with the theory of optimal transport.

Again, fix $x, y \in \mathbb{R}^{d}$, and for ease of notation write $\mu_{t}:=\delta_{x} P_{t}$ and $\nu_{t}:=\delta_{y} P_{t}$. We define a surrogate process $\left\{\mu_{t}^{\prime}\right\}_{t \in[0, T]}$ such that $\mu_{0}^{\prime}=\nu_{0}$ and $\mu_{T}^{\prime}=\mu_{T}$ as follows: let $X_{0}^{\prime} \sim \nu_{0}$ be a random variable that evolves according to the ODE

$$
\begin{equation*}
\dot{X}_{t}^{\prime}=-\nabla \log \frac{\mu_{t}^{\prime}}{\pi}\left(X_{t}^{\prime}\right)+\eta_{t}\left(T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}-\mathrm{id}\right)\left(X_{t}^{\prime}\right) \tag{4.7}
\end{equation*}
$$

where $\mu_{t}^{\prime}:=\operatorname{law}\left(X_{t}^{\prime}\right)$ and $T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}$ denotes the optimal transport map from $\mu_{t}^{\prime}$ to $\mu_{t}$. To interpret

this equation, recall that $\nabla \log \frac{\mu_{t}^{\prime}}{\pi}$ is the Wasserstein gradient of the $\mathrm{KL}$ divergence $\mathrm{KL}(\cdot \| \pi)$ at
$\mu_{t}^{\prime}$ (see $\left[\right.$ AGS08, §10.4]). Thus, the dynamics $\dot{X}_{t}^{\prime}=-\nabla \log \frac{\mu_{t}^{\prime}}{\pi}\left(X_{t}^{\prime}\right)$ yields the Wasserstein gradient flow of the relative entropy, which was shown in the work of R. Jordan, D. Kinderlehrer, and F. Otto [JKO98] to describe the evolution of the marginal law of the Langevin diffusion. The dynamics (4.7) adds onto the Wasserstein gradient flow an additional term which drives the auxiliary process towards the original process $\left\{\mu_{t}\right\}_{t \in[0, T]}$.

More precisely, through the description of solutions to the continuity equation (see [AGS08, §8]), the process (4.7) leads to the following PDE in the space of measures, which holds in the weak sense (in duality with space-time test functions):

$$
\begin{equation*}
\partial_{t} \mu_{t}^{\prime}=\operatorname{div}\left(\mu_{t}^{\prime}\left(\nabla \log \frac{\mu_{t}^{\prime}}{\pi}-\eta_{t}\left(T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}-\mathrm{id}\right)\right)\right) \tag{4.8}
\end{equation*}
$$

We next show how the auxiliary dynamics (4.8) can also be used to reach optimal reverse transport inequalities; to the best of our knowledge, this argument is new. To avoid obfuscating the flow of ideas with technical details, we keep our discussion at a formal level, i.e., we do not elaborate on the approximation arguments needed to make the following proof fully rigorous. For brevity, we also just focus on the KL bound here and defer the extenstion to Rényi divergence to $\S$ A.4.

Analogously to Theorem 3.2, the proof is based on two key bounds: on the KL divergence and the Wasserstein distance.

Divergence bound for the auxiliary process. We first differentiate $t \mapsto \mathrm{KL}\left(\mu_{t}^{\prime} \| \nu_{t}\right)$, where both arguments evolve simultaneously in time. This calculation is based on the simultaneous differentiation of KL divergence when both processes are evolving, a trick that has been used in other contexts in [VW19; Che+22]. We recall the calculation here for convenience.

$$
\begin{align*}
\partial_{t} \mathrm{KL}\left(\mu_{t}^{\prime} \| \nu_{t}\right) & =\int \partial_{t}\left(\mu_{t}^{\prime} \log \frac{\mu_{t}^{\prime}}{\nu_{t}}\right)=\int\left(\partial_{t} \mu_{t}^{\prime}\right) \log \frac{\mu_{t}^{\prime}}{\nu_{t}}+\int \mu_{t}^{\prime}\left(\frac{\partial_{t} \mu_{t}^{\prime}}{\mu_{t}^{\prime}}-\frac{\partial_{t} \nu_{t}}{\nu_{t}}\right) \\
& =-\int \mu_{t}^{\prime}\left\langle\nabla \log \frac{\mu_{t}^{\prime}}{\nu_{t}}, \nabla \log \frac{\mu_{t}^{\prime}}{\pi}-\eta_{t}\left(T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}-\mathrm{id}\right)\right\rangle+\int \nu_{t}\left\langle\nabla \frac{\mu_{t}^{\prime}}{\nu_{t}}, \nabla \log \frac{\nu_{t}}{\pi}\right\rangle \\
& =-\int \mu_{t}^{\prime}\left\langle\nabla \log \frac{\mu_{t}^{\prime}}{\nu_{t}}, \nabla \log \frac{\mu_{t}^{\prime}}{\pi}-\eta_{t}\left(T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}-\mathrm{id}\right)\right\rangle+\int \mu_{t}^{\prime}\left\langle\nabla \log \frac{\mu_{t}^{\prime}}{\nu_{t}}, \nabla \log \frac{\nu_{t}}{\pi}\right\rangle \\
& =-\int \mu_{t}^{\prime}\left\|\nabla \log \frac{\mu_{t}^{\prime}}{\nu_{t}}\right\|^{2}+\eta_{t} \int \mu_{t}^{\prime}\left\langle\nabla \log \frac{\mu_{t}^{\prime}}{\nu_{t}}, T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}-\mathrm{id}\right\rangle \\
& \leqslant \frac{\eta_{t}^{2}}{4} W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right) \tag{4.9}
\end{align*}
$$

where the last inequality follows from the Cauchy-Schwarz inequality, Young's inequality, and $\left\|T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}-\mathrm{id}\right\|_{L^{2}\left(\mu_{t}^{\prime}\right)}=W_{2}\left(\mu_{t}, \mu_{t}^{\prime}\right)$.

Distance bound for the auxiliary process. We next differentiate $t \mapsto W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right)$. Invoking $[$ Vilo9, Theorem 23.9],

$$
\begin{align*}
\frac{1}{2} \partial_{t} W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right) & =\int\left\langle T_{\mu_{t} \rightarrow \mu_{t}^{\prime}}-\mathrm{id}, \nabla \log \frac{\mu_{t}}{\pi}\right\rangle \mathrm{d} \mu_{t}+\int\left\langle T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}-\mathrm{id}, \nabla \log \frac{\mu_{t}^{\prime}}{\pi}-\eta_{t}\left(T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}-\mathrm{id}\right)\right\rangle \mathrm{d} \mu_{t}^{\prime} \\
& =\int\left\langle T_{\mu_{t} \rightarrow \mu_{t}^{\prime}}-\mathrm{id}, \nabla \log \frac{\mu_{t}}{\pi}\right\rangle \mathrm{d} \mu_{t}+\int\left\langle T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}-\mathrm{id}, \nabla \log \frac{\mu_{t}^{\prime}}{\pi}\right\rangle \mathrm{d} \mu_{t}^{\prime}-\eta_{t} W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right) \tag{4.10}
\end{align*}
$$

From the $\alpha$-geodesic convexity of the KL divergence (see [Vil09, Particular Case 23.15]),

$$
\begin{aligned}
& \mathrm{KL}\left(\mu_{t} \| \pi\right) \geqslant \mathrm{KL}\left(\mu_{t}^{\prime} \| \pi\right)+\int\left\langle\nabla \log \frac{\mu_{t}^{\prime}}{\pi}, T_{\mu_{t}^{\prime} \rightarrow \mu_{t}}-\mathrm{id}\right\rangle \mathrm{d} \mu_{t}^{\prime}+\frac{\alpha}{2} W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right) \\
& \operatorname{KL}\left(\mu_{t}^{\prime} \| \pi\right) \geqslant \mathrm{KL}\left(\mu_{t} \| \pi\right)+\int\left\langle\nabla \log \frac{\mu_{t}}{\pi}, T_{\mu_{t} \rightarrow \mu_{t}^{\prime}}-\mathrm{id}\right\rangle \mathrm{d} \mu_{t}+\frac{\alpha}{2} W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right)
\end{aligned}
$$

Summing these two inequalities and combining with (4.10), we obtain

$$
\partial_{t} W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right) \leqslant-2\left(\alpha+\eta_{t}\right) W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right)
$$

By Grönwall's inequality,

$$
\begin{equation*}
W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right) \leqslant \exp \left(-2 \alpha t-2 \int_{0}^{t} \eta_{s} \mathrm{~d} s\right)\|x-y\|^{2} \tag{4.11}
\end{equation*}
$$

Concluding the argument. Substituting (4.11) into (4.9) yields

$$
\mathrm{KL}\left(\delta_{x} P_{T} \| \delta_{y} P_{T}\right)=\operatorname{KL}\left(\mu_{T}^{\prime} \| \nu_{T}\right) \leqslant \frac{\|x-y\|^{2}}{4} \int_{0}^{T} \eta_{t}^{2} \exp \left(-2 \alpha t-2 \int_{0}^{t} \eta_{s} \mathrm{~d} s\right) \mathrm{d} t
$$

Note that this leads to the same bound as the one obtained in §4.1. In particular, with the same choice of $\eta_{t}=2 \alpha /\{\exp (2 \alpha(T-t))-1\}$, we once again arrive at the reverse transport inequality

$$
\mathrm{KL}\left(\delta_{x} P_{T} \| \delta_{y} P_{T}\right) \leqslant \frac{\alpha\|x-y\|^{2}}{2(\exp (2 \alpha T)-1)}
$$

We conclude this section with a remark on a different method of organizing the calculations that leads to a link with the JKO scheme of $[\mathrm{JKO} 98]$.

Remark 4.5 (Connection with the JKO scheme). Through (4.9) and (4.11), we have bounded the derivative in time for the KL divergence and Wasserstein distance separately; however, since these derivatives are expressed in terms of the same quantity $W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right)$, it also leads to the decay of a joint Lyapunov functional which incorporates both terms simultaneously. Namely, define

$$
\mathcal{L}_{t}:=\mathrm{KL}\left(\mu_{t}^{\prime} \| \pi\right)+\frac{1}{2 \lambda_{t}} W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right)
$$

Differentiating this quantity and using (4.9), (4.11) yields

$$
\dot{\mathcal{L}}_{t} \leqslant\left(\frac{\eta_{t}^{2}}{4}-\frac{\dot{\lambda}_{t}}{2 \lambda_{t}^{2}}-\frac{\alpha+\eta_{t}}{\lambda_{t}}\right) W_{2}^{2}\left(\mu_{t}, \mu_{t}^{\prime}\right)
$$

Now we can optimize over the choice of $\eta_{t}$, leading to $\eta_{t}=2 / \lambda_{t}$, and then $\dot{\mathcal{L}}_{t} \leqslant 0$ provided that $\dot{\lambda}_{t}+2\left(\alpha \lambda_{t}+1\right) \geqslant 0$. If we solve this differential inequality, enforcing that $\lambda_{t} \rightarrow 0$ as $t \nearrow T$, it then leads to the choice $\lambda_{t}=\{\exp (2 \alpha(T-t))-1\} / \alpha$.

This result can be reinterpreted as follows. For $\lambda>0$, define

$$
\begin{equation*}
\operatorname{MY}_{\mathrm{KL}(\cdot \| \pi)}(\mu ; \lambda):=\inf _{\mu^{\prime} \in \mathcal{P}_{2}\left(\mathbb{R}^{d}\right)}\left\{\operatorname{KL}\left(\mu^{\prime} \| \pi\right)+\frac{1}{2 \lambda} W_{2}^{2}\left(\mu, \mu^{\prime}\right)\right\} \tag{4.12}
\end{equation*}
$$

This is the generalization to the Wasserstein space of the Moreau-Yosida envelope, which is classically studied in conjunction with proximal methods in optimization [Roc97; Bec17]. The Moreau-Yosida envelope also appears as the Hopf-Lax solution to the Hamilton-Jacobi equation in classical mechanics.

In the Wasserstein space, the proximal point algorithm for minimizing the $K L$ divergence (the iterates of which are generated by successively minimizing (4.12)) is commonly referred to as the "minimizing movements scheme" or the "JKO scheme". With our choice of $\left\{\lambda_{t}\right\}_{t \in[0, T]}$, for any $t<T$,

$$
\begin{equation*}
\mathrm{MY}_{\mathrm{KL}(\cdot \| \pi)}\left(\mu_{t} ; \lambda_{t}\right) \leqslant \mathcal{L}_{t} \leqslant \mathcal{L}_{0}=\mathrm{MY}_{\mathrm{KL}(\cdot \| \pi)}\left(\mu_{0} ; \lambda_{0}\right) \tag{4.13}
\end{equation*}
$$

where the last equality holds if we choose ${ }^{3} \nu_{0}$ to be the minimizer in $\mathrm{M} \mathrm{KL}_{\mathrm{KL}(\| \pi)}\left(\mu_{0} ; \lambda_{0}\right)$. In particular, (4.13) readily implies

$$
\mathrm{KL}\left(\mu_{T} \| \pi\right) \leqslant \liminf _{t>T} \mathrm{MY}_{\mathrm{KL}(\cdot \| \pi)}\left(\mu_{t} ; \lambda_{t}\right) \leqslant \mathrm{MY}_{\mathrm{KL}(\cdot \| \pi)}\left(\mu_{0} ; \lambda_{0}\right) \leqslant \frac{\alpha W_{2}^{2}\left(\mu_{0}, \pi\right)}{2(\exp (2 \alpha T)-1)}
$$

The moral of the story, then, is that the Moreau-Yosida envelope with time-varying parameter $\lambda$ can be used as a Lyapunov functional for the gradient flow. This seems to be a new observation, and the use of this principle as a unifying analysis framework for optimization will be explored in a separate work.

## 5 Extensions to other settings

In this section, we consider extensions of our results to a few different settings, focusing on KL regularity throughout for simplicity.

### 5.1 Diffusions on manifolds

We briefly note that the proofs in $\S 4$ readily extend to the setting of a diffusion on a complete Riemannian manifold $\mathcal{M}$. Suppose that the generator of the diffusion is $\Delta-\langle\nabla V, \nabla \cdot\rangle$ where $\Delta$ is the Laplace-Beltrami operator, and that the condition $\operatorname{Ric}_{\mathcal{M}}+\nabla^{2} V \geq \alpha$ holds for some $\alpha \in \mathbb{R}$. The coupling introduced in $\S 4.1$ was in fact originally developed to prove Harnack inequalities in the Riemannian setting in [ATW06]; see [Wan14a, Theorem 2.3.2] for general definitions. As for the optimal transport approach of $\S 4.2$, the proofs go through as before using the calculus of optimal transport over Riemannian manifolds, see $[$ Vilo9, $\S 23]$.

### 5.2 Multiplicative noise

In this section, we consider the more general Itô SDE

$$
\mathrm{d} X_{t}=b_{t}\left(X_{t}\right) \mathrm{d} t+\sigma_{t}\left(X_{t}\right) \mathrm{d} B_{t},
$$

and we assume that the SDE is well-posed. In the paper [Wan11b], F.-Y. Wang obtained a log-Harnack inequality under the following assumptions.

Assumption 5.1. The following hold.

![](https://cdn.mathpix.com/cropped/2024_05_26_4ffb174681cea29d40d3g-21.jpg?height=60&width=1269&top_left_y=2168&top_left_x=298)

$$
\left\langle b_{t}(x)-b_{t}(y), x-y\right\rangle+\frac{1}{2}\left\|\sigma_{t}(x)-\sigma_{t}(y)\right\|_{\mathrm{HS}}^{2} \leqslant-\alpha\|x-y\|^{2}
$$[^2]- (Uniformly elliptic) There exists $\lambda>0$ such that for all $x \in \mathbb{R}^{d}$ and $t \in[0, T]$,

$$
\sigma_{t}(x) \sigma_{t}(x)^{\top} \geq \lambda I
$$

Here, we illustrate that his result can be obtained in a simple manner via our techniques. In fact, we will do so by obtaining a reverse transport inequality for the discretization

$$
\begin{equation*}
\hat{X}_{(n+1) h}=\hat{X}_{n h}+h b_{n h}\left(\hat{X}_{n h}\right)+\sqrt{h} \sigma_{n h}\left(\hat{X}_{n h}\right) \xi_{n h} \tag{5.1}
\end{equation*}
$$

where $\left\{\xi_{n h}\right\}_{n \in \mathbb{N}}$ is an i.i.d. sequence of standard Gaussian vectors, and then passing to the limit $h \searrow 0$. To the best of our knowledge, our result for the discretization (5.1) is new.

For the discretization, we must also impose an additional assumption. In our final bound, however, the dependence on the parameters $\beta, \Lambda$ below will vanish when we take $h \searrow 0$.

Assumption 5.2. There exist $\beta, \Lambda>0$ such that for all $x, y \in \mathbb{R}^{d}$ and $t \in[0, T]$,

$$
\left\|b_{t}(x)-b_{t}(y)\right\| \leqslant \beta\|x-y\| \quad \text { and } \quad \sigma_{t}(x) \sigma_{t}(x)^{\top} \leq \Lambda I
$$

We can now state our main result for this section.

Theorem 5.3. Let $\left\{\hat{\mu}_{n h}\right\}_{n=0}^{N}$ and $\left\{\hat{\nu}_{n h}\right\}_{n=0}^{N}$ denote the marginal laws of the process (5.1) started from $x$ and from $y$ respectively. Suppose that Assumptions 5.1 and 5.2 hold with $T=N h$. Then,

$$
\mathrm{KL}\left(\hat{\mu}_{N h} \| \hat{\nu}_{N h}\right) \leqslant\left(1+\frac{4(\beta-\alpha)(\Lambda / \lambda)^{3} h}{L^{2}}\right) \frac{\alpha(1-\beta h / 2)}{\lambda\left(L^{-2 N}-1\right)}\|x-y\|^{2}, \quad \text { where } L^{2}:=1-2 \alpha h+\beta^{2} h^{2}
$$

In particular, if we let $h \searrow 0$ with $N h \rightarrow T$, then with the obvious notation,

$$
\mathrm{KL}\left(\mu_{T} \| \nu_{T}\right) \leqslant \frac{\alpha\|x-y\|^{2}}{\lambda(\exp (2 \alpha T)-1)}
$$

This recovers the result of [Wan11b], at least for the log-Harnack inequality. It also includes the results for the Langevin diffusion as a special case. We do not treat the other Harnack inequalities corresponding to Rényi regularity here, as the use of Rényi divergences introduces substantial new complications in this setting.

We now give the proof, which simply amounts to checking the two conditions of Theorem 3.2.

Proof of Theorem 5.3. Let $t \in[0, T]$ and consider the kernel $P$ such that for any $x \in \mathbb{R}^{d}, \delta_{x} P=$ $\mathcal{N}\left(x+h b_{t}(x), h \sigma_{t}(x) \sigma_{t}(x)^{\top}\right)$. Write $\Sigma_{t}(x):=\sigma_{t}(x) \sigma_{t}(x)^{\top}$ and $\Sigma_{t}(y):=\sigma_{t}(y) \sigma_{t}(y)^{\top}$ for simplicity.

One-step regularity. Using the closed-form expression for the KL divergence between Gaussians, we can compute

$$
\mathrm{KL}\left(\delta_{x} P \| \delta_{y} P\right)=\frac{1}{2}\left[\frac{1}{h}\left\langle\Sigma_{t}(y)^{-1},\left(x+h b_{t}(x)-y-h b_{t}(y)\right)^{\otimes 2}\right\rangle+\operatorname{tr} f\left(\Sigma_{t}(y)^{-1 / 2} \Sigma_{t}(x) \Sigma_{t}(y)^{-1 / 2}\right)\right]
$$

where $f$ is the mapping $\zeta \mapsto \zeta-1-\log \zeta$.

For the first term, we use the uniform ellipticity $\Sigma_{t}(y) \geq \lambda I$, so it suffices to bound the quantity $\left\|x+h b_{t}(x)-y-h b_{t}(y)\right\|^{2}$. Expanding the square and applying Assumptions 5.1 and 5.2,

$$
\begin{aligned}
\left\|x+h b_{t}(x)-y-h b_{t}(y)\right\|^{2} & =\|x-y\|^{2}+2 h\left\langle b_{t}(x)-b_{t}(y), x-y\right\rangle+h^{2}\left\|b_{t}(x)-b_{t}(y)\right\|^{2} \\
& \leqslant\left(1-2 \alpha h+\beta^{2} h^{2}\right)\|x-y\|^{2}
\end{aligned}
$$

For the second term, let $\left\{\zeta_{i}\right\}_{i=1}^{d}$ denote the eigenvalues of $\Sigma_{t}(y)^{-1 / 2} \Sigma_{t}(x) \Sigma_{t}(y)^{-1 / 2}$. From Assumptions 5.1 and 5.2 , we have $\zeta_{i} \geqslant \lambda / \Lambda$ for all $i=1, \ldots, d$. Also, since $f(1)=f^{\prime}(1)=0$ and $f^{\prime \prime}(\zeta)=1 / \zeta^{2} \leqslant(\Lambda / \lambda)^{2}$ for all $\zeta \geqslant \lambda / \Lambda$, it follows that $f\left(\zeta_{i}\right) \leqslant \frac{\Lambda^{2}}{2 \lambda^{2}}\left(\zeta_{i}-1\right)^{2}$ for all $i=1, \ldots, d$. Thus,

$$
\sum_{i=1}^{d} f\left(\zeta_{i}\right) \leqslant \frac{\Lambda^{2}}{2 \lambda^{2}} \sum_{i=1}^{d}\left(\zeta_{i}-1\right)^{2}=\frac{\Lambda^{2}}{2 \lambda^{2}}\left\|\Sigma_{t}(y)^{-1 / 2} \Sigma_{t}(x) \Sigma_{t}(y)^{-1 / 2}-I\right\|_{\mathrm{HS}}^{2} \leqslant \frac{\Lambda^{2}}{2 \lambda^{4}}\left\|\Sigma_{t}(x)-\Sigma_{t}(y)\right\|_{\mathrm{HS}}^{2}
$$

We can also expand

$$
\begin{aligned}
\left\|\Sigma_{t}(x)-\Sigma_{t}(y)\right\|_{\mathrm{HS}}^{2} & =\left\|\sigma_{t}(x) \sigma_{t}(x)^{\top}-\sigma_{t}(y) \sigma_{t}(y)^{\top}\right\|_{\mathrm{HS}}^{2} \\
& \leqslant\left(\left\|\sigma_{t}(x) \sigma_{t}(x)^{\top}-\sigma_{t}(x) \sigma_{t}(y)^{\top}\right\|_{\mathrm{HS}}+\left\|\sigma_{t}(x) \sigma_{t}(y)^{\top}-\sigma_{t}(y) \sigma_{t}(y)^{\top}\right\|_{\mathrm{HS}}\right)^{2} \\
& \leqslant 2\left(\left\|\sigma_{t}(x) \sigma_{t}(x)^{\top}-\sigma_{t}(x) \sigma_{t}(y)^{\top}\right\|_{\mathrm{HS}}^{2}+\left\|\sigma_{t}(x) \sigma_{t}(y)^{\top}-\sigma_{t}(y) \sigma_{t}(y)^{\top}\right\|_{\mathrm{HS}}^{2}\right) \\
& \leqslant 4 \Lambda\left\|\sigma_{t}(x)-\sigma_{t}(y)\right\|_{\mathrm{HS}}^{2} \\
& \leqslant 8 \Lambda\left(-\alpha\|x-y\|^{2}-\left\langle b_{t}(x)-b_{t}(y), x-y\right\rangle\right) \leqslant 8(\beta-\alpha) \Lambda\|x-y\|^{2} .
\end{aligned}
$$

Combining everything together,

$$
\mathrm{KL}\left(\delta_{x} P \| \delta_{y} P\right) \leqslant \frac{1-2 \alpha h+\beta^{2} h^{2}+4(\beta-\alpha)(\Lambda / \lambda)^{3} h}{2 \lambda h}\|x-y\|^{2}
$$

Lipschitzness of the kernel. Next, consider a synchronous coupling of $\delta_{x} P$ and $\delta_{y} P$, i.e., we use the same noise random variable $\xi \sim \mathcal{N}(0, I)$. Then, by Assumptions 5.1 and 5.2,

$$
\begin{aligned}
W_{2}^{2}\left(\delta_{x} P, \delta_{y} P\right) & \leqslant \mathbb{E}\left[\left\|x+h b_{t}(x)+\sqrt{h} \sigma_{t}(x) \xi-y-h b_{t}(y)-\sqrt{h} \sigma_{t}(y) \xi\right\|^{2}\right] \\
& =\left\|x+h b_{t}(x)-y-h b_{t}(y)\right\|^{2}+h \mathbb{E}\left[\left\|\left(\sigma_{t}(x)-\sigma_{t}(y)\right) \xi\right\|^{2}\right] \\
& =\|x-y\|^{2}+2 h\left\langle b_{t}(x)-b_{t}(y), x-y\right\rangle+\left\|b_{t}(x)-b_{t}(y)\right\|^{2}+h\left\|\sigma_{t}(x)-\sigma_{t}(y)\right\|_{\mathrm{HS}}^{2} \\
& \leqslant\left(1-2 \alpha h+\beta^{2} h^{2}\right)\|x-y\|^{2}
\end{aligned}
$$

Concluding the proof. We can now invoke Theorem 3.2. Indeed, to prove a reverse transport inequality for KL divergence, it suffices to have $W_{2}$-Lipschitzness of the kernel (see Remark 3.4). Note that Theorem 3.2 was stated for repeated applications of a single Markov kernel $P$, whereas (due to the time dependence of the coefficients of the process (5.1)), here the Markov kernel changes with each iteration. However, since our bounds for the one-step regularity and Lipschitz constant of the kernel are uniform in time, it is easy to see that the proof of Theorem 3.2 can be adapted to this case straightforwardly.

### 5.3 Sums of i.i.d. random variables

Here, we leverage the discrete-time nature of our arguments to establish a reverse transport inequality for i.i.d. sum processes. Let $\rho$ be a probability density on $\mathbb{R}^{d}$ with zero mean, and let $P_{h}$ denote the Markov kernel representing convolution with the rescaled distribution $\rho_{h}(\cdot):=h^{-d} \rho(\cdot / h)$. We will apply the shifted chain rule for the KL divergence in order to bound the quantity $\mathrm{KL}\left(\delta_{x} P_{h}^{N} \| \delta_{y} P_{h}^{N}\right)$ where $h=\frac{1}{\sqrt{N}}$ is chosen according to the central limit scaling and we send $N \rightarrow \infty$.

Our argument relies on the Taylor expansion of the $\log$-density $\log \rho$, and hence we adopt the following assumptions to facilitate the proof.

Assumption 5.4. The density $\rho$ is strictly positive on $\mathbb{R}^{d}$. Also, the log-density $\log \rho$ is twice continuously differentiable and there exists $\varepsilon>0$ such that $\int\left(\sup _{B(z, \varepsilon)}\left\|\nabla^{2} \log \rho\right\|\right) \rho(\mathrm{d} z)<\infty$, where $B(z, \varepsilon)$ is the ball of radius $\varepsilon$ centered at $z$.

In the statistics literature, the so-called differentiability in quadratic mean (DQM) condition, which amounts to $L^{2}$ differentiability of the square root of the density, has been shown to imply important consequences such as local asymptotic normality of the log-likelihood (c.f. [Vaa98, §7.2]). Although the DQM condition appears to be too weak to establish the following theorem, Assumption 5.4 is certainly stronger than necessary. We leave the problem of formulating the minimal set of assumptions for future work.

Theorem 5.5. Let $\rho$ be a probability density on $\mathbb{R}^{d}$ with zero mean, satisfying Assumption 5.4. Let $h:=\frac{1}{\sqrt{N}}$ and let $P_{h}$ stand for the Markov kernel representing convolution with the rescaled density $\rho_{h}(\cdot):=h^{-d} \rho(\cdot / h)$. Then, for all $x, y \in \mathbb{R}^{d}$,

$$
\limsup _{N \rightarrow \infty} \mathrm{KL}\left(\delta_{x} P_{h}^{N} \| \delta_{y} P_{h}^{N}\right) \leqslant \frac{1}{2}\left\langle y-x,\left(\mathbb{E}_{\rho} \nabla^{2} \log \frac{1}{\rho}\right)(y-x)\right\rangle
$$

Example 5.6 (Gaussian convolution). Consider $\rho=\mathcal{N}(0, \Sigma)$, where $\Sigma>0$. Then $\rho_{h}=\mathcal{N}\left(0, h^{2} \Sigma\right)$, so $\delta_{x} P_{h}^{N}=\mathcal{N}\left(x, N h^{2} \Sigma\right)=\mathcal{N}(x, \Sigma)$ because $h=N^{-1 / 2}$, thus

$$
\mathrm{KL}\left(\delta_{x} P_{h}^{N} \| \delta_{y} P_{h}^{N}\right)=\frac{1}{2}\left\langle x-y, \Sigma^{-1}(x-y)\right\rangle
$$

Thus in this setting of Gaussian $\rho$, Theorem 5.5 is tight because $\mathbb{E}_{\rho} \nabla^{2} \log (1 / \rho)=\Sigma^{-1}$.

The matrix $\mathbb{E}_{\rho} \nabla^{2} \log (1 / \rho)$ is usually called the Fisher information matrix, and from integration by parts it can also be written $\mathbb{E}_{\rho}\left[(\nabla \log \rho)^{\otimes 2}\right]$. While the Fisher information matrix is equal to the inverse covariance in the special case of Gaussians (Example 5.6), this equality does not hold in general. The Cramér-Rao inequality (see [CP22, Appendix A] for a self-contained proof) states that $\mathbb{E}_{\rho} \nabla^{2} \log (1 / \rho) \geq\left(\operatorname{cov}_{\rho}\right)^{-1}$, thus the bound in Theorem 5.5 is always at least as big as

$$
\begin{equation*}
\frac{1}{2}\left\langle y-x,\left(\operatorname{cov}_{\rho}\right)^{-1}(y-x)\right\rangle \tag{5.2}
\end{equation*}
$$

which is what we would expect from CLT heuristics. In fact, under our (stringent) assumptions, the proof above actually implies the Cramé-Rao inequality. Indeed, the central limit theorem implies $\delta_{z} P_{h}^{N} \rightarrow \mathcal{N}\left(z, \operatorname{cov}_{\rho}\right)$ weakly for any $z \in \mathbb{R}^{d}$, and together with the lower semicontinuity of the KL divergence and Theorem 5.5,

$$
\begin{aligned}
\frac{1}{2}\left\langle y-x,\left(\operatorname{cov}_{\rho}\right)^{-1}(y-x)\right\rangle & =\mathrm{KL}\left(\mathcal{N}\left(x, \operatorname{cov}_{\rho}\right) \| \mathcal{N}\left(y, \operatorname{cov}_{\rho}\right)\right) \\
& \leqslant \liminf _{N \rightarrow \infty} \mathrm{KL}\left(\delta_{x} P_{h}^{N} \| \delta_{y} P_{h}^{N}\right) \leqslant \frac{1}{2}\left\langle y-x,\left(\mathbb{E}_{\rho} \nabla^{2} \log \frac{1}{\rho}\right)(y-x)\right\rangle
\end{aligned}
$$

Since this holds for all $x, y \in \mathbb{R}^{d}$, we conclude that $\left(\operatorname{cov}_{\rho}\right)^{-1} \leq \mathbb{E}_{\rho} \nabla^{2} \log (1 / \rho)$.

Although we have phrased Theorem 5.5 as an asymptotic statement, non-asymptotic statements can be extracted from the proof below. For example, if $\nabla^{2} \log \rho$ is Lipschitz, then one obtains a non-asymptotic version of Theorem 5.5 with an error term of order $O(1 / \sqrt{N})$. We conjecture that under suitable assumptions, the non-asymptotic bounds can be further replaced by bounds with leading term (5.2), but we are unable to reach this with our techniques.

Proof of Theorem 5.5. By translation invariance, it suffices to bound $\operatorname{KL}\left(\rho_{h}(\cdot) \| \rho_{h}(\cdot-v)\right)$ where $v:=(y-x) / N$. Taylor expansion of the log-density yields

$$
\begin{align*}
\operatorname{KL}\left(\rho_{h}(\cdot) \| \rho_{h}(\cdot-v)\right) & =\int \log \left(\frac{\rho(z)}{\rho(z-v / h)}\right) \rho(\mathrm{d} z) \\
& =\int\left(\left\langle\nabla \log \rho(z), \frac{v}{h}\right\rangle-\frac{1}{2}\left\langle\nabla^{2} \log \rho(\tilde{z}),\left(\frac{v}{h}\right)^{\otimes 2}\right\rangle\right) \rho(\mathrm{d} z) \tag{5.3}
\end{align*}
$$

where $z$ is a point lying between $z$ and $z-v / h$. In particular, since $v / h=(y-x) / \sqrt{N}$, we have $\|\tilde{z}-z\| \lesssim 1 / \sqrt{N}$ uniformly in $z$. The first term in this expansion vanishes due to integration by parts. For the second term, we observe that pointwise,

$$
N\left\langle\nabla^{2} \log \rho(\tilde{z}),\left(\frac{v}{h}\right)^{\otimes 2}\right\rangle=\left\langle\nabla^{2} \log \rho(\tilde{z}),(y-x)^{\otimes 2}\right\rangle \rightarrow\left\langle\nabla^{2} \log \rho(z),(y-x)^{\otimes 2}\right\rangle
$$

by continuity. On the other hand, for large $N$,

$$
N\left|\left\langle\nabla^{2} \log \rho(\tilde{z}),\left(\frac{v}{h}\right)^{\otimes 2}\right)\right| \leqslant\left(\sup _{B(z, \varepsilon)}\left\|\nabla^{2} \log \rho\right\|\right)\|y-x\|^{2}
$$

Thus by Assumption 5.4 we may appeal to the dominated convergence theorem, which gives

$$
\begin{equation*}
N \int\left\langle\nabla^{2} \log \rho(\tilde{z}),\left(\frac{v}{h}\right)^{\otimes 2}\right\rangle \rho(\mathrm{d} z) \rightarrow \int\left\langle\nabla^{2} \log \rho(z),(y-x)^{\otimes 2}\right\rangle \rho(\mathrm{d} z) \tag{5.4}
\end{equation*}
$$

To conclude the proof, we can appeal to the shifted chain rule. In the present setting, however, we can provide a more direct argument which could be illuminating. We wish to bound

$$
\operatorname{KL}\left(\delta_{x} P_{h}^{N} \| \delta_{y} P_{h}^{N}\right)=\operatorname{KL}\left(\operatorname{law}\left(x+\frac{1}{\sqrt{N}} \sum_{i=1}^{N} \xi_{i}\right) \| \operatorname{law}\left(y+\frac{1}{\sqrt{N}} \sum_{i=1}^{N} \xi_{i}\right)\right)
$$

where $\left(\xi_{i}: i=1, \ldots, N\right)$ is a family of i.i.d. random variables with law $\rho$. By the data-processing inequality, this quantity is at most

$$
\mathrm{KL}\left(\operatorname{law}\left(x+\frac{1}{\sqrt{N}} \sum_{i=1}^{j} \xi_{i}, j=0,1, \ldots, N-1\right) \| \operatorname{law}\left(x+j v+\frac{1}{\sqrt{N}} \sum_{i=1}^{j} \xi_{i}, j=0,1, \ldots, N-1\right)\right)
$$

since the last coordinates of these $N$-tuples are $x+\frac{1}{\sqrt{N}} \sum_{i=1}^{N} \xi_{i}$ and $y+\frac{1}{\sqrt{N}} \sum_{i=1}^{N} \xi_{i}$ respectively. Writing $\mu_{j}$ for the law of $x+\frac{1}{\sqrt{N}} \sum_{i=1}^{j} \xi_{j}$, we can now apply the chain rule for the KL divergence to bound this quantity by

$$
\sum_{j=0}^{N-1} \int \mathrm{KL}\left(\operatorname{law}\left(\zeta+\frac{1}{\sqrt{N}} \xi_{j+1}\right) \| \operatorname{law}\left(\zeta+v+\frac{1}{\sqrt{N}} \xi_{j+1}\right)\right) \mu_{j}(\mathrm{~d} \zeta)=N \mathrm{KL}\left(\rho_{h}(\cdot) \| \rho_{h}(\cdot-v)\right)
$$

The claimed result now follows from (5.3) and (5.4).

## 6 Applications to Harnack inequalities

In this section, we give applications of our results to Harnack inequalities. Background on Harnack inequalities is provided in §6.1. Originally, these inequalities were established in [Wan97] via semigroup methods, but since they are known to be dual to reverse transport inequalities (this duality is recalled in $\$ 6.2$ ), they also follow as a consequence of our information-theoretic methods. We exploit this to give new Harnack inequalities in $\S 6.3$ and $\S 6.4$.

### 6.1 Background on Harnack inequalities

In this section, we briefly provide background on Harnack inequalities. Let a Markov semigroup $\left(P_{t}\right)_{t \geqslant 0}$ and a compactly supported, positive, and smooth function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ be given. A parabolic Harnack inequality, in the strictest sense of the term, might refer to an inequality of the form

$$
\begin{equation*}
P_{t} f(x) \leqslant C(x, y, t) P_{t} f(y) \quad \text { for all } x, y \in \mathbb{R}^{d} \tag{6.1}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_05_26_4ffb174681cea29d40d3g-26.jpg?height=328&width=1350&top_left_y=234&top_left_x=385)

Name

Pure Harnack

Power Harnack

Log Harnack

Power Harnack

Reverse Harnack
Inequality

$P_{t} f(x) \leqslant C(x, y, t) P_{t} f(y)$

$P_{t} f(x) \leqslant C_{p}(x, y, t) P_{t}\left(f^{p}\right)(y)^{1 / p}$

$P_{t} f(x) \leqslant C(x, y, t)+\log P_{t}(\exp f)(y)$

$P_{t} f(x) \geqslant C_{p}(x, y, t) P_{t}\left(f^{p}\right)(y)^{1 / p} \quad p \in(0,1)$

$P_{t} f(x) \geqslant C_{p}(x, y, t) P_{t}\left(f^{p}\right)(y)^{1 / p} \quad p \in(-\infty, 0)($ see $\S 6.3)$

TABLE 1: A family of parabolic Harnack inequalities.

While Harnack inequalities have played a central role in the theory of elliptic PDEs, parabolic Harnack inequalities turn out to be more subtle. Namely, it is well-known that for typical diffusion processes (such as the Langevin diffusion (1.3)), an inequality such as (6.1) cannot hold for all functions $f$. To circumvent this, in the pioneering work [LY86], P. Li and S.-T. Yau introduced an alternative Harnack inequality which instead compares the semigroup at two different times $t$ and $s+t$. Their inequality, however, depends on the ambient dimension, which is at odds with the intrinsically infinite-dimensional character of many diffusion processes (in the sense of Bakry-Émery). Hence, in [Wan97], F.-Y. Wang introduced an infinite-dimensional Harnack inequality, obtained through commutation of the power function $(\cdot)^{p}(p>1)$ with the semigroup; we refer to these inequalities as power Harnack inequalities:

$$
\begin{equation*}
\left(P_{t} f(x)\right)^{p} \leqslant P_{t}\left(f^{p}\right)(y) \exp \left(\frac{\alpha p\|x-y\|^{2}}{2(p-1)(\exp (2 \alpha t)-1)}\right), \quad \forall x, y \in \mathbb{R}^{d}, t>0 \tag{6.2}
\end{equation*}
$$

By replacing $f$ with $f^{1 / p}$ and letting $p \rightarrow \infty$ in (6.2), one obtains the log Harnack inequality

$$
\begin{equation*}
P_{t}(\log f)(x) \leqslant \log P_{t} f(y)+\frac{\alpha\|x-y\|^{2}}{2(\exp (2 \alpha t)-1)} \tag{6.3}
\end{equation*}
$$

In Table 1, we record these inequalities in a form which emphasizes their similarities. This table includes power Harnack inequalities for $p \in(0,1)$, which are obviously equivalent to the $p \in(1, \infty)$ case up to replacing $p$ with $1 / p$, as noted in [AZ22]. We also include reverse Harnack inequalities corresponding to the case $p \in(-\infty, 0)$, named in analogy to the family of reverse hypercontractivity inequalities, which we explore in $\S 6.3$.

Equivalences with the curvature-dimension condition. Part of the importance of Harnack inequalities stems from their equivalence to a large family of other properties, including the curvaturedimension condition. We list a few of these equivalences below, focusing on the Langevin semigroup for ease of exposition although the equivalences hold much more generally. The reader can find further equivalences in the book [BGL14] or in [Wan14a, Theorem 2.3.3].

Theorem 6.1. Let $\left(P_{t}\right)_{t \geqslant 0}$ denote the Markov semigroup corresponding to the Langevin diffusion (1.3) with potential $V$, and let $\alpha \in \mathbb{R}, p, q>1$. The following are equivalent.

1. (Curvature-dimension condition) $\nabla^{2} V \geq \alpha I$ on $\mathbb{R}^{d}$.
2. (Wasserstein contraction) For any $t>0$ and any $x, y \in \mathbb{R}^{d}$,

$$
W_{p}\left(\delta_{x} P_{t}, \delta_{y} P_{t}\right) \leqslant \exp (-\alpha t)\|x-y\| .
$$

3. (Gradient bound) For any $f \in \mathcal{C}_{\mathrm{c}}^{\infty}\left(\mathbb{R}^{d}\right)$ and any $t>0$,

$$
\left\|\nabla P_{t} f\right\| \leqslant \exp (-\alpha t) P_{t}\|\nabla f\| .
$$

4. (Power Harnack) The inequality (6.2) holds.
5. (Rényi regularity) For any $t>0$ and any $x, y \in \mathbb{R}^{d}$,

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P_{t} \| \delta_{y} P_{t}\right) \leqslant \frac{\alpha q\|x-y\|^{2}}{2(\exp (2 \alpha t)-1)} \tag{6.4}
\end{equation*}
$$

6. (Log Harnack) The inequality (6.3) holds.
7. (KL regularity) For any $t>0$ and any $x, y \in \mathbb{R}^{d}$,

$$
\begin{equation*}
\mathrm{KL}\left(\delta_{x} P_{t} \| \delta_{y} P_{t}\right) \leqslant \frac{\alpha\|x-y\|^{2}}{2(\exp (2 \alpha t)-1)} \tag{6.5}
\end{equation*}
$$

For example, [Wan10] showed that the validity of (6.2) for any fixed $p>1$ implies the log-Harnack inequality, which in turn implies the curvature-dimension condition. We give a dual version of the former statement, namely that (6.4) implies (6.5), in §B.2.

Implications of the Harnack inequalities. The dimension-free Harnack inequalities encode a wealth of information about the semigroup and have been successfully applied to establish functional inequalities [Wan97; Wan99; BGL01; Wan01; Wan17], heat kernel estimates [AK01; GW01; AZ02], higher-order eigenvalue estimates [Wan02; GW04], and ultracontractivity [Wan06]; see, e.g., [Wan14a, §1] for further statements.

Let us briefly note that these properties correspond to regularity of Kolmogorov's backward equation. Indeed, we will show that the reverse transport inequality (6.4) for $q=2$ yields

$$
\begin{equation*}
P_{t}\left(f^{2}\right)-\left(P_{t} f\right)^{2} \geqslant \frac{\exp (2 \alpha t)-1}{\alpha}\left\|\nabla P_{t} f\right\|^{2}, \quad \forall t>0 \tag{6.6}
\end{equation*}
$$

Conversely, (6.6) is a form of the local Poincare inequality which is equivalent to the curvaturedimension condition (c.f. [BGL14, Theorem 4.7.2]), and therefore implies back (6.4) for any $q>1$ by Theorem 6.1.

The inequality (6.6) implies that the semigroup $P_{t}$ maps bounded measurable functions to differentiable functions, which is a smoothing property of the semigroup. On the other hand, in [Wan14b], F.-Y. Wang introduced the family of shift Harnack inequalities which instead capture the regularity of Kolmogorov's forward equation, and we will revisit them from the lens of information theory in a forthcoming work.

Proof of (6.6) from (6.4) for $q=2$. For $h \in \mathbb{R}^{d} \backslash\{0\}$, by the Cauchy-Schwarz inequality,

$$
\begin{aligned}
\left|\frac{P_{t} f(x+h)-P_{t} f(x)}{\|h\|}\right| & =\frac{1}{\|h\|}\left|\int f\left(\frac{\mathrm{d}\left(\delta_{x+h} P_{t}\right)}{\mathrm{d}\left(\delta_{x} P_{t}\right)}-1\right) \mathrm{d}\left(\delta_{x} P_{t}\right)\right| \\
& \leqslant \sqrt{\operatorname{var}_{\delta_{x}} P_{t}(f)} \frac{\sqrt{\chi^{2}\left(\delta_{x+h} P_{t} \| \delta_{x} P_{t}\right)}}{\|h\|}
\end{aligned}
$$

Applying (6.4) and sending $\|h\| \searrow 0$,

$$
\left\|\nabla P_{t} f(x)\right\| \leqslant \sqrt{\operatorname{var}_{\delta_{x} P_{t}}(f) \frac{\alpha}{\exp (2 \alpha T)-1}}
$$

Square both sides of the inequality to recover the result.

### 6.2 Duality between Harnack and reverse transport inequalities

Since the equivalences in Theorem 6.1 are by now well-known, we will not prove them all in this paper. However, since we will later use the duality between reverse transport inequalities and Harnack inequalities to deduce the latter from the former, we recall this duality below.

Equivalence between (6.2) and (6.4) for $q=\frac{p}{p-1}$, and between (6.3) and (6.5). Let $P$ be a Markov kernel on a Polish space $\mathcal{X}$ and write $C_{p}(x, y)$ for the best constant in the power Harnack inequality

$$
P f(x) \leqslant C_{p}(x, y) P\left(f^{p}\right)(y)^{1 / p}
$$

The best constant $C_{p}(x, y)$ is simply the operator norm $\left\|L_{x}\right\|_{L^{p}\left(\delta_{y} P\right) \rightarrow \mathbb{R}}$ of the linear function $L_{x}: f \mapsto$ $P f(x)$ because $P\left(f^{p}\right)(y)^{1 / p}=\|f\|_{L^{p}\left(\delta_{y} P\right)}$. By re-writing $P f(x)=\int f \mathrm{~d}\left(\delta_{x} P\right)=\int f \frac{\mathrm{d}\left(\delta_{x} P\right)}{\mathrm{d}\left(\delta_{y} P\right)} \mathrm{d}\left(\delta_{y} P\right)$ and appealing to Hölder duality, this operator norm equals

$$
C_{p}(x, y)=\left\|L_{x}\right\|_{L^{p}\left(\delta_{y} P\right) \rightarrow \mathbb{R}}=\left\|\frac{\mathrm{d}\left(\delta_{x} P\right)}{\mathrm{d}\left(\delta_{y} P\right)}\right\|_{L^{q}\left(\delta_{y} P\right)}=\exp \left(\frac{q-1}{q} \mathrm{R}_{q}\left(\delta_{x} P \| \delta_{y} P\right)\right)
$$

which yields the equivalence between (6.2) and (6.4). In particular, $\mathrm{R}_{q}\left(\delta_{x} P \| \delta_{y} P\right) \leqslant \rho(x, y)$ if and only if $C_{p}(x, y) \leqslant \exp \left(\frac{q-1}{q} \rho(x, y)\right)$.

The equivalence between (6.3) and (6.5) follows along similar lines, replacing Hölder duality with the Donsker-Varadhan variational principle

$$
\int f \mathrm{~d} \mu \leqslant \mathrm{KL}(\mu \| \nu)+\log \int \exp (f) \mathrm{d} \nu
$$

with equality if and only if $f=\log \frac{\mathrm{d} \mu}{\mathrm{d} \nu}$. It yields that if $C_{\log }(x, y)$ is the best constant in the inequality

$$
P f(x) \leqslant C_{\log }(x, y)+\log P(\exp f)(y)
$$

then $\mathrm{KL}\left(\delta_{x} P \| \delta_{y} P\right) \leqslant \rho(x, y)$ if and only if $C_{\log }(x, y) \leqslant \rho(x, y)$.

Remark 6.2 (Distributional Harnack inequalities). This duality argument extends from measures $\delta_{x} P, \delta_{y} P$ to measures $\mu P, \nu P$. Through the convexity principle in $\S 3.3$, we have obtained reverse transport inequalities from arbitrary initializations $\mu, \nu$. By dualizing these results, we obtain distributional Harnack inequalities: under $\operatorname{CD}(\alpha, \infty)$, it holds that for $p>1, t>0$, and $x, y \in \mathbb{R}^{d}$,

$$
\begin{equation*}
\|f\|_{L^{1}\left(\mu P_{t}\right)} \leqslant\|f\|_{L^{p}\left(\nu P_{t}\right)} \inf _{\gamma \in \mathscr{C}(\mu, \nu)}\left[\int \exp \left(\frac{\alpha p\|x-y\|^{2}}{2(p-1)^{2}(\exp (2 \alpha t)-1)}\right) \gamma(\mathrm{d} x, \mathrm{~d} y)\right]^{(p-1) / p} \tag{6.7}
\end{equation*}
$$

and

$$
\begin{equation*}
\mathbb{E}_{\mu P_{t}} f \leqslant \log \mathbb{E}_{\nu P_{t}} \exp (f)+\frac{\alpha W_{2}^{2}(\mu, \nu)}{2(\exp (2 \alpha t)-1)} \tag{6.8}
\end{equation*}
$$

We show how to obtain these distributional Harnack inequalities via direct arguments (i.e., without dualizing standard Harnack inequalities, appealing to the convexity principle, and dualizing back), and similarly for the dual versions of the refined Rényi bounds of Theorem 3.9, in §B.1.

### 6.3 Reverse Harnack inequalities

As we have shown in the preceding section, the family of reverse transport inequalities of order $q \in(1, \infty)$ is equivalent, via duality, to the power Harnack inequalities with exponent $p \in(1, \infty)$, with an additional equivalence between the case of $q=1$ (i.e., the KL divergence) and the $\log$-Harnack inequality. In $\S 3$, we also established reverse transport inequalities of order $q \in(0,1)$, for which the corresponding dual exponent $p:=-q /(1-q)$ ranges in $(-\infty, 0)$. In this section, we will show that we consequently obtain a family of reverse Harnack inequalities.

The following lemma provides the key duality result.

Lemma 6.3 (Duality). Let $q \in(0,1)$ and $p=-\frac{q}{1-q} \in(-\infty, 0)$. For any positive function $f$ and any probability measures $\mu, \nu$,

$$
\mathbb{E}_{\mu} f \geqslant\left(\mathbb{E}_{\nu} f^{p}\right)^{1 / p} \exp \left(-\frac{1}{|p|} \mathrm{R}_{q}(\mu \| \nu)\right)
$$

Proof. Let $g:=f^{q}$. To simplify the notation, we assume that $\mu \ll \nu$, although this assumption is not necessary for the proof. Then

$$
\exp \left((q-1) \mathbb{R}_{q}(\mu \| \nu)\right)=\mathbb{E}_{\nu}\left[\frac{g}{g}\left(\frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right)^{q}\right] \leqslant \mathbb{E}_{\mu}\left[g^{1 / q}\right]^{q} \mathbb{E}_{\nu}\left[g^{-1 /(1-q)}\right]^{1-q}=\mathbb{E}_{\mu}[f]^{q} \mathbb{E}_{\nu}\left[f^{-q /(1-q)}\right]^{1-q}
$$

Above, the first step is the definition of Rényi divergence and multiplying and dividing by the same quantity $g$, the second step is by Hölder's inequality with dual exponents $1 / q$ and $1 /(1-q)$, and the final step is by definition of $g$. Rearranging the above display, raising everything to the power of $1 / q$, and simplifying by using the definition of $p$ completes the proof.

From Theorem 3.10 and Lemma 6.3, we immediately obtain the following inequalities.

Theorem 6.4 (Reverse Harnack inequalities). Let $\left(P_{t}\right)_{t \geqslant 0}$ denote the Langevin semigroup corresponding to a potential satisfying $\nabla^{2} V \geq \alpha I$ on $\mathbb{R}^{d}$, where $\alpha \in \mathbb{R}$. Then, for all functions $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}$, all $p \in(-\infty, 0)$, all $t>0$, and all $x, y \in \mathbb{R}^{d}$, it holds that

$$
\begin{equation*}
P_{t} f(x) \geqslant P_{t}\left(f^{p}\right)(y)^{1 / p} \exp \left(-\frac{\alpha\|x-y\|^{2}}{2|p-1|(\exp (2 \alpha t)-1)}\right) \tag{6.9}
\end{equation*}
$$

We remark that replacing $f$ with $f^{1 / p}$ transforms the reverse Harnack inequality with exponent $p$ into the one with exponent $1 / p$. This fact corresponds to the relation $\mathrm{R}_{q}(\mu \| \nu)=\frac{q}{1-q} \mathrm{R}_{1-q}(\nu \| \mu)$ which holds for $q \in(0,1)$.

Once the form of the reverse Harnack inequalities are known, it is not difficult to prove them. For example, we show in $\S$ A. 5 that for a diffusion process on a Riemannian manifold for which the curvature-dimension condition $\mathrm{CD}(\alpha, \infty)$ holds, they can be established via the usual semigroup calculations. In fact, they are implied by the power Harnack inequalities (6.2) through a simple application of Jensen's inequality: for $p>1$, replacing $f$ with $f^{1 / p}$ in (6.2) and interchanging $x$ and $y$, followed by applying Jensen's inequality to the convex function $(\cdot)^{-1}$, yields

$$
\begin{aligned}
P_{t} f(x) & \geqslant P_{t}\left(f^{1 / p}\right)(y)^{p} \exp \left(-\frac{\alpha p\|x-y\|^{2}}{2(p-1)(\exp (2 \alpha t)-1)}\right) \\
& \geqslant P_{t}\left(f^{-1 / p}\right)(y)^{-p} \exp \left(-\frac{\alpha p\|x-y\|^{2}}{2(p-1)(\exp (2 \alpha t)-1)}\right)
\end{aligned}
$$

which is seen to be equivalent to (6.9) for exponent $-1 / p \in(-\infty, 0)$.

On the other hand, we also show in $\S$ A. 5 that the reverse Harnack inequality (6.9) implies back $\operatorname{CD}(\alpha, \infty)$. We can therefore add two more equivalences to Theorem 6.1.

Theorem 6.5. Consider the setting of Theorem 6.1, let $p \in(-\infty, 0)$, and let $q \in(0,1)$. Then, any of the statements of Theorem 6.1 are equivalent to any of the following.

8. (Reverse Harnack) The inequality (6.9) holds.
9. (Rényi regularity for $q \in(0,1)$ ) For any $t>0$ and any $x, y \in \mathbb{R}^{d}$,

$$
\mathrm{R}_{q}\left(\delta_{x} P_{t} \| \delta_{y} P_{t}\right) \leqslant \frac{\alpha q\|x-y\|^{2}}{2(\exp (2 \alpha t)-1)}
$$

### 6.4 Harnack inequalities for discretizations of diffusions

We conclude by noting that the techniques of $\S 3$, which apply to discrete-time processes, combined with the duality arguments of $\S 6.2$, enable us to prove Harnack inequalities for iterations of discretetime Markov kernels. Such discrete-time processes arise naturally in the study of sampling algorithms based on time-discretizations of SDEs; processes which have limiting SDE descriptions (e.g., the CLT setting of $\S 5.3$ ); and more generally, discrete dynamical systems (e.g., [DGW04]). Here, we provide a simple illustration by dualizing the result of $\$ 5.2$. Similar results can also be deduced in analogous way for, e.g., the settings of $\S 3.4$ and $\S 5.3$.

Corollary 6.6 (Log-Harnack inequality). Consider the setting of Theorem 5.3 pertaining to the discrete-time process (5.1). Let $\hat{P}_{h}$ denote the corresponding Markov transition kernel. Then, for all positive measurable functions $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ and all $x, y \in \mathbb{R}^{d}$,

$$
\hat{P}_{h}^{N} f(x) \leqslant\left(1+\frac{4(\beta-\alpha)(\Lambda / \lambda)^{3} h}{L^{2}}\right) \frac{\alpha(1-\beta h / 2)}{\lambda\left(L^{-2 N}-1\right)}+\log \hat{P}_{h}^{N}(\exp f)(y)
$$

where $L^{2}:=1-2 \alpha h+\beta^{2} h^{2}$.

## References

[AK01] S. Aida and H. Kawabi. "Short time asymptotics of a certain infinite dimensional diffusion process". In: Stochastic analysis and related topics, VII (Kusadasi, 1998). Vol. 48. Progr. Probab. Birkhäuser Boston, Boston, MA, 2001, pp. 77-124.

[AZ02] S. Aida and T. Zhang. "On the small time asymptotics of diffusion processes on path groups". In: Potential Anal. 16.1 (2002), pp. $67-78$.

[AC23] J. M. Altschuler and S. Chewi. "Faster high-accuracy log-concave sampling via algorithmic warm starts". In: Symposium on Foundations of Computer Science. 2023.

[AT22] J. M. Altschuler and K. Talwar. "Privacy of noisy stochastic gradient descent: more iterations without more privacy loss". In: Advances in Neural Information Processing Systems. 2022.

[AT23] J. M. Altschuler and K. Talwar. "Resolving the mixing time of the Langevin algorithm to its stationary distribution for log-concave sampling". In: Conference on Learning Theory. Vol. 195. PMLR, July 2023, pp. 2509-2510.

[AGS08] L. Ambrosio, N. Gigli, and G. Savaré. Gradient flows in metric spaces and in the space of probability measures. Second. Lectures in Mathematics ETH Zürich. Birkhäuser Verlag, Basel, 2008, pp. x+334.

[AZ22] Z. An and G. Zong. "Bilateral Harnack inequalities for stochastic differential equation with multiplicative noise". In: J. Funct. Spaces (2022), Art. ID 5464688, 11.

[ATW06] M. Arnaudon, A. Thalmaier, and F.-Y. Wang. "Harnack inequality and heat kernel estimates on manifolds with curvature unbounded below". In: Bull. Sci. Math. 130.3 (2006), pp. 223-233.

[ATW09] M. Arnaudon, A. Thalmaier, and F.-Y. Wang. "Gradient estimates and Harnack inequalities on non-compact Riemannian manifolds". In: Stochastic Process. Appl. 119.10 (2009), pp. 3653-3670.

[BP22] J. D. Backhoff-Veraguas and G. Pammer. "Applications of weak transport theory". In: Bernoulli 28.1 (2022), pp. 370-394.

[BGL14] D. Bakry, I. Gentil, and M. Ledoux. Analysis and geometry of Markov diffusion operators. Vol. 103. Springer, 2014.

[Bec17] A. Beck. First-order methods in optimization. Vol. 25. MOS-SIAM Series on Optimization. Society for Industrial and Applied Mathematics, Mathematical Optimization Society, 2017 , pp. xii +475 .

[BGL01] S. G. Bobkov, I. Gentil, and M. Ledoux. "Hypercontractivity of Hamilton-Jacobi equations". In: Journal de Mathématiques Pures et Appliquées 80.7 (2001), pp. 669-696.

[Bor00] C. Borell. "Diffusion equations and geometric inequalities". In: Potential Anal. 12.1 (2000), pp. 49-71.

[CG14] P. Cattiaux and A. Guillin. "Semi log-concave Markov diffusions". In: Séminaire de Probabilités XLVI. Vol. 2123. Lecture Notes in Math. Springer, Cham, 2014, pp. 231-292.

[Che+22] Y. Chen, S. Chewi, A. Salim, and A. Wibisono. "Improved analysis for a proximal algorithm for sampling". In: Conference on Learning Theory. Vol. 178. PMLR, 2022, pp. 2984-3014.

[CP22] S. Chewi and A.-A. Pooladian. "An entropic generalization of Caffarelli's contraction theorem via covariance inequalities". In: arXiv preprint 2203.04954 (2022).

[DRW09] G. Da Prato, M. Röckner, and F.-Y. Wang. "Singular stochastic equations on Hilbert spaces: Harnack inequalities for their transition semigroups". In: J. Funct. Anal. 257.4 (2009), pp. 992-1017.

[DGW04] H. Djellout, A. Guillin, and L. M. Wu. "Transportation cost-information inequalities and applications to random dynamical systems and diffusions". In: Ann. Probab. 32.3B (2004), pp. 2702-2732.

[EL18] R. Eldan and J. R. Lee. "Regularization under diffusion and anticoncentration of the information content". In: Duke Math. J. 167.5 (2018), pp. 969-993.

[Fan15] X. Fan. "Stochastic Volterra equations driven by fractional Brownian motion". In: Front. Math. China 10.3 (2015), pp. 595-620.

[Fel+18] V. Feldman, I. Mironov, K. Talwar, and A. Thakurta. "Privacy amplification by iteration". In: Symposium on Foundations of Computer Science. IEEE. 2018, pp. 521532.

[Föl85] H. Föllmer. "An entropy approach to the time reversal of diffusion processes". In: Stochastic differential systems (Marseille-Luminy, 1984). Vol. 69. Lect. Notes Control Inf. Sci. Springer, Berlin, 1985, pp. 156-163.

[GW01] F.-Z. Gong and F.-Y. Wang. "Heat kernel estimates with application to compactness of manifolds". In: Q. J. Math. 52.2 (2001), pp. 171-180.

[GW04] F.-Z. Gong and F.-Y. Wang. "On Gromov's theorem and $L^{2}$-Hodge decomposition". In: Int. J. Math. Math. Sci. 1-4 (2004), pp. 25-44.

[Goz+17] N. Gozlan, C. Roberto, P.-M. Samson, and P. Tetali. "Kantorovich duality for general transport costs and applications". In: J. Funct. Anal. 273.11 (2017), pp. 3327-3405.

[HW19] X. Huang and F.-Y. Wang. "Distribution dependent SDEs with singular coefficients". In: Stochastic Process. Appl. 129.11 (2019), pp. 4747-4770.

[HW22] X. Huang and F.-Y. Wang. "Log-Harnack inequality and Bismut formula for singular McKean-Vlasov SDEs". In: arXiv preprint 2207.11536 (2022).

[HZ19] X. Huang and S.-Q. Zhang. "Mild solutions and Harnack inequality for functional stochastic partial differential equations with Dini drift". In: J. Theoret. Probab. 32.1 (2019), pp. 303-329.

[JKO98] R. Jordan, D. Kinderlehrer, and F. Otto. "The variational formulation of the FokkerPlanck equation". In: SIAM Journal on Mathematical Analysis 29.1 (1998), pp. 117.

[KP21] B. Klartag and E. Putterman. "Spectral monotonicity under Gaussian convolution". In: $(2021)$.

[Le 16] J.-F. Le Gall. Brownian motion, martingales, and stochastic calculus. Vol. 274. Graduate Texts in Mathematics. Springer, 2016, p. 273.

[Leh13] J. Lehec. "Representation formula for the entropy and functional inequalities". In: Ann. Inst. Henri Poincaré Probab. Stat. 49.3 (2013), pp. 885-899.

[LY86] P. Li and S.-T. Yau. "On the parabolic kernel of the Schrödinger operator". In: Acta Math. 156.3-4 (1986), pp. 153-201.

[Liu09] W. Liu. "Harnack inequality and applications for stochastic evolution equations with monotone drifts". In: J. Evol. Equ. 9.4 (2009), pp. 747-770.

[LW08] W. Liu and F.-Y. Wang. "Harnack inequality and strong Feller property for stochastic fast-diffusion equations". In: J. Math. Anal. Appl. 342.1 (2008), pp. 651-662.

[MS21] D. Mikulincer and Y. Shenfeld. "The Brownian transport map". In: arXiv preprint 2111.11521 (2021).

[Mir17] I. Mironov. "Rényi differential privacy". In: Computer Security Foundations Symposium. IEEE. 2017, pp. 263-275.

[Ott01] F. Otto. "The geometry of dissipative evolution equations: the porous medium equation". In: Comm. Partial Differential Equations 26.1-2 (2001), pp. 101-174.

[Ouy11] S.-X. Ouyang. "Harnack inequalities and applications for multivalued stochastic evolution equations". In: Infin. Dimens. Anal. Quantum Probab. Relat. Top. 14.2 (2011), pp. 261278.

[ORW12] S.-X. Ouyang, M. Röckner, and F.-Y. Wang. "Harnack inequalities and applications for Ornstein-Uhlenbeck semigroups with jump". In: Potential Anal. 36.2 (2012), pp. 301315.

[Roc97] R. T. Rockafellar. Convex analysis. Vol. 11. Princeton University Press, 1997.

[San15] F. Santambrogio. Optimal transport for applied mathematicians. Vol. 87. Progress in Nonlinear Differential Equations and their Applications. Calculus of variations, PDEs, and modeling. Birkhäuser/Springer, Cham, 2015, pp. xxvii+353.

[ERS09] A. Es-Sarhir, M.-K. von Renesse, and M. Scheutzow. "Harnack inequality for functional SDEs with bounded memory". In: Electron. Commun. Probab. 14 (2009), pp. 560-565.

[Sha13] J. Shao. "Harnack inequalities and heat kernel estimates for SDEs with singular drifts". In: Bull. Sci. Math. 137.5 (2013), pp. 589-601.

[Vaa98] A. W. van der Vaart. Asymptotic statistics. Vol. 3. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge, 1998, pp. xvi+443.

[VH14] T. Van Erven and P. Harremos. "Rényi divergence and Kullback-Leibler divergence". In: IEEE Transactions on Information Theory 60.7 (2014), pp. 3797-3820.

[VW19] S. Vempala and A. Wibisono. "Rapid convergence of the unadjusted Langevin algorithm: isoperimetry suffices". In: Advances in Neural Information Processing Systems 32. 2019, pp. 8092-8104.

[Vil03] C. Villani. Topics in optimal transportation. Vol. 58. Graduate Studies in Mathematics. American Mathematical Society, Providence, RI, 2003, pp. xvi+370.

[Vi109] C. Villani. Optimal transport: old and new. Vol. 338. Grundlehren der Mathematischen Wissenschaften. Springer-Verlag, Berlin, 2009, p. 973.

[Wan97] F.-Y. Wang. "Logarithmic Sobolev inequalities on noncompact Riemannian manifolds". In: Probab. Theory Related Fields 109.3 (1997), pp. 417-424.

[Wan99] F.-Y. Wang. "Harnack inequalities for log-Sobolev functions and estimates of log-Sobolev constants". In: Ann. Probab. 27.2 (1999), pp. 653-663.

[Wan01] F.-Y. Wang. "Logarithmic Sobolev inequalities: conditions and counterexamples". In: $J$. Operator Theory 46.1 (2001), pp. 183-197.

[Wan02] F.-Y. Wang. "Functional inequalities and spectrum estimates: the infinite measure case". In: J. Funct. Anal. 194.2 (2002), pp. 288-310.

[Wan06] F.-Y. Wang. "Dimension-free Harnack inequality and its applications". In: Front. Math. China 1.1 (2006), pp. 53-72.

[Wan07] F.-Y. Wang. "Harnack inequality and applications for stochastic generalized porous media equations". In: Ann. Probab. 35.4 (2007), pp. 1333-1350.

[Wan10] F.-Y. Wang. "Harnack inequalities on manifolds with boundary and applications". In: $J$. Math. Pures Appl. (9) 94.3 (2010), pp. 304-321.

[Wan11a] F.-Y. Wang. "Coupling for Ornstein-Uhlenbeck processes with jumps". In: Bernoulli 17.4 (2011), pp. $1136-1158$.

[Wan11b] F.-Y. Wang. "Harnack inequality for SDE with multiplicative noise and extension to Neumann semigroup on nonconvex manifolds". In: Ann. Probab. 39.4 (2011), pp. 14491467.

[Wan12] F.-Y. Wang. "Coupling and applications". In: Stochastic analysis and applications to finance. Vol. 13. Interdiscip. Math. Sci. World Sci. Publ., Hackensack, NJ, 2012, pp. 411424.

[Wan13] F.-Y. Wang. Harnack inequalities for stochastic partial differential equations. SpringerBriefs in Mathematics. Springer, New York, 2013, pp. x+125.

[Wan14a] F.-Y. Wang. Analysis for diffusion processes on Riemannian manifolds. Vol. 18. Advanced Series on Statistical Science \& Applied Probability. World Scientific Publishing Co. Pte. Ltd., Hackensack, NJ, 2014, pp. xii+379.

[Wan14b] F.-Y. Wang. "Integration by parts formula and shift Harnack inequality for stochastic equations". In: Ann. Probab. 42.3 (2014), pp. 994-1019.

[Wan17] F.-Y. Wang. "Hypercontractivity and applications for stochastic Hamiltonian systems". In: J. Funct. Anal. 272.12 (2017), pp. 5360-5383.

[Wan18] F.-Y. Wang. "Distribution dependent SDEs for Landau type equations". In: Stochastic Process. Appl. 128.2 (2018), pp. 595-621.

[WY11] F.-Y. Wang and C. Yuan. "Harnack inequalities for functional SDEs with multiplicative noise and applications". In: Stochastic Process. Appl. 121.11 (2011), pp. 2692-2710.

[WZ13] F.-Y. Wang and X.-C. Zhang. "Derivative formula and applications for degenerate diffusion semigroups". In: J. Math. Pures Appl. (9) 99.6 (2013), pp. 726-740.

[WZ15] L. Wang and X. Zhang. "Harnack inequalities for SDEs driven by cylindrical $\alpha$-stable processes". In: Potential Anal. 42.3 (2015), pp. 657-669.

[ZY21] S.-Q. Zhang and C. Yuan. "A Zvonkin's transformation for stochastic differential equations with singular drift and applications". In: J. Differential Equations 297 (2021), pp. $277-319$.

[Zha10] T. Zhang. "White noise driven SPDEs with reflection: strong Feller properties and Harnack inequalities". In: Potential Anal. 33.2 (2010), pp. 137-151.
