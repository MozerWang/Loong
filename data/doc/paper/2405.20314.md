# S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs 

Wei Zhong<br>wei . zhong@lge.com

Manasa Bharadwaj<br>manasa.bharadwaj@lge.com

LG Electronics, Toronto AI Lab


#### Abstract

Speculative decoding (SD) has attracted a significant amount of research attention due to the substantial speedup it can achieve for LLM inference. However, despite the high speedups they offer, speculative decoding methods often achieve optimal performance on high-end devices or with a substantial GPU memory overhead. Given limited memory and the necessity of quantization, a high-performing model on a high-end GPU can slow down by up to 7 times. To this end, we propose Skippy Simultaneous Speculative Decoding (or S3D), a cost-effective self-speculative SD method based on simultaneous multi-token decoding and mid-layer skipping. When compared against recent effective open-source SD systems, our method has achieved one of the top performance-memory ratios while requiring minimal architecture changes and training data. Leveraging our memory efficiency, we created a smaller yet more effective SD model based on Phi-3. It is 1.4 to 2 times faster than the quantized EAGLE model and operates in half-precision while using less VRAM.


## 1 Introduction

Speculative decoding (SD) (Stern et al., 2018; Zhang et al., 2024; Xia et al., 2024) can accelerate LLM inference without sacrificing the quality. As a result, it is becoming one of the most common optimization techniques in LLMs. At a high level, typical speculative decoding (SD) works by drafting tokens at a relatively faster speed, and then verifying the guessed tokens at the end of an iteration using a full forward pass. The speedup is based on the assumption that the accepted tokens in one forward pass during the verification step will offset the cost of the drafting steps.

However, greater speedups are not always free. On one hand, some popular SD systems (Cai et al.,
![](https://cdn.mathpix.com/cropped/2024_06_04_d26bf3b0c67ef08b2180g-01.jpg?height=612&width=734&top_left_y=770&top_left_x=1069)

Figure 1: Training efficiency, inference efficiency per memory unit, and load-time VRAM evaluated for different models on MT-Bench. From left to right: The most recent open-source SD systems ordered by release dates. All systems use 7B target models with 8 -bit quantization. Our model (S3D) stands out in both training efficiency and memory-speed trade-offs.

2024; Li et al., 2024; Chen et al., 2024) add a considerable amount of memory, e.g., due to the extra modules or a large token trees used for drafting. When models are deployed at scale, even a minor memory overhead can largely increase the cost of inference, given the current high cost of using HBM VRAM in inference. On the other hand, high-performing SD can achieve remarkable speedups using a large model (Zhang et al., 2023a; Zhao et al., 2024; Yang et al., 2024) or on high-end GPUs (Zhang et al., 2023a; Chen et al., 2024; Kou et al., 2024; Elhoushi et al., 2024). However, we notice that these speedups become limited or even start underperforming when a smaller model or low-memory device is used where quantization is necessary. Surprisingly, the state-of-the-art opensource SD model (in speedups) may slow down by up to 7 times when applying quantization under constrained GPU memory, highlighting the signifi-
cant overheads from quantization (Lin et al., 2024). In such cases, we question the cost-effectiveness of existing SD methods, even if they show greater speedups on high-end GPUs.

Although recently Chen et al. (2024) designed a GPU-agnostic optimization by pre-profiling the GPU times for the draft and verify stages, their approach relies on the hard assumption of fixed acceptance rates among different levels of the draft token tree, making it less applicable to various SD methods. Additionally, the optimal trees have fewer differences for smaller models or slower GPUs, limiting their potential on low-end devices.

In this work, we introduce Skippy Simultaneous Speculative Decoding (or S3D) to achive fast inference, low VRAM costs, and high training efficiency. Our key contributions are listed below:

Effective Self-speculative SD: We propose a simple and memory-saving self-speculative decoding scheme named $S 3 D$ for low-memory GPUs. Our scheme features mid-layer skipping and simultaneous multi-token predictions, offering no added VRAM costs and high training efficiency. Compared to Zhang et al. (2023a), S3D overcomes the limited speedups in smaller models.

Optimal hyper-parameters: Instead of relying on statistical optimization, we formalize the relationship between the number of skipped layers and speedup in self-speculative decoding, as previously studied in Zhang et al. (2023a) empirically. Based on our formalization, we can also verify the optimal number of token predictors, as empirically observed by Gloeckle et al. (2024).

Optimal speed-memory ratio: Our SD method demonstrates optimal performance-memory ratios among recent open-source SD models. By exploiting the memory efficiency, we can avoid the significant quantization overheads under certain VRAM constraints and outperform the previous fastest SD method, i.e., EAGLE (Li et al., 2024), under 8-bit quantization by up to $3.9 \mathrm{x}$ in speedups on $\mathrm{A} 10 \mathrm{G}$ GPU. Moreover, by switching to a smaller target model, we have created a more effective SD model based on Phi-3, which decodes 1.4 to 2 times faster than EAGLE on A10G while using less VRAM.

## 2 Related Work

Early work in speculative decoding (SD) using Transformers (Stern et al., 2018; Sun et al., 2021; Xia et al., 2023) focused on in-domain tasks such as translation and grammar error correction, where significant speedups are easily achieved. These methods are characterized by single-branch speculation using additional modules (Stern et al., 2018) or an independent draft model (Xia et al., 2023). By using speculative sampling (Chen et al., 2023), SD can also sample tokens from target model distribution (Leviathan et al., 2023). In contrast, our work targets general domain tasks and focuses on greedy ${ }^{1}$ and non-batching decoding via simultaneous multi-token prediction. We save memory and gain training efficiency through layer-skipping.

### 2.1 Multi-Token Predictions

Since Stern et al. (2018), predicting the next $k$ tokens simultaneously has proven effective, but it requires adding $k$ feed-forward decoder layers from the last encoder state. Cai et al. (2024) popularized this idea using Medusa heads, additionally predicting multiple token branches using tree attention (Miao et al., 2024; Spector and Re, 2023).

In the SpecDec method (Xia et al., 2023), multitoken prediction is done by unmasking future tokens from multiple decoder heads attending to different encoder states, utilizing distinct attention queries for predicting different tokens. However, SpecDec requires full model fine-tuning for all layers as the decoder-only target model has not been pretrained on multi-token unmasking tasks. More recently, Bhendawade et al. (2024) predict multiple tokens by adding streaming embeddings initialized from upper layers, with the token tree reduced by early exiting. However, they made major changes to the original architecture, and their speedups are considered limited.

Multi-token prediction can also be implemented auto-regressively (Yang et al., 2024; Li et al., 2024; Ankner et al., 2024), which takes multiple steps to predict the next draft token conditioned on previously drafted tokens in one iteration. To mitigate the substantial overheads incurred by multistep drafting within a single iteration, the draft overhead should be minimal while ensuring it retains the capability to generate acceptable tokens. In the case of EAGLE (Li et al., 2024), this is achieved by efficiently concatenating the target model's high-level features with the embeddings or hidden states of the next tokens for regression via an additional layer of Transformer decoder.[^0]

Another line of work to generate multiple draft tokens in parallel is based on Jacobi iteration methods, treating auto-regressive decoding in LLM as a non-linear system of equations, or Jacobi decoding (Song et al., 2021; Santilli et al., 2023). In practice, however, an LLM may obtain marginal speedups from Jacobi decoding as it can rarely produce an accepted token if a previous token in the trajectory is predicted incorrectly. Lookahead decoding (Fu et al., 2024) attempts to address this issue by introducing memory costs and caching n-gram tokens from previous Jacobi trajectories. Inspired by the Consistency Model (Song et al., 2023), CLLMs (Kou et al., 2024) additionally train their target models to minimize the distances between Jacobi trajectories and the fixed point, leading to faster convergence and thus greater speedups. Compared to regular SD methods, Jacobi decoding does not have a separate draft phase.

### 2.2 Layer Skipping

Layer skipping is a type of structured pruning technique (Anwar et al., 2017; Louizos et al., 2018; Xia et al., 2022) that reduces a model by only using a subset of its layers. Structured pruning is particularly intriguing for LLM optimizations due to its compatibility with GPU acceleration. This is because it enables immediate gains in memory and compute by discarding substructures entirely (Ouderaa et al., 2024).

Various layer skipping schemes explored for Transformer models are discussed next. Early Exiting (Dehghani et al., 2019; Teerapittayanon et al., 2017) utilizes early layers and skips the rest. LayerDrop (Fan et al., 2019) randomly drops layers during training and skips layers during inference. Lagunas et al. (2021); Ouderaa et al. (2024) have identified sub-layer structures (e.g., attention heads or weight blocks) to be pruned during training. Sajjad et al. (2022) propose symmetric dropping of complete top and bottom layers, based on the observation that middle layers of a Transformer are less critical. This observation has been validated by Ma et al. (2023); Wu and Tu (2024) on larger LLMs and for KV-cache as well. Recently, Raposo et al. (2024) have trained additional parameters to dynamically skip layers.

Combining layer skipping with SD leads to an intriguing way to save memory, known as selfspeculative decoding (Bae et al., 2023; Zhang et al., 2023a; Liu et al., 2024a; Elhoushi et al., 2024), where a static or adaptive number of layers of the target model are used for drafting tokens. These approaches mitigate the common memory overheads of SD by incorporating minimal or no extra modules for the draft stage. Specifically, they either entail no additional training (Zhang et al., 2023a) or only necessitate training for learning an adaptive early exit threshold (Bae et al., 2023; Elhoushi et al., 2024) with a potential trade-off in quality. In Elhoushi et al. (2024), they reuse KVcache from draft stages, reducing the computation needed for the remaining layers in the verify stage. However, their approach involves training LayerDrop (Fan et al., 2019) and requires complex early exit thresholds during inference.

A concurrent work by Gloeckle et al. (2024) combines self-speculative decoding with multitoken predictions. However, their approach trains additional independent heads comprising entire Transformer layers, potentially adding more memory cost compared to EAGLE (Li et al., 2024). In contrast, our SD scheme imposes no extra model load cost and has minimal training requirements through applying mid-layer skipping.

## 3 Preliminaries

Given a Transformer decoder-only model (Radford et al., 2018) $M_{p}$, its generated next-token distribution $p\left(t_{\leq i}\right)$ given the current context tokens $t_{\leq i}=t_{1}, t_{2}, \ldots, t_{i}$ can be expressed in terms of Transformer layers. For layer $\ell=1,2, \ldots, L$,

$$
\begin{align*}
h_{i}^{(0)} & =\operatorname{Emb}\left(t_{i}\right)  \tag{1}\\
h_{i}^{(\ell)} & =T^{(\ell)}\left(h_{\leq i}^{(\ell-1)}, \operatorname{Pos}_{\leq i}\right)  \tag{2}\\
t_{i+1} & \sim p\left(t_{\leq i}\right)=\operatorname{LM}-\operatorname{Head}\left(h_{i}^{(L)}\right) \tag{3}
\end{align*}
$$

where Emb represents the embedding transformation, and $T^{(\ell)}$ denotes the Transformer layer at level $l$, which receives the context hidden states from the previous layer $h_{\leq i}^{(\ell-1)}$, associated with their position information $\mathrm{Pos}_{\leq i}$. The LM-Head maps the hidden space to a vocabulary distribution $p$ for the next token sampling.

The decoder-only language model is typically trained using the next-token prediction task, where training involves employing cross entropy loss across tokens in parallel. Given a sample of sequential tokens $t_{i}, i=1,2, \ldots, N$, the loss is

$$
\begin{equation*}
\mathcal{L}=\frac{1}{N-1} \sum_{i=1}^{N-1}-\log p\left(t_{\leq i}\right)_{t_{i+1}} \tag{4}
\end{equation*}
$$

During a SD iteration, a more efficient draft model $M_{q}$ is often used to predict the next $\gamma$ token(s) from the target model $M_{p}$ through sampling $t_{i+j+1} \sim q\left(t_{\leq i+j}\right)$ where $j=0,1,2, \ldots, \gamma-1$. To produce tokens as if they were sampled from the target distribution, Leviathan et al. (2023) show that we can verify drafted tokens by comparing $p\left(t_{i+j}\right)$ with $q\left(t_{i+j}\right)$ successively, and accept each token with a probability of $\min \left(1, \frac{p\left(t_{i+j}\right)}{q\left(t_{i+j}\right)}\right)$. Upon completion, one more last token $t_{i+\gamma+1}$ can be sampled from the target distribution $p$. On rejection, sampling is done from a normalized distribution of $\max (0, p-q)$. In greedy decoding, this process is equivalent to accepting only the matched tokens produced from $p$ and $q$.

## 4 S3D

We propose a self-speculative SD scheme called Skippy Simultaneous Speculative Decoding (or S3D). In S3D, the draft model $M_{q}$ uses partial layers from target model $M_{p}$.

To adhere to the Transformer decoder architecture and circumvent the need for auxiliary modules, we opt to emulate the Masked Language Modeling (MLM) task commonly employed in Transformer encoder training. This involves inserting a special mask token, denoted as $\langle M\rangle$, into future inputs to predict the next $\gamma$ tokens concurrently. Specifically,

$$
\begin{equation*}
t_{i+1}, \ldots, t_{i+\gamma} \sim q(t_{\leq i}, \underbrace{<\mathrm{M}>, \ldots,<\mathrm{M}>}_{\gamma-1}) \tag{5}
\end{equation*}
$$

where the draft model $M_{q}$ uses all previous hidden states of the target model $h_{\leq i}^{(\ell)}, \ell=1,2, \ldots, L$ :

$$
\begin{equation*}
h_{i}^{(\ell)}=T^{(\ell)}\left(h_{\leq i}^{(\ell-1)}, \operatorname{Pos}_{\leq i}\right) \tag{6}
\end{equation*}
$$

Different from Xia et al. (2023), the simultaneously generated tokens at $j=i+1, i+2, \ldots, i+\gamma$ require only propagating through lower and top layers, skipping middle $m$-th to $n$-th layers of the target model:

$$
\begin{align*}
h_{j}^{(n-1)} & =h_{j}^{(m)}  \tag{7}\\
h_{j}^{\left(\ell^{\prime}\right)} & =T^{\left(\ell^{\prime}\right)}\left(h_{\leq j}^{\left(\ell^{\prime}-1\right)}, \operatorname{Pos}_{\leq j}\right) \tag{8}
\end{align*}
$$

where non-skipping layers $\ell^{\prime}=1,2, \ldots, m, n, n+$ $1, \ldots, L$. Unlike Zhang et al. (2023a), a nonskipping layer $\ell$ is able to utilize previous states of the target model, i.e., $h_{\leq i}^{(\ell-1)}$. Furthermore, in contrast to early exiting in Zhang et al. (2023a), the current draft states from top layers are kept for decoding. Additionally, we do not necessarily skip lower layers due to the adjustments required in lower-level representations for skipped middle layers, a notion explored similarly by Ma et al. (2023) and $\mathrm{Wu}$ and $\mathrm{Tu}$ (2024). We will further justify this skipping scheme in Section 5.3.

Training: Our training objective is to accurately uncover masked tokens while preserving the original next-token prediction capability. To this end, we train the draft model to decode both the next token right after $i$ and its following masked tokens. Assume the masked tokens are located at $i+1, i+2, \ldots, i+\gamma-1$, our training loss is

$$
\begin{equation*}
\mathcal{L}^{(S 3 D)}=\frac{1}{|D|} \sum_{j \in D}-\log q\left(t_{\leq j}\right)_{t_{j+1}} \tag{9}
\end{equation*}
$$

where decoding set $D=\{i, i+1, \ldots, i+\gamma-1\}$.

During training, we freeze the skipped layers to preserve the target model distribution. Instead of predicting next tokens sequentially, we assign masked tokens randomly so that training samples can be processed in one batch, utilizing the parallelism of Transformer. An illustration of our modeling is shown in Figure 2.

Predicting speedup: Given target ratio $\beta \in[0,1]$, which represents the ratio of target model parameters that the draft model uses during decoding, the acceptance rate $\alpha$ of the frist drafted token should be a function of $\beta$. Naturally, in the selfspeculative case, $\alpha(1)=1$ and $\alpha \rightarrow 0$ when $\beta \rightarrow 0$.

In this work, we have hypothesized a function to estimate draft token acceptance rate as a function of model size (parameterized by $U$ ):

$$
\begin{equation*}
\alpha(\beta ; U)=\frac{1-U^{\beta}}{1-U} \tag{10}
\end{equation*}
$$

We will show in Section 5.3 that the above function aligns well with empirical observations.

In multi-token predictions, assume the true acceptance rate at the $k$-th draft token, i.e., $\alpha_{k}(\beta)$, is discounted by $k$ in a discrete function (which we may readily estimate from empirical data). Following the notation in Li et al. (2024), the expected newly generated tokens $\tau$ is ${ }^{2}$

$$
\begin{equation*}
\tau(\gamma, \beta)=\sum_{n=1}^{\gamma+1} n \cdot \prod_{k=1}^{n-1} \alpha_{k}(\beta) \cdot z_{n}(\beta) \tag{11}
\end{equation*}
$$[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_d26bf3b0c67ef08b2180g-05.jpg?height=606&width=1290&top_left_y=228&top_left_x=383)

Figure 2: An illustration of S3D based on simultaneous predictions of the last $\gamma$ tokens $(\gamma=2)$. A mask token $\langle M\rangle$ is added into vocabulary prior to training, and a partial model is trained to predict the next tokens simultaneously. Tree attention is adopted to verify multiple branches of predictions give top candidates of the $k$-th draft token. Unlike other self-speculative decoding methods based on fully-skipped layers, we only skip the middle layers on top of the draft tokens so that the draft model can access high-level features from top layers as well as the previous states verified by the complete target model.

where the shorthand notation $z_{n}(\beta)=1-\alpha_{n}(\beta)$ if $n \neq \gamma+1$ and 1 otherwise.

When the number of drafting tokens is not significant (which commonly happens on low-end devices), it is reasonable to assume the time cost for a full-model forward pass to be a constant $C$. We also assume there is a fixed overhead $H_{0}$ for each iteration, proportionally to $C$, i.e., $H_{0}=\delta \cdot C$, then deriving the decoding speed by taking out $C$ leaves us the Improvement Factor (Leviathan et al., 2023) for self-speculative decoding, i.e.,

$$
\begin{equation*}
I F(\gamma, \beta)=\frac{\tau(\gamma, \beta)}{\delta+\beta+1} \tag{12}
\end{equation*}
$$

assuming that the forward time for a partial model scales linearly with the number of its parameters.

Note that Eq. 12 represents a different improvement factor compared to the one in auto-regressive drafting schemes (Leviathan et al., 2023), where $\gamma$ predictions are performed sequentially.

## 5 Experiments

### 5.1 Experimental Setup

Datasets We consider datasets commonly used in SD evaluations, including MT-Bench (Zheng et al., 2023) for multi-turn conversation, HumanEval (Chen et al., 2021) for coding, and CNNDaily (Hermann et al., 2015) for summarization. For CNN-Daily, we only use the 1,000 samples similar to Zhang et al. (2023a), while the complete datasets are used for the others. For MTBench, we use Gemini Pro (Anil et al., 2023) for effectiveness evaluation. We report accuracy scores for Human-Eval and Rouge-1 and Rouge-L scores (Lin, 2004) for CNN-Daily.

Baselines A fair comparison is conducted by running all systems on identical samples and hardware using a uniform evaluation framework. To this end, our model is compared to open-source SD systems including: Self-Spec (Zhang et al., 2023a), Medusa (Cai et al., 2024), EAGLE (Li et al., 2024), MCSD (Yang et al., 2024), Sequoia (Chen et al., 2024), Ouroboros (Zhao et al., 2024), and CLLMs (Kou et al., 2024). SelfSpec is a training-free self-speculative method that predicts the next single token via layer skipping. Medusa is a popular SD method that adds parallel decoder heads to predict multiple next tokens. EAGLE concatenates target model late-layer hidden states with the last token embedding to predict the next 5 tokens auto-regressively via an additional Transfomer decoder layer. And recent work MCSD, Sequoia, and Ouroboros generate draft tokens through a separate draft model. In particular, Sequoia constructs an optimal draft token tree from profiling the underlying hardware. In these three systems, we adopt the $68 \mathrm{M}$ JackFram LLaMA (Miao et al., 2024) as the draft model, which is also the default and most efficient option for their LLaMA target models. Lastly, CLLMs is considered as the latest development in the direction of Jacobi or Lookahead decoding (Santilli et al., 2023; Fu et al., 2024).

Table 1: The cost-effectiveness comparisons on A10G GPU considering peak VRAM costs. All models are 8-bit quantized and are based on the 7B LLaMA-v2 target model except mentioned otherwise in parentheses. The largest 3 numbers in each column are highlighted in italics or bold. "Peak" denotes the peak VRAM usage in GiB. The overall (averaged) results count for both $M$ speed and the relative effectiveness metrics compared to the baseline.

| Model \Metric | MT-Bench |  |  |  | Human-Eval |  |  |  | CNN-Daily |  |  |  |  | Overall |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Peak $\downarrow$ | Tok $/ \mathrm{s}$ | $M$ | Score | Peak $\downarrow$ | Tok / s | $M$ | Acc. \% | Peak $\downarrow$ | Tok $/ \mathrm{s}$ | $M$ | $\mathbf{R}-\mathbf{1}$ | $\mathbf{R}-\mathbf{L}$ | $M$ | Eff. |
| Baseline | 8.53 | 7.02 | 1.00 | 7.05 | 8.43 | 7.02 | 1.00 | 671 | 8.96 | 6.53 | 1.00 | 0.19 | 0.13 | 1.00 | 1.00 |
| Self-: | 7.77 | 5.00 | 0.78 | 7. | 7.46 | 4.89 | 0.79 | 6 | 8.09 | 5.01 | 0.85 | 0.19 | 14 | 0.81 | 0.97 |
|  | 9.09 | 9.27 | 1.24 | 4. | 8.94 | 10.69 | 1.44 | 7.93 | 9.36 | 7.62 | 1.12 | 0.24 | 14 | 1.26 | 1.03 |
| EAGL | 9.58 | 13.03 | 1.65 | 6.98 | 9.45 | 15.22 | 1.93 | 5.49 | 9.86 | 12.43 | 1.73 | 0.19 | 0.13 | 1.77 | 0.94 |
| MCS | 7.76 | 7.72 | 1.21 | 6.79 | 7.40 | 7.77 | 1.26 | 9.7 | 8.10 | 6.74 | 1.14 | 0.20 | 0.14 | 1.21 | 1.16 |
| Sequoia | 8.44 | 8.64 | 1.24 | 6.46 | 8.35 | 9.01 | 1.30 | 3.05 | 8.57 | 7.99 | 1.28 | 0.18 | 0.12 | 1.27 | 0.77 |
|  | 7.95 | 5.47 | 0.84 | 7.0 | 7.61 | 5.83 | 0.92 | 8.54 | 8.30 | 4.91 | 0.81 | 0.18 | 0.13 | 0.86 | 1.09 |
| CLLM | 7.51 | 11.75 | 1.90 | 5.31 | 7.37 | 16.29 | 2.66 | 3.66 | 7.53 | 8.06 | 1.47 | 0.20 | 0.14 | 2.01 | 0.79 |
| Our $-x_{1}$ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| S3D | 7.79 | 12.39 | 1.93 | 5.68 | 7.60 | 13.85 | 2.19 | 6.71 | 8.80 | 9.58 | 1.49 | 0.28 | 0.19 | 1.87 | 1.09 |
| S3D (Phi-3, fp16) | 8.14 | 25.31 | 3.78 | 7.04 | 7.92 | 28.13 | 4.27 | 20.12 | 8.87 | 17.35 | 2.69 | 0.25 | 0.18 | 3.58 | 1.77 |

Implementation details All implementations use eager-mode decoding based on Huggingface Transformers (Wolf et al., 2020) and we adopt the native BNB 8 -bit quantization (Dettmers et al., 2022) for quantized models. We configure each system with greedy decoding and keep the other SD configurations default for different systems. A non-speculative implementation is used as the common baseline to calculate speedups and relative memory costs as it shares the same instructional LLaMA-v2 (Touvron et al., 2023) backbone for most of our evaluated systems.

We mostly consider the $7 \mathrm{~B}$ target model size. In exception to this, we also train a 3.8B Phi-3 Mini (Abdin et al., 2024) target model and a 13B LLaMA-v2 target model to demonstrate the generalization ability of our method. Unless specified otherwise, our S3D implementations use the optimal hyper-parameters suggested by Section 5.3.

Cost effectiveness metric We propose a memorynormalized speed metric $M$, which divides a speedup by the relative memory usage compared to the baseline model:

$$
\begin{equation*}
M=\frac{v_{1}}{v_{0}} / \frac{m_{1}}{m_{0}}=\frac{v_{1}}{m_{1}} / \frac{v_{0}}{m_{0}} \tag{13}
\end{equation*}
$$

where $v_{0}$ and $m_{0}$ are the generation speed and memory consumption of the baseline model, and $v_{1}$ and $m_{1}$ are the generation speed and memory consumption of the evaluating model. This metric quantifies the generation speedup per memory unit, ensuring a fair and memory-aware comparison for target models of the same size.

Training Similar to Medusa, EAGLE, CLLMs, et al.(Cai et al., 2024; Li et al., 2024; Kou et al., 2024), we train our models on the ShareGPT dataset. All training is conducted using bf16 and
FlashAttention-2 (Dao, 2023) with a batch size of 64 for one epoch on A10G GPUs. Please refer to Appendix A for detailed training descriptions.

### 5.2 Main Results

Initially, we discuss the cost-effectiveness of our model. In Table 1 and 2, we observe that S3D models and CLLM exhibit the highest overall $M$ speeds among the evaluated systems, which remains consistent across different GPUs.

Importantly, models producing high speedups are not necessarily the most cost-effective, as seen in the case of Medusa and EAGLE, where the cost of extra draft module(s) must be considered. We also observed discrepancies in the effectiveness scores of models when using the original target model for token verification, likely due to implementation issues or numerical errors. However, our model generally maintains baseline effectiveness and achieves the highest overall effectiveness among systems requiring target model training (i.e., Self-Spec., CLLM, and ours). By using layer adapters like LoRA (Hu et al., 2021), we can easily enable lossless decoding at the expense of efficiency costs (see analysis in Appendix B).

Interestingly, the vanilla self-speculative decoding method, i.e., Self-Spec, underperforms the baseline in terms of speed for the 7B target model. This highlights the limitation of naive selfspeculative decoding $(\gamma=1)$ in smaller models, where the partial draft model becomes further constrained and unable to propose good draft tokens. In contrast, we alleviate this issue by allowing the draft model to attend to previous target model states and training the model to predict multiple tokens $(\gamma>1)$, thereby enhancing the effectiveness of smaller self-speculative models.

![](https://cdn.mathpix.com/cropped/2024_06_04_d26bf3b0c67ef08b2180g-07.jpg?height=551&width=674&top_left_y=250&top_left_x=288)

Figure 3: Speed comparison between ours (S3D) and EAGLE on different GPU devices (MT-Bench samples, 7B LLaMA target model). The dashed bars represents the full speed potentials of the EAGLE model without memory restrictions. However, when constrained with a VRAM limit of $16 \mathrm{GiB}$, the quantized EAGLE model (indicated by red bars) suffers from severe speed degradation, highlighting the significant overheads associated with quantization.

Admittedly, the LLaMA-based S3D model ties closely to EAGLE and underperforms CLLM in overall $M$ speed, primarily due to the high speedups and optimal memory efficiency achieved by EAGLE and CLLM, respectively. However, as shown in Table 1 and 2, we are able to exploit our memory efficiency and outperform EAGLE in both efficiency and effectiveness while using less amount of VRAM by switching to a non-quantized Phi-3 target model. ${ }^{3}$ Even without switching to a different target model, we demonstrate in Figure 3 that our LLaMA-based S3D model can operate in half-precision within a VRAM limit of $16 \mathrm{GiB}$, and outperform EAGLE by up to 3.9 times when EAGLE needs to be quantized. This underscores the critical importance of memory efficiency.

On the other hand, we find that the training objectives of CLLM may encourage repeating patterns in its outputs, leading to degraded effectiveness scores, as seen in Table 1 and additional case studies in Appendix D. In contrast, our model can preserve effectiveness scores more robustly while achieving the optimal speed-memory ratios.[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_d26bf3b0c67ef08b2180g-07.jpg?height=520&width=785&top_left_y=248&top_left_x=1024)

Figure 4: The overall acceptance rates and individual acceptance rates at different drafting depths (w/ only a single branch of future tokens). L, LMH, and Emb stand for regular layer, LM heads, and the embedding layer, respectively. Skipping the middle layers symmetrically has shown better acceptance rates in general. Note that we distinguish embedding layer and $1 \mathrm{~m}$ _head here although in practice they may have tied weights.

### 5.3 Optimal Hyper-Parameters

We first study the optimal layer skipping choices. To this end, we have empirically explored three different schemes by skipping a fixed number of original layers of LLaMA 7B target model: (1) Skipping asymmetrically from the middle, including early existing and using only late layers while skipping all early layers. (2) Skipping symmetric layers from the middle, i.e., layer $5 \rightarrow 26$ or skipping the middle 20 layers. (3) Alternate evenly between skipping and non-skipping layers.

Looking at Figure 4 (and Appendix Figure 9 for training efficiencies), skipping symmetrical layers from the middle performs best, achieving higher overall acceptance rates and optimal training efficiency. In contrast, skipping from layer 11 to the top (LM-head) layer and skipping from the bottom (embedding) layer to layer 20 have the worst performance, highlighting the importance of both early and late layers. Lastly, skipping symmetric middle layers or alternating every 3 layers has similar training efficiencies.

To answer what is the optimal number of layers and what is the best number of tokens to be predicted in parallel, we train different number of layers $(\beta)$ skipped symmetrically from the middle layer, each trained model is evaluated for different $\gamma$ values up to 5 . We run different models on MT-Bench for 50 samples, and linearly interpolate the acceptance rate discount function $\alpha_{k}(\beta)$ (detailed in Appendix E). As summarized
![](https://cdn.mathpix.com/cropped/2024_06_04_d26bf3b0c67ef08b2180g-08.jpg?height=730&width=622&top_left_y=246&top_left_x=314)

Figure 5: Upper: The predicted (in dashes) and sampled acceptance rates (interpolated orange dots) of various draft model sizes ( $\beta$ ). Lower: The predicted (in curves) and sampled (in dots) speeds of different draft model sizes and different number of guesses $(\gamma)$. Our prediction curves justify the optimality of using around half the number of parameters and $\gamma=4$, as observed individually and respectively in Zhang et al. (2023a) and Gloeckle et al. (2024).

in Figure 5, our proposed formula for predicting self-speculative acceptance rates in Eq. 10 mostly matches with the empirical results except for the lowest $\beta$ value, and this outlier may be explained by the less predictability in training a small partial model $(<1.5 \mathrm{~B})$. Addtionally, the speedup formula in Eq. 12 successfully predicts both the trend and the sweet spot in speedups. Higher $\gamma$ values align less with the prediction because the acceptance rates for far-future tokens have higher uncertainty and variance as reflected by Figure 4.

In addition to the findings from Zhang et al. (2023a); Gloeckle et al. (2024), we have unified multi-token predictions with layer skipping. Our prediction in Eq. 12 has also justified their findings that the optimal speed is achieved by skipping around half of the layers $(\beta \rightarrow 0.5)$ and the optimal $\gamma$ is 4 (as shown in Figure 5, a higher $\gamma$ results in an almost diminished speedup, offset by fewer accepted tokens).

### 5.4 Training Efficiency

In addition to its cost-effectiveness, S3D also demonstrates greater training efficiency compared to other effective SD models (see Figure 1). So we hypothesize that the self-speculative decod-

![](https://cdn.mathpix.com/cropped/2024_06_04_d26bf3b0c67ef08b2180g-08.jpg?height=503&width=623&top_left_y=294&top_left_x=1116)

Figure 6: Training loss comparisons between EAGLE's classification loss (Li et al., 2024) and our (S3D) training loss in Eq. 9. EAGLE requires training an extra layer of Transformer with additional linear mappings.

ing method used in S3D inherently lowers training costs, as the training task leverages the existing model weights. In light of this, we compare and train both the S3D and EAGLE models using 20,000 data rows (the original EAGLE was trained on 68,000 data rows).

As shown in Figure 6, S3D consistently exhibits lower training losses, even when considering more layers and including far-future tokens, which are generally challenging to predict. In the worstperforming case, where early exiting or early layer skipping is used, our loss values remain mostly below those of EAGLE. Additionally, S3D training loss shows less variance and converges more steadily.

## 6 Conclusion

We have proposed S3D, a self-speculative SD method based on simultaneous multi-token predictions and mid-layer skipping. S3D demonstrates one of the best cost-effectiveness among recent open SD systems, while also exhibiting high training efficiency and maintaining the effectiveness of the original model. We have also verified the optimal hyper-parameters for our proposed method in a principled manner, without requiring any black-box optimizations beforehand. By leveraging memory efficiency, S3D can avoid quantization and surpass the speed of quantized EAGLE when a 16 GiB VRAM limit is imposed. Additionally, S3D, based on the smaller Phi-3 target model, decodes 1.4 to 2 times faster than quantized EAGLE on A10G, with reduced VRAM usage and better effectiveness.

## Acknowledgments

We extend our gratitude to Touqir Sajed for his suggestions and brainstorming throughout the preparation of this paper. Additionally, we thank Kevin Ferreira, Yipeng (Penny) Ji, and Paria Nejat for their support in arranging computational resources.

## References

Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 technical report: A highly capable language model locally on your phone. Preprint, arXiv:2404.14219.

Rohan Anil, Sebastian Borgeaud, Yonghui Wu, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: A family of highly capable multimodal models.

Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan RaganKelley, and William Brandon. 2024. Hydra: Sequentially-dependent draft heads for medusa decoding. Preprint, arXiv:2402.05109.

Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. 2017. Structured pruning of deep convolutional neural networks. JETC.

Sangmin Bae, Jongwoo Ko, Hwanjun Song, and SeYoung Yun. 2023. Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding. In EMNLP.
Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, and Mahyar Najibi. 2024. Speculative streaming: Fast LLM inference without auxiliary models. Preprint, arXiv:2402.11131.

Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. 2024. Medusa: Simple LLM inference acceleration framework with multiple decoding heads. Preprint, arXiv:2401.10774.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model decoding with speculative sampling. Preprint, arXiv:2302.01318.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. Preprint, arXiv:2107.03374.

Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. 2024. Sequoia: Scalable, robust, and hardware-aware speculative decoding. Preprint, arXiv:2402.12374.

Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. Preprint, arXiv:2307.08691.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. 2019. Universal transformers. Preprint, arXiv:1807.03819.

Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Preprint, arXiv:2208.07339.

Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, and Carole-Jean Wu. 2024. LayerSkip: Enabling early exit inference and self-speculative decoding. Preprint, arXiv:2404.16710.

Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reducing transformer depth on demand with structured dropout. Preprint, arXiv:1909.11556.

Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. 2024. Break the sequential dependency of LLM inference using lookahead decoding. Preprint, $\operatorname{arXiv}: 2402.02057$.

Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. 2024. Better $\&$ faster large language models via multi-token prediction. Preprint, arXiv:2404.19737.

Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Preprint, arXiv:1506.03340.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Preprint, $\operatorname{arXiv}: 2106.09685$.

Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. 2024. CLLMs: Consistency large language models.

François Lagunas, Ella Charlaix, Victor Sanh, and Alexander M. Rush. 2021. Block pruning for faster transformers. Preprint, arXiv:2109.04838.

Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. Preprint, arXiv:2211.17192.

Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024. Eagle: Speculative sampling requires rethinking feature uncertainty. In ICML.

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. ACL.

Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. 2024. QServe: W4A8KV4 quantization and system co-design for efficient LLM serving. Preprint, $\operatorname{arXiv}: 2405.04532$.

Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, and Yunhe Wang. 2024a. Kangaroo: Lossless self-speculative decoding via double early exiting. Preprint, arXiv:2404.18911.

James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, and Tianle Cai. 2024b. Bitdelta: Your fine-tune may only be worth one bit. Preprint, arXiv:2402.10193.

Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. Preprint, arXiv:1711.05101.

Christos Louizos, Max Welling, and Diederik P. Kingma. 2018. Learning sparse neural networks through $l_{0}$ regularization. Preprint, arXiv:1712.01312.
Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. LLM-Pruner: On the structural pruning of large language models. Preprint, arXiv:2305.11627.

Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. 2024. SpecInfer: Accelerating large language model serving with tree-based speculative inference and verification. In ASPLOS.

Tycho Ouderaa, Markus Nagel, Mart van Baalen, Yuki M. Asano, and Tijmen Blankevoort. 2024. The LLM surgeon. Preprint, arXiv:2312.17244.

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. Preprint, arXiv:1910.02054.

David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. 2024. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. Preprint, arXiv:2404.02258.

Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2022. On the effect of dropping layers of pre-trained transformer models. Preprint, arXiv:2004.03844.

Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. 2023. Accelerating transformer inference for translation via parallel decoding. In $A C L$.

Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. 2023. Consistency models. Preprint, arXiv:2303.01469.

Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. 2021. Accelerating feedforward computation via parallel nonlinear equation solving. Preprint, arXiv:2002.03629.

Benjamin Spector and Chris Re. 2023. Accelerating LLM inference with staged speculative decoding. Preprint, arXiv:2308.04623.

Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018. Blockwise parallel decoding for deep autoregressive models. Preprint, arXiv:1811.03115.

Xin Sun, Tao Ge, Furu Wei, and Houfeng Wang. 2021. Instantaneous grammatical error correction with shallow aggressive decoding. In $A C L$.

Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. 2017. BranchyNet: Fast inference via early exiting from deep neural networks. Preprint, arXiv:1709.01686.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020 Huggingface's transformers: State-of-the-art natural language processing. Preprint, arXiv:1910.03771.

Haoyi Wu and Kewei Tu. 2024. Layer-Condensed KV Cache for efficient inference of large language models. Preprint, arXiv:2405.10637.

Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. 2023. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. Preprint, arXiv:2203.16487.

Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. Preprint, arXiv:2401.07851.

Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured pruning learns compact and accurate models. In $A C L$.

Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. 2024. Multi-candidate speculative decoding. Preprint, arXiv:2401.06706.

Chen Zhang, Zhuorui Liu, and Dawei Song. 2024. Beyond the speculative game: A survey of speculative execution in large language models. Preprint, arXiv:2404.14897.

Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. 2023a. Draft
\& Verify: Lossless large language model acceleration via self-speculative decoding. Preprint, arXiv:2309.08168.

Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. 2023b. AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning. Preprint, arXiv:2303.10512.

Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun. 2024. Ouroboros: Speculative decoding with large model enhanced drafting. Preprint, arXiv:2402.13720.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. Preprint, arXiv:2306.05685.
