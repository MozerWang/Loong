# Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation 

Jingchang Chen ${ }^{1 *}$ Hongxuan Tang ${ }^{1 *}$ Zheng Chu ${ }^{1} \quad$ Qianglong Chen $^{2}$<br>Zekun Wang ${ }^{1} \quad$ Ming Liu ${ }^{1 \dagger}$ Bing Qin ${ }^{1}$<br>${ }^{1}$ Harbin Institute of Technology<br>${ }^{2}$ Zhejiang University<br>\{jcchen, zchu, zkwang, mliu, qinb\}@ir.hit.edu.cn<br>jeffswt@outlook.com chenqianglong.ai@gmail.com


#### Abstract

Despite recent progress made by large language models in code generation, they still struggle with programs that meet complex requirements. Recent work utilizes plan-and-solve decomposition to decrease the complexity and leverage selftests to refine the generated program. Yet, planning deep-inside requirements in advance can be challenging, and the tests need to be accurate to accomplish self-improvement. To this end, we propose FUNCODER, a code generation framework incorporating the divide-and-conquer strategy with functional consensus. Specifically, FUNCODER recursively branches off sub-functions as smaller goals during code generation, represented by a tree hierarchy. These sub-functions are then composited to attain more complex objectives. Additionally, we designate functions via a consensus formed by identifying similarities in program behavior, mitigating error propagation. FUNCODER outperforms state-of-the-art methods by $+9.8 \%$ on average in HumanEval, MBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method demonstrates superiority on smaller models: With FUNCODER, StableCode ${ }_{3 b}$ surpasses GPT-3.5 by $+18.6 \%$ and achieves $97.7 \%$ of GPT-4's performance on HumanEval. Further analysis reveals that our proposed dynamic function decomposition is capable of handling complex requirements, and the functional consensus prevails over self-testing in correctness evaluation.


## 1 Introduction

Over the past few years, large language models have been observed to attain significant advancements in coding capabilities (OpenAI, 2023, Touvron et al. 2023). Meanwhile, models designed specifically for coding tasks have also been introduced (Rozière et al., 2023; Lozhkov et al., 2024; Pinnaparaju et al. 2024). Although LLMs can proficiently generate simple code snippets, they suffer from a decline in performance as code requirements become complicated.

Numerous efforts have been made to tackle this complexity. The two-stage methods (Jiang et al., 2023; Zelikman et al., 2023) employ the plan-and-solve strategy, which first generates a draft outline for the complex task and uses it as guidance for implementing the code in the second stage. Multiagent development frameworks (Hong et al., 2024, Qian et al., 2023) mimic real-world software development workflows, assign different roles to LLMs and collaborate to solve a complex goal. Self-improvement (Shinn et al. 2023; Chen et al., 2024), on the other hand, refines the program in accordance with execution feedback from self-generated unit tests.

Despite fruitful efforts made by the previous methods in dealing with complex problems, certain challenges still remain unsolved: (1) Two-stage approaches need to design a complete plan at the[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_94bfc0883b286484df06g-02.jpg?height=740&width=1400&top_left_y=190&top_left_x=358)

Figure 1: A flowgraph illustrates FUNCODER. FUNCODER branches off new functions to have sub-goals tackled iteratively (left), re-composites sub-functions, and selects the best using functional consensus (right). Bottom-right figure shows how FUNCODER writes functions at hierarchy-level.

beginning and lack the ability to adjust the top-level design during implementation, leading to suboptimal decomposition. (2) Multi-agent collaboration frameworks are cumbersome and rely heavily on LLM capabilities, making them difficult to generalize to smaller open-source models. (3) Code refinement through self-tests depends on the correctness of generated unit-tests. Our preliminary study ( $\$ 3.1 .3$ ) finds that models generate unreliable self-tests in abundance. These incorrect tests may mislead self-improvement and, at worse, exacerbate program errors.

To address these issues, we propose FUNCODER, a code generation framework utilizing a divide-andconquer strategy and a novel functional consensus mechanism on functions to decompose complex problems. Starting from the main problem, FUNCODER introduces new functions to cope with certain sub-problems. The new functions will be decomposed recursively, eventually forming a tree of functions. FUNCODER then combines functions bottom-up to achieve increasingly complicated objectives. By dividing-and-conquering tasks into simpler sub-functions, complexity can be gradually reduced. However, errors in sub-functions may propagate to the whole program, thereby damaging overall reliability. We propose functional consensus that samples multiple functions and selects the one demonstrating consensus, measured by the aggregated similarity among candidates. By reaching a consensus, we reduce the discrepancies in code behavior and thus alleviate cascading errors.

We conduct extensive experiments on code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Khan et al., 2023) with GPT (Ouyang et al., 2022; OpenAI, 2023), outperforming state-ofthe-art methods by $+9.8 \%$ on average. Experiments are further carried out on the mathematical competition benchmark, MATH (Hendrycks et al. 2021b), achieving a $+\mathbf{6 . 0}$ improvement with GPT-4, indicating that FUNCODER can also generalize to complex reasoning. Our method is observed to be equally effective on open-source models (Rozière et al., 2023, Pinnaparaju et al. 2024, Meta AI. 2024), with an average gain over baseline of $+\mathbf{3 8 . 0} \%$ on HumanEval and $+\mathbf{6 1 . 1} \%$ on MATH. Additional analysis also shows the advantage of both divide-and-conquer and functional consensus.

## 2 FunCoder: Divide-and-Conquer Meets Consensus

### 2.1 Divide-and-Conquer for Iterative Programming

A function is defined as a relation between a set of inputs and outputs where each input is assigned exactly one output (Halmos, 1998), denoted as $y=f(x)$. In computer programming, a function is identified by its header $h_{f}$ with its body $b_{f}$, and is commonly accompanied by a documentation $d_{f}$ to improve readability. Functions can be invoked from other procedures, allowing for the decomposition of large and complicated requirements into smaller structures that exhibit high comprehensibility and quality (Dahl et al. 1972). Generally, human programmers tend to decompose tasks into clearly

```
Algorithm 1 FUNCODER procedure
Require: Entry func, $f_{\text {root }}=\left\{h_{\text {root }}, d_{\text {root }}, \phi\right\}$
Require: Large language model, LLM
    function FUNCODER $\left(f_{\text {cur }}\right)$
        - Divide -
        $f_{\text {cur }}^{\prime},\left\{f_{i}\right\} \leftarrow \operatorname{EXTRACT}\left(\operatorname{LLM}\left(f_{\text {cur }}\right)\right)$
        for $f_{i} \in\left\{f_{i}\right\}$ do
            if $b_{i}$ is NOTIMPLEMENTED then
                $f_{i}^{*} \leftarrow \operatorname{FUNCODER}\left(f_{i}\right) \triangleright$ recursion
            end if
            $\operatorname{AdDCHILD}\left(f_{\text {cur }}, f_{i}^{*}\right)$
        end for
        - Conquer -
        $F_{\text {cur }} \leftarrow \operatorname{SAMPLE}\left(\operatorname{LLM}\left(f_{\text {cur }}^{\prime}, \operatorname{CHILD}\left(f_{\text {cur }}\right)\right)\right.$
        $f_{\text {cur }}^{*} \leftarrow$ FUNCONSENSUS $\left(F_{\text {cur }}\right)$
        return $f_{\text {cur }}^{*}$
    end function
    return FUNCODER $\left(f_{\text {root }}\right) \quad \triangleright$ starts from root
```

![](https://cdn.mathpix.com/cropped/2024_06_04_94bfc0883b286484df06g-03.jpg?height=228&width=624&top_left_y=257&top_left_x=1127)

(b) Decompose Through Coding (ours)

![](https://cdn.mathpix.com/cropped/2024_06_04_94bfc0883b286484df06g-03.jpg?height=461&width=615&top_left_y=526&top_left_x=1140)

Figure 2: Left: Algorithm for FUNCODER procedure. Right: Comparison between decomposition by planning and our approach. FUNCODER introduces new functions to describe sub-goals solely with code, achieving a more natural way of requirement decomposition.

defined sub-functions and then implement them recursively, making functions eligible for re-usage, taking advantage of the divide-and-conquer principle. Inspired by this, FUNCODER recursively divides the requirement and conquers functions to formulate a sophisticated solution, unleashing the potential of LLMs in code generation.

Divide is a top-down process that iteratively breaks down problems. Given a code generation problem, the process begins from the entry function $f_{\text {root }}$. We instruct the model to introduce new functions $f_{i} \in \operatorname{CHILD}\left(f_{\text {cur }}\right)$ that solve certain sub-goals while writing the current $f_{\text {cur }}$. To reduce the complexity involved in each generation, we only require the headers $h_{f_{i}}$ and documentation $d_{f_{i}}$ of new functions to be generated, while their implementations $b_{f_{i}}$ can be postponed. After completing the current function, the model starts to address those unimplemented sub-functions and complete $b_{f_{i}}$ into $f_{i}^{\prime}$. This process stops when the model deems functions too simple to be further divided, finally forming a dependency tree $T=\operatorname{TREE}\left(f_{\text {root }}, \operatorname{CHILD}\left(f_{\text {root }}\right)\right)$. The divide process is similar to a search starting from the entry function, gradually involving new sub-functions while writing the current, and implementing them recursively. We guide the entire process through a depth-first search.

Conquer is a process of achieving complex objectives through aggregating smaller functions. We notice that child functions are not yet implemented during the top-down process of writing parent functions. As a result, these parent functions may not be able to effectively utilize the child functions, or misuse them at worst. FUNCODER deals with this issue by re-generating functions in inverse topological order on the dependency tree $T$ - starting from leaves, complex goals are handled by compositing solved children as $f_{\text {cur }}^{*} \leftarrow \mathcal{F}\left(f_{\text {cur }}^{\prime},\left\{f_{1}^{*}, f_{2}^{*}, \ldots\right\}\right) \mid f_{i}^{*} \in \operatorname{CHILD}\left(f_{\text {cur }}\right)$.

Divide and conquer naturally achieve both decomposition and composition during code generation. Unlike two-stage and agent-based methods, our approach dynamically introduces new functions along the process, making it less burdensome than producing a complete plan at the very beginning. Moreover, while planning or agents require chat capabilities, FUNCODER represents sub-tasks through functions (Figure 2, making it more applicable to specialized code generation models.

### 2.2 Functionality Similarity as a Consensus

The decomposition of complex tasks benefits from solving easier sub-goals, but might introduce the risks of cascading errors. To mitigate this, we introduce Functional Consensus which aims at reducing inconsistencies in program behavior. This is achieved by sampling multiple functions and selecting the one that exhibits consensus, as measured by the aggregated similarity of functionality between candidates, thus abating outlier functionalities.

Functionality Similarity A program specifies its functionality (or behavior) through the control flow defined by its code semantics. However, comparing the functionalities between two programs based on their semantics is somewhat challenging. By decomposing the requirement into functions, FUNCODER is able to view the function behavior as a black box that maps arguments into return values. Considering two functions $f$ and $g$ with the same input domain $D(f)=D(g)$, we define the similarity between them $\operatorname{sim}(f, g)$ as the identicalness of outputs when given the same input values.

$$
\begin{equation*}
\operatorname{sim}(f, g)=\int_{x \in D(f)} \frac{\mathbb{1}[f(x)=g(x)]}{|D(f)|} \approx \sum_{x \in X \mid X \sim D(f)} \frac{\mathbb{1}[f(x)=g(x)]}{|X|} \tag{1}
\end{equation*}
$$

The similarity becomes 1 if and only if two functions output consistent values for all inputs: $\forall x \in$ $D(f): f(x)=g(x) \Leftrightarrow \operatorname{sim}(f, g)=1$. We notice that the input domain $D(f)$ is unbounded in most cases, making its measurement barely feasible in practice. Thus, we approximate it by sampling a subset of possible inputs $X \sim D(f)$ with an LLM.

Consensus is reached by selecting the candidate $f^{*}$ holding maximal similarity with others after sampling multiple function implementations $F=\left\{f_{(i)}\right\}$ for the same requirements.

$$
\begin{equation*}
f^{*}=\operatorname{FUNCONSENSUS}(F)=\underset{f_{(i)} \in F}{\arg \max } \sum_{f_{(j)} \in F \backslash\left\{f_{(i)}\right\}} \operatorname{sim}\left(f_{(i)}, f_{(j)}\right) \tag{2}
\end{equation*}
$$

By introducing functional consensus, FUNCODER produces functions that are more consistent and common in functionality, while omitting abnormal samples. The process is applied to not just the final program, but also to every sub-tree during the bottom-up conquering stage, resulting in step-by-step, thorough verification from the most fundamental functions all the way up to the whole program.

### 2.3 FunCoder is a Function Coder

We design FUNCODER as a procedure that takes a problem in the form of a function signature $f(x)$, and produces a final solution $f^{*}(x)$, as exemplified in Figure 1 Given a problem $f(x)$, FUNCODER partially implements the function as $f^{\prime}(x)$ referring to unimplemented sub-functions $g(y)$ and $h(z)$. These sub-functions are then fed into FUNCODER to be recursively coped with. We then sample $k$ implementations $f_{(i)}^{\prime}(x)$ based on solved children $g^{*}(y)$ and $h^{*}(z)$. Functional consensus is calculated by evaluating candidates on possible inputs. The function sharing maximal behavioral similarity is combined with solved children to formulate the final solution.

## 3 Experiments

We conduct experiments on competition-level code generation and mathematical reasoning benchmarks with state-of-the-art LLMs, which are covered in section $\$ 3.1$ and $\$ 3.2$, respectively. In addition to GPT models (Ouyang et al., 2022, OpenAI, 2023), we also conduct experiments with community models like Llama3 $3 b$ (Meta AI, 2024), StableCode $3 b$ (Pinnaparaju et al., 2024), and CodeLlama $_{34 b}$ (Rozière et al. 2023). We use the instruct variant of these models and inference on a single A100-80G under BF16 precision with vLLM (Kwon et al., 2023).

### 3.1 Code Generation

We choose three benchmarks for code generation evaluation: (a) HumanEval (Chen et al., 2021) includes entry-level coding questions; (b) MBPP (Austin et al., 2021) contains questions of standard library invocation and programming basics; and (c) xCodeEval (Khan et al., 2023) consists of algorithmic challenges sourced from the competitive programming platform CodeForces.

### 3.1.1 Experiment Setup

Benchmarks We adopt the full test set (164 problems) for HumanEval, and sample 200 for MBPP and 500 for xCodeEval, respectively. Following EbTech (2024), we split the xCodeEval into 4 subsets based on problem difficulty: Easy ( $\leq 1200$ ), Mid (1200-1599), Hard (1600-1999) and Expert $(\geq 2000)$. The evaluation metric for code generation is Pass @ 1 unless specified.

Table 1: Experiment results on code generation benchmarks. We report Pass @ 1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.

| Model | Method | HumanEval |  | MBPP |  | xCodeEval |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Pass@1 | $\Delta \uparrow$ | Pass@1 | $\Delta \uparrow$ | Easy | Mid | Hard | Expert | $\overline{\text { All }}$ |
| GPT-4 | Standard | 82.9 | - | 73.5 | - | 68.5 | 39.3 | 19.5 | 1.7 | 37.4 |
|  | Parsel | 85.0 | +2.1 | ![](https://cdn.mathpix.com/cropped/2024_06_04_94bfc0883b286484df06g-05.jpg?height=43&width=134&top_left_y=502&top_left_x=993) | - | - | - | - | - | - |
|  | CodeT | 90.9 | +8.0 | 77.0 | +3.5 | 76.4 | 51.8 | 21.8 | 3.4 | 44.0 |
|  | Reflexio | 91.0 | +8.1 | 77.1 | +3.6 | 71.3 | 41.1 | 19.5 | 2.5 | 38.6 |
|  | MetaGPT | $\overline{85.9}$ | +3.0 | ![](https://cdn.mathpix.com/cropped/2024_06_04_94bfc0883b286484df06g-05.jpg?height=43&width=134&top_left_y=613&top_left_x=993) | - | - | - | - | - | - |
|  | FUNCODER | 94.5 | +11.6 | 79.5 | +6.0 | 83.1 | 58.0 | 26.4 | 3.4 | 48.6 |
| GPT-3.5 | Stanc | 61 | ![](https://cdn.mathpix.com/cropped/2024_06_04_94bfc0883b286484df06g-05.jpg?height=50&width=92&top_left_y=707&top_left_x=875) | 72.0 | - | 44.4 | 15.2 | 4.6 | 0.0 | 20.2 |
|  | Code | 81.1 | +12.8 | 76.0 | +4.0 | 50.6 | 16.1 | 8.0 | 0.0 | 23.2 |
|  | Reflexion | 69.5 | +1.2 | 72.5 | +0.5 | 44.4 | 17.0 | 5.7 | 0.0 | 20.6 |
|  | LDB | 82.9 | +14.6 | 76.0 | +4.0 | - | - | - | - | - |
|  | FUNCODER | $\overline{\mathbf{8 5 . 4}}$ | +17.1 | 78.5 | +6.5 | 62.4 | 29.5 | 11.6 | 0.0 | 31.4 |

Baselines We compare FUNCODER with standard prompting (Brown et al., 2020), two-stage decomposition method Parsel (Zelikman et al. 2023), self-testing method CodeT (Chen et al., 2023a), self-improvement methods Reflexion and LDB (Shinn et al., 2023; Zhong et al., 2024), and multiagent developing framework MetaGPT (Hong et al. 2024). We implement Standard prompting with a 1-shot demonstration. CodeT samples 11 solutions with standard prompting and evaluates them on model-generated tests. The results for Reflexion are reproduced from the original code.

Implementation Details FUNCODER uses a 2-shot prompt in the divide stage and 1-shot for conquering sub-functions. The number of sampled implementations in the functional consensus is set to 11 for code generation tasks. For further implementation details, please refer to Appendix A. 1

### 3.1.2 Results

Table 1 shows the code generation performance on advanced proprietary models, GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023). For basic programming questions, HumanEval and MBPP, FUNCODER surpass previous SOTA methods by $+3.3 \%$ in Pass @ 1 and reduce the error rate by $18.6 \%$. Furthermore, FUNCODER demonstrates a substantial improvement on competition-level problems, outperforming others by $10.4 \%$ in GPT-4 and $35.3 \%$ with GPT-3.5. We observe that FUNCODER can enhance LLM's capability of solving more complex programming tasks, with an average accuracy improvement of $82.3 \%$ over the baseline on the Mid and Hard subsets of xCodeEval. Expert level programs, however, still remain a colossal challenge for even the most cutting-edge LLMs.

Table 2: Code generation performance with open-source models on HumanEval.

| Model | Category | Param | Standard | CodeT | Reflexion | FUNCODER |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| Llama3 | Text/Chat | 8B | 61.6 | 68.9 | 59.1 | $\mathbf{7 9 . 9}(+11.0)$ |
| StableCode | Code | 3B | 61.0 | 75.0 | 61.6 | $\mathbf{8 1 . 0}(+6.0)$ |
| CodeLlama | Code | 34B | 43.9 | 55.5 | 41.5 | $\mathbf{6 6 . 5}(+11.0)$ |

Evaluation is also performed over community LLMs, Llama3 (Meta AI, 2024), StableCode (Pinnaparaju et al. 2024), and CodeLlama (Rozière et al. 2023) with results in Table 2, 10. FUNCODER consistently boosts the performance of smaller models in code generation, with an averaged improvement of $+38.0 \%$ compared to standard prompting, and outperforms the previous best method CodeT by $+14.6 \%$ on HumanEval. Experiment results demonstrate that our method archives state-of-the-art performance on various models, ranging from basic programming to competition contests.

### 3.1.3 Analysis

FUNCODER Democratize to Smaller LLMs Limited by the LLM capabilities, the application of selfimprovement or multi-agent methods on smaller models is without ease. By keeping decomposition
(a) Preliminary Study on Self-testing

![](https://cdn.mathpix.com/cropped/2024_06_04_94bfc0883b286484df06g-06.jpg?height=350&width=894&top_left_y=272&top_left_x=366)

(b) Effectiveness of Ranking Strategy

![](https://cdn.mathpix.com/cropped/2024_06_04_94bfc0883b286484df06g-06.jpg?height=342&width=501&top_left_y=279&top_left_x=1254)

Figure 3: (a) Preliminary study on self-testing, the programs are evaluated using unit-tests generated by LLMs. (b) The effectiveness of different ranking strategies. We compute the Pass @ $\mathrm{k}$ over top-k programs ranked by functional consensus, self-test, and random on 11 candidates. (higher is better)

and composition within the code generation process, our approach exhibits better generalization. As shown in Table 1, 2, with FUNCoDER, Llama3 ${ }_{8 b}$ and StableCode $3 b$ achieve around $1.18 \times$ performance to standard GPT-3.5, and are closely aligned with GPT-4 by about $97 \%$ on HumanEval.

Preliminary Study on Self-Testing Method We conduct a preliminary study targeting the self-testing method on HumanEval, results are shown in Figure 3 a with further details in Appendix A.5. We first verify whether model-generated programs can also pass model-generated self-tests: (a) If a program passes self-tests, most from GPT-3.5 would also work on system tests, as much as $19.5 \% / 64 \% \approx 30.5 \%$ programs from StableCode are rejected, indicating that smaller models like StableCode may not effectively self-test and detect program errors on its own. (b) In the event of failed self-tests, a large portion of failures are attributed to issues in self-tests instead of the programs, on both GPT-3.5 and StableCode. These phenomena indicate that self-testing methods have limitations in generating correct and reliable unit tests. As a result, we design functional consensus to not require any assertion, but perform mutual verification between solutions instead, as opposed to self-testing.

Effectiveness of Functional Consensus Functional consensus or self-testing may be viewed as ranking algorithms for selecting functions. To measure ranking effectiveness, we conduct an analysis on HumanEval with GPT-3.5. For each problem, 11 candidates are ranked with 3 strategies: consensus, self-test, and random shuffle (as a baseline). Effectiveness is measured via Pass @k, i.e. if any of the top-k ranked programs pass the system test. Figure 3 b shows that functional consensus achieves $94.7 \%$ upper bound (Pass @ 11) performance by selecting a single function (Pass@ 1), and is close to that of self-test on Pass @4. This clearly demonstrates that functional consensus can effectively evaluate correctness and pick the most promising implementation on the first attempt.

Table 3: Ablation study of FUNCODER on HumanEval with GPT-3.5. The setting in our main experiment is highlighted in bold. Tokens are calculated as the sum of prompts and completions.

| Setting | Divide | Conquer | Ranking | Pass@1 | Avg. Tokens |
| :--- | :---: | :---: | :---: | :---: | :---: |
| Standard | $\boldsymbol{x}$ | $\boldsymbol{x}$ | $\boldsymbol{x}$ | 68.3 | $\mathbf{8 8 6 . 7}$ |
| One-pass | $\boldsymbol{\checkmark}$ | $\boldsymbol{x}$ | $\boldsymbol{x}$ | $72.6(+4.3)$ | 1233.7 |
| Two-pass | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ | $\boldsymbol{x}$ | $78.7(+10.4)$ | 3343.2 |
| Two-pass + ST@11 | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ | Self-Test@11 | $80.5(+12.2)$ | 5408.3 |
| FUNCODER @5 | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ | Consensus@ 5 | $83.5(+15.2)$ | 4040.8 |
| FUNCODER@11 | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ | Consensus@11 | $\mathbf{8 5 . 4}(+\mathbf{1 7 . 1})$ | 5402.0 |

[^1]Table 4: Experimental results on MATH, a competition-level mathematical reasoning benchmark. Best results are in bold. Text-based reasoning methods are denoted with ${ }^{\dagger}$, while others use programaided reasoning. We report both overall results and results in seven subjects: Prealgebra, Algebra, Number Theory, Counting \& Probability, Geometry, Intermediate Algebra, and Precalculus.

| Model | Method | Prealg. | Alg. | $N T$ | Prob. | Geo. | InterAlg. | Precalc. | Overall |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| GPT-4 | Standard $^{\dagger}$ | 81.7 | 82.7 | 71.1 | 72.3 | 59.5 | 46.7 | 47.3 | 68.2 |
|  | $\mathrm{CoT}^{\dagger}$ | 84.1 | 87.1 | 62.2 | 68.1 | 45.2 | 48.9 | 54.5 | 68.6 |
|  | PoT | 79.3 | 80.6 | 75.6 | 72.3 | 50.0 | 47.8 | 58.2 | 68.2 |
|  | Self-Refine | 82.9 | 82.0 | 77.8 | 76.6 | 54.8 | 55.6 | 63.6 | 72.2 |
|  | CR | 86.6 | 86.3 | 88.7 | 71.1 | 53.7 | 51.5 | 51.8 | 72.2 |
|  | FUNCODER | 89.0 | 92.8 | 82.2 | 83.0 | 59.5 | 63.3 | 56.4 | 78.2 |
| GPT-3.5 | Standard $^{\dagger}$ | 62.2 | 37.4 | 20.0 | 29.8 | 31.0 | 24.4 | 21.8 | 34.6 |
|  | $\mathrm{CoT}^{\dagger}$ | 59.8 | 51.1 | 28.9 | 29.8 | 28.6 | 26.7 | 30.9 | 40.0 |
|  | PoT | 68.3 | 50.4 | 33.3 | 48.9 | 21.4 | 18.2 | 29.1 | 41.0 |
|  | Self-Refine | 74.4 | 49.6 | 48.9 | 57.4 | 28.6 | 35.6 | 36.4 | 48.6 |
|  | FUNCODER | 76.8 | 61.2 | 55.6 | 59.6 | 34.1 | 36.0 | 41.8 | 54.0 |
| Llama3 $_{8 b}$ | $\mathrm{CoT}^{\dagger}$ | 56.1 | 47.5 | 31.1 | 34.0 | 40.5 | 14.4 | 38.2 | 38.6 |
|  | PoT | 67.1 | 32.4 | 24.4 | 34.0 | 16.7 | 21.1 | 18.2 | 32.6 |
|  | FUNCODER | 67.9 | 45.7 | 51.1 | 53.2 | 19.0 | 37.8 | 30.9 | 45.0 |
| StableCode $_{3 b}$ | PoT | 20.7 | 14.4 | 17.8 | 25.5 | 4.8 | 8.9 | 9.1 | 14.4 |
|  | FUNCODER | 46.3 | 30.2 | 20.0 | 29.8 | 4.8 | 20.0 | 18.2 | 26.6 |
| CodeLlama $_{34 b}$ | PoT | 35.5 | 26.1 | 15.0 | 16.7 | 0.0 | 5.5 | 33.3 | 15.2 |
|  | FUNCODER | 44.8 | 46.1 | 37.8 | 34.1 | 13.6 | 24.6 | 37.5 | 24.4 |

### 3.2 Mathematical Reasoning

Code can be viewed as a tool for augmenting the reasoning capabilities of LLMs (Chen et al., 2023b). Alternative to text-based reasoning like Chain-of-Thought (Wei et al., 2022), programs can offer unique advantages in terms of iteration and calculations. To test the generalizability of FUNCODER beyond algorithm challenges, we conduct an experiment on MATH (Hendrycks et al., 2021b), a competition-level mathematical reasoning benchmark.

### 3.2.1 Experiment Setup

Benchmark The experiment is conducted on a subset of the MATH test set, including 500 randomly sampled problems that can be classified into 7 disjoint subjects or 5 difficulty levels. It can be noticed that labels in MATH are formatted in $\mathrm{LT}_{\mathrm{E}} \mathrm{X}$, rendering exact-match verdicts impractical. We, therefore, follow previous work (Zhang et al. 2024) and adopt GPT-4 to determine the correspondence between predictions and labels, with further details provided in Appendix A.4.

Baselines We compare FUnCoDER with the text-based baselines: Standard Prompting and Chainof-Thought (Wei et al., 2022), and program-aided baselines: Program-of-Thought (Chen et al. 2023b), Self-Refine (Madaan et al., 2023), Cumulative Reasoning (Zhang et al., 2024). The results of Cumulative reasoning are reported in the original paper. Standard prompting and chain-of-thought reasoning use 7-shot demonstrations constructed from the train set. Program-of-Thought and SelfRefine prompt the model with 1-shot demonstration to generate a solution() function that solves the problem. Additionally, self-refine iteratively refines programs based on runtime feedback. All baseline methods are run with self-consistency (Wang et al., 2023) at 5.

Implementation Details FUNCODER adopts a program-aided reasoning setting that writes a solution() function and obtains the final prediction by running this program. The number of sampled implementations $|F|$ in functional consensus is set to 5 to match baseline methods.

### 3.2.2 Results

The experimental results on MATH are shown in Table 4. It shows that program-aided reasoning generally outperforms text-based reasoning. With GPT-4 as the backbone, FUNCODER outperforms the strongest baseline Cumulative Reasoning (Zhang et al. 2024) by ( 6.0 / 8.3\%) and surpasses the
vanilla program-aided baseline PoT (Chen et al. 2023b) by (10.0 / 14.7\%). When using GPT-3.5turbo as the backbone, FUNCODER exceeds the strongest baseline by $(6.2 / 11.1 \%)$ and outperforms PoT by as much as (13.0 / 31.7\%), which indicates that our approach has a strong advantage over both text-based reasoning and other program-aided reasoning methods.

On open-source models, FUNCODER with Llama3 outperforms PoT by (12.4 / 38.0\%). It has even reached competitive performance against the state-of-the-art method based on GPT-3.5 ( 45.0 v.s. 48.6). When employing StableCode and CodeLLaMA as the backbone, our approach achieves significant improvements by ( 12.2 / 84.7\%) and ( 9.2 / 60.5\%), respectively. This improvement demonstrates that our approach can significantly boost smaller LLMs, democratizing the complex reasoning capabilities of open-source LLMs through programming.

### 3.2.3 Analysis

## Fun Coder Can Handle Harder Questions

 Figure 4 compares between CoT, PoT, and FUnCODER across varying difficulty levels. It illustrates that CoT performs comparatively well on the easiest questions, but suffers from a steep decline in performance as difficulty increases. This suggests that text-based reasoning is inadequate for tackling challenging mathematical reasoning problems. The same situation is also observed in PoT. In contrast, our method consistently demonstrates high performance even on challenging problems, particularly excelling on![](https://cdn.mathpix.com/cropped/2024_06_04_94bfc0883b286484df06g-08.jpg?height=325&width=699&top_left_y=737&top_left_x=1057)

Figure 4: Average accuracy in each level with the chat model (GPT-3.5) and the code model (StableCode ${ }_{3 b}$ ) on the MATH benchmark. level 5 difficulty with nearly double the performance compared to PoT and CoT. This reflects that our method, with divide-and-conquer applied, can effectively cope with complex problems.

Decomposed Functions are Domain-Specific We hypothesize that questions from the same subject require similar knowledge reserves, which should be reflected in the functionality of the sub-functions. To verify this hypothesis, we statisticize the common sub-functions of FUNCODER in each MATH subject, as shown in Table 5. It is apparent that different subjects require different abilities, each with its own set of sub-functions closely associated with the domain knowledge. In addition, these common sub-functions are fundamentally basic and straightforward. As exemplified in Appendix B.2. our method is able to leverage and combine these basic sub-functions to achieve more complex goals, thereby reducing the complexity of reasoning and enhancing performance.

Table 5: Top-3 most commonly used functions in each subject of MATH, listed in descending order.

| Subject | Functions |
| :--- | :--- |
| Prealgebra | is_prime / factorial / gcd |
| Algebra | find_roots / is_perfect_square / find_domain |
| Number Theory | get_divisors / mod_inverse / gcd |
| Counting \& Probability | factorial / combinations / binomial_coefficient |
| Geometry | distance / simplify_fraction / calculate_triangle_area |
| Intermediate Algebra | find_roots / evaluate_polynomial / lagrange_interpolation |
| Precalculus | cross_product / fraction_from_angle / dot |

## 4 Related Work

Large Language Model for Code Code pre-training has received widespread attention, with early models based on small language models (SLM) (Feng et al., 2020, Lu et al., 2021, Wang et al. 2021). In recent years, with the development of large-scale pre-training techniques, code LLM has emerged, showing remarkable performance in downstream code tasks (Chen et al., 2021; Nijkamp et al., 2023, Li et al., 2022, Rozière et al., 2023, Li et al., 2023b, Guo et al., 2024). Tasks between code and natural language (NL) can be generally divided into three major categories: NL2Code tasks such as
code generation (Austin et al., 2021; Chen et al., 2021; Hendrycks et al., 2021a; Khan et al., 2023) and code search (Husain et al. 2019a); Code2Code tasks including code completion (Lu et al., 2021, Zhang et al., 2023a, Liu et al., 2024), code translation (Ahmad et al., 2023, Zhu et al., 2022; Yan et al., 2023), and test generation (Siddiq et al. 2023, Schäfer et al. 2024); Code2NL tasks like code summarization (Husain et al. 2019b; Jin et al. 2023). This paper focuses on code generation tasks, ranging from basic to competition level.

Code Refinement and Self-Testing Code doesn't always run as expected; it could contain syntax errors, dead loops, or bugs. It's essential to debug and refine the code to ensure better quality. CodeT (Chen et al., 2023a) generates unit-tests to score the implementation. Self-improvement methods (Madaan et al. 2023; Shinn et al., 2023, Chen et al., 2024, Zhong et al., 2024) design closed-loop procedures that repeatedly refine the code based on the feedback. Like real-life software development processes, multi-agent frameworks (Hong et al., 2024, Qian et al., 2023) construct specific LLM roles, Tester or $Q A$ to generate tests. These studies adopt a shared paradigm wherein self-tests are generated through LLMs. However, Olausson et al. (2024) points out the challenge that LLMs have certain shortcomings in self-repairing their code. This paper avoids these shortcomings by proposing functional consensus as a reliable method of evaluation.

Program-Aided Reasoning and Agents Aside from code generation tasks, the program can be a tool that augments LLM to solve complex reasoning questions or interact with external environments. Program-of-Thought (Chen et al. 2023b) and PAL (Gao et al. 2023) prompt the model to generate a program that solves mathematical or symbolic problems. MathPrompter (Imani et al. 2023) and Chain-of-Code (Li et al. 2023a) fuse the text-based chain-of-thought with code-based program-of-thought prompting to complement each other in mathematical reasoning. Cumulative Reasoning (Zhang et al. 2024) conducts bottom-up reasoning to derive the final answer progressively. Numerous work (Sun et al. 2023; Wang et al. 2024, Yang et al. 2024) also use code as an intermediate component to bridge LLM agents with external environments.

Decompose for Complex Problems Several recent works employ decomposition to reduce the complexity of hard problems. Least-to-Most (Zhou et al. 2023) adopts a two-stage approach, which first decomposes complex problems, and then solves each sub-problem individually to tackle complex reasoning tasks. Successive Prompting (Dua et al., 2022) adopts a dynamic decomposition, iteratively breaking down problems and addressing sub-problems. Tree-of-Thought (Yao et al., 2023) breaks down complex problems into state spaces and uses tree search to solve them. Parsel (Zelikman et al. 2023) introduces decomposition to code generation tasks, taking a three-stage to break down requirements into draft and intermediate parsel programs. RepoCoder (Zhang et al. 2023b) performs a retrieval in repositories to complete unfinished code one by one. Unlike these methods, FUNCODER recursively decomposes problems into a tree structure, hence gradually reduces its complexity.

## 5 Discussion

Limitations Our approach unleashes the potential power of functions in programming, which is advantageous on well-defined problems such as competitive programming, or program-augmented reasoning tasks. These scenarios do not however represent all use cases, such as open-ended problems or casual software development. Nevertheless, we believe that the idea of divide-and-conquer and sub-modular consensus utilized by FUNCODER can be extended to a wider range of problems, and we consider this as a future exploration.

Broader Impact While code generation is increasingly utilized in software development, Large Language Models (LLMs) are still prone to generating toxic, vulnerable, or malicious code. Such programs pose risks and should be used or executed with extra caution.

## 6 Conclusion

In this paper, we presented FUNCODER, a novel code generation framework that integrates the divideand-conquer strategy with functional consensus to address complex requirements. FUNCODER had demonstrated superior performance compared to state-of-the-art methods on various benchmarks and models. Our findings highlighted the effectiveness of dynamic decomposition and functional consensus in writing complex code, which suggests that FUNCODER may have the potential to empower further improvements in code generation and other fields.

## References

Wasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. AVATAR: A parallel corpus for Java-python program translation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 2268-2281, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.143. URL https://aclanthology.org/2023.findings-acl.143

Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv preprint, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html

Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. Multipl-e: A scalable and polyglot approach to benchmarking neural code generation. IEEE Trans. Software Eng., 49(7):3675-3691, 2023. doi: 10.1109/TSE.2023.3267446. URL https: //doi.org/10.1109/TSE.2023.3267446

Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023a. URL https://openreview.net/ forum?id=ktrw68Cmu9c

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374, 2021. URL https: //arxiv.org/abs/2107.03374

Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023b. ISSN 2835-8856. URL https://openreview.net/forum?id=YfZ4ZPt8zd

Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=KuPixIqPiq

Ole-Johan Dahl, Edsger W. Dijkstra, and Charles Antony Richard Hoare. Structured programming, volume 8 of A.P.I.C. Studies in data processing. Academic Press, 1972. ISBN 978-0-12-200550-3.

Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive prompting for decomposing complex questions. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1251-1265, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.81. URL https://aclanthology.org/2022.emnlp-main. 81

EbTech. How to Interpret Contest Ratings - Codeforces, 2024. URL https://codeforces.com/blog/ entry/68288

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for programming and natural languages. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1536-1547, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.findings-emnlp.139. URL https://aclanthology.org/2020.findings-emnlp. 139

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: program-aided language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 10764-10799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.html

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. ArXiv preprint, abs/2401.14196, 2024. URL https: //arxiv.org/abs/2401.14196

P.R. Halmos. Naive Set Theory. Undergraduate Texts in Mathematics. Springer New York, 1998. ISBN 9780387900926. URL https://books.google.com.hk/books?id=x6cZBQ9qtgoC

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with APPS. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021a. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/ hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html

Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=VtmBAGCN70

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. ArXiv preprint, abs/1909.09436, 2019a. URL https://arxiv.org/abs/1909.09436

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. ArXiv preprint, abs/1909.09436, 2019b. URL https://arxiv.org/abs/1909.09436

Shima Imani, Liang Du, and Harsh Shrivastava. MathPrompter: Mathematical reasoning using large language models. In Sunayana Sitaram, Beata Beigman Klebanov, and Jason D Williams (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pp. 37-42, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-industry.4. URLhttps://aclanthology.org/2023.acl-industry. 4

Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. Self-planning code generation with large language model. ArXiv preprint, abs/2303.06689, 2023. URL https://arxiv.org/abs/2303.06689

Xin Jin, Jonathan Larson, Weiwei Yang, and Zhiqiang Lin. Binary code summarization: Benchmarking chatgpt/gpt-4 and other large language models. ArXiv preprint, abs/2312.09601, 2023. URL https: //arxiv.org/abs/2312.09601

Mohammad Abdullah Matin Khan, M. Saiful Bari, Xuan Long Do, Weishi Wang, Md. Rizwan Parvez, and Shafiq R. Joty. xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. ArXiv preprint, abs/2303.03004, 2023. URL https://arxiv.org/abs/2303. 03004 .

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.

Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with a language model-augmented code emulator. ArXiv preprint, abs/2312.04474, 2023a. URL https://arxiv.org/abs/2312.04474

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! ArXiv preprint, abs/2305.06161, 2023b. URL https://arxiv.org/abs/2305.06161

Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. ArXiv preprint, abs/2203.07814, 2022. URLhttps://arxiv.org/abs/2203.07814

Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code autocompletion systems. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= $\mathrm{pPjZIOuQuF}$.

Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian J. McAuley, Han Hu, Torsten Scholak, Sébastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, and et al. Starcoder 2 and the stack v2: The next generation. ArXiv preprint, abs/2402.19173, 2024. URL https://arxiv.org/abs/2402.19173

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding and generation. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ c16a5320fa475530d9583c34fd356ef5-Abstract-round1.html

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with selffeedback. In Thirty-seventh Conference on Neural Information Processing Systems, NeurIPS 2023, 2023. URL https://openreview.net/forum?id=S37h0erQLB

Meta AI. Meta Llama 3 - homepage, 2024. URL https://llama.meta.com/llama3/

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=iaYcJKpY2B_

Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. Is self-repair a silver bullet for code generation? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna Austria, May 7-11, 2024. OpenReview.net, 2024. URL https:// openreview.net/forum?id=yOGJXRungR

OpenAI. GPT-4 technical report. ArXiv preprint, abs/2303.08774, 2023. URL https://arxiv.org/abs/ 2303.08774

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html

Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, and Nathan Cooper. Stable code technical report. ArXiv preprint, abs/2404.01226, 2024. URL https://arxiv.org/abs/2404.01226

Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. ArXiv preprint, abs/2307.07924, 2023. URL https://arxiv.org/abs/2307.07924

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. ArXiv preprint, abs/2308.12950, 2023. URL https://arxiv.org/abs/2308. 12950

Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. An empirical evaluation of using large language models for automated unit test generation. IEEE Trans. Software Eng., 50(1):85-105, 2024. doi: 10.1109/ TSE.2023.3334955. URLhttps://doi.org/10.1109/TSE.2023.3334955

Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, NeurIPS 2023, 2023. URL https://openreview.net/forum?id=vAElhFcKW6

Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat, and Vinicius Carvalho Lopes. Exploring the effectiveness of large language models in generating unit tests. ArXiv preprint, abs/2305.00418, 2023. URL https://arxiv.org/abs/2305.00418

Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. In Thirty-seventh Conference on Neural Information Processing Systems, NeurIPS 2023, 2023. URL https://openreview.net/forum?id=rnKgbKmelt.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023. URL https://arxiv.org/abs/2307.09288

Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better LLM agents. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. URL https://openreview.net/forum?id=8oJyuXfrPv

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw

Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8696-8708, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.685. URL https: //aclanthology.org/2021.emnlp-main. 685

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_ files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface's transformers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771, 2019. URL https://arxiv.org/abs/1910. 03771

Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, and Wen Wang. CodeTransOcean: A comprehensive multilingual benchmark for code translation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 5067-5089, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.337. URL https://aclanthology. org/2023.findings-emnlp.337

Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Heng Ji, and ChengXiang Zhai. If LLM is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. URL https://openreview.net/forum?id=8dmNOD9hbq

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, NeurIPS 2023, 2023. URL https://openreview.net/forum? id $=5 \mathrm{Xc1ecx01h}$

Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, and Nick Haber. Parsel: Algorithmic reasoning with language models by composing decompositions. In Thirty-seventh Conference on Neural Information Processing Systems, NeurIPS 2023, 2023. URL https://openreview.net/forum?id=qd9qcbVAwQ

Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. RepoCoder: Repository-level code completion through iterative retrieval and generation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 2471-2484, Singapore, 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.151. URL https://aclanthology.org/2023.emnlp-main. 151

Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. RepoCoder: Repository-level code completion through iterative retrieval and generation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 2471-2484, Singapore, 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.151. URL https://aclanthology.org/2023.emnlp-main. 151

Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. Cumulative reasoning with large language models. In ICLR 2024 Workshop on Bridging the Gap Between Practice and Theory in Deep Learning, 2024. URL https://openreview.net/forum?id=XAAYyRxTlQ

Lily Zhong, Zilong Wang, and Jingbo Shang. LDB: A large language model debugger via verifying runtime execution step-by-step. ArXiv preprint, abs/2402.16906, 2024. URL https://arxiv.org/abs/2402 16906

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum? id=WZH7099tgfM

Ming Zhu, Karthik Suresh, and Chandan K. Reddy. Multilingual code snippets training for program translation. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, pp. 11783-11790. AAAI Press, 2022. URL https://ojs.aaai.org/index.php/AAAI/article/view/21434
