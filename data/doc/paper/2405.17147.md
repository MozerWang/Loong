# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability 

Haiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE


#### Abstract

The rapid advancement of Large Language Models (LLMs) has significantly impacted human-computer interaction, epitomized by the release of GPT-4o, which introduced comprehensive multi-modality capabilities. In this paper, we first explored the deployment strategies, economic considerations, and sustainability challenges associated with the state-of-the-art LLMs. More specifically, we discussed the deployment debate between Retrieval-Augmented Generation (RAG) and fine-tuning, highlighting their respective advantages and limitations. After that, we quantitatively analyzed the requirement of xPUs in training and inference. Additionally, for the tokenomics of LLM services, we examined the balance between performance and cost from the quality of experience ( $\mathbf{Q O E}$ )'s perspective of end users. Lastly, we envisioned the future hybrid architecture of LLM processing and its corresponding sustainability concerns, particularly in the environmental carbon footprint impact. Through these discussions, we provided a comprehensive overview of the operational and strategic considerations essential for the responsible development and deployment of LLMs.


## I. UBIQUITOUS LLMS

The recent unveiling of GPT-4o by OpenAI on May 13, 2024 marks a pivotal moment in the evolution of large language models (LLMs) [1]. This groundbreaking model, aptly named with "o" signifying "omni" for its comprehensive capabilities, transcends the limitations of its predecessors by incorporating multi modality. This signifies a significant step towards achieving more natural and intuitive human-computer interaction.

The emergence of LLMs started from the launch of ChatGPT in November 2022 after two months of which, it reached 100 million monthly users. Since then, many technical companies started to build their open or closed foundation LLM models, such as Gemini (Google), GPT-4 (OpenAI), LLaMA (Meta), Claude (Anthropic), Falcon (TII), Mistral (Mixtral), etc. Initially, text-to-text is the conventional scenario of the LLMs. All questions from users and replied answers from LLMs are words. Starting from 2023, LLMs' capability was extended to multi-modality. In contrast to simple text-to-text mode, the LLMs now support:

- Text-to-Image: Generate an image from text descriptions.
- Text-to-Video: Generate a video or movie from text descriptions, a bit like story-telling technologies.
- Text-to-Audio/Sound: Generate a human-like speech or various environmental sounds from text descriptions.
- Image-to-Text: Summarize an image in a few sentences, like generating a caption of an image or photo.
- Video-to-Text: Summarize a video in a few sentences, e.g., creating a summary of a YouTube video clip or Netflix movie.
- Audio/Sound-to-Text: Describe a speech or surrounding sounds in a few sentences.

If we consider 3D objects as a kind of modality, the list of LLMs' capability could even extended in the following:

- Text-to-3D: Generate realistic 3D objects from text descriptions.
- Image-to-3D: Generate 3D scenes with an image or a 3D avatar with an portrait photo.
- Video-to-3D: Generate 3D scenes or eventually an entire 3D virtual environment by videos.

It is noted that if it is from low-dimensional modality to high-dimensional modality, it is a decoder/diffusion mode depicted as "generation". Conversely, it is an encoder mode described as "summarization".

The use cases of LLMs are always as digital assistants in consumer electronics, such as smartphones, laptops, wearable devices, automotive, etc. For example, users speak to their smartwatch about a query which is converted to text by automatic speech recognition (ASR) module, like Whisper. The LLMs running on central/fog/edge cloud answer the query by considering the context of users (e.g., schedule, preference, etc.) with prompt engineering and finally provide a customized natural response by automatic text-to-speech conversion. For eXtended reality (including VR/AR/MR) use case [2], the LLMs not only can generate human-like conversations for avatars, but also generate high-quality point clouds of the $3 \mathrm{D}$ objects and their corresponding textures. In general, in our vision, we speculate that LLMs would replace the role of CPU in our traditional computer architecture (shown in Fig. 1) and interact with web browsers, legacy components of PCs, local file systems, various multimedia modalities (including but not limited to images, audios, videos), and other LLM models.

![](https://cdn.mathpix.com/cropped/2024_06_04_f7e371b5f251db051297g-1.jpg?height=477&width=854&top_left_y=1987&top_left_x=1096)

Fig. 1. LLMs will be the computational core to interact with multimodal multimedia, legacy file system, components of PC, and other LLMs.

## II. LLM DEPLOYMENT DEBATE: RAG VS FINE-TUNING

LLMs are highly effective in a wide range of natural language processing (NLP) tasks [3], but they often fall short in specific real-world applications due to a lack of specialized knowledge during their training. These models are typically trained on extensive, web-sourced annotated data, which broadens their general applicability but diminishes their accuracy in specialized areas. For instance, BloombergGPT [4], which includes a significant proportion of finance-specific data, has shown improved performance in financial contexts, illustrating the benefits of targeted data training. However, assembling large, clean datasets for specialized training involves considerable logistical and cost challenges. In this case, deploying LLMs that have been trained primarily with generic datasets can lead to challenges in processing long, complex texts, thereby limiting their effectiveness in detailed domain discussions and increasing the likelihood of generating hallucinations and producing irrelevant or incorrect content [5]. To address this challenge, two main deployment strategies have emerged: Retrieval-Augmented Generation (RAG) and fine-tuning.

## A. RAG: Solving Context Window Limitation

The RAG model enhances LLMs by integrating the external knowledge base to efficiently retrieve relevant information that complements the LLM's inherent knowledge. This retrieved information is then incorporated into the LLM's prompt, providing essential contexts. The RAG mechanism is particularly beneficial for tasks that require factual retrieval or similarity searches, as it directly supplies relevant context, thereby reducing the likelihood of generating inaccurate or hallucinations. RAG can also be further enhanced by integrating techniques for in-context examples' retrieval during runtime. This method involves fetching relevant few-shot examples that are semantically similar to the user's query.

An illustration of a RAG-based LLM deployment system is depicted in Fig. 2. This system utilizes a knowledge graph database to extract external knowledge and a vector database for retrieving relevant examples. To obtain reference context from the knowledge graph database, the LLM first generates Cypher queries based on the user's prompts and the fields within the Resource Description Framework (RDF) knowledge graph. These queries are then executed in specific knowledge graph databases, such as Neo4j or SPARQL. The retrieved context may need to be re-ranked to ensure it aligns with specific business requirements. For retrieving few-shot examples, which are represented as vectors in a high-dimensional space, the user's query is transformed into the corresponding embedding space. A similarity search is then performed to select the top-n semantically close examples. The final prompt integrates the aforementioned information with the initial user prompt, enabling the LLM to generate responses that are both informative and accurate.

## B. Fine-tuning: Deeper Domain Expertise

Fine-tuning focuses on adapting a pre-trained LLM to a specific task through further supervised learning. This involves

![](https://cdn.mathpix.com/cropped/2024_06_04_f7e371b5f251db051297g-2.jpg?height=737&width=789&top_left_y=236&top_left_x=1145)

Fig. 2. Retrieval-Augmented Generation (RAG) architecture for large language models. RAG retrieves relevant data from a knowledge base and optionally few-shot examples to address context window limitations and improve response generation.

adjusting the weights of a pre-trained model on a smaller dataset of task-specific examples. Compared to training a model from scratch, fine-tuning offers significant advantages in terms of computational efficiency and leveraging the pretrained model's general language understanding.

Several fine-tuning techniques have been developed to enhance LLM performance on specific tasks. These include instruction tuning which provides the LLM with explicit instructions or constraints alongside the training data. LowRank Adaptation (LoRA) which introduces a small adapter module on top of the pre-trained LLM (shown in Fig. 3). This adapter module is specifically trained on the task-specific data, enabling efficient adaptation without modifying the core LLM architecture [6]. Recent advancements in fine-tuning incorporate techniques from Reinforcement Learning from Human Feedback (RLHF) to achieve better alignment between the model's outputs and human expectations [7]. RLHF allows the model to learn through interactions with a human in the loop, who provides feedback on the model's outputs. This feedback is then used to refine the model's parameters and improve its performance on the target task.

![](https://cdn.mathpix.com/cropped/2024_06_04_f7e371b5f251db051297g-2.jpg?height=203&width=828&top_left_y=2162&top_left_x=1123)

Fig. 3. LLM fine-tuning for a specific task. The pre-trained model has learned parameters from a massive dataset, while the parameters are updated through fine-tuning in a smaller target dataset. The red dots are the learned parameters in the original models, while the green dots are the new parameters learned from LoRA training. RLHF aligns the model for optimal performance.

## C. Takeaway Message

This section highlights the key considerations when choosing between RAG and fine-tuning for LLM applications. RAG offers faster adaptation and leverages external knowledge bases for factual retrieval and similarity search. However, the quality of the retrieved information heavily relies on the underlying knowledge base. If the knowledge base is outdated, inaccurate, or incomplete, the retrieved information can mislead the LLM to generate factually incorrect outputs. Additionally, RAG struggles with tasks requiring complex reasoning or inference beyond the information explicitly contained in the knowledge base [8]. Integrating and reasoning over information from multiple sources can also be challenging for RAG models, potentially leading to inconsistencies or inaccuracies in the generated response. One approach to alleviate these limitations is to employ reranking techniques. Reranking involves applying a secondary sorting step to the retrieved information from the knowledge base. This step can be based on various factors, such as using a confidence score assigned by the retrieval model [9], or evaluating whether the retrieved items collectively address all aspects of the user query.

Fine-tuning, on the other hand, provides superior performance for tasks requiring deep domain expertise [10]. However, it typically requires large, high-quality datasets to achieve optimal performance whereas creating such datasets can be expensive and time-consuming. To address this challenge, some researchers are exploring the use of teacher-student learning. In this approach, a large, pre-trained model (teacher) like GPT4 [11] is used to generate synthetic data that can be used to fine-tune a smaller, open-source model (student) like LLaMA 8B [12]. This approach can be significantly cheaper than creating high-quality human-annotated datasets, especially for tasks where large amounts of data are required. However, it is important to ensure that the synthetic data accurately reflects the real-world distribution of data and that the teacher model itself is not biased. Another challenge is the computational cost of fine-tuning. Fine-tuning often requires significant computational resources, especially for large LLMs. This can be a barrier for smaller organizations or researchers with limited access to computational resources.

The choice between RAG and fine-tuning depends on the specific needs of LLM applications. RAG offers a faster and more adaptable solution for retrieval-based tasks, while finetuning provides superior performance for tasks demanding deeper domain expertise.

## III. TRaIning and InfERENCE BY XPUs: CPU IS OUT-OF-THE-DATE?

The training of LLMs always consumes a lot of computational resources, particularly during the training of foundation models. The referred computational resources herein are xPUs typically including GPUs (graphical processing units), TPUs (tensor processing units), NPUs (neural processing units), LPUs (language processing units), and CPUs (central processing units). The former four are designed for tensor processing (i.e., vector manipulation and calculation) which are composed of tens of thousands of cores. For instance, the newly-release heavy-duty Blackwell GPU from Nvidia, GB202 GPU, has 24,576 CUDA cores. Even the light-duty Nvidia GeForce RTX 4090 has 16,384 CUDA cores. In contrast, CPUs only have very few cores which are designed for sequential tasks rather than parallel matrix or vector calculations. For example, Intel i9-14900 14th Gen has 24 cores.

There is no doubt that GPUs/TPUs/NPUs/LPUs are the only options to train LLMs. In this case, thousands of xPU cards are occupied to train an LLM for several months. Take the LLaMA $65 \mathrm{~B}$ model as an example, it has 65 billion parameters. If each parameter is represented by a floating point number, i.e., 2 bytes to be stored, the total required storage would be 130 billion bytes, i.e., 130 gigabytes. As it is more than the memory limit of any single xPU card, the model parameters need to be distributed in different xPUs, named model parallelism. To further improve the training efficiency and training completion time, the training data is shared across xPU cards called data parallelism. Specifically, if we use Nvidia A100 GPU (80GB memory) cards to train the model, considering the overall dataset contains 1.4 trillion tokens (1 token is approximately 0.75 English words), it takes 2048 A100 GPUs for approximately 21 days to complete the training. Even in the fine-tuning phase where domain knowledge tokens are from a few thousand to tens of thousands, 8 GPU cards are always at least required.

Certainly, the LLM inference can be done by GPUs/TPUs/NPUs/LPUs. For some service providers, the strategy is to use a big number of small xPU cards (in the perspective of memory) to conduct inference. For example, still for LLaMA 65B model, if each parameter is stored in the format of int 8 that is one byte, the total required storage is 65 gigabytes. If we use Groq LPU cards in this inference task, each Groq card has 230 megabytes memory. Therefore, the approximate number of Groq cards for LLaMA 65B model inference is $(65 \times 1000 \times 1000 \times 1000) \div(230 \times 1000 \times 1000)=$ $282.6 \doteq 283$.

However, these high-end xPU cards are not the only ones that fit the LLM inference scenarios. The main reason is that the inference models are always optimized to be much smaller models in the perspective of parameter number. In addition, each parameter can be used even less accurate format to represent it. In that case, CPUs are also capable of conducting inference without much loss of accuracy. Furthermore, CPUs are the ideal hardware to fulfill the tasks of essential LLM preprocessing, such as embedding and tokenization. Finally, the infrastructures of most existing data centers are composed of CPU servers. By considering the expensive cost of upgrading to high-end AI data centers, the service providers would prefer cost-effective yet usable CPUs for the inference business.

## IV. Tokenomics vs. QuALITY of EXPERIENCE: THE COMPROMISE OF PERFORMANCE AND COST

Tokenomics is a compound word for token and economics referring to the analysis of generative tokens in LLM inference from the perspective of economics. Here, we usually consider two aspects: throughput (tokens per second) and price (USD per 1 million tokens). According to the up-todate data [13], the throughput of most unicorn companies

![](https://cdn.mathpix.com/cropped/2024_06_04_f7e371b5f251db051297g-4.jpg?height=401&width=1619&top_left_y=184&top_left_x=253)

Fig. 4. The hybrid architecture of AI processing of LLMs. The central and edge clouds and devices work together to deliver high QoE LLM service by balancing factors, including inference accuracy, latency, device capacity, privacy, and security.

(such as Mistral, Perplexity, Toghether.ai, Anyscale, Deepinfra, Fireworks, Groq, Leption) lies about 50 to 200 tokens per second whereas Groq leads the benchmark to be more than 400 tokens per second. In regard to prices, most of the aforementioned companies can achieve between $\$ 0.2$ USD to $\$ 1.0$ USD per 1 million tokens. Perplexity and Groq lead the board herein around $\$ 0.25$ USD.

If we consider the LLM inference as a service, the customers are users who ask questions by prompts and receive answers in a paid (e.g., OpenAI GPT-4) or unpaid way (e.g., OpenAI GPT-3.5 or GPT-4o). Each data center with thousands of or even tens of thousands of AI servers is like a factory whose products are tokens. During this service, the metrics to evaluate the quality of experience (QoE) of end users include [14]:

- Time to first token (TTFT): The waiting time of the users to receive the response after entering their query. Typically, one second of TTFT is considered to be good enough.
- Time per output token (TPOT): The time to receive each generated token by the users. If we consider a TPOT as 100 milliseconds per token, equivalent to 10 tokens per second, leading to 600 words per minute that is faster than most of the users can read.
- Latency: The overall time of receiving an answer composed of tokens after the initial query from the users. To be more precise, it can be described as Latency $=$ TTFT + TOPT * (number of total generated tokens).
- Throughput: The generated tokens per second. It can be considered as a median value in a time window considering applicable queries and users.

It is important for users to receive high QoE while using the LLM service. However, better service requires more computational and networking resources. It is speculated that the LLM service will be provided with different levels relating to various subscription options or an one-time charge from users. It would be finally a compromise decision of users in the consideration of performance and cost.

## V. HYbRid LLM: GRaVitating TOWARDS THE EdGE

In our vision, the training of the LLMs happens in central clouds or more specifically AI data centers that are composed of training clusters, inference clusters, and dedicated storage, as shown in Fig. 4 The models in the AI data centers are the full model without any approximation, such as weight pruning and neural pruning. In order to reduce end-to-end latency and operation costs, the AI processing of LLM inference would be as close to the users as possible in a hierarchy, from central clouds to edge clouds, until the devices. The potential devices to provide LLM service include autonomous vehicles / IoVs (Internet of Vehicles), IoT (Internet of Things), Drones, XR (eXtended Reality) headsets [15], [16], Wearable devices (e.g., smartwatch), robots (e.g., humanoid robot assistant) [17], etc. Due to the computational capability of the devices, the LLM models running on the device would be optimized: a much smaller model size ( 7 to 10 times) while keeping the accuracy satisfied. In practice, the LLMs on the device generate a few tokens (typically four for example) as "drafts" which are then sent to the cloud for checking the accuracy. Once confirmed or corrected by the cloud LLMs, the speculative decoding process is complete and users receive the LLM answers.

In regard to communication, the xPU servers are connected by ToR (Top-of-Rack) Switches (electrical, optical, or hybrid) which are further connected together according to a specific topology, such as fat-tree, dragonfly, etc. via InfiniBand or Ethernet. Communication between central clouds and fog/edge clouds is WAN (Wide Area Network) which might also involve DCI (Data Center Interconnect) if a training task is distributed in multiple clouds. Between edge clouds and user devices, we believe wireless $5 \mathrm{G}$ or $6 \mathrm{G}$ in the future would be the most convenient and efficient way. Due to cybersecurity factors, like privacy and security requirements [18], all packets need to go through firewalls and be encrypted.

## VI. Carbon FootPrint and SuStainABILITY OF LLMS

The carbon footprint of a LLM comprises two fundamental components: the operational footprint and the embodied footprint [19]. The operational footprint encompasses emissions stemming from the energy consumption of the hardware used during pre-training, fine-tuning, and inference. The embodied footprint encapsulates the lifecycle emissions associated with hardware manufacturing, including material extraction, processing, and transportation. Accurately estimating the carbon footprint of LLMs before training is crucial for promoting environmentally conscious development practices. This enables researchers and developers to make informed decisions regarding model design and training procedures, promoting
the development of more sustainable LLMs. Tools like mlco2 [20] offer a preliminary assessment based on GPU usage, but they often have limitations, such as being unable to account for dense or mixture-of-experts (MoE) architectures. LLMCarbon [21], a recently introduced end-to-end carbon footprint projection model, addresses these shortcomings by providing more comprehensive and nuanced estimations for various LLM architectures, including dense and MoE models. For instance, LLMCarbon estimates that training a GPT-3 model could generate around $553.87 \mathrm{tCO}_{2} e q$ (tonnes of $\mathrm{CO}_{2}$ equivalent), compared to actual data, the disparity is only $+0.32 \%$ with the actual emit is $536.69 t C O_{2}$ eq. However, the training operational carbon footprint estimation made by mlco2 is $69 \%$ higher than the actual, because mlco2 assumes all devices consistently operate at the peak computing throughput using the peak power.

The sustainability of LLMs can be viewed through a two-fold lens: economic and environmental. LLMs achieve economic sustainability if the value they generate for organizations (e.g., enhanced efficiency, and improved customer service) exceeds the costs incurred from training, inference, and hardware maintenance. Environmental sustainability requires a multifaceted approach, encompassing renewable energy sources for data centers, energy-efficient model architectures, and hardware designed for low-power AI workloads. By prioritizing both economic and environmental considerations, LLM development can become a powerful force for positive change, driving innovation while minimizing its environmental impact.

![](https://cdn.mathpix.com/cropped/2024_06_04_f7e371b5f251db051297g-5.jpg?height=336&width=271&top_left_y=1559&top_left_x=163)

Haiwei Dong (haiwei.dong @ieee.org) is currently a Director and Principal Researcher with Huawei Canada, and an Adjunct Professor with the University of Ottawa. He was a Principal Engineer with Artificial Intelligence Competency Center, Huawei Technologies Canada, Toronto, ON, Canada, a Research Scientist with the University of Ottawa, Ottawa, ON, Canada, a Postdoctoral Fellow with New York University, New York City, NY, USA, a Research Associate with the University of Toronto, Toronto, ON, Canada, and a Research Fellow (PD) with the Japan Society for the Promotion of Science, Tokyo, Japan. He received the Ph.D. degree from Kobe University, Kobe, Japan in 2010 and the M.Eng. degree from Shanghai Jiao Tong University, Shanghai, China, in 2008. His research interests include artificial intelligence, multimedia, metaverse, and robotics. He also serves as a Column Editor of IEEE Multimedia Magazine; an Associate Editor of ACM Transactions on Multimedia Computing, Communications, and Applications; and an Associate Editor of IEEE Consumer Electronics Magazine. He is a Senior Member of IEEE, a Senior Member of ACM, and a registered Professional Engineer in Ontario.

![](https://cdn.mathpix.com/cropped/2024_06_04_f7e371b5f251db051297g-5.jpg?height=325&width=244&top_left_y=2274&top_left_x=171)

Shuang Xie (shuang.xie @ ieee.org) is currently a senior Machine Learning Engineer at Shopify, Canada. Her research interests include artificial intelligence, large language models, and computer vision. She received the Master degree from the University of Ottawa, Ottawa, Canada, 2019. Prior to that, she received the M.Eng. degree from Sichuan University, Sichuan, China, 2017. She is a Member of IEEE, and a Member of IEEE Women in Engineering.

## REFERENCES

[1] OpenAI. (2024). [Online]. Available: https://openai.com/index/ hello-gpt-4o/

[2] Z. Long, H. Dong, and A. El Saddik, "Interacting with New York city data by Hololens through remote rendering," IEEE Consumer Electronics Magazine, vol. 11, no. 5, pp. 64-72, 2022.

[3] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., "A survey of large language models," arXiv preprint arXiv:2303.18223, 2023.

[4] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann, "BloombergGPT: A Large Language Model for Finance," arXiv preprint arXiv:2303.17564, 2023.

[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in Neural Information Processing Systems, vol. 30, pp. 5998-6008, 2017.

[6] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "Lora: Low-rank adaptation of large language models," arXiv preprint arXiv:2106.09685, 2021.

[7] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., "Training language models to follow instructions with human feedback," Advances in Neural Information Processing Systems, vol. 35, pp. 27730-27 744, 2022.

[8] J. Chen, H. Lin, X. Han, and L. Sun, "Benchmarking large language models in retrieval-augmented generation," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 1775417762 .

[9] Z. Cao, W. Li, S. Li, and F. Wei, "Retrieve, rerank and rewrite: Soft template based neural summarization," in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 2018, pp. 152-161.

[10] A. Balaguer, V. Benara, R. L. de Freitas Cunha, R. d. M. Estevão Filho, T. Hendry, D. Holstein, J. Marsman, N. Mecklenburg, S. Malvar, L. O. Nunes et al., "Rag vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture," arXiv e-prints, pp. arXiv-2401, 2024.

[11] OpenAI. (2023). [Online]. Available: https://openai.com/index/ gpt-4-research/

[12] B. Peng, C. Li, P. He, M. Galley, and J. Gao, "Instruction tuning with GPT-4," arXiv preprint arXiv:2304.03277, 2023.

[13] D. Patel and D. Nishball. (2024) Groq inference tokenomics: Speed, but at what cost? [Online]. Available: https://www.semianalysis.com/p/ groq-inference-tokenomics-speed-but

[14] M. Agarwal, A. Qureshi, N. Sardana, L. Li, J. Quevedo, and D. Khudia. (2024) LLM inference performance engineering: Best practices. [Online]. Available: https://www.databricks.com/blog/ llm-inference-performance-engineering-best-practices

[15] H. Dong and Y. Liu, "Metaverse meets consumer electronics," IEEE Consumer Electronics Magazine, vol. 12, no. 3, pp. 17-19, 2023.

[16] Z. Long, H. Dong, and A. El Saddik, "Human-centric resource allocation for the metaverse with multiaccess edge computing," IEEE Internet of Things Journal, vol. 10, no. 22, pp. 19 993-20 005, 2023.

[17] H. Dong, Y. Liu, T. Chu, and A. El Saddik, "Bringing robots home: The rise of AI robots in consumer electronics," IEEE Consumer Electronics Magazine, vol. 13, 2024.

[18] "The future of AI is hybrid," Qualcomm, Tech. Rep., 2023.

[19] U. Gupta, Y. G. Kim, S. Lee, J. Tse, H.-H. S. Lee, G.-Y. Wei, D. Brooks, and C.-J. Wu, "Chasing carbon: The elusive environmental footprint of computing," in Proceedings of the IEEE International Symposium on High-Performance Computer Architecture. IEEE, 2021, pp. 854-867.

[20] A. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres, "Quantifying the carbon emissions of machine learning," arXiv preprint arXiv:1910.09700, 2019.

[21] A. Faiz, S. Kaneda, R. Wang, R. Osi, P. Sharma, F. Chen, and L. Jiang, "LLMCarbon: Modeling the end-to-end carbon footprint of large language models," arXiv preprint arXiv:2309.14393, 2023.

