# StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving 

Chang Gao ${ }^{\boldsymbol{\phi} *}$, Haiyun Jiang ${ }^{\diamond \dagger}$, Deng Cai ${ }^{\ominus}$, Shuming Shi ${ }^{\ominus}$, Wai Lam ${ }^{\boldsymbol{\bullet}}$<br>${ }^{\boldsymbol{}}$ The Chinese University of Hong Kong ${ }^{\diamond}$ Sun Yat-sen University ${ }^{\circ}$ Tencent AI Lab<br>\{gaochang,wlam\}@se.cuhk.edu.hk haiyunjiangnlp@gmail.com<br>\{jcykcai,shumingshi\}@tencent.com


#### Abstract

Most existing prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other instances and lack task-level consistency across the selected few-shot examples. To address these limitations, we propose a comprehensive framework, StrategyLLM, allowing LLMs to perform inductive reasoning, deriving general strategies from specific task instances, and deductive reasoning, applying these general strategies to particular task examples, for constructing generalizable and consistent few-shot prompts. It employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. Experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning $(34.2 \% \rightarrow 38.8 \%)$, commonsense reasoning $(70.3 \% \rightarrow 72.5 \%)$, algorithmic reasoning $(73.7 \% \rightarrow 85.0 \%)$, and symbolic reasoning $(30.0 \% \rightarrow 79.2 \%)$. Further analysis reveals that StrategyLLM is applicable to various LLMs and demonstrates advantages across numerous scenarios.


## 1 Introduction

Recent advances in large language models (LLMs) have facilitated the development of prompting techniques [26, 43, 20, 8]. In particular, chain-of-thought (CoT) prompting methods [43, 6, 12, 41], which condition LLMs on a few task examples with step-by-step solutions, guide LLMs to break down complex reasoning processes into simpler steps. These approaches have markedly improved performance compared to standard few-shot prompting across a variety of tasks.

Despite their potential, current CoT approaches employing few-shot prompts with instance-specific solutions may face challenges in terms of generalizability and consistency. Concerning generalizability, the solution can be highly specific to the question in each instance, limiting its applicability to other instances. For example, as illustrated in the left part of Figure 1, a solution for a particular system of linear equations with two variables may not provide valuable insights for addressing another system with three variables. Furthermore, the solutions in different instances within the few-shot prompt may exhibit a lack of task-level consistency, which complicates the process for LLMs to develop effective solutions for tackling new instances. As demonstrated in the left part of Figure 1 , the two specific solutions are based on different approaches: Solution 1 employs expression substitution, while Solution 2 utilizes equation subtraction, which may not provide consistent guidance for LLMs to solve new instances. To address these limitations, it is crucial to incorporate effective[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_e4f6f23a3e3cdd5ee862g-02.jpg?height=591&width=1152&top_left_y=236&top_left_x=476)

Figure 1: Comparison of specific solutions and strategy-based solutions.

problem-solving strategies and develop consistent strategy-based solutions within few-shot prompts. The right part of Figure 1 presents an effective strategy, i.e., Gaussian Elimination Method, offering generalizable steps applicable to any system of linear equations. By providing this strategy and consistently applying it across various instances in the few-shot prompt, LLMs can be better equipped to generate effective solutions for new task instances.

This paper aims to construct generalizable and consistent strategy-based few-shot prompts for various tasks automatically, while being highly cost-efficient. Our proposed framework, StrategyLLM, draws inspiration from human cognitive processes to derive general problem-solving strategies. This approach enables LLMs to reason inductively, i.e., deriving general strategies from specific task instances, and deductively, i.e., applying general strategies to particular task examples, to formulate prompts. An example of strategy-based prompts can be seen in Figure 3. The inductive reasoning process enhances generalizability by formulating general problem-solving strategies, while the deductive reasoning process improves consistency by producing consistent solutions using a given strategy. Developing effective problem-solving strategies is crucial to the success of our framework. To achieve this, we design StrategyLLM as a multi-agent collaboration framework comprising four LLM-based agents-strategy generator, executor, optimizer, and evaluator, as shown in Figure 2. The strategy generator initially creates a pool of strategies that are executed on task examples to assess accuracy, with qualified strategies cached based on a threshold and further evaluated. Unqualified ones may be optimized and re-evaluated iteratively. Through the collaboration of these intelligent agents, our framework is capable of autonomously generating, evaluating, and selecting effective strategies for various tasks and eliminating the need for human involvement.

Crucially, the strategy-based few-shot prompt generation phase is applied once for a given task, after which the learned prompt can be employed for inference on the entire test set. This inference process does not require any additional input beyond the standard few-shot prompting settings. The prompt generation process is highly cost-effective as it necessitates only a few task examples. In particular, StrategyLLM expends less than $\$ 0.24$ to develop a strategy-based prompt for a variety of tasks using the latest version of GPT-3.5-Turbo.

We conduct comprehensive evaluations of StrategyLLM on 13 datasets across 4 challenging tasks: math reasoning, commonsense reasoning, algorithmic reasoning, and symbolic reasoning. The experimental results reveal the following key findings: (1) StrategyLLM outperforms competitive baselines on all tasks without using any human-annotated reasoning processes; (2) StrategyLLM can be applied to various LLMs and is robust to different groups of task examples; (3) StrategyLLM can generate generalizable and consistent prompts in a cost-effective manner. These findings demonstrate the potential of StrategyLLM as an effective, efficient, and reliable problem-solving framework.

## 2 StrategyLLM

Our StrategyLLM framework is designed to efficiently create strategy-based few-shot prompts for a wide range of tasks. Subsequently, these prompts can be utilized for inference. In this section, we will introduce our framework in detail. The inference procedure will be discussed in Section 3 .

![](https://cdn.mathpix.com/cropped/2024_06_04_e4f6f23a3e3cdd5ee862g-03.jpg?height=437&width=1333&top_left_y=237&top_left_x=385)

Figure 2: Overview of StrategyLLM. Initially, the strategy generator creates a pool of strategies, which are then applied by the strategy executor to task examples to calculate execution accuracy. Qualified strategies meeting a pre-defined threshold are cached, and if necessary, unqualified strategies are optimized and re-evaluated in iterative cycles. Once a sufficient number of qualified strategies are obtained or the maximum iteration number is reached, the top $k$ strategies are ranked by execution accuracy and evaluated using a validation set.

Overview of StrategyLLM As presented in Figure2, our framework consists of four key agents: strategy generator, executor, optimizer, and evaluator. The prompts of the generator, executor, and optimizer are in Appendix C. Typically, only a few task examples are used in the collaboration process, making our framework highly efficient.

The collaboration process begins with the strategy generator formulating a pool of strategies based on its understanding of the target task. Subsequently, the strategy executor applies each strategy to a set of task examples to yield its execution result and compute its execution accuracy. Strategies that meet or exceed a pre-set threshold of execution accuracy are deemed qualified and are cached with their corresponding execution results and accuracy. If the number of qualified strategies is less than a pre-defined number $k$, the optimizer refines the unqualified strategies using their execution results. These enhanced strategies are then sent back to the strategy executor for the next iteration. This cycle may repeat until a sufficient number of qualified strategies are achieved or the maximum iteration limit is reached. Following this, all cached strategies are ranked based on their execution accuracy, and the top $k$ strategies are selected. Lastly, the strategy evaluator constructs strategy-based few-shot prompts for each candidate strategy using itself and its execution result and assesses all candidate strategies using their corresponding prompts for inference on a validation set.

Notations We use $p, q, s t$, so, and $a$ to denote the prompt, question, strategy, solution, and answer, respectively. During inference, given a question $q$, the language model $M:(p, q) \rightarrow(s o, a)$ generates a solution so and an answer $a$ for it conditioned on the prompt $p$. We denote the target task as $t$, its definition as $d$, and the set of task examples as $\mathcal{E}$. Each example in $\mathcal{E}$ is a $(q, a)$ pair.

Strategy Definition In this paper, a task-solving strategy $s t$ is defined as a systematic approach designed for universal application across task examples, comprising a series of subtasks that encode task knowledge to address the target task $t$. It is characterized by the following properties: (1) Task-Level Applicability: The strategy is formulated in a manner that allows for its application across all task instances, ensuring universality and consistency in its implementation. (2) Structured Organization: The strategy comprises a sequence of subtasks that are organized in a logical order to collectively tackle the target task. These subtasks are interconnected and contribute to the overall achievement of the task objective. (3) Task Knowledge Encoding: The strategy encapsulates general task knowledge and principles, avoiding any specific details unique to individual task examples. These properties collectively contribute to the effectiveness and efficiency of a strategy by promoting consistency, clarity, and informed decision-making in addressing task-level challenges. By embodying these properties, a strategy can serve as a valuable tool to navigate complex tasks and achieve optimal outcomes. An example of the strategy is presented in Figure 3

Strategy Generator $G \quad$ The strategy generator, represented as $G:(\mathcal{E}, d, n) \rightarrow\left\{s t_{j}\right\}_{j=1}^{n}$, aims to generate $n$ diverse strategies for the target task $t$ based on a set of task examples $\mathcal{E}$ and the task definition $d$ using temperature sampling.

| Standard Prompt | Strateav: $\quad$ Strategy-based Prompt |
| :---: | :---: |
| Question: Jason had 20 lollipops. He gave <br> Denny some lollipops. Now Jason has 12 lollipops. <br> How many lollipops did Jason give to Denny? <br> Answer: The answer is 8 . <br> ...... | Strategy: <br> 1. Identify the given information in the word problem <br> 2. Determine the desired information or what needs to be calculated <br> 3. Identify the mathematical operation needed to solve the problem <br> 4. Calculate the solution using the given information and the appropriate mathematical operation <br> 5. Verify the solution by checking if it satisfies the conditions stated in the problem <br> Question: Jason had 20 Iollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. <br> How many lollipops did Jason give to Denny? <br> Solution: 1. Given information: Jason had 20 Iollipops and now he has 12 lollipops. <br> 2. Desired information: Number of Iollipops Jason gave to Denny. <br> 3. Mathematical operation: Subtraction. <br> 4. Calculation: find the difference between the initial and final number of lollipops: 20 - $12=8$. <br> 5. Verification: If Jason had 20 lollipops and gave 8 to Denny, he would have 12 lollipops left. <br> Answer: The answer is 8 . <br> ..... |
| CoT Prompt |  |
| Question: Jason had 20 lollipops. He gave <br> Denny some lollipops. Now Jason has 12 lollipops. <br> How many lollipops did Jason give to Denny? <br> Solution: Jason started with 20 lollipops. Then he <br> had 12 after giving some to Denny. So he gave <br> Denny 20 - $12=8$. <br> Answer: The answer is 8 . |  |

Figure 3: Comparison of the strategy-based, standard, and chain-of-thought (CoT) [43] prompt.

Strategy Executor $X \quad$ The strategy executor, denoted as $X:(\mathcal{E}, d, s t) \rightarrow\left(\mathcal{R}_{s t}, e^{2 c c} c_{s t}\right)$, writes solutions to a set of task examples $\mathcal{E}$ following the strategy st to obtain the execution result $\mathcal{R}_{s t}=$ $\{(q, s o, a)\}_{i=1}^{|\mathcal{E}|}$ of st. The execution accuracy eacc ${ }_{s t}$ is calculated as the proportion of examples whose solutions yield correct answers, reflecting the degree of alignment between the strategy and task. Therefore, we select strategies with high execution accuracy as qualified strategies.

Strategy Optimizer $O \quad$ The strategy optimizer, represented as $O:\left(\mathcal{E}, d, s t, \mathcal{R}_{s t}\right) \rightarrow s t^{o}$, optimize the strategy st according to its execution result $\mathcal{R}_{s t}$ to obtain the updated strategy $s t^{o}$. Firstly, the strategy optimizer $O$ analyzes why some solutions in $\mathcal{R}_{s t}$ are not correct and provides suggestions for improving st. Secondly, it modifies st to obtain $s t^{\circ}$ based on the analysis and suggestions.

Strategy Evaluator $E \quad$ We select top $k$ candidate strategies according to the execution accuracy. However, to ensure efficiency, we use a limited number of task examples for execution, making the execution accuracy not a very informative metric for choosing strategies. Therefore, we introduce a strategy evaluator to further evaluate the candidate strategies on a validation set $\mathcal{V}$. This process only requires to perform inference once for each candidate strategy and is efficient. The strategy evaluator, denoted as $E:\left(s t, \mathcal{R}_{s t}, \mathcal{V}\right) \rightarrow v a c c_{s t}$, computes the validation accuracy $v a c c_{s t}$ of the strategy st on $\mathcal{V}$. To achieve this, it constructs the strategy-based few-shot prompt $p_{s t}=\left(s t, \mathcal{R}_{s t}\right)$ and conducts inference on $\mathcal{V}$ using $p_{s t}$. An example of strategy-based prompts is presented in Figure

![](https://cdn.mathpix.com/cropped/2024_06_04_e4f6f23a3e3cdd5ee862g-04.jpg?height=44&width=1388&top_left_y=1515&top_left_x=366)
answers are correct, reflecting the effectiveness of st in real-world scenarios. Strategies with high validation accuracy can be used for inference.

## 3 Inference

Through collaborative efforts among multiple agents, we have obtained multiple candidate strategies, each with its few-shot prompt and validation accuracy. Depending on the task at hand, we can select one or more strategies with high validation accuracy for inference. For simple or specific tasks, a single optimal strategy that solves all task examples effectively may exist, making it sufficient to use only one strategy. However, for complex or diverse tasks, it is unlikely to find a strategy with absolute superiority. In such cases, adopting multiple strategies for inference is more appropriate, as they may excel in different task examples. To harness the strengths of multiple strategies, we employ two methods. The first method involves taking a majority vote on all answers obtained by multiple strategies, akin to the self-consistency (SC) method [41]. The second method requires LLMs to determine the final answer by considering the solutions derived from multiple strategies in a zero-shot (ZS) manner, making it more proper for complex and diverse tasks. We denote the first and second methods as StrategyLLM-SC and StrategyLLM-ZS, respectively. The prompt for the second approach is provided in Appendix D

## 4 Experiments

### 4.1 Experimental Setup

Evaluation Tasks and Datasets We evaluate StrategyLLM on a variety of tasks:

Table 1: Experimental results on the math reasoning task. The numbers in parentheses represent the relative improvement compared to CoT-SC.

| Methods | AL | PA | IA | CP | NT | GE | PC | Avg |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :--- |
| SP | 32.0 | 50.0 | 17.5 | 27.0 | 20.5 | 21.0 | 20.5 | 26.9 |
| SolutionLLM | 58.5 | 56.5 | 13.5 | 33.0 | 32.0 | 28.0 | 19.5 | 34.4 |
| CoT | 57.0 | 57.5 | 15.0 | 33.5 | 28.0 | 23.0 | 20.0 | 33.4 |
| CoT-SC | 59.0 | 62.0 | 16.5 | 34.5 | 28.0 | 24.5 | 15.0 | 34.2 |
| StrategyLLM | 58.5 | 57.5 | 18.0 | 35.0 | 29.5 | 24.5 | 22.5 | 35.1 |
| StrategyLLM-SC | 60.0 | 61.5 | 18.0 | 38.5 | 30.5 | 28.0 | $\mathbf{2 4 . 0}$ | $37.2(+8.8 \%)$ |
| StrategyLLM-ZS | $\mathbf{6 4 . 5}$ | $\mathbf{6 5 . 5}$ | $\mathbf{1 9 . 0}$ | $\mathbf{3 9 . 0}$ | $\mathbf{3 2 . 5}$ | $\mathbf{2 8 . 5}$ | 22.5 | $\mathbf{3 8 . 8}(+\mathbf{1 3 . 4 \% )}$ |

Table 2: Experimental results on the commonsense, algorithmic, and symbolic reasoning tasks. The numbers in parentheses represent the relative improvement compared to CoT-SC.

| Methods | Commonsense |  |  | Algorithmic |  |  | Symbolic |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | StrategyQA | DU | Avg | WS | MA | Avg | $\mathrm{LLC}-4$ | $\mathrm{LLC}-8$ | $\mathrm{LLC}-16$ | Avg |
| $\mathrm{SP}$ | 56.5 | 48.5 | 52.5 | 73.3 | 2.0 | 37.7 | 0 | 0 | 0 | 0 |
| SolutionLLM | 59.5 | 52.0 | 55.8 | 74.7 | 55.3 | 65.0 | 81.5 | 25.5 | 0 | 35.7 |
| CoT | 64.0 | 70.5 | 67.3 | 67.2 | 84.0 | 75.6 | 68.5 | 22.0 | 0 | 30.2 |
| CoT-SC | 70.0 | 70.5 | 70.3 | 61.3 | 86.0 | 73.7 | 68.0 | 22.0 | 0 | 30.0 |
| StrategyLLM | 67.5 | 68.5 | 68.0 | 80.0  | 86.7 | 83.4 | 98.0 | 86.5 | 51.5 | 78.7 |
| StrategyLLM-SC | 71.0 | 74.0 | $72.5(+3.1 \%)$ | 79.3 | 90.7 | $85.0(+15.4 \%)$ | 98.0 | 87.0 | 52.5 | $79.2(+164.0 \%)$ |
| StrategyLLM-ZS | 70.0 | 72.5 | $71.3(+1.4 \%)$ | 78.7 | 89.3 | $84.0(+14.1 \%)$ | 98.0 | 86.0 | 44.0 | $76.0(+153.3 \%)$ |

- Math Reasoning: We use the challenging MATH benchmark [16] which comprises problems from mathematics competitions that require more than standard K-12 mathematics tools. It consists of seven datasets of different subjects, namely, Algebra (AL), Prealgebra (PA), Intermediate Algebra (IA), Counting and Probability (CP), Number Theory (NT), Geometry (GE), and Precalculus (PC).
- Commonsense Reasoning: We employ StrategyQA [14] and the Date Understanding (DU) task from Big-Bench Hard [38, 9]. StrategyQA necessitates inferring a multi-hop strategy to answer questions, while the DU task involves deducing a date from a given context.
- Algorithmic Reasoning: We adopt the Word Sorting (WS) task and the Multi-step Arithmetic (MA) task from Big-Bench Hard [38, 9]. The WS task involves sorting a list of words lexicographically, and the MA task requires solving multi-step equations with basic arithmetic operations.
- Symbolic Reasoning: We utilize the Last Letter Concatenation (LLC) task from [43], which requires concatenating the last letters of words in a sequence. In the few-shot prompt, the model only sees examples with two words. To evaluate the generalization abilities of different methods, we construct three out-of-distribution test sets (LLC-4, LLC-8, and LLC-16) with 4, 8, and 16 words in a sequence, respectively.

Baselines We conduct experiments in the few-shot setting and compare StrategyLLM with the following baselines:

- Standard Prompting (SP): SP is the most direct approach for problem-solving. In SP, the prompt $p$ contains a set of question-answer pairs without intermediate reasoning steps.
- Chain-of-Thought (CoT) Prompting [43]: CoT incorporates step-by-step solutions for questions in the prompt $p$ to elicit the multi-step reasoning capabilities of LLMs. We use few-shot CoT prompts from [43] for GSM8K, StrategyQA, DU, and LLC, and prompts from [38] for WS and MA. For MATH datasets, we create few-shot CoT prompts by randomly sampling 4 examples from each dataset's training set since these datasets contain human-annotated solutions. The CoT prompts for these datasets are in Appendix $\mathrm{H}$.
- Self-Consistency with CoT (CoT-SC) [41]: CoT-SC generates a set of solutions using CoT via temperature sampling to obtain multiple answers. Subsequently, it takes a majority vote over these answers to determine the final answer. For experiments, we sample 3 reasoning paths using temperature sampling with a temperature of 0.7 .

Table 3: Experimental results on two math reasoning datasets, namely $\mathrm{AL}$ and $\mathrm{CP}$, with different groups of examples.

| Methods | AL-dev | AL-random | CP-dev | CP-random |
| :--- | :---: | :---: | :---: | :---: |
| SP | 36.0 | $29.1 \pm 3.9$ | 25.5 | $26.8 \pm 2.5$ |
| SolutionLLM | 58.0 | $56.5 \pm 2.2$ | 31.0 | $32.2 \pm 2.8$ |
| CoT | 57.5 | $55.1 \pm 1.5$ | 34.0 | $33.4 \pm 1.2$ |
| CoT-SC | 59.5 | $58.3 \pm 1.2$ | 31.5 | $33.0 \pm 1.2$ |
| StrategyLLM | 57.0 | $54.7 \pm 2.5$ | 34.5 | $35.6 \pm 2.3$ |
| StrategyLLM-SC | $\mathbf{6 4 . 0}$ | $58.9 \pm 1.1$ | 36.5 | $38.4 \pm 1.3$ |
| StrategyLLM-ZS | 62.5 | $\mathbf{6 0 . 8} \pm \mathbf{2 . 6}$ | $\mathbf{3 8 . 5}$ | $\mathbf{3 8 . 8} \pm \mathbf{1 . 7}$ |

- SolutionLLM: We construct this baseline to leverage LLMs to directly write the solution for each example in the few-shot prompts using greedy decoding, without using any strategies. The prompt of SolutionLLM is in Appendix E Since both SolutionLLM and StrategyLLM generate prompts using LLMs, we can eliminate the potential effect of human expertise in the comparison, isolating the impact of incorporating effective strategies.

Implementation Details We employ GPT-3.5 (gpt-3.5-turbo-16k-0613) [32] as the language model for our main experiments, serving as the backend for the strategy generator, executor, optimizer, and evaluator. For a fair comparison with baselines such as CoT, we use the same examples in their few-shot prompts for strategy generation, execution, and optimization. We select the top 1 or 3 strategies with the highest validation accuracy for inference. This allows us to demonstrate the performance of the optimal strategy and the benefits of using multiple strategies. We adopt greedy decoding for inference. Details of the strategies for each dataset can be found in Appendix G. The validation set size is 100 for all the datasets. For datasets with over 200 test examples, we randomly sample 200 examples for testing to reduce API costs. More details can be found in Appendix B

### 4.2 Main Results

Tables 1 and 2 present the experimental results of StrategyLLM and several baselines across four reasoning tasks. We have the following observations:

- StrategyLLM is an effective and efficient framework for problem-solving. StrategyLLM using multiple strategies, i.e., StrategyLLM-SC and StrategyLLM-ZS, outperforms all baselines across the four reasoning tasks. Furthermore, StrategyLLM employing the optimal strategy consistently outperforms CoT and SolutionLLM. Notably, StrategyLLM automatically constructs generalizable and consistent few-shot prompts for tackling various tasks without human expertise, while CoT relies on human-annotated examples for each task.
- Explicitly incorporating effective strategies significantly enhance the complex reasoning and out-ofdistribution (OOD) generalization abilities of LLMs. For example, our framework demonstrates more considerable improvements on the MATH benchmark compared to the simpler commonsense reasoning datasets. Furthermore, StrategyLLM substantially surpasses CoT and SolutionLLM on the three OOD test sets of the LLC task, showcasing the generalizability of effective strategies.
- Adopting multiple strategies brings obvious benefits on complex or diverse tasks. The performance of StrategyLLM is significantly improved by using multiple strategies on the math and commonsense reasoning tasks. The benefits of leveraging multiple strategies on simpler or more specific tasks, i.e., symbolic and algorithmic reasoning, is less significant. These observations indicate that our framework is capable of creating multiple complementary strategies for diverse or complex tasks Furthermore, StrategyLLM-ZS outperforms StrategyLLM-SC on the math reasoning task, showing that allowing LLMs to determine the answer is more appropriate for intricate tasks.


## 5 Analysis

Evaluating the robustness of StrategyLLM We conduct an investigation to assess the robustness of our StrategyLLM framework with respect to varying groups of examples. For this purpose, we

Table 4: Experimental results of closed-source models on the CP, StrategyQA, and MA datasets. The numbers in parentheses represent the relative improvement compared to CoT-SC.

| Methods | GPT-4 |  |  |  | Claude-3-Sonnet |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | CP | StrategyQA | MA | Avg | $\mathrm{CP}$ | StrategyQA | MA | Avg |
| SolutionLLM | 52.0 | 75.5 | 96.7 | 74.7 | 21.0 | 73.5 | 69.3 | 54.6 |
| CoT | 49.5 | 80.5 | 92.7 | 74.2 | 26.0 | 69.0 | 72.7 | 55.9 |
| CoT-SC | 54.5 | 83.5 | 94.7 | 77.6 | 26.0 | 75.0 | 76.7 | 59.2 |
| Stra | 52.5 | 81 . $\quad$. | 98.7 | . | 28.0 | 75.0 | 83.3 | 6 |
| StrategyLLM-S | 56.0 | 83.5 | $\mathbf{9 8 . 7}$ | 79.4 (+2.4\%) | 28.0 | 77.0 | $\mathbf{8 8 . 0}$ | 64.3 |

Table 5: Experimental results of open-source models on the CP, StrategyQA, and MA datasets. The numbers in parentheses represent the relative improvement compared to CoT-SC.

![](https://cdn.mathpix.com/cropped/2024_06_04_e4f6f23a3e3cdd5ee862g-07.jpg?height=637&width=1377&top_left_y=798&top_left_x=379)

select two math reasoning datasets with diverse examples, namely $\mathrm{AL}$ and $\mathrm{CP}$, and randomly sample 5 distinct groups of examples from their respective training sets. We then report the mean and standard deviation of the results. Additionally, we employ the validation set to identify a group of 4 examples from the training set. Specifically, we use the OpenAI embedding model API (the text-embedding3-large model) to map training and validation questions to embeddings and subsequently select the 4 training examples with the highest cosine similarities to all validation examples. We designate these groups of examples as AL-dev and CP-dev, respectively. The results, as presented in Table 3. demonstrate that StrategyLLM consistently delivers satisfactory performance on both datasets, suggesting that StrategyLLM is a robust and reliable framework for problem-solving.

Exploring the universality of StrategyLLM To investigate the universality of our StrategyLLM framework, we apply it to a variety of LLMs to evaluate its effectiveness. For closed-source models, we utilize GPT-4 (gpt-4-0613) [31] and Claude-3-Sonnet (claude-3-sonnet-20240229) [2]. For opensource models, we employ Meta-Llama-3-8B-Instruct, Meta-Llama-3-70B-Instruct [1], Mixtral-8x7BInstruct-v0.1, and Mixtral-8x22B-Instruct-v0.1 [18]. We conduct experiments on the CP, StrategyQA, and MA datasets, which represent three distinct reasoning tasks. The results, summarized in Tables 4 and 5. reveal that integrating effective strategies for constructing generalizable and consistent fewshot prompts yields significant benefits across a range of model capabilities and task complexities, underscoring the framework's universality. StrategyLLM notably enhances performance in opensource models such as Meta-Llama-3-8B-Instruct and Mixtral-8x7B-Instruct-v0.1, particularly on the $\mathrm{CP}$ and MA datasets which demand complex reasoning, indicating the effectiveness of our framework in scenarios requiring sophisticated problem-solving. These findings further corroborate StrategyLLM's robustness and reliability as a problem-solving framework.

Comparing reasoning via task-level strategy and instance-specific planning Our framework facilitates generalizable and consistent reasoning by developing task-level strategies. To evaluate

Table 6: Comparison of Plan-and-Solve, CoT+Strategy, and StrategyLLM.

| Methods | CP | StrategyQA | MA | Avg |
| :--- | :---: | :---: | :---: | :---: |
| Plan-and-Solve | 26.0 | 54.0 | 69.3 | 49.8 |
| Plan-and-Solve-SC | 27.5 | 64.5 | 70.0 | 54.0 |
| CoT+Strategy | 30.5 | 63.0 | 62.7 | 52.1 |
| CoT+Strategy-SC | 36.5 | 70.0 | 70.0 | 58.8 |
| StrategyLLM | 35.0 | 67.5 | 86.7 | 63.1 |
| StrategyLLM-SC | $\mathbf{3 8 . 5}$ | $\mathbf{7 1 . 0}$ | $\mathbf{9 0 . 7}$ | $\mathbf{6 6 . 7}$ |

the necessity of effective task-level strategies, we compare our framework against two baselines: (1) Plan-and-Solve Prompting [40], which directs LLMs to formulate specific plans for each test instance at inference time and execute these plans to solve the instances; (2) CoT+Strategy, which combines the CoT prompt with instructions that guide LLMs to devise a task-solving strategy and apply it to a specific test example at inference time. The prompt for CoT+Strategy is detailed in Appendix F

The performance of GPT-3.5 on the CP, StrategyQA, and MA datasets, representing three distinct reasoning tasks, is presented in Table 6 Our observations are as follows: (1) StrategyLLM significantly outperforms both Plan-and-Solve Prompting and CoT+Strategy across all three datasets. This highlights the superiority of generalizable task-level strategies over instance-specific plans in enhancing performance across various problem-solving contexts. This improvement can be attributed to two key factors: (a) our task-level strategies encapsulate essential task-level knowledge, thereby providing professional and high-level guidance; (b) generating high-quality, specific plans for each test example at inference time is inherently challenging, making it difficult to ensure the quality of these plans. (2) Even when explicitly encouraged to devise a general task-solving strategy in the CoT+Strategy method, the LLM tends to produce strategies that are highly specific to the test example and encode limited task-level knowledge. This underscores the necessity of creating generalizable strategy-based few-shot prompts.

Analyzing the cost of strategy-based prompt generation In this analysis, we evaluate the cost of the strategy-based prompt generation process. The process includes the strategy generator, executor, optimizer, and evaluator, each contributing to the overall cost for each reasoning task. Table 7 details the average cost incurred by our StrategyLLM framework in generating a candidate strategy-based prompt, calculated by dividing the total cost of the process by the number of candidate strategies $k$. The costs are presented in terms of input and output tokens and the money associated with using GPT-3.5-Turbo. The results indicate that our framework is economically efficient. The average cost for gpt -3.5 -turbo $-16 k-0613$ ranges from $\$ 0.33$ to $\$ 1.12$ across the four reasoning tasks. For the latest version of GPT-3.5-Turbo, specifically gpt-3.5-turbo-0125, the cost is considerably lower, ranging from $\$ 0.08$ to $\$ 0.24$. Generally, tasks of higher complexity consume more tokens due to their inherently longer solutions.

Examining results across various difficulty levels The problems in the MATH benchmark are classified by difficulty on a scale of 1 to 5. The easiest problems are assigned a difficulty level of 1 , while the most challenging problems are given a difficulty level of 5. Figure 4 illustrates the performance of CoT-SC and StrategyLLM-SC on the seven datasets within the MATH benchmark, considering different difficulty levels. It is evident that the enhanced performance of StrategyLLM-SC over CoT-SC stems from its ability to tackle more complex problems, underscoring the significance of generalizable strategies in augmenting intricate reasoning.

More analysis can be found in Appendix A

![](https://cdn.mathpix.com/cropped/2024_06_04_e4f6f23a3e3cdd5ee862g-08.jpg?height=523&width=686&top_left_y=1817&top_left_x=1061)

Figure 4: Comparison of CoT-SC and StrategyLLM-SC performance on the MATH benchmark across various difficulty levels.

Table 7: Average cost of prompt generation across four reasoning tasks.

|  | Math | Commonsense Algorithmic | Symbolic |  |
| :--- | :---: | :---: | :---: | :---: |
| \# Input Tokens | $287.83 \mathrm{~K}$ | $228.67 \mathrm{~K}$ | $107.27 \mathrm{~K}$ | $70.94 \mathrm{~K}$ |
| \# Output Tokens | $63.14 \mathrm{~K}$ | $33.15 \mathrm{~K}$ | $32.95 \mathrm{~K}$ | $28.48 \mathrm{~K}$ |
| Cost of gpt-3.5-turbo-16k-0613 | $\$ 1.12$ | $\$ 0.82$ | $\$ 0.45$ | $\$ 0.33$ |
| Cost of gpt-3.5-turbo-0125 | $\$ 0.24$ | $\$ 0.16$ | $\$ 0.10$ | $\$ 0.08$ |

## 6 Related Work

Prompting LLMs for Problem Solving The prominent chain-of-thought (CoT) prompting approach [43] has inspired a variety of prompting methods aimed at enhancing the problem-solving abilities of LLMs. These methods include using programming languages to describe the reasoning process [6, 13, 28], representing the reasoning process with complex structures such as trees or graphs [46, 3, 36, 47], applying task decomposition [49, 19, 34, 4], implementing self-correction with automatic feedback [22, 29, 30, 5, 7], and combining different prompting techniques [27, 50]. However, most of these approaches require manual annotation of reasoning processes, limiting their generalizability and flexibility. By comparison, our StrategyLLM framework can automatically construct strategy-based few-shot prompts for any task, ensuring generalizable and consistent solutions following effective strategies. This approach sets it apart from existing automatic prompt construction methods [48, 37, 45], which may generate inconsistent solutions within the prompt. The plan-and-solve prompting method [40] aims to address missing-step errors by requesting LLMs to generate a plan before solving a specific example in a zero-shot manner. The plan is instance-specific and significantly different from the task-solving strategy which can be applied to all task examples. The learning-to-plan approach [15】 learns a text plan for each task to assist LLMs in problem-solving. The plan, which is not necessarily a strategy, can be any instruction helpful for solving the task. Moreover, it demands a large training and validation set during the learning process, resulting in high costs. In contrast, our framework is efficient and cost-effective.

LLM-based Autonomous Agents The adoption of autonomous agents driven by LLMs across various disciplines is revolutionizing our methodologies for tackling problems, making decisions, and fostering innovation [39, 44]. These agents have been utilized to enhance the reasoning capabilities of LLMs [42, 24, 11], contribute to social simulation [33, 23, 25, 21], and advance software development [35, 17, 10]. In this paper, we employ multiple LLM-based agents to collaborate in the generation, execution, optimization, and evaluation of problem-solving strategies.

## 7 Discussion

Limitation and Impact The key idea behind StrategyLLM is to harness the knowledge and reasoning capabilities of LLMs to develop and refine task-solving strategies tailored to specific tasks. By utilizing the extensive knowledge embedded in these LLMs, which are trained on diverse data sources spanning multiple domains, StrategyLLM is able to generate generalizable strategies that incorporate domain-specific expertise. However, if the model possesses limited knowledge in a particular domain, it is unlikely to create effective strategies for that domain. In such cases, merely optimizing the prompt may not significantly improve performance, and domain-specific continual training may be necessary. As LLMs continue to expand their knowledge bases and enhance their reasoning capabilities, their ability to generate generalizable strategies for diverse tasks is expected to improve, implying the potential of our StrategyLLM framework.

Conclusion This paper proposes StrategyLLM, harnessing the power of LLMs to construct generalizable and consistent few-shot prompts for various tasks efficiently. Our framework's effectiveness and reliability are substantiated through extensive evaluations on four challenging tasks: mathematical reasoning, commonsense reasoning, algorithmic reasoning, and symbolic reasoning. Further analysis reveals that our framework exhibits robustness across different task example groups, application to various LLMs, cost-efficiency in prompt generation, and effectiveness in complex reasoning.

## References

[1] Meta AI. Introducing meta llama 3: The most capable openly available llm to date. 2024.

[2] Anthropic. Introducing the next generation of claude. 2024.

[3] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17682-17690, 2024.

[4] Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, and Jianshu Chen. Skills-in-context prompting: Unlocking compositionality in large language models. arXiv preprint arXiv:2308.00304, 2023.

[5] Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Kenneth Heafield. Iterative translation refinement with large language models. arXiv preprint arXiv:2306.03856, 2023.

[6] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023.

[7] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, 2024.

[8] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. A survey of chain of thought reasoning: Advances, frontiers and future. arXiv preprint arXiv:2309.15402, 2023.

[9] BIG-Bench collaboration. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023.

[10] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv preprint arXiv:2304.07590, 2023.

[11] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.

[12] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, 2023.

[13] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 10764-10799. PMLR, 23-29 Jul 2023.

[14] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.

[15] Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, and Nan Duan. Learning to plan with natural language. arXiv preprint arXiv:2304.10464, 2023.

[16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.

[17] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024.

[18] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

[19] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representations, 2023.

[20] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.

[21] Grgur Kovač, Rémy Portelas, Peter Ford Dominey, and Pierre-Yves Oudeyer. The socialai school: Insights from developmental psychology towards artificial socio-cultural agents. arXiv preprint arXiv:2307.07871, 2023.

[22] Miaoran Li, Baolin Peng, and Zhu Zhang. Self-checker: Plug-and-play modules for factchecking with large language models. arXiv preprint arXiv:2305.14623, 2023.

[23] Siyu Li, Jin Yang, and Kui Zhao. Are you in a masquerade? exploring the behavior and impact of large language model driven social bots in online social networks. arXiv preprint arXiv:2307.10337, 2023.

[24] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.

[25] Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. Agentsims: An open-source sandbox for large language model evaluation. arXiv preprint arXiv:2308.04026, 2023.

[26] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv., 55(9), jan 2023.

[27] Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Plan, verify and switch: Integrated reasoning with diverse X-of-thoughts. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2807-2822, Singapore, December 2023. Association for Computational Linguistics.

[28] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought reasoning. In Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, and Adila Alfa Krisnadhi, editors, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 305-329, Nusa Dua, Bali, November 2023. Association for Computational Linguistics.

[29] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[30] Ning Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using LLMs to zero-shot check their own step-by-step reasoning. In The Twelfth International Conference on Learning Representations, 2024.

[31] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[32] OpenAI. Introducing chatgpt. 2023.

[33] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST '23, New York, NY, USA, 2023. Association for Computing Machinery.

[34] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687-5711, Singapore, December 2023. Association for Computational Linguistics.

[35] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.

[36] Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Lu Wang, Ruoxi Jia, and Ming Jin. Algorithm of thoughts: Enhancing exploration of ideas in large language models. arXiv preprint arXiv:2308.10379, 2023.

[37] Kashun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with chain-of-thought from labeled data. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12113-12139, Singapore, December 2023. Association for Computational Linguistics.

[38] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13003-13051, Toronto, Canada, July 2023. Association for Computational Linguistics.

[39] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.

[40] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and EePeng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2609-2634, Toronto, Canada, July 2023. Association for Computational Linguistics.

[41] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023.

[42] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. arXiv preprint arXiv:2307.05300, 2023.

[43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.

[44] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.

[45] Weijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jojic. Reprompting: Automated chain-ofthought prompt inference through gibbs sampling. arXiv preprint arXiv:2305.09993, 2023.

[46] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[47] Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. Cumulative reasoning with large language models. arXiv preprint arXiv:2308.04371, 2023.

[48] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, 2023.

[49] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023.

[50] Jianpeng Zhou, Wanjun Zhong, Yanlin Wang, and Jiahai Wang. Adaptive-solver framework for dynamic strategy selection in large language model reasoning. arXiv preprint arXiv:2310.01446, 2023 .
