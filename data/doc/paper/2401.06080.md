# Secrets of RLHF in Large Language Models Part II: Reward Modeling 

Binghai Wang*, Rui Zheng*â€ , Lu Chen*, Yan Liu*, Shihan Dou,<br>Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi,<br>Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao,<br>Xiao Wang, Tao Ji, Hang Yan, Lixing Shen ${ }^{\star}$, Zhan Chen ${ }^{\star}$,<br>Tao Gui ${ }^{\dagger}$, Qi Zhang ${ }^{\dagger}$, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang<br>Fudan NLP Lab \& Fudan Vision and Learning Lab<br>'Hikvision Inc


#### Abstract

Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.

In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental results confirm that data with varying preference strengths have different impacts on reward model performance. We introduce a series of novel methods to mitigate the influence of incorrect and ambiguous preferences in the dataset and fully leverage high-quality preference data. (2) From an algorithmic standpoint, we introduce contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses, thereby improving model generalization. Furthermore, we employ meta-learning to enable the reward model to maintain the ability to differentiate subtle differences in out-of-distribution samples, and this approach can be utilized for iterative RLHF optimization.

We have open-sourced the training code used in this report, the Anthropic's HHRLHF dataset with preference strength information, and additionally, the validation set cleaned by GPT-4, which is used in our analysis experiments. All of these resources can be found on our project website ${ }^{1}$.


[^0]
## 1 Introduction

In the field of artificial intelligence and language models, "alignment" is an important concept [1-3]. It refers to the process of ensuring that the behavior of AI systems aligns with the intentions of their designers and the expectations of users [4, 5]. Compared to supervised fine-tuning (SFT) in learning to generate, reinforcement learning from human feedback (RLHF) requires learning to discriminate first, which is simpler and more generalizable [6, 7]. RLHF involves two main steps: first, using preference data collected from a large number of crowdsource workers to train a reward model. Secondly, using reinforcement learning methods to optimize the language model to maximize the reward. The reward model plays a crucial role in the RLHF process, and our goal is to make it a reliable proxy for human preferences.

However, many researchers have pointed out the shortcomings of reward models and the difficulties in using them to accurately represent human preferences [8, 9]. At present, two pressing issues need to be addressed: (1) The presence of incorrect and ambiguous preferences in the dataset due to the low agreement among annotators during preference labeling (about 0.6 to 0.7) [4, 5]. Since we assume that human choice behavior is a noisy representation of underlying truths, detecting and mitigating noisy data is essential for aligning learned rewards with true human preferences. (2) The generalization ability of the reward model is poor. When a reward model is trained on data with a specific distribution, it may perform poorly when it encounters out-of-distribution (OOD) examples [10]. This limitation may not only lead to instability in the reinforcement learning process but also necessitate the annotation of new preference data for online iterative RLHF processes.

To address noise and ambiguity in preference data, we propose a preference strength measurement metric based on a multi-reward model voting approach. Using this proposed metric, we can distinguish between incorrect, ambiguous, and normal preferences within the original dataset. Then, we can correct the labels of wrong preferences and smooth the labels of ambiguous preferences to avoid the model's overfitting on these low-quality data [11]. In the loss function for preference modeling, we also introduce an adaptive margin based on the preference strength, making it easier to distinguish between similar responses. Our experimental results confirm that using reward models trained through the above heuristic methods can lead to a more stable reinforcement learning process and significantly improve the final alignment performance.

To enhance the generalization ability of the reward model, we explore contrastive learning and meta-learning. By introducing unsupervised contrastive loss during the reward modeling process, the reward model can better distinguish subtle preference differences among responses. To bridge the gap between the preference data distribution and the model output distribution, we employ meta-learning to ensure that the reward model not only performs well on the preference data but also can distinguish the differences in target domain outputs. In this way, we make the reward model trained only on specific distribution preference data that can be transferred to OOD data. In addition, we can use the proposed method to continuously train new reward models to adapt to the output distribution of the newly aligned model, achieving iterative RLHF. On Anthropic's HH-RLHF [5] and OpenAI's summarization [12] datasets, we can achieve consistent improvement of the language model in 3 to 4 rounds.

## 2 How Data Impacts the Modeling of Human Preference?

The reward model infers human values and intent from preference data, so preference data needs to accurately and comprehensively represent human intent. However, in practical applications, preference data has some drawbacks. Firstly, preference datasets contain incorrect and ambiguous preferences. For example, in the annotations of preference data, there is a poor average agreement (about $63 \%$ ) between Anthropic researchers and their crowd workers [5], and OpenAI found that the inter-annotator agreement rates among training labelers are at $72.6 \pm 1.5 \%$ [4]. Secondly, different data contain preferences of varying strength. The responses in preference data are sampled from the SFT model, and most of the data exhibit low preference strength. The main focus of this section is to deal with the impact of incorrect or ambiguous data and make full use of data with different preference strengths.

### 2.1 Preliminaries

We review the RLHF pipeline from [13], which has been applied to tasks like dialogue [14], instruction following [4], and summarization [12]. This pipeline typically includes three phases: supervised fine-tuning (SFT), preference sampling and reward model (RM) training, and RL fine-tuning using proximal policy optimization (PPO) [15]. The process usually starts with a generic pre-trained language model, which undergoes supervised learning on a high-quality dataset for specific downstream tasks, resulting in a model denoted as $\pi^{\mathrm{SFT}}$. In this study, we focus on improving the remaining two stages.

Reward modeling from human preference. In the second stage, the SFT model $\pi^{\mathrm{SFT}}$ is prompted with a user query denoted as $x$ to produce two distinct outputs $\left(y_{1}, y_{2}\right) \sim \pi^{\mathrm{SFT}}(y \mid x)$. Human labelers are instructed to choose their preferred output, resulting in $y_{\mathrm{c}} \succ y_{\mathrm{r}}$, where $y_{\mathrm{c}}$ and $y_{\mathrm{r}}$ represent the chosen and rejected outputs, respectively, from the pair $\left(y_{1}, y_{2}\right)$. By following the Bradley-Terry model [16], we formulate a preference distribution by employing the reward function $r_{\psi}(x, y)$ as outlined below:

$$
\begin{align*}
p_{\psi}\left(y_{\mathrm{c}} \succ y_{\mathrm{r}} \mid x\right) & =\frac{\exp \left(r_{\psi}\left(x, y_{\mathrm{c}}\right)\right)}{\exp \left(r_{\psi}\left(x, y_{\mathrm{r}}\right)\right)+\exp \left(r_{\psi}\left(x, y_{\mathrm{r}}\right)\right)}  \tag{1}\\
& =\sigma\left(r_{\psi}\left(x, y_{\mathrm{c}}\right)-r_{\psi}\left(x, y_{\mathrm{r}}\right)\right)
\end{align*}
$$

which $\sigma$ is the logistic function. Treating the problem as a binary classification task yields the negative log-likelihood loss function:

$$
\begin{equation*}
\mathcal{L}\left(r_{\psi}\right)=-\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}}\left[\log \sigma\left(r_{\psi}\left(x, y_{\mathrm{c}}\right)-r_{\psi}\left(x, y_{\mathrm{r}}\right)\right)\right] \tag{2}
\end{equation*}
$$

where dataset is composed of comparisons denoted as $\mathcal{D}_{\mathrm{rm}}=\left\{x^{(i)}, y_{\mathrm{c}}^{(i)}, y_{\mathrm{r}}^{(i)}\right\}_{i=1}^{N}$. In the realm of LMs, the network $r_{\psi}(x, y)$ is often initialized using the SFT model $\pi^{\mathrm{SFT}}(y \mid x)$. It then incorporates an additional linear layer on the final transformer layer to generate a singular scalar prediction representing the reward value.

RL fine-tuning. In the RL stage, we utilize the learned reward function to provide feedback to the language model. More precisely, we optimize the policy model $\pi^{\mathrm{RL}}$ to maximize the following reward objective:

$$
\begin{equation*}
r_{\text {total }}=r_{\psi}(x, y)-\eta \mathrm{KL}\left(\pi^{\mathrm{RL}}(y \mid x) \| \pi^{\mathrm{SFT}}(y \mid x)\right) \tag{3}
\end{equation*}
$$

where $\eta$ is a coefficient that governs the magnitude of the KL penalty. The KL divergence term serves two primary purposes in this context. First, it acts as an entropy bonus, preserving generation diversity and preventing mode-collapse into singular high-reward answers [17]. Second, it ensures that the RL policy's output does not deviate drastically from the distribution where the reward model is accurate [18].

### 2.2 Measuring the Strength of Preferences

The preference strength (difference) between chosen and rejected responses can be quantified using $d_{i, \psi}=r_{\psi}\left(x^{(i)}, y_{\mathrm{c}}^{(i)}\right)-r_{\psi}\left(x^{(i)}, y_{\mathrm{r}}^{(i)}\right)$. We train $N$ reward models using the same preference data, with the training order randomized. By utilizing the ensemble of reward scores from these $M$ reward models, we can calculate the mean and standard deviation (std) of preference strength for each comparison pair:

$$
\begin{equation*}
\hat{\mu}_{i}=\frac{1}{M} \sum_{m=1}^{M} d_{i, \psi_{m}}, \quad \hat{\sigma}_{i}=\sqrt{\frac{\sum_{m=1}^{M}\left(d_{i, \psi_{m}}-\hat{\mu}_{i}\right)^{2}}{M}} \tag{4}
\end{equation*}
$$

In the following experiment, $M$ is set to 10 . Figure 2 displays the distribution of mean and std for all pairwise responses calculated from the Anthropic's HH-RLHF training set using Equation 4. We observe that the mean of preference differences for approximately $25 \%$ of the data is less than 0 . Despite these data being involved in the training of reward models, the final votes from the 10 models indicate that the models still lack trust in this data, which may have incorrect preference labels. Additionally, the mean of preference differences for some data is slightly greater than 0 , indicating that the preference differences in these data are not pronounced. The long-tailed distribution of standard deviation indicates that the reward model may not be robust in evaluating some preferences. Table
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-04.jpg?height=512&width=1374&top_left_y=237&top_left_x=365)

Figure 1: Mean and standard deviation of preference differences derived from 10 reward models for all paired data. Left figure displays that a substantial number of preference difference means are near 0 , indicating that the preference strength is not strong, while means less than 0 suggest potential incorrect preferences. Right figure reveals that the distribution of standard deviations has a long-tail characteristic, indicating low consistency among different reward models in scoring this portion of the data.

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-04.jpg?height=480&width=683&top_left_y=1026&top_left_x=366)

Figure 2: Mean and standard deviation of preference differences for each data group. When we arrange the data in ascending order of mean preference difference, the standard deviation exhibits a U-shaped curve.

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-04.jpg?height=458&width=615&top_left_y=1037&top_left_x=1080)

Figure 3: Consistency between the original annotations and GPT-4's annotations for data groups with varying preference differences. The greater the preference strength, the higher the consistency.

1 presents some dialogue examples, and our method can distinguish data with varying preference intensities.

We sort the training data in ascending order based on their preference strength and divide the data into 20 groups. Subsequently, for each group, we calculate the mean of the means and the mean of the standard deviations of the preference differences derived from 10 different models, as illustrated in Figure 2. The standard deviation exhibits a U-shaped pattern, remaining stable in the middle and gradually increasing on both sides. Strong preferences and significant errors can make it difficult for different models to reach a consensus. This is because normal preference data constitutes the majority, and the language model exhibits a certain robustness towards incorrect data and does not overly trust them. In addition, preference data with significant differences may contain simple patterns, leading the model to learn simple and non-robust features.

To validate the preference strength generated by the ten reward models against the true data labels (which have noise in the original preference labels), we analyze the data in the validation set. After obtaining the preference strength for the validation set data, we sort the data in ascending order based on this value and divide them into groups of 500 data points each. We annotate the validation set data using GPT-4 and calculate the consistency between the original labels and the labels generated by GPT-4 for each group, as shown in Figure 3. There is a strong correlation between the preference strength and the consistency with GPT-4 annotations; the higher the preference strength, the higher the

\begin{aligned} \& \hline Human Prompt $\rightarrow$ How do you study for a test efficiently? (Incorrect Preference) <br>

\& \hline Chosen Response $\rightarrow$| That's a tough one. |
| :--- |
| Studying for a test can be very challenging, but there are several effective strategies |
| you can use to study efficiently. First, it's important to break down the material |
| that you need to learn into smaller parts and focus on learning each part thoroughly. |
| In addition, it's helpful to set aside plenty of time each day to study and review |
| the material regularly. Studying efficiently also anvolves avoiding distractions and |
| staying well-rested, as these can reduce your ability to learn effectively. Overall, |
| using these strategies can help you prepare for a test in an efficient and thorough |
| manner. | <br>

\& Mean:-5.86, Standard Deviation: 0.75 \end{aligned}

Table 1: Examples of human feedback data with different preference strengths. Using the proposed metric, we can categorize the data into incorrect, ambiguous, and strong preferences.

consistency. The 500 data with the highest preference strength have a consistency of 0.956 , while the 500 data with the lowest preference strength only have a consistency of 0.164 . Meanwhile, for data with preference strength near zero, the consistency is 0.544 , confirming that the preference signals in these data are not strong. Although using GPT-4 for annotation is not perfect, the strong correlation phenomenon mentioned above indicates that to some extent, the preference strength obtained by using multi-model voting can be used to evaluate the correctness of preference annotation.

### 2.3 Impacts of Different Data on RM Performance

As previously mentioned, we can use preference strength to partition the training data into different groups. We are curious about the contributions that different groups of training sets have made to modeling preferences. We train a reward model from scratch for each group, where each group's data size is $10 \%$ of the original training data size, and then evaluate its performance on the validation set. The results are depicted in Figure 4. For more experimental results regarding the performance of training models with different ratios of data, please refer to Figures 21 and 22.

According to the results, we can observe that: 1) For the top $20 \%$ of data with the lowest preference strength, they have a negative impact on the model's performance on the validation set. The preference strength for these data subsets is less than 0. 2) For data ranked between $20 \%$ and $40 \%$, after training, the model's prediction accuracy on the validation set is approximately 0.5 . The preference strength for this type of data is around 0.3 ) The remaining data significantly improves the model's performance. However, the top $10 \%$ of data with the highest preference strength does not achieve the best performance when trained alone. Based on the above results, we can roughly categorize preference data into three types: incorrect data, ambiguous data (almost no difference), and normal data (clear differences). These three types of preference data play different roles and make different contributions to preference modeling. It is necessary for us to conduct a more detailed analysis of them and then consider how to handle each type.
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-06.jpg?height=1036&width=1378&top_left_y=232&top_left_x=362)

Figure 4: We evenly divide the training set into 10 subsets based on preference strength and retrain the reward model on each subset. Incorrect preference data would result in the model's performance on the validation set being worse than random guessing, while reward models trained on ambiguous preference data would perform approximately as well as random guessing. Strong preference data, on the other hand, would teach the model to achieve good performance.

### 2.4 Analyze and Leverage Diverse Data to its Fullest Potential

### 2.4.1 Mitigate the Impact of Incorrect Data

According to our findings, the bottom $20 \%$ of data with the lowest preference strength significantly hinders the performance of the reward model on the test set. By flipping the labels of these preference pairs, the model could more effectively learn preference information for modeling, as demonstrated in Figure 5. This result once again confirms the presence of noise in the preference dataset, which is primarily due to inconsistent annotations. We tried traditional noise learning methods; however, these methods are typically instance-independent and therefore not well-suited for preference modeling [19]. The label flipping and label smoothing used in this report can effectively alleviate preference noise.

Label smoothing is another widely known technique to mitigate the overfitting problem by penalizing overconfident model outputs [20]. For a reward model trained with hard labels, we minimize the expected value of the cross-entropy between the true preference label and the model's output $p_{\psi}\left(y_{\mathrm{c}} \succ y_{\mathrm{r}} \mid x\right)$, where label " 1 " is assigned to the preference $y_{\mathrm{c}} \succ y_{\mathrm{r}}$ and ' 0 " is used for $y_{\mathrm{r}} \succ y_{\mathrm{c}}$. For a reward model trained with label smoothing, we minimize the cross-entropy between the modified label and the model's output:

$$
\begin{equation*}
\mathcal{L}_{\mathrm{LS}}\left(r_{\psi}\right)=-\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}}\left[(1-\alpha) \log \left(p_{\psi}\left(y_{\mathrm{c}} \succ y_{\mathrm{r}} \mid x\right)\right)+\alpha \log \left(1-p_{\psi}\left(y_{\mathrm{c}} \succ y_{\mathrm{r}} \mid x\right)\right)\right] \tag{5}
\end{equation*}
$$

where $p_{\psi}\left(y_{\mathrm{c}} \succ y_{\mathrm{r}} \mid x\right)=\sigma\left(r_{\psi}\left(x, y_{\mathrm{c}}\right)-r_{\psi}\left(x, y_{\mathrm{r}}\right)\right.$ and $\alpha$ is the smoothing parameter. In Figure 25 , we demonstrate how label smoothing can be used to avoid the impact of noisy data.
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-07.jpg?height=914&width=1286&top_left_y=226&top_left_x=408)

Figure 5: For the two subsets with incorrect preferences, we flip the labels of these data and retrain the reward model. Label flipping for these data effectively improves the model's performance on the validation set, indicating that our proposed metrics can efficiently identify incorrect preferences and that even incorrect preferences contain useful preference information.
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-07.jpg?height=954&width=1268&top_left_y=1315&top_left_x=426)

Figure 6: When training the reward model on data with the strongest preferences, the training loss rapidly converges to 0 , and the model learns surface patterns in the data. When using soft labels, the model's loss cannot approach 0 , and the model learns robust features in the data, leading to a significant improvement in performance.
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-08.jpg?height=1042&width=1394&top_left_y=232&top_left_x=362)

Figure 7: Adding an adaptive margin component to the reward modeling loss will significantly enhance model performance.

### 2.4.2 Adaptive Margin

As mentioned in section 2.2, we can calculate the preference strength of the data. Using preference strength information, we can guide the reward model to assign more discrepant scores to responses with higher preference strength, which has been shown to be beneficial for preference modeling [21]. Therefore, we add an adaptive margin component to the loss of the reward model:

$$
\begin{equation*}
\mathcal{L}\left(r_{\psi}\right)=-\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}}\left[\log \sigma\left(r_{\psi}\left(x, y_{\mathrm{c}}\right)-r_{\psi}\left(x, y_{\mathrm{r}}\right)\right)-\hat{\mu}(x, y)\right] \tag{6}
\end{equation*}
$$

where the marginal function $\hat{\mu}(x, y)$ serves as a continuous measure of preference strength. Adaptively, we use larger margins for pairs with distinct responses, and smaller margins for pairs with similar responses. This margin component improves the accuracy of the reward model, especially for samples where the two responses are more easily distinguishable [21].

In this part, we focus on the top $10 \%$ of the dataset, characterized by the highest preference strength. Our findings, as illustrated in Figure 4, reveal that the training loss for our reward model decreases more rapidly for this subset compared to the rest, while the validation set loss shows an increase. We examine the effects of implementing soft labels and adaptive margins in the training process, and the results are shown in Figure 6. The key conclusions are as follows: 1) The use of only adaptive margin brings minor performance improvements because the preference differences of these data are already large. 2) The use of soft labels seems to benefit the learning of strong preference data. It can prevent the training loss from decreasing too quickly, ensuring that more general features are learned from these data. 3) The combination of soft labels and adaptive margin is particularly effective for learning strong preference data.

As shown in Figure 7, adding a margin to all the data effectively enhances the performance of preference modeling.

### 2.4.3 Takeaways

- Label Flipping and Label Smoothing can effectively avoid the impact of noisy preferences and improve performance, provided that you can accurately identify noisy preference data.
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-09.jpg?height=488&width=360&top_left_y=237&top_left_x=364)
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-09.jpg?height=490&width=1050&top_left_y=238&top_left_x=709)

Figure 8: We demonstrate the performance of our proposed reward modeling approaches compared to the baseline method on three different validation sets. When combined with the suppression and correction of incorrect and ambiguous preferences, along with the adaptive margin method, our proposed approach not only exhibits better performance but also effectively mitigates overfitting.
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-09.jpg?height=1046&width=1396&top_left_y=943&top_left_x=364)

Figure 9: Fine-tuning the SFT model using PPO, guided by the reward models mentioned earlier, without employing the KL penalty in all experiments. When the reward models suppress incorrect preferences and ambiguous preferences, the PPO process becomes more stable, with KL divergence steadily increasing with training steps and PPL experiencing no drastic fluctuations.

- When learning data with strong preference strength, the reward model may be prone to overfitting, which can be mitigated by using Label Smoothing.
- Adaptive margin almost always benefits all preference data and can be widely applied to reward modeling.

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-10.jpg?height=404&width=664&top_left_y=264&top_left_x=384)

(a) Ours vs. baseline on harmless evaluation

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-10.jpg?height=401&width=664&top_left_y=743&top_left_x=384)

(c) Ours vs. baseline on helpful evaluation

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-10.jpg?height=404&width=666&top_left_y=264&top_left_x=1071)

(b) Ours vs. SFT on harmless evaluation

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-10.jpg?height=406&width=671&top_left_y=743&top_left_x=1071)

(d) Ours vs. SFT on helpful evaluation

Figure 10: Evaluation results, as judged by GPT-4, show the harmlessness and helpfulness of models trained by our methods when compared to the baseline and SFT model.

### 2.5 How to Better Model Human Preference?

Three validation sets. There are inevitably some noisy data in the original validation set. Considering that the reward modeling process may overfit the noise data in the dataset, we additionally supplement the validation set labeled by GPT-4 for evaluation. In the complete training process, we comprehensively evaluate the performance of the model on the following three validation sets: (1) The original validation set, (2) GPT-4 labeled dataset, and (3) The subset of data with consistent labels between the original and GPT-4 labeling.

Methods. In this report, we mainly consider four methods to improve reward modeling. In our practical experiments, these methods show improvements over the original reward modeling method:

- Flip: Flip the noise data labels in the preference data.
- Margin: Add an adaptive margin to the loss function for all preference pairs.
- Flip + Margin: Flip the noise data labels in the preference data and add an adaptive margin to the loss function for all preference pairs.
- Soft Label + Margin: Apply label smoothing to data with the preference strength less than 0 and add an adaptive margin to the loss function for all preference pairs.

The performance of the aforementioned methods as well as the baseline method on three distinct test sets and the training set is illustrated in Figure 8. The performance of the baseline and the margin on the original test set keeps improving, reaching its peak around 4500 steps, and then declining. Although they exhibit superior performance on the original validation set compared to other methods, they are overfitting to the noise. Further analysis experiments can be found in Appendix C. Both the baseline and the margin have significant performance fluctuations on the other two validation sets. The denoising methods demonstrate stable performance across all three validation sets, delivering better overall performance.

RL Fine-tuning. In our previous report [22], we emphasized the importance of the KL penalty for stabilizing the PPO process. In this report, we will demonstrate that even when the KL penalty

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-11.jpg?height=521&width=1141&top_left_y=249&top_left_x=489)

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-11.jpg?height=447&width=567&top_left_y=259&top_left_x=497)

(a) Baseline

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-11.jpg?height=447&width=564&top_left_y=264&top_left_x=1060)

(b) Reward modeling with SimCSE

Figure 11: Feature distribution obtained through t-SNE reveals a significant overlap in the features of chosen and rejected responses in the baseline model. However, when SimCSE is introduced into the reward modeling, this overlap between chosen and rejected responses decreases.

is removed, the PPO process can still remain highly stable, consistent with the observations in Anthropic's work [5]. Please refer to Appendix B for experimental details.

In Figure 18, we present the PPO training curves for various methods. We remove the KL penalty to closely examine the impact of different reward models on the training process. We first focus on the KL divergence between the policy model's output and the reference model's output. It can be observed that in the later stages of training, both the baseline and margin methods exhibit a rapid increase in KL divergence, accompanied by significant fluctuations. On the other hand, the three denoising reward models result in a linear increase in KL divergence, ensuring the stability of the training process. When we examine the perplexity of the model outputs, it can be seen that reward models with noise introduce perplexity fluctuations during the later stages of training, while other models remain relatively stable. Since different reward models have different score ranges, directly comparing absolute score values is not meaningful. The goal of PPO is to maximize the improvement in the model's reward score on the validation set.

Finally, we utilize GPT-4-turbo as an evaluator to assess the quality of different outputs, comparing them in terms of their helpfulness and harmlessness. The prompts used for testing the model's harmlessness were drawn from Anthropic's red teaming dataset, specifically selecting the aggressive prompt. For assessing helpfulness, we employ our reserved HH-RLHF test dataset, randomly selecting 100 prompts. The GPT-4 evaluation prompts used are detailed in Appendix B.4. When comparing the responses of our four proposed methods and traditional RM against harmful prompts, our four methods demonstrate a significant improvement. This improvement may be attributed to the potential impact of noisy data in the preference data related to harmful prompts, making denoising particularly effective. However, the improvement is less pronounced when responding to helpful prompts. There might be conflicts in the model's learning between harmless and helpful intentions. Recent research has been focused on better integrating various human intentions, and this aspect will be a subject of our future investigations.

## 3 Preference Generalization and Iterated RLHF

In this section, we will attempt to improve the generalization of the reward model using contrastive learning and meta-learning.

### 3.1 Contrastive Learning for Reward Modeling

In reward modeling, a significant challenge is that models often exhibit a high degree of feature similarity between "chosen" and "rejected" responses, as shown in Figure 11, indicating that the model fails to capture subtle differences and distinctions between responses. Lack of discriminative ability may lead to poor performance, as the model may struggle to learn which behaviors or outcomes are preferable or not. In contrast, contrastive learning has some inherent advantages: 1) Effective
feature extraction: contrastive learning trains the model by comparing similar and dissimilar samples, which helps the model to efficiently learn the unique features within the data. 2) Strong generalization capabilities: by learning to distinguish between different samples, models trained with contrastive learning typically exhibit better generalization capabilities, enabling them to handle new, unseen data more effectively.

### 3.1.1 Choice of Positive and Negative Samples

In the context of RLHF, the integration of contrastive learning for preference modeling requires careful consideration of the choice of contrastive samples. There are two approaches to choosing these examples: 1) Preference Pairs: Performing contrastive learning with representations of response pairs from preference data, that is $\mathbf{H}=\left\{f\left(x^{(i)}, y_{\mathrm{c}}^{(i)}\right), f\left(x^{(i)}, y_{\mathrm{r}}^{(i)}\right)\right\}_{i=1}^{N}$. 2) Preference Difference: From Equation 2, it can be seen that the loss function of the reward model depends on the learned preference differences. Therefore, we attempt to have contrastive learning directly capture preference differences, formally, $\mathbf{H}=\left\{f\left(x^{(i)}, y_{\mathrm{c}}^{(i)}\right)-f\left(x^{(i)}, y_{\mathrm{r}}^{(i)}\right), f\left(x^{(i)}, y_{\mathrm{r}}^{(i)}\right)-f\left(x^{(i)}, y_{\mathrm{c}}^{(i)}\right)\right\}_{i=1}^{N}$.

### 3.1.2 Methods

SwAV (Swapping Assignments between Views) [23] is an approach for unsupervised learning of features that differs from traditional contrastive learning methods. SwAV simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or 'views') of the same instance. This method involves creating multiple views of an instance, predicting the cluster assignment for each view, and then using a swapping mechanism where the goal is to match the cluster assignment of one view with the predictions of another view. This approach allows for more efficient learning and avoids the necessity of comparing every possible pair of images, which can be computationally expensive.

For two distinct augmentations of the same instance, we derive their respective features, $\mathbf{h}_{t}$ and $\mathbf{h}_{s}$. These features are then aligned with their cluster assignments, $\mathbf{q}_{t}$ and $\mathbf{q}_{s}$, by correlating them with a set of $K$ prototypes, denoted as $\left\{\mathbf{c}_{1}, \ldots, \mathbf{c}_{K}\right\}$. Subsequently, we establish a "swapped" prediction task, employing the following loss function:

$$
\begin{equation*}
\ell\left(\mathbf{h}_{t}^{(i)}, \mathbf{h}_{s}^{(i)}\right)=\ell\left(\mathbf{h}_{t}^{(i)}, \mathbf{q}_{s}^{(i)}\right)+\ell\left(\mathbf{h}_{s}^{(i)}, \mathbf{q}_{t}^{(i)}\right) \tag{7}
\end{equation*}
$$

where the function $\ell\left(\mathbf{h}_{t}, \mathbf{q}_{s}\right)$ measures the fit between features $\mathbf{h}_{t}$ and a cluster assignment $\mathbf{q}_{s}$. Formally,

$$
\begin{equation*}
\ell\left(\mathbf{h}_{t}, \mathbf{q}_{s}\right)=-\sum_{k} \mathbf{q}_{s}^{(k)} \log \mathbf{p}_{t}^{(k)}, \quad \text { where } \quad \mathbf{p}_{t}^{(k)}=\frac{\exp \left(\frac{1}{\tau} \mathbf{h}_{t}^{T} \mathbf{c}_{k}\right)}{\sum_{k^{\prime}} \exp \left(\frac{1}{\tau} \mathbf{h}_{t}^{T} \mathbf{c}_{k^{\prime}}\right)} \tag{8}
\end{equation*}
$$

where $\tau$ represents a temperature parameter, and the details about $\mathbf{q}_{s}$ and $\mathbf{c}_{k}$ can be found in [23]. In simple terms, this method utilizes the intermediate cluster assignments $\mathbf{q}_{t}$ and $\mathbf{q}_{s}$ to compare the features $\mathbf{h}_{t}$ and $\mathbf{h}_{s}$. If these two features capture the same information, it should be possible to predict the cluster assignment from one feature to the other.

SimCSE SimCSE (Simple Contrastive Learning of Sentence Embeddings) [24] is a method for learning sentence embeddings using contrastive learning but with a simpler approach compared to previous methods. It involves using identical sentences as positive pairs, which are fed into a Transformer-based model to obtain embeddings. The key aspect is that these identical sentences are passed through the model under different dropout masks, creating variations in their embeddings. Negative pairs are formed from different sentences. This approach allows for efficient and effective learning of sentence representations without the need for complex data augmentation or externally labeled data.

In the SimCSE framework, the objective is to enhance the similarity of sentence embeddings corresponding to the same sentence while reducing the similarity among embeddings of different sentences. We simply input the same input twice into the encoder, obtaining two embeddings with different dropout masks. The training objective for SimCSE is:

$$
\begin{equation*}
\ell_{i}=-\log \left(\frac{e^{\operatorname{sim}\left(\mathbf{h}_{s}^{(i)}, \mathbf{h}_{t}^{(i)}\right) / \tau}}{\sum_{j=1}^{N^{\prime}} e^{\operatorname{sim}\left(\mathbf{h}_{s}^{(i)}, \mathbf{h}_{t}^{(j)}\right) / \tau}}\right) \tag{9}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-13.jpg?height=1108&width=1404&top_left_y=232&top_left_x=358)

Figure 12: Using a reward model trained through contrastive learning to optimize the language model, no KL penalty is applied in any of the experiments. The reward model obtained through contrastive learning leads to more stable returns and rewards during the PPO training process.

Here, $\ell_{i}$ denotes the loss of sample $\left(x_{i}, y_{i}\right)$ in a batch of $N^{\prime}$ samples. For each sentence $i$ in the batch, $\mathbf{h}_{s}^{(i)}$ and $\mathbf{h}_{t}^{(i)}$ represent the embeddings obtained from two different dropout masks. The function $\operatorname{sim}(\cdot, \cdot)$ computes the cosine similarity between the two embeddings. The loss for each sentence is the negative log probability of the true pair $\left(\mathbf{h}_{s}^{(i)}, \mathbf{h}_{t}^{(i)}\right)$ being more similar than any other pair $\left(\mathbf{h}_{s}^{(i)}, \mathbf{h}_{t}^{(j)}\right)$, where $j$ ranges over all sentences in the batch, including the true pair itself. The temperature parameter $\tau$ controls the sharpness of the distribution over similarities. This contrastive objective effectively encourages the model to pull together the embeddings of the same sentence (positive pairs) and push apart the embeddings of different sentences (negative pairs), thereby learning robust sentence representations.

Optimization Objective. The total reward model loss is a combination of the original RM loss and the contrastive learning loss, i.e., $\mathcal{L}_{\text {total }}=\mathcal{L}_{\mathrm{rm}}+\beta \mathcal{L}_{\mathrm{cl}}$. In this setup, $\mathcal{L}_{\mathrm{rm}}$ denotes the $\mathrm{RM}$ loss, which is computed using all original samples and their augmentations. The $\mathcal{L}_{\mathrm{cl}}$ represents the loss of the contrastive learning component, utilizing methods such as SwAV or SimCSE to enhance the model's ability to recognize subtle variations and similarities in the data. The hyperparameter $\beta$ is introduced to adjust the impact of the contrastive learning loss on the overall reward model loss, ensuring a suitable influence on the model's optimization.

Figure 12 illustrates the training curves for the reward model trained using contrastive learning and the baseline in PPO training. The methods based on contrastive learning are more stable in terms of training set reward and returns, ensuring a consistently stable RL process. In Figure 13, we compare the our RLHF models with the baseline and SFT in terms of harmless and helpful evaluation. It can be observed that the language model trained with the reward model based on contrastive learning performs slightly better, with the best overall performance achieved by directly incorporating SimCSE into the reward modeling phase.

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-14.jpg?height=450&width=680&top_left_y=263&top_left_x=365)

(a) Ours vs. baseline on harmless evaluation

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-14.jpg?height=458&width=675&top_left_y=785&top_left_x=378)

(c) Ours vs. baseline on helpful evaluation

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-14.jpg?height=452&width=681&top_left_y=262&top_left_x=1058)

(b) Ours vs. SFT on harmless evaluation

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-14.jpg?height=461&width=683&top_left_y=791&top_left_x=1057)

(d) Ours vs. SFT on helpful evaluation

Figure 13: Evaluation results of the contrastive learning-based method proposed in comparison to the baseline and SFT model indicate that, overall, SimCSE with direct comparison outperforms all other methods.

### 3.2 MetaRM: Aligning with Shifted Distributions via Meta Learning

Our goal is that when the distribution of the policy model shifts with the PPO training, the reward model should still maintain the discrimination of responses sampled from the new distribution. In this section, we introduce MetaRM, a method that aligns the original preference pairs with the shifted distribution through meta-learning. The key idea of MetaRM is: the training phase of the RM should minimize the loss on the original preference pairs while maximizing the differentiation between responses sampled from the shifted policy distribution.

The original reward model is trained using a dataset of comparisons between two model responses generated by the same prompt [25]. Formally, for a given prompt $x$ inputted to the SFT model $\pi^{\mathrm{SFT}}(y \mid x)$, the two responses generated by $\pi^{\mathrm{SFT}}$ are denoted as $y_{1}$ and $y_{2}$. The labeler provides a preference for these two responses $y_{1}$ and $y_{2}$, denoted $y_{c} \succ y_{r}$, where $y_{c}$ is the response more consistent with prompt $x$. Let the training dataset of the $\mathrm{RM}$ is $\mathcal{D}=\left\{\left(x^{i}, y_{c}^{i}, y_{r}^{i}\right), 1 \leq i \leq N\right\}$ and $N$ is the number of preference pairs. The loss function of the vanilla reward model can be simplified as follows:

$$
\begin{equation*}
\mathcal{L}_{\theta}=-E_{\left(x, y_{c}, y_{r}\right) \sim \mathcal{D}}\left[\log \sigma\left(r_{\theta}\left(x, y_{c}\right)-r_{\theta}\left(x, y_{r}\right)\right)\right] \tag{10}
\end{equation*}
$$

where $r_{\theta}$ denotes the reward model which is often initialized from the SFT model $\pi^{\mathrm{SFT}}$ and $\theta$ is the parameters of the reward model $r_{\theta}$.

When putting reinforcement learning in the realm of large language models, the environment distribution and the output distribution of the policy model $\pi^{\mathrm{RL}}(y \mid x)$ are identical. It means that the distribution of the environment shifts as $\pi^{\mathrm{RL}}(y \mid x)$ is optimized. We find that the RM does not significantly distinguish between responses sampled from the same prompt in the shifted environment. To measure the degree of difference in the responses' scores, we define the difference loss function $\mathcal{J}_{\theta}$ of the reward model $r_{\theta}$. Formally, let $s=\left\{s_{i}, 1 \leq i \leq k\right\}$ be the sequence of responses generated multiple times by the policy model $\pi^{\mathrm{RL}}(y \mid x)$ under the same prompt $x$, where $k$ denotes the number

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-15.jpg?height=715&width=1395&top_left_y=236&top_left_x=365)

Figure 14: Pipeline of our method MetaRM. MetaRM consists of four simple steps: 1. Compute the difference loss on responses sampled from the shifted distribution. 2. Calculate the gradient of the loss wrt. the RM parameters $\theta_{t}$ and adjust the parameters according to the ascent direction. 3. Compute the vanilla loss on the original preference pairs using the updated parameters $\theta_{t}^{\prime}$. 4 . Calculate the gradient of the vanilla loss wrt. $\theta_{t}^{\prime}$ and optimize the original parameters $\theta$ following the descent direction.

of responses. The difference function $\mathcal{J}_{\theta}$ can be written as follows:

$$
\begin{equation*}
\mathcal{J}_{\theta}=\frac{2}{k^{2}} \sum_{i=1}^{k} \sum_{j=i+1}^{k} \sigma\left(\left|r_{\theta}\left(x, s_{i}\right)-r_{\theta}\left(x, s_{j}\right)\right|\right) \tag{11}
\end{equation*}
$$

which represents the degree of difference in the scores given by the reward model $r_{\theta}$ for responses $s$. When there is a shift in distribution, $\mathcal{J}_{\theta}$ tends to have a lower value. In contrast, a reward model that aligns with the shifted distribution exhibits a higher loss value, reflecting its enhanced ability to clearly distinguish between responses.

To recover the ability of the reward model to distinguish responses sampled from a shifted distribution, we introduce meta-learning to iteratively train the RM to align with the new environment.

Specifically, we maximize the difference loss function $\mathcal{J}_{\theta}$ in a meta-process and perform the meta update prior to the vanilla gradient update of the reward model. Let $\mathcal{S}=\left\{\left(x^{i}, s^{i}\right), 1 \leq i \leq M\right\}$ denotes the meta dataset sampled from a shifted distribution. The meta-process can be represented as a meta gradient ascent of the difference loss function $\mathcal{J}_{\theta}$ on a mini-batch $X_{s}$ of the meta dataset $\mathcal{S}$. At step $t$ of the training phase, the parameters of the $\mathrm{RM} r_{\theta}$ are adjusted according to the ascent direction:

$$
\begin{equation*}
\theta_{t}^{\prime}=\theta_{t}+\eta \frac{\partial \mathcal{J}_{\theta}\left(X_{s}\right)}{\partial \theta} \tag{12}
\end{equation*}
$$

In turn, we compute the gradient of the vanilla loss function $\mathcal{L}_{\theta^{\prime}}$ wrt. the parameters $\theta^{\prime}$ of the $\mathrm{RM}$ on a mini-batch $X_{t}=\left\{\left(x^{i}, y_{c}^{i}, y_{r}^{i}\right), 1 \leq i \leq n\right\}$ of the original preference pairs dataset $\mathcal{D}$, which can be represented as follows:

$$
\begin{equation*}
\nabla \theta=\frac{\partial \mathcal{L}_{\theta^{\prime}}\left(X_{t}\right)}{\partial \theta^{\prime}} \tag{13}
\end{equation*}
$$

Note that the MetaRM-optimization using the gradient $\nabla \theta$ is performed over the RM parameters $\theta$, whereas the objective $\mathcal{L}_{\theta}$ is computed using the updated RM parameters $\theta^{\prime}$. In effect, MetaRM aims to make the reward model learn more about the original preference pairs that provide more differentiation between responses sampled from the shifted distribution. Formally, the MetaRM-optimization is performed via gradient descent and the RM parameters $\theta$ are optimized as follows:

$$
\begin{equation*}
\theta_{t+1}=\theta_{t}-\alpha \nabla \theta \tag{14}
\end{equation*}
$$

To clearly show the aim of MetaRM, we derive the gradient $\nabla \theta$ (i.e., Equation 13) for optimizing the reward model $r_{\theta}$ :

$$
\begin{align*}
\nabla \theta & =\frac{\partial \mathcal{L}_{\theta^{\prime}}\left(X_{t}\right)}{\partial \theta^{\prime}} \\
& =\frac{\partial \mathcal{L}_{\theta^{\prime}}\left(X_{t}\right)}{\partial \theta}\left(\frac{\partial \theta^{\prime}}{\partial \theta}\right)^{-1} \\
& =\frac{\partial \mathcal{L}_{\theta^{\prime}}\left(X_{t}\right)}{\partial \theta}\left(1+\eta \frac{\partial^{2} \mathcal{J}_{\theta}\left(X_{s}\right)}{\partial \theta^{2}}\right)^{-1} \tag{15}
\end{align*}
$$

where $\left(1+\eta \frac{\partial^{2} \mathcal{J}_{\mathcal{O}}\left(X_{s}\right)}{\partial \theta^{2}}\right)^{-1}$ is deterministic for $X_{t}$ when the meta-dataset $\mathcal{S}$ is sampled, so it can be considered as a constant. We then apply Taylor expansion to $\mathcal{L}_{\theta^{\prime}}\left(X_{t}\right)$ about point $\theta$, which can be written as follows:

$$
\begin{align*}
\mathcal{L}_{\theta^{\prime}}\left(X_{t}\right) & =\mathcal{L}_{\theta}\left(X_{t}\right)+\frac{\partial \mathcal{L}_{\theta}\left(X_{t}\right)}{\partial \theta}\left(\theta^{\prime}-\theta\right)+o\left(\theta^{\prime}-\theta\right)^{2} \\
& =\mathcal{L}_{\theta}\left(X_{t}\right)+\eta \frac{\partial \mathcal{L}_{\theta}\left(X_{t}\right)}{\partial \theta} \frac{\partial \mathcal{J}_{\theta}\left(X_{s}\right)}{\partial \theta}+o\left(\theta^{\prime}-\theta\right)^{2} \\
& =\mathcal{L}_{\theta}\left(X_{t}\right)+\eta \sum_{i=1}^{n} \frac{\partial \mathcal{L}_{\theta}\left(x_{i}\right)}{\partial \theta} \frac{\partial \mathcal{J}_{\theta}\left(X_{s}\right)}{\partial \theta}+o\left(\theta^{\prime}-\theta\right)^{2} \tag{16}
\end{align*}
$$

where $o$ is infinitesimals that can be ignored.

Substituting Equation 16 into Equation 13, we obtain the gradient $\nabla \theta$ :

$$
\begin{equation*}
\nabla \theta \propto \frac{\partial}{\partial \theta}\left[\mathcal{L}_{\theta}\left(X_{t}\right)+\sum_{i=1}^{n} \frac{\partial \mathcal{L}_{\theta}\left(x_{i}\right)}{\partial \theta} \frac{\partial \mathcal{J}_{\theta}\left(X_{s}\right)}{\partial \theta}\right] \tag{17}
\end{equation*}
$$

Equation 17 suggests that MetaRM-optimization essentially adds a sum of dot products to the vanilla loss function. The dot product computes the similarity between the gradient directions of the meta loss $\mathcal{J}_{\theta}$ wrt. $\theta$ and the vanilla loss wrt. $\theta$. Specifically, when the direction of minimizing the vanilla loss on the preference pairs $X_{t}$ and maximizing the difference between the scores of the responses $X_{s}$ are similar, the dot product of both is greater. In such instances, the gradient $\nabla \theta$ in the MetaRM-optimization is larger, and the reward model $r_{\theta}$ can learn more about these preference pairs. Conversely, if the gradients are in different directions, these preference pairs may not be more helpful in aligning with the shifted distribution, so it is necessary to reduce the degree of optimization. The full algorithm is detailed in Algorithm 1.

```
Algorithm 1 MetaRM: Training the reward model by aligning the preference pairs with the
shifted distribution through meta-learning
Require: $\theta, \mathcal{D}, \mathcal{S}, n, m$
Require: $\eta, \alpha$
    : for $t=0, \cdots, T-1$ do
        Sample a mini-batch $X_{t}=\left\{\left(x^{i}, y_{w}^{i}, y_{l}^{i}\right), 1 \leq i \leq n\right\}$ of size $n$ from the preference pairs
        dataset $\mathcal{D}$
        Sample a mini-batch $X_{s}=\left\{\left(x^{i}, s^{i}\right), 1 \leq i \leq m\right\}$ of size $m$ from the meta dataset $\mathcal{S}$
        Compute the difference loss $\mathcal{J}_{\theta}\left(X_{s}\right)$ with the parameters $\theta_{t}$ on $X_{s}$
        (Meta-process) Compute adapted parameters $\theta_{t}^{\prime}$ with gradient ascent: $\theta_{t}^{\prime} \leftarrow \theta_{t}+$
        $\eta \nabla_{\theta} \mathcal{J}_{\theta}\left(X_{s}\right)$
        Compute the vanilla loss $\mathcal{L}_{\theta^{\prime}}\left(X_{t}\right)$ with the parameters $\theta_{t}^{\prime}$ on $X_{t}$
        (MetaRM-optimization) Update the parameters $\theta_{t}$ with gradient descent: $\theta_{t+1} \leftarrow$
        $\theta_{t}-\alpha \nabla_{\theta^{\prime}} \mathcal{L}_{\theta^{\prime}}\left(X_{t}\right)$
    end for
```


### 3.2.1 Experiments

In-distribution Task Evaluation. As shown in Table 2, we present the win, tie, and lose ratios when comparing the responses of our method to those of the SFT model. Because we cycled through several rounds of PPO training based on MetaRM, the round number refers to the responses generated by the

| Dataset | Opponent vs SFT | GPT-4 |  |  | Human |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\operatorname{Win} \uparrow$ | Tie | Lose $\downarrow$ | $\overline{\operatorname{Win} \uparrow}$ | Tie | Lose $\downarrow$ |
| Anthropic-Harmless | Round 1 | 44 | 44 | 12 | 48 | 32 | 20 |
|  | Round 2 | 65 | 31 | 4 | 63 | 28 | 9 |
|  | Round 3 | 69 | 28 | 3 | 72 | 22 | 6 |
|  | Round 4 | 64 | 31 | 5 | 68 | 27 | 5 |
| Anthropic-Helpful | Round 1 | 39 | 52 | 9 | 44 | 39 | 17 |
|  | Round 2 | 62 | 33 | 5 | 65 | 27 | 8 |
|  | Round 3 | 73 | 23 | 4 | 69 | 29 | 2 |
|  | Round 4 | 67 | 27 | 6 | 65 | 23 | 12 |
| Summary | Round 1 | 51 | 11 | 38 | 54 | 16 | 30 |
|  | Round 2 | 55 | 15 | 30 | 57 | 12 | 31 |
|  | Round 3 | 67 | 14 | 19 | 63 | 15 | 22 |
|  | Round 4 | 78 | 5 | 17 | 77 | 7 | 16 |
|  | Round 5 | 72 | 8 | 20 | 69 | 12 | 19 |

Table 2: Main results on the comparison of win, tie, and lose ratios of our method in the different rounds against the SFT model under both GPT-4 and human evaluations. The results demonstrate the superior and stable performance of our method and also highlight the consistency between human and GPT-4 evaluations.

| Dataset | Opponent | GPT-4 |  |  | Human |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Win $\uparrow$ | Tie | Lose $\downarrow$ | Win $\uparrow$ | Tie | Lose $\downarrow$ |
| Anthropic-Harmless | SFT | 69 | 28 | 3 | 72 | 22 | 6 |
|  | Vanilla PPO | 54 | 31 | 15 | 58 | 24 | 18 |
| Anthropic-Helpful | SFT | 73 | 23 | 4 | 69 | 29 | 2 |
|  | Vanilla PPO | 65 | 30 | 5 | 67 | 28 | 5 |
| Summary | SFT | 78 | 5 | 17 | 77 | 7 | 16 |
|  | Vanilla PPO | 62 | 7 | 31 | 54 | 19 | 27 |

Table 3: Results on comparison of the upper performance of our method against that of the SFT model and vanilla PPO model under both GPT-4 and human evaluations. For all datasets, MetaRM used the best round (i.e., the selected rounds are three, three and four for the Anthropic-Harmless dataset, the Anthropic-Helpful dataset, and the Summary dataset, respectively) to compare with other methods.

model for the corresponding round. Besides, to more comprehensively demonstrate the superiority of our approach, we also show the upper performance of our method during our loop process (i.e., for Generation Dialogue and Summarization tasks, the round number is 3 and 4 respectively) against other baselines including vanilla PPO in Table 3. We provide evaluation results on both GPT-4 and human assessments. From the results of the two tables, we can observe that: (1) Each round markedly outperforms the SFT model, and in the first few rounds, with the increase in rounds, the improvement becomes more significant. (2) In the fourth round of the dialogue generation task and the fifth round of the Summarization task, there is a decline in the win rate, indicating that there is an upper limit to the effectiveness of our method, which varies depending on the task. (3) Our method outperforms all other baselines significantly. (4) Human assessments align closely with the evaluations conducted using GPT-4. Therefore in subsequent experimental analyses, our primary reliance is placed upon the assessments from GPT-4.

Out-of-distribution Task Evaluation. As shown in Figure 15, our approach continues to outperform baselines even in OOD scenarios. This indicates that our method can be used to achieve alignment in a new domain without the need for cost-intensive preference labeling of a set of queries, significantly reducing the training costs for RM training. Also, we observe that when compared to the indistribution evaluation results in Table 15, our approach shows a slight decline in win rate. This may

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-18.jpg?height=602&width=1049&top_left_y=249&top_left_x=535)

Figure 15: Experimental results on out-of-distribution data. (Top) The win, tie, and lose ratios when comparing our method against the SFT model, (Bottom) that against the vanilla PPO model. The results on OOD data further substantiated the effectiveness of our approach.

be attributed to that tasks on OOD involve query distribution shift, in comparison to in-distribution context.

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-18.jpg?height=659&width=870&top_left_y=1123&top_left_x=625)

Figure 16: Reward score difference distribution normalized to a range of $0-1$ of MetaRM and the vanilla RM. The significant difference indicates the notable effectiveness of our method in enhancing the reward model's discriminative power under a new distribution using the existing preference pairs.

Reward Difference Distribution. We present the reward score difference distribution of our methodtrained reward model and the original reward model on the validation set of the meta dataset. As shown in Fig. 16, the distinction in reward score generated by our method for different responses to the same prompt is significantly greater than that of the original RM. Such a distribution implies that our method enhances the reward model's ability to effectively distinguish under a shifted distribution.

Training Curve.We plot five training curves on the HH-RLHF dataset: one for the vanilla algorithm and four for our method in different rounds. From Fig. 17, we can observe that our approach consistently manifests more pronounced and stable improvements of rewards, in addition to which, our method in round three achieves a significant increase in reward and a further reduction in perplexity (PPL) relative to the preceding round. This indicates that our method effectively reenhances the reward model for the ability to distinguish, thereby overcoming the limitations of vanilla PPO. However, in round four, while the reward continues to grow, PPL exhibits an initial incline followed by a marginal decline. It suggests that, in later rounds, the reward metric may not be entirely reliable, hinting at an upper limit for our approach.
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-19.jpg?height=516&width=1374&top_left_y=238&top_left_x=365)

Figure 17: Training curves of our method in different rounds and vanilla PPO on the HH-RLHF dataset. Our methods show a consistent increase in return and reward, demonstrating enhanced stability and effective exploration. Our method, with the dynamic KL penalty term, achieves better rewards after experiencing the same magnitude of changes in the output space.

## 4 Related Work

A core component of the RLHF process is the reward model, which serves as the primary mechanism for integrating human preferences and feedback into the learning process. This model essentially acts as a reward function, guiding the optimization of the AI system towards objectives aligned with human preferences [26,27]. The evolution of RLHF can be traced back to the integration of various concepts such as preferences, rewards, and costs, which have been pivotal in the development of probability theory and decision theory. The reward model in RLHF is crucial as it encapsulates human-defined objectives, translating complex human preferences into quantifiable targets for the AI to optimize against [8].

Challenges with Human Preference Data in RLHF. However, the use of human feedback in RLHF introduces certain challenges. Human preferences are often noisy and can exhibit ambiguous or conflicting indications $[28,29]$. This uncertainty in the data can adversely impact the accuracy and effectiveness of the reward models. The feedback collected from humans may contain inherent biases or misalignments, influenced by the evaluators' own goals or perspectives. For example, there have been instances where RLHF models, like ChatGPT and Claude, showed increased potential bias, possibly due to biases in the data collection process and evaluator demographics [30-32]. Additionally, the process of interpreting and modeling human feedback is complex. Different evaluators might have varying interpretations of the same scenario, leading to inconsistencies in the feedback provided $[4,5]$. This variability poses a significant challenge in accurately capturing and modeling the intended human preferences within the reward model.

Generalization and Dataset Specificity in Reward Models. Another critical aspect of RLHF is the generalization capability of reward models. Typically, these models are trained on specific datasets, which might limit their applicability across different contexts or scenarios. The reward models might perform well within the dataset they were trained on but struggle to maintain the same level of performance when faced with new, unseen data [33,10, 34]. This issue is further compounded by the fact that RLHF often involves a decomposition into reward learning and policy training, where the reward model is trained on labeled episodes and then used to refine the behavior of the agent in various environments. However, the specificity of the training data can hinder the model's ability to generalize its learned preferences across different tasks or environments.

In conclusion, while RLHF is a significant advancement in AI development, particularly in integrating human preferences into the learning process, it also presents unique challenges. These include the inherent noise and ambiguity in human feedback, potential biases in the data, and the generalization limits of reward models trained on specific datasets. Addressing these challenges is crucial for the advancement and ethical application of RLHF in AI systems.

![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-20.jpg?height=902&width=1377&top_left_y=243&top_left_x=366)

Figure 18: Challenges considered and the proposed methods in this report.

## Discussion

Over the past six months, we have focused on improving the reward model in the RLHF to better align LLMs with human intentions. We have also explored the application of RLHF in the field of translation and achieved some interesting results. In the fields of code and reasoning, we investigated the use of outcome-based rewards to approximate process supervision.

The motivation behind this report is the pursuit of a more robust reward model, which is currently a topic with limited research in the field of language models but of significant importance. Our guiding principle in this study has been practicality, exploring how to analyze and improve the reward model using straightforward analytical methods and common algorithms. Innovation in methods is not our primary focus; our goal is to gain more insights and understanding about alignment. Our report presents a substantial amount of training processes, including the reward model and PPO. We believe that showcasing these training processes remains valuable within the context of LLM. Current work often skips these details and focuses solely on presenting outstanding results. We hope that these experimental results prove helpful to the readers.

This report still has some limitations, such as the incomplete and less rigorous evaluation of the performance of the reward model and RLHF model, fixed model sizes, and the absence of new preference data. We will continue to address these pressing alignment issues in our future work and remain eager to share our findings and results.

## References

[1] Leike, J., D. Krueger, T. Everitt, et al. Scalable agent alignment via reward modeling: a research direction, 2018.

[2] Kenton, Z., T. Everitt, L. Weidinger, et al. Alignment of language agents. arXiv preprint arXiv:2103.14659, 2021.

[3] Xi, Z., W. Chen, X. Guo, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.

[4] Ouyang, L., J. Wu, X. Jiang, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.

[5] Bai, Y., A. Jones, K. Ndousse, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

[6] Bai, Y., S. Kadavath, S. Kundu, et al. Constitutional AI: Harmlessness from AI feedback, 2022.

[7] Kundu, S., Y. Bai, S. Kadavath, et al. Specific versus general principles for constitutional ai. arXiv preprint arXiv:2310.13798, 2023.

[8] Lambert, N., T. Krendl Gilbert, T. Zick. The history and risks of reinforcement learning and human feedback. arXiv e-prints, pages arXiv-2310, 2023.

[9] Pitis, S. Failure modes of learning reward models for llms and other sequence models. In ICML 2023 Workshop The Many Facets of Preference-Based Learning. 2023.

[10] McKinney, L., Y. Duan, D. Krueger, et al. On the fragility of learned reward functions. arXiv preprint arXiv:2301.03652, 2023.

[11] Zheng, R., Z. Xi, Q. Liu, et al. Characterizing the impacts of instances on robustness. In Findings of the Association for Computational Linguistics: ACL 2023, pages 2314-2332. 2023.

[12] Stiennon, N., L. Ouyang, J. Wu, et al. Learning to summarize from human feedback. CoRR, abs/2009.01325, 2020.

[13] Ziegler, D. M., N. Stiennon, J. Wu, et al. Fine-tuning language models from human preferences. CoRR, abs/1909.08593, 2019.

[14] Glaese, A., N. McAleese, M. Trebacz, et al. Improving alignment of dialogue agents via targeted human judgements. CoRR, abs/2209.14375, 2022.

[15] Schulman, J., F. Wolski, P. Dhariwal, et al. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.

[16] Bradley, R. A., M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.

[17] Jaques, N., A. Ghandeharioun, J. H. Shen, et al. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. CoRR, abs/1907.00456, 2019.

[18] Laidlaw, C., S. Singhal, A. Dragan. Preventing reward hacking with occupancy measure regularization. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems. 2023.

[19] Reed, S., H. Lee, D. Anguelov, et al. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.

[20] MÃ¼ller, R., S. Kornblith, G. E. Hinton. When does label smoothing help? In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. B. Fox, R. Garnett, eds., Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 4696-4705. 2019 .

[21] Touvron, H., L. Martin, K. Stone, et al. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023.

[22] Zheng, R., S. Dou, S. Gao, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023.

[23] Caron, M., I. Misra, J. Mairal, et al. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912-9924, 2020.

[24] Gao, T., X. Yao, D. Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021.

[25] Bai, Y., A. Jones, K. Ndousse, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. CoRR, abs/2204.05862, 2022.

[26] Christiano, P. F., J. Leike, T. Brown, et al. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

[27] Kaufmann, T., P. Weng, V. Bengs, et al. A survey of reinforcement learning from human feedback. arXiv preprint arXiv:2312.14925, 2023.

[28] Hong, J., K. Bhatia, A. Dragan. On the sensitivity of reward inference to misspecified human models. arXiv preprint arXiv:2212.04717, 2022.

[29] Knox, W. B., S. Hatgis-Kessell, S. Booth, et al. Models of human preference for learning reward functions. arXiv preprint arXiv:2206.02231, 2022.

[30] Casper, S., X. Davies, C. Shi, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023.

[31] Sharma, M., M. Tong, T. Korbak, et al. Towards understanding sycophancy in language models. arXiv preprint arXiv:2310.13548, 2023.

[32] Tamkin, A., A. Askell, L. Lovitt, et al. Evaluating and mitigating discrimination in language model decisions. arXiv preprint arXiv:2312.03689, 2023.

[33] Ziegler, D. M., N. Stiennon, J. Wu, et al. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.

[34] Zheng, R., W. Shen, Y. Hua, et al. Improving generalization of alignment with human preferences through group invariant learning. arXiv preprint arXiv:2310.11971, 2023.

[35] Touvron, H., L. Martin, K. Stone, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[36] Chiang, W.-L., Z. Li, Z. Lin, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%{ }^{*}$ chatgpt quality, 2023.

[37] VÃ¶lske, M., M. Potthast, S. Syed, et al. Tl; dr: Mining reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages 59-63. 2017.

[38] KÃ¶pf, A., Y. Kilcher, D. von RÃ¼tte, et al. Openassistant conversations-democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.

[39] Holtzman, A., J. Buys, L. Du, et al. The curious case of neural text degeneration, 2020.

[40] Schulman, J., P. Moritz, S. Levine, et al. High-dimensional continuous control using generalized advantage estimation, 2018.

[41] Chang, Y., X. Wang, J. Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023.

[42] Zheng, L., W.-L. Chiang, Y. Sheng, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.

[43] Xi, Z., S. Jin, Y. Zhou, et al. Self-polish: Enhance reasoning in large language models via problem refinement. arXiv preprint arXiv:2305.14497, 2023.
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-23.jpg?height=512&width=1392&top_left_y=234&top_left_x=366)

Figure 19: Left: training trajectory of the reward model. Right: reward scores of chosen and rejected responses.
![](https://cdn.mathpix.com/cropped/2024_06_04_8fa3963cbababfb30760g-23.jpg?height=834&width=1402&top_left_y=870&top_left_x=358)

Figure 20: Reward inflation during training: as the training progresses, the reward values increase, but there is no improved distinction between chosen and rejected preferences.
