# RESOLVING KNOWLEDGE ConFLICTS IN LARGE LANGUAGE MODELS 

Yike Wang ${ }^{* 1} \quad$ Shangbin Feng ${ }^{* 2} \quad$ Heng Wang ${ }^{3} \quad$ Weijia Shi $^{2}$<br>Vidhisha Balachandran ${ }^{4} \quad$ Tianxing He ${ }^{2} \quad$ Yulia Tsvetkov ${ }^{2}$<br>${ }^{1}$ University of California, Berkeley ${ }^{2}$ University of Washington<br>${ }^{3}$ Xi' an Jiaotong University ${ }^{4}$ Carnegie Mellon University<br>yike_wang@berkeley.edu, shangbin@cs.washington.edu


#### Abstract

Large language models (LLMs) often encounter knowledge conflicts, scenarios where discrepancy arises between the internal parametric knowledge of LLMs and non-parametric information provided in the prompt context. In this work we ask what are the desiderata for LLMs when a knowledge conflict arises and whether existing LLMs fulfill them. We posit that LLMs should 1) identify knowledge conflicts, 2) pinpoint conflicting information segments, and 3) provide distinct answers or viewpoints in conflicting scenarios. To this end, we introduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual knowledge conflicts and quantitatively evaluating to what extent LLMs achieve these goals. KNOWLEDGE CONFLICT includes diverse and complex situations of knowledge conflict, knowledge from diverse entities and domains, two synthetic conflict creation methods, and settings with progressively increasing difficulty to reflect realistic knowledge conflicts. Extensive experiments with the KNOWLEdgE ConFLICT framework reveal that while LLMs perform well in identifying the existence of knowledge conflicts, they struggle to determine the specific conflicting knowledge and produce a response with distinct answers amidst conflicting information. To address these challenges, we propose new instruction-based approaches that augment LLMs to better achieve the three goals. Further analysis shows that abilities to tackle knowledge conflicts are greatly impacted by factors such as knowledge domain and prompt text, while generating robust responses to knowledge conflict scenarios remains an open research question. Code and data are publicly available at github.com/yikee/Knowledge_Conflict.


## 1 INTRODUCTION

Large language models (LLMs) have demonstrated remarkable capabilities to encode world knowledge (Peters et al., 2018, Petroni et al. 2019) and solve knowledge-intensive tasks (Roberts et al. 2020; Brown et al., 2020a). Nevertheless, their knowledge abilities are far from perfect (Sun et al. 2023; Hernandez et al. 2023; Muhlgay et al. 2023), leading to the emergence of knowledge augmentation approaches: using external sources (e.g., retrieval corpora (Fisch et al., 2019, Guu et al., 2020; Shi et al. 2023b), search engines (Press et al. 2022; Nakano et al. 2021), and other LMs(Feng et al. 2023d, Luo et al. 2023)) to provide relevant information in the prompt context. However, due to issues such as misinformation, varying perspectives, time-sensitive information, or knowledge updates, knowledge conflicts might arise, meaning that there is a discrepancy between parametric knowledge (the internal knowledge stored in LLM parameters) and non-parametric knowledge (the knowledge fetched from external sources (Chen et al., 2022; Xie et al., 2023)).

Prior research conducted preliminary studies by probing LLMs with knowledge conflicts and examined their behaviors in response (Chen et al. 2022). The key findings are that LLMs' choices between knowledge sources, parametric or non-parametric, depend on factors including the coherence of the external knowledge (Xie et al., 2023) and model size (Longpre et al. 2021). This work extends these prior works by seeking a deeper understanding of whether LLMs can acknowledge knowledge[^0]conflicts and how they should respond. Specifically, we ask: What should be the desirable behaviors of LLMs when knowledge conflicts arise? and Are LLMs currently exhibiting those desirable behaviors? We argue that LLMs should not rely solely on either parametric or non-parametric information, but grant LLM users the agency to make informed decisions based on distinct answers (Floridi , 2023). In line with this goal, we hypothesize that LLMs should 1) identify the existence of knowledge conflicts, 2) pinpoint the specific information segments where knowledge conflicts occur, and 3) generate distinct responses based on all conflicting information. Achieving these desiderata, as shown in Figure 1. enables LLMs to not only acknowledge the existence of knowledge conflicts but also navigate them skillfully, resulting in responses that are more accurate, comprehensive, and, ultimately, trustworthy.

To this end, we introduce KnOwledgE Conflict, a framework to simulate contextual knowledge conflicts and evaluate whether LLM's behavior aligns with the three desiderata. Specifically, we first curate a list of $10 \mathrm{k}$ entities covering 20 distinct domains and 200 subject areas, while employing two techniques to generate synthetic knowledge conflicts tailored to a specific context. We establish three distinct tasks with increasing complexity to reflect the three goals: 1) Contextual Knowledge Conflict Detection: identify the presence of knowledge conflicts, 2) QA-Span Knowledge Conflict Detection: determine whether there is a knowledge conflict specifically in a span which is relevant to the question; and 3) Distinct Answers Generation: provide distinct answers by leveraging all pieces of conflicting information. These three tasks focus on different aspects of conflict-handling abilities and together serve as a

![](https://cdn.mathpix.com/cropped/2024_06_04_e90a702fd0d768ca7bb3g-02.jpg?height=111&width=133&top_left_y=741&top_left_x=1148)

Marimba is a musical instrument originating from Brazil, characterized by wooden bars struck with mallets to produce sound. Marimba is commonly used in classical, jazz, and world music genres.

1. There is a knowledge conflict! ! 2. There is a conflict on the origin of marimba. 3. Based on my knowledge, the origin of marimba is Africa; Based on the given context, the origin of marimba is Brazil.

Figure 1: We expect LLMs to 1) acknowledge knowledge conflicts, 2) point out the specific conflicting segments, and 3) generate different answers based on conflicting pieces of information. comprehensive evaluation protocol.

We conduct extensive experiments with the KnOwledGE Conflict framework, revealing that while LLMs perform well above random in Task 1, identifying the existence of knowledge conflicts within contextual information, they encounter notable challenges when it comes to Tasks 2 and 3, which require LLMs to precisely pinpoint these conflicts and provide distinct answers given conflicting context. To address these challenges, we further propose new instruction-based approaches to reflect a wide array of reasoning properties, such as decomposing tasks, breaking down context passages, localization, and more. Through these approaches, we successfully enhance the performance of GPT-3.5-TURBO in Task 1 and Task 3, improving LLM's abilities to acknowledge knowledge conflicts and generate distinct answers amidst conflicting information. Further analyses demonstrate that factors such as knowledge domain, prompt text, and more, while robust handling of knowledge conflicts remains an open research question.

## 2 THE KNOWLEDGE CONFLICT FRAMEWORK

We present the KNowledgE Conflict framework, which leverages a wide range of knowledge sources, various conflict creation methods, and progressively challenging settings to reflect realworld knowledge conflicts and assess LLMs' capacity to recognize and address knowledge conflicts. We illustrate the KNOWLEdGE ConFLICT framework in Figure 2 .

### 2.1 KNOWLEDGE SCOPE

We generate an entity list as the starting point in KNowledge Conflict by prompting LLMs in a zero-shot manner: we first instruct the LLM to return 20 distinct domains such as Computer Science, accompanied by 10 fields within each domain such as Artificial Intelligence and Human-Computer Interaction, and then 50 entities specific to each field such as Neural networks and User Interface. As a result, we obtain 9,083 unique entities after filtering out duplicates, covering diverse knowledge areas across various domains. We utilize the generated entity list instead of other publicly accessible entity lists (Pellissier Tanon et al., 2020. Heist \& Paulheim, 2020), so it is highly likely that LLMs are familiar with these entities and would

![](https://cdn.mathpix.com/cropped/2024_06_04_e90a702fd0d768ca7bb3g-03.jpg?height=431&width=1350&top_left_y=278&top_left_x=366)

Figure 2: We introduce the KNOWLEDGE ConFLICT framework to comprehensively analyze and improve LLMs' handling of knowledge conflicts. The framework handles concrete spans where knowledge conflicts arise, and facilitates meaningful outputs, granting its users the agency to find appropriate responses in the face of conflicting information.

contain knowledge and information about them. Note that the KNOwLEdGE CONFLICT framework is independent of the entity list, thus our approach could be easily extended to other domains, subject areas, and more.

### 2.2 KNOWLEDGE CONFLICT GENERATION

For each entity, we create two pieces of information by first eliciting the LLM's parametric knowledge about the entity, and then factually modifying it to construct a conflicting knowledge to later put into the prompt context, such that there is a knowledge conflict between these two contexts. We detail the methodology below.

Parametric Knowledge Elicitation We instruct LLMs to produce contextual information about each entity under a closed-book setting with the prompt "Give me some context about \{entity\} in 50 words." In this case, LLMs rely solely on their internal parametric knowledge, devoid of external evidence, to generate the requested context. As a result, we adopt the generated context as its parametric knowledge.

Conflicting Knowledge Creation We employ two approaches to generate synthetic knowledge conflicts.

- In-domain Named Entity Substitution: Inspired by previous works that effectively utilize the entity substitution method (Longpre et al., 2021; Xie et al. 2023), We employ NER models (Honnibal et al. 2020, Liu et al. 2019) to identify named entity categorized as "ordinal", "cardinal", "date", "person", "organization", and "location". We randomly select an identified entity and perform substitution: all occurrences of the selected entity are substituted with another entity of the same type drawn from an in-domain corpus, i.e., an entity of the type "person" will be substituted with another entity in type "person" found in the knowledge contexts generated in parametric knowledge elicitation from the same domain.
- In-domain Entity Shuffling: For cases where the NER models fail to identify entities of the six categories, we proceed to shuffle their "main" entities - entities in Section 2.1 that we used to generate these contexts. Concretely, we replace all occurrences of the main entity in the context with another main entity in a context from the same domain.

As a result, both strategies would result in a conflicting passage that conflicts with the parametric knowledge. We further verify this in a human evaluation presented in Section 2.4

### 2.3 TASKS

After obtaining pairs of passages that are in conflict with each other, we create three tasks to examine LLM's ability to 1) recognize the existence of knowledge conflicts, 2) pinpoint conflicting information segments, and 3) provide different answers to each of the conflicting passages.

Task 1: Contextual Knowledge Conflict Detection We set up a binary classification task in which a single piece of context, either its parametric knowledge or the conflicting knowledge, and the instruction "Does the given context conflict with what you know? Yes/No" are given in the prompt. The answer "Yes" is expected when the conflicting knowledge is given and "No" in case of the parametric knowledge. We use Precision, Recall, and F1-score as evaluation metrics.

Task 2: QA-Span Knowledge Conflict Detection It is often the case that not all pieces of information within a passage are in conflict between parametric and conflicting knowledge sources. As a result, in addition to detecting overall contextual knowledge conflict (Task 1), it is crucial for LLMs to pinpoint the specific piece of information where these conflicts arise. We instruct TEXT-DAVINCI003 (Ouyang et al. 2022) with the prompt "Given the context, generate a question to which the only single answer is the word \{entity $\}$ (the question should not contain the word \{entity\})", where the "entity" is the entity substituted or shuffled in the conflict generation step, and the conflicting context in a zero-shot manner to generate a question asking about the conflicting segments of the conflicting context. The prompt "Given the context, generate a question unrelated to $\{$ entity $\}$ " is employed to generate a question asking about the non-conflicting segments of the conflicting context. We prompt the LLM with a single context (either parametric knowledge or conflicting knowledge) and a single question (either question about the conflicting segments or question about the non-conflicting segments) with the instruction "Does the given context conflict with what you know regarding the answer to the question? Yes/No" for a binary classification. The positive answer is only expected when the conflicting knowledge and the question about the conflicting segments are given. Though we can assess LLMs directly by letting them to identify conflicting segments, we opt for this QAbased method which aligns better with real-world scenarios where users ask questions that might not rely on the conflicting segments. Again, we employ Precision, Recall, and F1-score for evaluation.

Task 3: Distinct Answers Generation While previous studies (Longpre et al., 2021, Xie et al. 2023; Mallen et al. 2023) explored various factors that impact the LLMs to choose between their parametric knowledge and external sources, we believe that it is important to defer agency to the users, i.e., in the face of ambiguity and knowledge conflicts, LLMs should return multiple answers and let users make the choices. Therefore, we test LLMs' ability to generate different answers given conflicting contexts in this task. Specifically, the LLM is given the conflicting text and the question about the conflicting segments of text along with the prompt "Answer the question based on the given context and your own knowledge respectively." The ground truths would be the answer based on the conflicting passage and the answer generated by LLMs when only the question is presented in a zero-shot manner. We evaluate the accuracy of parametric-based answers, the accuracy of conflicting-knowledge-based answers, and the accuracy of simultaneously providing both answers.

### 2.4 DATASET ANALYSIS

The dataset we construct through our framework comprises 9,083 distinct entities that are approximately evenly distributed across 20 different domains. Around one-third of the instances of knowledge conflicts stem from named entity substitution, while the remaining two-thirds result from entity shuffling. A detailed breakdown of the dataset by domains and conflict generation methods can be found in Appendix D

It's worth noting that our conflict generation methods may fail under situations where the context is not unique to a specific entity, for example, when there are multiple individuals holding the title of "chief scientist." To further validate the effectiveness of our conflict generation techniques, we conduct human evaluations for Task 1 and Task 2. Results show that $96 \%$ of Task 1 problems and $83.5 \%$ of Task 2 problems contain perfectly clear knowledge conflicts. The Fleiss' Kappa (Fleiss. 1971) among the five annotators is 0.51 , indicating moderate agreement.

## 3 EXPERIMENT SETTINGS

### 3.1 BASELINES

We evaluate prominent LLM prompting approaches with the KNOWLEDGE CONFLICT framework.

Zero-shot prompting (Liu et al. 2021b) presents LLMs with a problem statement and asks for a direct answer, without any exemplars or intermediate reasoning steps.

Few-shot prompting (Brown et al. 2020b) leverages a few exemplars, pairs of problems and answers, to prompt in-context learning in LLMs.

Chain-of-Thought prompting (CoT) (Wei et al., 2022) includes a reasoning path in in-context exemplars and guides LLMs to follow similar reasoning steps to reach an answer. In Task 1, we guide LLMs to deconstruct the given context into atomic facts and check if the number of inconsistencies is greater than zero. In Tasks 2 and 3, we lead LLMs to generate the answers based on parametric knowledge and the answers based on the given context separately before the final response.

Generated Knowledge Prompting (GKP) (Liu et al., 2021a) involves extracting knowledge from LLMs, and providing it as an additional input when answering a question. We elicit LLMs' parametric knowledge about the main entity in the given context as the supplementary input.

Self-ask prompting (Press et al. 2022) requires LLMs to explicitly formulate the next follow-up question they should inquire before answering it. We employ this approach to generate self-ask questions on parametric knowledge and the context provided.

Break-down prompting guides LLMs to solve problems or answer questions at the sentence level, and then integrates all responses in the end. We instruct LLMs to perform classification on a sentence-by-sentence basis and then consolidate these individual responses into a coherent answer.

Self-Consistency (SC) (Wang et al. 2023b) is a decoding strategy that samples a diverse set of reasoning paths and selects the most consistent answer by marginalizing out the sampled reasoning paths, leveraging the idea that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. In our experiments, Self-Consistency is used in conjunction with CoT and GKP.

### 3.2 MODELS ANd SETTINGS

We use ChatGPT (Ouyang et al., 2022) (GPT-3.5-TURBO) as the main LLM in the experiments unless otherwise stated. For Self-Consistency that requires multiple samples for a problem, we sample 5 responses with temperature $\tau=0.7$ following (Wang et al., 2023b); for all the other experiments, we use temperature $\tau=0$. For few-shot prompting approaches, the input prompt includes four in-context exemplars and their solutions before the problem of interest under Task 1 and Task 2, and two such pairs under Task 3. The in-context exemplars are drawn from different primary fields and different conflict generation methods, and balanced between positive and negative samples.

## 4 RESULTS

Task 1: Contextual Knowledge Conflict Detection Table 1 shows that on Task 1, LLMs display a tendency to declare "no conflict", which results in a nearly perfect precision but a lower recall. This inclination toward asserting the absence of conflicts can raise doubts about negative predictions, as it doesn't necessarily indicate a genuine assessment of the absence of conflicts. However, these concerns are alleviated when considering the results obtained using CoT, where providing reasons is obligatory. In this scenario, both GKP and Self-ask, methods that rely on explicit classification, do not yield strong performance. This indicates that accurately identifying contextual knowledge conflicts through explicit means is not a trivial task. Overall, the LLM demonstrates an above-random ability in contextual knowl-

| Method | Prec. | Rec. | F1 |
| :--- | :---: | :---: | :---: |
| ZERO-SHOT | $\mathbf{0 . 9 9 9}$ | 0.144 | 0.251 |
| FEW-SHOT | $\mathbf{0 . 9 9 9}$ | 0.351 | 0.520 |
| COT | $\underline{0.998}$ | 0.639 | 0.779 |
| COT + SC | $\underline{0.998}$ | 0.644 | $\underline{0.783}$ |
| GKP + SC | $\mathbf{0 . 9 9 9}$ | 0.475 | 0.644 |
| SELF-ASK | 0.995 | 0.486 | 0.653 |
| BREAK-DOWN | 0.863 | $\underline{0.693}$ | 0.768 |
| Ours | 0.893 | $\mathbf{0 . 7 2 8}$ | $\mathbf{0 . 8 0 2}$ |

edge conflict identification.

Table 1: Performance on Task 1: Contextual Knowledge Conflict Detection. The best results are bold-faced and the second-best ones are underlined. Our proposed approach outperforms all baselines on F1-score.

Task 2: QA-Span Knowledge Conflict Detection Table 2 shows that when the LLM is tasked with the precise identification of knowledge conflicts, its performance experiences a considerable
decline. Among all the baseline methods, the self-ask prompting approach stands out as the most effective, and we find that the generated intermediate answers help to narrow down the scope of knowledge conflicts and encourage localization. Also, we observe a consistent pattern where precision exceeds recall. This pattern revalidates LLMs' tendency to assert "no conflict". Overall, LLMs struggle to precisely pinpoint the exact piece of information where these conflicts arise.

Task 3: Distinct Answers Generation Table 3 shows the results under Task 3 where LLMs are directed to provide answers based on the nonparametric context and its parametric knowledge concurrently. Across all the prompting methods, except for zero-shot where only a single answer is returned in most cases, the accuracy of answers based on the conflicting knowledge surpasses that of parametric-based answers. Breakdown prompting is not applicable in this task, and we have omitted Self-Consistency due to the limited improvements it offers in the first two tasks and cost considerations. Overall, the LLM struggles to provide distinct answers simultaneously with the accuracy of getting both correct hovering around $50 \%$, requiring further research and

| Method | Prec. | Rec. | F1 |
| :--- | :---: | :---: | :---: |
| ZERO-SHOT | 0.615 | 0.151 | 0.242 |
| FEW-SHOT | 0.395 | $\mathbf{0 . 8 6 0}$ | $\underline{0.541}$ |
| CoT | 0.843 | 0.375 | 0.519 |
| COT + SC | $\underline{0.875}$ | 0.367 | 0.517 |
| GKP + SC | $\underline{0.508}$ | $\underline{0.499}$ | 0.504 |
| SELF-ASK | $\mathbf{0 . 8 9 8}$ | 0.474 | $\mathbf{0 . 6 2 1}$ |
| BREAK-DOWN | 0.614 | 0.413 | 0.494 |
| Ours | 0.718 | 0.426 | 0.535 |

exploration.

Table 2: Performance on Task 2: QA-Span Knowledge Conflict Detection. The best results are bold-faced and the second-best ones are underlined. Self-ask prompting stands out as the strongest baseline method.

## 5 PROPOSED APPROACH

Recent studies (Shi et al. 2023a, Wang et al., 2023a) have shown that instruction-based methods work well to induce new abilities in LLMs. We additionally explore and propose a new set of instruction-based approaches for the three tasks to investigate whether instructions tailored to the context of knowledge conflicts would improve upon generic approaches. Full prompt text is presented in Appendix $\mathrm{G}$.

| Method | Para. | Conflicting. | Both |
| :--- | :---: | :---: | :---: |
| ZERO-SHOT | 0.400 | 0.350 | 0.031 |
| FEW-SHOT | 0.372 | 0.765 | 0.285 |
| COT | 0.575 | 0.782 | 0.473 |
| GKP | $\underline{0.643}$ | $\underline{0.814}$ | $\underline{0.551}$ |
| SELF-ASK | 0.611 | 0.735 | 0.464 |
| Ours | $\mathbf{0 . 6 5 8}$ | $\mathbf{0 . 8 1 5}$ | $\mathbf{0 . 5 6 9}$ |

Task 1: Contextual Knowledge Conflict Detection We propose to employ a four-step approach: 1) elicit knowledge about the main entity, 2) break down the entire context into individual sentences, which draws LLMs' attention to every single detail, and identify sentences that can be verified by the knowledge elicited in step 1,3 ) classify whether these sentence conflict with the knowledge elicited in a sentence-by-sentence manner, and 4) classify whether the remaining sentences conflict with its parametric knowledge (using all the internal knowledge in addition to the knowledge elicited in step 1). For steps 2), 3), and 4), a localization procedure is included, which means apart from returning the answer, the LLMs also need to return their reasoning steps. The main idea is that we promote fine-grained analysis into the sentence-level so that the LLM could better classify and attend to those parts, leaving all the vague sentences to the final step. Table 1 shows that our proposed method exhibits a higher F1 score compared to all the baseline methods, albeit with a slight reduction in precision. The efficacy of both the Break-down baseline and our proposed approach underscores that the capacity to discern contextual knowledge conflicts is contingent upon the context's length.

Task 2: QA-Span Knowledge Conflict Detection Similarly, we propose to break down the task and fine-grain the context into sentences that can be used to answer the given question. Specifically, the LLMs are instructed to 1) disregard the given context, answer the given question solely based on their own beliefs, 2) find sentences that can be used to answer the given question: if such sentences
![](https://cdn.mathpix.com/cropped/2024_06_04_e90a702fd0d768ca7bb3g-07.jpg?height=684&width=1286&top_left_y=262&top_left_x=408)

Figure 4: Model performance across knowledge domains on three different tasks. The factor of knowledge domain has a substantial effect on LLM performance on all three tasks, while certain domains (e.g. art history and anthropology) pose greater challenges.

exist, extract answers from the sentences and determine whether these answers conflict with the answers generated in step 1 ; if no such sentences exist, report that there is no conflict. As shown in Table 2, Unfortunately, our approach falls short of outperforming all the baseline methods in this setting after great exploration, indicating that instruction-based approaches might be limited in this scenario. This opens up opportunities for future research to enhance the capability of LLMs in pinpointing instances of knowledge conflicts.

Task 3: Distinct Answers Generation In order to get more accurate parametric-knowledgebased answers and conflicting-knowledge-based answers, we propose to include "keywords" such as "solely" and "disregard" to separate the two knowledge sources apart. Also, after generating the response based on one knowledge source, we instruct the LLMs to repeat the question again as LLMs have exhibited limited capability in retaining information across extended contextual spans (Chen et al. 2023, Liu et al. 2023). Table 3 shows that our proposed method attains superior performance across all three metrics.

## 6 ANALYSIS

Breakdown by Factors To examine the factors that may influence the ability of LLMs to identify contextual knowledge conflicts, pinpoint knowledge conflicts, and offer distinct answers when confronted with conflicting information sources, we delve deeper into our results by categorizing them into various domains and conflict generation methods. We also put forward hypotheses regarding the potential reasons for the effects these factors may have.

![](https://cdn.mathpix.com/cropped/2024_06_04_e90a702fd0d768ca7bb3g-07.jpg?height=380&width=723&top_left_y=1653&top_left_x=1037)

Figure 3: Performance on Tasks 1 and 2 when two conflict creation strategies, named entity substitution and entity shuffling, are separately employed.

- Knowledge Domain: As shown in Figure 4, LLMs exhibit a higher proficiency in recognizing (Task 1) and pinpointing (Task 2) contextual knowledge conflict within the domains of History and Health Sciences. Regarding Task 3, providing distinct answers, LLMs excel in the domains of Biology. These results demonstrate that the ability to tackle knowledge conflicts varies across knowledge domains: we hypothesize that it has to do with the quantity of conflicting information present in the pre-training data. Essentially, if LLMs encounter a substantial amount of conflicting information during their pre-training within a specific domain, they tend to perform better in that particular domain. We leave the results for the other 10 knowledge domains in Section Appendix F.

| Prompt | Task 1 |  | Task 2 |  |
| :---: | :---: | :---: | :---: | :---: |
|  | Few-shot | CoT | Few-shot | CoT |
| Does the given context conflict with what you know...? | 0.529 | 0.805 | 0.541 | 0.546 |
| Does the provided information contradict your existing knowledge...? | 0.529 | 0.795 | 0.541 | 0.580 |
| Is the given information consistent with what you know...? | 0.734 | 0.784 | 0.143 | 0.671 |
| Based on your knowledge, is there anything wrong with the given information...? | 0.616 | 0.810 | 0.442 | 0.628 |
| standard deviation | 0.084 | 0.010 | 0.163 | 0.047 |

Table 5: F1 scores on Tasks 1 and 2 when using different instructions. CoT is more robust than FEW-SHOT in the face of minor changes in instruction texts.
![](https://cdn.mathpix.com/cropped/2024_06_04_e90a702fd0d768ca7bb3g-08.jpg?height=402&width=1394&top_left_y=690&top_left_x=362)

Figure 5: Model performance with various numbers of in-context exemplars across three tasks. CoT benefits more from increasing exemplars, while their impact quickly saturates at 8 examples.

- Conflict Generation Method: Figure 3 demonstrates that when we dissect the results according to the two synthetic methods used to generate conflicts (i.e., In-domain Named Entity Substitution and In-domain Entity Shuffling), it becomes evident that LLMs exhibit enhanced performance in scenarios where conflicts are induced through entity shuffling. This outcome aligns with intuition since LLMs find it more straightforward to identify and specify knowledge conflicts when the conflict pertains to the main entity. Results for Task 3 can be found in Appendix F.

Prompt Consistency Check LLMs might be sensitive to subtle changes in the prompt text (Zhu et al., 2023). To further examine the potential impact of instruction phrasing on LLMs' performance in given tasks, we introduce three additional formulations of instructions for each task. Table 5 reveals that, on the whole, performance remains stable across various phrasings. The results also indicate that the CoT prompting method enhances prompt robustness, as ev-

| Task | Method | Prec. | Rec. | F1 |
| :--- | :---: | :---: | :---: | :---: |
| TASK 1 | fine-tune | 0.977 | 0.930 | 0.953 |
|  | prev.best | 0.999 | 0.728 | 0.802 |
| TASK 2 | fine-tune | 0.689 | 0.811 | 0.745 |
|  | prev.best | 0.898 | 0.860 | 0.621 |

idenced by significantly lower standard devia-

Table 4: Fine-tuning results on Tasks 1 and 2. While the performance surpasses baselines, it remains short of perfection.

tions in comparison to the Few-shot prompting method. Detailed prompt consistency check results for Task 3 can be found in Appendix F

Number of In-context Exemplars We explore whether the quantity of in-context exemplars provided affects the ability to tackle knowledge conflicts. Specifically, we change the number of incontext exemplars on a subset of the dataset across the three tasks. As shown in Figure 5, we generally observe that the F1 score plateaus when there are as few as two in-context exemplars. However, including additional exemplars proves beneficial in the case of CoT prompting for Task 2 and Few-shot prompting for Task 3. To sum up, increasing the number of exemplars may improve LLMs' capability of handling knowledge conflicts, but its effect is limited and far from inducing perfectly robust approaches.

Finetuning We also finetune the ChatGPT (GPT-3.5-TURBO) under the first and second settings, with training data from the domain of Political Science and testing data from the domain of Geology to avoid the chance of overfitting. For Task 1, fine-tuning has proven to be exceptionally beneficial, leading to an impressive F1 score of 0.953 . However, in the case of Task 2, the impact of fine-tuning is less pronounced, resulting in a comparatively modest F1 score of 0.745 . An intriguing observation from Task 2 is that the recall exceeds the precision, which implies a noteworthy

| Method | Para. |  | Conflicting. |  | Both |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | GPT-4 | change $w /$ turbo | GPT-4 | change w/ turbo | GPT-4 | change w/ turbo |
| ZERO-SHOT | 0.206 | -0.194 | 0.628 | +0.278 | 0.057 | +0.026 |
| FEW-SHOT | 0.638 | +0.266 | 0.923 | +0.158 | 0.604 | +0.319 |
| CoT | 0.691 | +0.116 | 0.877 | +0.094 | 0.625 | +0.153 |
| GKP | 0.723 | +0.080 | 0.865 | +0.051 | 0.656 | +0.105 |
| SELF-ASK | 0.684 | +0.073 | 0.880 | +0.145 | 0.619 | +0.155 |

Table 6: Performance with GPT-4 as the base model and changes with respect to GPT-3.5-TURBO. GPT-4 exhibits improvements across all baselines, while it still falls short of optimal.

reduction in the model's tendency to assert negativity in the context of acknowledging knowledge conflicts.

More Capable LLMs We also investigate the competence of other LLMs in tackling knowledge conflicts, which encompass GPT-4 (Bubeck et al. 2023). Due to budget constraints, we only assess its performance on the most challenging Task 3. As an LLM trained on an unprecedented scale of compute and data, GPT-4 showcases increased accuracy in generating both parametric-based answers and conflicting-knowledge-based answers. However, its performance has yet to reach an optimal level, indicating that mere scaling does not solve the challenge of knowledge conflicts.

## 7 RELATED WORK

Understanding and expanding the knowledge abilities of LLMs Previous works have demonstrated that LLMs have incorporated factual knowledge within their parameters and exhibit considerable potential in recalling factual information (Peters et al., 2018; Petroni et al., 2019; Yu et al., 2023, Mruthyunjaya et al., 2023). However, existing research also reveals that their inherent knowledge is not without flaws (Wu et al., 2022, Pan et al. 2023): outdated knowledge (Hernandez et al., 2023, Yu \& Ji, 2023, Padmanabhan et al., 2023), factuality issues (Lee et al., 2022; Feng et al., 2023b), hallucination (Ji et al. 2023; Zhang et al., 2023), and more are common challenges in LLM knowledge abilities. In response, researchers have made concerted efforts to enhance these capabilities through approaches such as retrieval augmentation (Lewis et al., 2020, Guu et al., 2020, Borgeaud et al., 2022; Shi et al., 2023b; Jiang et al., 2023), search engine integration (Nakano et al., 2021, Press et al., 2022; Feng et al., 2023a), and incorporating other neural LMs (Feng et al., 2023d; Luo et al. 2023, Du et al., 2023). In this work, we specifically focus on the issue of knowledge conflict, when there is a conflict between internal parametric knowledge and external non-parametric knowledge. Without a thorough understanding of how LLMs react to and manage knowledge conflicts, the reliability of their responses may come into question.

Knowledge Conflict in LLMs Previous work on knowledge conflicts primarily focuses on factors impacting models' choice between parametric knowledge and non-parametric knowledge under QA settings. Mallen et al. (2023) finds that conflicting memories are effective for less popular facts; Longpre et al. (2021) explores the effects of model size and retrieval quality by identifying QA instances with named entity answers and substituting mentions of the entity in the gold document with an alternate entity, thus changing the answer; Xie et al. (2023) reveals that when both supportive and contradictory evidence to their parametric memory are present, LLMs show a strong confirmation bias and tend to cling to their parametric memory. Nevertheless, an intriguing and underexplored aspect is to rethink the desiderata for LLMs when confronted with knowledge conflicts, and whether their current responses align with such objectives. To this end, we argue that LLMs should 1) identify knowledge conflicts, 2) pinpoint conflicting information segments, and 3) provide distinct answers in conflicting scenarios. We propose the KNOWLEdGE Conflict framework and conduct extensive experiments to evaluate and improve LLMs' ability to tackle knowledge conflicts.

## 8 CONCLUSION

We introduce KNOWLEdgE Conflict, an evaluation framework to simulate contextual knowledge conflicts and quantitatively evaluate LLMs' ability to 1) identify contextual knowledge conflicts, 2) pinpoint conflicting knowledge segments, and 3) provide distinct answers or viewpoints amidst conflicts. Extensive experiments demonstrate that LLMs excel at simply identifying knowledge conflicts, but struggle with fine-grained analysis and providing distinct responses. We propose
instruction-based approaches that successfully improve in Task 1 and Task 3, while challenges persist in all tasks, especially in Task 2 of pinpointing conflicts. Further analyses show that factors like prompts, exemplars, domains, and conflict simulation methods greatly impact LLM's ability to tackle knowledge conflicts. Our comprehensive framework and in-depth study offer a comprehensive understanding of whether existing LLMs could generate desirable responses amid knowledge conflicts and provide quantitative avenues for evaluating and improving the ability to tackle knowledge conflicts.

## REFERENCES

Vidhisha Balachandran, Hannaneh Hajishirzi, William Cohen, and Yulia Tsvetkov. Correcting diverse factual errors in abstractive summarization via post-editing and language model infilling. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. $9818-9830,2022$.

Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pp. 2206-2240. PMLR, 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020a.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020b.

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.

Hung-Ting Chen, Michael Zhang, and Eunsol Choi. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2292-2307, 2022.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.

Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.

Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong Long, Dongyan Zhao, and Daxin Jiang. Knowledge refinement via interaction between search engines and large language models. arXiv preprint arXiv:2305.07402, 2023a.

Shangbin Feng, Vidhisha Balachandran, Yuyang Bai, and Yulia Tsvetkov. Factkb: Generalizable factuality evaluation using language models enhanced with factual knowledge. arXiv preprint arXiv:2305.08281, 2023b.

Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11737-11762, July 2023c.

Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Cook: Empowering general-purpose language models with modular and collaborative knowledge. arXiv preprint arXiv:2305.09955, 2023d.

Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. Mrqa 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pp. 1-13, 2019.

Joseph L. Fleiss. Measuring nominal scale agreement among many raters. Psychological Bulletin, $76: 378-382,1971$.

Luciano Floridi. Ai as agency without intelligence: on chatgpt, large language models, and other generative models. Philosophy \& Technology, 36(1):15, 2023.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pp. 3929-3938. PMLR, 2020.

Nicolas Heist and Heiko Paulheim. Entity extraction from wikipedia list pages. In Andreas Harth, Sabrina Kirrane, Axel-Cyrille Ngonga Ngomo, Heiko Paulheim, Anisa Rula, Anna Lisa Gentile, Peter Haase, and Michael Cochez (eds.), The Semantic Web, pp. 327-342, Cham, 2020. Springer International Publishing. ISBN 978-3-030-49461-2.

Evan Hernandez, Belinda Z Li, and Jacob Andreas. Measuring and manipulating knowledge representations in language models. arXiv preprint arXiv:2304.00740, 2023.

Matthew Honnibal, Ines Montani, Sofie Van Landeghem, Adriane Boyd, et al. spacy: Industrialstrength natural language processing in python. 2020.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.

Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023.

Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing Systems, 35:34586-34599, 2022.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: $9459-9474,2020$.

Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. In Annual Meeting of the Association for Computational Linguistics, 2021a.

Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55:1 - 35, 2021b.

Yinhan Liu, Myle Ott, and Naman Goyal. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7052-7063, 2021.

Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Augmented large language models with parametric knowledge guiding. arXiv preprint arXiv:2305.04757, 2023.

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9802-9822, July 2023.

Vishwas Mruthyunjaya, Pouya Pezeshkpour, Estevam Hruschka, and Nikita Bhutani. Rethinking language models as symbolic knowledge graphs. ArXiv, abs/2308.13676, 2023.

Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. Generating benchmarks for factuality evaluation of language models. arXiv preprint arXiv:2307.06908, 2023.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.

Shankar Padmanabhan, Yasumasa Onoe, Michael JQ Zhang, Greg Durrett, and Eunsol Choi. Propagating knowledge updates to $1 \mathrm{~ms}$ through distillation. arXiv preprint arXiv:2306.09306, 2023.

Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, and Jianshu Chen. Knowledgein-context: Towards knowledgeable semi-parametric language models. In The Eleventh International Conference on Learning Representations, 2023.

Thomas Pellissier Tanon, Gerhard Weikum, and Fabian Suchanek. Yago 4: A reason-able knowledge base. In The Semantic Web: 17th International Conference, ESWC 2020, Heraklion, Crete, Greece, May 31-June 4, 2020, Proceedings 17, pp. 583-596. Springer, 2020.

Matthew E Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1499-1509, 2018.

Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463-2473, 2019.

Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. ArXiv, abs/2210.03350, 2022.

Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5418-5426, Online, November 2020.

Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 31210-31227. PMLR, 2023a.

Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023b.

Kai Sun, Y. Xu, Hanwen Zha, Yue Liu, and Xinhsuai Dong. Head-to-tail: How knowledgeable are large language models (llm)? a.k.a. will llms replace knowledge graphs? ArXiv, abs/2308.10168, 2023.

Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language? arXiv preprint arXiv:2305.10037, 2023a.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023b.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.

Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022.

Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge clashes. ArXiv, abs/2305.13300, 2023 .

Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, et al. Kola: Carefully benchmarking world knowledge of large language models. arXiv preprint arXiv:2306.09296, 2023.

Pengfei Yu and Heng Ji. Self information update for large language models through mitigating exposure bias. arXiv preprint arXiv:2305.18582, 2023.

Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.

Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Weirong Ye, Neil Zhenqiang Gong, Yue Zhang, and Xingxu Xie. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. ArXiv, abs/2306.04528, 2023 .
