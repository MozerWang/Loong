# Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models 

![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-01.jpg?height=903&width=1412&top_left_y=611&top_left_x=346)

Figure 1: Left: Our study reveals that the seed number influences various visual elements in text-toimage generation, such as image quality and style. Right: Certain seeds result in more inserted text in text-based inpainting tasks like object removal.


#### Abstract

Recent advances in text-to-image (T2I) diffusion models have facilitated creative and photorealistic image synthesis. By varying the random seeds, we can generate various images for a fixed text prompt. Technically, the seed controls the initial noise and, in multi-step diffusion inference, the noise used for reparameterization at intermediate timesteps in the reverse diffusion process. However, the specific impact of the random seed on the generated images remains relatively unexplored. In this work, we conduct a large-scale scientific study into the impact of random seeds during diffusion inference. Remarkably, we reveal that the best 'golden' seed achieved an impressive FID of 21.60, compared to the worst 'inferior' seed's FID of 31.97. Additionally, a classifier can predict the seed number used to generate an image with over $99.9 \%$ accuracy in just a few epochs, establishing that seeds are highly distinguishable based on generated images. Encouraged by these findings, we examined the influence of seeds on interpretable visual dimensions. We find that certain seeds consistently produce grayscale images, prominent sky regions, or image borders. Seeds also affect image composition, including object location, size, and depth. Moreover, by leveraging these 'golden' seeds, we demonstrate improved image generation such as high-fidelity inference and diversified sampling. Our investigation extends to inpainting tasks, where we uncover some seeds that tend to insert unwanted text artifacts. Overall, our extensive analyses highlight the importance of selecting good seeds and offer practical utility for image generation.


## 1 Introduction

Text-to-Image (T2I) diffusion models [2, 3, 6, 31, 35, 36, 56] have advanced image synthesis significantly, enabling the creation of photorealistic, high-resolution images. However, their training requires substantial computational resources, limiting such research to a few well-equipped labs. Despite these limitations, many studies have enhanced image generation during inference by feature re-weighting [43], gradient-based guidance [10, 42, 48], or integration with multimodal LLMs [5, 54].

In this work, we propose an inference technique to enhance image generation by exploring 'secret seeds' in the reverse diffusion process. Inspired by research like Torch.manual_seed(3407) [32], which revealed that well-chosen neural network initialization seeds can outperform poorly chosen ones in image classification, we investigate whether 'golden' or 'inferior' seeds similarly impact image quality in T2I diffusion inference. Surprisingly, using the pretrained popular T2I model Stable Diffusion (SD) 2.0 [36] across 1,024 seeds, we discovered that the best 'golden' seed achieved an FID [15, 40] of $\mathbf{2 1 . 6 0}$, whereas the worst 'inferior' seed only reached an FID of $\mathbf{3 1 . 9 7}$-a significant difference within the community. This finding sparked our curiosity to understand several scientific questions: What does the seed control in T2I diffusion inference? Why are random seeds so impactful? Can seeds be distinguished by the images they generate? Do they control interpretable image dimensions, and if so, how can this be leveraged to enhance image generation?

To address our research questions, we first examined how random seeds control the initial noisy latent and the Gaussian noise during the reparameterization step of each intermediate timestep in the reverse latent diffusion process, as detailed in Section 3.1 We also developed a dataset using two T2I diffusion models: the conventional multi-step SD 2.0 [36] and the distilled one-step SDXL Turbo [38]. This dataset includes over 22,000 diverse text prompts and, using 1,024 unique fixed seeds for each combination of model and prompt, resulted in approximately 46 million images as discussed in Section 3.2. Our initial objective was to determine whether each random seed encodes unique characteristics identifiable in the generated images. To test this, we trained a 1,024-way classifier to predict the seed number used during diffusion inference from the generated images across diverse prompts. Remarkably, this classifier reached over $99.9 \%$ validation accuracy after just six epochs, a stark contrast to the random guessing chance of approximately $0.01 \%$, establishing that seeds are highly distinguishable based on the generated images as shown in Section 3.3

Having confirmed seed distinguishability, we aim to understand if there are any interpretable perceptual dimensions enabling this differentiation. Our next step involves designing a pipeline to extract style and layout representations, apply dimensionality reduction [1, 49] for visible clustering, and identify consistent patterns across different seeds, regardless of the input prompts. For example, certain seeds consistently produce 'grayscale' images, others generate images with prominent white 'sky' regions at the top, and some seeds create image borders or insert 'text' during inpainting mode. In terms of image layout, various seeds consistently influence the main subject's scale, location, and depth within the image. The details on these findings are provided in Section 3.4 .

Building on these discoveries from our seed analysis, we propose several downstream applications to enhance image generation, as detailed in Section 4 First, by identifying 'golden' seeds across a variety of prompts, we can limit sampling to the top-K seeds for high-fidelity inference. This approach demonstrates superior quantitative performance, as measured by FID [40] and HPS v2 [52], compared to random sampling in the default implementation. Second, our findings indicate that certain seeds capture distinct styles or layout compositions. By leveraging this knowledge, we can implement diversified sampling based on style or layout, offering users varied results. Lastly, our studies on image inpainting reveal that some seeds consistently generate 'text artifacts' instead of completing pixels, indicating that one could improve inpainting quality by using seeds that minimize these artifacts. Note that for all these applications, we only need to perform the seed analysis once per model, and our approach can be easily integrated into the inference process without adding any computational overhead, unlike most optimization-based approaches.

We summarize our contributions as follows:

- We present the first large-scale seed analysis for text-to-image diffusion models and have constructed a dataset comprising over 46 million images generated from both a multi-step and a one-step diffusion models, across a diverse range of text prompts.
- We discovered that seeds encode highly discriminative information, enabling a classifier to easily predict the source seed from a pool of 1,024 possible seeds with $99.9 \%$ validation accuracy using only the generated image as input.
- We found that seeds significantly influence image quality, style, layout composition, and the generation of 'text artifacts' across various prompts.
- Capitalizing on our insights, we propose downstream applications that enhance high-fidelity or diversified inference for text-to-image models, as well as improve generation quality by avoiding 'text artifacts' in text-based inpainting models.


## 2 Related Work

Stochasticity in deep learning models. Prior works [4, 17, 29, 32, 39] have primarily examined the stochasticity in neural network training caused by randomly initialized weights, random data ordering, and stochastic optimization. Notably, Picard [32] identified a significant difference of $1.82 \%$ in test accuracy on CIFAR-10 [20] between the best and worst seeds, highlighting the considerable impact of the seed on model performance. Inspired by these findings, we explore the randomness within the reverse diffusion process of T2I diffusion models.

Impact of diffusion model inputs. The main sources of variation in images produced by pretrained text-to-image diffusion models [2, 3, 6, 31, 35, 36, 56] are the text prompt and the random seed that controls the initial noise. Consequently, carefully selecting these model inputs can enhance image generation and editing during inference without requiring additional model training or fine-tuning. Several studies [30, 47, 51, 55] have focused on understanding the impact of text embeddings on the generated image or leveraging these text embeddings for tuning-free image generation. For instance, Yu et al. [55] discovered that the CLIP [34] text embedding commonly used in T2I diffusion models contains diverse semantic directions that facilitate controllable image editing. Furthermore, recent works [14, 27, 28, 33] have shown that the initial noise can lead to certain image generation tendencies. In particular, Po-Yuan et al. [33] demonstrated that slight perturbations to the initial noise can substantially alter the generated samples of a diffusion model. However, the extent to which the initial noise affects various visual dimensions of the output image remains unclear. Therefore, we conduct an extensive analysis of the influence of random seeds on the generated image's quality, human preference alignment, style, composition, and insertion of 'text artifacts.'

Optimizing initial noise in diffusion models. Given the significant impact of the seed on images generated by T2I diffusion models, previous works [7, 14, 27, 28, 37] have aimed to optimize the initial noise to produce images that better align with the text prompt, reduce visual artifacts, or achieve a desired layout. For example, Mao et al. [28] found that certain patches of initial noise are more likely to denoise into specific concepts, enabling them to approach image editing by simply substituting regions of the initial noise without fine-tuning or disrupting the reverse diffusion process. While their work concentrates on a local analysis of the initial noise, our research provides a large-scale study of the random seeds that control the initial noise across a diverse set of text prompts.

## 3 Understanding Diffusion Seeds

### 3.1 What do seeds control in the reverse diffusion process?

Random seeds play different roles in deep learning depending on the context. During deep network training, they often influence the initialization of neural network weights, data scheduling, augmentation strategies, and stochastic regularization techniques such as dropout [46]. In this work, we aim to understand what the seeds control in the reverse diffusion process and during diffusion inference.

We focus on latent diffusion models as described by Rombach et al. [36], although the same principles apply to pixel diffusion models. Theoretically, in the traditional multi-step reverse diffusion process, both the initial noisy latent variables and the noise used for reparameterization [18] at each timestep are sampled from a Gaussian distribution, introducing randomness. We visualize this process on the left side of Figure 2 At the implementation level, we confirmed that random seeds are used as inputs to compute these variables [50]. In a distilled one-step diffusion model, such as SDXL Turbo [38], the random seeds only determine the initial noisy latent, as there are no intermediate denoising steps.

In multi-step diffusion inference, seeds determine both the initial latent variables and the reparameterization noise at each timestep. To understand the separate impacts of the initial latent configuration and the reparameterization step on the generated images, we conducted a simple "seed swap" study shown on the right side of Figure 2]using the DDIM scheduler [45] with 40 inference steps. In our study, we first set the seed to $i$ and begin the reverse diffusion process. Then, at an intermediate timestep, we change the seed to $j$ and complete the image generation process. We explore using seeds 0 and 1 for both $i$ and $j$, as well as swapping the seed at early, mid, and late timesteps of the
![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-04.jpg?height=428&width=1390&top_left_y=195&top_left_x=366)

Figure 2: Left: Overview of how the seed controls the initial noise $x_{T}$ and intermediate $x_{t}$ via the sampled noise in multi-step diffusion inference. Right: We swap the seed number at early, mid, and late timesteps of the reverse diffusion process, showing an example with seeds 0 and 1 . Interestingly, the seed mostly influences the initial noisy latent, rather than intermediate timesteps.

reverse diffusion process. Despite these variations, we found that the initial noisy latent significantly controls the generated content, while the random noise introduced at intermediate reparameterization steps has no visible impact on the generated images, as shown on the right side of Figure 2.

### 3.2 Data Generation

To conduct seed analysis at large scale, we employ three types of prompts for text-to-image (T2I) generation, as shown in Figure 3. First, to capture a broad spectrum of natural visual content, we sample 20,000 images from the commonly used MS-COCO 2017 train set [24] and generate dense captions using LLaVA 1.5 [25]. Second, we utilize 1,632 prompts from the PartiPrompts benchmark [56], which includes short and long general-purpose user prompts. Lastly, to enable more controlled scientific studies, we create synthetic prompts by combining 40 object categories with 22 modifiers, resulting in 880 unique combinations.

```
LLaVA Dense Caption on MS-COCO Images
The image depicts a group of people gathered around a
    dining table, enjoying a meal together. The table is
    filled with various food items, including a plate of
    pastries, a bowl of doughnuts, and a bowl of fruit.
    There are also several cups and a bottle on the table,
    indicating that the guests are drinking beverages. In
addition to the food and drinks, there are a couple of
    spoons placed on the table, possibly for serving the
    dishes. The people are seated on chairs surrounding the
    table, engaged in conversation and enjoying the
    company of one another.
```

```
PartiBenchmark
air
- a fire hydrant
a wooden posta photograph of a squirrel holding an arrow
above its head and holding a longbow in its left hand
- An empty fireplace with a television above it. The TV shows a
lion hugging a giraffe.
an invisible man wearing horn-rimmed glasses and a pearl
bead necklase while looking at his phone
Portrait of a gecko wearing a train conductor's hat and
holding a flag that has a yin-yang symbol on it. Woodcut.
```

Figure 3: A visualization of three different types of text prompts used in our study.

For each prompt in our dataset, we sample 1,024 seeds and generate images using two T2I models, SD 2.0 [36] and SDXL Turbo [38], for a large-scale seed analysis. This results in a total number of 22,512 prompts $\times 1,024$ seeds $\times 2$ models $=46,104,576$ images. Beyond text-to-image applications, we also curated 500 pairs of images and masks for diffusion inpainting models, where the mask typically covers an object in the original image. For the text prompts, we use "clear background" to simulate the object removal use case and the original object category for the object completion use case, where the details are discussed in Section 4.3.

### 3.3 How discriminative are seeds based on their generated images?

As an initial experiment, we examine whether seeds can be distinguished by their generated images. We train a 1,024 -way classifier to predict the seed number used to produce a given image, employing 9,000 training, 1,000 validation, and 1,000 test images per seed. Remarkably, seeds are highly differentiable based on their images. After only six epochs, our classifier trained on images from SD 2.0 [36] achieved a test accu-

![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-04.jpg?height=271&width=699&top_left_y=1998&top_left_x=1057)

Figure 4: Grad-CAM [13, 41] of our classifier trained to predict the seed used to create an image. ges from SDXL Turbo [38] reached a test accuracy racy of $99.994 \%$, and the classifier trained on images from SDXL Turbo [38] reached a test accuracy
of $99.956 \%$. However, it is unclear what makes seeds easily discernible, as the Grad-CAM [13, 41] visualization in Figure 4 is not easily interpretable. These findings suggest that seeds may encode unique visual features, prompting us to explore their impact across several interpretable dimensions.

### 3.4 Impact of Seeds on Interpretable Visual Dimensions

In Section 3.3, we observed that a classifier trained to predict the seed used to generate an image achieves over $99.9 \%$ accuracy in just a few epochs of training. However, it remains unclear what aspects of the generated images enable these seeds to be highly distinguishable. Therefore, we present an extensive empirical study on the influence of seed number on interpretable visual dimensions.

Image Quality and Human Preference Alignment. As mentioned in Section 3.2, we used 20,000 prompts from MS-COCO dense captions [24, 25]. For each prompt, we generated images using 1,024 seeds. To evaluate the image quality associated with each seed, we selected 10,000 prompts and their corresponding generated images, and then computed the FID score [15, 40] against 10,000 real MS-COCO images [24]. Surprisingly, we observed a significant difference in FID scores between the best and worst seeds. For instance, the 'golden' seed 469 for SD 2.0 achieved a low FID of 21.60, while the 'inferior' seed 696 scored 31.97 -a disparity considered significant within the community. Additionally, we assess the seeds using HPS v2 [52], a new metric trained on large-scale human preference pairs to quantify human preferences for AI-generated images. For each seed, we sampled 1,000 prompts and their corresponding images to calculate HPS v2. As shown in Figure 5, the top and bottom three seeds according to FID and HPS v2 indeed reveal that the highest-rated seeds produce images that are more visually pleasing and aligned with human preferences.

![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-05.jpg?height=789&width=1391&top_left_y=928&top_left_x=367)

Figure 5: We compare the top three best and worst seeds for SD 2.0 using FID [15] and HPS v2 [52].

Next, we determine whether these seed rankings are generalizable across a different set of 10,000 prompts for FID and 1,000 prompts for HPS v2. In Figures 6 and 7, we plot the ranked seeds for FID and HPS v2 using images from SD 2.0 and SDXL Turbo. We compare scores from the first set of prompts ("Prompt Set 1" in blue) against scores from another set of prompts ("Prompt Set 2 " in orange). We reveal a high degree of overlap between the seed patterns for quality and human preference, indicating that they are truly applicable across sets of prompts. This consistency underpins our proposed enhancements to inference strategies detailed in Sections 4.1 and 4.2

![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-05.jpg?height=198&width=1391&top_left_y=2053&top_left_x=367)

SD

![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-05.jpg?height=184&width=683&top_left_y=2063&top_left_x=1071)

Figure 6: We sort seeds by FID [15] using 10,000 images in Prompt Set 1, and then display the FID for the same seeds using another 10,000 images in Prompt Set 2. Lower FID indicates better quality.

Image Style. Given the visual variations in images generated using different seeds, we investigate whether specific seeds consistently produce unique style patterns across various prompts. Drawing on established methods in image texture and style transfer [11, 12], we compute style representations by extracting the Gram matrix - which measures pairwise cosine similarity across channels -
![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-06.jpg?height=212&width=1396&top_left_y=200&top_left_x=364)

Figure 7: We sort seeds by Human Preference Score v2 [52] using 1,000 images in Prompt Set 1, and then plot the score for the same seeds using another 1,000 images in Prompt Set 2. A higher HPS v2 score indicates the images are more aligned with human preferences.

from a pretrained deep network [44] at multiple layers. Next, we reshape the Gram matrix into a single-column vector for each image and reduce its dimensionality to two using PCA and t-SNE [1, 49]. Now, for each image, we have a compact 2D vector that captures its style. For $N=1024$ seeds and $P$ prompts, this results in a feature dimension of $N \times(2 \times P)$, combining the style representation across the generated images for each seed. We further reduce [1, 49] this aggregated style representation per seed from $N \times(2 \times P)$ to $N \times 2$. Finally, a subset of seeds are visualized in 2D in Figure 8, providing a clear visual representation of style clustering at the seed level.

![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-06.jpg?height=773&width=1401&top_left_y=828&top_left_x=362)

Figure 8: Style embedding clustering across various prompts, with each position corresponding to a unique seed. Certain seeds tend to generate grayscale images for SD 2.0 (top), while others frequently produce images with 'white sky' regions for SDXL Turbo (bottom). Please zoom-in to check.

In Figure 8 , the positions within the embedding space correspond to the same seeds across various subplots. As depicted in the first row, certain seed groups consistently generate grayscale images irrespective of the prompts used. Similarly, the second row shows that some seeds tend to produce images with prominent sky regions, while others do not. Furthermore, in Figure 9 , we observe that a select group of seeds consistently generate images with a 'border' effect near the edges, regardless of the text prompt. Collectively, these findings demonstrate that individual seeds exhibit distinct tendencies in style generation across varying prompts.

![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-06.jpg?height=388&width=1393&top_left_y=2015&top_left_x=363)

Figure 9: We observe that certain seeds produce a "border" around the image for SD 2.0. Often, these borders appear as horizontal bars at the top and bottom. Surprisingly, seed 0 occasionally generates a thick dark border on the left side of the image, while seed 50 sometimes adds a "photo frame."
![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-07.jpg?height=712&width=1400&top_left_y=197&top_left_x=362)

Figure 10: We observe that seeds produce images with unique and consistent compositions for a given object category. Each data point represents a seed. For each seed, we combine image composition features from 22 prompts with slight variations like "a pale bowl" and "a round bowl." Then, we apply dimensionality reduction [1, 49] for visualization. Left: Distribution of object centroid $(x, y)$ coordinates. Right: Distribution of object depth and size relative to the image.

Image Composition. Moving beyond style, we examine whether seeds create distinctive image compositions, such as consistent object locations and sizes. As described in Section 3.2, we generate images using 880 synthetic prompts consisting of 40 object categories paired with 22 modifiers, which includes adjectives and the empty string. For each image, we segment [8] the object and compute an image composition feature vector that contains the object's centroid $(x, y)$ coordinates, size, and depth [53] relative to the image. On the left side of Figure 10, we visualize the distribution of the object mask's centroid for the category "horse." Remarkably, the object's position stays relatively the same despite slight prompt alterations. On the right side of Figure 10, we observe an analogous pattern in the object's size and depth for the category "bowl." Overall, we observe that the location, size, and depth of generated objects are largely dependent on the specific seed used, consistent across the same object categories and irrespective of the text modifiers in the prompts.

## 4 Practical Applications

### 4.1 High-Fidelity Inference

In Section 3.4, we observed that 'golden' seeds tend to generate images with significantly better quality and human preference alignment. This inspires us to think-how much can we improve the image quality compared to random generations by simply leveraging these 'golden' seeds?

Specifically, we identified $k$ 'golden' seeds that excel in both image quality and human preference alignment. We subsequently tested these $k$ 'golden' seeds by generating images with a different set of 10,000 prompts to evaluate their performance relative to random seeds. We identified $k=65$ 'golden' seeds for SD 2.0 and $k=67$ for SDXL Turbo, where $k$ was determined by selecting seeds that ranked among the top 256 in both FID [40] and HPS v2 [52]. We propose that a sampling pool of $60+$ 'golden' seeds is sufficiently large in practical applications for a single prompt. As demonstrated in Table 1 leveraging these well-chosen seeds significantly improves the FID and HPS v2 scores for both SD 2.0 and SDXL Turbo, and on both MS-COCO [24] and the PartiPrompts benchmark [56].

Table 1: We demonstrate that well-chosen seeds can outperform random generations by comparing the visual quality and human preference alignment using our 'golden' seeds and random seeds. Additionally, our 'golden' seeds lead to improved human preference alignment on a greater variety of prompts in the PartiPrompts benchmark [56]. Mean and standard deviation based on three trials.

|  | SD 2.0 |  |  | SDXL Turbo |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\operatorname{FID}(\downarrow)$ | HPS v2 $(\uparrow)$ | Parti HPS v2 $(\uparrow)$ | $\operatorname{FID}(\downarrow)$ | HPS v2 $(\uparrow)$ | Parti HPS v2 ( $\uparrow)$ |
| I | $19.334 \pm 0.21$ | $0.250 \pm 0.000$ | $0.263 \pm 0$ | $24.859 \pm 0.123$ | $0.266 \pm 0.000$ | $=0.000$ |
| Our Golden Seeds | $\mathbf{1 9 . 0 4 5} \pm \mathbf{0 . 0 5 8}$ | $0.257 \pm \mathbf{0 . 0 0 0}$ | $\mathbf{0 . 2 6 8} \pm 0.001$ | $24.209 \pm \mathbf{0 . 1 0 8}$ | $\mathbf{0 . 2 7 2} \pm \mathbf{0 . 0 0 0}$ | $0.293 \pm 0.001$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-08.jpg?height=775&width=1393&top_left_y=203&top_left_x=363)

Figure 11: We show that simply generating images using "diverse" seeds can promote more variation in style (top) and image composition, as measured by object centroid, size, and depth (bottom).

### 4.2 Controllable Diversity in Style and Composition

A typical image generation interface presents the user with four samples per prompt. Moreover, prior methods aim to promote the diversity of generated images using primarily gradient-based methods, such as Particle Guidance [9]. In Section 3.4, our results highlight that the choice of seed has a strong influence on the stylistic and spatial attributes in the generated images. Therefore, we explore whether we can obtain more diverse images in style or composition by merely sampling 'diverse' seeds.

To select four diverse seeds per prompt, we represent each seed by a feature vector capturing its style or composition, as discussed in Section 3.4. We then employ farthest point sampling using these features. Specifically, we randomly pick the first seed $s_{0} \sim \mathcal{U}\{0,1023\}$ and iteratively select the next three seeds to maximize the distance in feature space from the already selected seeds.

$$
\begin{equation*}
s_{i}=\arg \max _{s \notin S} \min _{s^{\prime} \in S}\left\|\mathbf{f}(s)-\mathbf{f}\left(s^{\prime}\right)\right\|, \quad \text { for } i=1, \ldots, C-1 \tag{1}
\end{equation*}
$$

where $S$ is our set of diverse seeds. To evaluate whether our well-chosen seeds improve diversity over random seeds and Particle Guidance [9], we calculate the similarity between images synthesized from a different set of $P$ prompts, where $P=500$ LLaVA [25] dense captions for image style and $P=440$ synthetic prompts for image composition. In particular, we measure the pairwise cosine similarity of image features and average the similarity scores across prompts. Intuitively, a lower pairwise similarity score means higher diversity. Mathematically, the metric score is represented as:

$$
\begin{equation*}
\text { Similarity }=\frac{1}{P} \sum_{i=1}^{P}\left(\frac{1}{\binom{C}{2}} \sum_{j=1}^{C} \sum_{k=j+1}^{C} \cos \left(\mathbf{f}_{i j}, \mathbf{f}_{i k}\right)\right) \tag{2}
\end{equation*}
$$

where there are $P$ prompts and $\mathbf{f}$ denotes the feature vector representing image style or composition. We typically use $C=4$ images per prompt, but it's important to note that if no objects are detected in an image, then the image is not used to compute similarity. In Table 2, we observe that our diverse seeds outperform random seeds and Particle Guidance [9] in generating images with varying styles and compositions for SD 2.0. Interestingly, our well-chosen seeds aid in diversifying image composition for SD 2.0 but not for SDXL Turbo. We show visual comparisons in Figure 11

Table 2: We compare the style and composition diversity of images generated using our diverse seeds, Particle Guidance [9], and random seeds. More diverse generations have lower similarity scores. We show the mean and standard deviation based on three trials.

|  | SD 2.0 |  |  | SDXL Turbo |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | Style Similarity $(\downarrow)$ | Composition Similarity $(\downarrow)$ | Style Similarity $(\downarrow)$ | Composition Similarity $(\downarrow)$ |  |
| Random Seeds | $0.981 \pm 0.001$ | $0.971 \pm 0.001$ |  | $0.993 \pm 0.000$ | $0.988 \pm 0.000$ |
| Particle Guidance | $0.980 \pm 0.000$ | $0.972 \pm 0.000$ |  | $-\mathbf{0 . 9 8 4} \pm \mathbf{0 . 0 0 0}$ | $0.988 \pm 0.000$ |
| Our Diverse Seeds | $\mathbf{0 . 9 7 0} \pm \mathbf{0 . 0 0 0}$ | $\mathbf{0 . 9 6 1} \pm \mathbf{0 . 0 0 1}$ |  | $\mathbf{0 . 9 8 0}$ |  |

![](https://cdn.mathpix.com/cropped/2024_06_04_54c2a5dd4a623a7ada47g-09.jpg?height=784&width=1399&top_left_y=215&top_left_x=360)

Figure 12: We discover that certain seeds tend to insert unwanted text within the inpainting region, outlined in pink. Top: We aim to remove the object using the prompt "clear background." Bottom: We attempt to complete the object using a prompt that specifies the object category.

### 4.3 Improved Text-based Inpainting

In Sections 4.1 and 4.2 we demonstrated that carefully selecting the seed provides a straightforward, training-free approach to enhance the visual quality, human preference, and diversity of images generated by text-to-image diffusion models. But, the potential of image generation extends beyond text-to-image applications. This poses an intriguing question-can we also uncover 'golden' seeds for text-based image inpainting tasks, such as object removal and object completion?

As described in Section 3.2, we gathered 500 pairs of images and inpainting masks for the object removal and object completion applications. We employed the text prompt "clear background" for the removal case, and we used a prompt corresponding to the original object category for the completion case. We then generated images using a text-based diffusion inpainting model. We observed that some images contain unwanted text in the inpainting region that often mimics the prompt. To quantify the presence of text, we applied optical character recognition [16] and calculated the average proportion of text artifacts within the inpainting mask across all images from each seed. As illustrated in Figure 12. certain seeds are prone to inserting text in both removal and completion scenarios.

## 5 Conclusion, Limitations, and Broader Impacts

In this work, we investigated the role of "random" seeds in the reverse diffusion process, exploring their differentiability based on generated images and their impact on interpretable visual dimensions. Notably, our 1,024-way classifier trained to predict the seed number for a generated image achieved over $99.9 \%$ test accuracy in just a few epochs. Encouraged by this finding, we conducted extensive analyses and identified 'golden' seeds that consistently produce images with better visual quality and human preference alignment. Additionally, we discovered that certain seeds create 'grayscale' images, add borders, or insert text during inpainting. Our studies also show that seeds influence the image composition, affecting object position, size, and depth. Leveraging these insights, we propose downstream applications such as high-fidelity inference and diversified generation for text-to-image diffusion models by merely sampling these special seeds. Our analyses offer new perspectives on enhancing image synthesis during inference without significant computational overhead.

Moreover, it's important to recognize that we employ text-to-image diffusion models pretrained on large-scale, uncurated web data that may contain biases and errors, and our text prompts include dense captions of MS-COCO [24] images that may produce human imagery. Additionally, due to budget constraints, we primarily study the impact of 1,024 seeds out of all possible seed values.

## References

[1] Hervé Abdi and Lynne J Williams. Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4):433-459, 2010.

[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.

[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.

[4] Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, et al. Accounting for variance in machine learning benchmarks. Proceedings of Machine Learning and Systems, 3:747-769, 2021.

[5] Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, and Bo Zhao. Synartifact: Classifying and alleviating artifacts in synthetic images via vision-language model. arXiv preprint arXiv:2402.18068, 2024.

[6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart- $\alpha$ : Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.

[7] Sherry X Chen, Yaron Vaxman, Elad Ben Baruch, David Asulin, Aviad Moreshet, Kuo-Chin Lien, Misha Sra, and Pradeep Sen. Tino-edit: Timestep and noise optimization for robust diffusion-based image editing. arXiv preprint arXiv:2404.11120, 2024.

[8] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1290-1299, 2022.

[9] Gabriele Corso, Yilun Xu, Valentin De Bortoli, Regina Barzilay, and Tommi Jaakkola. Particle guidance: non-iid diverse sampling with diffusion models. arXiv preprint arXiv:2310.13102, 2023.

[10] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems, 36:16222-16239, 2023.

[11] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. In Neural Information Processing Systems, 2015.

[12] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2414-2423, 2016.

[13] Jacob Gildenblat and contributors. Pytorch library for cam methods. https://github.com/ jacobgil/pytorch-grad-cam. 2021.

[14] Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, and Di Huang. Initno: Boosting text-to-image diffusion models via initial noise optimization. arXiv preprint arXiv:2404.04650, 2024.

[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, $30,2017$.

[16] Samuel Hoffstaetter and contributors. Python tesseract. https://github.com/h/pytes seract. 2014.

[17] Keller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless and inevitable. arXiv preprint arXiv:2304.01910, 2023.

[18] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.

[19] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Shahab Kamali, Matteo Malloci, Jordi Pont-Tuset, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://storage.googleapis.com/openimages/web/index.html, 2017.

[20] Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 052012.

[21] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020 .

[22] Hyunmin Lee and Jaesik Park. Instance-wise occlusion and depth orders in natural scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21210-21221, 2022.

[23] Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. Advances in Neural Information Processing Systems, 35:12934-12949, 2022.

[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014:
13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.

[25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.

[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.

[27] Jiafeng Mao, Xueting Wang, and Kiyoharu Aizawa. Guided image synthesis via initial image editing in diffusion model. In Proceedings of the 31st ACM International Conference on Multimedia, pages $5321-5329,2023$.

[28] Jiafeng Mao, Xueting Wang, and Kiyoharu Aizawa. Semantic-driven initial image construction for guided image synthesis in diffusion model. arXiv preprint arXiv:2312.08872, 2023.

[29] Johannes Mehrer, Courtney Spoerer, Nikolaus Kriegeskorte, and Tim Kietzmann. Individual differences among deep neural network models. Nature Communications, 11, 112020.

[30] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. Localizing objectlevel shape variations with text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23051-23061, 2023.

[31] Pablo Pernias, Dominic Rampas, Mats L. Richter, Christopher J. Pal, and Marc Aubreville. Wuerstchen: An efficient architecture for large-scale text-to-image diffusion models, 2023.

[32] David Picard. Torch. manual_seed (3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision. arXiv preprint arXiv:2109.08203, 2021.

[33] Mao Po-Yuan, Shashank Kotyan, Tham Yik Foong, and Danilo Vasconcellos Vargas. Synthetic shifts to initial seed vector exposes the brittle nature of latent-based diffusion models. arXiv preprint arXiv:2312.11473, 2023.

[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.

[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022.

[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022.

[37] Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, and Gal Chechik. It is all about where you start: Text-to-image generation with seed selection. arXiv preprint arXiv:2304.14530, 2023.

[38] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023.

[39] Leonardo Scabini, Bernard De Baets, and Odemir M Bruno. Improving deep neural network random initialization through neuronal rewiring. arXiv preprint arXiv:2207.08148, 2022.

[40] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/ pytorch-fid August 2020. Version 0.3.0.

[41] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618-626, 2017.

[42] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. arXiv preprint arXiv:2306.14435, 2023.

[43] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net. arXiv preprint arXiv:2309.11497, 2023.

[44] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.

[45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.

[46] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, $15(1): 1929-1958,2014$.

[47] Michael Toker, Hadas Orgad, Mor Ventura, Dana Arad, and Yonatan Belinkov. Diffusion lens: Interpreting text encoders in text-to-image pipelines. arXiv preprint arXiv:2403.05846, 2024.

[48] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for textdriven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1921-1930, 2023.

[49] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, $9(11), 2008$.

[50] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers 2022.

[51] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1900-1910, 2023.

[52] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023.

[53] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024.

[54] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. arXiv preprint arXiv:2401.11708, 2024.

[55] Hu Yu, Hao Luo, Fan Wang, and Feng Zhao. Uncovering the text embedding in text-to-image diffusion models. arXiv preprint arXiv:2404.01154, 2024.

[56] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.
