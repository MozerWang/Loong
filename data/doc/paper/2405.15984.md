# Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models 

Simon Chi Lok U, Jie He, Pasquale Minervini \& Jeff Z. Pan*<br>School of Informatics<br>University of Edinburgh<br>\{s1967531,j.he,j.z.pan\}@ed.ac.uk,pminervi@exseed.ed.ac.uk


#### Abstract

With the emergence of large language models, such as LLaMA and OpenAI GPT-3, In-Context Learning (ICL) gained significant attention due to its effectiveness and efficiency. However, ICL is very sensitive to the choice, order, and verbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented ICL methods try to address this problem by leveraging retrievers to extract semantically related examples as demonstrations. While this approach yields more accurate results, its robustness against various types of adversarial attacks, including perturbations on test samples, demonstrations, and retrieved data, remains under-explored. Our study reveals that retrieval-augmented models can enhance robustness against test sample attacks, outperforming vanilla ICL with a $4.87 \%$ reduction in Attack Success Rate (ASR); however, they exhibit overconfidence in the demonstrations, leading to a $2 \%$ increase in ASR for demonstration attacks. Adversarial training can help improve the robustness of ICL methods to adversarial attacks; however, such a training scheme can be too costly in the context of LLMs. As an alternative, we introduce an effective trainingfree adversarial defence method, $D A R D$, which enriches the example pool with those attacked samples. We show that DARD yields improvements in performance and robustness, achieving a $15 \%$ reduction in ASR over the baselines. Code and data are released to encourage further research ${ }^{1}$.


## 1 Introduction

Large Language models (LLMs) are revolutionising the field of NLP (Brown et al., 2020; Wei et al., 2022; Touvron et al., 2023). However, this also raises concerns about their robustness and trustworthiness. Although significant efforts are put into aligning LLMs for safety purposes, recent works still show LLMs can be vulnerable to adversarial inputs and jailbreaking (Zou et al., 2023; Xu et al., 2023b; Zeng et al., 2024). As a result, it is essential to understand LLM security properties to guide the direction of LLMs that are secure and robust. In this paper, we focus on examining a common LLM inference method, namely In-Context Learning (ICL) and its variants, on how they might be susceptible to perturbations and their robustness against adversarial attacks.

ICL is a common few-shot training method for prompting LLM for downstream tasks (Brown et al., 2020). Its variants include retrieval-based ICL models, which utilise retrievers to identify similar examples as demonstrations (Wang et al., 2023c; Li et al., 2023); or $k N N-I C L$, which compute label demonstrations using a nearest neighbours algorithm; have been adopted and shown promising performance improvements when compared to vanilla ICL (Shi et al., 2022; Xu et al., 2023a). However, the robustness of these methods against adversarial attacks received less attention and remains unclear. Prior work-which we review in Section 2-shows that LLMs can still be vulnerable to different types of input perturbations, such as task instructions (Zhu et al., 2023; Sun et al., 2023) or inputs (Wang[^0]et al., 2023a); however, the robustness of ICL-based methods to such perturbations is still under-explored.

We address (in Section 4) the following question: How sensitive are vanilla and retrieval-based ICL methods to perturbations to their test samples or demonstration instances, which is particularly important given that the primary motivation of ICL is to facilitate few-shot learning without training; should any methods exhibit excessive sensitivity, this would limit their applicability. To investigate this problem, we employ commonly used perturbation attacks-namely character-level, word-level, and sentence-level attacks. Furthermore, we propose new attack methods that target the demonstration instances and datastore examples. Our findings indicate that retrieval-based ICL methods show greater robustness against test-sample attacks; however, they can be more vulnerable when their demonstrations are adversarially perturbed. Detailed ablation studies further reveal that this vulnerability to adversarial attacks persists in newer models, such as Mistral (Jiang et al., 2023) and Gemma (Gemma Team et al., 2024). Many of these attacks can be transferable, both from the same family (LLaMA-2-7B $\rightarrow$ LLaMA-2-13B, 70B) or different family (LLaMA-2-7B $\rightarrow$ Mistral-7B). We showed Mixture of Expert (MoE) models still posed a similar adversarial threat to its dense version. This encourages further investigation on enhancing model robustness.

We further explore the advantages of utilising retrieval-based models to improve robustness (in Section 5), considering that existing defence strategies often rely on an adversarial training process (Li et al., 2021; Si et al., 2021). However, this process is memory intensive in the context of LLMs. Instead, we show that retrieval-based ICL methods, combined with adversarially augmented examples, yield more robust results than no augmentation methods while maintaining their predictive accuracy on in-distribution evaluation sets. Therefore, we propose DARD, a training-free adversarial defence method which leverages adversarially perturbed samples to augment the retrieval pools, potentially improving the models' robustness. The paper is summarized in the Figure 1.

Our main contributions are summarized as follows.

- We first perform a comprehensive and in-depth evaluation on variants of ICL: vanilla, $k N N-I C L$, and retrieval-based ICL methods against Test-Sample, Demonstration and Datastore Adversarial Attacks.
- We show that Retrieval-based models can bring positive results on robustness when test samples are under adversarial attack, but it further reduces the robustness while demonstrations are perturbed. We show these attacks persist on larger models and are transferable.
- We propose DARD, a novel training-free adversarial defence method which leverages adversarially perturbed examples to improve the robustness.


## 2 Related Work

Adversarial attack and defences Adversarial attack has always been a topic in deep neural networks, where previous works showed neural networks can be vulnerable to adversarial examples that are crafted by adding imperceptible but adversarial noise on natural examples (Szegedy et al., 2013; Evtimov et al., 2017). In the real world, adversarial robustness is important for the application of a method (Tramèr et al., 2017).

Several works studied this problem and the main finding is that LMs struggle to answer correctly and factually under attacks and propose their defence via label smoothing or prompt tuning (Yang et al., 2022; Raman et al., 2023). It is important to note, however, that these experiments were mainly conducted using relatively small LMs such as BERT, RoBERTa. Recently, several works demonstrated LLM can still be vulnerable to these type of attacks, by showing the vulnerability of attacking the task instructions (PromptBench (Zhu et al., 2023)), or perform adversarial attacks on zero-shot settings and investigate the transferability of attacks from open-sourced models (i.e. Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023)) to close-sourced (i.e. GPT-3.5 and GPT-4) (DecodingTrust (Wang et al., 2023a)). However, previous study (Gu et al., 2022) has shown that the LLMs are less sensitive to prompt / instruction variation when few-shot examples are provided in context (a.k.a ICL). However, there is still a lack of systematic analysis on this case

![](https://cdn.mathpix.com/cropped/2024_06_04_f14ec8eda5865c777b95g-03.jpg?height=786&width=1309&top_left_y=276&top_left_x=408)

Figure 1: Overview of the paper. We visualize our seven adversarial attacks in (a), (c) and (d) (only 3 shots are used in the plot for display purposes). And our adversarial defence method, DARD, is showcased in the top right corner (Plot (b)).

which is the main contribution of this paper. Wang et al. (2023b) shows that attacking demonstrations can still be an effective way for adversarial attacks where LLMs are still vulnerable. Therefore, we aim to extend and evaluate the adversarial attack and defence performance on retrieval-based models.

ICL Sensitivity The existing literature on ICL analysis can be broadly divided into two streams, each focusing on different aspects. The first stream explores the influencing factors of ICL based on input perturbation, such as the order (Min et al., 2022), the formatting (Yoo et al., 2022; Lu et al., 2022) and the selection of the demonstration (Liu et al., 2022). Designing proper demonstration construction strategies (Ye et al., 2023; Li et al., 2023) could bring clear boosts to the ICL performance, while Chen et al. (2023) analyses the sensitivity of ICL via perturb on instructions and example ordering, and our work is a complement of their work on sensitivity.

This paper provides a different view of when different ICL methods fail and when retrievalbased ICL can be a better approach for robustness.

Retrieval-based LLM Retrieval-based LLM has shown great success in lifting LLM performance and faithfulness and reducing hallucination, where the idea is to gather related articles or demonstrations from a collection of data. Specifically, we are interested in whether they can be used to improve the robustness against perturbations (Lin \& Lee, 2024). Meanwhile, many papers also focus on analysing the robustness of RAG methods on irrelevant contexts (Shi et al., 2023; Asai et al., 2024), in which we adopt similar methods in our context for analysing how retrieval-based ICL being sensitive to irrelevant contexts.

## 3 Problem Formulation

### 3.1 Background

In this work, we consider several types of ICL methods, namely Vanilla ICL, and two types of retrieval-based ICL: kNN-ICL (Xu et al., 2023a) and Retrieval-based ICL (Wang et al., 2023c; Li et al., 2023). Retrieval-based ICL methods demonstrated promising perfor-
mance improvements over vanilla ICL ones; however, their robustness against adversarial perturbations remains unexplored.

Vanilla ICL is the most commonly used few-shot learning method in LLM. Given a training data set $\mathcal{T}=\left\{\left(x_{i}, y_{i}\right)\right\}$, where $x_{i} \in X$ denote the instances and $y_{i} \in Y$ represent their labels, the concatenation of the $k$-shot input-label pairs is used to generate the prompt $P$ with respect to a test instance $x_{\text {test }}$ :

$$
\begin{equation*}
P=\mathcal{I} \oplus \pi\left(x_{1}, y_{1}\right) \oplus \pi\left(x_{2}, y_{2}\right) \oplus \cdots \oplus \pi\left(x_{k}, y_{k}\right) \oplus \pi\left(x_{\text {test }}, *\right) \tag{1}
\end{equation*}
$$

where $\mathcal{I}$ denotes the task instructions, $\pi$ denotes the templates and verbaliser to turn inputs into prompts (details for each dataset are in Appendix A.2), $\oplus$ is a concatenation operator, and $|Y|$ is the number of distinct labels. The label prediction $y_{\text {test }}$ for the test instance $x_{\text {test }}$ is then computed as:

$$
\begin{equation*}
y_{\text {test }}=\arg \max _{y \in Y} p\left(y \mid P ; \theta_{\mathrm{LLM}}\right) \tag{2}
\end{equation*}
$$

$k \mathbf{N N}$-Prompting was first introduced by Shi et al. (2022) to perform zero-shot inference and extended in later work by $\mathrm{Xu}$ et al. (2023a) to support few-shot inferences, simulating the ICL. Throughout the paper, we follow the implementation of Xu et al. (2023a) and call it $k \mathrm{NN}$-ICL. The idea is to map each training instance into a latent representation using a LLM and cache the last token distribution into a datastore. At inference time, the prediction is based on the interpolation between the LLM prediction and the $k$-nearest neighbours label distribution in the datastore. Specifically, for each training instance $x_{i}$, Xu et al. (2023a) concatenate it into prompt $P_{i}$ together with $k$ in-context examples, where $k=|Y|$ :

$$
\begin{equation*}
P_{i}=\pi\left(x_{1}, y_{1}\right) \oplus \ldots \pi\left(x_{|Y|}, y_{|Y|}\right) \oplus \pi\left(x_{i}, *\right) \tag{3}
\end{equation*}
$$

By querying the LLM using $P_{i}$, we obtain a distribution over tokens $p\left(v \mid P_{i}, \theta\right)$. Rather than mapping it back to label space $Y$, the distribution is cached as the key representation for the instance $x_{i}$ :

$$
\begin{equation*}
\mathbf{k}_{\mathbf{i}}=p\left(v \mid P_{i} ; \theta_{\mathrm{LLM}}\right) \tag{4}
\end{equation*}
$$

which is mapped to the label $y_{i}$. The entire datastore thus consists of $\left\{\left(\mathbf{k}_{\mathbf{i}}, y_{i}\right)\right\}$ pairs, where $K=\left\{\mathbf{k}_{\mathbf{i}}\right\}_{i}$ denotes the set of keys. At inference time, we construct the same prompt and obtain the distribution $P_{\text {test }}=p\left(v \mid P_{\text {test }} ; \theta_{\text {LLM }}\right)$. We then match the distribution against cached $K$ in the datastore, where standard $K L$-divergence is used to measure the distance:

$$
\begin{equation*}
D_{\mathrm{KL}}\left(P_{\text {test }}|| k_{i}\right)=\sum_{v} p\left(v \mid P_{\text {test }} ; \theta_{\mathrm{LLM}}\right) \log \frac{p\left(v \mid P_{\text {test }} ; \theta_{\mathrm{LLM}}\right)}{p\left(v \mid P_{i} ; \theta_{\mathrm{LLM}}\right)} \tag{5}
\end{equation*}
$$

The predictions are then calculated by interpolating the LLM output and its $m$ nearest neighbours distribution, where $\mathrm{NN}_{m}(*, K)$ denotes the set of $m$ nearest neighbours in $K$ :

$$
\begin{equation*}
\hat{y}_{\text {pred }}=\arg \max _{y \in Y}\left[(1-\alpha) \cdot p\left(y \mid P_{\text {test }} ; \theta_{\mathrm{LLM}}\right)+\alpha \cdot p \sum_{i \in \mathrm{NN}_{m}\left(P_{\text {test }}, K\right)} \mathbb{I}\left(y_{i}^{a}=y\right)\right] \tag{6}
\end{equation*}
$$

Following Xu et al. (2023a), in this work, we use $\alpha=0.2$ and $m=k / 2$.

R-ICL Retrieval-augmented ICL (Lin et al., 2022) was proposed to mitigate the sensitivity of ICL methods to the choice and order of the in-context examples: rather than randomly sampling $\left(x_{i}, y_{i}\right)$, R-ICL use a retriever to identify the most similar examples to the test instance $x_{\text {test }}$ from the training set $\mathcal{T}$. In this paper, we follow Li et al. (2023) and consider the following retrievers: BM25 (Robertson \& Zaragoza, 2009), SBERT (Reimers \& Gurevych, 2019) and Instructor (Su et al., 2022); more details are available in Appendix A.1.

### 3.2 Attack Methods

This paper aims to study how perturbations can affect the performance of different ICL methods through adversarial attacks. We classify our 7 attacks into 3 categories: Test Sample Attack, Demonstration Attack, and DataStore Attack.

Test Sample Attacks We focus on the three different types of attack: typo-based perturbations - TextBugger (TB) (Li et al., 2018); embedding-similarity-based perturbations TextFooler (TF) (Jin et al., 2019); and context-aware perturbations - BERT-Attack (BA) (Li et al., 2020).

Demonstration Attacks For demonstration attacks, we refer to adversarial attacks where the training demonstrations are adversarially perturbed. We consider this type of attack to investigate the sensitivity of LLMs to adversarial perturbations to the training demonstrations and their influence on the generalisation properties of the models.

Datastore Attacks One of our objectives is to analyse the sensitivity of retrieval-based ICL methods to adversarial perturbations. Therefore, we consider Irrelevant Context Attacks (Irr.), which work by contaminating the demonstration pools with irrelevant contexts. Our Irrelevant Context Attacks extended Min et al. (2022) - we replace 50\% of the original demonstrations in the training data with Out-of-Distrubtion (OoD) examples. Further details and analyses are available in Appendix A.3.

## 4 Experiments

### 4.1 Settings

Datasets To assess the robustness, we selected six sentiment and multiple-choice text classification datasets: SST2 (Socher et al., 2013), RTE (Dagan et al., 2005), CR (Hu \& Liu, 2004), MR (Pang \& Lee, 2005), MNLI-matched (Williams et al., 2018), and TREC (Voorhees \& Tice, 2000). We provide further details, including prompt templates and dataset statistics, in Appendix A.2. Unless explicitly mentioned, LLaMA-2-7B was mainly used as the base model for our experiments (Touvron et al., 2023). For experiments involving randomness (i.e., Vanilla ICL and $k \mathbf{N N}$-ICL), we conducted trials with three different random seeds to ensure the reliability of our findings.

Evaluation To evaluate and compare the vulnerabilities across different types of attacks, in line with Wang et al. (2023a); Zhu et al. (2023), we assess the model's performance in a benign (attack-free) context (Clean) and calculate the Attack Success Rate (ASR). The ASR is defined as $\frac{\text { Clean Accuracy-Attack Accuracy }}{\text { Clean Accuracy }}$, where Attack Accuracy denotes the model's accuracy under attack.

### 4.2 Results

The 8 -shot attack results are presented in Table 1 with 8 -shot attacks, and detailed results are available in Appendix B.1.

Overview: Retrieval-based models exhibit higher Clean accuracy: $3.27 \%$ higher than vanilla ICL with the best performed SBERT retriever; similar finding with Rubin et al. (2021) that related demonstrations enhance the performance. For adversarial attacks, our proposed adversarial attack methods, Swap-Labels and Swap-Labels (Fix), are effective techniques for manipulating LLM predictions. On the other hand, irrelevant context attacks have a negligible effect on performance. We summarise two major findings about the adversarial robustness across different ICL methods.

Finding 1: Retrieval-based models are more robust against test-sample attacks; however, it could fall short under demonstration attack.

As shown in Table 1, all three R-ICL methods outperform vanilla ICL and $\mathrm{kNN}$-ICL among all Text Sample Attacks (TB, TF and BA), with a drop in ASR by $4.87 \%$ and $2.47 \%$, respectively. ${ }^{2}$ In Figure 2, we observe an increased gap in the ASR between ICL and R-ICL when the number of shots goes up. Therefore, we believe in-context learning benefits from[^1]

| Tasks | Method | Clean | $\mathbf{T B}$ | $\mathbf{T F}^{\downarrow}$ | BA | AdvICL ${ }^{\downarrow}$ | S-L | S-L (Fix) | Irr. $\downarrow$ | Avg |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| SST-2 | ICL | 92.66 | 37.29 | 43.69 | 68.40 | 33.64 | 92.33 | 62.99 | 1.69 | 48.58 |
|  | $k N N-I C L$ | 92.01 | 41.01 | 45.45 | 61.66 | 79.05 | 67.28 | 66.00 | 0.0 | 51.49 |
|  | $\mathrm{R}_{\mathrm{BM} 25}-\mathrm{ICL}$ | 94.61 | 43.39 | 45.94 | 57.33 | 25.63 | 99.88 | 63.88 | 0.85 | 46.7 |
|  | $\mathrm{R}_{\mathrm{SBERT}}-\mathrm{ICL}$ | 95.18 | 33.01 | 33.01 | 56.26 | 27.21 | 99.64 | 66.84 | 6.09 | $\overline{46.01}$ |
|  | $\mathrm{R}_{\text {Instructor- }} \mathrm{ICL}$ | $\underline{94.84}$ | 38.33 | 32.62 | 58.65 | 28.22 | 99.54 | 69.32 | 6.73 | 47.63 |
| RTE | ICL | 73.04 | 82.54 | 75.62 | 97.86 | 14.32 | 91.59 | 58.31 | 11.04 | 61.61 |
|  | $k N N-I C L$ | $\overline{70.52}$ | 90.61 | 80.37 | 95.43 | 96.13 | 63.66 | 63.15 | 6.83 | $\overline{70.88}$ |
|  | $\mathrm{R}_{\mathrm{BM} 25}-\mathrm{ICL}$ | 71.48 | 68.69 | 63.14 | 90.4 | 18.77 | 92.42 | 60.60 | 15.15 | 58.45 |
|  | $\mathrm{R}_{\mathrm{SBERT}}-\mathrm{ICL}$ | 72.20 | 83.50 | 83.01 | 98.12 | 27.49 | 95.50 | 64.50 | 15.50 | 66.80 |
|  | $\mathrm{R}_{\text {Instructor- }} \mathrm{ICL}$ | 73.29 | 78.33 | 72.41 | 96.67 | 28.08 | 90.64 | 66.00 | 10.36 | 63.21 |
| MR | ICL | 92.67 | 36.41 | 46.72 | 67.52 | 22.86 | 94.49 | 75.43 | 0.15 | 49.08 |
|  | $k \mathrm{NN}-\mathrm{ICL}$ | 91.77 | 39.60 | 47.18 | 65.95 | 78.99 | 71.81 | 72.90 | 2.78 | 54.17 |
|  | $\mathrm{R}_{\mathrm{BM} 25}-\mathrm{ICL}$ | 92.50 | 37.95 | 45.84 | 65.95 | 25.10 | 99.68 | 63.89 | 1.51 | $\underline{48.56}$ |
|  | $\mathrm{R}_{\mathrm{SBERT}}-\mathrm{ICL}$ | 92.40 | 38.53 | 46.10 | 64.52 | 28.83 | 99.78 | 60.06 | 0.0 | $\overline{48.26}$ |
|  | $\mathrm{R}_{\text {Instructor- }} \mathrm{ICL}$ | $\underline{92.6}$ | 37.47 | 47.30 | 66.22 | 26.26 | 98.06 | 65.33 | 0.43 | 48.72 |
| CR | ICL | 91.31 | 35.54 | 52.43 | 71.16 | 22.8 | 99.90 | 91.87 | 1.22 | 51.56 |
|  | $k N N-I C L$ | 91.87 | 19.87 | 39.13 | 58.78 | 74.07 | 67.33 | 57.74 | 6.03 | 44.13 |
|  | $\mathrm{R}_{\mathrm{BM} 25}-\mathrm{ICL}$ | 93.09 | 29.14 | 47.43 | 65.72 | 33.20 | 100.00 | 56.29 | 7.30 | 48.44 |
|  | $\mathrm{R}_{\mathrm{SBERT}}-\mathrm{ICL}$ | $\underline{93.62}$ | 31.25 | 46.30 | 71.59 | 33.81 | 100.00 | 62.32 | 4.76 | 50.00 |
|  | $\mathrm{R}_{\text {Instructor- }} \mathrm{ICL}$ | 93.88 | 34.55 | 39.38 | 53.82 | 35.81 | 100.00 | 64.88 | 4.02 | $\underline{47.49}$ |
| MNLI-mm | ICL | 53.63 | 68.49 | 57.92 | 40.85 | 32.48 | 88.81 | 43.84 | 1.73 | 47.73 |
|  | $k \mathrm{NN}-\mathrm{ICL}$ | 55.89 | 65.61 | 52.10 | 51.30 | 51.30 | 62.39 | 56.00 | 10.14 | 49.83 |
|  | $\mathrm{R}_{\mathrm{BM} 25}-\mathrm{ICL}$ | $\underline{57.3}$ | 68.41 | 59.16 | 40.98 | 38.45 | 95.99 | 63.89 | 0.35 | $\overline{52.46}$ |
|  | $\mathrm{R}_{\mathrm{SBERT}}-\mathrm{ICL}$ | $\frac{57.8}{5}$ | 69.03 | 55.07 | 43.27 | 37.01 | 99.31 | 63.55 | 2.77 | 52.86 |
|  | $\mathrm{R}_{\text {Instructor }}-\mathrm{ICL}$ | 54.30 | 66.48 | 51.93 | 36.06 | 37.83 | 100.00 | 69.67 | 2.58 | 52.08 |
| TREC | ICL | 76.07 | 61.09 | 57.50 | 60.65 | 18.69 | 65.56 | 40.58 | 8.19 | 44.61 |
|  | kNN-ICL | 77.87 | 59.68 | 54.54 | 50.02 | 83.65 | 58.74 | 52.61 | 10.88 | 52.87 |
|  | $\mathrm{R}_{\mathrm{BM} 25}-\mathrm{ICL}$ | 79.20 | 50.00 | 50.76 | 57.07 | 28.91 | 55.56 | 9.60 | 3.79 | 36.53 |
|  | $\mathrm{R}_{\mathrm{SBERT}}-\mathrm{ICL}$ | 87.8 | 43.74 | 45.56 | 47.15 | 34.61 | 53.08 | 10.02 | 9.11 | 34.75 |
|  | $\mathrm{R}_{\text {Instructor- }} \mathrm{ICL}$ | $\underline{86.5}$ | 45.43 | 70.87 | 47.51 | 31.31 | 44.97 | 3.35 | 7.28 | $\underline{35.82}$ |
| Avg | ICL | 79.90 | 53.56 | 55.65 | 67.74 | 24.13 | 88.78 | 59.84 | 4.00 | 50.53 |
|  | $\mathrm{kNN}$. | 79.98 | 52.73 | 53.13 | 63.86 | 77.20 | 65.2 | 61.40 | 6.11 | 54.18 |
|  | $\mathrm{R}_{\mathrm{BM} 25}-\mathrm{ICL}$ | 81.36 | 49.6 | $\underline{52.04}$ | 62.91 | $\underline{27.68}$ | 90.59 | 53.02 | 4.82 | 48.52 |
|  | $\mathrm{R}_{\mathrm{SBERT}}-\mathrm{ICL}$ | 83.17 | $\underline{49.84}$ | $\overline{51.51}$ | $\overline{63.48}$ | $\overline{31.49}$ | 91.22 | 54.55 | 6.37 | 49.78 |
|  | $\mathrm{R}_{\text {Instructor- }} \mathrm{ICL}$ | $\underline{82.57}$ | $\overline{50.10}$ | 52.42 | 59.82 | 31.25 | 88.87 | 56.42 | 5.23 | $\underline{49.16}$ |

Table 1: Adversarial Attack results with 8-shot ICL. Clean refers to the Benign accuracy, while all other columns refer to the Attack Success Rate (\%) under corresponding Adversarial Attacks. The last column Avg refers to the mean ASR across 7 attacks. The higher the ASR, the lower the robustness of models against the attack.

![](https://cdn.mathpix.com/cropped/2024_06_04_f14ec8eda5865c777b95g-06.jpg?height=450&width=1260&top_left_y=1694&top_left_x=430)

Figure 2: Attack Success Rate (ASR) across shots among different ICL methods. We aggregated the results for ICL and $k \mathrm{NN}-\mathrm{ICL}$ across 3 seeds and R-ICL results across 3 retrievers.

drawing semantically related demonstrations against adversarial perturbations on both performance and robustness. Recall that these textual attacks are designed to deceive LLMs by replacing words with their synonyms or introducing typos in the input sentences. This strategy can be viewed as exploiting LLM's parametric knowledge (Szegedy et al., 2013; Goyal et al., 2022). In this case, R-ICL can provide supported demonstrations by retrieving related

![](https://cdn.mathpix.com/cropped/2024_06_04_f14ec8eda5865c777b95g-07.jpg?height=391&width=1114&top_left_y=276&top_left_x=495)

Figure 3: Attack Success Rate (\%) for adversarial attacks across various models, based on experiments conducted on the RTE dataset with 8 -shot demonstrations. The results are based on R-ICL and involve the aggregation of results from retrievers: BM25, SBERT, and Instructor.

examples to enhance the contextual knowledge to the LLM, therefore reducing the noise of the LLM prediction.

On the other side, we see an increase in ASR for Retrieval-based ICL when demonstrations are under attack, demonstrating a $1 \%-6 \%$ increase in ASR compared with vanilla ICL. This demonstrates the deficit of retrieval-based models that can sometimes overly rely on the retrieved demonstrations and are more vulnerable when demonstrations are perturbed.

Finding 2: $\quad k N N-I C L$ is highly sensitive to adversarial perturbations on test samples and demonstration.

In Table 1, $k$ NN-ICL shows the highest ASR in Demonstration Attack, specifically showing the highest vulnerability to AdvICL, $40 \%-50 \%$ higher than ICL and R-ICL. Recall that for $k$ NN-ICL, demonstrations and test samples are mapped into the hidden space to compute the label distribution with nearest neighbours. Any perturbations on the demonstration might notably affect their position in the hidden space and alter the label distribution significantly. Also, in Figure 2, kNN-ICL exhibits a consistently ascending trend in ASR as the number of examples increases, with $15 \%$ higher than the other methods. This further illustrates that incorporating nearest neighbours algorithms inevitably includes more noise, thereby increasing vulnerability.

Overall, our experiment results show that few-shot methods in LLM could still be vulnerable to adversarial perturbations, leading to a $50 \%$ drop in performance on average. Pairing with previous works on jailbreaking leads to the need for a continuous effort to improve the robustness of LLMs. We include quantitative results in the Appendix B.2.

### 4.3 Ablation Study

In this section, we extend our experiments to four other models: LLaMA-2-13B, larger version of 7B model; Vicuna-7B (Chiang et al., 2023), instruction-tuned from LLaMA-27B; Mistral-7B (Jiang et al., 2023) and Gemma-7B (Gemma Team et al., 2024), aiming to provide a view for models from different families to prevent potential bias posed within the pretraining data for LLaMA models. These results allowing us to analyze the vulnerability of modern models with better coverage. We conducted the experiments on RTE datasets with 8 -shot demonstrations. Even larger models such as LLaMA-2-70B or Mixtral-8×7B (Jiang et al., 2024) are ignored due to attacking them is expensive and slow. Instead, we leave them in the following section for attack transferability (\$4.4).

The results are shown in Figure 3 for R-ICL (corresponding results for ICL are presented in Figure 5). It can be observed that most adversarial attacks remain effective across all models, including the Swap-Labels attack we proposed. Thus, it still posed a major threat on modern LLMs. Specifically, Vicuna-7B tends to be more sensitive when attack on text sample (TextBugger) or when demonstrations are under attack (AdvICL); while Gemma7B-most recently released model-shows a higher vulnerability when the context or label in demonstrations is adversarial perturbed, compared to other base models. Notably, the

![](https://cdn.mathpix.com/cropped/2024_06_04_f14ec8eda5865c777b95g-08.jpg?height=439&width=1244&top_left_y=282&top_left_x=430)

Figure 4: Analysis of the attack transferability for Retrieval-ICL on larger variant models within the same family: (Left) the LLaMA family; (Mid) the Mistral family; and (Right) across models from different families. The models' orders are sorted by their parameter sizes and release date. The models highlighted in bold are the models being attacked.

irrelevant context attacks become less vulnerable to more recent release models, especially Vicuna, which shows nearly no effect after being tuned with instructions, showing improved capability in the latest LLMs for filtering OoD contexts (Shi et al., 2023).

### 4.4 Attack Transferability

In this section, we would like to answer two questions: Are these attacks transferable across different models, and can larger models mitigate the vulnerability to these attacks? To this end, we conducted two sets of experiments: (1) transfer attacks within the same model family from small models to their large variants, by testing on LLaMA-2 and Mistral famililes; (2) whether attacks still be effective across models from different family, by transferring attacks to Vicuna-7B, Mistral-7B and Gemma-7B, following the same settings as Section 4.3. Our experiments focused on the RTE dataset, chosen for its pronounced drop in robustness when subjected to these attacks. Due to memory constraints, both Mixtral- $8 \times 7 \mathrm{~B}$ and LLaMA2-70B were run with 8-bit quantization (Frantar et al., 2022), while the remaining models were in full precision. Hong et al. (2024) tested on AdvGLUE++ and showed that quantized LLMs have negligible drops in adversarial robustness to their source model.

The results of the transfer attack with R-ICL are shown in Figure 4 (corresponding results for ICL are presented in Figure 6). Most attacks are transferable and achieve an ASR of $15 \%-40 \%$ on similar scale models; except for AdvICL, which cannot transfer between models. Also, Swap-Labels attacks are shown to be less effective when targeting instruction-tuned models (e.g. Vicuna and Mistral-7B-Instruct), pointing to the instruction-tuning process leading models to be less sensitive to its demonstration (Sun et al., 2023). Specifically, the results shown in Figure 4(L) confirm that larger models tend to be more robust, as indicated by a reduction of $6 \%$ on average in ASR between the 13B and 70B models. However, the MoE variants of Mistral-7B-Instruct exhibit a similar or even higher ASR compared to its dense version, as illustrated in Figure 4(M). This finding deviates from previous work by Puigcerver et al. (2022), which investigates adversarial robustness in MoE, showing Vision MoE models are less vulnerable to adversarial attacks. We believe this discrepancy might be subject to the difference between on how MoE is applied in NLP (token) and Vision (image-patch) models. Remember that in the training of MoE models, experts are specialised to handle different tasks (Xue et al., 2024); perturbations in the input texts might change the router output and be routed to experts that are less capable for the targeted tasks, leading to more vulnerability and changes in the output. We hope our findings encourage future work to study the robustness of MoE models, both from the training and router perspectives, given their widespread use nowadays.

| Defence | Shots | Clean ${ }^{\uparrow}$ | TB | $\mathbf{T F} \downarrow$ | $\mathbf{B A}^{\downarrow}$ | Avg |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $\hookrightarrow$ No Defence <br> Augmentation | 8 | 71.48 | 68.69 | 63.14 | 90.4 | 74.08 |
| $\hookrightarrow$ Random Addition | 8 | 72.01 | 67.75 | 67.76 | 91.6 | 75.70 |
| $\hookrightarrow$ Random Deletion <br> DARD | 8 | 69.18 | 65.72 | 62.78 | 85.96 | 71.49 |
| $\hookrightarrow$ R-ICL (BM25) | 8 | 74.39 | 58.30 | 51.43 | 79.32 | $63.01 \quad$ |
| $\hookrightarrow$ R-ICL (SBERT) | 8 | 72.16 | 58.53 | 44.46 | 72.60 | 58.53 |
| $\hookrightarrow$ R-ICL (Instructor) | 8 | 71.26 | 69.00 | 61.81 | 84.31 | 71.70 |
| Adversarial Training | 8 | 77.22 | 62.22 | 41.96 | $\underline{77.22}$ | 60.47 |

Table 2: DARD for adversarial defences. We show the Clean Accuracy and the ASR. For Random Addition and Deletion baselines, BM25 is the retriever as it performs the best.

## 5 Defence Methods: DARD

In this section, we propose an easy yet effective method to enhance the robustness of retrieval-based models against adversarial attacks on test samples. Specifically, we introduce an adversarial defence method named Demonstration Augmentation Retrieval Defences (DARD). This approach performs adversarial augmentation on the training data and mixes them into the retrieval bases for R-ICL. The initiative is that adversarial training in LLM can be costly in memory, and Xie et al. (2021); Dai et al. (2022) showed that in-context learning can be seen as an implicit finetuning method.

### 5.1 Methodology

The methods begin by adversarially perturbing the training examples, similar to previous approaches. Our experiments use the RTE dataset, focusing on an 8-shot setting for this section. For each of the 872 test samples in RTE, the retriever selects the 8 -shot most similar examples for demonstrations. After deduplication, we are left with 7,293 distinct examples. These are concatenated with one-shot examples, and adversarial attacks (TF, TB, and BA) are performed on the extracted training samples. Only the successful examples are retained, resulting in 16,206 perturbed examples. In the adversarial training paradigm, such perturbed examples would typically be used to fine-tune models. However, our approach, $D A R D$, reintegrates these examples into the training demonstration for retrieval during the inference stage. It is important to note that we impose a constraint whereby each example and its perturbed examples can be retrieved no more than once in same prompt for ICL. This is because we observed that most retrieved samples were variants of the same examples with perturbations without this constraint, which reduced both performance and robustness.

### 5.2 Results

For baselines, we compared with No Defence, Random Augmentation (Addition, Deletion) inspired by Robey et al. (2023), which are smoothing methods used to defend against attacks. We also compared with adversarially trained models following vanilla ICL setup. We show the results in Table 2 (detailed results are in Table 18). Although the adversarially trained models show a higher clean accuracy as it's directly fine-tuned on RTE data, our methods with SBERT outperform it by $2 \%$ reduction in ASR; both BM25 and SBERT variants of DARD have significant improvement on robustness against No Defence baseline.

## 6 Summary

While ICL sensitivity is a widely known issue, our paper provides an alternative view of the problem through adversarial attacks. In summary, our paper conducts a comprehensive analysis of the effects of perturbations on modern few-shot methods for LLMs inference and also proposes new adversarial attacks targeted at ICL models. Besides their perfor-
mance, the findings show the other side regarding retrieval-based models related to their robustness. Furthermore, we also demonstrate the potential of retrieval-based models in data augmentation strategies for adversarial defense.

There are many open questions for future work.One of the most profound findings is that our study on attack transferability demonstrates that larger models generally exhibit better robustness; however, this does not necessarily hold true for Mixture of Experts (MoE) models. Given the recent surge in their popularity, it is crucial to investigate the conditions under which MoE models may fail and to understand the reasons behind their vulnerabilities.

## References

Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hanna Hajishirzi, and Wen tau Yih. Reliable, adaptable, and attributable language models with retrieval. 2024. URL https://api.semanticscholar.org/CorpusID:268248911.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020. URL https://api. semanticscholar. org/CorpusID:218971783.

Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, and He He. On the relation between sensitivity and accuracy in in-context learning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 155167, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.findings-emnlp.12. URL https://aclanthology.org/2023.findings-emnlp. 12.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, 2005. URL https: //api.semanticscholar.org/CorpusID:8587959.

Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. 2022. URL https://api.semanticscholar.org/CorpusID:258686544.

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. ArXiv, abs/2307.08691, 2023. URL https://api.semanticscholar.org/CorpusID: 259936734

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. URL https://api . semanticscholar. org/CorpusID:52967399.

I. Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Xiaodong Song. Robust physical-world attacks on deep learning models. arXiv: Cryptography and Security, 2017. URL https://api.semanticscholar.org/ CorpusID: 4730292 .

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate posttraining quantization for generative pre-trained transformers. ArXiv, abs/2210.17323, 2022. URL https://api.semanticscholar.org/CorpusID:253237200.

Thomas Mesnard Gemma Team, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, and et al. Gemma. 2024. doi: 10.34740/KAGGLE/M/3301. URL https: //www.kaggle.com/m/3301.

Shreyansh Goyal, Sumanth Doddapaneni, Mitesh M.Khapra, and Balaraman Ravindran. A survey of adversarial defences and robustness in nlp. 2022. URL https: //api.semanticscholar.org/CorpusID:247447518.

Jiasheng Gu, Hanzi Xu, Liang Nie, and Wenpeng Yin. Robustness of learning from task instructions. In Annual Meeting of the Association for Computational Linguistics, 2022. URL https://api.semanticscholar.org/CorpusID:254366640.

Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. news-please: A generic news crawler and extractor. In Proceedings of the 15th International Symposium of Information Science, pp. 218-223, March 2017. doi: 10.5281/ zenodo.4120316.

Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian Bartoldson, Ajay Jaiswal, Kaidi Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, and Bo Li. Decoding compressed trust: Scrutinizing the trustworthiness of efficient llms under compression. 2024. URL https: //api.semanticscholar.org/CorpusID:268680727.

Minqing Hu and Bing Liu. Mining and summarizing customer reviews. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, 2004. URL https://api.semanticscholar.org/CorpusID:207155218.

Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L'elio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts. ArXiv, abs/2401.04088, 2024. URL https://api.semanticscholar.org/CorpusID:266844877.

Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023. URL https://api.semanticscholar.org/ CorpusID:263830494.

Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In AAAI Conference on Artificial Intelligence, 2019. URL https://api .semanticscholar.org/CorpusID:202539059.

Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8018-8025, Apr. 2020. doi: 10.1609/ aaai.v34i05.6311. URL https://ojs.aaai.org/index.php/AAAI/article/view/6311.

Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text against real-world applications. ArXiv, abs/1812.05271, 2018. URL https://api . semanticscholar.org/CorpusID:54815878.

Linyang Li, Ruotian Ma, Qipeng Guo, X. Xue, and Xipeng Qiu. Bert-attack: Adversarial attack against bert using bert. ArXiv, abs/2004.09984, 2020. URL https://api. semanticscholar.org/CorpusID:216036179.

Xiaonan Li, Kai Lv, Hang Yan, Tianya Lin, Wei Zhu, Yuan Ni, Guo Tong Xie, Xiaoling Wang, and Xipeng Qiu. Unified demonstration retriever for in-context learning. ArXiv, abs/2305.04320, 2023. URL https://api.semanticscholar.org/CorpusID:258557751.

Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang, Kai-Wei Chang, and Cho-Jui Hsieh. Searching for an effective defender: Benchmarking defense against adversarial word substitution. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3137-3147, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.251. URL https://aclanthology.org/2021.emnlp-main. 251.

Bill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, and Xiang Ren. Unsupervised cross-task generalization via retrieval augmentation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 22003-22017. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/8a0d3ae989a382ce6e50312bc35bf7e1-Paper-Conference.pdf.

Ziqian Lin and Kangwook Lee. Dual operating modes of in-context learning. 2024. URL https://api.semanticscholar.org/CorpusID:268063278.

Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Eneko Agirre, Marianna Apidianaki, and Ivan Vulić (eds.), Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022.deelio-1.10.

Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086-8098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long. 556.

Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Zhao. Dr.icl: Demonstration-retrieved in-context learning. ArXiv, abs/2305.14128, 2023. URL https://api.semanticscholar.org/CorpusID:258841276.

Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1104811064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.759. URL https://aclanthology .org/ 2022.emnlp-main. 759 .

John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 119-126, 2020.

Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Kevin Knight, Hwee Tou Ng, and Kemal Oflazer (eds.), Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), pp. 115-124, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. doi: 10.3115/1219840.1219855. URL https://aclanthology .org/P05-1015.

Joan Puigcerver, Rodolphe Jenatton, Carlos Riquelme, Pranjal Awasthi, and Srinadh Bhojanapalli. On the adversarial robustness of mixture of experts. ArXiv, abs/2210.10253, 2022. URL https://api.semanticscholar.org/CorpusID:252992497.

Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary Chase Lipton, and Danish Pruthi. Model-tuning via prompts makes nlp models adversarially robust. ArXiv, abs/2303.07320, 2023. URL https://api.semanticscholar.org/CorpusID:257495746.

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ $\mathrm{abs} / 1908.10084$.

Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3:333-389, 2009. URL https://api . semanticscholar. org/CorpusID:207178704.

Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pappas. Smoothllm: Defending large language models against jailbreaking attacks. ArXiv, abs/2310.03684, 2023. URL https://api.semanticscholar.org/CorpusID:263671542.

Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. ArXiv, abs/2112.08633, 2021. URL https://api.semanticscholar. org/CorpusID:245218561.

Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 31210-31227. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/shi23a.html.

Weijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. Nearest neighbor zero-shot inference. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3254-3265, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.214. URL https: //aclanthology.org/2022.emnlp-main. 214.

Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, and Maosong Sun. Better robustness by more coverage: Adversarial and mixup data augmentation for robust finetuning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 15691576, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.findings-acl.137. URL https://aclanthology.org/2021.findings-acl.137.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew $\mathrm{Ng}$, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170.

Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instructionfinetuned text embeddings. 2022. URL https://arxiv.org/abs/2212.09741.

Jiu Sun, Chantal Shaib, and Byron Wallace. Evaluating the zero-shot robustness of instruction-tuned language models. ArXiv, abs/2306.11270, 2023. URL https://api . semanticscholar.org/CorpusID:259203613.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, D. Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. URL https://api.semanticscholar.org/CorpusID:604334.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M.

Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. URL https://api .semanticscholar.org/CorpusID: 259950998 .

Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick Mcdaniel. Ensemble adversarial training: Attacks and defenses. ArXiv, abs/1705.07204, 2017. URL https://api.semanticscholar.org/CorpusID:21946795.

Ellen M. Voorhees and Dawn M. Tice. The TREC-8 question answering track. In M. Gavrilidou, G. Carayannis, S. Markantonatou, S. Piperidis, and G. Stainhauer (eds.), Proceedings of the Second International Conference on Language Resources and Evaluation (LREC'00), Athens, Greece, May 2000. European Language Resources Association (ELRA). URL http://www.1rec-conf.org/proceedings/lrec2000/pdf/26.pdf.

Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zi-Han Lin, Yuk-Kit Cheng, Sanmi Koyejo, Dawn Xiaodong Song, and Bo Li. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. ArXiv, abs/2306.11698, 2023a. URL https://api.semanticscholar.org/ CorpusID:259202782.

Jiong Wang, Zi yang Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. Adversarial demonstration attacks on large language models. ArXiv, abs/2305.14950, 2023b. URL https://api.semanticscholar.org/CorpusID:258865399.

Liang Wang, Nan Yang, and Furu Wei. Learning to retrieve in-context examples for large language models. ArXiv, abs/2307.07164, 2023c. URL https://api.semanticscholar. org/CorpusID: 259924840 .

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=gEZrGCozdqR.

Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112-1122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101.

Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. ArXiv, abs/2111.02080, 2021. URL https://api.semanticscholar.org/CorpusID:241035330.

Benfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu, Qiaoqiao She, and Yongdong Zhang. knn prompting: Beyond-context learning with calibration-free nearest neighbor inference. ArXiv, abs/2303.13824, 2023a. URL https://api .semanticscholar.org/CorpusID: 257756989 .

Nan Xu, Fei Wang, Ben Zhou, Bangzheng Li, Chaowei Xiao, and Muhao Chen. Cognitive overload: Jailbreaking large language models with overloaded logical thinking. ArXiv, abs/2311.09827, 2023b. URL https://api.semanticscholar.org/CorpusID:265221395.

Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: An early effort on open mixture-of-experts language models. arXiv preprint arXiv:2402.01739, 2024.

Yahan Yang, Soham Dan, Dan Roth, and Insup Lee. In and out-of-domain text adversarial robustness via label smoothing. ArXiv, abs/2212.10258, 2022. URL https: //api.semanticscholar.org/CorpusID:254877265.

Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context learning. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:256826793.

Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2422-2437, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.155. URL https://aclanthology.org/2022.emnlp-main. 155.

Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. ArXiv, abs/2401.06373, 2024. URL https://api. semanticscholar.org/ CorpusID:266977395.

Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Weirong Ye, Neil Zhenqiang Gong, Yue Zhang, and Xingxu Xie. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. ArXiv, abs/2306.04528, 2023. URL https://api.semanticscholar.org/CorpusID:259095572.

Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. ArXiv, abs/2307.15043, 2023. URL https://api.semanticscholar.org/CorpusID:260202961.
