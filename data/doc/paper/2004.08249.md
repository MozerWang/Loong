# Understanding the Difficulty of Training Transformers 

\author{
Liyuan Liu ${ }^{\dagger \ddagger}$ Xiaodong Liu ${ }^{\ddagger}$ Jianfeng Gao ${ }^{\ddagger}$ Weizhu Chen ${ }^{\S}$ Jiawei Han ${ }^{\dagger}$ <br> \{ll2, hanj\}@illinois.edu, \{xiaodl, jfgao, wzchen\}@microsoft.com <br> ${ }^{\dagger}$ University of Illinois at Urbana-Champaign <br> ${ }^{\ddagger}$ Microsoft Research

![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-01.jpg?height=52&width=523&top_left_y=568&top_left_x=778)


#### Abstract

Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding designing cuttingedge optimizers and learning rate schedulers carefully (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand what complicates Transformer training from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially-for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage's training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance ${ }^{1}$.


## 1 Introduction

Transformers (Vaswani et al., 2017) have led to a series of breakthroughs in various deep learning tasks (Devlin et al., 2019; Velickovic et al., 2018). They do not contain recurrent connections and can parallelize all computations in the same layer, thus improving effectiveness, efficiency, and scalability. Training Transformers, however, requires extra efforts. For example, although stochastic gradient descent (SGD) is the standard algorithm for conventional RNNs and CNNs, it converges to bad/suspicious local optima for Trans-[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-01.jpg?height=483&width=762&top_left_y=775&top_left_x=1064)

Figure 1: Lacking enough robustness and stability, the 18-Layer Post-LN Transformer training (i.e.the original architecture) diverges and is omitted in the left graph. Admin not only stabilizes model training but unleashes the model potential for better performance.

formers (Zhang et al., 2019b). Moreover, comparing to other neural architectures, removing the warmup stage in Transformer training results in more severe consequences such as model divergence (Popel and Bojar, 2018; Liu et al., 2020a). Here, we conduct comprehensive analyses in empirical and theoretical manners to answer the question: what complicates Transformer training.

Our analysis starts from the observation: the original Transformer (referred to as Post-LN) is less robust than its Pre-LN variant ${ }^{2}$ (Baevski and Auli, 2019; Xiong et al., 2019; Nguyen and Salazar, 2019). We recognize that gradient vanishing issue is not the direct reason causing such difference, since fixing this issue alone cannot stabilize Post$\mathrm{LN}$ training. It implies that, besides unbalanced gradients, there exist other factors influencing model training greatly.

With further analysis, we recognize that for each Transformer residual block, the dependency on its[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-02.jpg?height=811&width=1602&top_left_y=214&top_left_x=227)

Figure 2: The Architecture and notations of Pre-LN Transformers (Left) and Post-LN Transformers (Right).

residual branch ${ }^{3}$ plays an essential role in training stability. First, we find that a Post-LN layer has a heavier dependency on its residual branch than a Pre-LN layer. As in Figure 7, at initialization, a Pre-LN layer has roughly the same dependency on its residual branch and any previous layer, whereas a Post-LN layer has a stronger dependency on its residual branch (more discussions are elaborated in Section 4.1). We find that strong dependencies of Post-LN amplify fluctuations brought by parameter changes and destabilize the training (as in Theorem 2 and Figure 4). Besides, the loose reliance on residual branches in Pre-LN generally limits the algorithm's potential and often produces inferior models.

In light of our analysis, we propose Admin, an adaptive initialization method which retains the merits of Pre-LN stability without hurting the performance. It restricts the layer dependency on its residual branches in the early stage and unleashes the model potential in the late stage. We conduct experiments on IWSLT'14 De-En, WMT'14 EnDe, and WMT'14 En-Fr; Admin is more stable, converges faster, and achieves better performance. For example, without introducing any additional hyper-parameters, Admin successfully stabilizes 72-layer Transformer training on WMT' 14 En-Fr and achieves a 43.80 BLEU score.[^2]

## 2 Preliminaries

Transformer Architectures and Notations. The Transformer architecture contains two types of sublayers, i.e., Attention sub-layers and Feedforward (FFN) sub-layers. They are composed of mainly three basic modules (Vaswani et al., 2017), i.e., Layer Norm $\left(f_{\mathrm{LN}}\right)$, Multi-head Attention ( $f_{\mathrm{ATT}}$ ), and Feedforward Network $\left(f_{\mathrm{FFN}}\right)$.

As illustrated in Figure 2, the Pre-LN Transformer and the Post-LN Transformer organize these modules differently. For example, a PreLN encoder organizes the Self-Attention sublayer as $\mathbf{x}_{2 i-1}^{(p e)}=\mathbf{x}_{2 i-2}^{(p e)}+f_{\mathrm{S}-\mathrm{ATT}}\left(f_{\mathrm{LN}}\left(\mathbf{x}_{2 i-2}^{(p e)}\right)\right)$ and a Post-LN encoder as $\mathbf{x}_{2 i-1}^{(o e)}=f_{\mathrm{LN}}\left(\mathbf{x}_{2 i-2}^{(o e)}+\right.$ $f_{\mathrm{S} \text {-ATT }}\left(\mathbf{x}_{2 i-2}^{(o e)}\right)$, where $\mathbf{x}_{2 i-2}^{(\cdot)}$ is the input of the $i$ th Transformer layer and $\mathbf{x}_{2 i-1}^{(\cdot)}$ is the output of the $i$-th Self-Attention sub-layer. Here, we refer $f_{\mathrm{S}-\mathrm{ATT}}\left(f_{\mathrm{LN}}\left(\mathbf{x}_{2 i-2}^{(p e)}\right)\right)$ and $f_{\mathrm{S} \text {-ATT }}\left(\mathbf{x}_{2 i-2}^{(o e)}\right)$ as the residual branches and their outputs as the residual outputs, in contrast to layer/sub-layer outputs, which integrates residual outputs and shortcut outputs.

Notation elaborations are shown in Figure 2. In particular, we use superscripts to indicate network architectures (i.e., the Pre-LN Encoder), use subscripts to indicate layer indexes (top layers have larger indexes), all inputs and outputs are formulated as Sequence-Len $\times$ Hidden-Dim.

Layer Norm. Layer norm (Ba et al., 2016) plays a vital role in Transformer architecture. It is defined

![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-03.jpg?height=620&width=1585&top_left_y=210&top_left_x=241)

Figure 3: Relative gradient norm histogram (on a log scale) of 18-layer Transformers on the WMT' 14 En-De dataset, i.e., the gradient norm of sub-layer outputs, scaled by the largest gradient norm in the same network.

as $f_{\mathrm{LN}}(\mathbf{x})=\gamma \frac{\mathbf{x}-\mu}{\sigma}+\nu$, where $\mu$ and $\sigma$ are the mean and standard deviation of $\mathbf{x}$.

Feedforward Network. Transformers use twolayer perceptrons as feedforward networks, i.e., $f_{\mathrm{FFN}}(\mathbf{x})=\phi\left(\mathbf{x} W^{(1)}\right) W^{(2)}$, where $\phi(\cdot)$ is the nonlinear function ${ }^{4}$, and $W^{(\cdot)}$ are parameters.

Multi-head Attention. Multi-head Attentions allows the network to have multiple focuses in a single layer and plays a crucial role in many tasks (Chen et al., 2018). It is defined as (with $H$ heads): $f_{\text {ATT }}(\mathbf{q}, \mathbf{k}, \mathbf{v})=$ $\sum_{h=1}^{H} f_{s}\left(\mathbf{q} W_{h}^{(Q)} W_{h}^{(K)} \mathbf{k}^{T}\right) \mathbf{v} W_{h}^{\left(V_{1}\right)} W_{h}^{\left(V_{2}\right)}$, where $f_{s}$ is the row-wise softmax function and $W_{h}^{(\cdot)}$ are parameters. $W_{h}^{(Q)}$ and $W_{h}^{\left(V_{1}\right)}$ are $D \times \frac{D}{H}$ matrices, $W_{h}^{(K)}$ and $W_{h}^{\left(V_{2}\right)}$ are $\frac{D}{H} \times D$ matrices, where $D$ is the hidden state dimension. Parameters without subscript refer the concatenation of all $\mathrm{H}$ head parameters, e.g., $W^{(Q)}=\left[W_{1}^{(Q)}, \cdots, W_{H}^{(Q)}\right]$. In Transformer, this module is used in two different settings: Encoder-Attention $\left(f_{\mathrm{E}-\mathrm{ATT}}(\mathbf{x})=\right.$ $f_{\text {ATT }}\left(\mathbf{x}, \mathbf{x}^{(\cdot e)}, \mathbf{x}^{(\cdot e)}\right)$ and $\mathbf{x}^{(\cdot e)}$ is the encoder output), and Self-Attention $\left(f_{\mathrm{S}-\mathrm{ATT}}(\mathbf{x})=f_{\mathrm{ATT}}(\mathbf{x}, \mathbf{x}, \mathbf{x})\right)$.

## 3 Unbalanced Gradients

In this study, we strive to answer the question: what complicates Transformer training. Our analysis starts from the observation: Pre-LN training is more robust than Post-LN, while Post-LN is more likely to reach a better performance than Pre-LN. In a parameter grid search (as in Figure 10), Pre-LN[^3]

converges in all 15 settings, and Post-LN diverges in 7 out of 15 settings; when Post-LN converges, it outperforms Pre-LN in 7 out of 8 settings. We seek to reveal the underlying factor that destabilizes Post-LN training and restricts the performance of Pre-LN.

In this section, we focus on the unbalanced gradients (e.g., gradient vanishing). We find that, although Post-LN suffers from gradient vanishing and Pre-LN does not, gradient vanishing is not the direct reason causing the instability of Post-LN. Specifically, we first theoretically and empirically establish that only Post-LN decoders suffer from gradient vanishing and Post-LN encoders do not. We then observe that fixing the gradient vanishing issue alone cannot stabilize training.

### 3.1 Gradients at Initialization

As gradient vanishing can hamper convergence from the beginning, it has been regarded as the major issue causing unstable training. Also, recent studies show that this issue exists in the PostLN Transformer, even after using residual connections (Xiong et al., 2019). Below, we establish that only Post-LN decoders suffer from the gradient vanishing, and neither Post-LN encoders, Pre-LN encoders, nor Pre-LN decoders.

We use $\Delta \mathbf{x}$ to denote gradients, i.e., $\Delta \mathbf{x}=\frac{\partial \mathcal{L}}{\partial \mathbf{x}}$ where $\mathcal{L}$ is the training objective. Following previous studies (Glorot and Bengio, 2010), we analyze the gradient distribution at the very beginning of training and find only Encoder-Attention sub-layers in Post-LN suffers from gradient vanishing.

First, we conduct analysis from a theoretical

![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-04.jpg?height=517&width=1153&top_left_y=210&top_left_x=243)

Figure 4: Encoder output changes for parameter changes, i.e., $\mid \mathcal{F}\left(\mathbf{x}_{0}, W\right)-$ $\left.\mathcal{F}\left(\mathbf{x}_{0}, W^{*}\right)\right|_{2} ^{2}$ where $W^{*}-W$ is random perturbations (left) or gradient updates (right). Intuitively, very large $\left|\mathcal{F}-\mathcal{F}^{*}\right|$ indicates the training to be ill-conditioned.

![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-04.jpg?height=417&width=391&top_left_y=223&top_left_x=1415)

Figure 5: Histogram of relative norm of gradient and $\left|W_{i+1}-W_{i}\right|$ where $W_{i}$ is the checkpoint saved after training for $i$ epochs.

| Encoder | Decoder | Gradient | Training |
| :---: | :---: | :---: | :---: |
| Post-LN | Post-LN | Vanishing | Diverged |
| Post-LN | Pre-LN | Diverged |  |
| Pre-LN | Pre-LN |  | Converged |

Table 1: Changing decoders from Post-LN to Pre-LN fixes gradient vanishing, but does not stabilize model training successfully. Encoder/Decoder have 18 layers.

perspective. Similar to Xiong et al. (2019), we establish that Pre-LN networks do not suffer from gradient vanishing (as elaborated in Appendix A.1). Unlike Xiong et al. (2019), we recognize that not all Post-LN networks suffer from gradient vanishing. As in Theorem 1, we establish that Post-LN Encoder networks do not suffer from gradient vanishing. Detailed derivations are elaborated in Appendix A.2.

Theorem 1. - For Post-LN Encoders, if $\gamma$ and $\boldsymbol{\nu}$ in the Layer Norm are initialized as 1 and 0 respectively; all other parameters are initialized by symmetric distributions with zero mean; $\mathbf{x}_{i}^{(o e)}$ and $\Delta \mathbf{x}_{i}^{(o e)}$ are subject to symmetric distributions with zero mean; the variance of $\mathbf{x}_{i}^{(o e)}$ is 1 (i.e., normalized by Layer Norm); $\Delta \mathbf{x}_{i}^{(o e)}$ and the derivatives of modules in $i$-th sub-layer are independent, we have $\operatorname{Var}\left[\Delta \mathbf{x}_{i-1}\right] \geq \operatorname{Var}\left[\Delta \mathbf{x}_{i}\right]$.

To make sure that the assumptions of Theorem 2 match the real-world situation, we further conduct empirical verification. At initialization, we calculate $\left\|\Delta \mathbf{x}_{i}^{(\cdot)}\right\|_{2}$ for 18-layer Transformers ${ }^{5}$[^4]

and visualize $\frac{\left\|\Delta \mathbf{x}_{i}^{(\cdot)}\right\|_{2}}{\max _{j}\left\|\Delta \mathbf{x}_{j}^{(\cdot)}\right\|_{2}}$ in Figure 3. It verifies that only Post-LN decoders suffer from the gradient vanishing. Besides, we can observe that the dropping of gradient norms mostly happens in the backpropagation from encoder-attention outputs (encoder-attention bars) to its inputs (self-attention bars, since the output of self-attention is the input of encoder-attention). This pattern is further explained in Appendix A.3.

### 3.2 Impact of the Gradient Vanishing

Now, we explore whether gradient vanishing is the direct cause of training instability.

First, we design a controlled experiment to show the relationship between gradient vanishing and training stability. We construct a hybrid Transformer by combining a Post-LN encoder and a Pre-LN decoder. As in Section 3.1, only Post-LN decoders suffer from gradient vanishing, but not Post-LN encoders. Therefore, this hybrid Transformer does not suffer from gradient vanishing. As shown in Table 1, fixing gradient vanishing alone (i.e., changing Post-LN decoders to Pre-LN decoders) fails to stabilize model training. This observation provides evidence supporting that the gradient vanishing issue is not the direct cause of unstable Post-LN training.

Moreover, we observe that gradients of all attention modules are unbalanced, while adaptive optimizers mostly address this issue. As in Figure 5 , adaptive optimizers successfully assign different learning rates to different parameters and lead to consistent update magnitudes even with unbalanced gradients. It explains why the standard SGD fails in training Transformers (i.e., lacking the

![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-05.jpg?height=317&width=794&top_left_y=190&top_left_x=237)

Figure 6: The major difference between Pre-LN and Post-LN is the position of layer norms.

ability to handle unbalanced gradients) and necessitates using adaptive optimizers. More discussions are included in Appendix A.4.

## 4 Instability from Amplification Effect

We find that unbalanced gradients are not the root cause of the instability of Post-LN, which implies the existence of other factors influencing model training. Now, we go beyond gradient vanishing and introduce the amplification effect. Specifically, we first examine the difference between Pre-LN and Post-LN, including their early-stage and latestage training. Then, we show that Post-LN's training instability is attributed to layer dependency's amplification effect, which intensifies gradient updates and destabilizes training.

### 4.1 Impact of Layer Norms Positions

As described in Section 2, both Pre-LN and Post$\mathrm{LN}$ employ layer norm to regularize inputs and outputs. Different residual outputs are aggregated and normalized in residual networks before serving as inputs of other layers (i.e., residual outputs will be scaled to ensure the integrated input to have a consistent variance). To some extend, layer norm treats the variance of residual outputs as weights to average them. For example, for Post-LN Self-Attention, we have $\mathbf{x}_{2 i-1}^{(o \cdot)}=\frac{\mathbf{x}_{2 i-2}^{(o \cdot)}+\mathbf{a}_{2 i-1}^{(o .)}}{\sqrt{\operatorname{Var}\left[\mathbf{x}_{2 i-2}^{(o .)}\right]+\operatorname{Var}\left[\mathbf{a}_{2 i-1}^{(o)}\right]}}$ at initialization. Larger $\operatorname{Var}\left[\mathbf{a}_{2 i-2}^{(o \cdot)}\right]$ not only increases the proportion of $\mathbf{a}_{2 i-2}^{(o \cdot)}$ in $\mathbf{x}_{2 i-2}^{(o \cdot)}$ but decreases the proportion of other residual outputs. Intuitively, this is similar to the weight mechanism of the weighted average.

The position of layer norms is the major difference between Pre-LN and Post-LN and makes them aggregate residual outputs differently (i.e., using different weights). As in Figure 6, all residual outputs in Pre-LN are only normalized once before feeding into other layers (thus only treating residual output variances as weights); in Post-LN, most

![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-05.jpg?height=614&width=760&top_left_y=224&top_left_x=1068)

Figure 7: $\beta_{i, j}$ in 6-Layer Post-LN and Pre-LN on the WMT-14 En-De dataset (contains 12 sub-layers).

residual outputs are normalized more than once, and different residual outputs are normalized for different times. For example, if all layers are initialized in the same way, output variances of different Pre-LN residual branches would be similar, and the aggregation would be similar to the simple average. Similarly, for Post-LN, nearby residual outputs are normalized by fewer times than others, thus having relatively larger weights. We proceed to calculate and analyze these weights to understand the impact of layer norm positions.

First, we use $\widehat{\mathbf{a}}_{i}$ to refer $\frac{\mathbf{a}_{i}}{\sqrt{\operatorname{Var} \mathbf{a}_{i}}}$ (i.e., normalized outputs of $i$-th residual branch) and $\widehat{\mathbf{x}}_{i}$ to refer $\frac{\mathbf{x}_{i}}{\sqrt{\operatorname{Var} \mathbf{x}_{i}}}($ i.e., normalized outputs of $i$-th layer or normalized inputs of $(i+1)$-th residual branch). Then, we describe their relationships as $\widehat{\mathbf{x}}_{i}=$ $\sum_{j \leq i} \beta_{i, j} \widehat{\mathbf{a}}_{j}$, where $\beta_{i, j}$ integrates scaling operations of all layer norms (including $\sqrt{\operatorname{Var}\left[\mathbf{a}_{i}\right]}$ ). For example, Pre-LN sets $\beta_{i, j}=\frac{\sqrt{\operatorname{Var}\left[\mathbf{a}_{j}\right]}}{\sqrt{\operatorname{Var}\left[\sum_{k \leq i} \mathbf{a}_{k}\right]}}$. Intuitively, $\beta_{i, j}$ describes the proportion of $j$-th residual branch outputs in $i$-th layer outputs, thus reflects the dependency among layers.

We visualize $\beta_{i, j}$ in Figure 7. For a Post-LN layer, its outputs rely more on its residual branch from the initialization to the end. At initialization, Pre-LN layer outputs have roughly the same reliance on all previous residual branches. As the training advances, each layer starts to rely more on its own residual outputs. However, comparing to Post-LN, Pre-LN layer outputs in the final model still has less reliance on their residual branches.

Intuitively, it is harder for Pre-LN layers to depend too much on their own residual branches. In

Pre-LN, layer outputs (i.e., $\mathbf{x}_{i}^{(p .)}$ ) are not normalized, and their variances are likely to be larger for higher layers ${ }^{6}$. Since $\beta_{i, i}=\frac{\sqrt{\operatorname{Var}\left[\mathbf{a}_{i}\right]}}{\sqrt{\operatorname{Var}\left[\mathbf{x}_{i-1}^{(p .1}+\mathbf{a}_{i}\right]}}, \beta_{i, i}$ is likely to be smaller for higher layers, which restricts $i$-th layer outputs from depending too much on its residual branch and inhibits the network from reaching its full potential. In other words, Pre-LN restricts the network from being too deep (i.e., if it is hard to distinguish $\mathbf{x}_{i}^{(p \cdot)}$ and $\mathbf{x}_{i+1}^{(p \cdot)}$, appending one layer would be similar to doubling the width of the last layer), while Post-LN gives the network the choice of being wider or deeper.

### 4.2 Amplification Effect at Initialization

Although depending more on residual branches allows the model to have a larger potential, it amplifies the fluctuation brought by parameter changes. For a network $\widehat{\mathbf{x}}=\mathcal{F}\left(\mathbf{x}_{0}, W\right)$ where $\mathbf{x}_{0}$ is the model input and $W$ is the parameter, the output change caused by parameter perturbations is $\operatorname{Var}\left[\mathcal{F}\left(\mathbf{x}_{0}, W\right)-\mathcal{F}\left(\mathbf{x}_{0}, W^{*}\right)\right]$, where $W^{*}=W+\delta$. Its relationship with $N$ is described in Theorem 2, and the derivation is elaborated in Appendix B.

Theorem 2. - Consider a $N$-layer Transformer $\widehat{\mathbf{x}}=\mathcal{F}\left(\widehat{\mathbf{x}}_{0}, W\right)$ at initialization, where $\widehat{\mathbf{x}}_{0}$ is the input and $W$ is the parameter. If the layer dependency stays the same after a parameter change (i.e., $\beta_{i, j}$ has the same value after changing $W$ to $W^{*}$, where $W$ is randomly initialized and $\delta=W^{*}-W$ is independent to $W$ ), the output change (i.e., $\left.\operatorname{Var}\left[\mathcal{F}\left(\mathbf{x}_{0}, W\right)-\mathcal{F}\left(\mathbf{x}_{0}, W^{*}\right)\right]\right)$ can be estimated as $\sum_{i=1}^{N} \beta_{i, i}^{2} C$ where $C$ is a constant.

If $\operatorname{Var}\left[\mathbf{a}_{i}\right]$ is the same for all layers, Pre-LN sets $\beta_{i, i}^{2}$ as $1 / i$, and Post-LN sets $\beta_{i, i}^{2}$ as a constant. Thus, we have Corollary 1 and 2 as below.

Corollary 1. - For a $N$-layer Pre-LN $\mathcal{F}$, we have $\operatorname{Var}\left[\mathcal{F}\left(\mathbf{x}_{0}, W\right)-\mathcal{F}\left(\mathbf{x}_{0}, W^{*}\right)\right]=O(\log N)$.

Corollary 2. - For a $N$-layer Post-LN $\mathcal{F}$, we have $\operatorname{Var}\left[\mathcal{F}\left(\mathbf{x}_{0}, W\right)-\mathcal{F}\left(\mathbf{x}_{0}, W^{*}\right)\right]=O(N)$.

They show that, since Post-LN relies more on residual branches than Pre-LN (i.e., has a larger $\left.\beta_{i, i}^{2}\right)$, the perturbation is amplified to a larger magnitude. To empirically verify these relationships, we calculate $\left|\mathcal{F}\left(\mathbf{x}_{0}, W\right)-\mathcal{F}\left(\mathbf{x}_{0}, W^{*}\right)\right|_{2}^{2}$ for PreLN and Post-LN and visualize the results in Fig-[^5]

ure 4. In Corollary 2, $N$ is linearly associated with $\left|\mathcal{F}-\mathcal{F}^{*}\right|_{2}^{2}$ for Post-LN; and in Corollary 1, $\log N$ is linearly associated with $\left|\mathcal{F}-\mathcal{F}^{*}\right|_{2}^{2}$ for Pre-LN. These relationships match the observation in our experiments (as in Figure 4). For further verification, we measure their correlation magnitudes by $R^{2}$ and find $R^{2}=0.99$ in both cases.

Moreover, we replace the random noise $\delta$ with optimization updates (i.e., setting $W^{*}=W+$ $\operatorname{Adam}(\Delta W)$, where opt $(\cdot)$ is update calculated by the Adam optimizer) and visualize output shifts. This replacement makes the correlation between $\left|\mathcal{F}-\mathcal{F}^{*}\right|_{2}^{2}$ and $N$ (for Post-LN) or $\log N$ (for Pre$\mathrm{LN}$ ) to be weaker (i.e., $R^{2}=0.75$ ). Still, as in Figure 4, the output shift $\left|\mathcal{F}-\mathcal{F}^{*}\right|_{2}^{2}$ for Post-LN is larger than Pre-LN by multiple magnitudes.

Intuitively, large output shifts would destabilize the training (Li et al., 2018). Also, as elaborated in Appendix B, the constant $C$ in Theorem 2 is related to network derivatives and would be smaller as training advances, which explains why warmup is also helpful for the standard SGD. Therefore, we conjecture it is the large output shift of Post-LN results in unstable training. We proceed to stabilize Post-LN by controlling the dependency on residual branches in the early stage of training.

### 4.3 Admin - Adaptive Model Initialization

In light of our analysis, we add additional parameters (i.e., $\boldsymbol{\omega}$ ) to control residual dependencies of Post-LN and stabilize training by adaptively initializing $\boldsymbol{\omega}$ to ensure an $O(\log N)$ output change.

Due to different training configurations and model specificities (e.g., different models may use different activation functions and dropout ratios), it is hard to derive a universal initialization method. Instead, we decompose model initialization into two phrases: Profiling and Initialization. Specifically, Admin adds new parameters $\boldsymbol{\omega}$ and constructs its i-th sub-layer as $\mathbf{x}_{i}=f_{\mathrm{LN}}\left(\mathbf{b}_{i}\right)$, where $\mathbf{b}_{i}=\mathbf{x}_{i-1} \cdot \omega_{i}+f_{i}\left(\mathbf{x}_{i-1}\right), \boldsymbol{\omega}_{i}$ is a $D$-dimension vector and $\cdot$ is element-wise product. Then the Profiling phrase and Initialization phrase are:

Profiling. After initializing the network with a standard method (initializing $\omega_{i}$ as $\mathbf{1}$ ), conduct forward propagation without parameter updating and record the output variance of residual branches (i.e., calculate $\operatorname{Var}\left[f_{i}\left(\mathbf{x}_{i-1}\right)\right]$ ). Since all elements in the same parameter/output matrix are independent to each other and are subject to the same distribution, it is sufficient to use a small number of instances in

![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-07.jpg?height=585&width=760&top_left_y=210&top_left_x=248)

Figure 8: $\beta_{i, j}$ of 18-Layer Admin (Post-LN) and PreLN on the WMT-14 En-De dataset.

this phrase. In our experiments, the first batch (no more than 8192 tokens) is used.

Initialization. Set $\boldsymbol{\omega}_{i}=\sqrt{\sum_{j<i} \operatorname{Var}\left[f_{j}\left(\mathbf{x}_{j-1}\right)\right]}$ and initialize all other parameters with the same method used in the Profiling phrase.

In the early stage, Admin sets $\beta_{i, i}^{2}$ to approximately $\frac{1}{i}$ and ensures an $O(\log N)$ output change, thus stabilizing training. Model training would become more stable in the late stage (the constant $C$ in Theorem 2 is related to parameter gradients), and each layer has the flexibility to adjust $\boldsymbol{\omega}$ and depends more on its residual branch to calculate the layer outputs. After training finishes, Admin can be reparameterized as the conventional Post-LN structure (i.e., removing $\boldsymbol{\omega}$ ). More implementation details are elaborated in Appendix C.

To verify our intuition, we calculate the layer dependency of 18-Layer models and visualize the result in Figure 8. Figures 7 and 8 show that Admin avoids over-large dependencies at initialization and unleashes the potential to make the layer outputs depend more on their residual outputs in the final model. Moreover, we visualize the output change of Admin in Figure 4. Benefiting from the adaptive initialization, the output change of Admin gets roughly the same increase speed as Pre-LN, even constructed in the Post-LN manner. Also, although Admin is formulated in a Post-LN manner and suffers from gradient vanishing, 18-layer Admin successfully converges and outperforms 18-layer Pre-LN (as in Table 2). This evidence supports our intuition that the large dependency on residual branches amplifies the output fluctuation and destabilizes training.

![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-07.jpg?height=446&width=760&top_left_y=211&top_left_x=1065)

Figure 9: Development PPL on the WMT'14 En-De dataset and the IWLST' 14 De-En dataset.

## 5 Experiments

We conduct experiments on IWSLT'14 De-En, WMT'14 En-De, and WMT'14 En-Fr. More details are elaborated in Appendix D.

### 5.1 Performance Comparison

We use BLEU as the evaluation matric and summarize the model performance in Table 2. On the WMT'14 dataset, we use Transformer-base models with 6,12 , or 18 layers. Admin achieves a better performance than Post-LN and Pre-LN in all three settings. Specifically, 12-Layer and 18-Layer Post-LN diverges without the adaptive initialization. Pre-LN converges in all settings, but it results in sub-optimal performance. Admin not only stabilizes the training of deeper models but benefits more from the increased model capacity then Pre$\mathrm{LN}$, which verifies our intuition that the Pre-LN structure limits the model potential. As in Figure 1 and Figure 9, although the 6-layer Pre-LN converges faster than Post-LN, its final performance is worse than Post-LN. In contrast, Admin not only achieves the same convergence speed with Pre-LN in the early stage but reaches a good performance in the late stage.

We use 6-layer Transformer-small (its hidden dimension is smaller than the base model) on the IWSLT' 14 dataset, and all methods perform similarly. Still, as in Figure 10, Admin outperforms the other two by a small margin. Together with WMT' 14 results, it implies the training stability is related to layer number. For shallow networks, the stability difference between Post-LN and Pre-LN is not significant (as in Figure 4), and all methods reach reasonable performance. It is worth mentioning that attention and activation dropouts have an enormous impact on IWSLT' 14, which is smaller than WMT' 14 datasets.

Table 2: BLEU on IWSLT'14 De-En and WMT'14 En-Fr/De (AL-BL refers A-layer encoder \& B-layer decoder).

| Dataset | IWSLT'14 De-En | WMT'14 En-Fr |  | WMT'14 En-De |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| Enc \#-Dec \# | 6L-6L (small) | 6L-6L | 60L-12L | 6L-6L | 12L-12L | 18L-18L |
| Post-LN | $35.64 \pm 0.23$ | 41.29 | failed | 27.80 | failed | failed |
| Pre-LN | $35.50 \pm 0.04$ | 40.74 | 43.10 | 27.27 | 28.26 | 28.38 |
| Admin | $\mathbf{3 5 . 6 7} \pm \mathbf{0 . 1 5}$ | $\mathbf{4 1 . 4 7}$ | $\mathbf{4 3 . 8 0}$ | $\mathbf{2 7 . 9 0}$ | $\mathbf{2 8 . 5 8}$ | $\mathbf{2 9 . 0 3}$ |

To further explore the potential of Admin, we train Transformers with a larger size. Specifically, we expand the Transformer-base configuration to have a 60-layer encoder and a 12-layer decoder. As in Table 2, our method achieves a BLEU score of 43.8 on the WMT' 14 En-Fr dataset, the new state-of-the-art without using additional annotations (e.g., back-translation). More discussions are conducted in Appendix F to compare this model with the current state of the art. Furthermore, in-depth analyses are summarized in Liu et al. (2020b), including systematic evaluations on the model performance (with TER, METEOR, and BLEU), comprehensive discussions on model dimensions (i.e., depth, head number, and hidden dimension), and fine-grained error analysis. It is worth mentioning that the $60 \mathrm{~L}-12 \mathrm{~L}$ Admin model achieves a 30.1 BLEU score on WMT'14 En-De (Liu et al., 2020b).

### 5.2 Connection to Warmup

Our previous work (Liu et al., 2020a) establishes that the need for warmup comes from the unstable adaptive learning rates in the early stage. Still, removing the warmup phrase results in more severe consequences for Transformers than other architectures. Also, warmup has been found to be useful for the vanilla SGD (Xiong et al., 2019).

Theorem 1 establishes that $\operatorname{Var}\left[\mathcal{F}\left(\mathbf{x}_{0}, W\right)-\right.$ $\left.\mathcal{F}\left(\mathbf{x}_{0}, W^{*}\right)\right] \approx \sum_{i=1}^{N} \beta_{i, i}^{2} C$ where $C=$ $\operatorname{Var}\left[\mathcal{G}_{i}\left(\widehat{\mathbf{x}}_{i-1}^{*}, W_{i}\right)-\mathcal{G}_{i}\left(\widehat{\mathbf{x}}_{i-1}^{*}, W_{i}^{*}\right)\right]$. In the early stage of training, the network has larger parameter gradients and thus larger $C$. Therefore, using a small learning rate at initialization helps to alleviate the massive output shift of Post-LN. We further conduct experiments to explore whether more prolonged warmups can make up the stability difference between Post-LN and Pre-LN. We observe that 18-layer Post-LN training still fails after extending the warmup phrase from 8 thousand updates to 16,24 , and 32 thousand. It shows that learning rate warmup alone cannot neutralize the
![](https://cdn.mathpix.com/cropped/2024_06_04_deb98724e8392d239770g-08.jpg?height=386&width=780&top_left_y=658&top_left_x=1049)

Figure 10: BLEU score of Post-LN, Pre-LN and Admin on the IWSLT'14 De-En dataset ( $\mathrm{x}$-axis is the $\beta_{2}$ for adaptive optimizers and $y$-axis is the learning rate). Pre-LN converges in all settings while Post-LN diverges in 7 out of 15 settings. When Post-LN converges, it outperforms Pre-LN in 7 out of 8 settings. Admin stabilizes Post-LN training and outperforms Pre-LN (its best performance is comparable with Post-LN).

instability of Post-LN. Intuitively, massive output shifts not only require a small learning rate but also unsmoothes the loss surface (Li et al., 2018) and make the training ill-conditioned.

Admin regularizes the model behavior at initialization and stabilizes the training. To explore whether Admin is able to stabilize the training alone, we remove the warmup phase and conduct a grid search on optimizer hyper-parameters. The results are visualized in Figure 10. It shows that as Post-LN is more sensitive to the choice of hyperparameters, Admin successfully stabilizes the training without hurting its potential.

### 5.3 Comparing to Other Initializations

We compare our methods with three initialization methods, i.e., ReZero (Bachlechner et al., 2020), FixUp (Zhang et al., 2019a), and LookLinear (Balduzzi et al., 2017a). Specifically, we first conduct experiments with 18-layer Transformers on the WMT'14 De-En dataset. In our experiments, we observe that all of ReZero (which does not contain layer normalization), FixUp (which also does not contain layer normalization), and LookLinear (which is incorporated with Post-LN) leads to di-
vergent training. With further analysis, we find that the half-precision training and dropout could destabilize FixUp and ReZero, due to the lack of layer normalization. Simultaneously, we find that even for shadow networks, having an over small reliance on residual branches hurts the model performance, which also supports our intuition. For example, as elaborated in Appendix E, applying ReZero to Transformer-small leads to a 1-2 BLEU score drop on the IWSLT' 14 De-En dataset.

## 6 Related Work

Transformer. Transformer (Vaswani et al., 2017) has led to a series of breakthroughs in various domains (Devlin et al., 2019; Velickovic et al., 2018; Huang et al., 2019; Parmar et al., 2018; Ramachandran et al., 2019). Liu et al. (2020a) show that compared to other architectures, removing the warmup phase is more damaging for Transformers, especially Post-LN. Similarly, it has been found that the original Transformer (referred to as Post-LN) is less robust than its Pre-LN variant (Baevski and Auli, 2019; Nguyen and Salazar, 2019; Wang et al., 2019). Our studies go beyond the existing literature on gradient vanishing (Xiong et al., 2019) and identify an essential factor influencing Transformer training greatly.

Deep Network Initialization. It has been observed that deeper networks can lead to better performance. For example, Dong et al. (2020) find that the network depth players a similar role with the sample number in numerical ODE solvers, which hinders the system from getting more precise results. Many attempts have been made to clear obstacles for training deep networks, including various initialization methods. Based on the independence among initialized parameters, one method is derived and found to be useful to handle the gradient vanishing (Glorot and Bengio, 2010). Similar methods are further developed for ReLU networks (He et al., 2015). He et al. (2016) find that deep network training is still hard even after addressing the gradient vanishing issue and propose residual networks. Balduzzi et al. (2017b) identifies the shattered gradient issue and proposes LookLinear initialization.

On the other hand, although it is observed that scaling residual outputs to smaller values helps to stabilize training (Hanin and Rolnick, 2018; Mishkin and Matas, 2015; Zhang et al., 2019a; Bachlechner et al., 2020; Goyal et al., 2017), there is no systematic analysis on what complicates Transformer training or its underlying connection to the dependency on residual branches. Here, we identify that unbalanced gradients are not the direct cause of the Post-LN instability, recognize the amplification effect, and propose a novel adaptive initialization method.

## 7 Conclusion

In this paper, we study the difficulties of training Transformers in theoretical and empirical manners. Our study in Section 3 suggests that the gradient vanishing problem is not the root cause of unstable Transformer training. Also, the unbalanced gradient distribution issue is mostly addressed by adaptive optimizers. In Section 4, we reveal the root cause of the instability to be the strong dependency on residual branches, which amplifies the fluctuation caused by parameter changes and destabilizes model training. In light of our analysis, we propose Admin, an adaptive initialization method to stabilize Transformers training. It controls the dependency at the beginning of training and maintains the flexibility to capture those dependencies once training stabilizes. Extensive experiments verify our intuitions and show that, without introducing additional hyper-parameters, Admin achieves more stable training, faster convergence, and better performance.

Our work opens up new possibilities to not only further push the state-of-the-art but understand deep network training better. It leads to many interesting future works, including generalizing Theorem 2 to other models, designing new algorithms to automatically adapt deep networks to different training configurations, upgrading the Transformer architecture, and applying our proposed Admin to conduct training in a larger scale.

## Acknowledge

We thank all reviewers for their constructive comments; Chengyu Dong, Haoming Jiang, Jingbo Shang, Xiaotao Gu, and Zihan Wang for valuable discussions and comments; Jingbo Shang for sharing GPU machines; and Microsoft for setting up GPU machines. The research was sponsored in part by DARPA No. W911NF-17-C-0099 and No. FA8750-19-2-1004, National Science Foundation IIS-19-56151, IIS-17-41317, IIS 17-04532, and IIS 16-18481, and DTRA HDTRA11810026.

## References

Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. ArXiv, abs/1607.06450.

Thomas C. Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cottrell, and Julian J. McAuley. 2020. Rezero is all you need: Fast convergence at large depth. ArXiv, abs/2003.04887.

Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In ICLR.

David Balduzzi, Marcus Frean, Lennox Leary, J. P. Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. 2017a. The shattered gradients problem: If resnets are the answer, then what is the question? In ICML.

David Balduzzi, Marcus Frean, Lennox Leary, J P Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. 2017b. The shattered gradients problem: If resnets are the answer, then what is the question? In ICML.

Yoshua Bengio, Patrice Y. Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks.

Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve SaintAmand, et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Workshop on Statistical Machine Translation.

Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th iwslt evaluation campaign, iwslt 2014. In International Workshop on Spoken Language Translation, Hanoi, Vietnam.

Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Michael Schuster, Zhi-Feng Chen, Yonghui Wu, and Macduff Hughes. 2018. The best of both worlds: Combining recent advances in neural machine translation. In $A C L$.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT

Chengyu Dong, Liyuan Liu, Zichao Li, and Jingbo Shang. 2020. Towards adaptive residual network training: A neural-ode perspective. In ICML.

Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In AISTATS.

Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour. ArXiv, abs/1706.02677.
Boris Hanin and David Rolnick. 2018. How to start training: The effect of initialization and architecture. In NeurIPS.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR.

Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. 2019. Music transformer: Generating music with long-term structure. In ICLR.

Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. 2018. Visualizing the loss landscape of neural nets. In NeurIPS.

Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. 2020a. On the variance of the adaptive learning rate and beyond. In ICLR.

Xiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng Gao. 2020b. Very deep transformers for neural machine translation. ArXiv, abs/2008.07772.

Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2020. Understanding and improving transformer from a multiparticle dynamic system point of view. In ICLR Workshop DeepDiffEq.

Dmytro Mishkin and Juan E. Sala Matas. 2015. All you need is a good init. In ICLR.

Toan Q. Nguyen and Julian Salazar. 2019. Transformers without tears: Improving the normalization of selfattention. In IWSLT.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In NAACL-HLT Demonstrations.

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. Image transformer. In ICML.

Martin Popel and Ondrej Bojar. 2018. Training tips for the transformer model. The Prague Bulletin of Mathematical Linguistics, 110:43 - 70 .

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683.

Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens. 2019. Stand-alone self-attention in vision models. In NeurIPS.

Andrew M Saxe, James L McClelland, and Surya Ganguli. 2013. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. ArXiv, $\mathrm{abs} / 1312.6120$.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In CVPR.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS.

Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In ICLR.

Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning deep transformer models for machine translation. In $A C L$.

Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. 2019a. Pay less attention with lightweight and dynamic convolutions. In ICLR.

Lijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei Gao, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. 2019b. Depth growing for neural machine translation. In ACL.

Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shu xin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Li-Wei Wang, and Tie-Yan Liu. 2019. On layer normalization in the transformer architecture. ArXiv, abs/2002.04745.

Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. 2019a. Fixup initialization: Residual learning without normalization. In ICLR.

Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J. Reddi, Surinder Kumar, and Suvrit Sra. 2019b. Why adam beats sgd for attention models. ArXiv, abs/1912.03194.

Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, and Liangchen Luo. 2019. Muse: Parallel multi-scale attention for sequence to sequence learning. ArXiv, $\mathrm{abs} / 1911.09483$.
