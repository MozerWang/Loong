# Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting 

Haoyang Huang ${ }^{1 *}$ Tianyi Tang ${ }^{2 *}$, Dongdong Zhang ${ }^{1 \dagger}$, Wayne Xin Zhao ${ }^{2}$<br>Ting Song ${ }^{1}$, Yan Xia ${ }^{1}$, Furu Wei ${ }^{1}$<br>${ }^{1}$ Microsoft Research Asia, China<br>${ }^{2}$ Gaoling School of Artificial Intelligence, Renmin University of China<br>https://github.com/microsoft/unilm

![](https://cdn.mathpix.com/cropped/2024_06_04_0233f41188380447f255g-01.jpg?height=594&width=643&top_left_y=725&top_left_x=341)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_0233f41188380447f255g-01.jpg?height=631&width=666&top_left_y=712&top_left_x=1072)

(b)

Figure 1: Comparing the effectiveness of the Cross-Lingual-Thought prompt versus the baseline basic prompt on 7 representative benchmarks covering 27 languages: (a) Enhancing the multilingual capability of text-davinci-003 under the zero-shot learning, and (b) Narrowing the gap between the average performance and the best performance of each task in different languages.


#### Abstract

Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingualthought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding, and generation tasks, covering both high-resource and low-resource languages. Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages. Notably, XLT brings over 10 points of average improvement in arithmetic reasoning and open-domain question-answering tasks.


[^0]
## 1 Introduction

Large language models (LLMs) demonstrate impressive multilingual capability in a wide range of natural language processing tasks, including language generation, knowledge utilization, and complex reasoning (Zhao et al., 2023). Their performance in downstream tasks has been shown to reach or even surpass human-level performance (Brown et al., 2020; Chowdhery et al., 2022; Scao et al., 2022). The capabilities of LLMs stem from the extensive volume of training data they leveraged (Kaplan et al., 2020). The training data for current models is primarily dominated by the English language corpus, but it also encompasses data from other languages, as described in GPT3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and BLOOM (Scao et al., 2022), etc.

There are over 7,000 languages worldwide, with the vast majority being low-resource or extremely low-resource languages (Forkel et al., 2022). Despite the latest GPT-4 model (OpenAI, 2023) demonstrating some generalization capabilities in

Input

Request: 詹姆斯决定每

周跑 3 次 3 段冲刺, 每段冲刺跑 60 米。他每周一共跑多少米?
I want you to act as an arithmetic reasoning expert for Chinese.

Request: 詹姆斯决定每周跑 3 次 3 段冲刺，每段冲刺跑 60 米。他每周一共跑多少米?

You should retell the request in English.

You should do step-by-step answer to obtain a number answer . You should step-by-step answer the request.

You should tell me the answer in this format 'Answer:'.

## Output

Request: James decides to run 3 sets of 60 -meter sprints three times a week. How many meters does he run in total each week?

Step-by-step answer:

1. James runs 3 sets of 60 -meter sprints, which means he runs $60 \times 3=180$ meters per sprint.
2. James runs 3 sets of 180 meters per week, which means he runs $180 \times 3=540$ meters per week.

Answer: James runs a total of 540 meters per week.

Figure 2: Overview of our method. Given a request, its associated meta information is filled into the placeholders of the XLT template to form the language-independent prompt, which is fed to the LLM to enhance the generation of responses in the desired format.

multilingual tasks as evaluated on the MMLU benchmark (Hendrycks et al., 2021), it is still the case that LLMs do not have equal capability to handle all languages, leading to imbalanced capability across different languages. Furthermore, several evaluation results (Bang et al., 2023; Jiao et al., 2023; Hendy et al., 2023; Zhu et al., 2023) indicate that large models struggle with understanding and generating non-English languages, particularly in low-resource or extremely low-resource languages. Therefore, to democratize language intelligence and minimize performance gaps in different language, it is essential and meaningful to stimulate and enhance the multilingual capability of models in non-English and low-resource languages.

Intuitively, LLMs can improve multilingual capability by augmenting data (Lin et al., 2022) or fine-tuning models (Chen et al., 2021, 2022), but both are computationally expensive. Alternatively, in-context learning with prompts can also boost performance (Brown et al., 2020; Ahuja et al., 2023; Wei et al., 2022c) but is limited to monolingual tasks (Sanh et al., 2022).

This work explores a universal in-context learning approach to enhance the multilingual capability of LLMs. We introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to enable models to handle various natu- ral language processing tasks across different target languages. Our method employs a generic and language-independent prompt, which eliminates the need to update model parameters. Depending on the task input type, cross-lingual-thought prompting guides the large language model to assume the role of an expert in a specific language for a particular task. Given its predefined meta information, XLT directs LLMs to respond logically through a process involving problem understanding, cross-lingual thinking, task analysis, task execution, and output formatting. During this process, our method is designed to stimulate models' cross-lingual and logical reasoning skills, enabling them to respond to input requests regardless of the language. For enhanced performance, few-shot learning can also be employed with our method by providing an LLM-generated response output as a demonstration using cross-lingual-thought prompting zero-shot learning.

We conduct a comprehensive evaluation to verify the effectiveness of XLT across seven representative multilingual benchmarks of natural language reasoning, understanding, and generation tasks. Each benchmark includes multilingual data covering both high-resource and low-resource languages. The experimental results demonstrate that our method can significantly improve the perfor-

```
I want you to act as a task_name expert for task_language
task_input
You should retell/repeat the input_tag in English.
You should task_goal .
You should step-by-step answer the request.
You should tell me the output_type (output_constraint) in this format ' output_type :'
```

Figure 3: Illustration of XLT template. Referring to Figure 2 and Appendix for instantiated examples.

mance of all benchmarks across languages under both zero-shot and few-shot learning settings. Notably, XLT achieves an average gain of over 10 points on the MGSM and MKQA benchmarks. Furthermore, we observe that our prompting method significantly reduces the gap between the average performance and the best performance of each task in different languages, indicating its potential to democratize language intelligence.

## 2 Cross-Lingual-Thought Prompting

Although LLMs are capable of accepting any input and generating responses, users typically structure their requests in the form of prompts to elicit the desired output. The design of these prompts is crucial for achieving optimal performance on downstream tasks, as LLMs are sensitive to the format of the prompts chosen (Zhao et al., 2021). Through a process called instruction tuning (Wei et al., 2022a), models can develop the ability to follow natural language instructions (Wei et al., 2022b), which can reduce their sensitivity to prompt engineering (Wei et al., 2022a). In accordance with the guidelines of the OpenAI cookbook ${ }^{1}$, we propose a cross-lingual thought prompting template, denoted as the XLT template. This generic template allows LLMs to respond to requests with cross-lingual thought and supports a wide range of multilingual tasks.

Figure 3 displays the XLT template, with the colored sections representing placeholders. Figure 2 showcases an example of instantiated prompt for the Chinese request. The following section will explain the details of constructing XLT.

### 2.1 Construction of XLT

The XLT template is designed to emulate the process humans employ when handling multilingual tasks. Our template is written in English, as English is the dominant language during LLM pre-[^1]

training, and existing research indicates that English prompting is more effective for multilingual tasks (Shi et al., 2023). In contrast to the vanilla prompt that only includes a task description, our XLT template aims to elicit multilingual capability through cross-lingual thoughts. This template comprises six logical instructions in sequence. To complete the template, only seven placeholders need to be filled in based on intrinsic knowledge of the task and the request, as depicted in igure 3.

Role Assigning . First, the model receives a role definition that helps establish the model's behavior. This concept is akin to the system role of ChatGPT $^{2}$. To achieve this, we simply need to fulfill the task name with a known category (such as commonsense reasoning or paraphrase identification), along with the language of the task in the task language field.

Task Inputting . Second, we explicitly append the request as the task input. The request is basically structured in terms of the task type so as to make sure the model can comprehend it. For example, in the natural language inference task, the two sentence inputs are specified with "premise" and "hypothesis", respectively.

Cross-lingual Thinking . We encourage the model to engage in cross-lingual thought by rephrasing the requested content in English, which is the dominant language used as a pivot language by Shi et al. (2023) and Ahuja et al. (2023). Rephrasing the requested content enclosed in the input tag helps the model better understand the request in its native language and knowledge. Our observations suggest that using keywords such as "retell" or "repeat" while rephrasing the content may result in better performance in practice.[^2]

Task Analyzing . After rephrasing the task input, we need to complete the task in task goal. This step is comparable to the task description used in conventional prompting methods. In practice, we can get the task information from the literature or seek assistance from ChatGPT to generate effective prompts for solving the task (Jiao et al., 2023).

CoT Task Solving . We then ask the model to follow the instructions and complete the task step by step. Since LLMs exhibit a strong ability to maintain a chain-of-thought (Wei et al., 2022c), we carefully design instructions to guide the model, with the hope that it will respond to our instructions in a step-by-step manner and utilize the intermediate outputs to aid in solving the task.

Output Formatting . Finally, we should regularize the output format of the model to obtain the exact answer. LLMs are utilized in a zero- or few-shot manner, and they tend to generate texts that may not conform to the format of the target answer. Fortunately, LLMs possess a strong ability to follow instructions, and we can define the output format in terms of output type and output constraint. The output type can be a number, index, or text, while the output constraint is optional and determined based on the task requirements. Output constraint may include length limitations, language specifications, and other relevant factors.

### 2.2 XLT for Few-shot Learning

The above construction of XLT can be directly fed to LLMs to yield outputs, which is performed in the zero-shot learning setting. In addition, we also explore incorporating demonstrations into XLT to enable few-shot learning. Different from previous work that just appends model outputs to the corresponding request (Shi et al., 2023) or utilizes a verbalizer to format the output, our method constructs the demonstrations with better formatted model outputs from a step-by-step processing-based XLT. As illustrated in Figure 4, we first sample a few examples from the development set and incorporate the requested parts into XLT. The zero-shot learning is performed over LLM to collect responses that are further aligned with those of the samples. Only response-aligned requests are assembled with the corresponding model responses to form final demonstrations for few-shot learning. In this way, the demonstrations are constructed with rich logical knowledge via XLT, which will cater to the XLT-based generation of new requests. In practice,

![](https://cdn.mathpix.com/cropped/2024_06_04_0233f41188380447f255g-04.jpg?height=406&width=780&top_left_y=231&top_left_x=1049)

Figure 4: Construction process for few-shot learning.

we can also correct or design the demonstrations for better alignment with the instruction logic.

## 3 Experiments

To comprehensively verify the effectiveness of our method on language-independent generality, we evaluate our XLT template on different LLMs covering various natural language processing tasks in multiple languages.

### 3.1 Experimental Setups

### 3.1.1 Tasks and Benchmarks

We conduct evaluations on seven typical benchmarks related to reasoning, understanding, and generation tasks that can represent different capabilities of LLMs, encompassing both high-resource and low-resource languages. These benchmarks cover 27 different languages, including English (en), German (de), Russian (ru), French (fr), Chinese Simplified (zh), Spanish (es), Japanese (ja), Italian (it), Vietnamese (vi), Turkish (tr), Indonesian (id), Swahili (sw), Arabic (ar), Korean (ko), Greek (el), Thai (th), Bulgarian (bg), Hindi (hi), Estonian (et), Bengali (bn), Tamil (ta), Galician (gl), Urdu (ur), Telugu (te), Javanese (jv), Haitian Creole (ht), and Southern Quechua (qu). In terms of the language distribution statistics in the Common Crawl Monthly Archives ${ }^{3}$ and the language performance of LLMs (Shi et al., 2023; Ahuja et al., 2023), we have arranged them in the order of language frequency from high-resource to low-resource. In particular, the frequency of some underrepresented languages is even less than $0.1 \%$ (e.g., bn, ta, gl, ur, te, jv, ht, qu).

## - Reasoning tasks

- Arithmetic Reasoning. The MGSM (Shi et al., 2023) benchmark contains grade school mathe-[^3]matical problems and asks the model to calculate the correct answer. It covers 11 languages, and we utilize the accuracy score for evaluation.
- Commonsense Reasoning. The XCOPA (Ponti et al., 2020) benchmark contains one premise and two choices. It asks the model to choose which one is the result or cause of the premise. It covers 11 languages from 11 diverse families, and we utilize the accuracy score for evaluation.


## - Understanding tasks

- Natural Language Inference. The XNLI (Conneau et al., 2018) benchmark contains one premise and one hypothesis and requires the model to determine whether the hypothesis is entailed, contradicted, or neutral conditioned on the premise. It covers 15 languages, and we utilize the accuracy score for evaluation.
- Paraphrase Identification. The PAWS-X (Yang et al., 2019) benchmark contains two sentences and requires the model to judge whether they paraphrase each other or not. It covers 7 languages, and we utilize the accuracy score for evaluation.


## - Generation tasks

- Question Answering. The MKQA (Longpre et al., 2021) benchmark contains an open-domain question and asks the model to predict a short answer. Since it has unanswerable questions or long questions that do not have precise answers, we remove these questions during evaluation. It covers 25 languages, and we choose a subset of 10 languages, including de, en, es, fr, ja, ru, th, tr, vi, and zh. We utilize the token overlap F1 score for evaluation.
- Summarization. The XL-Sum* (Hasan et al., 2021) (250 test samples randomly sampled from XL-Sum per language) benchmark contains a long news article and wants the model to summarize it into a short text. It covers 44 languages, and we choose a subset of 6 languages, including en, es, fr, tr, vi, and zh. We utilize the ROUGE-1 score (Lin, 2004) for evaluation.
- Machine Translation. The FLORES* (Costajussà et al., 2022) (200 test samples randomly sampled from FLORES-200 per language) benchmark contains parallel text from Wikimedia projects for 204 languages, yielding over 40,000 translation directions. We choose a subset of 12 directions, including high resource to high resource translation (i.e., $\mathrm{zh} \leftrightarrow \mathrm{ru}$ and $\mathrm{de} \leftrightarrow \mathrm{vi}$ ), high resource to low resource translation (i.e., $\mathrm{zh} \leftrightarrow$ th and $\mathrm{zh} \leftrightarrow \mathrm{jv}$ ), and low resource to low resource translation (i.e., th $\leftrightarrow \mathrm{gl}$ and $\mathrm{jv} \leftrightarrow$ th). We utilize the SacreBLEU score (Papineni et al., 2002; Post, 2018) for evaluation.

Among these benchmarks, MGSM, XCOPA, XNLI, PAWS-X, and MKQA are parallel, i.e., the instances are semantics-equivalent across each language. For all benchmarks, we report the results on the test sets using all instances (Table 5), except for XL-Sum and FLORES-200, where we only sample 250 and 200 examples respectively to show the trend of generation performance. In the fewshot setting, we randomly choose examples from the development set if they have, otherwise, we translate the English training set into corresponding languages to construct several examples.

### 3.1.2 Baselines

Basic Prompt are the vanilla in our experiments that were proposed and suggested in previous work. After determining the prompt, we format each monolingual instance using the English basic prompt. This setting is similar to the monolingual prompting in MEGA (Ahuja et al., 2023). The basic prompts used for the evaluation of each benchmark are listed in Table 5. Note that, we dismiss the baseline using native-language, since MEGA (Ahuja et al., 2023) reveals monolingual prompting is superior to cross-lingual prompting.

Chain-of-Thought (CoT) prompting invokes LLMs to generate a series of intermediate results to solve reasoning tasks (Wei et al., 2022c), which is still effective under multilingual scenarios (Shi et al., 2023). In experiments, we append the instruction "Let's think step-by-step and tell me the answer in the end" after the input to prompt LLMs.

Translate-English leverages the robust capabilities of LLMs in English to tackle multilingual tasks, as suggested by both Shi et al. (2023) and Ahuja et al. (2023). This approach translates instances from other languages into English beforehand. In practice, we utilize the Google Translate API to translate examples into English and apply the basic prompt to format them. Note that, we do not apply this method to generation tasks since they require the output in respective language rather English.

XLT utilizes the proposed template consisting of multiple instructions introduced in Section 2. The
instantiated XLT templates for each benchmark are listed in Table 6.

In few-shot learning scenarios, for basic prompt, we use the same template as an additional input to the model. For XLT, we provide the exemplars with XLT template inputs and anticipate desirable step-by-step outputs as outlined in Figure 4. In the subsequent evaluation, we apply the 5 -shot setting, except for the XL-Sum* experiments, which use the 3 -shot setting due to input length constraints.

### 3.1.3 LLMs

We mainly evaluate two LLMs from the GPT-3.5 series models:

- text-davinci-003 ${ }^{4}$ is trained using instruction tuning and reinforcement learning from human feedback (Ouyang et al., 2022). It can perform a wide range of natural language tasks with satisfactory results.
- gpt-3.5-turbo ${ }^{4}$ is optimized for chat based on text-davinci-003 and suitable for traditional NLP tasks. It is the most capable GPT-3.5 model.

To verify the compatibility of our XLT template, we further incorporate LLaMA-2-Chat (Touvron et al., 2023) (Llama-2-70b-chat-hf) as our base models. It is an open-source model that has been trained through supervised fine-tuning and reinforcement learning from human feedback on the base LLaMA 2 model. In addition, we also refer to the existing results from other LLMs, such as code-davinci-002 ${ }^{4}$, when the evaluation is comparable. During inference, we employ greedy search (i.e., temperature $=0$ ) to generate the LLM responses. We find LLMs have excellent instructionfollowing abilities to respond to our instructions in the given format. Therefore, we just extract the part after "Answer format:" as labels.

### 3.2 Experimental Results

Multilingual Capability. We comprehensively evaluate XLT's performance over seven tasks. The average score of text-davinci-003 is summarized in Figure 1(a) and Table 1, and more details are listed in Appendix A. As for the CoT prompting, it can enhance reasoning tasks while becomes less effective on understanding and generation tasks. In terms of the Translate-En prompting, it can boost the performance in the zero-shot settings while[^4]

may not work well in the few-shot settings. Overall, compared to the three baseline methods, XLT achieves significant improvements over two LLMs for all tasks on both zero-shot and few-shot settings regardless of the language difference, except for a slight drop on the PAWS-X benchmark in the zero-shot setting. It is noted that XLT achieves remarkable gains of nearly 20 points on average in the MGSM benchmark for the arithmetic reasoning task and around 10 points on average in the MKQA benchmark for the open-domain question answering task. The experiments demonstrates the effectiveness of XLT for empowering LLM with multilingual capability.

As for the compatibility test, we list the results of LLaMA-2-Chat on the MGSM benchmark in Table 7. It is notable that LLaMA 2 can also benefit from our cross-lingual-thought, which further demonstrates the generality of our XLT template. However, the gains of LLaMA-2-Chat is not as good as GPT-based models. Our analysis reveals this gap can primarily be attributed to LLaMA 2's poorer multi-step instruction-following ability.

Language Democratization. Furthermore, we try to assess the democratization degree of tasks between languages by defining a "democratization score", which calculates the average percentage of performance attained by different languages relative to the best performance among all languages. Given the evaluation scores of $s_{1}, s_{2}, \ldots, s_{l}$ corresponding to $l$ language on a task, the democratization score is formulated as:

$$
\begin{equation*}
\frac{\sum_{i=1}^{l} s_{i}}{l} / \max \left\{s_{i}\right\}_{i=1}^{l} \tag{1}
\end{equation*}
$$

Table 2 presents the degree of democratization for tasks across languages under both zero-shot learning and few-shot learning, and we further summarize it in Figure 1(b) by averaging all scores per task regardless of the setting and model differences. We can observe that XLT leads to higher democratization scores in general, particularly for XCOPA, and MKQA. As for MGSM, XNLI, and PAWS-X, our XLT can improve performance in multiple languages, where the overall performance of the baseline is consistently lower but the gap between languages is smaller as shown in Tables 7, 9, and 10 . In conclusion, our method can reduce the performance gap between languages and improve the language democratization of LLMs.

| Settings |  | Reasoning |  | Understanding |  | Generation |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | MGSM | XCOPA | XNLI | PAWS-X | MKQA | XL-Sum* | FLORES* |
| Zero-shot | text-davinci-e | 303 |  |  |  |  |  |  |
|  | Basic Prompt | 12.5 | 70.1 | 53.3 | 52.0 | 29.0 | 23.7 | 15.4 |
|  | CoT | 25.7 | 70.9 | 53.0 | 57.8 | 30.9 | 23.8 | 15.8 |
|  | Translate-En | 15.7 | 68.0 | 54.8 | 55.0 | - | - | - |
|  | XLT | 23.9 | 73.3 | 62.4 | 57.1 | 40.2 | 25.2 | 17.7 |
|  | gpt-3.5-turbo |  |  |  |  |  |  |  |
|  | Basic Prompt | 23.3 | 76.9 | 52.6 | 65.5 | 31.6 | 24.7 | 19.1 |
|  | CoT | 45.5 | 78.3 | 54.8 | 61.0 | 14.8 | 25.4 | 19.7 |
|  | Translate-En | 27.1 | 75.7 | 52.2 | 66.8 | - | - | - |
|  | $\overline{\text { XLT }}$ | 70.0 | 80.3 | 65.5 | 63.6 | 42.7 | 26.1 | 21.2 |
| Few-shot | text-davinci-6 |  |  |  |  |  |  |  |
|  | Basic Prompt | 45.5 | 75.6 | 59.1 | 68.7 | 39.1 | 26.8 | - |
|  | Translate-En | 46.5 | 77.4 | 56.9 | 68.5 | - | - | - |
|  | XLT | 55.4 | 81.3 | 67.5 | 72.2 | 49.6 | 27.3 | - |
|  | gpt-3.5-turbo |  |  |  |  |  |  |  |
|  | Basic Prompt |  |  |  |  |  |  | - |
|  | Translate-En | 65.1 | 81.9 | 58.3 | 63.7 | - | - | - |
|  | XLT | 72.5 | 85.9 | 65.0 | 69.1 | $\mathbf{5 2 . 5}$ | 27.9 | - |

Table 1: The average scores in different languages for the seven benchmarks in zero-shot and few-shot settings. We omit the results (denoted as "-") of Translate-En since it is not applicable for generation tasks.

| Settings | Reasoning |  | Understanding |  | Generation <br> MKQA |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  | MGSM | XCOPA | XNLI | PAWS-X |  |
| Zero-shot setting |  |  |  |  |  |
| text-davinci-003 |  |  |  |  |  |
| Basic Prompt | 65.2 | 77.8 | 83.8 | 97.1 | 60.2 |
| CoT | 65.4 | 80.1 | 83.5 | 89.5 | 61.4 |
| Translate-En | 77.2 | 78.7 | 86.0 | 95.3 | 51.6 |
| XLT | 68.5 | 82.1 | 80.7 | 88.4 | 78.7 |
| gpt-3.5-turbo |  |  |  |  |  |
| Basic Prompt | 73.0 | 83.6 | 80.5 | 89.0 | 61.8 |
| CoT | 66.7 | 85.7 | 80.7 | 88.9 | 46.4 |
| Translate-En | 80.4 | 84.6 | 79.8 | 90.7 | 54.1 |
| XLT | 84.1 | 89.1 | 88.0 | 96.2 | 75.3 |
| Few-shot setting |  |  |  |  |  |
| text-davinci-003 |  |  |  |  |  |
| Basic Prompt | 75.4 | 82.0 | 82.5 | 88.2 | 74.3 |
| Translate-En | 77.1 | 82.6 | 79.5 | 87.8 | 68.5 |
| XLT | 84.5 | 85.6 | 85.3 | 91.6 | 82.7 |
| gpt-3.5-turbo |  |  |  |  |  |
| Basic Prompt | 76.1 | 84.1 | 83.6 | 94.4 | 82.1 |
| Translate-En | 78.6 | 86.4 | 79.2 | 95.4 | 71.3 |
| XLT | 86.2 | 89.7 | 84.3 | 94.1 | 83.1 |

Table 2: The democratization degree of tasks against languages.

### 3.3 Further Analysis

In this section, we further investigate the factors that affect the performance of XLT and how they affect various multilingual benchmarks.

### 3.3.1 Ablation of XLT

For the XLT variants, we mainly conduct experiments to compare the following strategies:

- Ablating the instructions. Since our XLT consists of six logical instructions, we disable the Role Assigning, Cross-lingual Thinking, and CoT Task Solving instructions separately to analyze the contribution per instruction.
- Reordering the instructions. Considering the logicality of our instructions, we further change the order of the instructions in XLT to explore whether LLMs will handle tasks differently and lead to different results.
- Changing the content word. As prompts are usually sensitive to the word choice, we verify the robustness of XLT when alternating the rephrasing keyword with "retell", "repeat", and "translate" in the cross-lingual thinking instruction.

The outcomes are presented in Table 3, indicating that XLT surpasses almost all the variants, thereby validating the effectiveness and reasonableness of our proposed XLT method.

The effectiveness of each instruction. The results from the "Instruction Ablation" row indicate that: (1) Cross-lingual Thinking yields more significant gains compared to other instructions. This suggests that the LLM's ability of cross-lingual thinking is activated, allowing it to utilize its knowledge in English to solve tasks effectively; (2) Removing Role Assigning from XLT impedes the model's

| Settings |  | MGSM |  | XNLI |  | FLORES* |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | de | $\mathbf{z h}$ | hi | vi | $\mathbf{j v} \rightarrow \mathbf{z h}$ | $\mathbf{z h} \rightarrow \mathbf{j v} \quad$ r |
|  | XLT | \| 79.8 | 72.6 | 61.3 | 64.8 | 19.0 | 10.5 |
| Instruction <br> Ablation | w/o Role Assigning | 76.6 | 69.2 | 57.8 | 63.9 | 16.2 | 8.8 |
|  | w/o Cross-lingual Thinking | 75.6 | 62.0 | 56.1 | 62.2 | 13.2 | 8.2 |
|  | w/o CoT Task Solving | 77.0 | 68.0 | 62.9 | 65.2 | 16.8 | 9.2 |
| Instruction <br> Order | Swap Role Assigning and Task Inputting | 77.2 | $71.8 \quad$ | 54.2 | 61.5 | 19.6 | 11.2 |
|  | Swap Role Assigning and Task Analyzing | 76.8 | 70.8 | 61.0 | $64.0 \quad 0$ | 15.8 | 8.8 |
|  | Swap Cross-lingual Thinking and Task Analyzing | 79.0 | 71.2 | 59.5 | 63.4 | 16.5 | 9.7 |
| Rephrasing <br> Word | $w /$ retell | 79.8 | 72.6 | 61.3 | $64.8 \quad$ | 18.2 | 10.3 |
|  | $w /$ repeat | 77.6 | 68.0 | 60.7 | 64.6 | 19.0 | 10.5 |
|  | w/ translate | 76.4 | 70.0 | 60.1 | 64.5 | 17.5 | 10.2 |

Table 3: Performance comparison across different variants of XLT. All the experiments are conducted using gpt-3.5-turbo under the zero-shot setting.

| Demonstration format | en | de | ru | fr | zh | es | ja | sw | th | bn | te | Avg. |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Basic input + Basic output | 84.0 | 79.2 | 78.8 | 78.8 | 70.8 | 81.2 | 68.8 | 70.8 | 68.8 | 65.2 | 44.8 | 71.9 |
| Basic input + XLT output | 82.4 | 72.4 | 71.2 | 75.2 | 64.4 | 78.8 | 63.2 | 66.8 | 53.6 | 54.8 | 32.4 | 65.0 |
| XLT input + XLT output | 84.8 | 81.4 | 80.2 | 79.2 | 71.8 | 81.6 | 72.8 | 71.2 | 69.8 | 64.4 | 40.8 | $\mathbf{7 2 . 5}$ |

Table 4: Performance comparison across different few-shot variants on the MGSM benchmark. All the experiments are conducted with 5 demonstrations using gpt-3.5-turbo.

understanding of the ultimate goal for diverse multilingual tasks, highlighting the task transferability of XLT; and (3) the better performance of XLT can also be attributed to CoT Task Solving, which requires the model to respond to complex instructions in a step-by-step manner.

The order of logical instructions. The performance drop is evident when the order of our designed logical instructions is switched. When designing XLT, we have taken into account the process by which humans solve multilingual problems, and this experiment further confirms the optimum order of our XLT template. Placing the Role Assigning instruction later may confuse the model initially. Additionally, conducting Cross-lingual Thinking before Task Analyzing is crucial since we rely on the English task-solving abilities of LLMs to handle multilingual tasks.

The robustness of word choice for rephrasing keywords. We can find that different words indeed affect the performance of XLT, but it is less sensitive to the other variants. Through experimentation, we have determined that "repeat" yields better results for text summarization and machine translation, while "retell" is more suitable for the remaining five tasks. Our aim is to provide XLT with a more unified template, while still allowing users to fine-tune specific keywords for optimal performance in their tasks.

### 3.3.2 Effectiveness of XLT Few-shot Learning

 As mentioned in Section 2.2, the construction of demonstrations for XLT few-shot learning differs from the previous method. We have compared XLT and basic prompt. Here, we focus on the construction of the demonstration input-output pairs and compare various demonstrations that may be used to perform XLT few-shot learning. The illustrations can be found in Figure 5.- Basic prompt input + Basic prompt output: This is the normal demonstration format used in most of the previous work.
- Basic prompt input + XLT output: This ablation is to separate the effect of input and output formats in the demonstration.
- XLT input + XLT output: This is the method that we used in this work.

Observing the experimental results presented in Table 4, we can conclude that: (1) Our XLT fewshot learning outperforms all other variants, thus confirming its effectiveness. (2) The use of normal demonstrations for XLT few-shot learning leads to a decrease in performance. (3) Merely incorporating XLT as a demonstration input without its output does not result in any improvements. (4) Consistency in the demonstration for few-shot learning
is crucial, implying that the demonstration inputoutput format should align better with its zero-shot learning input-output format.

## 4 Related Work

### 4.1 LLM Capability Understanding

Despite the impressive capabilities of LLMs, it is crucial to determine their impact on natural language processing tasks. Liang et al. (2022) conduct a comprehensive evaluation of LLMs from various perspectives, such as accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency. Bang et al. (2023) extensively evaluate the ChatGPT model on multiple natural language processing tasks and find that the model performs well in high-resource languages but exhibits certain limitations in low-resource and non-Latin script languages. Additionally, studies by Jiao et al. (2023) and Hendy et al. (2023) compare different GPT models with supervised models for machine translation tasks and find that GPT models have competitive translation abilities in high-resource languages but perform less effectively in low-resource languages. It is worth noting that achieving multilingual generative AI capability necessitates crosslingual knowledge to further improve the model's performance. In this context, Ahuja et al. (2023) evaluate the multilingual task understanding ability of GPT models and attempt to enhance their task processing abilities in other languages using English knowledge. Our work also focuses on evaluating the multilingual capabilities of LLMs, including reasoning, understanding, and generative capabilities. Our evaluations indicate that LLMs exhibit differences in high-resource and low-resource abilities, which necessitates additional efforts to enhance their multilingual capability.

### 4.2 Multilingual Task Processing

Multilingual knowledge has been shown to be exploitable and transferable between languages to improve model performance (Devlin et al., 2019; Conneau et al., 2020; Raffel et al., 2020; Ouyang et al., 2021; Chi et al., 2021). While much research has been devoted to multilingual understanding tasks, multilingual generation tasks are more challenging, particularly when the target language is lowresource or non-English (Ma et al., 2021; Liu et al., 2020). Two methods can enable models to support multilingual task processing: one is training a supervised model that covers multiple languages for multilingual processing (Costa-jussà et al., 2022), and the other is training a pre-trained model and using fine-tuning to transfer knowledge among languages to achieve multilingual capability (Chen et al., 2021, 2022). However, the emergence of LLMs has made it possible to directly process multilingual tasks via in-context learning (Brown et al., 2020; Ahuja et al., 2023). These LLMs, with hundreds of billions or even trillions of parameters, require a significant amount of computation resources for training, making traditional fine-tuning methods less feasible. To improve the generative ability of LLMs, researchers explore in-context learning methods that do not require updating model parameters, such as few-shot prompting (Vilar et al., 2022), automatic prompt learning (Shin et al., 2020), task-instruction prompting (Ye et al., 2023), chain-of-thought prompting (Wei et al., 2022c), etc. Our work builds upon these methods and proposes an optimized, generic, and language-independent prompt to enhance the multilingual capability of LLMs.

## 5 Conclusion

This work investigates the language processing capabilities of large language models in multilingual settings and expects to develop a universal framework for handling diverse multilingual tasks. To accomplish this goal, we propose a generic prompt, referred to as XLT, to enhance the multilingual capability and reduce the performance gaps among languages in tasks related to language understanding, reasoning, and generation in non-English and low-resource languages. Although our method is generally applicable across tasks and languages, we discovered that prompting design factors such as instruction logic and word choice have explicit impacts on its effectiveness. Cross-language thinking in XLT is particularly effective. Finally, we hope this work can inspire further research to prioritize the development of generic prompting. By doing so, large language models can encompass a wider range of modalities and languages.

## Acknowledgements

Tianyi Tang and Xin Zhao are supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. 4222027 and L233008.

## Limitations

Due to limitations imposed by the evaluation benchmarks and OpenAI API cost, we conducted tests on 27 languages, which merely scratch the surface of the vast array of languages in the world. Besides, our XLT template is based on English. It deserves to explore whether the template written in task language can lead to better performance and how to better construct the instruction in each language. Furthermore, we only verify the effectiveness of our method on two GPT-based models (i.e., text-davinci-003 and gpt-3.5-turbo) and LLaMA-2-Chat. It is worthwhile to investigate the generality of our template on more models, such as BLOOM and PaLM.

## References

Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, et al. 2023. Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528.

Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.

Guanhua Chen, Shuming Ma, Yun Chen, Li Dong, Dongdong Zhang, Jia Pan, Wenping Wang, and Furu Wei. 2021. Zero-shot cross-lingual transfer of neural machine translation with multilingual pretrained encoders. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 15-26, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Guanhua Chen, Shuming Ma, Yun Chen, Dongdong Zhang, Jia Pan, Wenping Wang, and Furu Wei. 2022. Towards making the most of cross-lingual transfer for zero-shot neural machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 142-157, Dublin, Ireland. Association for Computational Linguistics.

Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. 2021. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3576-3588, Online. Association for Computational Linguistics.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics.

Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475-2485, Brussels, Belgium. Association for Computational Linguistics.

Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Robert Forkel et al. 2022. Glottocodes: Identifiers linking families, languages and dialects to comprehensive reference information. Semantic Web, 13(6):917924 .

Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,

M. Sohel Rahman, and Rifat Shahriyar. 2021. XLsum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693-4703, Online. Association for Computational Linguistics.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations.

Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210.

Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? yes with gpt-4 as the engine. arXiv preprint arXiv:2301.08745.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.

Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. arXiv preprint arXiv:2304.05613.

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.

Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019-9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising pretraining for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726-742.

Shayne Longpre, Yi Lu, and Joachim Daiber. 2021. MKQA: A linguistically diverse benchmark for multilingual open domain question answering. Transactions of the Association for Computational Linguistics, 9:1389-1406.

Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Alexandre Muzio, Saksham Singhal, Hany Hassan Awadalla, Xia Song, and Furu Wei. 2021. Deltalm: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders. arXiv preprint arXiv:2106.13736.

OpenAI. 2023. Gpt-4 technical report. arXiv.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744. Curran Associates, Inc.

Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2021. ERNIE-M: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 27-38, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulić, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2362-2376, Online. Association for Computational Linguistics.

Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Belgium, Brussels. Association for Computational Linguistics.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi

Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.

Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.

Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations.

Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2022. Prompting palm for translation: Assessing strategies and performance. arXiv preprint arXiv:2211.09102.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022b. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022c. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc.

Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3687-3692, Hong Kong, China. Association for Computational Linguistics.

Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, and Minjoon Seo. 2023. In-context instruction learning. arXiv preprint arXiv:2302.14691.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.

Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.

Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675.
