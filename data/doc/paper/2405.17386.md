# MindMerger: Efficient Boosting LLM Reasoning in non-English Languages 

Zixian Huang ${ }^{1}$, Wenhao Zhu ${ }^{1}$, Gong Cheng ${ }^{1}$, Lei Li $^{2}$, Fei Yuan ${ }^{3 *}$<br>${ }^{1}$ State Key Laboratory for Novel Software Technology, Nanjing University<br>${ }^{2}$ Carnegie Mellon University<br>${ }^{3}$ Shanghai Artificial Intelligence Laboratory<br>\{zixianhuang, zhuwh\}@smail.nju.edu.cn, gcheng@nju.edu.cn<br>leili@cs.cmu.edu, yuanfei@pjlab.org.cn


#### Abstract

Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by $6.7 \%$ and $8.0 \%$ across all languages and low-resource languages on the MGSM dataset, respectively 2 .


## 1 Introduction

One of the primary focuses of Artificial Intelligence research currently revolves around improving its reasoning capabilities [Ahn et al., 2024, Minaee et al., 2024], which is derived from the need to enable Large Language Models (LLMs) [Ouyang et al. 2022, Touvron et al., 2023, Jiang et al. 2023a| to think rationally and perform functions like humans |Imani et al.||2023||Jiang et al.| 2023b]. Substantial progress has been made in English reasoning |Yu et al.| 2023, Yuan et al.| 2023a], but the performance in non-English, especially low-resource languages, still lags behind [Shi et al. 2023] due to the scarce of multilingual training data [Touvron et al. 2023].

Existing work tries to use external models to compensate for the deficiencies of LLM in multilingual reasoning. Some works use the relearning-based strategy, which uses translation models to generate multilingual training data for fine-tuning LLMs to relearn reasoning in each language |Chen et al. 2023. Chai et al., 2024]. Some other works use the replacement-based strategy, which use translation models to translate queries from non-English to English text for replacing the non-English input of[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-02.jpg?height=320&width=1354&top_left_y=246&top_left_x=382)

Figure 1: Examples of multilingual mathematical reasoning from the MGSM dataset. LLM can generate correct and incorrect answers when asked in different languages.

LLM [Shi et al., 2023]. Both strategies try to use the translation model to help LLMs master new capabilities, but the insufficient translation quality constrains the performance of these methods.

Moreover, certain capabilities, such as reasoning and language understanding, are built-in in LLMs and should be utilized without the need to develop them from scratch. For the reasoning capabilities, as illustrated in Figure 1, regardless of the language in which the same mathematical question is posed, the correct reasoning process remains consistent. It shows that the reasoning capabilities is universal rather than language-specific [Brannon, 2005]. For language understanding capabilities, while the Chinese question in the first example of Figure 1 may cause a reasoning error by failing in understanding the English-origin term "dozen of", the LLM successfully differentiated between "the week" and "the weekdays" in the Chinese question, contrasting with the failure of the English question in the second example. It shows that although the proficiency may not match that of English, expressions in non-English languages remain valuable to LLMs.

Considering that the built-in reasoning and language understanding capabilities of LLMs need to be better utilized, in this paper, we propose a new method MindMerger, which preserves the minds of reasoning and language understanding capabilities in LLMs, and merges the external language understanding capabilities from pre-trained multilingual models to boost the multilingual reasoning effects. To address the challenge of insufficient generation quality of external models, MindMerger feeds the LLM the undecoded query representation from the multilingual model rather than text. Additionally, it uses an augmentation strategy that combines the encoded query representation with the input of LLM to utilize both external and built-in language understanding capabilities.

Given the inconsistency in the representation space, understanding the query representation encoded by multilingual model is not trivial for LLMs. To address this, we propose a two-stage training scheme including the mapping stage and the augmentation stage. In the mapping stage, we train MindMerger to embed the language capabilities of the multilingual model into the LLM by using accessible general bilingual pairs such as translation data. In the augmentation stage, MindMerger is further trained to collaboratively utilize the built-in capabilities of LLM and the embedded external language capabilities by using query translation task data generated from translation model. Throughout the two stages, only a small number of parameters that connect two models are trained, while both the multilingual model and the LLM are frozen to prevent their built-in capabilities from forgetting.

Extensive experiments are conducted on three multilingual reasoning datasets and a language understanding dataset. Taking on the MGSM dataset [Shi et al. 2023] as an example, MindMerger outperforms all baselines and achieves a lead of at least $6.7 \%$ on the average accuracy across all languages, and its performance in low-resource languages is even more significant leading by at least $8.0 \%(\$ 4.3$ ). Compared with the replacement-based method that translates non-English text into English as the LLM input, MindMerger can lead by at least $6.6 \%$ in average accuracy based on the same translation model ( $\$ 5.1$ ). Benefiting from the accessible general bilingual pairs used in the mapping stage, the average accuracy across low-resource languages increased by $14.9 \%$ ( $\$ 5.2$.

Our contributions can be summarized as follows:

- We propose a new method MindMerger to boost the multilingual reasoning of LLMs, which preserves the built-in reasoning and language understanding capabilities of LLMs while augmenting them with the external language understanding capabilities from multilingual models.
- We propose a two-stage training scheme to help MindMerger sequentially learn to embed external capabilities and collaboratively utilize internal and external capabilities.
- MindMerger achieves the best results on four dataset about multilingual reasoning or understanding datasets, notably improving all languages, including low-resource languages, with an average accuracy increase of at least $6.7 \%$ and $8.0 \%$ on the MGSM datasets.


## 2 Related Work

Multilingual Reasoning. There have been some attempts to improve LLM's performance on multilingual reasoning. Several works design crafted prompts to assist LLMs in reasoning [Huang et al. 2023, Qin et al., 2023|, but their effectiveness diminishes when used with open-source LLMs like Llama [Touvron et al. 2023] due to limited capacity for multi-step instruction-following [Huang et al. 2023]. Supervised fine-tuning LLMs is another effective way, where some works use translation models to translate query-response [Chen et al., 2023], or query-only [Zhu et al., 2024] to build multilingual task datasets, enabling LLMs to relearn reasoning or language understanding. Some other works utilize external models to generate English text [Shi et al. 2023] to replace the nonEnglish input of LLMs, aiming to utilize the reasoning capabilities of LLMs directly. However, the above methods are limited by the generation quality of the translation model, and the built-in reasoning or language understanding capabilities of LLM are neglected. In contrast, MindMerger avoid the loss introduced by autoregressive decoding and employs an augmentation-based strategy that utilizes the built-in capabilities of LLMs to enhance multilingual reasoning performance.

Model Merging. Model Merging is a popular topic in recent LLM research. Generally, it aims to combine an external module to strengthen the capabilities that LLM lacks, such as multi-modal vision capabilities [Liu et al., 2023, Li et al., 2023, Zhu et al., 2023]. Some works [Sun et al., 2021. Bansal et al. 2024] find that interpolating models with multilingual capabilities using cross-attention can improve the performance of multilingual tasks, but research on merging multilingual models and LLMs with English reasoning capabilities is still scarce. Recently, Yoon et al. [2024] merge the encoder of multilingual model and the LLM to improve the multilingual reasoning performance. However, the input of LLMs is completely replaced by the output of a multilingual encoder, making its built-in multilingual capabilities underutilized. In addition, only using English task data for training limits the performance of model merging. Instead of replacing the input of LLMs, we collaboratively utilize the features of the multilingual model and use the available general bilingual pair to train to obtain multilingual reasoning capabilities.

## 3 Approach

![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-03.jpg?height=414&width=898&top_left_y=1710&top_left_x=602)

Figure 2: Overview of the model structure and training scheme of MindMerger, which consists of an LLM (blue) and a external model (yellow) and is trained by a two-stage scheme.

Given an LLM with skilled reasoning capabilities and useful language understanding capabilities, our target is to maintain the built-in capabilities and compensate for its shortcomings in non-English language understanding capabilities with an external multilingual model. To this end, as shown in Figure 2, we propose MindMerger that uses the output of the multilingual model as an augmentation complementing to the original input ( $\$ 3.1$. We further design a two-stage training scheme to learn the collaborative utilizing both the external and built-in capabilities $(\$ 3.2$.

### 3.1 Model Structure

Given a query $q$ with $l$ tokens, we first utilize the multilingual model to understand and encode it into a representation $\boldsymbol{X}$ that is more general and reduces the challenge of multilingual understanding:

$$
\begin{equation*}
\boldsymbol{X}=\operatorname{Encoder}(q) \tag{1}
\end{equation*}
$$

where Encoder $(\cdot)$ is a pre-trained multilingual model, typically using its encoder part, and $\boldsymbol{X} \in$ $\mathbb{R}^{l \times d_{1}}$ is the hidden state output of the multilingual model with a dimension of $d_{1}$.

The representation $\boldsymbol{X}$ resides in the multilingual model space, which is separate from the LLM space and cannot be used directly. Therefore, we introduce a mapping layer:

$$
\begin{equation*}
\widetilde{\boldsymbol{X}}=\operatorname{Mapping}(\boldsymbol{X}) \tag{2}
\end{equation*}
$$

where $\widetilde{\boldsymbol{X}} \in \mathbb{R}^{l \times d_{2}}$ is the projection of $\boldsymbol{X}$ on the space of LLM. Unless otherwise stated, the implementation of Mapping $(\cdot)$ is a two-layer multi-layer perceptron (MLP).

The acquisition of $\boldsymbol{X}$ utilizes the language understanding capabilities of the external multilingual model. In order to take advantage of the built-in capabilities of the LLM, we prompt it directly:

$$
\begin{equation*}
\boldsymbol{T}=\operatorname{Embedding}(q) \tag{3}
\end{equation*}
$$

where $\boldsymbol{T} \in \mathbb{R}^{l \times d_{2}}$ is the representation of query $q$ in the space of LLM and Embedding $(\cdot)$ is the embedding layer of LLM.

Then, we concatenate the query representation from the multilingual model and the LLM for the collaborative utilizing of LLM's capabilities:

$$
\begin{equation*}
(\tilde{\boldsymbol{X}}, \boldsymbol{T})=[\langle\mathrm{bos}\rangle ; \widetilde{\boldsymbol{X}} ;\langle\mathrm{sep}\rangle ; \boldsymbol{T}] \tag{4}
\end{equation*}
$$

where $\langle$ bos $\rangle \in \mathbb{R}^{d_{2}}$ is the representation of the start token of LLM and $\langle$ sep $\rangle \in \mathbb{R}^{d_{2}}$ is a trainable variable that denotes the boundary of $\widetilde{\boldsymbol{X}}$. Finally, $(\widetilde{\boldsymbol{X}}, \boldsymbol{T})$ is fed to LLM to generate the response.

### 3.2 Two-Stage Training

The training scheme of MindMerger is divided into two stages: mapping stage and augmentation stage. The former helps LLM learn to use the capabilities of multilingual model, and the latter helps LLM collaboratively utilize its own and the capabilities from multilingual model. Examples of training data for each stage are shown in Appendix D.

Mapping Stage. Given the representation spaces between multilingual model and LLM are distant different, it is not trivial for LLM to understand and utilize the external capabilities provided by multilingual model. In order to better learn to utilize external capabilities, we force MindMerger to focus on input from the multilingual model using the replacement-based strategy during this stage. Specifically, we simplify the input of Equation (4) as follows:

$$
\begin{equation*}
\widetilde{\boldsymbol{M}}=[\langle\mathrm{bos}\rangle ; \widetilde{\boldsymbol{X}} ;\langle\mathrm{sep}\rangle] \tag{5}
\end{equation*}
$$

Since general bilingual pairs such as translation data is in vast availability, we use it from various languages to English to train MindMerger in this stage. The loss function is outlined as follows:

$$
\begin{equation*}
-\underset{\sigma}{\arg \min } \log \mathcal{P}(Y \mid \widetilde{\boldsymbol{M}}, \theta, \phi, \sigma) \tag{6}
\end{equation*}
$$

where $Y$ is the text of training target, $\theta$ and $\phi$ are the parameters of the multilingual model and LLM respectively, which are frozen during the training to prevent forgetting, and $\sigma$ contains the trainable parameters of the mapping layer in Equation (2) and the boundary token $\langle$ sep $\rangle$.

Augmentation Stage. Although MindMerger have learned to utilize external capabilities in the mapping stage, the replacement-based strategy may cause LLM to neglect the use of its own built-in capabilities. To help MindMerger further learn to merge capabilities from external and built-in LLM, in this stage we use the augmentation-based strategy as described in the Equation (4). The loss function is calculated as follows:

$$
\begin{equation*}
-\underset{\hat{\sigma}}{\arg \min } \log \mathcal{P}(Y \mid(\widetilde{\boldsymbol{X}}, \boldsymbol{T}), \theta, \phi, \hat{\sigma}) \tag{7}
\end{equation*}
$$

where $\hat{\sigma}$ is initiated from the checkpoint of $\sigma$ trained at the mapping stage, and the parameters of the multilingual model and LLM represented as $\theta$ and $\phi$, respectively, are kept constant during training. In this stage, the query translation task data generated from public translation models is used as the training data to adapt MindMerger to downstream task.

## 4 Experiments

### 4.1 Compared Methods

Our Methods. Two variants of MindMerger were implemented. (1) The implementation described in $\S 3.1$ is denoted as MindMerger-Soft. (2) MindMerger-Hard augments the prompts of LLM with the translated query given by the translation model (Appendix D for the details of prompts).

Baselines. We compared our methods with three categories of baselines. (1) One basic method MonoReason [Yu et al. 2023, Zhu et al. 2024] which is fine-tuned on the English task dataset. (2) Three relearning-based methods that use task data with query translation, including the fullparameter fine-tuning model MultiReason-SFT [Zhu et al., 2024], parameter-efficient fine-tuning model MultiReason-Lora [Hu et al., 2022], and the state-of-the-art method QAlign [Zhu et al. 2024] which first learns language understanding by training LLM on query translation data and then further fine-tunes LLM as MonoReason. (3) Two replacement-based methods that introduce external models, including Translate-En [Shi et al. 2023] and LangBridge [Yoon et al., 2024]. Translate-En is a hard replacement-based which translates the query into English to replace the prompt of LLM. LangBridge is a soft replacement-based method which replaces the input of LLMs with the hidden states output by mT5-xl [Xue et al., 2021]. The prompts of each baselines are presented in Appendix D.

Details. Unless otherwise stated, we used the encoder part of mT5-xl [Xue et al. 2021] as the multilingual model in our methods, used the NLLB200-3.3B as the translation model for baselines, and used the Llama 2-7B as the LLM for all methods. The influence of different multilingual models including encoder-only, decoder-only and encoder-decoder architectures will be analyzed in $\S 5.1$ Both MindMerger and all the baselines, except QAlign, are based on the same MonoReason model. Additionally, MonoReason and QAlign are trained based on the same LLM. Specifically, we used the publicly available checkpoint given by Yu et al. [2023] as MonoReason model for mathematical reasoning task. For all models, we set learning-rate $=2 \mathrm{e}-5$, batch size $=128$, max length $=512$, and epoch $=3$ and used 8 NVIDIA A100 GPUs for training.

### 4.2 Datasets

Evaluation Datasets. We experimented MindMerger with the latest multilingual mathematical reasoning MGSM [Shi et al. 2023] and MSVAMP [Chen et al. 2023], where MSVAMP serves as an out-of-domain test set. In order to evaluate the diverse multilingual reasoning capabilities of the models, a challenging multilingual dataset X-CSQA [Lin et al., 2021] in commonsense reasoning task and a language understanding dataset XNLI [Conneau et al. 2018] in natural language inference (NLI) task were used. The statistics of these datasets are presented in Appendix D

Training Datasets. Three categories of training data were used in our methods and baselines. (1) General bilingual pairs. We used the translation data from the multilingual language to English and randomly sampled 100K of data for each language (except English) from the Lego-MT [Yuan et al. 2023b| dataset, which is a large-scale translation dateset that contains all the languages that involved in our experiments. (2) English task data. We used MetaMathQA [Yu et al., 2023] and MultiNLI Williams et al. [2018] datasets for mathematical reasoning and NLI task, respectively. Similar to Huang et al. [2022], we unified the training set of X-CSQA, OpenBookQA [Mihaylov et al. 2018], ARC [Bhakthavatsalam et al., 2021] and QASC [Khot et al., 2020] to train commonsense reasoning task more fully. (3) Query translation task data. We used the translated results given by Chen et al. [2023] and the official dev set of XNLI for mathematical reasoning and the NLI task, respectively, and translated the X-CSQA training set based on M2M100-1.2B [Fan et al., 2021].

### 4.3 Experimental Results

MindMerger improves LLM performance on all datasets, especially benefiting low-resource languages. MindMerger-Soft has an average accuracy better than all other baselines at least $6.7 \%$, $3.2 \%$ on the MGSM and MSVAMP datasets in Table 1, demonstrating its remarkable multilingual reasoning capabilities. For the commonsense reasoning task in Table 2. MindMerger-Soft also significantly leads all baselines by at least $4.1 \%$. In Table 3. MindMerger-Soft demonstrates advantages in language understanding, which significantly outperformed all the baselines (with p-value $<0.01$ ). MindMerger-Hard achieves the best results except MindMerger-Soft on three out of four datasets, demonstrating the advantages of augmentation-based methods over other categories of methods.

Table 1: Experimental results on MGSM and MSVAMP datasets. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages. We used the checkpoints given by $\mathrm{Yu}$ et al. [2023] as the MonoReason models.

| MGSM | Bn | Th | Sw | Ja | $\mathbf{Z h}$ | De | $\mathrm{Fr}$ | $\mathbf{R u}$ | Es | En | Lrl. | Hrl. | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| MonoReason Yu et al., 2023, Zhu et al., 2024] | 6.8 | 7.2 | 6.8 | 36.4 | 38.4 | 55.2 | 54.4 | 52.0 | 57.2 | 68.8 | 6.9 | ![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-06.jpg?height=28&width=55&top_left_y=510&top_left_x=1605) | 38.3 |
| MultiReason-Lora Hu et al., $\angle 0 \angle 2$ | 29.6 | 35.2 | 28.0 | 52.0 | 54.8 | 59.6 | $58.4 \quad$ | 62.4 | 59.6 | 64.8 | 30.9 | 58.8 | $50.4 \quad$ |
| MultiReason-SFT Zhu et al., 2024] | 33.2 | 40.0 | 42.0 | 42.0 | 42.0 | 45.2 | 44.8 | 45.2 | 48.0 | 52.0 | 38.4 | 45.6 | ![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-06.jpg?height=28&width=46&top_left_y=555&top_left_x=1680) |
| QAlign Zhu et al., 2024$]$ | 32.4 | 39.6 | 40.4 | 44.0 | 48.4 | $54.8 \quad$ | 56.8 | 52.4 | 59.6 | 68.0 | 37.5 | 54.9 | 49.6 |
| LangBridge Yoon et a1.., 2024] | 42.8 | 50.4 | 43.2 | 40.0 | 45.2 | 56.4 | $50.8 \quad$ | 52.4 | 58.0 | 63.2 | 45.5 | 52.3 | $50.2 \quad$ |
| Translate-En Sni et al., 2023$]$ | 48.4 | 37.6 | 37.6 | 49.2 | 46.8 | 60.4 | 56.4 | 47.6 | 59.6 | 65.5 | 41.2 | 55.1 | 50.6 |
| MindMerger-Hard | 46.0 | 36.0 | 48.4 | 52.4 5 | 54.4 | 60.4 | 56.0 | 60.4 | 62.0 | 71.2 | 43.5 | 59.5 | $54.7 \quad$ |
| MindMerger-Soft | 50.4 | 52.8 | 57.2 | 54.4 | 53.6 | 61.2 | 57.6 | 60.8 | 58.4 | 66.8 | 53.5 | 59.0 | 57.3 |
| MSVAMP | Bn | Th | Sw | Ja | $\mathbf{Z h}$ | De | $\mathrm{Fr}$ | $\mathbf{R u}$ | Es | En | Lrl. | Hrl. | Avg. |
| MonoReason Yu et al., 2023, Zhu et al., 2024] | 12.5 | 15.8 | 17.2 | 54.0 | 55.9 | 63.9 | 65.2 | 58.1 | 64.3 | 67.1 | 15.2 | 61.2 | 47.4 |
| MultiReason-Lora Hu et al., $20 \angle 2 \mid$ | 39.4 | 43.8 | 39.1 | 55.2 | 55.1 | $60.4-4-2-1$ | 59.1 | 56.8 | 60.6 | 64.2 | $40.8 \quad$ | 58.8 <br> 5 | $53.4 \quad$ |
| MultiReason-SFT Znu et al., 2024] | 34.8 | 38.1 | $39.8 \quad$ | 43.4 | 42.9 | 45.6 | 45.8 | 45.0 | 46.1 | 46.8 | 37.6 | 45.1 | $42.8 \quad$ |
| QAlign Zhu et al.. 2024 | 41.7 | 47.7 | 54.8 | 58.0 | 55.7 | 62.8 | 63.2 | 61.1 | 63.3 | 65.3 | 48.1 | 61.3 | 57.2 |
| LangBridge Yoon et al.. 2024] | 46.8 | 46.3 | 42.1 | 45.5 | 50.4 | 58.1 | 57.0 | 55.8 | 56.9 | 60.6 | 45.1 | 54.9 | 52.0 |
| Translate-En Sni et al., 2023\| | 47.9 | 51.3 | 43.1 | 50.4 | 55.8 | 43.9 | 50.9 | 53.4 | 51.4 | 60.6 | 47.4 | 52.3 | 50.9 |
| $\mathrm{M}$ | 43 | $46 \quad$ | 50 | 61 | 60 | 65 | 65 | 62 | 65. | 67 | $47 \quad$ | 64.1 | 58.9 |
| MindMerger-Soft | $\mathbf{5 2 . 0}$ | 53.4 | 54.0 | 59.0 | 61.7 | 64.1 | 64.0 | 63.3 | 65.0 | 67.7 | 53.1 | 63.5 | 60.4 |

MindMerger-Soft stands out notably in enhancing the reasoning capabilities of low-resource languages. Compared to all baselines, it delivers an average accuracy improvement of at least $8.0 \%$ and $5.0 \%$ across low-resource languages on the MGSM, MSVAMP datasets.

Utilizing the built-in reasoning capabilities of the LLM rather than relearning for non-English is a more effective way to boost multilingual reasoning capabilities. It is not easy for relearning-based methods to improve the performance of high-resource and low-resource languages simultaneously. For instance, MultiReason-SFT improves its performance in low-resource languages from an average of $6.9 \%$ to $38.4 \%$ on the MGSM dataset, but its performance in high-resource languages declines from an average of $51.8 \%$ to $45.6 \%$. Compared with the three relearning-based baselines, MindMerger-Soft achieves a lead on both low-resource and high-resource languages and outperforms them by at least $6.9 \%$ on the MGSM datasets for the average accuracy across all languages.

Utilizing the built-in language understanding capabilities of $L L M$ rather than replacing them boosts multilingual reasoning capabilities more significantly. Comparing Translate-En and MindMerger-Hard which are completely consistent except for the input strategy, the augmentationbased MindMerger-Hard has a higher average accuracy than the replacement-based Translate-En of 4.4\% and $11.8 \%$ across high-resource languages in the MGSM and MSVAMP datasets respectively, showing that non-English input can help LLM enhance query understanding. Although the understanding capabilities of LLM in low-resource languages are limited, MindMerger-Hard still leads Translate-En by $2.3 \%$ on the MSGM dataset. Larger boost comes from MindMerger-Soft, which leads all replacement-based methods with an average accuracy of at least $6.7 \%$ across all languages.

Table 2: Results on the X-CSQA dataset. Avg. represents the average accuracy across all languages.

| X-CSQA | $\mathbf{S w}$ | Ur | $\mathrm{Hi}$ | $\mathrm{Ar}$ | $\overline{V i}$ | Ja | Pl | $\mathbf{Z h}$ | $\mathbf{N l}$ | $\mathbf{R u}$ | It | $\overline{D e}$ | Pt | $\mathbf{F r}$ | Es | En | ![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-06.jpg?height=28&width=46&top_left_y=1946&top_left_x=1693) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| MonoReason Yu et al., 2023. Zhu et al., 2024\| | 24.2 | 25.1 | 32.9 | 32.3 | 50.9 | 49.1 | 50.6 | 56.5 | 57.5 | 56.0 | 56.0 | 61.2 | 61.7 | 63.5 | 64.0 | 76.3 | 51.3 |
| MultiReason-Lora Huet al.. 2 HLL | 25.1 | 32.0 | 39.2 | 42.2 | 56.6 | 55.9 | 60.6 | 62.2 | 61.3 | 62.8 | 66.3 | 64.9 | 66.2 | 67.4 | 67.7 | 79.3 | 56.9 |
| MultiReason-SFT Znuer a.., 2024 | 27.6 | 29.2 | 32.0 | 28.7 | 38.8 | 38.7 | 45.5 | 43.8 | 45.9 | 46.5 | 50.2 | 49.1 | 51.2 | 52.1 | 54.3 | 67.2 | 43.8 |
| QAlign Zhu et al., 2024 | 35.1 | 32.6 | 37.8 | 36.3 | 50.5 | 49.2 | 51.3 | $54.8 \quad$ | 56.3 | 56.3 | $58.3 \quad$ | $58.8 \quad$ | $59.8 \quad$ | 60.3 | 63.1 | 75.7 | $52.3 \quad$ |
| LangBriage Yoon et al., 2024. | $31.8 \quad$ | 30.5 | 30.6 | 30.6 | 33.3 | 33.9 | $39.8 \quad$ | $39.8 \quad$ | 38.4 | 35.1 | 39.1 | 37.4 | 36.3 | 38.2 | 38.4 | 44.4 | 36.1 |
| Translate-En Sh1 et al., 2023 | 36.5 | 41.3 | 48.4 | 44.6 | 51.8  | 47.1 | 53.3 | 51.5 | 55.0 | 54.4 | 56.3 | 57.3 | 54.7 | 57.2 | 55.5 | 71.3 | 52.3 |
| Min $\quad$ - | 33.1 | 29.9 | 40.4 | $37.7 \quad$ | 52.9 | 49.9 | 54. | 55.4 | 58.0 5 | $58 \quad$ | 59.7 | 58.6 | 61.9 | 62.5 | 63.6 | 75.2 | 53.1 |
| MindMerger-Soft | 45.5 | 46.2 | 48.4 | 51.4 {f9e9f8f21-d2fb-4976-a532-ca82b191a327}   | 60.6 | 53.9 | 63.3 | 62.9 | 63.8  | 63.7 | 66.8  | 67.0  | 67.1  | 68.1  | 69.1  | 78.1 | 61.0   |

## 5 Analysis

### 5.1 The Usage of Multilingual Model

MindMerger can flexibly interpolate among various multilingual models. We experimented with a wide variety of multilingual models, including decoder-only models mGPT [Shliazhko et al., 2022], encoder-only models mBERT [Devlin et al. 2018] and XLM-RoBERTa-large [Conneau et al. 2019],

Table 3: Results on the XNLI dataset. Avg. represents the average accuracy across all languages.

| $\overline{\text { XNLI }}$ | Sw | $\mathbf{U r}$ | $\mathbf{H i}$ | Th | $\overline{A r}$ | $\mathrm{Tr}$ | El | $\overline{\mathbf{V i}}$ | $\mathbf{Z h}$ | $\mathbf{R u}$ | $\mathrm{Bg}$ | $\mathrm{De}$ | $\mathrm{Fr}$ | Es | En | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| son Yu et al.. 2023, Zu et al., 2024] | 45.9 | 49.2 | 55.7 | 55.4 | 60.9 | 61.9 | 63.7 | 73.7  | 74.7 7 | 77.6 | 76.7  | 80.6 | 82.2 | $82.8 \quad$ | $9 \mathbf{9 0 . 0}$ | $68.7 \quad$ |
| MultiReason-Lora Hu et al. ZULL | 45.9 | 49.3 | 56.4 | 55.7 | 60.9 | $61.9 \quad$ | $64.7 \quad$ | 73.7 | 74.7 7 | 76.7 | 76.7 7 | 80.6 | 82.2 | 82.8  | ![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-07.jpg?height=30&width=55&top_left_y=367&top_left_x=1623) | $68.9 \quad$ |
| MultiReason-SFT Znuet al., 2024] | $56.3 \quad$ | 57.5 | 61.7 | 60.1 | 61.7 | 65.6 | 67.0 | 73.7 | 79.1 | 79.7 | 78.7  | 82.3 | 82.9 | 83.9 | 88.8  | 71.9  |
| QAlign Zhu et al., $\angle 0 \angle 4$ | 65.2 | 62.2 | 63.3 | 65.2 | 67.0 | 67.9 | 66.5 | 73.7 | 76.6 | 79.2 | 79.4 | 80.9 | $83.1 \quad$ | $83.8 \quad$ | 89.1  | 73.5 |
| LangBriage Yoon ela1.., 2024] | 71.7 | 66.9 | 71.1 | {fcc0ebe69-6e11-4c8c-85bb-61f7b563e0c6} <br> 72 | 75.2 | $74.8 \quad$ | 79.1 | 78.5 | 77.4  <br> 1  | 77.4 | 79.6 | 78.8  | 79.9 | 80.5 | 83.4 | 76.5 |
| Translate-En Shi et al., 20231 | 65.3 | 61.6 | 68.7 | 69.5 | 68.9 | 74.5 | 79.3 | 76.7 | 74.8 | 76.0 | 80.8 | 80.6 | 80.4 | 81.4 | 87.4 | 75.1 |
| sing | 65 . | 56.4 | 58 | 64 | 63.6 | 70 | $62 \quad 62$ | 56.6 | 61 | $58 \quad 2 \quad-2-1$ | 61 | $64.2 \quad$ | 6 | $63 \quad 63$ | $80.8 \quad$ | $63.2 \quad$ |
| MindMerger-Soft | 66.6 | 69.4 | 74.7 | $71.8 \quad$ | 76.2 | 75.7 | 78.5 | 80.3 | 80.0 | 80.7 | 82.4 | 83.5 | 83.9 | 84.4 | 88.7 | 78.4 |

Table 4: Merging with different multilingual models on the MGSM dataset. \# Parm represents the number of parameters of the used external model. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard $\mathrm{Bn}$, Th, and $\mathrm{Sw}$ as low-resourse languages, and regard the remaining languages as high-resource languages.

| MGSM | \# Parm | Bn | Th | Sw | Ja | Zh | De | Fr | Ru | Es | En | Lrl. | Hrl. | Avg. |
| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
| Translate-En |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| M2M100-418M | $484 \mathrm{M}$ | 30.0 | $\mathbf{3 8 . 0}$ | 38.8 | 31.6 | $\mathbf{5 0 . 8}$ | 52.0 | 50.0 | 42.4 | 54.0 | $\mathbf{6 5 . 5}$ | 35.6 | 49.5 | 44.7 |
| M2M100-1.2B | $1,239 \mathrm{M}$ | 42.4 | 34.0 | $\mathbf{4 9 . 6}$ | 40.8 | 42.8 | 55.2 | 50.4 | $\mathbf{4 9 . 2}$ | 46.8 | $\mathbf{6 5 . 5}$ | $\mathbf{4 2 . 0}$ | 50.1 | 47.1 |
| NLLB-200-1.3B | $1,371 \mathrm{M}$ | 46.0 | 32.0 | 40.4 | 47.2 | 45.6 | 55.2 | 51.2 | 46.0 | 55.2 | $\mathbf{6 5 . 5}$ | 39.5 | 52.3 | 47.9 |
| NLLB-200-3.3B | $3,345 \mathrm{M}$ | $\mathbf{4 8 . 4}$ | 37.6 | 37.6 | $\mathbf{4 9 . 2}$ | 46.8 | $\mathbf{6 0 . 4}$ | $\mathbf{5 6 . 4}$ | 47.6 | $\mathbf{5 9 . 6}$ | $\mathbf{6 5 . 5}$ | 41.2 | $\mathbf{5 5 . 1}$ | $\mathbf{5 0 . 6}$ |
| MindMerger-Hard |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| M2M100-418M | $484 \mathrm{M}$ | 39.6 | 28.0 | 36.4 | 49.2 | 48.8 | 60.0 | $\mathbf{5 6 . 4}$ | 55.6 | 58.4 | 64.8 | 34.7 | 56.2 | 49.7 |
| M2M100-1.2B | $1,239 \mathrm{M}$ | 40.0 | $\mathbf{3 6 . 0}$ | 47.6 | $\mathbf{5 2 . 4}$ | 50.8 | 58.0 | $\mathbf{5 6 . 4}$ | $\mathbf{6 0 . 8}$ | 61.2 | 66.8 | 41.2 | 58.1 | 53.0 |
| NLLB-200-1.3B | $1,371 \mathrm{M}$ | 44.0 | 30.0 | 42.8 | 48.0 | 53.6 | $\mathbf{6 1 . 6}$ | $\mathbf{5 6 . 4}$ | 54.8 | $\mathbf{6 3 . 6}$ | 70.8 | 38.9 | 58.4 | 52.6 |
| NLLB-200-3.3B | $3,345 \mathrm{M}$ | $\mathbf{4 6 . 0}$ | $\mathbf{3 6 . 0}$ | $\mathbf{4 8 . 4}$ | $\mathbf{5 2 . 4}$ | $\mathbf{5 4 . 4}$ | 60.4 | 56.0 | 60.4 | 62.0 | $\mathbf{7 1 . 2}$ | $\mathbf{4 3 . 5}$ | $\mathbf{5 9 . 5}$ | $\mathbf{5 4 . 7}$ |
| MindMerger-Soft |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| mGPT | $1,418 \mathrm{M}$ | 19.6 | 20.4 | 15.6 | 42.8 | 48.0 | 59.2 | 59.6 | 54.0 | $\mathbf{6 1 . 2}$ | 64.0 | 18.5 | 55.5 | 44.4 |
| mBERT | $178 \mathrm{M}$ | 30.8 | 37.6 | 46.8 | 50.0 | 48.8 | 55.6 | 52.4 | 59.6 | 60.8 | 66.4 | 38.4 | 56.2 | 50.9 |
| XLM-RoBERTa-large | $560 \mathrm{M}$ | 44.0 | 52.4 | 50.4 | 52.4 | 54.0 | 60.8 | 58.4 | 56.8 | 56.8 | 66.4 | 48.9 | 57.9 | 55.2 |
| M2M100-418M | $282 \mathrm{M}$ | 49.2 | $\mathbf{5 2 . 8}$ | 46.0 | 48.8 | 52.4 | 59.6 | 58.0 | 59.2 | 60.8 | 65.6 | 49.3 | 57.8 | 55.2 |
| M2M100-1.2B | $635 \mathrm{M}$ | 49.6 | 52.4 | 53.2 | 52.8 | $\mathbf{5 4 . 4}$ | 60.0 | 56.4 | 60.0 | 58.0 | 66.0 | 51.7 | 58.2 | 56.3 |
| NLLB-200-1.3B | $766 \mathrm{M}$ | 45.6 | 47.6 | $\mathbf{5 7 . 6}$ | $\mathbf{5 4 . 4}$ | 52.4 | 57.2 | 57.2 | $\mathbf{6 0 . 8}$ | 60.8 | 66.8 | 50.3 | 58.5 | 56.2 |
| NLLB-200-3.3B | $1,733 \mathrm{M}$ | $\mathbf{5 2 . 4}$ | 51.6 | 53.6 | 52.8 | 53.2 | 60.4 | $\mathbf{6 0 . 0}$ | 60.4 | 60.4 | $\mathbf{6 7 . 6}$ | 52.5 | $\mathbf{5 9 . 3}$ | 57.2 |
| mT5-large | $564 \mathrm{M}$ | 40.4 | 47.2 | 53.6 | 47.6 | 51.6 | 59.2 | 55.2 | 57.6 | 56.8 | 66.4 | 47.1 | 56.3 | 53.6 |
| mT5-xl | $1,670 \mathrm{M}$ | 50.4 | $\mathbf{5 2 . 8}$ | 57.2 | $\mathbf{5 4 . 4}$ | 53.6 | $\mathbf{6 1 . 2}$ | 57.6 | $\mathbf{6 0 . 8}$ | 58.4 | 66.8 | $\mathbf{5 3 . 5}$ | 59.0 | $\mathbf{5 7 . 3}$ |

and the encoder part of encoder-decoder models M2M100 [Fan et al., 2021], NLLB-300 [Costa-jussà et al. 2022] and mT5 [Xue et al. 2021]. The experimental results in Table 4 show that the encoder part of the encoder-decoder model and the encoder-only model are more suitable interpolate into MindMerger-Soft than the decoder-only model, which can achieve better performance than mGPT with a smaller number of parameters. M2M100 is the most cost-effective model, reaching or even exceeding the performance of XLM-RoBERTa-large and mT5-large while using only half of the parameters of XLM-RoBERTa-large and mT5-large.

Our augmentation-based strategy outperforms the translate-then-replace strategy. Comparing Translate-En and MindMerger-Hard which only differ in input strategy, augmentation-based MindMerger-Hard, based on the same translation model, consistently exceeds Translate-En by $5.0 \%$, $5.9 \%, 4.7 \%$, and $4.1 \%$ in the average accuracy. MindMerger-Soft further expands its lead with an increment of the average accuracy by at least $6.6 \%$ based on the same translation model.

MindMerger-Soft is a better utilization of existing multilingual model. As shown in Table 4 , the performance of MindMerger-Soft consistently exceeds MindMerger-Hard under the same multilingual model with increases of average accuracy of $5.5 \%$ and $3.3 \%$ on two versions of M2M100, and 3.6\% and $2.5 \%$ on two versions of NLLB-200. Although only the encoder part of the multilingual model is used, MindMerger-Soft merges LLM with a dense representation rather than decoded text based on sparse bag-of-words, enhancing the effectiveness of utilizing multilingual model.

A more powerful multilingual model can better enhance the multilingual capabilities. We compared the different sizes of each encoder-decoder models and consistently observed that the larger version outperformed the smaller one in Table 4 . With the help of the larger model size, the average accuracy is improved to $1.1 \%, 1.0 \%$, and $3.7 \%$ on M2M100, NLLB-200, and mT5, respectively. The improvement in low-resource languages is even more obvious with an average increase in accuracy of $1.1 \%, 1.0 \%$ and $3.7 \%$ in M2M100, NLLB-200, and mT5, respectively. This underscores the greater imperative to enhance language understanding in low-resource languages.

![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-08.jpg?height=385&width=1353&top_left_y=266&top_left_x=386)

![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-08.jpg?height=306&width=417&top_left_y=281&top_left_x=407)

(a) Mapping Stage

![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-08.jpg?height=303&width=403&top_left_y=282&top_left_x=861)

(b) Augmentation Stage

![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-08.jpg?height=303&width=418&top_left_y=282&top_left_x=1298)

(c) Replacement vs. Augmentation

Figure 3: Ablation experiments of MindMerger-Soft on the MGSM dataset. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages.

### 5.2 Ablation Studies

Mapping Stage. In Figure 3a, we ablated the mapping stage to observe its necessity. A significant drop in the performance of low-resource languages can be observed when ablating the mapping stage, which shows that the accessible general bilingual pairs are beneficial to help MindMerger-Soft understand the low-resource language information that exists in the multilingual model representation space. More detailed are reported in Appendix B. 1

Alignment Stage. In Figure 3b, we ablated the augmentation stage to observe its necessity. It can be observed that after ablating the augmentation stage, even without using any task-related data, MindMerger-Soft can still outperform MonoReason on low-resource languages, which demonstrates the generalization of MindMerger-Soft that benefits from using accessible general bilingual pairs in the mapping stage. Furthermore, when the augmentation stage is added, MindMerger-Soft exhibits a significant improvement, demonstrating its effectiveness in learning the utilization of both external and built-in capabilities. More detailed are reported in Appendix B. 2 .

Replacement vs. Augmentation. MindMerger uses the representation $\boldsymbol{X}$ outputted by multilingual model to augment the LLM's original representation $\boldsymbol{T}$ rather than replace it to better utilize the builtin capabilities of LLMs. To verify the advantages of the augmentation-based strategy, we removed $T$ to make MindMerger-Soft a replacement-based method. As shown in Figure 3c although with exactly the same training data and process, the performance of the replacement-based MindMerger-Soft drops significantly, indicating that it is valuable to use the built-in capabilities of LLM to understand the original input. More detailed are reported in Appendix B.3.

Table 5: Results on the MGSM dataset based on MetaMath-Llama-13B and MetaMath-Mistral-7B. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. [2023], we regard Bn, Th, and Sw as low-resourse languages, and regard the remaining languages as high-resource languages.

| MetaMath-Llama-13B | Bn | Th | Sw | Ja | $\mathbf{Z h}$ | $\mathbf{D e}$ | $\mathbf{F r}$ | $\mathbf{R u}$ | Es | En | Lrl. | Hrl. | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| MonoReason Yu et al., 2023, Zhu et al., 2024] | 12.0 | 8.8 | 6.4 | 48.0 | 56.0 | 64.0 | 63.6 | 62.0 | 67.2 | 70.8 | 9.1 | 61.7 | 45.9 |
| MultiReason-Lora Hu et al., $20 \angle 2$ | 44.0 | 49.2 | 40.8 | 58.0 | 61.2 | 64.0 | 64.4 | $64.8 \quad$ | 67.6 | 68.4 | 44.7 | 64.1 | 58.2 |
| MultiReason-SFT Zhu et al., 2024 | 44.8 | 51.6 | 50.8 | 58.0 | 61.6 | $64.8 \quad$ | 59.2 | 60.8 | 67.6 | 66.4 | 49.1 | 62.6 | 58.6 |
| QAlign Zhu et al., 2024 | 38.4 | 49.6 | 46.0 | 52.4 | 59.2 | 62.0 | 62.4 | 64.4 | 67.2 | 69.2 | 44.7 | 62.4 | $57.1 \quad$ |
| LangBridge Yoon et al., 2024] | 39.2 | 42.8 | 42.0 | 33.6 | 42.0 | 55.2 | 54.8 | 58.8 | 60.8 | 65.2 | 41.3 | 52.9 | 49.4 |
| Translate-En Shı et al., 2023] | 34.8 | 54.0 | 44.4 | 44.4 | 58.0 | 53.6 | 54.0 | 45.6 | 62.4 | 70.8 | 44.4 | 55.5 | 52.2 |
| MindMerger-Hard | 48.0 | 38.4 | 53.6 | 51.6 | 52.8 | 66.8 | 61.6 | 60.8 | 68.4 | 67.6 | 46.7 | 61.4 | 57.0 |
| MindMerger-Soft | 55.2 | 59.6 | 56.4 | 60.0 | 60.4 | 65.2 | 63.6 | 68.0 | 69.6 | 68.8 | 57.1 | 65.1 | 62.7 |
| MetaMath-Mistral-7B | Bn | Th | Sw | $\mathrm{Ja}$ | $\mathbf{Z h}$ | $\mathrm{De}$ | $\mathrm{Fr}$ | $\mathbf{R u}$ | Es | En | Lrl. | Hrl. | Avg. |
| MonoReason Yu et al., 2023, Zhu et al., 2024] | 38.4 | 34.8 | 16.8 | 50.8 | 57.2 | $\overline{70.4}$ | 70.8 | 67.2 | 71.2 | 78.0 | 30.0 | 66.5 | $\overline{55.6}$ |
| MultiReason-Lora Hu et al.. $20 \angle 2$ | 46.8 | 51.2 | 39.6 | 54.4 | 62.4 | 72.0 | 66.0 | 68.4 | 70.0 | 76.0 | 45.9 | 67.0 | 60.7 |
| MultiReason-SFT Zhu et al., 2024 | 18.4 | 26.4 | 26.8 | $30.8 \quad$ | 28.8 | $32.4 \quad$ | $34.8 \quad$ | 32.0 | 38.0 | 39.6 | 23.9 | $33.8 \quad$ | $30.8 \quad$ |
| QAlign Zhu et al., 2024 | 45.6 | 51.2 | 55.2 | 49.4 | 57.2 | 59.2 | 59.8 | 60.2 | 63.6 | 65.8 | 50.7 | 59.3 | $56.7 \quad$ |
| LangBridge Yoon et al., 2024 | 50.0 | 60.0 | 47.2 | 58.4 | 65.6 | 68.4 | 68.8 | 68.4 | 65.6 | 65.6 | 52.4 | 65.8 | 61.8 |
| Translate-En Shı et al., 2023] | 54.6 | 58.7 | 47.7 | 57.2 | 63.1 | 50.4 | 56.7 | 64.9 | 58.6 | 69.7 | 53.7 | 60.1 | 58.2 |
| -Hard | 52.4 | 48.4 | 57.6 | 62 | 60.0 | 66.4 | 66 | 69.6 | 71 | 76.4 | 52.8 | 67.6 | 63.2 |
| MindMerger-Soft | 57.6 | 59.6 | 53.2 | 57.2 | 68.8 | 69.2 | 69.6 | 68.4 | 71.6 | 79.2 | 56.8 | 69.1 | 65.4 |

### 5.3 Merging with Different LLMs

MindMerger can be flexibly integrated with different LLMs. To verify this, we conducted experiments on a different type of LLM, MetaMath-Mistral-7B [Jiang et al., 2023a. Yu et al., 2023], and a larger size, MetaMath-Llama-13B [Touvron et al., 2023, Yu et al., 2023]. The experimental results are shown in Table 5, MindMerger-Soft has achieved superior performance across various baselines. The average accuracy of MindMerger-Soft on the MetaMath-Llama-13B and MetaMath-Mistral-7B versions is at least 4.1 and 4.7 higher than all baselines, respectively, demonstrating the potential of extending MindMerger to a wider range of LLMs.

![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-09.jpg?height=582&width=1287&top_left_y=663&top_left_x=408)

![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-09.jpg?height=501&width=610&top_left_y=671&top_left_x=427)

(a) LLM embeddings

![](https://cdn.mathpix.com/cropped/2024_06_04_63d950493b514e1e11a4g-09.jpg?height=507&width=610&top_left_y=668&top_left_x=1083)

(b) mapping layer outputs

Figure 4: T-SNE visualization in the spaces of the LLM embeddings and mapping layer outputs.

### 5.4 Representation Space Changes

For each language, we selected the same 100 texts from the Flores-101 [Fan et al., 2021] dataset, used the mean pooling operation to obtain the representation vectors of the LLM embedding and the hidden states output by and the mapping layer, and visualized it based on T-SNE [Van der Maaten and Hinton, 2008]. As shown in Figure 4, the representation spaces of non-English on the LLM embedding are independent and away from English, especially in low-resource languages, which leads to understanding challenges for non-English and the inability to use built-in reasoning capabilities. By contrast, as shown in Figure 4b, the representations of all languages outputted by the mapping layer almost overlap with English, which reduces the difficulty for LLM to understand non-English languages and enables various languages to utilize the built-in reasoning capabilities.

### 5.5 Supplementary Experiments

We experimented with several supplementary experiments, including the influence of training dataset size used in augmentation stage (Appendix A.1), the selection of mapping layers structure (Appendix A.2), the usage of encoder-decoder model in MindMerger-Soft (Appendix A.3), the quantitative analysis on representation space changes (Appendix A.4), and the translation performance of MindMerger-Soft after mapping stage (Appendix A.5).

## 6 Conclusion

This paper explores a way to more fully utilize the built-in capabilities of LLMs to improve multilingual reasoning effects. We proposed MindMerger to merge the expert multilingual capabilities in the multilingual model with the skilled reasoning and not very proficient but useful multilingual capabilities in the LLM. Through more fully utilizing the potential of LLM and more effective fusion of multilingual models, the performance of MindMerger exceeds all baselines on three reasoning datasets and a language understanding dataset. In the future, we will explore the possibility of MindMerger empowering more professional skills besides reasoning, such as code generation.

## References

Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. In Neele Falk, Sara Papi, and Mike Zhang, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024: Student Research Workshop, St. Julian's, Malta, March 21-22, 2024, pages 225-237. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.eacl-srw.17.

Shervin Minaee, Tomás Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey. CoRR, abs/2402.06196, 2024. doi: 10.48550/ARXIV.2402.06196. URL https://doi.org/10.48550/arXiv.2402.06196.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. doi: 10.48550/ARXIV.2307.09288. URL https://doi.org/10.48550/ arXiv.2307.09288.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023a. doi: 10.48550/ARXIV.2310.06825. URL https://doi.org/10.48550/arXiv.2310.06825

Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. In Sunayana Sitaram, Beata Beigman Klebanov, and Jason D. Williams, editors, Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics: Industry Track, ACL 2023, Toronto, Canada, July 9-14, 2023, pages 37-42. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-INDUSTRY.4. URL https: //doi.org/10.18653/v1/2023.acl-industry. 4 .

Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James T. Kwok. Forward-backward reasoning in large language models for verification. CoRR, abs/2308.07758, 2023b. doi: 10.48550/ARXIV.2308.07758. URL https://doi.org/10.48550/arXiv. 2308 07758

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. CoRR, abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.48550/arXiv.2309.12284.

Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. CoRR, abs/2308.01825, 2023a. doi: 10.48550/ARXIV.2308.01825. URL https://doi.org/10.48550/arXiv. 2308 01825

Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=fR3wGCk-IXp

Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, and Jia Li. Breaking language barriers in multilingual mathematical reasoning: Insights and observations. CoRR, abs/2310.20246, 2023. doi: 10.48550/ARXIV.2310.20246. URL https: //doi.org/10.48550/arXiv.2310.20246.

Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo, Jiaheng Liu, Bing Wang, Xinnian Liang, Jiaqi Bai, Tongliang Li, Qiyao Peng, and Zhoujun Li. xcot: Cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning. CoRR, abs/2401.07037, 2024. doi: 10.48550/ARXIV. 2401.07037. URL https://doi.org/10.48550/arXiv.2401.07037.

Elizabeth M Brannon. The independence of language and mathematical reasoning. Proceedings of the National Academy of Sciences, 102(9):3177-3178, 2005.

Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei. Not all languages are created equal in llms: Improving multilingual capability by crosslingual-thought prompting. arXiv preprint arXiv:2305.07004, 2023.

Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023.

Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. Question translation training for better multilingual reasoning. CoRR, abs/2401.07817, 2024. doi: 10.48550/ ARXIV.2401.07817. URL https://doi.org/10.48550/arXiv.2401.07817

Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730-19742. PMLR, 2023.

Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations, 2023.

Zewei Sun, Mingxuan Wang, and Lei Li. Multilingual translation via grafting pre-trained language models. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 2735-2747. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.FINDINGS-EMNLP.233. URL https: //doi.org/10.18653/v1/2021.findings-emnlp. 233

Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, and Partha Talukdar. LLM augmented llms: Expanding capabilities through composition. CoRR, abs/2401.02412, 2024. doi: 10.48550/ARXIV.2401.02412. URLhttps://doi.org/10.48550/arXiv.2401.02412.

Dongkeun Yoon, Joel Jang, Sungdong Kim, Seungone Kim, Sheikh Shafayat, and Minjoon Seo. Langbridge: Multilingual reasoning without multilingual supervision. CoRR, abs/2401.10695, 2024. doi: 10.48550/ARXIV.2401.10695. URL https://doi.org/10.48550/arXiv. 2401 10695

Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= nZeVKeeFYf9

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 483-498. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.NAACL-MAIN.41. URLhttps://doi.org/10.18653/v1/2021.naacl-main. 41

Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xiang Ren. Common sense beyond english: Evaluating and improving multilingual language models for commonsense reasoning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 1274-1287. Association for Computational Linguistics, 2021. doi: 10.18653/V1/ 2021.ACL-LONG.102. URL https://doi.org/10.18653/v1/2021.acl-long. 102

Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018.

Fei Yuan, Yinquan Lu, Wenhao Zhu, Lingpeng Kong, Lei Li, Yu Qiao, and Jingjing Xu. Lego-mt: Learning detachable models for massively multilingual machine translation. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 11518-11533. Association for Computational Linguistics, 2023b. doi: 10.18653/V1/2023.FINDINGS-ACL.731. URL https://doi.org/10.18653/v1/2023.findings-acl.731.

Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Marilyn A. Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics, 2018. doi: 10.18653/V1/N18-1101. URL https://doi.org/10. $18653 / \mathrm{v} 1 / \mathrm{n} 18-1101$.

Zixian Huang, Ao Wu, Jiaying Zhou, Yu Gu, Yue Zhao, and Gong Cheng. Clues before answers: Generation-enhanced multiple-choice QA. In Marine Carpuat, Marie-Catherine de Marneffe, and Iván Vladimir Meza Ruíz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 3272-3287. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.NAACL-MAIN.239. URL https://doi.org/10.18653/v1/2022.naacl-main.239.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2381-2391. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1260. URL https://doi.org/10.18653/v1/d18-1260

Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. Think you have solved direct-answer question answering? try arc-da, the direct-answer AI2 reasoning challenge. CoRR, abs/2102.03315, 2021. URLhttps://arxiv.org/abs/2102.03315.

Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. QASC: A dataset for question answering via sentence composition. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8082-8090. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6319. URL https://doi.org/10.1609/aaai.v34i05.6319.

Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin. Beyond englishcentric multilingual machine translation. J. Mach. Learn. Res., 22:107:1-107:48, 2021. URL http://jmlr.org/papers/v22/20-1307.html

Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. mgpt: Few-shot learners go multilingual, 2022. URL https://arxiv.org/ $\mathrm{abs} / 2204.07580$

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. CoRR, abs/1911.02116, 2019. URL http://arxiv.org/abs/1911.02116

Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Y. Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling humancentered machine translation. CoRR, abs/2207.04672, 2022. doi: 10.48550/ARXIV.2207.04672. URLhttps://doi.org/10.48550/arXiv.2207.04672

Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, $9(11), 2008$.
