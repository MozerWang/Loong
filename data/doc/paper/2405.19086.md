# MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors 

Renzhi Wang ${ }^{1,2}$, Piji Li $^{1,2 *}$<br>${ }^{1}$ College of Computer Science and Technology,<br>Nanjing University of Aeronautics and Astronautics, China<br>${ }^{2}$ MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, China<br>${ }^{1}\{r z h w a n g$, pjli\}@nuaa.edu.cn


#### Abstract

Model editing aims to efficiently alter the behavior of Large Language Models (LLMs) within a desired scope, while ensuring no adverse impact on other inputs. Recent years have witnessed various model editing methods been proposed. However, these methods either exhibit poor overall performance or struggle to strike a balance between generalization and locality. We propose MEMoE, a model editing adapter utilizing a Mixture of Experts (MoE) architecture with a knowledge anchor routing strategy. MEMoE updates knowledge using a bypass MoE structure, keeping the original parameters unchanged to preserve the general ability of LLMs. And, the knowledge anchor routing ensures that inputs requiring similar knowledge are routed to the same expert, thereby enhancing the generalization of the updated knowledge. Experimental results show the superiority of our approach over both batch editing and sequential batch editing tasks, exhibiting exceptional overall performance alongside outstanding balance between generalization and locality. Our code will be available.


## 1 Introduction

Large Language Models [38, 47, 48] learn a vast repository of world knowledge during pre-training, which can be accessed and utilized through natural language prompts [39]. Despite this extensive base of information, the dynamic nature of the real-world demands regular updates to these models to correct outdated information or integrate new knowledge [51, 52]. However, frequently retraining or fine-tuning the LLMs to incorporate these updates is often impractical, given the substantial resources and time required [29, 52].

To address this, the concept of model editing, also known as knowledge editing, has been introduced [54]. This approach aims to efficiently modify the outputs of LLMs for target queries while preserving the overall performance for other unrelated inputs. Recent years have witnessed significant efforts in developing model editing techniques, with numerous methods proposed in various editing tasks and settings. For instance, specific approaches such as ROME [33] for single knowledge editing, MEMIT [34] for batch editing, and GRACE [16] for sequential editing have been introduced. Currently, evaluation for model editing revolves three dimensions: reliability, generality, and locality [51, 54]. To illustrate, suppose the original model predicts "Trump" for the input "Who is the president of the United States?" and the desired post-edit model prediction is "Joe Biden". To assess reliability, the same original statement is used as input to verify whether the post-edit model predicts "Joe Biden" as intended. For generality, a paraphrased statement like "Who currently holds the position of the U.S. presidency?" can be presented to the edited model to ensure consistent output modification to "Joe[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_6919f628b6104e81f363g-02.jpg?height=546&width=1290&top_left_y=236&top_left_x=426)

Figure 1: Left: Apart from our MEMoE, no method achieves both high accuracy and high balance. Right: Significant room for improvement in the overall performance of current methods.

Biden". Locality implies that the model output for an unrelated statement such as "What is the capital of the United States?" should remain unaffected. As illustrated in Figure 1 there is a significant disparity between the generality and locality scores of current methods, and their overall performance is suboptimal. None of the existing approaches have succeeded in simultaneously achieving high accuracy and high balance. This underscores the challenge of striking a balance between locality and generality in model editing and reveals ample opportunity for enhancing the overall performance.

In light of these, we propose MEMoE, a novel framework leveraging the MoE architecture alongside knowledge anchor routing to enhance the overall performance of model editing. Considering the intrinsic sparsity of knowledge information and the advantage of MoE in handling sparse features [3, 45], MEMoE extends upon a parallel MoE structure to enhance the accuracy of knowledge learning. This MoE-style adapter is confined to only one layer of the model, preserving all original parameters, thereby enhancing the locality of model editing and further reducing the impact on model's general ability. On the other hand, prior research has highlighted the considerable benefits stemming from the specialize feature of MoE's experts in multi-task learning, and pointed out that appropriate routing strategy can lead to improved generalization performance [57, 50]. We introduce the knowledge anchor routing strategy, enabling routers to selectively focus on specific knowledge aspects of the inputs, ensuring that queries requiring similar knowledge are routed to the same experts. Through this "professional people do professional things" approach, the generalization performance of MEMoE has been improved.

The main contributions of our work can be summarized as follows:

- We present MEMoE, a method utilizing a MoE architecture with a knowledge anchor routing strategy for model editing.
- Experiments show that our proposed method achieves state-of-the-art editing performance. MEMoE achieves high accuracy while effectively balancing generality and locality.
- We conducted further experiments to confirm that this method has minimal impact on model's general ability and present detailed analysis on various model settings.


## 2 Preliminaries of Model Editing

Based on the prior works [51, 54, 29], the task of model editing involves effectively modify an initial base model $f_{\theta}$ ( $\theta$ represents the model's parameters) into an edited model $f_{\theta^{\prime}}$. The goal is to adjust the model's responses to a set of specified edit instances as desired, while preserving its behavior on all other instances [29]. The intended edit descriptor is denoted as $\left\{\left(x_{i}^{e}, y_{i}^{e}\right)\right\}_{i \in[1, N]}$, where $f_{\theta}\left(x_{i}^{e}\right) \neq y_{i}^{e}$. This set of intended instances is referred to as the editing scope $I_{\text {edit }}$, while the out-of-s cope $O_{e d i t}$ refers to inputs set that are not relevant to the editing examples. Formally, a
successful edit can be expressed as:

$$
f_{\theta^{\prime}}\left(x_{i}\right)= \begin{cases}y_{i}^{e} & \text { if } x_{i} \in I_{e d i t}  \tag{1}\\ f_{\theta}\left(x_{i}\right) & \text { if } x_{i} \in O_{e d i t}\end{cases}
$$

Problem settings for model editing usually fall into four categories [51, 29]:

1) Single Editing assesses model performance after a single knowledge update.:

$$
\begin{equation*}
\theta^{\prime} \leftarrow \underset{\theta}{\operatorname{argmin}}\left(\left\|f_{\theta}\left(x_{i}^{e}\right)-y_{i}^{e}\right\|\right) \tag{2}
\end{equation*}
$$

2) Batch Editing assesses model performance when multiple knowledge pieces are modified simultaneously ( $n \leq N$ represents the batch size):

$$
\begin{equation*}
\theta^{\prime} \leftarrow \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^{n}\left(\left\|f_{\theta}\left(x_{i}^{e}\right)-y_{i}^{e}\right\|\right) \tag{3}
\end{equation*}
$$

3) Sequential Editing requires that every single edit is executed successively and evaluation conducted only after all edits are completed [16]:

$$
\begin{equation*}
\theta^{\prime} \leftarrow \underset{\theta}{\operatorname{argmin}} \sum_{i=1}^{N}\left(\left\|f_{\theta}\left(x_{i}^{e}\right)-y_{i}^{e}\right\|\right) \tag{4}
\end{equation*}
$$

4) Sequential Batch Editing aims to perform edits in a sequential manner and in batches ( $n$ represents the batch size, $S$ represents the sequential editing step):

$$
\begin{equation*}
\theta^{\prime} \leftarrow \underset{\theta}{\operatorname{argmin}} \sum_{s=0}^{S} \sum_{i=s \times n}^{(s+1) \times n}\left(\left\|f_{\theta}\left(x_{i}^{e}\right)-y_{i}^{e}\right\|\right) \tag{5}
\end{equation*}
$$

Based on the above settings, a successful model editor should meet requirements of the following three properties: Reliability, Generality, and Locality [51]. Formally, these can be expressed as [54]:

1) Reliability measures the average accuracy of the post-edit model $f_{\theta^{\prime}}$ on intended edits:

$$
\begin{equation*}
\mathbb{E}_{\left(x_{i}^{e}, y_{i}^{e}\right) \sim I_{\text {edit }}} \mathbb{1}\left\{\operatorname{argmax}_{y} f_{\theta^{\prime}}\left(y \mid x_{i}^{e}\right)=y_{i}^{e}\right\} \tag{6}
\end{equation*}
$$

2) Generality measures the average accuracy of the model $f_{\theta^{\prime}}$ on examples drawn uniformly from the equivalence neighborhood $N_{\text {edit }}$ which includes input/output pairs related to $I_{\text {edit }}$ :

$$
\begin{equation*}
\mathbb{E}_{\left(x_{i}, y_{i}^{e}\right) \sim N_{e d i t}} \mathbb{1}\left\{\operatorname{argmax}_{y} f_{\theta^{\prime}}\left(y \mid x_{i}\right)=y_{i}^{e}\right\} \tag{7}
\end{equation*}
$$

3) Locality is evaluated by the rate at which the predictions of the post-edit model $f_{\theta^{\prime}}$ remain unchanged compared to the pre-edit model $f_{\theta}$ :

$$
\begin{equation*}
\mathbb{E}_{\left(x_{i}, y_{i}\right) \sim O_{e d i t}} \mathbb{1}\left\{f_{\theta^{\prime}}\left(y \mid x_{i}\right)=f_{\theta}\left(y \mid x_{i}\right)\right\} \tag{8}
\end{equation*}
$$

## 3 Methodology

In this section, we provide a detailed introduction to MEMoE, a model editing adapter based on MoE structure and knowledge anchor routing strategy, as shown in Figure 2 This method achieves a balance between generality and locality, while enabling highly precise model editing.

### 3.1 MEMoE Architecture

One of the core ideas of MEMoE is to introduce several MOE-style experts via bypasses to facilitate knowledge updates and learning, while freezing all the original parameters of LLM to maintain its general ability to the greatest extent. The right of Figure 2 illustrates the forward process of MEMoE, sharing both similarities and distinctions with traditional MoE depicted on the left.

Similar to traditional MoE, MEMoE employs a structure that integrates multiple parallel experts within the transformer feed-forward network (FFN). The choice to use the FFN module is not only

![](https://cdn.mathpix.com/cropped/2024_06_04_6919f628b6104e81f363g-04.jpg?height=664&width=1374&top_left_y=248&top_left_x=381)

Figure 2: The architecture of MEMoE, compared with conventional MoE. Same color denote inputs requiring same knowledge. Pentagrams symbolize knowledge anchors within the input sentences, while squares and triangles represent ordinary input tokens during editing process and generality evaluation respectively. The distribution of tokens within the FFN illustrates that knowledge anchor consolidate inputs requiring same knowledge to the same experts.

due to its traditional role in $\mathrm{MoE}$ [20] but also aligns with recent experimental findings of knowledge probing technologies that the MLP layers within FFN store knowledge [7, 33, 34].

Differently, MEMoE incorporate the additional parallel experts through a bypass mechanism, thereby preserving the original parameters of the model to enhance the locality of model editing. This bypass structure also provides potential to further enhance the generalization performance of model editing (more details in $\$ 3.2$. Moreover, this adaptation is applied to only one layer of the model. The selective addition of adapters in one layer is based on two considerations: first, previous model editing techniques have demonstrated that modifying parameters in a single layer can effectively achieve knowledge updates [33, 34]; second, this strategy further maintains the structure and parameters of the original model, ensuring its general ability is preserved to the greatest extent.

Specifically, given token $x_{i}$ in the input sequence $X=\left\{x_{i}\right\}_{i=1}^{L}$, MEMoE with $E$ experts first introduces a gate decision vector $\mathcal{G} \in \mathbb{R}^{E}$ that dispatches different input tokens to different experts, which is calculated as:

$$
\begin{equation*}
\mathcal{G}=\operatorname{top}_{k}\left(\operatorname{softmax}\left(\mathbf{W}_{g} \cdot R\left(x_{i}\right)+\epsilon\right)\right) \tag{9}
\end{equation*}
$$

where $R(\cdot)$ defines a routing strategy for gate decision (more details in $\$ 3.2), \mathbf{W}_{g}$ is the trainable weights in gate decision, while $\epsilon$ denotes the noise term. The top ${ }_{k}(\cdot)$ operator zeros out all but the top- $k$ values. After getting the gate decision vector $\mathcal{G}$, the corresponding output $h_{i}$ is generated through a weighted aggregation of each expert's computation on $x_{i}$, as follows:

$$
\begin{equation*}
h_{i}=\sum_{e=1}^{E} \mathcal{G}_{e} \cdot \mathbf{W}_{e} \cdot x_{i} \tag{10}
\end{equation*}
$$

where $\mathbf{W}_{\mathbf{e}}$ is the linear projection weights of the $e$-th expert and gate decision $\mathcal{G}_{e}$ determines how much the $e$-th expert contributes to the output $h_{i}$. Note that, experts with $\mathcal{G}_{e}=0$ does not need to be computed for saving computation.

Overall, the forward process of the MEMoE layer, combined with the frozen original parameters $\mathbf{W}_{0}$, can be expressed as:

$$
\begin{equation*}
h_{i}=\mathbf{W}_{0} \cdot x_{i}+\lambda \sum_{e=1}^{E} \mathcal{G}_{e} \cdot \mathbf{W}_{e} \cdot x_{i} \tag{11}
\end{equation*}
$$

where $\lambda$ is a non-negative weighting coefficient used to balance the old and new knowledge.

### 3.2 Knowledge Anchor Routing

Another core idea of MEMoE is the routing strategy based on the knowledge anchors. Inspired by the specialize nature of experts in MoE architecture [53], we aim to route inputs requiring similar knowledge to the same expert during both training and testing phases, thereby enhancing the model's generalization performance when dealing with new knowledge.

In MEMoE, we define the named entities in input sentences as "knowledge anchors". For example, in the input "Who is the president of the United States?" the entities "president" and "United States" serve as knowledge anchors. The routing strategy allocate tokens to the appropriate experts based on these anchors, ensuring that inputs requiring similar knowledge are routed to the same expert. The effect is demonstrated in Figure 2, showing the token distribution within the FFN. This approach better captures and retains the semantic associations of knowledge in input data. Consequently, it enhances the model's generalization performance when handling knowledge and also optimizes the efficiency of expert utilization to a certain extent (as validated in $\$ 5.2$.

Specifically, given an input sequence $X=\left\{x_{i}\right\}_{i=1}^{L}$, we first identify the named entities $x_{\text {anchor }}$ within $X$ using named entity recognition (NER) techniques. We obtain the vector representation of the identified entity through the model's embedding layer, denoted as embed $\left(x_{\text {anchor }}\right)$. To help the gate function notice the knowledge anchor, we use the combination of the anchor embedding and the local token representation. Overall, the knowledge anchors routing strategy can be expressed as:

$$
\begin{equation*}
R_{\text {anchor }}\left(x_{i}\right)=\operatorname{concat}\left(x_{i}, \operatorname{embed}\left(x_{\text {anchor }}\right)\right) \tag{12}
\end{equation*}
$$

Additionally, to address the common issue of expert utilization imbalance in MoE, whether caused by the introduction of knowledge anchor or not, we adopt the auxiliary loss [11] for balancing the top-k selection of routing.

## 4 Experiments

In this section, we first describe our experimental setup. Then, we show the remarkable performance of MEMoE on two challenging model editing tasks: batch editing and sequential batch editing.

### 4.1 Experimental Setups

Datasets and Metrics We use two prominent model editing datasets: ZsRE [26] and CounTERFACT [33], with the split provided by [54, 51]. ZsRE is a context-free Question Answering (QA) dataset built upon zero-shot relation extraction and COUNTERFACT is a more challenging dataset that accounts for counter facts that start with low scores in comparison to correct facts. Further details are provided in Appendix C. In terms of evaluation metrics, we use the three metrics mentioned in $\$ 2$. Reliability, Generality, and Locality, along with the average scores over these metrics.

Baselines We compare the proposed method with mainstream model editing methods, which can be categorized into the following four types [51]:

- Fine-tuning based methods: FT-L [33], FT-M [16], and LoRA[18]. FT-L directly fine-tunes a single layer's FFN and FT-M is a small variation of FT-L using a different loss computation procedure. LoRA is a parameter-efficient fine-tuning method which decomposes the update gradient matrix into two small rank matrices.
- Locate and edit methods: MEMIT [34]. MEMIT treats the feed-forward layer of transformer as a linear associative memory and uses a minimum square error optimization to add new key-value associations to layer weights.
- Meta-learning methods: MEND [35] and COMEBA-HK [29]. MEND learns a hyper-network using additional training data to transform gradient obtained by standard fine-tuning, while COMEBA-HK (COMEBA for short) develop hook layers to identify the editing scope.
- Memory based methods: SERAC [36] and GRACE [16]. SERAC uses an external cache to store explicit editing cases, while GRACE preserves the original model parameters and adopts a codebook to store relevant edits.

Table 1: Batch editing results. Bold is the best result, and underline is the second-best result.

| Method | Model | ZsRE |  |  |  | COUNTERFACT |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Reliability $\uparrow$ | Generality $\uparrow$ | Locality $\uparrow$ | Average $\uparrow$ | $\overline{\text { Reliability } \uparrow}$ | Generality $\uparrow$ | Locality $\uparrow$ | Average |
| FT-L |  | 16.85 | 16.34 | 71.55 | 34.91 | 0.27 | 0.34 | 85.18 | 28.60 |
| FT-M |  | 17.95 | 17.32 | 71.26 | 35.51 | 0.36 | 0.42 | 82.81 | 27.86 |
| LoRA |  | 30.10 | 29.08 | 80.54 | 46.57 | 5.64 | 3.46 | 69.45 | 26.18 |
| MEMIT | CDTา YI | 61.19 | 49.97 | 97.51 | 69.56 | 81.01 | 27.67 | 95.80 | 68.16 |
| MEND | GP12-XL | 2.16 | 2.11 | 20.34 | 8.20 | 0.13 | 0.03 | $\overline{4.22}$ | 1.46 |
| COMEBA |  | 82.21 | 66.61 | 99.40 | 82.74 | 88.28 | 40.38 | 97.66 | 75.44 |
| SERAC |  | 98.64 | $\overline{48.12}$ | 35.68 | $\overline{60.81}$ | 17.88 | 14.55 | 82.25 | $\overline{38.23}$ |
| GRACE |  | 95.56 | 39.76 | 99.93 | 78.41 | $\underline{94.23}$ | 32.56 | 94.58 | 73.79 |
| MEMoE |  | $\overline{95.69}$ | 88.18 | $\overline{100.0}$ | 94.62 | $\overline{93.78}$ | $\overline{85.15}$ | 100.0 | 92.98 |
| FT-L |  | 14.19 | 13.07 | 70.16 | 32.47 | 0.21 | 0.30 | 80.69 | 27.07 |
| FT-M |  | 16.57 | 15.62 | 70.15 | 34.11 | 0.29 | 0.38 | 81.83 | 27.50 |
| LoRA |  | 25.32 | 23.15 | 52.01 | 33.49 | 21.70 | 22.32 | 40.37 | 28.13 |
| MEMIT | LLaMA2-/B | 24.02 | 39.97 | 17.00 | 27.00 | 18.57 | 31.29 | 14.88 | 21.58 |
| MEND |  | 1.01 | 2.83 | 96.77 | 33.54 | 0.45 | 2.24 | 97.89 | 33.53 |
| SERAC |  | 89.08 | 16.29 | 81.82 | 62.39 | 80.67 | 17.34 | 82.05 | 60.02 |
| GRACE |  | 94.50 | 38.20 | 99.90 | 77.53 | 82.14 | 32.09 | 98.93 | 71.05 |
| MEMoE |  | $\overline{100.0}$ | 90.30 | $\overline{100.0}$ | $\overline{96.77}$ | $\overline{99.69}$ | $\overline{88.30}$ | $\overline{100.0}$ | $\overline{96.33}$ |

Table 2: Sequential batch editing results. Bold is the best result, and underline is the second-best.

| Method | Model | ZsRE |  |  |  | COUNTERFACT |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\overline{\text { Reliability } \uparrow}$ | Generality $\uparrow$ | Locality $\uparrow$ | Average $\uparrow$ | Reliability $\uparrow$ | Generality $\uparrow$ | Locality $\uparrow$ | Average $\uparrow$ |
| $\overline{\text { FT-L }}$ |  | 3.79 | 2.48 | 6.60 | 4.29 | 1.00 | 1.00 | 6.00 | 2.67 |
| FT-M |  | 8.92 | 8.41 | 6.22 | 7.85 | 4.00 | 3.50 | 5.50 | 4.33 |
| LoRA |  | 0.96 | 1.29 | 0.03 | 0.76 | 0.50 | 0.02 | 0.50 | 0.34 |
| MEMIT | GPT2_XI | 34.88 | 32.96 | 70.74 | 46.19 | 56.00 | 37.00 | 31.00 | 41.33 |
| MEND | GP12-XL | 20.95 | 18.29 | 93.69 | 47.01 | 0.01 | 0.00 | 0.08 | 0.03 |
| COMEBA |  | 66.91 | 56.11 | 97.23 | 73.42 | 86.00 | 38.00 | 59.00 | 61.00 |
| SERAC |  | 100.0 | $\overline{36.03}$ | 35.95 | $\overline{57.33}$ | 15.41 | $\overline{12.96}$ | 81.00 | 36.46 |
| GRACE |  | 100.0 | 0.04 | 100.0 | 66.68 | 100.0 | 0.40 | 100.0 | 66.80 |
| MEMoE |  | 74.69 | 58.18 | $\underline{98.93}$ | 77.27 | $\underline{88.12}$ | 54.78 | 99.45 | $\overline{80.78}$ |
| $\overline{\text { FT-L }}$ |  | 2.33 | 1.59 | 6.67 | 3.53 | 0.23 | 0.18 | 10.66 | 3.69 |
| FT-M |  | 6.72 | 4.37 | 7.78 | 6.29 | 0.33 | 0.70 | 8.54 | 3.19 |
| LoRA |  | 0.35 | 1.89 | 0.07 | 0.77 | 0.31 | 0.99 | 0.17 | 0.49 |
| MEMIT | LLaMA2-/B | 12.29 | 29.95 | 15.38 | 19.21 | 10.37 | 32.96 | 12.79 | 18.71 |
| SERAC |  | 67.78 | 33.98 | 34.55 | 45.44 | 20.21 | $\overline{14.05}$ | 34.90 | 23.05 |
| GRACE |  | 89.70 | 0.09 | 98.32 | 62.70 | 74.41 | 1.03 | $\underline{96.67}$ | 57.70 |
| MEMoE |  | 69.50 | 42.63 | $\overline{99.70}$ | $\overline{70.61}$ | 54.62 | 43.40 | $\overline{99.69}$ | 65.9 |

Implementation Details We select GPT2-XL and LLaMA2-7B as the base models. We opted for the more challenging model editing tasks: batch editing and sequential batch editing, to evaluate the performance of MEMoE. For batch editing, following [29], the batch size is set to 30 and the model is rolled back to the initial state after each batch editing. For sequential batch editing, the batch size is 10 for a total of 1000 edits, without rollback. Further details of the baselines and the implementation are provided in the Appendix D

### 4.2 Batch Editing

We first evaluate the effectiveness of MEMoE under batch editing settings. The evaluation results are presented in Table 1. For all models and all metrics, our method consistently achieves the best scores. MEMoE's reliability scores are all above 90 , generalization scores are all above 85 , and locality scores are perfect at 100. The improvements across various metrics are significant. Compared to GPT2-XL, our method demonstrates even more remarkable improvements on LLaMA2-7B, with a maximum improvement of up to 17.55 points in accuracy and 56.21 in generality. In the $\$ 5.2$, we conduct further experimental analysis to explore the reasons behind the substantial enhancement in generalization. Considering some current researches concern that model editing methods may significantly affect a model's general ability [14, 15, 40], we perform a more detailed general ability evaluation using a broader task datasets in $\$ 5.1$.
![](https://cdn.mathpix.com/cropped/2024_06_04_6919f628b6104e81f363g-07.jpg?height=718&width=1400&top_left_y=232&top_left_x=362)

Figure 3: Performance on general tasks of edited models using MEMoE, MEMIT and MEND, with different batch sizes for edits.

### 4.3 Sequential Batch Editing

We evaluate MEMoE on 1,000 samples from both datasets for sequential batch editing. The evaluation is conducted after the entire editing process is completed. The results, shown in Table 2 , indicate that MEMoE achieves the best scores in most cases. It only ranks second because GRACE [16] achieves perfect scores of 100 in some matrices. Similar to the batch editing results, MEMoE performs better on LLaMA2-7B, demonstrating a significant advantage in generality while maintaining accuracy and locality much close to 100 . Regarding GRACE, it excels in reliability and locality but performs poorly in generality due to its use of a codebook to memorize encountered editing instances [16, 12]. However, its poor performance in generality suggests a problem with regurgitation. Overall, the reliability and generality consistently lag behind in comparison to batch editing, indicating there is still room for improvement in this field.

## 5 Detailed Analysis and Discussion

In this section, we conduct a further evaluation and analysis of the performance of MEMoE. Firstly, we assess the impact of MEMoE on the model's general ability using a broader range of task datasets. Secondly, we show potential sources of MEMoE's generalization advantage through an analysis of expert specialize phenomena. Finally, we present an extensive serious of ablation study to evaluate the efficacy of various model configurations, including the number of experts, the target layer and the routing strategies.

### 5.1 General Ability Test

To investigate the potential impact of model editing on the general ability of LLMs, we select eight representative task categories for evaluation, as outlined below following [14]. For reasoning, we utilized the GSM8K dataset [5], with performance assessed by solve rate. Natural language inference (NLI) tasks were evaluated on the RTE dataset [1], with accuracy measured through twoway classification. For open-domain question answering, the Natural Question dataset [23] was employed, evaluating exact match against reference answers after minor normalization as in [2] and [24]. Similarly, closed-domain QA tasks were assessed using the BoolQ dataset [4], also measured by EM. Dialogue evaluation utilized the MuTual dataset [6], with results determined by selecting the most suitable response from four options, denoted as Recall $4_{4}$ 1 [31]. Evaluation for summarization tasks was conducted on the SAMSum dataset [13], using the average of ROUGE-1, ROUGE-2, and ROUGE-L as evaluation metrics. For named entity recognition (NER), the CoNLL03 dataset

Table 3: Expert Specific Experimental Results. "Dynamic" indicates that we dynamically select input data, while "Static" refers to evaluation conducted using models trained on previous experiments. "id" stands for group id. "Similar" refers to similar knowledge, and "same" refers to same knowledge.

| Type | Number of data from each group |  |  |  |  | Consistency $\uparrow$ |  | Generality $\uparrow$ | Reliability $\uparrow$ | Locality ${ }^{\uparrow}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | id $=1$ | id $=\mathbf{2}$ | id $=3$ | id $=4$ | id $=5$ | similar | same |  |  |  |
| Dynamic | 10 | 10 | 10 | 10 | 10 | 63.09 | 73.77 | 0 | 9.84 | 100 |
|  | 20 | 10 | 10 | 10 | 0 | 65.47 | 73.79 | 8 |  |  |
|  | 30 | 10 | 10 | 0 | 0 | 69.67 | 76.33 | 90.12 | 99.84 | 100 |
|  | 40 | 10 | 0 | 0 | 0 | 74.63 | 80.13 | 92.01 | 99.84 | 100 |
|  | 50 | ![](https://cdn.mathpix.com/cropped/2024_06_04_6919f628b6104e81f363g-08.jpg?height=42&width=70&top_left_y=612&top_left_x=630) | 0 | 0 | 0 | 79.69 | 84.82   | 94.78 | 99.84 | 100 |
| Static | ![](https://cdn.mathpix.com/cropped/2024_06_04_6919f628b6104e81f363g-08.jpg?height=43&width=85&top_left_y=643&top_left_x=532) | - | - | - | - | 65.14 | 77.77 | 90.00 | 99.84 | 100 |

![](https://cdn.mathpix.com/cropped/2024_06_04_6919f628b6104e81f363g-08.jpg?height=468&width=1390&top_left_y=730&top_left_x=365)

Figure 4: Left: Performance across different numbers of experts. Middle: Performance across different target model layers. Right: Effectiveness of activating experts. All experiments are based on LLaMA2-7B, utilizing the ZsRE dataset and batch editing settings.

[42] was employed, with performance measured using entity-level F1-score. Lastly, for sentiment analysis, we utilized SST2 dataset [46], with accuracy assessed through a two-way classification.

We conduct evaluations on LLaMA2-7B based on batch editing settings, progressively increasing the batch size to show the impact of more edited samples (the model is rolled back to the initial state after each batch editing). The results are shown in the Figure 3. It is important to note that any increase or decrease in performance metrics implies an impact on model's general ability. Compared to the MEMIT and MEND, the MEMoE yields consistently stable model performance under various batch editing conditions. With the increase in batch size and edited samples, both MEMIT and MEND significantly diminish the model's general ability, while the influence of MEMoE fluctuates within a smaller range. This further corroborates MEMoE's advantage in locality score in $\$ 4.2$.

### 5.2 Experts Specialize

As claimed in Section 3.2, we hypothesize that knowledge anchor routing has the ability to direct inputs requiring similar or same knowledge to the same expert, thereby improving the generalization performance. This hypothesis comprises two parts: (1) same knowledge being handled by the same expert, and (2) similar knowledge being handled by the same expert. We proceed to verify it.

In this context, we define "inputs requiring same knowledge" to specifically denote editing inputs and generalization testing inputs that target the same knowledge. For example, training input like "Who is the president of the United States?" and generalization test input like "Who currently holds the position of the U.S. presidency?" require the same knowledge: "Joe Biden is the president of the United States". In contrast, "similar knowledge" refers to inputs that contain semantically related knowledge (based on semantic distance). We apply the simple traditional K-means algorithm to cluster all the data in ZsRE into 5 groups based on the cosine similarity, defining knowledge with the same group id as similar.

We design the metric Consistency to evaluate the extend to which similar or same knowledge is processed by the same experts. Specifically, for an input sequence $S_{k}=\left\{x_{i}\right\}_{i=1}^{L}$ in group $k$ and
the expert $E_{k}$ handling the majority of knowledge in this group is used as the ground truth, the Consistency of this group is defined as:

$$
\begin{equation*}
\mathbb{E}_{x_{i} \sim S_{k}} \mathbb{1}\left\{E_{k}=R_{\text {anchor }}\left(x_{i}\right)\right\} \tag{13}
\end{equation*}
$$

where $R_{\text {anchor }}$ is the knowledge anchor routing defined by Equation 12. The ground truth for generalization test input is the expert processing the corresponding training input that contains the same knowledge. The overall consistency score is calculated as the average across all groups.

In order to fully analyze the behavior of experts routing when dealing with different knowledge inputs, we first conduct a static analysis based the trained MEMoE module in 4.2. That is to say, at this stage, the editing data is fixed; we analyze the behavior of experts solely through the model inference process. Further, we dynamically select the knowledge to be edited as input to observe the behavior of knowledge anchor routing. The results are shown in Table 3. As the concentration of the input knowledge categories increases, the consistency of the experts also improves, and the generality simultaneously rise. Additionally, the consistency of the same experts handling the same knowledge is higher and closely aligned with the generalization scores. Given the near-perfect accuracy score, we speculate that errors in generalization assessments may be due to incorrect routing of inputs to the wrong expert. Analysis of the bad cases in the generalization test supports this hypothesis. By adhering to the principle that "professional people do professional things", the strategy of routing inputs requiring similar or same knowledge to the same expert proves effective in improving knowledge generalization.

### 5.3 Ablation Study

Effect of Expert Number How does the number of experts impact the performance of MEMoE? The left plot in Figure 4 illustrates the performance of MEMoE with different numbers of experts. Due to computational resource limitations, we could only add up to 6 additional experts. Aligning with the primary experimental results in $\$ 4$, we set the $t o p_{k}$ value to 1 . We find that the reliability and locality of model editing do not change with the number of experts; there is neither a decrease nor an improvement in performance. However, the generalization of knowledge fluctuated with the number of experts, peaking when the number of experts is 4. Inspired by [53], we hypothesize that this optimal experts number is related to editing batch size. When the number of editing samples is not large, more experts may introduce interference, thereby reducing the generalization performance.

Effect of Target Layer What is the optimal layer for applying MEMoE? As shown in the middle of Figure 4, similar to the results of experts number experiment, the reliability and locality score remain unaffected, with only the generality score exhibiting fluctuations and peaking at the 16th layer. The best editing layer identified from these validation experiments aligns with the results obtained using knowledge probes technology in [33]. Combining the findings from above experiments, we can infer that the accuracy and locality performance are inherently guaranteed by the characteristics of the bypass MoE structure, which is consistent with the design principles of MEMoE discussed in $\$ 3.1$.

Routing Strategy: Soft vs Discrete What is the best routing strategy in MEMoE? In Figure 4 . the rightmost plot illustrates the performance when using various routing strategies for MEMoE. Specifically, we compare the soft merging [53] of experts with discrete top-1, top-2, and top-3 routing strategy. The top-1 routing setting yields the best performance. Further, based on the experiment introduced in $\$ 5.2$, we examined the consistency score of routing under different values of $k$. We observed that as $\mathrm{k}$ increases, more experts participate in the computation, but the consistency score decreases. We believe that this inconsistency in expert utilization leads to the decrease in generalization performance. Further experimental results, detailed in the Appendix E. demonstrate that regardless of the expert number and the target layer, the top-1 performance remains the best. In addition, the discrete top-1 routing has an advantage in computational efficiency by requiring only one experts to be activated during inference.

Batch Editing vs Sequential Editing From Table 1 and 2 , it is evident that MEMoE demonstrates a significant performance advantage in batch editing tasks over sequential editing. To further evaluate MEMoE's ability in batch editing, we progressively increased the batch size. Experimental results are shown in Table 4. When the batch size for batch editing reached 1000, equivalent to the total number of sequential batch editing, MEMoE exhibited significant performance advantages. Reliability and

Table 4: Comparison of batch editing with larger batch size and sequential batch editing.

| Task Settings | Size | Reliability $\uparrow$ | Generality $\uparrow$ | Locality $\uparrow$ | Average $\uparrow$ |
| :--- | :--- | :---: | :---: | :---: | :---: |
|  | 10 | 100.0 | 90.12 | 100.0 | 96.71 |
| Batch Editing | 100 | 99.84 | 80.91 | 100.0 | 93.58 |
|  | 1000 | 99.30 | 75.70 | 100.0 | 91.67 |
| Sequential Batch Editing | 1000 | 69.50 | 42.63 | 99.70 | 70.61 |

locality are scarcely affected by the increase in batch size, maintaining close to 100. Meanwhile, the generality score surpassed by 33.07 points, highlighting MEMoE's performance advantage in batch editing. As for the decline in sequential editing, our analysis of bad cases indicates that can be attributed to catastrophic forgetting: edits complete earlier are more prone to errors.

## 6 Conclusion

In this paper, we present MEMoE, a model editing adapter utilizing MoE architecture with knowledge anchor routing strategy. MEMoE updates knowledge using a bypass MoE structure, keeping the original parameters unchanged to preserve the model's general ability. And, the knowledge anchor ensures that questions requiring similar knowledge are handled by the same expert, thereby enhancing the generalization of the updated knowledge. Experiment results demonstrate that our method significantly surpasses all compared baselines. MEMoE shows near-perfect accuracy and locality scores close to 100 , along with a generalization score exceeding 90 , indicating exciting promise for practical applications of model editing technology.

## Acknowledgements

This research is supported by the National Natural Science Foundation of China (No.62106105), the CCF-Baidu Open Fund (No.CCF-Baidu202307), the Scientific Research Starting Foundation of Nanjing University of Aeronautics and Astronautics (No.YQR21022), and the High Performance Computing Platform of Nanjing University of Aeronautics and Astronautics.

## References

[1] J. Q. Candela, I. Dagan, B. Magnini, and F. d'Alché-Buc, editors. Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, volume 3944 of Lecture Notes in Computer Science, 2006. Springer. ISBN 3-540-33427-0. doi: 10.1007/11736790. URL https://doi.org/10.1007/11736790.

[2] D. Chen, A. Fisch, J. Weston, and A. Bordes. Reading wikipedia to answer open-domain questions. In R. Barzilay and M. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 1870-1879. Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1171. URL https://doi.org/10.18653/v1/P17-1171.

[3] Z. Chen, Y. Deng, Y. Wu, Q. Gu, and Y. Li. Towards understanding the mixture-of-experts layer in deep learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html.

[4] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT

2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 29242936. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1300. URL https://doi.org/10.18653/v1/n19-1300.

[5] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.

[6] L. Cui, Y. Wu, S. Liu, Y. Zhang, and M. Zhou. Mutual: A dataset for multi-turn dialogue reasoning. In D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 1406-1416. Association for Computational Linguistics, 2020. doi: 10.18653/ V1/2020.ACL-MAIN.130. URL https://doi.org/10.18653/v1/2020.acl-main. 130.

[7] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei. Knowledge neurons in pretrained transformers. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8493-8502. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.581. URL https: //doi.org/10.18653/v1/2022.acl-long.581.

[8] Q. Dong, D. Dai, Y. Song, J. Xu, Z. Sui, and L. Li. Calibrating factual knowledge in pretrained language models. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 5937-5947. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.FINDINGS-EMNLP.438. URL https://doi.org/10.18653/v1/2022 findings-emnlp. 438

[9] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. S. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui. Glam: Efficient scaling of language models with mixture-of-experts. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, and S. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 5547-5569. PMLR, 2022. URL https://proceedings.mlr.press/v162/du22c.html.

[10] D. Eigen, M. Ranzato, and I. Sutskever. Learning factored representations in a deep mixture of experts. In Y. Bengio and Y. LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings, 2014. URL http://arxiv.org/abs/1312.4314.

[11] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23:120:1-120:39, 2022. URL http: //jmlr.org/papers/v23/21-0998.html.

[12] C. Gao, K. Chen, J. Rao, B. Sun, R. Liu, D. Peng, Y. Zhang, X. Guo, J. Yang, and V. S. Subrahmanian. Higher layers need more lora experts. CoRR, abs/2402.08562, 2024. doi: 10.48550/ARXIV.2402.08562. URL https://doi.org/10.48550/arXiv.2402.08562.

[13] B. Gliwa, I. Mochol, M. Biesek, and A. Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. CoRR, abs/1911.12237, 2019. URL http://arxiv org/abs/1911.12237.

[14] J. Gu, H. Xu, J. Ma, P. Lu, Z. Ling, K. Chang, and N. Peng. Model editing can hurt general abilities of large language models. CoRR, abs/2401.04700, 2024. doi: 10.48550/ARXIV.2401. 04700. URLhttps://doi.org/10.48550/arXiv.2401.04700.

[15] A. Gupta, A. Rao, and G. Anumanchipalli. Model editing at scale leads to gradual and catastrophic forgetting. CoRR, abs/2401.07453, 2024. doi: 10.48550/ARXIV.2401.07453. URL https://doi.org/10.48550/arXiv.2401.07453

[16] T. Hartvigsen, S. Sankaranarayanan, H. Palangi, Y. Kim, and M. Ghassemi. Aging with GRACE: lifelong model editing with discrete key-value adaptors. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 95b6e2ff961580e03c0a662a63a71812-Abstract-Conference.html

[17] H. Hazimeh, Z. Zhao, A. Chowdhery, M. Sathiamoorthy, Y. Chen, R. Mazumder, L. Hong, and E. H. Chi. Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 29335-29347, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ f5ac21cd0ef1b88e9848571aeb53551a-Abstract.html.

[18] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.

[19] Z. Huang, Y. Shen, X. Zhang, J. Zhou, W. Rong, and Z. Xiong. Transformer-patcher: One mistake worth one neuron. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=4oYUGeGBPm.

[20] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural Comput., 3(1):79-87, 1991. doi: 10.1162/NECO.1991.3.1.79. URL https: //doi.org/10.1162/neco.1991.3.1.79

[21] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de Las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mixtral of experts. CoRR, abs/2401.04088, 2024. doi: 10.48550/ARXIV.2401.04088. URL https://doi.org/10.48550/arXiv. 2401. 04088

[22] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.

[23] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452-466, 2019. doi: 10.1162/TACL\A\_00276. URL https://doi.org/10.1162/tacl_a_00276

[24] K. Lee, M. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain question answering. In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 6086-6096. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1612. URL https://doi.org/10. $18653 / \mathrm{v} 1 / \mathrm{p} 19-1612$

[25] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb.

[26] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer. Zero-shot relation extraction via reading comprehension. In R. Levy and L. Specia, editors, Proceedings of the 21st Conference on

Computational Natural Language Learning (CoNLL 2017), Vancouver, Canada, August 3-4, 2017, pages 333-342. Association for Computational Linguistics, 2017. doi: 10.18653/V1/ K17-1034. URL https://doi.org/10.18653/v1/K17-1034.

[27] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer. BASE layers: Simplifying training of large, sparse models. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 6265-6274. PMLR, 2021. URL http://proceedings.mlr.press/v139/lewis21a.html.

[28] D. Li, A. S. Rawat, M. Zaheer, X. Wang, M. Lukasik, A. Veit, F. X. Yu, and S. Kumar. Large language models with controllable working memory. In A. Rogers, J. L. Boyd-Graber, and N. Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1774-1793. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-ACL.112. URL https://doi.org/10.18653/ v1/2023.findings-acl. 112

[29] S. Li, Y. Deng, D. Cai, H. Lu, L. Chen, and W. Lam. Consecutive model editing with batch alongside hook layers. CoRR, abs/2403.05330, 2024. doi: 10.48550/ARXIV.2403.05330. URL https://doi.org/10.48550/arXiv.2403.05330

[30] X. Li, S. Li, S. Song, J. Yang, J. Ma, and J. Yu. PMET: precise model editing in a transformer. CoRR, abs/2308.08742, 2023. doi: 10.48550/ARXIV.2308.08742. URL https://doi.org/ 10.48550/arXiv. 2308.08742 .

[31] R. Lowe, N. Pow, I. Serban, and J. Pineau. The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. In Proceedings of the SIGDIAL 2015 Conference, The 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2-4 September 2015, Prague, Czech Republic, pages 285-294. The Association for Computer Linguistics, 2015. doi: 10.18653/V1/W15-4640. URL https://doi.org/10.18653/v1/ $\mathrm{w} 15-4640$.

[32] A. Madaan, N. Tandon, P. Clark, and Y. Yang. Memory-assisted prompt editing to improve GPT-3 after deployment. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 2833-2861. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.183. URL https://doi.org/10.18653/v1/2022.emnlp-main.183.

[33] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in GPT. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html

[34] K. Meng, A. S. Sharma, A. J. Andonian, Y. Belinkov, and D. Bau. Mass-editing memory in a transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview net/pdf?id=MkbcAHIYgyS

[35] E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning. Fast model editing at scale. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id= ODcZxeWfOPt

[36] E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, and C. Finn. Memory-based model editing at scale. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, and S. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 1581715831. PMLR, 2022. URLhttps://proceedings.mlr.press/v162/mitchell22a.html

[37] S. Murty, C. D. Manning, S. M. Lundberg, and M. T. Ribeiro. Fixing model bugs with natural language patches. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11600-11613. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.797. URL https://doi.org/10.18653/v1/2022.emnlp-main.797.

[38] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303. 08774. URL https://doi.org/10.48550/arXiv.2303.08774.

[39] F. Petroni, T. Rocktäschel, S. Riedel, P. S. H. Lewis, A. Bakhtin, Y. Wu, and A. H. Miller. Language models as knowledge bases? In K. Inui, J. Jiang, V. Ng, and X. Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2463-2473. Association for Computational Linguistics, 2019. doi: 10.18653/V1/D19-1250. URL https://doi.org/10.18653/v1/ D19-1250

[40] Y. Pinter and M. Elhadad. Emptying the ocean with a spoon: Should we edit models? In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 15164-15172. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-EMNLP.1012. URL https://doi.org/10.18653/v1/2023.findings-emnlp.1012.

[41] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. S. Pinto, D. Keysers, and N. Houlsby. Scaling vision with sparse mixture of experts. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 8583-8595, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 48237d9f2dea8c74c2a72126cf63d933-Abstract.html.

[42] E. F. T. K. Sang and F. D. Meulder. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In W. Daelemans and M. Osborne, editors, Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003, Edmonton, Canada, May 31 - June 1, 2003, pages 142-147. ACL, 2003. URL https://aclanthology.org/W03-0419/

[43] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. URL http://arxiv.org/abs/ 1910.01108

[44] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum? id=B1ckMDqlg.

[45] S. Shen, L. Hou, Y. Zhou, N. Du, S. Longpre, J. Wei, H. W. Chung, B. Zoph, W. Fedus, X. Chen, et al. Mixture-of-experts meets instruction tuning: A winning combination for large language models. arXiv preprint arXiv:2305.14705, 2023. URL https://arxiv.org/abs/ 2305.14705

[46] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1631-1642. ACL, 2013. URL https://aclanthology. org/D13-1170/

[47] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and
efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/ARXIV. 2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.

[48] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288, 2023. doi: 10.48550/ARXIV.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288

[49] R. Wang and P. Li. Semantic are beacons: A semantic perspective for unveiling parameterefficient fine-tuning in knowledge learning. arXiv preprint arXiv:2405.18292, 2024. URL https://arxiv.org/abs/2405.18292.

[50] Y. Xie, S. Huang, T. Chen, and F. Wei. Moec: Mixture of expert clusters. In B. Williams, Y. Chen, and J. Neville, editors, Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 13807-13815. AAAI Press, 2023. doi: 10.1609/AAAI.V37I11.26617. URL https://doi.org/10.1609/aaai.v37i11.26617.

[51] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang. Editing large language models: Problems, methods, and opportunities. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 10222-10240. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.632. URL https://doi.org/10.18653/v1/2023.emnlp-main.632.

[52] Z. Yao, Y. He, T. Qi, and M. Li. Scalable model editing via customized expert networks. CoRR, abs/2404.02699, 2024. doi: 10.48550/ARXIV.2404.02699. URL https://doi.org/ 10.48550/arXiv. 2404.02699 .

[53] T. Zadouri, A. Üstün, A. Ahmadian, B. Ermis, A. Locatelli, and S. Hooker. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. CoRR, abs/2309.05444, 2023. doi: 10.48550/ARXIV.2309.05444. URL https://doi.org/10. 48550/arXiv. 2309.05444

[54] N. Zhang, Y. Yao, B. Tian, P. Wang, S. Deng, M. Wang, Z. Xi, S. Mao, J. Zhang, Y. Ni, S. Cheng, Z. Xu, X. Xu, J. Gu, Y. Jiang, P. Xie, F. Huang, L. Liang, Z. Zhang, X. Zhu, J. Zhou, and H. Chen. A comprehensive study of knowledge editing for large language models. CoRR, abs/2401.01286, 2024. doi: 10.48550/ARXIV.2401.01286. URL https: //doi.org/10.48550/arXiv. 2401.01286

[55] C. Zheng, L. Li, Q. Dong, Y. Fan, Z. Wu, J. Xu, and B. Chang. Can we edit factual knowledge by in-context learning? In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 4862-4876. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.emnlp-main. 296

[56] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Y. Zhao, A. M. Dai, Z. Chen, Q. V. Le, and J. Laudon. Mixture-of-experts with expert choice routing. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 2f00ecd787b432c1d36f3de9800728eb-Abstract-Conference.html

[57] J. Zhu, X. Zhu, W. Wang, X. Wang, H. Li, X. Wang, and J. Dai. Uni-perceivermoe: Learning sparse generalist models with conditional moes. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 11fc8c98b46d4cbdfe8157267228f7d7-Abstract-Conference.html

[58] S. Zuo, X. Liu, J. Jiao, Y. J. Kim, H. Hassan, R. Zhang, J. Gao, and T. Zhao. Taming sparsely activated transformer with stochastic experts. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=B72HXs80q4
