# Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models 

Jiaqi Li ${ }^{1,3 *}$, Qianshan Wei ${ }^{1,3 *}$, Chuanyi Zhang ${ }^{2}$, Guilin $\mathbf{Q i}^{3,4}$, Miaozeng Du $^{3,4}$, Yongrui Chen $^{3,4},{\text { Sheng } \mathbf{B i}^{3,4}}^{3}$<br>${ }^{1}$ School of Cyber Science and Engineering, Southeast University, Nanjing, China<br>${ }^{2}$ College of Artificial Intelligence and Automation, Hohai University, Nanjing, China<br>${ }^{3}$ Key Laboratory of New Generation Artificial Intelligence Technology and Its<br>Interdisciplinary Applications (Southeast University), Ministry of Education, China<br>${ }^{4}$ School of Computer Science and Engineering, Southeast University, Nanjing, China<br>jqli@seu.edu.cn, 213223283@seu.edu.cn, 20231104@hhu.edu.cn, gqi@seu.edu.cn<br>miaozengdu@seu.edu.cn,yrchen@seu.edu.cn, shengbi@seu.edu.cn


#### Abstract

Machine unlearning (MU) empowers individuals with the 'right to be forgotten' by removing their private or sensitive information encoded in machine learning models. However, it remains uncertain whether MU can be effectively applied to Multimodal Large Language Models (MLLMs), particularly in scenarios of forgetting the leaked visual data of concepts. To overcome the challenge, we propose an efficient method, Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps. SIU consists of two key aspects: (i) Constructing Multifaceted fine-tuning data. We introduce four targets, based on which we construct fine-tuning data for the concepts to be forgotten; (ii) Joint training loss. To synchronously forget the visual recognition of concepts and preserve the utility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence Loss combined with Cross Entropy loss. Alongside our method, we establish MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics for its evaluation. Experimental results on MMUBench show that SIU completely surpasses the performance of existing methods. Furthermore, we surprisingly find that SIU can avoid invasive membership inference attacks and jailbreak attacks. To the best of our knowledge, we are the first to explore MU in MLLMs. We will release the code and benchmark in the near future.


## 1 Introduction

Recent years have witnessed the great success of Large Language Models (LLMs) [33, 3] and Multimodal Large Language Models (MLLMs) [47, 49]. They play dominant roles in NLP [5, 37] and multimodal applications [50, 17] ascribed to the large-scale pre-training data [2, 35, 29]. Unfortunately, these data may contain overlooked elements of personal privacy and copyright infringement, posing potential risks of data leakage [32, 36]. Retraining the models from scratch to exclude the risky data is a waste of resource and practically untenable due to the inaccessible pre-training data. To address the issue, prior works [12, 46, 45, 27, 31] have shown that approximate machine unlearning (MU) methods can forget specific pieces of knowledge embedded within LLMs.

Nevertheless, it remains unclear if such strategies of knowledge forgetting are transferable to MLLMs, especially for forgetting the visual recognition of various concepts. The challenge of unlearning visual[^0]recognition in MLLMs is formidable. A primary obstacle is limited training data. Recent work [12] utilizes a text of original book (2.1M tokens) combined with synthetic sentences (1M tokens) as the forgetting dataset. To forget the character 'Harry Potter', this work fine-tunes Llama-7b-chat-hf [41] on the entire forgetting dataset for 3 epochs. However, in the real scenario of unlearning the visual recognition of concepts, collecting sufficient images of targeted concepts is challenging. The limited amount of training data poses a significant barrier to unlearning all concept-wise visual knowledge encoded in pre-trained MLLMs. Another challenge is model degradation [52, 19], which pervasively exists in large generative models. Researchers [46] discover that LLMs could stop generating harmful texts by employing Gradient Ascent (GA) on forgetting datasets, thus reducing the need for synthetic data. However, GA often results in meaningless outputs such as only a whitespace or repeated tokens, which eliminate the utility of LLMs. To address this issue, several studies 45, 46] combine GA with minimizing KL-divergence between unlearned and original LLMs to preserve the utility of LLMs. Despite mitigating the meaningless response problem, the method may output self-contradictory answers, as if the concept is not unlearned. This issue may arise from a conflict between objectives of GA and KL-divergence. GA aims to make LLMs cease generating tokens of targeted unlearning concepts, whereas KL-divergence seeks to align the output probability distribution of the unlearning model with that of the original model. The distribution includes the probabilities of generating tokens of targeted unlearning concepts, which are high in the original model.

To address the challenges, we take the first step to explore MU in MLLMs and propose an efficient method, Single Image Unlearning (SIU). SIU requires only a single training image of the targeted concepts to enable MLLMs to forget the visual recognition of these concepts. We first put forward four targets, namely Aligning with Unseen Concepts, Assigning New Visual Description, Decoupling Factual Knowledge and Preserving Non-targeted Knowledge. In accordance with these four targets, we construct the fine-tuning data. Moreover, we introduce an innovative Dual Masked KL-divergence (DMK) Loss to be jointly trained with Cross Entropy Loss. Different from prior works, the joint training loss is optimized by Gradient Descent. The DMK Loss incorporates two levels of masking on fine-tuning data, which are Token-Level Masking and Vocabulary-Level Masking. At the token-level, it masks tokens contradicting original knowledge in the sentence to exclude them from KL loss calculations. At the vocabulary-level, it specifically masks tokens of the targeted unlearning concepts across the entire vocabulary during KL loss computation.

Alongside our method we introduce MMUBench, a comprehensive benchmark designed to assess MU within MLLMs. This benchmark includes a curated dataset with a minimum of 50 images for each of 20 concepts. One image per concept is designated for the forgetting training set, with the remainder serving to assess generality. To provide a thorough evaluation of MU, we develop an evaluation scheme including efficacy, generality, specificity, fluency and diversity. Efficacy and generality assess the effectiveness of the unlearning methods, while specificity, fluency and diversity evaluate the utility of MLLMs post-unlearning. MMUBench includes the application of existing methods as baselines, facilitating comparative analysis. The experimental results reveal that our approach surpasses these methods in all evaluation metrics. We observe that SIU could trigger positive butterfly effects, details of which are discussed in the experimental sections. Furthermore, we conduct membership inference attack and jailbreak attack [24,34] experiments to examine the robustness of unlearning methods.

We summarize main contributions as follows:

- To the best of our knowledge, we are the pioneers in exploring unlearning the visual recognition of concepts in MLLMs, extending machine unlearning to multimodal settings.
- We propose a new method, namely SIU, to efficiently forget the visual recognition of concepts with only one training image. SIU incorporates Multifaceted Fine-tuning Data and Dual Masked KL-divergence Loss, both of which significantly enhance unlearning performance.
- We establish MMUBench, a new benchmark to evaluate the efficacy, generality, specificity, fluency and diversity of machine unlearning methods in MLLMs.
- The experimental results on MMUBench demonstrate the superiority of our method compared to existing methods. Furthermore, the ability to defend against membership inference attacks and jailbreak attacks reveal the robustness of our method.


## 2 Related Work

Machine Unlearning. In recent years, there has been a notable increase in interest concerning machine unlearning (MU) problems. The primary works [13, 6, 8] mainly focused on MU in classification tasks. However, the research of MU in LLMs is far from being developed. Different from classification task, MU in LLMs [39, 51] should not only stop generating harmful or private texts, but also remain the utility of LLMs. Yao et al. [46] employ Gradient Ascent (GA) method to forget original harful output. Wang et al. [42] propose a method to align the knowledge between the pre-trained model and fine-tuning model. Chen and Yang [7] introduce an efficient method to handle a deletion quest by introducing lightweight unlearning layers. Yao et al. [45] combine GA with KL-divergence to constrain the output probability distribution. Eldan and Russinovich [12] construct a dictionary of generic prediction to substitute the unlearning target in fine-tuning data. In our paper, we further extend the MU setting to MLLMs and propose a new method to efficiently forget the visual recognition of concepts for MLLMs.

Multimodal Large Language Model. MLLMs are architected by integrating a language model with a visual encoder, linked through an intermediary connector. A pioneering method introduced by [1] employs a query-based cross-attention mechanism, establishing an advanced and robust visionlanguage interaction module. In contrast, BLIP-2 [23] employs a Q-Former, which is a streamlined Transformer model, in place of the typical cross-attention. Enhancements in BLIP-2's performance are achieved by MiniGPT-4 [54] and InstructBLIP [10], which both incorporate instruction tuning datasets collected from a diverse range of public sources. To augment the models' comprehension capabilities, LLaVA, mPLUG-2 and Otter [26, 44, 21] have developed a system of instructional data. Progressing beyond earlier training methodologies, a novel three-stage training strategy [4] has been proposed to further refine multimodal representations. Additionally, CogVLM [43] introduces a visual expert system to elevate model performance.

## 3 Problem Definition

In our work, we mainly focus on unlearning the visual recognition of the concepts (e.g., Recognize Donald Trump in an image) rather than forgetting the factual knowledge (if have, e.g., Donald Trump is the former president) in MLLMs. Formally, let $\mathcal{M}_{\theta}$ denote the original MLLM, where $\theta$ is the parameters of original MLLM. $\mathcal{M}_{\theta}$ is trained with a dataset that encompasses pairs of visual and textual data, $\mathcal{D}=\left\{\left(\mathcal{I}_{i}, \mathcal{T}_{i}\right)\right\}_{i=1}^{N}$, where $\mathcal{I}_{i}$ represents an image and $\mathcal{T}_{i}$ is a text consisting of $t_{i}$ tokens $\left\{w_{1}^{i}, w_{2}^{i}, \ldots, w_{t_{i}}^{i}\right\}$. We define the forgetting set $\mathcal{D}^{f}=\left\{\left(\mathcal{I}_{j}^{\mathcal{C}}, \mathcal{T}_{j}^{\mathcal{C}}\right)\right\}_{j=1}^{K}$ as a collection of $K$ image-text pairs associated with the visual recognition of targeted unlearning concepts $\mathcal{C}$. Each $\mathcal{I}^{\mathcal{C}}$ is an image depicting $\mathcal{C}$ and each $\mathcal{T}^{\mathcal{C}}$ is the question-answer text about the image content pointing to $\mathcal{C}$, where the answer reflects the forgetting of $\mathcal{C}$. To facilitate the unlearning process and assess its impact, we partition $\mathcal{D}^{f}$ into a training subset $\mathcal{D}_{\text {train }}^{f}$ and a testing subset $\mathcal{D}_{\text {test }}^{f}$. $\mathcal{D}_{\text {train }}^{f}$ contains a single image-text pair used to train the unlearned model, and $\mathcal{D}_{\text {test }}^{f}$ contains the remainder of the pairs used to evaluate the generality of unlearning.

We define the goal of MU in MLLMs as follows:

Machine unlearning in MLLMs aims to eliminate learned patterns associated with visual recognition of specific "to-be-forgotten" concepts, while preserving the MLLMs' prediction capabilities on inputs unrelated to those eliminated patterns.

By employing the negative log-likelihood of predicting the next token, the training objective is to obtain an unlearned model $\mathcal{M}_{\hat{\theta}}$ and can be formulated as follows:

$$
\begin{align*}
& \arg \min _{\hat{\theta}}\left\{\mathbb{E}_{\left(\mathcal{I}_{j}, \mathcal{T}_{j}\right) \in \mathcal{D}^{f}}\left[-\sum_{t=1}^{t_{j}} \log P_{\mathcal{M}_{\hat{\theta}}}\left(w_{t}^{j} \mid \mathcal{I}_{j}, w_{1}^{j}, \ldots, w_{t-1}^{j}\right)\right]\right.  \tag{1}\\
& \left.\quad+\mathbb{E}_{\left(\mathcal{I}_{i}, \mathcal{T}_{i}\right) \in \mathcal{D} \backslash \mathcal{D}^{f}}\left[-\sum_{t=1}^{t_{i}} \log P_{\mathcal{M}_{\hat{\theta}}}\left(w_{t}^{i} \mid \mathcal{I}_{i}, w_{1}^{i}, \ldots, w_{t-1}^{i}\right)\right]\right\}, \mathcal{T}=w_{1}, \ldots, w_{t}
\end{align*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_e9c81bee14393febf785g-04.jpg?height=688&width=1396&top_left_y=236&top_left_x=364)

![](https://cdn.mathpix.com/cropped/2024_06_04_e9c81bee14393febf785g-04.jpg?height=325&width=412&top_left_y=244&top_left_x=369)

![](https://cdn.mathpix.com/cropped/2024_06_04_e9c81bee14393febf785g-04.jpg?height=366&width=401&top_left_y=549&top_left_x=366)

Figure 1: Overview of the Unlearning Process in MLLMs Using SIU. The process starts with a user request to unlearn the visual recognition of concepts, utilizing MMUBench (introduced in Section 5) to provide concepts for unlearning. SIU has two elements which are Multifaceted Fine-tuning Data and Dual Masked KL-divergence Loss. After unlearning, the unlearned MLLM is evaluated for generality, specificity, diversity, fluency, and resistance to membership inference and jailbreak attacks.

## 4 Methodology

In this section, we present our proposed method, namely SIU, for MU in MLLMs. As shown in Figure 1., we take Donald Trump as an example of $\mathcal{C}$. SIU consists of two parts, Multifaceted Fine-tuning Data and Dual Masked KL-divergence Loss. MMUBench will be introduced in Section 5 .

### 4.1 Multifaceted Fine-tuning Data

As stated in Section 3. for each $\mathcal{C}$ we have a single image-text pair as forgetting training subset $\mathcal{D}_{\text {train }}^{f}$. Based on $\mathcal{D}_{\text {train }}^{f}$, we construct fine-tuning data centering on four targets. The details of fine-tuning data are shown in Figure 7 and Appendix A. 3[^1]

Our approach reinterprets the objective of MU, aiming to align the output distribution of $\mathcal{M}_{\hat{\theta}}$ with that of $\mathcal{M}_{\theta}$ under $\mathcal{D}^{f}$ when the visual representations of $\mathcal{C}$ are not present during the pre-training phase. To find the characteristics of output distribution, we conduct a set of tiny experiments on 190 private images of people that surely have not appeared in the pre-training phase of $\mathcal{M}_{\theta}$ (detailed in Appendix A.1). We observe that $\mathcal{M}_{\theta}$ is unaware of concepts they have not seen and tends to generate factually vague or incorrect responses such as 'man', 'woman' or 'John'. We assume though an incorrect response might be a hallucination, it actually achieves the purpose of unlearning. Moreover, in MU of classification tasks the model after unlearning would also output a wrong label [13, 6]. Thus, to guide $\mathcal{M}_{\hat{\theta}}$ output incorrect names, the fine-tuning data for the first target is shown in Figure 7a. The proof of effectiveness of this target is presented in Appendix A.2.

Assigning New Visual Description. In our primary experiments, it is found that utilizing only the fine-tuning data of the first target will lead MLLMs to recognize $\mathcal{C}$ as both Donald Trump and the new incorrect name. This phenomenon indicates that MLLMs correspond the same visual representations to the original name and the newly given name. Thus, we mitigate the risk of the MLLMs confusing the original and the new name by fabricating a new visual description for $\mathcal{C}$. The constructed data for the target is shown in Figure $7 \mathrm{~b}$.

Decoupling Factual Knowledge. Leveraging fine-tuning data only of the first two objectives could lead MLLMs to completely forget $\mathcal{C}$ including the factual knowledge. This observation contradicts our definition in Section 3 For Donald Trump, he possesses many attributes, such as being a former U.S. President and a politician. Therefore, to decouple the factual knowledge of the concept, we use a specific factual piece of knowledge about him as fine-tuning data as depicted in Figure 7c

Preserving Non-targeted Knowledge. We find that only fine-tuning MLLMs on data associated with $\mathcal{C}$ may lead to the forgetting of non-targeted knowledge. However, it is essential to ensure that unlearning process does not diminish its ability to accurately respond to other unrelated knowledge domains. Finally, we introduce examples which describe the knowledge of non-targeted concepts to alleviate this issue as shown in Figure $7 \mathrm{~d}$.

### 4.2 Dual Masked KL-divergence Loss

We propose a novel Dual Masked KL-divergence (DMK) Loss which refines the unlearning process by incorporating a dual masking technique into KL-divergence loss. The motivation of DMK is discussed in Appendix B. The masks of DMK are twofold:

Token-Level Masking. This mask operates at the token level, masking out tokens that contradicts original knowledge. Masked tokens are excluded from the computation of the KL divergence, preventing the model from increasing their probability in the output distribution. For instance, as stated in Section 4.1, we assign an alternative name such as 'Jacob Campbell' for Donald Trump. We then apply the mask to the tokens of 'Jacob Campbell' in the fine-tuning sentence, where the KLdivergence loss is not computed. Formally, for a training sample $\mathcal{T}$ consisting of $\left\{w_{1}, w_{2}, \ldots, w_{n}\right\}$, the token-level mask is defined as:

$$
K_{\mathcal{S}}=\left\{m_{1}, m_{2}, \ldots, m_{n}\right\}, \text { where } m_{j}= \begin{cases}0, & \text { if } w_{j} \text { is a specified token }  \tag{2}\\ 1, & \text { otherwise }\end{cases}
$$

Vocabulary-Level Masking. The second level of masking operates across the entire vocabulary. For those tokens where KL-divergence loss is computed, we introduce a mask within the MLLMs' vocabulary specifically for the tokens of $\mathcal{C}$ 's name. Mathematically, if $\mathcal{V}$ is the vocabulary, the vocabulary-level mask for the vocabulary is:

$$
K_{\mathcal{V}}=\left\{m_{v_{1}}, m_{v_{2}}, \ldots, m_{v_{|\mathcal{V}|}}\right\}, \text { where } m_{v_{i}}= \begin{cases}0, & \text { if } v_{i} \in \mathcal{C}  \tag{3}\\ 1, & \text { otherwise }\end{cases}
$$

The formulation of the DMK Loss is as follows:

$$
\begin{equation*}
\mathcal{L}_{D M K}\left(\mathcal{I}_{i}, \mathcal{T}_{i} ; \hat{\theta}\right)=\sum_{t=1}^{t_{i}} K_{\mathcal{S}} \cdot K_{\mathcal{V}} \cdot P_{\mathcal{M}_{\theta}}\left(w_{t}^{i} \mid \mathcal{I}_{i}, w_{1}^{i}, \ldots, w_{t-1}^{i}\right) \log \frac{P_{\mathcal{M}_{\theta}}\left(w_{t}^{i} \mid \mathcal{I}_{i}, w_{1}^{i}, \ldots, w_{t-1}^{i}\right)}{P_{\mathcal{M}_{\hat{\theta}}}\left(w_{t}^{i} \mid \mathcal{I}_{i}, w_{1}^{i}, \ldots, w_{t-1}^{i}\right)} \tag{4}
\end{equation*}
$$

Finally, we optimize Cross Entropy Loss and $\mathcal{L}_{D M K}$ using Gradient Descent:

$$
\begin{equation*}
\mathcal{L}_{\text {total }}\left(\mathcal{I}_{i}, \mathcal{T}_{i} ; \hat{\theta}\right)=-\alpha \cdot \sum_{t=1}^{t_{i}} \log P_{\mathcal{M}_{\hat{\theta}}}\left(w_{t}^{i} \mid \mathcal{I}_{i}, w_{1}^{i}, \ldots, w_{t-1}^{i}\right)+\beta \cdot \mathcal{L}_{D M K}\left(\mathcal{I}_{i}, \mathcal{T}_{i} ; \hat{\theta}\right) \tag{5}
\end{equation*}
$$

where $\alpha$ and $\beta$ are the hyper-parameters of weighing the two losses.

## 5 MMUBench

We establish MMUBench, a comprehensive benchmark for advancing MU within MLLMs. MMUBench is designed to evaluate the process of unlearning across various dimensions of model performance and behavior. The construction of dataset is detailed in Appendix C.1. In this section, we introduce the evaluation settings of MMUBench:

Efficacy. This dimension assesses how effectively $\mathcal{M}_{\hat{\theta}}$ have unlearned seen examples. Efficacy measures the accuracy of answers given the inputs of $\mathcal{D}_{\text {train }}^{f}$. It inspects if the $\mathcal{M}_{\hat{\theta}}$ 's outputs are now aligned with the objectives of the MU in MLLMs.

Generality. Generality examines the $\mathcal{M}_{\hat{\theta}}$ 's ability on $\mathcal{D}_{\text {test }}^{f}$. This evaluation ensures that MLLMs does not recognize $\mathcal{C}$ across a set of unseen images. In addition to the visual generality, we also test the $\mathcal{M}_{\hat{\theta}}$ 's adaptability to a variety of textual prompts, providing a comprehensive evaluation of the $\mathcal{M}_{\hat{\theta}}$ 's ability to generalize the unlearning process across both modalities. Generality is quantified using three types of measurements within MMUBench, which are Exact Match (EM), GPT-4 Evaluation (G-Eval) and $\mathcal{C}$ Probability Distance ( $\mathcal{C}$-Dis). The three measurements are detailed in Appendix C. 3

Specificity. Specificity measures the impact of unlearning on non-targeted knowledge. As we have no access to the whole remaining data of the pre-training phase, we employ a diverse set of public multimodal benchmarks to assess specificity. The evaluation benchmarks include GQA [18], VQA-v2 [14], VisWiz [15], SQA ${ }^{\mathrm{I}}$ [30], VQA ${ }^{\mathrm{T}}$ [40], POPE [25], MMB [28], Mm-Vet [48]. We take the average of all benchmark performance as Specificity.

Fluency. Fluency evaluates the readability of responses of $\mathcal{M}_{\hat{\theta}}$, which ensures the utility of $\mathcal{M}_{\hat{\theta}}$. We compare the perplexity of sentences generated by the model before and after unlearning. When the name of $\mathcal{C}$ appears in the output from $\mathcal{M}_{\theta}$, we apply a mask to avoid distorting the fluency measurement:

$$
\begin{align*}
& \text { Fluency }=\exp \left(-\frac{1}{t_{i}} \sum_{t=1}^{t_{i}} \log P_{\mathcal{M}_{\hat{\theta}}}^{\operatorname{mask}}\left(w_{t}^{i} \mid \mathcal{I}_{i}, w_{1}^{i}, \ldots, w_{t-1}^{i}\right)\right. \\
& P_{\mathcal{M}_{\hat{\theta}}}^{\text {mask }}\left(w_{t}^{i} \mid \mathcal{I}_{i}, w_{1}^{i}, \ldots, w_{t-1}^{i}\right)= \begin{cases}P_{\mathcal{M}_{\hat{\theta}}}\left(w_{t}^{i} \mid \mathcal{I}_{i}, w_{1}^{i}, \ldots, w_{t-1}^{i}\right), & \text { if } w_{t}^{i} \notin \mathcal{C} \\
\frac{1}{\text { vocabulary size }}, & \text { if } w_{t}^{i} \in \mathcal{C}\end{cases} \tag{6}
\end{align*}
$$

where 'vocabulary size' is dependent on the specific MLLM.

Diversity. Diversity can measure whether $\mathcal{M}_{\hat{\theta}}$ can generate unique answers. It also ensures that the output of $\mathcal{M}_{\hat{\theta}}$ does not over-fit to a few templates that appear in the unlearning process. We count the number of unique words in the total generated output.

Membership Inference Attack. Membership inference attacks (MIA) could reveal whether the visual representations of $\mathcal{C}$ are still encoded in $\mathcal{M}_{\hat{\theta}}$. As we could not get access to the pre-training data of MLLMs, we use Min-K\% PROB [38], an MIA method without knowing the pre-training data. The detailed calculation of this measurement is stated in Appendix D.2.

Jailbreak. Jailbreak attacks are designed to assess how $\mathcal{M}_{\hat{\theta}}$ performs under deliberately challenging or edge-case conditions, checking if $\mathcal{M}_{\hat{\theta}}$ truly cannot generate outputs related to $\mathcal{C}$. We utilize multilingual test [11] and multi-hop question test [53] as our jailbreak experiments.

## 6 Experiments

### 6.1 Experiment setup

Model and Training. As stated in Appendix C.1, the concept filtering process is implemented by LLAVA [26] to construct dataset. To accurately compare the knowledge before and after unlearning, we also use LLAVA (7B and 13B) to obtain the unlearned model. The optimizer is Adam and the learning rate is $3 \mathrm{e}-4$. Lora [16] is employed to fine-tune LLAVA with batch size 4. The training step is set to 6 . We use four A100 40G GPUs to train the model. $\alpha$ and $\beta$ are 0.9 and 0.75 respectively.

Baselines. We compare our method with several existing methods: (i) Preference Optimization (PO). Following TOFU [31], we use 'I do not know.' and its variants as the responses to the questions correspond with $\mathcal{C}$. (ii) Gradient Ascent (GA) [46]. It optimizes MLLMs to decrease their ability to recall or generate texts related to $\mathcal{C}$. (iii) GA+KL [45]. To preserve the utility of MLLMs, KL-divergence loss is combined with GA.

Evaluate Concepts. In the experimental section, we primarily present the experimental results related to Donald Trump due to the limited space. We report several other concepts covering different

Table 1: Comparison with the existing machine unlearning methods. We report the means and standard deviation of 3 independent trials. It is noted that the Specificity of each benchmark is summarized in Table 7 .

| Method | Efficacy $\uparrow$ | Generality |  |  | Specificity $\uparrow$ | Fluency $\downarrow$ | Diversity $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $\mathbf{E M} \uparrow$ | G-Eval $\downarrow$ | $\overline{\mathcal{C} \text {-Dis } \uparrow}$ |  |  |  |
| $\mathbf{L L A V A}_{7 \mathbf{B}}$ |  |  |  |  |  |  |  |
| PO 31 <br> GA 46 <br> GA+KL 45 <br> SIU | $100.0_{ \pm 0}$ <br> $100.0_{ \pm 0}$ <br> $100.0_{ \pm 0}$ <br> $100.0_{ \pm 0}$ | $58.3_{ \pm 4.0}$ <br> $36.3_{ \pm 5.4}$ <br> $33.0_{ \pm 1.7}$ <br> $\mathbf{9 9 . 0}_{ \pm \mathbf{0 . 0}}$ | $2.0_{ \pm 0.8}$ <br> $\mathbf{1 . 8}$ <br> $2.8_{ \pm 0.4}$ <br> $1.9_{ \pm 0.5}$ | $0.4_{ \pm 0.1}$ <br> $1.6_{ \pm 1.2}$ <br> $0.8_{ \pm 0.6}$ <br> $\mathbf{1 . 8}_{ \pm \mathbf{0 . 3}}$ | $58.3_{ \pm 1.3}$ <br> $9.0_{ \pm 1.9}$ <br> $60.0_{ \pm 0.3}$ <br> $\mathbf{6 0 . 7} 7_{ \pm \mathbf{0 . 7}}$ | $75.1_{ \pm 0.9}$ <br> $373.6_{ \pm 3.5}$ <br> $198.1_{ \pm 2.3}$ <br> $\mathbf{6 1 . 2} \pm \mathbf{1 . 2}$ | $93.5_{ \pm 2.1}$ <br> $6.3_{ \pm 2.6}$ <br> $48.0_{ \pm 5.2}$ <br> $\mathbf{9 7 . 0}_{ \pm \mathbf{0 . 2}}$ |
| LLAVA $_{13 \mathrm{~B}}$ |  |  |  |  |  |  |  |
| PO <br> GA <br> GA+KL <br> SIU | ![](https://cdn.mathpix.com/cropped/2024_06_04_e9c81bee14393febf785g-07.jpg?height=124&width=125&top_left_y=606&top_left_x=692) | $10.7_{ \pm 3.1}$ <br> $24.7_{ \pm 1.7}$ <br> $17.3_{ \pm 1.2}$ <br> $\mathbf{9 0 . 0}$ <br> $\mathbf{0 . 8}$ | $4.6_{ \pm 0.2}$ <br> $4.6_{ \pm 0.1}$ <br> $4.8_{ \pm 0.1}$ <br> $\mathbf{2 . 1}$ | $0.5_{ \pm 0.2}$ <br> $1.6_{ \pm 1.4}$ <br> $1.5_{ \pm 0.4}$ <br> $\mathbf{3 . 6}_{ \pm \mathbf{1 . 0}}$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_e9c81bee14393febf785g-07.jpg?height=121&width=141&top_left_y=606&top_left_x=1175) | $60.7_{ \pm 0.3}$ <br> $144.7_{ \pm 7.4}$ <br> $114.1_{ \pm 3.8}$ <br> $\mathbf{5 4 . 3}_{ \pm \mathbf{0 . 9}}$ | $89.7_{ \pm 1.4}$ <br> $74.5_{ \pm 4.9}$ <br> $75.0_{ \pm 2.4}$ <br> $\mathbf{9 6 . 5}$ |

types, such as Cartoon concepts (Hello Kitty and Mario) and abstract concepts about painting style (Doodle, Picasso and Van Gogh). Moreover, we evaluate the effects of synchronously unlearning all the 20 concepts of MMUBench. The details of $\mathcal{D}_{\text {train }}^{f}$ and $\mathcal{D}_{\text {test }}^{f}$ are presented in Appendix C. 2 .

### 6.2 Experiment Results

Main Results. The experimental results in Table 1 present a comprehensive evaluation of various methods for machine unlearning in MLLMs. The observations are as follows: (i) Efficacy across all methods is at $100 \%$, which indicates that each method is equally capable of unlearning the seen examples and aligning well with the objectives of machine unlearning. (ii) GA shows an outstanding performance in G-Eval with 1.8 score. However, this high score in generality is a result of GA's method always outputting whitespace or repeated tokens. SIU also performs a high Generality with $99.0 \%$ EM score, showcasing its effectiveness at extending unlearning to unseen data. (iii) GA performs 9.0 in Specificity score, indicating that there's a strong impact on the model's knowledge base. SIU achieves a reasonable balance, with a score of 60.7 , illustrating that it maintains a good level of model performance on non-targeted tasks. (iv) Fluency is where the GA method notably fails, with a score of 373.6. In contrast, SIU's fluency score of 61.2 suggests that it manages to retain coherent language outputs post-unlearning. (v) The PO method seems to have maintained a degree of diversity, as indicated by a moderate score. GA+KL shows a limited score of 48.0 in Diversity. GA's score is essentially at rock bottom (6.3), due to its most responses of whitespace or repeated tokens. SIU performs admirably with a score of 97.0 , indicating its maintenance in generating diverse responses post-unlearning. (vi) As the model size increases from 7B to $13 \mathrm{~B}$, there is a noticeable decline in the effectiveness of non-SIU methods in Generality. For example, the EM score for GA falls from $36.3 \%$ to $24.7 \%$, and both PO and GA+KL experience severe drops in their generality scores. This sharp decline highlights a critical vulnerability in these methods due to the change in model size. (vii) SIU shows a relatively minor decline in generality (from $99 \%$ to $90 \%$ EM) when scaling up from the 7B to the $13 \mathrm{~B}$ model. This slight reduction indicates that SIU is more adaptable and stable. (viii) Across all methods, there is an observed improvement in specificity, fluency, and diversity from the 7B to the 13B models. This enhancement suggests a trade-off between the effectiveness of unlearning and the preservation of model utility.

Ablation Study of DMK Loss. We perform an ablation study to evaluate the significance of TokenLevel Masking and Vocabulary-Level Masking as shown in Table 2 Every masking is individually subjected to ablation to examine its effect. We use Mm-Vet benchmark as the specificity. It could be observed that the EM score without Token-Level Masking and Vocabulary-Level Masking both degrade compared to SIU. Moreover, the $\mathcal{C}$-Dis also goes down if SIU is not equipped with Token-Level Masking or Vocabulary-Level Masking. The results show that The two levels of masking could both improve the generality of unlearning and reduce the probability of generating tokens of $\mathcal{C}$. We also observe that the Specificity of SIU is worse than the model without vocabulary-level. The reason may be that masking several tokens during the computation of $\mathrm{KL}$ affects the logic of general output to a certain extent.
Table 2: Ablation study of DMK Loss. We utilize LLAVA $_{7 \mathrm{~B}}$ to conduct the experiments.

| Method | Generality |  |  | Specificity $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: |
|  | $\overline{\mathbf{E M} \uparrow}$ | G-Eval $\downarrow$ | $\mathcal{C}$-Dis $\uparrow$ |  |
| w/o token <br> w/o vocabulary <br> SIU | $92.0_{ \pm 0.0}$ <br> $94.3_{ \pm 1.2}$ <br> $\mathbf{9 9 . 0} 0_{ \pm 0.0}$ | $2.0_{ \pm 0.3}$ <br> $2.1_{ \pm 0.2}$ <br> $\mathbf{1 . 9}$ | $1.5_{ \pm 0.1}$ <br> $1.6_{ \pm 0.2}$ <br> $\mathbf{1 . 8} \pm 0.4$ | $27.7_{ \pm 2.5}$ <br> $\mathbf{2 9 . 4} \pm \pm 1.7$ <br> $28.9_{ \pm 1.4}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_e9c81bee14393febf785g-08.jpg?height=436&width=1374&top_left_y=232&top_left_x=381)

Figure 2: Visualization of various metrics across different methods over steps using LLAVA 7 в.
![](https://cdn.mathpix.com/cropped/2024_06_04_e9c81bee14393febf785g-08.jpg?height=438&width=1360&top_left_y=732&top_left_x=382)

![](https://cdn.mathpix.com/cropped/2024_06_04_e9c81bee14393febf785g-08.jpg?height=43&width=1209&top_left_y=1171&top_left_x=455)

Impacts of Fine-tuning Steps. In this section, we analyze the impact of fine-tuning steps as shown in Figure 2 and Figure 3 We utilize Mm-Vet as the Specificity. SIU demonstrates minimal fluctuations in each metric, which suggests that SIU is less sensitive to the number of fine-tuning steps. In contrast, other methods like GA and PO show significant variability with increased fine-tuning steps. For instance, GA's performance in Specificity and Fluency metrics tends to degrade seriously as the number of steps increases. Compared with the 7B model, the 13B model shows a slower adaptation speed. The 7B model displays a rapid increase in EM scores, reaching near-maximum values by step 10 across most methods. The 13B model shows a slower increase in EM scores over steps. PO method exhibits nearly constant values as steps increase in $\mathcal{C}$-Dis, regardless of the model size (both 7B and 13B). This consistency indicates that the PO method has primarily learned to respond with ' $I$ do not know.' rather than reducing the probability of recognizing the unlearned concept.

Effects of Unlearning Different Concepts. We evaluates several other concepts in our benchmark. The results of Generality (EM) are shown in Figure 4 and the overall results are summarized in Table 6. It could be observed that SIU consistently achieves nearly $100 \%$ accuracy in unlearning across all tested concepts, demonstrating its robustness and effectiveness. We also find all methods perform notably well on more abstract concepts such as Doodle and Picasso, which indicates that abstract concepts are easier to disassociate from the model's knowledge base. The case studies of these concepts are presented in Figures 16 to 22 .

![](https://cdn.mathpix.com/cropped/2024_06_04_e9c81bee14393febf785g-08.jpg?height=393&width=1355&top_left_y=2042&top_left_x=385)

Figure 4: EM performance comparison of methods SIU, GA+KL, PO, and GA across different concepts.

Table 3: Results of unlearning 20 concepts simultaneously using LLAVA 7 . Inf denotes an infinite value. We do not test G-Eval for GA and GA+KL because they only generate repeated tokens in all responses.

| Method | Efficacy $\uparrow$ | Generality |  |  | Specificity $\uparrow$ | Fluency $\downarrow$ | Diversity $\uparrow$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | EM $\uparrow$ | G-Eval $\downarrow$ | $\mathcal{C}$-Dis $\uparrow$ |  |  |  |
| PO [31] | $\mathbf{1 0 0 . 0}$ | 80.0 | 2.7 | 0.5 | 12.7 | 59.7 | 96.9 |
| GA [46 | $\mathbf{1 0 0 . 0}$ | $\mathbf{1 0 0 . 0}$ | - | $\mathbf{3 0 . 4}$ | 0 | Inf | 0.67 |
| GA+KL 45 | $\mathbf{1 0 0 . 0}$ | $\mathbf{1 0 0 . 0}$ | - | 15.7 | 0 | 695.2 | 0.67 |
| SIU | $\mathbf{1 0 0 . 0}$ | 97.0 | $\mathbf{1 . 7}$ | 5.0 | $\mathbf{2 4 . 9}$ | $\mathbf{5 4 . 4}$ | $\mathbf{9 9 . 3}$ |

Positive Butterfly Effect. We observe that our method could trigger surprising positive butterfly effects which can further illustrate the effects of machine unlearning. As shown in Figure 9 , we input an image featuring Donald Trump with his family into $\mathcal{M}_{\theta}$ and $\mathcal{M}_{\hat{\theta}}$ respectively. $\mathcal{M}_{\theta}$ is able to identify each person's name in the image correctly and $\mathcal{M}_{\hat{\theta}}$ misidentifies Donald Trump due to our unlearning method. However, his wife Melania is also misidentified by $\mathcal{M}_{\hat{\theta}}$. At first, we assume that our unlearning method causes the model to lose the ability to identify some other concepts. Further examination reveals an additional layer to this phenomenon. As can be seen in Figure 10, when the image is cropped to only include Melania Trump and presented to $\mathcal{M}_{\hat{\theta}}$, it accurately recognizes her and 'remember' her relationship with Donald Trump. This discovery points to a fascinating aspect of machine unlearning: the selective retention of knowledge. The reason of this observation might be that the model's failure to identify the central male figure as Trump in the original image leads to an inference that the adjacent female could not be Melania. These positive butterfly effects suggest that unlearning is not a blunt tool that erases all traces of a concept but rather can result in a refined restructuring of knowledge within the model.

Results of Unlearning Multiple Concepts Simultaneously. Table 3 reports the results of synchronously unlearning all the concepts of MMUBench. We concat all the forgetting training sets of these concepts as fine-tuning data and the training step is set to 120 . We find that after unlearning, the utility of MLLMs collapses using GA and GA+KL. All the responses of GA and GA+KL are repeated tokens 'image image image...' It could be observed that there is some decline in Specificity and Fluency of PO. In contrast, each metric is nearly the same with unlearning a single concept utilizing SIU, which illustrates the robustness of SIU.

MIA and Jailbreak. Table 4 displays the results of MIA and Jailbreaks tests. The experimental details of MIA are stated in Appendix D. 2 It could be observed that SIU achieves the lowest ROUGE-L score, indicating that the outputs of SIU diverge most from that of $\mathcal{M}_{\theta}$. We find PO also performs well under MIA. The reason may be that it tends to output 'I do not know.', leading to a low similarity score with the output of $\mathcal{M}_{\theta}$.

Table 4: Performance of MIA and Jailbreak with LLAVA $_{7 \mathrm{~B}}$. We do not evaluate GA method because the most of outputs are whitespace or repeated tokens.

| Method | MIA $\downarrow$ | Jailbreak |  |
| :--- | :---: | :---: | :---: |
|  |  | Multilingual $\downarrow$ | Multi-hop $\downarrow$ |
| PO | 0.32 | 2.5 | 0.18 |
| GA+KL | 0.44 | 2.9 | 0.38 |
| SIU | $\mathbf{0 . 2 7}$ | $\mathbf{2 . 3}$ | $\mathbf{0 . 1 6}$ |

For Jailbreak, we conduct two types of tests, which are multilingual test and multi-hop question test. The experiments are detailed in Appendix D. 3 and Appendix D. 4 Combining Table 1 and Table4 we find that the performance of GA+KL and SIU on multilingual are both slightly improved from 2.8 to 2.9 and from 1.9 to 2.3. The case studies are shown in Figures 12 to 14 From the specific examples we find PO always outputs 'I do not know.' in different languages. The outputs of SIU are diverse in different languages, illustrating the preservation of utility. For multi-hop question test, as shown in Table 4, it could be observed that SIU performs well in Multi-hop questions, indicating the capability of defending hard examples. The case study of Multi-hop question is displayed in Figure 15. We find that though GA+KL avoids generating the name of $\mathcal{C}$, it could still answer the right factual knowledge of the question. This self-contradictory answer illustrates the analysis in Section 1 We also observe that SIU could 'make up some lies' such as 'having gold courses in St.Andrews'. This phenomenon also confirms the findings of positive butterfly effects.

## 7 Conclusion

We introduce SIU, an efficient method to unlearn the visual recognition of concepts in MLLMs with only one training image. We propose four targets to construct little fine-tuning data. To mitigate the
degradation of MLLMs, we introduce Dual Masked KL-divergence Loss to be jointly trained with Cross Entropy Loss. Together with the method we present MMUBench, a benchmark to evaluate machine unlearning in MLLMs. The benchmark is composed of 1000 images, with 50 images for each of the 20 concepts, and a set of evaluation metrics. The experimental results illustrate the effectiveness and robustness of our method. For future work, we would try to extend this work mainly in the following aspects: (i) exploring new machine unlearning methods in MLLMs; (ii) evaluating machine unlearning for data points rather than concept-wise knowledge in MLLMs.

## References

[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, and et al. Flamingo: a visual language model for few-shot learning. In NeurIPS, 2022.

[2] Ido Amos, Jonathan Berant, and Ankit Gupta. Never train from scratch: Fair comparison of long-sequence models requires data-driven priors. In ICLR. OpenReview.net, 2024.

[3] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, and et al. Palm 2 technical report. CoRR, abs/2305.10403, 2023

[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966, 2023.

[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, and et al. Language models are few-shot learners. In NeurIPS, 2020.

[6] Sungmin Cha, Sungjun Cho, Dasol Hwang, Honglak Lee, Taesup Moon, and Moontae Lee. Learning to unlearn: Instance-wise unlearning for pre-trained classifiers. In AAAI, pages 11186-11194. AAAI Press, 2024.

[7] Jiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for llms. In EMNLP, pages 12041-12052. Association for Computational Linguistics, 2023.

[8] Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, and Zuozhu Liu. Fast model debias with machine unlearning. In NeurIPS, 2023.

[9] Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Zhengfu He, Kai Chen, and Xipeng Qiu. Can AI assistants know what they don't know? CoRR, abs/2401.13275, 2024.

[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. CoRR, abs/2305.06500, 2023.

[11] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak challenges in large language models. CoRR, abs/2310.06474, 2023.

[12] Ronen Eldan and Mark Russinovich. Who's harry potter? approximate unlearning in llms. CoRR, abs/2310.02238, 2023.

[13] Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation. CoRR, abs/2310.12508, 2023.

[14] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the $\mathrm{V}$ in VQA matter: Elevating the role of image understanding in visual question answering. In CVPR, pages 6325-6334. IEEE Computer Society, 2017.

[15] Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR, pages 3608-3617. Computer Vision Foundation / IEEE Computer Society, 2018.

[16] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR. OpenReview.net, 2022.

[17] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, and et al. Language is not all you need: Aligning perception with language models. In NeurIPS, 2023.

[18] Drew A. Hudson and Christopher D. Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, pages 6700-6709. Computer Vision Foundation / IEEE, 2019.

[19] Shotaro Ishihara, Hiromu Takahashi, and Hono Shirai. Semantic shift stability: Efficient way to detect performance degradation of word embeddings and pre-trained language models. In AACL/IJCNLP (1), pages 205-216. Association for Computational Linguistics, 2022.

[20] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. In NeurIPS, 2023.

[21] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. CoRR, abs/2305.03726, 2023.

[22] Jiaqi Li, Miaozeng Du, Chuanyi Zhang, Yongrui Chen, Nan Hu, Guilin Qi, Haiyun Jiang, Siyuan Cheng, and Bozhong Tian. MIKE: A new benchmark for fine-grained multimodal entity knowledge editing. CoRR, abs/2402.14835, 2024.

[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping languageimage pre-training with frozen image encoders and large language models. In ICML, volume 202 of Proceedings of Machine Learning Research, pages 19730-19742. PMLR, 2023.

[24] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize large language model to be jailbreaker. CoRR, abs/2311.03191, 2023.

[25] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, pages 292-305. Association for Computational Linguistics, 2023.

[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. CoRR, $\mathrm{abs} / 2304.08485,2023$.

[27] Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, and Yang Liu. Rethinking machine unlearning for large language models. CoRR, abs/2402.08787, 2024.

[28] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? CoRR, abs/2307.06281, 2023.

[29] Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Xiangyang Ji, Antoni B. Chan, and Rong Jin. Improved fine-tuning by better leveraging pre-training data. In NeurIPS, 2022.

[30] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022.

[31] Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. TOFU: A task of fictitious unlearning for llms. CoRR, abs/2401.06121, 2024.

[32] Alessandro Mantelero. The EU proposal for a general data protection regulation and the roots of the 'right to be forgotten'. Comput. Law Secur. Rev., 29(3):229-235, 2013.

[33] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303. 08774. URL https://doi.org/10.48550/arXiv.2303.08774.

[34] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! CoRR, abs/2310.03693, 2023.

[35] Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ali Farhadi, and Ludwig Schmidt. On the connection between pre-training data diversity and fine-tuning robustness. In NeurIPS, 2023.

[36] Joachim Scherer and Gerd Kiparski. Buchbesprechungen. feiler, lukas / forgó, nikolaus / weigl, michaela: The eu general data protection regulation (gdpr): A commentary. Comput. und Recht, 34(6):69-70, 2018 .

[37] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, and et al. LAION-5B: an open large-scale dataset for training next generation image-text models. In NeurIPS, 2022.

[38] Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. CoRR, abs/2310.16789, 2023.

[39] Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang. Knowledge unlearning for llms: Tasks, methods, and challenges. CoRR, abs/2311.15766, 2023.

[40] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In CVPR, pages 8317-8326. Computer Vision Foundation / IEEE, 2019.

[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, and et al. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023.

[42] Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, and Hongzhi Yin. KGA: A general machine unlearning framework based on knowledge gap alignment. In ACL (1), pages 13264-13276. Association for Computational Linguistics, 2023.

[43] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. CoRR, abs/2311.03079, 2023.

[44] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, and et al. mplug-2: A modularized multi-modal foundation model across text, image and video. In ICML, volume 202 of Proceedings of Machine Learning Research, pages 38728-38748. PMLR, 2023.

[45] Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and Xiang Yue. Machine unlearning of pre-trained large language models. CoRR, abs/2402.15159, 2024.

[46] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. CoRR, abs/2310.10683, 2023.

[47] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. CoRR, abs/2306.13549, 2023.

[48] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. CoRR, abs/2308.02490, 2023.

[49] Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models. CoRR, abs/2401.13601, 2024.

[50] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. CoRR, abs/2302.00923, 2023.

[51] Jiachen Zhao, Zhun Deng, David Madras, James Zou, and Mengye Ren. Learning and forgetting unsafe examples in large language models. CoRR, abs/2312.12736, 2023.

[52] Zangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xiangyu Yue, and Yang You. Preventing zero-shot transfer degradation in continual learning of vision-language models. In ICCV, pages 19068-19079. IEEE, 2023.

[53] Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, and Danqi Chen. Mquake: Assessing knowledge editing in language models via multi-hop questions. In EMNLP, pages 15686-15702. Association for Computational Linguistics, 2023.

[54] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. CoRR, abs/2304.10592, 2023.
