# DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ 

Jonas Belouadi ${ }^{*} \quad$ Simone Paolo Ponzetto ${ }^{\dagger} \quad$ Steffen Eger ${ }^{*}$<br>Natural Language Learning Group, Data and Web Science Group ${ }^{\dagger}$<br>University of Mannheim, Germany<br>jonas.belouadi@posteo.net


#### Abstract

Creating high-quality scientific figures can be time-consuming and challenging, even though sketching ideas on paper is relatively easy. Furthermore, recreating existing figures that are not stored in formats preserving semantic information is equally complex. To tackle this problem, we introduce DeTiK $Z_{\text {IFY, a novel }}$ multimodal language model that automatically synthesizes scientific figures as semantics-preserving TikZ graphics programs based on sketches and existing figures. To achieve this, we create three new datasets: DAT $\mathrm{IK}_{\mathrm{v} 2}$, the largest TikZ dataset to date, containing over 360k human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn sketches with their corresponding scientific figures; and $\mathrm{SCICAP}^{++}$, a collection of diverse scientific figures and associated metadata. We train $\mathrm{DeT}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}$ on $\mathrm{SCICAP}^{++}$and $\mathrm{DAT}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{V} 2}$, along with synthetically generated sketches learned from SkeтchFig. We also introduce an MCTS-based inference algorithm that enables $\mathrm{DE}_{\mathrm{I}} \mathrm{T}_{\text {IFY }}$ to iteratively refine its outputs without the need for additional training. Through both automatic and human evaluation, we demonstrate that DeTiKZifY outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ programs, with the MCTS algorithm effectively boosting its performance. We make our code, models, and datasets publicly available. ${ }^{1}$


## 1 Introduction

Creating high-quality scientific figures is similar to typesetting scientific documents in many ways. When it comes to typesetting, markup languages like ITEX enjoy widespread popularity, as exemplified by major machine learning conferences that either mandate or strongly encourage $\mathrm{IT}_{\mathrm{E}} \mathrm{X}$-formatted submissions. ${ }^{2}$ The advantages of using such languages go beyond producing high-quality outputs; documents expressed as high-level, semantics-preserving programs enhance accessibility, serve archival purposes, and remain easily editable and human-readable (facilitating language modeling applications; Moosavi et al., 2021; Lu et al., 2023). Consequently, efforts have been made to recover this type of information from outputs stored in lower-level vector graphics formats like PDF or SVG, or raster graphics formats (Desai et al., 2021; Blecher et al., 2024). At the other end of the spectrum, the versatility of IATEX comes with a steep learning curve, and typesetting can often be challenging for end users. In response, researchers have been working on assisting authors with certain aspects of the problem, such as typesetting math based on hand-drawn sketches (Kirsch, 2010; Wu et al., 2020).

Just like documents, scientific figures can also be created using markup languages. A popular example is the TikZ graphics language (Tantau, 2023), which can be integrated into ITTEX documents, providing comparable benefits and encountering similar challenges. However, unlike $\mathrm{IT}_{\mathrm{E}} \mathrm{X}$, the prospects of TikZ in research contexts remain largely unexplored. Although the promise of simplifying editing and[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-02.jpg?height=423&width=1396&top_left_y=233&top_left_x=362)

Figure 1: Overview of the DeTIKZ $\mathrm{Z}_{\mathrm{IFY}}$ architecture: A multimodal language model converts sketches or figures into TikZ programs, which are compiled by a ETEX engine. This provides a reward signal to the model via MCTS, allowing it to iteratively refine the output until satisfactory results are achieved.

enabling applications in visual understanding (Masry et al., 2022; Huang et al., 2023) is evident, there are currently no viable solutions for recovering graphics programs from compiled figures. Moreover, there is a lack of tools that assist in creating graphics programs, e.g., based on hand-drawn sketches, despite the clear demand for such approaches on the TEX Stack Exchange (TEX.SE), ${ }^{3}$ where nearly $10 \%$ of all questions revolve around TikZ, making it the most frequently discussed topic on the site. Addressing this gap could greatly improve the accessibility of existing figures and support researchers at all levels of programming proficiency when creating new ones, fostering diversity and inclusion. In response, we introduce $\mathrm{DET}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}$, a multimodal language model that automatically synthesizes TikZ programs for scientific figures and sketches (cf. Figure 1). Our key contributions are as follows:

(i) As part of $\mathrm{DeT}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}$, we introduce (a) $\mathrm{DAT}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{V} 2}$, a large TikZ dataset with over $360 \mathrm{k}$ human-created TikZ graphics; (b) SketchFig, a dataset of human-created sketches with paired scientific figures; and (c) SCICAP++, a large meta-dataset of scientific figures and associated texts.

(ii) We train $\mathrm{DeT}_{\text {IK }} \mathrm{Z}_{\mathrm{IFY}}$ on $\mathrm{SCICAP}^{++}$and $\mathrm{DAT}_{I K} \mathrm{Z}_{\mathrm{v} 2}$, augmented with synthetic sketches that mimic SкeтchFig. We demonstrate that DeTiKZifY can effectively synthesize TikZ programs for both existing scientific figures and sketches, outperforming the commercial large language models (LLMs) GPT-4V and Claude 3 (OpenAI, 2023b; Anthropic, 2024).

(iii) We also present an inference algorithm based on Monte Carlo Tree Search (MCTS) that is tailored to graphics programs and allows $\mathrm{DET}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}$ to iteratively refine its own outputs for a given computational budget, further improving performance without additional training.

## 2 Related Work

Image-to-IATE Conversion A closely related task is the translation of mathematical illustrations into IATE markup. In inspirational work, Kirsch (2010) tackle the recognition of single hand-drawn symbols to find corresponding LATEX commands. Subsequent works by Deng et al. (2017); Zhang et al. (2017, 2019); Wu et al. (2020); Wang and Liu (2021) expand on this concept to handle hand-drawn and scanned math formulas. Suzuki et al. (2003); Wang and Liu (2020); Blecher et al. (2024); Lv et al. (2023) further extend the scope by extracting $\mathrm{IAT}_{\mathrm{E}} \mathrm{X}$ formulas alongside text from entire documents.

Image Vectorization Similarly, converting (rasterized) figures into TikZ programs can be characterized as a form of image vectorization (Sun et al., 2007; Diebel, 2008; Ganin et al., 2018; Li et al., 2020; Ma et al., 2022; Zhu et al., 2024). Most existing methods vectorize images into low-level graphics primitives in the SVG format (Tian and Günther, 2024). Although this works well for specific domains like fonts, icons, and emoji (Lopes et al., 2019; Carlier et al., 2020; Reddy, 2021; Rodriguez et al., 2023b), it does not capture higher-level semantics and does not generalize well to our scientific context (cf. Appendix E). Closer to our work, Ellis et al. (2018) generate vector representations as graphics programs based on a limited subset of $\mathrm{EAT}_{\mathrm{E}} \mathrm{X}$ commands. Their approach even handles[^1]sketches, but their experiments are restricted to a synthetic dataset with only basic shapes of limited complexity. Belouadi et al. (2024) also generate TikZ programs, but their primary emphasis is on conditioning the generation on textual descriptions, with images serving only as a secondary input.

Code Generation As TikZ is implemented in the Turing-complete $\mathrm{T}_{\mathrm{E}} \mathrm{X}$ macro system (Erdweg and Ostermann, 2011), our work is also closely tied to code generation (Xu et al., 2022). Despite continuing progress in this field (Chen et al., 2021; Li et al., 2022, 2023; Guo et al., 2024; Lozhkov et al., 2024), most research concentrates on high-resource languages like Python, Java, and JavaScript (Zan et al., 2023), typically overlooking $\mathrm{T}_{\mathrm{E}} \mathrm{X}$ in evaluations. However, $\mathrm{T}_{\mathrm{E}} \mathrm{X}$ and $\mathrm{Ti} Z$ may still find their way into the training data, as demonstrated by the zero-shot ability of some models to understand and generate code in these languages (Bubeck et al., 2023; Belouadi et al., 2024; Sharma et al., 2024).

## 3 Datasets

We introduce DAT $\mathrm{I}_{\mathrm{I}} \mathrm{Z}_{\mathrm{v} 2}$, to our knowledge, the most comprehensive dataset of TikZ graphics to date; SkeтchFig, the first dataset comprising human-created sketches of scientific figures; and $\mathrm{ScICAP}^{++}$, a large-scale scientific figure dataset with rich metadata. See Appendix F for examples.

![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-03.jpg?height=43&width=819&top_left_y=1041&top_left_x=371)
graphics for training DeTIKZIFY. It is an expanded version of DAT ${ }_{I K} \mathrm{Z}_{\mathrm{v} 1}$ (Belouadi et al., 2024), incorporating graphics from the same sources, namely curated repositories, $\mathrm{T}_{\mathrm{E}} \mathrm{X}$.SE,

| Source | DATIKZ $_{\mathbf{v} \mathbf{1}}$ | DATIK $_{\mathbf{v} \mathbf{2}}$ |
| :--- | ---: | ---: |
| curated | 981 | 1566 |
| $\mathrm{~T}_{\mathrm{E} X . S E}$ | 29238 | 30609 |
| arXiv | 85656 | 326450 |
| artificial | 1957 | 1958 |
| all | 117832 | 360583 |

arXiv papers, and artificial examples. The key difference

Table 1: Breakdown of the number of unique TikZ graphics in DATIK $\mathrm{Z}_{\mathrm{v} 2}$ compared to its predecessor DATIK $\mathrm{Z}_{\mathrm{v} 1}$. is that $\mathrm{DAT}_{\mathrm{I}} \mathrm{Z}_{\mathrm{v} 2}$ includes all TikZ programs that compile with $\mathrm{T}_{\mathrm{E}} \mathrm{L}$ Live 2023, ${ }^{4}$ regardless of whether they have associated captions, which was a requirement for inclusion in DAT $\mathrm{T}_{\mathrm{I}} \mathrm{Z}_{\mathrm{v} 1}$ but is not needed for $\mathrm{DeT}_{\mathrm{I}} \mathrm{Z}_{\mathrm{IFY}}$. This approach allows us to create a dataset that is more than three times as large as its predecessor (cf. Table 1).

SketchFig To create realistic synthetic sketches of scientific figures in DaT $\mathrm{D}_{\mathrm{I}} \mathrm{Z}_{\mathrm{v} 2}$, we rely on examples of real human-created sketches. $\mathrm{T}_{\mathrm{E}} \mathrm{X}$.SE is a suitable source for collecting these, as users often illustrate their questions with sketches, and the answers provide the desired figure. We semiautomatically extract these figure-sketch pairs by first ranking all questions on the site that contain images based on their similarity to the string "a sketch of a scientific figure" using a multimodal vision encoder (Zhai et al., 2023). We retain the ones with high similarity scores, manually filter for true positives, and align them with the best matching figure provided in the answers. In total, we collect 549 figure-sketch pairs this way. As we also want to use this dataset for evaluation (cf. §6), we ensure that for a subset of these sketches, no code provided in the answers is included in DAT $\mathrm{D}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{v} 2}$.

SciCAP++ Beyond TikZ graphics, there is a much larger pool of figures where the underlying source is not available. Existing datasets that collect such figures frequently come with rich metadata, such as captions, OCR tokens, and paragraphs that mention the figures (Hsu et al., 2021; Karishma et al., 2023; Rodriguez et al., 2023a). Since such high-level descriptions are useful for pretraining (cf. §4; Liu et al., 2023b), we collect these datasets and merge them with the subset of figures in DATIK $Z_{\mathrm{v} 2}$ that have captions. This results in over $734 \mathrm{k}$ figure-text pairs, more than twice the size of DATIK $\mathrm{Z}_{\mathrm{v} 2}$.

## 4 The DeTiKZify Model

Building on previous work (Liu et al., 2023b,a; Dai et al., 2023; McKinzie et al., 2024), we build

![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-03.jpg?height=44&width=1385&top_left_y=2214&top_left_x=367)
where the vision encoder receives figures or sketches as input images, and the language model generates corresponding TikZ programs as output. We focus on code language models that have been pretrained on $\mathrm{T}_{\mathrm{E}} \mathrm{X}$, as this prior knowledge may be helpful for our task. All the models we end up using follow the LLaMA architecture (Touvron et al., 2023): CodeLLaMA (Rozière et al., 2023) has likely been trained on $\mathrm{T}_{\mathrm{E}} \mathrm{X}$ code from arXiv (Touvron et al., 2023), as has been TinyLLAMA (Zhang et al.,[^2]

2024), while DeEpSeek (code variant; Guo et al., 2024) was trained on $\mathrm{T}_{\mathrm{E}} \mathrm{X}$ code from GitHub. For the vision encoder, we use SIGLIP (Zhai et al., 2023), which has been trained on OCR annotations (Chen et al., 2023c) and demonstrates state-of-the-art understanding of text-rich images (Tong et al., 2024; Chen et al., 2023b), a crucial skill for our task. We then condition the LLMs on SigLIP's patch embedding vectors. To reduce the prompt length, we concatenate adjacent patch embeddings (Chen et al., 2023a). A feed-forward layer with dimensions $2 \delta_{\text {SIGLIP }} \times \delta_{\text {LLM }}$ serves as a connector, mapping image features of dimension $\delta_{\text {SIGLIP }}$ to the LLM word embedding space of dimension $\delta_{\text {LLM }}$.

![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-04.jpg?height=46&width=1383&top_left_y=557&top_left_x=371)
with approximately 1 billion parameters, and CodeLLaMA $7_{\text {в }}$ and DeepSeeK $7_{\mathrm{в}}$ with 7 billion parameters. When referring to specific variants of $\mathrm{DeT}_{I K} Z_{\mathrm{IFY}}$, we use the names $\mathrm{DeT}_{I K} Z_{\mathrm{IFY}}-\mathrm{TL}_{1.1 \mathrm{~B}}$, $\mathrm{DeT}_{\text {IK }} \mathrm{Z}_{\text {IFY- }} \mathrm{DS}_{1.3 \mathrm{~B}}, \mathrm{DeT}_{\text {IK }} \mathrm{Z}_{\mathrm{IFY}}-\mathrm{CL}_{7 \mathrm{~B}}$, and $\mathrm{DeT}_{\text {IK }} Z_{\text {IFY- }} \mathrm{DS}_{7 \mathrm{~B}}$, respectively. For all models, we use the $\mathrm{SoVIT}_{400 \mathrm{~m}}$ variant of SigLIP as the vision encoder. Following Liu et al. (2023b,a), we first pretrain the connector while keeping other model parameters frozen. We pretrain for one epoch on SciCAP++ with AdamW (Loshchilov and Hutter, 2019), a batch size of 256, a learning rate of 1e-3, and a cosine learning rate decay with a $3 \%$ warmup ratio. Next, we unfreeze the language model (keeping the vision encoder frozen) and fine-tune on examples from DAT $\mathrm{I}_{\mathrm{I}} \mathrm{Z}_{\mathrm{V} 2}$ that fit within a 2048 token context window. We use a batch size of 128 , a learning rate of $4 \mathrm{e}-5$, and train for three epochs.

Synthetic Sketches When training $\operatorname{DeT}_{I K} Z_{I F Y}$ on $D_{A} T_{I K} Z_{\mathrm{V} 2}$, we randomly replace figures with synthetic sketches $50 \%$ of the time. Sketches are generated on the fly, meaning that each time a figure is sampled as a sketch, a different synthetic sketch will be generated. Creating realistic sketches requires high-level image manipulation methods that go beyond traditional transformations like zooming or cropping. We, therefore, adopt Instruct-Pix2PIX (Brooks et al., 2023), a model capable of diversely editing images based on human instructions. We chose this model due to its remarkable zero-shot performance in generating synthetic sketches during our initial experiments. By then fine-tuning the model on SketchFig, we further improve its performance (cf. $\S 7$ and Appendix C).

## 5 Iterative Refinement with Monte Carlo Tree Search

Due to the inherent probabilistic nature of language models, generating valid TikZ programs during inference can be a challenging task. The generated code may not always comply with the syntactic and semantic rules of $\mathrm{T}_{\mathrm{E}} \mathrm{X}$ and TikZ, potentially leading to compilation errors. While constrained decoding algorithms can assist in guiding models towards generating valid programs (Ugare et al., 2024; Poesia et al., 2022; Scholak et al., 2021), these approaches are limited to programming languages defined by context-free grammars (CFGs). However, $\mathrm{T}_{\mathrm{E}} \mathrm{X}$ and TikZ are not defined by CFGs (Erdweg and Ostermann, 2011), rendering these methods ineffective for our purpose. Moreover, even if the generated code compiles successfully, fidelity errors such as misaligned elements, inconsistent scaling, repetitions, or mislabeling may only become apparent in the rendered output.

Despite these challenges, which make it difficult to guide $\mathrm{DET}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}$ based on intermediate states, we can still analyze completed outputs in a straightforward manner (e.g., by examining compiler diagnostics or comparing rendered outputs to the input image), allowing us to make informed decisions during subsequent sampling iterations. This concept of making decisions based on random sampling of the search space forms the core of Monte Carlo Tree Search (MCTS; Coulom, 2007). By integrating $\mathrm{DeT}_{\text {IK }} \mathrm{Z}_{\text {IFY }}$ with MCTS and adapting the standard MCTS algorithm to our problem domain, we can iteratively steer $\mathrm{DET}_{\text {IK }} \mathrm{Z}_{\text {IFY }}$ towards more promising regions of the output space (cf. Figure 1). In the following, we outline our fundamental approach, with further extensions discussed in Appendix D.

### 5.1 Integrating MCTS into DeTIK $Z_{\text {IFY }}$

MCTS is a versatile search algorithm that has been successfully applied to various domains, including board games (Silver et al., 2016, 2017), procedural content generation (Kartal et al., 2016a,b; Summerville et al., 2015), and more recently, guiding language models to achieve long-term goals (Brandfonbrener et al., 2024; Zhang et al., 2023b; Chaffin et al., 2022). The algorithm incrementally builds a search tree and repeatedly runs simulations until an exit condition is met or a computational budget is exhausted. In our context, at depth $n$, each node's state consists of $n$ lines of TikZ code, and edges represent continuations for generating the next line. Initially, MCTS starts with only an empty root node and then iteratively performs the following four steps (cf. Figure 2):
(i) Selection

![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-05.jpg?height=200&width=224&top_left_y=301&top_left_x=403)

(ii) Rollout

![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-05.jpg?height=201&width=214&top_left_y=298&top_left_x=701)

(iii) Expansion

![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-05.jpg?height=198&width=336&top_left_y=302&top_left_x=995)

(iv) Backpropagation

![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-05.jpg?height=198&width=338&top_left_y=302&top_left_x=1376)

Figure 2: An example of the four steps of an MCTS simulation: The selection policy (i) reaches a green backtracking node (normal nodes are blue), causing new nodes from the rollout (ii) to be added to the parent node during expansion (iii). The reward is backpropagated (iv) accordingly.

Selection Each simulation starts at the root node and successively selects child nodes based on a selection policy until a leaf node is reached. The policy determines which parts of the tree should be explored further, balancing the exploitation of high-value regions and exploration of less-visited areas. Following previous work, we use Upper Confidence Trees (UCT; Kocsis and Szepesvári, 2006) as our selection policy, iteratively selecting the successor node $i$ that maximizes the formula

$$
\begin{equation*}
\operatorname{UCT}(i)=\frac{\sum_{j=1}^{n_{i}} V_{i, j}}{n_{i}}+c \sqrt{\frac{\ln \left(n_{\mathrm{p}(i)}\right)}{n_{i}}} \tag{1}
\end{equation*}
$$

where $V_{i, j} \in[-1,1]$ is the estimated value of $i$ at the $j$ th visit, $n_{i}$ and $n_{\mathrm{p}(i)}$ are the visit counts at $i$ and its parent $\mathrm{p}(i)$, respectively, and $c$ is a coefficient that controls the degree of exploration.

Rollout Once a leaf node is selected, we utilize $\mathrm{DeT}_{\mathrm{IK}} Z_{\mathrm{IFY}}$ as a rollout policy. By conditioning it on the node's state, we continue to sample TikZ code until the end-of-sequence token is encountered. This so-called rollout is then stored for reuse in the subsequent steps.

Expansion Next, the tree is expanded by adding nodes from the rollout as new leaf nodes. While most implementations add only one node (i.e., one line of TikZ code) per simulation, computing rollouts with LLMs is computationally expensive. Therefore, inspired by MCTS for real-time settings (Soemers et al., 2016), we instead add multiple nodes. Specifically, we add $\sqrt{|r|-d_{l}}$ new nodes, where $|r|$ is the number of lines in rollout $r$ and $d_{l}$ is the depth of the old leaf node $l$. This approach allows our tree to grow quickly in early simulations while converging to the standard case in the long run. To enable the tree to grow in multiple directions, we also introduce backtracking nodes (Brandfonbrener et al., 2024). For each added node $i$, we add a backtracking node as a sibling that mirrors the parent node $\mathrm{p}(i)$. When a backtracking node is expanded, its descendants are added to $p(i)$, ensuring that the backtracking node remains a leaf. This enables a practically infinite search space anywhere in the tree while still maintaining a bounded branching factor.

Backpropagation Finally, we calculate the value for rollout $r$ using a predefined reward function (cf. §5.2) and backpropagate it to every node $i$ on the path from the root node to the newly added nodes by appending it to $\boldsymbol{V}_{i,:}$. We also increment the visit counts $n_{i}$ for the same nodes. For backtracking nodes, only the visit counts are updated. Finally, we check any exit conditions. If MCTS terminates, we return the TikZ program of the rollout that achieved the highest value.

### 5.2 Reward Functions

We explore two distinct reward functions to guide the search process. The first reward function utilizes compiler diagnostics to identify documents that compile successfully. The second reward function provides a visual signal based on perceptual image similarity, which, in addition, helps find TikZ programs that better match the input image. We explore further reward functions in Appendix D.

Compiler Diagnostics The diagnostics-based reward function is based on analyzing the log file from compiling the generated TikZ program. We assign rewards according to the error state and whether an output file was produced. The reward function is defined as follows:

$$
V_{i, j}=\left\{\begin{align*}
1 & \text { if the code compiles without issues }  \tag{2}\\
0 & \text { if the code compiles with recoverable errors } \\
-1 & \text { if compilation fails due to a fatal error. }
\end{align*}\right.
$$

| Models | Reference Figures |  |  |  |  |  |  | Synthetic Sketches |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\mathbf{M T E}_{\uparrow}$ | $\mathbf{c B L E U}_{\uparrow}$ | $\mathbf{T E D}_{\downarrow}$ | $\operatorname{DSIM}_{\uparrow}$ | $\operatorname{SSIM}_{\uparrow}$ | $\mathbf{K I D}_{\downarrow}$ | $\underline{\mathbf{A V G}_{\uparrow}}$ | $\overline{\mathbf{M T E}}_{\uparrow}$ | $\mathbf{c B L E U}_{\uparrow}$ | $\mathbf{T E D}_{\downarrow}$ | $\operatorname{DSIM}_{\uparrow}$ | $\operatorname{SSIM}_{\uparrow}$ | $\mathbf{K I D}_{\downarrow}$ | $\underline{\mathbf{A V G}}_{\uparrow}$ |
| Claude 3 | 51.812 | 0.111 | 57.389 | 64.896 | 83.372 | 17.822 | 0.148 | 50.156 | 0.024 | 59.731 | 59.102 | 73.954 | 29.541 | 0.189 |
| GPT-4V | 61.975 | 0.286 | 57.178 | 69.741 | 86.215 | 6.714 | 0.612 | 54.126 | 0.024 | 60.298 | 61.98 | 75.687 | 33.203 | 0.15 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-06.jpg?height=46&width=119&top_left_y=414&top_left_x=376) | $\underline{88.03}$ | 1.168 | $\overline{58.815}$ | 65.538 | 84.161 | $\overline{15.747}$ | 0.207 | $\underline{90.597}$ | 0.502 | 60.202 | 60.585 | 77.947 | 21.851 | 0.454 |
| DT-DS $1.3 \mathrm{~B}$ | 83.771 | 1.336 | 57.661 | 68.659 | 86.079 | 11.536 | 0.572 | 87.446 | 0.541 | 60.112 | 62.756 | 79.097 | $\underline{17.334}$ | 0.642 |
| DT-CL $_{7 \mathrm{~B}}$ | 88.593 | $\underline{1.477}$ | 56.893 | $\underline{72.315}$ | $\underline{87.466}$ | 8.301 | $\underline{0.869}$ | 91.221 | $\underline{0.555}$ | 59.563 | 65.118 | $\underline{79.717}$ | 12.207 | $\underline{0.941}$ |
| $\underline{\text { DT-DS }}_{7 \mathrm{~b}}$ | 82.366 | $\overline{1.815}$ | 57.227 | $\overline{73.01}$ | $\overline{88.323}$ | 5.951 | $\overline{0.965}$ | 89.299 | 0.69 | 59.693 | $\overline{65.198}$ | $\overline{80.207}$ | 12.207 | $\overline{0.965}$ |

Table 2: System-level scores for output-driven inference (DETIK $\mathrm{ZIFY}^{\text {abbreviated as DT). Bold and }}$ underlined values indicate the best and second-best scores for each metric column, respectively. Cell shading reflects the relative score magnitudes across input types. Arrows indicate metric directionality.

Self-Assessed Perceptual Similarity (SelfSim) SelfSim computes the reward as the perceptual similarity (Zhang et al., 2018) between the input image and the compiled output figure. We hypothesize that $\mathrm{DeT}_{\text {IK }} \mathrm{Z}_{\mathrm{IFY}}$ itself can assess this similarity, enabling the model to guide its own search process. To achieve this, we encode both images into embedding vectors using DeTIKZIFY's vision encoder and calculate SelfSim as their cosine similarity (Fu et al., 2023; Hessel et al., 2021). In cases where compilation fails, we assign a reward of -1 . In $\S 7$, we demonstrate that SelfSim correlates well with human judgments and outperforms other baseline methods.

## 6 Experiments

Before training on $\mathrm{DAT}_{\mathrm{I}} \mathrm{Z}_{\mathrm{v} 2}$, we extract $1 \mathrm{k}$ samples to serve as our test set for an automatic evaluation and additionally generate synthetic sketches. Addressing data leakage from pretraining to testing, we only include items created after the cut-off date of CodeLLAMA and exclude repositories that may have been used in training DeepSeek. We also use an $n$-gram matching algorithm to prevent cross-contamination with our train split (OpenAI, 2023a). For a human evaluation involving real, human-created sketches, we also select 100 items from SKETchFig that do not overlap with $\mathrm{DAT}_{\mathrm{I} K} \mathrm{Z}_{\mathrm{v} 2}$ (cf. §3). Across all models, we set the temperature to 0.8 and the exploration coefficient $c$ to 0.6. We provide examples of real and synthetic sketches as well as generated outputs in Appendix F.

Baselines Given Claude 3 and GPT-4V's potential for our task (cf. §2), we use them as baselines. Similar to DeTIKZIFY, we instruct these models to generate TikZ programs for given images. However, as proprietary chatbots, they often mix code and natural language (Zhang et al., 2023c; Belouadi et al., 2024) and do not expose the internals needed to compute SelfSim. This makes it impractical to apply our MCTS-based refinement algorithm, which is designed for code-only outputs and open models. Instead, we compare our approach to equivalent chat-oriented refinement methods, i.e., we use Self-Refine as an alternative to diagnostics-based MCTS and Visual Self-Refine as an alternative to SelfSim-based MCTS (Madaan et al., 2023; cf. Appendix C for additional inference details). In Appendix E, we also explore SVG as an alternative to TikZ but find it less effective for our domain.

### 6.1 Automatic Evaluation

We introduce two inference tasks to automatically evaluate our models on the test split of DATIK $\mathrm{Z}_{\mathrm{v} 2}$. During output-driven inference (OI), we employ the diagnostics-based reward and use successful compilation as an early exit condition (we consider compilation successful if an output artifact is produced). For time-budgeted inference (TI), we use the more fine-grained SelfSim-based reward and continue from OI until a computational budget of 10 minutes is exhausted (cf. Brandfonbrener et al., 2024), investigating the extent of achievable improvement. We report results for the two use cases where either (rasterized) reference figures or (synthetic) sketches serve as model inputs (cf. §1). Due to high inference costs, we only evaluate commercial Claude 3 and GPT-4V in OI using Self-Refine, leaving TI with Visual Self-Refine for human evaluation. We evaluate the following properties:

Code Similarity To measure the similarity between generated and reference TikZ programs, we use CrystalBLEU (cBLEU), a variant of BLEU optimized for evaluating code (Eghbali and Pradel, 2023; Papineni et al., 2002), and the TEX Edit Distance (TED), our adapted version of the Extended Edit Distance (Stanchev et al., 2019) combined with a TEX tokenizer.

| Models | Reference Figures |  |  |  |  |  |  | Synthetic Sketches |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | MST $_{\uparrow}$ | $\mathbf{c B L E U}_{\uparrow}$ | $\mathbf{T E D}_{\downarrow}$ | $\operatorname{DSIM}_{\uparrow}$ | $\mathbf{S S I M}_{\uparrow}$ | $\mathbf{K I D}_{\downarrow}$ | $\underline{\mathbf{A V G}_{\uparrow}}$ | MST $_{\uparrow}$ | $\mathbf{c B L E U}_{\uparrow}$ | $\mathbf{T E D}_{\downarrow}$ | $\operatorname{DSIM}_{\uparrow}$ | $\operatorname{SSIm}_{\uparrow}$ | $\mathbf{K I D}_{\downarrow}$ | $\underline{\mathbf{A V G}}_{\uparrow}$ |
| DT-TL $1.1 \mathrm{~B}$ | 33.775 | -0.011 | -2.001 | +8.704 | +5.561 | -12.146 | 0.128 | 35.975 | +0.094 | -0.628 | +5.82 | +3.026 | +0.854 | 0.014 |
| DT-DS $1.3 \mathrm{~B}$ | 29.975 | -0.028 | -1.303 | +8.464 | +5.108 | -8.728 | 0.531 | 32.429 | +0.061 | -0.504 | +5.573 | +2.685 | +5.493 | 0.22 |
| $\mathrm{DT}^{-} \mathrm{CL}_{7 \mathrm{~B}}$ | 25.124 | +0.07 | -1.351 | +7.797 | +4.93 | -4.868 | 0.876 | ![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-07.jpg?height=34&width=90&top_left_y=397&top_left_x=1128) | +0.073 | -0.468 | +5.079 | +2.455 | +5.493 | 0.681 |
| DT-DS $_{7 \mathrm{~b}}$ | 24.145 | $\overline{-0.073}$ | -1.542 | +6.974 | +3.893 | $\overline{-0.946}$ | 0.76 | 26.195 | $\overline{+0.054}$ | $\overline{-0.696}$ | +4.887 | $\overline{+2.241}$ | $\overline{+1.099}$ | $\overline{0.994}$ |

Table 3: System-level scores for time-budgeted inference, displaying relative changes for metrics shared with output-driven inference (Table 2; colored green for improvements and red for declines) and absolute scores for independent metrics. Bold and underlined values indicate the best and second-best absolute scores for each metric column, respectively. Arrows indicate metric directionality.

Image Similarity In addition to SELFSIM (SSIM), which can also be used as a metric, we report DreamSim (DSim; Fu et al., 2023), a fine-tuned metric for perceptual similarity. We also compute the Kernel Inception Distance (KID $\times 10^{3}$; Bińkowski et al., 2018), which assesses the overall quality of generated figures by comparing their distribution with the distribution of reference figures. These metrics are always computed by comparing the generated figures to the reference figures, regardless of what the model receives as input.

Average Similarity We also compute the arithmetic mean of the min-max normalized scores of each code and image similarity metric (AVG). While this aggregate measure may not capture every nuance, it serves as a useful summary statistic of each model's relative performance.

Efficiency For OI, we compute the Mean Token Efficiency (MTE) as the $10 \%$ winsorized mean of the ratio of the number of tokens in the final TikZ program to the total number of tokens generated to arrive at that program. For TI, we instead compute the Mean Sampling Throughput (MST), measuring the throughput of unique TikZ graphics for the given budget.

Results Table 2 presents the system-level metric scores for OI. As expected, the scores for reference figures are, on average, $38 \%$ higher than those for synthetic sketches, but similar patterns emerge across both input types. $\mathrm{DET}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}-\mathrm{CL}_{7_{\mathrm{B}}}$ and $\mathrm{DET}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}-\mathrm{DS}_{7_{\mathrm{B}}}$ consistently outperform all other models, achieving AVG scores of $0.869 \& 0.965$ for figures and $0.941 \& 0.965$ for sketches, respectively. In contrast, GPT-4V reaches AVG scores of only 0.612 and 0.15 , placing it in competition with the smaller $1 \mathrm{~b}$ models: for figures, GPT-4V surpasses $\mathrm{DET}_{I K} Z_{I F Y}-\mathrm{TL}_{1.1 \mathrm{1}}$ and $\mathrm{DeT}_{\text {IK }} Z_{\text {IFY- }} \mathrm{DS}_{1.3 \mathrm{~B}}$, which achieve scores of 0.207 and 0.572 , respectively. However, these smaller models outperform GPT-4V on sketches, where they achieve scores of 0.454 and 0.642. ClaUde 3 trails behind all our models, with an AVG of only 0.148 and 0.189 . When examining individual similarity metrics, DETIKZIFY-DS $7_{\mathrm{B}}$, the top-performing DeTIKZIFY model overall, surpasses GPT-4V, the best baseline, by more than 3 pp (percentage points) on average for DreamSim and SelFSim, while maintaining a noticeably lower KID. In terms of cBLEU, GPT-4V, and ClaUde 3 only reach $6.5-18.5 \%$ of the performance achieved by the lowest-scoring DeTIK $Z_{\text {IFY model ( }} \mathrm{DeT}_{\text {IK }} Z_{\text {IFY- }}-\mathrm{LL}_{1.1 . \mathrm{B}}$ ). The differences in TED are less pronounced, possibly due to the influence of boilerplate code, which cBLEU inherently ignores.

For efficiency, all DETIKZIFY models demonstrate an MTE of $82-91 \%$, indicating that only 1-2 out of 10 inference runs require a second simulation to generate a compilable TikZ program. Interestingly, the model size does not seem to particularly influence this score, with the pretraining setup appearing to be the key factor instead. For instance, $\mathrm{DeT}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}-\mathrm{TL}_{1.1 \mathrm{~B}}$ and $\mathrm{DeT}_{\mathrm{IK}} Z_{\mathrm{IFY}}-\mathrm{CL}_{7 \mathrm{~B}}$ share a similar pretraining setup and exhibit comparable MTE values, as do DET $\mathrm{D}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}-\mathrm{DS}_{1.3 \mathrm{~B}}$ and $\mathrm{DET}_{\text {IK }} \mathrm{Z}_{\mathrm{IFY}}-\mathrm{DS}_{7 \text { B }}$. We can further observe that (i) MTE is generally higher for sketches compared to figures, and (ii) for figures, the MTE of similarly pretrained models is inversely correlated with their scores on other metrics. These phenomena likely stem from models making fewer mistakes when the input is less detailed or when their understanding of it is limited-a finding that aligns well with other studies (Tong et al., 2024). Compared to DeTiKZify, Claude 3 and GPT-4V perform considerably worse, with an MTE of only $50-62 \%$. Notably, for these models, $98.5 \%$ of the items already compile after the initial Self-Refine step, meaning that this inefficacy primarily originates from the natural language texts surrounding the code and that Self-Refine is nearly equivalent to regular sampling-based inference.

The results for $\mathrm{DeT}_{\mathrm{I}} \mathrm{Z}_{\mathrm{IFY}}$ on TI are presented in Table 3. Remarkably, increasing the computational budget for MCTS improves nearly all metrics for both reference figures and sketches as input without requiring access to any additional knowledge. The improvement with sketches is particularly noteworthy, as it demonstrates that the refinement process enhances the desired properties even when
![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-08.jpg?height=516&width=1394&top_left_y=238&top_left_x=362)

Figure 3: Bivariate distributions of BWS scores (higher is better) using kernel density estimation (left) and log-linear regression over TI reward scores for different generation strategies over time (right).

the model input type differs from the one used for evaluation. The 2.2-5.6pp increase of SelfSim for all models is not surprising since it serves as the reward signal we optimize, but DREAMSim and TED also increase by $4.9-8.7 \mathrm{pp}$ and $0.5-2 \mathrm{pp}$, respectively, demonstrating the efficacy of our approach. While KID improves by $1-12.1$ points with reference figures, it drops by $0.9-5.5$ points with sketches. We believe this is because sketches often omit minor details, such as axis tick labels, which is reflected more in the output of the TI models, biasing their overall output distributions. Therefore, we consider the substantial improvement of metrics capturing instance-level similarities to be more important. For cBLEU, we observe only minor changes (less than $\pm 0.1 \mathrm{pp}$ ), aligning with findings that BLEU-based metrics become less effective as performance increases (Ma et al., 2019). The MST and AVG reveal that, although $1 \mathrm{~b}$ models produce more unique outputs within the time frame compared to their larger 7 b counterparts (30-36 vs. 24.1-26.2), they still fail to close the overall gap in performance, with AVG scores ranging between $0.014-0.531$ compared to $0.681-0.994$ for $7 \mathrm{~b}$ models.

Overall, all $\mathrm{DET}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}$ models are capable of generating compilable outputs with reasonable efficiency. Upon examination of these outputs, it becomes evident that the $7 \mathrm{~b}$ models, particularly $\mathrm{DET}_{\text {IK }} \mathrm{Z}_{\text {IFY- }}$ $\mathrm{DS}_{7 \mathrm{~B}}$, consistently outperform both CLAUDE 3 and GPT-4V, whose performance is more comparable to the $1 \mathrm{~b}$ range. Increasing the computational budget for $\mathrm{DET}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}$ further improves performance.

### 6.2 Human Evaluation

To further assess the quality of the generated figures, we perform a human evaluation on SketchFig using Best-Worst Scaling (BWS; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). In this process, for each reference figure, we present annotators with a tuple of generated figures and ask them to identify the most and least perceptually similar figure. We then transform this data into scores ranging from -1 (poor) to 1 (excellent) by calculating the difference between the proportion of times a figure is selected as the best and the proportion of times it is chosen as the worst (Orme, 2009). To keep the workload manageable, we focus on the most promising $\mathrm{DeT}_{\text {IK }} Z_{\text {IFY model }}$ (DETIK $Z_{\text {IFY }}-\mathrm{DS}_{7 \mathrm{~B}}$ ) and the strongest baseline (GPT-4V). Building upon the automatic evaluation, we assess these models in the OI and TI configurations, using either reference figures or human-created sketches as input. For each input type, we engage six unique expert annotators (cf. Appendix B for more details).

Results Figure 3 (left) shows kernel density estimates for the computed BWS scores, revealing intriguing findings that are consistent across input types. In contrast to the automatic evaluation,

![](https://cdn.mathpix.com/cropped/2024_06_04_7b65f4a557d159098786g-08.jpg?height=41&width=1385&top_left_y=2164&top_left_x=370)
could be attributed to the fact that $\mathrm{T}_{\mathrm{E}} \mathrm{X}$.SE, the sole source of SketchFig, emphasizes minimum working examples, a type on which GPT-4V particularly excels (Belouadi et al., 2024). However, when we increase the computational budget, as in $\mathrm{DET}_{\mathrm{IK}} \mathrm{Z}_{\mathrm{IFY}}-\mathrm{DS}_{7_{\mathrm{B}}}$ (TI), it not only improves over OI results ( $\mu=0.39$; in line with automatic evaluation) but also surpasses GPT-4V in both configurations by a considerable margin. Interestingly, GPT-4V's performance in TI ( $\mu=-0.16$ ) is lower than its performance in OI, indicating that GPT-4V (TI) struggles to refine its own outputs effectively and quickly deteriorates. Overall, this demonstrates how difficult it is for models to refine their own outputs and highlights the effectiveness of our MCTS-based approach.

## 7 Analysis

In this section, we delve deeper into our methodologies and evaluation strategies. We also demonstrate that our models are not affected by memorization of the training data, as shown in Appendix E.

Correlating Humans and Metrics To assess the reliability of our human evaluation results, we investigate the agreement between annotators. To this end, we calculate the split-half reliability (SHR; Kiritchenko and Mohammad, 2017) by randomly splitting our annotations into two subsets, computing BWS scores for each subset, and measuring their correlation with Spearman's $\rho$. The SHR values of 0.66 for images and 0.68 for sketches indicate a moderate to strong correlation between annotators, supporting the validity of our human evaluation results. Motivated by these findings, we explore whether metrics that also assess perceptual

| Metric | Segment | System |
| :--- | :---: | :---: |
| LPIPS | 0.231 | $\underline{\underline{0.642}}$ |
| DISTS | 0.326 | $\underline{0.642}$ |
| DSIM | $\underline{0.427}$ | $\mathbf{0 . 9 5 4}$ |
| SSIM | $\mathbf{0 . 4 4 1}$ | $\underline{0.642}$ |

Table 4: Correlations of image similarity metrics with humans at the segment and system level. similarity (i.e., SelfSim and DreamSim) correlate with these human judgments. We again calculate Spearman's $\rho$ and show the average correlations (David M. Corey and Burke, 1998) at the segment and system level in Table 4. For comparison, we also include the popular LPIPS and DISTS metrics (Zhang et al., 2018; Ding et al., 2020). At the segment level, SelFSim outperforms all other metrics, which is remarkable considering it is the only untrained metric. Segment-level performance is particularly important for fine-grained reward functions, justifying our choice of SelfSim in our MCTS algorithm. At the system level, DreamSim performs the best, showcasing its strength in evaluation settings.

Synthetic Sketch Quality We also assess the quality of our synthetic sketches by measuring their congruence coefficient (Lorenzo-Seva and ten Berge, 2006) with real sketches. We embed humancreated figure-sketch pairs from SKetchFig using SigLIP, subtract each sketch embedding from the corresponding figure embedding to obtain local sketch vectors, and perform a single-component Principal Component Analysis to derive a global sketch vector (Zou et al., 2023). We repeat this process for synthetic sketches generated for the test split of $\mathrm{DAT}_{\mathrm{I}} \mathrm{Z}_{\mathrm{v} 2}$ and compare the global vectors using cosine similarity. Base Instruct-Pix2PiX generates synthetic sketches with a congruence coefficient of 0.66 , which increases to 0.7 after fine-tuning. These results demonstrate a high correlation with human-created sketches, suggesting that our generated sketches are of good quality.

MCTS Convergence To gain insights into the long-term characteristics of our MCTS algorithm, we visualize the trends in achieved TI reward scores over time in Figure 3 (right) and compare them to conventional sampling-based inference. As expected, sampling does not lead to improvements over time due to the absence of a feedback loop. In contrast, MCTS consistently improves throughout the entire time frame, and even at the end of our budget of 10 minutes, it does not appear to converge, suggesting potential additional gains for larger budgets. Apart from this, MCTS is not only more effective but also faster. With an average MST of 25.17 , compared to 18.7 for sampling, our MCTS algorithm generates considerably more unique TikZ programs within the same amount of time.

## 8 Conclusion

In this work, we showcase the potential of $\mathrm{DET}_{\mathrm{I}} \mathrm{Z}_{\mathrm{IFY}}$ in generating TikZ programs for two practical use cases. First, it can convert existing figures from lower-level formats into TikZ, paving the way for semantic image editing and downstream tasks (Zhang et al., 2023a). Second, it can develop hand-drawn sketches into TikZ graphics, which could aid researchers in creating high-quality scientific illustrations. In both cases, DETIKZIFY substantially outperforms the commercial LLMs GPT-4V and Claude 3 despite its presumably much smaller size. We hope that our datasets (DaTiK $\mathrm{Z}_{\mathrm{v} 2}$, SketchFig, and SciCAP++), our method for generating synthetic sketches, and our MCTS-based inference algorithm will pave the way towards future research on graphics program synthesis and bolster the cause of open science. Potential concerns and limitations are addressed in Appendix A.

Looking ahead, we plan to extend our approach to other graphics languages, such as MetaPost or PSTricks (Hobby, 2014; Van Zandt, 2007). We also intend to explore alternatives to perceptual similarity as an MCTS reward signal, including per-pixel measures and point cloud metrics (Wang and Bovik, 2009; Wu et al., 2021). Furthermore, we aim to investigate reinforcement learning from reward functions, for example, using Direct Preference Optimization (Rafailov et al., 2023).

## Acknowledgments

We would like to express our sincere gratitude to the following individuals for their valuable contributions to our work: JiWoo Kim, Tommaso Green, Christoph Leiter, Ines Reinig, Martin Kerscher, Margret Keuper, Christopher Klamm, Daniil Larionov, Yanran Chen, Tornike Tsereteli, and Daniel Ruffinelli. Their assistance with our human evaluation campaign, proofreading, insightful discussions, and constructive comments have been invaluable. We also acknowledge the OpenMoji project for providing the open-source icons used throughout this work.

## References

Anthropic. 2024. The Claude 3 model family: Opus, Sonnet, Haiku.

Jonas Belouadi and Steffen Eger. 2023. UScore: An effective approach to fully unsupervised evaluation metrics for machine translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 358-374, Dubrovnik, Croatia. Association for Computational Linguistics.

Jonas Belouadi, Anne Lauscher, and Steffen Eger. 2024. AutomaTikZ: Text-guided synthesis of scientific vector graphics with TikZ. In The Twelfth International Conference on Learning Representations.

Mikołaj Bińkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. 2018. Demystifying MMD GANs. In International Conference on Learning Representations.

Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. 2024. Nougat: Neural optical understanding for academic documents. In The Twelfth International Conference on Learning Representations.

Ali Borji. 2023. Qualitative failures of image generation models and their application in detecting deepfakes. Image and Vision Computing, 137:104771.

David Brandfonbrener, Sibi Raja, Tarun Prasad, Chloe Loughridge, Jianang Yang, Simon Henniger, William E. Byrd, Robert Zinkov, and Nada Amin. 2024. Verified multi-step synthesis using large language models and monte carlo tree search. Preprint, arXiv:2402.08147.

Tim Brooks, Aleksander Holynski, and Alexei A. Efros. 2023. InstructPix2Pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18392-18402.

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with GPT-4. Preprint, arXiv:2303.12712.

Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte. 2020. Deepsvg: A hierarchical generative network for vector graphics animation. In Advances in Neural Information Processing Systems, volume 33, pages 16351-16361. Curran Associates, Inc.

Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations.

Antoine Chaffin, Vincent Claveau, and Ewa Kijak. 2022. PPL-MCTS: Constrained textual generation through discriminator-guided MCTS decoding. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2953-2967, Seattle, United States. Association for Computational Linguistics.

Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023a. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. Preprint, arXiv:2310.09478.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. Preprint, arXiv:2107.03374.

Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. 2023b. PaLI-3 vision language models: Smaller, faster, stronger. Preprint, arXiv:2310.09199.

Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme Ruiz, Andreas Peter Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. 2023c. PaLI: A jointly-scaled multilingual language-image model. In The Eleventh International Conference on Learning Representations.

Rémi Coulom. 2007. Efficient selectivity and backup operators in Monte-Carlo tree search. In Computers and Games, pages 72-83, Berlin, Heidelberg. Springer Berlin Heidelberg.

Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems.

Giannis Daras and Alex Dimakis. 2022. Discovering the hidden vocabulary of DALLE-2. In NeurIPS 2022 Workshop on Score-Based Methods.

William P. Dunlap David M. Corey and Michael J. Burke. 1998. Averaging correlations: Expected values and bias in combined pearson rs and fisher's z transformations. The Journal of General Psychology, 125(3):245-261.

Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, and Alexander M. Rush. 2017. Image-to-markup generation with coarse-to-fine attention. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 980-989. PMLR.

Harsh Desai, Pratik Kayal, and Mayank Singh. 2021. TabLeX: A benchmark dataset for structure and content information extraction from scientific tables. In Document Analysis and Recognition ICDAR 2021, pages 554-569, Cham. Springer International Publishing.

James Richard Diebel. 2008. Bayesian image vectorization: The probabilistic inversion of vector image rasterization. Ph.D. thesis, Stanford University, Stanford, CA, USA. AAI3332816.

Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli. 2020. Image quality assessment: Unifying structure and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(5):2567-2581.

Aryaz Eghbali and Michael Pradel. 2023. CrystalBLEU: Precisely and efficiently measuring the similarity of code. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, ASE '22, New York, NY, USA. Association for Computing Machinery.

Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. 2018. Learning to infer graphics programs from hand-drawn images. In Thirty-second Conference on Neural Information Processing Systems, pages 6062-6071.

Sebastian Thore Erdweg and Klaus Ostermann. 2011. Featherweight TeX and parser correctness. In Software Language Engineering, pages 397-416, Berlin, Heidelberg. Springer Berlin Heidelberg.

Stephanie Fu, Netanel Yakir Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. 2023. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. In Thirty-seventh Conference on Neural Information Processing Systems.

Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, and Oriol Vinyals. 2018. Synthesizing programs for images using reinforced adversarial learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1666-1675. PMLR.

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. DeepSeek-Coder: When the large language model meets programming - the rise of code intelligence. Preprint, arXiv:2401.14196.

Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7514-7528, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

John D. Hobby. 2014. MetaPost.

Ting-Yao Hsu, C Lee Giles, and Ting-Hao Huang. 2021. SciCap: Generating captions for scientific figures. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3258-3264, Punta Cana, Dominican Republic. Association for Computational Linguistics.

Chieh-Yang Huang, Ting-Yao Hsu, Ryan Rossi, Ani Nenkova, Sungchul Kim, Gromit Yeuk-Yin Chan, Eunyee Koh, C Lee Giles, and Ting-Hao Huang. 2023. Summaries as captions: Generating figure captions for scientific documents with automated text summarization. In Proceedings of the 16th International Natural Language Generation Conference, pages 80-92, Prague, Czechia. Association for Computational Linguistics.

Zeba Karishma, Shaurya Rohatgi, Kavya Shrinivas Puranik, Jian Wu, and C. Lee Giles. 2023. Acl-fig: A dataset for scientific figure classification. In Proceedings of the Workshop on Scientific Document Understanding co-located with 37th AAAI Conference on Artificial Inteligence (AAAI 2023), Remote, February 14, 2023, volume 3656 of CEUR Workshop Proceedings. CEUR-WS.org.

Bilal Kartal, Nick Sohre, and Stephen Guy. 2016a. Data driven Sokoban puzzle generation with Monte Carlo tree search. Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, 12(1):58-64.

Bilal Kartal, Nick Sohre, and Stephen Guy. 2016b. Generating Sokoban puzzle game levels with Monte Carlo tree search. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI-16, The IJCAI-16 Workshop on General Game Playing, pages 47-54. International Joint Conferences on Artificial Intelligence Organization.

Svetlana Kiritchenko and Saif Mohammad. 2017. Best-worst scaling more reliable than rating scales: A case study on sentiment intensity annotation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 465-470, Vancouver, Canada. Association for Computational Linguistics.

Svetlana Kiritchenko and Saif M. Mohammad. 2016. Capturing reliable fine-grained sentiment associations by crowdsourcing and best-worst scaling. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 811-817, San Diego, California. Association for Computational Linguistics.

Daniel Kirsch. 2010. Detexify: Recognition of hand-drawn LaTeX symbols. Diploma thesis, University of Münster, Münster, Germany, October.

Levente Kocsis and Csaba Szepesvári. 2006. Bandit based Monte-Carlo planning. In Machine Learning: ECML 2006, pages 282-293, Berlin, Heidelberg. Springer Berlin Heidelberg.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 957-966, Lille, France. PMLR.

Mengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer, and Deva Ramanan. 2019. Photo-Sketching: Inferring contour drawings from images. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1403-1412.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. Starcoder: may the source be with you! Preprint, arXiv:2305.06161.

Tzu-Mao Li, Michal Lukáč, Michaël Gharbi, and Jonathan Ragan-Kelley. 2020. Differentiable vector graphics rasterization for editing and learning. ACM Trans. Graph., 39(6).

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with AlphaCode. Science, 378(6624):1092-1097.

Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. Preprint, arXiv:2310.03744.

Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems.

Raphael Gontijo Lopes, David Ha, Douglas Eck, and Jonathon Shlens. 2019. A learned representation for scalable vector graphics. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).

Urbano Lorenzo-Seva and Jos M. F. ten Berge. 2006. Tucker's congruence coefficient as a meaningful index of factor similarity. Methodology: European Journal of Research Methods for the Behavioral and Social Sciences, 2(2):57-64.

Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.

Jordan J. Louviere, Terry N. Flynn, and A. A. J. Marley. 2015. Best-Worst Scaling: Theory, Methods and Applications. Cambridge University Press.

Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2024. Starcoder 2 and the stack v2: The next generation. Preprint, arXiv:2402.19173.

Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, and Min-Yen Kan. 2023. SCITAB: A challenging benchmark for compositional reasoning and claim verification on scientific tables. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7787-7813, Singapore. Association for Computational Linguistics.

Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, and Furu Wei. 2023. Kosmos-2.5: A multimodal literate model. Preprint, arXiv:2309.11419.

Qingsong Ma, Johnny Wei, Ondřej Bojar, and Yvette Graham. 2019. Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 62-90, Florence, Italy. Association for Computational Linguistics.

Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev, Nikita Orlov, Yun Fu, and Humphrey Shi. 2022. Towards layer-wise image vectorization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16314-16323.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems.

Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2263-2279, Dublin, Ireland. Association for Computational Linguistics.

R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz. 2023. How much do language models copy from their training data? evaluating linguistic novelty in text generation using RAVEN. Transactions of the Association for Computational Linguistics, 11:652-670.

Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. 2024. Mm1: Methods, analysis \& insights from multimodal llm pre-training. Preprint, arXiv:2403.09611.

Casey Meehan, Kamalika Chaudhuri, and Sanjoy Dasgupta. 2020. A three sample hypothesis test for evaluating generative models. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 3546-3556. PMLR.

Nafise Sadat Moosavi, Andreas Rücklé, Dan Roth, and Iryna Gurevych. 2021. SciGen: a dataset for reasoning-aware text generation from scientific tables. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).

OpenAI. 2023a. GPT-4 technical report. Preprint, arXiv:2303.08774.

OpenAI. 2023b. GPT-4V(ision) system card.

Bryan K. Orme. 2009. MaxDiff analysis: Simple counting, individual-level logit, and HB. Sawtooth Software Research Paper Series.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Sayak Paul. 2023. Instruction-tuning Stable Diffusion with InstructPix2Pix. Hugging Face Blog. Https://huggingface.co/blog/instruction-tuning-sd.

Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable code generation from pre-trained language models. In International Conference on Learning Representations.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '20. IEEE Press.

Vikas Raunak and Arul Menezes. 2022. Finding memo: Extractive memorization in constrained sequence generation tasks. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5153-5162, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Pradyumna Reddy. 2021. Im2Vec: Synthesizing vector graphics without vector supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 2124-2133.

J. A. Rodriguez, D. Vazquez, I. Laradji, M. Pedersoli, and P. Rodriguez. 2023a. OCR-VQGAN: Taming text-within-image generation. In 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 3678-3687, Los Alamitos, CA, USA. IEEE Computer Society.

Juan A. Rodriguez, Shubham Agarwal, Issam H. Laradji, Pau Rodriguez, David Vazquez, Christopher Pal, and Marco Pedersoli. 2023b. Starvector: Generating scalable vector graphics code from images. Preprint, arXiv:2312.11556.

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code LLaMA: Open foundation models for code. Preprint, arXiv:2308.12950.

Y. Rubner, C. Tomasi, and L.J. Guibas. 1998. A metric for distributions with applications to image databases. In Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271), pages 59-66.

Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays. 2016. The sketchy database: learning to retrieve badly drawn bunnies. ACM Trans. Graph., 35(4).

Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9895-9901, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, and Antonio Torralba. 2024. A vision check-up for language models. Preprint, arXiv:2401.01862.

David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2016. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. 2017. Mastering the game of go without human knowledge. Nature, 550(7676):354-359.

Dennis J. N. J. Soemers, Chiara F. Sironi, Torsten Schuster, and Mark H. M. Winands. 2016. Enhancements for real-time monte-carlo tree search in general video game playing. In 2016 IEEE Conference on Computational Intelligence and Games (CIG), pages 1-8.

Yurun Song, Junchen Zhao, and Lucia Specia. 2021. SentSim: Crosslingual semantic evaluation of machine translation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3143-3156, Online. Association for Computational Linguistics.

Peter Stanchev, Weiyue Wang, and Hermann Ney. 2019. EED: Extended edit distance measure for machine translation. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 514-520, Florence, Italy. Association for Computational Linguistics.

Adam Summerville, Shweta Philip, and Michael Mateas. 2015. MCMCTS PCG 4 SMB: Monte Carlo tree search to guide platformer level generation. Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, 11(3):68-74.

Jian Sun, Lin Liang, Fang Wen, and Heung-Yeung Shum. 2007. Image vectorization using optimized gradient meshes. ACM Trans. Graph., 26(3):11-es.

Masakazu Suzuki, Fumikazu Tamari, Ryoji Fukuda, Seiichi Uchida, and Toshihiro Kanahori. 2003. INFTY: an integrated OCR system for mathematical documents. In Proceedings of the 2003 ACM Symposium on Document Engineering, DocEng '03, page 95-104, New York, NY, USA. Association for Computing Machinery.

Till Tantau. 2023. The TikZ and PGF Packages.

Xingze Tian and Tobias Günther. 2024. A survey of smooth vector graphics: Recent advances in representation, creation, rasterization, and image vectorization. IEEE Transactions on Visualization and Computer Graphics, 30(3):1652-1671.

Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring the visual shortcomings of multimodal LLMs. Preprint, arXiv:2401.06209.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and efficient foundation language models. Preprint, arXiv:2302.13971.

Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, and Gagandeep Singh. 2024. Improving llm code generation with grammar augmentation. Preprint, arXiv:2403.01632.

Timothy van Zandt. 2007. PSTricks: PostScript macros for Generic TeX.

Zelun Wang and Jyh-Charn Liu. 2020. PDF2LaTeX: A deep learning system to convert mathematical documents from PDF to LaTeX. In Proceedings of the ACM Symposium on Document Engineering 2020, DocEng '20, New York, NY, USA. Association for Computing Machinery.

Zelun Wang and Jyh-Charn Liu. 2021. Translating math formula images to LaTeX sequences using deep neural networks with sequence-level training. International Journal on Document Analysis and Recognition (IJDAR), 24(1):63-75.

Zhou Wang and Alan C. Bovik. 2009. Mean squared error: Love it or leave it? a new look at signal fidelity measures. IEEE Signal Processing Magazine, 26(1):98-117.

Jin-Wen Wu, Fei Yin, Yan-Ming Zhang, Xu-Yao Zhang, and Cheng-Lin Liu. 2020. Handwritten mathematical expression recognition via paired adversarial learning. International Journal of Computer Vision, 128(10):2386-2401.

Tong Wu, Liang Pan, Junzhe Zhang, Tai WANG, Ziwei Liu, and Dahua Lin. 2021. Balanced Chamfer distance as a comprehensive metric for point cloud completion. In Advances in Neural Information Processing Systems.

Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In MAPS@PLDI 2022: 6th ACM SIGPLAN International Symposium on Machine Programming, San Diego, CA, USA, 13 June 2022, pages 1-10. ACM.

Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou. 2023. Large language models meet NL2Code: A survey. In Proceedings of the 61 st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7443-7464, Toronto, Canada. Association for Computational Linguistics.

Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11975-11986.

Jianshu Zhang, Jun Du, and Lirong Dai. 2017. A gru-based encoder-decoder approach with attention for online handwritten mathematical expression recognition. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 01, pages 902-907.

Peiying Zhang, Nanxuan Zhao, and Jing Liao. 2023a. Text-guided vector graphics customization. In SIGGRAPH Asia 2023 Conference Papers, SA '23, New York, NY, USA. Association for Computing Machinery.

Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLlama: An open-source small language model. Preprint, arXiv:2401.02385.

Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 586-595.

Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. 2023b. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations.

Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, and Xin Wang. 2023c. Controllable text-to-image generation with GPT-4. Preprint, arXiv:2305.18583.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In International Conference on Learning Representations.

Wei Zhang, Zhiqiang Bai, and Yuesheng Zhu. 2019. An improved approach based on cnn-rnns for mathematical expression recognition. In Proceedings of the 2019 4th International Conference on Multimedia Systems and Signal Processing, ICMSSP '19, page 57-61, New York, NY, USA. Association for Computing Machinery.

Wei Zhao, Goran Glavaš, Maxime Peyrard, Yang Gao, Robert West, and Steffen Eger. 2020. On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1656-1671, Online. Association for Computational Linguistics.

Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Linguistics.

Haokun Zhu, Juang Ian Chong, Teng Hu, Ran Yi, Yu-Kun Lai, and Paul L. Rosin. 2024. SAMVG: A multi-stage image vectorization model with the segment-anything model. In ICASSP 2024 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages $4350-4354$.

Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. 2023. Representation engineering: A top-down approach to ai transparency. Preprint, arXiv:2310.01405.
