# A General Theoretical Paradigm to Understand Learning from Human Preferences 

Mohammad Gheshlaghi Azar Mark Rowland<br>Daniel Guo<br>Daniele Calandriello<br>Michal Valko<br>Google DeepMind


#### Abstract

The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.

In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi P O$ for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of $\Psi \mathrm{PO}$ ) and to identify their potential pitfalls. We then consider another special case for $\Psi$ PO by setting $\Psi$ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.


## 1 Introduction

Learning from human preferences (Christiano et al., 2017) is a paradigm adopted in the natural language processing literature to better align pretrained (Radford et al., 2018; Ramachandran et al., 2016) and instruction-tuned (Wei et al., 2022) generative language models to human desiderata. It consists in first collecting large amounts of data where each datum is composed of a context, pairs of continuations of the context, also called generations, and a pairwise human preference that indicates which generation is the best. Then, a policy generating good generations given a context is learnt from the collected data. We frame the problem of learning from human preferences as an offline contextual bandit problem (Lu et al., 2010). The goal of this bandit problem is that given a context to choose an action (playing the role of the generation) which is most preferred by a human rater under the constraint that the resulting bandit policy should be close to some known reference policy. The constraint of staying close to a known reference policy can be satisfied e.g., by using KL regularisation (Geist et al., 2019) and its role is to avoid model drift (Lazaridou et al., 2020; Lu et al., 2020).

A prominent approach to tackle the problem of learning from human preferences is through reinforcement learning from human feedback (RLHF, Ouyang et al., 2022; Stiennon et al., 2020) in which first a reward model is trained in the form of a classifier of preferred and dispreferred actions. Then the bandit policy is trained through RL to maximize this learned reward model while minimizing the distance with the reference policy. Recently RLHF has been used successfully in solving the problem of aligning generative language models with human preferences (Ouyang et al., 2022). Furthermore recent works such as direct preference optimisation (DPO, Rafailov et al., 2023) and (SLiC-HF, Zhao et al., 2023) have shown that it is possible to optimize the bandit policy directly from human preferences without learning a reward model. They also have shown that on a selection of standard language
tasks they are competitive with the state of the art RLHF while they are simpler to implement and require less resources.

Despite this practical success, little is known regarding theoretical foundations of these practical methods. Notable exceptions, that consider specific special cases, are (Wang et al., 2023; Chen et al., 2022) and prior work on preference-based (Busa-Fekete et al., 2014, 2013) and dueling bandits and RL (Novoseller et al., 2020; Pacchiano et al., 2023). However, these theoretical works focus on providing theoretical guarantees in terms of regret bounds in the standard bandit setting and they do not deal with the practical setting of RLHF, DPO and SLiC-HF.

In this work, our focus is on bridging the gap between theory and practice by introducing a simple and general theoretical representation of the practical algorithms for learning from human preferences. In particular, we show that it is possible to characterise the objective functions of RLHF and DPO as special cases of a more general objective exclusively expressed in terms of pairwise preferences. We call this objective $\Psi$-preference optimisation ( $\Psi P 0$ ) objective, where $\Psi$ is an arbitrary non-deceasing mapping. We then analyze this objective function in the special cases of RLHF and DPO and investigate its potential pitfalls. Our theoretical investigation of RLHF and DPO reveals that in principle they can be both vulnerable to overfitting. This is due to the fact that those methods rely on the strong assumption that pairwise preferences can be substituted with ELo-score (pointwise rewards) via a Bradley-Terry (BT) modelisation (Bradley and Terry, 1952). In particular, this assumption could be problematic when the (sampled) preferences are deterministic or nearly deterministic as it leads to over-fitting to the preference dataset at the expense of ignoring the KL-regularisation term (see Sec. 4.2). We then present a simple solution to avoid the problem of overfitting, namely by setting $\Psi$ to identity in the $\Psi$ PO. This method is called Identity-PO (IPO) and by construction bypasses the BT modelisation assumption for preferences (see Sec. 5). Finally, we propose a practical solution, via a sampled loss function (see Sec. 5.2), to optimize this simplified version of $\Psi P O$ empirically and, we compare its performance with DPO on simple bandit examples, providing empirical support for our theoretical findings (see Sec. 5.3 and Sec. 5.4).

## 2 Notations

In the remaining, we build on the notations of DPO (Rafailov et al., 2023). Given a context $x \in \mathcal{X}$, where $\mathcal{X}$ is the finite space of contexts, we assume a finite action space $\mathcal{Y}$. A policy $\pi \in \Delta_{\mathcal{Y}}^{\mathcal{X}}$ associates to each context $x \in \mathcal{X}$ a discrete probability distribution $\pi(. \mid x) \in \Delta_{\mathcal{Y}}$ where $\Delta_{\mathcal{Y}}$ is the set of discrete distributions over $\mathcal{Y}$. We denote $\mu \in \Delta_{\mathcal{Y}}^{\mathcal{X}}$ the behavior policy. From a given context $x$, let $y, y^{\prime} \sim \mu(x)$ be two actions generated independently by the reference policy. These are then presented to human raters who express preferences for one of the generations, denoted as $y_{w} \succ y_{l}$ where $y_{w}$ and $y_{l}$ denote the preferred and dispreferred actions amongst $\left\{y, y^{\prime}\right\}$ respectively. We then write true human preference $p^{*}\left(y \succ y^{\prime} \mid x\right)$ the probability of $y$ being preferred to $y^{\prime}$ knowing the context $x$. The probability comes from the randomness of the choice of the human we ask for their preference. So $p^{*}\left(y \succ y^{\prime} \mid x\right)=\mathbb{E}_{h}\left[\mathbb{I}\left\{h\right.\right.$ prefers $y$ to $y^{\prime}$ given $\left.\left.x\right\}\right]$, where the expectation is over humans $h$. We also introduce the expected preference of a generation $y$ over a distribution $\mu$ knowing $x$, noted $p^{*}(y \succ \mu \mid x)$, via the following equation:

$$
p^{*}(y \succ \mu \mid x)=\underset{y^{\prime} \sim \mu(. \mid x)}{\mathbb{E}}\left[p^{*}\left(y \succ y^{\prime} \mid x\right)\right]
$$

For any two policy $\pi, \mu \in \Delta_{\mathcal{Y}}^{\mathcal{X}}$ and a context distribution $\rho$ we denote the total preference of policy $\pi$ to $\mu$ as

$$
p_{\rho}^{*}(\pi \succ \mu \mid x)=\underset{\substack{x \sim \rho \\ y \sim \pi(. \mid x)}}{\mathbb{E}}\left[p^{*}(y \succ \mu \mid x)\right]
$$

In practice, we do not observe $p^{*}$ directly, but samples $I\left(y, y^{\prime} \mid x\right)$ from a Bernoulli distribution with mean $p^{*}\left(y \succ y^{\prime} \mid x\right)$ (i.e., $I\left(y, y^{\prime} \mid x\right)$ is 1 with probability $p^{*}\left(y \succ y^{\prime} \mid x\right)$ and 0 otherwise). In particular, we assume we have access to the preferences through a dataset of rated generations $\mathcal{D}=\left(x_{i}, y_{i}, y_{i}^{\prime}\right)_{i=1}^{N}=$ $\left(x_{i}, y_{w, i} \succ y_{l, i}\right)_{i=i}^{N}$, where $N$ is the dataset size. In addition, for a general finite set $\mathcal{S}$, a discrete probability distribution $\eta \in \Delta_{\mathcal{S}}$ and a real function $f \in \mathbb{R}^{\mathcal{S}}$, we note the expectation of $f$ under $\eta$ as $\mathbb{E}_{s \sim \eta}[f(s)]=$ $\sum_{s \in \mathcal{S}} f(s) \eta(s)$. For a finite dataset $\mathcal{D}=\left(s_{i}\right)_{i=1}^{N}$, with $s_{i} \in \mathcal{S}$ for each $i$, and a real function $f \in \mathbb{R}^{\mathcal{S}}$, we denote the empirical expectation of $f$ under $\mathcal{D}$ as $\mathbb{E}_{s \sim D}[f(s)]=\frac{1}{N} \sum_{i=1}^{N} f\left(s_{i}\right)$.

## 3 Background

### 3.1 Reinforcement Learning from Human Feedback (RLHF)

The standard RLHF paradigm (Christiano et al., 2017; Stiennon et al., 2020) consists of two main stages: (i) learning the reward model; (ii) policy optimisation using the learned reward. Here we provide a recap of these stages.

### 3.1.1 Learning the Reward Model

Learning a reward model consists in training a binary classifier to discriminate between the preferred and dispreferred actions using a logistic regression loss. For the classifier, a popular choice is Bradley-Terry model: for a given context $x$ and action $y$, we denote the pointwise reward, which can also be interpreted as an Elo score, of $y$ given $x$ by $r(x, y)$. The Bradley-Terry model represents the preference function $p\left(y \succ y^{\prime} \mid x\right)$ (classifier) as a sigmoid of the difference of rewards:

$$
\begin{equation*}
p\left(y \succ y^{\prime} \mid x\right)=\sigma\left(r(x, y)-r\left(x, y^{\prime}\right)\right) \tag{1}
\end{equation*}
$$

where $\sigma(\cdot)$ denotes the sigmoid function and plays the role of normalisation. Given the dataset $\mathcal{D}=$ $\left(x_{i}, y_{w, i} \succ y_{l, i}\right)_{i=1}^{N}$ one can learn the reward function by optimizing the following logistic regression loss

$$
\begin{equation*}
\mathcal{L}(r)=-\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}\left[\log \left(p\left(y_{w} \succ y_{l} \mid x\right)\right)\right] \tag{2}
\end{equation*}
$$

Assuming that $p^{*}\left(y \succ y^{\prime} \mid x\right)$ conforms to the BradleyTerry model, one can show that as the size of the dataset $\mathcal{D}$ grows, $p\left(y \succ y^{\prime} \mid x\right)$ becomes a more and more accurate estimate of true $p^{*}\left(y \succ y^{\prime} \mid x\right)$ and in the limit converges to $p^{*}\left(y \succ y^{\prime} \mid x\right)$.

### 3.1.2 Policy Optimisation with the Learned Reward

Using the reward (Elo-score) $r(x, y)$ the RLHF objective is simply to optimize for the policy $\pi \in \Delta_{\mathcal{Y}}^{\mathcal{X}}$ that maximizes the expected reward while minimizing the distance between $\pi$ and some reference policy $\pi_{\mathrm{ref}} \in \Delta_{\mathcal{Y}}^{\mathcal{X}}$ through the following KL-regularized objective function:

$$
\begin{equation*}
J(\pi)=\mathbb{E}_{\pi}[r(x, y)]-\tau D_{\mathrm{KL}}\left(\pi \| \pi_{\mathrm{ref}}\right) \tag{3}
\end{equation*}
$$

in which the context $x$ is drawn from $\rho$ and the action $y$ is drawn from $\pi(. \mid x)$. The divergence $D_{\mathrm{KL}}\left(\pi \| \pi_{\text {ref }}\right)$ is defined as follows:

$$
D_{\mathrm{KL}}\left(\pi \| \pi_{\mathrm{ref}}\right)=\mathbb{E}_{x \sim \rho}\left[\mathrm{KL}\left(\pi(. \mid x) \| \pi_{\mathrm{ref}}(. \mid x)\right)\right]
$$

where:

$$
\mathrm{KL}\left(\pi(. \mid x) \| \pi_{\mathrm{ref}}(. \mid x)\right)=\mathbb{E}_{y \sim \pi(. \mid x)}\left[\log \left(\frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}\right)\right]
$$

The objective in Equation (3) is essentially optimized by PPO (Schulman et al., 2017) or similar approaches.

The combination of RLHF +PPO has been used with great success in practice (e.g., InsturctGPT and GPT4 Ouyang et al., 2022; OpenAI, 2023).

### 3.2 Direct Preference Optimisation

An alternative approach to the RL paradigm described above is direct preference optimisation (DPO; Rafailov et al., 2023), which avoids the training of a reward model altogether. The loss that DPO optimises, given an empirical dataset $\mathcal{D}$, as a function of $\pi$, is given by

$$
\begin{align*}
\min _{\pi} \mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}[-\log \sigma( & \left(\log \left(\frac{\pi\left(y_{w} \mid x\right)}{\pi\left(y_{l} \mid x\right)}\right)-\right. \\
& \left.\left.\tau \log \left(\frac{\pi_{\mathrm{ref}}\left(y_{w} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{l} \mid x\right)}\right)\right)\right] \tag{4}
\end{align*}
$$

In its population form, the loss takes on the form

$$
\begin{array}{r}
\min _{\pi} \underset{\substack{x, \rho \\
y, y^{\prime} \sim \mu}}{\mathbb{E}}\left[-p^{*}\left(y \succ y^{\prime} \mid x\right) \log \sigma\left(\tau \log \left(\frac{\pi(y \mid x)}{\pi\left(y^{\prime} \mid x\right)}\right)-\right.\right. \\
\left.\left.\tau \log \left(\frac{\pi_{\mathrm{ref}}(y \mid x)}{\pi_{\mathrm{ref}}\left(y^{\prime} \mid x\right)}\right)\right)\right] \tag{5}
\end{array}
$$

Rafailov et al. (2023) show that when (i) the BradleyTerry model in Equation (1) perfectly fits the preference data and (ii) the optimal reward function $r$ is obtained from the loss in Equation (2), then the global optimisers of the RLHF objective in Equation (3) and the DPO objective in Equation (5) perfectly coincide. In fact, this correspondence is true more generally; see Proposition 4 in Appendix B.

## 4 A General Objective for Preference Optimisation

A central conceptual contribution of the paper is to propose a general objective for RLHF, based on maximizing a non-linear function of preferences. To this end, we consider a general non-decreasing function $\Psi:[0,1] \rightarrow \mathbb{R}$, a reference policy $\pi_{\text {ref }} \in \Delta_{\mathcal{Y}}^{\mathcal{X}}$, and a real positive regularisation parameter $\tau \in \mathbb{R}_{+}^{*}$, and define the $\Psi$-preference optimisation objective ( $\Psi \mathrm{PO})$ as

$$
\begin{equation*}
\max _{\pi} \underset{\substack{x \sim \rho \\ y \sim \pi(.|| x) \\ y^{\prime} \sim \mu(\cdot \mid x)}}{\mathbb{E}}\left[\Psi\left(p^{*}\left(y \succ y^{\prime} \mid x\right)\right)\right]-\tau D_{\mathrm{KL}}\left(\pi \| \pi_{\mathrm{ref}}\right) \tag{6}
\end{equation*}
$$

This objective balances the maximisation of a potentially non-linear function of preference probabilities with the KL regularisation term which encourages policies to be close to the reference $\pi_{\text {ref }}$. This is motivated by the form of Equation (3), and we will see in the next subsection that it strictly generalises both RLHF and DPO, when the BT model holds.

### 4.1 A Deeper Analysis of DPO and RLHF

In the remaining, we omit the dependency on $x$ for the ease of notations. This is without losing generality and all the following results are true for all $x \in \operatorname{Supp}(\rho)$.

We first connect DPO and RLHF with the $\Psi$-preference objective in Equation (6), under the special choice of $\Psi(q)=\log (q /(1-q))$. More precisely, the following proposition establishes this connection.

Proposition 1. Suppose $\Psi(q)=\log (q /(1-q))$. When the Bradley-Terry model holds for $p^{*}$, that is, there exists $r: \mathcal{Y} \rightarrow \mathbb{R}$ such that

$$
p^{*}\left(y \succ y^{\prime}\right)=\sigma\left(r(y)-r\left(y^{\prime}\right)\right)
$$

then the optimal policy for Equation (6), for the RLHF objective in Equation (3), and for the standard DPO objective in Equation (5) are identical.

Proof. Note that under the assumption that the Bradley-Terry model holds, we have

$$
\begin{aligned}
\underset{y^{\prime} \sim \mu}{\mathbb{E}}\left[\Psi\left(p^{*}\left(y \succ y^{\prime}\right)\right)\right] & =\underset{y^{\prime} \sim \mu}{\mathbb{E}}\left[\Psi\left(\frac{e^{r(y)}}{e^{r(y)}+e^{r\left(y^{\prime}\right)}}\right)\right] \\
& =\underset{y^{\prime} \sim \mu}{\mathbb{E}}\left[\log \left(e^{r(y)} / e^{r\left(y^{\prime}\right)}\right)\right] \\
& =\underset{y^{\prime} \sim \mu}{\mathbb{E}}\left[r(y)-r\left(y^{\prime}\right)\right] \\
& =r(y)-\underset{y^{\prime} \sim \mu}{\mathbb{E}}\left[r\left(y^{\prime}\right)\right] .
\end{aligned}
$$

This is equal to the reward in Equation (3), up to an additive constant, and so it therefore follows that the optimal policy for Equation (6) and for optimizing the objective in Equation (3) are identical. Further, as shown by Rafailov et al. (2023), the optimal policy for the DPO objective in Equation (5) and the objective in Equation (3) are identical, which gives the statement of the proposition.

Applying this proposition to the objective function of Equation (6), for which there exists an analytical solution, reveals that under the BT assumption the closedform solution to DPO and RLHF can be written as

$$
\begin{equation*}
\pi^{*}(y) \propto \pi_{\mathrm{ref}}(y) \exp \left(\tau^{-1} \mathbb{E}_{y^{\prime} \sim \mu}\left[\Psi\left(p^{*}\left(y \succ y^{\prime}\right)\right)\right]\right) \tag{7}
\end{equation*}
$$

The derivations leading to Equation 7 is a well known result and is provided in App. A. 1 for completeness.

### 4.2 Weak Regularisation and Overfitting

It is worth taking a step back, and asking what kinds of policies the above objective leads us to discover. This highly non-linear transformation of the preference probabilities means that small increases in preference probabilities already close to 1 are just as incentivized as larger increases in preference probabilities around $50 \%$, which may be undesirable. The maximisation of logit-preferences, or Elo score in game-theoretic terminology, can also have counter-intuitive effects, even in transitive settings (Bertrand et al., 2023).

Consider the simple example where we have two actions $y$ and $y^{\prime}$ such that $p^{*}\left(y \succ y^{\prime}\right)=1$, i.e., $y$ is always preferred to $y^{\prime}$. Then the Bradley-Terry model would require that $\left(r(y)-r\left(y^{\prime}\right)\right) \rightarrow+\infty$ to satisfy (1). If we plug this into the optimal policy (7) then we would get that $\frac{\pi^{*}\left(y^{\prime}\right)}{\pi^{*}(y)}=0$ (i.e., $\pi^{*}\left(y^{\prime}\right)=0$ ) irrespective of what constant $\tau$ is used for the KL-regularisation. Thus the strength of the KL-regularisation becomes weaker and weaker the more deterministic the preferences.

The weakness of the KL-regularisation becomes even more pronounced in the finite data regime, where we only have access to a sample estimate of the preference $\hat{p}\left(y \succ y^{\prime}\right)$. Even if the true preference is, e.g., $p^{*}(y \succ$ $\left.y^{\prime}\right)=0.8$, empirically it can be very possible when we only have a few data points to estimate $\hat{p}\left(y \succ y^{\prime}\right)=1$, in which case the empirical optimal policy would make $\pi\left(y^{\prime}\right)=0$ for any $\tau$. This means that overfitting can be a substantial empirical issue, especially when the context and action spaces are extremely large as it is for large language models.

Why may standard RLHF be more robust to this problem in practice? While a purported advantage of DPO is that it avoids the need to fit a reward function, we observe that in practice when empirical preference probabilities are in the set $\{0,1\}$, the reward function ends up being underfit. The optimal rewards in the presence of $\{0,1\}$ preference probabilities are infinite, but these values are avoided, and indeed regularisation of the reward function has been observed to be an important aspect of RLHF training in practice (Christiano et al., 2017). This underfitting of the reward function is thus crucial in obtaining a final policy that is sufficiently regularised towards the reference policy $\pi_{\text {ref }}$, and DPO, in avoiding the training of the reward function, loses the regularisation of the policy that the underfitted reward function affords.

While standard empirical practices such as earlystopping can still be used as an additional form of regularisation to curtail this kind of overfitting, in the next section, we will introduce a modification of the $\Psi$ PO objective such that the optimal empirical policy can be close to $\pi_{\text {ref }}$ even when preferences are deterministic.

## 5 IPO: $\Psi P O$ with identity mapping

We have observed in the previous section that DPO is prone to overfitting, and this stems from a combination of the unboundedness of $\Psi$, together with not training an explicit reward function. Not training a reward function directly is a clear advantage of DPO, but we would like to avoid the problems of overfitting as well.

This analysis of DPO motivates choices of $\Psi$ which are bounded, ensuring that the KL regularisation in Equation 6 remains effective even in the regime of $\{0,1\}$ valued preferences, as it is often the case when working with empirical datasets. A particularly natural form of objective to consider is given by taking $\Psi$ to be the identity mapping in Equation (6), leading to direct regularized optimisation of total preferences:

$$
\begin{equation*}
\max _{\pi} p_{\rho}^{*}(\pi \succ \mu)-\tau D_{\mathrm{KL}}\left(\pi \| \pi_{\mathrm{ref}}\right) \tag{8}
\end{equation*}
$$

The standard approach to optimize the objective function of Equation (8) is through RLHF with the choice of reward $r(y)=p^{*}(y \succ \mu)$. However both using RL and estimating the reward model $r(y)$ can be costly. Inspired by DPO one would like to devise an empirical solution for the optimisation problem of Equation (8) which can directly learn from the preference dataset. Thus it would be able to avoid RL and reward modeling altogether.

### 5.1 Derivations and Computationally Efficient Algorithm

As with DPO, it will be beneficial to re-express Equation (8) as an offline learning objective. To derive such an expression, we begin by following the derivation of Rafailov et al. (2023), manipulating the analytic expression for the optimal policy into a system of rootfinding problems. As in the previous section, we drop dependence on the context $x$ from our notation, as all arguments can be applied on a per-context basis.

Root-finding problems. Let $g(y)=$ $\mathbb{E}_{y^{\prime} \sim \mu}\left[\Psi\left(p^{*}\left(y \succ y^{\prime}\right)\right)\right]$. Then we have

$$
\begin{equation*}
\pi^{*}(y) \propto \pi_{\mathrm{ref}}(y) \exp \left(\tau^{-1} g(y)\right) \tag{9}
\end{equation*}
$$

For any $y, y^{\prime} \in \operatorname{Supp}\left(\pi_{\text {ref }}\right)$, we therefore have

$$
\begin{equation*}
\frac{\pi^{*}(y)}{\pi^{*}\left(y^{\prime}\right)}=\frac{\pi_{\mathrm{ref}}(y)}{\pi_{\mathrm{ref}}\left(y^{\prime}\right)} \exp \left(\tau^{-1}\left(g(y)-g\left(y^{\prime}\right)\right)\right) \tag{10}
\end{equation*}
$$

By letting

$$
h^{*}\left(y, y^{\prime}\right)=\log \left(\frac{\pi^{*}(y) \pi_{\mathrm{ref}}\left(y^{\prime}\right)}{\pi^{*}\left(y^{\prime}\right) \pi_{\mathrm{ref}}(y)}\right)
$$

and rearranging Equation (10), we obtain

$$
\begin{equation*}
h^{*}\left(y, y^{\prime}\right)=\tau^{-1}\left(g(y)-g\left(y^{\prime}\right)\right) \tag{11}
\end{equation*}
$$

The core idea now is to consider a policy $\pi$, define

$$
h_{\pi}\left(y, y^{\prime}\right)=\log \left(\frac{\pi(y) \pi_{\mathrm{ref}}\left(y^{\prime}\right.}{\pi\left(y^{\prime}\right) \pi_{\mathrm{ref}}(y)}\right)
$$

and aim to solve the equations:

$$
\begin{equation*}
h_{\pi}\left(y, y^{\prime}\right)=\tau^{-1}\left(g(y)-g\left(y^{\prime}\right)\right) \tag{12}
\end{equation*}
$$

Loss for IPO. We now depart from the approach to the analysis employed by Rafailov et al. (2023), to obtain a novel offline formulation of Equation (6), in the specific case of $\Psi$ as the identity function. In this case, Equation (12) reduces to

$$
h_{\pi}\left(y, y^{\prime}\right)=\tau^{-1}\left(p^{*}(y \succ \mu)-p^{*}\left(y^{\prime} \succ \mu\right)\right)
$$

We begin by re-expressing these root-finding problems as a single optimisation problem $L(\pi)$ :

$L(\pi)=\underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[\left(h_{\pi}\left(y, y^{\prime}\right)-\frac{p^{*}(y \succ \mu)-p^{*}\left(y^{\prime} \succ \mu\right)}{\tau}\right)^{2}\right]$.

One can easily show that for the choice of $\pi^{*}$ we have $L\left(\pi^{*}\right)=0$. Thus $\pi^{*}$ is a global minimizer of $L(\pi)$. The following theorem establishes the uniqueness of this solution.

Theorem 2 (Uniqueness of Global/Local Optima). Assume that $\operatorname{Supp}(\mu)=\operatorname{Supp}\left(\pi_{\text {ref }}\right)$ and define $\Pi$ to be the set of policies $\pi$ such that $\operatorname{Supp}(\pi)=\operatorname{Supp}(\mu)$. Then $\pi \mapsto L(\pi)$ has a unique local/global minimum in $\Pi$, which is $\pi^{*}$.

Proof. By assumption, $\pi^{*} \in \Pi$, and by definition $\forall \pi \in \Pi, L(\pi) \geq 0$ as $L(\pi)$ is an expectation of squared terms. Further, from Equation (11), it follows immediately that $L\left(\pi^{*}\right)=0$, and so we deduce that $\pi^{*}$ is a global optimum for $L$. We now show that there are no other local/global minima for $L$ in $\Pi$.

We write $J=\operatorname{Supp}(\mu)$. We parametrise the set $\Pi$ via vectors of logits $s \in \mathbb{R}^{J}$, setting $\pi_{s}(y)=$ $\exp (s(y)) / \sum_{y^{\prime} \in J} \exp \left(s\left(y^{\prime}\right)\right)$ for $y \in J$, and $\pi_{s}(y)=0$ otherwise. Let us write $\mathcal{L}(s)=L\left(\pi_{s}\right)$ for the objective as a function of the logits $s$.

$$
\begin{align*}
\mathcal{L}(s) & =\mathbb{E}_{y, y^{\prime} \sim \mu}\left[\left[\frac{p^{*}(y \succ \mu)-p^{*}\left(y^{\prime} \succ \mu\right)}{\tau}\right.\right.  \tag{14}\\
& \left.\left.-\left(s(y)-s\left(y^{\prime}\right)\right)-\log \left(\frac{\pi_{\mathrm{ref}}\left(y^{\prime}\right)}{\pi_{\mathrm{ref}}(y)}\right)\right]^{2}\right]
\end{align*}
$$

The objective is quadratic as a function of the logits $s$. Further, by expanding the quadratic above, we see that the loss can be expressed as a sum of squares

$$
\begin{equation*}
\sum_{y, y^{\prime} \in J} \mu(y) \mu\left(y^{\prime}\right)\left(s(y)-s\left(y^{\prime}\right)\right)^{2} \tag{15}
\end{equation*}
$$

plus linear and constant terms. This is therefore a positive-semidefinite quadratic, and hence is convex. We thus deduce that all local minimisers of the loss $\mathcal{L}(s)$ are global minimisers as well (Boyd and Vandenberghe, 2004, Chap. 4). We now notice since $\pi_{s}$ is a surjective continuous mapping from $s$ to $\pi$ one can easily show from the definition of local minimum that every local minimiser $\pi$ of $L$ corresponds to a set of local minimisers $\mathcal{S}_{\pi}$ of $\mathcal{L}$. Thus all local minimums of $L$ are also global minimums as well.

Finally, the only direction $s$ in which the quadratic in Equation (15) does not increase away from 0 is when all bracketed terms remain 0 ; that is, in the direction $(1, \ldots, 1) \in \mathbb{R}^{J}$. Thus, $\mathcal{L}(s)$ is strictly convex, except in the direction $(1, \ldots, 1)$. (Boyd and Vandenberghe, 2004, Chap. 3). However, modifying logits in the direction $e=(1, \ldots, 1)$ does not modify the resulting policy $\pi_{s}$, since, for $y \in J$

$$
\pi_{s+\lambda e}(y)=\frac{e^{s(y)+\lambda}}{\sum_{y^{\prime} \in J} e^{s\left(y^{\prime}\right)+\lambda}}=\frac{e^{s(y)}}{\sum_{y^{\prime} \in J} e^{s\left(y^{\prime}\right)}}=\pi_{s}(y)
$$

The strict convexity combined with the fact that $\pi^{*}$ is a global minima proves that $\pi^{*}$ is the unique global/local minima in $\Pi$ (Boyd and Vandenberghe, 2004, Chap. 4).

### 5.2 Sampled Loss for IPO

In order to obtain the sampled loss for IPO we need to show that we can build an unbiased estimate of the right-hand side of the equation (13). To this end, we consider the Population IPO Loss:

$$
\begin{equation*}
\underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[\left(h_{\pi}\left(y, y^{\prime}\right)-\tau^{-1} I\left(y, y^{\prime}\right)\right)^{2}\right] \tag{16}
\end{equation*}
$$

where $I\left(y, y^{\prime}\right)$ is drawn from a Bernoulli distribution with mean $p^{*}\left(y \succ y^{\prime}\right)$, i.e., $I\left(y, y^{\prime}\right)$ is 1 if $y$ is preferred to $y^{\prime}$ (which happens with probability $p^{*}(y \succ$ $\left.y^{\prime}\right)$ ), and 0 otherwise. This straightforwardly yields a sample-based loss that can be used, by sampling a pair $\left(y, y^{\prime}\right)$ from the preference dataset, and consulting the recorded preference to obtain a sample from $I\left(y, y^{\prime}\right)$. The following proposition justifies the switch from Equation (13) to Equation (16), by demonstrating their equality.

Proposition 3. The expressions in Equation (13) and Equation (16) are equal, up to an additive constant independent of $\pi$.

Proof. This equivalence is not completely trivial, since in general the conditional expectation

$$
\mathbb{E}\left[h_{\pi}\left(Y, Y^{\prime}\right)-\tau^{-1} I\left(Y, Y^{\prime}\right) \mid Y=y, Y^{\prime}=y^{\prime}\right]
$$

is not equal to the corresponding quantity appearing in Equation (13), namely

$$
h_{\pi}\left(y, y^{\prime}\right)-\tau^{-1}\left(p^{*}(y \succ \mu)-p^{*}\left(y^{\prime} \succ \mu\right)\right)
$$

We instead need to exploit some symmetry between the distributions of $y$ and $y^{\prime}$, and use the fact that $h_{\pi}\left(y, y^{\prime}\right)$ decomposes as an additive function of $y$ and $y^{\prime}$. To show this equality of losses, it is enough to focus on the "cross-terms" obtained when expanding the quadratics in Equations (13) and (16); that is, to show

$$
\begin{aligned}
& \underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[h_{\pi}\left(y, y^{\prime}\right) I\left(y, y^{\prime}\right)\right] \\
= & \underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[h_{\pi}\left(y, y^{\prime}\right)\left(p^{*}(y \succ \mu)-p^{*}\left(y^{\prime} \succ \mu\right)\right)\right] .
\end{aligned}
$$

Now, starting with the right-hand side, and using the shorthand $\pi_{y}=\log (\pi(y)), \pi_{y}^{\mathrm{R}}=\log \left(\pi_{\mathrm{ref}}(y)\right), p_{y}=$ $p^{*}(y \succ \mu)$, and similarly for $y^{\prime}$, we have

$$
\begin{aligned}
& \underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[h_{\pi}\left(y, y^{\prime}\right)\left(p^{*}(y \succ \mu)-p^{*}\left(y^{\prime} \succ \mu\right)\right)\right] \\
&=\underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[\left(\pi_{y}-\pi_{y^{\prime}}+\pi_{y^{\prime}}^{\mathrm{R}}-\pi_{y}^{\mathrm{R}}\right)\left(p_{y}-p_{y^{\prime}}\right)\right] \\
&=\underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[\pi_{y} p_{y}-\pi_{y} p_{y^{\prime}}-\pi_{y^{\prime}} p_{y}+\pi_{y^{\prime}}\right. \\
&\left.+p_{y^{\prime}}+\pi_{y^{\prime}}^{\mathrm{R}} p_{y}-\pi_{y^{\prime}}^{\mathrm{R}} p_{y^{\prime}}-\pi_{y}^{\mathrm{R}} p_{y}+\pi_{y}^{\mathrm{R}} p_{y^{\prime}}\right] \\
&= \underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[\left(2 p_{y}-1\right) \pi_{y}-\left(2 p_{y}-1\right) \pi_{y}^{\mathrm{R}}\right]
\end{aligned}
$$

where we have used iid-ness of $y$ and $y^{\prime}$, and $\mathbb{E}_{y \sim \mu}\left[p_{y}\right]=1 / 2$. Turning to the left-hand side, we have

$$
\begin{aligned}
& \underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[h_{\pi}\left(y, y^{\prime}\right) I\left(y, y^{\prime}\right)\right] \\
&=\underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[\left(\pi_{y}-\pi_{y^{\prime}}+\pi_{y^{\prime}}^{\mathrm{R}}-\pi_{y}^{\mathrm{R}}\right) I\left(y, y^{\prime}\right)\right] \\
&=\underset{y \sim \mu}{\mathbb{E}}\left[\left(\pi_{y}-\pi_{y}^{\mathrm{R}}\right) \underset{y^{\prime} \sim \mu}{\mathbb{E}}\left[I\left(y, y^{\prime}\right) \mid y\right]\right] \\
&+\underset{y^{\prime} \sim \mu}{\mathbb{E}}\left[\left(-\pi_{y^{\prime}}+\pi_{y^{\prime}}^{\mathrm{R}}\right) \underset{y \sim \mu}{\mathbb{E}}\left[I\left(y, y^{\prime}\right) \mid y^{\prime}\right]\right] \\
&=\underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[\pi_{y} p_{y}-\pi_{y^{\prime}}\left(1-p_{y^{\prime}}\right)+\pi_{y^{\prime}}^{\mathrm{R}}\left(1-p_{y^{\prime}}\right)-\pi_{y}^{\mathrm{R}} p_{y}\right] \\
&=\underset{y, y^{\prime} \sim \mu}{\mathbb{E}}\left[\left(2 p_{y}-1\right) \pi_{y}-\left(2 p_{y}-1\right) \pi_{y}^{R}\right]
\end{aligned}
$$

where we use the fact that $\mathbb{E}_{y^{\prime} \sim \mu} I\left(y, y^{\prime}\right)=p_{y}$ and $\mathbb{E}_{y \sim \mu} I\left(y, y^{\prime}\right)=1-p_{y^{\prime}}$. This demonstrates equality of the losses, as required.

We now discuss how to approximate the loss in Equation (16) with an empirical dataset. As in our earlier discussion, the empirical dataset $\mathcal{D}$ takes the form
$\left(y_{w, i}, y_{l, i}\right)_{i=i}^{N}$. Note that each datapoint $\left(y_{w, i}, y_{l, i}\right)$ contributes two terms to an empirical approximation of Equation (16), with $\left(y, y^{\prime}, I\left(y, y^{\prime}\right)\right)=\left(y_{w, i}, y_{l, i}, 1\right)$, and also $\left(y, y^{\prime}, I\left(y, y^{\prime}\right)\right)=\left(y_{l, i}, y_{w, i}, 0\right)$. This symmetry is important to exploit, and leads to a reduction in the variance of the loss. The overall empirical loss is therefore given by

$$
\begin{aligned}
& \frac{1}{2} \underset{\left(y_{w}, y_{l}\right) \sim D}{\mathbb{E}}\left[\left(h_{\pi}\left(y_{w}, y_{l}\right)-\tau^{-1}\right)^{2}+h_{\pi}\left(y_{l}, y_{w}\right)^{2}\right]= \\
& \frac{1}{2} \underset{\left(y_{w}, y_{l}\right) \sim D}{\mathbb{E}}\left[\left(h_{\pi}\left(y_{w}, y_{l}\right)-\tau^{-1}\right)^{2}+h_{\pi}\left(y_{w}, y_{l}\right)^{2}\right]
\end{aligned}
$$

which up to a constant equals:

$$
\begin{equation*}
\underset{\left(y_{w}, y_{l}\right) \sim D}{\mathbb{E}}\left[\left(h_{\pi}\left(y_{w}, y_{l}\right)-\frac{\tau^{-1}}{2}\right)^{2}\right] \tag{17}
\end{equation*}
$$

This simplified form of the loss provides some valuable insights on the way in which IPO optimizes the policy $\pi$ : IPO learns from preferences dataset simply by regressing the gap between log-likelihood ratios $\log \left(\pi\left(y_{w}\right) / \pi\left(y_{l}\right)\right)$ and $\log \left(\pi_{\mathrm{ref}}\left(y_{w}\right) / \pi_{\mathrm{ref}}\left(y_{l}\right)\right)$ to $\frac{\tau^{-1}}{2}$. So the weaker the regularisation becomes, the higher would be the log-likelihood ratio of $y_{w}$ to $y_{l}$. In other words IPO, unlike DPO, always regularizes its solution towards $\pi_{\text {ref }}$ by controlling the gap between the $\log$-likelihood ratios $\log \left(\pi\left(y_{w}\right) / \pi\left(y_{l}\right)\right)$ and $\log \left(\pi_{\text {ref }}\left(y_{w}\right) / \pi_{\text {ref }}\left(y_{l}\right)\right)$, thus avoiding the over-fitting to the preference dataset. We summarize the sampled IPO in Algorithm 1:

Algorithm 1 Sampled IPO

Require: Dataset $\mathcal{D}$ of prompts, preferred and dispreferred generations $x, y_{w}$ and $y_{l}$, respectively. A reference policy $\pi_{\text {ref }}$

1: Define

$$
h_{\pi}\left(y, y^{\prime}, x\right)=\log \left(\frac{\pi(y \mid x) \pi_{\mathrm{ref}}\left(y^{\prime} \mid x\right)}{\pi\left(y^{\prime} \mid x\right) \pi_{\mathrm{ref}}(y \mid x)}\right)
$$

2: Starting from $\pi=\pi_{\text {ref }}$ minimize

$$
\underset{\left(y_{w}, y_{l}, x\right) \sim D}{\mathbb{E}}\left(h_{\pi}\left(y_{w}, y_{l}, x\right)-\frac{\tau^{-1}}{2}\right)^{2}
$$

### 5.3 Illustrative Examples

To illustrate the qualitative difference between our algorithm and DPO we will consider a few simple cases. For simplicity we assume there is no context $x$, i.e., we are in the bandit setting.

### 5.3.1 Asymptotic Setting

We first consider the simple case where we have 2 actions only, $y_{1}$ and $y_{2}$, and a deterministic preference between them: $p^{*}\left(y_{1} \succ y_{2}\right)=1$. Suppose we start with a uniform $\pi_{\text {ref }}$ and $\mu$. We know from Section 4.2 that DPO will converge to the deterministic policy $\pi^{*}\left(y_{1}\right)=1, \pi^{*}\left(y_{2}\right)=0$ regardless of the value of $\tau$. Thus even when the regularisation coefficient $\tau$ is very large, this is very different from the uniform $\pi_{\text {ref }}$.

Now, let us derive the optimal policy for IPO. We have $p^{*}\left(y_{1} \succ \mu\right)=3 / 4$ and $p^{*}\left(y_{2} \succ \mu\right)=1 / 4$. Plugging this into equation (9) with $\Psi=I$ we get that $\pi^{*}\left(y_{1}\right)=\frac{\exp \left(0.75 \tau^{-1}\right)}{\exp \left(0.75 \tau^{-1}\right)+\exp \left(0.25 \tau^{-1}\right)}=\sigma\left(0.5 \tau^{-1}\right)$, and $\pi^{*}\left(y_{2}\right)=\sigma\left(-0.5 \tau^{-1}\right)$, where $\sigma$ is the sigmoid function. Hence we see that if we have large regularisation as $\tau \rightarrow+\infty$, then $\pi^{*}$ converges to the uniform policy $\pi_{\text {ref }}$, and on the flip side as $\tau \rightarrow+0$, then $\pi^{*}\left(y_{1}\right) \rightarrow 1$ and $\pi^{*}\left(y_{2}\right) \rightarrow 0$, which is the deterministic optimal policy. The regularisation parameter $\tau$ can now actually be used to control how close to $\pi_{\text {ref }}$ we are.

### 5.4 Sampled Preferences

So far we relied on the closed-form optimal policy from Eq. (9) to study DPO and IPO's stability, but this equation is not applicable to more complex settings where we only have access to sampled preference instead of $p^{\star}$. We can still however find accurate approximations of the optimal policy by choosing a parametrisation $\pi_{\theta}$ and optimize $\theta$ with an empirical loss over a dataset and iterative gradient-based updates. We will use this approach to show two non-asymptotic examples where DPO over-fits the dataset of preferences and ignore $\pi_{\text {ref }}$ : when one action $y$ wins against all others DPO pushes $\pi_{\theta}(y)$ to 1 regardless of $\tau$, and conversely when one action $y$ never wins against the others DPO pushes $\pi_{\theta}(y)$ to 0 again regardless of $\tau$. In the same scenarios, IPO does not converge to these degenerate solutions but instead remains close to $\pi_{\text {ref }}$ based on the strength of the regularisation $\tau$.

For both scenarios we consider a discrete space $\mathcal{Y}=$ $\left\{y_{a}, y_{b}, y_{c}\right\}$ with 3 actions, and select a dataset of pairs $\mathcal{D}=\left\{\left(y_{w, i}, y_{l, j}\right)\right\}$. Given $\mathcal{D}$, we leverage the empirical losses from Eq. 4 and Eq. 13 to find DPO's and IPO's optimal policy. We encode policies as $\pi_{\theta}\left(y_{i}\right)=$ softmax $(\theta)_{i}$ using a vector $\theta \in \mathbb{R}^{3}$, and optimize them for 18000 steps using Adam (Kingma and Ba, 2014) with learning rate 0.01 and mini-batch size 9 . Minibatches are constructed using uniform sampling with replacement from $\mathcal{D}$. Both policies and losses are implemented using the flax python framework (Bradbury et al., 2018; Heek et al., 2023), and the Adam implementation is from optax (Babuschkin et al., 2020).
![](https://cdn.mathpix.com/cropped/2024_05_26_0533405910d4f2338df2g-08.jpg?height=812&width=830&top_left_y=244&top_left_x=212)

Figure 1: Comparison Between the Learning Curves of Action Probabilities of IPO and DPO for $\mathcal{D}_{1}$

For each set of hyper-parameters we repeat the experiment 10 times with different seeds, and report mean and $95 \%$ confidence intervals. All experiments are executed on a modern cloud virtual machine with 4 cores and $32 \mathrm{~GB}$ of ram.

IPO Avoids Greedy Policies For the first example we sample each unique action pair once to collect a dataset $\mathcal{D}$ containing 3 observed preferences. Due to symmetries of pairwise preferences sampling only 3 preferences can results in only two outcomes (up to permutations of the actions):

$$
\begin{aligned}
& \mathcal{D}_{1}=\left\{\left(y_{a}, y_{b}\right),\left(y_{b}, y_{c}\right),\left(y_{a}, y_{c}\right)\right\} \\
& \mathcal{D}_{2}=\left\{\left(y_{a}, y_{b}\right),\left(y_{b}, y_{c}\right),\left(y_{c}, y_{a}\right)\right\}
\end{aligned}
$$

where we focus on $\mathcal{D}_{1}$, which represent a total ordering, rather than $\mathcal{D}_{2}$, which represent a cycle. The outcome of the experiment is reported in Fig. 1 in which, we report the learning curves for varying values of $\tau$. We observe that DPO always converges to the deterministic policy for all values of $\tau$. In other word DPO completely ignores the reference policy, no matter how strong is the regularisation term, and converges to the action which is preferred in the dataset. On the other hand, IPO prevent the policy from becoming greedy when the regularisation is strong.

IPO Does not Exclude Actions In the first example DPO converges to a deterministic policy because one action strictly dominates all others and the loss continues to push up its likelihood until it saturates. The opposite effect happens for the logical opposite
![](https://cdn.mathpix.com/cropped/2024_05_26_0533405910d4f2338df2g-08.jpg?height=808&width=830&top_left_y=250&top_left_x=1081)

Figure 2: Comparison Between the Learning Curves of Action Probabilities of IPO and DPO for $\mathcal{D}_{3}$

condition, i.e., when one action does not have at least a victory in the dataset DPO will sets its probability to 0 regardless of $\tau$. While this is less disruptive than the first example (a single probability is perturbed whereas previously the whole policy was warped by an over-achieving action) it is also much more common in real-world data. In particular, whenever the action space is large but the dataset small, some actions will necessarily be sampled rarely or only once, making it likely to never observe a victory. Especially because we do not have data on their performance $\pi$ should stick close to $\pi_{\text {ref }}$ for safety, but DPO's objective does not promote this.

In the final example the dataset consists of two observed preferences $\mathcal{D}_{3}=\left\{\left(y_{a}, y_{b}\right),\left(y_{b}, y_{a}\right)\right\}$ and leave the pair $\left(y_{a}, y_{c}\right)$ completely unobserved. We compute solutions using Adam once again, and report the results in Fig. 2 for varying values of $\tau$. We observe again here that DPO ignores the prior $\pi_{\text {ref }}$ completely, no matter how strong we regularize the objective, whereas IPO gradually decreases the probability of unobserved action with $\tau$.

## 6 Conclusion and Future Work

We presented a unified objective, called $\Psi \mathrm{PO}$, for learning from preferences. It unifies RLHF and DPO methods. In addition, we introduced a particular case of $\Psi \mathrm{PO}$, called IPO, that allows to learn directly from preferences without a reward modelling stage and without relying on the Bradley-Terry modelisation assumption
that assumes that pairwise preferences can be substituted with pointwise rewards. This is important because it allows to avoid the overfitting problem. This theoretical contribution is only useful in practice if an empirical sampled loss function can be derived. This is what we have done in Sec 5 where we show that IPO can be formulated as a root-finding problem from which an empirical sampled loss function can be derived. The IPO loss function is simple, easy to implement and theoretically justified. Finally, in Sec. 5.3 and Sec. 5.4, we provide illustrative examples where we highlight the instabilities of DPO when the preferences are fully-known as well as when they are sampled. Those minimal experiments are sufficient to prove that IPO is better suited to learn from sampled preferences than DPO. Future works should scale those experiments to more complex settings such as training language models on human preferences data.

## References

Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, et al. The DeepMind JAX ecosystem, 2020, 2020. URL http://github.com/deepmind.

Quentin Bertrand, Wojciech Marian Czarnecki, and Gauthier Gidel. On the limitations of the Elo: Realworld games are transitive, not additive. In Proceedings of the International Conference on Artificial Intelligence and Statistics, 2023.

Stephen P. Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, 2004.

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:// github.com/google/jax.

Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.

Róbert Busa-Fekete, Balázs Szörényi, Paul Weng, Weiwei Cheng, and Eyke Hüllermeier. Preference-based reinforcement learning: Evolutionary direct policy search using a preference-based racing algorithm. Machine Learning, (3):327-351, 2014.

Róbert Busa-Fekete, Balázs Szörenyi, Paul Weng, Weiwei Cheng, and Eyke Hüllermeier. Preference-based evolutionary direct policy search. In Autonomous Learning Workshop @ ICRA, 2013.

Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In Proceedings of the International Conference on Machine Learning, 2022.

Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In $A d$ vances in Neural Information Processing Systems, 2017.

Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov decision processes. In Proceedings of the International Conference on Machine Learning, 2019.

Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network li- brary and ecosystem for JAX, 2023. URL http: //github.com/google/flax.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations, 2014.

Angeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication meets natural language: Synergies between functional and structural language learning. In Proceedings of the Annual Meeting of Association for Computational Linguistics, 2020.

Tyler Lu, Dávid Pál, and Martin Pál. Contextual multi-armed bandits. In Proceedings of the International Conference on Artificial Intelligence and Statistics, 2010.

Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In Proceedings of the International Conference on Machine Learning, 2020.

Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick. Dueling posterior sampling for preference-based reinforcement learning. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2020.

OpenAI. Gpt-4 technical report, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller amd Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022.

Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling RL: Reinforcement learning with trajectory preferences. arXiv, 2023.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv, 2023.

Prajit Ramachandran, Peter J. Liu, and Quoc V. Le. Unsupervised pretraining for sequence to sequence learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processings, 2016.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv, 2017.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 2020.

Yuanhao Wang, Qinghua Liu, and Chi Jin. Is RLHF more difficult than standard RL? arXiv, 2023.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In Proceedings of the International Conference on Learning Representations, 2022.

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. SLiC-HF: Sequence likelihood calibration with human feedback. arXiv, 2023.
