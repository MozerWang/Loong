# Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models 

Asma Gandeharioun ${ }^{* 1}$ Avi Caciularu *1 Adam Pearce $^{1}$ Lucas Dixon ${ }^{1}$ Mor Geva ${ }^{12}$


#### Abstract

Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction ${ }^{1}$.


## 1. Introduction

The question of what information is captured in the hidden representations of large language models (LLMs) is of key importance in control and understanding of modern generative AI, and has drawn substantial attention recently (Casper et al., 2022; Madsen et al., 2022; Patel \& Pavlick, 2021; Nanda et al., 2023). To tackle this question, prior work has introduced a diverse array of interpretability methods, which largely rely on three prominent approaches: training linear[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_92998034bd56a769976cg-01.jpg?height=608&width=827&top_left_y=688&top_left_x=1061)

Figure 1. Illustration of our framework, showing a P atchscope for decoding the hidden representation of "CEO" in the source prompt (left). We use a target prompt (right) comprised of fewshot demonstrations of string repetitions to encourage the LLM to explain its internal representation. Step 1: Run the forward computation on the source prompt in the source model. Step 2: Optionally transform the hidden state at the source layer. Step 3: Run the forward computation on the target prompt up to the target layer in the target model. Step 4: Patch the target representation of the token "?" at the target layer with the transformed representation (from step 2), then continue the forward computation from that layer onward. The modularity of $\mathrm{P}$ atchscopes allows designing a variety of methods by configuring the target prompt, model and transformation.

classifiers, called probes, on top of hidden representations (Belinkov \& Glass, 2019; Belinkov, 2022; Alain \& Bengio, 2017), projecting representations to the model's vocabulary space (nostalgebraist, 2020; Din et al., 2023; Belrose et al., 2023), and intervening on the computation to identify if a representation is critical for certain predictions (Meng et al., 2022a; Wallat et al., 2020; Wang et al., 2022; Conmy et al., 2023; Geva et al., 2023).

Despite the wide success of these methods, they each exhibit practical shortcomings. First, probing relies on supervised training for pre-defined classes, which is hard to scale when there is a large number of classes or when all the categories are not known a priori. Second, the accuracy of vocabulary

Table 1. Many prior inspection methods with various objectives can be viewed as Patchscopes. The rows highlighted in green show Patchscope configurations that overcome several limitations of prior methods through more expressive inspection that is training-data free and is more robust across layers.

| Inspection <br> Objective |  | Expressive | Training <br> Data <br> Free | Robust <br> Across <br> Layers |
| :--- | :--- | :--- | :--- | :--- |
|  | Few-Shot Token Identity Patchscope (\$4.1) <br> Logit Lens (nostalgebraist, 2020), | $\checkmark \checkmark$ | $\checkmark$ | $\checkmark \checkmark$ |
| Inspecting <br> Output <br> Distribution | Tmbedding Space Analysis (Dar et al., 2023) |  |  |  |

projections substantially decreases in early layers and the outputs are often hard to interpret. Last, all the above methods are not as expressive as one might like: they provide class probabilities or most likely tokens, as opposed to a high-quality explanation in natural language.

In this work, we argue that the advanced capabilities of LLMs in generating human-like text can be leveraged for "translating" the information in their representations for humans. We introduce a modular framework, called Patchscopes (\$3), that can be configured to query various kinds of information from LLM representations. Patchscopes decode specific information from a representation within an LLM by "patching" it into the inference pass on a different prompt that has been designed to encourage the extraction of that information. ${ }^{2}$ Such configuration (a Patchscope) can be viewed as an inspection tool geared towards a particular objective, as illustrated in Fig. 1.

We show that many existing methods, including those that rely on vocabulary projections and computation interventions, can be cast as Patchscopes. Moreover, new configurations of our framework introduce more effective tools in addressing the same questions, while mitigating several limitations of prior approaches. Also, Patchscopes enables addressing underexplored questions, such as finegrained analysis of the input contextualization process and the extent to which a more expressive model can be used to inspect hidden representations of a smaller model.[^1]

We conduct a series of experiments to evaluate the benefits and opportunities introduced by Patchscopes, focusing on auto-regressive LLMs. First, we consider the problem of estimating the model's next-token prediction from its intermediate representations (see $\S 4.1$ ). Across multiple LLMs, we show that using a few-shot token identity prompt, a prompt in the form of " $\mathrm{tok}_{1} \rightarrow \mathrm{tok}_{1}$; tok ${ }_{2} \rightarrow$ tok $_{2} ; \ldots ;$ tok $_{k}$ " where tok $_{i}$ refers to a random token, leads to substantial gains over vocabulary projection methods. Next, we evaluate how well Patchscopes can decode specific attributes of an entity from its LLM representations, when these are detached from the original context (see $\S 4.2$ ). We observe that, despite using no training data, Patchscopes significantly outperforms probing in six out of twelve commonsense and factual reasoning tasks, and works comparably well in all but one of the remaining six.

Beyond output estimation and attribute decoding, Patchscopes can address questions that are hard to answer with existing methods. In $\S 4.3$, we apply Patchscopes to study how LLMs contextualize input entity names in early layers, where vocabulary projections mostly fail and other methods, at best, provide only a binary signal of whether the entity has been resolved (Youssef et al., 2023; Tenney et al., 2019). With a new Patchscope, we are able to verbalize the gradual entity resolution process. For example, we show that, as the model processes the final token of "Alexander the Great" throughout the layers, it reflects different entities starting from "Great Britain", to "the Great Depression", to finally resolving "Alexander the Great". Then, in $\S 4.4$ we show how one can further improve Patchscope expressivity by using a stronger target model, e.g., Vicuna 13B instead of Vicuna 7B.

Lastly, we showcase the utility of Patchscopes for fixing latent multi-hop reasoning errors, particularly when the model is capable of conducting each reasoning step correctly, but fails when they need to be composed incontext (§5). Building on top of the data provided by Hernandez et al. (2023b), we introduce a more complex task that requires two steps of factual reasoning. Patchscope achieves $50 \%$ accuracy on this task, outperforming chain-ofthought (Wei et al., 2022) (35.71\%) and vanilla generations $(19.57 \%)$.

To conclude, our work makes the following contributions: We propose Patchscopes, a general modular framework for decoding information from the hidden representations in LLMs. We show that prominent interpretability methods can be viewed as instances of Patchscopes, and new configurations result in more expressive, robust across layers, and training-data free alternatives that mitigate their shortcomings. In addition, novel configurations introduce unexplored possibilities of stronger inspection techniques, as well as practical benefits, such as correcting multi-hop
reasoning errors.

## 2. Related Work

Activation patching is a causal intervention, commonly used as a tool for studying if certain activations play a key role in a model's computation (Geiger et al., 2021; Vig et al., 2020). Patching has been used largely for localizing specific information to specific layers and token positions (GoldowskyDill et al., 2023; Meng et al., 2022a;b; Stolfo et al., 2023; Merullo et al., 2023), and for finding information propagation paths in the computation (Wang et al., 2022; Geva et al., 2023; Hendel et al., 2023; Hanna et al., 2023; Lieberum et al., 2023). Prior works have also used specific forms of cross-model patching called stitching, in non-transformer architectures, mostly to analyze representational similarity (e.g., Bansal et al., 2021; Csiszárik et al., 2021; Lenc \& Vedaldi, 2015). Despite certain limitations (Hase et al., 2023; Zhang \& Nanda, 2023), patching remains a principal tool for mechanistic interpretability (Conmy et al., 2023).

Given promising results from emerging interpretability efforts that employ LLMs to generate human-like text for inspection (e.g., Mousi et al., 2023; Slobodkin et al., 2023; Bills et al., 2023), we argue that using patching only for localization purposes is myopic, and propose to use it for "translating" LLM representations into natural language. Very recently, patching has been used to study new problem setups (e.g., Pal et al., 2023; Hernandez et al., 2023b), all of which can be seen as different configurations of our proposed framework (see $\S 3.2$ ).

Among the growing research efforts in inspecting hidden representations of neural networks, probing classifiers are perhaps the most common (e.g., Alain \& Bengio, 2017; Belinkov \& Glass, 2019; Belinkov, 2022; Wang et al., 2023), and methods using projections into the vocabulary space or their extensions to other domains are another key category (e.g., Merullo et al., 2023; Geva et al., 2022b; nostalgebraist, 2020; Belrose et al., 2023; Dar et al., 2023; Din et al., 2023; Langedijk et al., 2023; Vilas et al., 2023). While various other latent inspection methods exist (e.g., Zhou et al., 2018; Strobelt et al., 2017; Ghandeharioun et al., 2021; Kim et al., 2018), the above are the most relevant to this work.

## 3. Patchscopes

In this section, we introduce Pat chscopes and show how it extends prior interpretability methods with new capabilities. While not limited to particular LLM architectures, this work focuses on auto-regressive transformer-based LLMs.

### 3.1. Framework Description

The key idea in Patchscopes is to leverage the advanced capabilities of LLMs to generate human-like text for "translating" the information encoded in their own hidden representations. Concretely, given a hidden representation obtained from an LLM inference pass, we propose to decode specific information from it by patching it into a different inference pass (of the same or a different LLM) that encourages the translation of that specific information.

Notably, the rest of the forward computation after patching can augment the representation with additional information, hence, this approach does not guarantee that the patched representation itself stores all that information. However, dispatching the representation from its original context (the source prompt) stops contextualization and guarantees that no further information from the source prompt is incorporated in the post-patching computation. Thus, our framework reveals if specific information can be decoded from the patched representation via the post-patching computation.

Given an input sequence of $n$ tokens $S=\left\langle s_{1}, \ldots, s_{n}\right\rangle$ and a model $\mathcal{M}$ with $L$ layers, $\boldsymbol{h}_{i}^{\ell}$ denotes the hidden representation obtained at layer $\ell \in[1, \ldots, L]$ and position $i \in[1, \ldots, n]$, when running $\mathcal{M}$ on $S$. To inspect $\boldsymbol{h}_{i}^{\ell}$, we consider a separate inference pass of a model $\mathcal{M}^{*}$ with $L^{*}$ layers on a target sequence $T=\left\langle t_{1}, \ldots, t_{m}\right\rangle$ of $m$ tokens. Specifically, we choose a hidden representation $\overline{\boldsymbol{h}}_{i^{*}}^{\ell^{*}}$ at layer $\ell^{*} \in\left[1, \ldots, L^{*}\right]$ and position $i^{*} \in[1, \ldots, m]$ in the execution of $\mathcal{M}^{*}$ on $T$. Moreover, we define a mapping function $f(\boldsymbol{h} ; \boldsymbol{\theta}): \mathbb{R}^{d} \mapsto \mathbb{R}^{d^{*}}$ parameterized by $\boldsymbol{\theta}$ that operates on hidden representations of $\mathcal{M}$, where $d$ and $d^{*}$ denote the hidden dimension of representations in $\mathcal{M}$ and $\mathcal{M}^{*}$, respectively. This function can be the identity function, a linear or affine function learned on task-specific pairs of representations, or even more complex functions that incorporate other sources of data. The patching operation refers to dynamically replacing the representation $\overline{\boldsymbol{h}}_{i^{*}}^{\ell^{*}}$ during the inference of $\mathcal{M}^{*}$ on $T$ with $f\left(\boldsymbol{h}_{i}^{\ell}\right)$. Namely, by applying $\overline{\boldsymbol{h}}_{i^{*}}^{\ell^{*}} \leftarrow f\left(\boldsymbol{h}_{i}^{\ell}\right)$, we intervene on the generation process and modify the computation after layer $\ell^{*}$.

Overall, a Patchscope intervention applied to a representation determined by $(S, i, \mathcal{M}, \ell)$, is defined by a quintuplet $\left(T, i^{*}, f, \mathcal{M}^{*}, \ell^{*}\right)$ of a target prompt $T$, a target position $i^{*}$ in this prompt, a mapping function $f$, a target model $\mathcal{M}^{*}$, and a target layer $\ell^{*}$ of this model. It is possible that $\mathcal{M}$ and $\mathcal{M}^{*}$ are the same model, $S$ and $T$ are the same prompt, and $f$ is the identity function $\mathbb{I}$ (i.e., $\mathbb{I}(\boldsymbol{h})=\boldsymbol{h}$ ). Next, we show how this formulation covers prior interpretability methods and further extends them with new capabilities.

### 3.2. Patchscopes Encompasses Prior Methods

Many recent methods inspect LLM representations by projecting them to the output vocabulary space (nostalgebraist, 2020; Din et al., 2023; Belrose et al., 2023). Formally, an estimation of the output distribution is obtained from the representation $\boldsymbol{h}_{i}^{\ell}$ at position $i$ and layer $\ell$ by:

$$
\boldsymbol{p}_{i}^{\ell}=\operatorname{softmax}\left(W_{U} f\left(\boldsymbol{h}_{i}^{\ell}\right)\right) \in \mathbb{R}^{|V|}
$$

where $W_{U} \in \mathbb{R}^{|V| \times d}$ is the model's unembedding matrix and $f$ is a simple mapping function, such as the identity function or an affine mapping. We note that the operation applied to $f\left(\boldsymbol{h}_{i}^{\ell}\right)$ is the same computation applied by the model to the last-layer representation for obtaining the nexttoken prediction. Therefore, prior methods that inspect representations in the vocabulary space can be viewed as a class of Patchscopes that maps representations from any source layer $\ell$ to the last target layer $L^{*}$. Differences between these methods lie in the choice of $f$; logit lens (nostalgebraist, 2020; Dar et al., 2023) applies the identity function, linear shortcuts (Din et al., 2023) uses a linear mapping function, and tuned lens (Belrose et al., 2023) trains an affine mapping. Recently, Hernandez et al. (2023b) introduced LRE Attribute Lens that builds $f$ based on a relation linearity assumption, and showed its effectiveness in attribute extraction.

This class of methods has proven to be effective for different applications, for example, in improving inference efficiency via early exiting (Din et al., 2023). While the majority of methods and applications in this category use a single model $\left(\mathcal{M}^{*}=\mathcal{M}\right)$, Merullo et al. (2022) had demonstrated successful caption generation with a generative image model as $\mathcal{M}$ and a language model as $\mathcal{M}^{*}$.

Another category of inspection methods intervene on the LLM computation. Contemporary to our work, Pal et al. (2023) have investigated whether it is possible to anticipate multiple generated tokens ahead from a given hidden representation, rather than estimating just the next-token prediction. Their method (Future Lens) uses a target prompt different from the original prompt (i.e., $T \neq S$ ) and is designed to decode subsequent tokens from information encoded in a hidden representation $\boldsymbol{h}_{i}^{\ell}$. Example target prompts are "The multi-tokens present here are" and "Hello! Could you please tell me more about ". Future Lens can be cast as another Patchscope with $\mathcal{M}^{*}=\mathcal{M}$ and $\ell^{*}=\ell$.

More broadly, Patchscopes also cover recent mechanistic interpretability methods that analyze internal processes in LLMs with inference computation interventions. Specifically, causal tracing (Meng et al., 2022a) uses a source prompt augmented with Gaussian noise as the target prompt. Other previous methods have intervened on one or more target layers during inference by patching zero vectors to the computation (Wang et al., 2022; Conmy et al., 2023; Geva et al., 2023), namely, setting $f(\boldsymbol{h})=\mathbf{0}$. For a configuration summary of how these interpretability methods can be cast as Patchscope instances, see $\S$ A, Tab. 4.

### 3.3. Patchscopes Enables Novel Inspection Methods

Prior work has utilized specific patching configurations for interpretability, largely patching the same model while using the same prompt (i.e., $\mathcal{M}^{*}=\mathcal{M}, T=S$ ). The framing of Patchscopes introduces a wide range of unexplored setups potentially unlocking new inspection capabilities.

Specifically, we observe that modifying the target prompt enables an expressive decoding of a wide range of features, detached from the source prompt computation. For instance, we can use the prompt "The capital of $X$ is" to check if the capital city of a given country is extractable from its (last token) hidden representation at a specific layer. Similarly, a prompt like "Tell me facts about $X$ " can be leveraged to assess whether the model has resolved the entity name in a specific layer. Contrary to probing, this approach is not restricted by the number of classes of the chosen feature.

Moreover, when the inspected model is not expressive enough to answer certain queries, patching representations into a more capable model could be useful (Hernandez et al., 2022; Singh et al., 2023; Schwettmann et al., 2023).

## 4. Experiments

In this section, we evaluate our framework on decoding next-token predictions (§4.1), extracting attributes (§4.2), analyzing the contextualization of entity names (§4.3), and leveraging stronger models for inspection via cross-model patching (§4.4). See a summary in Tab. 1.

### 4.1. Decoding of Next-Token Predictions

As introduced in $\S 3.2$, let $\boldsymbol{p}^{L}$ be the output probability distribution for some input, obtained by multiplying the finallayer last-position hidden representation $\boldsymbol{h}^{L}$ by the unembedding matrix $W_{U} \in \mathbb{R}^{|V| \times d}$. We wish to estimate $\boldsymbol{p}^{L}$ from intermediate representations $\mathbf{h}^{\ell}$ s.t. $\ell<L$. Particularly, we ask how early in the computation the model has concluded its final prediction from the given context. In our experiments, we consider multiple LLMs - LLaMA2 (13B) (Touvron et al., 2023b), Vicuna (13B) (Chiang et al., 2023), GPT-J (6B) (Wang \& Komatsuzaki, 2021), and Pythia (12B) (Biderman et al., 2023) (see more details in §B.1).

Methods We compare vocabulary projection methods (§3.2) with a new Patchscope. Each method yields an estimated output probability $\tilde{\boldsymbol{p}}^{\ell}$ by patching an intermediate representation $\boldsymbol{h}^{\ell}$ to the model's final layer. Here, we focus on the common setting where $\mathcal{M}=\mathcal{M}^{*}$, and discuss extensions to $\mathcal{M} \neq \mathcal{M}^{*}$ in $\S 4.4$.
![](https://cdn.mathpix.com/cropped/2024_06_04_92998034bd56a769976cg-05.jpg?height=476&width=830&top_left_y=218&top_left_x=190)

Figure 2. Precision@ 1 ( $\uparrow$ is better) and Surprisal ( $\downarrow$ is better) of next-token prediction estimation in multiple models. From layer 10 and upwards, the token identity method (ours) consistently outperforms the rest of the baselines across all the models.

- Logit Lens: Following prior work (nostalgebraist, 2020; Geva et al., 2022a), we define $f$ as the identity function, meaning no change is applied to the patched representation. That is, $f(\boldsymbol{h}):=\mathbb{I}(\boldsymbol{h})$.
- Tuned Lens: Motivated by Belrose et al. (2023); Din et al. (2023), we employ an affine mapping function between representations at layer $\ell$ and the final layer $L$. Specifically, we feed the model examples from a training set $\mathcal{T}$ and for each example $s \in \mathcal{T}$ obtain a pair $\left(\boldsymbol{h}_{s}^{\ell}, \boldsymbol{h}_{s}^{L}\right)$ of hidden representations. Then, we fit a linear regression model to find the matrix $A^{\ell} \in \mathbb{R}^{d \times d}$ and the bias vector $\boldsymbol{b}^{\ell} \in \mathbb{R}^{d}$ that are numerical minimizers for $\sum_{s \in \mathcal{T}}\left\|A \boldsymbol{h}_{s}^{\ell}-\boldsymbol{h}_{s}^{L}+\boldsymbol{b}\right\|^{2}$. We define $f$ as: $f\left(\boldsymbol{h}^{\ell}\right):=A^{\ell} \boldsymbol{h}^{\ell}+\boldsymbol{b}^{\ell}$.
- Token Identity Patchscope: Unlike the previous methods, here we use a target prompt that is different from the source prompt $(T \neq S)$ and is meant to encourage decoding the token identity of the hidden representation. Also, while the above methods skip the computation between layers $l$ and $L$, here we modify it such that all the information from the source prompt computation is discarded, except for the patched representation. We craft a prompt with $k$ demonstrations representing an identity-like function, formatted as "tok ${ }_{1} \rightarrow$ tok $_{1} ;$ tok $_{2} \rightarrow$ tok $_{2} ; \ldots ;$ tok $_{k}$ ". See $\S \mathrm{B} .3$ for further details and an experiment showing this method's robustness to different demonstrations. Note that this Patchscope does not require any training.

Evaluation Following Din et al. (2023), we use Pile evaluation set and the following metrics (see details in §B.2):

- Precision@1 ( $\uparrow$ is better): The portion of examples for which the highest-probability token $t$ in the estimated probability distribution matches the highest-probability token in the original output distribution. That is, if $\arg \max _{t}\left(\tilde{\boldsymbol{p}}_{t}^{\ell}\right)=\arg \max _{t}\left(\boldsymbol{p}_{t}^{L}\right)$.
- Surprisal ( $\downarrow$ is better): The minus log-probability of the highest-probability token in the predicted distribution $\tilde{\boldsymbol{p}}^{\ell}$ according to $\boldsymbol{p}^{L}$, i.e., $-\log \boldsymbol{p}_{\tilde{t}}^{L}$, where $\tilde{t}=\arg \max _{t}\left(\tilde{\boldsymbol{p}}_{t}^{\ell}\right)$.

Results Across all the models, from layer 10 and upwards, the token identity Patchscope consistently outperforms the other baselines, obtaining a gain of up to $98 \%$ in layers 18-22 (see Fig. 2). This demonstrates the utility of leveraging the model's decoding procedure for inspecting representations of different source prompts, and shows that in most cases hidden representations in early layers carry the prediction information regardless of their context.

In the first 10 layers, performance of all methods is worsened, with the token identity prompt performing on-par with logit lens, and tuned lens performing slightly better, which could be due to the additional training of its mappings. Low performance in these layers is expected, as it is where the input contextualization happens. In $\S 4.3$, we introduce a Patchscope geared towards unraveling this process.

### 4.2. Extraction of Specific Attributes

Classification probes are arguably the most commonly used method for checking if certain attributes are encoded in hidden representations (Belinkov, 2022; Belinkov \& Glass, 2019). However, they need to be trained, and the range of attribute classes needs to be known a priori. Here we show that repurposing Patchscopes for attribute extraction overcomes these limitations. First, it does not require training. Second, it is not limited by a predefined set of labels, but rather benefits from an open vocabulary. In addition, by taking advantage of the model's nonlinearities, it can capture more complex relations compared to linear probes.

Experimental Setup Consider factual and commonsense knowledge represented as triplets $(\sigma, \rho, \omega)$ of a subject (e.g., "United States"), a relation (e.g., "largest city of"), and an object (e.g., "New York City"). We investigate to what extent the object $\omega$ can be extracted from the last token representation of the subject $\sigma$ in an arbitrary input context. To this end, we conduct experiments on 8 commonsense and 25 factual knowledge tasks curated by Hernandez et al. (2023b). This dataset includes $(\sigma, \rho, \omega)$ triplets for different relations, along with prompt templates that verbalize them in natural language. We conduct experiments with GPT-J (6B) (Wang \& Komatsuzaki, 2021), filtering the data to keep only the examples where $\omega$ appears in the the model's continuation of the prompt up to 20 tokens. The choice of 20 balances computation cost with accommodating the open-ended nature of Patchscopes, as the ground truth token does not necessarily appear in the next immediate token in a fluent

Table 2. Feature extraction accuracy (mean $\pm$ std). Comparing zeroshot feature extraction Patchscope to a logistic regression probe shows that despite using no training data, it has a significantly higher accuracy than baseline in 6 out of 12 tasks. We use pairwise t-test with Bonferroni correction for comparing the two methods. ${ }^{* *}$ and ${ }^{*}$ indicate $p<1 e-5$ and $p<1 e-4$, respectively.

|  | Task | Probe | Patchscope |
| :---: | :---: | :---: | :---: |
| $\dot{u}$ <br> $\tilde{U}$ <br> $\tilde{u}$ <br> 0 <br> $\vdots$ <br> 0 | Fruit inside color | $37.4 \pm 6.6$ | $38.0 \pm 18.7$ |
|  | Fruit outside color | $35.5 \pm 3.1$ | $71.0 \pm 13.3^{* *}$ |
|  | Object superclass | $65.6 \pm 10.5^{*}$ | $54.8 \pm 11.3$ |
|  | Substance phase | $73.8 \pm 3.7$ | $91.9 \pm 1.7^{* *}$ |
|  | Task done by tool | $10.1 \pm 3.2$ | $48.1 \pm 13.2^{* *}$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_92998034bd56a769976cg-06.jpg?height=293&width=46&top_left_y=811&top_left_x=223) | Company CEO | $5.0 \pm 2.6$ | $47.8 \pm 13.9^{* *}$ |
|  | Country currency | $17.7 \pm 2.2$ | $51.0 \pm 8.9^{* *}$ |
|  | Food from country | $5.1 \pm 3.7$ | $63.8 \pm 11.3^{* *}$ |
|  | Plays pos. in sport | $75.9 \pm 9.1$ | $72.2 \pm 7.2$ |
|  | Plays pro sport | $53.8 \pm 10.3$ | $46.3 \pm 14.2$ |
|  | Product by co. | $58.9 \pm 7.2$ | $63.2 \pm 10.7$ |
|  | Star constellation | $17.5 \pm 5.3$ | $18.4 \pm 5.1$ |

response. For each example, we sample 5 utterances from the WikiText-103 dataset (Merity et al., 2016) that include $\sigma$ and use them as $S$. Lastly, we keep tasks with at least 15 samples, which results in 5 commonsense and 7 factual tasks with a total of 1,453 datapoints. See details in $\S$ C.

Methods We compare our proposed Patchscope against linear probing (Köhn, 2015; Gupta et al., 2015).

- Zero-shot Feature Extraction Patchscope: We craft $T$ as a general verbalization of $\rho$ followed by a placeholder for $\sigma$, such that $i^{*}=m$. For example, we use $T \leftarrow$ "The largest city in x" with "x" as a placeholder for the subject. To extract the object from the entity representation in $S$, we patch the representation of token " $x$ " at layer $\ell^{*}$ with the representation of "States" from layer $\ell$, and consider if the generated text includes $\omega$. The remaining configurations of this Patchscope are $f \leftarrow \mathbb{I}, \mathcal{M}^{*} \leftarrow \mathcal{M}, i \leftarrow$ the last token of $\sigma$ in $S$. We consider all combinations of $\ell \in[1, \ldots, L] \times \ell^{*} \in\left[1, \ldots, L^{*}\right]$. Later in this section, we discuss the role of $\ell$ pertaining to attribute extraction.
- Logistic Regression Probe: Let $\Omega$ represent the range of possible objects for a given relation. We use the set of unique values of $\omega$ in the training set as a proxy for $\Omega$. We train a logistic regression probe (Köhn, 2015; Gupta et al., 2015) for each layer that predicts $\omega \in \Omega$ from the last token representation of $\sigma$. Given that 6 out of 12 tasks have fewer than 40 datapoints, we use three-fold cross-validation for training and evaluation of this baseline. Note that we have excluded tasks where the probe fails completely due to insufficient number of training examples (fewer than 15 datapoints).

Evaluation We measure the average attribute extraction accuracy. For a given sample, the Patchscope is considered correct if $\exists \ell^{*} \in\left[1, \ldots, L^{*}\right]$ where the generated text up to 20 tokens includes $\omega$. For the probe, a prediction is correct if the highest probability is assigned to $\omega$.

Results Tab. 2 summarizes the results, averaged over $\ell \in[1, \ldots, L]$. We conduct a T-test with Bonferroni correction to compare the two methods. Despite using no training data and having no restrictions on the output, the Pat ch scope achieves a significantly higher accuracy than the probe on six out of twelve tasks ( $p<1 e-5$ ), and works comparably well in all but one of the remaining six. These results suggest that, in the majority of cases, the source representation without its original context carries enough information about many attributes that a targeted Patchscope can extract. We also study how the accuracy changes across the source layers, and observe that Patchscope consistently outperforms the baseline in early layers, outperforms or works on par with the baseline in mid layers, and almost all cases where it performs worse than the baseline occur in later layers. Our interpretation is that given the language modeling training objective, the representations shift toward next-token prediction in the later layers. Therefore, the attribute of interest would not be as readily accessible via the model's computation in these layers. This interpretation is also aligned with recent findings that show no decline in using linear relational embedding in predicting $\omega$ only when the next token also happens to be $\omega$ (Hernandez et al., 2023b). Note that this pattern explains the higher standard deviation of Patchscope accuracy observed in Tab. 2. We discuss this phenomenon in more detail in §C (see Fig. 6).

### 4.3. Analyzing Entity Resolution in Early Layers

The previous sections focused on analyzing the information encoded in a single hidden state. Here we turn to consider a more global question of how LLMs resolve entity mentions across multiple layers. Concretely, given a subject entity name, such as "the summer Olympics of 1996", how does the model contextualize the input tokens of the entity and at which layer is it fully resolved?

Answering these questions is hard with existing methods; vocabulary projections focus on the output prediction and fail to show clear patterns in early layers, and probing is restricted to outputs from a fixed number of classes, which may not be expressive enough to describe this process. Alternative approaches have studied this process indirectly via interventions (Meng et al., 2022a), showing that the model constructs a subject representation at the last token of the entity name. However, it is still unclear how this

Table 3. Illustrating entity resolution via a qualitative example. The expressive generations show that as we go through the layers, more tokens from the context get integrated into the current representation. The "Generation" column shows the automatically generated text. The "Explanation" column shows our own manually coded interpretation, aiming to specify what entity the generation refers to and how that relates to the tokens processed. $\mathcal{M}^{*}=\mathcal{M} \leftarrow$ Vicuna (13B), $\ell^{*}=\ell, S \leftarrow$ "Diana, Princess of Wales".

| $\ell$ | Generation | Explanation |
| :--- | :--- | :--- |
| $1-2$ | : Country in the United Kingdom | Wales |
| 3 | : Country in Europe | Wales |
| 4 | : Title held by female sovereigns in their own <br> right or by queens consort | Princess of Wales <br> (unspecific) |
| 5 | : Title given to the wife of the Prince of Wales <br> (and later King) | Princess of Wales <br> (unspecific) |
| 6 | : Diana, Princess of Wales (1961-1997), the first <br> wife of Prince Charles, Prince of Wales, who was <br> famous for her beauty and humanitarian work | Diana, <br> Princess of Wales |

contextualization is performed.

We analyze how LLMs contextualize input entity names by leveraging Patchscopes. Particularly, we craft a target prompt for generating a description of a given subject, and apply it to the hidden representation at the last subject position in the source prompt - where the model forms the subject representation (Geva et al., 2023; Hernandez et al., 2023a) - across the early layers. This will allow us to see how the model describes the subject in each layer.

Analysis Setting We use a few-shot target prompt template for decoding an entity description: "subject 1 : description ${ }_{1}, \ldots$, subject ${ }_{k}$ : description ${ }_{k}$ x", while patching the last position corresponding to $\mathrm{x}$. We take the 200 most popular and 200 least popular subject entities from the PopQA dataset (Mallen et al., 2023). The popular entities should appear frequently in LLMs' pre-training data, and are thus likely to be captured by the model, while resolving the rare entities is expected to be more challenging (Kandpal et al., 2023; Mallen et al., 2023). Then, for the source prompt we use the entity name, and for the target prompt we sample $k=3$ random subject entities. We obtain a short (up to one sentence) description of every subject entity from Wikipedia. Our target prompt and more technical details are provided in $\S \mathrm{D} .1$. We patch the last position representations from the first 10 layers of Vicuna 13B to the target prompt and evaluate the generated subject name and description. Specifically, the generated descriptions are evaluated against the descriptions from Wikipedia using RougeL (Lin, 2004). Evaluation with Rouge1 (Lin, 2004) and Sentence-Bert (Reimers \& Gurevych, 2019) shows

![](https://cdn.mathpix.com/cropped/2024_06_04_92998034bd56a769976cg-07.jpg?height=390&width=811&top_left_y=233&top_left_x=1058)

Figure 3. RougeL scores of the generated descriptions against descriptions from Wikipedia, using Vicuna models.

similar trends (see $\S$ D.2).

Results Tab. 3 illustrates the generations by Vicuna 13B for a sample subject entity, when patching its representation at different layers to the target prompt (see more examples in $\S$ D.3). For most entities, the contextualization process is spread over the first layers, with the last subject token gradually encompassing more distant positions across layers.

This trend can be quantitatively observed by the similarity between the generated descriptions and the descriptions from Wikipedia, as measured by RougeL. See Fig. 3 where $\mathcal{M}=\mathcal{M}^{*}$. For both models, similarity increases in the first 5 layers and then slowly decreases. This decrease could potentially be attributed to contamination caused by the representation of the placeholder token " $x$ " remaining in the early layers, when patching is applied to a later layer. Note that this potential issue is only applicable to multi-token generation scenarios as future positions can still attend to the placeholder position in early layers, potentially interfering with the model's ability to accurately generate descriptions for the patched token. See $\S$ D. 3 for qualitative examples corroborating this interpretation. As expected, the scores for rare, long-tail entities are significantly lower than those of popular entities. See $\S$ D. 2 for additional results with Pythia where the smaller model seems to outperform the larger model, possibly because the larger model is biased toward output generation at the expense of input contextualization. To summarize, this analysis shows Patchscopes' utility for inspecting the contextualization process in early layers.

### 4.4. Expressiveness from Cross-Model Patching

A possible avenue for improving inspection capabilities is to explain a given model with a model that is more expressive (Bills et al., 2023). In Patchscopes, this means to patch a representation of $\mathcal{M}$ into a more expressive model $\mathcal{M}^{*}$. However, it is not clear if such an intervention would yield plausible results, due to possible discrepancies between the two models resulting from different architectures, optimization processes, and so on. Here, we present experiments on next-token prediction and entity resolution that exemplify

![](https://cdn.mathpix.com/cropped/2024_06_04_92998034bd56a769976cg-08.jpg?height=762&width=1330&top_left_y=253&top_left_x=365)

Figure 4. An illustration of CoT Patchscope on a single example. In this example, $\pi_{1} \leftarrow$ "the company that created Visual Basic Script", $\pi_{2} \leftarrow$ "The current CEO of", $S=T \leftarrow\left[\pi_{2}\right]\left[\pi_{1}\right]=$ "The current CEO of the company that created Visual Basic Script". Note that $\mathcal{M}=\mathcal{M}$ and $f \leftarrow \mathbb{I}$.

not only the feasibility, but also the opportunities unlocked by patching across models of the same family.

Next-Token Prediction We repeat the experiment in $\S 4.1$, using the token identity Patchscope. To overcome discrepancies between the models, we learn affine mappings between their layers (similarly to Tuned Lens). Our results show that source and target layers on the diagonal exhibit the highest precision values, and that patching representations to an early layer of the larger model are the most effective. Overall, suggesting that when $\mathcal{M}^{*}$ and $\mathcal{M}$ are from the same model family, it is possible to leverage $\mathcal{M}^{*}$ for decoding information from the representations of $\mathcal{M}$. For detailed results on Precision @ 1 and Surprisal, see $\S E$.

Entity Resolution in Early Layers We now show that using a large model as $\mathcal{M}^{*}$ can enhance the output expressivity. To this end, we repeat our entity resolution experiment in §4.3 with Vicuna model family, setting $\mathcal{M} \leftarrow 7 \mathrm{~B}, \mathcal{M}^{*} \leftarrow 13 \mathrm{~B}$. Fig. 3 shows the cross-model patching results (green lines) compared to the same-model patching $\mathcal{M} \leftarrow 7 \mathrm{~B}, \mathcal{M}^{*} \leftarrow 7 \mathrm{~B}$ (blue line). The results show that cross-model patching from a smaller model to its larger version generally improves the ability to inspect the input contextualization, both for popular and rare entities. For Pythia, since the smaller model outperforms the larger one, cross-model patching is not as effective (see $\S$ D.2).

## 5. Application: Correcting Multi-Hop Errors

Multi-hop reasoning is a challenging problem (Zhong et al., 2023). While a language model may be capable of correctly answering each step independently, it could still fail at processing the connection between different steps, resulting in an incorrect prediction. Recent attempts to improve multihop reasoning rely on prompting the model to generate $\mathrm{a}$ step-by-step answer autoregressively (e.g., Wei et al., 2022; Yao et al., 2023; Besta et al., 2024), some with an iterative process of self-refinement (e.g., Madaan et al., 2023). However, achieving similar benefits might be possible via directly rewiring the model's intermediate computation.

Here, we show that Patchscopes can improve multi-hop reasoning performance without generating the reasoning steps, particularly in cases where the model fails at completing a multi-hop query despite being successful in each reasoning step independently. Via Patchscopes, one can surgically operate on the model representations, reroute its intermediate answer to one reasoning step, simplify the consequent step, and ultimately correct the final prediction.

Data Building on Hernandez et al. (2023b), we systematically generate all valid multi-hop factual and commonsense reasoning queries where $\omega_{1}=\sigma_{2}$. We conduct experiments on Vicuna (13B), focusing on samples where $\mathcal{M}$ accurately represents both $\tau_{1}$ and $\tau_{2}$ independently, that is, $\omega$ appears in the next 20 tokens $\mathcal{M}$ generates conditioned on the prompt $\pi$ that verbalizes $\sigma$ and $\rho$. This process yields 1,104 multi-
hop reasoning samples, out of which 46 satisfy the above criteria and are used for evaluation. See more details in $\S$ F.

Experimental Setup Following the notation in $\S 4.2$, let $\tau_{1}=\left(\sigma_{1}, \rho_{1}, \omega_{1}\right)$ represent the relation $\rho_{1}$ between a subject entity $\sigma_{1}$ and an object entity $\omega_{1}$. Let $\tau_{2}=\left(\sigma_{2}, \rho_{2}, \omega_{2}\right)$ represent another tuple such that $\sigma_{2}=\omega_{1}$. A multi-hop reasoning query pertaining to $\tau_{1}$ and $\tau_{2}$ is a prompt composed of two parts: $\pi_{1}$ is a verbalization of $\sigma_{1}$ and $\rho_{1}$ from which $\omega_{1}$ can be inferred; $\pi_{2}$ is a verbalization of $\rho_{2}$, from which $\omega_{2}$ can be inferred after its concatenation with $\pi_{1}$. For example, Let $\tau_{1} \leftarrow$ ("Visual Basic", "product of", "Microsoft") and $\tau_{2} \leftarrow$ ("Microsoft", "company CEO","Satya Nadella"). An example verbalization of these tuples is $\pi_{1} \leftarrow$ "the company that created Visual Basic", $\pi_{2} \leftarrow$ "The current CEO of", leading to the multi-hop query $\left[\pi_{2}\right]\left[\pi_{1}\right]=$ "The current CEO of the company that created Visual Basic".

Method We introduce a Chain-of-Thought (CoT) Patchscope to fix multi-hop reasoning via intervening on the computation graph and rerouting representation likely to capture $\omega_{1}$ in place of $\sigma_{2}$. Concretely, $S$ refers to the formed query discussed above, and we use the following configuration: $T \leftarrow S, \mathcal{M}^{*} \leftarrow \mathcal{M}, i \leftarrow n, i^{*} \leftarrow$ the token preceding $\pi_{1}$. We evaluate the outputs in terms of accuracy, similarly to $\S 4.2$. For a sample $S$, the Patchscope is considered accurate if $\exists\left(\ell, \ell^{*}\right): \ell \in[1, \ldots, L], \ell^{*} \in\left[1, \ldots, L^{*}\right]$ where the autoregressive generation up to 20 tokens includes $\omega_{2}$. Fig. 4 illustrates the CoT Patchscope with an example. We use the following configuration for CoT Patchscope: $S \leftarrow \pi_{1}, T \leftarrow \pi_{2}, i \leftarrow n, i^{*} \leftarrow m$, which is equivalent to $S=T \leftarrow\left[\pi_{2}\right]\left[\pi_{1}\right]$ and adjusting the attention mask such that no token in $S$ has visibility to $\pi_{2}$ and no token in $T$ has visibility to $\pi_{1}$. In addition, we consider two baselines.

Vanilla Baseline For this baseline, we set $S \leftarrow\left[\pi_{1}\right]\left[\pi_{2}\right]$, we let the model autoregressively generate up to 20 tokens and check whether $\omega_{2}$ appears in the generation.

Chain-of-Thought Baseline Here, the setup and evaluation is similar to the vanilla baseline, except that we prepend "Let's think step by step." to $S$, following (Wei et al., 2022). We then let the model generate up to 20 tokens and check whether $\omega_{2}$ appears in the generation. Note that this experiment uses Vicuna (13B). Vicuna is based on LLaMA, with supervised finetuning on additional instruction data, which makes it amenable to chain-of-thought prompting.

Results While the vanilla baseline accuracy is only $19.57 \%$, and CoT baseline accuracy is $35.71 \%$, our proposed Patchscope achieves $50 \%$ accuracy. For more details about the interaction between $\ell$ and $\ell^{*}$, and how it affects the success rate, see Fig. 12 in $\S$ F.

We emphasize that our primary goal in this section is not to devise a new method to solve multi-hop queries that is necessarily a competitor to CoT, but rather to make a proof-of-concept while comparing with CoT as a common reference. We highlight that we have taken advantage of the extra structural information about the queries given the prior knowledge about how they were synthesized. One could potentially automate this by learning the right places to patch. Li et al. (2024) recently proposed an optimization-based approach for directly patching multi-head self-attention in mid-layers, which can be viewed as soft position-selection, and works effectively in multihop error correction, suggesting that inferring the right positions to patch could be effective. However, even if optimal source and target position are automatically decided upfront, Patchscope and CoT may not be directly comparable. CoT generates multiple steps which fundamentally extend the computational power of the LLM (Merrill \& Sabharwal, 2024), but a Patchscope with pre-identified $\left(l, l^{*}\right)$ only makes $O(1)$ inference passes.

## 6. Conclusion

We present Patchscopes, a simple and effective framework that leverages the ability of LLMs to generate humanlike text for decoding information from intermediate LLM representations. We show that many existing interpretability methods can be cast as specific Patchscope instances, and even these only cover a small portion of the framework's possible configurations. Moreover, new underexplored Patchscopes substantially improve our ability to decode various types of information from the model's internal computation, such as the output prediction and knowledge attributes, typically outperforming prominent methods that rely on projection to the vocabulary and probing. In addition, our framework enables new capabilities, such as analyzing the contextualization process of input tokens in early LLM layers, and can correct multi-hop reasoning.

There are multiple future research directions to consider. An important factor in the effectiveness of a chosen target prompt is how the information from the patched position propagates during inference to other positions and across layers. Understanding how to best use a given target prompt, perhaps automatically, is an important factor for using Patchscopes. Another avenue for future work is investigating the effectiveness of few-shot target prompts compared to zero-shot/instruction-based prompts. More expressive instruction-based target prompts, for example, could enable extracting more complex information. Additionally, while our cross-model patching focused on models from the same family, it will be valuable to explore which mapping functions would enable patching across models
from different families and perhaps with different architectures. Other directions for future work include applications across different domains and modalities, investigating variants with simultaneous multi-token patching or multi-layer patching to mitigate the risks of placeholder contamination, and presenting recipes for task-specific and task-agnostic Patchscopes.

## Acknowledgements

We thank Amir Globerson for feedback on writing and presentation of results. We thank Ardavan Saeedi, Martin Wattenberg, Ellie Pavlick, the AI Explorables team ${ }^{3}$ at Google Research, and Jasmijn Bastings for their helpful comments.

## Impact Statement

Societal Impact This paper presents a new framework for interpreting hidden representations of large language models. Interpretability methods in general can be used to investigate models' reliability and safety prior to deployment. We hope that Patchscopes framework facilitates progress in this area with the introduction of inspection tools that are more expressive, robust across layers, and do not require training data.

Limitations While our proposed framework is not limited to any particular architecture or domain, the experimental evidence provided in this paper focuses on autoregressive Transformer-based language models, and future work is needed to verify its effectiveness in other setups.

## References

Alain, G. and Bengio, Y. Understanding intermediate layers using linear classifier probes. 5th International Conference on Learning Representations, Workshop Track Proceedings, 2017.

Bansal, Y., Nakkiran, P., and Barak, B. Revisiting model stitching to compare neural representations. Advances in neural information processing systems, 34:225-236, 2021.

Belinkov, Y. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207-219, 2022 .

Belinkov, Y. and Glass, J. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49-72, 2019.

Belrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I., McKinney, L., Biderman, S., and Steinhardt, J. Eliciting[^2]

latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.

Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the 38th AAAI Conference on Artificial Intelligence, 2024.

Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning (ICML), 2023.

Bills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L., Goh, G., Sutskever, I., Leike, J., Wu, J., and Saunders, W. Language models can explain neurons in language models. URL https://openaipublic. blob. core windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023), 2023.

Casper, S., Rauker, T., Ho, A., and Hadfield-Menell, D. Sok: Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. In First IEEE Conference on Secure and Trustworthy Machine Learning, 2022.

Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing GPT-4 with $90 \% *$ ChatGPT quality, March 2023. URL https: / / lmsys . org/blog/ 2023-03-30-vicuna/.

Conmy, A., Mavor-Parker, A. N., Lynch, A., Heimersheim, S., and Garriga-Alonso, A. Towards automated circuit discovery for mechanistic interpretability. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

Csiszárik, A., Kőrösi-Szabó, P., Matszangosz, Á. K., Papp, G., and Varga, D. Similarity and matching of neural network representations. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https: / /openreview.net/forum?id=aedFIIRRfXr.

Dar, G., Geva, M., Gupta, A., and Berant, J. Analyzing transformers in embedding space. In Annual Meeting of the Association for Computational Linguistics, 2023.

Din, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump to conclusions: Short-cutting transformers with linear transformations. arXiv preprint arXiv:2303.09435, 2023.

Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020 .

Geiger, A., Lu, H., Icard, T., and Potts, C. Causal abstractions of neural networks. Advances in Neural Information Processing Systems, 34:9574-9586, 2021.

Geva, M., Caciularu, A., Dar, G., Roit, P., Sadde, S., Shlain, M., Tamir, B., and Goldberg, Y. LM-debugger: An interactive tool for inspection and intervention in transformerbased language models. In Che, W. and Shutova, E. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 12-21, Abu Dhabi, UAE, December 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-demos.2. URL https: //aclanthology.org/2022.emnlp-demos.2.

Geva, M., Caciularu, A., Wang, K., and Goldberg, Y. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 30-45, Abu Dhabi, United Arab Emirates, December 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main. 3. URL https://aclanthology.org/2022. emnlp-main. 3 .

Geva, M., Bastings, J., Filippova, K., and Globerson, A. Dissecting recall of factual associations in auto-regressive language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 12216-12235, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.751. URL https://aclanthology. org/2023.emnlp-main. 751.

Ghandeharioun, A., Kim, B., Li, C.-L., Jou, B., Eoff, B., and Picard, R. W. Dissect: Disentangled simultaneous explanations via concept traversals. arXiv preprint arXiv:2105.15164, 2021.

Goldowsky-Dill, N., MacLeod, C., Sato, L., and Arora, A. Localizing model behavior with path patching. arXiv preprint arXiv:2304.05969, 2023.

Gupta, A., Boleda, G., Baroni, M., and Padó, S. Distributional vectors encode referential attributes. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 12-21, 2015.
Hanna, M., Liu, O., and Variengien, A. How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=p4PckNQR8k.

Hase, P., Bansal, M., Kim, B., and Ghandeharioun, A. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. arXiv preprint arXiv:2301.04213, 2023.

Hendel, R., Geva, M., and Globerson, A. In-context learning creates task vectors. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.

Hernandez, E., Schwettmann, S., Bau, D., Bagashvili, T., Torralba, A., and Andreas, J. Natural language descriptions of deep features. In International Conference on Learning Representations, 2022. URL https : / openreview.net/forum?id=NudBMY-tzDr.

Hernandez, E., Li, B. Z., and Andreas, J. Measuring and manipulating knowledge representations in language models. arXiv preprint arXiv:2304.00740, 2023a.

Hernandez, E., Sharma, A. S., Haklay, T., Meng, K., Wattenberg, M., Andreas, J., Belinkov, Y., and Bau, D. Linearity of relation decoding in transformer language models. arXiv preprint arXiv:2308.09124, 2023b.

Kandpal, N., Deng, H., Roberts, A., Wallace, E., and Raffel, C. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pp. 15696-15707. PMLR, 2023.

Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning, pp. 2668-2677. PMLR, 2018.

Köhn, A. What's in an embedding? analyzing word embeddings through multilingual evaluation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 2067-2073, 2015.

Langedijk, A., Mohebbi, H., Sarti, G., Zuidema, W., and Jumelet, J. Decoderlens: Layerwise interpretation of encoder-decoder transformers. arXiv preprint arXiv:2310.03686, 2023.

Lenc, K. and Vedaldi, A. Understanding image representations by measuring their equivariance and equivalence. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 991-999, 2015.

Li, Z., Jiang, G., Xie, H., Song, L., Lian, D., and Wei, Y. Understanding and patching compositional reasoning in llms. arXiv preprint arXiv:2402.14328, 2024.

Lieberum, T., Rahtz, M., Kramár, J., Irving, G., Shah, R., and Mikulik, V. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv preprint arXiv:2307.09458, 2023.

Lin, C.-Y. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013.

Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., and Clark, P. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=S37hOerQLB.

Madsen, A., Reddy, S., and Chandar, S. Post-hoc interpretability for neural nlp: A survey. ACM Computing Surveys, 55(8):1-42, 2022.

Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi, H. When not to trust language models: Investigating effectiveness of parametric and nonparametric memories. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9802-9822, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.546. URL https: //aclanthology.org/2023.acl-long.546.

Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372, 2022a.

Meng, K., Sharma, A. S., Andonian, A. J., Belinkov, Y., and Bau, D. Mass-editing memory in a transformer. In The Eleventh International Conference on Learning Representations, 2022b.

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2016.

Merrill, W. and Sabharwal, A. The expressive power of transformers with chain of thought. In The Twelfth International Conference on Learning Representations, 2024.
Merullo, J., Castricato, L., Eickhoff, C., and Pavlick, E. Linearly mapping from image to text space. In The Eleventh International Conference on Learning Representations, 2022.

Merullo, J., Eickhoff, C., and Pavlick, E. A mechanism for solving relational tasks in transformer language models. arXiv preprint arXiv:2305.16130, 2023.

Mousi, B., Durrani, N., and Dalvi, F. Can llms facilitate interpretation of pre-trained language models? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, December 2023. URL https: //browse.arxiv.org/pdf/2305.13386.pdf.

Nanda, N., Lee, A., and Wattenberg, M. Emergent linear representations in world models of self-supervised sequence models. In Belinkov, Y., Hao, S., Jumelet, J., Kim, N., McCarthy, A., and Mohebbi, H. (eds.), Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. 16-30, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. blackboxnlp-1.2. URL https://aclanthology . org/2023.blackboxnlp-1.2.

nostalgebraist. interpreting gpt: the logit lens. LessWrong, 2020. URL https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens.

Pal, K., Sun, J., Yuan, A., Wallace, B. C., and Bau, D. Future lens: Anticipating subsequent tokens from a single hidden state. In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pp. 548-560, 2023.

Patel, R. and Pavlick, E. Mapping language models to grounded conceptual spaces. In International Conference on Learning Representations, 2021.

Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ $\mathrm{abs} / 1908.10084$.

Schwettmann, S., Chowdhury, N., Klein, S., Bau, D., and Torralba, A. Multimodal neurons in pretrained text-only transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2862-2867, 2023.

Singh, C., Hsu, A., Antonello, R., Jain, S., Huth, A., Yu, B., and Gao, J. Explaining black box text modules in natural
language with language models. In XAI in Action: Past, Present, and Future Applications, 2023. URL https: / /openreview.net/forum?id=3BX9tM03GT.

Slobodkin, A., Goldman, O., Caciularu, A., Dagan, I., and Ravfogel, S. The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 3607-3625, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.220. URL https: / / aclanthology.org/2023.emnlp-main. 220.

Stolfo, A., Belinkov, Y., and Sachan, M. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 7035-7052, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.435. URL https: / / aclanthology.org/2023.emnlp-main. 435.

Strobelt, H., Gehrmann, S., Pfister, H., and Rush, A. M. Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks. IEEE transactions on visualization and computer graphics, 24(1):667-676, 2017.

Tenney, I., Das, D., and Pavlick, E. Bert rediscovers the classical nlp pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593-4601, 2019.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Vig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S. Investigating gender bias in language models using causal mediation analysis. Advances in neural information processing systems, 33: $12388-12401,2020$.

Vilas, M. G., Schaumlöffel, T., and Roig, G. Analyzing vision transformers for image classification in class embedding space. arXiv preprint arXiv:2310.18969, 2023.
Wallat, J., Singh, J., and Anand, A. BERTnesia: Investigating the capture and forgetting of knowledge in BERT. In Alishahi, A., Belinkov, Y., Chrupała, G., Hupkes, D., Pinter, Y., and Sajjad, H. (eds.), Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 174183, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.blackboxnlp-1. 17. URL https://aclanthology.org/2020. blackboxnlp-1.17.

Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax, May 2021.

Wang, K. R., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, 2022.

Wang, Z., Ku, A., Baldridge, J., Griffiths, T. L., and Kim, B. Gaussian Process Probes (GPP) for uncertainty-aware probing. arXiv preprint arXiv:2305.18213, 2023.

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824-24837, 2022.

Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. R. Tree of thoughts: Deliberate problem solving with large language models. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.

Youssef, P., Koraş, O., Li, M., Schlötterer, J., and Seifert, C. Give me the facts! a survey on factual knowledge probing in pre-trained language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 15588-15605, 2023.

Zhang, F. and Nanda, N. Towards best practices of activation patching in language models: Metrics and methods. arXiv preprint arXiv:2309.16042, 2023.

Zhong, Z., Wu, Z., Manning, C. D., Potts, C., and Chen, D. Mquake: Assessing knowledge editing in language models via multi-hop questions. arXiv preprint arXiv:2305.14795, 2023.

Zhou, B., Bau, D., Oliva, A., and Torralba, A. Interpreting deep visual representations via network dissection. IEEE transactions on pattern analysis and machine intelligence, 41(9):2131-2145, 2018.
