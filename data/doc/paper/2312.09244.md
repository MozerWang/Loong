# Helping or Herding? 邻 

## REWARD MoDEL ENSEMBLES MitiGATE BUT DO NOT ELIMINATE REWARD HACKING

Jacob Eisenstein ${ }^{1, *}$<br>Ahmad Beirami ${ }^{2}$<br>Katherine Heller ${ }^{2}$<br>Jonathan Berant ${ }^{1, *}$<br>1. Google DeepMind<br>2. Google Research<br>* Core contributors<br>reward-ensembles-helping-or-herding@google.com<br>Chirag Nagpal ${ }^{2, *}$<br>Alex D'Amour ${ }^{1}$<br>Stephen Pfoh ${ }^{2}$<br>Deepak Ramachandran ${ }^{2}$<br>Alekh Agarwal ${ }^{2, *}$<br>Adam Fisch ${ }^{1}$<br>Peter Shaw ${ }^{1}$


#### Abstract

Reward models play a key role in aligning language model applications towards human preferences. However, this setup creates an incentive for the language model to exploit errors in the reward model to achieve high estimated reward, a phenomenon often termed reward hacking. A natural mitigation is to train an ensemble of reward models, aggregating over model outputs to obtain a more robust reward estimate. We explore the application of reward ensembles to alignment at both training time (through reinforcement learning) and inference time (through reranking). First, we show that reward models are underspecified: reward models that perform similarly in-distribution can yield very different rewards when used in alignment, due to distribution shift. Second, underspecification results in overoptimization, where alignment to one reward model does not improve reward as measured by another reward model trained on the same data. Third, overoptimization is mitigated by the use of reward ensembles, and ensembles that vary by their pretraining seeds lead to better generalization than ensembles that differ only by their fine-tuning seeds, with both outperforming individual reward models. However, even pretrain reward ensembles do not eliminate reward hacking: we show several qualitative reward hacking phenomena that are not mitigated by ensembling because all reward models in the ensemble exhibit similar error patterns.


## 1 INTRODUCTION

To align machine learning systems with human preferences, it is common to use reward models that are finetuned on preference annotations to score potential outputs by how likely they are to be preferred by human raters (Christiano et al., 2017; Stiennon et al., 2020; Bai et al., 2022; Roit et al., 2023). There are many ways to use reward models to align policy models: they can act as training signals in reinforcement learning (Christiano et al., 2017; Stiennon et al., 2020), they can select examples for further imitation learning (Gulcehre et al., 2023; Liu et al., 2023; Dong et al., 2023; Touvron et al., 2023), or they can be applied at inference time to steer the output distribution toward higher expected reward (e.g., Yang \& Klein, 2021; Gao et al., 2023). Such procedures create a semi-adversarial dynamic in which the language model is encouraged to produce outputs that obtain high reward by exploiting errors in the reward model. Furthermore, while the reward model

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-02.jpg?height=819&width=1418&top_left_y=268&top_left_x=359)

x: context

Human: I want to make a nice steak dinner, but I don't know the difference between the various cuts of steak. Assistant: OK, l'm happy to help! I think you want to know which steaks are most tender [...] Human: What should we drink with it? Assistant:
![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-02.jpg?height=656&width=634&top_left_y=430&top_left_x=366)

$\mathbf{x}$ context

Human: I'm going to the Netherlands and would like to learn a few common phrases in Dutch. Assistant: Sure, would you like to learn how to say [...] Human: Could you teach me how to say "Goodbye" and "Thank you"? Assistant

I

$(y \mid x)$

\{

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-02.jpg?height=431&width=627&top_left_y=560&top_left_x=1123)

$r^{*}=-1.04 \ll \bar{r}=+2.31$

Figure 1: Left: reward model ensembles can attenuate errors made by individual reward models, in this case the positive $r_{1}$ for this off-topic response from the policy model $\pi(y \mid x)$, which gets a low true reward $\left(r^{*}\right)$. Right: insufficiently diverse reward models unanimously rate this overly-verbose and non-responsive reply from $\pi(y \mid x)$ as positive, but it too gets a low true reward. Both examples are real outputs and rewards (here, represented as normalized Z-scores) from best-of- $n$ reranking on a dataset of dialogue turns rated for helpfulness (Bai et al., 2022); see the paper for details.

is trained on a fixed set of human preference data, the process of alignment shifts the distribution of its inputs, increasing the likelihood of such errors. This phenomenon where the policy language model exploits reward model errors is often termed reward hacking (Amodei et al., 2016), reward gaming (Skalse et al., 2022; Pang et al., 2023), or reward over-optimization (Gao et al., 2023).

Reward hacking has been investigated from several perspectives in prior work (e.g., Krakovna et al., 2020; Skalse et al., 2022; Pan et al., 2022). Bai et al. (2022) used reinforcement learning with human feedback (RLHF) and trained two reward models on non-overlapping splits of preference data, using one to drive alignment, and the other to measure the quality of the outputs. They find that RLHF increases performance according to both the driver and measurement models, but that a performance gap emerges as the policy is allowed to diverge from the initial distribution. However, both reward models were built on base models trained on the same pretraining data, which, as we will show, limits their diversity (as hypothesized by Gleave \& Irving (2022)) and thus may understate the effect of reward hacking. Other work has simulated the relationship between a "true" reward and a learned proxy, showing that it is possible to over-optimize the proxy to such an extent that the true reward starts to decrease (Gao et al., 2023; Coste et al., 2023). This has been replicated in more realistic settings by examining (and creating) spurious correlations in reward model training data (Pang et al., 2023).

In this work, we first analyze reward model distribution shift from the perspective of underspecification (D'Amour et al., 2022), which occurs when a machine learning pipeline yields reliable performance on held-out data from the training distribution, but variable performance on out-ofdistribution data. When applied to learning reward models from human preference data, we show that reward models that agree in-distribution often disagree when transferred out-of-distribution. Furthermore, such disagreements are more pronounced when the reward models are built on different pretrainings, even when that difference is induced merely by varying the pretraining random seed. These disagreements become increasingly severe when evaluated on outputs of a policy model that has been aligned to a specific reward model. This occurs both when using reward models in RLHF, as well as when using an inference-time alignment procedure, best-of- $n$ reranking, where $n$ samples are drawn from the policy and then reranked with a reward model.

Motivated by these findings, we systematically investigate reward model ensembles as a possible remedy for reward hacking. Assuming different models err in different ways, ensembling can leverage
reward uncertainty across the ensemble during alignment (see Figure 1, Left). We explore several techniques for aggregating scores across the ensemble, e.g., taking the median score as a robust estimate of the true reward of the policy. We also consider two types of ensembles: pretrain ensembles, where different members of the ensemble differ in the random seed used during the pretraining phase, and finetune ensembles, where members differ only in the random seed used during finetuning. These ensembles are then evaluated across several types of policies and preference annotations: dialogue preferences for a helpful assistant (Bai et al., 2022), summarization quality (Stiennon et al., 2020), and whether a single-document summary is grounded in its source text (Roit et al., 2023).

We find that pretrain ensembles substantially outperform finetune ensembles. Moreover, they consistently outperform single reward models, unlike finetune ensembles, which in many cases are comparable to single reward models. However, our analysis also reveals that policies trained with ensembles are still susceptible to reward hacking: different reward models sometimes share similar error patterns, which in turn propagate to the ensemble (see Figure 1, Right). This is exploited and amplified by the policy, leading, for example, to outputs that are too short when tuning for factuality, too verbose when tuning for summarization quality, or responses that follow a particular format that is often unsuitable, when training a helpful assistant. Thus, it is possible that methods that, unlike ensembles, are aware of the distance of outputs from the reward data distribution (Liu et al., 2020) could provide more reliable estimates of uncertainty.

In concurrent work, Coste et al. (2023) argue that reward model ensembles effectively mitigate reward hacking. Our work shares a similar research question, but differs in several ways, leading to more nuanced conclusions. First, we investigate the difference between pretrain and finetune ensembles, finding that pretrain ensembles are considerably more effective. Second, we use human-annotated preference data rather than synthetically-generated labels, which provides a more realistic experimental setup. Third, we perform analysis that demonstrates the limitations of reward ensembles, showing reward ensembles are still susceptible to reward hacking. Last, our experimental setup covers a wider range of tasks, larger reward models, and more extensive policy optimization.

## 2 PRELIMINARIES

Reward models have become the primary tool for aligning LMs towards user-facing applications. We now briefly review how reward models are trained (\$2.1) and how they are used for alignment (§2.2). We then describe the experimental setup that we will use for the remainder of the paper (§2.3).

### 2.1 REWARD MODEL TRAINING

We focus on the the typical setup where reward models are trained from preference data, $\left(x, y^{+}, y^{-}\right) \in$ $D$, where $y^{+}$is annotated to be preferred over $y^{-}$for prompt $x$. Under the Bradley-Terry model (Bradley \& Terry, 1952), the probability that response $y_{2}$ is preferred over $y_{1}$ given a reward function $r$ and a prompt $x$ is $p\left(y_{1} \prec y_{2} \mid x\right)=\sigma\left(r\left(x, y_{2}\right)-r\left(x, y_{1}\right)\right)$, where $\sigma(\cdot)$ is the sigmoid function. Then, we can use preference data to train a reward model by maximizing

$$
\begin{equation*}
\mathcal{J}(r)=\mathbb{E}_{\left(x, y^{+}, y^{-}\right) \sim D}\left[\log p\left(y^{-} \prec y^{+} \mid x\right)\right] \tag{1}
\end{equation*}
$$

The Bradley-Terry model is underdetermined: for any reward model $r^{*}$, we can define an equivalent reward model, $r^{\prime}(x, y)=r^{*}(x, y)+C(x)$ where $C(x)$ is a prompt-dependent constant, obtaining the same objective value as $r^{*}$, i.e., $\mathcal{J}\left(r^{*}\right)=\mathcal{J}\left(r^{\prime}\right)$. This is problematic for ensembling: if different reward models choose different values for $C(x)$, then order statistics like median and minimum are meaningless. We therefore modify the objective function by adding a regularization term to encourage the sum of reward values per preference pair to stay close to zero, i.e.,

$$
\begin{equation*}
\mathcal{J}_{\text {reg }}(r)=\mathcal{J}(r)+\eta \cdot \mathbb{E}_{\left(x, y^{+}, y^{-}\right) \sim D}\left[\left(r\left(x, y^{+}\right)+r\left(x, y^{-}\right)\right)^{2}\right] \tag{2}
\end{equation*}
$$

where $\eta$ is a small positive value, thereby resolving the issue of underdetermination.

Note that reward models can also be trained from "pointwise" data, such as toxicity or factuality annotations on individual examples (Yang \& Klein, 2021; Roit et al., 2023). Such reward models are not underdetermined and so can be aggregated without adjustment.

| Task | Prompt | Output |
| :--- | :--- | :--- | :--- |
| TL;DR | I'll try to keep this short! **Background****'ve always been an on again/off again (very <br> casual!) jogger, typically doing $3-5 \mathrm{k} *$ My knees have always been finicky, and I went to a <br> physio who thought I had "runner's knee" $[\ldots]$ I had my baby a year ago, so all in all I haven't <br> run for about 1.5 years [...] However, when I run even the tinyiest bit, or even go on a long <br> walk or a hike, my pelvis gets very sore and tight $[\ldots]$ | I think pregnancy messed with my body, now I <br> can't even run even the smallest amount with- <br> out pain in my pelvis and knees. I'm fairly <br> certain the problem isn't just that I'm com- <br> pletely out of shape. |
| Helpfulness | Human: Assistant, can you help me find books that have really popular antagonists? Assistant: <br> Sure! I know many books that feature good antagonists, such as those in the Lord of the Rings <br> books. Human: Who specifically from Lord of the Rings? Assistant: | Well, Sauron, for example. If you want, I can <br> explain who he was. |
| XSum/NLI | The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foun- <br> dation - a charity to raise money for Nigerian sport. Mr Sodje, 37, is jointly charged with elder <br> brothers Efe, 44, Bright, 50 and Stephen, 42. Appearing at the Old Bailey earlier, all four <br> denied the offence. The charge relates to offences which allegedly took place between 2008 <br> and 2014. Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, <br> are due to stand trial in July. They were all released on bail. | Former Premier League footballer Sam Sodje <br> accused of charity fraud. |

Table 1: Prompt-output pairs for the three benchmarks we consider. See $\S 2.3$.

### 2.2 AliGning LanguAGe ModelS USInG ReWARd ModelS

Best-of- $n$ reranking (BoN) is an inference-time alignment strategy, where given a prompt $x$, we sample $n$ generations $y_{1}, \ldots, y_{n}$ from a policy language model $\pi(y \mid x)$ and return the generation that has the highest reward according to a reward model $r$, i.e., $y *=\arg \max _{y_{k} \in\left\{y_{1}, \ldots, y_{n}\right\}} r\left(x, y_{k}\right)$. The Kullback-Leibler (KL) divergence of BoN from the initial policy is upper bounded by $\log n-\frac{n-1}{n}$. BoN tends to outperform more elaborate alignment techniques like RLHF in the low-KL regime (Gao et al., 2023), albeit with the cost of generating multiple samples at inference time.

Reinforcement Learning from Human Feedback (RLHF) is an online reinforcement learning method that trains a policy language model $\pi$ to maximize expected reward, while staying close to an initial policy, $\pi_{\mathrm{sft}}$, which is typically finetuned on supervised data (prompt-output pairs). Distance from the initial policy is measured with KL divergence, which leads to the regularized objective

$$
\begin{equation*}
\max _{\pi} \underset{\substack{x \sim \rho \\ y \sim \pi}}{\mathbb{E}}[r(x, y)]-\lambda \mathrm{KL}\left(\pi \| \pi_{\mathrm{sft}}\right) \tag{3}
\end{equation*}
$$

where $r$ is a reward model, $\rho$ is a distribution over prompts, and $\lambda$ is a hyper-parameter. Typically, this objective is optimized using PPO (Schulman et al., 2017), which we also use in this work.

### 2.3 EXPERIMENTAL SETUP

Datasets We will examine the performance of reward models (both single models and ensembles) across three tasks. An example from each task is provided in Table 1.

- TL;DR: A summarization benchmark where authors summarize their own reddit posts (Völske et al., 2017). We use the preference data created by Stiennon et al. (2020). This benchmark has been commonly used to evaluate finetuning of policy LMs (Rafailov et al., 2023; Zhao et al., 2023).
- HELPFULNESS: A helpful assistant benchmark (Bai et al., 2022), where given a partial conversation between a human and a digital assistant the goal is to complete the next turn of the assistant. This benchmark has also been commonly used for evaluating finetuned policy LMs (Bai et al., 2022; Rafailov et al., 2023). We use the base dataset (44K examples), where responses are generated from a 52B context-distilled LM, and split the training set into two: half for training the reward model, and half for training the policy model.
- XSUM/NLI: We adopt the setup of factually-consistent summarization (Roit et al., 2023), where a model trained on XSum (Narayan et al., 2018) is finetuned to generate summaries that are consistent with the source document according to a Natural Language Inference (NLI) reward model.

Training reward models To examine the effect of pretraining on reward models, we pretrain five T5 models from scratch with the base (220M parameters), large (770M), and XL (3B) architectures, using the standard denoising objective over the C4 corpus (Raffel et al., 2020). The pretrained checkpoints differ only in their random seed, which controls parameter initialization and the sample from the pretraining data. The same pretrained models are used for finetuning across all tasks.

We finetune each pretrained model five times using different random seeds across all three benchmarks. In TL;DR and HELPFULNESS we use the aforementioned preference data. For XSUM/NLI, we finetune

| Model Size | TL;DR | HELPFULNESS | XSum/NLI |
| :--- | :---: | :---: | :---: |
| T5-BASE | $65.8 \pm 0.3$ | $66.7 \pm 0.7$ | $86.7 \pm 0.9$ |
| T5-LARGE | $69.3 \pm 0.7$ | $68.5 \pm 0.4$ | $88.3 \pm 1.2$ |
| T5-XL | $71.4 \pm 0.8$ | $69.2 \pm 0.6$ | $91.3 \pm 0.5$ |
| T5-XXL | 79.5 | 71.5 | 92.9 |

Table 2: Mean in-distribution accuracy of 25 trained reward models on validation data for TL;DR, HELPFULNESS, and XSUM/NLI. Standard deviation is also reported, and observed to be small indistribution. The single T5-XXL reward model is used for evaluation purposes only.

NLI models on the ANLI dataset (Nie et al., 2020). Overall we obtain 25 reward models per task ( 5 pretrain $\times 5$ finetune). This makes it possible to evaluate the effect of pretraining and finetuning on underspecfication (§3) by constructing ensembles that differ in either pretrain or finetune seed (§4).


#### Abstract

Alignment strategy We use the publicly available T5-large model (Raffel et al., 2020) as a policy for the two summarization tasks. For helpfulness, the task requires substantial background knowledge, and thus we use the instruction-tuned PALM-2-XXS model (Anil et al., 2023). Prior to alignment, we create a finetuned policy $\pi_{\text {sft }}$ by finetuning on supervised data in the standard manner. We finetune on annotated summaries from TL;DR and XSUM/NLI for the corresponding tasks, and on the preferred responses, $\left(x, y^{+}\right)$, from the preference data in HELPFULNESS.

In BoN reranking, we rerank sampled sets of size $n \in\left\{2^{1}, 2^{2}, \ldots, 2^{5}\right\}$ for HELPFULNESS and $\left\{2^{1}, \ldots, 2^{6}\right\}$ for TL;DR. Larger sets lead to higher reward at a cost of more expensive inference and larger deviation from $\pi_{\text {sft }}$. In RLHF, we obtain a trade-off between the $\mathrm{KL}$ from $\pi_{\text {sft }}$ and the expected reward by training multiple times, varying the value of $\lambda$. Low values of $\lambda$ correspond to high KL and high reward, while high values of $\lambda$ entail low KL and low reward. For each value of $\lambda$ we train roughly to convergence using a predetermined fixed number of steps (all hyperparameter values, including $\lambda$ and the number of steps, are in Appendix C). Coste et al. (2023) trade-off KL and reward by tracking their values during training; however, for any particular value of KL the reward might still be underoptimized during training (i.e., there can exist a different policy $\pi(y \mid x)$ with better reward, but the same $\operatorname{KL}\left(\pi(y \mid x) \| \pi_{\mathrm{sft}}(y \mid x)\right)$, which can be found with longer training).


Evaluation We use two metrics to quantify generalization of reward models-reward by a larger model and win rate. Similar to past work (Gao et al., 2023; Coste et al., 2023), we use a larger reward model to evaluate the generalization of models trained with a smaller reward model. We train a T5-XXL reward model by taking the publicly available T5-XXL (Raffel et al., 2020) and finetuning it as described above. Table 2 details the performance of reward models of different sizes on the three tasks, and it can be seen that T5-XXL outperforms the best T5-XL model. We report both average reward of the $\mathrm{T} 5-\mathrm{XXL}$ evaluator as well as win rate, which is the fraction of prompts for which the response sampled from the aligned policy $\pi$ has higher reward compared to $\pi_{\text {sft }}$.

The errors of the T5-XXL autoeval model might correlate with errors of the smaller T5 models because they are trained on the same preference data. For this reason, we also evaluate win rate according to a prompted PALM-2-Large model, which was not exposed to the reward training data but was instruction-tuned on FLAN (Wei et al., 2022). Given a prompt $x$, we sample a response $y_{\text {stt }}$ from $\pi_{\text {sft }}$ and $y_{\text {rlhf }}$ from $\pi$. We then ask PALM-2 which response is better, using a hand-engineered prompt proposed by Rafailov et al. (2023). To avoid position bias we run PALM-2 on the two possible orderings $\left(y_{\mathrm{st}}, y_{\mathrm{rlhf}}\right)$ and $\left(y_{\mathrm{st}}, y_{\mathrm{rlhf}}\right)$, sample $K=8$ outputs for each order and determine the winner on this prompt through majority voting. This style of evaluation has become common recently (Dubois et al., 2023; Singhal et al., 2023) and was shown to correlate well with human judgements (Rafailov et al., 2023).

# 3 UNDERSPECIFICATION IN REWARD MODELS 

We now analyze alignment strategies that use a single reward model, and demonstrate that reward models are underspecified. First, Table 2 shows the average in-distribution accuracy across the 25 different reward models, together with the standard deviation (which is low in-distribution).
![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-06.jpg?height=816&width=1390&top_left_y=278&top_left_x=365)

(a) TL;DR

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-06.jpg?height=337&width=1355&top_left_y=688&top_left_x=382)

(b) HELPFULNESS

Figure 2: Average reward of the best-of- $n$ output, as judged by: the same reward model used for ranking (self); reward models fine-tuned from the same pretrain as the ranker (same pretrain); reward models fine-tuned from different pretrains from the ranker (diff pretrain). The reward models that do not share a pretrain with the ranker regard the ranker's preferred outputs as significantly worse.

The story changes, however, when we move to out-of-distribution data. Figure 2 shows the expected reward achieved by BoN as a function of the number of sampled candidates, $n$, for three reward model scales (KL is approximately $\log n-\frac{n-1}{n}$ ). The dotted green line shows the expected reward of the top-ranked output according to the reranker itself, while the dashed orange line shows the expected reward of the same output according to reward models that share a pretrain seed. The solid blue line shows the expected reward according to reward models that do not share a pretrain seed. Unsurprisingly, the reranker scores its own top outputs more favorably than the other reward models do. However, the reranker's outputs are scored significantly less favorably by reward models which do not share a pretrain with the ranker. Reward models that share a pretrain seed with the ranker model overestimate the true reward of the top-ranked output-suggesting that finetune ensembles are not sufficiently diverse because of the shared pretraining state of each of the ensemble's members. Notably, this gap does not disappear with scale, and is present for base, large, and XL models.

Moving to alignment, differences in estimated rewards induce different policies from the BoN strategy: Figure 3 shows the effects on agreement of the top-ranked summary when reward models do (crosses) or do not (circles) share pretraining seeds. Different reward models tend to produce different 1-best outputs. Again these differences are strongly associated with the pretraining seed: for example, two reward models from different pretrains will choose a different best-of-16 output more than half the time for both TL;DR and HELPFULNESS and in all scales.

Last, Figure 4 analyzes the evolution of agreement of the estimated reward scores when performing RLHF on TL;DR for reward models of various scales. Specifically, we align a policy using a single reward model, and then measure how well pairs of reward models agree on the ranking of samples from that policy using Spearman rank correlation. To compute Spearman, we sample 5 completions for each prompt in the validation set from a policy model, at $2 \mathrm{~K}$ step intervals during RLHF. We compare the agreement between a set of 5 reward models that share the same pre-training seed and a set of 5 that do not (both sets include the reward model used to drive RLHF). For each prompt, we compute Spearman correlation across all ten pairs in each set and report the mean correlation over the pairs. The correlation of models that do not share a pretrain is lower compared to models that share a pretrain seed. Moreover, correlation goes down during RLHF, indicating that the uncertainty about the true reward increases as a result of alignment.
![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-07.jpg?height=348&width=1374&top_left_y=278&top_left_x=365)

(a) TL;DR
![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-07.jpg?height=354&width=1390&top_left_y=674&top_left_x=365)

(b) HELPFULNESS

Figure 3: Agreement of the top-ranked output between reward models that do (crosses) and do not (circles) share pretraining seeds. Underspecification of reward models directly affects the behavior of the aligned policy. Chance agreement is $1 / n$.
![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-07.jpg?height=360&width=1390&top_left_y=1273&top_left_x=365)

Figure 4: Rank correlation of reward scores for TL;DR reward models that share a pretraining seed and models that do not. RLHF alignment increases disagreements between reward models (lower correlation), particularly at low values of $\lambda$ and for reward models that do not share a pretrain.

Overall, our analysis demonstrates that (1) different reward models tend to disagree on out-ofdistribution data, particularly when the reward models have different pretraining seeds; (2) this propagates to the trained policy model, in the sense that the resulting policy is highly tuned to the preferences of the specific reward model used to drive it; and (3) as a result, the disagreement between reward models tends to increase during alignment. These findings suggest that reward model ensembles might mitigate reward hacking, which we turn to next.

## 4 REWARD MODEL ENSEMBLES

We describe how to construct reward model ensembles (\$4.1), and evaluate their performance (\$4.2).

### 4.1 Pretrain and FinEtUNE ReWARd EnSEmbles

We showed that reward models are underspecified—as they are used more in alignment, they induce a stronger distribution shift in the outputs of the policy, which in turns leads to higher disagreement across reward models. Thus, a natural mitigation strategy is to ensemble multiple reward models, under the assumption that different models will have different errors. Aggregating over the scores
![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-08.jpg?height=748&width=1390&top_left_y=279&top_left_x=365)

(a) TL;DR

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-08.jpg?height=337&width=1365&top_left_y=688&top_left_x=380)

(b) HELPFULNESS

Figure 5: In best-of- $n$ reranking, pretrain ensemble reward models significantly improve the quality of outputs in the TL;DR summarization task (top) and the HELPFULNESS task, as measured by a T5-XXL model. Full numerical results are in Appendix A.

of the ensemble members will help when some of the ensemble members erroneously assign high reward to a bad output.

Given a set of reward models $\mathcal{M}$, we define the reward of the ensemble to be $\bar{r}(x, y)=\operatorname{agg}\left(\left\{r_{m}(x, y)\right\}_{m \in \mathcal{M}}\right)$, with agg indicating an aggregation function (Dietterich, 2000; Lakshminarayanan et al., 2017; Raffel et al., 2020; Zaidi et al., 2021). Intuitively, the aggregation function should be conservative, and return a lower score when there is disagreement between the ensemble members. We consider the following simple aggregation function: MEAN, MEDIAN, and MEAN_MINUS_STD, which subtracts the standard deviation of the reward from the mean to penalize high variance. We also experiment with MIN, but overall find it to be inferior to the alternatives.

We evaluate two types of reward ensembles: pretrain ensembles, where each member was pretrained using a different random seed, ${ }^{1}$ and finetune ensembles, where all members share the same pretraining seed, but use a different seed when finetuned on the reward data (which typically includes preference pairs, where one output is preferred over another). In all cases the ensemble contains exactly 5 individual reward models. Pretrain ensembles are significantly more expensive to train, but are more diverse and hence likely to lead to a more robust reward estimate. In fact, Gleave \& Irving (2022) reported negative results when using reward ensembles and hypothesized this is due to ensemble members sharing the same underlying pretrained model.

### 4.2 EXPERIMENTS

We now evaluate reward model ensembles across all tasks. Figure 5 shows the results of ensembling in best-of- $n$ reranking, as measured by an XXL-scale fine-tuned reward model. Pretrain ensembles consistently improve performance over individual reward models, especially for higher values of $n$ for both TL;DR and HELPFULNESS. Finetune ensembles, conversely, improve performance in some cases and are comparable in others. For example, on TL;DR a pretrain ensemble with the MEAN aggregator achieves a win rate of $90 \%$ over the SFT outputs at the XL scale, while the win rate of a finetune ensemble with the same MEAN aggregator is $87.3 \%$. The win rate of the average individual XL-scale reward model is $85.3 \%$ (see Table 7). For visual clarity, in Figure 5 we show only two[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-09.jpg?height=1044&width=1400&top_left_y=275&top_left_x=362)

(a) TL;DR
![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-09.jpg?height=450&width=1376&top_left_y=797&top_left_x=367)

(b) HELPFULNESS

Figure 6: In RLHF, pretrain ensemble reward models lead to significantly more favorable reward-KL tradeoffs, as judged by a T5-XXL autoeval model. Each point corresponds to training of models to convergence with a particular value of $\lambda$. We show the MEDIAN aggregator here, full numerical results are in Appendix B.

aggregators: MEAN and MEAN_MINUS_STD; see Appendix A for results with other aggregators. In general, the differences between aggregators are small, with MEAN usually performing at, or near, the top. More conservative aggregators (MIN and MEAN_MINUS_STD) come out slightly ahead of MEAN at the smaller scales on TL;DR, suggesting that high variance may be a bigger issue in this setting.

Figure 6 shows the KL-reward trade-off of ensemble reward models in RLHF for TL;DR and HELPFULNESS (evaluated with the finetuned T5-XXL model). In such plots, a better model is one that improves reward and/or reduces the value of KL from the original SFT policy (Gao et al., 2023; Coste et al., 2023). Indeed, similar to BoN, pretrain ensembles consistently outperform both finetune ensembles as well as the average individual model. We present results for the MEDIAN and MEAN aggregators for visual clarity, and report full numerical results in Appendix B. In RLHF, KL values are much higher than BoN (which is bounded by $\approx 3.17$ for $n=64$ ). Consequently, in this setting we witness explicit reward hacking, in which the T5-XXL rewards decrease even as the RLHF objective improves. This happens most prominently for individual models, in many cases for finetune ensembles, and most rarely for pretrain ensembles-where T5-XXL reward scores decrease only when RLHF uses a T5-Base reward model. Thus, our experiments on real data yield more negative conclusions than Coste et al. (2023) about the potential of ensembles to eliminate reward overoptimization.

Because the T5-XXL autoeval model is trained on the same data distribution as the reward models used for best-of- $n$ and RLHF, it may overstate their performance. For this reason, we also use a zero-shot autoeval model (PALM-2-Large), as described in Section 2.3. Because this evaluation is more computationally expensive, we apply it only to the largest-scale reward models (XL). Results are shown in Figure 7. Ensemble reward models consistently achieve higher win rates on both tasks and with both alignment techniques. For best-of- $n$, pretrain ensembles get significantly higher win rates on TL;DR at $n=64$ ( $p<.001$ by a permutation test); on HELPFULNESS the differences

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-10.jpg?height=919&width=1401&top_left_y=316&top_left_x=362)

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-10.jpg?height=406&width=618&top_left_y=331&top_left_x=382)

(a) BoN + TL;DR

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-10.jpg?height=361&width=697&top_left_y=798&top_left_x=367)

(c) RLHF + TL;DR

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-10.jpg?height=423&width=637&top_left_y=325&top_left_x=1058)

(b) BoN + HELPFULNESS

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-10.jpg?height=358&width=697&top_left_y=802&top_left_x=1061)

(d) RLHF + HELPFULNESS

Figure 7: Using a prompted autoevaluator (PALM-2-FLAN), ensemble reward models offer significantly better win rates on both TL;DR and HELPFULNESS. Here all reward models are XL-scale.

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-10.jpg?height=380&width=708&top_left_y=1393&top_left_x=706)

Figure 8: XSUM/NLI KL-reward tradeoff for pretrain ensembles, finetune ensembles, and individual models. Reward is measured with T5-XXL. Both pretrain and finetune ensembles slightly improve over individual models.

between ensembling techniques are not significant at $n=32$. On both tasks, single reward models are significantly worse, $p<.001$. For RLHF, pretrain ensembles generally achieve better or equal win rates at lower KL divergence from the reference policy, with particularly strong performance on HELPFULNESS. Overall, these results mirror the T5-XXL evaluation, with one interesting difference: the PALM-2 autoeval model reveals more reward hacking for RLHF, where win rate decreases with KL. This suggests that fine-tuned autoevaluators can overestimate performance when they are trained on the same preference data as the alignment reward models.

Figure 8 shows RLHF results for XSUM/NLI. Here we see a relatively small improvement for ensembles compared to individual models, and a very small difference between pretrain and finetune ensembles. We conjecture this is because XSUM/NLI optimizes for a particular aspect of the response, namely its factuality. This allows all models to find simple and similar strategies that lead to high reward (for example, emitting short responses with limited content), and thus ensembling does not lead to large gains in performance. We further elaborate on this when discussing limitations of ensembles in $\S 5$.

## 5 WhEN Do ReWard Model EnSEMBLes FaIL?

We saw that ensembles improve performance according to automatic evaluation metrics. We now conduct a complementary analysis that illustrates that, for some types of errors, ensembling is ineffective. When all reward models share a similar error pattern, this error propagates to the ensemble. Systematic errors across ensemble members can arise due to biases in the finite reward model training data.

To demonstrate this, we manually analyze ensemble outputs to detect frequent errors, and then perform a qualitative analysis. Figure 9 shows the results of this analysis on all three benchmarks. The x-axis corresponds to outputs of the model after training for a certain number of steps, and the y-axis is a statistic of interest (e.g., average output length). We plot the statistic value for the pretrained ensemble (using MEAN as a representative aggregation function) and for its members. In addition, for TL;DR and HELPFULNESS, where the reward model is trained on the preference data, we show the statistic value on the preference data validation set, conditioned on the label 'Preferred' or 'Rejected'.

- For HELPFULNESS (Figure 9a), outputs tend to be in a format of a list, and thus we write a regular expression that captures this format. The fraction of outputs that have this pattern increases to roughly $50 \%$ for 3 members of the ensemble and to the ensemble itself. Looking at the preference data, we do not detect a tendency to produce list outputs in the preferred responses, as the fraction of outputs that matches this format is roughly $8 \%$ for both the preferred and rejected responses.
- For TL;DR (Figure 9b), RLHF alignment leads to longer summaries (Singhal et al., 2023) and also outputs that are more extractive, i.e., copy more from the input. Summary length in characters grows substantially for the ensemble and all its members, where for the ensemble, length increases by a factor of two. On the preference data, indeed preferred responses are slightly longer than rejected responses, but much shorter than outputs post-RLHF. We also compute the longest common subsequence (in characters) between the document and the summary and find that it increases for the ensemble from 28.2 to 49.1. Again, the tendency for copying from the document already occurs in the preference data to a small degree, but is amplified by RLHF. ${ }^{2}$
- For XSUM/NLI (Figure 9c), training for factuality tends to make summaries shorter. Additionally, precise numbers are typically omitted from the summaries. Figure 9 shows how all members of the ensemble and the ensemble itself exhibit this phenomenon, with length in characters decreasing rapidly, as well as the fraction of examples that contain any numeric value whatsoever.

Overall, these qualitative findings are symptoms of the tendency for different pretrain reward models to learn to associate certain features with high reward. Policy models can then exploit this association, and use these features to produce outputs that are dramatically different from the reward training data, and that achieve (spuriously) high reward for both single reward models and the ensemble.

Why does this happen for both single reward models and reward model ensembles? As one indication, Lakshminarayanan et al. (2017) have proposed distance-awareness, i.e., the ability to quantify the distance of an example from the training set, as a necessary condition for achieving good uncertainty estimates. They showed in a synthetic binary classfication setup that deep ensembles provide good estimates when examples are on the decision boundary, but underestimate uncertainty in areas that are far from the training distribution. In LM alignment, the policy can shift the output distribution away from the decision boundary to areas where all reward models erroneously extrapolate in the same manner. While we focus on ensembles in this work, we hypothesize that the same phenomenon will occur in other approaches for uncertainty estimation that are not distance-aware, such as Monte-Carlo Dropout (Gal \& Ghahramani, 2016) and Epistemic Neural Networks (Osband et al., 2021).

## 6 CONCLUSION

In this work, we investigate reward model ensembles as a method for mitigating reward hacking. We find that diversity of the reward ensemble is crucial, and that a pretrain ensemble that contains members that do not share a pretrain seed leads to stronger generalization during alignment when compared to an ensemble whose members share a pretrain seed. However, reward ensembles are not always effective-for example, we find that they can still assign reward based on spurious correlations[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-12.jpg?height=556&width=653&top_left_y=291&top_left_x=736)

(a) HELPFULNESS. Fraction of answers containing lists (as matched by a regular expression).
![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-12.jpg?height=570&width=1350&top_left_y=904&top_left_x=363)

(b) TL;DR. Left: extractiveness, as measured by average longest common substring between the summary and the context document. Right: length.
![](https://cdn.mathpix.com/cropped/2024_06_04_498da604b11aa2464a64g-12.jpg?height=564&width=1348&top_left_y=1569&top_left_x=366)

(c) XSUM/NLI. Left: length. Right: specificity, as measured by fraction of numerical tokens in the output.

Figure 9: Limitations of reward model ensembles. The x-axis is number of RLHF steps, the $y$-axis plots different statistics of the average validation output at that step, and the curves correspond to the pretrain ensemble (solid blue) and its members (dashed orange). For preference data, we plot the same statistics conditioned on the preference data label (Preferred vs. Rejected). On HELPFULNESS $(\lambda=0.05$, top), the ensemble tends to return a list of items. On TL;DR (center, $\lambda=0.01$ ), summaries become longer and copy longer spans from the original document. For XSUM/NLI $(\lambda=0.03$, bottom), responses are short and less specific, as measured by lack of numerical information. In HELPFULNESS and TL;DR, the statistics of the "aligned" outputs are far from their values in the preference data.
between the input and the label. If all members of the ensemble capture the same correlations, the ensemble will inherit the same undesirable behaviour. In such cases, the policy can exploit this vulnerability and shift the distribution towards outputs that overuse this correlation, which results in reward hacking. Consequently, reward model ensembles mitigate, but do not fully eliminate, reward hacking. Future work should examine methods for uncertainty estimation that are more robust to the type of distribution shift that occurs during alignment, particularly those that are aware of how different model policy outputs are from the preference data-such as Gaussian processes (Kuss \& Rasmussen, 2003; Chu \& Ghahramani, 2005; Liu et al., 2020) and conformal prediction under covariate shift (Tibshirani et al., 2019).

Acknowledgments Thanks to Sharat Chikkerur, Mohammad Havaei, and the anonymous reviewers for feedback on this paper. The research also benefited from feedback from David Bruns-Smith, Ming-Wei Chang, Michael Collins, Patrick Fernandez, Mandar Joshi, Rishabh Joshi, Balaji Lakshminarayanan, Kenton Lee, Kristina Toutanova, Victor Veitch, and Zihao Wang. Finally, we thank the people who built the infrastructure used in our experiments, including the T5X team and Léonard Hussenot, Johan Ferret, Robert Dadashi, Geoffrey Cideron, Alexis Jacq, Sabela Ramos, Piotr Stanczyk, Sertan Girgin, Danila Sinopalnikov, Amélie Héliou, Bobak Shahriari, Bilal Piot, Matt Hoffmann, Nikola Momchev, and Olivier Bachem.

## REFERENCES

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952. ISSN 00063444. URL http://www.jstor.org/stable/2334029.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Wei Chu and Zoubin Ghahramani. Preference learning with gaussian processes. In Proceedings of the 22nd international conference on Machine learning, pp. 137-144, 2005.

Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023.

Alexander D' Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Underspecification presents challenges for credibility in modern machine learning. The Journal of Machine Learning Research, 23(1):10237-10297, 2022.

Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multiple classifier systems, pp. 1-15. Springer, 2000.

Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaFarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059. PMLR, 2016.

Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 10835-10866. PMLR, 2023.

Adam Gleave and Geoffrey Irving. Uncertainty estimation for language reward models. arXiv preprint arXiv: $2203.07472,2022$.

Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (ReST) for language modeling. arXiv preprint arXiv:2308.08998, 2023.

Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Specification gaming: the flip side of ai ingenuity. DeepMind Blog, 3, 2020.

Malte Kuss and Carl Rasmussen. Gaussian processes in reinforcement learning. Advances in neural information processing systems, 16, 2003.

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, $30,2017$.

Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. Advances in Neural Information Processing Systems, 33:7498-7512, 2020.

Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018.

Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.

Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Morteza Ibrahimi, Xiuyuan Lu, and Benjamin Van Roy. Epistemic neural networks. arXiv preprint arXiv:2107.08924, 2021.

Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. In International Conference on Learning Representations (ICLR), 2022.

Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur Parikh, and He He. Reward gaming in conditional text generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vlume 1: Long Papers), 2023.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.

Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Leonard Hussenot, Orgad Keller, Nikola Momchev, Sabela Ramos Garea, Piotr Stanczyk, Nino Vieillard, Olivier Bachem, Gal Elidan, Avinatan Hassidim, Olivier Pietquin, and Idan Szpektor. Factually consistent summarization via reinforcement learning with textual entailment feedback. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023.

Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460-9471, 2022.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.

Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal prediction under covariate shift. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dÁlché Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, 2019.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.

Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pp. $59-63,2017$.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https: / openreview. net/forum? id=gEZrGCozdqR.

Kevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021.

Sheheryar Zaidi, Arber Zela, Thomas Elsken, Chris C Holmes, Frank Hutter, and Yee Teh. Neural ensemble search for uncertainty estimation and dataset shift. Advances in Neural Information Processing Systems, 34:7898-7911, 2021.

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. SLiC-HF: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.
