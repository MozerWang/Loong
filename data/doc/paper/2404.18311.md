# Towards Incremental Learning in Large Language Models: A Critical Review 

Mlađan Jovanović, Singidunum University, Belgrade Serbia, mjovanovic@singidunum.ac.rs<br>Peter Voss, AIGO.ai, Austin TX, USA, peter@aigo.ai


#### Abstract

Incremental learning is the ability of systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks. It is a critical ability for intelligent, real-world systems, especially when data changes frequently or is limited. This review provides a comprehensive analysis of incremental learning in Large Language Models. It synthesizes the state-of-the-art incremental learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning. We demonstrate their utility for incremental learning by describing specific achievements from these related topics and their critical factors. An important finding is that many of these approaches do not update the core model, and none of them update incrementally in real-time. The paper highlights current problems and challenges for future research in the field. By consolidating the latest relevant research developments, this review offers a comprehensive understanding of incremental learning and its implications for designing and developing LLM-based learning systems.


Keywords: Incremental learning, Large language models, Continual learning, Meta-learning, Parameter-efficient learning, Mixture-of-experts learning.

## 1. Introduction

### 1.1. Contribution and Structure

Incremental learning (IL) is the ability of a learning system to make decisions based on continuously incoming inputs rather than relying only on previously processed data. This allows the system to adapt and respond to changing conditions and user behavior immediately rather than waiting for periodic retraining.

Given the paradigm's complexity, the existing LLM solutions aim to enable IL but still haven't achieved it in practice. Therefore, this review takes a broader perspective on incremental LLM updates, considering them batch-incremental periodic model updates. It also differentiates between core model updates and external system updates.

Thus, the critical review primarily discusses IL approaches, methods, and techniques in Large Language Models (LLMs) that acquire new abilities to solve unseen tasks while preserving existing ones.

Distinct from typical systematized reviews, the paper highlights the following key factors:

1) Analysis of topics related to IL in LLMs. The paper covers fundamental aspects of continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning, including learning from scarce, changing, or non-existent task information, learning multimodal tasks, and learning task sequences. We provide a deeper understanding and highlight the challenges and opportunities in these areas. Moreover, exploring the relationships and synergies between IL and related topics reveals valuable insights into how IL can be effectively integrated into existing LLM solutions.
2) Comprehensible and compact exposition. This paper recognizes the importance and complexity of IL and describes the concepts, methods, and techniques clearly and concisely. It aims to reach a broad audience, including engineers and researchers. Through brief descriptions, explanations, and references to original works, the paper helps readers grasp fundamental IL principles and their practical implications.
3) Synthesis of essential information. IL is a fast-growing field, with information scattered across various fields and sources. The paper consolidates the relevant information about IL, offering a comprehensive overview to interested readers. Synthesis of the latest practical developments aims to guide researchers and engineers looking for a deeper understanding of IL and its potential applications in LLM-based architectures and frameworks.

By highlighting these contributions, the paper complements existing related reviews and offers a unique perspective into the current situation and outlooks for IL in LLMs. The paper consulted relevant sources, including Scopus, ACM Digital Library, and IEEE Xplore Digital Library. The inclusion criteria for each paper analyzed is the practical verification of the proposed approach describing the solution, learning process, training data, and experiment results.

This paper provides critical insights into existing IL approaches within the LLM design space. To do so, we first define the key LLM concepts and notations used throughout the paper in the current section. Section 2 delves into the current state of IL approaches from a unified perspective by categorizing them into three related topics: continual learning methods, meta-learning methods, parameter-efficient tuning methods, and a mixture of expert methods. The paper describes the representative examples from different approaches. Section 3 extracts the main findings with implications concerning the application of IL to real-world problems with an overview of the challenges ahead. The concluding section summarizes these challenges and research opportunities.

### 1.2. Context and Motivation

LLMs are based on a transformer neural-network architecture [1] and produce text in natural language. Transformers introduced the attention mechanism that tracks the relations between words across lengthy text sequences in forward and reverse directions. It pays attention to (co-) appearances of words as chunks of information in terms of probabilities, not the meaning of those words. Thus, it cannot understand since there is no reference to the "real world" or "conceptual world" to ground as explicit knowledge. Instead, they approximate complex probability distributions on a large number of variables (the context of a word in text) and predict next words by drawing from these probability distributions. That said, it represents domain-specific knowledge as a language structure relying on context-dependent long-range associations (aka parametric knowledge).

Over time, the architecture expanded for generating other media as embedding vectors have become numerical representations of text, audio, image and video (i.e., encoders for different data modalities) [2,3].

We have witnessed the rapid advancements in LLMs that produce plausible human-like content in various domains. Models with these generative capabilities are called Foundation Models (FMs). They learn about existing data artifacts and use learned patterns to generate new data instances, including text, images, music, design, and motion. Massive models process vast amounts of unlabeled data, prioritizing general-purpose applications. These models are then customized to create tools and infrastructure for specific domains. This way, FMs can be adapted across a variety of scenarios.

Large Language Models (LLMs) paved the way for many other FMs by generating meaningful text at scale, including machine translation, automated dialogs, writing assistance, code generation, and summarization. Their content can be easily converted to images, audio, video, design, and motion. Many are available through conversational frontends, including Open Al's ChatGPT, Google's Bard, and Anthropic's Claude.

Al aggregators, such as HuggingFace, maintain LLM leaderboards with benchmarks for evaluating and ranking models using various performance metrics ${ }^{1}$.

Table 1 lists fundamental LLM architectures. The encoder transforms input text into a fixed-size set of features as words' positional encodings (embedding vectors) for NLU tasks, including named entity recognition and sentiment classification. The decoder accepts features to produce output[^0]text, such as dialog, question answering, and text summarization. Combined architecture encodes the input text into vectorial representation and decodes embeddings to generate output, such as machine translation.

Table 1. Fundamental LLM architectures.

| Type | Application | Learning | Models |
| :--- | :--- | :--- | :--- |
| Transformer encoder <br> (text-features) | Natural language <br> understanding <br> (NLU) | Masked language <br> modeling | BERT, DeBERTa, <br> ALBERT |
| Transformer decoder <br> (features-text) | Natural language <br> generation (NLG) | Language modeling | GPT4, ChatGPT, Gemini, <br> LLaMA, Falcon, Bard |
| Transformer encoder- <br> decoder (text-features- <br> text) | Sequence-to- <br> sequence tasks | Sequence-to- <br> sequence modeling | GLM, T5, BART |

To better understand LLMs, we illustrate its Transformer-decoder architecture (Figure 1). It consists of the embedding component, a decoder stack, and a head component with linear and softmax layers.

The embedding component transforms NL text into numerical vectors (or tokens) chunks. The tokens are forwarded to the decoders for subsequent processing.

The decoder comprises Multi-head Self-Attention (MSA) and Feedforward Network (FFN) modules. The MSA clusters each token by an attention map obtained from two linear mappings of the input tokens (as their dot product). An FFN then processes the tokens.

LLMs stand out from other Deep Neural Network (DNN) architectures in two ways.

First, they exhibit the autoregressive pattern, performing the generation task in iterations.

Next, they incorporate the attention mechanism, which quadratically scales computational complexity with the input length. At the same time, LLM's inherent computations occur in the attention blocks within each decoder layer.

LLMs iteratively generate tokens (words) based on the previously generated sequence. The process repeats until the model processes and outputs the entire input sequence.

![](https://cdn.mathpix.com/cropped/2024_06_04_997dafbdbb7da41a6ac3g-03.jpg?height=819&width=1696&top_left_y=1718&top_left_x=203)

Figure 1. The transformer-decoder architecture (left), and auto-regressive behavior (right). Adapted from $[1]$.

Typical LLM learning strategies are as follows $[4,3]$ :

1) Pretraining-finetuning includes unsupervised training with a massive amount of data for general NLP tasks (also known as upstream tasks), followed by supervised training on a smaller amount of domain-specific data for specific tasks (aka downstream tasks).
2) Instruction tuning involves obtaining a new, instruction-financed model capable of following instructions. The model learns new tasks via natural language instructions. It involves actual training of the model by changing its weights. Soft prompt tuning is a case in which the model is frozen, and only the soft prompts (learnable tensors obtained through backpropagation) added to the input embedding are learned [5]. Unlike finetuning, which involves billions of parameters to train, in soft prompting, there are typically several thousand parameters to train. Similarly, prefix-tuning optimizes a small continuous task-specific vector (aka prefix) where subsequent tokens attend to the prefix [6]. It obtains state-of-the-art performance compared to full-data training and outperforms finetuning in low-data training by learning only $0.1 \%$ of the parameters [6].
3) Prompting refers to figuring out a prompt (or user input) that will likely yield the best LLM response in a given situation - in other words, conceiving the input to get an appropriate response pattern. No training is involved (i.e., the LLM's weights are not changed due to no gradient updates). A prompt is a flexible task specification containing $\mathrm{NL}$ descriptions of intentions with examples and expected responses [7]. Various templates have been proposed and resourced ${ }^{23}$ as prompting techniques, including zero-shot (instructions with task-specific formatting but without task-specific examples), few-shot (instructions with a few task-specific demonstrations to guide the model's outputs), chain-of-thought (stepwise instructions with logical context to simulate reasoning), and tree-of-thought (guidance to select among multiple solutions) [7, 8, 9, 10]. In-context learning (ICL) is sometimes used to refer to few-shot prompting as the model learns to address a new task during inference by receiving a prompt containing task demonstrations [7].

Reinforcement Learning from Human Feedback (RLHF) represents an alternative in which the model is refined based on human feedback or corrections [11]. Initially, users express their preferences among different responses provided by the model to the same prompt. Subsequently, a reward mechanism is established. The model enters a reinforcement learning loop aiming to maximize the rewards by continuously adapting and refining its responses to better match the user's preferences. Ranking the model's responses from human evaluators offers a superior objective function compared to those utilized in pre-training. This method aligns the model's outputs with human values, preferences, and ethical guidelines (Al alignment). A recent study [12] highlighted practical challenges in using RLHF to customize LLM as misaligned humans (evaluators may hold different goals and values), quality control with difficult oversight (humans making mistakes), issues in obtaining representative data (biased data), and challenges with reward model (misspecification, misgeneralization, reward hacking, and evaluation). This often requires retraining these models, which is impractical regarding computation, time, and privacy.

Retrieval-augmented generation (RAG) [13] is a framework for increasing the quality of LLMs' outputs by grounding the models on external knowledge retrieval components to supplement the LLM's internal representation of information. Implementing RAG verifies LLM-based outputs against reliable facts and sources, ensuring users can check its claims for accuracy and trust. RAG helps reduce hallucinations, outdated knowledge, and non-transparent, untraceable content generation process $[14,15]$.

The resources required by LLMs (compute, data, environment impact) are enormous as they are based on statistical pattern matching on large quantities of human intelligence outputs. At the same time, LLMs are still out of proportion to achieving human-like intelligence, such as planning, reasoning, and self-verification $[16,17,18]$.

IL of knowledge emerging in the real world is essential to achieve human-like intelligence [19]. However, standard neural network (NN) topologies are hindered by catastrophic forgetting (CF), a[^1]limitation that prevents them from learning a sequence of tasks. This issue must be addressed for an NN to successfully adapt to lifelong or continuous learning, a fundamental requirement for Al. Two common goals of continuous learning are to learn new tasks from known classes (online learning) and learn unknown classes (class-incremental learning) [20].

Rehearsing techniques can avoid CF [21]. This implies that when new data is introduced, the NN gets retrained on some previously learned data. However, previously learned knowledge may not be available for such retraining in general.

In addition, CF may occur when compensating for concept drift [21]. Deep Learning (DL) assumes that a training set is a representative sample of the underlying unknown distribution. If the distribution space shifts, new modeling should expand this shift. If the initial space is merely shifted instead of adjusting the size of the distribution space, then $\mathrm{CF}$ will occur.

The current situation inspired us to look deeper into existing work on topics related to IL in LLMs, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning, and the challenges posed by the extensive parameterization of LLMs and continual model adjustments to address deficiencies or undesirable behaviors.

## 2. Related work

### 2.1. Continual Learning

Continual learning (CL) learns new, emerging tasks efficiently while mitigating CF in LLMs. The existing work on CL can be classified as follows [22, 23, 24, 25, 26]:

1) Consolidation-based methods, such as regularization [27] or distillation [28], protect important parameters from significant shifts. They do this by aligning the current output space with previous ones (distillation loss) or restricting the model parameters by estimating loss or averaging weights (regularization loss). Weight regularization concerns parameters, whereas function regularization influences behaviors of the existing model.

Weight regularization selectively regularizes variation of NN's parameters. It typically penalizes each parameter's variation based on its importance in performing the previous tasks [29].

Function regularization is a robust approach targeting the model's in-between results or the prediction. It typically implements Knowledge Distillation (KD) [30] by learning a small student model from a large teacher model previously learned to mitigate CF. The student model can learn from new training examples, replay previous examples, and use available unlabeled data or data generated from previously learned tasks.

Aside from regularization, some methods focus on error optimization, such as calculating gradient propagation on the old tasks' input data [31] (e.g., rectifying the current gradient directions through parameter updates), meta-learning of arriving tasks [32] (e.g., obtaining an inductive bias for new tasks internally, rather than specifying it explicitly), and investigating loss spaces [33] (e.g., discovering a local minimum by adjusting the training variables, including learning rate decay, dropout rate, and batch size).

Self-supervised learning (generating implicit labels from unstructured data) and upstream pretraining with downstream finetuning (pre-trained representations fixed when learning downstream tasks) also belong to this class as they create and employ internal representations in LLMs. However, they require large amounts of data.

While these methods may only partially utilize historical data, their ability to safeguard crucial parameters is a significant advantage.

2) Dynamic-architecture-based methods [34] train a model by increasing its size (number of parameters) with the task number (also known as parameter expansion). These methods introduce task-specific parameters through a model adaptation by adding task-specific parameters (parameter allocation), creating task-specific modules (modular network), and differentiating shared and task-specific model components (model decomposition).

Parameter allocation dedicates a parameter set in the model to each task. The expansion of such models should be balanced with the increase in tasks [35].

Model decomposition separates a model into shared and task-specific elements. For instance, the parameters can be decomposed by additive decomposition, singular value decomposition, and low-rank factorization [36].

A modular network includes modules or sub-networks learning incremental tasks simultaneously without sharing task-specific components. Mixture-of-Experts (MOEs) architectures are examples of this method. We describe them in Section 2.4.

Visual Prompt Tuning (VPT) [37] combines previous techniques by utilizing lightweight trainable modules to adjust the pre-trained model by prepending learnable parameters (aka prompts) to the base features. The model concatenates parameters and features for the vision transformer blocks. Encoding task-specific information into the prompts while keeping pretrained weights unchanged minimizes the cross-entropy loss.

The previous methods have a major drawback - the linear growth of training costs [25, 26] as the model size grows with new tasks linearly. This cost increase can be a deterrent despite their ability to overcome CF.

3) Memory-based methods [38] keep additional memory of examples of previous tasks, which are used (replayed) when learning new tasks. While they retain some information from previous tasks, they face challenges such as overfitting and low scalability due to frequent retraining of the model (aka replay-based or rehearsal-based).

These methods typically approximate and reconstruct old data distributions. Experience replay records a small amount of old training examples [39]. Generative replay employs a generative model to produce synthetic examples for training [40]. Feature replay restores the distribution of old features [41] through saving prototypes (i.e., feature distillation between the old and new model or representation shift to update the preserved old features), statistical information (e.g., mean and covariance), or training a generative model.

However, selecting exemplars from pretraining datasets may not be possible or can be difficult, and downstream data may not always be suitable for preserving the pretraining knowledge.

![](https://cdn.mathpix.com/cropped/2024_06_04_997dafbdbb7da41a6ac3g-06.jpg?height=786&width=1137&top_left_y=1229&top_left_x=448)

Figure 2. Approaches to Continual Learning.

The research from [42] analyzes $C L$ as a multi-stage training in which training objectives are delegated to specific models. Based on the target information updated and distinct LLM-based learning strategies, CL can be classified as:

1) Continual Pre-training (CPT) expands the model's general NL understanding and generation capabilities [43]. It conducts self-supervised training to enrich the model's general NLP knowledge and general knowledge of new domains, such as learning new natural or programming languages, factual knowledge (e.g., most recent information), and domain knowledge (e.g., e-commerce).
2) Continual Instruction Tuning (CIT) improves the model's response to specific user inputs [44]. It conducts supervised instruction-following finetuning to learn domain-specific tasks. For instance, learning to solve new tasks (e.g., code generation), tasks in novel domains (e.g., medicine), or to use new tools to solve problems (e.g., search engines).
3) Continual Alignment (CA) ensures the model's outputs adhere to human values, preferences, and ethical and societal norms [45]. For example, learning to align to ethical and social norms (e.g., culture-specific norms) or users' preferences (e.g., emerging values on new users).

The challenges of the previous strategies include training costs concerning computational and human resources (e.g., RLHF for CA), preserving privacy and safety while processing usergenerated data, and combining different strategies. For example, when the instruction-tuned or aligned model follows continual pre-training, CF may occur, resulting in reduced performance on instruction-following or human-aligned tasks.

The research described in [46] the CF in continual pre-training of the finetuned LLaMA2 model with 7 billion parameters, followed by supervised finetuning and RLHF by comparing outputs for different alignment techniques. During alignment, the study employed various techniques to prevent CF, including freezing specific model layers, freezing specific model modules (e.g., selfattention and FF modules), and model adapters introducing additional trainable parameters (e.g., LoRA [47].) The performance analysis concerned output format (language identification and repetition analysis tasks), knowledge (4 accuracy benchmarks), and reliability (truthfulness, toxicity, and bias benchmarks). The results showed the repetition problem (the greedy approach of choosing the next token, where the highest probability token is always selected, causes the model to generate a repetitive token sequence) and a decline in reliability. In contrast, the model's knowledge remained unaffected mainly. However, the model was evaluated on only two distinct tasks.

The existing definition of IL refers to continual learning techniques in NN-based models [23] as task-incremental, domain-incremental, and class-incremental learning, depending on whether task identity is specified at test time or is inferred by the model. Task-incremental learning learns to solve several distinct tasks. Domain-incremental learning learns to solve the same problem in different contexts (input distribution changes). Thus, the context refers to the underlying distribution from which observations are sampled. Class-incremental learning learns to discriminate between incrementally observed classes. The distinction between these scenarios lies in whether context identity must be inferred and whether different contexts contain the same or different classes. The main challenges in domain-incremental and class-incremental learning are preventing forgetting and correctly identifying tasks, respectively [48].

IL in LLMs can also be classified as offline IL and online in-context learning [49], considering training data sources.

Offline IL, while allowing for specific adaptation with new annotated data sets (e.g., supervised fine-tuning and RLHF [11]), has its limitations. It requires frequent model updates for evolving tasks and scenarios, and static post-updates are inflexible to adapt to real-time changes.

Online in-context learning assumes an LLM interacts with an external Al model or agent or with an offline or online knowledge source (e.g., RAG [13]). Such methods depend on external knowledge and do not retain persistent learned information, which may lead to a loss of used knowledge.

Diffusion-based Class Incremental Learning (DiffClass) [50] uses Diffusion Models (DMs) [51], generative models that produce synthetic samples, to improve the classifier performance in CIL tasks.

DMs generate images in a stochastic process by progressively denoising them [51]. They incrementally introduce Gaussian noise into the input data (a forward diffusion step). The procedure is gradually reversed to generate synthetic samples, which are used to update the classifier by removing the noise from noised input (a reverse diffusion step). Synthetic samples are similar to the real ones.

The DiffClass produces artificial examples using Multi-Distribution Matching (MDM) models to balance the quality of the generated samples and their diversity. The models are finetuned using LoRA [47]. MDM models ensure that the classifier is exposed to a diverse range of examples and maintains its performance in previous classes while learning new ones. Before generating samples, Selective Synthetic Image Augmentation (SSIA) enlarges the training data distribution and enhances the model's plasticity.

The method demonstrates state-of-the-art performance in example-free settings in CIFAR100 ${ }^{4}$ ( 60000 images with 100 classes) and ImageNet $100^{5}$ ( 135,000 images with 100 classes) benchmarks. However, increasing the number of new classes to learn prolongs finetuning time.

Some approaches integrate PET with CL. For instance, the study in [52] continuously introduces and trains soft prompts for each new task to tackle CF. However, the prolonged accumulation of new prompts raises scalability issues by inducing higher training costs.

Excessive LLM modifications can cause forgetting, while insufficient adjustments make inadequate fits for new classes. ExpAndable Subspace Ensemble (EASE) is an approach to PTM-based CIL addressing the above issue [48]. The expanded model initializes and trains an adapter with trainable parameters encoding task-specific information per incoming task. The model extracts embeddings by aggregating historical features to synthesize prototypes of old classes later. In addition, all adapters extract the prototypes of the current dataset for new classes. Accordingly, separate synthesis of prototypes of former and new classes prevents CF.

Verification on seven benchmark datasets using ViT-B/16-IN21K [53] (a vision transformer encoder model with 86.4 million parameters pre-trained on ImageNet-21k containing 14 million images with 21,843 classes) as a backbone PTM demonstrated state-of-the-art performance.

However, although adapters are small ( $0.3 \%$ of the base model), model size increases with additional adapters.The model, datasets, and experiment details are available from: https://github.com/sun-hailong/CVPR24-Ease.

Prototype and Low-Rank Adaptation (PLoRA) [54] starts from a pre-trained backbone and finetunes it using LoRA with a small number of parameters to learn new classes in Federated Class-Incremental Learning (FCIL). FCIL is a method to continuously learn new classes while addressing the challenge of forgetting old classes [55]. It is based on a distributed ML paradigm called Federated Learning (FL), where multiple data owners (or local models) collaborate to train a shared or server model. Existing methods [55, 56] propose different loss functions on the client and server sides to alleviate local and global CF but result in significant memory and communication costs.

PLoRA introduces a shared prototype module for the global server, aggregating (re-weighting) the local prototypes from clients. The local prototype vectors are derived from prototypes of corresponding class features. Then LoRA [47] finetunes only trainable parameters and sends them to the global server. In particular, each client uses local data to train LoRA parameters to obtain prototypes of each class. Only the trainable parameters and the average class feature are shared with the server. The classification bias is mitigated by consulting the variations between the class average feature and corresponding prototypes. The server re-weights the local prototypes it received to form the global prototype. PLoRA utilizes ViT-B/16-IN21K [53] as the backbone PTM for server and client models.

While PLoRA outperforms state-of-the-art approaches, it has high training costs for data, memory, and computational resources for server and client models.

Few-Shot Class-Incremental Learning (FSCIL) [57] trains a base model with sufficient data. Later, it utilizes the knowledge of base classes to learn of new classes with limited data while preserving the understanding of old ones. FSCIL usually fine-tunes the entire model, being susceptible to overfitting and with reduced performance in learning new classes [57].

The Attention-aware Self-adaptive Prompt (ASP) framework [58] adds specific prompts between attention blocks as attention-aware task-invariant prompts (TIP) and self-adaptive task-specific prompts (TSP). The TIP contains task information common for all classes (old and new). The prompts are constructed by encoding input images strongly correlated with semantically relevant information and weakly correlated with irrelevant image information. It aggregates prompt features for the training data to prevent overfitting by averaging them. The TSP is constructed for a particular input image, combining the average and image prompt features. This way, it fine-tunes only the task-specific parameters and needs sufficient data to train new TSPs for new classes.[^2]

ASP uses ViT-B/16-1K [53] as the base model, pre-trained on the ImageNet1K dataset, which spans 1000 classes and $1,281,167$ training images, 50,000 validation images, and 100,000 test images.

Validation shows ASP outperforming state-of-the-art FSCIL methods in learning new classes and mitigating forgetting. However, training costs are high, and learning new classes can drop the performance of previous ones.

MiniGPT-4 [59] connects a pre-trained visual encoder and a pre-trained LLM. In particular, it combines a vision encoder (a pretrained ViT coupled with Q-Former [61]), Vicuna LLM [60], and a projection layer connecting the two models. The linear projection layer aligns the visual features with the textual ones. It does so in a two-step training method. First, it pretrains the vision encoder on many image-text pairs to learn vision-language skills. Then, it finetunes the model using a small number of high-quality image-text examples and a predefined prompt template to elicit more natural and reliable NL outputs.

MiniGPT-4 showcases compositional vision-language abilities in a variety of tasks. These include generating detailed image descriptions, extracting facts from movie photos, identifying humorous aspects within memes, providing food recipes from photos, writing poems for images, designing websites from hand-written drafts, and image captioning. However, the model inherits the backbone LLM's limitations and may hallucinate (e.g., identifying the presence of non-existent objects in the image), especially with longer captions.

The model and the data can be found at: https://minigpt-4.github.io/.

The Generative Multi-modal Model (GMM) framework for CIL [62] generates detailed image descriptions by incorporating visual and textual information with a customized generative model. The model contains an encoder producing image and text embeddings image and text, which are further processed by an auto-aggressive decoder to generate image descriptions. Then, it uses a text encoder to produce textual features from the image descriptions and matches features to decide on a label that is highly similar to the prediction.

The framework employs Bootstrapping Language-Image Pre-training with frozen unimodal models (BLIP) [63], a vision-language model pre-trained from existing pre-trained image encoders and LLMs with significantly fewer trainable parameters than existing methods. The model bridges modalities using a lightweight Querying Transformer learning vision-language representation with the image encoder and vision-to-language generation with the language model.

GMM is evaluated on three Few-shot CIL benchmark datasets with improved classification accuracy regarding the state-of-the-art while significantly less forgetting. However, the main challenge is transferring the generated textual information into classifying distinct categories.

The framework details are available at: https://github.com/DoubleClass/GMM.

The INCremental Prompting (INCPrompt) [64] utilizes flexible task-aware prompts to effectively represent task-specific information and mitigate the CF of old tasks. It implements an attentionbased transformation of the token sequence whose results are passed to a group of FF layers with a non-linear activation function (named the prompter) to generate the prompt as key and value components.

Experimental evaluations on Split CIFAR-100 ${ }^{6}$ (60,000 images in 100 classes) and Split ImageNet-R [65] (30,000 images in 200 classes) CL benchmarks showed improved performance over existing methods. However, its scalability concerning task number and data complexity and generalizability to other $\mathrm{CL}$ scenarios with different data distributions remain unclear.

Instruction-based Continual Learning (InsCL) [66] replays data for each previous task based on their similarity calculated as Wasserstein Distance (WD). The instruction embeddings are produced for pairs of tasks, and the proportions of instructions for each task are calculated as probability distributions. In particular, when learning a new task, a replay dataset is formed by sampling examples from previous tasks, where the size of the datasets is estimated as WD between new and previous task training data distributions. The number of previous examples to replay is determined based on their similarity with the task at hand. Moreover, it introduces the[^3]

Instruction Information Metric (InsInfo) to measure the instructions' quality and diversity and guide the replay process to use high-quality data.

Evaluation of 16 distinct tasks (classification, coding, sentiment analysis, QA, comprehension, dialogue, generation, and summarization among others) with different training orders demonstrates consistent performance improvements. However, InsCL requires high-quality instruction-based data, where fuzzy instructions can affect the task similarity calculation and decrease its performance.

The study in [67] presents an Interactive Continual Learning (ICL) framework inspired by the Complementary Learning System theory [68]. ICL utilizes interactions among different models. Specifically, it explores the interactions between ViT (simulating System1) and MLLM (being System2). It adapts System1 parameters while keeping System2 parameters unchanged. System2 handles complex examples and interacts with System1. The Class-Knowledge-Task Multi-Head Attention (CKT-MHA) module enables continuous ViT finetuning using category features and from Class-Task collections. This way, System1 retains old parameters, avoiding ineffective updates. The von Mises-Fisher Outlier Detection and Interaction (vMF-ODI) mechanism estimates sample complexity so that System1 can select complex enough examples. The examples are used for System1 inference, whose predictions represent background knowledge for System2, thereby coordinating the two systems. System 1 uses ViT-B/16-IN21K [53] as a backbone, whereas System 2 employs MiniGPT [69] as a base model.

ICL was evaluated using CIFAR-10 (10 classes, with 50,000 training and 10,000 test color images per class), CIFAR-100 (100 classes, 500 training, and 100 testing images per class), and ImageNet-R (200 classes, 30,000 images). The evaluation was conducted in both TIL and CIL settings and showed CF resistance and superior performance compared to existing methods. However, the limitations of the backbone models restrict the performance enhancement of System2.

The framework is available at: https://github.com/Biqing-Qi/Interactive-continual-Learning-Fastand-Slow-Thinking.

Incremental Vision-Language Object Detection (IVLOD) [70] adapts pre-trained Vision-Language Object Detection Models (VLODMs) to different domains while preserving their general capabilities.

The method employs Zero-interference Reparameterizable Adaptation (ZiRa) with Zerointerference Loss (ZiL) and reparameterization techniques for IVLOD without incurring computation and memory costs. In particular, it retains the original model parameters and adds a parallel branch component (Reparameterizable Dual Branch - RDB) for tuning on downstream tasks without overwriting existing knowledge. Based on the RDB's dual-branch structure, the ZiL component reduces the interference of new knowledge on learned knowledge by penalizing the RDB's output. The RDB learns new tasks while protecting pretraining and downstream task knowledge.

The IVLOD uses a Swin Transformer [71] as a backbone VLM. The evaluation demonstrates that ZiRa preserves VLODM general capabilities while accurately learning new tasks, outperforming state-of-the-art object detection methods, including CL-DETR [72] and iDETR [73]. However, the enhanced performance comes with an increased model size.

The study on Knowledge Editing (KE) for LLMs presents practical and efficient methods for modifying and evaluating LLMs post-training to learn new tasks or correct outdated information [74]. These methods, such as knowledge insertion, modification, and erasure, are designed to allow on-the-fly model modifications while maintaining overall performance across various inputs. In analogy to the types of human learning, KE methods are categorized as:

1) Recognition - exposing the model to new knowledge within a relevant context.
2) Association - forming connections between the model's new knowledge and previous knowledge in the model.
3) Mastery - the model acquires and utilizes the knowledge through its parameters.

However, the above methods focus on LLMs' components - memory, retriever, layers, and parameters which reflect correlations rather than explicit knowledge. In particular, recognition as resorting to external knowledge is implemented by expanding LLMs' layers (e.g., FFNs) and
parameters (e.g., LoRA) [75]. The association is realized via user interactions with the LLM without retraining as additional memory of instruction examples [76] (i.e., various prompting techniques). Mastery manifests through cost-efficient alternatives to fine-tuning. For instance, meta-learning methods edit the model by learning the change in the model instead of updating the weights directly. Model Editor Networks using Gradient Decomposition (MEND) [77] uses the rank-one decomposition to split the model into separate rank-one matrices to compute the change of the weights, reducing the number of parameters but failing on multiple edits as it ignores the conflicts between them. Some techniques try to locate where the knowledge was stored and edit that part of the model [78] (e.g., FFN's area or a set of model weights). However, the side effects are unclear as the underlying LLM mechanisms still need to be clarified [79].

A framework of KE methods and benchmarks with datasets is available at: https://github.com/zjunlp/EasyEdit.

The study [80] examined how pre-trained language models (PLMs) tackle out-of-distribution (OOD) data in CL scenarios. In particular, the model without fine-tuning has poor generalization to OOD data while performing strongly on in-distribution (ID) data. Thus, it must learn from a data stream containing new, unseen examples over time.

It fine-tuned two transformer-based architectures (GPT-2 decoder with 110 million parameters and RoBERTa encoder with 135 million parameters) on two downstream tasks (predicting API usage and API call). The datasets with API usage sequences were collected from GitHub as Java programs managing one of the associated APIs (10 million samples as ID to pre-train the models and 147,000 samples as OOD to fine-tune the models continually).

Reply-based and regularization-based CL techniques mitigated CF to some extent while maintaining or slightly improving PLMs' performance in downstream tasks. The work builds on a carefully selected ID/OOD scenario and cannot guarantee generalizability on novel, unseen examples. Information on the datasets, pre-training, and fine-tuning is available at https://github.com/martin-wey/cl-code-apis.

Research from [81] pre-trained and evaluated the T0 model with 3 billion parameters [82] on 50 datasets related to 50 distinct textual QA, classification, and summarization tasks ( 100,000 examples for training per task). The model was extended to and verified through CL on 8 new language generation tasks (e.g., headline generation, text simplification, and haiku generation). On the one hand, the resulting model demonstrated acceptable performance in learning new tasks (e.g., concept drift) while maintaining nearly $100 \%$ of the initial performance. On the other hand, the study showed that CL emerges from the intensive pre-training stage. Moreover, in cases where the number of tasks to learn would grow by several orders of magnitude, the users should explore more sophisticated methods for efficiency reasons. The model information and results are available at: https://github.com/ThomasScialom/T0_continual_learning.

Progressive Prompts [83] is a technique of learning a soft prompt per task, concatenating it with the learned prompts while keeping the base model unchanged. The goals are to keep the knowledge of previous tasks and allow the transfer of existing knowledge for new tasks. The model parameters consist of the fixed PLM parameters and fine-tuned prompt parameters. The study used an encoder-only BERT model and an encoder-decoder T5 model on text classification tasks. The proposed approach eliminates the need for data replay or storing many task-specific parameters. It shows superior performance on a standard text classification CL benchmark on longer CL sequences spanning 15 tasks. Nevertheless, the model size grows when the number of new tasks increases.

$\mathrm{I}-\mathrm{CL}$ is commonly used to perform diverse language modeling tasks in underrepresented languages, using only short in-context information to narrow the gap between high-resource and low-resource languages [84].

The study from [85] investigates the performance of Few-Shot In-Context learning in LLMs in lowresource languages. It outlines multilingual LLMs' challenges in generalizing to low-resource languages compared to mid- and high-resource languages to emphasize the importance of crosslingual prompting. Relatedly, it proposes the Cross-lingual In-Context learning (X-ICL) method which combines cross-lingual in-context alignment (injecting a label aligner translating the source
labels into the target ones to the prompt in between the in-context exemplars and the input query), alignment formatting, label configuration, and cross-lingual retrieval methods (retrieving one or more labeled source exemplars semantically relevant to the targets) to propose a query alignment technique enhancing model performance on different language resource levels.

X-ICL underwent an evaluation using BLOOM-7B [84] and XGLM-7.5B [86] as baseline multilingual LLMs on 25 low-resource and 7 relatively higher-resource languages across four different datasets for topic classification, NL inference, and sentiment analysis tasks. The results show improved performance compared to the baseline models. However, the method does not enhance the inherent capabilities of the employed LLMs. Moreover, it has limited coverage concerning low-resource languages, datasets, tasks, and chosen LLMs.

Multimodal LLMs (MLLMs) [87] learn incrementally by using specific techniques, such as Multimodal Instruction Tuning (MIT), Multimodal In-Context Learning (MI-CL), and Multimodal Chain of Thought (MCoT).

MIT uses instruction-formatted data to finetune pre-trained LLMs. The model learns new tasks by following novel instructions, where an instruction-formated sample contains an NL sentence describing the task and an input-output pair. However, instruction tuning from textual LLMs to multimodality adds to the complexity of the learning pipeline in terms of the enlarged datasets or additional models, which further increases the computation burden-for example, aligning embeddings of different modalities directly to the LLMs or resorting to other models transforming different modalities into NL content that is usable for subsequent LLMs (e.g., vision-to-text for image understanding). In addition, insufficient alignment between visual and textual modalities can increase the risk of hallucinations (providing statistically valid but factually incorrect answers).

MI-CL differs from traditional supervised learning, which extracts patterns implicitly from underlying data and learns from analogies [88]. Specifically, it uses a few examples with optional instruction, extrapolating to unseen tasks at the inference stage. Nonetheless, scaling up the context size increases the computation burden on LLMs, whose input length is usually limited.

The idea behind MCoT is to prompt the model to generate the reasoning process and the immediate output, resembling human cognitive processing [89]. Though CoT can produce results that simulate human reasoning by crafting a sequence of logically coherent user prompts manually, LLMs do not pose reasoning capabilities by their internal workings (see [16, 17, 90, 18]). The catalogue of MLLMs can be found at: https://github.com/BradyFU/Awesome-MultimodalLarge-Language-Models.

The study from [91] introduced Evaluating MulTimodality (EMT) for measuring CF in MLLMs. The EMT framework employs MLLMs for image classification. It prompts the MLLM to classify the input image based on the text-image pair and evaluates its classification accuracy using another LLM. The study also investigated the hallucination effects when finetuning LLaVA pre-trained MLLMs.

The majority of the open-source MLLMs tested, including LLaVA [92], Otter [93], InstructBLIP [94], and LENS [95], do not achieve a level of accuracy similar to their base vision encoder performance on standard image classification tasks.

Moderate finetuning (e.g., LoRA [47]) improves the LLaVA's performance on similar, non-finetuned tasks, aligning visual and text features early.

However, progressive finetuning using one dataset decreases LLaVA's performance by overfitting and hallucinating (e.g., producing text related to the dataset used for finetuning and not to the input question).

Visual instruction tuning uses machine-generated instruction-following data to improve LLMs in the language-to-image tasks. It extends instruction tuning to connect vision and language understanding with models like LLaVA (Large Language and Vision Assistant) [96]. LLaVa-1.5 architecture contains a pre-trained visual encoder, a pre-trained LLM to interact with users (Vicuna1.5 with 13 billion parameters [60]), and a vision-language cross-modal connector to align the vision encoder outputs to the language model.

The model improved baselines on VQA (Visual question answering) tasks that answer an NL question about visual images. The results also suggest that visual instruction tuning is more critical in improving an LMM's capabilities than pretraining. Although it requires high-quality, targeted visual instruction-tuning data. Besides, it cannot process multiple images due to insufficient
instruction-following data. It incurs high computation costs being trained on 600 thousand imagetext pairs in approximately one day on a single 8-A100 machine.

Details are available at: https://llava-vl.github.io/.

The study in [97] introduces a post-training adjustment method to mitigate CF and improve CL in MLLMs. The Model Tailor retains the pre-trained parameters and replaces fewer finetuned parameters (up to $10 \%$ ). Building on the tailor metaphor that selects patches for a garment, it identifies and adjusts a minimal parameter set as a sparse mask (or the model patch) from the finetuned model. It extracts the patch by inspecting parameter and loss changes to identify a critical subset of finetuned parameters essential for learning a target task. Moreover, it decorates the patch by compensating weights with an inverse Hessian matrix to enhance the performance on both current and previous tasks.

The method is evaluated using InstructBLIP [94] and LLaVA-1.5 [96] MMLMs for VQA and image captioning tasks with available benchmark datasets. The results show improved CL while preserving inherent pre-trained capabilities compared to traditional finetuning and adaptability in multi-task scenarios. However, the effectiveness of extracted and decorated patches remains to be verified in more complex and open domains.

Contrastive Learning (CoL) learns meaningful representations from large-scale unlabeled data by maximizing the agreement of the positive pairs formed by the original instance and its corresponding augmented instance while minimizing the agreement with the negative pairs formed by other instances [98]. CoL can also use supervised learning by adding label information based on category discrimination. Some image classification tasks introduce prototypes, which force the embedding of instances closer to their corresponding prototypes in the embedding space while moving them away from other prototypes [99].

Contrastive Language-Vision Pre-training (CLIP) [100] represents a VLM pre-trained on large image-text datasets. The model contains one image and one text encoder. CoL is conducted during pre-training, where a correctly paired image and text is a positive pair. In contrast, incorrectly paired image and text (belonging to distinct image-text pairs) are a negative pair. The prediction is formed by finding the closest text embedding given the image. CLIP is known for its ability to perform zero-shot image classification, which can classify images based on NL descriptions without specific training in those categories [100].

The study from [101] introduced Zero-Shot Continual Learning (ZSCL) to address the CF in continual learning with the VLM through feature distillation and parameters ensemble. Distilling the initial model features on a reference dataset significantly enhances its performance. Parameter ensemble among different training stages effectively mitigates the forgetting issue. The study also proposed a Multi-domain Task Incremental Learning (MTIL) benchmark to evaluate learning tasks from different domains, surpassing state-of-the-art methods.

However, the limitation is that an extensive, high-quality reference dataset is needed. Sometimes, the pre-training dataset must be filtered out of errors or the existing information updated. Conducting these tasks on a given dataset dataset is difficult.

The details are available at: https://github.com/Thunderbeee/ZSCL.

Language Guidance for Prompt-based Continual Learning (LGCL) [24] introduces a pool of learnable prompts to store task information. The prompts instruct a pretrained image encoder in solving tasks, allowing the model to learn without storing large amounts of data or increasing the number of learnable parameters. Prompt-based continual learning methods leverage pre-trained vision encoders and learn prompts that guide the model in solving new tasks based on stored knowledge without extra storage or additional parameters. The rationale is that the model can describe the tasks' label space with the shared language space despite task distribution variations between tasks. Mapping to this space can mitigate CF, leading to performance improvement. The study from [24] introduces task- and class-level language guidance.

Task-level guidance encodes the language feature matching to the classes represented with the prompt keys in the pool. Relevant prompts are selected from the pool based on the keys to guide a pre-trained model.

Class-level guidance encodes the language feature matching the ground truth class from the pretrained model output.

The levels are evaluated using BERT, RoBERTa, and CLIP pre-trained textual encoder models. Although the approach mitigates CF without including additional trainable parameters, prompting does not teach the model in a traditional sense since its underlying weights are unchanged.

Sequential fine-tuning (SEQ) described in [102] examined fine-tuning PLMs to learn a sequence of 15 downstream tasks of 4 types, including text and intent classification, named entity recognition, and relation extraction. For encoder backbones, the study used BERT (versions with 109 and 335 million parameters). Decoder backbones used GPT2 (124 and 774 million parameters) and GPTNeoX (19 million to 1,21 billion parameters). The study revealed that the inherent anti-forgetting ability of PLMs comes from the pre-training stage and that classifiers learned in SEQ cause forgetting. Specifically, the relative position changes between the class embeddings in classifiers and the features extracted by PLMs led to decreased performance on old tasks even when PLMs do not forget. Besides, the experiments focused on IL classification tasks.

Class-incremental Learning (CIL) is employed by Vision-Language Models (VLM) to learn general representations from textual information without forgetting previously learned tasks [103]. PROjectiOn Fusion (PROOF) [104] approach alleviates CF in VLM. The model appends linear projections on the pre-trained image/text backbones. The respective projection layer encodes the task-specific information by mapping the projected features. Learning new tasks extends new projections while freezing old ones. The benchmarks indicate PROOF learns new classes while retaining the old ones, with state-of-the-art classification performance. Nevertheless, learning new tasks enlarges the model size and requires more training exemplars, increasing memory and computation requirements.

The study in [105] introduces a CoLeCLIP, a method for Open-Domain CL (ODCL) in VLMs that enables them to learn from a sequence of tasks with non-overlapping classes without forgetting previously learned knowledge. CoLeCLIP uses a class vocabulary and a prompt-based approach to learn task-specific patterns and refine class embeddings while mitigating forgetting through replay-free cross-domain vocabulary learning. It simultaneously learns task prompts and class embeddings in CLIP's text and image encoders [53].

The model outperformed state-of-the-art methods concerning classification accuracy and forgetting on 11 datasets for open-domain CL in both CIL and task-incremental learning (TIL) scenarios. However, real-world ODCL can use very different datasets representing incoming tasks, which may exhibit significant domain gaps and large variations in class correlations and data distributions.

The study [106] investigated the acquisition of recommendation capabilities through LLM's incontext learning. In particular, the interaction data (collaborative information) is utilized in the instruction tuning of pre-trained LLMs to improve performance in warm and cold recommendation scenarios. The scenarios describe sufficient or limited or no historical interaction data between users and items, respectively. The resulting LLM encodes collaborative information in an external model, mapping it to the input token embeddings. Collaborative embeddings formed in this way improve recommendations without modifying the LLM itself. The researchers finetuned the Vicuna 7 billion parameters model [60] using the movie recommendation dataset (MovieLens ${ }^{7}$ ) with 1 million interactions and the Amazon-Book ${ }^{8}$ dataset with around 4 million interactions. However, the results confirm the necessity of incorporating high-quality collaborative information and suggest that the improved performance in both scenarios primarily comes from this information and not from adjustable finetuning instructions.

An approach to ingrain learning-in-realtime capabilities is an integration of LLMs with knowledge graphs (KGs) [107]. Neurosymbolic computing [108] combines neural networks' (NNs') patternrecognition capabilities with KGs' reasoning abilities by either compressing knowledge structures into vectorized representations suitable for NNs [109] or linking neural patterns with symbolic knowledge by extracting pertinent pattern information [108]. Currently, the LLM-KG integration is[^4]mainly implemented in learning pipelines with separate LLM and KG components, losing some of the original semantics in the produced representations in both techniques [107, 110].

DynaMind is a continual learning pipeline [111] that combines pre-trained LLMs (GPT4 ${ }^{9}$, Llama $^{10}$ and Falcon ${ }^{11}$ models), VDs (serving as memory), and symbolic planners (coordinator, searcher, browser, responder and discriminator). Essentially, the memory and symbolic planners are used to populate and verify prompt templates so that LLMs are guided to extract new information without tuning their parameters. In other words, it conducts prompting without updating model weights. The framework and experiment details are available at: https://github.com/Elfsong/DynaMind.

Incremental prompt learning [112] employs pre-trained LLMs to help robotic systems learn new tasks. In particular, the content of the current scene (what the robot perceives) is used to construct prompts as LLM inputs (ChatGPT in this case). The responses are fed into the robot's memory to generate interaction examples containing prompts and responses based on which the robot learns new actions. The robot learns from the interaction with an LLM. This approach does not improve the capabilities of the LLM itself and is susceptible to hallucinations requiring human supervision in generating interaction examples.

Similarly, LEAGUE++ [113] uses LLMS to facilitate Deep Reinforcement Learning (DRL) and Task and Motion Planning (TAMP) for continuous learning of new skills in robotic devices.

With LEAGUE++, an LLM accepts task decomposition and operators involved in generating skill plans for TAMP. Another LLM then generates skill rewards for RL skill learning, which a formal planner further verifies. This process enables online autonomous learning for long-horizon tasks.

Although the framework demonstrates superior performance compared to baselines in four simulated task domains, it doesn't enhance CL in the LLMs employed.

Decision-making workflows for autonomous vehicles use standard and chain-of-thought prompting of LLMs in highway scenarios to generate driving plans and influence driving behaviors through verbal commands [114]. Although workflows are improved and verified in a simulated environment, they do not enhance LLMs' capabilities.

AutoGPT+P utilizes LLM with formal planners for affordance-based task planning [115]. In particular, the study presents an affordance-based scene representation combining object detention and automatic object-affordance mapping (OAM) using ChatGPT. The OAM associates object classes with the set of affordances assigned to the instances of those classes. The scene objects are then represented as a set of object-affordance-pair (OAP). The study employs different prompting techniques for querying ChatGPT for OAMs, which are then used to generate plans and explore and suggest alternatives in case of missing objects needed to achieve a task goal specified by the user in NL. The proposed planning architecture was evaluated by simulating 150 scenarios with different tasks of humanoid robots, such as picking and placing, handover, pouring, chopping, heating, wiping, and sorting.

However, given the probabilistic representation of the OAPs obtained from LLMs outputs, generated plans need external verification for improved accuracy in real-world deployment.

Continual Optimal Policy Regularization (COPR) is a method that aligns LLMs with human preferences in a CL setting [116]. RLHF-based techniques often require retraining when encountering new queries or feedback, as human preferences vary across domains or tasks. The method computes the distribution of optimal policy bypassing the partition function and then regularizes the policy based on optimal distribution to mitigate CF and improve preference learning. Besides, it can learn from unlabeled data by maintaining a scoring module similar to the reward model, making it flexible for CL without human feedback.

Although the method outperforms existing CL methods in the task and domain CL settings, it still needs RLHF reward and policy models to start from, which may reflect the preferences of a particular user group and induce a bias when optimizing the current policy.[^5]

The study in [117] proposes a post-pretraining method for LLMs that expands Transformer blocks. It starts with an LLM pre-trained on a massive unlabeled corpus. Then, it employs backbone expansion and finetunes the expanded blocks using only a new corpus while freezing the blocks inherited from the base model. This way, it aims to improve the model's knowledge without CF. The model after post-pretraining can be used for instruction tuning.

The method was demonstrated using coding and mathematics training data, resulting in LLaMA Pro-8.3B, obtained from LLaMA2-7B as the base model.

LLaMA Pro and its instruction-following finetuned version (LLaMA Pro-Instruct) have outperformed existing LLaMA-based open models in general tasks, programming, and mathematics. This result, however, is based on an increase in model size and evaluation in specific applications (coding and mathematics). Therefore, the effectiveness of the block expansion method in more complex and open-ended domains is yet to be verified.

Processing long input sequences during inference is challenging for LLMs as they perform poorly on inputs whose lengths are larger than those in training sequences. SelfExtend [118] is a method that aims to handle long contexts without fine-tuning. It extends the LLMs' context window by structuring the attention component into a group and a neighbor attention. The group attention manages long-distance token relationships by tracking the dependencies among distant tokens. The neighbor or standard attention tracks dependencies among adjacent or proximate tokens based on a specified range. The attention matrices are merged, and the softmax operation is applied. This way, the method connects the distant positions at inference time to positions seen during training so that LLM can handle longer texts without additional fine-tuning.

Extensive evaluations with LLaMA-2 [119] and its successors, Phi-2 [120], Mistral [121], and SOLAR [122], on short-context tasks, synthetic and real-world long-context tasks, and language modeling tasks have consistently demonstrated the effectiveness of SelfExtend. However, this extension in context window length comes at the cost of increased computation, as it requires calculating extra attention across all query-key pairs.

The details are available from: https://github.com/datamllab/LongLM.

Rehearsal-based methods use past training data, which may be unavailable or nonexistent at inference time.

Self-Synthesized Rehearsal (SSR) [123] utilizes the LLM to produce synthetic rehearsal data. It finetunes the base LLM to create synthetic examples through I-CL with few-shot examples. The examples are collected from the previous data or created manually. Next, it employs the latest LLM to refine outputs using the synthetic instances. Finally, it selects diverse, high-quality synthetic instances augmented with currently available data for future rehearsals.

SSR employs LLaMA-2-7B, LLaMA-2-7B-Chat [124], and Alpaca-7B [125] LLMs using LoRA [47] and Super-Natural Instructions (SuperNI) [126], a comprehensive instruction tuning benchmark dataset.

The results indicate that SSR is more data-efficient and achieves superior or comparable state-ofthe-art performance. Moreover, it preserves LLMs' pretrained generalization capabilities. However, synthetic instances may contain unreliable content from biases in data and may not always show the best generalization ability on unseen tasks.

Without user-generated training data, some approaches train LLMs from synthetic data [127]. The study from [49] proposes an interactive interface that allows users to prompt the GPT-4 LLM by providing specific instructions where the model responses are cleaned and used for instruction tuning the base model. The system named Online Training using External Interactions supports three prompt types as input data augmentation techniques: textual instructions (aka InstructionGuided Learning), textual instructions with offline sources (aka Document-Driven Learning), and textual instructions with online sources (aka Web Search-Enabled Learning). The instructions generate corresponding training data, which are further filtered for quality control (toxic or biased responses) and used to train the base model.

The system was compared with the online parameter-invariant method (prompting) and the offline parameter-variant method (training all parameters). The former uses LLaMA2-7B-Chat [124] as the
base model to produce answers from few-shot prompts. The latter method trains the model with external annotated training data and then uses the same prompts for the test set as in the former. Although the system has reduced training costs regarding the data amount compared to the offline method, it still underperforms in accuracy, which remains similar when increasing the number of training examples.

The Scalable Language Model (SLM) [128] adapts the base LLM to new tasks from distinct domains while keeping its performance on the older tasks. SLM integrates the task distribution within the vector space retrieval into the language model for knowledge expansion and management. The Dynamic Task-related Knowledge Retrieval (DTKR) component identifies the most relevant knowledge for each input instance, assuming each task has a distinct data distribution in the vector space. It preserves relevant knowledge by compiling weight increments through low-rank adaptation to decrease computation. The Joint Adaptive Re-parameterization (JARe) then uses these weight increments to obtain adaptive re-parameterization of the pretrained model, aligning it with downstream tasks based on their distribution.

The SLM is efficient and stable on existing benchmarks, achieving state-of-the-art performance on different base models, including BERT [129], T5 [130], and LLaMA-2 [124]. However, the method introduces a separate retrieval component, increasing memory and computational costs.

The model and the experiment data are available at: https://github.com/Pbihao/SLM.

The study from [131] combined learning rate (LR) re-warm, LR re-decay, and replay techniques to compare the performance with full-data re-training. The study trained the GPT-NeoX [132] decoder-only model and compared the performance by measuring final loss and LLM evaluation benchmarks.

The results show that LLMs can be successfully adapted by combining the above elements. The performance matches the re-training baseline with a compute fraction. In particular, re-warming and re-decaying the LR from a large to a small value in pre-training improves adaptation to new tasks.

The study also suggests $5 \%$ replay as a default value and using more replay with significant distribution shifts.

While the experiments updated the model on two subsequent tasks, the approach's efficacy in settings involving larger distribution shifts, model and dataset scales, infinite LR schedules, growing models, and tokenizer adaptation for handling larger changes in data distribution remains to be verified.

### 2.2. Meta-Learning

The meta-learning (MeL) paradigm goes beyond existing approaches in that a model first learns how to learn (meta-training phase) and then learns new tasks efficiently (meta-testing phase) [133]. It is usually concerned with efficient model adaptation to new tasks quickly, using few examples, and generalization across these tasks [134, 135]. MeL is critical when data is scarce, difficult to obtain, or its distribution constantly changes. Currently, it is analyzed by looking at how $\mathrm{NN}$-based architectures learn reusable representations using a small number of training examples (aka few-shot learning) [133]. This way, tasks share specific structures, enabling models to transfer knowledge across tasks. The tasks include image recognition [136], sentiment analysis [136], machine translation [137], and dialog management [138].

$\mathrm{NN}$-based perspective on MeL introduces two components: a meta-learner that produces taskspecific parameters using a smaller amount of labeled data from a task and a base learner that makes predictions on unlabeled test data from the same task [133].

Depending on the implementation of the above components, MeL methods can be categorized as $[134,133]:$

1) Black-box methods - A black-box neural network takes the entire training dataset and predicts task-specific parameters, which are then used to parameterize the base network and make predictions for test data [134]. While these methods are versatile regarding tasks and learning problems, the meta-learner architectures are complex with high computational and data requirements.
2) Optimization-based methods - the meta-learner is an efficient optimization process (e.g., gradient descent (GD)). It obtains a set of meta-parameters that are easy to learn via GD and fine-tunes them on new tasks. In other words, it learns task-specific parameters from training data and then updates the initial set of meta-parameters by optimizing the performance on the test set of the same task. The meta-parameters can be inner optimizers, NN architectures, and other network hyperparameters [139, 140]. Nonetheless, the process of backpropagating through multiple gradient adaptation steps is computationally expensive.
3) Distance metric learning methods - while the previous two methods use NNs as parametric base models, this method employs parametric meta-learners to produce non-parametric learners (base models). These methods learn an appropriate distance metric (e.g., Nearest Neighbors) for comparing instances in a low-dimensional space, enabling the model to learn an adequate embedding space for classification tasks. During the meta-test, each example in the training set is compared with the test instance to determine whether they belong to the same class. For example, prototypical networks aggregate multiple class information to create a prototypical embedding, which is then used for N-way classification [141]. Although the methods can classify data in a few shots by embedding data and applying Nearest Neighbors, they may not capture complex relationships between data points.
![](https://cdn.mathpix.com/cropped/2024_06_04_997dafbdbb7da41a6ac3g-18.jpg?height=886&width=1648&top_left_y=986&top_left_x=204)

Figure 3. Approaches to Meta-Learning: black-box methods (top left), optimization-based methods (top right), and distance metric learning methods (bottom).

Other approaches to learning across multiple tasks by parameter sharing include Multitask Learning (MTL) and Transfer Learning (TL).

MTL aims to improve task performance by learning them simultaneously [142]. One approach to MTL is hard parameter sharing, where the model parameters are split into shared and task-specific parameters [143]. Another approach is soft parameter sharing, which measures parameter similarity across task-specific models using regularization penalties [144]. However, choosing the appropriate approach, designing the model architecture, and determining the extent of parameter sharing across tasks depend on the problem being solved [142].

TL fine-tunes a pre-trained model on a new task using less training data [124]. The model leverages representations learned from previous task(s) when solving new tasks. The standard transfer learning approach is fine-tuning, which starts from a pre-trained model whose parameters are then fine-tuned on the training data from the target task using GD or other optimizer functions [145]. However, fine-tuning can destroy initialized features from the pre-training phase [146]. Preventative measures include a lower learning rate for earlier layers, freezing earlier layers, and
gradually unfreezing or re-initializing the last layer [147]. Besides, fine-tuning may not be effective when the target task dataset is very small or very different from the one used in pre-training [148].

MeL is used in some complex learning scenarios. These include learning from diverse multimodal task distributions sampled from a task distribution with multiple unknown modes [149]; distributed learning without sharing data across client models as personalized federated learning [150]; learning that adapts to data distribution shifts (i.e., the input data distribution changes between the training and testing) for more effective domain adaptation-generalization tradeoff (i.e., efficient adaptation to new domains with limited data while better generalization on domains not trained on) [151]; and unsupervised meta-learning using unlabeled data [152].

However, MeL faces some fundamental challenges.

Generalization to out-of-distribution tasks and long-tailed task distributions where tasks in metatesting are from a different task distribution seen at the meta-training time remains difficult to achieve $[134,133]$.

Meta-training on multiple data modalities occurs by constructing separate meta-learners focused on tasks from one modality, as different modalities have different dimensionalities [153]. Learning a meta-learner that captures multiple data modalities remains unsolved.

Current meta-learning methods have significant computational and memory costs [134, 133]. Thus, deeper theoretical improvements concerning more comprehensive sample complexity and generalization metrics, the design of better optimization functions, and proper integration of domain/task-specific knowledge can lead to more efficient and effective MeL techniques.

With their representation ability, graphs are crucial in modeling real-world objects and enhancing LLM's reasoning capabilities.

Graph Neural Networks (GNNs) model and learn on graph-like representations with textual node attributes (aka text-attributed graphs) [154]. They represent graph nodes as textual embeddings, exposing limitations in interpreting causal knowledge and understanding semantics [154]. Besides, they are usually retrained to accommodate different tasks on graphs.

On the other hand, LLMs are commonly used for graph node classification tasks by enhancing nodes' text attributes and generating predictions through GNNs or as independent predictors [155].

The Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining (MuseGraph) [156] extracts compact graph descriptions using graph analysis techniques such as random walks and one-hop neighbors. In other words, it textualizes graphs with essential semantic and structural details that LLMs can process under limited input token size.

These descriptions are further used to produce diverse instructions tailored for different graph mining tasks (e.g., node classification and link prediction) to enrich the LLM's capabilities in understanding and analyzing graph data. In particular, by designing CoT templates [9] for selected graph tasks and prompting GPT-4 to produce a small set of CoT-based instructions.

Finally, the instructions guide graph-aware instruction tuning of LLM for each target graph mining task.

It employs LLaMA-7B LLM [124] with LoRA [47] as a starting model for instruction tuning.

The evaluation results demonstrate performance improvements in different graph mining tasks compared to GNN-based and LLM-based methods. However, this is verified in a limited setting regarding task coverage and instruction tuning set.

The study from [157] introduces NPHardEval4V, a dynamic benchmark to understand the extent of reasoning capabilities in MLLMs. It is constructed from textual questions in NPHardEval [158] by converting them to images. The NPHardEval benchmark contains 9 different algorithmic problems-3 polynomial time (e.g., Shortest Path Problem), 3 NP-complete (e.g., Traveling Salesman Problem Decision Version), and 3 NP-hard (e.g., Meeting Scheduling Problem). Each problem includes 100 examples of varying difficulties.

Moreover, NPHardEval4V contains evolving datasets, a crucial aspect for CL, as it demonstrates the potential of MLLMs to adapt rather than optimize for static tasks.

The evaluation is conducted on various open- and closed-source MLLMs, including GPT-4V [159] Gemini 1.0 Pro [160], CogVLM 17B [161], LLaVA-1.5-13B [92], BLIP-2 FLAN-T5-XXL 11B [63], Otter 8B [93], Qwen-VL-7B [162], and Kosmos2 1.5B [163].

The study experimented with visual, text, and combined prompts.

The recognition and reasoning experiments conducted in this study show that closed-source models perform better in all tasks. However, this may be attributed to the model size. The MLLMs' reasoning performance degrades when increasing question difficulty in individual reasoning tasks. The effects of prompt design combining visual and textual inputs vary across the MLLMs, emphasizing the importance of an appropriate balance when combining visual and textual information.

The benchmark is available at: https://github.com/lizhouf/NPHardEval4V.

### 2.3. Parameter-Efficient Learning

Parameter-efficient tuning (PET) adjusts a subset of the pre-trained model's parameters by adding new parameters or modifying a smaller subset during the finetuning process [164, 165]. Selective adjustment of a smaller set of parameters reduces computation costs and memory consumption, uses less training data, and is less prone to overfitting.

PET for LLMs occurs in the following principal ways [165]:

1) Addition-based methods [166] add trainable parameters while keeping the original LLM intact. Specifically, they modify the model structure by injecting trainable parameters or modules. Finetuning updates only the injected parameters, reducing memory and computational demands. Several additive methods exist.

Adapter approaches insert smaller adapter layers within Transformer blocks. Serial adapters [167] are positioned after the self-attention and FFN layers. They can reduce the model's parallelism and increase its complexity. Parallel adapters introduce adapter layers parallel to Transformer layers [168]. For example, CoDA [168] identifies relevant tokens processed by all layers to maintain overall model performance. The adapter only processes the' unimportant' tokens for efficient inference without compromising overall performance.

Soft prompts are an alternative to adapters in that they prepend adjustable vectors (soft prompts) to input sequences [169]. Similarly, prefix-tuning [6] generates learnable vectors as prefixes to all Transformer keys and values. Fine-tuning optimizes the prefix vectors for inference.

2) Specification-based methods [170] adjust only a portion of the parameters. In particular, they select and finetune a smaller parameter subset for downstream tasks. For example, Diff pruning [171] applies a binary mask by learning a task-specific difference vector added to the pre-trained model parameters that remain fixed during fine-tuning. The mask identifies the finetuned parameters by selective pruning that structures the target parameters.
3) Reparameterization-based methods [47] transform a model's architecture by converting LLMs' adaptive parameters into parameter-efficient forms during optimization. Transforming the original model into a functionally equivalent but simplified representation optimizes inference speed. They introduce additional low-rank trainable representations for finetuning combined with the original model. The goal is an effective low-dimensional fine-tuning reparameterization. Low-Rank Adaptation (LoRA) [47] is the widely used technique introducing parameters to model the weight differences and performs better than most mainstream PET methods [164, 172]. Specifically, it constructs two trainable weight matrices. Upon fine-tuning, its weights are integrated with the pre-trained weights to maintain the inference efficiency without extra costs. For example, the PET time on the LLaMA-7B model [124] was approximately one-fourth compared to tuning all parameters [173]. The main challenge in LoRA is choosing an appropriate rank for trainable matrices.
4) Hybrid methods integrate PET methods and techniques, utilizing their advantages while mitigating drawbacks. UniPELT [174] incorporates prefix-tuning, adapters, and LoRA into LLM blocks, demonstrating consistent improvements in accuracy. S4 [175] explores design spaces to combine and formulate different PET methods as design patterns. LLM-Adapters [176] builds a framework incorporating different PET methods into LLMs. The smaller LLMs utilizing PET and appropriate finetuning data show competitive performance compared to training a larger parameter set.

![](https://cdn.mathpix.com/cropped/2024_06_04_997dafbdbb7da41a6ac3g-21.jpg?height=569&width=1357&top_left_y=201&top_left_x=338)

Figure 4. Approaches to Parameter-Efficient Tuning: addition-based (left), specification-based (middle), and reparameterization-based (right). Trainable parts are in green, while the frozen parts are in red.

Continual Parameter-Efficient Tuning (ConPET) is an extension for continual LLM learning [177]. Static ConPET combines data replay with PET. Dynamic ConPET comprises lightweight, taskspecific PET modules as experts tuned without changing the original LLM parameters.

The results demonstrate a significant reduction in tuning costs. However, this remains to be verified using more diverse continual learning scenarios with further improvement of task split strategies among experts.

The models and data are available from: https://github.com/Raincleared-Song/ConPET.

The study from [178] introduces a PET framework for vision-language models for CL that alleviates long-term forgetting in IL. It expands a pre-trained CLIP model [53] by adding LoRAs [47] as sparse experts in all image and textual encoders. The Distribution Discriminative Auto-Selector (DDAS) routes inputs to the expert adapters or the pre-trained CLIP, preserving the general capabilities. In addition, task-specific routers activate corresponding experts responsible for particular tasks for effective predictions on known data and forward transfer for unknown data.

The framework outperforms state-of-the-art solutions on classification tasks and reduces parameter training costs. However, the number of experts increases with the number of tasks and learned tasks do not improve the original CLIP's zero-shot transfer ability.

The details are available at: https://github.com/JiazuoYu/MoE-Adapters4CL.

The SSF (Scale and Shift the deep Features) method [179] scales and shifts the pre-trained model's features and employs the derived versions in finetuning. It assumes that upstream and downstream datasets have different data distributions. Accordingly, it introduces scale and shift parameters falling in discriminative and unified parameter space, as variance and mean, to modulate the trainable features for downstream tasks.

The method is evaluated using different pre-trained MLLM backbones, including ViT-B/16 [53] and Swin Transformer [71], on various classification, robustness, and out-of-distribution datasets. It outperforms other PET methods. However, obtaining tunable parameters by scaling and shifting original parameters needs further verification in more complex domains whose training data might significantly vary from the ones used in pretraining.

The SSF and its data are available at: https://github.com/dongzelian/SSF.

Post-deployment LLM updates to deal with errors and capture changing data is known as Model Editing (ME). Adjusting all LLM elements is expensive and leads to CF, so different methods focus on a specific subset of trainable properties or introduce new ones. The MELO [75] represents a plug-in ME solution integrating LoRA modules indexed in an internal VD. It extends backbone LLM capabilities by activating specific LoRAs based on their VD indices. LoRA blocks are activated by searching inputs in the VD during inference. LoRA blocks from different layers but shared indices are active, and training updates VD clusters for prospective LoRA searches during inference.

The MELO employs multiple LLM backbones for editing, including BERT, T5-Small, and T5-Large. Extensive experiments on sequential tasks of classifying documents, QA, and correcting hallucinations indicate state-of-the-art performance while requiring the least trainable parameters and computational cost. However, its size increases with the number of learning tasks and introduces a VD as an additional component to the backbone models.

While existing PET methods focus on updating a small number of LLM weight parameters, the study from [180] develops a family of Representation Finetuning (ReFT) methods that update LLM representations that encode rich semantic information. The methods work on a frozen backbone and learn task-specific functions, manipulating a smaller fraction of hidden representations to drive the model to solve new tasks during inference.

ReFT instance called Low-rank Linear Subspace ReFT (LoReFT) finds internal representations from a low-rank projection matrix using the Distributed Alignment Search (DAS) [181]. DAS finds an optimal alignment between an interpretable high-level causal model and a low-level DL system it simplifies using GDs.

LoReFT is evaluated on LLMs of different sizes, from RoBERTa 125M [182] to LLaMA 13B [124], and benchmarked against existing PET methods, including prefix-tuning, serial and parallel adapters, LoRA [47], and DoRA [172], regarding performance and parameter efficiency. LoReFT uses ten times fewer parameters than LoRA, showing comparable performance.

The ReFT training library is available at: https://github.com/stanfordnlp/pyreft.

The Shared Attention Framework for Parameter-efficient conTinual learning (SAPT) [183] introduces the Shared Attentive Learning \& Selection module (SALS). Specifically, the SALS preprocesses training samples to find the optimal PET block combinations for completing the current task. It does so by obtaining an attention weight via instance-level shared attention operation. Then, inputs during the testing time can follow the same shared attention operation to reach the attention weight and choose the appropriate PET blocks accordingly. The Attentive Reflection Module (ARM) assists SALS in recalling the shared attention operation of samples of earlier tasks that should be performed initially through generative replay with synthesized samples to prevent forgetting.

The SAPT has been evaluated on the SuperNI CL NLP benchmark [126], including dialogue generation, information extraction, QA, summarization and sentiment analysis tasks, and Long Sequence [184], a CL benchmark of 15 classification datasets benchmarks. It has consistently achieved state-of-the-art performance compared to other PET methods and demonstrated superior performance with different model sizes (from 770M to 13B) and architectures, including the T5 encoder-decoder [130] and the LLaMA-2 decoder [124].

However, increasing the learning sequence (number of tasks) requires additional PET blocks, which increases computation and storage costs. Moreover, tasks should be identified for training to determine task-specific PET parameters.

### 2.4. Mixture of Experts

Another stream of work extending LLM capabilities over time is an MoE architecture comprising multiple models handling different portions of input data. The goal is to extend LLM capabilities by adding parameters without increasing inference costs. What constitutes an expert differs, as we illustrate with typical examples in practice.
![](https://cdn.mathpix.com/cropped/2024_06_04_997dafbdbb7da41a6ac3g-23.jpg?height=684&width=1698&top_left_y=180&top_left_x=178)

Figure 5. Approaches to Mixture of Experts: LLM components as experts (left), and LLMs as experts (right).

Domain expert mixture (DEMix) [185] layer is a collection of domain-specialized expert FFNs. The experts are added, removed, or mixed after pretraining. However, introducing additional layers modifies existing structures in Transformer-based architectures [1] (e.g., FFN layer). Besides, it keeps all experts in memory and can face scalability issues.

The study from [186] employed instruction tuning on MoE models consisting of FFN experts. It shows that instruction tuning on downstream tasks improves the MoE models' capabilities (e.g., FLAN-MOE with 32 billion parameters). However, effective ICL, high-quality instruction examples, and fine-tuning training data still require larger models.

The research in [187] combined MoE layers with other methods, such as PET and instruction tuning on 'lightweight' experts (i.e., updating less than $1 \%$ of an 11B parameter model) to generalize to unseen tasks. However, the parameter percentage (or expert size) varies with the pre-trained model size and the number of experts involved.

Another study trained multiple domain-specific LoRA modules as mixture-of-LoRAs (MoAs) in a supervised manner [188]. The modules are aligned as experts using an explicit routing strategy, where each LoRA model can be iteratively adapted to unseen tasks. However, expert training can require labeled data at scale to specialize in a specific domain and the adaptation of the routing algorithm.

The Mixture of Prompt Experts (MoPE) [189] is a conditional multimodal prompt tuning method. MoPE utilizes the multimodal input fusion information by introducing multiple prompt experts rather than extending individual prompt lengths. It learns multiple fixed-length prompts as experts and an expert router per layer. The router selects the appropriate prompt for each input instance based on its distinct modalities embedings.

Its results are comparable to fine-tuning, requiring less trainable parameters and outperforming state-of-the-art prompt methods. However, increasing the number of tasks increases the number of experts, and different routing strategies affect expert learning.

Finally, the study in [190] extended Mistral 7B LLM such that each layer comprises eight feedforward blocks as experts (Mixtral 8x7B). Two experts process the current state and combine their outputs for every token at each layer. Each token can access 47 billion parameters but only uses 13 billion active parameters during inference. Although it demonstrates performance and efficiency gains concerning other LLMs, its capability to learn new tasks depends on the model size (number of parameters).

## 3. Principal findings

### 3.1. Continuous Learning

CL learns incremental tasks with dynamic data distributions in various domains, including object detection and semantic segmentation (computer vision), NLP tasks, conditional generation (generative models), and autonomous driving (perception, motion planning, motion control, and user-vehicle interaction) [2].

One of the key challenges in CL is to achieve an appropriate balance between stability and plasticity while ensuring adequate generalizability intra-task (from training to test data) and intertask (accommodating data distribution changes) $[25,165]$.

Stability concerns LLM's memory as the difference between the task's current and maximum past performance (aka forgetting measure) and how learning a new task affects learned tasks (aka backward transfer).

Plasticity is an LLM learning capacity, defined as the extent to which the model cannot learn new tasks or the joint influence of learned tasks on learning the current task.

The CL techniques working in embedding space (e.g., soft prompting, adapters and prefix-tuning) are more expressive and effective than prompting which operates in the discrete token space. However, most of the work described focuses on learning plasticity and inter-task generalizability $[22,23,24,25]$.

The challenges of continuous learning of LLMs include:

1) Multimodal Learning - LLMs are pre-trained with enormous datasets and can pick up very complex and unexpected behaviors (e.g., hallucinations). Real-world data are not restricted to a specific modality but a random combination of multiple ones. Integrating and merging various modalities into a coherent CL workflow requires efficient modalities encoding, multi-modal databases, assigning weights during fusions, and comprehensive benchmarks, which inevitably leads to increased computational costs without general performance guarantees [191].
2) Data Scaling and Quality - CL methods require task-specific datasets using sources similar to old tasks [192]. The size and quality should adapt to obtain an appropriate trade-off between the efficiency and the effectiveness of learning incremental tasks. Inherent biases in training data can manifest in harmful or incorrect model decisions [193]. Besides, the transferability of the task- and domain-specific training data and their compositionality with other tasks and domains without extra inference cost and scalability issues in large-scale training remain unsolved. Finally, effective and efficient CL methods may differ for smaller and larger models. Exploring scaling strategies that adapt and remain effective as the model size varies is crucial due to the ever-evolving landscape of LLM architectures.
3) Sustainability - The environmental impact of training and deploying LLMs is still at stake due to enormous resources required. Future CL models and pipelines should be more energyefficient and operate in low-resource environments to reduce environmental footprint and increase their accessibility [194].
4) Trustworthiness - LLMs are susceptible to adversarial attacks. Modifying inputs can produce unexpected and harmful outputs [195]. CL system design should include encryption protocols for personal data and intermediate training and inference results without compromising performance. Improving CL models' robustness against such attacks is important, particularly for safety-critical applications.
5) Few-Shot Generalization - LLMs struggle to learn tasks from few examples or domain-specific knowledge [196]. CL should improve its performance with limited training data. These data should be carefully selected as their distribution shift can degrade performance compared to previous data or cause poor model adaptation.
6) Multilingual Learning - Making CL more accessible and effective in underrepresented languages with limited training data [197].
7) Responsiveness - The inference in CL should generate predictions in real time for applications such as chatbots or recommenders, where low-latency responses are critical for user satisfaction [198]. LLM inference efficiency is affected by the model size, increased computational costs of longer inputs, the number of requests, and dependencies on external knowledge sources. Rethinking what constitutes efficiency is necessary as additional
parameters and representations are computed and stored. Current model compression techniques [199], such as pruning and quantization, solve the problem partially by focusing on the LLM's parameters representation, not their architectural elements.
8) Context Size - CL uses a limited amount of input tokens to generate subsequent words due to LLMs' constrained context window [200]. This can cause difficulties processing lengthy sequences concerning coherence and relevance since the model can neglect or lose track of the relevant information (e.g., large document inputs or long conversations).
9) Up-to-date Knowledge - LLMs learn from data snapshots, and their knowledge is restricted to what is available on a particular date. Consequently, $\mathrm{CL}$ needs access to the most recent information. At the same time, updating LLMs with up-to-date knowledge may outdate their internal knowledge [201]. Besides, managing conflicting knowledge from one or more sources to choose the correct one is non-trivial for LLMs and CL. For example, different datasets can contain contradictory information about the same fact or carry conflicting biases and inaccuracies. Since LLMs can not naturally resolve such conflicts, external resolution strategies must assess the data's reliability.
10) Unified benchmarking - Various benchmark tools and datasets make it difficult to reliably compare the performance of different methods and algorithms against a standard set of tasks and metrics. If the metric is the goal, it may not continue to be a good metric. There is no guarantee the benchmark data was not used for LLM training directly or indirectly. Moreover, the performance against research benchmarks does not necessarily imply usefulness in a realworld setting. These may be small for industry standards, and data can be very different. Openended evaluation is the biggest bottleneck to Al adoption, and current efforts focus on solutions that can be evaluated.

A recent study [202] discovered that popular context-based parameter-efficient fine-tuning methods, including prompting, in-context learning, prompt tuning, and prefix-tuning, are potentially less effective than full fine-tuning, even with the same number of learnable parameters. In particular, they have structural limitations when learning new attention patterns since they cannot change the attention pattern over the content and can only drive the outputs of an attention layer in a fixed direction. Therefore, despite their success in learning tasks on the fly, they may be unable to learn novel tasks that require new attention patterns.

### 3.2. LLMs versus Humans

LLMs engage in an iterative communication process, generating streams of tokens. They rely on underlying probabilistic models to produce token arrays in response to token arrays. When presented with an input token string, they utilize previously calculated weights to iteratively generate and return a token stream that aligns with the input based on a certain probability threshold. This communication is constrained by the maximum number of tokens LLMs accept at once (context size) to generate a self-contained output. External data can be incorporated as vector embeddings and query vector databases (VDs) to augment the trained model during inference (e.g., RAG).

Mainstream LLM research focuses on improving capabilities within existing knowledge boundaries. Understanding the boundaries of their knowledge as limitations regarding the unknown, referred to as self-knowledge, is essential ${ }^{12}$. Evaluating 20 available LLMs' self-knowledge [203] by measuring their ability to identify questions they do not know or can not answer revealed a considerable gap between their capabilities and the human ability to recognize the limitations of their knowledge.

While humans can quickly acquire new skills by leveraging prior experience (or mental models) as generalists, LLMs are still specialists performing well specific tasks.

Hyperparameters (e.g., adapter size, LoRA's rank, placement of added components, optimizer choice, and number of fine-tuned layers) influence the CL performance. Performance ultimately depends on the manual tuning of hyperparameters, which requires considerable effort. Automated[^6]optimization of hyperparameters is still an issue for LLMs, lacking simpler solutions with better performance-efficiency tradeoffs [204].

Current ME approaches aim to make the existing model architectures more flexible to provide costefficient targeted updates without retraining the entire model. However, changing one model component can affect other components. Ensuring partial modifications do not reduce general performance or introduce algorithmic- and data-related biases is challenging.

VDs are helpful as a cost-efficient information retrieval solution within LLMs' frameworks [15]. However, they are still not optimized for retrieving the information with the intended meaning, such as incremental query adjustment and free-text search using keywords in established retrieval paradigms (e.g., relational and graph databases).

LLMs can store massive amounts of real-world knowledge through intensive training. However, even with the existing $\mathrm{IL}$ approaches, they still cannot update their knowledge fast enough to keep pace with constant external, real-world knowledge change and emergence. Accordingly, the IL approaches described and existing solutions [205, 206] still need improvement concerning computational demands and effectiveness.

Moreover, KGs mainly rely on textual graph structures to represent knowledge, whereas real-world knowledge inherently comprises different modalities. Effective graph representations for multimodal knowledge that represent and align different modalities into coherent entities remain an issue for KGs [207]. MLLMs encode and align distinct modalities in a vectorial form. This gap between multimodal KG and LLM representations hampers their synergy.

### 3.3. LLMs and Metacognition

LLMs can answer questions and generate useful content on a wide range of topics.

Prompting techniques guide them to generate particular outputs in response to specific types of inputs. However, they expose shallow reasoning and are prone to errors as their complexity increases. They fail to address complex knowledge because they do not actually know how they learned to produce outputs. LLMs generate statistical responses emerging from numerical weights calculated at training time.

Adapting their behaviors to continuously evolving knowledge is challenging for LLMs. This requires comprehensive and responsible guidance to incorporate emerging knowledge in a timely manner while conforming to ethical norms to ensure safe usage.

The language modeling paradigm is based on the next token prediction, but its underlying structures and processes remain opaque. Thus, it is unclear whether current KE methods constitute meta-learning stemming from variable probability distributions of training data. Therefore, achieving meaningful and intentional KE with LLM is questionable. For example, a failure in LLM's logical deduction is known as the reversal curse [208], where if a model is trained on a pattern 'A is B,' it will not necessarily generalize in the reverse direction ' $\mathrm{B}$ is $A$.'

KE in LLMs may be more effective in task-specific scenarios, where the consequences of model changes can be anticipated.

Knowledge injection usually leads to overfitting LLMs as they struggle to obtain recent factual knowledge via unsupervised fine-tuning [209]. A key challenge is the timely injection of relevant and updated knowledge in LLMs to develop user trust. Maintaining knowledge persistency in LLMs is another challenge, as their current knowledge is represented by generally nonpersistent parametric memory and derived internal representations influenced by inherent CL-CF tradeoff.

LLMs can sometimes solve reasoning problems, and they do so as a result of the token prediction process. In other words, reasoning problems are translated into pattern completion problems. Building trustworthy reasoning systems using LLMs means integrating them with a logic tool that implements causal mechanisms [16, 17]. Otherwise, we should be able to reverse-engineer their responses to understand an internal process that leads to the reasoning-driven response [210]. Relatedly, the amount of LLMs generalizability corresponds to the extent of token prediction, which ultimately depends on the number of trained parameters and dataset size.

Although they optimize LLMs in terms of size and architecture to adapt to new knowledge, these issues persist in IL.

## 4. Conclusion

Significant advancements have been made in developing specialized LLM-based systems to solve specific problems. However, the goals of generality and adaptability across multiple tasks remain an open problem.

With its potential to bridge this gap, IL holds the promise of enabling algorithms to acquire new knowledge over time, learn from limited data, transfer knowledge across tasks, and efficiently adapt to novel domains.

The review investigated various IL approaches in LLMs that have demonstrated promising results. Nonetheless, open problems and challenges remain, calling for further investigation. Periodic batch updates of LLMs do not reflect the 'Real' real-time updates to the models.

A key focus is on unifying various Al fields beyond LLMs. We can produce foundational advancements in IL by integrating and collaborating across these fields. These advancements will enable more versatile algorithms that learn multiple tasks, generalize across domains, and continuously accumulate knowledge.

This paper is a step forward in encouraging research in this direction. By examining the current state of IL in LLMs and highlighting the challenges and opportunities, we aim to inspire researchers and practitioners to explore synergies among AI research fields and contribute to the progress of IL in solving complex, real-world problems.

## 5. References

[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., et al., (2017). Attention Is All You Need. Advances in Neural Information Processing Systems, 30.

[2] Cui, C., Ma, Y., Cao, X., Ye, W., Zhou, Y., et al., (2024). A Survey on Multimodal Large Language Models for Autonomous Driving. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 958-979.

[3] Raiaan, M. A. K., Mukta, M. S. H., Fatema, K., Fahad, N. M., et al., (2024). A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges. IEEE Access.

[4] Wei, J., Bosma, M., Zhao, V. Y., Guu, et al., (2021). Finetuned Language Models Are Zeroshot Learners. arXiv preprint arXiv:2109.01652.

[5] Lester, B., Al-Rfou, R., \& Constant, N. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 3045-3059.

[6] Li, X. L., \& Liang, P. (2021). Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, 4582-4597.

[7] Brown, T., Mann, B., Ryder, N., Subbiah, et al., (2020). Language Models Are Few-shot Learners. Advances in neural information processing systems, 33, 1877-1901.

[8] White, J., Fu, Q., Hays, S., Sandborn, M., et al., (2023). A Prompt Pattern Catalog To Enhance Prompt Engineering With Chatgpt. arXiv preprint arXiv:2302.11382.

[9] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., \& Zhou, D. (2022). Chain Of Thought Prompting Elicits Reasoning In Large Language Models. arXiv preprint arXiv:2201.11903.

[10] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., \& Narasimhan, K. (2023). Tree of Thoughts: Deliberate Problem Solving With Large Language Models. arXiv preprint arXiv:2305.10601.

[11] Ouyang, L., Wu, J., Jiang, X., Almeida, D., et al., (2022). Training Language Models To Follow Instructions With Human Feedback. Advances In Neural Information Processing Systems, 35, 27730-27744.

[12] Casper, S., Davies, X., Shi, C., Gilbert, T. K., et al., (2023). Open Problems And Fundamental Limitations Of Reinforcement Learning From Human Feedback. arXiv preprint arXiv:2307.15217.

[13] Lewis, P., Perez, E., Piktus, A., Petroni, F., et al., (2020). Retrieval-Augmented Generation For Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems, 33, 9459-9474.

[14] Gao, Y., Xiong, Y., Gao, X., Jia, K., et al., (2023). Retrieval-Augmented Generation For Large Language Models: A Survey. arXiv preprint arXiv:2312.10997.

[15] Jing, Z., Su, Y., Han, Y., Yuan, B., et al., (2024). When Large Language Models Meet Vector Databases: A Survey. arXiv preprint arXiv:2402.01763.

[16] Guan, L., Valmeekam, K., Sreedharan, S., \& Kambhampati, S. (2023). Leveraging PreTrained Large Language Models To Construct And Utilize World Models For Model-Based Task Planning. Advances in Neural Information Processing Systems, 36, 79081-79094.

[17] Kambhampati, S., Valmeekam, K., Guan, L., Stechly, K., et al., (2024). LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks. arXiv preprint arXiv:2402.01817.

[18] Stechly, K., Valmeekam, K., \& Kambhampati, S. (2024). On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks. arXiv preprint arXiv:2402.08115.

[19] Wu, Y., Chen, Y., Wang, L., Ye, Y., Liu, Z., Guo, Y., \& Fu, Y. (2019). Large Scale Incremental Learning. In Proceedings of The IEEE/CVF Conference on Computer Vision and Pattern Recognition, 374-382.

[20] Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., et al., (2019). Continual Lifelong Learning With Neural Networks: A Review. Neural Networks, 113, 54-71.

[21] Ramasesh, V. V., Dyer, E., \& Raghu, M. (2020). Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics. In International Conference on Learning Representations.

[22] Wang, L., Zhang, X., Su, H., \& Zhu, J. (2024). A Comprehensive Survey Of Continual Learning: Theory, Method And Application. IEEE Transactions on Pattern Analysis and Machine Intelligence.

[23] Van de Ven, G. M., Tuytelaars, T., \& Tolias, A. S. (2022). Three Types of Incremental Learning. Nature Machine Intelligence, 4(12), 1185-1197.

[24] Khan, M. G. Z. A., Naeem, M. F., Van Gool, L., Stricker, D., et al., (2023). Introducing Language Guidance In Prompt-Based Continual Learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 11463-11473.

[25] Wang, L., Zhang, X., Su, H., \& Zhu, J. (2024). A Comprehensive Survey of Continual Learning: Theory, Method and Application. IEEE Transactions on Pattern Analysis \& Machine Intelligence, (01), 1-20.

[26] Zhou, D. W., Sun, H. L., Ning, J., Ye, H. J., \& Zhan, D. C. (2024). Continual Learning with Pre-Trained Models: A Survey. arXiv preprint arXiv:2401.16386.

[27] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, et al., (2017). Overcoming Catastrophic Forgetting In Neural Networks. Proceedings Of The National Academy Of Sciences, 114(13), 3521-3526.

[28] Zhang, J., Zhang, J., Ghosh, S., Li, D., et al., (2020). Class-Incremental Learning Via Deep Model Consolidation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, 1131-1140.

[29] Wang, L., Zhang, X., Li, Q., Zhang, M., et. al., (2023). Incorporating Neuro-Inspired Adaptability For Continual Learning In Artificial Intelligence. Nature Machine Intelligence, 5(12), 1356-1368.

[30] Gou, J., Yu, B., Maybank, S. J., \& Tao, D. (2021). Knowledge Distillation: A Survey. International Journal of Computer Vision, 129(6), 1789-1819.

[31] Liu, H., \& Liu, H. (2022). Continual Learning with Recursive Gradient Optimization. In International Conference on Learning Representations.

[32] Wu, Y., Chi, Z., Wang, Y., \& Feng, S. (2023). MetaGCD: Learning To Continually Learn In Generalized Category Discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1655-1665.

[33] Gupta, K., Thérien, B., Ibrahim, A., Richter, M. L., et al., (2023). Continual Pre-Training of Large Language Models: How to re-warm your model?. In Workshop on Efficient Systems for Foundation Models@ ICML2023.

[34] Gu, X., Liu, L., Yu, H., Li, J., Chen, C., \& Han, J. (2021). On the Transformer Growth for Progressive BERT Training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, $5174-5180$.

[35] Qin, Q., Hu, W., Peng, H., Zhao, D., \& Liu, B. (2021). BNS: Building Network Structures Dynamically For Continual Learning. Advances in Neural Information Processing Systems, 34, 20608-20620.

[36] Mehta, N., Liang, K., Verma, V. K., \& Carin, L. (2021). Continual Learning Using A Bayesian Nonparametric Dictionary Of Weight Factors. In International Conference on Artificial Intelligence and Statistics, 100-108.

[37] Jia, M., Tang, L., Chen, B. C., Cardie, C., et al., (2022). Visual Prompt Tuning. In European Conference on Computer Vision, 709-727.

[38] Zhao, K., Xu, H., Yang, J., \& Gao, K. (2022). Consistent Representation Learning for Continual Relation Extraction. In Findings of the Association for Computational Linguistics: ACL 2022, 3402-3411.

[39] Boschini, M., Bonicelli, L., Buzzega, P., Porrello, A., \& Calderara, S. (2022). ClassIncremental Continual Learning Into the eXtended DER-Verse. IEEE Transactions on Pattern Analysis And Machine Intelligence, 45(5), 5497-5512.

[40] Wang, L., Lei, B., Li, Q., Su, H., Zhu, J., \& Zhong, Y. (2021). Triple-Memory Networks: A Brain-Inspired Method For Continual Learning. IEEE Transactions on Neural Networks and Learning Systems, 33(5), 1925-1934.

[41] Petit, G., Popescu, A., Schindler, H., Picard, D., \& Delezoide, B. (2023). Fetril: Feature Translation For Exemplar-Free Class-Incremental Learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 3911-3920.

[42] Wu, T., Luo, L., Li, Y. F., Pan, S., et al., (2024). Continual Learning for Large Language Models: A Survey. arXiv preprint arXiv:2402.01364.

[43] Jin, X., Zhang, D., Zhu, H., Xiao, W., et al., (2022). Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 4764-4780.

[44] Zhang, Z., Fang, M., Chen, L., \& Rad, M. R. N. (2023). CITB: A Benchmark for Continual Instruction Tuning. In The 2023 Conference on Empirical Methods in Natural Language Processing.

[45] Zhang, H., Gui, L., Zhai, Y., Wang, H., et al., (2024). COPR: Continual Learning Human Preference Through Optimal Policy Fitting. arXiv preprint arXiv:2310.15694.

[46] Li, C. A., \& Lee, H. Y. (2024). Examining Forgetting in Continual Pre-training of Aligned Large Language Models. arXiv preprint arXiv:2401.03129.

[47] Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., et al., (2021). LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations.

[48] Zhou, D. W., Sun, H. L., Ye, H. J., \& Zhan, D. C. (2024). Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning. arXiv preprint arXiv:2403.12030.

[49] Liang, J., Wang, Z., Ma, Z., Li, J., et al., (2024). Online Training of Large Language Models: Learn while Chatting. arXiv preprint arXiv:2403.04790.

[50] Meng, Z., Zhang, J., Yang, C., Zhan, Z., et al., (2024). DiffClass: Diffusion-Based Class Incremental Learning. arXiv preprint arXiv:2403.05016.

[51] Nichol, A. Q., \& Dhariwal, P. (2021). Improved Denoising Diffusion Probabilistic Models. In International Conference on Machine Learning, 8162-8171.

[52] Han, J., Na, J., \& Hwang, W. (2024). Semantic Prompting with Image-Token for Continual Learning. arXiv preprint arXiv:2403.11537.

[53] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., et al., (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations.

[54] Guo, H., Zhu, F., Liu, W., Zhang, X. Y., et al., (2024). Federated Class-Incremental Learning with Prototype Guided Transformer. arXiv preprint arXiv:2401.02094.

[55] Dong, J., Wang, L., Fang, Z., Sun, G., et al.C (2022). Federated Class-Incremental Learning. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10164-10173.

[56] Dong, J., Li, H., Cong, Y., Sun, G., et al., (2023). One Left Behind: Real-World Federated Class-Incremental Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence.

[57] Tian, S., Li, L., Li, W., Ran, H., et al., (2024). A Survey on Few-Shot Class-Incremental Learning. Neural Networks, 169, 307-324.

[58] Liu, C., Wang, Z., Xiong, T., Chen, R., et al., (2024). Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt. arXiv preprint arXiv:2403.09857.

[59] Zhu, D., Chen, J., Shen, X., Li, X., \& Elhoseiny, M. (2023). MiniGPT-4: Enhancing VisionLanguage Understanding with Advanced Large Language Models. In The Twelfth International Conference on Learning Representations.

[60] Chiang, W. L., Li, Z., Lin, Z., Sheng, Y., et al., (2023). Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \%{ }^{*}$ ChatGPT Quality. https://vicuna.Imsys.org (Accessed 20 April 2024).

[61] Fang, Y., Wang, W., Xie, B., Sun, Q., et al., (2023). EVA: Exploring the Limits of Masked Visual Representation Learning at Scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 19358-19369.

[62] Cao, X., Lu, H., Huang, L., Liu, X., et al., (2024). Generative Multi-modal Models are Good Class-Incremental Learners. arXiv preprint arXiv:2403.18383.

[63] Li, J., Li, D., Savarese, S., \& Hoi, S. (2023). BLIP-2: Bootstrapping Language-Image Pretraining with Frozen Image Encoders and Large Language Models. In International Conference on Machine Learning, 19730-19742.

[64] Wang, Z., Qu, X., Xiao, J., Chen, B., et al., (2024). INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free Class-incremental Learning. arXiv preprint arXiv:2401.11667.

[65] Hendrycks, D., Basart, S., Mu, N., Kadavath, S., et al., (2021). The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. In Proceedings of the IEEE/CVF international conference on computer vision, 8340-8349.

[66] Wang, Y., Liu, Y., Shi, C., Li, H., et al., (2024). InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions. arXiv preprint arXiv:2403.11435.

[67] Qi, B., Chen, X., Gao, J., Liu, J., et al., (2024). Interactive Continual Learning: Fast and Slow Thinking. arXiv preprint arXiv:2403.02628.

[68] Kumaran, D., Hassabis, D., \& McClelland, J. L. (2016). What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated. Trends in Cognitive Sciences, 20(7), 512-534.

[69] Zhu, D., Chen, J., Shen, X., Li, X., et al., (2024). MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. In The Twelfth International Conference on Learning Representations.

[70] Deng, J., Zhang, H., Ding, K., Hu, J., et al., (2024). Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection. arXiv preprint arXiv:2403.01680.

[71] Liu, Z., Lin, Y., Cao, Y., Hu, H., et al., (2021). Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 10012-10022.

[72] Liu, Y., Schiele, B., Vedaldi, A., \& Rupprecht, C. (2023). Continual Detection Transformer for Incremental Object Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 23799-23808.

[73] Dong, N., Zhang, Y., Ding, M., \& Lee, G. H. (2023). Incremental-DETR: Incremental FewShot Object Detection via Self-Supervised Learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 37(1), 543-551.

[74] Zhang, N., Yao, Y., Tian, B., Wang, P., et al., (2024). A Comprehensive Study of Knowledge Editing for Large Language Models. arXiv preprint arXiv:2401.01286.

[75] Yu, L., Chen, Q., Zhou, J., \& He, L. (2024). MELO: Enhancing Model Editing With NeuronIndexed Dynamic Lora. In Proceedings of the AAAI Conference on Artificial Intelligence, $19449-19457$.

[76] Zhong, Z., Wu, Z., Manning, C. D., Potts, C., et al., (2023). MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 15686-15702.

[77] Mitchell, E., Lin, C., Bosselut, A., Finn, C., \& Manning, C. D. (2022). Fast Model Editing at Scale. In International Conference on Learning Representations.

[78] Ma, J. Y., Gu, J. C., Ling, Z. H., Liu, Q., \& Liu, C. (2023). Untying the Reversal Curse Via Bidirectional Language Model Editing. arXiv preprint arXiv:2310.10322.

[79] Pinter, Y., \& Elhadad, M. (2023). Emptying the Ocean with a Spoon: Should We Edit Models?. In the 2023 Conference on Empirical Methods in Natural Language Processing.

[80] Weyssow, M., Zhou, X., Kim, K., Lo, D., \& Sahraoui, H. (2023). On The Usage Of Continual Learning For Out-of-distribution Generalization In Pre-trained Language Models Of Code. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 1470-1482.

[81] Scialom, T., Chakrabarty, T., \& Muresan, S. (2022). Fine-Tuned Language Models Are Continual Learners. arXiv preprint arXiv:2205.12393.

[82] Sanh, V., Webson, A., Raffel, C., Bach, S. H., et al., (2021). Multitask Prompted Training Enables Zero-Shot Task Generalization. arXiv preprint arXiv:2110.08207.

[83] Razdaibiedina, A., Mao, Y., Hou, R., Khabsa, M., et al., (2023). Progressive Prompts: Continual Learning For Language Models. arXiv preprint arXiv:2301.12314.

[84] Le Scao, T., Fan, A., Akiki, C., Pavlick, E., et al., (2022). BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.

[85] Cahyawijaya, S., Lovenia, H., \& Fung, P. (2024). LLMs Are Few-Shot In-Context LowResource Language Learners. arXiv preprint arXiv:2403.16512.

[86] Lin, X. V., Mihaylov, T., Artetxe, M., Wang, T., et al., (2021). Few-shot Learning with Multilingual Generative Language Models. arXiv preprint arXiv:2112.10668.

[87] Yin, S., Fu, C., Zhao, S., Li, K., et al., (2023). A Survey on Multimodal Large Language Models. arXiv preprint arXiv:2306.13549.

[88] Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., et al., (2023). A Survey on In-Context Learning. arXiv preprint arXiv:2301.00234.

[89] Wei, J., Wang, X., Schuurmans, D., Bosma, et al., (2022). Chain-Of-Thought Prompting Elicits Reasoning in Large Language Models. Advances in Neural Information Processing Systems, 35, 24824-24837.

[90] Valmeekam, K., Marquez, M., Sreedharan, S., \& Kambhampati, S. (2024). On The Planning Abilities Of Large Language Models - A Critical Investigation. Advances in Neural Information Processing Systems, 36.

[91] Zhai, Y., Tong, S., Li, X., Cai, M., et al., (2023). Investigating the Catastrophic Forgetting in Multimodal Large Language Models. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following.

[92] Liu, H., Li, C., Wu, Q., \& Lee, Y. J. (2023). Visual Instruction Tuning. Advances in Neural Information Processing Systems, 36.

[93] Li, B., Zhang, Y., Chen, L., Wang, J., et al., (2023). MIMIC-IT: Multi-Modal In-Context Instruction Tuning. arXiv preprint arXiv:2306.05425.

[94] Dai, W., Li, J., Li, D., Tiong, A. M. H., et al., (2023). InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. Advances in Neural Information Processing Systems, 36 .

[95] Berrios, W., Mittal, G., Thrush, T., Kiela, D., \& Singh, A. (2023). Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language. arXiv preprint arXiv:2306.16410.

[96] Liu, H., Li, C., Li, Y., \& Lee, Y. J. (2023). Improved Baselines with Visual Instruction Tuning. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following.

[97] Zhu, D., Sun, Z., Li, Z., Shen, T., et al., (2024). Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models. arXiv preprint arXiv:2402.12048.

[98] Gao, T., Yao, X., \& Chen, D. (2021). SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Conference on Empirical Methods in Natural Language Processing, 68946910.

[99] Li, J., Zhou, P., Xiong, C., \& Hoi, S. (2021). Prototypical Contrastive Learning of Unsupervised Representations. In International Conference on Learning Representations.

[100] Thengane, V., Khan, S., Hayat, M., \& Khan, F. (2022). CLIP Model Is An Efficient Continual Learner. arXiv preprint arXiv:2210.03114.

[101] Zheng, Z., Ma, M., Wang, K., Qin, Z., et al., (2023). Preventing Zero-Shot Transfer Degradation In Continual Learning Of Vision-Language Models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 19125-19136.

[102] Zheng, J., Qiu, S., \& Ma, Q. (2023). Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models. arXiv preprint arXiv:2312.07887.

[103] Zhou, D. W., Wang, Q. W., Qi, Z. H., Ye, H. J., Zhan, D. C., \& Liu, Z. (2023). Deep Classincremental Learning: A Survey. arXiv preprint arXiv:2302.03648.

[104] Zhou, D. W., Zhang, Y., Ning, J., Ye, H. J., et al., (2023). Learning Without Forgetting for Vision-Language Models. arXiv preprint arXiv:2305.19270.

[105] Li, Y., Pang, G., Suo, W., Jing, C., et al., (2024). CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning. arXiv preprint arXiv:2403.10245.

[106] Zhang, Y., Feng, F., Zhang, J., Bao, K., et al., (2023). CoLLM: Integrating Collaborative Embeddings into Large Language Models for Recommendation. arXiv preprint arXiv:2310.19488.

[107] Jovanović, M., \& Campbell, M. (2023). Connecting Al: Merging Large Language Models and Knowledge Graph. Computer, 56(11), 103-108.

[108] Garcez, A. D. A., \& Lamb, L. C. (2023). Neurosymbolic Al: The 3rd Wave. Artificial Intelligence Review, 56(11), 12387-12406.

[109] Wang, X., Gao, T., Zhu, Z., Zhang, Z., Liu, Z., Li, J., \& Tang, J. (2021). KEPLER: A Unified Model For Knowledge Embedding And Pre-Trained Language Representation. Transactions of the Association for Computational Linguistics, 9, 176-194.

[110] Sheth, A., Roy, K., \& Gaur, M. (2023). Neurosymbolic Artificial Intelligence (Why, What, And How). IEEE Intelligent Systems, 38(3), 56-62.

[111] Du, M., Luu, A. T., Ji, B., \& Ng, S. K. (2023). From Static to Dynamic: A Continual Learning Framework for Large Language Models. arXiv preprint arXiv:2310.14248.

[112] Bärmann, L., Kartmann, R., Peller-Konrad, F., Waibel, A., et al., (2023). Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models. In 2nd Workshop on Language and Robot Learning: Language as Grounding.

[113] Li, Z., Yu, K., Cheng, S., \& Xu, D. (2024). LEAGUE++: Empowering Continual Robot Learning Through Guided Skill Acquisition With Large Language Models. In ICLR Workshop on Large Language Model (LLM) Agents.

[114] Cui, C., Ma, Y., Cao, X., Ye, W., et al., (2023). Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles. arXiv preprint arXiv:2310.08034.

[115] Birr, T., Pohl, C., Younes, A., \& Asfour, T. (2024). AutoGPT+ P: Affordance-based Task Planning with Large Language Models. arXiv preprint arXiv:2402.10778.

[116] Zhang, H., Gui, L., Zhai, Y., Wang, H., et al., (2024). COPR: Continual Learning Human Preference through Optimal Policy Fitting. arXiv preprint arXiv:2310.15694.

[117] Wu, C., Gan, Y., Ge, Y., Lu, Z., et al., (2024). LLAMA PRO: Progressive LLaMA with Block Expansion. arXiv preprint arXiv:2401.02415.

[118] Jin, H., Han, X., Yang, J., Jiang, Z., et al., (2024). LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning. arXiv preprint arXiv:2401.01325.

[119] Touvron, H., Martin, L., Stone, K., Albert, P., et al., (2023). LLaMA 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288.

[120] Javaheripi, M., Bubeck, S., Abdin, M., Aneja, J., et al., (2023). Phi-2: The Surprising Power of Small Language Models. Microsoft Research Blog.

[121] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., et al., (2023). Mistral 7B. arXiv preprint arXiv:2310.06825.

[122] Kim, D., Park, C., Kim, S., Lee, W., et al., (2023). SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. arXiv preprint arXiv:2312.15166.

[123] Huang, J., Cui, L., Wang, A., Yang, C., et. al., (2024). Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal. arXiv preprint arXiv:2403.01244.

[124] Touvron, H., Lavril, T., Izacard, G., Martinet, X., et al., (2023). LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971.

[125] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., et al., (2023). Stanford Alpaca: An Instructionfollowing LLaMA Model. https://github.com/tatsu-lab/stanford_alpaca (Accessed 20 April 2024).

[126] Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., et al., (2022). SuperNaturallnstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 5085-5109.

[127] Sun, T., Zhang, X., He, Z., Li, P., et al., (2023). MOSS: Training Conversational Language Models from Synthetic Data. arXiv preprint arXiv:2307.15020, 7.

[128] Peng, B., Tian, Z., Liu, S., Yang, M., et al., (2024). Scalable Language Model with Generalized Continual Learning. arXiv preprint arXiv:2404.07470.

[129] Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 4171-4186.

[130] Raffel, C., Shazeer, N., Roberts, A., Lee, K., et al., (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140), 1-67.

[131] Ibrahim, A., Thérien, B., Gupta, K., Richter, M. L., et al., (2024). Simple and Scalable Strategies to Continually Pre-train Large Language Models. arXiv preprint arXiv:2403.08763.

[132] Andonian, A., Anthony, Q., Biderman, S., Black, S., et al., (2021). GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch. https://github.com/eleutherai/gpt-neox (Accessed 20 April 2024).

[133] Vettoruzzo, A., Bouguelia, M. R., Vanschoren, J., Rognvaldsson, T., \& Santosh, K. C. (2024). Advances and Challenges in Meta-Learning: A Technical Review. IEEE Transactions on Pattern Analysis and Machine Intelligence.

[134] Hospedales, T., Antoniou, A., Micaelli, P., \& Storkey, A. (2022). Meta-Learning in Neural Networks: A Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9), 5149-5169.

[135] FiWang, Y., Ji, Y., Wang, W., \& Wang, B. (2024). Bi-channel Attention Meta Learning for Few-shot Fine-grained Image Recognition. Expert Systems with Applications, 242, 122741.

[136] Liang, B., Li, X., Gui, L., Fu, Y., He, Y., Yang, M., \& Xu, R. (2023). Few-shot Aspect Category Sentiment Analysis Via Meta-learning. ACM Transactions on Information Systems, 41(1), 1-31.

[137] Sherborne, T., \& Lapata, M. (2023). Meta-learning a Cross-lingual Manifold for Semantic Parsing. Transactions of the Association for Computational Linguistics, 11, 49-67.

[138] Lee, H. Y., Li, S. W., \& Vu, T. (2022). Meta Learning for Natural Language Processing: A Survey. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 666-684.

[139] Xu, S., Li, Y., Lin, M., Gao, P., et al., (2023). Q-DETR: An Efficient Low-Bit Quantized Detection Transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3842-3851.

[140] Qin, X., Song, X., \& Jiang, S. (2023). Bi-Level Meta-Learning For Few-Shot Domain Generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15900-15910.

[141] Zhao, Y., Zhang, T., Li, J., \& Tian, Y. (2023). Dual Adaptive Representation Alignment for Cross-Domain Few-Shot Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence.

[142] Zhang, Y., \& Yang, Q. (2021). A Survey on Multi-Task Learning. IEEE Transactions on Knowledge and Data Engineering, 34(12), 5586-5609.

[143] Wang, J., Zheng, Y., Ma, J., Li, X., et al., (2023). Information Bottleneck-Based Interpretable Multitask Network For Breast Cancer Classification And Segmentation. Medical Image Analysis, 83, 102687.

[144] Rathnayake, H., Sumanapala, J., Rukshani, R., \& Ranathunga, S. (2024). AdapterFusionBased Multi-Task Learning for Code-Mixed and Code-Switched Text Classification. Engineering Applications of Artificial Intelligence, 127, 107239.

[145] Chowdhery, A., Narang, S., Devlin, J., et al., (2023). PaLM: Scaling Language Modeling with Pathways. Journal of Machine Learning Research, 24(240), 1-113.

[146] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., et al., (2022). Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations.

[147] Lee, Y., Chen, A. S., Tajwar, F., et al., (2023). Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. In The Eleventh International Conference on Learning Representations.

[148] Howard, J., \& Ruder, S. (2018). Universal Language Model Fine-tuning for Text Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 328-339.

[149] Jiang, W., Kwok, T. Y., \& Zhang, Y. (2022). Subspace Learning for Effective MetaLearning. Proceedings of Machine Learning Research.

[150] Yang, L., Huang, J., Lin, W., \& Cao, J. (2023). Personalized Federated Learning On Nonlid Data Via Group-Based Meta-Learning. ACM Transactions on Knowledge Discovery from Data, 17(4), 1-20.

[151] Yao, H., Wang, Y., Li, S., Zhang, L., et al., (2022). Improving Out-Of-Distribution Robustness via Selective Augmentation. In International Conference on Machine Learning, 25407-25437.

[152] Lee, D. B., Lee, S., Kawaguchi, K., Kim, Y., et al., (2023). Self-Supervised Set Representation Learning for Unsupervised Meta-Learning. In The Eleventh International Conference on Learning Representations.

[153] Alayrac, J. B., Donahue, J., Luc, P., Miech, A., et al., (2022). Flamingo: A Visual Language Model for Few-Shot Learning. Advances in Neural Information Processing Systems, 35, 2371623736 .

[154] Wu, Z., Pan, S., Chen, F., Long, G., et al., (2020). A Comprehensive Survey on Graph Neural Networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1), 4-24.

[155] Chen, Z., Mao, H., Li, H., Jin, W., et al., (2024). Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. ACM SIGKDD Explorations Newsletter, 25(2), 42-61.

[156] Tan, Y., Lv, H., Huang, X., Zhang, J., et al., (2024). MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining. arXiv preprint arXiv:2403.04780.

[157] Fan, L., Hua, W., Li, X., Zhu, K., et al., (2024). NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models. arXiv preprint arXiv:2403.01777.

[158] Fan, L., Hua, W., Li, L., Ling, H., et al., (2023). NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. arXiv preprint arXiv:2312.14890.

[159] OpenAI. GPT-4V(ision) System Card, (2023). https://cdn.openai.com/papers/GPTV System_Card.pdf (Accessed 20 April 2024).

[160] Team, G., Anil, R., Borgeaud, S., Wu, Y., et al., (2024). Gemini: A Family of Highly Capable Multimodal Models arXiv preprint arXiv:2312.11805.

[161] Wang, W., Lv, Q., Yu, W., Hong, W., et al., (2024). CogVLM: Visual Expert for Pretrained Language Models. arXiv preprint arXiv:2311.03079.

[162] Bai, J., Bai, S., Yang, S., Wang, S., et al., (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv preprint arXiv:2308.12966.

[163] Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., \& Wei, F. (2023). Kosmos-2: Grounding Multimodal Large Language Models to the World. arXiv preprint arXiv:2306.14824.

[164] Ding, N., Qin, Y., Yang, G., Wei, F., et al., (2022). Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904.

[165] Han, Z., Gao, C., Liu, J., \& Zhang, S. Q. (2024). Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey. arXiv preprint arXiv:2403.14608.

[166] Gao, T., Fisch, A., \& Chen, D. (2021). Making Pre-trained Language Models Better Fewshot Learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 3816-3830.

[167] Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., et al., (202). AdapterFusion: Non-Destructive Task Composition for Transfer Learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, 487-503.

[168] Lei, T., Bai, J., Brahma, S., Ainslie, J., et al., (2023). Conditional Adapters: Parameterefficient Transfer Learning with Fast Inference. In 37th Conference on Neural Information Processing Systems.

[169] Petrov, A., Torr, P., \& Bibi, A. (2024). When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations. In The Twelfth International Conference on Learning Representations.

[170] Zaken, E. B., Ravfogel, S., \& Goldberg, Y. (2021). BitFit: Simple Parameter-Efficient FineTuning for Transformer-Based Masked Language-Models. arXiv preprint arXiv:2106.10199.

[171] Guo, D., Rush, A. M., \& Kim, Y. (2021). Parameter-Efficient Transfer Learning with Diff Pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, 48844896.

[172] Liu, S. Y., Wang, C. Y., Yin, H., Molchanov, P., et al., (2024). DoRA: Weight-Decomposed Low-Rank Adaptation. arXiv preprint arXiv:2402.09353.

[173] Sun, X., Ji, Y., Ma, B., \& Li, X. (2023). A Comparative Study Between Full-Parameter and Lora-Based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model. arXiv preprint arXiv:2304.08109.

[174] Mao, Y., Mathias, L., Hou, R., Almahairi, A., et al., (2022). UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 6253-6264.

[175] Chen, J., Zhang, A., Shi, X., Li, M., et al., (2023). Parameter-Efficient Fine-Tuning Design Spaces. In The Eleventh International Conference on Learning Representations.

[176] Hu, Z., Wang, L., Lan, Y., Xu, W., et al., (2023). LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. In The 2023 Conference on Empirical Methods in Natural Language Processing.

[177] Song, C., Han, X., Zeng, Z., Li, K., et al., (2023). ConPET: Continual Parameter-efficient Tuning for Large Language Models. arXiv preprint arXiv:2309.14763.

[178] Yu, J., Zhuge, Y., Zhang, L., Wang, D., et al., (2024). Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters. arXiv preprint arXiv:2403.11549.

[179] Lian, D., Zhou, D., Feng, J., \& Wang, X. (2022). Scaling \& Shifting Your Features: A New Baseline for Efficient Model Tuning. Advances in Neural Information Processing Systems, 35, 109-123.

[180] Wu, Z., Arora, A., Wang, Z., Geiger, et al., (2024). ReFT: Representation Finetuning for Language Models. arXiv preprint arXiv:2404.03592.

[181] Geiger, A., Wu, Z., Potts, C., Icard, T., \& Goodman, N. (2024). Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations. In Causal Learning and Reasoning, 160-187.

[182] Liu, Y., Ott, M., Goyal, N., Du, J., et al., (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692.

[183] Zhao, W., Wang, S., Hu, Y., Zhao, Y., et al., (2024). SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models. arXiv preprint arXiv:2401.08295.

[184] Razdaibiedina, A., Mao, Y., Hou, R., Khabsa, M., et al., (2023). Progressive Prompts: Continual Learning for Language Models. In The Eleventh International Conference on Learning Representations.

[185] Gururangan, S., Lewis, M., Holtzman, A., Smith, N. A., \& Zettlemoyer, L. (2022). DEMix Layers: Disentangling Domains for Modular Language Modeling. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 5557-5576.

[186] Shen, S., Hou, L., Zhou, Y., Du, et al., (2023). Mixture-Of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models. arXiv preprint arXiv:2305.14705.

[187] Zadouri, T., Üstün, A., Ahmadian, A., Ermiş, B., et al., (2023). Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning. arXiv preprint arXiv:2309.05444.

[188] Feng, W., Hao, C., Zhang, Y., Han, Y., \& Wang, H. (2024). Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models. arXiv preprint arXiv:2403.03432.

[189] Jiang, R., Liu, L., \& Chen, C. (2024). MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts. arXiv preprint arXiv:2403.10568.

[190] Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., et al., (2024). Mixtral of Experts. arXiv preprint arXiv:2401.04088.

[191] Cao, X., Lu, H., Huang, L., Liu, X., \& Cheng, M. M. (2024). Generative Multi-modal Models are Good Class-Incremental Learners. arXiv preprint arXiv:2403.18383.

[192] Harun, M. Y., Gallardo, J., Hayes, T. L., \& Kanan, C. (2023). How Efficient Are Today's Continual Learning Algorithms?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2430-2435.

[193] Truong, T. D., Nguyen, H. Q., Raj, B., \& Luu, K. (2024). Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments. Advances in Neural Information Processing Systems, 36.

[194] Yıldız, Ç., Ravichandran, N. K., Punia, P., Bethge, M., et al., (2024). Investigating Continual Pretraining in Large Language Models: Insights and Implications.arXiv preprint arXiv:2402.17400.

[195] Zou, A., Wang, Z., Kolter, J. Z., \& Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv preprint arXiv:2307.15043.

[196] Song, Y., Wang, T., Cai, P., Mondal, S. K., \& Sahoo, J. P. (2023). A Comprehensive Survey of Few-Shot Learning: Evolution, Applications, Challenges, and Opportunities. ACM Computing Surveys, 55(13), 1-40.

[197] Cahyawijaya, S., Lovenia, H., \& Fung, P. (2024). LLMs Are Few-Shot In-Context LowResource Language Learners. arXiv preprint arXiv:2403.16512.

[198] Liu, Z., Wang, J., Dao, T., Zhou, T., et al., (2023). Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. In International Conference on Machine Learning, 22137-22176.

[199] Zhu, X., Li, J., Liu, Y., Ma, C., \& Wang, W. (2023). A Survey on Model Compression for Large Language Models. arXiv preprint arXiv:2308.07633.

[200] Jin, H., Han, X., Yang, J., Jiang, Z., et al., (2024). LLM Maybe LongLM: Self-extend LLM Context Window Without Tuning. arXiv preprint arXiv:2401.01325.

[201] Onoe, Y., Zhang, M., Choi, E., \& Durrett, G. (2022). Entity Cloze By Date: What LMs Know About Unseen Entities. In Findings of the Association for Computational Linguistics, 693-702.

[202] Petrov, A., Torr, P., \& Bibi, A. (2024). When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations. In The Twelfth International Conference on Learning Representations.

[203] Yin, Z., Sun, Q., Guo, Q., Wu, J., Qiu, X., \& Huang, X. J. (2023, July). Do Large Language Models Know What They Don't Know?. In Findings of the Association for Computational Linguistics: ACL 2023, 8653-8665.

[204] Zhou, H., Wan, X., Vulić, I., \& Korhonen, A. (2024). AUTOPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. arXiv preprint arXiv:2301.12132.

[205] Li, Z., Zhang, N., Yao, Y., Wang, M., et al., (2024). Unveiling the Pitfalls of Knowledge Editing for Large Language Models. In The Twelfth International Conference on Learning Representations.

[206] Cohen, R., Biran, E., Yoran, O., Globerson, A., \& Geva, M. (2024). Evaluating The Ripple Effects of Knowledge Editing in Language Models. Transactions of the Association for Computational Linguistics, 12, 283-298.

[207] Pan, S., Luo, L., Wang, Y., Chen, C., et al., (2024). Unifying Large Language Models and Knowledge Graphs: A Roadmap. IEEE Transactions on Knowledge and Data Engineering.

[208] Berglund, L., Tong, M., Kaufmann, M., Balesni, M., et al., (2023). The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". In The Twelfth International Conference on Learning Representations.

[209] Ovadia, O., Brief, M., Mishaeli, M., \& Elisha, O. (2023). Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLM. arXiv preprint arXiv:2312.05934.

[210] Shanahan, M. (2024). Talking about Large Language Models. Communications of the ACM, 67(2), 68-79.

## 6. Abbreviations

FM - Foundation Model

NLP - Natural Language Processing

NLU - Natural Language Understanding

NN - Neural Network

ML - Machine Learning

CL - Continual Learning

CoT - Chain of Thought

LLM - Large Language Model

MLLM - Multimodal Large Language Model

MIT - Multimodal Instruction Tuning

MI-CL - Multimodal In-Context Learning

MCoT - Multimodal Chain of Thought

$\mathrm{NL}$ - Natural Language

KG - Knowledge Graph

PET - Parameter-efficient fine-tuning

MoAs - Mixture-of-LoRAs

DEMix - Domain expert mixture

FFN - Feedforward Network

LLaVA - Large Language and Vision Assistant

VQA - Visual question answering

LGCL - Language Guidance for Prompt-based Continual Learning

PLM - Pre-trained language model

SEQ - Sequential fine-tuning

IL - Incremental Learning

CIL - Class-incremental Learning

VLM - Vision-Language Models

OOD - Out-of-distribution

RAG - Retrieval-augmented generation

ConPET - Continual Parameter-Efficient Tuning

PROOF - PROjectiOn Fusion

CLIP - Contrastive Language-Image Pre-training

ZSCL - Zero-Shot Continual Learning

MTIL - Multi-domain Task Incremental Learning

CoL - Contrastive Learning

KE - Knowledge Editing

MEND - Model Editor Networks using Gradient Decomposition

KD - Knowledge Distillation

MTL - Multitask Learning

TL - Transfer Learning

GD - Gradient Descent

MeL - Meta-Learning

OAM - Object-Affordance Mapping

OAPT - Object-Affordance-Pair

DDAS - Distribution Discriminative Auto-Selector

TIL - Task-Incremental Learning

OCDL - Open-Domain Continual Learning

CPT - Continual Pre-training

CIT - Continual Instruction Tuning

CA - Continual Alignment

VPT - Visual Prompt Tuning

CF - Catastrophic Forgetting

COPR - Continual Optimal Policy Regularization

DiffClass - Diffusion-based Class Incremental Learning

DM - Diffusion Model

MDM - Multi-Distribution Matching

SSIA - Selective Synthetic Image Augmentation

EASE - ExpAndable Subspace Ensemble

LoRA - Low-Rank Adaptation

DoRA - Weight-Decomposed LowRank Adaptation

PLoRA - Prototype and Low-Rank Adaptation

FCIL - Federated Class-Incremental Learning

FL - Federated Learning

FSCIL - Few-Shot Class-Incremental Learning

ASP - Attention-aware Self-adaptive Prompt

TIP - Task-Invariant Prompts

TSP - Task-Specific Prompts

GMM - Generative Multi-modal Model

BLIP - Bootstrapping Language-Image Pre-training

INCPrompt - INCremental Prompting

InsCL - Instruction-based Continual Learning

WD - Wasserstein Distance

InsInfo - Instruction Information

ICL - Interactive Continual Learning

ViT - Vision Transformer

CKT-MHA - Class-Knowledge-Task Multi-Head Attention

vMF-ODI - von Mises-Fisher Outlier Detection and Interaction

EMT - Evaluating MulTimodality

DRL - Deep Reinforcement Learning

TAMP - Task and Motion Planning

$\mathrm{I}-\mathrm{CL}$ - In-Context Learning

X-ICL - Cross-lingual In-Context Learning

ME - Model Editing

MELO - Model Editing with neuron-indexed dynamic LOw-rank adapter

VD - Vector Database

SSR - Self-Synthesized Rehearsal

SuperNI - Super-Natural Instructions

GNN - Graph Neural Network

MuseGraph - Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining

NP-problem - Nondeterministic Polynomial time problem

MSA - Multi-head Self-Attention

DNN - Deep Neural Network

CoDA - Conditional Adapter

ReFT - Representation Finetuning

LoReFT - Low-rank Linear Subspace ReFT

DAS - Distributed Alignment Search

SSF - Scale and Shift the deep Features

SAPT - Shared Attention Framework for Parameter-efficient conTinual learning

SALS - Shared Attentive Learning \& Selection module

ARM - Attentive Reflection Module

SLM - Scalable Language Model

JARe - Joint Adaptive Re-parameterization

DTKR - Dynamic Task-related Knowledge Retrieval

LR - Learning Rate

MoPE - Mixture of Prompt Experts

IVLOD - Incremental Vision-Language Object Detection

VLODM - Vision-Language Object Detection Model

ZiRa - Zero-interference Reparameterizable Adaptation

RDB - Reparameterizable Dual Branch

ZiL - Zero-interference Loss

CL-DETR - ContinuaL DEtection TRansformer

iDETR - Incremental DEtection TRansformer


[^0]:    ${ }^{1}$ https://huggingface.co/spaces/Imsys/chatbot-arena-leaderboard (Accessed 20 April 2024).

[^1]:    ${ }^{2}$ https://platform.openai.com/docs/guides/prompt-engineering (Accessed 20 April 2024).

    ${ }^{3}$ https://github.com/microsoft/promptbase (Accessed 20 April 2024).

[^2]:    ${ }^{4}$ https://www.cs.toronto.edu/ kriz/cifar.html (Accessed 20 April 2024).

    5 https://www.kaggle.com/datasets/ambityga/imagenet100 (Accessed 20 April 2024).

[^3]:    ${ }^{6}$ https://www.cs.toronto.edu/ kriz/cifar.html (Accessed 20 April 2024).

[^4]:    ${ }^{7}$ https://grouplens.org/datasets/movielens/1m/ (Accessed 20 April 2024).

    ${ }^{8}$ https://cseweb.ucsd.edu/ jmcauley/datasets.html - amazon_reviews (Accessed 20 April 2024).

[^5]:    ${ }^{9}$ https://openai.com/blog/openai-api (Accessed 20 April 2024).

    ${ }^{10}$ https://huggingface.co/huggyllama/llama-30b (Accessed 20 April 2024).

    ${ }^{11}$ https://huggingface.co/tiiuae/falcon-40b (Accessed 20 April 2024).

[^6]:    12 "True wisdom is knowing what you don't know." - Confucius

