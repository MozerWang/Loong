# AGENTTUNing: EnABLING GENERALIZED AGENT ABILITIES FOR LLMS 

Aohan Zeng ${ }^{\ddagger \S *}$, Mingdao Liu ${ }^{\ddagger *}$, Rui Lu ${ }^{\ddagger *}$, Bowen Wang ${ }^{\ddagger}$, Xiao Liu ${ }^{\ddagger \S}$, Yuxiao Dong ${ }^{\ddagger}$, Jie Tang ${ }^{\ddagger}$<br>${ }^{\ddagger}$ Tsinghua University $\quad{ }^{\text {ZZhipu.AI }}$


#### Abstract

Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing highquality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs' agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct dataset and AgentLM-7B, 13B, and 70B models at https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks.


![](https://cdn.mathpix.com/cropped/2024_06_04_307df45ecf542b35e342g-01.jpg?height=630&width=1396&top_left_y=1547&top_left_x=364)

(a) Overall score in our held-in and held-out tasks.

(b) Closed \& open LLMs on agent tasks (Liu et al., 2023)

Figure 1: (a) AgentLM exhibits superior performance. AgentLM is a series of models fine-tuned on the foundation of Llama 2 chat. Moreover, its generalization capability on held-out tasks is on par with GPT-3.5; (b) This figure is directly re-printed from AgentBench (Liu et al., 2023) with permission. Open LLMs significantly underperforms API-based LLMs.[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_307df45ecf542b35e342g-02.jpg?height=458&width=1393&top_left_y=275&top_left_x=363)

Figure 2: An overview of AgentInstruct and AgentTuning. The construction of AgentInstruct, consisting of instruction generation, trajectory interaction, and trajectory filter. AgentLM is finetuned using a mixture of AgentInstruct and general-domain instructions.

## 1 INTRODUCTION

An agent refers to an entity capable of perceiving its environment, making decisions, and taking actions (Maes, 1994; Wooldridge \& Jennings, 1995). Traditional AI agents have been effective in specialized domains, but often fall short in adaptability and generalization. Through alignment training, large language models (LLMs) (Ouyang et al., 2022; Wei et al., 2022a), initially designed for language tasks, have displayed unprecedented capabilities in instruction following (Ouyang et al., 2022), reasoning (Wei et al., 2022b), planning, and even tool utilization (Schick et al., 2023). These capabilities make LLMs an ideal foundation for advancing AI agents toward broad, versatile functionality. Recent projects such as AutoGPT (Richards, 2023), GPT-Engineer (Osika, 2023), and BabyAGI (Nakajima, 2023) have employed LLMs as the core controllers, building powerful agents capable of solving complex problems in the real world.

However, a recent study (Liu et al., 2023) shows that open LLMs like Llama (Touvron et al., 2023a;b) and Vicuna (Chiang et al., 2023) significantly lag behind in agent capabilities in complex, real-world scenarios when compared to GPT-3.5 and GPT-4 (OpenAI, 2022; 2023) in Figure 1 , though they have performed well in traditional NLP tasks and largely advanced the development of LLMs. The performance gap in agent tasks hampers the advancement of in-depth LLM research and community innovation.

Existing studies on LLMs as agents have thus far largely focused on designing prompts or a framework for completing one particular agent task (Yao et al., 2023; Kim et al., 2023; Deng et al., 2023), rather than fundamentally enhancing the agent capabilities of the LLMs themselves. In addition, many efforts are dedicated to improving LLMs in specific aspects, involving fine-tuning the LLMs using datasets tailored to specific tasks (Deng et al., 2023; Qin et al., 2023). This overemphasis on specialized capabilities comes at the expense of the LLMs' general abilities and also compromises their generalizability.

To fundamentally enable generalized agent abilities for LLMs, we introduce a simple and general approach AgentTuning as shown in Figure 2. AgentTuning consists of two components: a lightweight instruct-tuning dataset AgentInstruct and a hybrid instruction-tuning strategy that enhances the agent's capabilities while preserving its generalization ability. As shown in Table 1, AgentInstruct covers 1,866 verified interaction trajectories with high-quality Chain-of-Thought (CoT) rationale (Wei et al., 2022b) for each decision step from six diverse agent tasks. For each agent task, one interaction trajectory is collected through three phases: instruction construction, trajectory interaction by employing GPT-4 as the agent, and trajectory filtering depending on its reward score. To enhance LLMs' agent capabilities while preserving their general abilities, we experiment with a hybrid instruction-tuning strategy. The idea is to mix AgentInstruct with high-quality and general data at a certain ratio for supervised fine-tuning.

We employ AgentTuning to fine-tune the open Llama 2 series (Touvron et al., 2023b), whose performance on agent tasks are significantly worse that GPT-3.5, resulting in the AgentLM-7B, 13B and 70B models. Our empirical evaluations have the following observations.

Table 1: Overview of our AgentInstruct dataset. AgentInstruct includes 1,866 trajectories from 6 agents tasks. "Inst." stands for instruction, the agent needs to interact with the environment to complete the task specified in the instruction.. "Traj." stands for interaction trajectory. "Filt. Traj.". stands for filtered trajectories. "Task Deri." stands for Task Derivation.

| Task | Inst. From | \# Inst. | \# Filt. <br> Traj. | Avg \# Filt. <br> Traj. Turns | Ratio |
| :--- | :---: | ---: | ---: | ---: | ---: |
| ALFWorld (Shridhar et al., 2020) | Train split | 954 | 336 | 13.52 | $35.2 \%$ |
| WebShop (Yao et al., 2022) | Train split | 1,485 | 351 | 3.68 | $23.6 \%$ |
| Mind2Web (Deng et al., 2023) | Train split | 23,378 | 122 | $1.00^{1}$ | $0.52 \%$ |
| Knowledge Graph (Liu et al., 2023) | Train split | 2,501 | 324 | 6.04 | $13.0 \%$ |
| Operating System (Liu et al., 2023) | Self-Instruct | 647 | 195 | 3.85 | $30.1 \%$ |
| Database (Liu et al., 2023) | Self-Instruct | 1,074 | 178 | 2.13 | $16.6 \%$ |
|  | Task Deri. | 5,302 | 360 | 2.03 | $6.79 \%$ |
| AgentInstruct | - | 35,341 | 1,866 | 5.24 | $5.29 \%$ |

First, AgentLM demonstrates strong performance on both held-in tasks in AgentInstruct and unseen held-out agent tasks, suggesting robust generalization on agent capabilities. It also makes AgentLM70B comparable to GPT-3.5 on unseen agent tasks without compromising its performance on general NLP tasks, such as on MMLU, GSM8K, HumanEval, and MT-Bench.

Second, our analysis on the ratio of agent data with general data suggests that the general capabilities of LLMs are crucial for the generalization of agent tasks. Training solely on agent data, in fact, leads to a decline in generalization performance. This can be explained by the fact that agent tasks demand that LLMs exhibit comprehensive abilities such as planning and reasoning.

Third, our error analysis on Llama 2 and AgentLM shows that AgentTuning significantly reduces instances of basic mistakes such as formatting errors, duplicated generation, and refusal to answer. This suggests that the model inherently possesses the capability to tackle agent tasks, and AgentTuning indeed enables the LLMs' agent abilities rather than causing it to overfit on agent tasks.

AgentTuning represents the very first attempt to instruction-tune LLMs using interaction trajectories across multiple agent tasks. Evaluation results indicate that AgentTuning enables the agent capabilities of LLMs with robust generalization on unseen agent tasks while remaining good on general language abilities. We have open-sourced the AgentInstruct dataset and AgentLM.

## 2 THE AGENTTUNING APPROACH

Given an agent task, the interaction trajectory of the LLM agent can be recorded as a conversation history $\left(u_{1}, a_{1}, \ldots, u_{n}, a_{n}\right)$. Given that the existing dialogue models typically encompass two roles, the user and the model, $u_{i}$ represents the input from the user and $a_{i}$ denotes the response from the model. Each trajectory has a final reward $r \in[0,1]$, reflecting the completion status of the task.

To date, there is no end-to-end attempt to improve the general agent abilities of LLMs. Most existing agent studies focused on either prompting one particular LLM or compiling a LLM-based framework for completing an agent task, such as building a Web agent in WebShop (Yao et al., 2022) and Mind2Web (Deng et al., 2023). According to AgentBench (Liu et al., 2023), all open LLMs are far behind of commercial ones such as GPT-4 and ChatGPT in terms of acting as agents though these models, such as Llama2, have demonstrated strong performance across various benchmarks. The goal of this work is to improve the generalized agent abilities of LLMs while at least maintaining their general LLM capacities such as their performance on MMLU, GSM8K, and HumanEval.

We present AgentTuning to achieve this goal, the first step of which is to build the AgentInstruct dataset that is used in the second step to instruction tune the LLMs. We carefully experiment and design these two steps such that the LLMs obtain good performance in (unseen) generalized agent task types while remaining good in general LLM tasks.[^1]

### 2.1 ConSTRUCTING AGENTINSTRUCT

Language instructions have been widely collected and used to tune pre-trained LLMs for better instruction-following capacity, such as FLAN (Wei et al., 2022a) and InstructGPT (Ouyang et al., 2022). It is however much more challenging to collect instructions for agent tasks, as it involves the trajectories of interactions when an agent navigates in a complex environment.

We take the very first attempt to build AgentInstruct for improving LLMs' generalized agent abilities. We detail the design choices during its construction process. It consists of three major stages: Instruction Construction (\$2.1.1), Trajectory Interaction (\$2.1.2), and Trajectory Filtering (\$2.1.3). This process was entirely automated using GPT-3.5 (gpt-3.5-turbo-0613) and GPT4 (gpt-4-0 613), allowing the approach to be easily extended to new agent tasks.

### 2.1.1 INSTRUCTION CONSTRUCTION

We construct AgentInstruct for six agent tasks, including AlfWorld (Shridhar et al., 2020), WebShop (Yao et al., 2022), Mind2Web (Deng et al., 2023), Knowledge Graph, Operating System, and Database (Liu et al., 2023), representative of a diverse range of real-world scenarios that are relatively easy to collect instructions. AgentInstruct comprises challenging 6 tasks from AgentBench (Liu et al., 2023), covering a wide range of real-world scenarios, with most open-source models performing poorly on them.

Table 1 lists the overview of AgentInstruct. If a task (e.g., ALFWorld, WebShop, Mind2Web, and Knowledge Graph) has a training set, we directly use the training split for subsequent phasestrajectory interaction and filtering. For Operating System and Database tasks without training sets, we leverage the idea of Task Derivation and Self-Instruct (Wang et al., 2023c) to construct corresponding instructions.

Task Derivation For agent tasks associated with scenarios that have been widely studied, we can directly construct instructions from similar datasets. Thus to construct instructions on the Database (DB) task, we derive instructions from BIRD (Li et al., 2023), a SELECT-only database benchmark. We ran two types of task derivation. First, we construct a trajectory using the question and the reference SQL statement in each BIRD subtask. We then query the database using the reference SQL statement to obtain output of the database and serve it as the submitted answer of the agent. Finally, we ask GPT-4 to fill in the thoughts of the agent given the above information. In this way, we can generate correct trajectories directly from BIRD dataset.

However, since this synthesis process determines the number of interaction turns to be fixed at 2 , we then propose another approach to improve the diversity by constructing instructions instead of trajectories directly. We prompt GPT-4 with a question from BIRD, and collect its interaction trajectory with the database. After collecting trajectories, we execute the reference SQL statement from BIRD and compare the result to the one from GPT-4. We filter out wrong answers, collecting trajectories that produce a correct answer only.

Self-Instruct For the Operating System (OS) task, due to the difficulty in obtaining instructions that involve manipulating OS in terminal, we employed the Self-Instruct method (Wang et al., 2023c) to construct the task. We first prompt GPT-4 to come up with some OS related tasks along with explanations to the task, a reference solution and an evaluation script. Then, we prompt another GPT-4 instance (the solver) with the task and collect its trajectory. After the task is completed, we run the reference solution and compare its result to the one from the solver GPT-4 using the evaluation script. We collect the trajectories where the reference solution and the solver's solution give the same answer. For the DB task, since BIRD only contains SELECT data, we construct other types of database operations (INSERT, UPDATE and DELETE) in a similar self-instruct approach.

It is worth noting that these two methods might risk test data leakage if GPT-4 outputs instructions identical to those in the test set, or if test tasks are constructed from the same dataset we derived from. To address this concern, we conducted a systematic analysis and found no evidence of data leakage. Details can be found in the Appendix B.

### 2.1.2 TRAJECTORY INTERACTION

With the initial instructions constructed, we use GPT-4 (gpt-4-0613) as agents for trajectory interaction. For the Mind2Web task, due to the large number of instructions and our budget constraints, we partially employed ChatGPT (gpt-3.5-turbo-0613) for interactions.

We utilize the 1-shot evaluation approach (Liu et al., 2023), primarily due to the stringent requirements for the output format in agent tasks. For each task, we provide a complete interaction process from the training set.

Interaction Process The interaction process has two main parts. First, we give the model a task description and a successful 1-shot example. Then, the actual interaction begins. We supply the model with the current instruction and necessary information. Based on this and previous feedback, the model forms a thought and takes an action. The environment then provides feedback, including possible changes or new information. This cycle continues until the model either achieves its goal or reaches its token limit. If the model repeats the same output three times consecutively, we consider it a repetitive failure. If the model's output format is wrong, we use the BLEU metric to compare it to all possible action choices and pick the closest match as the model's action for that step.

CoT Rationales The Chain-of-Thought (CoT) method has significantly enhanced the inferential capabilities of LLMs by a step-by-step reasoning progress (Wei et al., 2022b). Thus, we employ ReAct (Yao et al., 2023) as the reasoning framework, which outputs CoT explanation (referred to as thought) before producing the final action. Consequently, every action within the collected interaction trajectories is accompanied by a detailed explanation trace, enabling the model to learn the reasoning process leading to the action. For trajectories generated using task derivation without thoughts, we use GPT-4 to supplement them with thoughts for consistency with ReAct prompting.

### 2.1.3 TRAJECTORY FILTERING

Agent tasks that encompass real-world scenarios present significant challenges. Even GPT-4 falls short of expectations on such tasks. To ensure the data quality, we rigorously filtered its interaction trajectories. Recall that each interaction trajectory receives a reward $r$, this allows us to automatically select high-quality trajectories based on the reward. We filter trajectories for all tasks, except for Mind2Web, based on a final reward of $r=1$, indicating complete correctness. However, due to the difficulty of the Mind2Web task, we use a threshold of $r \geq \frac{2}{3}$ to ensure we obtain a sufficient number of trajectories. In Table 2, we demonstrate the effectiveness of our filtering strategy by fine-tuning on both filtered and unfiltered trajectories at 7B scale. Compared to models trained on filtered trajectories, those trained on unfiltered trajectories perform significantly worse on both held-in and held-out tasks. This underscores the importance of data quality over data quantity for agent tasks.

Following these steps, the AgentInstruct dataset as shown in Table 1 contains 1,866 final trajectories.

### 2.2 INSTRUCTION TUNING

In this section, we introduce our hybrid instruction-tuning strategy. The goal is to enhance the LLMs' agent capabilities without compromising its general abilities.

### 2.2.1 GENERAL DOMAIN INSTRUCTIONS

Recent studies suggest that training with diverse user prompts enhances model performance (Chiang et al., 2023; Wang et al., 2023b). Using the ShareGPT dataset², we selectively extracted Englishlanguage conversation, yielding 57,096 conversations with GPT-3.5 and 3,670 with GPT-4. Recognizing the superior quality of GPT-4 responses as highlighted by (Wang et al., 2023a), we adopted a sampling ratio of 1:4 between GPT-4 and GPT-3.5 for better performance.[^2]

Table 3: Overview of our evaluation tasks. We introduce 6 held-in and 6 held-out tasks for comprehensive evaluation, encompassing a wide range of real-world scenarios. Weight ${ }^{-1}$ represents the weight of the task when computing the overall score (Cf. Section 3.1). "\#Inst." denotes the number of query samples for the task. "SR" stands for Success Rate.

| Task | Weight $^{-1}$ | \# Shots | \# Inst. | Avg <br> \# Turns | Metric | Characteristics |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Held-in Tasks |  |  |  |  |  |  |
| ALFWorld (Shridhar et al., 2020) | 20 | 1 | 50 | 35 | SR | Daily Household Routines |
| WebShop (Yao et al., 2022) | 28 | 1 | 200 | 5 | Reward | Online Shopping |
| Mind2Web (Deng et al., 2023) | 9 | 3 | 1,173 | 7 | Step SR | Website Navigation |
| Knowledge Graph (Liu et al., 2023) | 16 | 1 | 150 | 15 | $\mathrm{~F} 1$ | Retrieve Entity from $\mathrm{KG}$ |
| Operating System (Liu et al., 2023) | 19 | 1 | 144 | 8 | $\mathrm{SR}$ | Interacting with OS |
| Database (Liu et al., 2023) | 12 | 0 | 300 | 5 | $\mathrm{SR}$ | Database Operations |
| Held-out Tasks |  |  |  |  |  |  |
| SciWorld (Wang et al., 2022) | 16 | 1 | 270 | 8 | Reward | Science Experiments |
| MiniWoB++ (Kim et al., 2023) | 31 | $\geq 0$ | 460 | 5 | SR | Daily Computer Tasks |
| HotpotQA (Yang et al., 2018) | 35 | 2 | 300 | 3 | Reward | Wiki QA |
| WebArena (Zhou et al., 2023) | 3 | 2 | 812 | 10 | SR | Real-world Web Interaction |
| ReWOO (Xu et al., 2023) | 61 | 1 | 350 | 2 | SR | Observation-Free Reasoning |
| Digital Card Game (Liu et al., 2023) | 16 | 0 | 200 | 30 | SR | Adversarial Card Game |

### 2.2.2 MiXTURe TraINING

Using the base model $\pi_{0}$, which represents the probability distribution $\pi_{0}(y \mid x)$ of response $y$ given instruction and history $x$, we consider two datasets: the AgentInstruct dataset $\mathcal{D}_{\text {agent }}$ and the general dataset $\mathcal{D}_{\text {general }}$. The mixure ratio of $\mathcal{D}_{\text {agent }}$ and $\mathcal{D}_{\text {general }}$ is defined as $\eta$. Our aim is to find the best policy $\pi_{\theta}(y \mid x)$ that minimizes the loss function $J(\theta)$, as shown in Equation 1.

$$
\begin{equation*}
J(\theta)=\eta \cdot \mathbb{E}_{(x, y) \sim \mathcal{D}_{\text {agent }}}\left[\log \pi_{\theta}(y \mid x)\right]+(1-\eta) \cdot \mathbb{E}_{(x, y) \sim \mathcal{D}_{\text {general }}}\left[\log \pi_{\theta}(y \mid x)\right] \tag{1}
\end{equation*}
$$

Intuitively, a larger $\eta$ should imply that the model is more inclined towards agent-specific capabilities rather than general capabilities. However, we observed that training solely on agent tasks performs worse on unseen tasks compared to mixed training. This suggests that general capabilities play a pivotal role in the generalization of agent abilities, which we discuss further in Section 3.4. To determine the best $\eta$, we scan from 0 to 1 in intervals of 0.1 on the 7B model and ultimately chose $\eta=0.2$ which performed the best on held-out tasks for final training.

### 2.2.3 TRAINING SETUP

We choose the chat version of open Llama 2 (Llama-2-\{7,13,70\}b-chat) (Touvron et al., 2023b) as our base models, given its better instruction-following capabilities than base models and commendable performance on traditional NLP tasks. Following Vicuna (Chiang et al., 2023), we standardize all data into a multi-turn chatbot-style format, allowing us to conveniently mix data from different sources. During fine-tuning, we only compute the loss on the model's output. We fine-tune models of sizes 7B, 13B, and 70B using Megatron-LM (Shoeybi et al., 2020). We use a learning rate of $5 \mathrm{e}-5$ for the 7B and 13B models, and 1e-5 for the 70B model. We set the batch size at 64 with 4,096 sequence length. We use AdamW optimizer (Loshchilov \& Hutter, 2019) with a cosine learning scheduler with $2 \%$ warm-up steps. For efficient training, we employ tensor parallelism (Shoeybi et al., 2020) for the 7B and 13B models, and for the 70B model, we also utilize pipeline parallelism (Huang et al., 2019). Detailed hyper-parameters during training can be found in Appendix A.

## 3 EXPERIMENTS

### 3.1 EVALUATION SETUP

Held-in/out Tasks Table 3 summarizes our evaluation tasks. We select six held-in tasks from AgentBench (Liu et al., 2023): ALFWorld (Shridhar et al., 2020), WebShop (Yao et al., 2022),

Table 4: Main results of AgentTuning. AgentLM significantly outperforms Llama 2 across different scales, excelling in both held-in and held-out tasks, without compromising its performance on general tasks. Overall stands for score calculated from a weighted average of all tasks within the same category (Cf. Section 3.1). (API-based models and open-source models are compared separately. bold: the best in API-based models and open-source models; underline: the second best in open-source models)

| Type | Task | API-based |  | Llama 2 (chat) |  |  | AgentLM |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | GPT-3.5 | GPT-4 | 7B | $13 \mathrm{~B}$ | $70 \mathrm{~B}$ | 7B | $13 \mathrm{~B}$ | $70 \mathrm{~B}$ |
| Held-in <br> Tasks | ALFWorld | 14.0 | 78.0 | 2.0 | 2.0 | 6.0 | $\underline{84.0}$ | 76.0 | 86.0 |
|  | WebShop | 67.2 | 58.6 | 4.4 | 7.2 | 1.5 | 63.6 | 70.8 | $\underline{64.9}$ |
|  | Mind2Web | 15.7 | 22.6 | 3.7 | 2.3 | 0.2 | 6.4 | $\underline{8.4}$ | 13.5 |
|  | $\mathrm{KG}$ | 27.2 | 52.1 | 0.0 | 0.0 | 0.0 | 18.1 | 26.8 | 47.0 |
|  | $\mathrm{OS}$ | 32.6 | 36.8 | 8.3 | 9.0 | 9.0 | 17.4 | $\underline{18.1}$ | 21.5 |
|  | Database | 15.0 | 33.7 | 0.3 | 1.3 | 9.3 | 30.6 | $\underline{33.7}$ | 37.7 |
|  | Overall | 1.59 | 2.75 | 0.19 | 0.20 | 0.27 | 1.96 | $\underline{2.11}$ | 2.55 |
| Held-out <br> Tasks | SciWorld | 21.2 | 36.4 | 5.9 | 6.4 | 7.9 | 13.7 | 18.0 | 20.8 |
|  | MiniWoB++ | 66.7 | 69.4 | 0.0 | 19.6 | 0.7 | 28.9 | 31.1 | 60.7 |
|  | WebArena | 4.56 | 6.28 | 1.23 | 1.11 | 0.62 | 0.74 | $\underline{1.60}$ | 3.81 |
|  | HotpotQA | 37.4 | 52.1 | 22.6 | 25.2 | $\underline{37.5}$ | 22.3 | $\overline{29.6}$ | 41.6 |
|  | ReWOO | 71.0 | 79.7 | 48.3 | 48.7 | 55.1 | 50.9 | $\underline{55.7}$ | 66.0 |
|  | $\mathrm{DCG}$ | 24.5 | 50.0 | 0.0 | 0.0 | 5.0 | $\underline{7.0}$ | 2.5 | 23.5 |
|  | Overall | 1.49 | 2.13 | 0.38 | 0.49 | 0.51 | 0.67 <br> $(+76 \%)$ | $\frac{0.78}{(+57 \%)}$ | 1.40 <br> $(+176 \%)$ |
| General <br> Tasks | MMLU | 70.0 | 86.4 | 48.0 | 54.3 | 62.1 | 48.7 | 53.6 | $\underline{59.5}$ |
|  | HumanEval | 48.1 | 67.0 | 13.9 | 18.4 | 30.8 | 15.4 | 14.8 | $\underline{28.7}$ |
|  | GSM8K | 57.1 | 87.1 | 27.7 | 37.5 | $\underline{54.7}$ | 24.6 | 32.4 | 59.7 |
|  | MT-Bench | 7.94 | 8.99 | 6.26 | 6.65 | $\underline{6.85}$ | 6.11 | 6.57 | 7.26 |
|  | Overall | 1.15 | 1.53 | 0.63 | 0.74 | $\underline{0.95}$ | 0.62 <br> $(-1 \%)$ | 0.69 <br> $(-7 \%)$ | $\mathbf{0 . 9 6}$ <br> $(+1 \%)$ |

Mind2Web (Deng et al., 2023), and three others, using AgentBench metrics. For held-out tasks, we choose SciWorld (Wang et al., 2022), MiniWoB++ (Kim et al., 2023), WebArena (Zhou et al., 2023), and three more, covering activities like science experiments (SciWrold) and web interactions (WebArena). These datasets ensure a robust evaluation of our model on diverse, unseen agent tasks.

General Tasks To comprehensively evaluate the model's general capabilities, we selected 4 tasks that are widely adopted in the field. These respectively reflect the model's knowledge capacity (MMLU (Hendrycks et al., 2021)), mathematical ability (GSM8K (Cobbe et al., 2021)), coding capability (Humaneval (Chen et al., 2021)), and human preference (MT-Bench (Zheng et al., 2023)).

Baselines In Figure 1, the api-based commercial model notably surpasses open-source ones in agent tasks. Hence, we selected GPT-3.5 (OpenAI, 2022) (gpt-3.5-turbo-0613) and GPT-4 (OpenAI, 2023) (gpt-4-0613) for their comprehensive agent capabilities. For comparison, we evaluated the open-source Llama 2 (Touvron et al., 2023b) chat version (Llama-2-\{7,13,70\}b-chat), chosen for its superior instruction-following capabilities over the base version, which is crucial for agent tasks. Following AgentBench (Liu et al., 2023), we truncate dialogue histories exceeding model length limits and typically use greedy decoding. For WebArena, we adopt nucleus sampling (Holtzman et al., 2020) with $p=0.9$ for exploration. Task prompts are in Appendix D.

Overall Score Calculation Differences in task difficulty may result in higher scores (e.g., ReWOO) overshadowing lower ones (e.g., WebArena) in direct averages. Based on (Liu et al., 2023),

![](https://cdn.mathpix.com/cropped/2024_06_04_307df45ecf542b35e342g-08.jpg?height=604&width=1331&top_left_y=267&top_left_x=386)

![](https://cdn.mathpix.com/cropped/2024_06_04_307df45ecf542b35e342g-08.jpg?height=483&width=745&top_left_y=279&top_left_x=403)

(a) Error analysis of the four models. Each failed trajectory might contribute to multiple error types.

![](https://cdn.mathpix.com/cropped/2024_06_04_307df45ecf542b35e342g-08.jpg?height=475&width=569&top_left_y=283&top_left_x=1144)

(b) Heatmap of task effect. We plot the relative improvements over the 7B model

Figure 3: Error and contribution analysis of AgentTuning. (a) Proportion of failed trajectories versus the type of the first error. AgentTuning significantly reduces the occurrence of elementary errors; (b) The contribution of each individual task. Training solely on one task also promotes performance on other tasks.

we normalize scores of each task across evaluated models, scaling to an average of 1 for balanced benchmark assessments. Task weights are detailed in Table 3 for future reference.

### 3.2 MAIN RESULTS

Table 4 presents the results on our held-in, held-out, and general tasks. Overall, AgentLM exhibits significant improvements over Llama 2 series different scales in both held-in and held-out tasks, while maintaining performance on general tasks. Although the improvement on the held-in tasks is more pronounced than on the held-out tasks, the enhancement in the held-out tasks still reaches up to $170 \%$. This results demonstrates the potential of our model as a general agent. On several tasks, the 13B and 70B versions of AgentLM even surpassed GPT-4.

For most of the held-in tasks, the performance of Llama 2 is nearly zero, indicating that the model is entirely incapable of handling these tasks. Detailed error analysis in the following subsection (Cf. Section 3.3) reveals that the majority of mistakes are elementary errors, such as invalid instructions or repetitions. AgentLM, on the other hand, commits notably fewer elementary errors, indicating that our approach effectively activates the agent capabilities of the model. Remarkably, the 70B AgentLM demonstrates performance nearly approaching GPT-4 overall.

On the held-out tasks, the 70B AgentLM demonstrates performance close to that of GPT-3.5. Furthermore, we observed a significantly larger improvement in the 70B model $(+176 \%)$ compared to the 7B model $(+76 \%)$. We believe this is because larger models possess stronger generalization capabilities, allowing them to better generalize to held-out tasks with the same train data.

On general tasks, AgentLM performs on par with Llama 2 across four dimensions: knowledge, mathematics, coding, and human preferences. This sufficiently demonstrates that our model maintains the same general capabilities even with enhanced agent abilities.

### 3.3 ERROR ANALYSIS

To delve into error analysis, we selected three tasks from the held-in set (ALFWorld, WebShop, Knowledge Graph) and identified common error types using a rule-based approach, such as invalid actions and repeated generations. The results can be seen in Figure 3a. Overall, the original Llama 2 exhibited more elementary mistakes like repetition or taking invalid actions. In contrast, GPT-3.5 and especially GPT-4 made fewer of such errors. However, the AgentLM noticeably reduced these basic errors. We speculate that while Llama 2 chat inherently possesses agent capabilities, its poor performance might be due to a lack of aligned training on agent data; the AgentTuning effectively activated its agent potential.

![](https://cdn.mathpix.com/cropped/2024_06_04_307df45ecf542b35e342g-09.jpg?height=967&width=1415&top_left_y=270&top_left_x=360)

Figure 4: Comparison case study on ALFWorld and Knowledge Graph between Llama-2-70.b-chat and AgentLM-70B. (a) For the ALFWorld task, Llama-2-70b-chat repeated the same action ultimately failing to complete the task, while AgentLM-70B adjusted its actions after a failure. (b) For the Knowledge Graph task, Llama-2-70b-chat refused to fix the function call and instead demanded the user to implement the function upon encountering a error. In contrast, AgentLM-70B provided the correct function call.

### 3.4 ABLATION StUDY

Effect of Agent \& General Instructions Table 5 illustrates the performance when trained exclusively on either agent or general instructions. It is observed that solely using agent data for training significantly improves the results on the held-in set. Yet, it struggles to generalize well across both agent and general tasks. When integrating general data, AgentLM performs almost at its best for both heldin and held-out tasks. This underscores the critical importance of general instructions in model generalization. Intriguingly, when considering the $7 \mathrm{~B} / 13 \mathrm{~B}$ scale, the enhancement seen in held-out tasks from mixed training is nearly equivalent to training with just the general data. A considerable leap in performance is only observed at the 70B scale. This leads us to speculate that achieving optimal generalization for agent tasks might necessitate a specific model size.

Effect of Different Tasks We examine mutual task enhancements by fine-tuning on individual tasks in AgentInstruct. We use Llama-7B-chat for ablation study. Figure 3b reveals that fine-tuning primarily benefits the respective task. Although many tasks aid others, Mind2Web stands out with minimal cross-task enhancement, possibly due to its single-round format contrasting with multiround tasks.
Table 5: Ablation study on the effect of agent and general instructions.

|  | Held-in | Held-out General |  |
| ---: | :---: | :---: | :---: |
| AgentLM-7B | $\mathbf{1 . 9 6}$ | $\mathbf{0 . 6 7}$ | $\mathbf{0 . 6 3}$ |
| - general only | 0.38 | 0.64 | 0.61 |
| - agent only | 1.34 | 0.09 | 0.22 |
| AgentLM-13B | $\mathbf{2 . 1 1}$ | 0.78 | $\mathbf{0 . 6 9}$ |
| - general only | 0.43 | $\mathbf{0 . 8 1}$ | 0.63 |
| - agent only | 1.57 | 0.10 | 0.19 |
| AgentLM-70B | $\mathbf{2 . 5 5}$ | $\mathbf{1 . 4 0}$ | 0.96 |
| - general only | 0.99 | 0.98 | $\mathbf{1 . 0 0}$ |
| - agent only | 2.47 | 0.87 | 0.83 |

## 4 RELATED WORK

LLM-as-Agent Before the rise of LLMs (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a; Zeng et al., 2022), agent tasks primarily relied on reinforcement learning or encoder models like BERT. With the advent of LLMs, research shifted towards LLM agents. Notably, ReAct (Yao et al., 2023) innovatively combined CoT reasoning with agent actions. Several studies also applied language models to specific agent tasks, such as online shopping (Yao et al., 2022), web browsing (Deng et al., 2023), and household exploration (Shridhar et al., 2020). Recently, with ChatGPT showcasing advanced planning and reasoning skills, research like ReWOO (Xu et al., 2023) and RCI (Kim et al., 2023) has delved into prompting strategies and frameworks to boost language model efficiency in agent tasks without the need for fine-tuning.

Instruction Tuning Instruction tuning aims at aligning the language models to follow human instructions and produce outputs that better fit human preferences. Instruction tuning mainly focus on training language models to follow human instructions among multiple general tasks. For instance, FLAN (Wei et al., 2022a) and T0 (Sanh et al., 2022) demonstrates the strong zero-shot generalization ability of language models fine-tuned on multiple task datasets. Further, FLAN-V2 (Longpre et al., 2023) explores the performance of instruction tuning across multiple scales of models and datasets. With the impressive alignment capability demonstrated by commercial LLMs, many recent works (Chiang et al., 2023; Wang et al., 2023a) propose methods to distill instruction tuning dataset from close-sourced model to enhance the alignment of open-source models.

## 5 CONCLUSION

In this work, we study how to enable generalized agent abilities for LLMs, bridging the disparity between open and commercial LLMs on agent tasks. We present the AgentTuning approach to achieve this goal. AgentTuning first introduces the AgentInstruct dataset covering 1,866 verified agent interaction trajectories and then designs an instruction-tuning strategy with the mixture of AgentInstruct and general-domain instructions. We generate the open AgentLM by employing AgentTuning to tune the Llama 2 models. AgentLM exhibits strong performance on unseen agent tasks while preserving their general abilities on MMLU, GSM8K, HumanEval, and MT-Bench. To date, AgentLM-70B is the first open LLM that matches GPT-3.5-turbo on agent tasks.

## REFERENCES

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An
open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.

Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview. net/forum?id=rygGQyrFvH.

Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019 .

Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks, 2023.

Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Chenhao Ma, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls, 2023.

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. ArXiv preprint, abs/2308.03688, 2023. URL https://arxiv.org/abs/2308.03688.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning, 2023.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.

Pattie Maes. Agents that reduce work and information overload. Commun. ACM, 37:30-40, 1994.

Yohei Nakajima. Babyagi. Python. https://github.com/yoheinakajima/babyagi, 2023.

OpenAI. Introducing chatgpt, 2022. URL https: / openai.com/blog/chatgpt.

OpenAI. Gpt-4 technical report. arXiv, pp. 2303-08774, 2023.

Anton Osika. Gpt-engineer. Python. https://github.com/AntonOsika/gpt-engineer, 2023.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.

Toran Bruce Richards. Auto-gpt: An autonomous gpt-4 experiment, 2023.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.

Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2020.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023a. URL https: //arxiv.org/abs/2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023b. URL https: //arxiv.org/abs/2307.09288.

Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data, 2023a.

Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. ScienceWorld: Is your agent smarter than a 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11279-11298, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022. emnlp-main. 775 .

Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023b.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions, $2023 \mathrm{c}$.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a. URL https://openreview.net/forum? id=gEZrGCozdqR.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022b.

Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge engineering review, 10(2):115-152, 1995.

Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. Rewoo: Decoupling reasoning from observations for efficient augmented language models, 2023.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369-2380, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.

Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744-20757, 2022.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. URL https : / / webarena. dev.
