# KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization 

Tianyi Zhang ${ }^{1}$, Jonah $\mathrm{Yi}^{1}$, Zhaozhuo Xu ${ }^{2}$, and Anshumali Shrivastava ${ }^{1,3}$<br>${ }^{1}$ Department of Computer Science, Rice University<br>${ }^{2}$ Department of Computer Science, Stevens Institute of Technology<br>${ }^{3}$ ThirdAI Corp.<br>\{tz21, jwy4, anshumali\}@rice.edu, zxu79@stevens.edu

May 8, 2024


#### Abstract

Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As the batch size, context length, or model size increases, the size of the key and value ( $\mathrm{KV}$ ) cache can quickly become the main contributor to GPU memory usage and the bottleneck of inference latency. Quantization has emerged as an effective technique for $\mathrm{KV}$ cache compression, but existing methods still fail at very low bit widths. We observe that distinct channels of a key/value activation embedding are highly inter-dependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropies. Based on this insight, we propose Coupled Quantization (CQ), which couples multiple key/value channels together to exploit their inter-dependency and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ outperforms or is competitive with existing baselines in preserving model quality. Furthermore, we demonstrate that $\mathrm{CQ}$ can preserve model quality with $\mathrm{KV}$ cache quantized down to 1-bit.


## 1 Introduction

Large Language Models (LLMs) have showcased remarkable generalization abilities across various tasks, including text generation, language translation, and reasoning, without needing specific finetuning [25]. These impressive capabilities have empowered LLMs to find applications in numerous domains, such as law [14], education [15], and patient care [35]. However, the high computational demands and the prohibitive deployment costs of LLMs have created significant barriers, hindering their widespread adoption $[14,3]$. Particularly, as LLMs move towards larger model size [9] and longer context length [34], they require faster graphics processing units (GPUs), or other specialized processors, with higher memory capacity for efficient inference. Hence it is crucial to develop approaches for reducing the computational costs and memory requirement of LLMs.

To accelerate LLM inference, key and value (KV) caching [20] has been proven to be an effective technique without affecting model quality. In autoregressive decoder-only LLMs, KV caching works through trading off memory for computations: the key and value activations of all previous tokens in the current batch are saved in memory to avoid their recomputation for generating the next token. However, since KV cache scales linearly with the number of tokens and batch size, it can quickly overwhelm the memory capacity of existing GPUs under long context or large batch size
settings. In addition, since past key and value activations are not shared between sequences (except for maybe a common prompt), reading the KV cache from GPU memory becomes the primary inference bottleneck as opposed to the computation of the attention scores and value activations of the next token [10]. Thus, it is worthwhile to explore techniques for compressing KV cache for two primary benefits: 1 . speeding up LLM inference through reducing the amount of memory reads for KV cache, 2. lowering the GPU memory requirements of inference for a given batch size and context length. Existing approaches typically compress KV cache through token eviction [37, 20] or activation quantization [21, 10]. While they can preserve model quality at moderate compression rates ( $4 \times$ compression or 4 bits per floating-point number), model quality quickly deteriorates at high compression rates ( $16 \times$ compression or 1 bit per floating-point number). In this work, we propose Coupled Quantization (CQ), a novel KV cache quantization method that preserves model quality up to $16 \times$ compression or 1 bit per floating-point number.

Our approach is motivated by the observation that different channels within the same key/value activation embedding are highly inter-dependent. Thus, it is more information efficient to encode multiple channels of a key/value activation embedding than quantizing each channel independently. Existing KV cache quantization methods employ per-channel or per-token quantization strategies, which fail to exploit the inter-dependence between channels and suffer catastrophic model quality degradation at high compression rates. Our proposed method exploits the mutual dependency between channels by jointly quantizing multiple channels and achieves better preservation of model quality than existing approaches in most cases, especially under low bit width settings. We summarize our contributions as follows,

1. We empirically observe the phenomenon that different channels within the same key/value activation embedding in an LLM share a high amount of dependency or mutual information, which is a key insight not leveraged by existing $\mathrm{KV}$ cache compression approaches.
2. We propose Coupled Quantization (CQ), a novel KV cache quantization method that takes advantage of the reduced entropy of jointly encoding multiple channels.
3. Through extensive experiments, we demonstrate the effectiveness of $\mathrm{CQ}$ against the most competitive existing methods. Furthermore, we showcase the ability of $\mathrm{CQ}$ in preserving model quality at an extreme KV cache compression level of 1-bit.

## 2 Background

In this section, we introduce the relevant background and context including the KV caching technique, the von Neumann bottleneck of KV cache, and channel-wise quantization.

### 2.1 LLM Attention and KV Cache

Decoder-only transformer-based LLMs employ masked self-attention [32], in which activations of the current token is only dependent on previous tokens and unaffected by future ones. This property enables training parallelism for the next-token prediction objective, and gives rise to the $\mathrm{KV}$ caching technique for efficient inference decoding. Consider the decoding step for the $t$-th token in a single head of attention in an LLM. The input embedding of the $t$-th token (a column vector), $e_{t}$, goes through three distinct transformations to become key, query, and value activation embeddings $f_{K}\left(e_{t}\right), f_{Q}\left(e_{t}\right), f_{V}\left(e_{t}\right)$, where the transformations $f_{K}, f_{Q}, f_{V}$ are composed of linear projection and positional encoding such as RoPE [29]. The output embedding of attention for the $t$-th token is
computed as

$$
\operatorname{attention}\left(e_{t}\right)=\left[\begin{array}{lll}
f_{V}\left(e_{1}\right) & \ldots & f_{V}\left(e_{t}\right)
\end{array}\right] \operatorname{softmax}\left(\left[\begin{array}{lll}
f_{K}\left(e_{1}\right) & \ldots & f_{K}\left(e_{t}\right) \tag{1}
\end{array}\right]^{\top} f_{Q}\left(e_{t}\right)\right)
$$

Thus, computing the output embedding of the current token requires the key and value activation embeddings of all previous tokens, $f_{K}\left(e_{i}\right)$ and $f_{V}\left(e_{i}\right)$ where $i \in\{1, \ldots, t-1\}$. These embeddings are cached in memory from previous decoding steps as $K V$ cache to avoid redundant computations and reduce inference latency. The size of KV cache can be calculated as $b \times n \times l \times 2 \times h \times c$ floating-point numbers, where $b$ is the batch size, $n$ is the number of tokens in each sequence, $l$ is the number of layers in the model, 2 is for key and value, $h$ is the number of key/value attention heads, and $c$ is the number of channels in a single head of key/value activation embedding. As batch size, context length, or model size increases, the size of KV quickly overwhelms limited GPU memory.

### 2.2 The von Neumann Bottleneck of KV Cache

The attention computation in Equation 1 is primarily bottlenecked by GPU memory bandwidth, known as the von Neumann bottleneck, due to low compute-to-global-memory-access ratio. GPUs are able to perform computations significantly faster than reading from global memory, and previous works have shown that attention computation can be accelerated by avoiding unnecessary global memory reads/writes through kernel fusion [5]. During the decoding phase in LLM attention, computing the output for the current token requires fetching the cached key and value embeddings of all previous tokens from global memory, resulting in a low ratio of arithmetic operations to global memory reads [10]. Furthermore, each sequence in a batch retains its own KV cache, leading to poor utilization of the parallel processing capabilities of GPUs. Therefore, KV cache compression algorithms can mitigate the von Neumann bottleneck and improve LLM inference efficiency, even if the approach introduces additional computational overheads in decompression or dequantization. As GPU compute cores are mostly stalled by KV cache memory accesses in attention, quantization approaches can effectively reduce memory reads while introducing negligible latency from dequantization computations.

### 2.3 Channel-wise Quantization

Existing KV cache quantization methods [10, 21] employ channel-wise quantization for keys and token-wise quantization for values, based on the observation that certain key channels exhibit outlier patterns in magnitude while value channels do not. Channel-wise and token-wise quantization are similar, except the direction along which the quantization centroids are learned. In non-uniform channel-wise quantization, a set of centroids is learned for each channel. Suppose $A$ is a key or value activation matrix, and $A_{i, *}$ denotes the $i$-th channel of $A$. Then, non-uniform $b$-bit channel-wise quantization aims to learn a set of centroids $C_{i}^{\star} \subset \mathbb{R}$ for each channel $i$ of $A$ independently through the objective

$$
\begin{equation*}
C_{i}^{\star}=\underset{\substack{C \subset \mathbb{R} \\|C|=2^{b}}}{\arg \min }\left\|A_{i, *}-q\left(A_{i, *}\right)\right\|_{2}^{2} \tag{2}
\end{equation*}
$$

where $q$ quantizes each value in $A_{i, *}$ to the nearest centroid in $C$.

![](https://cdn.mathpix.com/cropped/2024_06_04_b55bdfbf55ed21b2a0f1g-04.jpg?height=713&width=1328&top_left_y=251&top_left_x=390)

Sum of Marginal Entropy - Joint Entropy
![](https://cdn.mathpix.com/cropped/2024_06_04_b55bdfbf55ed21b2a0f1g-04.jpg?height=638&width=1304&top_left_y=316&top_left_x=409)

Figure 1: Growth rate of joint entropy versus sum of marginal entropies of the LLaMA-7b key/value activation embeddings on $262 \mathrm{k}$ tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that jointly quantizing more channels requires fewer bits than quantizing each channel independently.

## 3 Methodology

In this section, we motivate our proposal using information theory and introduce the Coupled Quantization (CQ) approach for $\mathrm{KV}$ cache compression.

### 3.1 Motivations

Our proposed approach is inspired by concepts in information theory [28]. We consider each channel in a key/value activation embedding as a random variable $X_{1}, X_{2}, \ldots$. The amount of information (or uncertainty) in channel $X$ can be measured by entropy, defined as $H(X)=-\int_{\mathbb{X}} p(x) \log _{2} p(x) d x$, where $p(\cdot)$ is the probability density function and $\mathbb{X}$ is the support of $X . H(X)$ measures the theoretical number of bits needed for losslessly encoding the channel $X$, so it can be used to gauge how "quantizable" a channel is: if $H\left(X_{1}\right)<H\left(X_{2}\right)$, then channel $X_{1}$ may be quantized to fewer bits than channel $X_{2}$ while achieving the same quantization error.

Our insight is that different channels from the same key/value activation embedding may be interdependent, which would reduce the number of bits required for jointly encoding multiple channels together compared to encoding them independently. The total amount of information (or uncertainty) in two channels $X_{1}, X_{2}$ is measured by joint entropy, defined as $H\left(X_{1}, X_{2}\right)=$ $-\int_{\mathbb{X}_{1}} \int_{\mathbb{X}_{2}} p\left(x_{1}, x_{2}\right) \log _{2} p\left(x_{1}, x_{2}\right) d x_{2} d x_{1}$, where $p(\cdot, \cdot)$ is the joint probability density function. The joint entropy of two channels is the difference between the sum of their marginal entropies and their mutual information, i.e., $H\left(X_{1}, X_{2}\right)=H\left(X_{1}\right)+H\left(X_{2}\right)-I\left(X_{1}, X_{2}\right)$, where $I(\cdot, \cdot)$ is a non-negative quantity for measuring the mutual dependency of two random variables. Thus, we have

$$
\begin{equation*}
H\left(X_{1}, X_{2}\right) \leq H\left(X_{1}\right)+H\left(X_{2}\right) \tag{3}
\end{equation*}
$$

which implies the number of bits needed for jointly encoding two channels is no more than the total number of bits needed for encoding them independently. Previous works have demonstrated that

![](https://cdn.mathpix.com/cropped/2024_06_04_b55bdfbf55ed21b2a0f1g-05.jpg?height=475&width=1635&top_left_y=234&top_left_x=234)

Figure 2: Correlation matrices of the first 32 channels of 8 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients.

deep neural networks [11] and attention-based networks [7] tend to produce low-rank embeddings, which suggests that channels of key/value embedding in LLM may exhibit high amount of mutual dependency.

It is hence beneficial to measure the difference between the joint entropy of multiple channels and the sum of their marginal entropies in key and value activation embeddings. A significant difference would suggest that encoding these channels together is more information-efficient than encoding them independently. However, it is intractable to derive the exact entropy or joint entropy of channels, since their probability density functions are not known. Therefore, we employ the "binning" trick [17] to estimate entropy. We first observe an empirical distribution of key and value channels by saving the $\mathrm{KV}$ cache on a dataset, and partition the support of each channel into equally sized bins. Then, values of each channel are discretized to the index of the bin they fall into. Finally, the joint entropy of $n$ channels $X_{1}, \ldots, X_{n}$ is estimated with the Riemann sum,

$$
\begin{equation*}
H\left(X_{1}, \ldots, X_{n}\right) \approx \sum_{x_{1} \in \mathbb{B}_{1}} \cdots \sum_{x_{n} \in \mathbb{B}_{n}} \widehat{p}\left(x_{1}, \ldots, x_{n}\right) \log _{2} \widehat{p}\left(x_{1}, \ldots, x_{n}\right) \tag{4}
\end{equation*}
$$

where $\mathbb{B}_{i}$ is the support of the binned or discretized $X_{i}$ and $\widehat{p}(\cdot)$ is the empirical probability density function. Specifically, we divide the channels of key and value embeddings of LLaMA-7b into nonoverlapping groups of $c$ contiguous channels, where $c \in\{1,2,3,4\}$, and estimate the joint entropy and the sum of marginal entropies of each group. The support of each channel is partitioned into 16 equally sized bins. Figure 1 shows the mean and standard deviation of the estimated joint entropy and sum of marginal entropies of three layers of LLaMA-7b on 262k tokens of the WikiText-2 dataset, averaged over groups. We only show a maximum group size of 4 , since increasing the group size requires saving exponentially more key and value embeddings to avoid empty bins and maintain estimation quality. As shown in Figure 1, the sum of marginal entropies grows at a linear rate while the joint entropy increases slower at a sub-linear rate. This implies that as the number of jointly quantized channels increases, the total amount of information needed for encoding decreases. This phenomenon is the foundation that motivates our proposed approach.

In addition to presenting the estimated marginal and joint entropy, we also show the Pearson correlation coefficients between channels of LLaMA-7b key and value activation embeddings on WikiText-2. Correlation coefficient captures the linear dependency between two random variables. Heat maps for the correlation matrices of 8 layers are shown in Figure 2, while the correlation matrices of all layers are presented in Section 6 of the Appendix. The key and value channels

![](https://cdn.mathpix.com/cropped/2024_06_04_b55bdfbf55ed21b2a0f1g-06.jpg?height=460&width=1653&top_left_y=239&top_left_x=233)

Figure 3: A comparison of 1-bit channel-wise quantization and our proposed Coupled Quantization (using 2 bits per 2 channels as an example). The quantization results on the first two channels of the first-layer key activation embeddings of LLaMA-7b on the WikiText-2 dataset are shown. Channel-wise quantization is ineffective at capturing the original values at low widths, while CQ leverages the dependency between channels to achieve low quantization errors.

exhibit high levels of linear dependency and they are clearly not independently distributed, as shown by high magnitudes of the correlation coefficients.

### 3.2 Coupled Quantization

Motivated by the finding that distinct key/value channels exhibit high dependency, we propose Coupled Quantization (CQ), an information-efficient quantization approach for compressing LLM KV cache. Unlike existing KV cache quantization methods which quantizes channel-wise or tokenwise, CQ performs channel-coupled quantization for keys and values. More concretely, channels of a key or value activation embedding are divided into equally sized, non-overlapping groups of contiguous channels. The channels in each group are coupled, as they are jointly quantized and share a single quantization code. For each group of coupled channels, a distinct set of multi-channel centroids are learned, where each centroid has dimensionality equal to the number of channels in that group. When quantizing a key or value activation embedding, each group of coupled channels are quantized to the nearest centroid in terms of $\mathrm{L} 2$ distance. We use the $\mathrm{CQ}-<\mathrm{c}>\mathrm{c}<\mathrm{b}>\mathrm{b}$ notation to denote the configuration of channel coupling and quantization bit width, where $<c>$ is the number of channels in each group and $\langle b\rangle$ indicates the number of bits in a quantized code for a group. For example, CQ-4c8b means that every 4 contiguous channels are coupled together and each coupled group shares an 8-bit code, which is equivalent to 2-bit channel-wise quantization in terms of storage overhead of quantized codes. A illustrative comparison of channel-wise quantization and CQ is shown in Figure 3. Although previous works [10, 21] opt to quantize keys channel-wise and values token-wise, we adopt channel-coupled quantization for both keys and values. Similar to existing approaches [10, 21], CQ quantizes keys before RoPE [29] is applied, which increases the quantization difficulty by introducing more outliers in key activations.

### 3.2.1 Centroid Learning

In CQ, the multi-channel centroids for each channel group are learned offline on a calibration dataset by leveraging uniform clustering or second-order-information-informed clustering. Specifically, for uniform centroid learning of the CQ- $c c b b$ configuration, a set of centroids $C_{i}^{\star} \subset \mathbb{R}^{c}$ is learned
![](https://cdn.mathpix.com/cropped/2024_06_04_b55bdfbf55ed21b2a0f1g-07.jpg?height=526&width=1638&top_left_y=238&top_left_x=234)

Figure 4: Perplexity and key/value quantization errors (averaged over all layers) of LLaMA-7b on WikiText-2. Channels coupling and Fisher-guided centroid learning are effective for improving perplexity.

independently for each channel group $i$ through the objective

$$
\begin{equation*}
C_{i}^{\star}=\underset{\substack{C \in \mathbb{R}^{c} \\|C|=2^{b}}}{\arg \min }\left\|A_{i c:(i c+c-1), *}-\operatorname{cq}\left(A_{i c:(i c+c-1), *}\right)\right\|_{F}^{2} \tag{5}
\end{equation*}
$$

where $A_{i c:(i c+c-1), *}$ is the sub-matrix of $A$ containing all coupled channels of the $i$-th group, and cq quantizes each column vector to the nearest centroid in $C$ in terms of L2 distance. We use the $\mathrm{k}$-means algorithm [22] with k-means++ initialization [1] to optimize the objective.

LLMs are more sensitive to the quantization precision of certain weights than others [16]. To better preserve model quality, centroids of $\mathrm{CQ}$ should be learned to be biased towards preserving the precision of more important activations. To this end, we leverage an approximation to the Hessian to perform second-order-information-informed centroid learning. More concretely, we use the diagonals of the Fisher information matrix $\mathcal{F}$ to identify the more influential key/value activations and guide the centroid learning process. This method was proposed by Li et al. [18] and used by Kim et al. [16] for channel-wise weight quantization, and we extend it to multi-channel CQ. For performing Fisher-guided centroid learning, we first save a key/value activation matrix $A$ and its gradient $g(A)=\frac{\partial}{\partial A} \mathcal{L}(A)$ on a calibration dataset, where $\mathcal{L}$ is the training loss function. We approximate the Hessian matrix using the diagonals of the Fisher information matrix, which is the element-wise square of the gradient matrix $\operatorname{diag}(\mathcal{F})=g(A) \odot g(A)$. We use the sum of diagonal entries of the Fisher information matrix as a measure of importance for each group of channel-coupled activations, and obtain the centroid set $C_{i}^{\star}$ for the $i$-th channel group using the objective

$$
\begin{equation*}
C_{i}^{\star}=\underset{\substack{\triangle \in \mathbb{R}^{c} \\|C|=2^{b}}}{\arg \min } \sum_{j} g\left(A_{i c:(i c+c-1), j}\right)^{\top} g\left(A_{i c:(i c+c-1), j}\right)\left\|A_{i c:(i c+c-1), j}-\operatorname{cq}\left(A_{i c:(i c+c-1), j}\right)\right\|_{F}^{2} \tag{6}
\end{equation*}
$$

We leverage weighted $\mathrm{k}$-means to optimize the objective. The overhead of the centroid learning process and centroid storage are discussed in Section 4.3.

We validate the effectiveness of our proposed channel-coupling and Fisher-guided centroid learning by compressing LLaMA-7b KV cache to 1-bit and 2-bit, and present the perplexity results and quantization errors $\left(\|A-\operatorname{cq}(A)\|_{F}^{2}\right.$ averaged over layers) on WikiText-2 under different CQ configurations in Figure 4. The experimental setup is given in Section 4. As the number of coupled channels increases, perplexity and quantization errors improve significantly, approaching the FP16

Table 1: Perplexity on WikiText-2 under different KV cache quantization methods at varying bitwidth. The results of INT, NF, and KVQuant (excluding $1 \mathrm{~b}$ and $1 \mathrm{~b}-1 \%$ ) are from [10]. "NaN" means Not a Number, which is caused by numerical stability issues of quantization. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1\%.

|  | Bits Per FPN | LLaMA-7b | LLaMA-13b | LLaMA-2-7b | LLaMA-2-13b | Mistral-7b |
| :--- | ---: | :---: | :---: | :---: | :---: | :---: |
| FP16 | 16 | 5.68 | 5.09 | 5.12 | 4.57 | 4.76 |
| INT4 | $4.00-4.01$ | 5.98 | 5.32 | 5.66 | 5.01 | 4.97 |
| INT4-gs128 | 4.16 | 5.77 | 5.16 | 5.32 | 4.71 | 4.82 |
| NF4 | $4.00-4.01$ | 5.87 | 5.23 | 5.47 | 4.90 | 4.91 |
| NF4-gs128 | 4.16 | 5.77 | 5.17 | 5.30 | 4.71 | 4.83 |
| KVQuant-4b | $4.00-4.02$ | $\underline{5.73}$ | $\underline{5.15}$ | $\underline{5.18}$ | $\underline{4.63}$ | 4.81 |
| KVQuant-4b-1\% | $4.32-4.35$ | 5.70 | $\mathbf{5 . 1 1}$ | $\mathbf{5 . 1 4}$ | 4.59 | 4.78 |
| CQ-2c8b | 4.00 | 5.70 | $\mathbf{5 . 1 1}$ | $\mathbf{5 . 1 4}$ | 4.59 | 4.79 |
| INT2 | $2.00-2.01$ | 11779 | 69965 | 4708 | 3942 | 573 |
| INT2-gs128 | 2.14 | 37.37 | 41.77 | 117.88 | 93.09 | 51.96 |
| NF2 | $2.00-2.02$ | 3210.5 | 5785.6 | 13601 | 4035.6 | 902.51 |
| NF2-gs128 | 2.14 | 351.23 | 141.19 | 634.59 | 642.44 | 252.85 |
| KVQuant-2b | $2.00-2.02$ | 8.17 | 7.29 | 9.75 | 29.25 | 7.33 |
| KVQuant-2b-1\% | $2.32-2.35$ | $\underline{6.06}$ | $\underline{5.40}$ | $\underline{5.50}$ | $\underline{4.92}$ | $\underline{5.16}$ |
| CQ-4c8b | 2.00 | 5.97 | 5.32 | 5.42 | 4.81 | 5.11 |
| KVQuant-1b | $1.00-1.02$ | 321.58 | 1617.40 | NaN | 4709.83 | 203.73 |
| KVQuant-1b-1\% | $1.32-1.35$ | 9.93 | 7.97 | 9.50 | 13.76 | 10.07 |
| CQ-8c8b | 1.00 | $\underline{8.09}$ | $\underline{7.02}$ | $\underline{7.75}$ | $\underline{6.55}$ | $\underline{7.25}$ |
| CQ-8c10b | 1.25 | $\mathbf{6 . 7 8}$ | $\mathbf{6 . 0 0}$ | $\mathbf{6 . 2 5}$ | $\mathbf{5 . 4 7}$ | $\mathbf{5 . 9 0}$ |

baseline performance. Although Fisher-guided centroid learning increases the quantization error, it better preserves the salient activations and achieve lower perplexity.

## 4 Experiments

In this section, we perform extensive experiments to validate the effectiveness of our proposed $\mathrm{CQ}$ approach for KV cache compression. We first introduce the experimental setups including hardware, software, metrics, datasets, and baselines used. Then, we present the detailed empirical results and provide discussions. Finally, we perform an ablation study to validate the effectiveness of each component of our proposal.

Hardware Testbed Experiments are performed on a Linux server running Ubuntu 20.04, equipped with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs.

Software Implementation Our software implementation of CQ is based on PyTorch [24] and the HuggingFace Transformers library [33].

Evaluation Metrics and Benchmarks We evaluate the quality of 5 popular open-source LLMs on various benchmarks under different $\mathrm{KV}$ cache quantization algorithms. The 5 LLMs considered are 1. LLaMA-7b, 2. LLaMA-13b [30], 3. LLaMA-2-7b, 4. LLaMA-2-13b [31], 5. Mistral-7b [13]. We evaluate LLM quality using the perplexity metric on 2 datasets: WikiText-2 [23] and C4 [26], and accuracy on 3 benchmarks in zero-shot setting: WinoGrande [27], PIQA [2], and ARC Challenge [4]. Perplexity is evaluated on the test set of the datasets at the maximum context length

Table 2: Perplexity on $\mathrm{C} 4$ under different KV cache quantization methods at varying bit-width. The results of INT, NF, and KVQuant (excluding $1 \mathrm{~b}$ and $1 \mathrm{~b}-1 \%$ ) are from [10]. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b- $1 \%$.

|  | Bits Per FPN | LLaMA-7b | LLaMA-13b | LLama-2-7b | LLaMA-2-13b | Mistral-7b |
| :--- | ---: | :---: | :---: | :---: | :---: | :---: |
| FP16 | 16 | 7.08 | 6.61 | 6.63 | 6.05 | 5.71 |
| INT4 | $4.00-4.01$ | 7.40 | 6.82 | 7.31 | 6.59 | 5.91 |
| INT4-gs128 | 4.16 | 7.16 | 6.67 | 6.87 | 6.20 | 5.76 |
| NF4 | $4.00-4.01$ | 7.27 | 6.74 | 7.09 | 6.45 | 5.85 |
| NF4-gs128 | 4.1 | 7.16 | 6.66 | 6.86 | 6.20 | 5.77 |
| KVQuant-4b | $4.00-4.0$ | 7.13 | 6.65 | 6.70 | 6.11 | 5.75 |
| KVQuant-4b-1\% | $4.32-4.35$ | $\mathbf{7 . 0 9}$ | $\mathbf{6 . 6 2}$ | $\mathbf{6 . 6 5}$ | $\mathbf{6 . 0 6}$ | $\mathbf{5 . 7 2}$ |
| CQ-2c8b | 4.00 | $\underline{7.11}$ | $\underline{6.64}$ | $\underline{6.67}$ | $\underline{6.09}$ | $\underline{5.74}$ |
| INT2 | $2.00-2.01$ | 10892 | 100870 | 4708 | 4220 | 477 |
| INT2-gs128 | 2.14 | 43.49 | 56.25 | 113.49 | 97.04 | 50.73 |
| NF2 | $2.00-2.02$ | 2850.1 | 4680.3 | 13081.2 | 4175.6 | 1102.3 |
| NF2-gs128 | 2.14 | 248.32 | 118.18 | 420.05 | 499.82 | 191.73 |
| KVQuant-2b | $2.00-2.02$ | 10.28 | 9.05 | 15.16 | 43.77 | 8.40 |
| KVQuant-2b-1\% | $2.32-2.35$ | $\mathbf{7 . 3 8}$ | $\mathbf{6 . 8 3}$ | $\mathbf{7 . 0 6}$ | $\mathbf{6 . 3 8}$ | $\mathbf{6 . 0 8}$ |
| CQ-4c8b | 2.00 | $\underline{7.52}$ | $\underline{6.96}$ | $\underline{7.23}$ | $\underline{6.52}$ | $\underline{6.17}$ |
| KVQuant-1b | $1.00-1.02$ | 168.90 | 1316.41 | 362.94 | 4223.37 | 127.07 |
| KVQuant-1b-1\% | $1.32-1.35$ | $\underline{11.18}$ | $\underline{9.56}$ | 16.04 | 22.87 | 10.53 |
| CQ-8c8b | 1.00 | 12.13 | 10.53 | $\underline{12.49}$ | $\underline{10.53}$ | $\underline{9.89}$ |
| CQ-8c10b | 1.25 | $\mathbf{9 . 1 2}$ | $\mathbf{8 . 2 3}$ | $\mathbf{9 . 0 3}$ | $\mathbf{8 . 0 1}$ | $\mathbf{7 . 4 6}$ |

of the LLM (2048 for LLaMA, 4096 for LLaMA-2, and 8192 for Mistral).

Baselines We compare our proposed approach with uncompressed FP16 KV cache and competitive KV cache quantization methods, including 1. uniform integer (INT) quantization (without grouping and with a group size of 128), 2. NormalFloat (NF) quantization [6] (without grouping and with a group size of 128), 3. KVQuant [10] (without sparse outliers and with $1 \%$ outliers stored in sparse format). KVQuant-<b>b-1\% is a dense-and-sparse method that requires an additional sparse matrix multiplication in addition to the dense matrix multiplication during inference, which introduces additional computational overhead. KVQuant and our proposed CQ both require learning centroids on a calibration dataset, and we use the same calibration set of 16 sequences (each with 2048 tokens) of WikiText-2 for both methods. Calibration is performed once on the training set of WikiText-2, while perplexity and accuracy are evaluated on the test sets of different datasets and benchmarks. For each method, we report bits per floating-point number (FPN) to measure the compression rate, which is calculated as the number of bits in the quantized $\mathrm{KV}$ cache of each token divided by the number of FPN in the uncompressed KV cache of each token, excluding the constant storage overheads of centroids, scaling factors, and zero points.

### 4.1 Results

The results of perplexity on WikiText-2 are presented in Table 1 and the results on $\mathrm{C} 4$ are presented in Table 2. CQ consistently outperforms non-dense-and-sparse quantization methods, especially in low bit-width regions. Despite using lower bit-width, CQ performs better or on par with the dense-and-sparse quantization method KVQuant-<b>b-1\%. Dense-and-sparse quantization methods

Table 3: Accuracy on 3 benchmarks under different $\mathrm{KV}$ cache quantization methods at varying bit-width.

|  | Bits Per FPN | Task | LLaMA-7b | LLaMA-13b | LLaMA-2-7b | LLaMA-2-13b | Mistral-7b |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $\mathrm{FP} 16$ | 16 | WinoGrande | 69.93 | 72.69 | 68.90 | 71.98 | 73.88 |
|  |  | PIQA | 78.67 | 79.16 | 78.07 | 79.16 | 80.58 |
|  |  | ARC Challenge | 41.72 | 46.42 | 43.43 | 48.29 | 50.34 |
| KVQuant- $4 \mathrm{~b}$ | $4.00-4.02$ | WinoGrande | 69.53 | $\underline{72.61}$ | 67.96 | 71.59 | 73.88 |
|  |  | PIQA | 78.62 | $\overline{79.22}$ | 77.86 | 78.94 | 80.58 |
|  |  | ARC Challenge | 42.32 | $\underline{45.99}$ | 42.75 | 46.67 | 49.06 |
| KVQuant- $4 \mathrm{~b}-1 \%$ | $4.32-4.35$ | WinoGrande | 70.72 | 73.40 | 68.67 | $\underline{72.30}$ | $\underline{73.72}$ |
|  |  | PIQA | 78.40 | 79.16 | 78.07 | $\overline{79.27}$ | $\overline{80.74}$ |
|  |  | ARC Challenge | 41.38 | $\overline{46.76}$ | 43.17 | 47.87 | 49.91 |
| $\mathrm{CQ}-2 \mathrm{c} 8 \mathrm{~b}$ | 4.00 | WinoGrande | 70.40 | 72.45 | 68.27 | 72.53 | 73.48 |
|  |  | PIQA | 78.61 | 79.11 | 77.91 | 78.62 | 80.52 |
|  |  | ARC Challenge | $\underline{41.55}$ | $\underline{45.99}$ | $\overline{43.34}$ | $\underline{47.78}$ | $\underline{49.15}$ |
| KVQuant-2b | $2.00-2.02$ | WinoGrande | 53.59 | 59.35 | 51.70 | 51.30 | 63.46 |
|  |  | PIQA | 72.47 | 74.81 | 63.38 | 65.40 | 75.46 |
|  |  | ARC Challenge | 32.00 | 34.47 | 22.44 | 24.66 | 38.57 |
| KVQuant-2b-1\% | $2.32-2.35$ | WinoGrande | 68.03 | 71.43 | 67.64 | 70.17 | 70.80 |
|  |  | PIQA | 77.69 | 78.51 | 76.60 | 78.51 | 79.65 |
|  |  | ARC Challenge | 38.74 | 45.14 | 41.47 | 44.97 | 47.53 |
| CQ-4c8b | 2.00 | WinoGrande | 67.48 | 70.72 | 66.45 | 69.06 | 69.38 |
|  |  | PIQA | $\overline{76.11}$ | $\underline{78.29}$ | $\underline{76.12}$ | $\overline{77.42}$ | $\underline{79.49}$ |
|  |  | ARC Challenge | $\underline{38.48}$ | $\underline{44.03}$ | $\underline{39.93}$ | $\underline{44.11}$ | $\underline{45.65}$ |
| KVQuant-1b | $1.00-1.02$ | WinoGrande | 50.51 | 48.46 | 50.91 | 49.41 | 49.80 |
|  |  | PIQA | 53.26 | 53.54 | 53.37 | 50.92 | 54.73 |
|  |  | ARC Challenge | 21.76 | 21.33 | 20.65 | 21.67 | 19.88 |
| KVQuant-1b-1\% | $1.32-1.35$ | WinoGrande | 56.67 | 61.01 | $\underline{57.77}$ | $\underline{57.30}$ | 58.17 |
|  |  | PIQA | $\underline{71.38}$ | $\underline{75.46}$ | $\overline{69.91}$ | $\overline{70.89}$ | 73.83 |
|  |  | ARC Challenge | $\overline{29.69}$ | $\underline{35.32}$ | $\underline{31.48}$ | 32.59 | 33.19 |
| CQ-8c8b | 1.00 | WinoGrande | 56.51 | $\underline{61.56}$ | 55.01 | 57.14 | 58.25 |
|  |  | PIQA | 71.16 | 73.99 | 71.22 | 73.01 | 75.24 |
|  |  | ARC Challenge | $\underline{30.20}$ | 33.79 | $\overline{30.20}$ | $\underline{34.30}$ | $\underline{33.79}$ |
| CQ-8c10b | 1.25 | WinoGrande | 60.46 | 65.27 | 59.19 | 62.98 | 63.93 |
|  |  | PIQA | 73.45 | 75.90 | 73.07 | 74.37 | 77.31 |
|  |  | ARC Challenge | 33.28 | 37.12 | 34.64 | 38.74 | 39.59 |

introduce additional inference overhead due to the extra sparse matrix multiplications for activation outliers which is inefficient on GPUs, while CQ does not have this limitation.

The accuracy results on different benchmarks are shown in Table 3. CQ consistently outperforms the non-dense-and-sparse baseline KVQuant-<b>b at bit-width of 1 and 2 , and performs better or on par with the dense-and-sparse baseline KVQuant-<b>b-1\%.

### 4.2 Ablation Study

We perform a set of ablation experiments to study the effectiveness of each component of our proposed approach. The results of the ablation experiments are shown in Table 4. We evaluate the perplexity of 2 models Mistral-7b and LLaMA-2-13b on WikiText-2 using CQ at 2 bits per FPN, with varying number of channels coupled and comparing uniform centroid learning and Fisher-guided centroid learning. Fisher-guided centroids significantly improve model quality as demonstrated by lower perplexity. With either uniform centroids or Fisher-guided centroids, perplexity improves as the number of coupled channels increases. Hence, our proposed techniques of channel coupling and

Table 4: Ablation study: perplexity on WikiText-2 using $\mathrm{CQ}$ with varying number of coupled channels and fisher-guide centroids. Perplexity consistently improves as the number of coupled channels increases.

| Bits Per FPN | Mistral-7b |  |  |  |  |  | LLaMA-2-13b |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 |
| Num. of Channels Coupled | 1 | 2 | 4 | 1 | 2 | 4 | 1 | 2 | 4 | 1 | 2 | 4 |
| Fisher-guided Centroids | $x$ | $x$ | $x$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ | $x$ | $x$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| Perplexity $\downarrow$ | 97.76 | 16.29 | 5.42 | 5.34 | 5.20 | 5.11 | 890.42 | 171.96 | 6.62 | 6.06 | 4.91 | 4.81 |

Fisher-guided centroid learning are both effective for maintaining model quality.

### 4.3 Overhead of Centroid Learning and Storage

In this section, we discuss the computational overhead of centroid learning and the memory overhead of centroid storage for $\mathrm{CQ}$. The centroid learning process of $\mathrm{CQ}$ consists of many independent kmeans runs, which can be time-consuming on CPUs. Hence, we leverage a GPU implementation to accelerate the learning process. In all our experiments, we use k-means++ initialization and run 100 iterations of $\mathrm{k}$-means on a single GPU to obtain the centroids. The memory overhead of storing the centroids can be calculated as $l \times 2 \times h \times c \times 2^{b}$ 16-bit floating-point numbers, where $l$ is the number of layers, 2 is for keys and values, $h$ is the number of key/value attention heads, $c$ is the number of channels in a single-head key/value activation embedding, and $b$ is the bit width of quantized codes. The detailed learning and memory overhead for different CQ configurations and models are given in Table 5. CQ can easily scale to large model sizes with low learning and memory overheads.

Table 5: Learning and memory overhead of different CQ configurations and models. The number of centroid parameters are shown in millions, and the percentage to the model weights is shown in brackets.

|  | Centroid Learning Time |  | Parameter Count in Centroids |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| CQ Config. | 2c8b | $4 \mathrm{c} 8 \mathrm{~b}$ | 8c8b | 2c8b | 4c8b | 8c8b |
| LLaMA-7b | $53 \mathrm{mins}$ | $28 \mathrm{mins}$ | $14 \mathrm{mins}$ | $67.11 \mathrm{M}(0.996 \%)$ | $67.11 \mathrm{M}(0.996 \%)$ | $67.11 \mathrm{M}(0.996 \%)$ |
| LLaMA-13b | $94 \mathrm{mins}$ | $44 \mathrm{mins}$ | $22 \mathrm{mins}$ | $104.86 \mathrm{M}(0.806 \%)$ | $104.86 \mathrm{M}(0.806 \%)$ | $104.86 \mathrm{M}(0.806 \%)$ |
| LLaMA-2-7b | $54 \mathrm{mins}$ | $28 \mathrm{mins}$ | $14 \mathrm{mins}$ | $67.11 \mathrm{M}(0.996 \%)$ | $67.11 \mathrm{M}(0.996 \%)$ | $67.11 \mathrm{M}(0.996 \%)$ |
| LLaMA-2-13b | $83 \mathrm{mins}$ | $44 \mathrm{mins}$ | $23 \mathrm{mins}$ | $104.86 \mathrm{M}(0.806 \%)$ | $104.86 \mathrm{M}(0.806 \%)$ | $104.86 \mathrm{M}(0.806 \%)$ |
| Mistral-7b | $13 \mathrm{mins}$ | $7 \mathrm{mins}$ | $4 \mathrm{mins}$ | $16.78 \mathrm{M}(0.232 \%)$ | $16.78 \mathrm{M}(0.232 \%)$ | $16.78 \mathrm{M}(0.232 \%)$ |

## 5 Related Works

The high memory requirements and computational demands of LLMs pose a great challenge to efficient inference. Post-training model weight quantization has been shown to be effective for reducing inference latency and memory requirements. GPTQ [8] scales approximate Hessian-based weight quantization to large-scale models, and AWQ [19] preserves the salient weights in LLMs to achieve better quantized model quality. SqueezeLLM [16] leverages sensitivity-based non-uniform clustering and dense-and-sparse quantization for preserving salient weights and outliers. In addition to weight quantization, KV cache compression approaches are also effective for improving inference efficiency. Scissorhands [20] and H2O [37] achieve KV cache compression while preserving model quality by
evicting unimportant tokens and only storing pivotal tokens. KIVI [21] quantizes key activations channel-wise and value activations token-wise, and uses a residual to achieve better model quality. KVQuant [10] proposes sensitivity-based quantization and dense-and-sparse quantization for KV cache. FlashAttention [5] improves the inference efficiency of attention-based models on GPUs by fusing kernels to eliminate unnecessary global memory reads/writes, while NoMAD-Attention [36] accelerates attention computations on CPUs by leveraging in-register shuffles. Product Quantization [12] is an approximate nearest neighbor search method that compresses vectors by decomposing the vector space into a Cartesian product of low-dimensional subspaces, and jointly quantizing the dimensions within each subspace.

## 6 Conclusion

In this work, we propose Coupled Quantization (CQ) for enabling more efficient LLM inference by compressing KV cache, which is the latency and throughput bottleneck in long context or large batch size settings. We observe that distinct channels of key/value activation embeddings exhibit high levels of dependency, which has not been leveraged by existing compression methods. Motivated by this insight, we propose channel coupling for exploiting the inter-channel dependency to achieve more information-efficient encoding of key/value activations. Furthermore, we propose Fisher-guided centroid learning to better preserve salient activations and model quality. Extensive experiments demonstrate that our method mostly outperforms existing methods in terms of model quality under the same quantization bit width. Moreover, $\mathrm{CQ}$ can preserve model quality reasonably well with KV cache quantized down to 1-bit.

## References

[1] David Arthur, Sergei Vassilvitskii, et al. k-means++: The advantages of careful seeding. In Soda, volume 7, pages 1027-1035, 2007.

[2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432-7439, 2020.

[3] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023.

[4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

[5] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022.

[6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.

[7] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning, pages 2793-2803. PMLR, 2021.

[8] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate posttraining quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022 .

[9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

[10] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024.

[11] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks. arXiv preprint arXiv:2103.10427, 2021.

[12] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117-128, 2010.

[13] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[14] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023.

[15] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and individual differences, 103:102274, 2023.

[16] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.

[17] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004.

[18] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021.

[19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for $1 l m$ compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.

[20] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for $1 \mathrm{~lm}$ kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024.

[21] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024.

[22] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, $28(2): 129-137,1982$.

[23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019 .

[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.

[27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021 .

[28] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, $27(3): 379-423,1948$.

[29] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.

[30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[33] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-ofthe-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38-45, 2020.

[34] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective longcontext scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.

[35] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health records. NPJ digital medicine, 5(1):194, 2022.

[36] Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, and Anshumali Shrivastava. Nomad-attention: Efficient llm inference on cpus through multiply-add-free attention. arXiv preprint arXiv:2403.01273, 2024.

[37] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024.
