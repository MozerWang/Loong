# Sparks of GPTs in Edge Intelligence for Metaverse: Caching and Inference for Mobile AIGC Services 

Minrui Xu, Dusit Niyato, Fellow, IEEE, Hongliang Zhang, Jiawen Kang, Zehui Xiong,<br>Shiwen Mao, Fellow, IEEE, and Zhu Han, Fellow, IEEE


#### Abstract

Aiming at achieving artificial general intelligence (AGI) for Metaverse, pretrained foundation models (PFMs), e.g., generative pretrained transformers (GPTs), can effectively provide various AI services, such as autonomous driving, digital twins, and AI-generated content (AIGC) for extended reality. With the advantages of low latency and privacy-preserving, serving PFMs of mobile AI services in edge intelligence is a viable solution for caching and executing PFMs on edge servers with limited computing resources and GPU memory. However, PFMs typically consist of billions of parameters that are computation and memory-intensive for edge servers during loading and execution. In this article, we investigate edge PFM serving problems for mobile AIGC services of Metaverse. First, we introduce the fundamentals of PFMs and discuss their characteristic fine-tuning and inference methods in edge intelligence. Then, we propose a novel framework of joint model caching and inference for managing models and allocating resources to satisfy users' requests efficiently. Furthermore, considering the in-context learning ability of PFMs, we propose a new metric to evaluate the freshness and relevance between examples in demonstrations and executing tasks, namely the Age of Context (AoC). Finally, we propose a least context algorithm for managing cached models at edge servers by balancing the tradeoff among latency, energy consumption, and accuracy.


Index Terms-Metaverse, mobile edge networks, artificial intelligence-generated content, generative pretrained transformers, joint caching and inference

## I. INTRODUCTION

Towards artificial general intelligence (AGI) in Metaverse [1], [2], pretrained foundation models (PFMs), e.g., generative pretrained transformers (GPTs) [3], with billions of parameters achieve great success across a variety of fields over the past few years due to their effectiveness at demonstrating emergence abilities in downstream tasks with different data modalities [4]. The pretraining approach offers a reasonable parameter initialization for extensive downstream applications, such as object detection, image generation, and text retrieval. Therefore, PFMs, including language foundation models (LFMs), visual foundation models (VFMs), and multimodal foundation models (MFMs), are in the paradigm of

Minrui Xu and Dusit Niyato are with the School of Computer Science and Engineering, Nanyang Technological University, Singapore 639798, Singapore. Hongliang Zhang is with the School of Electronics, Peking University, Beijing 100871, China. Jiawen Kang is with the School of Automation, Guangdong University of Technology, China. Zehui Xiong is with the Pillar of Information Systems Technology and Design, Singapore University of Technology and Design, Singapore 487372, Singapore. Shiwen Mao is with the Department of Electrical and Computer Engineering, Auburn University, Auburn, AL 36849-5201 USA. Zhu Han is with the Department of Electrical and Computer Engineering, University of Houston, Houston, TX 77004 USA, and also with the Department of Computer Science and Engineering, Kyung Hee University, Seoul 446-701, South Korea. transfer learning that can generalize to new tasks and domains without any task-specific data during pretraining.

PFMs can empower a multitude of intelligent services for Metaverse, such as autonomous driving, digital twins (DTs), and artificial intelligence-generated content (AIGC) for extended reality (XR). For instance, PFMs can facilitate complex driving decisions and generate traffic simulations for autonomous driving [5]. Moreover, PFMs can help understand and respond to human emotions and behaviors during immersive human-avatar interactions. For example, based on the GPT-3 [3], which is an LFM with 175 billion parameters, ChatGPT ${ }^{1}$ enables long and fluent conversations with humans using world knowledge and contextual awareness. In addition to serving PFMs at cloud servers, edge servers equipped with GPU resources can also support fine-tuning and inference processes of Metaverse services, which brings the sparks of GPTs to mobile edge networks. Therefore, deploying PFMs in mobile edge networks allows the provision of low-latency, personalized, customized, and privacy-preserving AI services.

However, compared to cloud servers, resource-constraint edge servers cannot load all PFMs simultaneously to satisfy the requests of services in Metaverse. Aiming at provisioning mobile AI services in edge networks, existing works primarily focus on offloading AI services to cloud servers for remote execution or caching inference outputs at edge servers for low-latency access [6]. On the one hand, offloading PFMs of AI services to cloud servers results in extra core networking latency, traffic overhead, and privacy risks for users utilizing AI services. On the other hand, caching inference outputs at edge servers is no longer efficient for provisioning realtime AI services. Therefore, directly deploying PFMs at edge servers requires effective and fine-grained resource and request management for executing AIGC requests with available computing and energy resources at edge servers.

Specifically, in contrast to existing works on joint service caching and task offloading [7], there are several unique difficulties for joint PFM caching and inference to balance the tradeoff among accuracy, latency, and energy consumption in edge intelligence as follows [8].
- Dynamic Runtime Configuration: During the execution of PFMs, there are varying numbers of requests and performance requirements for downstream tasks, such as accuracy and latency [6].
- Equivalent Model Adaptation: Different PFMs that can be applied to similar downstream tasks in various[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_88220f7b73a0a0e27aa4g-2.jpg?height=716&width=1791&top_left_y=184&top_left_x=167)

Fig. 1: Categories of PFMs and their characteristic fine-tuning and inference methods. (1)-(3) The workflows of LFMs, VFMs, and MFMs. (a)-(c) The illustration of parameter-efficient fine-tuning. (d) An example of in-context learning.

Metaverse services adaptively [4]. This introduces a challenge for edge servers, as cached PFMs can be used for inference interchangeably to minimize model misses.

- Continuous In-context Learning: PFMs, like GPT-3, can continuously learn and adapt to new domains and tasks based on interactive demonstrations for personalization and customization [9]. The ability of in-context learning enables cached PFMs to improve their performance during inference without parameter updates. This adds complexity in making cache replacement and deployment decisions, as it presents a new tradeoff among inference latency, resource consumption, and accuracy.

To address these issues, this article investigates the potential but scarcely studied problems of PFM caching and inference in mobile edge networks. We first introduce the fundamentals of PFMs for serving mobile AIGC services of Metaverse, and their fine-tuning, and inference methods in edge networks. Then, we present a joint model caching and inference framework in edge networks to serve PFMs of mobile AI services of Metaverse. Furthermore, we discuss potential applications and challenges of serving PFMs for Metaverse services. Finally, to balance the tradeoff among inference latency, resource consumption, and accuracy, we propose a novel metric to indicate the freshness and relevance of examples in demonstrations and current tasks, namely the Age of Context (AoC). The AoC follows the non-increasing utility function that affects the effective examples in context from the entirety of demonstrations resulting from historical interactions. Based on this metric and the number of examples in context, we propose a least context (LC) algorithm to manage PFMs at edge servers. Experimental results demonstrate that the proposed $\mathrm{LC}$ algorithm can reduce the total system cost by improving the accuracy of edgecached PFMs, reducing offloading latency, and utilizing the caching and computing resources of edge servers efficiently.

## II. Serving PFMs of Services in Metaverse

## A. Fundamentals of Pretrained Foundation Models

PFMs belong to the transfer learning paradigm that is used to initialize parameters for downstream tasks. PFMs, such as BERT, GPT-3, Stable Diffusion, CLIP, and ChatGPT, leverage large-scale datasets and pretraining techniques to provide reasonable parameter initialization for various AI services [4]. As shown in Fig. 1. there are primarily three types of PFMs, i.e., LFMs, VFMs, and MFMs, which are widely employed to provide AI services.

1) Language Foundation Models: LFMs, also known as large-scale language models, are PFMs designed to understand, process, and generate human languages. LFMs are trained on massive amounts of text data and can develop a broad understanding of language, including grammar, syntax, semantics, and even some aspects of common knowledge. Two examples of PFMs are GPT and ChatGPT, which have demonstrated impressive abilities in natural language understanding and generation. GPT-3 can enable conversations with humans based on world knowledge and contextual awareness, while ChatGPT is designed to generate human-like responses in a chatbot setting. These models employ self-attention mechanisms to better understand the context and relationships between words in a given text and can be adopted in various downstream tasks, such as sentiment analysis, machine translation, text summarization, question-answering, and text generation.
2) Visual Foundation Models: VFMs specialize in understanding and generating complex images and videos, which are designed to process visual information and generate target outputs. VFMs have shown great potential in advancing the field of computer vision, but they are computing-intensive, particularly during the inference stage. For example, the UNet in Stable Diffusion [10], which is a generative model that can produce high-quality images by iteratively refining a noise vector. Stable Diffusion uses a diffusion process to

![](https://cdn.mathpix.com/cropped/2024_06_04_88220f7b73a0a0e27aa4g-3.jpg?height=440&width=1395&top_left_y=192&top_left_x=365)

Fig. 2: An illustration of the performance of zero-, one-, and few-shot accuracy under different model caching settings [3].

create realistic and high-quality images, and it has been shown to outperform other generative models on a variety of tasks.

3) Multimodal Foundation Models: MFMs can process multiple types of data, such as text, images, and audio simultaneously. They are trained on datasets containing various data modalities to learn the relationships, patterns, and structures within and across different data types. For instance, CLIP is one of the MFMs that classify images based on textual descriptions [11], which uses contrastive learning to train on text and image pairs, distinguishing between positive and negative pairs. During inference, the model takes in an image and a textual description and outputs a score representing the likelihood that the image matches the description, calculated through a dot product. Furthermore, MFMs can be fine-tuned on specific tasks by training them on a smaller dataset.

## B. Fine-Tuning of Pretrained Foundation Models

Fine-tuning refers to the process of improving the performance of PFMs to a specific downstream task by updating its parameters. Since PFMs usually consist of billions of parameters, the fine-tuning process is computationally intensive. Therefore, parameter-efficient fine-tuning of PFMs is utilized for achieving comparable performance to traditional finetuning while reducing resource consumption [12]. As shown in Fig. 1, parameter-efficient fine-tuning can be categorized into three types, including addition-based, specification-based, and reparameterization-based methods as follows [13].

- Addition-based methods involve adding a small number of parameters to the PFMs and fine-tuning them. These methods, which include scalar addition, vector addition, and layer addition, add parameters to the PFMs that are specific to the fine-tuning data. For instance, such parameters include additional layers or heads after the output layer of PFMs.
- Specification-based methods modify the architecture of PFMs to better suit downstream tasks. These methods, such as layer removal, layer replacement, and layer scaling, adjust the PFMs' parameters and architecture to improve performance.
- Reparameterization-based methods reduce the number of tunable parameters in PFMs by reparameterizing their parameters. These methods, such as low-rank factorization, matrix decomposition, and subspace projection, reparameterize the $\mathrm{PFMs}$ to reduce the number of tunable parameters while preserving the PFMs' expressiveness.

Depending on applications such as Metaverse, the finetuning methods can be selected adaptively depending on the resource and performance requirements.

## C. Inference of Pretrained Foundation Models

Different from fine-tuning that updates the parameters of PFMs, the inference is to make predictions on input service requests without changing the parameters. Instead of injecting or updating neural modules in AI models, PFMs can provide accurate output for the task that does not exist in the training, fine-tuning, and inference from instructions and demonstrations from interaction without parameter updates. As shown in Fig. 2, there are three scenarios during the inference of PFMs [3], including zero-shot, one-shot, and fewshot learning. First, zero-shot learning refers to the PFMs that are evaluated on a task for which it has not been explicitly trained. Then, one-shot learning indicates the PFMs need to perform the inference for a new task based on only one example of that task. Finally, few-shot learning implies that a few demonstrations are provided before the inference of the new task. Based on the few-shot learning, the PFMs can perform a meta-gradient in the self-attention layer for adaptation to the new task. Different from fine-tuning, fewshot learning or in-context learning can perform meta-gradient in the attention layers during inference without changing its model parameters. Therefore, few-shot learning can improve the model performance based on examples in instructions and/or demonstrations. However, extra computation consumption and latency are required by processing the examples which depend on the size of the context window in PFMs.

## III. JOINT MODEL CACHING AND INFERENCE

FRAMEWORK

To serve PFMs in edge intelligence for Metaverse, we develop a framework of joint model caching and inference to satisfy service level objectives by utilizing caching, computing, and communication resources in mobile edge networks. Unlike content caching in content delivery networks (CDNs), such as text, images, and videos, the cached models have different cache structures. The cache structure in CDNs is static, with fixed cache sizes and independent of computation
resources [7]. However, due to the flexible configuration of PFMs, the cache structures are dynamic, adjusting to the service requirements in the Metaverse service layer and depending on computation resources during fine-tuning and inference. In the framework, we discuss the model caching configuration and model caching and eviction policy in the PFM layer. Then, we introduce a collaborative mobile edgecloud layer for joint model caching and inference.

## A. Model Caching Configuration

The configuration of each cached PFM consists of the following information.

- Frequency of Use: Frequency of use for PFMs refers to the rate at which a particular model is executed for services in Metaverse. It can be measured in terms of the number of requests per second, the total time spent on processing each request, and other metrics that measure how often a PFM is being utilized.
- Model Sizes: Model size indicates the number of parameters, weights, and other necessary components of PFMs, which affects the latency and energy cost of edge servers for loading and executing PFMs [6].
- Runtime GPU Memory: Runtime GPU memory measures how much RAM or VRAM (video random access memory) is needed by loading the PFM to execute on a given edge/cloud server with its current configuration settings. The runtime GPU memory not only depends on the model sizes but also the runtime precision configuration of the precision. Therefore, there is a trade-off between model precision and GPU memory usage.
- Model Speed: Model speed of PFMs refers to the time complexity or speed at which a particular model can process inference requests. It is usually measured in terms of inference times, i.e., how long it takes for a PFM to complete its task given certain inputs and parameters. Model Speed also has implications on accuracy as faster processing often leads to lower accuracy due to less computation power being available for each request.
- Model Accuracy: Model accuracy of PFMs refers to the degree of correctness or precision with which a model can predict outcomes. For PFMs, model accuracy has implications on speed as higher accuracy often requires more computation power for each request, leading to longer processing times overall.
- Number of Examples in Context: Cached PFMs can accumulate instructions and demonstrations while processing inference requests. The number of examples in context represents the number of related examples in demonstrations the PFMs have gathered. Due to the incontext learning ability of PFMs, the number of examples in context can also impact the accuracy of the models. The size of the context window limits the maximum number of examples in context that can be utilized for each PFM during each inference.


## B. Model Caching and Eviction

Since the cache structure of PFMs is more complicated than traditional web/content caching, the model caching and

![](https://cdn.mathpix.com/cropped/2024_06_04_88220f7b73a0a0e27aa4g-4.jpg?height=651&width=881&top_left_y=192&top_left_x=1077)

Fig. 3: An illustration of the collaborative mobile edge-cloud computing architecture for serving PFMs for Metaverse.

eviction are also more intractable. Model caching and eviction mainly consist of two types of operations, i.e., the passive and active operation as well as binary and partial operation.

- Passive and Active Caching and Eviction: Passive caching is a reactive approach where models are evicted from the cache only when there is not enough GPU memory to load a requested model. Additionally, active caching is a proactive approach where models are evicted and loaded into GPU memory based on predictions of future demand. Active caching can be more efficient than passive caching [6], but requires more sophisticated prediction algorithms and can be less responsive to sudden changes in demand.
- Binary and Partial Caching and Eviction: Binary caching involves loading the entire model into GPU memory before starting inference. In contrast, with partial caching, only a portion of the model is loaded into memory, and inference can begin using that portion. This approach provides a lower level of inference but can be useful when memory resources are limited. When additional memory becomes available, the remaining portions of the model can be loaded into memory, improving inference quality.

If the framework can accurately predict future service request demand, it can leverage active and partial caching to enhance the quality of mobile AI services in Metaverse and reduce resource consumption in mobile edge networks.

## C. Collaborative Mobile Edge-Cloud Caching and Inference

As shown in Fig. 3, collaborative resource allocation among heterogeneous mobile edge-cloud infrastructures is critical in paving the way toward AGI at the edge.

1) Mobile Caching and Inference: Pedestrians and vehicles can process the services to cache and execute PFMs with their local computing resources on mobile devices or devices nearby. This solution can be useful in situations where internet connectivity is limited or unreliable.
2) Edge Caching and Inference: When the local resources of mobile devices and vehicles are not enough for executing PFMs, offloading these services to edge servers via radio access networks become an alternative solution for enabling AI services on edge servers with limited resources. However, due to the limited GPU resources of edge servers, they can only cache several PFMs to react to the user's request. If the edge server does not cache the model requested by the user, it can migrate the user's request to the cloud for execution via core networks or load the model and then execute the model requested by the user. This approach can improve response time and reduce the load on the cloud infrastructure, making it more scalable and cost-effective.
3) Cloud Caching and Inference: Cloud caching and inference solutions involve the utilization of powerful cloud servers to provide almost all PFMs for serving users' requests. However, offloading services to cloud servers incur additional core network latency, which might cause congestion in core networks if there are too many service requests.

## D. Model Caching and Eviction Policy

To design the model caching and eviction policy, three issues should be considered carefully.

- Reducing model miss rate: Actively preloading models and optimizing GPU utilization through dynamic scheduling of AI models can minimize latency and model miss rates, streamlining memory use and request handling.
- Addressing model misses: Handling model misses at edge servers involves offloading service requests to cloud servers, incurring extra core network latency, or loading missing models, leading to switching costs, such as additional latency and energy consumption for allocating resources as well as hardware wear-and-tear.
- Timing model cache decisions: Making cache decisions when the model is first loaded and upon receiving new requests enables dynamic adjustments based on current conditions and usage patterns, promoting efficient caching and lower latency responses.

Therefore, an effective and efficient caching algorithm in this framework should address these three questions properly. Then, we can summarize two remarks for serving PFMs of mobile AI services as follows.

Remark 1: Different from traditional edge content caching in content delivery networks, whose cache structures are static and independent from computation, the cache structures can be dynamic based on the service runtime configuration, such as batch size. This makes the cache loading and eviction of AI services more complex, which requires not only the consideration of user preferences but also the prediction of the intensity of future service requests.

Remark 2: Unlike traditional computation and task offloading in mobile edge networks, where different computation tasks are independent, the inference tasks of PFM-related services are in-contextual. Therefore, before performing these inference tasks, the AIGC model needs to be preloaded into the edge servers' GPU memory, which can cache a limited number of models to provide AI services. Furthermore, as more incontext examples are collected during the interaction, the performance of cached services can be further improved [3].

## IV. Potential ApPlications and ChallenGES

## A. Applications

The potential PFM-based applications in Metaverse include autonomous driving, DTs, semantic communication, and AIGC for XR.

1) Autonomous Driving: Autonomous driving in Metaverse necessitates $\mathrm{AI}$ services such as traffic and driving simulation, which are dependent on computationally-intensive PFMs [5]. To enable this on resource-limited edge servers, model caching, and efficient inference scheduling are essential. In autonomous driving, active model switching can enhance traffic efficiency and safety by adapting to changing road conditions or traffic patterns.
2) Digital Twins: DTs, virtual representations of physical objects or systems, utilize PFMs for AI capabilities like predictive maintenance, anomaly detection, and optimization. The complexity of these systems demands numerous PFMs for accurate modeling across diverse scenarios. Therefore, effective management of the vast number of PFMs, maintaining quality and consistency, and optimizing caching and inference policies can enable DTs to accurately represent physical systems, thus enhancing decision-making and operational efficiency.
3) Semantic Communication: Semantic Communication, a novel paradigm that employs semantic representations, can transform wireless communication systems' design and operation. Its device-to-device pattern enables efficient and secure communication without centralized cloud infrastructure. However, this pattern necessitates advanced model caching algorithms to manage edge servers' limited resources while ensuring cached models' quality and consistency. Implementing progressive caching techniques like active and partial caching can optimize the device-to-device pattern, leading to faster and more reliable AI services on edge servers.
4) AIGC for XR: AIGC is generated by AI methods that utilize PFMs to create content that resembles humanproduced content [12]. To provide AI-generated XR services, multiple PFMs are integrated to handle different types of data and produce relevant and meaningful 3D immersive content. The model caching algorithm ensures that the PFMs work smoothly together, maintaining seamless and immersive experiences for Metaverse users. Achieving this requires careful consideration of the interplay between PFMs and the development of advanced context-aware caching algorithms for efficient cached model management and coordination.

## B. Challenges

1) Dynamic User Service Requests and Objectives: Joint caching and inference services at edge servers face challenges due to dynamic user requests and objectives, such as service latency and accuracy. To tackle these challenges, edge servers must efficiently manage limited resources, ensure cached model quality and consistency, and design joint caching and inference policies to satisfy users' objectives, considering

TABLE I: Detailed parameters and performance of PFMs ( $\mathrm{K}=$ number of examples in context).

|  | Models | Downstream Tasks | Model Size (M) | GFLOPs | $\mathbf{K}$ | Model Performance Score |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  |  |  | Zero-shot | One-shot | Few-shot |
| LFMs | GPT3-13B [3] | Translation | 12850 | 26.54 | 64 | 15.45 | 26.12 | 30.83 |
|  |  | Basic arithmetic |  |  | 50 | 3.79 | $15.98 \quad$ | 14.34 |
|  |  | SuperGLUE |  |  | 32 | $54.4 \quad$ | 64.3 | 66.9 |
|  | GPT-3-175B [3] | Translation | 174600 | 354.03 | 64 | 22.03 | 29.63 | 33.77 |
|  |  | Basic arithmetic |  |  | 50 | 25.99 | 40.71 | $49.55 \quad$ |
|  |  | SuperGLUE |  |  | 32 | 58.2 | 68.9 | 73.2 |
| VFMs | UniFormer-S [14] | Image classification | 22 | 3.6 | - | 82.9 | - | - |
|  |  | Video classification | 22 | 167 |  | 82.8 | - | - |
|  |  | Object detection and <br> instance segmentation | 41 | 269 |  | 45.6 | - | - |
|  |  | Semantic segmentation | 25 | 247 |  | 46.6 | - | - |
|  |  | Pose estimation | 25 | 4.7 |  | 74.0 | $\overline{-} \quad$ - $\quad$ - $\quad$ - | $\overline{-} \quad$ - $\quad$ - $\quad$ - |
|  | UniFormer-B [14] | Image classification | 50 | 8.3 | - | 83.9 | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_88220f7b73a0a0e27aa4g-6.jpg?height=43&width=126&top_left_y=723&top_left_x=1791) |
|  |  | Video classification | 22 | 389 |  | 84.0 | - | - |
|  |  | Object detection and <br> instance segmentation | 69 | 399 |  | 47.4 | - | - |
|  |  | Semantic segmentation | 54 | 471 |  | 48.0 | - | - |
|  |  | Pose estimation | 54 | 9.2 |  | 75.0 | - | - |
| MFMs | CLIP-ViT-L/14 11] | Classification | 428 | 175.5 | - | 75.20 | - | ![](https://cdn.mathpix.com/cropped/2024_06_04_88220f7b73a0a0e27aa4g-6.jpg?height=42&width=126&top_left_y=936&top_left_x=1791) |
|  |  | Image retrieval |  |  |  | 71.08 | - | - |
|  |  | Text retrieval |  |  |  | 84.00 | - | - |
|  | CLIP-ViT-H/14 [11] | Classification | 986 | 381.9 | - | 77.97 | $\underline{-} \quad$ - | $\underline{-} \quad$ - |
|  |  | Image retrieval |  |  |  | 73.43 | - | - |
|  |  | Text retrieval |  |  |  | 86.04 | - | - |

factors like model size, frequency of use, and accuracy. Addressing these challenges might require edge servers to develop prediction models for various mobile AI services and propose active caching and inference algorithms.

2) Heterogeneous Model Configuration and Computing Resources: Heterogeneous model configuration and computing resources present challenges in proposing joint model caching and inference algorithms. In detail, PFMs' structure and available edge servers result in varying GPU memory and compute resource requirements, which is typically formulated as an NP-hard mixed integer programming problem, complicating the optimization of caching and inference policies. Moreover, distinct model architectures and computation requirements add complexity. To address these challenges, edge servers must efficiently allocate resources according to each model's specific requirements while considering local computing device availability and capabilities.
3) Context-aware Caching and Inference Algorithms: Codesigning caching and inference algorithms considering contextual information in mobile AI services at edge servers is challenging due to the indirect correlation between model caching and inference duration. Joint policies need to optimize resource allocation according to each model and inference requests' specific requirements while considering user objectives, model size, usage frequency, and accuracy. By codesigning caching and inference algorithms considering the number of examples in context, as shown in Fig. 4, edge servers can utilize extra computation resources for improving the accuracy of PFMs.

## V. Use CaSE of SERVing GPTs in Edge IntelLIGENCE FOR METAVERSE

## A. Mobile AIGC Service Serving Model

We consider an intelligent transportation system in Metaverse system with a remote cloud center, an edge server, and multiple vehicles, serving different Metaverse services, including autonomous driving, DTs, and AIGC-based XR, based on various PFMs. For instance, pedestrians and passengers can immerse themselves in Metaverse with XR by creating and interacting with AI-generated XR content synthesized by PFMs. When users do not have enough resources on their devices and onboard units for executing PFMs, they need to offload requests to edge servers or cloud servers for remote execution. Usually, an AIGC service requires multiple PFMs to work in synergy to satisfy the user's requirements in Metaverse. For example, the Stable Diffusion services consist of three types of PFMs [10], including a Variational Autoencoder that compresses images into a smaller dimensional latent space, a pretrained CLIP ViT-L/14 for conditioning, and a U-Net block that denoises the output from forward diffusion backward to obtain a latent representation.

The detailed parameters and performance of PFMs need to be considered in intelligent transportation systems of Metaverse are listed in Table I, including GPT3-13B [3], GPT3175B [3], Uniformer-S [14], Uniformer-B [14], CLIP-ViTL/14 [11], and CLIP-ViT-H/14 [11]. As we can observe, only LFMs are large enough to have in-context learning ability, while VFMs and MFMs are relatively small. As shown in Table I and Fig. 4, PFMs utilize meta-gradients to learn from context and improve performance as the user interacts with them. Then, the few-shot accuracy can be fit using the data in Table I. Therefore, contextual information has a corresponding

![](https://cdn.mathpix.com/cropped/2024_06_04_88220f7b73a0a0e27aa4g-7.jpg?height=545&width=702&top_left_y=172&top_left_x=251)

Fig. 4: The accuracy in downstream tasks of GPT3-13B/ 175B versus number of examples in context. The few-shot accuracy $a_{2}=a_{0}+a_{1} \log _{2}\left(1+K^{\alpha}\right)$, where $a_{0}$ is zero-shot accuracy, $a_{1}$ is one-shot accuracy, and $\alpha$ is coefficient.

impact on the quality of service provided by AIGC, such as the accuracy of PFMs. Although the introduction of context in PFMs can improve the model performance, the size of the context window also affects the resource consumption and latency during the inference of the model. As shown in Fig. 2, the freshness and relevance of the examples in demonstrations decrease over time until it is no longer pertinent to the current generation task, which is rarely measured in previous work.

## B. Age of Context and Lease Context Algorithm

Therefore, we propose the AoC for evaluating the relevance and freshness of examples in demonstrations that affect the quality of services of PFMs in currently executing downstream tasks. During inference of PFMs, the questions and answers can be recorded in the context windows as examples in demonstrations and instructions. These examples can be leveraged to improve the accuracy of PFMs as they can perform metagradient to fit these examples. However, the meta-gradient might have positive or negative effects on the accuracy, which depends on their quality, relevance, and timeliness. Similar to the age of information (AoI) [15], the AoC indicates the relevance and timeliness of historical contextual examples in demonstrations to the cached PFM and the current inference task. As shown in Fig. 2, the AoC follows the non-increasing age utility function, factoring with a vanishing coefficient of context. Based on the AoC, the number of examples in content can be calculated as the weighted sum of number of examples in demonstrations. Then, the accuracy of PFMs can be obtained by some function w.r.t. number of examples in context as the functions demonstrated in Fig. 4

Finally, we introduce the LC algorithm, based on the AoC, to manage PFMs for mobile AIGC services efficiently. The LC algorithm tracks the number of examples in context, calculating them and removing the cached PFM with the least contexts, i.e., number of examples in context, when GPU memory is needed for loading a new PFM. This approach is effective for large numbers of PFMs on edge servers with limited GPU memory, prioritizing the removal of the least
TABLE II: Detailed system performance comparison for different caching algorithms.

|  | Random | Cloud | FIFO | LFU | LC |
| :---: | :---: | :---: | :---: | :---: | :---: |
| System <br> cost | 25.67 | 7.29 | 27.51 | 5.93 | $\mathbf{4 . 8 8}$ |
| Switching <br> cost | 18.72 | 0 | 23.28 | 0.37 | $\mathbf{0 . 3 2}$ |
| Total <br> accuracy <br> cost | 0.13 | 0 | 0.52 | $\mathbf{0 . 3 6}$ | 0.44 |
| Average <br> accuracy <br> cost | 0.0151 | 0 | 0.0085 | 0.0083 | $\mathbf{0 . 0 0 7 6}$ |
| Inference <br> latency | 0.12 | 0 | 1.30 | 1.32 | $\mathbf{1 . 2 6}$ |
| Offloading <br> latency | 0.04 | 0 | 0.35 | $\mathbf{0 . 2 4}$ | 0.31 |
| Cloud <br> cost | 6.63 | 7.29 | $\mathbf{2 . 0 5}$ | 3.63 | 2.52 |
| Edge <br> Execution <br> Ratio | $9.8 \%$ | $0 \%$ | $\mathbf{7 0 . 7 \%}$ | $49.4 \%$ | $65.0 \%$ |

relevant PFM for the current inference task. Consequently, the accuracy of PFMs of mobile AIGC services is improved by leveraging more contextual information during inference.

In the experiment, we compare the proposed LC algorithm with Random, cloud-only, first-in-first-out (FIFO), and least frequently used (LFU) baselines. With the objective of minimizing service latency and accuracy loss, the system cost is calculated as the sum of the switching cost, the total accuracy cost, the edge inference latency, the edge offloading latency, and the cloud computing cost. As listed in Table II, the performance of the proposed LC algorithm can achieve minimum total system cost while maintaining a high edge execution ratio, which indicates that most of the services are executed at edge servers. Especially, compared with the LFU algorithm, the least context (LC) algorithm can achieve a lower average service accuracy cost by efficiently leveraging the incontext learning ability of PFMs and contextual information.

## VI. CONCLUSIONS

In the article, we have studied edge caching and inference for serving PFMs in edge intelligence for Metaverse. We have proposed a joint model caching and inference framework for bringing the sparks of GPTs to mobile edge networks, toward achieving AGI. Specifically, we have proposed a new metric for evaluating the relevance and freshness of contextual examples and currently executing tasks. Furthermore, we have proposed the LC algorithm for cache replacement to improve the utilization of historical contextual information and thus increase the accuracy of mobile AIGC services. The experimental results demonstrate that the LC algorithm can reduce system costs and improve the execution ratio at edge servers.

## REFERENCES

[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., "Sparks of artificial general intelligence: Early experiments with GPT-4," arXiv preprint arXiv:2303.12712, Mar. 2023, [Online]. Available: https://arxiv.org/abs/ 2303.12712

[2] P. Zhou, J. Zhu, Y. Wang, Y. Lu, Z. Wei, H. Shi, Y. Ding, Y. Gao, Q. Huang, Y. Shi et al., "Vetaverse: Technologies, applications, and visions toward the intersection of metaverse, vehicles, and transportation systems," arXiv preprint arXiv:2210.15109, Oct. 2022, [Online]. Available: https://arxiv.org/abs/2210.15109

[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Proc. of the Advances in Neural Information Processing Systems, vol. 33, pp. 1877-1901, Dec. 2020.

[4] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He et al., "A comprehensive survey on pretrained foundation models: A history from BERT to ChatGPT," arXiv preprint arXiv:2302.09419, Feb. 2023, [Online]. Available: https://arxiv.org/abs/2302.09419

[5] M. Xu, D. Niyato, J. Chen, H. Zhang, J. Kang, Z. Xiong, S. Mao, and Z. Han, "Generative AI-empowered simulation for autonomous driving in vehicular mixed reality metaverses," arXiv preprint arXiv:2302.08418, Feb. 2023, [Online]. Available: https://arxiv.org/abs/2302.08418

[6] G. R. Gilman, S. S. Ogden, R. J. Walls, and T. Guo, "Challenges and opportunities of dnn model execution caching," in Proc. of the Workshop on Distributed Infrastructures for Deep Learning, Davis, CA, Dec. 2019, pp. 7-12.

[7] J. Xu, L. Chen, and P. Zhou, "Joint service caching and task offloading for mobile edge computing in dense networks," in Prof. of the IEEE INFOCOM, Honolulu, HI, May 2018, pp. 207-215.

[8] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, "Edge intelligence: Paving the last mile of artificial intelligence with edge computing," Proc. of the IEEE, vol. 107, no. 8, pp. 1738-1762, Jun. 2019.

[9] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, "A survey for in-context learning," arXiv preprint arXiv:2301.00234, Jan. 2023, [Online]. Available: https://arxiv.org/abs/ 2301.00234

[10] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "Highresolution image synthesis with latent diffusion models," in Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, New Orleans, LA, Jun. 2022, pp. 10684-10695.

[11] M. Cherti, R. Beaumont, R. Wightman, M. Wortsman, G. Ilharco, C. Gordon, C. Schuhmann, L. Schmidt, and J. Jitsev, "Reproducible scaling laws for contrastive language-image learning," arXiv preprint arXiv:2212.07143, Dec. 2022, [Online]. Available: https://arxiv.org/abs/ 2212.07143

[12] M. Xu, H. Du, D. Niyato, J. Kang, Z. Xiong, S. Mao, Z. Han, A. Jamalipour, D. I. Kim, V. Leung et al., "Unleashing the power of edge-cloud generative ai in mobile networks: A survey of aigc services," arXiv preprint arXiv:2303.16129, Mar. 2023, [Online]. Available: https://arxiv.org/abs/2303.16129

[13] N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C.M. Chan, W. Chen et al., "Parameter-efficient fine-tuning of large-scale pre-trained language models," Nature Machine Intelligence, vol. 5, pp. 220--235, Mar. 2023.

[14] K. Li, Y. Wang, J. Zhang, P. Gao, G. Song, Y. Liu, H. Li, and Y. Qiao, "Uniformer: Unifying convolution and self-attention for visual recognition," arXiv preprint arXiv:2201.09450, Jan. 2022, [Online]. Available: https://arxiv.org/abs/2201.09450

[15] X. Chen, C. Wu, T. Chen, Z. Liu, H. Zhang, M. Bennis, H. Liu, and Y. Ji, "Information freshness-aware task offloading in air-ground integrated edge computing systems," IEEE Journal on Selected Areas in Communications, vol. 40, no. 1, pp. 243-258, Nov. 2021.

Minrui Xu (minrui001@e.ntu.edu.sg) received the B.S. degree from Sun Yat-Sen University, Guangzhou, China, in 2021. He is currently working toward the Ph.D. degree in the School of Computer Science and Engineering, Nanyang Technological University, Singapore. His research interests mainly focus on mobile edge computing, deep reinforcement learning, and incentive mechanism design.
Hongliang Zhang [M'19] (hongliang.zhang92@gmail.com) is an assistant professor in the School of Electronics at Peking University, Beiijng, China. He was the recipient of the 2021 IEEE ComSoc Heinrich Hertz Award.

Dusit Niyato [M'09, SM'15, F'17] (dniyato@ ntu.edu.sg) is currently a professor in the School of Computer Science and Engineering, Nanyang Technological University, Singapore. He received the B.Eng. degree from King Mongkuts Institute of Technology Ladkrabang (KMITL), Thailand in 1999 and Ph.D. in electrical and computer engineering from the University of Manitoba, Canada in 2008. His research interests are in the areas of Internet of Things (IoT), machine learning, and incentive mechanism design.

Jiawen Kang [M'18] received the Ph.D. degree from the Guangdong University of Technology, China in 2018. He was a postdoc at Nanyang Technological University, Singapore from 2018 to 2021. He currently is a professor at Guangdong University of Technology, China. His research interests mainly focus on blockchain, security, and privacy protection in wireless communications and networking.

Zehui Xiong [M'20] (zehui_xiong @sutd.edu.sg) is an Assistant Professor at Singapore University of Technology and Design. Prior to that, he was a researcher with Alibaba-NTU Joint Research Institute, Singapore. He received the Ph.D. degree in Computer Science and Engineering at Nanyang Technological University, Singapore. He was a visiting scholar with Princeton University and University of Waterloo. His research interests include wireless communications, network games and economics, blockchain, and edge intelligence.

Shiwen Mao [S'99, Mâ€™04, SM'09, F'19] (smao@ieee.org) received his Ph.D. in electrical and computer engineering from Polytechnic University, Brooklyn, NY. He is a Professor and Earle C. Williams Eminent Scholar, and Director of the Wireless Engineering Research and Education Center at Auburn University. His research interests include wireless networks and multimedia communications.

Zhu Han [S'01, M'04, SM'09, F'14] (zhuhan22@ gmail.com) currently is a professor in the Electrical and Computer Engineering Department at the University of Houston, Texas. He has been an AAAS Fellow since 2019. He received the IEEE Kiyo Tomiyasu Award in 2020. He has been a 1 percent highly cited researcher since 2017 according to Web of Science.


[^0]:    ${ }^{1}$ https://openai.com/blog/chatgpt/

