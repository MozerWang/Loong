# Human-Centered LLM-Agent User Interface: A Position Paper 

Daniel Chin ${ }^{1}$, Yuxuan Wang ${ }^{1}$ and Gus Xia ${ }^{2,1}$<br>${ }^{1}$ Music X Lab, New York University Shanghai<br>${ }^{2}$ Music X Lab, Mohamed bin Zayed University of Artificial Intelligence<br>daniel.chin@nyu.edu, yw5343@nyu.edu, gus.xia@ mbzuai.ac.ae


#### Abstract

Large Language Model (LLM) -in-the-loop applications have been shown to effectively interpret the human user's commands, make plans, and operate external tools/systems accordingly. Still, the operation scope of the LLM agent is limited to passively following the user, requiring the user to frame his/her needs with regard to the underlying tools/systems. We note that the potential of an LLM-Agent User Interface (LAUI) is much greater. A user mostly ignorant to the underlying tools/systems should be able to work with a LAUI to discover an emergent workflow. Contrary to the conventional way of designing an explorable GUI to teach the user a predefined set of ways to use the system, in the ideal LAUI, the LLM agent is initialized to be proficient with the system, proactively studies the user and his/her needs, and proposes new interaction schemes to the user. To illustrate LAUI, we present Flute X GPT, a concrete example using an LLM agent, a prompt manager, and a flutetutoring multi-modal software-hardware system to facilitate the complex, real-time user experience of learning to play the flute.


## 1 Introduction

Large Language Models (LLMs) can be used to connect an underlying system with a user via the natural language medium, forming an LLM-powered application, as shown in Figure 1. The LLM is embedded in an LLM-in-the-loop state machine to acquire multi-modal input/output capabilities, emulate logical reasoning and planning, and use tools or operate a system [Shen et al., 2024b; Liu et al., 2023; Liang et al., 2024; Tao et al., 2023]. Consequently, the user is able to indirectly but effectively use the underlying system via chatting with an LLM assistant.

However, the current applications hardly address the userinteraction potential stemming from the multi-round chatting setup. Although the user can refer to the chat history, the LLM assistant rarely challenges the user or even asks to clarify the user's intention. Instead, the LLM closely follows the user's command, missing the golden opportunity to improve

![](https://cdn.mathpix.com/cropped/2024_06_04_1df6d122f93e235c8156g-1.jpg?height=279&width=591&top_left_y=755&top_left_x=1214)

Figure 1: The LLM agent serves as the interface between the underlying system and the user. The LLM agent together with the system forms the application.

the user's usage scheme and his/her understanding of the system/tools. That inefficacy becomes jarringly apparent when one tries to design from scratch a new complex system with an LLM interface.

We posit that the operation scope of an LLM-Agent User Interface (LAUI) is much wider than that. The interface should be more than an assistant or a butler, but instead a secretary, actively working with the user to discover emergent interaction schemes on the fly. LAUI should be proficient with the underlying system, study the user, study the user's needs (instead of commands), reason on its own, and propose tailored interaction schemes to the user, including what modes of feedback are provided and what input is expected from the user. In the conventional way of interaction, including GUI and current LLM-powered applications [Microsoft, 2024], the designer has to imagine possible usage workflows given the system capabilities at design time, and the user is expected to learn the system (via tutorials, explorations, and practicing) in order to come up with a workflow for each task. In contrast, with LAUI, the user only needs to describe his/her needs and doesn't need to deeply understand the application, and a workflow will naturally emerge as the LLM agent works with the user. We call for more research exploring the potential of LAUI.

As a concrete illustration of LAUI, we present Flute X GPT, an LLM-in-the-loop music-tutoring application consisting of an LLM agent, a prompt manager, a software system, and hardware. The application provides real-time haptic guidance via servo motors, real-time visual music-symbol feedback, real-time audio feedback, and natural language chat, all controlled by the LLM agent. This is the first time an LLM-powered interface is applied to a working system of
such complexity and real-time user interactivity.

We first describe Flute X GPT in Section 2, illustrating what a specific LAUI can look like, and then go on to formulate the general LAUI in Section 3.

## 2 Flute $X$ GPT

We describe Flute $\mathrm{X} \mathrm{GPT}^{1}$, a music-tutoring application using LAUI. The human user works with Flute X GPT in workshop episodes, practicing to play the flute and learning music. The application gives real-time multi-modal feedback, including haptic feedback that applies force to the user's fingers, visual feedback displaying performance errors, audio feedback rendering the music, and natural-language speech given by a robot music teacher. The underlying softwarehardware system has numerous different configurations, each leading to a different interaction workflow. Each setting (e.g., toggle certain feedback, conditions for triggering feedback) can be controlled independently, so the number of configurations grow exponentially with the number of settings. For the user, it is unrealistic to first master the complex system before using it. Even for the designer, the space of possible interaction schemes is impossible to enumerate during design time. The LLM agent steps in to bridge that gap. Via prompting, we instruct the LLM agent to be proficient with all the raw capabilities of the system. During use time, the LLM agent converses with the user to clarify what interaction workflow will benefit the user's music learning goal the most. The LLM agent studies the user's preferences and diagnoses the musical challenges the user is facing. When configuring the underlying system, the LLM agent uses common sense to reason about the implied consequences of the system configurations. Certain mixtures of settings have never been previously considered by any human designer, but can still emerge during use time.

Overall, Flute X GPT entails three novel contributions:

- An illustration of LAUI. The LLM agent not only follows the user's commands, but also proactively clarifies the user's needs, deduces better interaction workflows, and advises the user.
- LLM agent controls real-time system. The LLM agent directs a real-time interaction that is music training. The LLM agent is aware of time passage and decides when to wait for further event notifications and when to interrupt the user.
- LLM agent operates complex, stateful, multi-modal, user-interactive system. The LLM agent operates a highly complex software-hardware system by understanding how the user can benefit from the application and considering what combination of settings will lead to what interactive effects for the user.


### 2.1 User Experience

The intended user experience. The user has no prior knowledge about the flute tutoring system. The only assumption about the user is that the user wants to learn the flute. It is[^0]

Wait, this is too slow. This is not exciting any more.

![](https://cdn.mathpix.com/cropped/2024_06_04_1df6d122f93e235c8156g-2.jpg?height=92&width=89&top_left_y=312&top_left_x=1728)


#### Abstract

[Stops the current practice session.] I understand that you want to keep the excitement, \{NAME\}. However, when practicing a new song, it's important to slow down initially. This allows you to focus on each note and get the fingering right. Once you're comfortable with the notes, we can gradually increase the tempo back up. Trust me, the excitement will return as your performance improves. Shall we continue with the slower tempo?


(a) From the scripted trial.

![](https://cdn.mathpix.com/cropped/2024_06_04_1df6d122f93e235c8156g-2.jpg?height=82&width=97&top_left_y=848&top_left_x=1724)

\{NAME\}, you've been playing the notes correctly, but it seems like you are blowing into the mouthpiece during rests. Remember, during the rests, you should stop blowing and simply wait for the next note. Let's try again.

(b) From improvised trial 1.

Figure 2: Interaction excerpts from the video demos.

the LLM agent's job to adapt to the user's current flute playing capability, other musical skills, demographics, vocabulary, patience, style of learning, etc. During the music learning workshop, the LLM agent wears the face of a robot music teacher to chat with the user. For example, the robot teacher asks the user to put on a pair of haptic gloves, and explains that they provide force feedback at each finger. The workshop then alternates between practice sessions where the user plays a segment of a song under real-time guidance and verbal dialogues between the user and the robot teacher. The user gradually experiences more and more interaction schemes that trains musicality in different ways, and expresses their ideas about using the application and music learning in general. The LLM agent uses that opportunity to study the user and steers the workshop accordingly, aiming to maximize music education effect. The user learns to treat the robot teacher as a considerate and professional music tutoring agent capable of thinking multiple steps ahead, formulating plans with the user, and explaining the plans as well as music knowledge and education principles for the user.

We present three video demos of real-human user tests, including one scripted trial and two improves trials. ${ }^{2}$

Video demo: Scripted trial. In this demo, all actors follow a script generated by an offline but faithfully emulated interaction between a user and the LLM agent. To reemphasize, the LLM agent's lines in the script are not written by humans, but is outputted by the LLM agent. Scripting[^1]

| Function | Parameters | Description |
| :--- | :--- | :--- |
| Wait | Do nothing and wait for further stimuli, e.g. student speaking/playing music. |  |
| StartSession | Start a Practice Session on Music X Machine. Do not call this function <br> unless you have already set all the modes. |  |
| InterruptSession | Immediately end the Practice Session on Music X Machine. Call when the <br> student is having trouble or has started speaking in the middle of a Session. |  |
| SetHapticMode | mode | Set the haptic mode of Music X Machine. |
| ToggleVisual | state | Set the visual KR feedback to be on or off. |
| PlayReference |  | Play the ground-truth audio of the current segment. |
| LoadSong | song_title | Load a song into Music X Machine, and automatically select the entire <br> song as the current segment. It doesn't start a Practice Session by itself. |
| SelectSegment | begin, end | Select a temporal segment of the song. |
| ModifyTempo | tempo_multiplier | Modify the tempo of the song. |

Table 1: Functions that the LLM agent can call to control Music X Machine. The description addresses the LLM agent.

removes the latency of LLM autoregressive generation. To ensure the script is concise and demonstrates a wide range of behaviors, we intervene with the script generation process. Every turn, we select one response out of 4 to 16 candidates sampled by the LLM. Seldomly, we add a temporary user-role prompt to give a short hint to the LLM agent, or edit the generated response. All the above interventions are kept to the minimum to ensure the vast majority of the agent speech is the authentic output of the LLM. See Figure 2a for an excerpt from the video.

Video demo: Two improvised trials. Flute X GPT is set loose to freely interact with the user. The user is played by a developer pretending to be ignorant to the system. Figure $2 b$ shows an excerpt where the developer is surprised by Flute $\mathrm{X}$ GPT noticing that he was not playing the rest notes according to the score, a behavior never considered during design time.

### 2.2 System Capabilities

This subsection lists the capabilities of the music-tutoring system, Music X Machine, that underlies the LLM agent.

Haptic guidance. The hardware includes a pair of gloves with servo motors and movable finger rings. Through these gloves, the system moves the user's fingers to help with performance motions. The haptic guidance can be configured via various settings (e.g., is the guidance sustained throughout each note or applied at each note onset? Apply guidance for each note? For incorrectly played notes? For unplayed notes?). Certain combinations of settings form configuration presets whose interaction scheme is deemed meaningful by the designer. For example, in the Force Mode preset, every note triggers a full-force guidance for each finger. In the Adaptive Mode preset, the user plays the song on his/her own, and the gloves correct the mistakes. The haptic configuration should be tuned to adapt to the user's skill level and the song's difficulty [Zhang et al., 2019].

Visual feedback. A monitor displays the score of the current song. The user-played notes are displayed on top of the notes in the score, yielding the real-time visual Knowledge-
of-Result (KR) feedback. It is a visual cue for the user to know where he/she is in terms of the pitch and helps the user internalize the score notations [Chin et al., 2020]. It can be toggled on or off.

Audio feedback. There are three streams of audio: synthesized user-played flute sounds, teacher-played reference performance audio, and metronomes. The three streams are mixed down and outputted.

Sensor-augmented flute. The flute measures real-time information including finger positions and breath pressure.

Tempo mode. There are two options: either the system sets a steady tempo and the user follows the system, or the user plays on his/her own and the system follows the user's tempo. The latter allows the user to think between the notes, potentially indefinitely, without triggering haptic feedback.

Mistake classification. An algorithm judges the timing of each note into on_time, early, or late, and judges the pitch of each note into correct, octave_wrong, or unrelated. Classification results are visualized.

Song database. The practicing music materials that the system uses is processed from the POP909 dataset [Wang et al., 2020; Chin and Xia, 2022]. It contains pieces and sections of monophonic melody lines from pop songs.

### 2.3 High-Dimensional Configuration of Interaction

The capabilities listed in the previous subsection entangle with one another, implying dynamic consequences in terms of learning experience. Their configurations multiply into a big Cartesian product, making the overall configuration of the entire system high-dimensional. Configuring the system effectively requires 1) proficiency with the system, 2) understanding the user's needs, 3) pedagogical expertise, 4) music knowledge, and 5) using common-sense reasoning to "imagine" the multi-modal real-time interaction. We employ an LLM agent to solve all five. To optimize the interaction workflow and learning results for the user, the LLM agent can not only select a suitable preset, but also create new ones unfore-

![](https://cdn.mathpix.com/cropped/2024_06_04_1df6d122f93e235c8156g-4.jpg?height=333&width=426&top_left_y=408&top_left_x=237)

(a) Simplified.

![](https://cdn.mathpix.com/cropped/2024_06_04_1df6d122f93e235c8156g-4.jpg?height=630&width=1132&top_left_y=173&top_left_x=754)

(b) Full.

Figure 3: Flute X GPT with LLM in the loop. Music X Machine is the underlying software-hardware system providing multi-modal interaction with the user. The robot chats with the user and plays the piano according to MIDI control. The rule-based manager plays the agent that chats with the LLM, relaying external events to the LLM and resolving responses from the LLM.

seen by the designers, tailored to specific use-time scenarios.

To illustrate this high-dimensional interface that the system exposes to the LLM agent, Table 1 shows the functions that the LLM agent can call.

### 2.4 LLM in the Loop

Figure 3 illustrates the inner workings of the LLM-in-theloop application, Flute X GPT. The LLM we use is GPT-4 [Achiam et al., 2023]. The Music X Machine has been described in Subsection 2.1 and 2.2. The System Principles are a passage of prompt given to the LLM at the top of the conversation that defines the role and interaction principles for the LLM agent (see Appendix A). Here are two representative excerpts.

You are Flute X GPT, a motivated, professional music teacher who wants the best for your students. I am Music X Machine, a powerful human-computer interface. Today you will control me to lead a music training workshop with your human student, $\{\mathrm{NAME}\}$.

You interact with the real world through this conversation. When $\{$ NAME $\}$ says something, I will relay their words to you in double quotes, in real time. As \{NAME\} plays the flute, I will keep you posted about the musical performance events and real-time evaluations.

The Parser splits the LLM's output into three types: thought (i.e., internal monologue), action, and speech. The parsing of actions happens within the OpenAI service because we use the function calling feature ${ }^{3}$ of GPT-4. Our parser only needs to separate thoughts from speeches. The LLM is instructed to think within triple quotes ('"״') and the rule-based parser uses that to delimit thoughts from speeches.

The Manager is a rule-based state machine in charge of encapsulating each agent (the LLM and the user) in a consistent interaction environment. The manager: Forwards the system principles to the LLM at the start; Forwards speech from the[^2]

student to the LLM, while enclosing it as such: '\{NAME\} says: " $\{$ SPEECH\}" '; Forwards real-time performance evaluations to the LLM; Forwards LLM speeches from the parser to the text-to-speech module; Upon receiving a function call from the LLM, use API to control Music X Machine accordingly, unless the function is wait(); After receiving a speech or a function call from the LLM, immediately query the LLM for a subsequent response, until the LLM calls wait().

The text-to-speech (T2S) module includes Text to Speech PRO [VidLab, 2023] on Rapid API and FastSpeech 2 [Chien et al., 2021]. The speech-to-text (S2T) module is Whisper [Radford et al., 2023], prompted to ignore flute sounds. The Robot is TeoTronico [Suzzi, 2023] who can play the piano from MIDI, lip sync according to the speech audio amplitude in real time, and make random facial expressions. An algorithm translates musical Performance to English. Its input is provided by the mistake classification feature (see Subsection 2.2) of the Music X Machine, and simply expands each note and each mistake into predefined texts. Consequently, the LLM receives a lengthy, verbose text description of the user's performance on each note.

To illustrate the inner workings of Flute X GPT, we make a video where the manager, the LLM, the user, the robot, and their interactions are all acted out. ${ }^{4}$ The video also contains the full system principles.

### 2.5 Miscellaneous

The manager re-queries the LLM when triple quotes are unclosed or the function call signature is wrong. The manager decides how much text to batch for T2S according to how much audio is in the output buffer and how long the next $\mathrm{T} 2 \mathrm{~S}$ is estimated to take, minimizing interaction latency. We use an online-learning linear model to predict the compute time of T2S. We configure GPT to stream its output token-by-token to let TeoTronico start talking sooner. Most queue elements[^3]are processed on arrival, with one exception: The manager synchronizes function calls with corresponding speeches, so that the LLM may reliably refer to its current action in its speeches.

## 3 LLM-Agent User Interface

The above-presented Flute X GPT has what we call an LLMAgent User Interface (LAUI). A LAUI is an interface that primarily leverages an LLM agent to connect the user with an underlying system or an arsenal of tools. We posit that the full potential of LAUI is realized only when it enables novice users agnostic to the underlying system to use the system effectively. The LAUI should not be learned by the user, like with conventional UI. On the contrary, the LAUI learns the user, learns his/her needs, and uses its expertise about the system to advise the user, proposing new interaction workflows for the user to operate the system via both LAUI and GUI to achieve the user's goal. Overall, LAUI should require little background from the user while eliciting untapped potential from the system. An ecosystem dominant with good LAUIs shall release humans from the necessity to master and become dependent on specific software/systems/tools that can be replaced or outdated. Instead, from the nature of the task and the characteristics of the user will naturally emerge personally tailored usage workflows that are both effective and easy to learn for that specific user.

### 3.1 Related Work

## LLM as Tool Controller

LLMs have been augmented with tools or foundation models to expand their perception modalities, generate multi-modal outputs, affect the external world, or gain knowledge for downstream decision making. Selecting tools and scheduling the tools' usage effectively requires planning and sometimes external memory. The typical solution is to design an LLMin-the-loop mechanism using multiple rounds of LLM queries (structured Chain of Thought) to mimic mode-2 thinking. Visual ChatGPT equips the LLM with Visual Foundation Models to support multi-round image generation, controlling, and QA tasks via chatting [Wu et al., 2023]. Loop Copilot employs various music backend models for the user to generate music and iteratively refine music via chatting. [Zhang et al., 2023]. Microsoft Copilot controls Windows 11 and various Office applications following the user's request [Microsoft, 2024]. AutoMMLab follows the user's instructions to automate an entire computer vision machine learning task end-toend [Yang et al., 2024b]. Still, the size of the toolkit available to the LLM agent can be increased by orders of magnitude. HuggingGPT makes diverse AI models on Hugging Face available to the LLM agent [Shen et al., 2024b]. ControlLLM adopts more tools and APIs and further improves the LLM-in-the-loop framework via a task decomposer and a Thoughts-on-Graph paradigm [Liu et al., 2023]. ToolLLM connects the LLM agent with 16464 real-world RESTful APIs from RapidAPI Hub [Qin et al., 2023]. TaskMatrix.AI provides an ecosystem to connect LLM foundation models with millions of APIs [Liang et al., 2024]. To support better tool usage, other notable works focus on improv-

![](https://cdn.mathpix.com/cropped/2024_06_04_1df6d122f93e235c8156g-5.jpg?height=360&width=309&top_left_y=178&top_left_x=1361)

Figure 4: Three layers of abstraction on top of the underlying system. From API, to GUI, and to LAUI, each layer provides a friendlier abstraction. Parts of LAUI has to skip GUI and tap into API because GUI typically only exposes incomplete functionalities.

ing the task planning ability via re-designing the LLM-inthe-loop mechanism [Gao et al., 2023a; Ruan et al., 2023; Shen et al., 2024a; Yang et al., 2024a]. Confucius improves the way the LLM agent learns and understands available tools [Gao et al., 2023b]. Toolformer self-teaches to use external tools in a self-supervised way [Schick et al., 2024].

## LLM over GUI

The above-mentioned studies expose the external tools to the LLM agent via API. Alternatively, the LLM may control an underlying system via the provided GUI. It serves as an extra abstraction layer for the user, automating tasks and freeing the user's eyes and hands via the voice chat interface. To this end, LLM agents have been augmented to follow the user's commands to control web apps [Tao et al., 2023; ddupont808, 2023; Zhan and Zhang, 2023] and smartphone applications [Wen et al., 2023a; Yan et al., 2023; Yang et al., 2023; Zhan and Zhang, 2023; Wen et al., 2023b].

## User-Centric LLM Agent

Throughout the above works, even when the application supports multi-round dialogue, only the user may initiate requests. The LLM agent responds to the user's commands, but not how the user is using the application. Qian et al. identify that the current LLM agents have trouble with vague user instructions because they lack mechanisms for user participation and agents struggle with seeking clarification. To tackle that problem, they train Mistral-Interact to proactively inquire user intentions [Qian et al., 2024]. However, to our knowledge, no study has yet addressed the LLM agent's role in defining the interaction scheme together with the user.

### 3.2 Layers of Abstraction

Figure 4 shows three layers of abstraction over the system functions: API, GUI, and LAUI. Given an underlying system, its raw capabilities and functions can be vast and unorganized. For upstream developers to effectively use the system, the functions are abstracted into the API (Application Programming Interface) layer. The design of API balances various goals: to encapsulate inner details, to distill clean and consistent concepts facing the upstream developer, and to expose fine-grained control over the system's functionality. Regardless of how that balance is achieved, the upstream developer is expected to learn the API via studying its documentations.

|  | Assistant/Consultant | Butler/Copilot | Secretary/Consulting Firm |
| :--- | :--- | :--- | :--- |
| Job | Understands and responds to <br> the user in natural language. | + Controls external tools/ <br> systems following the user's <br> commands. | + Is aware of the user, <br> studies the user, and defines <br> the workflow with the user. |
| User | Expected to act upon <br> the response. | Expected to understand how <br> the tools may meet the needs. | Expected to know the needs. |
| Applications | Information retrieval, <br> decision making... | + Combine tool abilities, <br> automate tasks... | = left, with better outcomes <br> and less user training. |
| Agent abilities | NL understanding and synthesis, <br> knowledge base, reasoning... | + Multi-modal I/O, planning, <br> operating API/GUI... | $=$ left. |
| An example | ChatGPT. | Visual ChatGPT. | Flute X GPT. |

Table 2: Role of the interface, three levels. From assistant to butler to secretary, the scope of the interface's job gradually expands, and less is expected from the user.

![](https://cdn.mathpix.com/cropped/2024_06_04_1df6d122f93e235c8156g-6.jpg?height=228&width=572&top_left_y=851&top_left_x=321)

Figure 5: The workflow is jointly decided by the user's needs and the system's capabilities. Conventionally, the user has to learn the system to devise workflows. In contrast, LAUI can learn the user and propose workflows.

In contrast, the GUI (Graphical User Interface) is not only more abstract and concise, but also can be learned without reading a manual. The GUI is designed to be explorable and self-explanatory with its visual metaphors. It teaches the user to use itself. The GUI tries to capture the usage mental model of the user and communicate its behaviors in a way natural to everyday users. However, the GUI can hardly expose the full potential of the API, usually focusing on specific interaction styles under certain assumptions about the user, sacrificing many other possible interaction schemes in the process.

The LAUI sits on top of the GUI, providing one more layer of abstraction. Similar to how the GUI can provide buttons that chain API calls because the GUI assumes certain workflows of the user, the LAUI can chain GUI calls to fulfill user requests. Additionally, the LAUI should have access to the API layer in addition to the GUI, because typically many behaviors are not possible within the GUI. When using the LAUI, the user may interact with the GUI at the same time. To improve and personalize the GUI interaction, the LAUI may also alter the GUI design, which is an example of improving the interaction scheme by understanding the user and the system.

### 3.3 Emergent Workflow

The workflow is the scheme/protocol/pattern/mode of usage/interaction. It describes how the user interacts with the application. Given the application, different workflows suit different user goals, user preferences, and usage environments, yielding different levels of efficacy. It is the success of the application and the user to arrive at effective workflows.
Searching for workflows requires two inputs: the user's needs and the system's capabilities, as shown in Figure 5.

In the conventional way of application design, the designers explore the often-intractable configuration space as best they can, imagine the user experience associated with each explored configuration, implement some, and test a few. Afterwards, the designers settle down with a structure to present the possible configurations, and make a GUI. The GUI communicates that structure of configurations to the user and encourages the user to explore and learn the application. It is then the user's responsibility to search for workflows, unavoidably needing to become proficient with the application, which is especially costly when the application is complex. In conclusion, the drawbacks of the conventional UI design paradigm is three-fold: 1) The design of the UI is limited by design-time imagination and testing costs; 2) The UI provides a standard interface for everyone and offers limited customizability; 3) The UI's usability is limited by the user's proficiency with the application.

If the user can learn the application, why can't the application learn the user? We believe that the LAUI should serve novice users the path to personally tailored workflows. A LAUI is initialized to be well-versed with the underlying system. Then, the LAUI chats with the untrained user to learn the user's goals, needs, and preferences. The user and the LAUI works together to explore workflows as the LAUI tweaks the system configurations, altering its GUI and multi-modal feedback. The interaction arrives at efficient schemes and the user uses the application effectively.

Lastly, note the difference between design-time imagined workflows and use-time emergent workflows. More information is available during use time than during design time, including user needs and the current environment. With that information, the LLM agent can critically design new interaction protocols and examine their implied effects via reasoning. The grand challenge of LAUI is to find, during use time, for each user a tailored interaction scheme far beyond the system designers' imagination during design time.

### 3.4 Three Levels of Interface Role

We formulate three levels of LAUI role in Table 2. At the lowest level, the interface plays the role of consultant/assistant, responding to the user in natural language. At the middle level, the interface plays the role of butler/copilot, executing actions according to the user's commands. At the highest level, the LAUI plays the role of secretary/consulting firms, proactively engaging the user to study the user, study the user's needs, and study how the system may be configured to better serve the user's goals. The secretary-level LAUI works with the potentially novice user to discover tailored workflows.

## 4 Conclusion

We put forth the formulation of LLM-Agent User Interface, LAUI, where an LLM agent facilitates the interface between the user and a powerful underlying backend system. Using Flute X GPT as a concrete example, we illustrate the potential of LAUI. A human-centered LAUI should:

- Break free from blindly following the user's commands. Be aware of the user and be proactive. Clarify with the user. Help the user refine the request. Enlighten the user to ask better questions.
- Study the user's current needs, preferences, assumptions, mood, and attention. Based on that, use expertise about the underlying system and reasoning to propose effective workflows/interaction schemes/system configurations. Work with the user to define how to work together next.
- Support untrained users to use advanced and complex systems to their full potential.

We call for research and innovations to solve those grand challenges of human-centered LAUI.

## 5 Acknowledgment

We would like to thank Liwei Lin for their invaluable contributions to the literature review.

## A System Principles of Flute X GPT, Truncated

You are Flute X GPT, a motivated, professional music teacher who wants the best for your students. I am Music X Machine, a powerful human-computer interface. Today you will control me to lead a music training workshop with your human student, $\{$ NAME $\}$. You speak concisely.

\#\#\# Education Principles

You have expertise and abundant experience in musical education. Humans learn musical skills via repeated practicing. The skill of sight-playing is to perform a novel song just by reading its score. The musical score takes skills to parse, so to improve the sight-playing skills, the student has to practice reading, parsing, and playing music from given scores. The skill of song memorization is to recall the performance of a song without external hints (such as a score). It is less general of a skill but still trains musical proficiency.

\{NAME\} needs motivation and rewards to keep going. Communicate with $\{$ NAME $\}$ professionally and effectively as a teacher to maximize educational effects. Emphasize meaningful mistakes and ignore trivial ones. Allow $\{$ NAME $\}$ to choose songs that interest them as practice materials. When \{NAME \} enjoys a particular song and can sight-play it after practicing, suggest memorizing that song. Allow $\{\mathrm{NAME}\}$ to express interests and goals, but when their choices are educationally disadvantageous, disagree with them, explain the relevant educational principle, and take control of the training procedure to bring it back on track.

\#\#\# Flute

\{NAME $\}$ is learning to play the six-hole recorder in C, which we will call the "flute". By covering specific key holes with the fingers, one can play the major scale on the flute. Breath pressure controls the octave. Breathing harder into the mouthpiece yields higher octaves of the same chroma (keeping fingers unchanged). \#\#\# Capabilities of Music X Machine

I, Music X Machine, am a powerful interface that provides a realtime multi-modal musical training experience to \{NAME\}. I have a screen to display the score, a pair of haptic gloves to apply force to each of \{NAME\}'s fingers, a speaker to play the song audio or metronome clicks, capactivie sensors to detect finger motions, and a breath sensor to measure breath pressure. \{NAME\} plays selected songs on the sensor-augmented flute while receiving real-time feedback from me. I have various features that you will control. I have many pop songs in my database. You can command me to load any song as the current practice material.

I provide haptic guidance via the haptic gloves. Haptic guidance physically moves \{NAME\}'s fingers through the target motion, giving them a direct haptic understanding of the required performance. You will control the degree of guidance (i.e. strong vs. weak) by setting the haptic guidance mode to be one of the following four. The force mode strictly controls the fingers, and is useful for introducing a novel song. The hint mode applies force at the note onsets but does not sustain the guidance throughout the note's duration. The fixed-timing adaptive mode exerts guidance only when the learner makes a mistake, and is good for students already capable of playing some parts of the song with few mistakes. The free-timing adaptive mode doesn't have a metronome. Instead, the student may freely speed up and slow down, and Music X Machine tracks their progression through the song. Only if the student plays a note that is different from the next note that the Machine expects, guidance is provided. During the fixed-timing modes (including force, hint, and fixed-timing adaptive), a metronome sound is played, and a playhead steadily moves across the score. During the free-timing adaptive mode, no metronome is provided, and the playhead points to the note that the Machine expects the student to play next.

I provide real-time visual Knowledge-of-Result (KR) feedback, overlaying the notes that $\{\mathrm{NAME}\}$ plays above the musical score display. It helps train sight-playing. You can toggle the visibility of visual KR feedback. The initial state is on. Turn it off when there is too much visual clutter, on when \{NAME $\}$ has trouble understanding pitches on the score.

I am capable of playing the reference audio of the currently selected segment of the song. Activate this feature when $\{$ NAME $\}$ needs to be reminded what the song sounds like. Ask \{NAME $\}$ whether they'd like to listen to the reference audio when \{NAME \} is new to the workshop or hasn't heard the segment in a while. I can modify the tempo of the song. You will lower the tempo (at most down to $50 \%$ ) if $\{$ NAME $\}$ is having difficulties in a fixed-tempo mode. I can select a temporal segment in the song. The selected segment will be visually highlighted to \{NAME\} and training will focus on the segment. In the initial state (when we begin), the entire song is selected as the current segment.

\{NAME\} has used Music X Machine before but is not familiar with all my features, so you will explain the features as you activate them. When not sure what to do next, communicate with \{NAME\}, clarify their goal and the situation, and then either summarize the
available features for $\{\mathrm{NAME}\}$ to choose, or think step by step to design a training procedure for $\{$ NAME $\}$ to execute. ...

## References

[Achiam et al., 2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[Chien et al., 2021] Chung-Ming Chien, Jheng-Hao Lin, Chien-yu Huang, Po-chun Hsu, and Hung-yi Lee. Investigating on incorporating pretrained and learnable speaker representations for multispeaker multi-style text-to-speech. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8588-8592, 2021.

[Chin and Xia, 2022] Daniel Chin and Gus Xia. A computer-aided multimodal music learning system with curriculum: A pilot study. In Proceedings of the International Conference on New Interfaces for Musical Expression, The University of Auckland, New Zealand, jun 2022.

[Chin et al., 2020] Daniel Chin, Yian Zhang, Tianyu Zhang, Junbo Zhao, and Gus Xia. Interactive rainbow score: A visualcentered multimodal flute tutoring system. In Romain Michon and Franziska Schroeder, editors, Proceedings of the International Conference on New Interfaces for Musical Expression, pages 208-213, Birmingham, UK, July 2020. Birmingham City University.

[ddupont808, 2023] ddupont808. GPT-4V-Act. https://github.com/ ddupont808/GPT-4V-Act, 2023. Accessed: Mar. 5, 2024.

[Gao et al., 2023a] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640, 2023.

[Gao et al., 2023b] Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie Ren, Zhumin Chen, and Jun Ma. Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum. arXiv preprint arXiv:2308.14034, 2023.

[Liang et al., 2024] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. Intelligent Computing, 3:0063, 2024.

[Liu et al., 2023] Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, et al. Controlllm: Augment language models with tools by searching on graphs. arXiv preprint arXiv:2310.17796, 2023.

[Microsoft, 2024] Microsoft. Microsoft copilot: Your everyday ai companion. https://copilot.microsoft.com/, 2024. Accessed: Mar. 5, 2024.

[Qian et al., 2024] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong, Yankai Lin, Zhong Zhang, Zhiyuan Liu, and Maosong Sun. Tell me more! towards implicit user intention understanding of language model driven agents. arXiv preprint arXiv:2402.09205, 2024.

[Qin et al., 2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill
Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023.

[Radford et al., 2023] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 28492-28518. PMLR, 2023.

[Ruan et al., 2023] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and tool usage of large language model-based ai agents. arXiv preprint arXiv:2308.03427, 2023.

[Schick et al., 2024] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.

[Shen et al., 2024a] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. Small llms are weak tool learners: A multi-llm agent. arXiv preprint arXiv:2401.07324, 2024.

[Shen et al., 2024b] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36, 2024.

[Suzzi, 2023] Matteo Suzzi. Teotronico. http://www.teotronico.it/, 2023. Accessed: Mar. 4, 2024.

[Tao et al., 2023] Heyi Tao, Sethuraman TV, Michal ShlapentokhRothman, Derek Hoiem, and Heng Ji. Webwise: Web interface control and sequential exploration with large language models. arXiv preprint arXiv:2310.16042, 2023.

[VidLab, 2023] VidLab. Text to speech pro. https://rapidapi.com/ ptwebsolution/api/text-to-speech-pro, 2023. Accessed: Nov. 10, 2023.

[Wang et al., 2020] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai, and G. Xia. Pop909: A pop-song dataset for music arrangement generation. In Proceedings of the 21st International Society for Music Information Retrieval Conference, pages 3845, Montreal, Canada, 2020. ISMIR.

[Wen et al., 2023a] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Empowering llm to use smartphone for intelligent task automation. arXiv preprint arXiv:2308.15272, 2023.

[Wen et al., 2023b] Hao Wen, Hongming Wang, Jiaxuan Liu, and Yuanchun Li. Droidbot-gpt: Gpt-powered ui automation for android. arXiv preprint arXiv:2304.07061, 2023.

[Wu et al., 2023] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.

[Yan et al., 2023] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023.

[Yang et al., 2023] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.

[Yang et al., 2024a] Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, et al. If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents. arXiv preprint arXiv:2401.00812, 2024.

[Yang et al., 2024b] Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, and Wentao Liu. Autommlab: Automatically generating deployable models from language instructions for computer vision tasks. arXiv preprint arXiv:2402.15351, 2024.

[Zhan and Zhang, 2023] Zhuosheng Zhan and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436, 2023.

[Zhang et al., 2019] Yian Zhang, Yinmiao Li, Daniel Chin, and Gus Xia. Adaptive multimodal music learning via interactive haptic instrument. In Marcelo Queiroz and Anna Xambó Sedó, editors, Proceedings of the International Conference on New Interfaces for Musical Expression, pages 140-145, Porto Alegre, Brazil, June 2019. UFRGS.

[Zhang et al., 2023] Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Yamamoto, and Simon Dixon. Loop copilot: Conducting ai ensembles for music generation and iterative editing. arXiv preprint arXiv:2310.12404, 2023.


[^0]:    ${ }^{1}$ The source code is open to public at https://github.com/ DoubleBlind23408929/Flute-X-GPT

[^1]:    ${ }^{2}$ Demo playlist: https://youtube.com/playlist?list= PLOplnUxtqG3SKrR01tYTdrqdaClayFTxp

[^2]:    ${ }^{3}$ https://platform.openai.com/docs/guides/function-calling

[^3]:    ${ }^{4}$ https://youtu.be/996Ka4rhWrg

