# MECHANISM DESIGN FOR LLM FINE-TUNING WITH MULTIPLE REWARD MODELS 

Haoran Sun<br>Peking University<br>sunhaoran0301@stu.pku.edu.cn

Wei Chen<br>Microsoft Research Asia<br>weic @microsoft.com

Yurong Chen

Peking University

chenyurong@pku.edu.cn
Siwei Wang

Microsoft Research Asia siweiwang@microsoft.com
Xiaotie Deng

Peking University

xiaotie@pku.edu.cn


#### Abstract

Recent research on fine-tuning large language models (LLMs) through the aggregation of multiple preferences has attracted considerable attention. However, the existing literature predominantly focuses on the empirical performance of aggregation algorithms, while neglecting the underlying motivation for agents to misreport their preferences. In this paper, we formalize this as a multiparameter mechanism design problem, where an LLM provider designs both training and payment rules to achieve specific objectives and promote the truthful reporting of preferences. Firstly, we claim the necessity of a payment scheme by demonstrating that without payments, truth-telling is a strictly dominated strategy under a wide range of training rules. Then, we introduce the affine maximizer payment scheme for the social welfare maximizing training rules that are widely used in practice, which ensures both dominant-strategy incentive compatibility (DSIC) and individual rationality (IR). Furthermore, we prove that under mild conditions, any other payment rule that also implements these training rules in DSIC can be converted to the affine maximizer payment by adding a factor irrelevant to the agents' own reports. We also show that this mechanism satisfies approximate DSIC when the input of the mechanism is a biased version of the reported preferences, showcasing its robustness in real-world applications.


## 1 Introduction

The pre-training and fine-tuning paradigm is fundamental in developing language models (Devlin et al. [2018], Radford et al. [2018], Liu et al. [2019], Touvron et al. [2023]). During pre-training, the model is fed with vast amounts of data to acquire a general capability to understand and generate language through self-supervised learning. The subsequent fine-tuning phase customizes these pre-trained models for specific downstream tasks using smaller, taskoriented datasets, ensuring that the model outputs are more closely aligned with particular requirements. As LLMs gain increasing popularity, there is a growing demand for fine-tuning basic LLMs, as basic models often fail to meet users' demands, especially in catering to individual preferences.

The process of fine-tuning an LLM to align with certain human preferences is challenging to achieve through supervision (Ji et al. [2023], Köpf et al. [2024], Wang et al. [2023b], Shen et al. [2023]), primarily due to the difficulty in constructing datasets with a substantial number of valid question-answer pairs for supervised training. Reinforcement learning from human feedback (RLHF) (Ouyang et al. [2022], Christiano et al. [2017]) offers a promising solution to this problem. In RLHF, a reward model is first trained to be used as a proxy for human judgment. This model then provides reward signals for the standard reinforcement learning process. This technique of fine-tuning with a reward model has proven effective in encoding human preferences into models and has become a fundamental component of the training process for most advanced LLMs. With the advancement of RLHF, numerous studies have investigated efficient methods for aggregating multiple preferences into a single fine-tuned model.

However, most of these studies focus primarily on improving empirical performance across various metrics (Ramé et al. [2024], Wu et al. [2024], Jang et al. [2023], Coste et al. [2023], Zhang et al. [2024], Wang et al.

[2024], Eisenstein et al. [2023]). They often implicitly assume that we are accessible to real preferences, neglecting the possibility of agents' misreporting their preferences. This problem becomes more crucial when we consider a real-world scenario, where different agents provide their preferences for the aggregation. In such cases, agents may engage in strategic misreporting to increase their utility. An intuitive example is that if an agent knows beforehand that the fine-tuning process aims to neutralize all preferences, it might pretend to have a more polarized preference as a beneficial strategy. These strategic behaviors can distort the final training results, even if the trained algorithm is highly effective. Nevertheless, this issue has not attracted sufficient attention in the existing literature, particularly concerning the fine-tuning process of LLMs.

Our Contribution. In this paper, we mainly study the incentive design in such scenarios. First, we formalize this as a multi-parameter mechanism design problem between a fine-tuning service provider and groups of agents seeking fine-tuning services. The provider proposes a mechanism that includes a training rule for integrating different groups' preferences into a fine-tuned model and a payment rule to charge the groups. After observing the mechanism, each group strategically reports its preference to maximize its utility. We consider that the subsequent fine-tuning process is implemented using RLHF, a standard method for aligning a model with human preference. Therefore, we abstract the preference of each group to be reward models, and term the whole scenario the RLHF Game.

Secondly, we demonstrate the profitability of misreporting a polarized preference under a wide range of mechanisms that include only a training rule (Theorem 3.3). This underscores the necessity of a payment rule to address incentive issues.

Thirdly, we focus on a representative set of training rules, termed the SW-Maximizing training rules, in which the provider aims to maximize social welfare while incorporating different regularization measures. For SW-Maximizing training rules, we propose the affine maximizer payment scheme, a weighted version of the Vickrey-Clarke-Groves (VCG) payment Vickrey [1961], Clarke [1971], Groves [1973]). We prove that agents truthfully reporting their preferences constitutes a dominant strategy in such mechanisms (Theorem 4.2). Utilizing the notion of payment equivalence, we prove that under a mild condition, any other payment rule that also implements these training rules in dominantstrategy incentive compatibility (DSIC) can be converted to the affine maximizer payment by adding a factor irrelevant to agents' own reports (Theorem4.5). We validate this condition for many commonly used regularization terms like KL-divergence (Proposition 4.4). Consequently, we derive the revenue-maximizing payment rule that implements SW-Maximizing training rules in both DSIC and individual rationality (IR) (Corollary 4.6). Furthermore, we show that this mechanism remains approximately DSIC when the input of the mechanism is a biased version of the reported preferences, which is an abstraction modeling for the inevitable errors that occur in practice. This showcases the robustness of the proposed mechanisms in real-world applications (Theorem 4.9).

Primary Related Work. Several studies have investigated similar scenarios. Among them, Duetting et al. [2023] and Soumalias et al. [2024] are most related to ours. Duetting et al. [2023] examines the problem of designing a mechanism to aggregate multiple agents' preferences based on each agent's bids and determine their payments. However, they exclude the case where preferences can be misreported, which is the primary concern in our study. The concurrent work by Soumalias et al. [2024] also considers the mechanism design for aggregating multiple preferences. Their focus is mainly on the practical implementation of SW-Maximizing training rule with KL-divergence and the payment scheme that obtains both DSIC and interpretability. However, in this scenario, we are more concerned with the theoretical properties of more general mechanisms, including the implementability and the property of payment equivalence.

Additionally, there are works studying other scenarios related to LLMs from the perspective of algorithmic game theory. Laufer et al. [2023] abstracts the fine-tuning process as a bargaining game and characterizes the perfect subgame equilibria. Dubey et al. [2024] proposes an auction where bidders compete to place their content within a summary generated by an LLM. Conitzer et al. [2024] considers incorporating social choice theory in LLM alignment. Feizi et al. [2023] explores the potential for leveraging LLMs in online advertising systems.

Paper Organization. In Section 2, we provide the preliminaries and the formal description of the RLHF Game. In Section 3, we study the incentive design for general training rules in the RLHF Game. We demonstrate the properties of mechanisms that consist of SW-Maximizing training rules and payment rules in Section4, Further related work is provided in Section 5, and we conclude in Section6.

## 2 Preliminaries and Model

### 2.1 Preliminaries

Large Language Models. Large language models (LLMs) function as mappings from a sequence of tokens to a probability distribution over the next token. The input sequence is usually constrained by a maximum length $K$, thereby making the set of all possible inputs finite. Let $T$ denote the finite set of all tokens, and let $T^{*}:=\emptyset \cup T \cup T^{2} \cup$ $\cdots \cup T^{K}$ represent the set of all possible input sequences with lengths up to $K$.

An LLM parameterized by $\theta \in \Theta$ is denoted as $g_{\theta}: T^{*} \rightarrow \Delta T$, where $\Delta T$ is the set of all probability distributions over the token set $T$. For practical purposes, the output sequence is also required to be of finite length. We assume the maximum output length is also $K$ so that the output space is also $T^{*}$. We denote $\operatorname{LLM}_{\theta}(\boldsymbol{x})$ as the probability of a sequence of tokens $\boldsymbol{x} \in T^{*}$ generated by $g_{\theta}$. Since the model generates a sequence by predicting the next token iteratively until a special ending token is encountered, the relationship between $\mathrm{LLM}_{\theta}$ and $g_{\theta}$ is given by:

$$
\operatorname{LLM}_{\theta}(\boldsymbol{x})=\prod_{t=1}^{|\boldsymbol{x}|} g_{\theta}\left(x_{t} \mid \boldsymbol{x}_{<t}\right)
$$

where $\boldsymbol{x}_{<t}$ denotes the prefix subsequence of $\boldsymbol{x}$ preceding $x_{t}$ and $\boldsymbol{x}_{<1}=\emptyset$. LLM $_{\theta}$ is a distribution over $T^{*}$ and can be represented as a $\left|T^{*}\right|$-dimensional vector, with each coordinate $\boldsymbol{x}$ the probability of $\boldsymbol{x}$ being generated under $g_{\theta}$.

Reward Modeling. Reward modeling is instrumental for aligning LLMs with human preferences, particularly within the context of RLHF. In this process, a reward model $\mathrm{rm}: T^{*} \rightarrow \mathbb{R}$ is first trained on the human-annotated preference dataset by using the Bradley-Terry model (Bradley and Terry [1952]). Essentially, the reward model is a function that maps a sequence of tokens to a real number indicative of the preference for that sequence. Similar to $\mathrm{LLM}_{\theta}$, rm can be also considered as a $\left|T^{*}\right|$-dimensional vector. Following prior empirical work for RLHF (Rafailov et al. [2023]), we consider the normalized reward models which are normalized to have the summation 1 over $T^{*}$ i.e. $\sum_{\boldsymbol{x} \in T^{*}} \operatorname{rm}(\boldsymbol{x})=1$. Furthermore, we also assume that the output rewards are all non-negative, i.e. $\operatorname{rm}(\boldsymbol{x}) \geq 0$ for all $\boldsymbol{x} \in T^{*}$. Unless otherwise stated, we use $\mathcal{R}$ to denote the domain of all reward model functions that satisfy the above conditions. In fact, the results in our paper are also applicable for some other normalization methods like $\max _{\boldsymbol{x} \in T^{*}} \operatorname{rm}(\boldsymbol{x})=1$.

### 2.2 Formulation of the RLHF Game

In this part, we present the formal description of the RLHF Game. There is one LLM provider and $n$ groups of agents, denoted by $[n]=\{1,2, \cdots, n\}$. The provider has an initial model $\operatorname{LLM}_{\theta_{\text {init }}}$ with non-negative probability for all sequences, i.e. $\operatorname{LLM}_{\theta_{\text {init }}}(\boldsymbol{x})>0$ for all $\boldsymbol{x} \in T^{*}$. Each group $i$ has $w_{i}$ agents and a joint preference represented by a reward model $\mathrm{rm}_{i}$. Let $\mathcal{R}$ and $\mathcal{W} \subseteq \mathbb{N}_{+}$denote the domains for each group's reward model and group size, respectively. We assume an upper bound $\bar{w}$ for $\mathcal{W}$. The exact reward model $\mathrm{rm}_{i}$ and the size $w_{i}$ are group $i$ 's private information. For an agent in group $i$, the valuation when it receives a model $\operatorname{LLM}_{\theta}$ is denoted by $v_{i}\left(\theta ; \mathrm{rm}_{i}\right)$. The form of the valuation function $v_{i}(\cdot ; \cdot)$ is known by both the provider and the agents.

The provider first announces the mechanism, including a training rule $\psi$ and a payment rule $p$,

$$
\psi: \mathcal{R}^{n} \times \mathcal{W}^{n} \times \Theta \rightarrow \Theta, \quad p: \mathcal{R}^{n} \times \mathcal{W}^{n} \times \Theta \rightarrow \mathbb{R}^{n}
$$

Both rules take $n$ reported reward models, $n$ reported sizes, and an initial model as input, and output the objective fine-tuned model and each group's payment, respectively. The provider can choose not to charge the users by setting $p$ always equal to 0 . In this case, the model coincides with most previous work, where agents' incentives are not considered (Ramé et al. [2024], Wu et al. [2024], Jang et al. [2023], Coste et al. [2023], Zhang et al. [2024], Wang et al. [2024], Eisenstein et al. [2023]). Specifically, the training rule seeks to find the model that maximizes a certain objective function $f$. That is,

$$
\psi\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta_{\text {init }}\right) \in \arg \max _{\theta \in \Theta} f\left(v_{1}\left(\theta ; \operatorname{rm}_{1}\right), \cdots, v_{n}\left(\theta ; \operatorname{rm}_{n}\right), \vec{w}, D\left(\operatorname{LLM}_{\theta} \| \operatorname{LLM}_{\theta_{\text {init }}}\right)\right)
$$

where $D$ is a measure of the distance between $\operatorname{LLM}_{\theta}$ and $\operatorname{LLM}_{\theta_{\text {init }}}$. We assume that the function $f$ has a unique global optimal point for any possible inputs. Hence, in the rest of the paper, the " $\in$ " in the definition of $\psi$ is substituted by "=".

After observing the announced mechanism $(\psi, p)$, each group $i$ reports a reward model, $\widetilde{\mathrm{rm}}_{i}$, and its group size $\tilde{w}_{i}$. We assume all reported sizes are in $\mathcal{W}$ and therefore bounded by $\bar{w}$. Based on the reported information, the provider fine-tunes the model until the model $\operatorname{LLM}_{\theta_{\text {final }}}$ is optimal, i.e., the final parameter satisfies $\theta_{\text {final }}=\psi\left(\overrightarrow{\mathrm{rm}}, \overrightarrow{\tilde{w}}, \theta_{\text {init }}\right)$. The
provider then charges group $i$ according to the payment rule, $p_{i}\left(\stackrel{\mathrm{rm}}{\vec{w}}, \overrightarrow{\tilde{w}}, \theta_{\text {init }}\right)$. All the members in the group have access to the fine-tuned model $\theta_{\text {final }}$, so the valuation for group $i$ is $w_{i} v_{i}\left(\theta_{\text {final }} ; \operatorname{rm}_{i}\right)$. We assume all groups have quasi-linear utilities. Therefore, group $i$ 's utility is

$$
u_{i}\left(\overrightarrow{\mathrm{rm}}, \overrightarrow{\tilde{w}} ; \psi, p, \operatorname{rm}_{i}, w_{i}\right)=w_{i} v_{i}\left(\theta_{\text {final }} ; \operatorname{rm}_{i}\right)-p_{i}\left(\overrightarrow{\mathrm{rm}}, \overrightarrow{\tilde{w}}, \theta_{\text {init }}\right)
$$

The groups may strategically report, thus $\overrightarrow{\mathrm{rm}}$ and $\overrightarrow{\vec{w}}$ do not necessarily equal the true $\overrightarrow{\mathrm{rm}}$ and $\vec{w}$. The goal of the LLM provider is to achieve its training objective based on the group's true preferences, taking into account that the misreporting may distort the training outcome. To this end, it is crucial to incentivize all groups to report their information truthfully so that the provider is accessible to the groups' private information. We formally define these desiderata of a mechanism as follows.

Definition 2.1. A mechanism $(\psi, p)$ satisfies dominant-strategy incentive compatibility (DSIC) if $\forall i, \operatorname{rm}_{i}, w_{i}, \mathrm{rm}_{i}^{\prime}, w_{i}^{\prime}$, $\overrightarrow{\operatorname{rm}}_{-i}, \vec{w}_{-i}, \theta_{\text {init }}$, we have

$$
\begin{equation*}
u_{i}\left(\left(\operatorname{rm}_{i}, \overrightarrow{\operatorname{rm}}_{-i}\right),\left(w_{i}, \vec{w}_{-i}\right) ; \psi, p, \operatorname{rm}_{i}, w_{i}\right) \geq u_{i}\left(\left(\mathrm{rm}_{i}^{\prime}, \overrightarrow{\mathrm{rm}}_{-i}\right),\left(w_{i}^{\prime}, \vec{w}_{-i}\right) ; \psi, p, \operatorname{rm}_{i}, w_{i}\right) \tag{DSIC}
\end{equation*}
$$

Definition 2.2. A mechanism $(\psi, p)$ satisfies individually rationality (IR) if $\forall i, \operatorname{rm}_{i}, w_{i}, \overrightarrow{\operatorname{rm}}_{-i}, \vec{w}_{-i}, \theta_{\text {init }}$, we have

$$
\begin{equation*}
u_{i}\left(\left(\operatorname{rm}_{i}, \overrightarrow{\operatorname{rm}}_{-i}\right),\left(w_{i}, \vec{w}_{-i}\right) ; \psi, p, \operatorname{rm}_{i}, w_{i}\right) \geq 0 \tag{IR}
\end{equation*}
$$

DSIC means that for any group, truthfully reporting the reward model and the group size yields the highest utility, regardless of other groups' reports. IR means that truthfully reporting always yields non-negative utilities. Only when both DSIC and IR are satisfied, all groups are incentivized to participate in this game and report truthfully. When a mechanism $(\psi, p)$ satisfies DSIC, IR, or both DSIC and IR, we say that the payment rule $p$ implements $\psi$ in DSIC, IR or both DSIC and IR. Especially, when we say the implementability of a training rule, we refer to the property of DSIC.

## 3 Incentives for General Training Rules

In this section, we discuss the incentive design within the RLHF Game framework. As a warm-up, we consider a simplified scenario where all group sizes are equal to 1 , i.e., $\vec{w}=1$, and this information is public to all groups and the provider. Consequently, each group is required only to report its reward model. For convenience, we let $\vec{w} \equiv 1$ and omit the notation of $\vec{w}$. Unless stated otherwise, the results directly apply to the more general case where $\vec{w}$ is also private information.

For the valuation function in this section, we consider a reasonable form $v(\cdot ; \cdot)$ defined as follows.

Assumption 3.1. For any agent with preference represented by reward model rm, its valuation on model $\mathrm{LLM}_{\theta}$ is its expected reward on the sequences generated by $\mathrm{LLM}_{\theta}$ :

$$
v(\theta ; \mathrm{rm})=\mathbb{E}_{\boldsymbol{x} \sim \mathrm{LLM}_{\theta}} \operatorname{rm}(\boldsymbol{x})=\sum_{\boldsymbol{x} \in T^{*}} \operatorname{LLM}_{\theta}(\boldsymbol{x}) \operatorname{rm}(\boldsymbol{x})
$$

In practice, this can be obtained by averaging the reward of the sequences sampled from an LLM. We discuss the influence of possible errors in this process in Section 4

### 3.1 Necessity of Payment Rule

We begin by demonstrating the necessity of payment rules to ensure incentive compatibility for training rules under the following assumptions.

Assumption 3.2. (1) For all $i \in[n], \partial f / \partial v_{i}$ exists and $\partial f / \partial v_{i}>0$. $\partial f / \partial D$ exists and $\partial f / \partial D<0$. (2) The distance measure function $D$ satisfies that for all $\boldsymbol{x} \in T^{*}, \partial^{2} D / \partial \operatorname{LLM}_{\theta}(\boldsymbol{x})^{2}$ exists and is positive. (3) For all $\overrightarrow{\mathrm{rm}}$ and $\theta_{\text {init }}$, the fine-tuned model $\theta=\psi\left(\overrightarrow{\mathrm{rm}}, \theta_{\text {init }}\right)$ satisfies that $\operatorname{LLM}_{\theta}(\boldsymbol{x})>0$ for all $\boldsymbol{x} \in T^{*}$.

The rationale of these assumptions is as follows: (1) is that we assume the training process aims to find a model $\mathrm{LLM}_{\theta}$ that not only brings higher valuation for all agents but also remains close to the initial model $\mathrm{LLM}_{\theta_{\text {initi }}}$. (2) is like a convex condition in which we assign an increasingly large penalty on $\operatorname{LLM}_{\theta}(\boldsymbol{x})$ when it becomes farther from $\operatorname{LLM}_{\theta_{\text {init }}}(\boldsymbol{x})$. And (3) is to exclude some extreme training rules that the training outcome remains the same for most input and changes drastically. In practice, (1) is satisfied for most training functions $f$, including those aiming to
maximize social welfare and Nash social welfare. (2) and (3) depend on the choice of the regularization measure $D$ and the strength of regularization. At least, they are satisfied by the commonly used KL-divergence.

Combining these three conditions, we show that when the preference for some $\boldsymbol{x}\left(\sum_{i=1}^{n} \mathrm{rm}_{i}(\boldsymbol{x})\right)$ increases and others remain, the probability of $\boldsymbol{x}$ for the optimal model will also increase. In this case, an intuitive manipulation is that the agent reports a polarized reward model: higher reward value $\widetilde{\mathrm{rm}}(\boldsymbol{x})$ for the $\boldsymbol{x}$ it values most. We show that this strategy will give strictly higher utility to the agent unless the agent is indifferent among outcomes $\boldsymbol{x}$ in a subset $S \subseteq T^{*}$ and does not care about the outcomes outside $S$ at all.

Theorem 3.3. Under Assumption 3.1 and Assumption 3.2 when the payment rule $p \equiv 0$, for any agent $i$, truthfully reporting $r m_{i}$ is a strongly dominated strategy, except for the case: $\exists S \subseteq T^{*}$, such that $r m_{i}(\boldsymbol{x})=1 /|S|$ if $\boldsymbol{x} \in S$ and $r m_{i}(\boldsymbol{x})=0$ if $\boldsymbol{x} \notin S$.

Here, we call a strategy strongly dominated when another strategy yields strictly higher utility regardless of others' reports. Theorem 3.3 tells us that truthful reporting is strongly dominated with only training rules, and thus will not be adopted by rational agents.

### 3.2 Characteristics of Payment Rules

Having established the necessity of payment rules in this scenario, we mainly address two questions in the remainder of this section: First, given a training rule $\psi$, can we find a payment rule $p$ such that the mechanism $(\psi, p)$ satisfies DSIC? This is the so-called implementability of a training rule $\psi$. Second, for an implementable training rule $\psi$, can we identify the relationship between the payment rules ps among all DSIC mechanisms $(\psi, p)$.

We resolve the first question primarily by utilizing the notion of cycle monotonicity, first proposed by Rochet 1987. Cycle monotonicity generalizes monotonicity defined in a single-parameter scenario ([Myerson, 1981]). In the RLHF Game, we define a function as $l\left(\mathrm{rm}^{\prime}, \mathrm{rm} ; \overrightarrow{\mathrm{rm}}_{-i}, \theta_{\text {init }}\right):=v_{i}\left(\psi\left((\mathrm{rm}, \overrightarrow{\mathrm{rm}}-i), \theta_{\text {init }}\right) ; \mathrm{rm}\right)-v_{i}\left(\psi\left(\left(\mathrm{rm}^{\prime}, \overrightarrow{\mathrm{rm}}_{-i}, \theta_{\text {init }}\right)\right) ; \mathrm{rm}\right)$. $l\left(\mathrm{rm}^{\prime}, \mathrm{rm} ; \overrightarrow{\mathrm{rm}}_{-i}, \theta_{\text {init }}\right)$ measures the valuation gains from misreporting $\left(\mathrm{rm}_{i}^{\prime}\right)$ to truthfully reporting $\left(\mathrm{rm}_{i}\right)$ under $\overrightarrow{\mathrm{rm}}-i$ and $\theta_{\text {init }}$. The cycle monotonicity is defined based on this function:

Definition 3.4 (Cycle Monotonicity). The training rule $\psi$ satisfies cycle monotonicity if for any $\mathrm{rm}_{i}, \mathrm{rm}_{i}^{\prime} \in \mathcal{R}_{i}$, any

![](https://cdn.mathpix.com/cropped/2024_06_04_1fc48031c34ce8b3fe25g-05.jpg?height=63&width=1645&top_left_y=1324&top_left_x=240)
have

$$
\sum_{j=0}^{k+1} l\left(\mathrm{rm}_{i}^{j}, \mathrm{rm}_{i}^{j+1} ; \overrightarrow{\mathrm{rm}}_{-i}, \theta_{\text {init }}\right) \geq 0 \quad \mathrm{rm}_{i}^{0}=\mathrm{rm}_{i}^{k+2}:=\mathrm{rm}_{i} \text { and } \mathrm{rm}_{i}^{k+1}:=\mathrm{rm}_{i}^{\prime}
$$

For general training rules, cycle monotonicity is a sufficient and necessary condition for implementability.

Theorem 3.5 (Rochet 1987]). A training rule $\psi$ is implementable if and only if it satisfies cycle monotonicity.

In fact, the proof of Theorem 3.5 is constructive. However, for general implementable training rules, the calculation of the payment rules is too complex to be practical.

The second question is more general, so we primarily consider the concept of payment equivalence (Ashlagi et al., 2010]) for an implementable training rule.

Definition 3.6 (Payment Equivalence). An implementable training rule $\psi$ satisfies payment equivalence if for any two mechanisms $(\psi, p)$ and $\left(\psi, p^{\prime}\right)$ satisfying DSIC, there exists a function $f$ such that

$$
p_{i}^{\prime}\left(\operatorname{rm}_{i}, \overrightarrow{\mathrm{rm}}_{-i} ; \theta_{\text {init }}\right)=p_{i}\left(\mathrm{rm}_{i}, \overrightarrow{\mathrm{rm}}_{-i} ; \theta_{\text {init }}\right)+f\left(\overrightarrow{\mathrm{rm}}_{-i}, \theta_{\text {init }}\right) \quad \forall \mathrm{rm}_{i} \in \mathcal{R}_{i}
$$

Or equivalently, when fixing $\overrightarrow{\mathrm{rm}}_{-i}$ and $\theta_{\text {init }}$, there exits a constant $c$ such that $p_{i}^{\prime}\left(\mathrm{rm}_{i}\right)=p_{i}\left(\mathrm{rm}_{i}\right)+c$ for all $\mathrm{rm}_{i} \in \mathcal{R}_{i}$.

Payment equivalence indicates that the only way to modify a DSIC mechanism $(\psi, p)$ to $\left(\psi, p^{\prime}\right)$ while maintaining incentive compatibility is to add a term that is independent of $i$ 's report to agent $i$ 's payment function $p_{i}$. Thus, the payment equivalence of $\psi$ is sometimes interpreted as the uniqueness of the payment rule $p$ that implements it in DSIC. This notion is strong and useful since when a training rule $\psi$ satisfies payment equivalence and we can figure out one mechanism $(\psi, p)$ that satisfies DSIC, then all the payment rules $p^{\prime}$ that implement $\psi$ in DSIC are characterized. In particular, it is possible to find the revenue-maximizing payment rule $p^{*}$ among all these payment rules that implement $\psi$ in both DSIC and IR.

Payment equivalence is influenced by the domain of the types: reward models and group sizes in the RLHF Game. When $\vec{w} \equiv 1$, the agents only report the reward models whose domain $\mathcal{R}$ contains all normalized reward models rm. Therefore, for all $i \in[n]$, the domain of the whole private information is exactly $\mathcal{R}$, which is a connected set in the Euclidean space. Thus, we can directly apply the result in Nisan et al. [2007] and get the following theorem.

Proposition 3.7. When $\vec{w} \equiv 1$ is public information and the agents only report the reward models, all implementable training rules satisfy payment equivalence.

However, when the group sizes $\vec{w}$ is also a part of the private information for all groups, the domain of the whole private information becomes $\mathcal{W} \times \mathcal{R}$ that is no longer a connected set because $\mathcal{W} \subseteq \mathbb{N}_{+}$. Thus, payment equivalence may not be satisfied for general training rules, and we will study this for a representative set of training rules in the following section.

## 4 Social Welfare Maximizing Mechanism

In this section, we consider the scenario where group $i$ consists of $w_{i}$ agents, and each group must simultaneously report its reward model and size. Our objective is to design a mechanism $(\psi, p)$ that incentivizes each group $i$ to truthfully report both $\mathrm{rm}_{i}$ and $w_{i}$. For general training rules $\psi$, though it is possible to adopt the method used in the constructive proof for Theorem 3.5 to derive the payment rule, the resulting payment rule can be complex and impractical.

Therefore, in this section, our primary focus is on a subset of training rules designed to maximize social welfare under regularization constraints, which is commonly used in practice to aggregate various preferences (Boyd and Vandenberghe [2004], Nocedal and Wright [1999]), balancing efficiency and fairness.

Definition 4.1 (SW-Maximizing Training Rules). Given the reports $\overrightarrow{\mathrm{rm}}, \vec{w}$, and the initial model $\theta_{\text {init }}$, a SWMaximizing training rule fine-tunes the model to maximize the social welfare under a regularization penalty measured by some metric $D$. Formally, it is represented as:

$$
\psi\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta_{\text {init }}\right)=\arg \max _{\theta \in \Theta} \sum_{i=1}^{n} w_{i} v_{i}\left(\theta ; \operatorname{rm}_{i}\right)-\lambda D\left(\operatorname{LLM}_{\theta} \| \operatorname{LLM}_{\theta_{\text {init }}}\right)
$$

Here, $\lambda$ is a hyperparameter that adjusts regularization strength.

Note that SW-Maximizing training rules constitute a set of training rules. We use $\psi \in \Psi^{S W}$ to indicate that $\psi$ is a member of this set. Furthermore, similar to the (3) in Assumption 3.2, we also assume that for all $\overrightarrow{\mathrm{rm}}, \vec{w}$ and $\theta_{\text {init }}$, the fine-tuned model $\theta=\psi\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta_{\text {init }}\right)$ satisfies that $\operatorname{LLM}_{\theta}(\boldsymbol{x})>0$ for $\forall \boldsymbol{x} \in T^{*}$. One simple way to achieve it is to set a large $\lambda$ and hence the training result is close enough to $\theta_{\text {init }}$.

### 4.1 Affine Maximizer Payment

We introduce the affine maximizer payment rule (Roberts [1979]) $p^{A F F}$, a weighted version of VCG payment Vickrey [1961], Clarke [1971], Groves [1973]):

$$
\begin{aligned}
p_{i}^{A F F}\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta_{\text {init }}\right)=A S W_{-i} & \left(\overrightarrow{\mathrm{rm}}, \vec{w}, \psi\left(\overrightarrow{\mathrm{rm}}_{-i}, \vec{w}_{-i}, \theta_{\text {init }}\right) ; \theta_{\text {init }}\right) \\
& -A S W_{-i}\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \psi\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta_{\text {init }}\right) ; \theta_{\text {init }}\right)
\end{aligned}
$$

The notations $A S W\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta ; \theta_{\text {init }}\right)$ and $A S W_{-j}\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta ; \theta_{\text {init }}\right)$ refer to the affine social welfare with and without group $j$ when the reported reward models are $\overrightarrow{\mathrm{rm}}$, the reported number of agents are $\vec{w}$, the initial model is $\operatorname{LLM}_{\theta_{\text {init }}}$, and the parameters of model is $\theta$. The affine social welfare consists of both the groups' valuations and the regularization term. Formally,

$$
\begin{gathered}
A S W\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta ; \theta_{\text {init }}\right):=\sum_{i=1}^{n} w_{i} v_{i}\left(\theta ; \operatorname{rm}_{i}\right)-\lambda D\left(\operatorname{LLM}_{\theta} \| \operatorname{LLM}_{\theta_{\text {init }}}\right) \\
A S W_{-j}\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta ; \theta_{\text {init }}\right):=\sum_{i=1, i \neq j}^{n} w_{i} v_{i}\left(\theta ; \operatorname{rm}_{i}\right)-\lambda D\left(\operatorname{LLM}_{\theta} \| \mid \mathrm{LLM}_{\theta_{\text {init }}}\right)
\end{gathered}
$$

We show that $p^{A F F}$ implements SW-Maximizing training rules in both DSIC and IR, which implies that truthfully reporting both reward models and group sizes constitutes a dominant Nash Equilibrium in this mechanism.

Theorem 4.2. For any $\psi \in \Psi^{S W}$, mechanism $\left(\psi, p^{A F F}\right)$ satisfies DSIC and IR.

Regarding payment equivalence, as we have mentioned in the previous section, the domain $\mathcal{W} \times \mathcal{R}$ is not connected in the Euclidean space since $\mathcal{W} \subseteq \mathbb{N}_{+}$, the results in Nisan et al. [2007] can not be directly applied. However, we show that under the following assumption, SW-Maximizing training rules satisfy payment equivalence.

Assumption 4.3. For any $\epsilon>0$, there exists a $\delta>0$ such that for any $\theta_{\text {init }}, \overrightarrow{\mathrm{rm}}, \overrightarrow{\mathrm{rm}^{\prime}}, \vec{w}$ and $\vec{w}^{\prime}$, if $\max _{\boldsymbol{x} \in T^{*}}\left|\sum_{i=1}^{n}\left(w_{i} \operatorname{rm}_{i}(\boldsymbol{x})-w_{i}^{\prime} \operatorname{rm}_{i}^{\prime}(\boldsymbol{x})\right)\right| \leq \delta$, then $\max _{\boldsymbol{x} \in T^{*}}\left|\operatorname{LLM}_{\theta}(\boldsymbol{x})-\operatorname{LLM}_{\theta^{\prime}}(\boldsymbol{x})\right| \leq \epsilon$, where $\theta:=$ $\psi\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta_{\text {init }}\right)$ and $\theta^{\prime}:=\left(\overrightarrow{\mathrm{rm}}^{\prime}, \vec{w}^{\prime}, \theta_{\text {init }}\right)$.

This assumption is reasonable for most measures $D$ in SW-Maximizing training rules as the space of $\theta$ is continuous. The continuity ensures that when the reported information $(\overrightarrow{\mathrm{rm}}, \vec{w})$ and $\left(\overrightarrow{\mathrm{rm}}^{\prime}, \vec{w}^{\prime}\right)$ are sufficiently close, the training outcomes $\theta$ and $\theta^{\prime}$ should also be close. Specifically, we validate this assumption for some widely used distance measures.

Proposition 4.4. Assumption 4.3 holds for SW-Maximizing training rules with regularizations KL-divergence, $D_{\mathrm{KL}}(p \| q)=\sum_{\boldsymbol{x} \in T^{*}} p(\boldsymbol{x}) \log p(\boldsymbol{x}) / q(\boldsymbol{x})$, and $L_{2}$ distance, $D_{2}(p \| q)=\sum_{\boldsymbol{x} \in T^{*}}(p(\boldsymbol{x})-q(\boldsymbol{x}))^{2}$.

Under this assumption, we derive the following result:

Theorem 4.5. Under Assumption 3.1 and Assumption 4.3 each training rule $\psi \in \Psi^{S W}$ satisfies payment equivalence.

With the property of payment equivalence, we further investigate the revenue-maximizing payment rule that implements SW-Maximizing training rules in both DSIC and IR. Finding the revenue-maximizing multi-parameter mechanism is a challenging problem in classic mechanism design theory. However, since we have proved the payment equivalence for SW-Maximizing training rules, we can utilize the necessary condition defined in Definition 3.6 to formulate it as a optimization problem. Solving this problem provides the optimal payment rule under the same conditions.

Corollary 4.6. Under Assumption 3.1 and Assumption 4.3 for each training rule $\psi \in \Psi^{S W}$, the revenue-maximizing payment rule $p^{*}$ that implements $\psi$ in both DSIC and IR is given by

$$
\begin{aligned}
& p_{i}^{*}\left(\left(r m_{i}, \overrightarrow{r m}_{-i}\right),\left(w_{i}, \vec{w}_{-i}\right), \theta_{\text {init }}\right)=p_{i}^{A F F}\left(\left(r m_{i}^{\prime}, \overrightarrow{r m}_{-i}\right),\left(w_{i}^{\prime}, \vec{w}_{-i}\right) ; \theta_{\text {init }}\right) \\
& +\inf _{r m_{i}^{\prime} \in \mathcal{R}, w_{i}^{\prime} \in \mathcal{W}} u_{i}\left(\left(r m_{i}^{\prime}, \overrightarrow{r m}_{-i}\right),\left(w_{i}^{\prime}, \vec{w}_{-i}\right) ; \psi, p^{A F F}, r m_{i}^{\prime}, w_{i}^{\prime}\right)
\end{aligned}
$$

The relationship between the domains $\mathcal{R} \times \mathcal{W}$, and this corollary is reflected in two aspects. First, the establishment of payment equivalence depends on the assumptions of the choice of $\mathcal{R}, \mathcal{W}$, particularly considering $\mathcal{R}$ includes all normalized reward models. Second, based on payment equivalence, finding the revenue-maximizing mechanism satisfying IR also needs information on the exact domains.

### 4.2 Approximate Valuation

In this part, we discuss the influence of error generated in practice on the incentive property in the RLHF Game. We abstract it as an approximate valuation problem (Chiesa et al. [2012]). Formally, when group $i$ reports its reward model $\mathrm{rm}_{i}$, the mechanism may not use $\mathrm{rm}_{i}$ exactly but rather a noisy reward model $\widehat{\mathrm{rm}}_{i}$ with a conditional distribution $F_{i}\left(\cdot \mid \mathrm{rm}_{i}\right)$ as the input into the mechanism. We argue that this abstraction has covered various error cases. One example is that the calculation of valuation defined in Assumption 3.1 requires sampling sequences from LLM, which may result in a deviation from the true valuation. Another example is that the fine-tuned model $\mathrm{LLM}_{\theta}$ may not be exactly optimal for the reported reward models. However, this model $\mathrm{LLM}_{\theta}$ can be considered as the optimal for the deviated reward models.

We assume that agent groups are aware of the noise when feeding preferences into the mechanism. Therefore, their utilities will take it into account and have a different form. We use the capital letter $U_{i}$ to represent agent $i$ 's revised utility. Formally, for group $i$ with reward model $\mathrm{rm}_{i}$ and group size $w_{i}$, its utility for reporting $\left(\mathrm{rm}_{i}^{\prime}, w_{i}^{\prime}\right)$ is given by

![](https://cdn.mathpix.com/cropped/2024_06_04_1fc48031c34ce8b3fe25g-07.jpg?height=61&width=1395&top_left_y=2013&top_left_x=365)

Note that in defining $U_{i}$, we implicitly assume that each group is unable to know the other group's noise information. Therefore, the expectation is not taken concerning $\overrightarrow{\mathrm{rm}}_{-i}$.

We only consider the case when the noised input to the mechanism and the reported reward models are close:

Assumption 4.7 (Bounded Error). For any profile of reported reward models $\overrightarrow{\mathrm{rm}}$, any profile of reward models $\overrightarrow{\mathrm{rm}}$ that can be generated from $F_{i}\left(\cdot \mid \mathrm{rm}_{i}\right) \mathrm{s}$ with non-zero probability satisfies

$$
\max _{\boldsymbol{x} \in T^{*}}\left|\widehat{\operatorname{rm}}_{i}(\boldsymbol{x})-\operatorname{rm}_{i}(\boldsymbol{x})\right| \leq \epsilon \quad \forall i \in[n]
$$

We first show that by directly applying results in Section 4.1 to the noised input, the loss in the social welfare is upper-bounded by $2 \epsilon \sum_{i=1}^{n} w_{i}$.

Lemma 4.8. Under Assumption 3.1 and Assumption 4.7 when the training rule $\psi \in \Psi^{S W}$, the loss in social welfare is bounded by

$$
A S W\left(\overrightarrow{r m}, \vec{w}, \psi\left(\overrightarrow{r m}, \vec{w}, \theta_{\text {init }}\right) ; \theta_{\text {init }}\right) \geq A S W\left(\overrightarrow{r m}, \vec{w}, \psi\left(\overrightarrow{r m}, \vec{w}, \theta_{\text {init }}\right) ; \theta_{\text {init }}\right)-2 \epsilon \sum_{i=1}^{n} w_{i}
$$

For training rule $\psi \in \Psi^{S W}$, a group's utility in the mechanism $\left(\psi, p^{A F F}\right)$ consists of an affine social welfare term $A S W$. Therefore, we can derive the following theorem based on Lemma4.8.

Theorem 4.9. Under Assumption 3.1 and Assumption 4.7 when the training rule $\psi \in \Psi^{S W}$, for group $i$ and any rm ${ }_{i}$, $r m_{i}^{\prime}, \overrightarrow{r m}_{-i}, w_{i}$ and $\vec{w}_{i}$, we have

$$
U_{i}\left(\left(r m_{i}, \overrightarrow{r m}_{-i}\right),\left(w_{i}, \vec{w}_{-i}\right) ; \psi, p^{A F F}, r m_{i}, w_{i}\right) \geq U_{i}\left(\left(r m_{i}^{\prime}, \overrightarrow{r m}_{-i}\right),\left(w_{i}, \vec{w}_{-i}\right) ; \psi, p^{A F F}, r m_{i}, w_{i}\right)-2 w_{i} \epsilon
$$

In other words, when $\vec{w}$ is truthfully reported, $\left(\psi, p^{A F F}\right)$ is $\max _{i \in[n]} 2 w_{i} \epsilon$-DSIC mechanism.

This means that for any group $i$, the maximum gain of misreporting is less than $2 w_{i} \epsilon$ regardless of the others' reports. Agents will tend to truthfully report in cases where finding the optimal strategy is costlier than $2 w_{i} \epsilon$.

## 5 Further Related Work

RLHF with Multiple Reward Models. Research involving multiple reward models primarily focuses on developing algorithms to enhance practical performance. Some studies design methods to simultaneously satisfy multiple preferences (Ramé et al. [2024], Wu et al. [2024], Jang et al. [2023], Park et al. [2024]). Additionally, there is a body of work that trains multiple models for a single preference and then ensembles them to improve the robustness of RLHF (Coste et al. [2023], Zhang et al. [2024]), mitigate the influence of incorrect and ambiguous preferences in the dataset (Wang et al. [2024]), and reduce reward hacking (Eisenstein et al. [2023]). Unlike these approaches, our work considers how to collect misaligned preferences truthfully from different agents.

Multi-parameter Auctions. Several studies have explored the properties relevant to our paper in various multiparameter auction scenarios, such as implementability (Rochet [1987], Miyake [1998], Conitzer and Sandholm [2004, Saks and Yu [2005], Bikhchandani et al. [2006], Ashlagi et al. [2010]) and payment equivalence (Ivanova-Stenzel and Salmon [2008], Heydenreich et al. [2009], Bergemann and Välimäki [2010], Pavan et al. [2014]). Another central topic in auction theory is to design mechanisms that satisfy DSIC and IR while maximizing the expected revenue for the auctioneer. Although the single-parameter scenario has been resolved by Myerson [1981], the optimal auction design for multi-parameter settings remains an open question. Therefore, there is a stream of research focusing on a specific subset, affine maximizer auctions, which inherently satisfy DSIC and IR (Sandholm and Likhodedov [2015], Roberts [1979], Likhodedov and Sandholm [2004], Briest et al. [2010], Tang and Sandholm [2012], Jehiel et al. [2007]), and proposes optimizations to enhance empirical performance (Curry et al. [2022], Duan et al. [2024a.b]). Compared to these works, we are the first to discuss the property of payment equivalence and the revenue-maximizing solution in the scenario of fine-tuning LLMs.

Game Theory and LLMs. Other works also explored the intersection of game theory and large language models. Some research has proposed algorithms for training LLMs inspired by concepts in game theory, such as Nash learning from human feedback (Munos et al. [2023]), consensus game (Jacob et al. [2023]), and direct Nash optimization (Rosset et al. [2024]), and Gemp et al. [2024]. Furthermore, various studies assess LLMs from a gametheoretical perspective, examining aspects such as rationality (Chen et al. [2023], Fan et al. [2023]), behavior in matrix games (Akata et al. [2023], Gandhi et al. [2023], Lorè and Heydari [2023]), and performance in strategic games like auctions (Guo et al. [2023, 2024]), Werewolf (Xu et al. [2023a] b]), and Avalon (Wang et al. [2023a]).

## 6 Discussion and Conclusion

Efficient Practical Implementation of $p^{A F F}$. In the RLHF Game with $n$ groups, calculating $p^{A F F}$ requires $n$ separate complete training processes of different $\psi$ s. This can result in inefficiency due to the costly training. To address this problem, we propose two modifications to $p^{A F F}$. Both modifications involve computing an approximate $\widehat{\psi}\left(\overrightarrow{\mathrm{mm}}_{-i}, \vec{w}_{-i}, \theta_{\text {init }}\right)$, instead of the true optimal $\psi\left(\overrightarrow{\mathrm{rm}}_{-i}, \vec{w}_{-i}, \theta_{\text {init }}\right)$ when calculating payments:

1. Calculate an approximate $\widehat{\psi}\left(\overrightarrow{\mathrm{rm}}_{-i}, \vec{w}_{-i}, \theta_{\text {init }}\right)=\arg \max _{\theta \in\left\{\theta_{1}, \cdots, \theta_{K}\right\}} A S W_{-i}\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta ; \theta_{\text {init }}\right)$, where $\left\{\theta_{1}, \cdots, \theta_{K}\right\}$ are the parameters saved in the process of training $\psi\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta_{\text {init }}\right)$.
2. Adopt less iterations in the training process for calculating $\psi\left(\overrightarrow{\mathrm{rm}}_{-i}, \vec{w}_{-i}, \theta_{\text {init }}\right)$. And thus get a result $\widehat{\psi}\left(\overrightarrow{\mathrm{rm}}_{-i}, \vec{w}_{-i}, \theta_{\text {init }}\right)$ that is not optimal.

The first method needs only one training process (for $\left.\psi\left(\overrightarrow{\mathrm{rm}}, \vec{w}, \theta_{\text {init }}\right)\right)$ but affects the property of DSIC since the saved parameters $\left\{\theta_{1}, \cdots, \theta_{K}\right\}$ are also influenced $i$ 's report. In comparison, the second approach incurs higher training costs but guarantees strict DSIC.

Conclusion and Future Work. This paper investigates incentive design in fine-tuning large language models using multiple reward models. We formalize this scenario as the RLHF Game, where a service provider proposes training and payment rules, and agents strategically report their preferences. We demonstrate the necessity of payment schemes for incentivizing truthful reporting in general training rules and provide a comprehensive characterization of payment schemes that implement SW-Maximizing training rules in dominant strategies. These findings enhance the theoretical understanding of mechanism design in LLM fine-tuning and offer guidelines for implementing effective RLHF-based systems in various contexts.

Future research in this field presents several promising directions. Firstly, investigating mechanisms integrating efficiency and incentive compatibility within the RLHF Game could significantly enhance its applicability in real-world scenarios. Secondly, modeling and examining more complex training rules, such as dynamic training rules, could deepen the understanding of this framework. Thirdly, designing mechanisms for more general cases that aggregate preferences into multiple models based on diversity considerations is crucial. Additionally, applying mechanism design theory to other scenarios related to large language models, such as API charge schemes, retrieval-augmented generation (RAG), and prompt engineering, offers valuable opportunities for further exploration.

## References

Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz. Playing repeated games with large language models. arXiv preprint arXiv:2305.16867, 2023.

Itai Ashlagi, Mark Braverman, Avinatan Hassidim, and Dov Monderer. Monotonicity and implementability. Econometrica, 78(5):1749-1772, 2010.

Dirk Bergemann and Juuso Välimäki. The dynamic pivot mechanism. Econometrica, 78(2):771-789, 2010.

Sushil Bikhchandani, Shurojit Chatterji, Ron Lavi, Ahuva Mu'alem, Noam Nisan, and Arunava Sen. Weak monotonicity characterizes deterministic dominant-strategy implementation. Econometrica, 74(4):1109-1132, 2006.

Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.

Patrick Briest, Shuchi Chawla, Robert Kleinberg, and S Matthew Weinberg. Pricing randomized allocations. In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pages 585-597. SIAM, 2010.

Yiting Chen, Tracy Xiao Liu, You Shan, and Songfa Zhong. The emergence of economic rationality of gpt. Proceedings of the National Academy of Sciences, 120(51):e2316205120, 2023.

Alessandro Chiesa, Silvio Micali, and Zeyuan Allen Zhu. Mechanism design with approximate valuations. In Proceedings of the 3rd Innovations in Theoretical Computer Science conference, pages 34-38, 2012.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Edward H Clarke. Multipart pricing of public goods. Public choice, pages 17-33, 1971.

Vincent Conitzer and Tuomas Sandholm. Self-interested automated mechanism design and implications for optimal combinatorial auctions. In Proceedings of the 5th ACM Conference on Electronic Commerce, pages 132-141, 2004.

Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H Holliday, Bob M Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, et al. Social choice for ai alignment: Dealing with diverse human feedback. arXiv preprint arXiv:2404.10271, 2024.

Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023.

Michael Curry, Tuomas Sandholm, and John Dickerson. Differentiable economics for randomized affine maximizer auctions. arXiv preprint arXiv:2202.02872, 2022.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Zhijian Duan, Haoran Sun, Yurong Chen, and Xiaotie Deng. A scalable neural network for dsic affine maximizer auction design. Advances in Neural Information Processing Systems, 36, 2024a.

Zhijian Duan, Haoran Sun, Yichong Xia, Siqiang Wang, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng, and Xiaotie Deng. Scalable virtual valuations combinatorial auction design by combining zeroth-order and first-order optimization method. arXiv preprint arXiv:2402.11904, 2024b.

Kumar Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, and Di Wang. Auctions with llm summaries. arXiv preprint arXiv:2404.08126, 2024.

Paul Duetting, Vahab Mirrokni, Renato Paes Leme, Haifeng Xu, and Song Zuo. Mechanism design for large language models. arXiv preprint arXiv:2310.10826, 2023.

Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D'Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244, 2023.

Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models serve as rational players in game theory? a systematic analysis. arXiv preprint arXiv:2312.05488, 2023.

Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei, and Suho Shin. Online advertisements with llms: Opportunities and challenges. arXiv preprint arXiv:2311.07601, 2023.

Kanishk Gandhi, Dorsa Sadigh, and Noah D Goodman. Strategic reasoning with language models. arXiv preprint arXiv:2305.19165, 2023.

Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari Dasagi, Luke Marris, Georgios Piliouras, and Karl Tuyls. States as strings as strategies: Steering language models with game-theoretic solvers. arXiv preprint arXiv:2402.01704, 2024.

Theodore Groves. Incentives in teams. Econometrica: Journal of the Econometric Society, pages 617-631, 1973.

Shangmin Guo, Haochuan Wang, Haoran Bu, Yi Ren, Dianbo Sui, Yu-Ming Shang, and Siting Lu. Large language models as rational players in competitive economics games. arXiv preprint arXiv:2308.10032, 2023.

Shangmin Guo, Haoran Bu, Haochuan Wang, Yi Ren, Dianbo Sui, Yuming Shang, and Siting Lu. Economics arena for large language models. arXiv preprint arXiv:2401.01735, 2024.

Birgit Heydenreich, Rudolf Müller, Marc Uetz, and Rakesh V Vohra. Characterization of revenue equivalence. Econometrica, 77(1):307-316, 2009.

Radosveta Ivanova-Stenzel and Timothy C Salmon. Revenue equivalence revisited. Games and Economic Behavior, 64(1):171-192, 2008 .

Athul Paul Jacob, Yikang Shen, Gabriele Farina, and Jacob Andreas. The consensus game: Language model generation via equilibrium search. arXiv preprint arXiv:2310.09139, 2023.

Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. arXiv preprint arXiv:2310.11564, 2023.

Philippe Jehiel, Moritz Meyer-Ter-Vehn, and Benny Moldovanu. Mixed bundling auctions. Journal of Economic Theory, 134(1):494-512, 2007.

Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852, 2023.

Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024.

Benjamin Laufer, Jon Kleinberg, and Hoda Heidari. Fine-tuning games: Bargaining and adaptation for generalpurpose models. arXiv preprint arXiv:2308.04399, 2023.

Anton Likhodedov and Tuomas Sandholm. Methods for boosting revenue in combinatorial auctions. In AAAI, pages 232-237, 2004.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Nunzio Lorè and Babak Heydari. Strategic behavior of large language models: Game structure vs. contextual framing. arXiv preprint arXiv:2309.05898, 2023.

David G Luenberger, Yinyu Ye, et al. Linear and nonlinear programming, volume 2. Springer, 1984.

Mitsunobu Miyake. On the incentive properties of multi-item auctions. International Journal of Game Theory, 27: $1-19,1998$.

Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023.

Roger B Myerson. Optimal auction design. Mathematics of operations research, 6(1):58-73, 1981.

Noam Nisan et al. Introduction to mechanism design (for computer scientists). Algorithmic game theory, 9:209-242, 2007.

Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.

Chanwoo Park, Mingyang Liu, Kaiqing Zhang, and Asuman Ozdaglar. Principled rlhf from heterogeneous feedback via personalization and preference aggregation. arXiv preprint arXiv:2405.00254, 2024.

Alessandro Pavan, Ilya Segal, and Juuso Toikka. Dynamic mechanism design: A myersonian approach. Econometrica, 82(2):601-653, 2014.

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2023.

Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. Warm: On the benefits of weight averaged reward models. arXiv preprint arXiv:2401.12187, 2024.

Kevin Roberts. The characterization of implementable choice rules. Aggregation and revelation of preferences, 12(2): 321-348, 1979 .

Jean-Charles Rochet. A necessary and sufficient condition for rationalizability in a quasi-linear context. Journal of mathematical Economics, 16(2):191-200, 1987.

Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715, 2024.

Michael Saks and Lan Yu. Weak monotonicity suffices for truthfulness on convex domains. In Proceedings of the 6th ACM conference on Electronic commerce, pages 286-293, 2005.

Tuomas Sandholm and Anton Likhodedov. Automated design of revenue-maximizing combinatorial auctions. Operations Research, 63(5):1000-1025, 2015.

Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey. arXiv preprint arXiv:2309.15025, 2023.

Ermis Soumalias, Michael J Curry, and Sven Seuken. Truthful aggregation of llms with an application to online advertising. arXiv preprint arXiv:2405.05905, 2024.

Pingzhong Tang and Tuomas Sandholm. Mixed-bundling auctions with reserve prices. In AAMAS, pages 729-736, 2012.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

William Vickrey. Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance, 16(1):8-37, 1961.

Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024.

Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon's game of thoughts: Battle against deception through recursive contemplation. arXiv preprint arXiv:2310.01320, 2023a.

Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966, 2023b.

Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems, 36, 2024.

Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023a.

Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, 2023b.

Shun Zhang, Zhenfang Chen, Sunli Chen, Yikang Shen, Zhiqing Sun, and Chuang Gan. Improving reinforcement learning from human feedback with efficient reward model ensemble. arXiv preprint arXiv:2401.16635, 2024.
