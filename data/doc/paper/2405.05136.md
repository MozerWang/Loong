# INTEGRATING LSTM AND BERT FOR LONG-SEQUENCE DATA ANALYSIS IN INTELLIGENT TUTORING SYSTEMS * 

Zhaoxing Li, Sebastian Stein<br>School of Electronics and Computer Science<br>University of Southampton<br>Southampton<br>\{zhaoxing.li, ss2.esc\}@soton.ac.uk

Jujie Yang, Jindi Wang<br>Department of Computer Science<br>Durham University<br>Durham<br>\{jujie.yang, jindi.wang\}@soton.ac.uk

Lei Shi<br>School of Computing<br>Newcastle University<br>Newcastle upon Tyne<br>lei.shi@ncl.ac.uk


#### Abstract

The field of Knowledge Tracing aims to understand how students learn and master knowledge over time by analyzing their historical behaviour data. To achieve this goal, many researchers have proposed Knowledge Tracing models that use data from Intelligent Tutoring Systems to predict students' subsequent actions. However, with the development of Intelligent Tutoring Systems, largescale datasets containing long-sequence data began to emerge. Recent deep learning based Knowledge Tracing models face obstacles such as low efficiency, low accuracy, and low interpretability when dealing with large-scale datasets containing long-sequence data. To address these issues and promote the sustainable development of Intelligent Tutoring Systems, we propose a LSTM BERT-based Knowledge Tracing model for long sequence data processing, namely LBKT, which uses a BERTbased architecture with a Rasch model-based embeddings block to deal with different difficulty levels information and an LSTM block to process the sequential characteristic in students' actions. LBKT achieves the best performance on most benchmark datasets on the metrics of ACC and AUC. Additionally, an ablation study is conducted to analyse the impact of each component of LBKT's overall performance. Moreover, we used t-SNE as the visualisation tool to demonstrate the model's embedding strategy. The results indicate that LBKT is faster, more interpretable, and has a lower memory cost than the traditional deep learning based Knowledge Tracing methods.


Keywords Knowledge Tracing $\cdot$ BERT $\cdot$ Student Modelling $\cdot$ Long-Sequence Data Processing $\cdot$ Technology Enhanced Learning (TEL)

## 1 Introduction

Technology Enhanced Learning (TEL) has become increasingly important in providing high-quality education to build a more sustainable world. The recent COVID-19 pandemic has significantly impacted traditional classroom education and sparked online learning, enabling teaching and learning remotely. Meanwhile, the development of online learning systems has made it possible to use Intelligent Tutoring Systems (ITS) to store and analyse a sizable amount of student behaviour data to improve intelligent educational services. As one of the widely applied TEL technologies, Knowledge Tracing (KT) has drawn a lot of attention. KT is the field of modelling students' learning trajectories and predicting their sequential actions based on historical interaction data between students and ITS [1].[^0]

With the development of ITS, large-scale datasets such as EdNet [2] and Junyi Academy [3] began to emerge. In these datasets, long-sequence student interaction data were gathered as an increasing number of students used the ITS for an extended period. The long- and short-sequence data in these datasets are unbalanced, which satisfies the long-tail distribution [4]. For instance, within the EdNet dataset, a substantial amount of student action sequences are included, ranging from the shortest sequence that may comprise just a single action to the longest sequence that encompasses 40,157 actions. Notably, the average action sequence length of the EdNet dataset is 121.5 , indicating a moderate length of data sequences overall. However, it is important to note that the distribution of sequence lengths is highly skewed, and this unbalanced distribution has an impact on the overall performance of the KT models [5]. Although the quantity of short-sequence data is larger than the long-sequence data, the latter is of more weight than the former in prediction tasks [4].

In general, KT models could be divided into three categories: probabilistic KT models, logistic KT models, and deep learning based KT methods (DKT) [6]. Traditional probabilistic KT models and logistic KT models are forced to confront difficulties such as decreased processing efficiency and increased memory usage as growing amounts of longer sequence data are released. Deep learning based KT models are known to suffer from inefficiencies when processing long-sequence action data problems, including issues related to the accuracy, speed, and memory usage [5, 4]. Therefore, allowing the processing of very long sequence data is key to achieving high performance for next-generation KT models [5]. Moreover, due to the black-box nature of traditional deep learning methods, the current deep learning based KT models also struggle with the lack of interpretability [7].

To address the above issues, in this paper, we propose LBKT, a novel LSTM BERT Knowledge Tracing model, for processing long sequence data. The model combines the strength of the Bidirectional Encoder Representations from Transformers (BERT) model in capturing the relations of complex data [8] with the strength of the LSTM model in handling long sequential data to improve its performance on large-scale datasets containing long-sequence data (here, the long-sequence data indicates a length longer than 400 interactions). Moreover, we utilise a Rasch model-based embedding method to process the difficulty level information in the historical behaviour data of students. The Rasch model is a classic yet powerful model in psychometrics [9], which could be utilised to construct raw questions and knowledge embeddings for KT tasks [7]. Rasch model based embedding could improve the model's performance and interpretability. The experimental results show that our proposed LBKT outperforms the baseline models in five datasets on metrics ACC and AUC. Moreover, it is faster at processing long-sequence data at two long-sequence datasets we extract from the two large-scale datasets. Furthermore, we use t-SNE as the visualisation tool to demonstrate the interpretability of the embedding strategy.

The main contributions of our paper lie in the following two aspects:

1. We propose LBKT 2 , a novel LSTM BERT Knowledge Tracing model for long sequence data processing. The LBKT leverages the power of BERT, Rasch-based embedding strategies, and LSTM.
2. The experimental results show that LBKT outperforms the baseline models on five ITS datasets on the metric of AUC(assist12, assist17, algebra06, EdNet, and Junyi Academy). Another comparative experiments show the effectiveness of LBKT when processing long-sequence datasets. LBKT model exhibits better interpretability than traditional deep learning based KT models and has advantages in training efficiency.

## 2 Related Work

### 2.1 Knowledge Tracing

Knowledge Tracing (KT) is used in Intelligent Tutoring Systems (ITS) to model and predict a student's mastery level of a specific skill or concept over time [10]. It is based on the assumption that a student's knowledge state is a hidden variable that can be inferred from their observable behaviour, such as their responses to questions or tasks related to the skill or concept being measured [11]. Its goal is to provide personalised feedback and support to students by tracking their progress and adapting instruction to meet their individual needs. This can help to improve student learning outcomes and enhance educational effectiveness. Broadly, there are three categories of KT methods: probabilistic KT models, logistic KT models, and deep learning-based KT models [6].

Probabilistic KT models assume that the student's learning process follows a Markov Process, where students' knowledge mastery could be measured by their observed learning performance [11]. Bayesian KT, or BKT, is the earliest and most classic probabilistic model, which was inspired by cognitive mastery learning [12]. BKT models generally use a probabilistic graphical model, such as Hidden Markov Model (HMM) 11] and Bayesian Belief Network [13], to track students' changing learning states. The major shortcoming of BKT is that it assumes a simplistic two-state[^1]

Running Title for Header

student modelling framework, where a student's knowledge is either learned or unlearned, and there is no concept of forgetting or decay in the model. However, in reality, a student's knowledge could be complex and multi-faceted and could change over time due to various factors such as decay and interference. Therefore, BKT may not be able to capture the nuances of student learning and may not provide an accurate representation of their knowledge state over time. For example, BKT assumes that each question only required one skill and that the various skills were irrelevant to each other [11, 14]. Therefore, in general, BKT models cannot process complicated problems, including the multiple skills and the complex relationship among the concepts, questions, and skills. To address this limitation, KÃ¤ser et al. proposed Dynamic BKT, or DBKT, based on Dynamic Bayesian Network (DBN), to model the prerequisite hierarchies and dependencies of multiple skills [15]. However, both BKT and DBKT still struggle with processing multiple topics or skills, failing to account for contextual factors that may impact student learning.

The logistic KT models are built on the principle of logistic regression, which is a statistical method used to model the probability of a binary outcome based on one or more predictor variables [6]. In the context of educational data, the predictor variables could include a student's prior performance on a set of related skills or concepts, their response time, and their correctness or incorrectness in answering assessment questions. The output of the logistic regression KT model is a probability estimate of a student's mastery level on a particular skill or concept, which can be used to inform personalized learning interventions and improve student outcomes. There are three logistic models. The Learning Factor Analysis model (LFA) incorporates the initial knowledge state, easiness of knowledge components (KCs), and learning rate of KCs to estimate the student's initial knowledge state, the easiness of different $\mathrm{KCs}$, and the learning rate of KCs [16]. The Performance Factor Analysis (PFA) model is an extension of the LFA model and takes into account the student's performance. PFA considers parameters for previous failures (f) and successes (s) for the KC, in addition to the easiness of KCs [17]. The Knowledge Tracing Machines (KTM) model uses factorization machines (FMs) to extend logistic models to higher dimensions [15].

Inspired by the recent success of deep learning (DL) [18], researchers have applied deep learning technologies into the KT field to develop DL-based Knowledge Tracing [19]. DL-based KT typically models a knowledge tracing task as a sequence prediction problem. With the self-attention architectures applied in the deep learning field, KT models based on the self-attention mechanism began to emerge. For example, SAKT [20] and SAINT+ [21] apply the self-attention mechanism to KT models and achieve higher performance than the traditional DL-based methods. With the development of the self-attention mechanism, Transformer based knowledge tracing models also have been proposed. Ghosh [7] proposes context-aware attentive knowledge tracing (AKT), which introduces a novel monotonic attention mechanism that accounts for the temporal nature of the learning process and the decay of students' knowledge. Nakagawa et al. proposed the Graph-based Knowledge Tracing (GKT) model, which incorporates the potential graph structure of KCs into a graph [22]. There were also KT methods based on BERT that had been proposed. MonacoBERT [23] is a BERT-based KT model that incorporates the monotonic convolutional multi-head attention and classical test-theory-based (CTT-based) embedding strategy to improve performance. BEKT [24] is a Bidirectional Encoder representation from the Transformers-based model that predicts student knowledge state by combining historical learning performance.

### 2.2 Transformer-based Model and Application

Transformer is a prominent neural network model proposed by Vaswani et al., which utilises the self-attention mechanism to extract inherent features. Transformer-based models have achieved significant success in the Deep Learning field, especially in Nature Language Processing (NLP) and image generation tasks [25, 26].

The evolution of Transformer-based models, such as BERT [8] and GPT[27], has achieved outstanding performance in the above tasks. BERT, first proposed by Devlin et al., is a successful application of Transformer [8]. BERT utilises the self-attention mechanism and the masked language model (MLM) to train the Transformer bidirectionally in the NLP fields [8]. BERT is renowned for its exceptional ability to process and comprehend natural language text efficiently. It has consistently outperformed other deep learning models in a broad range of tasks, extending beyond the field of NLP. BERT's success can be attributed to several key features, including its bidirectional context, which allows it to capture the dependencies between both preceding and succeeding tokens in a sequence. Additionally, BERT's large pre-training corpus enables it to learn a robust language representation that can be fine-tuned for downstream tasks with relatively small amounts of labelled data. BERT's transformer architecture, which uses self-attention mechanisms to capture global dependencies between tokens in a sequence, is also a significant factor contributing to its performance. The self-attention mechanism allows BERT to weigh the importance of different tokens in a sequence dynamically, which improves its ability to capture complex patterns and relationships in the data [8]. BERT is also known for its ability to generate high-quality embeddings, which are crucial for many natural language processing tasks [8]. There have been a lot of BERT variants applied in other deep learning fields, demonstrating their outstanding performances. For example, ConvBERT [28] applies the original BERT architecture in the image processing field; BERT4Rec uses BERT model to

Running Title for Header

| Rasch | BERT-based | LSTM |
| :--- | :--- | :--- |
| Embeddings | Architecture | Correctness |
| Sequence |  |  |

![](https://cdn.mathpix.com/cropped/2024_06_04_fbc09b27ccdcf9132dc2g-04.jpg?height=683&width=1610&top_left_y=431&top_left_x=255)

Figure 1: The architecture of LBKT. LBKT consists of three components: 1) the Rasch model-based embeddings (on the left), 2) the BERT-based architecture (in the middle), and 3) the LSTM block (on the right).

improve recommendation systems [29]; LakhNES uses BERT model to enhance Music Generation [30]. However, in the Knowledge Tracing field, although some BERT-based models, such as BEKT [31, 32, 33, 34, 35, 36, 37, 38, 39] and BiDKT [40], are proposed to improve performance, they are unable to outperform state-of-the-art KT methods in large-scale datasets containing long-sequence data.

## 3 Methodology

### 3.1 Problem Statement

The key to knowledge tracing is to predict the correctness of a student's next answer in a sequence. Let $x_{1}, \ldots, x_{t}$ denote the student's actions, and let the $t$-th action be represented as $x_{t}=\left(q_{t}, a_{t}\right)$, where $q_{t}$ is the question presented to the student and $a_{t}$ is the student's response. The goal is to estimate the correctness $P\left(a_{t}=1 \mid x_{1}, \ldots, x_{t-1}\right)$, that is, the correctness of student's response to the current question, given their previous actions in the sequence.

### 3.2 Proposed Model Architecture

We propose a novel model, LBKT, for the task of knowledge tracing on large-scale datasets containing long-sequence data. While previous BERT-based KT models have shown remarkable success in capturing the relations of complex data, they also have inefficiencies when dealing with long sequence student action data [24]. On the other hand, LSTM models have been proven to excel in handling long sequential data. In response to these challenges, we propose a novel KT model that combines the strengths of both the BERT and LSTM models to improve performance on large-scale datasets containing long-sequence data (where long-sequence data indicates a length longer than 400 interactions). The Rasch embedding (also known as the 1PL IRT model) is a method to represent questions and concepts in a mathematical space [9]. The embeddings are created using a vector that summarizes the variation in questions covering a concept and a scalar difficulty parameter that controls how far a question deviates from the concept it covers. The embeddings are used as raw embeddings for questions and responses, which is a way to track a learner's knowledge state. By leveraging the strengths of a BERT-based model, Rasch model-based embeddings, and long short-term memory (LSTM) unit, our proposed model architecture has the potential to effectively process and understand relationships among different features in long-sequence data, as illustrated in Fig. 1.

The first component of LBKT is the Rasch model-based embeddings proposed by Ghosh [7]. The Rasch model-based embeddings consist of difficulty level embeddings $E_{\mathrm{d}}$ and question embeddings $E_{\mathrm{q}}$. These embeddings are multiplied and added to the BERT token embeddings and the $\sin$ and $\cos$ positional embeddings to build the final embeddings, as shown in the following equation:

$$
\begin{equation*}
E=E_{\text {Rasch }}+E_{\text {BERT Token }}+E_{\text {Position }} \tag{1}
\end{equation*}
$$

where the Rasch model-based embeddings $E_{\text {Rasch }}$ are defined as:

$$
\begin{equation*}
E_{\text {Rasch }}=E_{\mathrm{d}}+E_{\mathrm{d}} \times E_{\mathrm{q}} \tag{2}
\end{equation*}
$$

The segment embeddings, which are typically used to represent information about the segment in the BERT model, are replaced by the Rasch embeddings mentioned above in our model's architecture. Rasch model-based embeddings are able to more accurately estimate students' knowledge states, as explained earlier, making them a key contributor to the effectiveness of LBKT for knowledge tracing tasks.

The second component of LBKT is a BERT-based block, which consists of 12 Transformer blocks. Each includes a multi-head attention mechanism, a feedforward network (FFN), and sublayer connections. The multi-head attention mechanism uses the "Scaled Dot Product Attention" method as implemented in BERT, along with queries $Q$, keys $K$, values $V$, and an attention mask for padded tokens. The FFN has a feedforward hidden layer with a size of four times that of the model's hidden layer and uses the GELU activation function rather than RELU.

The sublayer connections in the Transformer block include a residual connection followed by layer normalization. The formulas for the attention mechanism and the FFN are as follows:

$$
\begin{align*}
& \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V  \tag{3}\\
& \quad \operatorname{FFN}(x)=\operatorname{GELU}\left(W_{1} x+b_{1}\right) W_{2}+b_{2} \tag{4}
\end{align*}
$$

In the third component of LBKT, we use a neural network (NN) linear transformation instead of the attention projection typically used in conjunction with the LSTM unit. This is based on our observed improved performance with the NN linear transformation in our experiments. It should be noted that this choice is not necessarily related to the length or complexity of the sequence but rather to the specific characteristics of the data and the task at hand.

Overall, LBKT is a model that is tailored specifically for use in the field of knowledge tracing. It combines the natural language processing capabilities of the BERT model with the ability to accurately estimate knowledge states using Rasch model-based embeddings and the ability to effectively handle long sequences of data using the LSTM unit and the NN linear transformation. This makes it an ideal choice for the task of knowledge tracing in large-scale datasets containing long-sequence data with unbalanced data distribution.

### 3.3 Experiment Setting

### 3.3.1 Datasets

We used five benchmark datasets to validate the effectiveness of the LBKT model, including assist $12 \sqrt{3}$, assist 174 . algebra06 5 , EdNet $[2]$ 6) and Junyi Academy [3] $]$. Table 1 shows the sizes of the above datasets. In general datasets, such as assist 12 and assist 17 , it could be challenging to identify and extract large amounts of long-sequence data. Therefore, we validated the speed performance of every model on two datasets with long-sequence student action data extracted from EdNet and Junyi Academy. The mean action sequence length of EdNet is 121.5. The mean interaction length of Junyi Academic is 104.7. Table 2 shows the action sequence length statistics of EdNet and Junyi Academy. Here, we define the longer action sequence as longer than 100 records. We extract 200 students' action sequences that include interactions longer than 100 actions from each dataset as the long-sequence dataset to validate the performance of different KT models. Lastly, we selected different lengths of action sequences from Ednet to test the[^2]

Running Title for Header

speed performance of each model. We selected four groups with average records lengths of 100, 200, 300, and 400, respectively. Each of these groups included 50 students.

Table 1: Benchmark dataset data statistics

| Dataset | Students | Concepts | Questions | Interactions |
| :---: | :---: | :---: | :---: | :---: |
| assist12 | 24,429 | 264 | 51,632 | $1,968,737$ |
| assist17 | 1,708 | 411 | 3,162 | 934,638 |
| algebra06 | 1,318 | 1,575 | 549,821 | $1,808,533$ |
| EdNet | 784,309 | 1,472 | 11,957 | 641,712 |
| Junyi Academy | 247,606 | 13,169 | 722 | $25,925,922$ |

Table 2: Action Length Statistics of EdNet and Junyi Academy

| Features | Junyi Academy | EdNet |
| :---: | :---: | :---: |
| Students | 247606 | 784309 |
| Max action length | 22067 | 40157 |
| Mean action length | 104.7 | 121.5 |

### 3.3.2 Baseline Models

We compared our LBKT to three state-of-the-art models, BEKT [24], AKT [7], DKVMN [41], as well as the two top baseline models in the Riiid Answer Correctness Prediction Competition provided by Kaggle8 including SSAKT[42], and LTMTI[2].

### 3.3.3 Evaluation Metrics and Validation

We used the accuracy (ACC) and the area under the curve (AUC) as performance metrics to compare the models' performance in five datasets. We also used the training speed, speed ratio, and memory usage as metrics to compare the performance in the large-scale datasets containing long-sequence data (i.e., EdNet and Junyi Academy). Moreover, we used five-fold cross-validation for the evaluation.

### 3.3.4 Hyperparameters for Experiments

To compare with each model, the same parameters were used for model training. The batch size was set to 64, and the train/test split was 0.8/0.2. The model used an embedding size of 128 and the Adam optimizer with a learning rate of 0.001. The loss function used was the Binary Cross Entropy with Logits Loss (BCEWithLogitsLoss). The scheduler was set to OneCycleLR with a maximum learning rate of 0.002 . Dropout was also being used at a rate of 0.2 . The training ran for a total of 100 epochs, with early stopping set to 10 epochs. If the validation loss does not decrease for the first three epochs, the training stops, in order to prevent overfitting and save resources. The maximum sequence length was 200, with an eight-attention head. Hidden sizes were 128 for BERT, 512 for FFN, and 128 for LSTM. The Transformer block/encoder layer was set to 12 .

## 4 Results and Discussion

### 4.1 Overall Performance

LBKT outperforms four baseline models on most metrics in the experiments on five benchmark datasets. Tabel 3 shows the overall performance of each model. We used five-fold cross-validation to estimate their performances. LBKT performed the best on EdNet and Junyi Academy datasets on both ACC and AUC metrics. It also achieved the best performance on the ACC metric on assist12 and AUC on assist17. On algebra06, AKT achieved the best performance on the ACC metric, BEKT achieved the best performance on the AUC metric, and LBKT achieved the second-best performance on both metrics. This result indicates that LBKT is an efficient KT model on most datasets, especially large-scale datasets containing long-sequence interaction data. This was affected by our LBKT model's unique architecture. The LSTM block enables the model to learn the sequential features of the long sequence and gives more importance to the recent actions of the students, which prevents the model from giving too much weight to the long-ago and low-relevance actions and thus improving the training efficiency.[^3]

Running Title for Header

Table 3: Comparison of different KT models on five benchmark datasets. The best performance is denoted in bold.

| Dataset | Metrics | LBKT | BEKT | SSAKT | LTMTI | AKT | DKVMN |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| assist12 | ACC | $\mathbf{0 . 8 1 5}$ | 0.786 | 0.675 | 0.813 | 0.769 | 0.756 |
|  | AUC | 0.768 | $\mathbf{0 . 8 1 2}$ | 0.741 | 0.785 | 0.753 | 0.701 |
| assist17 | ACC | 0.792 | 0.795 | 0.771 | 0.796 | 0.733 | $\mathbf{0 . 7 9 7}$ |
|  | AUC | $\mathbf{0 . 8 1 4}$ | 0.801 | 0.735 | 0.683 | 0.803 | 0.709 |
| algebra06 | ACC | 0.801 | 0.797 | 0.795 | 0.811 | $\mathbf{0 . 8 3 1}$ | 0.800 |
|  | AUC | 0.799 | $\mathbf{0 . 8 1 5}$ | 0.774 | 0.791 | 0.814 | 0.793 |
| EdNet | ACC | $\mathbf{0 . 8 0 3}$ | 0.781 | 0.761 | 0.799 | 0.756 | 0.800 |
|  | AUC | $\mathbf{0 . 8 1 5}$ | 0.795 | 0.798 | 0.802 | 0.798 | 0.796 |
| Junyi | ACC | $\mathbf{0 . 8 3 2}$ | 0.807 | 0.777 | 0.797 | 0.791 | 0.790 |
| Academy | AUC | $\mathbf{0 . 8 5 1}$ | 0.831 | 0.845 | 0.812 | 0.799 | 0.769 |

Tabel 6 shows the performance comparison on the two large-scale datasets. On both datasets, LBKT achieved the best training efficiency. It was 4.29x faster than BEKT on EdNet and 4.77x faster than BEKT on Junyi Academy. Compared with the second-best model, AKT, LBKT was 1.32x faster on EdNet and 1.42x faster on Junyi Academy. For the memory cost, LBKT was about one-third of BEKT and lower than LTMTL on both datasets. Although the memory cost of LBKT was not the smallest, LBKT has achieved the best results in both ACC and AUC metrics running on the same GPU. This allows LBKT to run on middle-range GPUs. To improve the training efficiency, we used a last input as the query method [5] in the Transformer block instead of the whole sequence, which decreased the complexity of the encoder to improve training speed and reduce memory cost.

Table 4: Performance comparison on the two large-scale datasets, EdNet and Junyi Academy. The best performance is denoted in bold.

| Model | EdNet |  |  | Junyi Academy |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | speed $\uparrow$ | speed ratio $\uparrow$ | memory $\downarrow$ | speed $\uparrow$ | speed ratio $\uparrow$ | memory $\downarrow$ |
| BEKT | 4.93 | $1.00 \mathrm{x}$ | $16.7 \mathrm{~GB}$ | 4.85 | $1.00 \mathrm{x}$ | $16.6 \mathrm{~GB}$ |
| SSAKT | 7.13 | $1.44 \mathrm{x}$ | $\mathbf{3 . 4} \mathbf{G B}$ | 6.22 | $1.28 \mathrm{x}$ | $\mathbf{3 . 2} \mathbf{~ G B}$ |
| LTMTI | 13.8 | $1.32 \mathrm{x}$ | $7.69 \mathrm{~GB}$ | 12.1 | $1.19 \mathrm{x}$ | $8.82 \mathrm{~GB}$ |
| AKT | 17.1 | $3.25 \mathrm{x}$ | $4.32 \mathrm{~GB}$ | 16.4 | $3.35 \mathrm{x}$ | $4.37 \mathrm{~GB}$ |
| DKNMN | 5.97 | $2.34 \mathrm{x}$ | $7.68 \mathrm{~GB}$ | 4.67 | $3.75 \mathrm{x}$ | $8.53 \mathrm{~GB}$ |
| LBKT | $\mathbf{2 1 . 3}$ | $\mathbf{4 . 2 9 x}$ | $6.09 \mathrm{~GB}$ | $\mathbf{2 2 . 2}$ | $\mathbf{4 . 7 7 x}$ | $6.08 \mathrm{~GB}$ |

To estimate the model performance on different lengths of data sequences, we sorted the data in the two datasets by length and divided them into four sub-datasets according to the average length. The average lengths of the four sub-datasets were set as $100,200,300$, and 400 , respectively. Sequences shorter than the mean were padded with 0s, and sequences longer than the mean were pruned.

Fig. 2 shows the results of the speed performance comparison of each model processing different lengths of data sequences. LBKT has a relatively high-speed performance compared to other KT models when processing data sequences with varying lengths. LBKT is the fastest model in all four groups of data lengths. AKT and DKMN also have relatively high speeds, with AKT being the second-fastest model in all groups and DKMN being the third-fastest model. Overall, the results suggest that LBKT is the fastest model, and that it is particularly efficient at dealing with long sequences of data. The fact that LBKT maintains its high speed even when processing longer sequences of data indicates that it is well-suited for tasks that require the analysis of large amounts of data over extended periods of time.

### 4.2 Ablation Study

In this section, we explore why LBKT performed better than other methods and which components affected the overall performance. Table 5 shows the results of the ablation study. We compared LBKT, LBKT without Rasch model-based embeddings block (denoted as LBKT-Rasch), LBKT without LSTM block (denoted as LBKT-LSTM), and LBKT without both Rasch model-based embeddings LSTM (denoted as BERT). The results show that LBKT achieved the best performance on EdNet and Junyi Academy on both ACC and AUC metrics. It also achieved the best performance on one metric in every dataset. BERT-only achieved the best performance on assist 17 on ACC, which shows that the combination with Rasch embeddings and LSTM could improve the performance of a single BERT model.

Table 4 shows the ablation study of speed performance comparison on the two large-scale datasets. The results show that LBKT has the highest speed performance among all models on both datasets, with a speed of 21.3 samples per

Running Title for Header

![](https://cdn.mathpix.com/cropped/2024_06_04_fbc09b27ccdcf9132dc2g-08.jpg?height=878&width=1594&top_left_y=499&top_left_x=276)

Figure 2: Speed performance comparison of each model when processing data sequences with varying lengths. The vertical axis is the speed ( $10^{4}$ samples per sec).

Table 5: Results of the ablation study. LBKT-Rasch denotes LBKT without Rasch embedding; LBKT-LTSM denotes LBKT without LSTM block; and BERT denotes only the transformer structure-based blocks are included. The best performance is denoted in bold.

| Dataset | Metrics | LBKT | LBKT-Rasch | LBKT-LSTM | BERT |
| :---: | :---: | :---: | :---: | :---: | :---: |
| assist12 | ACC | $\mathbf{0 . 8 0 4}$ | 0.785 | 0.799 | 0.793 |
|  | AUC | 0.768 | 0.768 | $\mathbf{0 . 7 8 3}$ | 0.750 |
| assist17 | ACC | 0.784 | $\mathbf{0 . 7 9 2}$ | 0.782 | $\mathbf{0 . 7 9 2}$ |
|  | AUC | $\mathbf{0 . 8 1 4}$ | 0.709 | 0.779 | 0.799 |
| algebra06 | ACC | $\mathbf{0 . 8 0 1}$ | 0.796 | 0.792 | 0.798 |
|  | AUC | 0.799 | 0.756 | $\mathbf{0 . 8 0 9}$ | 0.765 |
| EdNet | ACC | $\mathbf{0 . 8 0 3}$ | 0.729 | 0.722 | 0.801 |
|  | AUC | $\mathbf{0 . 8 1 5}$ | 0.758 | 0.794 | 0.809 |
| Junyi Academy | ACC | $\mathbf{0 . 8 8 2}$ | 0.856 | 0.874 | 0.879 |
|  | AUC | $\mathbf{0 . 9 0 7}$ | 0.893 | 0.877 | 0.901 |

![](https://cdn.mathpix.com/cropped/2024_06_04_fbc09b27ccdcf9132dc2g-09.jpg?height=660&width=1660&top_left_y=290&top_left_x=232)

Figure 3: Visualisation of the embedding vector using t-SNE: without Rasch embeddings (on the left) and with Rasch embeddings (on the right). The colour bar is the predicted probability of the outputs.

second on EdNet and 22.2 samples per second on Junyi Academy. This suggests that LBKT is a highly efficient model for processing large amounts of data in real-time. Interestingly, LBKT-LSTM, which removes the LSTM layer from the proposed model, has a significantly lower speed performance compared to LBKT, with a speed ratio of only 1.29x on EdNet and 1.19x on Junyi Academy. This suggests that the LSTM layer is an important component in the proposed model and contributes significantly to its speed performance. This is likely due to the ability of LSTM to capture long-term dependencies and sequential patterns in the data, which can be crucial in educational applications.

Table 6: Speed performance comparison ablation study on the two large-scale datasets, EdNet and Junyi Academy. The best performance is denoted in bold.

| Model | EdNet |  |  | Junyi Academy |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | speed | speed ratio | memory | speed | speed ratio | memory |
| LBKT | $\mathbf{2 1 . 3}$ | $\mathbf{4 . 2 9 x}$ | $6.09 \mathrm{~GB}$ | $\mathbf{2 2 . 2}$ | $\mathbf{4 . 7 7 x}$ | $6.08 \mathrm{~GB}$ |
| LBKT-Rash | 19.1 | $3.51 \mathrm{x}$ | $5.6 \mathrm{~GB}$ | 20.31 | $3.67 \mathrm{x}$ | $\mathbf{3 . 1} \mathbf{G B}$ |
| LBKT-LSTM | 13.8 | $1.29 \mathrm{x}$ | $7.87 \mathrm{~GB}$ | 12.1 | $1.09 \mathrm{x}$ | $8.37 \mathrm{~GB}$ |
| BERT | 12.1 | $3.27 \mathrm{x}$ | $\mathbf{4 . 3 2 G B}$ | 11.4 | $3.52 \mathrm{x}$ | $4.34 \mathrm{~GB}$ |

### 4.3 Analysis of Embedding Strategy

In this section, We used t-SNE as the visualisation tool to show the interpretability of LBKT's embedding strategy. Fig. 3 -left shows the results of No-Rasch-embedding, and Fig 3 right shows the Rasch embedding strategy. We can see that, in the No-Rasch-embedding scenario, the difficult questions' embeddings (dark blue vectors) mixed with the easy questions' embeddings (yellow to light blue vectors). In figure 3 right, the difficult level embeddings were separated to avoid mixing with easy level embeddings.

Questions at a higher difficulty level are typically associated with longer sequence data, as students spend more time and steps on difficult exercises, which results in longer interaction sequences. Rasch model-based embeddings could divide different difficulty-level parts before the start of the model training and not mix them with other difficulty-level embeddings. As a result, it might increase training efficiency to converge faster.

## 5 Conclusion and Future work

In this study, we have developed LBKT, which employs a BERT-based architecture with an LSTM block for processing long-sequence data, and Rasch model-based embeddings for different difficulty levels of questions. Experiments show

Running Title for Header

that LBKT outperforms baseline models on most benchmark datasets. We also conducted the speed performance experiment on the two large-scale datasets containing long-sequence data. The results suggest that LBKT could process long-sequence data faster and is more resource-efficient. Moreover, we conducted an ablation study for different components of LBKT. The results indicate that the LSTM component aided in improving the performance of dealing with long-sequence data. Furthermore, we conducted an analysis of the embedding strategy using t-SNE. The result shows that Rasch embedding could process the difficulty-level features effectively. In future work, we plan to improve the architecture to process more comprehensive data to satisfy the sustainable development requirement of increasing large-scale datasets containing long-sequence data emerging. Furthermore, we plan to build a more advanced embedding strategy to allow the model to process multiple data types simultaneously, such as exercises and concepts information from the dataset.

## Acknowledgments

This was was supported in part by......

## References

[1] Ghodai Abdelrahman, Qing Wang, and Bernardo Pereira Nunes. Knowledge tracing: A survey. ACM Computing Surveys, 2022.

[2] Youngduck Choi, Youngnam Lee, Dongmin Shin, Junghyun Cho, Seoyon Park, Seewoo Lee, Jineon Baek, Chan Bae, Byungsoo Kim, and Jaewe Heo. Ednet: A large-scale hierarchical dataset in education. In International Conference on Artificial Intelligence in Education, pages 69-73. Springer, 2020.

[3] Haw-Shiuan Chang, Hwai-Jung Hsu, and Kuan-Ta Chen. Modeling exercise relationships in e-learning: A unified approach. In EDM, pages 532-535, 2015.

[4] Yang Liu, Jing Zhou, and Weiguo Lin. Efficient attentive knowledge tracing for long-tail distributed records. In 2021 IEEE/ACIS 6th International Conference on Big Data, Cloud Computing, and Data Science (BCD), pages 104-109. IEEE, 2021.

[5] SeungKee Jeon. Last query transformer rnn for knowledge tracing. arXiv preprint arXiv:2102.05038, 2021.

[6] Qi Liu, Shuanghong Shen, Zhenya Huang, Enhong Chen, and Yonghe Zheng. A survey of knowledge tracing. arXiv preprint arXiv:2105.15106, 2021.

[7] Aritra Ghosh, Neil Heffernan, and Andrew S Lan. Context-aware attentive knowledge tracing. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining, pages 2330-2339, 2020.

[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[9] Georg Rasch. Probabilistic models for some intelligence and attainment tests. ERIC, 1993.

[10] Ghodai Abdelrahman, Qing Wang, and Bernardo Nunes. Knowledge tracing: A survey. ACM Computing Surveys, 55(11):1-37, 2023.

[11] Albert T Corbett and John R Anderson. Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and user-adapted interaction, 4(4):253-278, 1994.

[12] Albert Corbett. Cognitive mastery learning in the act programming tutor. In Adaptive User Interfaces. AAAI SS-00-01. Retrieved from https://aaai. org/Library/Symposia/Spring/ss00-01. php, 2000.

[13] Michael Villano. Probabilistic student models: Bayesian belief networks and knowledge space theory. In International Conference on Intelligent Tutoring Systems, pages 491-498. Springer, 1992.

[14] Michael V Yudelson, Kenneth R Koedinger, and Geoffrey J Gordon. Individualized bayesian knowledge tracing models. In International conference on artificial intelligence in education, pages 171-180. Springer, 2013.

[15] Tanja KÃ¤ser, Severin Klingler, Alexander G Schwing, and Markus Gross. Dynamic bayesian networks for student modeling. IEEE Transactions on Learning Technologies, 10(4):450-462, 2017.

[16] Hao Cen, Kenneth Koedinger, and Brian Junker. Learning factors analysis-a general method for cognitive model evaluation and improvement. In Intelligent Tutoring Systems: 8th International Conference, ITS 2006, Jhongli, Taiwan, June 26-30, 2006. Proceedings 8, pages 164-175. Springer, 2006.

[17] Philip I Pavlik Jr, Hao Cen, and Kenneth R Koedinger. Performance factors analysis-a new alternative to knowledge tracing. Online Submission, 2009.

Running Title for Header

[18] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444, 2015.

[19] Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J Guibas, and Jascha Sohl-Dickstein. Deep knowledge tracing. Advances in neural information processing systems, 28, 2015.

[20] Shalini Pandey and George Karypis. A self-attentive model for knowledge tracing. arXiv preprint arXiv:1907.06837, 2019.

[21] Dongmin Shin, Yugeun Shim, Hangyeol Yu, Seewoo Lee, Byungsoo Kim, and Youngduck Choi. Saint+: Integrating temporal features for ednet correctness prediction. In LAK21: 11th International Learning Analytics and Knowledge Conference, pages 490-496, 2021.

[22] Hiromi Nakagawa, Yusuke Iwasawa, and Yutaka Matsuo. Graph-based knowledge tracing: modeling student proficiency using graph neural network. In IEEE/WIC/ACM International Conference on Web Intelligence, pages $156-163,2019$.

[23] Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim. Monacobert: Monotonic attention based convbert for knowledge tracing. arXiv preprint arXiv:2208.12615, 2022.

[24] Zejie Tiana, Guangcong Zhengc, Brendan Flanaganb, Jiazhi Mic, and Hiroaki Ogatab. Bekt: Deep knowledge tracing with bidirectional encoder representations from transformers. In Proceedings of the 29th International Conference on Computers in Education, 2021.

[25] Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha. Ammus: A survey of transformerbased pretrained models in natural language processing. arXiv preprint arXiv:2108.05542, 2021.

[26] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, pages 4055-4064. PMLR, 2018.

[27] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30(4):681-694, 2020.

[28] Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. Convbert: Improving bert with span-based dynamic convolution. Advances in Neural Information Processing Systems, 33:12837-12848, 2020.

[29] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 1441-1450, 2019.

[30] Chris Donahue, Huanru Henry Mao, Yiting Ethan Li, Garrison W Cottrell, and Julian McAuley. Lakhnes: Improving multi-instrumental music generation with cross-domain pre-training. arXiv preprint arXiv:1907.04868, 2019.

[31] Zhaoxing Li, Mark Jacobsen, Lei Shi, Yunzhan Zhou, and Jindi Wang. Broader and deeper: A multi-features with latent relations bert knowledge tracing model. In European Conference on Technology Enhanced Learning, pages 183-197. Springer, 2023.

[32] Zhaoxing Li, Lei Shi, Alexandra Cristea, Yunzhan Zhou, Chenghao Xiao, and Ziqi Pan. Simstu-transformer: A transformer-based approach to simulating student behaviour. In International Conference on Artificial Intelligence in Education, pages 348-351. Springer, 2022.

[33] Zhaoxing Li, Lei Shi, Yunzhan Zhou, and Jindi Wang. Towards student behaviour simulation: a decision transformer based approach. In International Conference on Intelligent Tutoring Systems, pages 553-562. Springer, 2023.

[34] Zhaoxing Li. Deep Reinforcement Learning Approaches for Technology Enhanced Learning. PhD thesis, Durham University, 2023.

[35] Zhaoxing Li, Lei Shi, Jindi Wang, Alexandra I Cristea, and Yunzhan Zhou. Sim-gail: A generative adversarial imitation learning approach of student modelling for intelligent tutoring systems. Neural Computing and Applications, 35(34):24369-24388, 2023.

[36] Jindi Wang, Ioannis Ivrissimtzis, Zhaoxing Li, Yunzhan Zhou, and Lei Shi. Exploring the potential of immersive virtual environments for learning american sign language. In European Conference on Technology Enhanced Learning, pages 459-474. Springer, 2023.

[37] Jindi Wang, Ioannis Ivrissimtzis, Zhaoxing Li, and Lei Shi. Comparative efficacy of $2 \mathrm{~d}$ and $3 \mathrm{~d}$ virtual reality games in american sign language learning. In The 31st IEEE Conference on Virtual Reality and 3D User Interfaces. Newcastle University, 2024.

Running Title for Header

[38] Jindi Wang, Ioannis Ivrissimtzis, Zhaoxing Li, Yunzhan Zhou, and Lei Shi. User-defined hand gesture interface to improve user experience of learning american sign language. In International Conference on Intelligent Tutoring Systems, pages 479-490. Springer, 2023.

[39] Jindi Wang, Ioannis Ivrissimtzis, Zhaoxing Li, and Lei Shi. Impact of personalised ai chat assistant on mediated human-human textual conversations: Exploring female-male differences. In Companion Proceedings of the 29th International Conference on Intelligent User Interfaces, pages 78-83, 2024.

[40] Weicong Tan, Yuan Jin, Ming Liu, and He Zhang. Bidkt: Deep knowledge tracing with bert. In International Conference on Ad Hoc Networks, International Conference on Testbeds and Research Infrastructures, pages 260-278. Springer, 2022.

[41] Xia Sun, Xu Zhao, Yuan Ma, Xinrui Yuan, Feijuan He, and Jun Feng. Muti-behavior features based knowledge tracking using decision tree improved dkvmn. In Proceedings of the ACM Turing Celebration Conference-China, pages 1-6, 2019 .

[42] Xuelong Zhang, Juntao Zhang, Nanzhou Lin, and Xiandi Yang. Sequential self-attentive model for knowledge tracing. In International Conference on Artificial Neural Networks, pages 318-330. Springer, 2021.


[^0]:    * Citation: Authors. Title. Pages.... DOI:000000/11111.

[^1]:    ${ }^{2}$ Source code and datasets are available at https://github.com/******/LBKT

[^2]:    ${ }^{3}$ https://sites.google.com/site/assistmentsdata/home

    ${ }^{4}$ https://sites.google.com/site/assistmentsdata/home

    ${ }^{5}$ https://pslcdatashop.web.cmu.edu/KDDCup

    ${ }^{6}$ https://github.com/riiid/ednet

    ${ }^{7}$ https://pslcdatashop.web.cmu.edu/Files?datasetId=1275

[^3]:    ${ }^{8}$ https://www.kaggle.com/code/datakite/riiid-answer-correctness

