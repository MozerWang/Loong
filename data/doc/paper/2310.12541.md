# Large Language Model for Multi-objective Evolutionary Optimization 

Fei Liu, Xi Lin, Zhenkun Wang, Member, IEEE, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, and Qingfu<br>Zhang, Fellow, IEEE


#### Abstract

Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the search operators need a carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well on new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose a new version of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on different test benchmarks show that our proposed method can achieve competitive performance with widely used MOEAs. It is also promising to see the operator only learned from a few instances can have robust generalization performance on unseen problems with quite different patterns and settings. The results reveal the potential benefits of using pre-trained LLMs in the design of MOEAs. To foster reproducibility and accessibility, the source code is https://github.com/FeiLiu36/LLM4MOEA.


Index Terms-Multi-objective optimization, Large language model, Evolutionary algorithm, Machine learning.

## I. INTRODUCTION

REAL-WORLD optimization problems usually have multiple conflicting objectives to deal with, which cannot be simultaneously optimized by a single solution. Multiobjective evolutionary algorithm (MOEA) is a promising approach to finding a set of trade-off solutions among the objectives in a single run [1]-[4]. Due to their effectiveness and flexibility, a large number of MOEAs have been proposed in the past decades. They have been applied to address different application problems in various fields including engineering

Fei Liu, Xi Lin, and Qingfu Zhang are with the Department of Computer Science, City University of Hong Kong, Hong Kong (e-mail: fliu36c@my.cityu.edu.hk; xi.lin@my.cityu.edu.hk; qingfu.zhang @cityu.edu.hk).

Zhenkun Wang is with the School of System Design and Intelligent Manufacturing and the Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China (e-mail: wangzhenkun90@gmail.com).

Shunyu Yao is with the School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen 518055, China (e-mail: 12032920@mail.sustech.edu.cn).

Xialiang Tong and Mingxuan Yuan are with Huawei Noah's Ark Lab (email: tongxialiang@huawei.com; yuan.mingxuan@huawei.com). design [5]-[7], finance [8], and machine learning [9]-[11]. However, these algorithms usually need to be carefully designed by experts, which could require a large effort and be time-consuming, especially for dealing with new problems.

In recent years, there has been a growing interest in leveraging machine learning (ML) techniques to automate and enhance the design of MOEAs [12]-[15]. By incorporating learning-based approaches, these algorithms can learn from historical information in both offline and online manner to adjust their behavior accordingly, resulting in improved performance and adaptability. Many studies have utilized ML techniques to enhance traditional MOEAs [16]-[22], of which the effectiveness is largely dependent on the chosen MOEA framework. Another approach involves training neural networks to learn heuristics that can replace search operators or even the entire algorithm itself [23]-[26]. These methods have demonstrated promising performance within relatively short running times. However, they require a time-consuming training phase and a well-designed training strategy. Additionally, they usually suffer from poor generalization performance across different distributions and problems.

In the past two years, large language models (LLMs) have demonstrated remarkable capabilities in various research domains [27], including natural language processing [28], programming [29], medicine [30]-[32], chemistry [33], chip design [34], [35], and optimization [36|-[38]. These LLMs excel at performing diverse tasks in a zero-shot manner [39], [40]. Very recently, a few works have been proposed to employ LLMs to serve as pre-trained black-box optimizers [15], [41]-[43]. However, these approaches only work for single-objective optimization, and the black-box and online interactive nature also hinder their usefulness in practice.

This work investigates the effectiveness of LLMs for multiobjective evolutionary optimization. We propose a novel approach where the multiobjective optimization problem (MOP) is decomposed into several single-objective subproblems (SOPs), and LLMs with prompts are employed as the search operators for each subproblem. Additionally, we propose to use a simple and explicit linear operator (LO) with randomness to interpret and approximate the LLM's behavior. With the explicit linear operator, we can obtain a white-box and explainable MOEA framework called MOEA/D-LO, which can also get rid of the costly online interaction with LLMs.

To the best of our knowledge, we present the first attempt to apply LLMs in the context of multiobjective evolutionary optimization. Our main contributions are summarized as follows:

- We propose a decomposition-based MOEA framework that leverages pre-trained LLMs with prompt engineering as search operators without problem-specific training.
- We develop a simple model-based approach to approximate the LLM behavior with an explicit linear operator with randomness, and propose a white-box framework MOEA/D-LO.
- We demonstrate the proposed MOEA/D-LO can achieve competitive overall performance on various MOPs, while it performs more robustly across diverse instances than existing MOEAs with handcrafted search operators.


## II. RELATED WORKS

## A. Multiobjective evolutionary algorithm (MOEA)

Multiobjective evolutionary algorithms (MOEAs) are major methods for multiobjective optimization. They can generate a set of approximate Pareto optimal solutions via a single run while offering good scalability and adaptability for handling complex and non-linear MOPs [1]. According to algorithm paradigms, MOEAs can be broadly categorized into three classes: dominant-based MOEAs [44], [45], indicate-based MOEAs [4], [46]-[48], and decompositionbased MOEAs [49], [50]. Domination-based MOEAs adopt a dual-level ranking scheme. The Pareto dominance relation determines the first-level ranking, while the second-level ranking focuses on solution diversity. Indicate-based MOEAs employ an indicator to assess the quality of solution sets. The evaluation of each individual depends on its contribution to the performance indicator. Decomposition-based MOEAs decompose the original MOP into multiple single-objective sub-problems, which are solved in a cooperative manner. Each sub-problem is formulated with a set of weights and targets distinct positions on the Pareto front.

In addition to the algorithm paradigms, the performance of MOEAs heavily relies on search operators such as the GA operators [44], PSO operators [51], and DE operators [52]. All these operators are handcrafted by researchers and might perform poorly on different problems [53]-[55]. To improve the traditional operators, one direction is the development of dynamic operators, which adaptively adjust their behavior based on the problem features and the state of optimization [10], [56], [57]. Another approach is to ensemble a diverse set of search operators to enable effective exploration of different regions of the search space to find diverse highquality solutions [58]. However, designing novel operators always requires expert knowledge and much effort [51], [55], [57], [59]-61].

## B. Learning-based MOEA

Learning-based MOEAs adopt machine learning techniques to assist or design MOEAs. One common approach is to use surrogate models, such as Gaussian processes and neural networks, to approximate the objective functions [62]-[64]. This approach aims to reduce the expensive evaluations of the true objective functions and thus accelerate the search process. However, properly training a high-quality model is challenging and time-consuming, especially for high-dimensional problems. Another direction is to use reinforcement learning techniques to learn the optimal algorithm configuration [65], selection policy for the search operators [22], [66], and construct partial solution [67]. Other works employ metalearning [68] and transfer-learning [69]-[71] methods to learn a generalized strategy and transfer knowledge from other tasks or solutions to improve the optimization efficiency.

Additionally, there is a group of recent works exploring end-to-end neural solvers for MOPs [23]-[26]. They train a neural network on a large dataset and directly generate tradeoff solutions in a very short inference time. These works have shown promising results, but they often require much effort in designing and training the neural models. Moreover, they usually suffer from poor generalization performance on outof-distribution problems.

## C. LLMs for MOEA

In the past two years, LLMs have become increasingly powerful with the exponentially increasing model size and the huge training datasets. They have demonstrated remarkable performance in various research domains [27] and received progress in personalization [72]. Several recent studies have explored the tuning and prompting of language models to emulate the functionality of mutation and crossover operators in evolutionary algorithms [43], [73], [74]. Despite this, the utilization of LLMs for designing optimization algorithms is still in its early stage. [75] presents a trial of using LLMs to generate novel optimization algorithms. A few works [41], [76] have shown the potential for optimization solely through prompting without the need for additional training. However, they are all used for single-objective optimization as a blackbox solver, which requires costly online interaction with LLMs during the whole optimization process. The effectiveness of employing LLMs for multiobjective evolutionary optimization remains unexplored.

## III. PRoblem FormULATION

Without loss of generality, we consider the following multiobjective minimization problem in this paper:

$$
\begin{align*}
& \min F(\mathbf{x})=\left(f_{1}(\mathbf{x}), \ldots, f_{m}(\mathbf{x})\right)^{T}  \tag{1}\\
& \text { s.t. } \mathbf{x} \in \Omega
\end{align*}
$$

where $m$ is the number of objectives, $f_{j}(\mathbf{x})$ is the $j$-th objective function, and $\mathbf{x}=\left\{x_{1}, \ldots, x_{d}\right\}^{T}$ is the decision variable in the $d$-dimensional decision space $\Omega \subset \Re^{d}$. In non-trivial cases, the objectives will contradict each other, and hence no single solution in $\Omega$ can minimize all the objectives simultaneously. Therefore, we aim to find the solutions with optimal trade-offs among the objectives, which can be defined in terms of Pareto optimality [77].

A solution $\mathbf{x}_{1}$ is said to dominate $\mathbf{x}_{2}$, denoted as $\mathbf{x}_{1} \prec \mathbf{x}_{2}$, if and only if $f_{j}\left(\mathbf{x}_{1}\right) \leq f_{j}\left(\mathbf{x}_{2}\right)$ for each $j \in\{1, \ldots, m\}$. and $f_{j}\left(\mathbf{x}_{1}\right)<f_{j}\left(\mathbf{x}_{2}\right)$ for at least one $j \in\{1, \ldots, m\} . \mathbf{x}^{*}$ is Pareto optimal if no $\mathbf{x} \in \Omega$ dominates $\mathbf{x}^{*}$. The set of all the Pareto optimal points is called the Pareto set (PS), and the set of their corresponding objective vectors is called the Pareto front (PF). The goal is to approximate the entire PF as closely and as diverse as possible.

## IV. LLM FOR MOEA

## A. Framework

In this work, we use the decomposition-based MOEA framework [49] to integrate LLM. As illustrated in Fig. 1, the original MOP is decomposed into a number of subproblems, which are solved simultaneously in a collaborative way. LLM is adopted for generating new individuals (offspring) for each subproblem.

```
Algorithm 1: Algorithm Framework
    Input: The maximum number of evaluations: $N_{\max }$;
        The number of subproblems: $N$; A set of
        uniform $N$ weight vectors: $\lambda^{1}, \ldots, \lambda^{N}$;
        Neighborhood size $T$. The number of parents $l$
        for LLM; The number of new individuals $s$
        generated by LLM.
    Output: External population $E P$.
    Initialization:
    Generate the associated neighborhood weight vectors
        $B^{i}$ for each weight vector $\lambda^{i}$;
    Generate initial population $P=\left\{\mathbf{x}^{1}, \ldots, \mathbf{x}^{N}\right\}$
        randomly ;
    Initialize external population $E P$ and reference point
    $\mathbf{z}=\left(z_{1}, \ldots, z_{m}\right)^{T}$;
    while $N_{\max }$ not reached do
        for $i=1, \ldots, N$ do
            Selection: select a subset of input individuals
            $p^{i}=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{l}\right\}$;
            Prompt engineering: generate textual Prompt
            for LLM given the subproblem $g^{i}\left(\mathbf{x} \mid \lambda^{i}\right)$ and
            the subset $p^{i}$;
            Reproduction: let LLM generate a set of new
                individuals $o^{i}=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{s}\right\}$ with the
                Prompt;
            Update: update reference point
                    $\mathbf{z}=\left(z_{1}, \ldots, z_{m}\right)^{T}$, population $P$ and external
                    population $E P$.
```

In initialization, the initial population is randomly generated, and a set of $N$ weight vectors $\left\{\lambda^{1}, \ldots, \lambda^{N}\right\}$ is initialized using the Das and Dennis method [78]. Subproblems $g^{i}\left(\mathbf{x} \mid \lambda^{i}\right), i=1, \ldots, N$ are defined with respect to the weight vectors $\left\{\lambda^{1}, \ldots, \lambda^{N}\right\}$. During the optimization process, a population and a neighborhood are maintained for each subproblem. The population consists of the best solution found so far for each subproblem, while only the solutions from neighboring subproblems are utilized when optimizing a specific subproblem. The neighborhood subproblems $B^{i}$ for $i$ th subproblem are defined based on the distances between their aggregation weight vectors. These subproblems are optimized simultaneously. The LLM with in-context learning serves as a black-box search operator to generate new individuals in each subproblem.

For the optimization of the $i$-th subproblem $g^{i}\left(\mathbf{x} \mid \lambda^{i}\right)$, a subset of population $p^{i}$ with $l$ individuals are randomly selected. Then a prompt is generated based on the subproblem and the

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-03.jpg?height=718&width=751&top_left_y=191&top_left_x=1142)

Fig. 1. A decomposition-based MOEA framework for integrating LLM. The original MOP is decomposed into several subproblems. The LLM is used as a black-box search operator to generate new offspring in each subproblem. A prompt engineering block is used for in-context learning for LLM.

subset of individuals with their function values (details will be introduced in subsection IV-B. A set of $s$ new individuals $o^{i}=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{s}\right\}$ are created by the LLM given the prompt content. These new individuals are then used to update reference points, neighboring solutions, and the external population (EP) using the same methods as described in [49].

The formulation of subproblems depends on the associated weight vector and the used aggregation function. Among various aggregation functions in the literature [49], we choose the Chebyshev weighting function [77]. The $i$-th subproblem of a multiobjective optimization problem with weight vector $\lambda^{i}=\left\{\lambda_{1}^{i}, \ldots, \lambda_{m}^{i}\right\}^{T}$ is defined as follows:

$$
\begin{equation*}
g^{i}\left(\mathbf{x} \mid \lambda^{\mathbf{i}}\right)=\max _{1 \leq j \leq m}\left\{\lambda_{j}^{i}\left(f_{j}(\mathbf{x})-z_{j}\right)\right\} \tag{2}
\end{equation*}
$$

where $\mathbf{z}=\left(z_{1}, \ldots, z_{m}\right)^{T}$ is a reference point that records the minimum objective value vector among all the evaluated individuals.

## B. LLM as a black-box optimizer

The original multi-objective optimization problem is transformed into a set of single-objective optimization tasks with the objective function formulated in equation (2). We adopt LLM as a pre-trained black-box optimizer to generate new individuals for each subproblem. The generation of new individuals is considered an in-context learning process facilitated by prompt engineering [79]-[81].

1) Selection: To promote the in-context learning of LLM, we select input $l$ individuals for the $i$-th subproblem. These individuals are selected from the neighborhood $B^{i}$ of the current subproblem with a probability $\sigma_{3}$, while from the entire population $P$ with a probability of $1-\sigma_{3}$. This selection strategy aligns with the conventional approach used in MOEA/D implementations. The difference is that the size of
the selected individuals in our framework is scalable because of the flexibility of the interaction with LLM. Subsequently, the selected individuals are sorted in descending order based on their fitness values in terms of the aggregation function of the $i$-th subproblem.
2) Prompt engineering: In prompt engineering, we provide the following three kinds of information to LLMs:

- Description of task: A description of the optimization task including the number of variables and objectives, as well as whether the goal is to minimize or maximize.
- In-context samples: A set of input samples in the form of variable-objective pairs.
- Expected outputs: A description of the expected responses we want. The responses should be given in a specific format.

An example of a prompt is illustrated below, which includes 1) Description of task: the task requires finding a more optimal solution with a smaller aggregation function value for the $i$-th subproblem; 2) In-context samples: it consists of a set of $l=10$ input individuals including their decision variables and aggregation function values. For each sample, the variables are denoted by the starting symbol $<$ start $>$ and ending symbol $<$ end $>$ and the values are listed below the variables; and 3) Expected outputs: the desired outcome includes $s=2$ new individuals (points) that are distinct from the input points. These points should be presented in an easily recognizable format (e.g., start with <start> and end with $<$ end $>$ ), which allows the MOEA algorithm to identify them from the textual responses. It is important to set limitations to prevent the responses from being too long and masking redundant information (e.g., do not write code and give any explanation) for efficient and stable interaction with LLM.

## Example Prompt:

Now you will help me minimize a function with 3 variables. I have some points and the function values of them. The points start with $<$ start $>$ and end with $<$ end $>$. The points are arranged in descending order based on their function values, where lower values are better.

point: <start $>0.344,0.940,0.582,0.878<$ end $>$

value: 4.582

point: $<$ start $>0.376,0.973,0.604,0.828<$ end $>$

value: 4.530

...

point: $<$ start $>0.787,0.61,0.053,0.420<$ end $>$

value: 2.399

point: $<$ start $>0.012,0.532,0.001,0.196<$ end $>$

value: 1.474

Give me 2 new points that are different from all points above, and have a function value lower than any of the above. Do not write code. Do not give any explanation. Each output new point must start with $<$ start $>$ and end with $<$ end $>$.

3) Reproduction: One important characteristic of LLM responses is the presence of randomness and uncertainty [82]. Unlike handcrafted evolutionary search operators that consis-
TABLE I

HV VALUES ON FIVE RE TEST INSTANCES

|  | RE21 | RE22 | RE23 | RE24 | RE25 |
| :---: | :---: | :---: | :---: | :---: | :---: |
| MOEA/D | 0.781 | $\mathbf{0 . 7 2 2 1}$ | 1.1491 | $\mathbf{1 . 1 5 7 4}$ | $\mathbf{1 . 0 8 1 2}$ |
| MOEA/D-LLM | $\mathbf{0 . 7 9 3 6}$ | 0.6853 | $\mathbf{1 . 1 5 6 1}$ | 1.153 | $\mathbf{1 . 0 8 1 2}$ |

tently generate individuals with exactly the same fixed format, LLM may yield unexpected responses. These unexpected responses could deviate from the required format and be unrecognizable by the program. To avoid this, we verify the textual response and initiate a new in-context learning process if no offspring is identified. It is also worth noting that LLM has the ability to retain conversation history, which can significantly influence its output. For example, it may generate the same response for a sequence of subproblems when the input individuals remain unchanged for several iterations. In this paper, we simply clear the cash before each call to LLM to exclude any history information. However, exploring how to effectively use the memory of LLM for optimization is an interesting future working direction.

With the two measures mentioned above, the LLM can almost always return the expected responses. Our algorithm identifies the suggested new points in the textual response and subsequently converts them into hard-coded new individuals.

4) Update: We use the same strategy as the original MOEA/D [49] to manage reference point $\mathbf{z}=\left(z_{1}, \ldots, z_{m}\right)^{T}$, population $P$, and external population $E P$.

## C. Demonstration on five instances

We test the proposed LLM-based MOEA/D on five biobjective optimization instances, namely RE21, RE22, RE23, RE24, and RE25, which are extracted from real-world engineering optimization problems with various features and different PF shapes [83].

We call the proposed method MOEA/D-LLM and evaluate it against the original MOEA/D using a genetic algorithm. The algorithm is implemented in Python using the pymoo framework [84], which allows for convenient implementation of custom search operators and interaction with the LLM API. For the experiments, we utilize the GPT-3.5 Turbo model $\square^{1}$ as the LLM optimizer.

The experimental settings are as follows:

- The number of subproblems $N: 50$;
- The number of weight vectors in the neighborhood $T$ : 10
- The maximum number of evaluations $N_{\max }: 1000$;
- The probability of crossover $\sigma_{1}: 1.0$;
- The probability of mutation $\sigma_{2}: 0.9$;
- The probability of neighborhood selection $\sigma_{3}: 0.9$;
- The number of input individuals $l: 10$;
- The number of output individuals $s$ : 2 .

Table I lists the hypervolume (HV) results on five RE instances and Fig. 2 compares the approximated PFs. The MOEA/D with GPT-3.5 generates competitive results with the[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-05.jpg?height=948&width=808&top_left_y=217&top_left_x=192)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-05.jpg?height=382&width=390&top_left_y=237&top_left_x=214)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-05.jpg?height=388&width=399&top_left_y=711&top_left_x=207)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-05.jpg?height=376&width=388&top_left_y=232&top_left_x=608)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-05.jpg?height=388&width=396&top_left_y=711&top_left_x=599)

(d)
Fig. 2. Comparison of approximated PFs obtained by MOEA/D with GPT-3.5 and MOEA/D with genetic algorithm: (a) RE21, (b) RE22, (c) RE24, and (d) RE25.

MOEA/D with the genetic algorithm. A large proportion of the PFs obtained by the two algorithms overlap, especially on RE25. The experimental results show:

- The proposed MOEA/D-LLM framework, which incorporates a pre-trained black-box LLM as the search operator, is effective in generating satisfactory results on the five test instances.
- MOEA/D-LLM demonstrates competitive performance in terms of the HV indicators and the final approximated Pareto front.


## D. Discussion

We present the following discussion on the advantages and limitations of our LLM-assisted MOEA framework: LLM encounters challenges in directly comprehending and optimizing MOPs due to their complex nature. The decomposition-based MOEA framework can significantly simplify this task. On the one hand, LLM is not directly required to provide the nondominated set for a given MOP. Instead, it is utilized iteratively to generate progressively improved individuals. On the other hand, the original MOP is decomposed into several singleobjective subproblems, wherein each subproblem focuses on optimizing a specific objective. LLM serves as the optimizer for each single-objective subproblem.

The employment of LLMs in the development of search operators provides significant advantages over the conventional hand-crafted approach. It automates the design process and minimizes the need for in-depth field knowledge and vast experience. Users are merely required to supply suitable prompts in a natural language format, enabling involvement from

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-05.jpg?height=461&width=704&top_left_y=192&top_left_x=1166)

Fig. 3. LLM inspired white-box linear operator (LO) with randomness.

practitioners across diverse disciplines, and simplifying the creation of novel algorithms. Additionally, LLM can handle input and output of varying sizes in a flexible way, unlike traditional MOEA operators designed for a fixed number of parents and offspring.

Although we have demonstrated the effectiveness of the proposed MOEA/D-LLM in some test instances, similar to previous works [37], [41], the LLM is used as a black-box optimizer. There are two remaining issues to be addressed. Firstly, treating LLM as a black-box optimizer limits our understanding and ability to explain its behavior. While LLM produces desired results, how to interpret and illustrate the internal decision-making process of LLM remains a challenge. Secondly, the extensive online interactions between our MOEA framework and the LLM can be time-consuming and resource-intensive. LLM processes tens of billions of parameters, and the inference time is significantly more expensive than a single run of conventional evolutionary operators.

## V. LLM INSPIRED WHITE-BOX OPERATOR

Several studies have been conducted to gain a deeper understanding and interpretation of the behavior of LLM and in-context learning [85]-[88]. In this paper, we interpret the results of LLM from the perspective of evolutionary optimization by treating it as a weighted linear operator with randomness from the input individuals to the offspring (Fig. 3). We first show that many existing manually crafted operators in evolutionary algorithms can be formulated as mappings from the current population to new offspring. Then, we use an explicit weighted linear operator (LO) with a randomness term to approximate the results of LLM. Finally, based on the LO, we propose a new version of MOEA/D, named MOEA/D-LO.

## A. A general formulation of search operators

In the past decades, many search operators have been proposed for evolutionary computation. They can be formulated in a general form as the mapping from the selected input individuals to new individuals [89]. For example, the following
is a typical implementation of crossover operators as mapping from two selected parents to a new offspring:

$$
\begin{align*}
\mathbf{x}_{o} & =\mathbf{w}_{i} \cdot \mathbf{x}_{i}+\mathbf{w}_{j} \cdot \mathbf{x}_{j} \\
& =0 \cdot \mathbf{x}_{1}+\cdots+\mathbf{w}_{i} \cdot \mathbf{x}_{i}+\cdots+\mathbf{w}_{j} \cdot \mathbf{x}_{j}+\cdots+0 \cdot \mathbf{x}_{N} \\
& =W^{C R} \cdot X \tag{3}
\end{align*}
$$

where $\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{N}$ denote the $N$ individuals in the current population. From these, $\mathbf{x}_{i}=\left(x_{i, 1}, \ldots, x_{i, d}\right)^{T}$ and $\mathbf{x}_{j}=$ $\left(x_{j, 1}, \ldots, x_{j, d}\right)^{T}$ represent two arbitrarily chosen parents, while $\mathbf{w}_{i}=\left(w_{i, 1}, \ldots, w_{i, d}\right)^{T}$ and $\mathbf{w}_{j}=\left(w_{j, 1}, \ldots, w_{j, d}\right)^{T}$ are their respective weight vectors spanning $d$ variables. The assigned weight for each variable at each stage varies based on the used crossover operators.

Differential evolution (DE) operators can also be formulated in the same manner. A typical implementation (DE/rand/1) [90] of DE is:

$$
\begin{align*}
\mathbf{x}_{o} & =\mathbf{x}_{i}+\mathbf{w} \cdot\left(\mathbf{x}_{j}-\mathbf{x}_{k}\right) \\
& =0 \cdot \mathbf{x}_{1}+\cdots+1 \cdot \mathbf{x}_{i}+\cdots+\mathbf{w} \cdot \mathbf{x}_{j}+\ldots \\
& -\mathbf{w} \cdot \mathbf{x}_{k}+\cdots+0 \cdot \mathbf{x}_{N}  \tag{4}\\
& =W^{D E} \cdot X
\end{align*}
$$

where $\mathbf{x}_{i}, \mathbf{x}_{j}$, and $\mathbf{x}_{k}$ are the three randomly selected parents. The weight vector $\mathbf{w}$ is a constant vector with the same value for all the variables. It is typically fixed during optimization and lies in the interval $[0.4,1]$ [90].

We interpret the results of LLM as a mapping from input individuals to new individuals in a similar way. The LLM operator can be approximately formulated as:

$$
\begin{equation*}
\mathbf{x}_{o}=W^{L L M} \cdot X \tag{5}
\end{equation*}
$$

where $W^{L L M}$ is the weight matrix that simulates the results of the LLM.

Therefore, we can explicitly formulate the behavior of LLM if we have the approximated matrix $W^{L L M}$. In order to approximate the matrix $W^{L L M}$, we employ a supervised learning approach. We assume that the weights for $d$ variables for each input individual are identical and simplify the original mapping $W^{L L M}$. Consequently, the approximating of matrix $W^{L L M}$ can be treated as a linear regression task to learn the $l$ weights for the $l$ input individuals.

## B. Data collection

We collected data from the input and output individuals during the interactions with LLM in the demonstration phase. Specifically, let $l$ represent the number of input individuals, with $\mathbf{x}_{1}, \ldots, \mathbf{x}_{l}$ as their corresponding decision variable values. Additionally, we have a single output individual represented by $\mathbf{x}_{o}$. Each input and output individual consists of $d$ variables, denoted as $\mathbf{x}_{i}=\left(x_{i, 1}, \ldots, x_{i, d}\right)^{T}$. Each variable dimension in the input-output pair of LLM is considered a sample-response pair for the regression task. Therefore, by invoking LLM, we acquire $d$ samples $\mathbf{s}_{1}, \ldots, \mathbf{s}_{d}$ with corresponding responses $r_{1}, \ldots, r_{d}$. Here, $\mathbf{s}_{i}=\left(x_{1, i}, \ldots, x_{l, i}\right)^{T}$ and $r_{i}=x_{o, i}$. In total, we collected 14,000 sample-response pairs.

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-06.jpg?height=366&width=700&top_left_y=188&top_left_x=1168)

Fig. 4. An illustration of weight vs. normalized rank in our weighted linear operator

## C. Approximate linear operator

We use a weighted linear operator with an additional randomness term to approximate the results of LLMs. We first perform linear regression without an interpreter on the data. Then, we approximate the weight $w_{i}$ for each input individual $\mathbf{x}_{i}$ as a polynomial function with respect to its rank $r_{i}$ among the inputs. Finally, we have the approximated weighted linear operator as follows:

$$
\begin{align*}
\mathbf{x}_{o} & =W^{L L M} \cdot X  \tag{6}\\
& =w_{1} \cdot \mathbf{x}_{1}+w_{2} \cdot \mathbf{x}_{2}+\cdots+w_{l} \cdot \mathbf{x}_{l}
\end{align*}
$$

where $X=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{l}\right\}$ are $l$ input individuals and $w_{1}, \ldots, w_{l}$ are the corresponding weights. Each weight $w_{i}$ is calculated as follows:

$$
\begin{equation*}
w_{i}=\operatorname{softmax}\left(a \cdot r_{i}^{3}+b \cdot r_{i}^{2}+c \cdot r_{i}+d\right) \tag{7}
\end{equation*}
$$

where $r_{i}=\operatorname{Rank}\left(\mathbf{x}_{i}\right) / l$ is the normalized rank of the input individual in terms of objective value, $a=-0.111, b=$ 1.037, $c=-1.291$, and $d=0.445$ are parameters obtained from polynomial regression of the learned weights with respect to the normalized rank. Fig. 4 plots the relationship (before softmax) between the weight $w_{i}$ and the normalized rank $r_{i}$.

We have some interesting observations from the results:

- The weights assigned to the top individuals are larger compared to the others. This observation aligns with common sense and experience, as it is expected for the new individuals to inherit more characteristics from the elite individuals.
- It is interesting that even the tail individuals, who have the worst objective function value, are still given a certain weight. One possible explanation for this is that the LLM avoids being too greedy to allow exploration.
- The remaining individuals are given small weights, which are not significant but still contribute to the generation of new individuals.

While the linear operator provides an explicit formulation to interpret the behavior of LLM, it only fits the average performance without considering the randomness. Randomness is an essential aspect of LLM's responses and also a crucial component in the evolutionary operator. To account for the randomness, we introduce a random term $w^{r}$ on each weight $w_{i}=w_{i}+\theta w^{r}$ to approximate the results of LLM, where $w^{r}$ is generated following the normal distribution $N(0,1)$,
and $\theta$ represents a fixed scale factor. The value $\theta=0.5$ is determined as the scaled standard deviation calculated by $\theta=\operatorname{std}\left(r_{\text {pred }}-r\right) /$ mean $(x)$, where $\operatorname{std}\left(r_{\text {pred }}-r\right)$ represents the standard deviations between the prediction results of the linear model $r_{\text {pred }}$ and the results of LLM $r$, while mean $(x)$ represents the mean of input samples.

The linear operator simultaneously assigns an equal weight to all dimensions of the variables for each input individual, which means all variable dimensions of the newly generated individuals are updated according to the same rule. It could lead to being overly greedy in high-dimensional MOPs. Therefore, in our experiments, we independently apply the linear operator to each variable dimension with a probability of $10 \%$. The dimensions that do not need to be updated copy the variable values of the current subproblem's solution directly.

## D. MOEA/D-LO

We developed an LLM-inspired version of MOEA/D, referred to as MOEA/D-LO, which incorporates the linear operator derived exclusively from the results of LLM. This new version preserves all the previous settings of the original MOEA/D framework [53] while replacing the crossover operator with our unique linear operator.

## VI. EXPERIMENTS

## A. Test instances

The experiments are carried out in the widely used ZDT [91] and UF [53] instances with varied problem features as well as diverse PF and PS shapes.

## B. Baseline algorithms

We conducted a comparison between MOEA/D-LO, MOEA/D, MOEA/D-DE, and NSGA-II. MOEA/D and MOEA/D-DE have precisely the same algorithmic framework as MOEA/D-LO, with the main distinction being their search operators. While MOEA/D employs a genetic algorithm (GA), MOEA/D-DE adopts differential evolution (DE). The implementation of MOEA/D-LO and all the compared algorithms are based on PlatEMO [92].

## C. Experimental settings

The settings for MOEA/D-based algorithms are as follows:

- The number of subproblems $N: 200$ for bi-objective instances and 300 for tri-objective instances;
- The number of weight vectors in the neighborhood $T$ : $N / 10$;
- The maximum number of evaluations $N_{\max }: 200,000$ for ZDTs and 300,000 for UFs;
- The probability of crossover/ $\mathrm{DE} /$ linear operator $\sigma_{1}: 1.0$;
- The probability of mutation $\sigma_{2}: 0.5$;
- The probability of mutation $\sigma_{2}: 0.9$;
- The probability of neighborhood selection $\sigma_{3}: 0.9$;
- The number of input individuals $l: 10$;
- The number of output individuals $s: 2$.

The additional settings not mentioned for NSGA-II are the same as in the original paper.

## D. Results

Table [II and Table III] list the average hypervolume (HV) indicator and inverted generational distance (IGD), accompanied by the standard deviation in parentheses, on different test instances. For each instance, we conducted 30 independent runs. The best value among all the algorithms for each instance is highlighted in red. Remarkably, our proposed MOEA/D-LO demonstrates highly promising performance when compared to commonly used MOEAs. Specifically, MOEA/D-LO yields superior average results on three instances and outperforms both NSGA-II and MOEA/D in terms of the sum rank test. These results are particularly surprising given that the linear operators are derived from the results of LLM without any human design and utilization of domain knowledge.

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-07.jpg?height=914&width=873&top_left_y=836&top_left_x=1081)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-07.jpg?height=380&width=421&top_left_y=848&top_left_x=1096)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-07.jpg?height=382&width=426&top_left_y=1311&top_left_x=1099)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-07.jpg?height=377&width=423&top_left_y=852&top_left_x=1512)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-07.jpg?height=371&width=423&top_left_y=1316&top_left_x=1512)

(d)
Fig. 5. HV values vs. number of evaluations on (a) ZDT1 , (b) ZDT6 , (c) UF1, and (d) UF6 test instances.

Fig. 5 depicts the convergence curves of HV with respect to the number of evaluations on four example test instances. On the two ZDT instances, all four algorithms converge to the optimal. Notably, MOEA/D-LO demonstrates faster convergence than the other algorithms on the ZDT instances, with its superiority being more evident on ZDT6. The results on UF instances have not yet reached complete convergence due to the complex nature of the test suite. Nevertheless, the results indicate that MOEA/D-LO remains highly competitive and robust across diverse instances. For example, In terms of convergence speed, NSGA-II outperforms MOEA/D-DE on the two ZDT instances but fails to converge within the given budget on the two UF instances. On the other hand, MOEA/DDE exhibits the best average performance on UF instances, while being significantly slower compared to other algorithms on ZDT instances. In contrast to both NSGA-II and MOEA/D$\mathrm{DE}$, our proposed algorithm, MOEA/D-LO, demonstrates robust performance across a variety of test instances. Although

TABLE II

A COMPARISON OF RESULTS ON ZDT AND UF INSTANCES IN TERMS OF HV.

| Problem | $m$ | $d$ | NSGA-II | MOEA/D | MOEA/D-DE | MOEA/D-LO |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ZDT1 | 2 | 30 | $7.2202 \mathrm{e}-1(6.38 \mathrm{e}-5)-$ | $7.2244 \mathrm{e}-1(8.12 \mathrm{e}-7)+$ | $7.2213 \mathrm{e}-1(9.06 \mathrm{e}-5)-$ | $7.2228 \mathrm{e}-1(2.18 \mathrm{e}-5)$ |
| ZDT2 | 2 | 30 | $4.4664 \mathrm{e}-1(7.58 \mathrm{e}-5)-$ | $4.4704 \mathrm{e}-1(2.82 \mathrm{e}-8)+$ | $4.4678 \mathrm{e}-1(6.35 \mathrm{e}-5)+$ | $4.4672 \mathrm{e}-1(5.20 \mathrm{e}-5)$ |
| ZDT3 | 2 | 30 | $6.0036 \mathrm{e}-1(2.83 \mathrm{e}-5)+$ | $5.9962 \mathrm{e}-1(1.41 \mathrm{e}-6)+$ | $5.9962 \mathrm{e}-1(2.63 \mathrm{e}-5)+$ | $5.9947 \mathrm{e}-1(1.67 \mathrm{e}-4)$ |
| ZDT4 | 2 | 30 | $7.1915 \mathrm{e}-1(1.20 \mathrm{e}-3)-$ | $7.1909 \mathrm{e}-1(1.27 \mathrm{e}-3)-$ | $3.0118 \mathrm{e}-1(2.70 \mathrm{e}-1)-$ | $7.2074 \mathrm{e}-1(2.81 \mathrm{e}-4)$ |
| ZDT6 | 2 | 30 | $3.8988 \mathrm{e}-1(5.85 \mathrm{e}-5)-$ | $3.9014 \mathrm{e}-1(5.57 \mathrm{e}-5)=$ | $3.8989 \mathrm{e}-1(1.31 \mathrm{e}-3)-$ | $3.9008 \mathrm{e}-1(1.87 \mathrm{e}-4)$ |
| UF1 | 2 | 30 | $6.0865 \mathrm{e}-1(2.41 \mathrm{e}-2)-$ | $5.5337 \mathrm{e}-1(5.37 \mathrm{e}-2)-$ | $7.2008 \mathrm{e}-1(2.93 \mathrm{e}-4)+$ | $6.5610 \mathrm{e}-1(2.55 \mathrm{e}-2)$ |
| UF2 | 2 | 30 | $6.9163 \mathrm{e}-1(7.76 \mathrm{e}-3)-$ | $6.8486 \mathrm{e}-1(1.57 \mathrm{e}-2)-$ | $7.1355 \mathrm{e}-1(1.76 \mathrm{e}-3)+$ | $7.0477 \mathrm{e}-1(3.78 \mathrm{e}-3)$ |
| UF3 | 2 | 30 | $4.6110 \mathrm{e}-1(4.74 \mathrm{e}-2)-$ | $3.7128 \mathrm{e}-1(3.74 \mathrm{e}-2)-$ | $7.1112 \mathrm{e}-1(1.02 \mathrm{e}-2)+$ | $4.9752 \mathrm{e}-1(8.19 \mathrm{e}-2)$ |
| UF4 | 2 | 30 | $3.9117 \mathrm{e}-1(7.96 \mathrm{e}-4)-$ | $3.6494 \mathrm{e}-1(6.09 \mathrm{e}-3)-$ | $3.5691 \mathrm{e}-1(7.69 \mathrm{e}-3)-$ | $4.0230 \mathrm{e}-1(8.42 \mathrm{e}-4)$ |
| UF5 | 2 | 30 | $2.6471 \mathrm{e}-1(5.30 \mathrm{e}-2)=$ | $1.5703 \mathrm{e}-1(1.00 \mathrm{e}-1)-$ | $2.0729 \mathrm{e}-1(8.69 \mathrm{e}-2)-$ | $2.6900 \mathrm{e}-1(5.33 \mathrm{e}-2)$ |
| UF6 | 2 | 30 | $3.4914 \mathrm{e}-1(4.97 \mathrm{e}-2)+$ | $1.8954 \mathrm{e}-1(8.08 \mathrm{e}-2)-$ | $3.1320 \mathrm{e}-1(7.89 \mathrm{e}-2)=$ | $3.0889 \mathrm{e}-1(3.77 \mathrm{e}-2)$ |
| UF7 | 2 | 30 | $4.9799 \mathrm{e}-1(8.13 \mathrm{e}-2)-$ | $2.8548 \mathrm{e}-1(1.39 \mathrm{e}-1)-$ | $5.7849 \mathrm{e}-1(6.91 \mathrm{e}-3)+$ | $5.5062 \mathrm{e}-1(4.80 \mathrm{e}-2)$ |
| UF8 | 3 | 30 | $3.1429 \mathrm{e}-1(4.99 \mathrm{e}-2)-$ | $3.8911 \mathrm{e}-1(5.84 \mathrm{e}-2)=$ | $4.6814 \mathrm{e}-1(1.29 \mathrm{e}-2)+$ | $4.2039 \mathrm{e}-1(3.38 \mathrm{e}-2)$ |
| UF9 | 3 | 30 | $5.9876 \mathrm{e}-1(7.90 \mathrm{e}-2)-$ | $5.9321 \mathrm{e}-1(1.95 \mathrm{e}-2)-$ | $6.7511 \mathrm{e}-1(5.35 \mathrm{e}-2)=$ | $6.7386 \mathrm{e}-1(4.40 \mathrm{e}-2)$ |
|  |  |  |  |  |  |  |

TABLE III

A COMPARISON OF RESULTS ON ZDT AND UF INSTANCES IN TERMS OF IGD.

| Problem | $m$ | $d$ | NSGA-II | MOEA/D | MOEA/D-DE | MOEA/D-LO |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ZDT1 | 2 | 30 | $2.3327 \mathrm{e}-3(6.52 \mathrm{e}-5)-$ | $1.9349 \mathrm{e}-3(2.20 \mathrm{e}-7)+$ | $1.9804 \mathrm{e}-3(2.49 \mathrm{e}-5)=$ | $1.9759 \mathrm{e}-3(8.59 \mathrm{e}-6)$ |
| ZDT2 | 2 | 30 | $2.3698 \mathrm{e}-3(8.75 \mathrm{e}-5)-$ | $1.8940 \mathrm{e}-3(6.42 \mathrm{e}-9)+$ | $1.9145 \mathrm{e}-3(7.79 \mathrm{e}-6)+$ | $1.9608 \mathrm{e}-3(1.32 \mathrm{e}-5)$ |
| ZDT3 | 2 | 30 | $2.6305 \mathrm{e}-3(7.63 \mathrm{e}-5)+$ | $5.3043 \mathrm{e}-3(3.07 \mathrm{e}-6)+$ | $5.2839 \mathrm{e}-3(7.66 \mathrm{e}-6)+$ | $5.5065 \mathrm{e}-3(9.12 \mathrm{e}-5)$ |
| ZDT4 | 2 | 30 | $3.6258 \mathrm{e}-3(7.54 \mathrm{e}-4)-$ | $3.5878 \mathrm{e}-3(8.34 \mathrm{e}-4)-$ | $5.4104 \mathrm{e}-1(4.85 \mathrm{e}-1)-$ | $2.9074 \mathrm{e}-3(2.17 \mathrm{e}-4)$ |
| ZDT6 | 2 | 30 | $1.8556 \mathrm{e}-3(4.42 \mathrm{e}-5)-$ | $1.5711 \mathrm{e}-3(9.46 \mathrm{e}-6)+$ | $1.8032 \mathrm{e}-3(7.15 \mathrm{e}-4)-$ | $1.6982 \mathrm{e}-3(3.97 \mathrm{e}-5)$ |
| UF1 | 2 | 30 | $9.3263 \mathrm{e}-2(1.78 \mathrm{e}-2)-$ | $1.3162 \mathrm{e}-1(5.25 \mathrm{e}-2)-$ | $2.6737 \mathrm{e}-3(1.09 \mathrm{e}-4)+$ | $4.2243 \mathrm{e}-2(1.19 \mathrm{e}-2)$ |
| UF2 | 2 | 30 | $3.0428 \mathrm{e}-2(1.20 \mathrm{e}-2)-$ | $4.6422 \mathrm{e}-2(2.49 \mathrm{e}-2)-$ | $8.1665 \mathrm{e}-3(1.52 \mathrm{e}-3)+$ | $1.4512 \mathrm{e}-2(2.76 \mathrm{e}-3)$ |
| UF3 | 2 | 30 | $2.1710 \mathrm{e}-1(5.71 \mathrm{e}-2)-$ | $2.9571 \mathrm{e}-1(3.14 \mathrm{e}-2)-$ | $8.0725 \mathrm{e}-3(6.34 \mathrm{e}-3)+$ | $1.6236 \mathrm{e}-1(5.66 \mathrm{e}-2)$ |
| UF4 | 2 | 30 | $4.1599 \mathrm{e}-2(3.94 \mathrm{e}-4)-$ | $5.6899 \mathrm{e}-2(4.67 \mathrm{e}-3)-$ | $6.4838 \mathrm{e}-2(6.20 \mathrm{e}-3)-$ | $3.2483 \mathrm{e}-2(1.25 \mathrm{e}-3)$ |
| UF5 | 2 | 30 | $2.4654 \mathrm{e}-1(4.81 \mathrm{e}-2)-$ | $4.7837 \mathrm{e}-1(1.58 \mathrm{e}-1)-$ | $2.7242 \mathrm{e}-1(7.31 \mathrm{e}-2)-$ | $2.0304 \mathrm{e}-1(2.41 \mathrm{e}-2)$ |
| UF6 | 2 | 30 | $1.3390 \mathrm{e}-1(5.41 \mathrm{e}-2)+$ | $4.3358 \mathrm{e}-1(1.37 \mathrm{e}-1)-$ | $1.8286 \mathrm{e}-1(1.39 \mathrm{e}-1)=$ | $1.5296 \mathrm{e}-1(3.50 \mathrm{e}-2)$ |
| UF7 | 2 | 30 | $9.0243 \mathrm{e}-2(1.12 \mathrm{e}-1)-$ | $3.8444 \mathrm{e}-1(1.98 \mathrm{e}-1)-$ | $5.3580 \mathrm{e}-3(4.91 \mathrm{e}-3)+$ | $2.9594 \mathrm{e}-2(6.17 \mathrm{e}-2)$ |
| UF8 | 3 | 30 | $2.5419 \mathrm{e}-1(6.25 \mathrm{e}-2)-$ | $2.0279 \mathrm{e}-1(7.28 \mathrm{e}-2)-$ | $9.1388 \mathrm{e}-2(1.82 \mathrm{e}-2)=$ | $1.0798 \mathrm{e}-1(4.66 \mathrm{e}-2)$ |
| UF9 | 3 | 30 | $1.9634 \mathrm{e}-1(8.26 \mathrm{e}-2)-$ | $2.2323 \mathrm{e}-1(3.06 \mathrm{e}-2)-$ | $1.4166 \mathrm{e}-1(6.86 \mathrm{e}-2)=$ | $1.1290 \mathrm{e}-1(5.16 \mathrm{e}-2)$ |
|  |  |  |  |  |  |  |

initially NSGA-II and MOEA/D were faster than MOEA/DLO on the UF instance, our algorithm consistently converged and ultimately only had a slightly lower performance than MOEA/D-DE. Fig. 6 to Fig. 10 plot the approximated PFs (best among 30 independent runs) obtained by four algorithms.

## E. Ablation Study

The proposed MOEA/D-LO generates promising results in the experiments. However, it uses ten input individuals in its linear operators, whereas conventional evolutionary operators, such as GA and DE typically employ fewer parents. An ablation study is required to directly verify whether the weight vector obtained from LLM contributes to the performance.

To conduct the ablation study, we introduced three additional weight settings for the weighted linear operators: random vector $W^{R}$, equal vector $W^{E}$, and linear vector $W^{L}$. The input size is set to be $l=10$. The random vector consists of randomly sampled values from the range $[0,1]$, which are then scaled to ensure that the sum equals 1 . The equal vector is simply a vector with all elements set to 0.1 , while the linear vector has elements ranging from 0.19 to 0.01 with a constant difference of 0.02 . Fig. 11 (a) visualizes the four weight vectors.
In addition to comparing the different weight settings, we also investigated the impact of the number of input individuals. We conducted tests using input sizes of $\{10,20,30,40\}$, while maintaining all other experimental settings fixed. By replacing the linear operator in our original MOEA/D-LO algorithm with these new operators, we obtained seven versions of MOEA/DLO, i.e., Random, Equal, Linear, LO40, LO30, LO20, LO10. We evaluated the performance of these seven algorithms using the same experimental settings on UF instances.

Fig. 11 (b) compares the average IGD values on UF instances and Table IV lists the detailed results. For each instance, we conducted 30 independent runs. The results of our experiments confirm that the linear operator with randomness derived from the LLM results yielded superior performance compared to the other simple weight settings. The input size of the linear operator influences the performance of MOEA/DLO and the algorithm with an input size of 10 outperforms others.

## VII. FUTURE WORKS

Our work reveals the potential of using LLMs for MOEA. There are many issues that remain to be explored in future works:

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=338&width=433&top_left_y=243&top_left_x=152)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=341&width=420&top_left_y=239&top_left_x=619)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=339&width=423&top_left_y=240&top_left_x=1076)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=341&width=423&top_left_y=239&top_left_x=1534)

(d)

Fig. 6. Approximated PFs on ZDT1 instance: (a) NSGA-II, (b) MOEA/D, (c) MOEA/D-DE, and (d) MOEA/D-LO.

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=414&width=1825&top_left_y=720&top_left_x=150)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=344&width=422&top_left_y=733&top_left_x=163)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=325&width=423&top_left_y=754&top_left_x=621)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=325&width=420&top_left_y=754&top_left_x=1080)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=331&width=427&top_left_y=751&top_left_x=1532)

(d)

Fig. 7. Approximated PFs on ZDT3 instance: (a) NSGA-II, (b) MOEA/D, (c) MOEA/D-DE, and (d) MOEA/D-LO.

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=393&width=1825&top_left_y=1191&top_left_x=150)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=325&width=420&top_left_y=1209&top_left_x=164)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=325&width=420&top_left_y=1209&top_left_x=625)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=336&width=420&top_left_y=1198&top_left_x=1080)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=338&width=426&top_left_y=1197&top_left_x=1530)

(d)

Fig. 8. Approximated PFs on UF1 instance: (a) NSGA-II, (b) MOEA/D, (c) MOEA/D-DE, and (d) MOEA/D-LO

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=339&width=437&top_left_y=1652&top_left_x=153)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=341&width=423&top_left_y=1651&top_left_x=621)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=339&width=425&top_left_y=1649&top_left_x=1075)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=350&width=440&top_left_y=1644&top_left_x=1531)

(d)

Fig. 9. Approximated PFs on UF4 instance: (a) NSGA-II, (b) MOEA/D, (c) MOEA/D-DE, and (d) MOEA/D-LO.

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=312&width=458&top_left_y=2121&top_left_x=148)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=309&width=426&top_left_y=2122&top_left_x=622)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=306&width=426&top_left_y=2124&top_left_x=1077)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-09.jpg?height=312&width=431&top_left_y=2121&top_left_x=1530)

(d)

Fig. 10. Approximated PFs on UF9 instance: (a) NSGA-II, (b) MOEA/D, (c) MOEA/D-DE, and (d) MOEA/D-LO.

TABLE IV

A COMPARISON OF IGD VALUES ON UF INSTANCES USING DIFFERENT WEIGHT SETTINGS FOR MOEA/D-LO.

| Problem | Random | Equal | Linear | LO40 | LO30 |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| UF1 | $9.8521 \mathrm{e}-2(5.51 \mathrm{e}-2)-$ | $8.2461 \mathrm{e}-2(3.72 \mathrm{e}-2)-$ | $9.3215 \mathrm{e}-2(4.50 \mathrm{e}-2)-$ | $5.0552 \mathrm{e}-2(1.61 \mathrm{e}-2)=$ | $4.5738 \mathrm{e}-2(1.19 \mathrm{e}-2)=$ | $4.2449 \mathrm{e}-2(1.51 \mathrm{e}-2)=$ | $4.2243 \mathrm{e}-2(1.19 \mathrm{e}-2)$ |
| UF2 | $5.5050 \mathrm{e}-2(3.70 \mathrm{e}-2)-$ | $4.8156 \mathrm{e}-2(3.58 \mathrm{e}-2)-$ | $3.3484 \mathrm{e}-2(2.81 \mathrm{e}-2)-$ | $2.1425 \mathrm{e}-2(2.43 \mathrm{e}-3)-$ | $2.0581 \mathrm{e}-2(3.24 \mathrm{e}-3)-$ | $1.7932 \mathrm{e}-2(3.48 \mathrm{e}-3)-$ | $1.4512 \mathrm{e}-2(2.76 \mathrm{e}-3)$ |
| UF3 | $2.8692 \mathrm{e}-1(2.73 \mathrm{e}-2)-$ | $2.9368 \mathrm{e}-1(2.83 \mathrm{e}-2)-$ | $2.9557 \mathrm{e}-1(1.78 \mathrm{e}-2)-$ | $2.3614 \mathrm{e}-1(7.26 \mathrm{e}-2)-$ | $1.9032 \mathrm{e}-1(2.77 \mathrm{e}-2)-$ | $1.7224 \mathrm{e}-1(2.31 \mathrm{e}-2)-$ | $1.6236 \mathrm{e}-1(5.66 \mathrm{e}-2)$ |
| UF4 | $4.9151 \mathrm{e}-2(3.70 \mathrm{e}-3)-$ | $4.9624 \mathrm{e}-2(3.25 \mathrm{e}-3)-$ | $4.8909 \mathrm{e}-2(3.48 \mathrm{e}-3)-$ | $3.5266 \mathrm{e}-2(1.08 \mathrm{e}-3)-$ | $3.5286 \mathrm{e}-2(8.67 \mathrm{e}-4)-$ | $3.3625 \mathrm{e}-2(1.67 \mathrm{e}-3)-$ | $3.2483 \mathrm{e}-2(1.25 \mathrm{e}-3)$ |
| UF5 | $3.5277 \mathrm{e}-1(1.35 \mathrm{e}-1)-$ | $3.8832 \mathrm{e}-1(1.47 \mathrm{e}-1)-$ | $3.7798 \mathrm{e}-1(1.43 \mathrm{e}-1)-$ | $2.2512 \mathrm{e}-1(2.56 \mathrm{e}-2)-$ | $2.2542 \mathrm{e}-1(2.86 \mathrm{e}-2)-$ | $2.1030 \mathrm{e}-1(3.43 \mathrm{e}-2)=$ | $2.0304 \mathrm{e}-1(2.41 \mathrm{e}-2)$ |
| UF6 | $2.6984 \mathrm{e}-1(1.64 \mathrm{e}-1)=$ | $3.3243 \mathrm{e}-1(1.47 \mathrm{e}-1)-$ | $2.8682 \mathrm{e}-1(1.40 \mathrm{e}-1)=$ | $2.0642 \mathrm{e}-1(2.90 \mathrm{e}-2)-$ | $1.9705 \mathrm{e}-1(3.18 \mathrm{e}-2)-$ | $1.7651 \mathrm{e}-1(2.91 \mathrm{e}-2)-$ | $1.5296 \mathrm{e}-1(3.50 \mathrm{e}-2)$ |
| UF7 | $4.0080 \mathrm{e}-1(1.31 \mathrm{e}-1)-$ | $2.8276 \mathrm{e}-1(1.70 \mathrm{e}-1)-$ | $2.9912 \mathrm{e}-1(1.54 \mathrm{e}-1)-$ | $2.2805 \mathrm{e}-2(4.68 \mathrm{e}-3)+$ | $4.3914 \mathrm{e}-2(7.32 \mathrm{e}-2)-$ | $3.0426 \mathrm{e}-2(5.17 \mathrm{e}-2)-$ | $2.9594 \mathrm{e}-2(6.17 \mathrm{e}-2)$ |
| UF8 | $2.5911 \mathrm{e}-1(1.08 \mathrm{e}-1)-$ | $2.5255 \mathrm{e}-1(1.03 \mathrm{e}-1)-$ | $2.1790 \mathrm{e}-1(9.82 \mathrm{e}-2)-$ | $1.2847 \mathrm{e}-1(2.74 \mathrm{e}-2)-$ | $1.3920 \mathrm{e}-1(4.86 \mathrm{e}-2)-$ | $1.3184 \mathrm{e}-1(4.65 \mathrm{e}-2)-$ | $1.0798 \mathrm{e}-1(4.66 \mathrm{e}-2)$ |
| UF9 | $2.2823 \mathrm{e}-1(6.51 \mathrm{e}-2)-$ | $2.3450 \mathrm{e}-1(5.65 \mathrm{e}-2)-$ | $2.2137 \mathrm{e}-1(7.20 \mathrm{e}-2)-$ | $1.3954 \mathrm{e}-1(1.38 \mathrm{e}-2)-$ | $1.2036 \mathrm{e}-1(7.20 \mathrm{e}-3)-$ | $1.1244 \mathrm{e}-1(1.96 \mathrm{e}-2)+$ | $1.1290 \mathrm{e}-1(5.16 \mathrm{e}-2)$ |
| $+/-/=$ | $0 / 8 / 1$ | $0 / 9 / 0$ | $0 / 8 / 1$ | $1 / 7 / 1$ | $0 / 8 / 1$ | $1 / 2$ |  |

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-10.jpg?height=534&width=728&top_left_y=779&top_left_x=210)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_fe6a588bba2058fefa89g-10.jpg?height=458&width=745&top_left_y=1454&top_left_x=191)

(b)

Fig. 11. (a) Visualization of four weight settings on ten input individuals, and (b) Comparison of average IGD on UF instances. Random: vector with random weight generated in the range $[0,1]$ and normalized to sum to one. Equal: vector with equal weights for all input individuals. Linear: vector with equal difference weights for input individuals. $\mathbf{L O}(\mathbf{L L M})$ : our proposed operator obtained from LLM with different input sizes.

- Advanced prompt engineering methods: For example, we can employ self-consistency sampling to generate multiple individuals and select the most promising individual or a combination of individuals [93]. Additionally, we can also let LLMs iteratively suggest a set of new individuals through a self-ask way [94].
- Guiding LLMs with additional information: Another interesting direction is to provide additional information to guide the LLM during the optimization process. This information can be in the form of history search trajectories, external archives, and rewards obtained during optimization [95], [96]. By incorporating this information, the LLM can better understand the underlying problem structure and generate more informed suggestions for the next generation.
- Handling complex MOPs: LLMs may be more compelling in tackling complex MOPs, which often prove difficult for humans to comprehend and analyze. For example, exploring how to handle constraints within the MOEA framework can lead to more practical solutions [97]. LLMs also offer possibilities in developing efficient methods for high-dimensional MOPs [98], [99]. Furthermore, LLMs will be helpful in automatically adjusting search strategy in dynamic environments [95].
- Exploring LLMs with different MOEA approaches: In addition to using LLMs with the current decompositionbased framework, it would be worthwhile to investigate the integration of LLMs with other types of MOEAs, such as Pareto-based and indicator-based methods [100]. When applying LLMs to these algorithms, helping LLMs understand the task and suggest better individuals in pure multi-objective settings is significant and challenging.


## VIII. CONCLUSION

This paper explored the application of large language models in multiobjective evolutionary optimization. By leveraging the power of pre-trained LLMs without the need to design or train a model from scratch, we proposed a novel approach where LLMs are used as black-box search operators in a decomposition-based MOEA framework. Additionally, we designed an explicable white-box operator with randomness to interpret the results of LLMs and introduced a new version of MOEA/D, termed MOEA/D-LO. Our experimental studies demonstrate the effectiveness of our proposed approach. MOEA/D-LO achieves competitive performance compared to widely used MOEAs and ranks first in some test instances. In addition, our ablation study confirms the superiority of the LLM operator over other weight settings.

To the best of our knowledge, this is the first attempt to apply LLMs in the context of multiobjective evolutionary optimization. Our research findings demonstrate the potential benefits of incorporating pre-trained LLMs in the design of MOEAs.

## REFERENCES

[1] A. Zhou, B.-Y. Qu, H. Li, S.-Z. Zhao, P. N. Suganthan, and Q. Zhang, "Multiobjective evolutionary algorithms: A survey of the state of the art," Swarm and evolutionary computation, vol. 1, no. 1, pp. 32-49, 2011.

[2] A. Trivedi, D. Srinivasan, K. Sanyal, and A. Ghosh, "A survey of multiobjective evolutionary algorithms based on decomposition," IEEE Transactions on Evolutionary Computation, vol. 21, no. 3, pp. 440-462, 2016.

[3] M. T. Emmerich and A. H. Deutz, "A tutorial on multiobjective optimization: fundamentals and evolutionary methods," Natural computing, vol. 17, pp. 585-609, 2018.

[4] J. G. Falcn-Cardona and C. A. C. Coello, "Indicator-based multiobjective evolutionary algorithms: A comprehensive survey," ACM Computing Surveys (CSUR), vol. 53, no. 2, pp. 1-35, 2020.

[5] H. Wang and Y. Jin, "A random forest-assisted evolutionary algorithm for data-driven constrained multiobjective combinatorial optimization of trauma systems," IEEE transactions on cybernetics, vol. 50, no. 2, pp. 536-549, 2018.

[6] Y. Jin, H. Wang, and C. Sun, Data-driven evolutionary optimization. Springer, 2021.

[7] Z. Wang, H.-L. Zhen, J. Deng, Q. Zhang, X. Li, M. Yuan, and J. Zeng, "Multiobjective optimization-aided decision-making system for largescale manufacturing planning," IEEE Transactions on Cybernetics, vol. 52, no. 8, pp. 8326-8339, 2021.

[8] A. Ponsich, A. L. Jaimes, and C. A. C. Coello, "A survey on multiobjective evolutionary algorithms for the solution of the portfolio optimization problem and other finance and economics applications," IEEE transactions on evolutionary computation, vol. 17, no. 3 , pp. 321-344, 2012.

[9] A. Mukhopadhyay, U. Maulik, S. Bandyopadhyay, and C. A. C. Coello, "Survey of multiobjective evolutionary algorithms for data mining: Part ii," IEEE Transactions on Evolutionary Computation, vol. 18, no. 1, pp. 20-35, 2013.

[10] Y. Xue, X. Cai, and F. Neri, "A multi-objective evolutionary algorithm with interval based initialization and self-adaptive crossover operator for large-scale feature selection in classification," Applied Soft Computing, vol. 127, p. 109420, 2022.

[11] R. Espinosa, F. Jimnez, and J. Palma, "Multi-surrogate assisted multiobjective evolutionary algorithms for feature selection in regression and classification problems with time series data," Information Sciences, vol. 622, pp. 1064-1091, 2023.

[12] X. Liu, J. Sun, Q. Zhang, Z. Wang, and Z. Xu, "Learning to learn evolutionary algorithm: A learnable differential evolution," IEEE Transactions on Emerging Topics in Computational Intelligence, 2023.

[13] X. Li, K. Wu, X. Zhang, H. Wang, and J. Liu, "Optformer: Beyond transformer for black-box optimization," 2022.

[14] L. Penghui, K. Wu, and J. Liu, "Decn: Evolution inspired deep convolution network for black-box optimization," 2022.

[15] R. Lange, T. Schaul, Y. Chen, T. Zahavy, V. Dalibard, C. Lu, S. Singh, and S. Flennerhag, "Discovering evolution strategies via meta-blackbox optimization," in Proceedings of the Companion Conference on Genetic and Evolutionary Computation, 2023, pp. 29-30.

[16] X. Ma, F. Liu, Y. Qi, M. Gong, M. Yin, L. Li, L. Jiao, and J. Wu, "MOEA/D with opposition-based learning for multiobjective optimization problem," Neurocomputing, vol. 146, pp. 48-64, 2014.

[17] K. Lwin, R. Qu, and G. Kendall, "A learning-guided multi-objective evolutionary algorithm for constrained portfolio optimization," Applied Soft Computing, vol. 24, pp. 757-772, 2014.

[18] M. Wu, K. Li, S. Kwong, Q. Zhang, and J. Zhang, "Learning to decompose: A paradigm for decomposition-based multiobjective optimization," IEEE Transactions on Evolutionary Computation, vol. 23, no. 3, pp. 376-390, 2018.

[19] J. Sun, H. Zhang, A. Zhou, Q. Zhang, and K. Zhang, "A new learningbased adaptive multi-objective evolutionary algorithm," Swarm and evolutionary computation, vol. 44, pp. 304-319, 2019.

[20] F. Wang, F. Liao, Y. Li, X. Yan, and X. Chen, "An ensemble learning based multi-objective evolutionary algorithm for the dynamic vehicle routing problem with time windows," Computers \& Industrial Engineering, vol. 154, p. 107131, 2021.

[21] R. Qi, J.-q. Li, J. Wang, H. Jin, and Y.-y. Han, "Qmoea: A q-learningbased multiobjective evolutionary algorithm for solving time-dependent green vehicle routing problems with time windows," Information Sciences, vol. 608, pp. 178-201, 2022.
[22] Y. Tian, X. Li, H. Ma, X. Zhang, K. C. Tan, and Y. Jin, "Deep reinforcement learning based adaptive operator selection for evolutionary multi-objective optimization," IEEE Transactions on Emerging Topics in Computational Intelligence, 2022.

[23] K. Li, T. Zhang, and R. Wang, "Deep reinforcement learning for multiobjective optimization," IEEE transactions on cybernetics, vol. 51, no. 6, pp. 3103-3114, 2020

[24] Y. Shao, J. C.-W. Lin, G. Srivastava, D. Guo, H. Zhang, H. Yi, and A. Jolfaei, "Multi-objective neural evolutionary algorithm for combinatorial optimization problems," IEEE transactions on neural networks and learning systems, 2021

[25] X. Lin, Z. Yang, and Q. Zhang, "Pareto set learning for neural multiobjective combinatorial optimization," 2022.

[26] Z. Zhang, Z. Wu, H. Zhang, and J. Wang, "Meta-learning-based deep reinforcement learning for multiobjective optimization problems," IEEE Transactions on Neural Networks and Learning Systems, 2022.

[27] K. Sanderson, "Gpt-4 is here: what scientists think," Nature, vol. 615, no. 7954, p. 773, 2023.

[28] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth, "Recent advances in natural language processing via large pre-trained language models: A survey," ACM Computing Surveys, 2021.

[29] H. Tian, W. Lu, T. O. Li, X. Tang, S.-C. Cheung, J. Klein, and T. F. Bissyand, "Is chatgpt the ultimate programming assistant-how far is it?" arXiv preprint arXiv:2304.11938, 2023.

[30] P. Lee, S. Bubeck, and J. Petro, "Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine," New England Journal of Medicine, vol. 388, no. 13, pp. 1233-1239, 2023.

[31] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, "Capabilities of gpt-4 on medical challenge problems," arXiv preprint arXiv:2303.13375, 2023

[32] K. Cheng, Q. Guo, Y. He, Y. Lu, S. Gu, and H. Wu, "Exploring the potential of gpt-4 in biomedical engineering: the dawn of a new era," Annals of Biomedical Engineering, pp. 1-9, 2023.

[33] K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero, and B. Smit, "Is gpt-3 all you need for low-data discovery in chemistry?" 2023.

[34] J. Blocklove, S. Garg, R. Karri, and H. Pearce, "Chip-chat: Challenges and opportunities in conversational hardware design," arXiv preprint arXiv:2305.13243, 2023.

[35] Z. He, H. Wu, X. Zhang, X. Yao, S. Zheng, H. Zheng, and B. Yu, "Chateda: A large language model powered autonomous agent for eda," arXiv preprint arXiv:2308.10204, 2023

[36] C. Yu, X. Liu, C. Tang, W. Feng, and J. Lv, "Gpt-nas: Neural architecture search with the generative pre-trained model," arXiv preprint arXiv:2305.05351, 2023.

[37] M. Zheng, X. Su, S. You, F. Wang, C. Qian, C. Xu, and S. Albanie, "Can gpt-4 perform neural architecture search?" arXiv preprint arXiv:2304.10970, 2023.

[38] S. Zhang, C. Gong, L. Wu, X. Liu, and M. Zhou, "Automl-gpt: Automatic machine learning with gpt," arXiv preprint arXiv:2305.02499, 2023 .

[39] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., "A survey of large language models," arXiv preprint arXiv:2303.18223, 2023.

[40] E. Kasneci, K. Seler, S. Kchemann, M. Bannert, D. Dementieva, F. Fischer, U. Gasser, G. Groh, S. Gnnemann, E. Hllermeier et al., "Chatgpt for good? on opportunities and challenges of large language models for education," Learning and individual differences, vol. 103 , p. 102274,2023

[41] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen, "Large language models as optimizers," arXiv preprint arXiv:2309.03409, 2023

[42] E. Meyerson, M. J. Nelson, H. Bradley, A. Moradi, A. K. Hoover, and J. Lehman, "Language model crossover: Variation through few-shot prompting," arXiv preprint arXiv:2302.12170, 2023.

[43] A. Chen, D. M. Dohan, and D. R. So, "Evoprompting: Language models for code-level neural architecture search," arXiv preprint arXiv:2302.14838, 2023

[44] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, "A fast and elitist multiobjective genetic algorithm: NSGA-II," IEEE transactions on evolutionary computation, vol. 6, no. 2, pp. 182-197, 2002.

[45] E. Zitzler, M. Laumanns, and L. Thiele, "Spea2: Improving the strength pareto evolutionary algorithm," TIK report, vol. 103, 2001.

[46] E. Zitzler and S. Knzli, "Indicator-based selection in multiobjective search," in International conference on parallel problem solving from nature. Springer, 2004, pp. 832-842.

[47] M. Emmerich, N. Beume, and B. Naujoks, "An emo algorithm using the hypervolume measure as selection criterion," in International Conference on Evolutionary Multi-Criterion Optimization. Springer, 2005, pp. 62-76.

[48] J. Bader and E. Zitzler, "Hype: An algorithm for fast hypervolumebased many-objective optimization," Evolutionary computation, vol. 19, no. 1, pp. 45-76, 2011.

[49] Q. Zhang and H. Li, "MOEA/D: A multiobjective evolutionary algorithm based on decomposition," IEEE Transactions on evolutionary computation, vol. 11, no. 6, pp. 712-731, 2007.

[50] K. Deb and H. Jain, "An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: solving problems with box constraints," IEEE transactions on evolutionary computation, vol. 18, no. 4, pp. 577-601, 2013.

[51] L. Ke, Q. Zhang, and R. Battiti, "MOEA/D-ACO: A multiobjective evolutionary algorithm using decomposition and antcolony," IEEE transactions on cybernetics, vol. 43, no. 6, pp. 1845-1859, 2013.

[52] R. Tanabe and H. Ishibuchi, "Review and analysis of three components of the differential evolution mutation operator in MOEA/D-DE," Soft Computing, vol. 23, no. 23, pp. 12 843-12857, 2019.

[53] H. Li and Q. Zhang, "Multiobjective optimization problems with complicated pareto sets, MOEA/D and NSGA-II," IEEE transactions on evolutionary computation, vol. 13, no. 2, pp. 284-302, 2008

[54] H. Li, K. Deb, Q. Zhang, P. N. Suganthan, and L. Chen, "Comparison between MOEA/D and NSGA-III on a set of novel many and multiobjective benchmark problems with challenging difficulties," Swarm and Evolutionary Computation, vol. 46, pp. 104-117, 2019.

[55] Z. Cui, Y. Chang, J. Zhang, X. Cai, and W. Zhang, "Improved NSGA-III with selection-and-elimination operator," Swarm and Evolutionary Computation, vol. 49, pp. 23-33, 2019.

[56] K. Li, A. Fialho, S. Kwong, and Q. Zhang, "Adaptive operator selection with bandits for a multiobjective evolutionary algorithm based on decomposition," IEEE Transactions on Evolutionary Computation, vol. 18, no. 1, pp. 114-130, 2013.

[57] X. Qiu, J.-X. Xu, K. C. Tan, and H. A. Abbass, "Adaptive crossgeneration differential evolution operators for multiobjective optimization," IEEE Transactions on Evolutionary Computation, vol. 20, no. 2, pp. 232-244, 2015.

[58] A. Lin, P. Yu, S. Cheng, and L. Xing, "One-to-one ensemble mechanism for decomposition-based multi-objective optimization," Swarm and Evolutionary Computation, vol. 68, p. 101007, 2022.

[59] Q. Zhu, Q. Lin, Z. Du, Z. Liang, W. Wang, Z. Zhu, J. Chen, P. Huang, and Z. Ming, "A novel adaptive hybrid crossover operator for multiobjective evolutionary algorithm," Information Sciences, vol. 345, pp. 177-198, 2016.

[60] V. Palakonda and J.-M. Kang, "Pre-demo: Preference-inspired differential evolution for multi/many-objective optimization," IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2023.

[61] I. Kropp, A. P. Nejadhashemi, and K. Deb, "Improved evolutionary operators for sparse large-scale multiobjective optimization problems," IEEE Transactions on Evolutionary Computation, 2023.

[62] Q. Zhang, W. Liu, E. Tsang, and B. Virginas, "Expensive multiobjective optimization by MOEA/D with gaussian process model," IEEE Transactions on Evolutionary Computation, vol. 14, no. 3, pp. 456474, 2009 .

[63] Y. Jin, H. Wang, T. Chugh, D. Guo, and K. Miettinen, "Datadriven evolutionary optimization: An overview and case studies," IEEE Transactions on Evolutionary Computation, vol. 23, no. 3, pp. 442-458, 2018 .

[64] Z. Song, H. Wang, C. He, and Y. Jin, "A kriging-assisted two-archive evolutionary algorithm for expensive many-objective optimization," IEEE Transactions on Evolutionary Computation, vol. 25, no. 6, pp. 1013-1027, 2021.

[65] L. Ma, N. Li, Y. Guo, X. Wang, S. Yang, M. Huang, and H. Zhang, "Learning to optimize: reference vector reinforcement learning adaption to constrained many-objective optimization of industrial copper burdening system," IEEE Transactions on Cybernetics, 2021

[66] Z. Zhang, Q. Tang, M. Chica, and Z. Li, "Reinforcement learning-based multiobjective evolutionary algorithm for mixed-model multimanned assembly line balancing under uncertain demand," IEEE Transactions on Cybernetics, 2023.

[67] W. Liu, R. Wang, T. Zhang, K. Li, W. Li, and H. Ishibuchi, "Hybridization of evolutionary algorithm and deep reinforcement learning for multi-objective orienteering optimization," IEEE Transactions on Evolutionary Computation, 2022.
[68] F.-Y. Liu and C. Qian, "Prediction guided meta-learning for multiobjective reinforcement learning," in 2021 IEEE Congress on Evolutionary Computation (CEC). IEEE, 2021, pp. 2171-2178.

[69] M. Jiang, Z. Wang, H. Hong, and G. G. Yen, "Knee point-based imbalanced transfer learning for dynamic multiobjective optimization," IEEE Transactions on Evolutionary Computation, vol. 25, no. 1, pp. $117-129,2020$.

[70] K. C. Tan, L. Feng, and M. Jiang, "Evolutionary transfer optimizationa new frontier in evolutionary computation research," IEEE Computational Intelligence Magazine, vol. 16, no. 1, pp. 22-33, 2021.

[71] Y. Ye, Q. Lin, L. Ma, K.-C. Wong, M. Gong, and C. A. C. Coello, "Multiple source transfer learning for dynamic multiobjective optimization," Information Sciences, vol. 607, pp. 739-757, 2022.

[72] J. Chen, Z. Liu, X. Huang, C. Wu, Q. Liu, G. Jiang, Y. Pu, Y. Lei, X. Chen, X. Wang et al., "When large language models meet personalization: Perspectives of challenges and opportunities," arXiv preprint arXiv:2307.16376, 2023.

[73] J. Lehman, J. Gordon, S. Jain, K. Ndousse, C. Yeh, and K. O. Stanley, "Evolution through large models," arXiv preprint arXiv:2206.08896, 2022.

[74] Y. Chen, X. Song, C. Lee, Z. Wang, R. Zhang, D. Dohan, K. Kawakami, G. Kochanski, A. Doucet, M. Ranzato et al., "Towards learning universal hyperparameter optimizers with transformers," Advances in Neural Information Processing Systems, vol. 35, pp. 32053-32068, 2022.

[75] M. Pluhacek, A. Kazikova, T. Kadavy, A. Viktorin, and R. Senkerik, "Leveraging large language models for the generation of novel metaheuristic optimization algorithms," in Proceedings of the Companion Conference on Genetic and Evolutionary Computation, 2023, pp. $1812-1820$.

[76] P.-F. Guo, Y.-H. Chen, Y.-D. Tsai, and S.-D. Lin, "Towards optimizing with large language models," arXiv preprint arXiv:2310.05204, 2023.

[77] K. Miettinen, Nonlinear multiobjective optimization. Springer Science \& Business Media, 2012, vol. 12.

[78] I. Das and J. E. Dennis, "Normal-boundary intersection: A new method for generating the pareto surface in nonlinear multicriteria optimization problems," SIAM journal on optimization, vol. 8, no. 3, pp. 631-657, 1998.

[79] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer, "Rethinking the role of demonstrations: What makes in-context learning work?" arXiv preprint arXiv:2202.12837, 2022.

[80] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou et al., "Larger language models do in-context learning differently," arXiv preprint arXiv:2303.03846, 2023.

[81] Y. Zhang, K. Zhou, and Z. Liu, "What makes good examples for visual in-context learning?" arXiv preprint arXiv:2301.13670, 2023.

[82] M. Xiong, Z. Hu, X. Lu, Y. Li, J. Fu, J. He, and B. Hooi, "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms," arXiv preprint arXiv:2306.13063, 2023.

[83] R. Tanabe and H. Ishibuchi, "An easy-to-use real-world multi-objective optimization problem suite," Applied Soft Computing, vol. 89, p. $106078,2020$.

[84] J. Blank and K. Deb, "Pymoo: Multi-objective optimization in python," Ieee access, vol. 8, pp. 89 497-89 509, 2020.

[85] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei, "Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers," arXiv preprint arXiv:2212.10559, 2022.

[86] J. Von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov, "Transformers learn incontext by gradient descent," in International Conference on Machine Learning. PMLR, 2023, pp. 35 151-35 174.

[87] E. Akyrek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou, "What learning algorithm is in-context learning? investigations with linear models," arXiv preprint arXiv:2211.15661, 2022.

[88] Y. Li, M. E. Ildiz, D. Papailiopoulos, and S. Oymak, "Transformers as algorithms: Generalization and stability in in-context learning," 2023.

[89] J. Zhang, C. Xu, J. Li, W. Chen, Y. Wang, Y. Tai, S. Chen, C. Wang, F. Huang, and Y. Liu, "Analogous to evolutionary algorithm: Designing a unified sequence model," Advances in Neural Information Processing Systems, vol. 34, pp. 26674-26688, 2021.

[90] S. Das and P. N. Suganthan, "Differential evolution: A survey of the state-of-the-art," IEEE transactions on evolutionary computation, vol. 15, no. 1, pp. 4-31, 2010.

[91] E. Zitzler, K. Deb, and L. Thiele, "Comparison of multiobjective evolutionary algorithms: Empirical results," Evolutionary computation, vol. 8, no. 2, pp. 173-195, 2000.

[92] Y. Tian, R. Cheng, X. Zhang, and Y. Jin, "Platemo: A matlab platform for evolutionary multi-objective optimization [educational forum]," IEEE Computational Intelligence Magazine, vol. 12, no. 4, pp. 7387, 2017.

[93] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, "Self-consistency improves chain of thought reasoning in language models," arXiv preprint arXiv:2203.11171, 2022.

[94] O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith, and M. Lewis, "Measuring and narrowing the compositionality gap in language models," arXiv preprint arXiv:2210.03350, 2022.

[95] S. Jiang, J. Zou, S. Yang, and X. Yao, "Evolutionary dynamic multiobjective optimisation: A survey," ACM Computing Surveys, vol. 55, no. 4 , pp. 1-47, 2022 .

[96] H. Ishibuchi, L. M. Pang, and K. Shang, "A new framework of evolutionary multi-objective algorithms with an unbounded external archive," Authorea Preprints, 2023.

[97] J. Liang, X. Ban, K. Yu, B. Qu, K. Qiao, C. Yue, K. Chen, and K. C. Tan, "A survey on evolutionary constrained multiobjective optimization," IEEE Transactions on Evolutionary Computation, vol. 27, no. 2, pp. 201-221, 2022.

[98] H. Qian and Y. Yu, "Solving high-dimensional multi-objective optimization problems with low effective dimensions," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no. 1, 2017.

[99] D. Guo, X. Wang, K. Gao, Y. Jin, J. Ding, and T. Chai, "Evolutionary optimization of high-dimensional multiobjective and many-objective expensive problems assisted by a dropout neural network," IEEE transactions on systems, man, and cybernetics: systems, vol. 52, no. 4, pp. 2084-2097, 2021.

[100] S. Liu, Q. Lin, J. Li, and K. C. Tan, "A survey on learnable evolutionary algorithms for scalable multiobjective optimization," IEEE Transactions on Evolutionary Computation, 2023.


[^0]:    ${ }^{1}$ https://platform.openai.com/docs/model-index-for-researchers

