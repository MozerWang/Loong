# Towards Optimal Statistical Watermarking 

Baihe Huang* ${ }^{*} \quad$ Hanlin $\mathrm{Zhu}^{\dagger} \quad$ Banghua Zhu ${ }^{\ddagger}$<br>Kannan Ramchandran ${ }^{\&} \quad$ Michael I. Jordan ${ }^{\mathbb{I I}} \quad$ Jason D. Lee ${ }^{\|}$<br>Jiantao Jiao**


#### Abstract

We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-offs between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $\Theta\left(h^{-1} \log (1 / h)\right)$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where the user is allowed to perform a class of perturbations on the generated texts, and characterize the optimal Type II error of robust UMP tests via a linear programming problem. To the best of our knowledge, this is the first systematic statistical treatment on the watermarking problem with near-optimal rates in the i.i.d. setting, which might be of interest for future works.


## 1 Introduction

The prevalence of large language models (LLMs) in recent years makes it challenging and important to detect whether a human-like text is produced by the LLM system (Kirchenbauer et al., 2023a; Kuditipudi et al., 2023; Christ et al., 2023; Yoo et al., 2023; Fernandez et al.,[^0]

2023; Fu et al., 2023; Wang et al., 2023; Yang et al., 2023; Liu et al., 2023; Zhao et al., 2023; Koike et al., 2023). On the one hand, some of the most advanced LLMs to date, such as GPT-4 (OpenAI, 2023a), are good at producing human-like texts, which might be hard to distinguish from human-generated texts even for humans in various scenarios. On the other hand, it is important to keep human-produced text datasets separated from machine-produced texts in order to avoid the spread of misleading information (Vincent, 2022) and the contamination of training datasets for future language models (Kuditipudi et al., 2023).

To detect machine-generated content, a recent line of work (Kirchenbauer et al., 2023a; Kuditipudi et al., 2023; Christ et al., 2023) proposes to inject statistical watermarks, a signal embedded within the generated texts which reveals the generation source, into texts. As discussed in Kuditipudi et al. (2023), there are three desirable properties of watermarking: 1. distortion-free: the watermark should not alert the distribution of the generated texts; 2. agnostic: the detector should not know the language model or the prompt; 3. robust: the detector should be able to detect the watermark even under slight perturbation of the generated texts. However, previously proposed methods are either heuristic or guaranteed by different, sub-optimal mathematical descriptions of the above properties, making it difficult to systematically evaluate the watermarking schemes and to draw useful statistical conclusions.

Motivated by this, we propose a unifying formulation of statistical watermarking based on hypothesis testing, and study the trade-off between the Type I error and the Type II error. More specifically, our contributions are summarized as follows:

- We formulate statistical watermarking as a hypothesis testing problem with a random rejection region, and specify model-agnostic watermarking, where the distribution of the rejection region is independent of the underlying model distribution, as a notion highly practical in real-world applications.
- We find the optimal Type II error among all level- $\alpha$ tests and explicitly characterize the most powerful watermarking scheme that achieves it. For model-agnostic watermarking, we construct the optimal distribution of the reject region and establish the minimax increase in Type II error in comparison to the most powerful watermarking schemes.
- In the context where the sample is a sequence of many i.i.d. tokens, we provide nearly-matching upper and lower bounds for the minimum number of tokens required to guarantee small type I and type II errors. Our rate $h^{-1} \log (1 / h)$ improves upon previous works featuring a rate of $h^{-2}$, in terms of $h$ - the average entropy of per generated tokens.
- Additionally, we formulate a robust watermarking problem where the watermarking scheme is robust to a class of perturbations that the user can employ to the outputs. In this setting, we also characterize the optimal type II error and the construction of the robust watermarking scheme via a linear program.


### 1.1 Related works

Watermarking is a powerful white-box method for detecting LLM-generated texts (Tang et al., 2023). Watermarks can be injected either into a pre-existing text (edit-based watermarks) or during the text generation (generative watermarks). Our work falls in the latter category. Edit-based watermarking (Rizzo et al., 2019; Abdelnabi \& Fritz, 2021; Yang et al., 2022; Kamaruddin et al., 2018) has been the focus of several studies in the past. The concept of generative watermarking dates back to the work of Venugopal et al. (2011), while our work is more relevant to a recent line of works (Aaronson, 2022a; Kirchenbauer et al., 2023a; Kuditipudi et al., 2023; Christ et al., 2023) that introduce statistical signals into text generation. Specifically, Kirchenbauer et al. (2023a) increases the probability that tokens are chosen from a randomly sampled 'green' list; Aaronson (2022a) selects the token $i$ that maximizes keys randomly sampled from exponential distributions with mean $1 / p_{i}$; Christ et al. (2023) samples the tokens by solving the optimal transport from uniform distribution in $[0,1]$; Kuditipudi et al. (2023) introduces inverse transform sampling as a distortion-free watermarking method; Zhao et al. (2023) proposes a simplified variation of Kirchenbauer et al. (2023a) where a fixed Green-Red split is used consistently. These watermarks are evaluated in the benchmark of Piet et al. (2023).

Statistical watermarking techniques share the similarity that the outputs are correlated with some secret keys (which could come from either external randomness or internal hashing), thereby coupling the rejection region and the outputs in the hypothesis testing. This fact is recognized by recent works of Kuditipudi et al. (2023); Zhao et al. (2023), where model-agnosticism in the detection phase is also emphasized. The exponential scheme in Aaronson (2022b), the inverse transform sampling scheme in Kuditipudi et al. (2023), and the binary scheme in Christ et al. (2023) come with theoretical guarantees that (i) the watermarked model distribution cannot be distinguished from the original distribution (called undetectability (Christ et al., 2023) or distortion-freeness (Kuditipudi et al., 2023)), and (ii) the outputs from the watermarked models are statistically detectable as long as the entropy is lower bounded. In contrast, Kirchenbauer et al. (2023a) is not distortion-free, nonetheless enjoying little degradation in generation quality and provable detectability (Zhao et al. 2023) with suitable parameter choice of the bias parameter (logits increase $\delta$ in the 'green' list). Despite the aforementioned theoretical efforts in establishing guarantees for existing watermarks, the fundamental tradeoff in this hypothesis testing problem and the rates on the required number of generated tokens remain unsolved.

Watermarks can also be injected with private forgeability and public verifiability (Fairoze et al., 2023), hence functioning effectively as digital signatures. Meanwhile, various attack algorithms against watermarking schemes were also studied (Kirchenbauer et al., 2023a b; Sato et al., 2023; Zhang et al., 2023; Kuditipudi et al., 2023). These attacking schemes apply quality-preserving perturbations to the watermarked outputs in delicate ways, and are therefore modelled by the perturbation graph (Definition 4.1) in the robust watermark framework in Section 4. With the success of various attacking methods, robustness becomes an important consideration in watermarking techniques. However, Zhang et al. (2023)
proves that it is only feasible to achieve robustness to a well-specified set of attacks, instead of all. This fact aligns with our Theorem 4.4, which characterizes the fundamental limits of robust watermarking under different attacking powers.

### 1.2 Notation

Define $(x)_{+}:=\max \{x, 0\}, x \wedge y:=\min \{x, y\}, x \vee y=\max \{x, y\}$. For any set $A$, we write $A^{c}$ as the complement of set $A,|A|$ as its cardinality, and $2^{A}:=\{B: B \subset A\}$ as the power set of $A$. We use notations $g(n)=O(f(n)), g(n)=\Omega(f(n))$, and $g(n)=\Theta(f(n))$ to denote that there exists numerical constants $C_{1}, c_{2}, C_{3}, c_{4}$ such that for all $n>0: g(n) \leq$ $C_{1} \cdot f(n), g(n) \geq c_{2} \cdot f(n)$, and $c_{4} \cdot f(n) \leq g(n) \leq C_{3} \cdot f(n)$, respectively. Throughout, we use $\ln$ to denote natural logarithm.

The total variation (TV) distance between two probability measures $\mu, \nu$ is denoted by $\operatorname{TV}(\mu \| \nu)$. We use $\operatorname{supp}(\mu)$ to denote the support of a probability measure $\rho$. Given a sample space $\Omega$, let $\Delta(\Omega)$ denote the set of all probability measures over $\Omega$ (take the discrete $\sigma$-algebra). We write $\delta_{x}$ as the Dirac measure on $x$, i.e., $\delta_{x}(A)=\left\{\begin{array}{ll}1, & x \in A \\ 0, & x \notin A\end{array}\right.$. A coupling for two distributions (i.e. probability measures) is a joint distribution of them.

## 2 Watermarking as a Hypothesis Testing Problem

In the problem of statistical watermarking, a service provider (e.g., a language model system), who possesses a distribution $\rho$ over a sample space $\Omega$, aims to make the samples from the service provider distinguishable by a detector, without changing $\rho$. The service provider achieves this by sharing a watermark key (generated from a distribution that is coupled with $\rho$ ) with the detector, with the goal of controlling both the Type I error (an independent output is falsely detected as from $\rho$ ) and the Type II error (an output from $\rho$ fails to be detected). This random key together with the detection rule constitute a (random) rejection region. In the following, we formulate this problem as hypothesis testing with random rejection regions.

Problem 2.1 (Watermarking). Fix $\epsilon \geq 0$. Given a probability measure $\rho$ over sample space $\Omega$. an $\epsilon$-distorted watermarking scheme of $\rho$ is a probability measure $\mathcal{P}$ (a joint probability of the output $X$ and the rejection region $R$ ) over the sample space $\Omega \otimes 2^{\Omega}$ such that $\operatorname{TV}\left(\mathcal{P}\left(\cdot, 2^{\Omega}\right) \| \rho\right) \leq \epsilon$, where $\mathcal{P}\left(\cdot, 2^{\Omega}\right)$ is the marginal probability of $X$ over $\Omega$. In the generation phase, the service provider samples $(X, R)$ from $\mathcal{P}$, provides the output $X$ to the service user, and sends the rejection region $R$ to the detector.

In the detection phase, a detector is given a tuple $(X, R) \in \Omega \otimes 2^{\Omega}$ where $X$ is sampled from an unknown distribution and $R$, given by the service provider, is sampled from the[^1]marginal probability $\mathcal{P}(\Omega, \cdot)$ over $2^{\Omega}$. The detector is tasked with using $R$ to conduct a hypothesis test that involves two competing hypotheses:

$$
H_{0}: X \text { is sampled independently from } R
$$

versus $H_{1}:(X, R)$ is sampled from the joint distribution $\mathcal{P}$.

The Type I error of $\mathcal{P}$, defined as $\alpha(\mathcal{P}):=\sup _{\pi \in \Delta(\Omega)} \mathbb{P}_{Y \sim \pi,(X, R) \sim \mathcal{P}}(Y \in R)$, is the maximum probability that an independent sample $Y$ is falsely rejected. The Type II error of $\mathcal{P}$, defined as $\beta(\mathcal{P}):=\mathbb{P}_{(X, R) \sim \mathcal{P}}(X \notin R)$, is the probability that the sample $(X, R)$ from the joint probability $\mathcal{P}$ is not detected.

A few remarks are in order.

Remark 2.2 (Difference between classical hypothesis testing). In classical hypothesis testing, the rejection region is often nonrandomized or independent from the test statistics. However, in watermarking problem, the service provider has the incentive to facilitate the detection. The key insight is that $\mathcal{P}$ is a coupling of the random output $X$ and the random rejection region $R$, so that $X \in R$ occurs with a high probability (low Type II error), while any independent sample $Y$ lies in $R$ with a low prob-

![](https://cdn.mathpix.com/cropped/2024_06_04_309aa83cb0d6ced259fag-05.jpg?height=564&width=897&top_left_y=886&top_left_x=907)

Figure 1: Illustration of watermarking in practice. ability (low Type I error).

Remark 2.3 (Implementation). In fact, it is imperative for the detector to observe the rejection region that is coupled with the output: otherwise, the output from the service provider and another independent output from the same marginal distribution would be statistically indistinguishable.

In practice, the process of coupling and sending the rejection region can be implemented by cryptographical techniques: the service provider could hash a secret key $s k$, and use pseudo-random functions $F_{1}, F_{2}$ to generate $(X, R)=\left(F_{1}(s k), F_{2}(s k)\right)$. Now it suffices to send the secret key to the detector, who can then reproduce the reject region using the pseudo-random function $F_{2}$. This process is illustrated in Figure 1 .

By introducing the coupled and random rejection region, we abstract away the minutiae of cryptographical implementations, therefore allowing us to focus solely on the statistical trade-offs.

For practical applications, it is additionally desirable for watermarking schemes to be model-agnostic, i.e, the marginal distribution of the rejection region is irrelevant to the
watermarked distribution. Recall from Remark 2.3 that in practice, detectors usually adopt a pseudo-random function to generate the reject region from the shared secret keys. If the watermarking scheme $\mathcal{P}$ depends on the underlying distribution $\rho$, then the pseudo-random function, and effectively the detector, need to know $\rho$. On the other hand, model-agnostic watermarking enables the detector to use a fixed, pre-determined pseudo-random function to generate the reject region, and hence perform hypothesis-testing without the knowledge of the underlying model that generates the output. This is an important property enjoyed by existing watermarks (Aaronson, 2022b; Kirchenbauer et al., 2023a; Christ et al., 2023; Kuditipudi et al., 2023). Therefore in the following, we formulate model-agnostic within our hypothesis testing framework.

Problem 2.4 (Model-Agnostic Watermarking). Given a sample space $\Omega$ and a set $\mathcal{Q} \subset \Delta(\Omega)$, a $\mathcal{Q}$-watermarking scheme is a tuple $\left(\eta,\left\{\mathcal{P}_{\rho}\right\}_{\rho \in \mathcal{Q}}\right)$ where $\eta$ is a probability measure over $2^{\Omega}$, such that for any probability measure $\rho \in \mathcal{Q}, \mathcal{P}_{\rho}$ is a distortion-free watermarking scheme of $\rho$ and its marginal distribution over $2^{\Omega}, \mathcal{P}_{\rho}(\Omega, \cdot)$, equals $\eta(\cdot)$.

A model-agnostic watermarking scheme is a $\Delta(\Omega)$-watermarking scheme.

Remark 2.5 (Information of the model). A $\mathcal{Q}$-watermarking scheme can be interpreted as a way to watermark all distributions in the set $\mathcal{Q}$ while revealing no information of the model used to generate the output other than the membership inside $\mathcal{Q}$ (i.e., observing the rejection region, one is only able to infer that the output comes from a model in $\mathcal{Q}$, but is unable to know which exactly the model is). By letting $\mathcal{Q}$ be $\Delta(\Omega)$, model-agnostic watermarking thus reveals no information of the model.

### 2.1 Examples

In the following examples, we show how existing watermarking schemes fit in our framework.

Example 2.6 (Text Generation with Soft Red List, Kirchenbauer et al. (2023a)). In Algorithm 2 of Kirchenbauer et al. (2023a), the watermarking scheme (over sample space $\Omega=V^{*}$ where $V$ is the 'vocabulary', i.e., the set of all tokens) of $\rho$ is given as follows:

- Fix threshold $C \in \mathbb{R}$, green list size $\gamma \in(0,1)$, and hardness parameter $\delta>0$
- For $i=1,2, \ldots$
- Randomly partition $V$ into a green list $G$ of size $\gamma|V|$, and a red list $R$ of size $(1-\gamma)|V|$.
- Sample the token $X_{i}$ from the following distribution from $\mathbb{P}$ where $\mathbb{P}\left(X_{i}=x\right)=$

$$
\begin{cases}\frac{\rho(x) \cdot \exp (\delta)}{\sum_{x \in G} \rho(x) \cdot \exp (\delta)+\sum_{x \in R} \rho(x)}, & \text { if } x \in G \\ \frac{\rho(x)}{\sum_{x \in G} \rho(x) \cdot \exp (\delta)+\sum_{x \in R} \rho(x)}, & \text { if } x \in R\end{cases}
$$

- Let the rejection region $R$ be

$$
\{X \in \Omega: \text { the number of green list tokens in } \mathrm{X} \geq C\}
$$

The above sampling procedures as a whole define the joint distribution of the output $X=X_{1} X_{2} \cdots$ and the rejection region $R$, i.e., the $\Theta(\delta)$-distorted watermarking scheme $\mathcal{P}_{\text {SoftRedList }}$. The detector observes the rejection region via the secret key that the service provider uses to generate the green and red lists.

Example 2.7 (Complete watermarking algorithm Wak $\mathrm{sk}_{\mathrm{sk}}$, Christ et al. (2023)). In Algorithm 3 of Christ et al. (2023), the watermarking scheme (over sample space $\Omega=\{0,1\}^{*}$ ) of $\rho$ is given as follows:

- Fix threshold $C \in \mathbb{R}$ and entropy threshold $\lambda>0$
- Select $i$ such that the empirical entropy of $X_{1} X_{2} \ldots X_{i}$ is greater than or equal to $\lambda$
- For $j=i+1, i+2, \ldots$
- Sample $u_{j} \in[0,1]$ uniformly at random.
- Let the binary token $X_{j}$ be given by $X_{j}=\left\{\begin{array}{ll}1, & \text { if } u_{j} \leq \rho\left(1 \mid X_{1}, \ldots, X_{j-1}\right) \\ 0, & \text { otherwise }\end{array}\right.$.
- Let the rejection region $R$ be given by

$$
\left\{X: \sum_{j=i+1}^{L} \log \frac{1}{X_{j} u_{j}+\left(1-X_{j}\right)\left(1-u_{j}\right)} \geq C\right\}
$$

The above sampling procedures as a whole define the joint distribution of the output $X=X_{1} X_{2} \cdots$ and the rejection region $R$, i.e., the 0 -distorted watermarking scheme $\mathcal{P}_{\text {Wak }_{\text {sk }}}$. The detector observes the rejection region via the index $i$ and $u_{j}(j>i)$.

![](https://cdn.mathpix.com/cropped/2024_06_04_309aa83cb0d6ced259fag-07.jpg?height=49&width=1512&top_left_y=1751&top_left_x=304)
transform sampling scheme in Christ et al. (2023) (over sample space $\Omega=[N]^{*}$ ) of $\rho$ is given as follows:

- Fix threshold $C \in \mathbb{R}$, resample size $T$, and block size $k$
- For $j=1,2, \ldots$,
- Let $\mu \leftarrow \rho\left(\cdot \mid X_{1}, \ldots, X_{j-1}\right)$.
- Sample $\xi_{j}=\left(u_{j}, \pi_{j}\right), \xi_{j}^{(t)}=\left(u_{j}^{\prime}, \pi_{j}^{\prime}\right)(t=1, \ldots, T)$ i.i.d. according to the following distribution:
* Sample $u \in[0,1]$ uniformly at random;
* Sample $\pi$ uniformly at random from the space of permutations over the vocabulary $[N]$.
- Let the token $X_{j}$ be given by

$$
\pi^{-1}(\min \{\pi(i): \mu(\{j: \pi(j) \leq \pi(i)\}) \geq u\})
$$

- Let the rejection region $R$ be

$$
R=\left\{X: \frac{1+\sum_{t=1}^{T} \mathbb{1}\left(\phi\left(X, \xi^{(t)}\right) \leq \phi(X, \xi)\right)}{T+1} \leq C\right\}
$$

where $\xi=\left(\xi_{1}, \ldots, \xi_{\operatorname{len}(X)}\right), \xi^{(t)}=\left(\xi_{1}^{(t)}, \ldots, \xi_{\operatorname{len}(X)}^{(t)}\right)$, and $\phi(y, \xi)$ is given by

$$
\min _{\substack{i=1, \ldots, \operatorname{len}(y)-k+1, j=1, \ldots, \operatorname{len}(\xi)}}\left\{d\left(\left\{y_{i+l}\right\}_{l=1}^{k-1},\left\{\xi_{(j+l) \% \operatorname{len}(\xi)}\right\}_{l=1}^{k-1}\right)\right\}
$$

Here $d$ is an alignment cost, set as $d(y,(u, \pi))=\sum_{i=1}^{\operatorname{len}(y)}\left|u_{i}-\frac{\pi_{i}\left(y_{i}\right)-1}{N-1}\right|$ in Kuditipudi et al. (2023). Additionally, a single permutation $\pi(\forall j, t)$ is used to reduce computation overhead. The above sampling procedures as a whole define the joint distribution of the output $X=X_{1} X_{2} \cdots$ and the rejection region $R$ in $\mathrm{Wak}_{\mathrm{ITS}}$. The detector observes the rejection region via $\xi, \xi^{\prime}$.

Using similar approaches as in the above examples, we can encompass the methods of a number of works (Aaronson, 2022b; Liu et al., 2023; Zhao et al., 2023; Kuditipudi et al., 2023) into our framework.

## 3 Statistical Limit in Watermarking

### 3.1 Rates under the general setting of Problem 2.1

Given the formulation of statistical watermarking, it is demanding to understand its statistical limit. In this section, we study the following notion of Uniformly Most Powerful (UMP) test, i.e., the watermarking scheme that achieves the minimum achievable Type II error among all possible tests with Type I error $\leq \alpha$.

Definition 3.1 (Uniformly Most Powerful Watermark). A watermarking scheme $\mathcal{P}$ is called Uniformly Most Powerful (UMP) $\epsilon$-distorted watermark of level $\alpha$, if it achieves the minimum achievable Type II error among all $\epsilon$-distorted watermarking with Type I error $\leq \alpha$.

The following result gives an exact characterization of the UMP watermark and its Type II error.

Theorem 3.2. For probability measure $\rho$, the Uniformly Most Powerful $\epsilon$-distorted watermark of level $\alpha$, denoted by $\mathcal{P}^{*}$, is given by

$$
\mathcal{P}^{*}\left(X=x, R=R_{0}\right)= \begin{cases}\rho^{*}(x) \cdot\left(1 \wedge \frac{\alpha}{\rho^{*}(x)}\right), & R_{0}=\{x\} \\ \rho^{*}(x) \cdot\left(1-\frac{\alpha}{\rho^{*}(x)}\right)_{+}, & R_{0}=\emptyset \\ 0, & \text { else }\end{cases}
$$

where $\rho^{*}=\arg \min _{T V\left(\rho^{\prime} \| \rho\right) \leq \epsilon} \sum_{x \in \Omega: \rho^{\prime}(x)>\alpha}\left(\rho^{\prime}(x)-\alpha\right)$. Its Type II error is given by

$$
\min _{T V\left(\rho^{\prime} \| \rho\right) \leq \epsilon} \sum_{x \in \Omega: \rho^{\prime}(x)>\alpha}\left(\rho^{\prime}(x)-\alpha\right)
$$

and when $|\Omega| \geq \frac{1}{\alpha}$ it simplifies to

$$
\begin{equation*}
\left(\sum_{x \in \Omega: \rho(x)>\alpha}(\rho(x)-\alpha)-\epsilon\right)_{+} \tag{1}
\end{equation*}
$$

The key insight for proving Theorem 3.2 is that maximizing Type II error over level $\alpha$ can be written as a linear program over the coupling distribution $\mathcal{P}$. The detailed proof is deferred to Appendix A. In the following, we make a few remarks on Theorem 3.2.

Remark 3.3 (Dependence on distortion parameter $\epsilon$ ). As seen from the theorem, when a larger distortion parameter $\epsilon$ is allowed, the Type II error would decrease. This aligns with the intuition that adding statistical bias would make the output easier to detect (Aaronson. 2022a: Kirchenbauer et al., 2023a). Among all choices of $\epsilon$, the case $\epsilon=0$ is of particular interest since it preserves the marginal distribution of the service provider's output. Therefore, we will focus on this distortion-free case in the following sections.

Remark 3.4 (Intuition behind $\mathcal{P}^{*}$ ). Recall that in practice, the watermarks are implemented via pseudo-random functions. Therefore, the uniformly most powerful test in Theorem 3.2 is effectively using a pseudo-random generator to approximate the distribution $\rho$, combined with an $\alpha$-clipping to control Type I error. This construction reveals a surprising message: simply using pseudo-random generator to approximate the distribution is optimal.

Remark 3.5 (Watermarking guarantees). To achieve the upper bound of Theorem 3.2 the detector needs to access the model and the prompt in order to generate the reject region, which is not always accessible in many real-world applications. Therefore, the upper bound of Theorem 3.2 achieves a weaker watermarking guarantee compared with previous works (Aaronson, 2022a; Kirchenbauer et al., 2023a; Christ et al., 2023). In Section 3.2, we study model-agnostic watermarking that overcomes this limitation.

Nonetheless, the lower bound in Theorem 3.2 characterizes a fundamental limit of Problem 2.1 thus providing an information-theoretic lower bound for all watermarks.

Remark 3.6 (Implementation). To implement the UMP watermark using a predetermined key, one may apply the key to the random seeds used in model generation, and sets the reject region to be the output with probability $1 \wedge \frac{\alpha}{\rho^{*}(x)}$. To implement the UMP watermark without the detector's knowledge on the secret key, one could hash the first few tokens to seed the pseudo-random function. In summary, the UMP watermark could use the same key to watermark many outputs, and the key needs not to be generated at the same time as the output itself.

Remark 3.7 (Use cases of the UMP watermark). The utilization of the UMP watermark offers an efficient approach for (language model) service providers to determine if instruction-
following datasets have been generated by a specific model. In the context of instructionfollowing datasets, both the prompt and response are explicitly provided to the detectors, enabling the UMP watermark to perform accurate watermarking and detection without extra source of information. This usage is beneficial in identifying and filtering out data points that have been comtaminated by texts generated from models like GPT-4 (OpenAI, 2023b), thereby preserving the purity and quality of the training data.

Remark 3.8 (Dependence on the randomness of $\rho$ ). If $\rho$ is deterministic, the Type II error $\left(\sum_{x \in \Omega: \rho(x)>\alpha}(\rho(x)-\alpha)-\epsilon\right)$ reduces to $1-\alpha-\epsilon$ and shows limited practical utility of statistical watermarking. This is expected since when the service provider deterministically outputs $z$, it would be impossible to distinguish the watermark distribution with an independent output from $\delta_{z}$. In general, Theorem 3.2 implies that the Type II error decreases when the randomness in $\rho$ increases, matching the reasoning in previous works Aaronson (2022a); Christ et al. (2023).

### 3.2 Rates of model-agnostic watermarking

It is noticeable that for large $\mathcal{Q}$, a $\mathcal{Q}$-watermarking scheme can not perform as good as a watermarking specifically designed for $\rho$ for any distribution $\rho \in \mathcal{Q}$. This means that Uniformly Most Powerful $\mathcal{Q}$-Watermarking might not exist in general. To evaluate modelagnostic watermarking schemes, a natural desideratum is therefore the maximum difference between its Type II error and the Type II error of the UMP watermarking of $\rho$ over all distributions $\rho$, under fixed Type I error. Specifically, we introduce the following notion.

Definition 3.9 (Minimax most powerful model-agnostic watermark). We say that a $\mathcal{Q}$ agnostic watermark $\left(\eta,\left\{\mathcal{P}_{\rho}\right\}_{\rho \in \mathcal{Q}}\right)$ is of level- $\alpha$ if the Type I error of $\mathcal{P}_{\rho}$ is less than or equal to $\alpha$ for any $\rho \in \mathcal{Q}$. Define the maximum Type II error loss of $\left(\eta,\left\{\mathcal{P}_{\rho}\right\}_{\rho \in \mathcal{Q}}\right)$ as

$$
\gamma(\eta):=\sup _{\rho \in \mathcal{Q}} \beta\left(\mathcal{P}_{\rho}\right)-\beta\left(\mathcal{P}_{\rho}^{*}\right)
$$

where $\mathcal{P}_{\rho}^{*}$ is the UMP distortion-free watermark of $\rho$.

We say that a $\mathcal{Q}$-agnostic watermarking scheme is minimax most powerful, if it minimizes the maximum Type II error loss among all $\mathcal{Q}$-agnostic watermarks of level $\alpha$.

The following result characterizes the Type II error loss of the minimax most powerful model-agnostic watermarking.

Theorem 3.10. Let $|\Omega|=n$ and suppose $\alpha n, \frac{1}{\alpha} \in \mathbb{Z}^{2}$. In the minimax most powerful model-agnostic watermarking scheme of level- $\alpha$, the marginal distribution of the reject[^2]region is given by

$$
\eta^{*}(A)= \begin{cases}\frac{1}{\binom{n}{\alpha}}, & \text { if }|A|=\alpha n \\ 0, & \text { otherwise }\end{cases}
$$

The maximum Type II error loss of the minimax most powerful model-agnostic watermarking scheme of level- $\alpha$ is given by $\gamma\left(\eta^{*}\right)=\frac{\binom{n-\frac{1}{\alpha}}{\alpha}}{\binom{n}{\alpha n}}$. In the regime $\alpha \rightarrow 0_{+}, n \rightarrow+\infty$, we have $\gamma\left(\eta^{*}\right) \rightarrow c$ for some constant $c \leq e^{-1}$, and when $1 /(\alpha n) \rightarrow 0_{+}$is further satisfied, $c=e^{-1}$.

The theorem establishes existence of $\mathcal{P}_{\rho}$ for any $\rho$ without explicit construction. To grasp this concept, consider three sets: $U$, the output space; $V$, the set of reject regions; and $W$, the subset of $U \times V$ defined by $\{(u, v): u \in v\}$. Notice that the type II error is essentially $1-\mathcal{P}_{\rho}(W)$. Therefore, our objective is to establish existence of a probability measure $P$ over $U \times V$ such that its marginal distributions align with $\eta$ and $\rho$, respectively, and the probability assigned to $W$, denoted as $P(W)$, meets a certain lower bound. This is the question studied by Strassen's theorem (Strassen, 1965), which stipulates conditions for the existence of such a measure. Hence, by verifying Strassen's conditions, we confirm the existence of the required measure without the necessity of explicitly constructing the coupling. We defer the detailed proof to Appendix C.

Remark 3.11. Theorem 3.10 implies that for any distribution $\rho$, the Type II error of modelagnostic watermark is upper bounded by $\frac{\binom{n-\frac{1}{\alpha}}{\alpha}}{\binom{n}{\alpha n}}+\sum_{x: \rho(x) \geq \alpha}(\rho(x)-\alpha)$. The convergence $\gamma\left(\eta^{*}\right) \rightarrow e^{-1}$ implies that the minimax optimal model-agnostic watermark exhibits an increase in Type II error by an additive factor of $e^{-1}$ compared to the UMP watermark in the worst-case scenario.

Remark 3.12. The $e^{-1}$ maximum Type II error loss does not contradict with the $h^{-2}$ rates in previous works (Aaronson, 2022a; Christ et al. 2023; Kuditipudi et al. 2023), because as $n \gtrsim h^{-2}$, the model distribution (of the sequences of $n$ tokens with average entropy $h$ per token) is beyond the worst case. Indeed, such distributions have higher differential entropy than the hard instances in the proof.

Remark 3.12 highlights that the hard instance constructed in Theorem 3.10 may possess a lower entropy than that of the actual model. Therefore, it raises an important question: for a smaller class $\mathcal{Q}$ that contains distributions with higher entropy, what is the minimum achievable Type II error loss for $\mathcal{Q}$-agnostic watermarking? It is obvious that the minimax rate over a higher entropy level should improve upon the previous rate of $e^{-1}$.

Towards answering this question, we consider the following class of distributions:

$$
\mathcal{Q}_{\kappa}:=\left\{\rho: \sup _{\omega \in \Omega} \rho(\{\omega\}) \leq \kappa\right\}
$$

where $\kappa$ represents the level of randomness and decreases as entropy increases. The maximum Type II error loss of $\mathcal{Q}_{\kappa}$-agnostic watermarking $\left(\eta,\left\{\mathcal{P}_{\rho}\right\}_{\rho \in \mathcal{Q}_{k}}\right)$ is thus given by

$$
\gamma(\eta, \kappa):=\max _{\rho \in \Delta(\Omega): \sup _{\omega \in \Omega} \rho(\{\omega\}) \leq \kappa} \beta\left(\mathcal{P}_{\rho}\right)-\beta\left(\mathcal{P}_{\rho}^{*}\right)
$$

where $\mathcal{P}_{\rho}^{*}$ is the UMP distortion-free watermark of $\rho$. The following result gives an upper bound of the above quantity, thus answering the question.

Theorem 3.13. Let $|\Omega|=n$ and suppose $\alpha n, \frac{1}{\kappa} \in \mathbb{Z}$. Then the maximum Type II error loss of the minimax $\mathcal{Q}_{\kappa}$-agnostic watermarking of level- $\alpha$ is upper bounded by

$$
\gamma\left(\eta^{*}, \kappa\right) \leq \frac{\binom{n-\alpha n}{1 / \kappa}}{\binom{n}{1 / \kappa}}
$$

The proof can be found in Appendix $\mathrm{D}$. When $\kappa \leq \alpha$, the bound $\frac{\binom{n-\alpha n}{1 / \kappa}}{\binom{n}{1 / \kappa}}$ improves over $e^{-1}$. In the next section, we will apply Theorem 3.13 to the i.i.d. setting where $\kappa$ can be exponentially small. This will lead to an negligible maximum Type II error loss for model-agnostic watermarking.

### 3.3 Rates in the i.i.d. setting

In practice, the sample space $\Omega$ is usually a Cartesian product in the form of $\Omega_{0}^{\otimes n}$. For example, in large language models, the output takes form of a sequence of tokens, each coming from the same vocabulary set $V$. The quantity of practical interest becomes the minimum number of tokens to achieve certain statistical watermarking guarantee. This demands specializing and transferring the results from Theorem 3.2 and Theorem 3.13 to deal with distributions in product measureable spaces, and finding the explicit rates of the minimum number of required tokens.

In this section, we consider the product distribution $\rho=\rho_{0}^{\otimes n}$ over $\Omega_{0}^{\otimes n}$ and the important setting of $\epsilon=0$ (distortion-free watermarking). We introduce the following two quantities:

- Let $h$ denote the entropy of $\rho_{0}$. We use $n_{\text {ump }}(h, \alpha, \beta)$ to denote the minimum number of tokens required by the UMP watermark to achieve Type I error $\leq \alpha$ and Type II error $\leq \beta$.
- Define $n_{\text {minmax }}(h, \alpha, \beta)$ as the number of tokens required by minimax $\mathcal{Q}^{h}$-agnostic watermark to achieve Type I error $\leq \alpha$ and Type II error $\leq \beta$, where $\mathcal{Q}^{h}:=$ $\left\{\rho=\rho_{0}^{\otimes n}: H\left(\rho_{0}\right) \geq h\right\}$, i.e. contains all distributions $\rho=\rho_{0}^{\otimes n}$ such that the entropy of $\rho_{0}$ is $\geq h$.

Together, $n_{\text {ump }}(h, \alpha, \beta)$ and $n_{\text {minmax }}(h, \alpha, \beta)$ serve as critical thresholds beyond which the desired statistical conclusions can be drawn regarding the output, making them essential parameters in watermarking applications.

We start by inspecting the rates in Theorem 3.2 in the i.i.d. setting. The following result gives a nearly-matching upper bound and lower bound of $n_{\text {ump }}(h, \alpha, \beta)$.

Theorem 3.14. Suppose $\alpha, \beta<0.1$. We have

$$
n_{\mathrm{ump}}(h, \alpha, \beta) \geq O\left(\left(\frac{\ln \frac{1}{h}\left(\ln \frac{1}{\alpha} \wedge \ln \frac{1}{\beta}\right)}{h}\right) \vee \frac{\ln \frac{1}{\alpha}}{h}\right)
$$

Furthermore, let $k=\left|\Omega_{0}\right|$, we have

$$
\begin{aligned}
& n_{\mathrm{ump}}(h, \alpha, \beta) \\
\leq & \Omega\left(\left(\frac{\ln \frac{k}{h} \cdot\left(\ln \frac{1}{\alpha} \wedge \ln \frac{1}{\beta}\right)}{h}\right) \vee \frac{\ln \frac{1}{\alpha} \ln k}{h}\right)
\end{aligned}
$$

Remark 3.15 (Tightness). Up to a constant and logarithmic factor in $k$, our upper bound matches the lower bound. Notice that since any model with an arbitrary token set can be reduced into a model with a binary token set (Christ et al. 2023) (i.e. $k=2$ ), our bound is therefore tight up to a constant factor.

Using Theorem 3.13 and Theorem 3.14, we are now in the position to characterize $n_{\operatorname{minmax}}(h, \alpha, \beta)$. Suppose the sample space is a Cartesian product $\Omega=\Omega_{0}^{\otimes n_{0}}$ and constrain to product measures over sequences of $n_{0}$ tokens, like in Section 3.3. We start by the following relationship. 3

$$
1-\max _{\rho_{0}: H\left(\rho_{0}\right) \geq h} \max _{\omega \in \Omega_{0}} \rho_{0}(\{\omega\}) \geq \Omega\left(\frac{h}{\ln (1 / h)}\right)
$$

where a detailed derivation can be found in Lemma B.3. It follows that

$$
\kappa \leq\left(\max _{\rho_{0}: H\left(\rho_{0}\right) \geq h} \max _{\omega \in \Omega_{0}} \rho_{0}(\{\omega\})\right)^{n_{0}}=e^{-\Omega\left(\frac{n_{0} h}{\ln (1 / h)}\right)}
$$

Using this observation and the derivation in Theorem 3.10, $\gamma\left(\eta^{*}, \kappa\right)$ can be bounded by

$$
(1-\alpha)^{1 / \kappa} \leq(1-\alpha)^{e^{\Omega\left(\frac{n_{0} h}{\ln (1 / h)}\right)}}
$$

This means that when $n_{0} \gtrsim \frac{\ln (1 / h)}{h} \cdot(\ln (1 / \alpha)+\ln (1 / \beta))$, the maximum Type II error loss given by Theorem 3.13 and the Type II error of the UMP watermarking given in Theorem 3.14 can be simultaneously bounded by $\beta$, thus establishing an upper bound. Furthermore, this rate matches the lower bound in Theorem 3.14, where the guarantee is weaker (model-nonagnostic). Combining the above arguments, the following result is thus immediate.

Corollary 3.16. Suppose $\alpha, \beta<0.1$. We have

$$
n_{\operatorname{minmax}}(h, \alpha, \beta)=\Theta\left(\frac{\ln (1 / h)}{h} \cdot(\ln (1 / \alpha)+\ln (1 / \beta))\right)
$$

Remark 3.17 (Comparison with previous works). As commented in Remark 3.8 the regime $h \ll 1$ is more important and challenging because it is the scenario where watermarking is difficult. In this regime, our rate of $\frac{\ln (1 / h)}{h}$ improves the previous rate of $h^{-2}$ in a line of works (Aaronson, 2022a; Kirchenbauer et al., 2023a; Zhao et al. 2023; Liu et al., 2023; Kuditipudi et al. 2023), and highlights a fundamental gap between the existing watermarks and the information-theoretic lower bound.[^3]

## 4 Robust Watermarking

In the context of watermarking large language models, it's crucial to acknowledge users' capability to modify or manipulate model outputs. These modifications include cropping, paraphrasing, and translating the text, all of which may be employed to subvert watermark detection. Therefore, in this section, we introduce a graphical framework, modified from Problem 2.1, to account for potential user perturbations and investigate the optimal watermarking schemes robust to these perturbations. The formulation here shares similarity with a concurrent work by Zhang et al. (2023).

Definition 4.1 (Perturbation graph). A perturbation graph over the discrete sample space $\Omega$ is a directed graph $G=(V, E)$ where $V$ equals $\Omega$ and $(u, u) \in E$ for any $u \in V$. For any $v \in V$, let $i n(v)=\{w \in V:(w, v) \in E\}$ denote the set of vertices with incoming edges to $v$, and let out $(v)=\{w \in V:(v, w) \in E\}$ denote the set of vertices with outcoming edges from $v$.

The perturbation graph specifies all the possible perturbations that could be made by the user: any $u \in V$ can be perturbed into $v \in V$ if and only if $(u, v) \in E$, i.e., there exists a directed edge from $u$ to $v$.

Example 4.2. Consider $\Omega=\Omega_{0}^{\otimes n}$. Let the user have the capacity to change no more than $c$ tokens, i.e., perturb any sequence of tokens $x=x_{1} x_{2} \cdots x_{n}$ to another sequence $y=y_{1} y_{2} \cdots y_{n}$ with Hamming distance less than or equal to $c$. Then the perturbation graph is given by $G=(V, E)$ where $V=\Omega^{n}$ and $E=\{(u, v): u, v \in V, d(u, v) \leq c\}$ ( $d$ is the Hamming distance, i.e., $\left.d(x, y)=\sum_{i=1}^{n} \mathbb{1}\left(x_{i} \neq y_{i}\right)\right)$.

Problem 4.3 (Robust watermarking scheme). A robust watermarking scheme with respect to a perturbation graph $G$ is a watermarking scheme except that its Type II error is defined as $\mathbb{E}_{X, R \sim \mathcal{P}}\left[\max _{Y \in \text { out }(X)} \mathbb{1}(Y \notin R)\right]$, i.e., the probability of false negative given that the user adversarially perturbs the output.

The next result characterize the optimum Type II error achievable by robust watermarking, where the proof can be found in Appendix $\mathrm{E}$.

Theorem 4.4. Define the shrinkage operator $\mathcal{S}_{G}: 2^{\Omega} \rightarrow 2^{\Omega}$ (of a perturbation graph $G$ ) by $\mathcal{S}_{G}(R)=\{x \in \Omega$ : out $(x) \subset R\}$ and its inverse $\mathcal{S}_{G}^{-1}(R)=\cup_{x \in R}$ out $(x)$. Then the minimum Type II error of the robust, 0 -distorted UMP test of level $\alpha$ in Problem 4.3 is given by the solution of the following Linear Program

$$
\begin{align*}
\min _{x \in \mathbb{R}^{|\Omega|}} & 1-\sum_{y \in \Omega} \rho(y) x(y)  \tag{2}\\
\text { s.t. } & \sum_{y \in i n(z)} \rho(y) x(y) \leq \alpha, \sum_{z \in \Omega} x(z) \leq 1, \\
& 0 \leq x(z) \leq 1, \forall z \in \Omega
\end{align*}
$$

| Scheme / Temperature | $\mathbf{0}$ | $\mathbf{0 . 3}$ | $\mathbf{0 . 7}$ | $\mathbf{1}$ |
| :--- | :---: | :---: | :---: | :---: |
| Distribution Shift (Kirchenbauer et al., 2023a | $\mathbf{6 5}$ | 63 | 77 | 136 |
| Exponential (Aaronson, 2022b) | impossible | 890 | 190 | 93 |
| Inverse Transform (Kuditipudi et al. 2023$)$ | impossible | $+\infty$ | 434 | 222 |
| Binary (Christ et al. 2023) | impossible | $+\infty$ | $+\infty$ | 386 |
| Ours | impossible | $\mathbf{6 0 . 5}$ | $\mathbf{2 4}$ | $\mathbf{1 5}$ |

Table 1: Comparison of our watermark scheme (in the model non-agnostic setting) to previous works tested on the MARKMYWOrds benchmark by (Piet et al., 2023). For each watermark scheme and each temperature, we show the (average) minimum number of tokens needed to detect the watermark under the constraint that type $\mathrm{I}$ error is less than $\alpha=0.02$. For the first four rows, one can refer to Figure 1 of Piet et al. (2023); $+\infty$ means over half of all generations are not watermarked and "impossible" means when the temperature is 0 , the text generation procedure is deterministic and the entropy is zero, and thus any distortion-free watermark scheme does not work.

The UMP watermarking is given by $\mathcal{P}^{*}\left(X=y, R=R_{0}\right)$

$$
= \begin{cases}\rho(y) \cdot x^{*}(y), & R_{0}=\mathcal{S}_{G}^{-1}(\{y\}) \\ \rho(y) \cdot\left(1-x^{*}(y)\right), & R_{0}=\emptyset \\ 0, & \text { otherwise }\end{cases}
$$

where $x^{*}$ is the solution of Eq. (2).

Remark 4.5 (Dependence on the sparsity of graph). From Eq. (2), we observe that the perturbation graph influence the optimal Type II error via the constraint set. Indeed, if the graph is dense, the constraints $\sum_{y \in i n(z)} \rho(y) x(y) \leq \alpha$ involve many entries of $y \in \Omega$ and thus decrease the value $\sum_{y \in \Omega} \rho(y) x(y)$, thereby increasing the Type II error. On the other extreme, when the edge set of the perturbation graph is $E=\{(u, u): u \in v\}$, i.e., the user can not perturb the output to a different value, then optimum of Eq. (2) reduces to Eq. (1) (setting $\epsilon=0$ ).

## 5 Experiments

In this section, we show experimental results comparing our watermark scheme to several previous works. We test our watermark scheme on the MARKMYWORDS benchmark by Piet et al. (2023). Table 1 shows the average number of tokens needed to detect the watermark for five different watermark schemes under different temperatures on the MARKMYWORDS benchmark. We choose Llama2-7B-chat (Touvron et al. 2023) as the model to be watermarked and enforce that the type I error is less than $\alpha=0.02$.

Table 1 shows that our watermark scheme needs significantly fewer tokens to detect the watermark in the model non-agnostic setting, which provides strong empirical evidence
that our watermark scheme is statistically optimal (Theorem 3.14). An exception is that for the distribution shift scheme (Kirchenbauer et al., 2023b) with low temperature 0.3, the number of tokens required is only slightly larger than our scheme because the distribution shift scheme is not distortion-free. Note that the comparison in Table 1 is made under the model non-agnostic setting (the rate in the model-nonagnostic setting is not fundamentally different from that in the model-agnostic setting, due to Corollary 3.16) without considering robustness, while the four previous schemes also work for model agnostic setting with robustness guarantees. Therefore, our experiments corroborate the improved statistical trade-offs and highlight the fundamental gap, instead of advocating for the superiority of any particular watermarking scheme.

## 6 Conclusions

The understanding of watermarking large language models is advanced by framing it within the paradigm of hypothesis testing. We find that using a pseudo-random generator to approximate the model distribution (with probability clipping) yields the optimal Type II error among all level- $\alpha$ tests. Model-agnostic watermarking, reflecting the practical scenarios where the detector does not have access to the model distribution, enjoys a minimax bound in Type II errors depending on the model class. In the context where the output is a sequence of several tokens, we find that the optimal number of i.i.d. tokens required to detect statistical watermarks is $h^{-1} \log (1 / h)$, improving upon the previous rate of $h^{-2}$ and highlighting a fundamental gap. Finally, the optimal Type II error of robust UMP watermarking can be characterized via a linear program, which exhibits the trade-off between robustness and detectability.

Watermarking is an essential technique to diminish the misuse of large language models. It tackles several critical social issues concerning the malicious usage of language models such as the contamination of datasets, academic misconduct, creation of fake news, and circulation of misinformation. By laying the theoretical foundation of statistical watermarking, our paper provides unifying and systematic approach to evaluate the statistical guarantees of existing and future watermarking schemes, elucidating the statistical limit of (robust) watermarking problems, and revealing the optimal rates in the important setting of i.i.d. tokens. In the above ways, our work contributes to the research endeavours on addressing these societal issues in language modelling, thus having potentially positive social impacts.

## Acknowledgements

We would like to thank Julien Piet for his invaluable assistance with the experiments. Additionally, we are thankful to Or Zamir for his insightful comments on the earlier version of this manuscript.

## References

Scott Aaronson. My ai safety lecture for ut effective altruism. Shtetl-Optimized: The blog of Scott Aaronson. Retrieved on September, 11:2023, 2022a. URL https:// scottaaronson.blog/?p=6823.

Scott Aaronson. Watermarking gpt outputs. Scott Aaronson, 2022b. URL https://www. scottaaronson.com/talks/watermark.ppt.

Sahar Abdelnabi and Mario Fritz. Adversarial watermarking transformer: Towards tracing text provenance with data hiding. In 2021 IEEE Symposium on Security and Privacy (SP), pp. 121-140. IEEE, 2021.

Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models. arXiv preprint arXiv:2306.09194, 2023.

Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, and Mingyuan Wang. Publicly detectable watermarking for language models. arXiv preprint arXiv:2310.18491, 2023.

Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, and Teddy Furon. Three bricks to consolidate watermarks for large language models. arXiv preprint arXiv:2308.00113, 2023.

Yu Fu, Deyi Xiong, and Yue Dong. Watermarking conditional text generation for ai detection: Unveiling challenges and a semantic-aware watermark remedy. arXiv preprint arXiv:2307.13808, 2023.

Nurul Shamimi Kamaruddin, Amirrudin Kamsin, Lip Yee Por, and Hameedur Rahman. A review of text watermarking: theory, methods, and applications. IEEE Access, 6: 8011-8028, 2018.

John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. arXiv preprint arXiv:2301.10226, 2023a.

John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language models. arXiv preprint arXiv:2306.04634, 2023b.

Ryuto Koike, Masahiro Kaneko, and Naoaki Okazaki. Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples. arXiv preprint arXiv:2307.11729, 2023.

Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free watermarks for language models. arXiv preprint arXiv:2307.15593, 2023.

Aiwei Liu, Leyi Pan, Xuming Hu, Shu'ang Li, Lijie Wen, Irwin King, and Philip S Yu. A private watermark for large language models. arXiv preprint arXiv:2307.16230, 2023.

OpenAI. Gpt-4 technical report, 2023a.

R OpenAI. Gpt-4 technical report. arXiv, pp. 2303-08774, 2023b.

Julien Piet, Chawin Sitawarin, Vivian Fang, Norman Mu, and David Wagner. Mark my words: Analyzing and evaluating language model watermarks. arXiv preprint arXiv:2312.00273, 2023.

Stefano Giovanni Rizzo, Flavio Bertini, and Danilo Montesi. Fine-grain watermarking for intellectual property protection. EURASIP Journal on Information Security, 2019:1-20, 2019.

Ryoma Sato, Yuki Takezawa, Han Bao, Kenta Niwa, and Makoto Yamada. Embarrassingly simple text watermarks. arXiv preprint arXiv:2310.08920, 2023.

Volker Strassen. The existence of probability measures with given marginals. The Annals of Mathematical Statistics, 36(2):423-439, 1965.

Ruixiang Tang, Yu-Neng Chuang, and Xia Hu. The science of detecting llm-generated texts. arXiv preprint arXiv:2303.07205, 2023.

Flemming Topse. Bounds for entropy and divergence for distributions over a two-element set. J. Ineq. Pure Appl. Math, 2(2), 2001.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Ashish Venugopal, Jakob Uszkoreit, David Talbot, Franz Och, and Juri Ganitkevitch. Watermarking the outputs of structured prediction with an application in statistical machine translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pp. 1363-1372, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL https://aclanthology.org/D11-1126.

James Vincent. AI-generated answers temporarily banned on coding q\&a site stack overflow. The Verge, 5, 2022.

Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong Meng, Jie Zhou, and Xu Sun. Towards codable text watermarking for large language models. arXiv preprint arXiv:2307.15992, 2023.

Borui Yang, Wei Li, Liyao Xiang, and Bo Li. Towards code watermarking with dual-channel transformations. arXiv preprint arXiv:2309.00860, 2023.

Xi Yang, Jie Zhang, Kejiang Chen, Weiming Zhang, Zehua Ma, Feng Wang, and Nenghai Yu. Tracing text provenance via context-aware lexical substitution. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 11613-11621, 2022.

KiYoon Yoo, Wonhyuk Ahn, and Nojun Kwak. Advancing beyond identification: Multi-bit watermark for language models. arXiv preprint arXiv:2308.00221, 2023.

Hanlin Zhang, Benjamin L Edelman, Danilo Francati, Daniele Venturi, Giuseppe Ateniese, and Boaz Barak. Watermarks in the sand: Impossibility of strong watermarking for generative models. arXiv preprint arXiv:2311.04378, 2023.

Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Yu-Xiang Wang. Provable robust watermarking for ai-generated text. arXiv preprint arXiv:2306.17439, 2023.
