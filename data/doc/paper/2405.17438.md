# An LLM-Tool Compiler for Fused Parallel Function Calling 

Simranjit Singh<br>Microsoft Corporation<br>Silicon Valley Campus, CA, USA<br>simsingh@microsoft.com

Andreas Karatzas<br>Southern Illinois University<br>Carbondale, IL, USA<br>andreas.karatzas@siu.edu

Michael Fore<br>Microsoft Corporation<br>Reston, VA, USA<br>mifore@microsoft.com

Iraklis Anagnostopoulos<br>Southern Illinois University<br>Carbondale, IL, USA<br>iraklis.anagno@siu.edu

Dimitrios Stamoulis

Microsoft Corporation

Redmond, WA, USA

stamoulis.dimitrios@microsoft.com

![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-1.jpg?height=612&width=1740&top_left_y=800&top_left_x=190)

Figure 1: Fused parallel function calling with LLM-Tool Compiler. For each user query, the compiler dynamically identifies tools to execute concurrently and/or fuses similar functions to single operations. By presenting them as unified tasks to GPT, our approach inherently enhances parallelization and overall efficiency. Results show that LLM-Tool Compiler increases parallel calls by up to five times when integrated with various prompting schemes.


#### Abstract

State-of-the-art sequential reasoning in Large Language Models (LLMs) has expanded the capabilities of Copilots beyond conversational tasks to complex function calling, managing thousands of API calls. However, the tendency of compositional prompting to segment tasks into multiple steps, each requiring a round-trip to the GPT APIs, leads to increased system latency and costs. Although recent advancements in parallel function calling have improved tool execution per API call, they may necessitate more detailed in-context instructions and task breakdown at the prompt level, resulting in higher engineering and production costs. Inspired by the hardware design principles of multiply-add (MAD) operations, which fuse multiple arithmetic operations into a single task from the compiler's perspective, we propose LLM-Tool Compiler, which selectively fuses similar types of tool operations under a single function at runtime, presenting them as a unified task to the LLM. This selective fusion inherently enhances parallelization and efficiency. Benchmarked on a large-scale Copilot platform, LLM-Tool Compiler achieves up to four times more parallel calls than existing methods, reducing token costs and latency by up to $40 \%$ and $12 \%$, respectively.


## 1 INTRODUCTION

Recent advances in Large Language Models (LLMs) have significantly enhanced their reasoning capabilities, positioning them as novel Copilots for diverse applications in robotics, AR/VR, and geospatial tasks [27]. These models proficiently manage APIs, UIWeb interfaces, operating systems, apps, and SQL backends [17]. However, this expanded functionality increasingly strains the entire system stack, from cloud endpoints to local execution devices, introducing substantial hardware overhead [10], yet offering numerous opportunities to improve metrics such as latency, memory usage, and energy efficiency [42]. The growing necessity for system-level optimizations signifies a paradigm shift toward system-oriented designs in the development of Copilots, a shift that is increasingly recognized as essential by our systems engineering community [16].

A novel advancement enabling impressive Copilot performance is robust reasoning through compositional and sequential prompting, with techniques such as Chain-of-Thought [36], ReAct [41], and Tree-of-Thought [40] enhancing function-calling benchmarks substantially. However, their inclination to decompose tasks into single steps [25], each necessitating a separate GPT OpenAI API call, introduces significant bottlenecks, thereby increasing token costs
and system latency (Figure 1, top). Recent developments in parallel function calling aim at selecting and executing multiple tools within a single API response (Figure 1, right) [31]. Notably, OpenAI's introduction of that very feature in their recent GPT Turbo release exemplifies this advancement ${ }^{1}$. Despite improved performance, these methods still frequently require nuanced prompting to effectively guide the agent on how to group functions. For instance, few-shot techniques that provide in-context previous examples of multi-tool execution that encourage effective function selection often necessitate complex, RAG-based [21] or intent-based [10] dynamic prompting schemes.

A key insight in our work is that in realistic LLM workloads, data often exhibits significant temporal similarity, with sequences of similar tools typically executed in succession. Consider as an example an LLM-enabled geospatial analysis platform in which a user may succinctly ask, "Show me satellite images around Newark, NJ from October." This query naturally involves filtering data first by location and then by date. If the LLM could interpret these sequential tasks as a single function - though the system still processes them with their respective code internally - the agent would, in essence, select one integrated tool. This approach inherently reduces the frequency of multiple single-tool API calls, streamlining the process and enhancing efficiency.

Drawing inspiration from hardware systems, akin to the principles of the multiply-add (MAD) operation that fuses different arithmetic steps into a single task from the compiler's perspective, we introduce a GPT-driven tool-compilation scheme, namely LLM-Tool Compiler. Prior to any tool execution, our compiler identifies groups of needed tools and compiles them into fused operations. Without modifying the function-calling API, tool selection proceeds as usual, but with a significant enhancement: at the LLM level, the agent is presented with and selects from an updated and more granular list of functions, thereby inherently achieving a higher parallelization rate in function calling. Upon selection, the compiler then "maps" the fused operations back to their corresponding code per tool. Through extensive evaluation on a state-of-the-art geospatial Copilot framework [34], we demonstrate that our approach enhances LLM performance, reducing token costs and latency by $12 \%$ and improving parallelization by up to four times across various LLM models and prompting techniques.

## 2 RELATED WORK

The nascent landscape of LLM optimization strategies encompasses a diverse array of approaches, ranging from model-level improvements $[2,6,11,15,18,23]$ to system-level enhancements [8, 19, 30]. As token length directly correlates with system overhead, significant advancements in LLM efficiency have been achieved through input compression by reducing the cost associated with longer prompts. Methods such as DYNAICL [44], Selective Context [22], and LLMLingua [12] dynamically remove non-essential elements[^0]

from prompts, streamlining input without compromising information quality. RECOMP [37] and SemanticCompression [9] condense prompts into concise summaries that maintain essential semantic content. All these approaches are orthogonal to our approach and could be flexibly integrated into the LLM-Tool Compiler, as we focus on the tool-calling agent level.

Recent strategies have been developed to enhance the decoding efficiency of LLMs, such as InContext Learning (ICL)[7] and Chainof-Thought (CoT)[36], which incorporate multiple examples and reasoning steps within prompts. Moreover, methods like RAG [21], FLARE [5], and intent-based function-calling [10] streamline tool selection for LLMs, leading to significant cost reductions. While these advancements are crucial in optimizing agent efficiency, they have primarily focused on agent performance metrics and only indirectly on hardware-related optimizations. Our work shares this motivation but also explicitly profiles and reports hardware metrics, including system latency and token cost, on top of a largescale cloud Copilot framework with hundreds of GPT endpoints, terabytes of data, and long-horizon complex tasks. This provides a more comprehensive approach that reflects industry-scale hardware and real-world applications.

Highlighting the growing demand for more efficient agents, OpenAI introduced parallel function calling support as part of their latest GPT release (Turbo) setting state-of-the-art performance [38]. Our results show that our LLM-Tool Compiler further improves upon this strong baseline. Skeleton-of-Thought (SoT)[29] and its enhancements like SGD[13] and APAR [24] restructure the output content into a manageable format conducive to batch processing and parallel decoding, significantly reducing generation latency while enriching the diversity and quality of responses. SGLang [43] introduces a domain-specific programming language tailored for LLM operations, promoting batch inference and efficient resource sharing. However, these approaches often assume embarrassingly parallel or inherently interdependent tasks, as highlighted by the authors in [16]. Concurrently with our submission, LLMCompi ler [16] introduced a compiler-based methodology for parallel functionlevel tool selection. However, their approach is meant more as an end-to-end prompting solution to replace existing schemes like ReAct, hence necessitating extensive user-specified prompting. In contrast, our framework operates directly at the toolset level and is nearly agnostic to the prompting scheme, allowing us to reuse the existing reasoning logic, thus minimizing engineering overhead.

## 3 METHODOLOGY

To illustrate the intuition behind the LLM-Tool Compiler, consider the task depicted in Figure 1, where a user asks, "Show me airplanes at NYC from October 2023 on xview1 and FAIR1M images." In the fully sequential baseline, this query is completed with the agent calling and executing one tool per step, i.e., per GPT API call (Figure 1 , top), where it is evident that several tools could be executed simultaneously. Indeed, in this particular example, OpenAI's latest release achieves parallelization by executing the load operations simultaneously. However, there is considerable potential for further improvement, especially for the subsequent data operations: in fact, as we discuss later in our Results, only a quarter of data filtering operations are currently executed in parallel by the GPT baseline.

We hypothesize that the limited parallelization of data operations may be partly due to the high diversity among 30 different data tools. Although each function definition is available to the agent, nuanced interdependencies among these tools might not be apparent. One potential approach could involve employing overly detailed prompting instructions and meticulously crafted tool-specific directions However, we consider this approach impractical for real-world, large-scale Copilot systems [17], where updating production code to reflect new tool aspects and interdependencies would be cumbersome. In fact, as we observe in practice, even more detailed prompting instructions to consider more data operations in a single step do not ensure that the agent will execute multi-tool operations, as reported by experiences from other practitioners [32].

The critical question is how to dynamically determine, for each new user prompt, which tasks to parallelize and orchestrate their execution without significantly altering the preexisting functioncalling process. To achieve this, we propose decoupling the "compilation" step as much as possible in an agent-agnostic manner, by introducing a dedicated GPT-driven module. Before any tool execution and given the current query and the full API tool-set, this module identifies groupings of similar tools that can be invoked together. This design choice is deliberate, as it confines the prompting effort and tool compilation within that module as a separate function call, without replacing or overly modifying existing agent baselines. As depicted in Figure 1 (bottom), the LLM-Tool Compiler accomplishes this through two primary components: (i) a fuser that identifies relevant tools and aggregates them into a single function, dynamically updating the tool-set visible to the LLM agent; and (ii) an executor that, during the subsequent function-calling process, monitors the tool selection by the agent. If a fused operation is selected, the executor is responsible for decomposing and executing their respective code blocks and dependencies:

Fuser. This lightweight, GPT-driven module runs prior to the main agent API calls and is responsible for identifying and assembling groups of tools that can be fused into more granular tasks. To enable this, we leverage user-supplied predefined prompts that instruct the module on which tools are associated with different types of tasks via intent-based in-context prompting [10]. Specifically, we adapt intent-based instructions to consider the aspects for parallel function calling, where we also guide how various groups of tasks can be parallelized. Referring to our earlier example query of load-filter-plot from Figure 1, this involves clarifying to the fuser that certain filtering operations (e.g., by date or coordinates) are candidates for grouping. We provide an example system prompt, abbreviated for brevity.

The fuser module invokes the GPT API with the user query, the system prompt, and the complete list of API tool descriptions. We force the GPT API to return a predefined JSON schema that contains the tool groupings needed to answer the user query. This involves calling the GPT API using the tool_choice: "none" API flag. Based on the GPT output message (in JSON format), we perform a sanity check to ensure that (i) the selected groupings correspond to existing function names, and (ii) they are associated with the correct tool types via a simple lookup table. Subsequently, for each fused list, we programmatically aggregate the tool definitions into unified tasks, which includes all variable names, descriptions, etc. This step simply consolidates all variables (i.e., name, description, parameters entries in function-call API) for each fused tool into unified arguments lists. Each fused tool is then incorporated into the tool-set in place of the sublist of tools it replaces. From the agent's perspective, this process is seamless, as it only interacts with an updated tool set.

Fuser prompt via intent-based in-context examples

As a Compiler with access to tools for geospatial tasks [..]

Intents: 1. load->filter->plot; 2. UI/web; 3. Doc retrieval

Given the examples below and *without* solving for the query, reason about which tools will be *likely* needed to complete the task, then compile lists of tools for each subset of similar ops (e.g., filter, load, map, doc ops) without calling any arguments.

User Query: \{question\}

In-context examples:

Example 1: Plot on the map the fmow images in Europe from June 2012 Thought 1: The user is asking for images of a specific dataset, so I need to load [..]

Thought 2: [..] images for a month (therefore date range) and a continent (therefore location)

"Action": To complete the task I would call [..]

- load_db(..load the fmow images..)
- filter_loc(..filter in Europe...)
- filter_date(..filter from June 2012..)

Answer:

Example 2: $[.$.

一

Answer the format of json: \{'load_ops': [..], 'filter_ops': $[.],. \ldots\}$. Think step by step

Function calling. It is critical to note that, aside from the fuser initial step, there are no further changes to the function-calling process and the reasoning scheme. The system prompts, underlying tool definitions, and prompting strategies remain unchanged, as the only modification involves aggregating groups of tools into more granular multi-tool function choices. In contrast to compiler methods that directly replace tool-selection schemes like ReAct [16], our method can be added on top of any technique. This compatibility is demonstrated in our results, which apply to both zero and fewshot paradigms for Chain-of-Thought and React. To illustrate, Step 3 in Figure 1 shows that following the same query example as before, the GPT agent sees an updated tool-set, but the function definitions are the original ones, allowing it to select the consolidated filterby-date-location tool to complete the task.

Executor. The executor module actively monitors the GPT tool selection output. When a fused function name is selected, it is responsible for de-fusing the operation into its constituent subtasks. Referring back to the example in Figure 1, if the agent selects the granular tool 'filter-by-date-by-location,' the executor seamlessly manages the execution of both 'filter-by-date' and 'filter-bylocation.' This operation is programmatically determined based on the tool definitions. For independent tasks, such as load operations, the executor executes them concurrently; for tasks that modify the same dataset - like sequentially filtering by dates and then by location - it processes them sequentially. As demonstrated in our ablation studies, the primary speed-up results from the reduction

Table 1: System and agent performance for GPT-3.5 Turbo and GPT-4 Turbo and various prompting techniques.

| Model | Compiler | Success <br> Rate $(\%) \uparrow$ | Avg Tokens <br> /Task $\downarrow$ | Token <br> Reduction $\uparrow$ | Avg Time <br> $/$ Task (s) $\downarrow$ | Speedup $\uparrow$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| GPT-3.5 Turbo (0125) |  |  |  |  |  |  |
| CoT - Zero-Shot | $\dagger$ Baseline: In-context | $65.73 \pm 0.45$ | $25.40 \pm 0.02$ | - | $8.11 \pm 0.01$ | - |
|  | †Baseline: In-context + OAI Parallel | $65.93 \pm 0.49$ | $25.15 \pm 0.02$ | $1.01 \times$ | $7.44 \pm 0.01$ | $1.09 \times$ |
|  | LLM-Tool Compiler | $58.59 \pm 0.57$ | $21.17 \pm 0.01$ | $1.20 \times$ | $5.84 \pm 0.00$ | $1.39 \times$ |
| CoT - Few-Shot | Baseline: In-context | $67.87 \pm 0.20$ | $30.36 \pm 0.01$ | - | $8.09 \pm 0.00$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-4.jpg?height=42&width=148&top_left_y=624&top_left_x=1726) |
|  | †Baseline: In-context + OAI Parallel | $67.17 \pm 0.57$ | $30.22 \pm 0.02$ | $1.00 \times$ | $7.49 \pm 0.01$ | $1.08 \times$ |
|  | LLM-Tool Compiler | $59.55 \pm 0.20$ | $25.19 \pm 0.01$ | $1.21 \times$ | $5.97 \pm 0.01$ | $1.36 \times$ |
|  | Baseline: In-context | $64.81 \pm 0.63$ | $27.52 \pm 0.14$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-4.jpg?height=43&width=182&top_left_y=742&top_left_x=1378) | $8.13 \pm 0.02$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-4.jpg?height=43&width=148&top_left_y=742&top_left_x=1726) |
| ReAct - Zero-Shot | † Baseline: In-context + OAI Parallel | $64.98 \pm 0.20$ | $23.92 \pm 0.09$ | $1.15 \times$ | $6.97 \pm 0.02$ | $1.17 \times$ |
|  | LLM-Tool Compiler | $59.33 \pm 0.67$ | $21.06 \pm 0.02$ | $1.31 \times$ | $5.12 \pm 0.02$ | $1.59 \times$ |
|  | Baseline: In-context | $73.59 \pm 0.13$ | $39.44 \pm 0.01$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-4.jpg?height=43&width=182&top_left_y=860&top_left_x=1378) | $8.19 \pm 0.01$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-4.jpg?height=43&width=148&top_left_y=860&top_left_x=1726) |
| ReAct - Few-Shot | †Baseline: In-context + OAI Parallel | $73.20 \pm 0.19$ | $35.24 \pm 0.05$ | $1.12 \times$ | $7.15 \pm 0.01$ | $1.15 \times$ |
|  | LLM-Tool Compiler | $66.12 \pm 1.05$ | $29.29 \pm 0.12$ | $1.35 \times$ | $5.30 \pm 0.04$ | $1.54 \times$ |
| GPT-4 Turbo (0125) |  |  |  |  |  |  |
| CoT - Zero-Shot | †Baseline: In-context | $84.39 \pm 0.14$ | $29.52 \pm 0.01$ | - | $8.98 \pm 0.00$ | - |
|  | †Baseline: In-context + OAI Parallel | $84.73 \pm 0.21$ | $24.49 \pm 0.03$ | $1.21 \times$ | $8.44 \pm 0.00$ | $1.06 \times$ |
|  | LLM-Tool Compiler | $84.15 \pm 0.20$ | $21.30 \pm 0.01$ | $1.39 \times$ | $7.40 \pm 0.00$ | $1.21 \times$ |
|  | Baseline: In-context | $83.15 \pm 0.22$ | $33.75 \pm 0.04$ | - | $8.83 \pm 0.02$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-4.jpg?height=42&width=148&top_left_y=1153&top_left_x=1726) |
| CoT - Few-Shot | †Baseline: In-context + OAI Parallel | $83.05 \pm 0.46$ | $26.50 \pm 0.00$ | $1.27 \times$ | $8.19 \pm 0.03$ | $1.08 \times$ |
|  | LLM-Tool Compiler | $82.51 \pm 0.03$ | $23.89 \pm 0.00$ | $1.41 \times$ | $7.40 \pm 0.00$ | $1.19 \times$ |
|  | Baseline: In-context | $82.02 \pm 0.27$ | $32.55 \pm 0.00$ | - | $8.97 \pm 0.00$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-4.jpg?height=43&width=148&top_left_y=1271&top_left_x=1726) |
| ReAct - Zero-Shot | †Baseline: In-context + OAI Parallel | $82.31 \pm 1.04$ | $27.50 \pm 0.00$ | $1.18 \times$ | $8.42 \pm 0.00$ | $1.06 \times$ |
|  | LLM-Tool Compiler | $81.68 \pm 0.31$ | $24.35 \pm 0.01$ | $1.34 \times$ | $7.38 \pm 0.00$ | $1.21 \times$ |
|  | Baseline: In-context | $84.26 \pm 0.29$ | $39.18 \pm 0.01$ | - | $8.94 \pm 0.00$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-4.jpg?height=43&width=148&top_left_y=1389&top_left_x=1726) |
| ReAct - Few-Shot | †Baseline: In-context + OAI Parallel | $84.29 \pm 0.42$ | $33.72 \pm 0.01$ | $1.16 \times$ | $8.41 \pm 0.01$ | $1.06 \times$ |
|  | LLM-Tool Compiler | $82.68 \pm 0.27$ | $29.36 \pm 0.01$ | $1.33 \times$ | $7.38 \pm 0.00$ | $1.21 \times$ |

in the number of steps through multi-tool groupings, rather than from faster execution of individual steps.

Error handling. Given that the fuser is a fully GPT-driven module, our method handles error cases at two levels. Initially, the fuser is instructed to return the JSON of fused tasks only when confident; otherwise, it provides an empty list, in which case the toolset remains unchanged and the agent proceeds as usual. Moreover, the agent is prompted to restart if it encounters issues during the process after a few failed attempts. Upon reset, we bypass the fuser API call and repeat the operation with the full toolset. This dynamic adaptability is crucial for rectifying inaccuracies in tool selection and enables the system to address issues in real-time.

## 4 RESULTS

Experimental Setup. We assess the LLM-Tool Compiler on the GeoLLM-Engine benchmark [34], highlighting the effectiveness of our methodology on complex long-horizon many-tool remote sensing (RS) applications and satellite imagery. The benchmark, along with its associated engine, serves as a real-world testbed equipped with a comprehensive suite of open-source APIs, dynamic map/web UIs, as it supports various Python packages tailored for data analytics and geospatial tasks, including RAG and vector stores, Mapbox
APIs for interactive mapping, and GeoPandas for geospatial data visualization and manipulation [33]. Designed to handle LLMs across hundreds of API tools, the platform leverages a vast repository of geospatial data from prominent RS LLM benchmarks, encompassing over 5 million satellite images and hundreds of thousands of tasks meticulously organized into SQL tables for efficient querying and manipulation. This setup provides an ideal environment for implementing and evaluating the LLM-Tool Compiler.

Metrics. For LLM performance, we follow established agent evaluation practices [26, 45] and we compute the Success Rate (the proportion of tasks successfully completed across the benchmark), the Correctness Ratio (the proportion of correct function-call operations, noting that a wrong tool call might not prevent the agent from successfully completing the task), and the ROUGE-L score. We also report performance with respect to the underlying remote sensing tasks in the GeoLLM-Engine benchmark, with F1 and recall for object detection and land coverage classification (LCC), respectively, alongside ROUGE for visual question answering (VQA). In terms of system costs, we compute the average number of tokens per task and the average time per API tool call. To capture the total latency of serving API requests in cloud deployments, we maintain a running average across multiple GPTs, discarding outliers more

Table 2: Agent performance metrics for LLM-Tool Compiler compared to baselines across various models and prompting techniques. First, we improve on the previously reported baselines in [34]. Using the original prompting, we frequently observe incorrect function calls yet correct underlying logic (Section 4.1). We incorporated in-context prompting to improve agent performance as much as possible, denoted as ${ }^{\dagger}$ Baseline. Baseline corresponds to the original results [34] without our prompting.

| Model | Compiler | Success <br> Rate $(\%) \uparrow$ | Correctness <br> Rate (\%) $\uparrow$ | Obj. Det <br> F1 (\%) $\uparrow$ | LCC <br> R $\% \uparrow$ | VQA <br> Rouge-L (\%) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| GPT-3.5 Turbo (0125) |  |  |  |  |  |  |
| CoT - Zero-Shot | Baseline [34] | 64.66 | 40.20 | 54.99 | 93.28 | 54.40 |
|  | ${ }^{\dagger}$ Baseline: In-context | $65.73 \pm 0.45$ | $69.22 \pm 0.34$ | $59.94 \pm 7.55$ | $96.73 \pm 4.85$ | $54.55 \pm 0.07$ |
|  | †Baseline: In-context + OAI Parallel | $65.93 \pm 0.49$ | $69.68 \pm 0.18$ | $55.22 \pm 24.98$ | $93.28 \pm 26.61$ | $54.50 \pm 0.16$ |
|  | ![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-5.jpg?height=74&width=474&top_left_y=737&top_left_x=520) | $\underset{-58.59 \pm 0.57}{68.45}$ | $-66.88 \pm 0.54$ | $\frac{60.45 \pm 6.31}{7 \overline{3} .81}-$ | $-\frac{79.04 \pm 3.34}{98.39}-$ | $52.35 \pm 0.07$ |
| CoT - Few-Shot | $\dagger$ Baseline: In-context | $67.87 \pm 0.20$ | $77.83 \pm 0.13$ | $70.46 \pm 13.29$ | $96.60 \pm 1.35$ | $57.55 \pm 0.02$ |
|  | †Baseline: In-context + OAI Parallel | $67.17 \pm 0.57$ | $77.61 \pm 0.04$ | $73.23 \pm 1.91$ | $96.29 \pm 0.32$ | $57.64 \pm 0.06$ |
|  | LLM-Tool Compiler | $59.55 \pm 0.20$ | $77.96 \pm 0.06$ | $59.81 \pm 16.52$ | $80.81 \pm 0.80$ | $54.65 \pm 0.07$ |
|  | Baseline $[3 \overline{4}]$ | $5 \overline{1} \overline{56}$ | $\overline{62.06}-\bar{l}$ | $5 \overline{4} \overline{16}$ | $\overline{92.57}$ | $\overline{5} \overline{6} \overline{4} \overline{5}$ |
| React - Zero-Shot | $\dagger$ Baseline: In-context | $64.81 \pm 0.63$ | $56.53 \pm 1.90$ | $58.60 \pm 34.94$ | $94.79 \pm 7.06$ | $60.67 \pm 0.38$ |
|  | †Baseline: In-context + OAI Parallel | $64.98 \pm 0.20$ | $58.04 \pm 2.30$ | $62.17 \pm 55.45$ | $96.37 \pm 8.51$ | $61.27 \pm 0.19$ |
|  | LLM-Tool Compiler | $59.33 \pm 0.67$ | $50.64 \pm 4.93$ | $51.86 \pm 62.10$ | $74.24 \pm 9.76$ | $57.46 \pm 0.14$ |
|  | - Baseline [3 $\overline{4}]-$ | $--7 \overline{7} . \overline{47}-$ | $-\overline{68 .} \overline{42}--$ | $-7 \overline{5} . \overline{01}-$ | $\overline{97.45}$ | $\overline{65.2 \overline{6}}$ |
| React - Few-Shot | $\dagger$ 'Baseline: In-context | $73.59 \pm 0.13$ | $80.15 \pm 0.34$ | $67.84 \pm 11.52$ | $97.34 \pm 4.13$ | $65.57 \pm 0.02$ |
|  | †Baseline: In-context + OAI Parallel | $73.20 \pm 0.19$ | $79.65 \pm 0.16$ | $70.04 \pm 19.57$ | $97.51 \pm 0.49$ | $65.32 \pm 0.01$ |
|  | LLM-Tool Compiler | $66.12 \pm 1.05$ | $73.63 \pm 1.65$ | $55.76 \pm 15.16$ | $86.82 \pm 5.97$ | $61.49 \pm 0.35$ |
| GPT-4 Turbo (0125) |  |  |  |  |  |  |
| CoT - Zero-Shot | Baseline [34] | 77.35 | 80.88 | 87.99 | 96.56 | 65.29 |
|  | $\dagger$ Baseline: In-context | $84.39 \pm 0.14$ | $93.25 \pm 0.09$ | $80.60 \pm 0.14$ | $95.98 \pm 0.73$ | $63.79 \pm 0.08$ |
|  | †Baseline: In-context+ OAI Parallel | $84.73 \pm 0.21$ | $93.37 \pm 0.05$ | $80.98 \pm 0.07$ | $95.67 \pm 1.43$ | $63.86 \pm 0.11$ |
|  | LLM-Tool Compiler | $84.15 \pm 0.20$ | $93.84 \pm 0.17$ | $74.16 \pm 0.08$ | $96.68 \pm 3.20$ | $61.37 \pm 0.08$ |
|  | Baseline $[3 \overline{4}]$ | -80.00 | $\overline{84.01}-\overline{-}$ | $-8 \overline{88.40} \overline{--}$ | $\overline{9} 9.8 \overline{9}$ | $\overline{67.65}$ |
| CoT - Few-Shot | $\dagger$ Baseline: In-context | $83.15 \pm 0.22$ | $93.45 \pm 0.14$ | $75.04 \pm 22.23$ | $97.53 \pm 0.51$ | $61.80 \pm 0.06$ |
|  | †Baseline: In-context + OAI Parallel | $83.05 \pm 0.46$ | $93.30 \pm 0.05$ | $74.21 \pm 23.21$ | $98.17 \pm 0.32$ | $62.16 \pm 0.17$ |
|  | LLM-Tool Compiler | $82.51 \pm 0.03$ | $94.60 \pm 0.07$ | $75.48 \pm 9.71$ | $95.51 \pm 0.88$ | $59.87 \pm 0.17$ |
|  | Baseline $[3 \overline{4}]$ | $80 . \overline{03}$ | $\overline{84.27}$ | $8 \overline{9} .34$ | $\overline{98.8} \overline{3}$ | $\overline{68.1} \overline{1}$ |
| React - Zero-Shot | $\dagger$ Baseline: In-context | $82.02 \pm 0.27$ | $92.61 \pm 0.06$ | $79.16 \pm 24.45$ | $96.23 \pm 0.75$ | $65.03 \pm 0.07$ |
|  | ${ }^{\dagger}$ Baseline: In-context + OAI Parallel | $82.31 \pm 1.04$ | $92.69 \pm 0.03$ | $81.26 \pm 0.05$ | $95.66 \pm 0.74$ | $65.07 \pm 0.02$ |
|  | LLM-Tool Compiler | $81.68 \pm 0.31$ | $93.09 \pm 0.07$ | $72.92 \pm 1.76$ | $95.41 \pm 1.56$ | $64.44 \pm 0.03$ |
|  | Baseline $[3 \overline{4}]$ | $8 \overline{1} . \overline{11}$ | $84 . \overline{3}-$ | $8 \overline{3} . \overline{85}$ | $\overline{99.63}$ | $\overline{69.3 \overline{7}}$ |
| React - Few-Shot | †Baseline: In-context | $84.26 \pm 0.29$ | $92.74 \pm 0.03$ | $75.98 \pm 12.74$ | $95.49 \pm 8.17$ | $66.54 \pm 0.04$ |
|  | ${ }^{\dagger}$ Baseline: In-context + OAI Parallel | $84.29 \pm 0.42$ | $92.45 \pm 0.02$ | $80.20 \pm 6.52$ | $96.91 \pm 1.79$ | $66.34 \pm 0.04$ |
|  | LLM-Tool Compiler | $82.68 \pm 0.27$ | $94.11 \pm 0.02$ | $76.77 \pm 10.34$ | $95.59 \pm 1.72$ | $65.48 \pm 0.02$ |

than two standard deviations from the mean to ensure representative latency measurements. For compiler performance, we calculate the parallelization rate as the average number of tools being called "in parallel" per API call.

### 4.1 Main results

Baselines. We summarize agent peformance and our extensinve analysis in Tables 2 and 1. Across all our main experiments, we run on the latest GPT releases (Turbo) with OpenAI's function calling API activated for both GPT-3.5 and GPT-4 Turbo (0125) versions. These releases are the top-performing OpenAI models in terms of function calling [38], particularly for parallel execution. We consider two widely established prompting techniques, namely Chainof-Thought (CoT) [36] and ReAct [39, 41]. While there are existing results previously reported on the GeoLLM-Engine benchmark for these methods [34], we aim to achieve the strongest baselines to the best of our abilities.

To this end, we first investigate the original prompting from [34] and observe partially incorrect function calls yet correct underlying logic (e.g., filter_loc calls with arguments corresponding to the correct area but with latitude-longitude values not covering a wide enough range hence failing to provide good recall for satellitebased object detections). Recognizing room for improvement, we

Table 3: Benchmark parallelization analysis: Evaluating the parallelization rate (\%) achieved per method with respect to the overall number of parallelizable function calls.

| Model | Compiler | Parallelization - Load Ops (\%) |  | Parallelization - Filter Ops (\%) |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | GPT-3.5 | GPT-4 | GPT-3.5 | GPT-4 |
| CoT - Zero-Shot | aseline: In-context + OAI F | $=0.08$ | $99.85 \pm 0.04$ | $0.74 \pm 0.10$ | $17.04 \pm 1.79$ |
|  | LLM-Tool Com | $0.46 \pm 0.09$ | $99.85 \pm 0.04$ | $94.73 \pm 0.18$ | $97.02 \pm 0.22$ |
| CoT - Few-Shot <br> ------- <br> ReAct - Zero-Shot <br> ------- | Baseline: In-context + $\overline{\mathrm{OAI}}$ Parallel | $2.84 \pm 0.11$ | $99.00 \pm 0.02$ | $0.25 \pm 0.01$ | $36.01 \pm 0.45$ |
|  | LLM-Tool Compiler | $4.87 \pm 1.12$ | $99.90 \pm 0.02$ | $97.68 \pm 0.15$ | $99.41 \pm 0.04$ |
|  | Baseline: In-context + $\bar{O} \bar{A} \bar{A} \bar{P}$ arallel | $42.39 \pm 2.91$ | $99.00 \pm 0.02$ | $15.25 \pm 1 . \overline{69}$ | $25.59 \pm 0.80$ |
|  | LLM-Tool Compiler | $40.00 \pm 3.06$ | $99.95 \pm$ | $79.68 \pm 2.00$ | $97.12 \pm 0.18$ |
| ReAct - Few-Shot | Baseline: In-context + OAII Parallel | ![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-6.jpg?height=49&width=154&top_left_y=741&top_left_x=1031) | $99.95=$ | $16.38 \pm 1.36$ | $21.33 \pm 0.96$ |
|  | LLM-Tool Compiler | $51.93 \pm 4.52$ | $99.75 \pm 0.05$ | $93.30 \pm 1.74$ | $97.56 \pm 0.27$ |
|  | Oracle | 1971 Qs) |  | $100.00 \%$ (4039 Qs) |  |

Table 4: Breaking down the improvement for average time per task. We ablate the LLM-Tool Compiler results with and without concurrent tool execution, i.e., executing tools simultaneously as concurrent system processes.

|  | Model | Fused <br> Only | Fused + <br> Concurrent |
| :---: | :---: | :---: | :---: |
| GPT-3.5 | CoT Zero-Shot | $16.60 \%$ | $28.03 \%$ |
|  | CoT - Few-Shot | $14.35 \%$ | $26.27 \%$ |
|  | ReAct - Zero-Shot | $22.50 \%$ | $37.05 \%$ |
|  | ReAct - Few-Shot | $21.63 \%$ | $35.25 \%$ |
| $\boldsymbol{G P T - 4}$ | CoT - Zero-Shot | $8.65 \%$ | $17.53 \%$ |
|  | CoT - Few-Shot | $7.33 \%$ | $16.19 \%$ |
|  | ReAct - Zero-Shot | $9.01 \%$ | $17.67 \%$ |
|  | ReAct - Few-Shot | $8.74 \%$ | $17.45 \%$ |

employed enhanced in-context prompting to improve agent performance as much as possible, denoting this as "† Baseline." Table 2 shows that all our ${ }^{\dagger}$ Baseline versions improve upon the original results reported in [34](Baseline) across all models and metrics of interest, with success rates increasing by as much as $13 \%$. Therefore, it is important to note that instead of comparing to a rudimentary sequential ReAct baseline [16] assuming previously suboptimal metrics, we ensure that LLM-Tool Compiler is benchmarked against already highly competitive baselines.

Improving agent effiency. We present the average token per task and average time per task in Table 1. The Token Reduction and Speedup compared to the baseline are presented in separate columns in addition to the Success Rate. We observe that with negligible deduction in performance, LLM-ToolCompiler is able to reduce token cost by $1.3 \times$ to $1.4 \times$ in case of GPT-4 and $1.2 \times$ to $1.35 \times$ in case of GPT-3.5. This is consistently higher than OAI Parallel function calling which achieves token reduction ranging from $1.16 \times$ to $1.27 \times$ in case of GPT- 4 and $1.01 \times$ to $1.15 \times$ in case of GPT-3.5. Interestingly, even with strong in-context prompting, OAI Parallel function calling is still able to provide some speedup for the tasks. The performance degradation for LLM-ToolCompiler is much less for GPT-4 (less that 1\%) as compared GPT-3.5 highlighting its ability for complex reasoning. We also observe that the speedup for LLM-ToolCompiler in terms of average time per task is higher than OAI Parallel function calling consistently. Interestingly even though token reduction in case of GPT-3.5 is relatively less than GPT-4, the speedup for the former is higher, this can be attributed majorly to low latency for GPT-3.5 API compared to GPT-4. There is no fixed pattern for improvement for different prompting techniques - CoT, ReAct or Few-Shot suggesting the improvement is majorly affected by the model being used.

Agent performance. We present the performance related metrics in Table 2. Despite the considerable improvements in latency discussed earlier, we observe only minor task-dependent degradation in agent performance. To provide a comprehensive overview, we include all metrics from the benchmark, and we find that our performance is well within the variance compared to OpenAl's parallel-function calling. This degradation in performance is slightly more noticeable with GPT-3.5 than with GPT-4. Interestingly, when using GPT-4, most metrics show almost no degradation at all. In fact, we see improvement in some cases, especially for Correctness Rate. Compared to the baseline, the only two aspects exhibiting larger degradation are object detection (F1) and language metrics (Rouge-L), but these also display similar non-negligible drops for OpenAI's parallel-function calling. This is primarily attributed to underlying task variance - for instance, runs that might miss certain prompts, such as failing to fully complete a remote-sensing task with granularity across an entire continent, would disproportionately affect overall recall due to the data point-intensive outcome of the completion. As this limitation has already been discussed in recent RS benchmark analyses [33], we investigated this variance by repeating each method multiple times and confirmed wider variations for these metrics. Overall, any degradation observed remains within this variance range.

### 4.2 Ablation studies

Benchmark parallelization analysis. As previously mentioned and inspired by prior research [16], conventional methodologies for parallel function calling often presume tasks with strong interdependencies, or they evaluate their approaches on custom parallelizable
benchmarks (e.g., ParallelQA [16]). In contrast, we build upon existing benchmarks that do not come with specific assumptions about parallelization, aiming to reflect real-world scenarios on large-scale Copilot systems. In this regard, we conduct an oracle analysis to explore the underlying benchmark's potential for overall parallelization achievable by an ideal agent. This analysis, while extending beyond the scope of our current methodology assessment, offers insights akin to an LLM equivalent of Amdahl's law. Given that the majority of question types in GeoLLM-Engine involve operations like load operations (where loading different data sources can occur concurrently) and filtering operations (where different operations are suitable for multi-tool execution within a single API call), we focus this particular ablation on these. We define parallelization rate as the percentage of tasks that are parallelized compared to the total number of parallelizable tasks - that an oracle agent would do.

In Table 3, we present our findings. Firstly, the choice of GPT version has a notable impact on performance. GPT-4 exhibits superior baseline performance compared to earlier versions, indicating optimization for Copilot tasks. For load ops, GPT-4 achieves near $100 \%$ parallelization and for filter ops it achieves $>97 \%$. Secondly, employing prompting techniques, particularly transitioning from chain-of-thought to ReAct, enhances parallelization rates for both load and filter operations. This improvement is attributed to the reasoning process facilitated by Thought-Reason-Act and the integration of few-shot examples, enabling concurrent execution of load operations. However, we also observe substantial room for improvement in baseline approaches. By leveraging a fuser, our method inherently achieves higher parallelization rates, surpassing baseline approaches by four to five times.

![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-7.jpg?height=285&width=848&top_left_y=1424&top_left_x=172)

Figure 2: Distributions of the number of tool calls on the GeoLLM-Engine-5k for the baseline vs. LLM-Tool Compiler with GPT-4 ReAct - Zero-Shot prompting.

Tool calls per step. In this section we study the numnber of tools invoked per API call to GPT (step) to answer a user query. We compare the distributions of number of tools per step for $\mathrm{c}$ and OpenAI's parallel function calling baseline in Figure 2. We consider the GPT-4 ReAct - Zero-Shot prompting method both the parallel ${ }^{\dagger}$ Baseline and our LLM-Tool Compiler. The 0 -tools per call refers to the queries where no tools were required or when GPT returns the final answer after using required tools, as well as to the fuser executions since we do not execute any tools (Figure 2, right). We also see that the percentage of calls where 2 tools are used is significantly higher for LLM-Tool Compiler compared to OpenAl's parallel function calling ( $28 \%$ vs $13 \%$ ) and percentage of calls with 1 tool per step is reduced for LLM-Tool Compiler. This shows the effectiveness of LLM-Tool Compiler for parallelizing and invoking more tools per API call.
Table 5: Ablating how fuser performance (via different GPT version) affects the overall effectiveness of our approach.

| Model | fuser <br> version | Success <br> $\operatorname{Rt}(\%) \uparrow$ | Correct. <br> $\operatorname{Rt}(\%) \uparrow$ | Token <br> Rdc. $\uparrow$ | $\underset{\uparrow}{\text { Speedup }}$ |
| :---: | :---: | :---: | :---: | :---: | :---: |
| CoT ZS | $G P T-3.5$ | 72.31 | 86.40 | $1.47 \times$ | $1.32 \times$ |
|  | GPT-4 | 84.15 | 93.84 | $1.39 \times$ | $1.21 \times$ |
| CoT FS | $\overline{G P} \bar{T}-3.5$ | $\overline{70.15}$ | $8 \overline{6} . \overline{0}-\bar{s}$ | $\overline{1} .5 \overline{1} \bar{x}$ | $1 . \overline{30} x^{-}$ |
|  | GPT-4 | 82.51 | 94.60 | $1.41 \times$ | $1.19 \times$ |
| ReAct ZS | $G \bar{P} \bar{T}-3.5$ | $\overline{69.49}$ | 85.10 | $\overline{1.44} \bar{x}$ | $1.32 \times$ |
|  | GPT-4 | 81.68 | 93.09 | $1.34 \times$ | $1.21 \times$ |
| ReAct FS | $\overline{G P} \bar{T}-\overline{3} .5^{-}$ | $\overline{71 .} \overline{20}$ | $8 \overline{5} . \overline{3}$ | $\overline{1.4 \overline{2}} \bar{x}$ | $1 . \overline{3} \times x^{-}$ |
|  | GPT-4 | 82.68 | 94.11 | $1.33 \times$ | $1.21 \times$ |

Efficiency modes. In our methodology section, we clarified that the term 'parallel' as used in OpenAI documentation, referring to multi-tool execution, does not imply simultaneous execution. The primary advantage, therefore, might initially stem from fewer API round-trips, with concurrent tool execution during each step further reducing latency. This analysis aims to determine the individual contributions of each component to the overall speed-up. We conducted tests with different methods, deactivating concurrent execution in our executor module to isolate the effects. In Table 4, we compare these modes and observe that the fuser module alone is able to able to bring the majority of the speedup implying efficient parallelization of tools bringing majority of the total speedup achieved in most cases. As expected, the concurrent execution in addition to the fuser module also contributes a significant jump in speed, in many cases doubling the speedup.

Compiler GPT version. Our analysis highlights a notable performance disparity between different GPT versions, as detailed in Table 2. It is well-established that GPT-4 outperforms earlier versions in function-calling tasks $[33,34]$. However, the lower efficacy of our GPT-3.5 baselines may not solely stem from inherent limitations in function calls but also from sub-optimal tool selection by the fuser module. To explore this, we implemented a hybrid model for our analysis: while retaining GPT-4 for general API calls, we utilized GPT-3.5 exclusively for fuser operations. Results in Table 5 indicate that the performance of this hybrid model generally lies between the all-GPT-3.5 configuration and the all-GPT-4 setup, supporting our hypothesis that GPT-3.5's inferior function-calling capability is further exacerbated by compiler inefficiencies. As we extend our methodology to include open-source models like Llama 3[2], this ablation study provides valuable insights into potential heterogeneous agent configurations, presenting exciting opportunities for hardware-related optimizations.

Latency Analysis: Motivated by the well-established practice in our community of hardware modeling that accurately captures hardware performance across various system metrics and components [28], we are intrigued by the potential of such approaches in the context of LLMs, as also underscored in recent studies [42]. While it falls outside the immediate scope of our work, we would like to provide a preliminary analysis here, interested in studying the observation in [16], that early stopping in function-calling could complicate accurate latency measurements.
![](https://cdn.mathpix.com/cropped/2024_06_04_96a2e50433d376fd4be8g-8.jpg?height=288&width=854&top_left_y=278&top_left_x=169)

Figure 3: As a preliminary analysis, we capture whether a simple LUT-based modeling approach can capture overall agent latency. Shown below is the profiled and prediced average runtime per task across the different baselines considered in our experiments.

We investigate whether a straightforward model assumption can effectively capture runtime in our experimental setup. We measure the average time per API tool call and per tool execution using the Python time package. To accurately assess total latency for API requests in cloud deployments, we maintain a running average across various endpoints and tool operations, excluding outliers that are more than two standard deviations from the mean. For these tests, we deploy hundreds of isolated GPT instances specifically for this evaluation to avoid congestion and allow for representative endpoint response times.

We construct a simple Look-up-Table (LUT) model where we record the average runtime for each tool and key API components (e.g., fuser call, final-answer API call, etc.). Next, we randomly sample a fifth of the datapoints and rerun them, recording both the overall runtime and aggregating per-tool time based on the LUT entry. To normalize for the fact that different queries have different horizons, we divide the total prompt latency by the number of tools/calls in the agent solution path, and we report the predicted and actual runtime in Figure 3 for both GPT versions and prompting techniques. While beyond the scope of our current work, we do observe that the LUT still provides a good approximation. Using the MSE-based metric from [4], we observe that modeling error is $14.28 \%$ and $17.46 \%$ for the LUT-based heuristics. Of course, this is only a case-study, and the findings could be specific to the underlying platform. As part of our ongoing efforts and as we expand to local execution of open-source LLMs, we will be expanding this preliminary analysis to investigate where scalable and accurate runtime or power consumption modeling is feasible.

## 5 DISCUSSION AND FUTURE WORK

We acknowledge certain limitations in our present methodology. First, our system is implemented focusing on optimizing latency and bandwidth with extensive use of cloud endpoints. Building upon our initial findings, we aim to expand our analysis to local execution. Motivated by the relationship between runtime efficiency and hardware operational costs [3], we anticipate that LLM-Tool Compiler could yield further energy and power consumption efficiencies. Towards enabling local execution, we will explore GPT alternatives that can be run locally, such as the recently intoduced Llama-3 [2] and Phi-3.5 [1] architectures. Given that approach is implemented in a nearly prompting-agnostic manner, we expect to flexibly incorporate it across different computational environments.
On that note, we note other prompting schemes such as RAG [20] and demonstration-retrieval [35], which we plan to explore in future work beyond CoT and ReAct.

Moreover, while we have not reported on other benchmarks, we plan to extend our evaluation beyond the geospatial domain to a wider range of orthogonal tasks also considered in recent system-level LLM optimization papers [16]. Last, we are currently investigating improving the LLM-Tool Compiler methodology further. For instance, one limiting aspect in the considered benchmark is that the order of operations does not matter. As an intriguing research direction, we aim to investigate graph-based approaches [14] that capture dependencies and further optimize system performance.

## 6 CONCLUSION

In this work, we introduced LLM-Tool Compiler, a GPT-driven approach that dynamically identifies and fuses similar tool operations into unified tasks, inherently enhancing parallelization in function calling and reducing system latency. We demonstrated significant improvements over existing methods on a large-scale geospatial Copilot platform, achieving up to four times more parallel calls than existing methods while reducing token costs and latency by up to $40 \%$ and $12 \%$, respectively. Our approach remains compatible with various established prompting techniques, offering a nearly agnostic solution that minimizes engineering overhead. By explicitly profiling and reporting hardware metrics alongside agent performance, LLM-Tool Compiler provides a comprehensive solution towards enhancing LLM efficiency on real-word multi-tool agent workloads.

## REFERENCES

[1] Abdin M. et al. 2024. Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone. arXiv:2404.14219 [cs.CL]

[2] Meta AI. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. Meta AI Blog (2024). https://ai.meta.com/blog/meta-llama-3/

[3] Konstantinos Balaskas, Andreas Karatzas, Christos Sad, Kostas Siozios, Iraklis Anagnostopoulos, Georgios Zervakis, et al. 2024. Hardware-Aware DNN Compression via Diverse Pruning and Mixed-Precision Quantization. IEEE Transactions on Emerging Topics in Computing (2024).

[4] Ermao Cai, Da-Cheng Juan, Dimitrios Stamoulis, and Diana Marculescu. 2017. Neuralpower: Predict and deploy energy-efficient convolutional neural networks. In Asian Conference on Machine Learning. PMLR, 622-637.

[5] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting Language Models to Compress Contexts. arXiv:2305.14788 [cs.CL]

[6] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. 2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. arXiv:2306.03078 [cs.CL]

[7] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A Survey on In-context Learning. arXiv:2301.00234 [cs.CL]

[8] Hugging Face. 2024. Text Generation Inference: A Rust, Python, and gRPC server for text generation inference. https://github.com/huggingface/text-generationinference.

[9] Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, and Wei Han. 2023. Extending Context Window of Large Language Models via Semantic Compression. arXiv:2312.09571 [cs.CL]

[10] Michael Fore, Simranjit Singh, and Dimitrios Stamoulis. 2024. GeckOpt: LLM System Efficiency via Intent-Based Tool Selection. In GLSVLSI 2024

[11] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. arXiv:2210.17323 [cs.LG]

[12] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. arXiv:2310.05736 [cs.CL]

[13] Shuowei Jin, Yongji Wu, Haizhong Zheng, Qingzhao Zhang, Matthew Lentz, Z. Morley Mao, Atul Prakash, Feng Qian, and Danyang Zhuo. 2024. Adaptive Skeleton Graph Decoding. arXiv:2402.12280 [cs.CL]

14] Andreas Karatzas and Iraklis Anagnostopoulos. 2023. OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN Workload. In 2023 60th ACM/IEEE Design Automation Conference (DAC). IEEE, 1-6.

[15] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, and Kurt Keutzer. 2024. SqueezeLLM: Dense-and-Sparse Quantization. arXiv:2306.07629 [cs.CL]

[16] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, and Amir Gholami. 2024. An LLM Compiler for Parallel Function Calling. arXiv:2312.04511 [cs.CL]

[17] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. 2024. VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. arXiv:2401.13649 [cs.LG]

[18] Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. 2022. A Fast Post-Training Pruning Framework for Transformers. arXiv:2204.09656 [cs.CL]

[19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention arXiv:2309.06180 [cs.LG]

[20] LangChain Docs. 2024. Retrieval Augmented Generation (RAG). https://python. langchain.com/docs/modules/data_connection/. Accessed: May-2024.

[21] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv:2005.11401 [cs.CL]

[22] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. 2023. Compressing Context to Enhance Inference Efficiency of Large Language Models arXiv:2310.06201 [cs.CL]

[23] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration arXiv:2306.00978 [cs.CL]

[24] Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, and Yuxiao Dong. 2024. APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding. arXiv:2401.06761 [cs.CL]

[25] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. arXiv:2304.09842 [cs.CL]

[26] Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. 2024. TOFU: A Task of Fictitious Unlearning for LLMs. arXiv:2401.06121 [cs.LG]

[27] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Sasha Sax, and Aravind Rajeswaran. 2024 OpenEQA: Embodied Question Answering in the Era of Foundation Models. In Conference on Computer Vision and Pattern Recognition (CVPR).

[28] Diana Marculescu, Dimitrios Stamoulis, and Ermao Cai. 2018. Hardware-aware machine learning: Modeling and optimization. In 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD). IEEE, 1-8.

[29] Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. 2024. Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation arXiv:2307.15337 [cs.CL]

[30] NVIDIA. 2024. TensorRT-LLM: A TensorRT Toolbox for Optimized Large Language Model Inference. https://github.com/NVIDIA/TensorRT-LLM.

[31] OpenAI API Docs. 2024. Function Calling. https://platform.openai.com/docs/ guides/function-calling/parallel-function-calling/. Accessed: May-2024.

[32] OpenAI Developer community. 2024. Parallel Function Calling. https:// community.openai.com/t/parallel-function-calling/. Accessed: May-2024.

[33] Simranjit Singh, Michael Fore, and Dimitrios Stamoulis. 2024. Evaluating ToolAugmented Agents in Remote Sensing Platforms. In ICLR 2024 Workshop: 2nd Machine Learning for Remote Sensing Workshop.

[34] Simranjit Singh, Michael Fore, and Dimitrios Stamoulis. 2024. GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots. In CVPR 2024 Workshop EARTHVISION 2024.

[35] Venkat Krishna Srinivasan, Zhen Dong, Banghua Zhu, Brian Yu, Hanzi Mao, Damon Mosk-Aoyama, Kurt Keutzer, Jiantao Jiao, and Jian Zhang. 2023. NexusRaven: a commercially-permissive Language Model for function calling. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. https: //openreview.net/forum?id=Md6RUrGz67

[36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fe Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL]
[37] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation. arXiv:2310.04408 [cs.CL]

[38] Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Berkeley Function Calling Leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_ leaderboard.html.

[39] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action. arXiv:2303.11381 [cs.CV]

[40] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. arXiv:2305.10601 [cs.CL]

[41] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs.CL]

[42] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, and Kurt Keutzer. 2024. LLM Inference Unveiled: Survey and Roofline Model Insights. arXiv:2402.16363 [cs.CL]

[43] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2023. Efficiently Programming Large Language Models using SGLang. arXiv:2312.07104 [cs.AI]

[44] Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan. 2023. Efficient Prompting via Dynamic In-Context Learning. arXiv:2305.11170 [cs.CL]

[45] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023. ToolQA: A Dataset for LLM Question Answering with External Tools. arXiv:2306.13304 [cs.CL]


[^0]:    ${ }^{1}$ We use the term parallel in the same sense as the OpenAI API documentation, which refers to returning multiple tools per GPT call to "reduce round trips with the API" [31] This should not be confused with the actual execution of the tools, which may not run as parallel processes. To avoid ambiguity, we define concurrent as the simultaneous execution of multiple tools, while parallel specifically denotes the capability of a single GPT API call to return multiple tools.

