# InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews 

Xintao Wang ${ }^{1}$, Yunze Xiao ${ }^{2}$, Jen-tse Huang ${ }^{3}$, Siyu Yuan ${ }^{1}$, Rui Xu ${ }^{1}$, Haoran Guo ${ }^{4}$,<br>Quan Tu ${ }^{5}$, Yaying Fei ${ }^{6}$, Ziang Leng ${ }^{7}$, Wei Wang ${ }^{1}$, Jiangjie Chen ${ }^{1}$, Cheng Li $^{8}$, Yanghua Xiao ${ }^{* 1}$<br>${ }^{1}$ Fudan University $\quad{ }^{2}$ Carnegie Mellon University ${ }^{3}$ The Chinese University of Hong Kong ${ }^{4}$ RhineAI<br>${ }^{5}$ Renmin University of China $\quad{ }^{6}$ Beijing University of Technology $\quad{ }^{7}$ Boston University $\quad{ }^{8}$ SenseTime<br>\{xtwang21, syyuan21, ruixu21\}@m.fudan.edu.cn, \{jjchen19, weiwang1, shawyh\}@fudan.edu.cn<br>yunzex@andrew.cmu.edu, jthuang@cse.cuhk.edu.hk, chengli@sensetime.com


#### Abstract

Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of applications. However, a key challenge lies in assessing whether RPAs accurately reproduce the personas of target characters, namely their character fidelity. Existing methods mainly focus on the knowledge and linguistic patterns of characters. This paper, instead, introduces a novel perspective to evaluate the personality fidelity of RPAs with psychological scales. Overcoming drawbacks of previous self-report assessments on RPAs, we propose INCHARACTER, namely Interviewing Character agents for personality tests. Experiments include various types of RPAs and LLMs, covering 32 distinct characters on 14 widely used psychological scales. The results validate the effectiveness of INCHARACTER in measuring RPA personalities. Then, with InCharacter, we show that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the characters, achieving an accuracy up to $80.7 \%$. Our demo ${ }^{1}$, code, dataset, and results are publicly available at https: //github.com/Neph0s/InCharacter.


## 1 Introduction

Recent advancements in large language models (LLMs) have catalyzed the emergence of roleplaying agents (RPAs). RPAs are interactive AI systems simulating diverse roles or characters. RPA applications have been extended to diverse contexts, such as AI agents of fictional characters (Li et al., 2023), digital clones for humans (Gao et al., 2023), and AI non-player characters in video games (Wang et al., 2023b). Recent research trends[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-01.jpg?height=434&width=779&top_left_y=865&top_left_x=1047)

Figure 1: The procedure of personality tests on RPAs. To evaluate the personality fidelity of RPAs, we apply various scales to measure their personalities and compare the results with the personality labels of the characters.

have increasingly focused on the development of RPAs, including building RPAs for specific characters (Li et al., 2023; Wang et al., 2023c) and improving the role-playing abilities of foundation models (Zhou et al., 2023).

However, the evaluation of character fidelity in RPAs remains a relatively underexplored area. Prior research mainly concentrates on the replication of knowledge, experience, and linguistic patterns of characters (Shao et al., 2023a; Zhou et al., 2023), which manifests in two primary issues: (1) They necessitate character-specific datasets, thereby complicating the evaluation of new characters. (2) They overlook evaluating RPAs' thoughts and underlying mindsets. Towards these issues, we propose to evaluate if RPAs faithfully reproduce the personalities of target characters, i.e., personality fidelity, as depicted in Figure 1. Personality tests, administered by psychological scales, measure an individual's interrelated behavioral, cognitive, and emotional patterns (Barrick and Mount, 1991; Bem, 1981). By measuring the personalities of RPAs and comparing them with the personalities of the char-
acters, we can attain a more nuanced understanding of RPAs' character fidelity.

Prior studies on LLM personalities are mainly based on self-report scales, which prompt LLMs to select options or assign ratings to specific items (Tu et al., 2023; Huang et al., 2023b). However, this method suffers from several limitations for RPAs. (1) The instruction to complete scales contradicts role-playing instructions, leading to RPAs' reluctance or inability to engage with personality tests. (2) More importantly, the selected options may conflict with the actual behaviors of RPAs, making the test results unindicative of their true personalities. RPAs might underperform owing to an inadequate understanding of scale instructions and the biases inherent in the training data.

Therefore, we propose INCHARACTER, a novel approach to Interviews Character agents for personality tests. While self-report scales are popular in humans for their cost-effectiveness, interviewbased scales evaluated by experts offer a more comprehensive analysis (Uher et al., 2012; Rush et al., 1987). Self-reports are sometimes influenced by an individual's lack of insight, denial, or bias. In contrast, an interviewer can be a guide to elicit thoughts of individuals, effectively identifying and addressing the nuances via conversations to overcome the previously mentioned limitations. InCHARACTER employs this interview-based procedure (Trull et al., 1998) on RPAs, which includes two stages: (1) Interview: RPAs are engaged with open-ended questions derived from psychological scales to elicit RPAs' mindsets and behaviors. (2) Assessment: We utilize LLMs to interpret the responses collected from the first stage. This can involve converting the responses to Likert levels or using LLMs to simulate a psychiatrist's role in judging RPA personalities.

We apply InCHARACTER to various RPAs on 14 personality tests, including the Big Five Inventory (BFI), 16Personalities ${ }^{2}$ (16P), and Dark Triad Dirty Dozen (DTDD). The personality labels for the BFI and 16P are accessible through the Personality Database (PDb) ${ }^{3}$. Additionally, we engage human annotators familiar with the characters to label them on other scales, thereby creating a comprehensive benchmark for evaluating RPA personalities. Our experiments include various types of existing RPAs. The results demonstrate that the IN-[^1]

CHARACTER effectively simulates interview-based tests conducted by human experts and yields RPAs personality measurement better aligned with the characters compared to self-report methods.

The contributions of this paper are mainly threefold: 1) We introduce a novel aspect for RPA evaluation, i.e., personality fidelity, based on psychological scales. 2) We propose INCHARACTER, an interview-based framework for personality tests on RPAs and collect the first benchmark for RPA personality evaluation, facilitating future research on developing better RPAs. 3) Our experiments on various RPAs and psychological scales demonstrate the efficacy of INCHARACTER.

## 2 Preliminaries

### 2.1 Role-Playing Agents

Recent advancements have led to the emergence and evolution of several pivotal abilities in LLMs to facilitate the development of RPAs, including in-context learning (Brown et al., 2020), instruction following (Ouyang et al., 2022), step-by-step reasoning (Wei et al., 2022), and human-like traits such as empathy (Sorin et al., 2023). RPAs are interactive AI systems that act as assigned personas, from fictional characters to celebrities. RPAs utilize persona data to simulate characters, drawing from training datasets, prompted contexts, or external databases. Typically, existing work develops RPAs by setting character descriptions as system prompts (Zhou et al., 2023; Shao et al., 2023a) and crafting memory modules with character dialogues (Li et al., 2023; Wang et al., 2023c).

### 2.2 Psychological Scales

Usually rated on Likert levels, psychological scales are commonly used for personality tests. SelfreportSelf-report scales require participants to respond to a series of items analyzed through a specific scoring scheme to determine their personality traits. A scale rated on Likert levels, denoted as $\mathcal{L}=(\mathcal{P}, \mathcal{D}, \mathcal{O}, f)$, comprises a set of items $\mathcal{P}$ (i.e., a questionnaire), a list of dimensions $\mathcal{D}$, a set of response options $\mathcal{O}$, and a scoring scheme $f$. Each item $p \in \mathcal{P}$ is a statement or question, positively or negatively corresponding to a dimension $d \in \mathcal{D}$. For example, the item "Values artistic, aesthetic experiences." is positively related to the Openness dimension in the BFI. Participants select an ordinal response $o \in \mathcal{O}$ for each item, such as Agree. Typically, these options are numerically coded, e.g., " 1 "
![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-03.jpg?height=578&width=1572&top_left_y=236&top_left_x=240)

Figure 2: The framework of InChARACTER for personality tests on RPAs. Left: Previous methods use self-report scales, which prompt LLMs to select an option directly. Right: InCHARACTER adopts an interview-based approach comprising two phases: the interview and assessment phases. The interview phase elicits the behavioral, cognitive and emotional patterns of RPAs that reflect their underlying mindsets. The assessment phase measures personalities based on interview results, with two alternative methodologies: option conversion and expert rating.

for Strongly Disagree and " 5 " for Strongly Agree. This process generates a response array $\mathcal{A}$. The scoring schema $f$ usually includes item-dimension mapping, identification of positive and negative items, conversion of options to scores, and an aggregation method (e.g., average or sum). Finally, the participant's personality scores $\mathcal{S}$ is derived as $\mathcal{S}=f(\mathcal{A})$, where $\mathcal{S}=\left(s_{d_{1}}, s_{d_{2}}, \ldots, s_{d_{|\mathcal{D}|}}\right)$ represents scores across each dimension.

## 3 INCHARACTER

This section introduces InChARACTER, a novel personality assessment methodology designed explicitly for RPAs, utilizing an interview-based procedure. Figure 2 illustrates our two-stage framework. The interview stage is detailed in $\S 3.1$, followed by an elaboration of the assessment stage in $\S 3.2$.

### 3.1 Interview

InCharacTER draws inspiration from the Structured Interview approach used in psychological testing (Trull et al., 1998). For a given scale, it transforms scale items into a series of open-ended questions, forming the basis for a structured interview. Then, our framework interviews RPAs using these open-ended questions to elicit their perspectives on topics indicative of personality traits.

Constructing Question List We develop the structured interview question list based on items of the scale. Specifically, each item $p \in \mathcal{P}$ is transformed into an open-ended question $q$ via LLMs and manually checked. Consequently, the question list $\mathcal{Q}$ comprises $|\mathcal{P}|$ questions. For instance, in the BFI, the item "Values artistic, aesthetic experiences." is rephrased as "Do you values artistic, aesthetic experiences?"

Interviewing RPAs We interview an RPA $C$ of character $c$, by presenting each question $q \in \mathcal{Q}$ and recording its corresponding response $r$. To avoid context effects (Nikolić, 2010), each question is posed in an isolated context, thereby avoiding potential interference among the questions.

### 3.2 Assessment

Based on the interview results, the assessment phase quantitatively evaluates the score $s_{d}$ of the RPA $C$ across each dimension $d \in \mathcal{D}$. To this end, we introduce two distinct methodologies for measuring and analyzing RPA personalities leveraging LLMs: option conversion (OC) and expert rating (ER).

Option Conversion This technique leverages LLMs to convert a response $r$ for a question $q$ into a corresponding answer option $a \in \mathcal{O}$ for item $p$, effectively bridging the gap between closed-ended and open-ended question formats. The idea follows the clinician-rated scales used in clinical psychiatry (Cuijpers et al., 2010; Uher et al., 2012), where professional clinicians assign ratings to each scale item based on their observations during patient interviews and compute the final scores following the scale's scoring scheme. For example, a response " $I$ believe that art transcends reality..." is converted
to "5 (Strongly Agree)" for the item. Afterward, the answer list $\mathcal{A}$ is input to the scoring scheme $f$ to compute the final personality scores. In practice, we observe that even state-of-the-art LLMs like GPT-4 (OpenAI, 2023) exhibit notable inaccuracies in categorizing the attitudes of RPAs. Therefore, we further introduce a dimensional-specific option conversion (d-OC) strategy, which divides $(q, r)$ pairs according to dimensions and substitutes Likert levels, such as "4 (Agree)" and "2 (Disagree)", with more descriptive options like "4 (Extroverted)" and " 2 (Introverted)" in the prompts for LLMs.

Expert Rating In contrast with the one-by-one question conversion in $\mathrm{OC}$, this method applies LLMs to directly evaluate personality scores of RPAs in each dimension, considering all corresponding $(q, r)$ pairs. This idea draws inspiration from the structured clinical interview in clinical psychiatry (First, 2014), where clinicians assess patients using a predefined question list and derive final scores based on the responses without intermediate ratings or scoring schemes. The interviewer LLM is prompted with comprehensive descriptions of the scale, dimension, and score range. It then generates the final personality score for each dimension based on the pertinent responses. The advantage of ER is that it re-implements the scoring schema with the interviewer LLM, which can intelligently weigh individual $(q, r)$ pairs instead of using equal weights in OC. Hence, it better recognizes personality-indicative responses from RPAs.

Details of our prompts for OC and ER are available in $\S \mathrm{F}$ in the appendix. To prevent the influence of data leakage in ER and d-OC, i.e., the interviewer LLM might have memorized the characters' personality types. Hence, we anonymize the character names in the input prompts.

## 4 Experimental Settings

RPAs and Characters This work primarily focuses on RPAs built on character data curated by ChatHaruhi (Li et al., 2023) and RoleLLM (Wang et al., 2023c). We select 32 widely-known characters, 16 from ChatHaruhi ${ }^{4}$ and 16 from RoleLLM. The characters are mainly from popular fictional works, such as Harry Potter, The Big Bang Theory and Genshin Impact. Please refer to $\S \mathrm{B}$ for the detailed character selection process. The character[^2]

data from ChatHaruhi and RoleLLM includes descriptions and dialogues used for system prompts and memory modules. To implement RPAs, we apply the Chat-Haruhi-Suzumiya ${ }^{5}$ library, and adopt GPT-3.5 (OpenAI, 2022) as the foundation LLM by default.

Psychological Scales We consider 14 psychological scales, including the BFI, the 16P, and 12 other scales following PsychoBench ${ }^{6}$ (Huang et al., 2024) to evaluate RPAs. Most scales apply scoring schemes like average and sum, while the $16 \mathrm{P}$ is close-sourced and accessed via its API. Detailed introduction of these scales can be found in $\S$ A. Due to page limitations, the main body presents results for the $\mathrm{BFI}$ and $16 \mathrm{P}$, while additional findings are detailed in the Appendix.

Personality Labels We collect labels for character personalities in the form of both scores and types, contributed by people familiar with these characters. From the PDb, an online platform for character personality annotation, we derive scores of the BFI and 16P on each dimension from its label percentage (e.g., $60 \%$ Extroverted). We then categorize it into s type of either positive, negative, or marginal if it is above $60 \%$, under $40 \%$, or otherwise. Then, we invite human annotators for comprehensive personality labels on all 14 scales. To select qualified annotators, we examine their character understanding of the BFI and $16 \mathrm{P}$, matching with labels from the PDb. We invite two to three annotators for each character ( 93 in total for 32 characters) and average their results for improved reliability and objectivity. The scores are re-scaled into the unit interval $[0,1]$ and categorized into types similarly. We measure the inter-annotator consistency via Cohen's kappa coefficient (Cohen, 1968a), and find the average coefficient across 14 scales $60.9 \%$. For the BFI and 16P, we adopt types from the $\mathrm{PDb}$ and scores from our invited annotators. The details about $\mathrm{PDb}$ annotations, our human annotation process, intra-annotator consistency, and other statistics can be found in $\S \mathrm{C}$.

Interviewer LLMs We use LLMs to accomplish the OC, d-OC and ER tasks in the assessment phase of INCHARACTER, or to extract selected options from RPA responses in self-report methods if RPAs[^3]do not provide exactly the choice. We consider three widely-acknowledge LLMs, including GPT3.5, GPT-4 and Gemini ${ }^{7}$.

Metrics We consider two sets of metrics, namely: (1) Measured alignment (MA) compares the measured personalities of RPAs and human-annotated personalities of characters. It depends both on the performance of RPAs and the effectiveness of personality test methods. We categorize RPAs as positive or negative on each dimension if the scores are above or below the median of the scoring range. Then, we calculate mean absolute error (MAE) and accuracy to measure alignment at the score and type level, respectively. We re-scale MAE by dividing it with the scoring range length. For accuracy, we report $\mathbf{A c c}_{\text {Dim }}$ and $\mathbf{A c c}_{\text {Full }}$, where correctness is judged on individual or all dimensions of each scale. The marginal dimensions of each character are ignored due to their ambiguity.

(2) Personality consistency (PC) indicates whether the measured personality of RPAs is consistent across various settings. We analyze the standard

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-05.jpg?height=51&width=780&top_left_y=1314&top_left_x=227)

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-05.jpg?height=54&width=780&top_left_y=1361&top_left_x=227)

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-05.jpg?height=49&width=780&top_left_y=1409&top_left_x=227)
scores on individual items. For InCharacter, we experiment with OC and d-OC to convert responses into scores. $\mathbf{S t d}_{\text {Item }}$ measures an RPA's consistency on the same item across multiple runs. Std $_{\text {Dim }}$ compares an RPA's responses across differ-

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-05.jpg?height=51&width=780&top_left_y=1688&top_left_x=227)
the variance of an RPA's score on each dimension across multiple runs. We divide these metrics with a length of the corresponding scoring range to rescale them into the unit interval.

## 5 Experimental Results

### 5.1 Personality Tests on RPAs

Baselines For INCHARACTER, we experiment with the ER, OC, and d-OC. For ER, we consider two settings, $\mathrm{ER}_{\mathrm{all}}$ and $\mathrm{ER}_{\text {batch }}$, where questionresponse pairs in one dimension are inputted into interviewer LLMs all-at-once or in-batch ${ }^{8}$. For self-reported (SR) baselines, we follow previous work on LLM Psychometrics (Huang et al., 2024) to prompt RPAs to provide exactly a choice for[^4]

| LLMs | Acc. | Pearson's $r$ | Spearman's $\rho$ | Kendall's $\tau$ |
| :--- | :---: | :---: | :---: | :---: |
| Option |  |  |  |  |
| Gemini | 69.5 | 54.5 | 55.9 | 53.2 |
| GPT-3.5 | 57.5 | 34.6 | 36.2 | 32.4 |
| GPT-4 | $\mathbf{7 1 . 0}$ | $\mathbf{6 0 . 0}$ | $\mathbf{6 4 . 3}$ | $\mathbf{5 9 . 5}$ |
| Dimension-specific |  |  |  |  |
| Gemini | 79.0 | 79.6 | 80.6 | 75.9 |
| GPT-3.5 | 76.5 | 79.2 | 81.7 | 74.5 |
| GPT-4 | $\mathbf{8 2 . 0}$ | $\mathbf{8 4 . 7}$ | $\mathbf{8 5 . 3}$ | $\mathbf{8 0 . 6}$ |
| Expert Rating (batch) |  |  |  |  |
| Gemini | 84.0 | 83.9 | 85.7 | 76.6 |
| GPT-3.5 | 84.0 | 90.6 | 89.9 | 80.4 |
| GPT-4 | $\mathbf{8 9 . 0}$ | $\mathbf{9 2 . 5}$ | $\mathbf{9 2 . 7}$ | $\mathbf{8 3 . 7}$ |

Table 1: The accuracy (Acc.) and consistency measurements of interviewer LLMs on the OC or ER tasks, compared with human labels.

each scale item. If their responses are not exactly the choices, we use interviewer LLMs to extract the choices. Then, the numbers are aggregated via the scoring schema to get the results. Besides, we experiment with SR-CoT, which enhances SR with chain-of-thought reasoning, i.e., explicitly asking RPAs to articulate their thoughts before choosing the options.

We compare these methods on the BFI and 16P. The experiments are repeated three times, including both the interview phase and the assessment phase. We report the average results of the three runs for MA metrics and $\mathbf{S t d}_{\text {Dim }}$, and calculate $\mathbf{S t d}_{\text {Item }}$ and Std $_{\text {Score }}$ across the three runs.

LLMs Simulating Human Interviewers We first validate the capability of interviewer LLMs on the OC and ER tasks given the interview results of RPAs. We compare their predictions with human judgments. We manually label 100 cases for each task based on RPA responses for the BFI, ranging from 1 to 5 . We report the Pearson's $r$ (Pearson, 1920), Spearman's $\rho$ (Spearman, 1961) and Kendall's $\tau$ (Kendall, 1938) correlations between human annotations and interviewer LLMs, as well as the accuracy. We consider LLM predictions varying from human labels by less than 1 point, exactly 1 , or more than 1 , as right, close (half-correct) or wrong, for accuracy calculation. More details can be found in $\S \mathrm{E} .1$.

The results presented in Table 1 lead to several findings. First, for ER, state-of-the-art LLMs can adequately rate participants' personalities based on interview results. We observe $4 \%$ of wrong cases, mainly when RPAs give contradictory responses. Second, for OC, the LLMs show sig-

| Method | Interviewe <br> Model | The Big Five Inventory |  |  |  |  |  | The 16 Personalities |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | MA |  |  | PC |  |  | MA |  |  | PC |  |  |
|  |  | $\overline{\mathbf{A c c}_{\text {Dim }}}$ | Acc $_{\text {Full }}$ | MAE $\downarrow$ | $\overline{\text { Std }_{\text {Item }}}$ | $\mathrm{Std}_{\mathrm{Dim}}$ | Std $_{\text {score }}$ | $\overline{\operatorname{Acc}_{\text {Dim }}}$ | Acc $_{\text {Full }}$ | MAE $\downarrow$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-06.jpg?height=53&width=118&top_left_y=355&top_left_x=1495) | $\operatorname{Std}_{\mathrm{Dim}}$ | Std $_{\text {Score }}$ |
| Self-reported Methods |  |  |  |  |  |  |  |  |  |  |  |  |  |
| SR | Gemini | 63.3 | 7.3 | 23.6 | 2.0 | 30.2 | 1.5 | 65.3 | 21.9 | 26.8 | 5.5 | 46.1 | 2.1 |
|  | GPT-3.5 | 63.7 | 7.3 | 23.4 | 2.3 | 30.2 | 1.5 | 66.1 | 22.9 | 26.7 | 2.8 | 38.2 | 1.9 |
|  | GPT-4 | 63.3 | 7.3 | 23.2 | 2.2 | 28.3 | 1.5 | 65.6 | $21.9 \quad$ | 26.5 | 3.3 | $37.7 \quad$ | 2.1 |
| SR-CoT | Gemini | 66.2 | 8.3 | 22.6 | 13.2 | 26.1 | 5.3 | 66.7 | 21.9 | 26.1 | 15.7 | 33.9 | 5.1 |
|  | GPT-3.5 | 66.9 | 9.4 | 22.6 | 12.9 | 25.2 | 5.5 | 68.0 | 24.0 | 26.1 | $14.9 \quad$ | 31.9 | 5.1 |
|  | GPT-4 | 67.1 | 9.4 | 22.3 | 12.5 | 25.0 | 5.1 | 66.9 | 24.0 | 25.6 | 14.4 | 30.3 | 4.8 |
| INCHARACTER: Interview-based Methods |  |  |  |  |  |  |  |  |  |  |  |  |  |
| OC | Gemini | 72.2 | 14.6 | 21.3 | 6.8 | 27.6 | 5.1 | 66.1 | 25.0 | 27.3 | 6.1 | 30.4 | 2.6 |
|  | GPT-3.5 | 65.4 | 3.1 | 24.2 | 4.5 | 31.5 | 2.7 | 65.0 | 28.1 | 27.8 | 4.5 | 27.7 | 2.0 |
|  | GPT-4 | 64.3 | 6.2 | 21.6 | 4.9 | 26.4 | 3.6 | 75.5 | 34.4 | 23.1 | 4.9 | 28.1 | 2.4 |
| $\mathrm{d}-\mathrm{OC}$ | Gemini | 72.8 | 18.8 | 20.4 | 4.2 | 20.8 | 3.3 | 73.6 | 36.5 | 22.6 | 4.1 | 23.5 | 2.9 |
|  | GPT-3.5 | 64.1 | 5.2 | 22.9 | 5.0 | 18.0 | 3.8 | 76.9 | 40.6 | 21.8 | 6.4 | 22.7 | 4.6 |
|  | GPT-4 | 72.2 | 14.6 | $\underline{18.6}$ | 3.8 | 19.6 | 2.9 | $\underline{80.2}$ | 45.8 | 21.2 | 3.3 | 21.3 | 2.1 |
| $\mathrm{ER}_{\mathrm{all}}$ | Gemini | 71.5 | 18.8 | 20.6 | - | - | 4.9 | 76.3 | 40.6 | 20.7 | - | - | 4.6 |
|  | GPT-3.5 | $\underline{74.1}$ | 25.0 | 20.5 | - | - | 5.2 | 79.1 | 45.8 | 22.1 | - | - | 5.9 |
|  | GPT-4 | $\overline{76.6}$ | $\underline{30.2}$ | 18.9 | - | - | 4.0 | 79.6 | 43.8 | 20.1 | - | - | 4.4 |
| $\mathrm{ER}_{\text {batch }}$ | Gemini | 73.9 | 24.0 | 19.2 | - | - | 4.7 | 77.1 | 37.5 | 20.9 | - | - | 3.2 |
|  | GPT-3.5 | 72.4 | 22.9 | 18.9 | - | - | 4.5 | 78.5 | 43.8 | 22.2 | - | - | 4.5 |
|  | GPT-4 | 76.6 | 31.2 | 18.2 | - | - | 3.6 | 80.7 | $\underline{44.8}$ | $\underline{20.5}$ | - | - | 2.9 |

Table 2: Metrics on personalities of the selected RPAs were measured via various personality test methods on the big five inventory and 16 personalities. For MA metrics, the best results are bolded, and the second best ones are

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-06.jpg?height=43&width=1533&top_left_y=1338&top_left_x=233)

nificant inaccuracy, while replacing Likert-level options with dimension-descriptive ones (d-OC) largely improves LLMs in this task. Considering the consistency measurements, state-of-the-art LLMs achieve acceptable performance in simulating human interviewers to assess RPA personalities through ER or d-OC.

## Alignment between RPAs' Measured Personalities and Characters' Labeled Personalities

 Then, we apply the InCHARACTER to measure RPA personalities. According to the results in Table 2, we have the following analyses: (1) Using INCHARACTER with ER and GPT-4, the measured RPA personalities are highly aligned with ground truth labels of corresponding characters. This suggests that state-of-the-art RPAs reproduce many of the characters' personality traits well, and our method can accurately measure their personalities. (2) RPA personalities measured via InCHARACTER are better aligned with corresponding characters than SR baselines. This validates the advantage of INCHARACTER over self-report for personality tests on RPAs, which will be further discussed. (3) The alignment measured via InCHARACTER correlates with the interviewer LLMs' capability on the assessment methodology. For the assessment method, InCHARACTER with ER generally achieves better alignment metrics than d-OC, while d-OC surpasses OC. However, Table 1 shows that interviewers LLMs still make mistakes on the ER and OC task, leading to potential inaccuracies in INCHARACTER and may underestimate the personality alignment of RPAs.
## Robustness, Consistency and Distinctiveness of

 RPA Personalities Generally, the measured RPA personalities are robust across our observations. The $\mathbf{S t d}_{\text {Score }}$ across three runs remain below $6 \%$ in various settings, which underlines the reliability of personality tests and the robustness of RPA personalities. Then, we study the consistency at the![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-06.jpg?height=51&width=777&top_left_y=2222&top_left_x=1051)
With InCHARACTER, after converting the interview results into scores via d-OC and GPT-4, We observe that RPAs respond to the same items consistently across multiple runs and exhibit a relatively consistent personality across different items on the same dimension. We visualize the distribution of RPA personalities on the BFI in Figure 3, and find that RPAs exhibit distinct personalities, especially when measured by INCHARACTER with

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-07.jpg?height=292&width=605&top_left_y=248&top_left_x=317)

Figure 3: Visualization of 32 RPAs' personalities on the BFI measured by different methods. We use principal component analysis (PCA) to map the results into $2 \mathrm{D}$ spaces. Black points represent the personality of GPT3.5 measured by corresponding methods.

ER $_{\text {batch }}$ and GPT-4.

Self-report v.s. Interview-based Methods As shown in Table 2, the personalities measured by INCHARACTER are more aligned with the characters, compared with self-report. Meanwhile, in interview-based tests, RPAs exhibit more consistent personalities across different questions, as well as greater distinctiveness, shown in Figure 3. These findings confirm the advantages of interview-based tests over self-report in measuring RPA personalities. Although SR-CoT attempts to enhance SR with the thought process, its improvement over SR is limited, and it encounters poor $\mathbf{S t d}_{\text {Item }}$. Further analyses and comparisons are detailed in $\S \mathrm{E} .2$.

## Comprehensive Personality Tests on 14 Scales

We extend personality tests on RPAs to 14 psychological scales, using INCHARACTER with $\mathrm{ER}_{\text {batch }}$ and GPT-3.5. Overall, we observe that state-of-theart RPAs exhibit personalities align with the target characters in comprehensive aspects with an aver-

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-07.jpg?height=51&width=765&top_left_y=1859&top_left_x=243)
(BFI, 16P), dark personalities (DTDD), interpersonal relationships (BSRI, ECR-R), basic interests (CABIN), motivation (GSE, LMS) and emotional intelligence (EIS, WLEIS), etc.. The detailed metrics on individual scales and individual dimensions are listed in $\S \mathrm{E} .5$.

### 5.2 Personality Fidelity of Different RPAs

With INCHARACTER, we compare the personality fidelity of various types of RPAs, covering different character data and foundation models. We apply

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-07.jpg?height=52&width=765&top_left_y=2427&top_left_x=243)
the interviewer LLM for personality tests. We report the MA metrics on the BFI and 16P in Table 3.

Character Data for RPAs Typically, existing RPAs utilize two types of character data: descrip-

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-07.jpg?height=486&width=551&top_left_y=248&top_left_x=1164)

![](https://cdn.mathpix.com/cropped/2024_06_04_415860d0a02b8e781b5eg-07.jpg?height=46&width=774&top_left_y=771&top_left_x=1052)
the-art RPAs on 14 scales.

tions and memories. Character descriptions serve as the system prompts for RPAs, while memories consist of characters' experiences and dialogues used for retrieval. With GPT-3.5, we evaluate RPAs with only descriptions (D), only memories $(\mathrm{M})$, and a combination of both (D+M). The results in Table 3 reveal that: (1) With only description, RPAs achieve MA metrics close to the full D+M setup, highlighting the importance of character description in shaping RPA personalities. (2) RPAs can well mimic character personalities exhibited in their past experiences, e.g., extraversion and openness, even if the experiences are not directly related to scale questions. Additionally, we compare RPAs with character data from ChatHaruhi, RoleLLM and c. ai in $\S \mathrm{E} .3$.

Foundation Models for RPAs We consider three types of LLMs: (1) General open-source models, including Qwen-7B (Bai et al., 2023), OpenChat3.5 7B (Wang et al., 2023a), Mistral-2 7B (Jiang et al., 2023), Llama-2-chat 13B (Touvron et al., 2023) and Mixtral 8x7B (Jiang et al., 2024). (2) Specialized open-source models for RPAs, including Character-GLM 6B (Zhou et al., 2023), RPQwen 7B ${ }^{9}$, and RP-Mistral-2 7B. We train RPMistral-2 7B with details shown in §D.2. (3) Closesource models: GPT-3.5 and GPT-4.

The results are shown in Table 9. We observe that, (1) RPAs with GPT-3.5 and GPT-4 achieve the best personality fidelity, and GPT-4 does not significantly surpass GPT-3.5. (2) With state-of-theart open-source LLMs, RPAs can also reproduce character personalities. However, such capacity depends largely on their ability to use specific languages, shown in $\S E$.4. (3) Incremental fine-tuning[^5]

| Agent Types |  | The Big Five Inventory |  |  | The 16 Personalities |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLMs | Data | $\overline{\operatorname{Acc}_{\text {Dim }}}$ | Acc $_{\text {Full }}$ | MAE $\downarrow$ | $\overline{\operatorname{Acc}_{\text {Dim }}}$ | Acc $_{\text {Full }}$ | $\overline{M A E} \downarrow$ |
| w/ General Open-source LLMs |  |  |  |  |  |  |  |
| Owen 7B | $D+M$ | 60.5 | 9.4 | 24.3 | 67.8 | 21.9 | 27.9 |
| OpenChat-3.5 7B | $D+M$ | 63.1 | 6.2 | 23.1 | 76.9 | 40.6 | 24.6 |
| Mistral-2 7B | $D+M$ | 66.2 | 18.8 |  | 68.6 | $\overline{21.9}$ | 26.0 |
| LLaMa-2-Chat 13B | $\mathrm{D}+\mathrm{M}$ | 66.9 | 12.5 | 26.8 | 66.9 | 28.1 | 27.7 |
| Mixtral 8x7B | $D+M$ | 68.2 | 15.6 | 20.8 | 71.9 | 31.2 | 25.3 |
| w/ Specialized Open-source LLMs |  |  |  |  |  |  |  |
| CharacterGLM 6B | $\mathrm{D}+\mathrm{M}$ | 54.1 | 0.0 | 25.8 | 52.1 | 15.6 | 29.7 |
| RP-Qwen | $D+M$ | 60.5 | 0.0 | 23.8 | 64.5 | 15.6 | 28.6 |
| RP-Mistral-2 7B | $D+M$ | 70.1 | 18.8 | 21.7 | 69.4 | 28.1 | 26.1 |
| w/ Close-source LLMs |  |  |  |  |  |  |  |
| character.c $-x-y-1$ | $\mathrm{D}^{*}$ | 52.2 | 9.4 | 31.2 | 52.9 | 21.9 | 31.6 |
| GPT-3.5 | $\mathrm{D}$ | 71.3 | 21.9 | 21.1 | 78.5 | 43.8 | 22.0 |
| GPT-3.5 | M | 71.3 | $\overline{18.8}$ | 21.8 | $\overline{71.9} \quad$ | 31.2 | 26.0 |
| GPT-3.5 | $D+M$ | 72.0 | 21.9 | 18.8 | 79.3 | 43.8 | 22.6 |
| GPT-4 | $\mathrm{D}+\mathrm{M}$ | 73.9 | $\overline{25.0}$ | 19.8 | 76.0 | 43.8 | $\overline{23.2}$ |

Table 3: Measured alignment (\%) of RPAs with different foundation models and character data. D and M represent descriptions and memories respectively, and $\mathrm{D}^{*}$ denote private descriptions of character.ai.

on open-source LLMs with role-playing datasets brings limited improvement in personality fidelity, especially when they are already equipped with excellent role-playing ability.

## c. ai RPAs Barely Reproduce Character Person-

 alities. It also significantly underperforms GPT3.5 (D), which shares a similar framework. According to our observation, while c.ai RPAs provide human-like answers, their answers tend to be compliant and pleasing to users instead of reproducing the target characters. Examples and further analysis are shown in $\S \mathrm{G} .2$.
## 6 Related Work

Role-Playing Agents RPAs learn and leverage character data in various ways, including training on raw scripts or dialogues (Shao et al., 2023b), prompting with character descriptions (Zhou et al., 2023), and retrieval from character experiences ( $\mathrm{Li}$ et al., 2023). Existing efforts mainly focus on developing character-specific RPAs or foundation models for RPAs. The former includes ChatHaruhi (Li et al., 2023) and RoleLLM (Wang et al., 2023c), which target well-established fictional characters. The latter includes c.ai and CharacterGLM (Zhou et al., 2023). For evaluation, prior research mainly concentrates on two facets: 1) Character-independent capabilities, which include conversational abilities (Duan et al., 2023), human-likeness (Tu et al., 2024), multi-turn consistency (Shao et al., 2023a), and attractiveness (Zhou et al., 2023); 2) Character fidelity, including the characters' knowledge, experience, and linguistic patterns (Wang et al., 2023c; Shao et al., 2023a). Overall, these methods generally require test sets for each character, and neglect the evaluation of RPAs' underlying mindset.

Psychological Analysis on LLMs Recent studies conducted personality tests using the BFI (Romero et al., 2023; Karra et al., 2022; Li et al., 2022; Jiang et al., 2022; Safdari et al., 2023; Bodroza et al., 2023), the MBTI (Rutinowski et al., 2023; Pan and Zeng, 2023) on various LLMs. Notably, Huang et al. (2023c) verified the reliability of the BFI on GPT-3.5, while Safdari et al. (2023) demonstrated the construct validity of the BFI on the PaLM model family. Other studies also investigates other mental perspectives, such as emotions (Huang et al., 2023a), values (Miotto et al., 2022; Rutinowski et al., 2023; Hartmann et al., 2023), consciousness (Butlin et al., 2023), and mental illness (Coda-Forno et al., 2023). Our research diverges by employing personality tests as an innovative approach to assess the character fidelity in RPAs.

## 7 Conclusion

In this study, we investigate the personality fidelity in RPAs, i.e., whether RPAs reproduce personalities of their intended characters. Addressing the shortcomings of previous methods on RPAs, we propose INCHARACTER, an interview-based approach that accurately measures RPA personalities based on their elicited mindsets and behaviors. Our experiments span various types of RPAs, covering

32 characters on 14 psychological scales. The results validate the effectiveness of INCHARACTER in measuring RPA personalities. Afterwards, with INCHARACTER, we comprehensive evaluate personality fidelity in existing RPAs, discovering that state-of-the-art RPAs successfully portray many personality traits of the characters.

## Limitations

There are several limitations in this study. First, the personality measurement in this paper relies on the interviewer LLMs. Consequently, the accuracy of the measured results may be compromised by potential errors or biases inherent in LLMs, potentially leading to an underestimation of the personality fidelity in RPAs. Second, the personalities of humans or fictional characters can change overtime. Since we use one static personality label for a specific character, there may be noise in our evaluation. For instance, the character of James Bond has experienced significant development over the past two decades across various films and television series. Our character annotations are derived from a singular, fixed time point in his storyline. Additionally, the progressive changes in RPA personalities remain unexplored within existing literature. We leave the study of RPA personality dynamics for future research.

## Ethical Statement

We hereby acknowledge that all authors of this work are aware of the provided ACL Code of Ethics and honor the code of conduct.

Use of Human Annotations In conducting our research, we have employed a methodology that incorporates personality labels, which were gathered through the online platform as well as by engaging a group of annotators. These annotators, who are university students, play a crucial role in our research process. To ensure fair treatment and to value their contribution, we offer them compensation that significantly exceeds the local minimum wage standards. Moreover, we maintain transparency regarding the application and purpose of their annotations, securing their informed consent for the use of these annotations in our research endeavors. Additionally, we are committed to upholding the privacy rights of our annotators throughout the annotation process, ensuring a respectful and ethical research environment.
Risks In this paper, we introduce a novel approach, referred to as INCHARACTER, designed to assess the personalities of Role-Play Agent (RPA) entities. An integral component of our evaluation process involves the use of interviewer Large Language Models (LLMs), which, while innovative, could potentially introduce bias into the assessment outcomes. It is important to acknowledge this limitation as LLMs may reflect the inherent biases present in their training data. Furthermore, our evaluation encompasses a comprehensive analysis across 14 personality scales, notably including the Dark Triad of Personality (DTDD) scale, which focuses on darker personality traits. While this inclusion is aimed at providing a thorough understanding of RPA personalities, it raises ethical concerns regarding the potential for generating harmful content. This aspect underscores the need for careful consideration and implementation of safeguards to mitigate the risks associated with exploring dark personality traits in RPAs.

## Acknowledgment

This work initiates from a project on Chat-HaruhiSuzumiya proposed by Cheng Li. We owe thanks for the early contributors. We are thankful for the support provided by Zheli Xuan at Wuhan University, and Dingding Hu at the Institute of Psychology, Chinese Academy of Sciences, who offered invaluable assistance as psychology researchers. Our gratitude extends to our invited annotators, primarily from Fudan University, for their contribution of high-quality annotations of character personalities. We also acknowledge the assistance provided by Rui Fu and Wenxin Gao at Fudan University during the annotation process. Finally, we express our sincere gratitude for the precious comments and suggestions from Yikai Zhang, Xinfeng Yuan, and Shuang Li at Fudan University.

## References

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.

Murray R Barrick and Michael K Mount. 1991. The big five personality dimensions and job performance: a meta-analysis. Personnel psychology, 44(1):1-26.

Sandra L Bem. 1981. Bem sex role inventory. Journal of personality and social psychology.

Bojana Bodroza, Bojana M Dinic, and Ljubisa Bojic. 2023. Personality testing of gpt-3: Limited temporal reliability, but highlighted social desirability of gpt3's personality instruments results. arXiv preprint arXiv:2306.04308.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.

Patrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, Stephen M Fleming, Chris Frith, Xu Ji, et al. 2023. Consciousness in artificial intelligence: Insights from the science of consciousness. arXiv preprint arXiv:2308.08708.

Julian Coda-Forno, Kristin Witte, Akshay K Jagadish, Marcel Binz, Zeynep Akata, and Eric Schulz. 2023. Inducing anxiety in large language models increases exploration and bias. arXiv preprint arXiv:2304.11111.

Jacob. Cohen. 1968a. Weighed kappa: Nominal scale agreement with provision for scaled disagreement or partial credit. Psychological Bulletin, 70(4):213220 .

Jacob Cohen. 1968b. Weighted kappa: nominal scale agreement provision for scaled disagreement or partial credit. Psychological bulletin, 70(4):213.

Pim Cuijpers, Juan Li, Stefan G Hofmann, and Gerhard Andersson. 2010. Self-reported versus clinicianrated symptoms of depression as outcome measures in psychotherapy research on depression: a metaanalysis. Clinical psychology review, 30(6):768-778.

Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang, Songyang Zhang, Dahua Lin, and Kai Chen. 2023. Botchat: Evaluating llms' capabilities of having multi-turn dialogues. arXiv preprint arXiv:2310.13650.

Michael B First. 2014. Structured clinical interview for the dsm (scid). The encyclopedia of clinical psychology, pages 1-6.

Jingsheng Gao, Yixin Lian, Ziyi Zhou, Yuzhuo Fu, and Baoyuan Wang. 2023. Livechat: A largescale personalized dialogue dataset automatically constructed from live streaming. arXiv preprint arXiv:2306.08401.
Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. 2023. The political ideology of conversational ai: Converging evidence on chatgpt's proenvironmental, left-libertarian orientation. arXiv preprint arXiv:2301.01768.

Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael R Lyu. 2023a. Emotionally numb or empathetic? evaluating how llms feel using emotionbench. arXiv preprint arXiv:2308.03656.

Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, and Michael R Lyu. 2023b. Chatgpt an enfj, bard an istj: Empirical study on personalities of large language models. arXiv preprint arXiv:2305.19926.

Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, and Michael R Lyu. 2023c. Revisiting the reliability of psychological scales on large language models. arXiv preprint arXiv:2305.19926.

Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, and Michael R Lyu. 2024. Who is chatgpt? benchmarking llms' psychological portrayal using psychobench. In Proceedings of the Twelfth International Conference on Learning Representations.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.

Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024 Mixtral of experts. arXiv preprint arXiv:2401.04088.

Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, and Yixin Zhu. 2022. Mpi: Evaluating and inducing personality in pre-trained language models. arXiv preprint arXiv:2206.07550.

Saketh Reddy Karra, Son The Nguyen, and Theja Tulabandhula. 2022. Estimating the personality of white-box language models. arXiv preprint arXiv:2204.12000.

Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81-93.

Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, et al. 2023. Chatharuhi: Reviving anime character in reality via large language model. arXiv preprint arXiv:2308.09597.

Xingxuan Li, Yutong Li, Shafiq Joty, Linlin Liu, Fei Huang, Lin Qiu, and Lidong Bing. 2022. Does gpt-3 demonstrate psychopathy? evaluating large language models from a psychological perspective. arXiv preprint arXiv:2212.10529.

Marilù Miotto, Nicola Rossberg, and Bennett Kleinberg. 2022. Who is GPT-3? an exploration of personality, values and demographics. In Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS), pages 218-227, Abu Dhabi, UAE. Association for Computational Linguistics.

Danko Nikolić. 2010. The brain is a context machine. Review of psychology, 17(1):33-38.

OpenAI. 2022. Openai: Introducing chatgpt.

OpenAI. 2023. Gpt-4 technical report.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744. Curran Associates, Inc.

Keyu Pan and Yawen Zeng. 2023. Do llms possess a personality? making the mbti test an amazing evaluation for large language models. arXiv preprint arXiv:2307.16180.

Karl Pearson. 1920. Notes on the history of correlation. Biometrika, 13(1):25-45.

Peter Romero, Stephen Fitz, and Teruo Nakatsuma. 2023. Do gpt language models suffer from split personality disorder? the advent of substrate-free psychometrics. ResearchSquare preprint.

A John Rush, William Hiser, and Donna E Giles. 1987. A comparison of self-reported versus clinicianrelated symptoms in depression. The Journal of clinical psychiatry, 48(6):246-248.

Jérôme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, and Markus Pauly. 2023. The selfperception and political biases of chatgpt. arXiv preprint arXiv:2304.07333.

Mustafa Safdari, Greg Serapio-García, Clément Crepy, Stephen Fitz, Peter Romero, Luning Sun, Marwa Abdulhai, Aleksandra Faust, and Maja Matarić. 2023. Personality traits in large language models. arXiv preprint arXiv:2307.00184.

Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023a. Character-LLM: A trainable agent for roleplaying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13153-13187, Singapore. Association for Computational Linguistics.

Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023b. Character-LLM: A trainable agent for roleplaying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13153-13187, Singapore. Association for Computational Linguistics.
Vera Sorin, Danna Brin, Yiftach Barash, Eli Konen, Alexander Charney, Girish Nadkarni, and Eyal Klang. 2023. Large language models (llms) and empathy-a systematic review. medRxiv, pages 2023-08.

Charles Spearman. 1961. The proof and measurement of association between two things.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.

Timothy J Trull, Thomas A Widiger, J David Useda, Jay Holcomb, Bao-Tran Doan, Seth R Axelrod, Barry L Stern, and Beth S Gershuny. 1998. A structured interview for the assessment of the five-factor model of personality. Psychological assessment, 10(3):229.

Quan Tu, Chuanqi Chen, Jinpeng Li, Yanran Li, Shuo Shang, Dongyan Zhao, Ran Wang, and Rui Yan. 2023. Characterchat: Learning towards conversational ai with personalized social support. arXiv preprint arXiv:2308.10278.

Quan Tu, Shilong Fan, Zihang Tian, and Rui Yan. 2024. Charactereval: A chinese benchmark for role-playing conversational agent evaluation. arXiv preprint arXiv:2401.01275.

Rudolf Uher, Roy H Perlis, Anna Placentino, Mojca Zvezdana Dernovšek, Neven Henigsberg, Ole Mors, Wolfgang Maier, Peter McGuffin, and Anne Farmer. 2012. Self-report and clinician-rated measures of depression severity: can one replace the other? Depression and anxiety, 29(12):1043-1049.

Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023a. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235.

Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023b. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv: Arxiv-2305.16291.

Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. 2023c. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc.

Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, et al. 2023. Characterglm: Customizing chinese conversational ai characters with large language models. arXiv preprint arXiv:2311.16832.
