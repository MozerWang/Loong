# WHEN LLMs MEET CYberSECURITY: A SYStEMATIC LITERATURE REVIEW 

Jie Zhang ${ }^{1,2}$, Haoyu Bu ${ }^{1,2}$, Hui Wen ${ }^{1}$, Yu Chen ${ }^{1,2}$, Lun Li ${ }^{1}$, Hongsong Zhu ${ }^{1}$<br>${ }^{1}$ Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China<br>${ }^{2}$ School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China<br>\{zhangjie, buhaoyu, wenhui, chenyu, lilun, zhuhongsong\}@iie.ac.cn


#### Abstract

The rapid advancements in large language models (LLMs) have opened new avenues across various fields, including cybersecurity, which faces an ever-evolving threat landscape and need for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper bridge this gap by providing a systematic literature review, encompassing an analysis of over 180 works, spanning across 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three critical research questions: the construction of cybersecurity-oriented LLMs, LLMs' applications in various cybersecurity tasks, and the existing challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices, and serve as a valuable resource for applying LLMs in this doamin. We also maintain and regularly updated list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/AwesomeLLM4Cybersecurity.


Keywords Cybersecurity $\cdot$ Large Language Model

## 1 Introduction

Large language models (LLMs), represented by breakthrough models such as ChatGPT [1], Llama [2], and their derivatives [3, 4, 5], have marked a significant advancement in artificial intelligence. These models, leveraging extensive datasets and advanced neural network architectures, demonstrate remarkable abilities in understanding and generating human language [6, 7]. They not only set new benchmarks for achieving artificial general intelligence (AGI) but also show unique adaptability and effectiveness when collaborating with domain experts [8, 9]. Such research enables LLMs to be tailored for the specific challenges of various fields, thus facilitating progress and development in healthcare, law, education, software engineering, and beyond [10, 11, 12, 13, 14, 15]. In cybersecurity, exploring LLM applications can lay foundations for further model exploration and utilization while highlighting potential transformative impacts [16, 17, 18, 19, 20].

Cybersecurity is a critical concern given the ever-growing number of cyber threats posing significant risks to individuals, organizations, and governments [21, 22, 23]. Modern cybersecurity's rapidly evolving and dynamic nature presents significant challenges, as adversaries continuously adapt their tactics to exploit vulnerabilities and evade detection [24, 25]. While traditional approaches such as signature-based detection and rule-based systems, often struggle to keep pace with the ever-changing threat landscape, advancements in AI, particularly LLMs have opened up new avenues for enhancing cybersecurity [26]. Efforts have been made to adapt LLMs to the cybersecurity domain. On one hand, open-sourced LLLs (e.g., LLaMA [2, 27]) enabled development of cybersecurity-enhanced domain LLMs like RepairLlama [28] and Hackmentor [29], tackling unique cybersecurity challenges. On the other hand, advanced LLMs like ChatGPT solve complex tasks via prompt engineering, in-context learning, and chains-of-thought, despite lacking cybersecurity-specific training [30]. These preliminary efforts show LLMs aid cybersecurity tasks with promising results.

Table 1: The main cybersecurity tasks and applications where LLMs have been utility.

|  | Vulnerability <br> Detection | (In)secure <br> Code <br> Generation | Program <br> Repairing | Binary | IT <br> Operations | Threat <br> Intelligence | Anomaly <br> Detection | LLM <br> Assisted <br> Attack | Others |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| RQ1 | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | - | - | - | $\checkmark$ |
| RQ2 | $\checkmark$ | $\checkmark$ | $\checkmark$ | - | - | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| RQ3 | - | - | - | - | $\checkmark$ | - | $\checkmark$ | $\checkmark$ | - |

Despite initial efforts of LLMs in cybersecurity, the field faces several challenges [17, 31]. First, many studies rely on case studies without comprehensive methodology, raising concerns about scalability and reproducibility. Additionally, the research landscape appears fragmented, lacking connectivity among studies and in-depth analysis. With the rapidly increasing volume of LLM research in this domain, conducting a systematic overview is critical for steering the field into a new development phase where LLM application is not just experimental but strategically impactful [18, 19, 20]. Thus, this work aims to conduct an extensive review of domain-specific LLMs tailored for cybersecurity, explore the breadth of LLM applications in this area, and identify emerging challenges, setting a foundation for future developments.

This survey aims to provide a comprehensive overview of LLM applications in cybersecurity. We seek to address three key questions:

- RQ1: How to construct cyber security-oriented domain LLMs?
- RQ2: What are the potential applications of LLMs in cybersecurity?
- RQ3: What are the existing challenges and further research directions about the application of LLMs in cybersecurity?

By exploring these questions, we aim to bridge the gap between the advancements in LLMs and their potential impact on enhancing cybersecurity practices. We will delve into the various cybersecurity tasks and applications where LLMs have shown promise, such as vulnerability detection, secure code generation, program repair, binary analysis, operational management, threat intelligence, anonymity detection, and LLM-assisted offensive security, as shown in Table 1 .

For the first question, we summarize the principles of existing cybersecurity LLMs, detailing their key techniques, the data used for model development, and the trained domain LLMs for special tasks. We provide insight into constructing domain models, valuable for researchers and cybersecurity practitioners looking to build custom LLMs tailored to specific needs, such as computational limits, private data, and local knowledge bases (Section 3). For the second question, we conduct an extensive survey on the use of existing LLMs across 10+ cybersecurity tasks, including threat intelligence, vulnerability detection, program repairing, and others. This analysis not only helps us understand how LLMs benefit cybersecurity in various aspects but also enables us to identify their strengths when applied to domain-specific tasks. By showcasing the diverse capabilities of LLMs, we aim to clarify their potential for enhancing and transforming the field of cybersecurity (Section 4). The third question emphasizes the challenges that need to be overcome when applying LLMs in cybersecurity. LLM's inherent vulnerabilities and susceptibilities cause these challenges to attacks, especially LLM-oriented attacks and LLM jailbreaking. Additionally, we also look into future directions for developing LLMs, providing a guide for researchers and practitioners seeking to advance this field and harness LLMs' potential in cybersecurity (Section5).

![](https://cdn.mathpix.com/cropped/2024_05_29_995460fc12e55382f0c9g-02.jpg?height=450&width=783&top_left_y=1884&top_left_x=671)

Figure 1: Conceptual framework for applying LLMs in cybersecurity.

In summary, this paper contributes by providing a comprehensive review of the state-of-the-art LLM applications in cybersecurity, highlighting the potential benefits and challenges, and proposing future research directions. The
subsequent sections of this paper are organized as follows. Section 2 provides an overview of the research scope of this paper. Section 3 summarizes the existing cybersecurity LLMs. Section 4 details how LLMs are applied in various cybersecurity tasks. Section 5 highlights the challenges and promising opportunities for future research. Section 6 draws the conclusions.

## 2 preliminary

In this review paper, we systematically investigate the application advancements of LLMs within the field of cybersecurity, covering over 180 academic papers since 2023. Through exhaustive research and comprehensive analysis of these papers, we aim to provide a detailed overview of the current state, challenges, and future directions of LLM applications in cybersecurity. As shown in Figure 2, this emerging research field has received continued attention, and LLM can be used to solve a variety of domain tasks. This not only highlights the current and potential impact of LLMs in cybersecurity but also offers practical guidance for future research. Therefore, this section first summarizes the surveyed papers from two aspects: one is the LLMs used in cybersecurity, and the other is the categories of cybersecurity tasks to which LLMs can be applied.

![](https://cdn.mathpix.com/cropped/2024_05_29_995460fc12e55382f0c9g-03.jpg?height=499&width=756&top_left_y=886&top_left_x=240)

(a) Time distribution over months

![](https://cdn.mathpix.com/cropped/2024_05_29_995460fc12e55382f0c9g-03.jpg?height=499&width=726&top_left_y=886&top_left_x=1144)

(b) Word cloud

Figure 2: Statistic of surveyed papers.

### 2.1 LLMs in Cybersecurity

LLMs have emerged as a transformative technology in the field of artificial intelligence, demonstrating remarkable capabilities in natural language understanding, generation, and reasoning [32, 6, 7]. These models, trained on vast amounts of textual data, have the potential to revolutionize various domains, including the critical area of cybersecurity $[18,19,20]$. The application of LLMs in cybersecurity holds promise in enhancing threat detection, automated vulnerability analysis, intelligent defense mechanisms, and others.

LLMs can be categorized into two main types: open-source and closed-source models. Open-source LLMs, such as Llama [2] and Mixtral [5], offer transparency and the ability for researchers to customize and fine-tune the models for specific cybersecurity tasks. This adaptability is particularly valuable in cybersecurity scenarios such as private data and fine-tuned models on customized needs. However, open-source LLMs may lack the performance and scale of their closed-source counterparts. On the other hand, closed-source LLMs, often referred to as commercial LLMs such as ChatGPT [1] and Gemini [33], provide state-of-the-art performance and are maintained by commercial entities, often with restricted access. While these models excel in terms of accuracy and efficiency, their lack of transparency can raise concerns about potential biases and limitations in cybersecurity applications.

In the realm of cybersecurity, there is a growing need for intelligent tools that can understand, analyze, and generate secure code. Code-based LLMs, such as CodeLlama [34] and StarCoder [35, 36], are particularly well-suited to address this demand. Unlike text-based LLMs are trained on vast amounts of natural language data, code-based LLMs are specifically designed to comprehend and work with programming languages. Code-based LLMs are trained on large codebases spanning multiple programming languages, allowing them to capture the intricacies of syntax, semantics, and common coding patterns. This specialized training enables them to perform a wide range of tasks, including code completion, bug detection, and automated code review. In the context of cybersecurity, these capabilities are invaluable for identifying potential vulnerabilities, suggesting secure coding practices, and automating the remediation of security flaws.

Table 2: A Summary of LLMs used in cybersecurity (this paper).

| Organization | LLMs | Size | Open-Source | Count | Link |
| :---: | :---: | :---: | :---: | :---: | :---: |
| OpenAI | GPT-3.5 | $175 \mathrm{~B}$ | $\bar{x}$ | 86 | https://chat.openai.com/ |
|  | GPT-4 | - | $\times$ | 56 | https://chat.opena1.com/ |
|  | Codex | - | $x$ | 13 | https://opena1.com/blog/opena1-codex |
|  | davinci $(-002,-003)$ | 175B | $\times$ | 9 | https://opena1.com/blog/opena1-ap1 |
| Google | Bard\&Gemini | - | $\times$ | 12 | https://gemini.google.com/ |
|  | $\operatorname{PaLM}(-1,-2)$ | $540 \mathrm{~B}$ | $\times$ | 7 | https://a1.google.dev/models/palm |
| Anthropic | Claude $(-1,-2)$ | - | $x$ | 2 | https://claude.ai/ |
| Github | Copilot | - | $\times$ | 2 | https://github.com/features/copilot |
| Microsoft | BingChat | - | $\times$ | 2 | https://www.bing.com/chat |
| EleutherAI | GPT-J | $6 \mathrm{~B}$ | $\checkmark$ | 2 | https://huggingface.co/EleutherAl/gpt-j-6b |
|  | GPT-Neo | 2.7B | $\checkmark$ | 3 | https://huggingtace.co/EleutherAI/gpt-neo-2.7B |
| Meta | $\operatorname{Llama}(-1,-2)$ | $7 \mathrm{~B} / 13 \mathrm{~B} / 70 \mathrm{~B}$ | $\checkmark$ | 38 | https://huggingface.co/meta-llama |
|  | LlamaGuard | $7 \mathrm{~B}$ | $\checkmark$ | 1 | https://huggingtace.co/meta-llama/LlamaGuard-7b |
|  | InCoder | $1 \mathrm{~B} / 6 \mathrm{~B}$ | $\checkmark$ | 4 | https://huggingtace.co/tacebook/incoder-IB |
| LMSYS | Vicuna | $7 \mathrm{~B} / 13 \mathrm{~B}$ | $\checkmark$ | 12 | https://huggingface.co/Imsys/vicuna-7b-vl.5 |
| LianjiaTech | BELLE | $7 \mathrm{~B} / 13 \mathrm{~B}$ | $\checkmark$ | 1 | https://github.com/LianjiaTech/BELLE/ |
| Databricks | Dolly | $6 \mathrm{~B}$ | $\checkmark$ | 3 | https://huggingface.co/databricks/dolly-vl-6b |
| - | Guanaco | $7 \mathrm{~B}$ | $\checkmark$ | 2 | https://huggingface.co/JosephusCheung/Guanaco |
| Salesforce | CodeGen( $-1,-2)$ | $3 \mathrm{~B} / 7 \mathrm{~B} / 16 \mathrm{~B}$ | $\checkmark$ | 9 | https://github.com/salesforce/CodeGen/ |
|  | CodeT5 | $6 \mathrm{~B}$ | $\checkmark$ | 3 | https://huggingtace.co/Salestorce/codet5p-6b |
| BigCode | StarCoder(-1,-2) | 3B/7B/15B | $\checkmark$ | 3 | https://huggingface.co/bigcode/ |
| THUDM | ChatGLM | $6 \mathrm{~B}$ | $\checkmark$ | 8 | https://github.com/THUDM/ChatGLM-6B |
| KaistAI | Prometheus | $7 \mathrm{~B} / 13 \mathrm{~B}$ | $\checkmark$ | 1 | https://github.com/kaistAI/Prometheus |
| MistralAI | Mistral | $7 \mathrm{~B}$ | $\checkmark$ | 6 | https://huggingface.co/mistralai/Mistral-7B-v0.1 |
|  | Mixtral | $8 * 7 B$ | $\checkmark$ | 5 | https://huggingtace.co/mistrala1/M1xtral-8x/B-v0.1 |

### 2.2 Cybersecurity Categories of LLMs Application

![](https://cdn.mathpix.com/cropped/2024_05_29_995460fc12e55382f0c9g-04.jpg?height=675&width=1316&top_left_y=1359&top_left_x=402)

Figure 3: Treemap for cybersecurity categories of $\mathbf{L L M s}$ ' application.

Cybersecurity has emerged as a critical concern in the digital age, with the increasing reliance on interconnected systems and the proliferation of sophisticated cyber threats [21, 23]. The field of cybersecurity encompasses a wide range of practices, technologies, and strategies aimed at protecting computer systems, networks, and data from unauthorized access, attacks, damage, or disruption [24, 25]. AI techniques, particularly LLMs, have shown immense potential in revolutionizing various aspects of cybersecurity [20]. The applications of LLMs in cybersecurity encompass a broad spectrum, ranging from threat detection and analysis, and security policy compliance, to automated vulnerability assessments and malware analysis.

- Vulnerability Detection: This is one of the most important tasks in cybersecurity. Combined with LLMs, novel approaches have been explored on detecting vulnerabilities.
- (In)secure Code Generation: Whether the code generated by LLMs are at risk? Moreover, can LLMs rectify their code through some strategies?
- Program Repairing: Program repairing is task-intensive and patching defects requires sufficient experience and knowledge. A number of researches proved the effectiveness on this issue.
- Binary: LLMs have shown great performance on dealing natural languages and advanced programming languages. Verifying LLMs' capabilities to understand disassembly is also a significant aspect.
- IT Operations: IT Operations involve a number of repetitive tasks. LLMs can be trained to automate these tasks, improving efficiency and reducing the potential for human error.
- Threat Intelligence: Extracting information from massive documents of threat intelligence is difficult, and some researchers have resorted to LLM to organize and analyze these massive and cluttered data.
- Anomaly Detection: We mainly refer to security anomalies such as malicious traffic in the flow, virus files in the system, anomaly in logs, etc.
- LLM Assisted Attacks: Many are not satisfied with these positive applications. They have discovered the effectiveness of LLM in launching network attacks such as phishing emails and penetration testing.
- Others: In addition to aspects mentioned above, we have also collected some researches which prove the importance of LLMs in the field of cybersecurity, although the researches of LLMs' application in their fields are less.


## 3 RQ1: How to construct cybersecurity-oriented domain LLMs?

The cybersecurity domain is facing escalating threats, demanding intelligent and efficient solutions to counter sophisticated and evolving attacks [37, 38, 39]. LLMs offers novel opportunities for the cybersecurity community [18, 19]. Trained on massive data, LLMs have acquired a wealth of knowledge and developed strong understanding and reasoning capabilities, providing powerful decision-making for cybersecurity.

Advancing cybersecurity requires LLMs tailored to the field, harnessing their potential to learn domain-specific data and knowledge. This section firstly focuses on key technologies for constructing cybersecurity LLMs, including training methods such as continual pre-training (CPT) [40, 41] and supervised fine-tuning (SFT) [42, 43] of LLMs, as well as technical implementations like training with full parameters and parameter-efficient fine-tuning (PEFT) [44]. Then, we introduce several domain datasets designed for evaluating the cybersecurity capabilities of LLMs [45, 46, 47], which can guide for selecting a suitable LLM as the base model when constructing cybersecurity LLMs. Lastly, we summarize existing works that constructed domain-specific models within cybersecurity by fine-tuning general LLMs [48, 29], including but not limited to vulnerability detection, program repair, secure code generation, etc.

![](https://cdn.mathpix.com/cropped/2024_05_29_995460fc12e55382f0c9g-05.jpg?height=385&width=1651&top_left_y=1653&top_left_x=234)

Figure 4: An overview of RQ1.

### 3.1 Key Technologies in Constructing Domain LLMs

LLMs, leveraging the transformer architecture and self-supervised pre-training strategies [49, 50, 32], demonstrate superior comprehension and content generation capabilities. However, developing a specialized LLM for cybersecurity from scratch would require substantial computational resources, impractical for most research teams. Fortunately, existing general LLMs already acquired broad knowledge and exhibit remarkable generalization [2, 27, 51, 5]. By integrating these pre-trained LLMs with cybersecurity-specific datasets for training purposes, we can adopt a more efficient approach to empower the models. This methodology not only significantly reduces the computational demands of pre-training but also maximizes the utilization of the knowledge already learned by LLMs, thus enhancing the models'
capability to comprehend and execute cybersecurity-related tasks, such as automated threat detection, vulnerability identification, and security policy recommendation.

![](https://cdn.mathpix.com/cropped/2024_05_29_995460fc12e55382f0c9g-06.jpg?height=631&width=1401&top_left_y=346&top_left_x=359)

Figure 5: Comparison of Domain LLM Training Approaches. CPT and SFT offer methods to enhance domainspecific performance based on existing LLMs, while $F U L L$ parameter training and $P E F T$ represent different technical pathways within these training processes.

To adapt general LLLs to cybersecurity, researchers primarily employ two methodologies: continual pre-training and supervised fine-tuning.

Continual pre-training involves further training of already pre-trained LLMs using a large volume of unlabeled domain-specific data [40, 41, 52, 53]. This method aims to improve the model's understanding and application of domain knowledge, significantly enhancing its broad applicability within the cybersecurity field. Continual pretraining is based on the core assumption that even after extensive pre-training, models retain the potential for further enhancement, particularly regarding their performance across specific domains or tasks. The process typically involves several key steps: firstly, selecting a dataset that represents the characteristics of the target domain appropriately; secondly, determining the strategy for continual pre-training; and lastly, executing the pre-training, adjusting the model architecture or optimization algorithms if necessary, to adapt to new training objectives.

Supervised Fine-Tuning, on the other hand, uses labeled domain-specific data for training, enabling direct optimization of the model's performance on specific cybersecurity tasks 42, 43]. Compared to continued pre-training, SFT is more narrowly focused on enhancing task-specific performance. In SFT, model weights are refined by the gradients calculated from a task-specific loss function. This function quantifies the deviation between the model's predictions and the actual labels, thus facilitating the learning of task-oriented patterns and nuances. SFT relies on the utilization of high-quality, human-annotated data, which is a collection of prompts and their corresponding responses. Supervised fine-tuning is especially important for LLMs such as ChatGPT, which have been designed to follow user instructions and stay on a specific task across long stretches of text. This specific type of fine-tuning is also referred to as instruction fine-tuning.

In the context of continual pre-training and SFT, researchers have the option of employing either full-parameter fine-tuning or parameter-efficient fine-tuning.

Full-parameter fine-tuning is a traditional approach where the entire model's parameters are adjusted during training. This allows the model to fully adapt and specialize to the target domain's nuances. By refining all parameters, the model can potentially achieve optimal performance for specific tasks or datasets. However, this exhaustive parameter update demands considerable computational power and time, posing challenges in terms of efficiency and scalability, especially with increasing LLMs.

Conversely, PEFT methods only fine-tune a small number of (extra) model parameters while freezing most parameters of the pre-trained LLMs, thereby greatly decreasing the computational and storage costs. And it also helps in portability wherein users can tune models using PEFT methods to get tiny checkpoints worth a few MBs compared to the large checkpoints of full fine-tuning. The PEFT approaches are favored because they enable users to get performance comparable to full fine-tuning while only having a small number of trainable parameters. There are several PEFT methods, such as adapter tuning, prefix tuning, prompt tuning, LoRA, QLoRA, and so on:

Adapter tuning [54] inserts adapters after the multi-head attention and feedforward layers in the transformer architecture, which updates only the parameters in the adapter during fine-tuning while keeping the rest of the model parameters
frozen. $P$-tuning [55] automatically learns optimal task-specific prompt embeddings by introducing trainable prompt tokens, eliminating the need for manual prompt design and potentially improving performance with the addition of anchor tokens. Prefix tuning [56] keeps the language model parameters frozen and optimizes small, continuous, task-specific vectors called prefixes. Prompt tuning [57] fine-tunes for specific tasks through learning soft prompts by backpropagating and merging labeled examples. LoRA [58] is a small trainable submodule that can be inserted into the transformer architecture, involving freezing the pre-trained model weights and injecting a trainable low-rank decomposition matrix into each layer of the transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. After the training is finished, the parameters of the low-rank decomposition matrix are combined with the parameters of the original LLM for use. QLoRA [59] is a further optimization of LoRA, which carries out gradient backpropagation to a low-rank adapter with a frozen 4-bit quantized pre-trained language model, drastically reducing the memory requirement for fine-tuning while being almost comparable to full fine-tuning.

By integrating these techniques, researchers can select appropriate methods to construct LLMs tailored to the specific needs and conditions of the cybersecurity domain, as shown in Figure 5. Furthermore, emerging technologies also provide insights for the construction of cybersecurity LLMs. For example, model editing techniques [60, 61] can precisely modify LLMs to incorporate cybersecurity knowledge without negatively impacting other knowledge. Prompt engineering [62, 63, 64], by designing effective prompts to guide LLMs towards desired outputs, can alleviate the bottleneck of training data and resources required for constructing cybersecurity LLMs.

### 3.2 Selection of Base Model for Constructing Domain LLM by Evaluating Cybersecurity Capabilities

As mentioned above, it is challenging to train a cybersecurity LLM from scratch. The general practice is to choose a general-purpose LLM as the base model and then fine-tune it. However, how to select the appropriate base model among various LLMs? The basic idea is to choose the LLM with strong cybersecurity capabilities or those that perform well in specific security tasks. Such models are better at understanding and addressing security-related issues. Existing evaluation of LLM cybersecurity capabilities can be categorized into three main categories: cybersecurity knowledge, secure code generation, and IT operations capability.

Cybersecurity knowledge evaluation focuses on evaluating the model's understanding of cybersecurity concepts and its ability to provide accurate information on security threats and mitigation strategies. CyberBench [65] emerges as a domain-specific, multi-task benchmarking tool for evaluating LLMs' capabilities in cybersecurity tasks. As a benchmark suite for LLMs in the cybersecurity domain, CyberBench offers a generic yet consistent approach, mitigating the limitations previously encountered in the evaluation of LLMs within this field. SecEval [66] is created for evaluating cybersecurity knowledge in LLMs. It offers over 2000 multiple-choice questions across 9 domains: Software Security, Application Security, System Security, Web Security, Cryptography, Memory Safety, Network Security, and PenTest. By facilitating the evaluation of ten state-of-the-art foundational models, this study offers new insights into their performance in the cybersecurity domain. By integrating expert knowledge with the collaboration of LLMs, Norbert T et al. [45] crafted the CyberMetric benchmark dataset, containing 10,000 questions aimed at evaluating the cybersecurity knowledge of various LLMs within the cybersecurity field. Moreover, SecQA [67], a dataset of multiple-choice questions generated by GPT-4 based on the "Computer Systems Security: Planning for Success" textbook, aims to specifically assess LLMs' understanding and application of security principles. Offering questions at two tiers of complexity, SecQA not only functions as an evaluation tool but also facilitates the progression of LLM applications within contexts demanding heightened security awareness.

Secure code generation tests the model's capability to generate code that is not only functional but also adheres to security best practices, aiming to minimize vulnerabilities. Manish B et al. [46] introduced a security coding benchmark named CyberSecEval, aimed at assessing the potential security risks and tendencies to facilitate cyber attacks when LLMs generate code. Through evaluating seven models, including Llama 2, Code Llama, and OpenAI's GPT, CyberSecEval effectively pinpointed key cybersecurity risks and provided practical insights for model improvement. Catherine T et al. [47] introduced LLMSecEval, a dataset comprising 150 natural language prompts based on the narrative descriptions of various vulnerabilities featured in MITRE's Top 25 Common Weakness Enumeration (CWE) rankings. By comparing the code generated by LLMs to the secure implementation examples for each prompt, LLMSecEval could evaluate the security of code generated by LLMs. Siddiq M et al. [68] proposed SecurityEval focuses on the security evaluation of code-generative models to prevent the creation of code susceptible to exploitation, averting potential misuse by developers. This dataset includes 130 samples covering 75 types of vulnerabilities, mapped to CWE. Kamel A et al. propose PythonSecurityEval [69], a real-world dataset collected from real-world scenarios on Stack Overflow to evaluate the LLMs' ability to generate secure python code and their capacity to fix security vulnerabilities. DebugBench [70], with 4,253 instances, covers four major bug categories and 18 minor types in $\mathrm{C}++$, Java, and Python. This comprehensive evaluation elucidates the strengths and weaknesses of LLMs in automated debugging, marking a significant stride in understanding their applicability and constraints in practical coding scenarios.

IT operations capability evaluation of the model's proficiency in managing and securing IT infrastructures, including aspects such as network security situation awareness, security threat analysis, and incident response. Yukai M et al. [71] introduced NetEval, an evaluation set designed to measure the common knowledge and reasoning abilities of LLMs in NetOps within a multilingual context. NetEval comprises 5,732 questions related to NetOps, spanning five distinct NetOps subdomains. Through NetEval, researchers systematically evaluated the NetOps capabilities of 26 publicly available LLMs. Additionally, OpsEval [72] contains 7184 multi-choice questions and 1736 question-answering formats in English and Chinese, designed for tasks such as fault root cause analysis, operational script generation, and alert information summarization, to comprehensively assess the performance of LLMs in IT operational tasks.

Evaluating the cybersecurity capabilities of LLMs not only guides selecting base models during fine-tuning but also demonstrates that general LLMs possess certain cybersecurity capabilities. This supports the feasibility of directly utilizing LLMs (without the need for fine-tuning) to aid cybersecurity applications, as discussed in section 4 . Furthermore, those studies help researchers and developers recognize the limitations of LLMs in the field of cybersecurity, offering direction for advancing artificial intelligence towards higher standards and more specialized security development.

### 3.3 Fine-tuned Domain LLMs for Cybersecurity

Researchers have leveraged the aforementioned technologies and base models to customize LLMs for addressing specific issues in the field of cybersecurity. These efforts highlight the profound potential of integrating domain-specific knowledge to enhance the capabilities of language models, especially for critical applications including vulnerability detection, fault Localization, program repairing, and so on.

Vulnerability detection involves identifying and classifying potential security weaknesses in software code. Alexey S et al. [73] fine-tunes WizardCoder [74] with Lora specifically for the task of vulnerability detection, focusing on the binary classification of whether Java functions contain vulnerabilities. Ferrag M et al. [48] performs partial parameter fine-tuning on FalconLLM [4] with C code samples, resulting in SecureFalcon, which can distinguish between vulnerable and non-vulnerable samples with a high detection accuracy of up to $96 \%$, and further proposes a method for repairing vulnerabilities using FalconLLM. Aidan Y et al. [75] introduces LLMAO, a new language model-based approach for fault localization, which adds bidirectional adapter layers on CodeGen [76, 77], enables the model to learn bidirectional representations of code and predict the probability of defects in code lines.

Secure code generate via LLMs endeavors to augment the safety of auto-generated code, mitigating vulnerability risks. Storhaug A et al. [78] presents an innovative method known as vulnerability-constrained decoding, which integrates vulnerability tags during model training. Avoiding the generation of tagged vulnerabilities during decoding significantly reduces the generation of vulnerable code. Fine-tuning on GPT-J [79] evidences a notable reduction in synthesized code vulnerabilities. Jingxuan $\mathrm{H}$ et al. [80] focuses on improving the safety of code generation by LLMs through instruction tuning. Supervised fine-tuning with a dataset containing both secure and insecure programs converts CodeLlama [34] into SafeCoder, thereby achieving a notable enhancement in security (approximately $30 \%$ ) across various popular LMs and datasets while maintaining practical usability.

Automated program repair aims at automatically fixing a software bug without human intervention. André S et al. [28] presents a new program repair approach called RepairLLaMA, which significantly improves LLMs' program repair capabilities by applying Lora fine-tuning to CodeLlama. It outperforms GPT-4 on the Java benchmarks Defects4J and HumanEval-Java.

Binary is the most basic form of computer code, it is important to learn what it means and how to use it. Nan J et al. [81] bring the benefit of LLMs to the binary domain, which continual train StarCoder [35, 36] on a specialized binary code pre-training corpus and new tasks, leads to the development of Nova and Nova ${ }^{+}$. After SFT, the enhanced LLMs effectively address specific tasks such as binary code similarity detection, binary code translation, and binary code recovery.

IT operations is entrusted with maintaining routine tasks and other activities that maintain the infrastructure for other services. Hongcheng G et al. [82] describes the development of a specialized LLM for IT operations, named Owl, achieved by supervised fine-tuning of Llama on the collected Owl-Instruct dataset. Owl surpasses existing models in IT-related tasks and demonstrates effective generalization capabilities on the Owl-Bench benchmark.

Cybersecurity knowledge assistants help to improve users' security awareness and assist users in defending against cyber attacks through interaction with users. Jie Z et al. [29] propose Hackmentor, by developing the dataset of instructions and conversations tailored to the cybersecurity field and applying LoRA fine-tuning to Llama and Vicuna [3] based on this dataset, effectively demonstrates the extensive potential of LLMs in cybersecurity applications.

These studies show the significant promise of LLMs in cybersecurity, which not only validates the effectiveness of adapting LLMs via SFT and continual pre-training but also opens new avenues for future research related to cybersecurity.

Answer to Q1: For researchers, it is a feasible technical route to construct the cybersecurity LLM by tuning a general LLM with cybersecurity data using methods such as CPT and SFT, and implementation techniques depend on the specific application scenario, resource availability, and the expected level of performance improvement.

## 4 RQ2: What are the potential applications of LLMs in cybersecurity?

Organized into distinct categories, the subsequent sections scrutinize the multifaceted contributions of LLMs, ranging from threat intelligence and code review to vulnerability discovery and program repair. By elucidating the pivotal advancements within each thematic cluster, this review seeks to provide a holistic perspective on the evolving landscape of information security bolstered by the integration of LLMs.

![](https://cdn.mathpix.com/cropped/2024_05_29_995460fc12e55382f0c9g-09.jpg?height=1201&width=1637&top_left_y=955&top_left_x=236)

Figure 6: An overview of RQ2.

### 4.1 Threat Intelligence

Due to the remarkable analytical and summarization capabilities demonstrated by large language models in Natural Language Processing (NLP) tasks, some researchers are endeavoring to employ them in assisting the generation and analysis of Cyber Threat Intelligence (CTI).

Shaswata M et al. [83] introduced a framework known as LocalIntel, designed to furnish users with reliable threat intelligence by allowing LLM to summarize knowledge after querying Global \& Local Knowledge Database. Global Knowledge mainly refers to well-documented reports on cybersecurity threats from CWE and CVE while Local Knowledge is customized by the organization for practical purposes, thereby supplementing the Global Knowledge. Filippo P et al. [84] also conducted a similar work on utilizing LLM to extract security knowledge from extensive knowledge repositories and generate reports automatically. A few similar efforts are as follows. Reza F et al. [85] employed LLM to generate descriptions of cyber attacks, subjecting the model to fine-tuning with information gathered from ATT\&CK and CAPEC. Then, they compared the performance between fine-tuned LLM (e.g. BERT) with directly used LLM (GPT-3.5) on describing attacks. In another work, Reza F et al. [86] examined the application of LLMs in cybersecurity to interpret and summarize cyberattack Tactics, Techniques, and Procedures (TTPs) from the MITRE ATT\&CK framework. It compares the effectiveness of encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5) models for TTP analysis and introduces Retrieval Augmented Generation (RAG) to enhance decoder-only models without fine-tuning. The study finds that RAG significantly improves the interpretation of TTPs by providing relevant context, highlighting the potential of LLMs in threat intelligence. Tanmay S et al. [87] discussed the capability of LLMs to automate the analysis and summarization of software supply chain security breaches. They evaluated LLMs' performance in replicating manual assessments of 69 failures, focusing on categorization accuracy. Results indicate that LLMs show promise, particularly with comprehensive data, but still fall short of replacing human analysts in this nuanced domain. Samaneh S et al. [88] evaluated the performance of various large language models, including ChatGPT, GPT4all, Dolly, and others, in the domain of threat intelligence. Utilizing a Twitter-based dataset for Open-Source Intelligence (OSINT), the research assesses these chatbots' capabilities in binary classification and Named Entity Recognition (NER) tasks. While the LLMs demonstrate promising results in binary classification, their effectiveness in NER for cybersecurity entity recognition is limited, highlighting the need for further advancements in LLM technology for enhanced CTI applications. Particular for digital forensics, Gaetan M et al. [89] proposed a method to automate the generation of reports. They examined the structure of forensic reports to identify common sections and assesses the feasibility of LLMs in generating these sections. Through a case study approach, the article evaluates the strengths and limitations of LLMs in creating different parts of a forensic report.

Particularly given that most threat intelligence providers offer information in an unstructured format, Giuseppe S et al. [90] and Yuelin H et al. [91] proposed innovative solutions to tackle the prevalent issue of extracting useful info from unstructured information. The former designed a framework named aCTIon, which includes downloading/parsing raw reports, extracting useful info with LLM, and exporting structured reports following STIX [92] standard. The latter constructed Knowledge Graph for unstructured threat intelligence and fine-tuned LLM to automate this task.

In addition to extracting valuable information from massive amounts of text, report de-duplication is also an important research point in the field of threat intelligence. Ting Z et al. [93] relied on LLM to alleviate the problem of bug report de-duplication. They leveraged LLM as an intermediate step to improve the performance of REP [94] (a traditional method on measuring the similarity between bug reports) by acting as a role in identifying key words.

Also, there have been some researches attempt to use LLMs as experienced security response specialists. YuZheng L et al. [95] uses LLM as a suggestions provider about mitigating vulnerabilities through prompt engineering. They design a system which can be used to retrieve relative CVE \& CWE information after input a description of vulnerabilities by user. LLM's mitigation suggestion is a sub-part of this system. Mehrdad K et al. [96] treat LLM not only as a question-and-answer assistant with specialized knowledge but also perform actions based on the user's description (e.g. giving instructions to the host computer's intrusion detection system to block specific IP). To enhance strategic reasoning in cybersecurity, 97] introduced Crimson, a system leveraging large language models to correlate CVEs with MITRE ATT\&CK techniques for improving threat anticipation and defense. Their key idea refers to a Retrieval-Aware Training (RAT) process, which refines LLMs to generate precise cybersecurity strategies, significantly reducing errors and hallucinations. By integrating real-time data retrieval and domain-specific fine-tuning, Crimson elevates the models' interpretability and strategic coherence, offering a proactive approach to cybersecurity threat intelligence.

### 4.2 Fuzz

Traditional fuzzing techniques, while effective in uncovering software vulnerabilities, have inherent limitations that can hinder their efficiency and effectiveness. One significant drawback is that traditional fuzzers operate in a largely random or semi-random manner, which can result in a time-consuming and sometimes ineffective approach to testing, as they may not explore all possible execution paths. Additionally, the seed to mutate is often crafted by human which is time-consuming. Although all of the above problems have been researched over the years and there are many methods to mitigate them, the emergence of LLM has provided a whole new way of thinking in the fuzz field

## What are the advantages of LLM fuzz over traditional methods?

Ying Z et al. [98] evaluated the performance of ChatGPT in generating test cases directly (without tuning) and compared it to two traditional testing tools (SIEGE and TRANSFER). Their expirements showed that When a detailed description of the vulnerability, possible exploits, and code context is given, LLM outperforms the conventional approach.

The following is a description of the advantages of LLM over traditional tools. One of the most important factors lies in the emergence of LLM leads to guided mutations from random mutations. Jie H et al. [99] add an GPT-based seed mutator to the traditional gray-box fuzzy test, which selects seeds from the seed pool and requests variants from ChatGPT to generate higher quality inputs. Another factor is that LLMs have a good cross-programming languages understanding and is therefore able to perform testing tasks in multiple programming languages. Chunqiu $\mathrm{S} \mathrm{X}$ et al. [100] took full advantage of LLM's comprehension on different programming languages. Most of traditional methods can only fuzz specific programing language while LLM-based fuzz could cover different languages. They test 6 languages code (C, C++, Go, SMT2, Java and Python) with a method named Fuzz-Loop which automatically mutate test cases. Most traditional fuzz methods don't achieve high code coverage in all code, and LLMs who have mastered the logic of the code can generate more targeted test cases against those low coverage code. For example, Caroline $\mathrm{L}$ et al. [101] used Codex to generate test cases against low-coverage functions when SBST (Search-Based Software Testing, a traditional fuzz method) reaches coverage plateau. Specifically, the raw character sequences generated by the Codex are deserialized into an internal test case representation of SBST in order to take advantage of SBST's mutation operations and fitness functions.

## Specific Fuzzing strategies for different testing objects.

Depending on what is being tested, the strategy needs to be adapted somewhat when using LLM. For testing against general APIs Cen $\mathrm{Z}$ et al.[102] investigated the effectiveness of LLMs in generating invocation code. The study compares LLM-based generation with traditional program analysis methods and finds that LLMs can automatically produce a significant number of effective fuzz drivers with less manual intervention. The research introduced query strategies, iterative improvements, and the use of examples to enhance LLM performance. Though it's all about testing APIs, the strategy for testing Deep-Learning Libraries would need to be modified. Programs calling deep learning libraries usually have strict requirements on tensor dimensions, otherwise fuzzers will perform a lot of meaningless tests. Yinlin D et al. [103] proposed TitanFuzz, a tool generating testing cases for Deep Learning Library. Their training corpus contains a large number of code snippets which call the DL library API, so that the language syntax/semantics and complex DL API constraints can be implicitly learned to efficiently generate DL programs. Another work, FuzzGPT [104], which is also conducted by Yinlin D et al., is also a research about fuzzing DL library. In contrast to the previous work, FuzzGPT focuses on using historical error-triggered code snippets to guide LLM generated test cases.

In addition to the above researches, we have collected some papers for other testing objects. Testing against Protocol. Ruijie M et al. [105] discussed how to find security vulnerabilities in protocol implementations in the absence of a machine-readable protocol specification. They trained LLM with massive human-readable protocol docs and asked LLM to mutate interactive messages for Protocol Fuzz (e.g. HTTP). Testing against BusyBox. Specifically targeting BusyBox, a prevalent utility in Linux-based devices, Asmita et al. [106] introduced two methods: leveraging LLMs to generate target-specific initial seeds for fuzzing, which significantly increases the efficiency in identifying crashes and potential vulnerabilities; and crash reuse, which employs previously acquired crash data to streamline the testing process for new targets.

### 4.3 Vulnerability Detection

This section provides an overview of key research papers that delve into vulnerability detection using LLMs. By examining these works, we aim to elucidate the advancements, challenges, and future directions in leveraging LLMs for enhancing cyber security.

(In this section, we blur the concepts of "vulnerability" and "software defect")

## Whether LLMs have the ability to detect vulnerabilities?

The several following papers conduct fundamental researches on this question. Though they may differ in their results on this question for a number of unknown reasons (e.g., maybe using different datasets), but overall they all illustrate that LLM holds good promise for this issue of vulnerability detection.

In the early stages, Anton C et al. [107] conducted tests to assess whether GPT-3 and GPT-3.5 could identify some known CWE vulnerabilities in Java code. The result shows that the application in vulnerability detection tasks is not that good and requires further refinement and investigation. In another work, Moumita D et al. [108] used LLMs (including GPT-3.5, CodeGen and GPT-4) to analyze several common vulnerabilities (e.g. SQL injection, overflow). The conclusion is that although it does confirm that LLM indeed has the ability to detect vulnerabilities, the false positive rate is high. However, Marwan O [109] fine-tuned GPT on various benchmark datasets of vulnerable code
to detecting software vulnerabilities. The results have a favorable performance. Similarly, Avishree K et al. [110] concluded that LLMs (including GPT-4 and CodeLlama) are often able to perform vulnerability detection better than existing static analysis and deep learning-based tools. By carefully designed prompt, desirable results can be obtained on synthetic datasets, but performance degrades on more challenging real-world datasets. Rasmus I T J et al. [111] compared the performance of a wide range open-source models and proprietary models with Python code snippets on assisting vulnerabilities discovery. Their research suggests that LLMs can be effectively utilized to enhance the efficiency and quality of code reviews, particularly in detecting security issues within software code. Alexey S et al.[73] fine-tuned WizardCoder for vulnerabilities detection task and investigated whether the encountered performance limit is due to the limited capacity of CodeBERT-like models. Their findings demonstrate that LLM has a promising future on vulnerability detection. Haonan L et al. [112] presents LLift, a framework that leverages LLMs to assist in static program analysis, specifically for detecting Use-Before-Initialization (UBI) defects. LLift interfaces with static analysis tools and LLMs, demonstrating a precision rate of $50 \%$ in real-world scenarios and identifying 13 previously unknown UBI bugs in the Linux kernel.

## Improving detection capabilities through different strategies.

Instead of supplying code to LLMs and asking for answer directly, many researchers conduct different strategies ahead. Partial researchers consider that simply providing the code is not enough, that is to say, the code needs further pre-processing or more information needs to be provided to LLM for vulnerability reasoning. Instead of providing the code directly to the model, Jin W et al. [113] did a Code Sequence Embedding (CSE) that combines the AST, DFG and CFG of the code as input to the model. They captured the semantic information of the input with the help of Conformer mechanism (an improved architecture of Transformer [114]). Chenyuan Z et al. [115] not only provided GPT with the code but also furnished API calling sequences and data flow diagrams. Atieh B et al. [116] carried out a similar experiment. They made a comparison by giving the model different levels of information (asking for the vulnerability point directly, giving some CWE information before asking LLM to locate, and telling LLM what vulnerabilities are in the code before asking LLM to locate). Noble S M et al. [117] focused on Android platform vulnerabilities and compared the performance of LLM on three conditions: asking LLM finding vulnerabilities directly, providing a summary of the vulnerability before asking, and conferring LLM the permission on requesting any files it wants in APK after only providing the core of the APK (AndroidManifest.xml and MainActivity.java).

In addition to those efforts above, researchers have come up with a number of innovative ideas to improve LLM's vulnerability detection capabilities. Sihao H et al. [118] proposed an innovative two-stage framework named GPTLENS which includes two adversarial agent roles: auditor and critic. The role of the auditor is performed during the generation phase and its main goal is to identify potential vulnerabilities in the smart contract. The role of the critic is performed during the identification phase and its main goal is to evaluate the vulnerabilities generated by the auditor. Zhihong L et al. [119] used traditional algorithms (TF-IDF and BM25) to match the code to be analyzed with codes in vulnerability corpus for similarity. The code to be analyzed, along with similar code in the corpus, will be given to LLM together. Based on the idea of In-Context learning, LLM can better analyze whether or not it belongs to this kind of vulnerability. Specifically for vulnerability detection in smart contracts, Yuqiang S et al. [120] propose a tool called GPTScan. GPTScan first parses the smart contract project to determine the reachability of the functions and only potentially vulnerable functions will be retained. Then GPTScan uses GPT to match candidate functions with predefined vulnerability types. At last, GPTScan asks GPT to verify the vulnerability. To improve LLM's ability to reason about vulnerabilities, Yuqiang S et al. [121] proposed LLM4Vuln, which separates the vulnerability reasoning capabilities of LLMs from others (e.g., proactively seeking additional information, employing relevant vulnerability knowledge, and following instructions to output structured results). They allow LLMs to ask for obtaining additional contextual information about the target code. Moreover, they concluded that it is not the case that the more information input to the LLM, the better performance will be. For example, full vulnerabilities report, large amount of invocation context, providing too much may lead to distractions. Zhenyu M et al. [122] presented a novel approach called MuCoLD which simulates a multi-role code review process for vulnerability detection in software. By enacting different roles, such as developers and testers, LLMs engage in discussions to reach a consensus on the presence and classification of vulnerabilities. The method, initialized with a binary judgment and reasoning, progresses through iterative dialogue to refine assessments.

The prevailing idea is to detect vulnerabilities in specific programs, but there has also been research into inferring the lists of the affected library from the vulnerability with the help of LLM. Tianyu C et al. concluded that many vulnerability reports in the NVD do not list the affected libraries, or list incomplete or incorrect library names, increasing the risk of third-party library vulnerabilities. They proposed a method named VulLibGen [123], aiming to detect vulnerabilities in third-party libraries. VulLibGen takes only the vulnerability descriptions as input and uses a priori knowledge of LLMs to generate a list of affected library names for a given vulnerability.

With a different focus from the previous researches, Peiyu L et al. [124] proposed a method about applying ChatGPT's to vulnerability management, assessing its capabilities in predicting security bugs, evaluating severity, repairing vulnerabilities, and verifying patch correctness using a large dataset. The study reveals that while ChatGPT can assist in identifying and mitigating software security threats, improvements are needed for tasks like vulnerability prioritization and patch validation.

Dataset Preparation In addition to the methods used to re-train or fine-tune the models, the construction of dataset is also an essential aspects.

Yizheng C et al. [125] present a new vulnerable source code dataset, which is called DiverseVul, containing 18,945 vulnerable functions (covering 150 CWEs) and 330,492 normal functions. All of these samples is $\mathrm{C} / \mathrm{C}++$ code. Furthermore, they discussed 11 different deep learning architectures and concluded that despite the success of LLMs, the models still face the challenges of high false positives, low F1 scores, and difficulty in detecting complex CWEs for vulnerability detection. Norbert T et al. [126] produced a dataset of 112,000 C code containing vulnerabilities, labeled in detail with information about the vulnerability (CWE number, location, and function name). All the codes in this dataset are generated by GPT-3.5. Zeyu G et al. 127] present a comprehensive vulnerability benchmark dataset called VulBench that includes high-quality data from CTF challenges and real-world applications with detailed annotations of vulnerability types and causes for each vulnerable function.

## 4.4 (In)secure Code Generation

There has been a lot of previous work that confirms that big prediction models do have good code understanding ability. However the security of the generated code is very significant. Many studies have been carried out on whether the LLMs can supply secure code.

## Evaluation of the security of LLM-generated code.

Does the code generated by LLM pose a security risk? Gustavo S et al. [128] conducted an experiment to explore whether code written by undergraduate computer-science majors with the assistance of LLM has security risks. The participants were tasked with implementing a singly-linked 'shopping list' structure in $\mathrm{C}$ and were divided into two groups: 'control' (without Codex LLM access) and 'assisted' (with Codex LLM access). The results show that LLM do not significantly increase the risk of introducing security vulnerabilities when used as code assistants. Florian T et al. [129] conducted an empirical study investigates bugs in code generated by Large Language Models (LLMs), focusing on three leading models: CodeGen, PanGu-Coder, and Codex. The research identifies 10 distinct bug patterns within 333 collected errors and validates these patterns through an online survey of 34 LLM practitioners and researchers.

So far, many researches have been conducted to explore the security of state-of-art LLM-generated code. Hammond P et al. [130] investigates the security of code generated by GitHub Copilot. The researchers designed 89 different scenarios for Copilot to complete, resulting in 1,689 programs. They analyzed these programs for vulnerabilities, particularly focusing on the top 25 CWEs counted by MITRE. [131] delve into the potential of LLMs in security-oriented program analysis. They focus on two representative LLMs, ChatGPT and CodeBERT, and evaluate their performance in solving analytic tasks of varying difficulty levels (including Vulnerabilities Analysis and Bug Fixing, Fuzzing, Assemble Code Analysis, etc.). Zhijie L [132] evaluated the code generated by ChatGPT, focusing on correctness, understandability, and security. Through an empirical study using LeetCode problems and CWE scenarios, they analyzed the quality of code snippets produced by ChatGPT and its ability to engage in multi-round dialogue for code improvement. The results reveal that while ChatGPT can generate functionally correct code, it struggles with complex reasoning and maintaining code security.

On the others hand, Mohammed L et al. [133] proposed a framework named SALLM, which is a benchmark specifically designed for evaluating the security of code generated by Large Language Models. SALLM consists of three components, a dataset of prompts describing the text of Python programs, a code generation environment that requires different solutions from LLM and a systematic model assessment method utilizing Docker to execute generated code. They test their method on 5 LLMs. Jiawei L et al. [134] focused on the quality evaluation of code generation. Existing benchmarks typically contain only a limited number of test cases. To address this problem, they propose EvalPlus, a code synthesis evaluation framework that dramatically expands the number of test cases in the evaluation dataset through an automated test input generator (combining LLM and mutation-based strategies). Saad U et al. [135] constructed a series of 228 code scenarios and analysed eight state-of-the-art LLMs in an automated framework to determine whether LLMs can reliably identify security-related vulnerabilities. They pointed out that current LLMs have yet to perform satisfactorily in automated vulnerability detection tasks, and lists a series of shortcomings exhibited by current LLMs.Alessio B [136 evaluated the performance of ChatGPT-3.5 on generating code (including the security of code), and they analyzed the potential of this ability on 10 programming languages.

## Do LLMs know whether the generated code is safe or not?

Raphaël $\mathrm{K}$ et al. [137] conducted a number of experiments to validate the security of LLM-generated code and find vulnerabilities in generated code in different scenarios. Although LLM may identify the vulnerabilities in the code generated itself after being asked to review, the experiments show that LLM still generates unsafe code unless explicitly indicating to do so. They also mentioned a critical problem in their research due to the non-explainability of deep neural networks, repeated questions to LLM often result in different answers (whether the code is insecure or not) without being able to find a strategy that maximises successful identification.

More directly, Jingxuan H et al. [138] try to specify LLM to generate safe or unsafe code through some mechanisms. They proposed a method named svGen which make LLM generate safe or unsafe code according to the user's needs. In addition to the descriptions for the generated code, they introduced property-specific continuous vectors, known as prefixes, which are sequences of vectors that match the shape of the LLM's hidden states. These prefixes are optimized to influence the LLM's generation process by providing initial hidden states that steer the code towards meeting the desired security criteria, all without modifying the underlying weights of the LM."

For enhancing the security of LLM-generated code, Jingxuan H et al. [80] introduced SafeCoder, an innovative instruction tuning approach that enhances the security of code generation by LLMs. It effectively combines standard instruction tuning with security-specific fine-tuning using a high-quality dataset collected via an automated pipeline from GitHub. SafeCoder significantly improves code security without compromising the LLMs' utility across various tasks, demonstrating its versatility and applicability in boosting the safety of LLM-generated code.

## LLMs as static analysis assistant.

Hammond P et al. [139] explored the application of LLM, such as OpenAI's Codex, in the field of reverse engineering, particularly in understanding software functionality and extracting information from code. LLMs are mainly used to analyse the functionality of $\mathrm{C}$-like codes given by reverse engineering tools such as Ghidra. These $\mathrm{C}$ codes are obtained from binary files through a decompilation process. In reverse engineering, decompiling is also an important task. Hanzhuo T et al. [140] introduced LLM tailored for decompilation, focusing on converting compiled machine code back into human-readable source code. They fine-tuned a LLM called DeepSeek-Coder on massive C code and assembly code pairs and evaluated the performance of their work by recompiling \& executing the decompiled code. Chongzhou F et al. [141] explored the potential and limitations of Large Language Models for code analysis tasks, especially in the context of dealing with obfuscated code. In their experiments, they also conducted experiments that allowed LLMs to generate de-obfuscated versions of code, that is to say, to recover the original, more readable code from the obfuscated code.

Jianyu Z et al.[142] focused on how to improve LLM's semantic understanding of programs through fuzz testing. Their core idea is that programs with their basic units (i.e., functions and subroutines) are designed to exhibit diverse behaviors and provide possible outputs given different inputs. Thus, through fuzz, various inputs trigger different functions of the code that can help LLMs understand the program. David N et al. [143] introduced ASTxplainer, an explainability method for Large Language Models used in coding. It aligns token predictions with Abstract Syntax Tree (AST) nodes, enabling detailed evaluation and visualization of model predictions. ASTxplainer consists of AsC-Eval for structural performance estimation, AsC-Causal for causal analysis, and AsC-Viz for visualization, which are able to better explain LLM.

Pei Y et al. [144] focused on how large-scale language models can be utilized to aid in dynamic analysis of malware. The core idea of the research is to use GPT-4 to generate explanatory text for each API call, and then use a pre-trained language model, BERT, to generation a series of the API sequence to execute based on former analysis. This approach can theoretically generate representations for all API calls without the need to train the dataset during the generation process. Himari F et al. [145] utilizes large language model, specifically ChatGPT, to analyze the linguistic and strategic elements of ransomware communications. By examining a collection of ransomware samples, the research identifies patterns and tactics used in ransom notes, revealing the evolution of ransomware tactics characterized by sophisticated language use and psychological manipulation. P.V. Sai C et al. [146] also discussed the potential and challenges of LLMs in terms of policy generation against ransomware. Nusrat Z et al. [147] employed GPT-3 and GPT-4 to detect potential malware in the npm ecosystem by analyzing JavaScript packages. The study introduced SocketAI Scanner, a multi-stage workflow that utilizes iterative self-refinement and Zero-Shot Role-Play Chain of Thought prompting techniques to enhance the model's ability to identify malicious intent within code. By comparing LLMs' performance against static analysis tools, the paper demonstrates that LLMs can effectively pinpoint malware with higher precision and lower false positives.

## LLMs as dynamic debugging assistant.

Runchu T et al. [70] introduced DebugBench, a benchmark for evaluating large language models' debugging capabilities in programming. It comprises 4253 instances across various bug categories in C++, Java, and Python. The benchmark is constructed by collecting code snippets from LeetCode, implanting bugs with GPT-4, and conducting rigorous quality checks. Zhe L et al. [148] addressed the challenge of automated Graphical User Interface (GUI) testing for mobile applications. They proposed a novel approach called GPTDroid, which formulates the GUI testing problem as a question $\&$ answering (Q\&A) task, where the LLM is asked to chat with the mobile apps by passing GUI page information to elicit testing scripts. These scripts are executed and the app feedback is iteratively passed back to the LLM to guide further exploration. Baleegh A et al. [149] presented an approach called FLAG to assist human debuggers in identifying and localizing security and functional bugs in code. FLAG works by inputting a code file and regenerating each line within that file for self-comparison. It compares the original code with LLM-generated alternatives to flag notable differences as anomalies for further inspection.

### 4.5 Program Repairing

The software development life-cycle is deeply impacted by the presence of bugs, with their detection and resolution taking enormous expenses. Researchers are motivated to find new methodologies to discern and rectify the bugs/vulnerabilities automatically.

## Evaluation of existing LLMs on program repairing.

Against state-of-art different LLMs (open-sourced or proprietary), a number of studies have evaluated their ability for program repairing. Julian Ar et al. [150] explored the application of OpenAI's Codex model to the field of automatic program repair (APR), specifically its ability to locate and fix bugs in software. They used the QuixBugs benchmark (containing 40 bugs in Python and Java) to evaluate Codex's performance on the APR task. Though withou retraining, the performance of Codex exceeds many existing APR techniques. Dominik S et al. [151] conducted a similar work with the previous one. They both tested LLM for automatic program repair on QuixBugs benchmark. In this work, ChatGPT is evaluated instead of Codex. Jan N et al. [152] discussed the application of large language models, Gemini, in automating the repair of software vulnerabilities, especially for vulnerabilities found by the sanitizer tool in $\mathrm{C} / \mathrm{C}++$, Java, and Go code. The authors argue that while success rate may seem modest, it has the potential to save significant engineering effort over time. Jiaxin Y et al. [153] evaluated three LLMs: Gemini Pro, GPT-4, and GPT-3.5, on codes with identified security defects from real-world code reviews. The findings indicate that GPT-4 performs relatively better than the other models, but all LLMs exhibit significant room for improvement, particularly in terms of response conciseness, clarity, and accuracy. Chunqiu S et al. [154] selected nine LLMs and compared them with traditional automated program repair methods, illustrating the superior performance of large language models in this field.

Hammond $\mathrm{P}$ et al. [155] examined the potential of large language models for zero-shot vulnerability repair in code. Through a large-scale experiment with multiple LLMs on synthetic, handcrafted, and real-world security scenarios, they demonstrate that while LLMs show promise in repairing simple scenarios, they struggle with more complex, real-world examples. The paper contributes to the understanding of LLMs' capabilities in cybersecurity and encourages further exploration into their use for vulnerability repair. Yi W et al. [156] compared the capabilities of LLMs and deep learning-based APR models in fixing Java vulnerabilities. They evaluated the performance of five LLMs (Codex, CodeGen, CodeT5, PLBART, and InCoder), four fine-tuned LLMs, and four deep learning-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench). They also designed code transformations to address the overlapping of training and testing data problem faced by Codex, and created a new Java vulnerability remediation benchmark, VJBench, to better evaluate LLMs and APR techniques.

## Combined LLMs with static analysis tools.

Instead of using LLMs alone for program repair, some studies have combined them with traditional program analysis tools to increase the efficiency of those tools. Kamel A et al. [69] proposed a new approach called Feedback-Driven Security Patching (FDSP), which pass feedback from Bandit, a static code analysis tool, to LLM. LLM will generate potential solutions to address security vulnerabilities. Each solution, along with vulnerable code, is sent back to the LLM for verification. Matthew J et al. [157] presented a program repair framework called InferFix that incorporates the latest static analyzers for fixing critical security and performance vulnerabilities. Inferfix is composed of a retriever and a generator. The retriever aims at searching for semantically equivalent bugs and corresponding fixes while the generator is fine-tuned on bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes.

## Improving repair capabilities through different strategies.

In order to improve the performance of LLM on program repair tasks, researchers have taken a number of improvements. David D et al. [158] fine-tuned LLM on datasets containing $\mathrm{C}$ code vulnerabilities. They specifically designed a
structured representation of the code to be provided to LLM, including the line number of the code that needs to be repaired, descriptions of the vulnerability (CWE desceriptions), the full source code, etc. The output of LLM is also structured and can be directly patched, which enables the code to be repaired automatically without human intervention. Xinyun C et al. [159] proposed an approach called SELF-DEBUGGING. Without any human feedback about the correctness of the code or error messages, the model is able to identify its errors by observing the execution results and interpreting the generated code in natural language. Toufique A et al. [160] explores the application of Self-Consistency [161] (an approach for improving model reasoning ability) in program repair. By incorporating commit-logs (bug-fix commits collected on Github) as reasoning paths in few-shot prompts, Self-Consistency allows the LLM to generate diverse solutions. The most frequent solution from multiple samples is selected to improve patch accuracy. Yuxiang $\mathrm{W}$ et al. [162] proposed a program repair framework named Repilot. It starts by masking the buggy code segment and then utilizes the LLM to generate candidate patches. During the generation, Repilot consults the Completion Engine to prune infeasible tokens and proactively complete the code when necessary. This approach enhances the compilation rate and correctness of patches while reducing the number of invalid attempts in the generation process. Nafis T et al. [163] presents SecRepair, a system that employs LLM to detect and fix code vulnerabilities in software. It utilizes reinforcement learning with a semantic reward mechanism to improve the model's ability to generate accurate code comments and descriptions, guiding developers to address security issues effectively. Jiaolong $\mathrm{K}$ et al. introduced ContrastRepair which enhances the conversation-driven repair framework by providing LLMs with contrastive test case pairs-consisting of a failing test and a passing test-to offer more precise feedback. The key insight is to minimize the difference between the generated passing test and the failing one, effectively isolating bug causes. ContrastRepair iteratively interacts with ChatGPT, generating patches until plausible fixes are produced. Different from the previous function-level methods, Yuxiao C et al. [164] investigated the performance of LLM on repository-level method for program repairing which needs to consider interactions and dependencies between code that may span multiple functions or files. In their work, they proposed a benchmark, RepoBugs, comprising 124 bugs from open-source repositories to assess LLM's performance in this context.

## Target-specific program repairing.

We also have a collection of articles that are program fixes for some specific targets. M.Caner T and Berk S [165] proposed a framework called ZeroLeak, exploring how large language models can be used to automatically generate repair code to address side-channel vulnerabilities in software. ZeroLeak guides LLM in generating patches for specific vulnerabilities zero-shot learning. Once generated, these patches will be inspected by dynamic analysis tools to ensure that they are not only functionally correct, but also secure against information leakage. Sudipta $\mathrm{P}$ et al. [166] proposed a novel framework named DIVAS. The framework maps user-defined SoC specifications to Common Weakness Enumerations (CWEs), generates SystemVerilog Assertions (SVAs) for verification, and enforces security policies. DIVAS automates the process of vulnerability detection and policy enforcement, reducing manual effort and enhancing SoC security. Baleegh A et al. [167] built a corpus of domain-representative hardware security bugs and utilized LLM to repair the Verilog code containing them automatically. Tan K L et al. [168] uniquely focused on the application of large language models, specifically ChatGPT and Bard, in repairing security vulnerabilities in JavaScript programs. Utilizing the 2023 CWE Top 25 List as a reference, it selects JavaScript-relevant vulnerabilities to assess the models' accuracy in generating correct patches. Their research highlights the potential of LLMs in JavaScript security, emphasizing their performance in a language predominantly used in web development.

### 4.6 Anomaly Detection

Distinguished from anomaly detection in machine-learning, in this section we mainly refer to security anomalies such as malicious traffic in the flow, virus files in the system, anomaly in logs, etc.

## Log-based anomaly detection.

Egil K et al. [169] tested 60 language models fine-tuned for log analysis, including models for different architectures such as BERT, RoBERTa, DistilRoBERTa, GPT-2 and GPT-Neo. The results show that with fine-tuning, these models can be effectively used for log analysis, especially for domain adaptation for specific log types. Targeting services logs on Huawei Cloud, Jinyang L et al. [170] proposed a framework called ScaleAD, which aims to provide an accurate, lightweight and adaptive solution for log anomaly detection in cloud systems. When ScaleAD's Trie-based Detection Agent (TDA) detects suspicious anomaly logs, it can issue a query request to the included LLM to obtain validation of those logs. The LLM provides a determination of whether or not it is an anomaly and a corresponding confidence score by understanding the semantics of the $\log$ content. Jiaxing $\mathrm{Q}$ et al. [171] propose a log anomaly detection framework, called LogGPT. The LogGPT framework consists of three main components: log preprocessing, prompt construction, and response parser. The log preprocessing component involves filtering, parsing and grouping raw log messages into a structured format for further analysis. The response parser is responsible for extracting the output returned by ChatGPT for further analysis and evaluation of the detected anomalies. Xiao H et al. [172] performed a
similar work. The difference is that they fine-tune GPT-2 by introducing a Top-K reward metric that directs the model to focus on the most relevant parts of the log sequence, improving the accuracy of anomaly detection. Yilun $\mathrm{L}$ et al. [173] introduced a method called LogPrompt for online log analysis. They designed a They specify a format that takes unstructured logs and parses them using LLM, generating reports with a specific structure. Then utilizing Chain of Thought, in-contextual learning methods, LogPrompt step-by-step reasons about the contents of the logs to generate justifications for normal/abnormal. Wei Z et al. [174] introduced LEMUR, a cutting-edge log parsing framework that enhances log analysis by integrating Entropy sampling for efficient log clustering and leveraging LLMs for semantic comprehension. LEMUR addresses the limitations of traditional parsers by discarding manual rules and focusing on semantic information. Utilizing the semantic understanding of LLMs, the framework accurately distinguishes between parameters and invariant tokens, leading to impressive efficiency and state-of-the-art performance in log template merging and categorization.

## Web content security.

Based on the content of the websites, Tamás V et al. [175] conducted a research to detect malicious url. Tailored for web content filtering, knowledge distillation is employed to transfer the teacher's knowledge to smaller student models. They classified unlabeled URLs by teacher models to generate labels. The student models, trained with labels generated by the teacher, achieve improved accuracy with significantly fewer parameters, making them suitable for malicious URLs detection. Michael G et al. [176] explored the potential of LLMs in detecting DDoS attacks, examining the performance of LLMs on two datasets (CICIDS 2017 and Urban IoT Dataset). In CICIDS 2017, through few-shot learning, they fine-tuned LLM with pcap files (labeled with DDoS or benign) to classify traffic. Urban IoT is a real world anonymized dataset about 4060 IoT devices. Due to the complexity of this dataset, they trained the LLM differently depending on whether the correlation of traffic between IoT devices was considered or not. Suhaima J et al. [177] presented a model named Improved Phishing and Spam Detection Model (IPSDM), a fine-tuned version of DistilBERT and RoBERTA. The paper emphasizes the potential of LLMs to revolutionize the field of email security and suggests that these models can be valuable tools for improving the security of information systems. Another work also conducted Spam detection with LLM. Yuwei W et al. [178] evaluated ChatGPT's performance in spam email detection, finding it outperforms BERT on a low-resource Chinese dataset but lags on a larger English dataset. The study also highlights the positive impact of increasing prompt examples on ChatGPT's accuracy. Daniel N et al. [179] introduced a spear-phishing detection approach utilizing large language models to generate "prompted contextual document vectors." By posing targeted questions to LLMs about email content, the method quantifies the presence of common persuasion principles, creating vectors that capture the malicious intent within spear-phishing emails. The technique leverages the reasoning capabilities of LLMs, outperforming traditional phishing detection methods. In addition to detecting phishing emails, attempts to generate phishing emails with LLM have also been researched. Fredrik H et al. [180] evaluated the performance of GPT-4 in creating phishing emails and compares their effectiveness to traditional phishing methods that rely on manual design based on general rules and cognitive heuristics (the V-Triad method). The paper also explores the use of LLMs in detecting phishing emails, with models like GPT, Claude, PaLM, and LLaMA demonstrating strong capabilities in identifying malicious intent, sometimes surpassing human detection rates. Noah $\mathrm{Z}$ et al. [181] conducted a research about the interpretation of decision tree models in network intrusion detection (NID) systems. They convert the path and structure data of the decision tree into text format and provide it to the LLM to generate explanations. Moreover, LLM provides additional background knowledge to help users understand why certain features are important in categorization. Mohamed A F et al. [182] encoded the network traffic by employing a novel encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE). Then they trained a model named SecurityBERT with these encoded data to perform a classification task on network traffic. Specifically, their model targets IoT devices, enabling efficient and accurate cyber threat detection on resource-constrained IoT devices. Tarek A et al. [183] introduced HuntGPT, a system that integrates large language models with traditional machine learning for network anomaly detection. The system utilizes a Random Forest classifier trained on the KDD99 dataset to identify network threats. To enhance explainability, it employs XAI techniques like SHAP and Lime, and combines them with a GPT-3.5 conversational agent.

## Digital Forensic.

Mark S et al. [184] assessed the applicability of ChatGPT for digital forensic. ChatGPT is utilized to help determine if a file is being downloaded to a PC and if the file is being executed by a specific user. In addition, chatgpt is also used to detect browser history, Windows event logs and interactions with cloud platform machines.

### 4.7 LLM Assisted Attack

It's important to note that while LLMs offer significant potential for improving cybersecurity, they also introduce new challenges related to data privacy, adversarial attacks, and the need for continuous training and updating to keep up with evolving threats. In a report [185] from a workshop organized by Google on January 1, 2024, the dual-use problem
of generative artificial intelligence (GenAI) technologies were emphasized. They are capable of being used for both positive purposes and potentially for malicious attacks. In this section, we will discuss in detail the current cyber-attacks launched with the help of LLM.

## Current status of LLM-assisted attacks.

Pawankumar S et al. [186] pointed out that ChatGPT has both positive and potentially negative effects on cybersecurity. In this article, they list the various types of threats currently facing cybersecurity, including malware attacks, phishing, password attacks, etc., and mentions the potential application of ChatGPT in social engineering attacks. Maanak G et al. [187] also conducted a similar work on the impact of Generative AI in cybersecurity and privacy. Furthermore, Stephen M et al. [188] explored the potential of Large Language Models for network threat testing, particularly in supporting threat-related actions and decisions. Using a virtual machine as an example, they discussed in detail how to launch an LLM-directed, automated attack on the devices in the network. The authors concluded that although this work is preliminary, it demonstrates that LLM shows strong potential for cyber threats. For the currently existing and accessible malicious LLM, Zilong L et al. [189] conducted a systematic study of 212 real-world Malla (malicious LLM), revealing how they proliferate and operate in the underground market. They provided a detailed examination of the Malla ecosystem, development frameworks, exploitation techniques, and the effectiveness of Malla in generating a wide range of malicious content. They also provided insight into understanding how cyber criminals utilize LLMs and strategies to combat this cybercrime.

## Means of launching malicious attacks with the help of LLM.

LLM-Assisted Privilege Escalation Attacks. Andreas H et al. [190] used LLM to assist in completing penetration tests. They developed an automated Linux privilege escalation benchmark to evaluate the performance of different LLMs and designed a tool, named Wintermute, for rapid exploration LLM's ability on guiding privilege escalation. LLM-Assisted CTF (Capture The Flag) Challenges. Wesley T et al. [191] investigated the potential of existing large language models in solving Capture The Flag (CTF) competition challenges. Employing three established models (GPT-3.5, PaLM2, and Prometheus), they curated a selection of representative challenges from common CTF categories. Subsequently, an analysis was conducted to assess the performance of LLMs in addressing these challenges. Their research results demonstrate that LLM does indeed possess the capability to assist participants in addressing CTF challenges to a certain extent, albeit not comprehensively. LLM-Assisted Phishing Website/Email Generation. Nils B et al. [192] used LLM to automate the generation of advanced phishing attacks. In the proposed attack method, LLM is utilized for the following functions: cloning target websites, modifying login forms to capture credentials, obfuscating code, automating domain name registration, and automating scripts deployment. Sayak S R et al. [193] examined the potential of LLMs like ChatGPT, GPT-4, Claude, and Bard to generate phishing attacks. It reveals that these models can effectively create convincing phishing websites and emails, mimicking well-known brands and employing evasive tactics to avoid detection. The research also develops a BERT-based detection tool achieving high accuracy in identifying malicious prompts, serving as a countermeasure against the misuse of LLMs for phishing scams. LLM-Assisted Payload Generation. [194] tried to write payload to launch cyber attacks with the help of LLM. This research systematically explored ChatGPT to generate executable code for the top 10 MITRE weakness observed in 2022 and compared their performance with Google's Bard. In addition to efficiency, LLM-generated payload tends to be more complex and targeted than the manual crafted. LLM-Enabled Automated Penetration Testing. Gelei D et al. [195] proposed a tool named PentestGPT which aims to conduct automatic penetration testing. In PentestGPT, three procedures are carried out to perform penetration, which includes Reasoning, Generation, and Parsing Modules. Each of module reflects specific roles within penetration testing teams so that the system could simulate automated penetration tests to the maximum extent possible. Andreas H et al. [196] also conducted a study on penetration testing with the help of the LLM. The study investigates two use cases: high-level task planning for security testing and low-level vulnerability hunting within a vulnerable virtual machine. The authors implemented a feedback loop between LLM-generated actions and a virtual machine, allowing the LLM to analyze system states for vulnerabilities and suggest attack vectors. Jiacen X et al. [197] introduced AUTOATTACKER, a system that leverages LLMs to automate "hands-on-keyboard" cyber-attacks, simulating human-operated stages. The system employs LLMs to generate precise attack commands for various techniques and environments, transforming potential manual operations into automated, efficient processes. AUTOATTACKER consists of modules that interact with LLMs iteratively, utilizing capabilities like summarization, planning, and action selection to construct complex attack sequences. Proxies for Attacks. Mika B et al. [198] used ChatGPT as a proxy between the victim and the network controlled by the attackers (C\&C), a mode that allows them to remotely control the victim's system without communicating directly, making it difficult to track down the attackers.

### 4.8 Others

Apart from the previously described categories, there are a few scattered studies on the application of LLM in the field of cybersecurity, which are also of research value.

IoT Fingerprint. Armin S et al. [199] proposed a method for Internet devices fingerprint generation. Their approach is divided into two steps. First, raw text data obtained from web scans will be transformed to a stable representation (embedding) with large language models, RoBERTa. Next, a downstream task was carried out to cluster the embedded codes, using the HDBSCAN algorithm. Thus, fingerprints will be generated depending on clustering.

Botnet. Kaicheng Y et al. [200] elucidated a botnet named fox8 powered by LLM on Twitter. The fox8 botnet contains over one thousand users controlled by AI. They post machine-generated content and stolen images to spread fake and harmful information, engaging with each other through replies and retweets.

Security Patch Detection. Xunzhu T et al. proposed a system named LLMDA, the mainly goal of which is to improve the identification of security patches in open source software (OSS). LLMs are used to generate explanatory descriptions of patches and synthetic data, which helps to augment existing datasets. [201]

SoC Security. Dipayan S et al. [202] explored the potential of integrating Large Language Models (LLMs) into the system-on-chip (SoC) security verification paradigm. They primarily evaluate the application of LLM in the following areas: Vulnerability Insertion, Security Assessment, Security Verification and Countermeasure Development.

Taint Analysis. Puzhuo L et al. [203] presented LATTE, a static binary taint analysis tool supported by LLMs. LLM helps to identify the chain of data dependencies (dangerous flows) between the source of taints and possible vulnerability triggers. LLM provides the understanding of code structure and semantics in the process.

LLMs' Input-Output Safeguard. [204] designed a model named Llama Guard which mainly focused on detecting the risk in LLM's prompts/response. Based on the Llama2-7b model, instruction-tuning is performed on the collected text which is labeled with security risks.

Honeypot. Muris S et al. [205] designed a dynamic and real-time fake honeypot by giving response generated by LLM which mainly focused on changing the limitation that honeypots are easily recognizable. In their experiment, most of the people can't recognize whether the remote host is a real one or a honeypot generated by LLM.

Incidence Response. Sam H and Jules W [206] advocated for the application of ChatGPT to enhance Incident Response Planning (IRP) in cybersecurity. It suggests that LLMs can draft initial plans, recommend best practices, and identify documentation gaps. The paper highlights the potential of LLMs to streamline IRP processes, emphasizing the value of human oversight to ensure accuracy and relevance.

Network Management. [207] explored how LLMs can be used to generate task-specific code from natural language queries to improve network management. They developed and released a test benchmark, NeMoEval, covering two network management applications: network traffic analysis and network lifecycle management.

Vulnerabilities Reproduction. Sidong F et al. [208] proposed an approach called AdbGPT that utilizes Large Language Models to automatically reproduce vulnerabilities from vulnerability reports by prompting engineering without training or hard coding.

Expertise Q\&A on cybersecurity domain. Samia K et al. [209] is an empirical study of ChatGPT's performance in answering Stack Overflow programming questions. The main drawbacks of the LLM responses lie in the fake information and the excessive length of the content. Still some testers liked its comprehensiveness and good style of language presentation. Due to the difficulty of recognizing misleading information given by LLM, this is an area that has yet to be researched.

Answer to Q2: LLMs have shown great potential in the field of cybersecurity, assisting in various aspects such as threat intelligence, anomaly detection, vulnerability detection, and so on. LLM security copilot can effectively empower the automation and intelligence of cybersecurity, helping to address security risk challenges. Although relevant research has made certain progress, it is still worth further exploration to better apply LLMs in the field of cybersecurity.

![](https://cdn.mathpix.com/cropped/2024_05_29_995460fc12e55382f0c9g-20.jpg?height=374&width=1648&top_left_y=249&top_left_x=236)

Figure 7: An overview of RQ3.

## 5 RQ3: What are the existing challenges and further research directions about the application of LLMs in cybersecurity?

### 5.1 Challenge

The application of LLMs in the realm of cybersecurity represents a cutting-edge frontier, showcasing LLMs' robust capabilities in addressing complex and dynamic cyber threats. However, despite their strengths, LLMs are not unchallenged, particularly in terms of inherent vulnerabilities and susceptibilities to attacks [16, 210]. Among the critical concerns are the phenomena of LLMs-oriented attacks and LLMs jailbreaking. These vulnerabilities underscore the double-edged nature of LLM applications in cybersecurity. On one hand, the powerful comprehension and predictive capabilities of LLMs can significantly promote the intelligence of cybersecurity systems. On the other hand, their intrinsic weaknesses present avenues for exploitation, posing severe security risks and undermining their reliability and integrity in cybersecurity applications.

We delve into these challenges from two pivotal perspectives: attacks against LLMs, which examines the susceptibility of LLMs to various forms of attacks [211, 212, 213], and LLMs Jailbreaking, focusing on the phenomenon of LLMs generating unsafe or unintended content when prompted in certain ways, despite being designed with safeguards [214, 215]. Through an analysis of these dimensions, we aim to illuminate the complexities of leveraging LLMs in cybersecurity, highlighting the need for caution and strategic foresight in their application.

## Attaks Against LLMs.

The vulnerabilities of LLMs make them susceptible to attacks by malicious users. We focus on two types of attacks: backdoor attacks and prompt injection attacks.

Backdoor Attack manipulates model outputs to achieve attackers' objectives by embedding specific triggers in the model or its inputs. Jiawen S et al. [216] propose a novel backdoor attack methodology termed BadGPT, specifically targeting language models that have been fine-tuned through reinforcement learning, such as ChatGPT. This approach involves embedding backdoors within the reward model, which can be activated via specific trigger prompts. Such activation allows attackers to skew the model's output to align with their preferences, showcasing a critical security vulnerability. In another study, Shuai Z et al. [217] introduces a novel backdoor attack strategy, ICLAttack, which aims at exploiting the inherent context learning capabilities of LLMs. The ICLAttack framework encompasses two primary attack vectors: poisoning demonstration examples and poisoning demonstration prompts. Through embedding backdoor triggers within the model's context, ICLAttack is capable of influencing the model's behavior without necessitating fine-tuning, thus revealing universal vulnerabilities within LLMs. Furthermore, Hongwei Y et al. [218] unveils a backdoor attack mechanism tailored to prompt-based LLMs, called PoisonPrompt. The method injects backdoors into the language model through two steps: poisoned prompt generation and bi-level optimization. This method can change the normal prediction of the model under the activation of specific triggers, without affecting the performance of the model on downstream tasks, presenting a subtle yet potent threat to the integrity of LLMs.

Prompt Injections Attack involve attackers inserting malicious commands into inputs, compelling the model to execute actions aligned with the attackers' intentions. Rodrigo $\mathrm{P}$ et al. [219] conducts a comprehensive investigation into prompt-to-SQL (P2SQL) injection attacks targeting web applications based on the Langchain framework. These attacks leverage user-input prompts to generate malicious SQL queries, thereby enabling attackers to tamper with databases or steal sensitive information. Shuyu J et al. [220] introduce the Compositional Instruction Attack (CIA), unveiling the susceptibility of LLMs to attacks that utilize composite instructions with potentially malicious intentions. Through two transformation methods, Talking-CIA and Writing-CIA, harmful instructions are masked as dialogue or writing tasks, rendering the model unable to discern underlying malevolent intents and thus generating harmful content. Yupei

Let al. [221] propose a novel black-box prompt injection attack technique named HOUYI for applications integrated with LLMs. HOUYI executes attacks through three key elements: pre-constructed prompts, injection prompts, and malicious payloads. Its deployment across 36 real-world scenarios underscores its efficacy in uncovering and exploiting vulnerabilities within LLM-integrated applications. et al. [222] focuses on Virtual Prompt Injection (VPI) attacks against instruction-tuned LLMs, allowing attackers to manipulate model behavior by specifying virtual prompts without directly injecting into model inputs, leading to the model disseminating biased information. Jatmo [223] leverages instruction-tuned models to generate datasets for specific tasks. These datasets are then utilized to fine-tune foundational models, enhancing their robustness to resist most prompt injection attacks.

Additionally, George K et al. [224] constructs the adversarial attack dataset named AttaQ, in a semi-automated manner, aiming to assess the security of LLMs when confronted with harmful or inappropriate inputs. Vulnerabilities are exposed by analyzing model responses to the AttaQ dataset, and specialized clustering techniques are further applied to identify and characterize the models' vulnerable semantic areas. Aysan E et al. [212] conducts a comprehensive survey of various attack types targeting LLMs, encompassing both direct attacks on the models themselves and indirect attacks on applications utilizing the models. This study delineates the impacts of these attacks on the privacy, security, and reliability of the models. And it underscores the critical importance of implementing proactive security measures in the development of AI models.

## LLMs Jailbreaking.

As mentioned above, LLM is susceptible to various attacks, with jailbreaking attacks being one of the most popular. Xinyue S et al. [225] investigates the security issues of LLMs when facing jailbreak prompts, collecting and analyzing 6,387 prompts to reveal the characteristics and attack strategies of these prompts. Despite various security measures implemented by LLMs, they found that effective jailbreak prompts still successfully induce models to generate harmful content, indicating the need for further improvements in the security aspects of LLMs. Junjie C et al. [214] conductes a comprehensive evaluation of LLMs jailbreaking, revealing the effectiveness of these attack methods and the vulnerabilities of LLMs across various violation categories.

There are various methods for generating adversarial prompts. Andy Z et al. [226] combines greedy search and gradientbased optimization techniques to propose a method that automatically generates adversarial suffixes to prompt models, including open-source and commercial models, to produce inappropriate content. Raz L et al. [227] introduces a novel approach using a genetic algorithm for black-box jailbreak attacks, which can manipulate LLMs to produce unexpected and potentially harmful outputs without accessing the model's internal structure and parameters by optimizing a universal adversarial prompt. Peng D et al. [228] conceptualizes the jailbreaking process as prompt rewriting and scenario nesting. They then introduce a jailbreaking prompt generation framework, ReNeLLM, which leverages LLMs themselves to generate effective jailbreaking prompts. ReNeLLM achieves a high attack success rate on multiple LLMs while significantly reducing the time cost compared to existing baselines. Gelei D et al. [229] explores jailbreak attacks on LLM Chatbots and proposes a framework named MASTERKEY to automate this process. Through temporal feature analysis and automated prompt generation, MASTERKEY reveals and bypasses the defense mechanisms of LLM chatbots, offering new perspectives for LLM security research and guidance for service providers to improve their security measures.

Research on LLMs jailbreaking can also be leveraged for red-teaming. Sicheng $\mathrm{Z}$ et al. [230] proposes AutoDAN, an interpretable, gradient-based adversarial attack method. By marrying the dual objectives of jailbreaking and readability, it generates interpretable and diverse attack prompts capable of effectively circumventing perplexity filters and demonstrates robust generalization in scenarios with limited training data. This method not only offers a novel approach for red-teaming of LLMs but also aids in understanding the mechanics of jailbreak attacks. Jiahao Y et al. [231] presents a new black-box jailbreak fuzzing framework named GPTFUZZER. By collecting human-written jailbreak templates from the internet as initial seeds, and then iterating through a process of seed selection, mutation, and evaluating the success of attacks, GPTFUZZER significantly enhances the efficiency and scalability of red team testing. Dongyu Y et al. [232] introduces FuzzLLM, a novel and universally applicable fuzz testing framework designed to proactively uncover jailbreak vulnerabilities within LLMs. Employing a template-based strategy, FuzzLLM can generate a variety of jailbreak prompts and identify potential security vulnerabilities through an automated testing. It demonstrates efficiency and comprehensiveness across various LLMs, effectively identifying and assessing jailbreak vulnerabilities.

Additionally, Zhenhua W et al. [233] introduces the concept of a semantic firewall to describe the defense mechanisms of LLMs against malicious prompts and proposes a self-deception attack method to bypass LLM semantic firewalls. This method involves designing a customizable dialogue template for experimenting with specific illegal payloads and automatically achieving LLM jailbreak. Huachuan Q et al. [234] develops a potential jailbreak prompt dataset embedded with malicious instructions and conducts a systematic analysis of LLM performance under different conditions, such as instruction positions, word substitutions, and instruction replacements, using a layered annotation framework. This is
aimed at evaluating the security and output robustness of LLMs when processing texts containing potential malicious instructions. Haoran L et al. [235] investigates the potential privacy threats associated with ChatGPT and the Bing search engine integrated with ChatGPT. By introducing a novel multi-step jailbreaking prompt, they successfully extract personal identifiable information from ChatGPT and demonstrate the privacy threats posed by the new Bing under direct prompts.

## Others.

Besides the extensively researched vulnerabilities, several other LLM risks limit their application in cybersecurity. Pasupuleti R et al. [236] underscores the dual-edged nature of generative AI and ChatGPT, bringing to light the myriad of cybersecurity and ethical challenges they introduce alongside their conveniences. Evan $\mathrm{H}$ et al. [237] investigates the deceptive behaviors that LLMs may exhibit under certain trigger conditions and finds that these behaviors might persist even after security training, posing a potential threat to the security of AI systems. Xianjun Y et al. [238] points out that even securely aligned LLMs can be easily manipulated to generate harmful content with simple data adjustments, highlighting the intricacies of maintaining LLM security. Fengqing J et al. [239] identifies critical vulnerabilities within LLM-integrated applications, which could stem from malicious app developers or external threats with the capability to control database access, manipulate, and polluting data. Sallou J et al. [240] also raises concerns over data leakage and issues of reproducibility associated with the use of closed-source LLMs.

Answer 1 to Q3: Despite the powerful capabilities of LLMs, they inherently possess certain weaknesses and vulnerabilities, making them susceptible to attacks. In particular, jailbreaking poses significant security risks to the application of LLMs.

### 5.2 Further Research

Despite the significant research into LLMs within the domain of cybersecurity, the exploration and application of such models remain in their initial stages, presenting substantial potential for growth [18, 19]. The complexity of cybersecurity stems not only from the diversity of attack methods but also from the intricate nature of network environments, coupled with the need for a comprehensive application of various tools and strategies to achieve effective protection [241, 242]. Facing these challenges necessitates AI systems with enhanced capabilities in planning, reasoning, tool use, memory, and so on. Consequently, the concept of the LLM Agent has emerged, garnering widespread attention among researchers.

LLM Agent is "a system that can use an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools" [243]. By simulating complex network behaviors and attack patterns, and integrating advanced natural language processing capabilities, LLM agents introduce new perspectives and solutions to the field of cybersecurity [96, 188, 244, 245, 246, 247]. With ongoing technological advancements and deeper research, LLM agents are expected to play a pivotal role in the generation of defense strategies, threat detection, and the formulation of security policies, significantly enhancing the efficiency and intelligence level of cybersecurity defenses.

The AI Agents framework based on LLMs possesses the critical capabilities required to solve complex problems [248]. Zhiheng X et al. [249] proposes an LLM Agent architecture that includes brain, perception, and action components, offering wide-ranging applications in single-agent scenarios, multi-agent environments, and human-agent collaboration. Moreover, the incorporation of Tool \& API calls endows LLM agents with the capacity to interact with the real world. ToolLLM [250] develops the ToolBench dataset and the DFSDT algorithm, enabling LLMs to successfully handle complex tasks involving numerous real-world APIs. Sum2Act [251] introduces a sophisticated tool invocation mechanism by summarizing outcomes and making informed decisions, enhancing LLMs' interaction with external tools. Additionally, Ke Y et al. [252] demonstrates that integrating code (programming languages) into LLMs significantly augments their capabilities, enabling them to undertake more complex tasks as Intelligent Agents. Bo Q et al. [253] proposes TaskWeaver, a code-first agent framework for seamlessly planning and executing data analytics tasks.

LLM agents can be applied to address complex cybersecurity tasks. LLMind [244] is an innovative framework that leverages LLMs as coordinators, integrated with Internet of Things devices and domain-specific AI modules, to execute complex tasks. This framework employs finite state machine methods to generate control scripts, thereby enhancing the accuracy and success rate of task execution. Also, LLMind introduces a mechanism for accumulating experience, which allows the system to continually learn and progress through ongoing interactions between users and machines. Maria R et al. [245] demonstrates the use of LLMs as agents within cybersecurity environments. Experiments in the NetSecGame and CyberBattleSim settings showed that LLM agents can achieve performance comparable to or better than extensively trained agents in sequential decision-making tasks, even without additional training. Furthermore, the study introduces the NetSecGame environment, a highly modular and adaptive cybersecurity setting designed to support complex multi-agent scenarios. Yudong H et al. [254] proposed ChatNet, a domain-specific network LLM framework
with access to a variety of external network tools. ChatNet significantly reduces the time required for tedious network planning tasks, resulting in a substantial increase in efficiency.

LLM agents can be employed to perform automated attacks. Richard F et al. [246] revealed the potential of LLM agents in cyber security attacks, particularly the capability of GPT-4 to autonomously conduct complex hacker attacks on websites without prior knowledge of vulnerabilities. The study shows that LLM agents have a success rate of up to $73.3 \%$ in hacking attempts and can autonomously discover vulnerabilities in real-world websites. Stephen M et al. [188] demonstrated the potential application of LLMs in cyber threat testing, especially in automating cyber attack activities. Through prompt engineering and the design of automated agents, LLMs can understand and execute complex cyber attack tasks.

LLM agents can also be utilized to assist in cyber defense. Nissist [247] is designed as a multi-agent system to precisely understand user queries and provide effective mitigation plans. Nissist leverages troubleshooting guides and event mitigation history to offer proactive suggestions, which significantly reduces the time for event mitigation, alleviates the workload of on-duty engineers and enhances the reliability of services. Cyber Sentinel [96] is a dialogue agent based on GPT-4, which can interpret potential cyber threats and execute security actions upon user instructions. The potential impact of Cyber Sentinel in cyber security operations includes improved threat detection and response capabilities, enhanced operational efficiency, real-time collaboration, and knowledge sharing.

LLM agents enhance cybersecurity applications with their remarkable capabilities, yet the security risks inherent in agent systems [255] pose challenges for their deployment in cybersecurity contexts. Fangzhou W et al. [256] introduced the concept of Web-based Indirect Prompt Injection, a novel cyber threat that embeds malicious instructions in web pages to indirectly control these agents, achieving high success rates and robustness across different user inputs. Qiusi Z et al. [257] highlighted that the integration of external tools by LLM agents could lead to the risk of Indirect Prompt Injection attacks, where attackers embed malicious commands within the content processed by LLMs, manipulating these agents to perform actions harmful to users.

In conclusion, the application of LLM-based agents in cybersecurity opens new avenues for addressing digital security threats. Although research in this area is still in its early stages, and the security vulnerabilities inherent to the agents have not yet been addressed, the direction of this research promises to significantly enhance the capability to counter complex cyber threats and has the potential to revolutionize the working methods of security professionals, thereby unleashing greater productivity. Therefore, further research into the application of LLM agents in cybersecurity is crucial for developing adaptive, intelligent, and comprehensive cybersecurity solutions.

Answer 2 to Q3: Extending the tool-use and API-call capabilities of LLM, coupled with the design of autonomous intelligent agents capable of understanding, planning decisions, and executing complex tasks within cybersecurity applications, will significantly advance the utilization of $\mathrm{AI}$ in the cybersecurity domain.

## 6 Conclusion

This paper delineate the methodologies for constructing cybersecurity-oriented domain LLMs, detailing how existing models can be fine-tuned with targeted data to meet specific needs. The investigation into the potential applications of LLMs has revealed their substantial potential across numerous cybersecurity tasks, such as threat intelligence, vulnerability detection, secure code generation and others. However, we has also acknowledged the inherent vulnerabilities of LLMs, notably their susceptibility to attacks such as jailbreaking, which pose significant security risks. Mitigating these vulnerabilities is crucial for the secure deployment of LLMs in sensitive environments. Additionally, we propose future research directions, such as extending the tool-use and API-call capabilities of LLMs, and developing autonomous intelligent agents for complex cybersecurity operations.

In summary, we bridges the gap between LLM advancements and cybersecurity demands, laying a groundwork for both researchers and practitioners. It guides them in leveraging LLMs' transformative potential while addressing the unique challenges that arise in this domain. Ongoing efforts in this direction are vital for defining the future of cybersecurity and its integration with LLMs' exceptional abilities.

## References

[1] OpenAI ChatGPT. https://openai.com/chatgpt

[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[3] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality. See https://vicuna. Imsys. org (accessed 14 April 2023), 2(3):6, 2023.

[4] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023.

[5] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

[6] Barret Zoph, Colin Raffel, Dale Schuurmans, Dani Yogatama, Denny Zhou, Don Metzler, Ed H Chi, Jason Wei, Jeff Dean, Liam B Fedus, et al. Emergent abilities of large language models. TMLR, 2022.

[7] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey. arXiv preprint arXiv:2402.06196, 2024.

[8] Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang, et al. Openagi: When 11 meets domain experts. Advances in Neural Information Processing Systems, 36, 2024.

[9] Pravneet Kaur, Gautam Siddharth Kashyap, Ankit Kumar, Md Tabrez Nafis, Sandeep Kumar, and Vikrant Shokeen. From text to transformation: A comprehensive review of large language models' versatility. arXiv preprint arXiv:2402.16142, 2024.

[10] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: A systematic literature review. arXiv preprint arXiv:2308.10620, 2023.

[11] Jinqi Lai, Wensheng Gan, Jiayang Wu, Zhenlian Qi, and Philip S. Yu. Large language models in law: A survey. arXiv preprint arXiv:2312.03718, 2023.

[12] Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Chenyu You, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo, and David A. Clifton. A survey of large language models in medicine: Progress, application, and challenge. arXiv preprint arXiv:2311.05112, 2024.

[13] Lixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li, Roberto Martinez-Maldonado, Guanliang Chen, Xinyu Li, Yueqiao Jin, and Dragan Gašević. Practical and ethical challenges of large language models in education: A systematic scoping review. British Journal of Educational Technology, 55(1):90-112, 2024.

[14] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. Large language models in finance: A survey. In Proceedings of the Fourth ACM International Conference on AI in Finance, pages 374-382, 2023.

[15] Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, et al. Chemdfm: Dialogue foundation model for chemistry. arXiv preprint arXiv:2401.14818, 2024.

[16] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun, and Yue Zhang. A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. arXiv preprint arXiv:2312.02003, 2023.

[17] Badhan Chandra Das, M. Hadi Amini, and Yanzhao Wu. Security and privacy challenges of large language models: A survey. arXiv preprint arXiv:2402.00888, 2024.

[18] Gabriel de Jesus Coelho da Silva and Carlos Becker Westphall. A survey of large language models in cybersecurity. arXiv preprint arXiv:2402.16968, 2024.

[19] Farzad Nourmohammadzadeh Motlagh, Mehrdad Hajizadeh, Mehryar Majd, Pejman Najafi, Feng Cheng, and Christoph Meinel. Large language models in cybersecurity: State-of-the-art. arXiv preprint arXiv:2402.00891, 2024.

[20] Yagmur Yigit, William J Buchanan, Madjid G Tehrani, and Leandros Maglaras. Review of generative ai methods in cybersecurity. arXiv preprint arXiv:2403.08701, 2024.

[21] Kutub Thakur, Meikang Qiu, Keke Gai, and Md Liakat Ali. An investigation on cyber security threats and security models. In 2015 IEEE 2nd international conference on cyber security and cloud computing, pages 307-311. IEEE, 2015.

[22] Natalie M Scala, Allison C Reilly, Paul L Goethals, and Michel Cukier. Risk and the five hard problems of cybersecurity. Risk Analysis, 39(10):2119-2126, 2019.

[23] Diptiben Ghelani. Cyber security, cyber threats, implications and future perspectives: A review. Authorea Preprints, 2022.

[24] Yuchong Li and Qinghui Liu. A comprehensive review study of cyber-attacks and cyber security; emerging trends and recent developments. Energy Reports, 7:8176-8186, 2021.

[25] Ömer Aslan, Semih Serkant Aktuğ, Merve Ozkan-Okay, Abdullah Asim Yilmaz, and Erdal Akin. A comprehensive review of cyber security vulnerabilities, threats, attacks, and solutions. Electronics, 12(6):1333, 2023.

[26] Pranav Kumar Chaudhary. Ai, ml, and large language models in cybersecurity.

[27] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[28] André Silva, Sen Fang, and Martin Monperrus. Repairllama: Efficient representations and fine-tuned adapters for program repair. arXiv preprint arXiv:2312.15698, 2023.

[29] Jie Zhang, Hui Wen, Liting Deng, Mingfeng Xin, Zhi Li, Lun Li, Hongsong Zhu, and Limin Sun. Hackmentor: Fine-tuning large language models for cybersecurity. In 2023 IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom). IEEE, 2023.

[30] Shafi Parvez Mohammed and Gahangir Hossain. Chatgpt in education, healthcare, and cybersecurity: Opportunities and challenges. In 2024 IEEE 14th Annual Computing and Communication Workshop and Conference (CCWC), pages 0316-0321. IEEE, 2024.

[31] Rahul Pankajakshan, Sumitra Biswal, Yuvaraj Govindarajulu, and Gilad Gressel. Mapping llm security landscapes: A comprehensive stakeholder risk assessment proposal. arXiv preprint arXiv:2403.13309, 2024.

[32] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

[33] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, and et al. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

[34] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

[35] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.

[36] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.

[37] Ramanpreet Kaur, Dušan Gabrijelčič, and Tomaž Klobučar. Artificial intelligence for cybersecurity: Literature review and future research directions. Information Fusion, page 101804, 2023.

[38] Sarvesh Kumar, Upasana Gupta, Arvind Kumar Singh, and Avadh Kishore Singh. Artificial intelligence: revolutionizing cyber security in the digital era. Journal of Computers, Mechanical and Management, 2(3):31-42, 2023.

[39] Maad Mijwil, Mohammad Aljanabi, et al. Towards artificial intelligence-based cybersecurity: the practices and chatgpt generated ways to combat cybercrime. Iraqi Journal For Computer Science and Mathematics, 4(1):65-70, 2023.

[40] Çağatay Yıldız, Nishaanth Kanna Ravichandran, Prishruit Punia, Matthias Bethge, and Beyza Ermis. Investigating continual pretraining in large language models: Insights and implications. arXiv preprint arXiv:2402.17400, 2024.

[41] Tiezheng Zhang, Xiaoxi Chen, Chongyu Qu, Alan Yuille, and Zongwei Zhou. Leveraging ai predicted and expert revised annotations in interactive segmentation: Continual tuning or full training? arXiv preprint arXiv:2402.19423, 2024.

[42] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792, 2023.

[43] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492, 2023.

[44] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220-235, 2023.

[45] Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, and Merouane Debbah. Cybermetric: A benchmark dataset for evaluating large language models knowledge in cybersecurity. arXiv preprint arXiv:2402.07688, 2024.

[46] Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, Sasha Frolov, Ravi Prakash Giri, Dhaval Kapil, Yiannis Kozyrakis, David LeBlanc, James Milazzo, Aleksandar Straumann, Gabriel Synnaeve, Varun Vontimitta, Spencer Whitman, and Joshua Saxe. Purple llama cyberseceval: A secure coding benchmark for language models. arXiv preprint arXiv:2312.04724, 2023.

[47] Catherine Tony, Markus Mutas, Nicolás E. Díaz Ferreyra, and Riccardo Scandariato. Llmseceval: A dataset of natural language prompts for security evaluations. arXiv preprint arXiv:2303.09384, 2023.

[48] Mohamed Amine Ferrag, Ammar Battah, Norbert Tihanyi, Merouane Debbah, Thierry Lestable, and Lucas C Cordeiro. Securefalcon: The next cyber reasoning system for cyber security. arXiv preprint arXiv:2307.06616, 2023.

[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.

[51] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.

[52] Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. Continual learning for large language models: A survey. arXiv preprint arXiv:2402.01364, 2024.

[53] Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continually pre-train large language models. arXiv preprint arXiv:2403.08763, 2024.

[54] Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si. On the effectiveness of adapter-based tuning for pretrained language model adaptation. arXiv preprint arXiv:2106.03164, 2021.

[55] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. AI Open, 2023.

[56] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021.

[57] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.

[58] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

[59] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.

[60] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172, 2023.

[61] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei

Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen. A comprehensive study of knowledge editing for large language models. arXiv preprint arXiv:2401.01286, 2024.

[62] Aras Bozkurt and Ramesh C Sharma. Generative ai and prompt engineering: The art of whispering to let the genie out of the algorithmic world. Asian Journal of Distance Education, 18(2):i-vii, 2023.

[63] Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. Prompt engineering a prompt engineer. arXiv preprint arXiv:2311.05661, 2023.

[64] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. A systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927, 2024.

[65] Zefang Liu, Jialei Shi, and John F Buford. Cyberbench: A multi-task benchmark for evaluating large language models in cybersecurity.

[66] Guancheng Li, Yifeng Li, Wang Guannan, Haoyu Yang, and Yang Yu. Seceval: A comprehensive benchmark for evaluating cybersecurity knowledge of foundation models. https://github.com/XuanwuAI/SecEval, 2023.

[67] Zefang Liu. Secqa: A concise question-answering dataset for evaluating large language models in computer security. arXiv preprint arXiv:2312.15838, 2023.

[68] Mohammed Latif Siddiq and Joanna C. S. Santos. Securityeval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques. In Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security, MSR4P\&S 2022, page 29-33, New York, NY, USA, 2022. Association for Computing Machinery.

[69] Kamel Alrashedy and Abdullah Aljasser. Can llms patch security issues? arXiv preprint arXiv:2312.00024, 2024.

[70] Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Debugbench: Evaluating debugging capability of large language models. arXiv preprint arXiv:2401.04621, 2024.

[71] Yukai Miao, Yu Bai, Li Chen, Dan Li, Haifeng Sun, Xizheng Wang, Ziqiu Luo, Yanyu Ren, Dapeng Sun, Xiuting $\mathrm{Xu}, \mathrm{Qi}$ Zhang, Chao Xiang, and Xinchi Li. An empirical study of netops capability of pre-trained large language models. arXiv preprint arXiv:2309.05557, 2023.

[72] Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, Jianhui Li, Gaogang Xie, Xidao Wen, Xiaohui Nie, Minghua Ma, and Dan Pei. Opseval: A comprehensive it operations benchmark suite for large language models. arXiv preprint arXiv:2310.07637, 2024.

[73] Alexey Shestov, Rodion Levichev, Ravil Mussabayev, and Anton Cheshkov. Finetuning large language models for vulnerability detection. arXiv preprint arXiv:2401.17010, 2024.

[74] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.

[75] Aidan ZH Yang, Claire Le Goues, Ruben Martins, and Vincent Hellendoorn. Large language models for test-free fault localization. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pages 1-12, 2024.

[76] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2023.

[77] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2: Lessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309, 2023.

[78] André Storhaug, Jingyue Li, and Tianyuan Hu. Efficient avoidance of vulnerabilities in auto-completed smart contract code using vulnerability-constrained decoding. In 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE), pages 683-693. IEEE, 2023.

[79] Ben Wang. Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX. https://github.com/kingoflolz/mesh-transformer-jax. May 2021.

[80] Jingxuan He, Mark Vero, Gabriela Krasnopolska, and Martin Vechev. Instruction tuning for secure code generation. arXiv preprint arXiv:2402.09497, 2024.

[81] Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, and Xiangyu Zhang. Nova ${ }^{+}$: Generative language models for binaries. arXiv preprint arXiv:2311.13721, 2023.

[82] Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, et al. Owl: A large language model for it operations. arXiv preprint arXiv:2309.09298, 2023 .

[83] Shaswata Mitra, Subash Neupane, Trisha Chakraborty, Sudip Mittal, Aritran Piplai, Manas Gaur, and Shahram Rahimi. Localintel: Generating organizational threat intelligence from global and local cyber knowledge. arXiv preprint arXiv:2401.10036, 2024.

[84] Filippo Perrina, Francesco Marchiori, Mauro Conti, and Nino Vincenzo Verde. Agir: Automating cyber threat intelligence reporting with natural language generation. arXiv preprint arXiv:2310.02655, 2023.

[85] Reza Fayyazi and Shanchieh Jay Yang. On the uses of large language models to interpret ambiguous cyberattack descriptions. arXiv preprint arXiv:2306.14062, 2023.

[86] Reza Fayyazi, Rozhina Taghdimi, and Shanchieh Jay Yang. Advancing ttp analysis: Harnessing the power of encoder-only and decoder-only language models with retrieval augmented generation. arXiv preprint arXiv:2401.00280, 2024.

[87] Tanmay Singla, Dharun Anandayuvaraj, Kelechi G. Kalu, Taylor R. Schorlemmer, and James C. Davis. An empirical study on using large language models to analyze software supply chain security failures. In Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses, SCORED '23, page 5-15, New York, NY, USA, 2023. Association for Computing Machinery.

[88] Samaneh Shafee, Alysson Bessani, and Pedro M. Ferreira. Evaluation of llm chatbots for osint-based cyber threat awareness. arXiv preprint arXiv:2401.15127, 2024.

[89] Gaëtan Michelet and Frank Breitinger. Chatgpt, llama, can you write my report? an experiment on assisted digital forensics reports written using (local) large language models. arXiv preprint arXiv:2312.14607, 2023.

[90] Giuseppe Siracusano, Davide Sanvito, Roberto Gonzalez, Manikantan Srinivasan, Sivakaman Kamatchi, Wataru Takahashi, Masaru Kawakita, Takahiro Kakumaru, and Roberto Bifulco. Time for action: Automated analysis of cyber threat intelligence in the wild. arXiv preprint arXiv:2307.10214, 2023.

[91] Yuelin Hu, Futai Zou, Jiajia Han, Xin Sun, and Yilei Wang. Llm-tikg: Threat intelligence knowledge graph construction utilizing large language model. Available at SSRN 4671345.

[92] Sean Barnum. Standardizing cyber threat intelligence information with the structured threat information expression (stix). Mitre Corporation, 11:1-22, 2012.

[93] Ting Zhang, Ivana Clairine Irsan, Ferdian Thung, and David Lo. Cupid: Leveraging chatgpt for more accurate duplicate bug report detection. arXiv preprint arXiv:2308.10022, 2023.

[94] Chengnian Sun, David Lo, Siau-Cheng Khoo, and Jing Jiang. Towards more accurate retrieval of duplicate bug reports. In 2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011), pages 253-262, 2011.

[95] Yu-Zheng Lin, Muntasir Mamun, Muhtasim Alam Chowdhury, Shuyu Cai, Mingyu Zhu, Banafsheh Saber Latibari, Kevin Immanuel Gubbi, Najmeh Nazari Bavarsad, Arjun Caputo, Avesta Sasan, Houman Homayoun, Setareh Rafatirad, Pratik Satam, and Soheil Salehi. Hw-v2w-map: Hardware vulnerability to weakness mapping framework for root cause analysis with gpt-assisted mitigation suggestion. arXiv preprint arXiv:2312.13530, 2023.

[96] Mehrdad Kaheh, Danial Khosh Kholgh, and Panos Kostakos. Cyber sentinel: Exploring conversational agents in streamlining security tasks with gpt-4. arXiv preprint arXiv:2309.16422, 2023.

[97] Jiandong Jin, Bowen Tang, Mingxuan Ma, Xiao Liu, Yunfei Wang, Qingnan Lai, Jia Yang, and Changling Zhou. Crimson: Empowering strategic reasoning in cybersecurity through large language models. arXiv preprint arXiv:2403.00878, 2024.

[98] Ying Zhang, Wenjia Song, Zhengjie Ji, Danfeng, Yao, and Na Meng. How well does llm generate security tests? arXiv preprint arXiv:2310.00710, 2023.

[99] Jie Hu, Qian Zhang, and Heng Yin. Augmenting greybox fuzzing with generative ai. arXiv preprint arXiv:2306.06782, 2023.

[100] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang. Fuzz4all: Universal fuzzing with large language models. arXiv preprint arXiv:2308.04748, 2024.

[101] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K. Lahiri, and Siddhartha Sen. Codamosa: Escaping coverage plateaus in test generation $\cdot$with pre-trained large language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pages 919-931, 2023.

[102] Cen Zhang, Mingqiang Bai, Yaowen Zheng, Yeting Li, Xiaofei Xie, Yuekang Li, Wei Ma, Limin Sun, and Yang Liu. Understanding large language model based fuzz driver generation. arXiv preprint arXiv:2307.12469, 2023.

[103] Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, and Lingming Zhang. Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models. In Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2023, page 423-435, New York, NY, USA, 2023. Association for Computing Machinery.

[104] Yinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan Zhang, Shujing Yang, and Lingming Zhang. Large language models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt. arXiv preprint arXiv:2304.02014, 2023.

[105] Ruijie Meng, Martin Mirchev, Marcel Böhme, and Abhik Roychoudhury. Large language model guided protocol fuzzing. In Proceedings of the 31st Annual Network and Distributed System Security Symposium (NDSS), 2024.

[106] Asmita, Yaroslav Oliinyk, Michael Scott, Ryan Tsang, Chongzhou Fang, and Houman Homayoun. Fuzzing busybox: Leveraging llm and crash reuse for embedded bug unearthing. arXiv preprint arXiv:2403.03897, 2024.

[107] Anton Cheshkov, Pavel Zadorozhny, and Rodion Levichev. Evaluation of chatgpt model for vulnerability detection. arXiv preprint arXiv:2304.07232, 2023.

[108] Moumita Das Purba, Arpita Ghosh, Benjamin J. Radford, and Bill Chu. Software vulnerability detection using large language models. In 2023 IEEE 34th International Symposium on Software Reliability Engineering Workshops (ISSREW), pages 112-119, 2023.

[109] Marwan Omar. Detecting software vulnerabilities using language models. arXiv preprint arXiv:2302.11773, 2023.

[110] Avishree Khare, Saikat Dutta, Ziyang Li, Alaia Solko-Breslin, Rajeev Alur, and Mayur Naik. Understanding the effectiveness of large language models in detecting security vulnerabilities. arXiv preprint arXiv:2311.16169, 2023 .

[111] Rasmus Ingemann Tuffveson Jensen, Vali Tawosi, and Salwa Alamir. Software vulnerability and functionality assessment using llms. arXiv preprint arXiv:2403.08429, 2024.

[112] Haonan Li, Yu Hao, Yizhuo Zhai, and Zhiyun Qian. The hitchhiker's guide to program analysis: A journey with large language models. arXiv preprint arXiv:2308.00245, 2023.

[113] Jin Wang, Zishan Huang, Hengli Liu, Nianyi Yang, and Yinhao Xiao. Defecthunter: A novel llm-driven boosted-conformer-based code vulnerability detection mechanism. arXiv preprint arXiv:2309.15324, 2023.

[114] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented transformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020.

[115] Chenyuan Zhang, Hao Liu, Jiutian Zeng, Kejing Yang, Yuhong Li, and Hui Li. Prompt-enhanced software vulnerability detection using chatgpt. arXiv preprint arXiv:2308.12697, 2023.

[116] Atieh Bakhshandeh, Abdalsamad Keramatfar, Amir Norouzi, and Mohammad Mahdi Chekidehkhoun. Using chatgpt as a static application security testing tool. arXiv preprint arXiv:2308.14434, 2023.

[117] Noble Saji Mathews, Yelizaveta Brus, Yousra Aafer, Mei Nagappan, and Shane McIntosh. Llbezpeky: Leveraging large language models for vulnerability detection. arXiv preprint arXiv:2401.01269, 2024.

[118] Sihao Hu, Tiansheng Huang, Fatih İlhan, Selim Furkan Tekin, and Ling Liu. Large language model-powered smart contract vulnerability detection: New perspectives. arXiv preprint arXiv:2310.01152, 2023.

[119] Zhihong Liu, Qing Liao, Wenchao Gu, and Cuiyun Gao. Software vulnerability detection with gpt and in-context learning. In 2023 8th International Conference on Data Science in Cyberspace (DSC), pages 229-236, 2023.

[120] Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Haijun Wang, Zhengzi Xu, Xiaofei Xie, and Yang Liu. Gptscan: Detecting logic vulnerabilities in smart contracts by combining gpt with program analysis. arXiv preprint arXiv:2308.03314, 2023.

[121] Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Wei Ma, Lyuye Zhang, Miaolei Shi, and Yang Liu. Llm4vuln: A unified evaluation framework for decoupling and enhancing llms' vulnerability reasoning. arXiv preprint arXiv:2401.16185, 2024.

[122] Zhenyu Mao, Jialong Li, Munan Li, and Kenji Tei. Multi-role consensus through llms discussions for vulnerability detection. arXiv preprint arXiv:2403.14274, 2024.

[123] Tianyu Chen, Lin Li, Liuchuan Zhu, Zongyang Li, Guangtai Liang, Ding Li, Qianxiang Wang, and Tao Xie. Vullibgen: Identifying vulnerable third-party libraries via generative pre-trained model. arXiv preprint arXiv:2308.04662, 2023.

[124] Peiyu Liu, Junming Liu, Lirong Fu, Kangjie Lu, Yifan Xia, Xuhong Zhang, Wenzhi Chen, Haiqin Weng, Shouling Ji, and Wenhai Wang. How chatgpt is solving vulnerability management problem. arXiv preprint arXiv:2311.06530, 2023.

[125] Yizheng Chen, Zhoujie Ding, Lamya Alowain, Xinyun Chen, and David Wagner. Diversevul: A new vulnerable source code dataset for deep learning based vulnerability detection. In Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses, RAID '23, page 654-668, New York, NY, USA, 2023. Association for Computing Machinery.

[126] Norbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag, Lucas C. Cordeiro, and Vasileios Mavroeidis. The formai dataset: Generative ai in software security through the lens of formal verification. In Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering, PROMISE 2023, page 33-43, New York, NY, USA, 2023. Association for Computing Machinery.

[127] Zeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, and Chao Zhang. How far have we gone in vulnerability detection using large language models. arXiv preprint arXiv:2311.12420, 2023.

[128] Gustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, and Brendan Dolan-Gavitt. Lost at c: A user study on the security implications of large language model code assistants. In 32nd USENIX Security Symposium (USENIX Security 23), pages 2205-2222, Anaheim, CA, August 2023. USENIX Association.

[129] Florian Tambon, Arghavan Moradi Dakhel, Amin Nikanjam, Foutse Khomh, Michel C. Desmarais, and Giuliano Antoniol. Bugs in large language models generated code: An empirical study. arXiv preprint arXiv:2403.08937, 2024.

[130] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. Asleep at the keyboard? assessing the security of github copilot's code contributions. In 2022 IEEE Symposium on Security and Privacy (SP), pages 754-768, 2022.

[131] Zhilong Wang, Lan Zhang, Chen Cao, and Peng Liu. The effectiveness of large language models (chatgpt and codebert) for security-oriented code analysis. arXiv preprint arXiv:2307.12488, 2023.

[132] Zhijie Liu, Yutian Tang, Xiapu Luo, Yuming Zhou, and Liang Feng Zhang. No need to lift a finger anymore? assessing the quality of code generation by chatgpt. arXiv preprint arXiv:2308.04838, 2023.

[133] Mohammed Latif Siddiq and Joanna C. S. Santos. Generate and pray: Using sallms to evaluate the security of llm generated code. arXiv preprint arXiv:2311.00889, 2023.

[134] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024.

[135] Saad Ullah, Mingji Han, Saurabh Pujar, Hammond Pearce, Ayse Coskun, and Gianluca Stringhini. Can large language models identify and reason about security vulnerabilities? not yet. arXiv preprint arXiv:2312.12575, 2023 .

[136] Alessio Buscemi. A comparative study of code generation using chatgpt 3.5 across 10 programming languages. arXiv preprint arXiv:2308.04477, 2023.

[137] Raphaël Khoury, Anderson R. Avila, Jacob Brunelle, and Baba Mamadou Camara. How secure is code generated by chatgpt? arXiv preprint arXiv:2304.09655, 2023.

[138] Jingxuan He and Martin Vechev. Large language models for code: Security hardening and adversarial testing. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, CCS '23. ACM, November 2023.

[139] Hammond Pearce, Benjamin Tan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, and Brendan Dolan-Gavitt. Pop quiz! can a large language model help with reverse engineering? arXiv preprint arXiv:2202.01142, 2022.

[140] Hanzhuo Tan, Qi Luo, Jing Li, and Yuqun Zhang. Llm4decompile: Decompiling binary code with large language models. arXiv preprint arXiv:2403.05286, 2024.

[141] Chongzhou Fang, Ning Miao, Shaurya Srivastav, Jialin Liu, Ruoyu Zhang, Ruijie Fang, Asmita Asmita, Ryan Tsang, Najmeh Nazari, Han Wang, and Houman Homayoun. Large language models for code analysis: Do llms really do their job? arXiv preprint arXiv:2310.12357, 2023.

[142] Jianyu Zhao, Yuyang Rong, Yiwen Guo, Yifeng He, and Hao Chen. Understanding programs by exploiting (fuzzing) test cases. arXiv preprint arXiv:2305.13592, 2023.

[143] David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, and Denys Poshyvanyk. Evaluating and explaining large language models for code using syntactic structures. arXiv preprint arXiv:2308.03873, 2023 .

[144] Pei Yan, Shunquan Tan, Miaohui Wang, and Jiwu Huang. Prompt engineering-assisted malware dynamic analysis using gpt-4. arXiv preprint arXiv:2312.08317, 2023.

[145] Himari Fujima, Takako Kumamoto, and Yunko Yoshida. Us-ing chatgpt to analyze ransomware messages and to predict ransomware threats, 2023.

[146] Fang Wang. Using large language models to mitigate ransomware threats. 2023.

[147] Nusrat Zahan, Philipp Burckhardt, Mikola Lysenko, Feross Aboukhadijeh, and Laurie Williams. Shifting the lens: Detecting malware in npm ecosystem with large language models. arXiv preprint arXiv:2403.12196, 2024.

[148] Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Xing Che, Dandan Wang, and Qing Wang. Make llm a testing expert: Bringing human-like interaction to mobile gui testing via functionality-aware decisions. arXiv preprint arXiv:2310.15780, 2023.

[149] Baleegh Ahmad, Benjamin Tan, Ramesh Karri, and Hammond Pearce. Flag: Finding line anomalies (in code) with generative ai. arXiv preprint arXiv:2306.12643, 2023.

[150] Julian Aron Prenner and Romain Robbes. Automatic program repair with openai's codex: Evaluating quixbugs. arXiv preprint arXiv:2111.03922, 2021.

[151] Dominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. An analysis of the automatic bug fixing performance of chatgpt. arXiv preprint arXiv:2301.08653, 2023.

[152] Jan Keller and Jan Nowakowski. Ai-powered patching: the future of automated vulnerability fixes. Technical report, 2024.

[153] Jiaxin Yu, Peng Liang, Yujia Fu, Amjed Tahir, Mojtaba Shahin, Chong Wang, and Yangxiao Cai. Security code review by llms: A deep dive into responses. arXiv preprint arXiv:2401.16310, 2024.

[154] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. Practical program repair in the era of large pre-trained language models. arXiv preprint arXiv:2210.14179, 2022.

[155] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. Examining zero-shot vulnerability repair with large language models. In 2023 IEEE Symposium on Security and Privacy (SP), pages 2339-2356, 2023.

[156] Yi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier, Jordan Davis, Lin Tan, Petr Babkin, and Sameena Shah. How effective are neural networks for fixing security vulnerabilities. In Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA '23. ACM, July 2023.

[157] Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, and Alexey Svyatkovskiy. Inferfix: End-to-end program repair with llms. arXiv preprint arXiv:2303.07263, 2023.

[158] David de Fitero-Dominguez, Eva Garcia-Lopez, Antonio Garcia-Cabot, and Jose-Javier Martinez-Herraiz. Enhanced automated code vulnerability repair using large language models. arXiv preprint arXiv:2401.03741, 2024.

[159] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.

[160] Toufique Ahmed and Premkumar Devanbu. Better patching using llm prompting, via self-consistency. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 1742-1746, 2023.

[161] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2023.

[162] Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang. Copiloting the copilots: Fusing large language models with completion engines for automated program repair. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023, page 172-184, New York, NY, USA, 2023. Association for Computing Machinery.

[163] Nafis Tanveer Islam, Joseph Khoury, Andrew Seong, Mohammad Bahrami Karkevandi, Gonzalo De La Torre Parra, Elias Bou-Harb, and Peyman Najafirad. Llm-powered code vulnerability repair with reinforcement learning and semantic reward. arXiv preprint arXiv:2401.03374, 2024.

[164] Yuxiao Chen, Jingzheng Wu, Xiang Ling, Changjiang Li, Zhiqing Rui, Tianyue Luo, and Yanjun Wu. When large language models confront repository-level automatic program repair: How well they done? arXiv preprint arXiv:2403.00448, 2024.

[165] M. Caner Tol and Berk Sunar. Zeroleak: Using llms for scalable and cost effective side-channel patching. arXiv preprint arXiv:2308.13062, 2023.

[166] Sudipta Paria, Aritra Dasgupta, and Swarup Bhunia. Divas: An llm-based end-to-end framework for soc security analysis and policy-based protection. arXiv preprint arXiv:2308.06932, 2023.

[167] Baleegh Ahmad, Shailja Thakur, Benjamin Tan, Ramesh Karri, and Hammond Pearce. Fixing hardware security bugs with large language models. arXiv preprint arXiv:2302.01215, 2023.

[168] Tan Khang Le, Saba Alimadadi, and Steven Y Ko. A study of vulnerability repair in javascript programs with large language models. arXiv e-prints, pages arXiv-2403, 2024.

[169] Egil Karlsen, Xiao Luo, Nur Zincir-Heywood, and Malcolm Heywood. Benchmarking large language models for $\log$ analysis, security, and interpretation. arXiv preprint arXiv:2311.14519, 2023.

[170] Jinyang Liu, Junjie Huang, Yintong Huo, Zhihan Jiang, Jiazhen Gu, Zhuangbin Chen, Cong Feng, Minzhi Yan, and Michael R. Lyu. Log-based anomaly detection based on evt theory with feedback. arXiv preprint arXiv:2306.05032, 2023.

[171] Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Carol Fung, Hailong Yang, and Depei Qian. Loggpt: Exploring chatgpt for log-based anomaly detection. arXiv preprint arXiv:2309.01189, 2023.

[172] Xiao Han, Shuhan Yuan, and Mohamed Trabelsi. Loggpt: Log anomaly detection via gpt. arXiv preprint arXiv:2309.14482, 2023.

[173] Yilun Liu, Shimin Tao, Weibin Meng, Jingyu Wang, Wenbing Ma, Yanqing Zhao, Yuhang Chen, Hao Yang, Yanfei Jiang, and Xun Chen. Interpretable online log analysis using large language models with prompt strategies. arXiv preprint arXiv:2308.07610, 2024.

[174] Wei Zhang, Hongcheng Guo, Anjie Le, Jian Yang, Jiaheng Liu, Zhoujun Li, Tieqiao Zheng, Shi Xu, Runqiang Zang, Liangfan Zheng, and Bo Zhang. Lemur: Log parsing with entropy sampling and chain-of-thought merging. arXiv preprint arXiv:2402.18205, 2024.

[175] Tamás Vörös, Sean Paul Bergeron, and Konstantin Berlin. Web content filtering through knowledge distillation of large language models. arXiv preprint arXiv:2305.05027, 2023.

[176] Michael Guastalla, Yiyi Li, Arvin Hekmati, and Bhaskar Krishnamachari. Application of large language models to ddos attack detection.

[177] Suhaima Jamal and Hayden Wimmer. An improved transformer-based model for detecting phishing, spam, and ham: A large language model approach. arXiv preprint arXiv:2311.04913, 2023.

[178] Yuwei Wu, Shijing Si, Yugui Zhang, Jiawen Gu, and Jedrek Wosik. Evaluating the performance of chatgpt for spam email detection. arXiv preprint arXiv:2402.15537, 2024.

[179] Daniel Nahmias, Gal Engelberg, Dan Klein, and Asaf Shabtai. Prompted contextual vectors for spear-phishing detection. arXiv preprint arXiv:2402.08309, 2024.

[180] Fredrik Heiding, Bruce Schneier, Arun Vishwanath, Jeremy Bernstein, and Peter S. Park. Devising and detecting phishing: Large language models vs. smaller human models. arXiv preprint arXiv:2308.12287, 2023.

[181] Noah Ziems, Gang Liu, John Flanagan, and Meng Jiang. Explaining tree model decisions in natural language for network intrusion detection. arXiv preprint arXiv:2310.19658, 2023.

[182] Mohamed Amine Ferrag, Mthandazo Ndhlovu, Norbert Tihanyi, Lucas C. Cordeiro, Merouane Debbah, and Thierry Lestable. Revolutionizing cyber threat detection with large language models: A privacy-preserving bert-based lightweight model for iot/iiot devices. arXiv preprint arXiv:2306.14263, 2023.

[183] Tarek Ali and Panos Kostakos. Huntgpt: Integrating machine learning-based anomaly detection and explainable ai with large language models (llms). arXiv preprint arXiv:2402.18205, 2023.

[184] Mark Scanlon, Frank Breitinger, Christopher Hargreaves, Jan-Niclas Hilgert, and John Sheppard. Chatgpt for digital forensic investigation: The good, the bad, and the unknown. Forensic Science International: Digital Investigation, 46:301609, 2023.

[185] Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, et al. Identifying and mitigating the security risks of generative ai. Foundations and Trends $®$ in Privacy and Security, 6(1):1-52, 2023.

[186] Pawankumar Sharma and Bibhu Dash. Impact of big data analytics and chatgpt on cybersecurity. In 2023 4th International Conference on Computing and Communication Systems (I3CS), pages 1-6, 2023.

[187] Maanak Gupta, Charankumar Akiri, Kshitiz Aryal, Eli Parker, and Lopamudra Praharaj. From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy. IEEE Access, 11:80218-80245, 2023.

[188] Stephen Moskal, Sam Laney, Erik Hemberg, and Una-May O'Reilly. Llms killed the script kiddie: How agents supported by large language models change the landscape of network threat testing. arXiv preprint arXiv:2310.06936, 2023.

[189] Zilong Lin, Jian Cui, Xiaojing Liao, and XiaoFeng Wang. Malla: Demystifying real-world large language model integrated malicious services. arXiv preprint arXiv:2401.03315, 2024.

[190] Andreas Happe, Aaron Kaplan, and Jürgen Cito. Evaluating llms for privilege-escalation scenarios. arXiv preprint arXiv:2310.11409, 2023.

[191] Wesley Tann, Yuancheng Liu, Jun Heng Sim, Choon Meng Seah, and Ee-Chien Chang. Using large language models for cybersecurity capture-the-flag challenges and certification questions. arXiv preprint arXiv:2308.10443, 2023 .

[192] Nils Begou, Jérémy Vinoy, Andrzej Duda, and Maciej Korczyński. Exploring the dark side of ai: Advanced phishing attack design and deployment using chatgpt. In 2023 IEEE Conference on Communications and Network Security (CNS), pages 1-6, 2023.

[193] Sayak Saha Roy, Poojitha Thota, Krishna Vamsi Naragam, and Shirin Nilizadeh. From chatbots to phishbots? preventing phishing scams created using chatgpt, google bard and claude. arXiv preprint arXiv:2310.19181, 2024.

[194] P. V. Sai Charan, Hrushikesh Chunduri, P. Mohan Anand, and Sandeep K Shukla. From text to mitre techniques: Exploring the malicious use of large language models for generating cyber attack payloads. arXiv preprint arXiv:2305.15336, 2023.

[195] Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, and Stefan Rass. Pentestgpt: An llm-empowered automatic penetration testing tool. arXiv preprint arXiv:2308.06782, 2023.

[196] Andreas Happe and Jürgen Cito. Getting pwn'd by ai: Penetration testing with large language models. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023, page 2082-2086, New York, NY, USA, 2023. Association for Computing Machinery.

[197] Jiacen Xu, Jack W. Stokes, Geoff McDonald, Xuesong Bai, David Marshall, Siyue Wang, Adith Swaminathan, and Zhou Li. Autoattacker: A large language model guided system to implement automatic cyber-attacks. arXiv preprint arXiv:2403.01038, 2024.

[198] Mika Beckerich, Laura Plein, and Sergio Coronado. Ratgpt: Turning online llms into proxies for malware attacks. arXiv preprint arXiv:2308.09183, 2023.

[199] Armin Sarabi, Tongxin Yin, and Mingyan Liu. An llm-based framework for fingerprinting internet-connected devices. In Proceedings of the 2023 ACM on Internet Measurement Conference, IMC '23, page 478-484, New York, NY, USA, 2023. Association for Computing Machinery.

[200] Kai-Cheng Yang and Filippo Menczer. Anatomy of an ai-powered malicious social botnet. arXiv preprint arXiv:2307.16336, 2023.

[201] Xunzhu Tang, Zhenghan Chen, Kisub Kim, Haoye Tian, Saad Ezzini, and Jacques Klein. Just-in-time security patch detection - llm at the rescue for data augmentation. arXiv preprint arXiv:2312.01241, 2023.

[202] Dipayan Saha, Shams Tarek, Katayoon Yahyaei, Sujan Kumar Saha, Jingbo Zhou, Mark Tehranipoor, and Farimah Farahmandi. Llm for soc security: A paradigm shift. arXiv preprint arXiv:2310.06046, 2023.

[203] Puzhuo Liu, Chengnian Sun, Yaowen Zheng, Xuan Feng, Chuan Qin, Yuncheng Wang, Zhi Li, and Limin Sun. Harnessing the power of llm to support binary taint analysis. arXiv preprint arXiv:2310.08275, 2023.

[204] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674, 2023.

[205] Muris Sladić, Veronica Valeros, Carlos Catania, and Sebastian Garcia. Llm in the shell: Generative honeypots. arXiv preprint arXiv:2309.00155, 2024.

[206] Sam Hays and Dr. Jules White. Employing llms for incident response planning and review. arXiv preprint arXiv:2403.01271, 2024.

[207] Sathiya Kumaran Mani, Yajie Zhou, Kevin Hsieh, Santiago Segarra, Trevor Eberl, Eliran Azulai, Ido Frizler, Ranveer Chandra, and Srikanth Kandula. Enhancing network management using code generated by large language models. In Proceedings of the 22nd ACM Workshop on Hot Topics in Networks, HotNets '23, page 196-204, New York, NY, USA, 2023. Association for Computing Machinery.

[208] Sidong Feng and Chunyang Chen. Prompting is all you need: Automated android bug replay with large language models. arXiv preprint arXiv:2306.01987, 2023.

[209] Samia Kabir, David N. Udo-Imeh, Bonan Kou, and Tianyi Zhang. Is stack overflow obsolete? an empirical study of the characteristics of chatgpt answers to stack overflow questions. arXiv preprint arXiv:2308.02312, 2024.

[210] Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and William Yang Wang. Weak-to-strong jailbreaking on large language models. arXiv preprint arXiv:2401.17256, 2024.

[211] Surender Suresh Kumar, ML Cummings, and Alexander Stimpson. Strengthening llm trust boundaries: A survey of prompt injection attacks.

[212] Aysan Esmradi, Daniel Wankit Yip, and Chun Fai Chan. A comprehensive survey of attack techniques, implementation, and mitigation strategies in large language models. arXiv preprint arXiv:2312.10982, 2023.

[213] Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, and Chaowei Xiao. A new era in llm security: Exploring security concerns in real-world llm-based systems. arXiv preprint arXiv:2402.18649, 2024.

[214] Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. Comprehensive assessment of jailbreak attacks against llms. arXiv preprint arXiv:2402.05668, 2024.

[215] Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, and Stjepan Picek. Llm jailbreak attack versus defense techniques-a comprehensive study. arXiv preprint arXiv:2402.13457, 2024.

[216] Jiawen Shi, Yixin Liu, Pan Zhou, and Lichao Sun. Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt. arXiv preprint arXiv:2304.12298, 2023.

[217] Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, and Jinming Wen. Universal vulnerabilities in large language models: Backdoor attacks for in-context learning. arXiv preprint arXiv:2401.05949, 2024.

[218] Hongwei Yao, Jian Lou, and Zhan Qin. Poisonprompt: Backdoor attack on prompt-based large language models. arXiv preprint arXiv:2310.12439, 2023.

[219] Rodrigo Pedro, Daniel Castro, Paulo Carreira, and Nuno Santos. From prompt injections to sql injection attacks: How protected is your llm-integrated web application? arXiv preprint arXiv:2308.01990, 2023.

[220] Shuyu Jiang, Xingshu Chen, and Rui Tang. Prompt packer: Deceiving llms through compositional instruction with hidden attacks. arXiv preprint arXiv:2310.10077, 2023.

[221] Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang Gong. Prompt injection attacks and defenses in llm-integrated applications. arXiv preprint arXiv:2310.12815, 2023.

[222] Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. Backdooring instruction-tuned large language models with virtual prompt injection. arXiv preprint arXiv:2307.16888, 2023.

[223] Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, and David Wagner. Jatmo: Prompt injection defense by task-specific finetuning. arXiv preprint arXiv:2312.17673, 2024.

[224] George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret AnabyTavor, Orna Raz, and Eitan Farchi. Unveiling safety vulnerabilities of large language models. arXiv preprint arXiv:2311.04124, 2023.

[225] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. "do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825, 2023.

[226] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.

[227] Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black box jailbreaking of large language models. arXiv preprint arXiv:2309.01446, 2023.

[228] Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, and Shujian Huang. A wolf in sheep's clothing: Generalized nested jailbreak prompts can fool large language models easily. arXiv preprint arXiv:2311.08268, 2024.

[229] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Masterkey: Automated jailbreaking of large language model chatbots. In Proceedings 2024 Network and Distributed System Security Symposium, NDSS 2024. Internet Society, 2024.

[230] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Interpretable gradient-based adversarial attacks on large language models. arXiv preprint arXiv:2310.15140, 2023.

[231] Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253, 2023.

[232] Dongyu Yao, Jianshu Zhang, Ian G. Harris, and Marcel Carlsson. Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models. arXiv preprint arXiv:2309.05274, 2023.

[233] Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, and Enze Wang. Self-deception: Reverse penetrating the semantic firewall of large language models. arXiv preprint arXiv:2308.11521, 2023.

[234] Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models. arXiv preprint arXiv:2307.08487, 2023.

[235] Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multi-step jailbreaking privacy attacks on chatgpt. arXiv preprint arXiv:2304.05197, 2023.

[236] Rajesh Pasupuleti, Ravi Vadapalli, and Christopher Mader. Cyber security issues and challenges related to generative ai and chatgpt. In 2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS), pages 1-5, 2023.

[237] Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, and Ethan Perez. Sleeper agents: Training deceptive llms that persist through safety training. arXiv preprint arXiv:2401.05566, 2024.

[238] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. arXiv preprint arXiv:2310.02949, 2023.

[239] Fengqing Jiang, Zhangchen Xu, Luyao Niu, Boxin Wang, Jinyuan Jia, Bo Li, and Radha Poovendran. Identifying and mitigating vulnerabilities in llm-integrated applications. arXiv preprint arXiv:2311.16153, 2023.

[240] June Sallou, Thomas Durieux, and Annibale Panichella. Breaking the silence: the threats of using llms in software engineering. arXiv preprint arXiv:2312.08055, 2023.

[241] Neda Azizi and Omid Haass. Cybersecurity issues and challenges. In Handbook of research on cybersecurity issues and challenges for business and FinTech applications, pages 21-48. IGI Global, 2023.

[242] Jabu Mtsweni, Noluxolo Gcaza, and Mphahlele Thaba. A unified cybersecurity framework for complex environments. In Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists, pages 1-9, 2018.

[243] Tanay Varshney. Introduction to llm agents, 2023.

[244] Hongwei Cui, Yuyang Du, Qun Yang, Yulin Shao, and Soung Chang Liew. Llmind: Orchestrating ai and iot with llm for complex task execution. arXiv preprint arXiv:2312.09007, 2024.

[245] Maria Rigaki, Ondřej Lukáš, Carlos A. Catania, and Sebastian Garcia. Out of the cage: How stochastic parrots win in cyber security environments. arXiv preprint arXiv:2308.12086, 2023.

[246] Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel Kang. Llm agents can autonomously hack websites. arXiv preprint arXiv:2402.06664, 2024.

[247] Kaikai An, Fangkai Yang, Liqun Li, Zhixing Ren, Hao Huang, Lu Wang, Pu Zhao, Yu Kang, Hua Ding, Qingwei Lin, Saravan Rajmohan, and Qi Zhang. Nissist: An incident mitigation copilot based on troubleshooting guides. arXiv preprint arXiv:2402.17531, 2024.

[248] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. Tptu: Large language model-based ai agents for task planning and tool usage. arXiv preprint arXiv:2308.03427, 2023.

[249] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.

[250] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023.

[251] Yulong Liu, Yunlong Yuan, Chunwei Wang, Jianhua Han, Yongqiang Ma, Li Zhang, Nanning Zheng, and Hang Xu. From summary to action: Enhancing large language models for complex tasks with open world apis. arXiv preprint arXiv:2402.18157, 2024.

[252] Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, and Chengxiang Zhai. If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents. arXiv preprint arXiv:2401.00812, 2024.

[253] Bo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang Dong, Jue Zhang, Lu Wang, Minghua Ma, Pu Zhao, Si Qin, Xiaoting Qin, Chao Du, Yong Xu, Qingwei Lin, Saravan Rajmohan, and Dongmei Zhang. Taskweaver: A code-first agent framework. arXiv preprint arXiv:2311.17541, 2023.

[254] Yudong Huang, Hongyang Du, Xinyuan Zhang, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shuo Wang, and Tao Huang. Large language models for networking: Applications, enabling techniques, and challenges. arXiv preprint arXiv:2311.17474, 2023.

[255] Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, and Gongshen Liu. R-judge: Benchmarking safety risk awareness for llm agents. arXiv preprint arXiv:2401.10019, 2024.

[256] Fangzhou Wu, Shutong Wu, Yulong Cao, and Chaowei Xiao. Wipi: A new web threat for llm-driven web agents. arXiv preprint arXiv:2402.16965, 2024.

[257] Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents. arXiv preprint arXiv:2403.02691, 2024.

