# Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction 

Jiaming Ji * Boyuan Chen* Hantao Lou Donghai Hong Borong Zhang Xuehai Pan Juntao Dai<br>Yaodong Yang ${ }^{\dagger}$

Center for AI Safety and Governance, Institute for AI, Peking University


#### Abstract

Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answercorrection dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on different open-source and API-based models. Remarkably, Aligner-7B improves 11 different LLMs by $21.9 \%$ in helpfulness and $23.8 \%$ in harmlessness on average (GPT-4 by $17.5 \%$ and $26.9 \%$ ). When finetuning (strong) Llama270B with (weak) Aligner-13B's supervision, we can improve Llama2 by $8.2 \%$ in helpfulness and $61.6 \%$ in harmlessness. See our dataset and code at https://aligner2024.github.io.


## 1. Introduction

The alignment of LLMs with human intentions and values has recently gained significant attention (Ji et al., 2023b;[^0]

![](https://cdn.mathpix.com/cropped/2024_05_26_887a92225a43ac3b6e42g-01.jpg?height=564&width=824&top_left_y=713&top_left_x=1060)

Figure 1. Architecture of the Aligner module and illustration of its behavior in semantic space. Left: Correction workflow. The Aligner, a plug-and-play model, stacks upon an upstream LLM (aligned or unaligned). The Aligner redistributes initial answers from the upstream model into more helpful and harmless answers, thus aligning the composed LLM responses with human intentions. Right: It is challenging to learn direct mappings from queries to aligned answers. Nonetheless, correcting answers based on the upstream model's output is a more tractable learning task.

Casper et al., 2023), with RLHF (Christiano et al., 2017; Ouyang et al., 2022; Rafailov et al., 2023) identified as a practical approach. RLHF trains a Reward Model (RM) on human preferences and finetunes LLMs using feedback signals from the RM by reinforcement learning (RL) methods (Schulman et al., 2017). However, obtaining highquality feedback that accurately represents human values is challenging, and datasets may be corrupted by individual annotators (Casper et al., 2023). Additionally, applying RLHF to API-based LLMs (Achiam et al., 2023; Anil et al., 2023) is difficult because model parameters often need to be available. While the moderation APIs (Jigsaw, 2017; OpenAI, 2023a) can filter out dangerous responses by refusing to answer, they tend to overcorrect, resulting in unhelpful templated replies.

The RLHF method is more difficult to train compared to supervised finetuning (SFT) because it involves the complex coordination of Actor, Critic, Reward, and Reference
models (Casper et al., 2023; Ouyang et al., 2022; Yao et al., 2023). The RM, essential for mapping human preferences (discrete) into numerical space (continuous), needs more robust generalization, as seen in the seq2seq models in the textual domain (Keneshloo et al., 2019; Cheng et al., 2020). Taking inspiration from residual learning (He et al., 2016) and scalable oversight (Amodei et al., 2016; Bowman et al., 2022), we simplify the alignment process by focusing on copy and correction operation, utilizing seq2seq models to learn implicit residuals for better alignment. Without involving any RL processes (refer to Table 1), we introduce an efficient alignment paradigm, the Aligner, based on the seq2seq model (Zhang et al., 2017; Daza \& Frank, 2018; Vernikos et al., 2023). In contrast to RLHF methods that need to train and serve multiple models, the Aligner requires only an extra module stacked onto the upstream LLM ${ }^{1}$ for alignment. Moreover, our method's computational resource demand depends solely on the desired efficacy of the Aligner, not on the parameter size of the upstream LLMs.

Recently, OpenAI (2023b) presents the superalignment problem - how humans can supervise AI systems that are smarter than the supervisors. Specifically, Burns et al. (2023) utilizes weak models to provide feedback for training strong models, a concept known as weak-to-strong generalization. Building on Aligner, we offer a novel perspective to understand weak-to-strong generalization and demonstrate its feasibility, as shown in Figure 3. Specifically, we integrate weak (a small Aligner) and strong models to supervise strong experts, embodying the principle of standing on the shoulders of giants to see further.

In summary, Aligner presents several significant advantages:
- Training Aligners does not involve any RLHF process. Without extra models such as the actor, critic, reward, and reference model, our Aligner is an autoregressive seq2seq model that is trained on the query-answercorrection dataset via supervised learning. It is more computationally efficient. Specifically, when aligning a 70B LLM, Aligner-7B occupies 16.67 times smaller than DPO and 30.7 times smaller than RLHF $^{2}$ regarding training parameters.
- The Aligner framework facilitates weak-to-strong generalization. Leveraging supervisory signals from the small Aligner model to finetune strong models significantly boosts performance. Specifically, when finetuning (strong) Llama2-70B with (weak) Aligner-13B's supervision, we can improve Llama2 by $8.2 \%$ and[^1]![](https://cdn.mathpix.com/cropped/2024_05_26_887a92225a43ac3b6e42g-02.jpg?height=438&width=828&top_left_y=236&top_left_x=1060)

Figure 2. Analogy of the Aligner as a residual learning enhancer for LLMs in both architecture and capability aspects. This schematic showcases the Aligner acting similarly to a residual block in neural networks. It takes an initial output $\boldsymbol{y}_{o}$ from the upstream LLM, then the Aligner applies its autoregressive capabilities to generate a corrected version $\boldsymbol{y}_{c}$. Just as a residual block uses a shortcut to add modifications without changing the base structure, the Aligner employs a "copy and correct" method, overlaying improvements onto the original answer without altering its fundamental structure. This parallel highlights the Aligner's dual role in preserving the initial response while enhancing it to better align with desired outcomes.

## $61.6 \%$ in helpfulness and harmlessness.

- The Aligner's plug-and-play nature and model agnosticism make it ideal for API-based models without parameter access. Once trained, the Aligner can be applied across different upstream LLMs without requiring parameter adjustments. Experiments showed that the Aligner-7B model enhances both the helpfulness and harmlessness across a spectrum of 11 models, including API-based, open-source, and safetyaligned/safety-unaligned models. Experiment results demonstrate that the Aligner-7B increased GPT-4's helpfulness by $17.5 \%$ and its harmlessness by $26.9 \%$.

We have released our entire code, data, intermediate model checkpoints, and evaluation methods, all of which can be easily adapted to other LLMs. Specifically, we have opensourced Aligner models that were trained on Llama2 models (Touvron et al., 2023) of various sizes, including 7B, 13B, and 70B versions. Additionally, we have made available models trained with different data volumes and intermediate model checkpoints from the training process, enabling the community to use and validate them.

## 2. Related Work

In this work, we focus on solving the alignment problems of LLMs, which is to align the model's behavior with human intentions and values. Our work intersects with four major research areas: (1) Large Language Models demonstrate impressive performance across tasks, matching or exceed-

Table 1. Overview of Alignment Methodologies. The Aligner module, noted for its flexibility, is not constrained by specific model parameters or configurations. In contrast, traditional methods such as RLHF are limited by their need for direct access to a model's parameters. With the growth of model sizes, such as those with over 70B parameters (Touvron et al., 2023), RLHF's computational demands have increased. Filter-based methods often overcorrect when replacing unsafe responses with refusals, sometimes eliminating even the safe parts of the response. An alternative approach combines both user prompts and model responses to moderation filtering (Ji et al., 2023a); however, it also depends on the model's ability to generate safe responses.

| Type | Method | Reward Model | Policy Model | Support API-based Models |
| :---: | :---: | :---: | :---: | :---: |
| - | SFT | $x$ | $\checkmark$ | No |
| Moderation Filters | Perspective API (Jigsaw, 2017) <br> Q Moderation (OpenAI, 2023a) <br> Q-A Moderation (Ji et al., 2023a) | - <br> - <br> - | - <br> - <br> - | Yes <br> Yes <br> Yes |
| RLHF \& its variants | RLHF (Ouyang et al., 2022) <br> DPO (Rafailov et al., 2023) <br> Safe RLHF (Dai et al., 2024) <br> SPO (Swamy et al., 2024) | $\checkmark$ | ![](https://cdn.mathpix.com/cropped/2024_05_26_887a92225a43ac3b6e42g-03.jpg?height=196&width=203&top_left_y=748&top_left_x=1251) | No <br> No <br> No <br> No |
| Seq2Seq models | Aligner | $x$ | $x$ | $\overline{\text { Yes }}$ |

ing human expertise in some areas (Achiam et al., 2023; Yang et al., 2023a; Team et al., 2023); (2) Reinforcement Learning from Human Feedback aims to align LLMs with human preferences, utilizing RL algorithms (Schulman et al., 2017) to train LLMs, specifically LLMs, to maximize cumulative rewards from RMs (Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022a; Rafailov et al., 2023; Lee et al., 2023; Yang et al., 2023b); (3) Refinement \& Self-Refinement that enhance models' initial outputs using iterative reasoning mechanisms (Mita et al., 2020; Reid \& Neubig, 2022; Huang et al., 2023; Yang et al., 2023c; Madaan et al., 2023; Shinn et al., 2023; Vernikos et al., 2023). (4) Weak-to-Strong Generalization, a phenomenon that finetunes strong pre-trained models on labels generated by a weak model, they consistently perform better than their weak supervisors (Burns et al., 2023). A more detailed review of related work can be found in Appendix B.

## 3. Aligner

Preliminary: Supervised Fine-Tuning (SFT) SFT is a method to finetune the pre-trained LLM to conditionally generate the target answer using supervised learning specifically, maximum likelihood estimation - on a curated high-quality dataset $\mathcal{D}_{\mathrm{SFT}}=\left\{\boldsymbol{x}^{(i)}, \boldsymbol{y}^{(i)}\right\}_{i=1}^{N}$. The goal is to obtain a model $\pi_{\theta}^{\mathrm{SFT}}$ with the following training objective:

$$
\begin{equation*}
\underset{\boldsymbol{\theta}}{\operatorname{minimize}} \mathcal{L}\left(\boldsymbol{\theta} ; \mathcal{D}_{\mathrm{SFT}}\right)=-\mathbb{E}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \mathcal{D}_{\mathrm{SFT}}}\left[\log \pi_{\boldsymbol{\theta}}(\boldsymbol{y} \mid \boldsymbol{x})\right] \tag{1}
\end{equation*}
$$

Similarly, illustrated in Figure 1, the Aligner improves alignment between the model and human intentions by redistributing the model's answers through conditional generation. In practical implementation, the Aligner only needs to make a minor adjustment to the SFT training code (only need to change one line of code), as detailed in Appendix D. Overall, the whole pipeline of Aligner training can be summarized as follows:

Collect Q-A Datasets We sourced queries from diverse open-source datasets, including the Stanford Alpaca (Taori et al., 2023), user-shared conversations from ShareGPT ${ }^{3}$, HH-RLHF (Ganguli et al., 2022; Bai et al., 2022a) and others. These queries underwent a process of repetitive pattern removal and quality filtering, resulting in a refined set of $27 \mathrm{~K}$ queries for the subsequent answer and corrected answer generation. The original answers were generated using various open-source models, including Alpaca-7B (Taori et al., 2023), Vicuna-(7B,13B,33B) (Chiang et al., 2023), Llama2-(7B,13B)-Chat (Touvron et al., 2023), and Alpaca2-(7B,13B) ${ }^{4}$. More details about the structure of Q-A Datasets can be found in Appendix F.1.

Answer Correction We used prompted GPT-4, prompted Llama2-70B-Chat, and human annotators to revise the answers in the above Q-A dataset. These revisions were based on a series of well-defined principles, which established constraints for the training of the seq2seq model. These principles were aimed at effectively extending to the characteristics we wish LLMs to embody. According to the 3H standard of LLMs (Helpful, Harmless, Honest) (Askell et al., 2021), we focused on the dimensions of helpfulness and harmlessness. In particular, regarding harmlessness, we referred to the 14 fundamental principles defined in $\mathrm{Ji}$ et al. (2023a); OpenAI (2023a). For those answers that[^2]

![](https://cdn.mathpix.com/cropped/2024_05_26_887a92225a43ac3b6e42g-04.jpg?height=586&width=1677&top_left_y=230&top_left_x=186)

Figure 3. An illustration of our methodology. The Superalignment problem focuses on scaling human oversight for supervising increasingly intelligent and complex AI systems. The Weak-to-Strong Generalization (Burns et al., 2023) analogy emphasizes using weaker models to supervise stronger ones. Our approach composes weak and strong models to offer iteratively scalable supervision.

conform to these fundamental principles, we retain the relevant content of the original answers. Figure 5 (a) visually shows the distribution shift before and after the data correction, thereby more clearly demonstrating the impact of the revision process on the dataset.

Model Training Based on the above procedures, we have constructed the dataset $\mathcal{M}=\left\{\boldsymbol{x}^{(i)}, \boldsymbol{y}_{o}^{(i)}, \boldsymbol{y}_{c}^{(i)}\right\}_{i=1}^{N}$, which $\boldsymbol{x}$ represents the user's query, $\boldsymbol{y}_{o}$ is the original answers to the query, and $\boldsymbol{y}_{c}$ is the corrected answer according to established principles. The model training process is relatively straightforward. We train the Aligner, a conditional seq2seq model $\mu_{\phi}\left(\boldsymbol{y}_{c} \mid \boldsymbol{y}_{o}, \boldsymbol{x}\right)$ parameterized by $\phi$, to redistribute the preliminary answers $\boldsymbol{y}_{o}$ to the aligned answer $\boldsymbol{y}_{c}$. Demonstrated in Figure 2, the composed answer generation process for aligned answers based on the upstream $\operatorname{LLM} \pi_{\boldsymbol{\theta}}$ is:

$$
\begin{equation*}
\pi^{\prime}\left(\boldsymbol{y}_{c} \mid \boldsymbol{x}\right)=\mu_{\boldsymbol{\phi}}\left(\boldsymbol{y}_{c} \mid \boldsymbol{y}_{o}, \boldsymbol{x}\right) \pi_{\boldsymbol{\theta}}\left(\boldsymbol{y}_{o} \mid \boldsymbol{x}\right) \tag{2}
\end{equation*}
$$

The empirical loss on dataset $\mathcal{M}$ is:

$$
\begin{align*}
& -\mathbb{E}_{\mathcal{M}}\left[\log \pi^{\prime}\left(\boldsymbol{y}_{c} \mid \boldsymbol{x}\right)\right] \\
= & -\mathbb{E}_{\mathcal{M}}\left[\log \mu_{\boldsymbol{\phi}}\left(\boldsymbol{y}_{c} \mid \boldsymbol{y}_{o}, \boldsymbol{x}\right)\right]-\mathbb{E}_{\mathcal{M}}\left[\log \pi_{\boldsymbol{\theta}}\left(\boldsymbol{y}_{o} \mid \boldsymbol{x}\right)\right] \tag{3}
\end{align*}
$$

The second term is not related to the Aligner parameter and the training objective for Aligner can be derived as:

$$
\underset{\phi}{\operatorname{minimize}} \mathcal{L}_{\text {Aligner }}(\boldsymbol{\phi}, \mathcal{M})=-\mathbb{E}_{\mathcal{M}}\left[\log \mu_{\phi}\left(\boldsymbol{y}_{c} \mid \boldsymbol{y}_{o}, \boldsymbol{x}\right)\right]
$$

It is worth noting that Aligner does not require access to the model parameters of the upstream LLM $\pi_{\boldsymbol{\theta}}$ during both training and inference phases. Aligner takes the user's query $\boldsymbol{x}$ and the initial answer $\boldsymbol{y}_{o}$ generated by the upstream LLM $\pi_{\boldsymbol{\theta}}$, then generates the answer $\boldsymbol{y}_{c}$ which is better aligned with human values. Improving existing answers $\boldsymbol{y}_{o}$ allows
Aligner to focus on how to align with human values rather than how to answer the given query directly. This significantly reduces the requirements on our model capacity, allowing us to achieve the expected alignment performance with only a small model.

### 3.1. Aligner vs. RLHF/DPO

Compared to RLHF (Bai et al., 2022a) and DPO (Rafailov et al., 2023), Aligner shows notable advantages in training resource requirements and interpretability. Regarding training resources, Aligner-7B is more efficient than other methods under similar performance conditions. Specifically, with a 7B source model, DPO requires 1.66 times, and RLHF 5.71 times more resources than Aligner. Additionally, as the source model's scale increases, the resource demands for other methods rise sharply: for a 70B model, DPO needs 16.67 times, and RLHF 30.7 times more resources than Aligner. However, as Aligner is insensitive to these changes, its training resource requirements remain constant regardless of the source model's scale. Regarding interpretability, Aligner, based on the seq2seq model, surpasses other methods. This advantage stems from the textual space's inherent generalizability - adjusting the training dataset's distribution allows Aligner's behavior to align with our expectations. However, RLHF's training process transforms text into a scalar reward, which lacks generalizability, thereby reducing information stored in the sequence and lacking interpretability.

### 3.2. Aligner's Training Strategy: Residual Correction

We develop an optimized training strategy, termed Residual Correction, which leverages the semantic residual between Answer and Correction (as shown in Figure 1 and 2). Specifically, we construct a Q-A-A Dataset using partial training

Table 2. Weak-to-strong generalization results demonstrate that Aligner-7B can achieve weak-to-strong generalization on 7B, 13B, and 70B upstream models with existing alignment methods using the labels given by the Aligner. This process entails enhancing the capabilities of a stronger model by finetuning it with labels generated from a weaker model.

| Method ${ }^{\dagger}$ | BeaverTails |  | HarmfulQA |  | Average |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Helpfulness | Harmlessness | Helpfulness | Harmlessness | Helpfulness | Harmlessness |
| Alpaca-7B w/Aligner-7B |  |  |  |  |  |  |
| $+\mathrm{SFT}$ | $+8.4 \%$ | $+53.5 \%$ | $+19.6 \%$ | $+73.9 \%$ | $+14.0 \%$ | $+63.7 \%$ |
| +RLHF | $-41.7 \%$ | $+51.4 \%$ | $-36.1 \%$ | $+73.9 \%$ | $-38.9 \%$ | $+62.6 \%$ |
| + DPO | $-48.2 \%$ | $+45.6 \%$ | $-54.4 \%$ | $+68.6 \%$ | $-51.3 \%$ | $+57.1 \%$ |
| Alpaca2-13B w/ Aligner-7B |  |  |  |  |  |  |
| + SFT | $+34.7 \%$ | $+49.4 \%$ | $+22.1 \%$ | $+69.7 \%$ | $+28.4 \%$ | $+59.6 \%$ |
| +RLHF | $+46.0 \%$ | $+20.2 \%$ | $-2.9 \%$ | $+67.6 \%$ | $++21.6 \%$ | $+43.9 \%$ |
| +DPO | $+1.3 \%$ | $+57.3 \%$ | $-20.4 \%$ | $+79.6 \%$ | $-9.6 \%$ | $+68.4 \%$ |
| Alpaca2-70B w/Aligner-13B |  |  |  |  |  |  |
| +SFT | $+9.3 \%$ | $+46.9 \%$ | $+7.2 \%$ | $+76.3 \%$ | $+8.2 \%$ | $+61.6 \%$ |

data to initially train an identity Aligner, a process we term "warm-up". Subsequently, we utilize the complete Q-A$\mathrm{C}$ dataset for training, building upon the identity Aligner. The details of our experiments on a $50 \mathrm{~K}$ training dataset are shown in Table 20. Overall, the warm-up step aids the Aligner initially learning identity mapping, improving training outcomes. Outside the alignment field, ResNet (He et al., 2016) also uses a similar approach to mitigate the accuracy decline and convergence difficulties caused by increased neural network depth. However, determining the specific data proportion for warm-up is challenging. In common practice, we bypass the warm-up step and directly train Aligner with the complete Q-A-C dataset.

## 4. Weak-to-Strong Generalization via Aligner

## If I have seen further it is by standing on the shoulders of giants.

-Isaac Newton

Weak-to-strong generalization is a training paradigm that leverages supervisor signals provided by weaker capability models to enhance the performance of stronger models. Burns et al. (2023) has conducted preliminary trials in NLP classification, chess puzzles, and reward modeling tasks, observing positive gains by simply finetuning strong pretrained models using pseudo-labels produced by weak models. This paradigm is analogous to the concept of "teaching" where the weak model instructs the strong one.

As shown in Figure 3, we propose a novel yet related weakto-strong generalization paradigm based on the nature of Aligner. The core insight is to utilize a weak Aligner model to teach a stronger upstream model, thereby generating labels for finetuning the strong upstream model to enhance its performance. We trained strong models using weak labels through three methods: SFT, RLHF, and DPO. Table

![](https://cdn.mathpix.com/cropped/2024_05_26_887a92225a43ac3b6e42g-05.jpg?height=659&width=805&top_left_y=221&top_left_x=1061)

Figure 4. Left: With the input of user prompts, Burns et al. (2023) directly uses a weak model to generate supervisory labels to finetune the strong model. Right (Ours): Based on both user prompts and the response from the strong model, the weak model (i.e, Aligner) generates an improved response, which can either serve as labels for fine-tuning the strong model or as another output during inference.

2 shows that the weak labels from Aligner-7B and Aligner13B improve the performance of Llama2 series strong model in all scenarios when used for finetuning an upstream model via $\mathrm{SFT}^{5}$. Additional observations are as follows:
- The RLHF and DPO methods significantly improve the upstream model's performance on certain metrics. However, they do not completely surpass the strong model's original capabilities, particularly regarding decreased helpfulness. This decline is due to these models' tendency to conservative patterns (i.e., qualitative answers with less informational content). This suggests that the two-stage learning process of reward modeling and policy optimization, compared to SFT's direct label-based mapping, may introduce more feature noise and information loss, making accurate optimization more challenging.
- The RLHF method outperforms the DPO method in general. Given that the training data for weak-to-strong generalization is based on the output from the upstream model, subsequently aligned by Aligner-7B. The RLHF method shows better performance in this semi-online setting.
- The safety improvement is more substantial than that in helpfulness. Safety is easier to assess compared to helpfulness and can more readily be enhanced through simple rejection.[^3]

Table 3. Performance of Aligner Models. It is shown that Aligner achieves significant performances in all the settings. All assessments in this table were conducted based on integrating various models with Aligners to compare with the original models to quantify the percentage increase in helpfulness and harmlessness. The background color represents the type of target language model: green represents API-based models, orange represents open-source models without safety alignment, and blue represents safety-aligned open-source models. The icon $\boldsymbol{\Omega}$ indicates the model parameters are not accessible and $\mathbf{D}$ indicates the model is safety-aligned.

| Aligner | LLM |  | BeaverTails |  | HarmfulQA |  | Average |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | Helpfulness | Harmlessness | Helpfulness | Harmlessness | Helpfulness | Harmlessness |
| 7B | GPT-4 | 00 | $+18.6 \%$ | $+25.8 \%$ | $+16.3 \%$ | $+28.0 \%$ | $+17.5 \%$ | $+26.9 \%$ |
|  | GPT-3.5 | 00 | $+9.3 \%$ | $+9.3 \%$ | $+8.4 \%$ | $+7.0 \%$ | $+8.9 \%$ | $+8.1 \%$ |
|  | Claude 2 | 00 | $+58.4 \%$ | $+\mathbf{3 0 . 3 \%}$ | $+69.4 \%$ | $+42.1 \%$ | $+63.9 \%$ | $+36.2 \%$ |
|  | Beaver-v1 | 0 | $+21.9 \%$ | $+12.0 \%$ | $+8.9 \%$ | $+6.0 \%$ | $+15.4 \%$ | $+9.0 \%$ |
|  | Llama2-7B-Chat | D | $+19.9 \%$ | $+7.4 \%$ | $-5.7 \%$ | $+22.1 \%$ | $+7.1 \%$ | $+14.8 \%$ |
|  | Llama2-13B-Chat | ర | $+20.1 \%$ | $+10.3 \%$ | $+15.5 \%$ | $+28.6 \%$ | $+17.8 \%$ | $+19.4 \%$ |
|  | Llama2-70B-Chat | 0 | $+5.2 \%$ | $+2.4 \%$ | $-6.6 \%$ | $+4.1 \%$ | $-0.7 \%$ | $+3.3 \%$ |
|  | Alpaca-7B |  | $+34.9 \%$ | $+47.0 \%$ | $+38.2 \%$ | $+70.7 \%$ | $+36.5 \%$ | $+\mathbf{5 8 . 9 \%}$ |
|  | Vicuna-7B |  | $+26.4 \%$ | $+15.9 \%$ | $+12.0 \%$ | $+29.3 \%$ | $+19.2 \%$ | $+22.6 \%$ |
|  | Vicuna-13B |  | $+\mathbf{3 7 . 6 \%}$ | $+16.6 \%$ | $+21.9 \%$ | $+18.9 \%$ | $+29.8 \%$ | $+17.7 \%$ |
|  | Vicuna-33B |  | $+51.0 \%$ | $+55.9 \%$ | $-1.0 \%$ | $+33.6 \%$ | $+25.0 \%$ | $+\mathbf{4 4 . 7 \%}$ |
| 13B | GPT-4 | 00 | $+\mathbf{3 3 . 9 \%}$ | $+25.1 \%$ | $+25.1 \%$ | $+20.1 \%$ | $+29.5 \%$ | $+22.6 \%$ |
|  | GPT-3.5 | 00 | $+15.1 \%$ | $+10.9 \%$ | $+7.6 \%$ | $+7.7 \%$ | $+11.3 \%$ | $+9.3 \%$ |
|  | Claude 2 | 00 | $+\mathbf{5 0 . 0 \%}$ | $+\mathbf{3 0 . 0 \%}$ | $+45.9 \%$ | $+28.6 \%$ | $+48.0 \%$ | $+29.3 \%$ |
|  | Beaver-v1 | 0 | $+14.2 \%$ | $+19.1 \%$ | $+8.0 \%$ | $+11.6 \%$ | $+11.1 \%$ | $+15.3 \%$ |
|  | Llama2-7B-Chat | D | $+13.5 \%$ | $+4.6 \%$ | $+12.6 \%$ | $+32.3 \%$ | $+13.1 \%$ | $+18.4 \%$ |
|  | Llama2-13B-Chat | ర | $+16.7 \%$ | $+10.6 \%$ | $+\mathbf{3 0 . 7 \%}$ | $+35.0 \%$ | $+23.7 \%$ | $+22.8 \%$ |
|  | Llama2-70B-Chat | ర | $+10.6 \%$ | $+1.9 \%$ | $+6.3 \%$ | $+7.6 \%$ | $+8.5 \%$ | $+4.7 \%$ |
|  | Alpaca-7B |  | $+8.5 \%$ | $\mathbf{+ 5 3 . 4 \%}$ | $+3.4 \%$ | $+75.9 \%$ | $+6.0 \%$ | $+64.6 \%$ |
|  | Vicuna-7B |  | $+19.1 \%$ | $+24.0 \%$ | $+19.5 \%$ | $+31.0 \%$ | $+19.3 \%$ | $+27.5 \%$ |
|  | Vicuna-13B |  | $+31.8 \%$ | $+26.7 \%$ | $+\mathbf{3 0 . 9 \%}$ | $+18.9 \%$ | $+31.3 \%$ | $+22.8 \%$ |
|  | Vicuna-33B |  | $+33.3 \%$ | $+63.3 \%$ | $+7.3 \%$ | $+33.3 \%$ | $+20.3 \%$ | $+48.3 \%$ |
| 70B | GPT-4 | $0 \pi$ | $+26.2 \%$ | $+29.3 \%$ | $+17.1 \%$ | $+31.7 \%$ | $+21.7 \%$ | $+30.5 \%$ |
|  | GPT-3.5 | 00 | $+16.4 \%$ | $+8.9 \%$ | $+25.2 \%$ | $+10.6 \%$ | $+20.8 \%$ | $+9.7 \%$ |
|  | Claude 2 | 00 | $+50.0 \%$ | $+29.4 \%$ | $+62.9 \%$ | $+\mathbf{3 9 . 7 \%}$ | $+56.4 \%$ | $+34.6 \%$ |
|  | Beaver-v1 | 0 | $+22.2 \%$ | $+11.7 \%$ | $+20.0 \%$ | $+7.9 \%$ | $+21.1 \%$ | $+9.8 \%$ |
|  | Llama2-7B-Chat | 0 | $+29.1 \%$ | $+6.4 \%$ | $+19.0 \%$ | $+25.6 \%$ | $+24.0 \%$ | $+16.0 \%$ |
|  | Llama2-13B-Chat | D | $+34.1 \%$ | $+9.3 \%$ | $+41.2 \%$ | $+29.0 \%$ | $+37.7 \%$ | $+19.1 \%$ |
|  | Llama2-70B-Chat | D | $+23.1 \%$ | $+1.9 \%$ | $+17.0 \%$ | $+6.9 \%$ | $+20.1 \%$ | $+4.4 \%$ |
|  | Alpaca-7B |  | $+38.5 \%$ | $+47.1 \%$ | $+39.7 \%$ | $+69.6 \%$ | $+39.1 \%$ | $+58.4 \%$ |
|  | Vicuna-7B |  | $+39.9 \%$ | $+15.4 \%$ | $+25.6 \%$ | $+29.7 \%$ | $+32.7 \%$ | $+22.6 \%$ |
|  | Vicuna-13B |  | $+49.4 \%$ | $+16.5 \%$ | $+19.4 \%$ | $+19.1 \%$ | $+34.4 \%$ | $+17.8 \%$ |
|  | Vicuna-33B |  | $+56.8 \%$ | $+57.6 \%$ | $+5.0 \%$ | $+33.3 \%$ | $+30.9 \%$ | $+45.5 \%$ |

## 5. Experiments

In this section, we assess the effectiveness of Aligner modules in different datasets and configurations. For detailed training parameters, see Appendix D.

### 5.1. Experiment Setup

Evaluation Datasets and Models To assess the Aligner module, we utilize two datasets: BeaverTails (Ji et al., 2023a) and HarmfulQA (Bhardwaj \& Poria, 2023). Please see the Appendix E. 1 for comprehensive details on these datasets. Our evaluation focus on two model categories: API-based models (e.g., GPT-4 (Achiam et al., 2023), Claude 2 (Anthropic, 2023)) and Open-Source models (Llama2-(7B, 13B, 70B)-Chat (Touvron et al., 2023);
Vicuna-(7B, 13B, 33B) (Chiang et al., 2023); Alpaca7B (Taori et al., 2023); Beaver-7B (Dai et al., 2024)). Notably, Llama2 and Beaver models have undergone different degrees of safety alignment processing, unlike the Alpaca7B model, which has not been safety-aligned.

Evaluation Metrics In line with Bai et al. (2022a); Dai et al. (2024), our evaluation of language model answers hinges on two key dimensions: helpfulness and harmlessness. These dimensions' distinct and independent characteristics provide a comprehensive perspective on the answers, allowing us to balance information quality with safety and ethical considerations when evaluating an answer's quality. For our evaluation, we use queries from BeaverTails (Ji et al., 2023a) and HarmfulQA (Bhardwaj \& Poria, 2023). Initial

![](https://cdn.mathpix.com/cropped/2024_05_26_887a92225a43ac3b6e42g-07.jpg?height=409&width=458&top_left_y=256&top_left_x=194)

(a) Aligner training dataset

- Answer $\cdot$ Correction

![](https://cdn.mathpix.com/cropped/2024_05_26_887a92225a43ac3b6e42g-07.jpg?height=409&width=399&top_left_y=256&top_left_x=646)

(b1) GPT-4

![](https://cdn.mathpix.com/cropped/2024_05_26_887a92225a43ac3b6e42g-07.jpg?height=409&width=382&top_left_y=259&top_left_x=1056)

(b2) Alpaca-7B

![](https://cdn.mathpix.com/cropped/2024_05_26_887a92225a43ac3b6e42g-07.jpg?height=407&width=388&top_left_y=260&top_left_x=1443)

(b3) Llama2-70B-Chat

- Original Answer $\quad$ Corrected by Aligner-7B $\square$ Answer Average $\square$ Correction Average

Figure 5. Distribution of helpfulness and harmlessness scores in training and evaluation sets. (a) The distribution shift in answers and correctional answers in the training dataset; (b) redistribution shift of Aligner-7B, based on upstream models such as GPT-4 (b1), Alpaca-7B (b2) and Llama2-70B-Chat (b3). We found that (1) The correctional answer in the training dataset surpasses the original answers in terms of both helpfulness and harmlessness; (2) The refuse-to-answer pattern of GPT-4 created an area of overcorrected answers where both helpful and harmless scores are low, and our Aligner-7B improved these answers by providing additional information and corrections. (3) The Alpaca-7B model, which is not aligned, had its answers corrected by our Aligner-7B, significantly increasing both scores. (4) The Llama2-70B-Chat model is already aligned (the average safety score is higher than the correction in the training dataset), and the correction of Aligner-7B enhanced the helpfulness significantly while maintaining the harmless score.

answers are generated by open-source and upstream models, which the Aligner refines to yield corrected answers. These are ranked separately in terms of helpfulness and harmlessness. More details and examples can be referred to in Appendix E.

### 5.2. Experiment Results

We have integrated the Aligner module with various upstream models to assess its impact on re-distributing the original answers regarding helpfulness and harmlessness. Table 3 illustrates that employing variously sized Aligners substantially improves the performance of all 11 models, achieving helpfulness by $24.3 \%$ and harmlessness by $24.7 \%$ on average. Aligner-7B can achieve an average improvement of $21.9 \%$ on helpfulness and $23.8 \%$ on harmlessness over 11 models. Remarkably, Aligner-7B can boost GPT-4' answers' helpfulness by $17.5 \%$ and harmlessness by $26.9 \%$, and similar experiments with Claude 2 yield even more pronounced improvements. Ablation studies reveal that Aligner delivers comparable results of RLHF and DPO with significantly reduced computational resources.

Parameter Efficiency of Aligner Module Unlike RLHFbased methods that increase resource demands with larger base models, our Aligner-focused technique allows the base model to remain unchanged, offering adaptability in Aligner model sizing according to available resources. Additionally, we utilized Aligner-7B to align Llama2-(7B, 13B, 70B)Chat models with varying capacities, as shown in Table 3. We noted that Aligner-7B consistently enhanced both helpfulness and harmlessness performance for each model size, even though they had significantly more parameters. The Aligner effectively transformed potentially harmful outputs into safe answers with minimal information loss (Table 14 for specific examples) and provided more informative answers to previously refused queries.

## Assessing Aligner's Impact on Safety-Aligned Models

Table 3 demonstrates how Aligner enhances the harmlessness and particularly the helpfulness of Llama2-Chat and Beaver models. Llama2-Chat, with its multi-stage alignment process (pre-training, SFT, RLHF), and Beaver, finetuned via Safe RLHF (Dai et al., 2024), both show modest safety improvements with Aligner. The key achievement of Aligner is its ability to amplify helpfulness, especially in models predisposed to avoid risky responses. By redistributing these overly conservative answers, Aligner significantly boosts overall helpfulness. This enhancement in helpfulness is visually represented in Figure 5, where a rightward shift in Llama2-70B-Chat's answer distribution, under the influence of Aligner-7B, indicates improved helpfulness, building on the model's strong safety foundation.

Performance Evaluation of Aligner with Different Parameter Scales and Data Volume In our expanded evaluation, we examined Aligner's efficacy across different model sizes (7B, 13B, 70B). Results showed that largerscale Aligners (13B and 70B) substantially improved the helpfulness and harmlessness of the answer compared to the Aligner-7B model. These larger models also produced answers with higher information density and coherence. Additionally, the performance of Aligner's seq2seq architecture scaled positively with increased training data, showing pro-

Table 4. Ablation study assessed Aligner's effectiveness against methods like CAI, Self-Refine, and Self-Critique. This analysis revealed that Aligner notably surpasses these baselines in both helpfulness and harmlessness metrics. For a more detailed exploration of these findings, please see Appendix C.3.1.

|  | BeaverTails |  |  | HarmfulQA |  |
| :--- | :---: | :---: | :--- | :--- | :---: |
| Method | Helpfulness | Harmlessness |  | Helpfulness | Harmlessness |
| GPT-4 |  |  |  |  |  |
| +CAI w/o training ${ }^{\dagger}$ | $+21.2 \%$ | $+11.0 \%$ |  | $+19.1 \%$ | $+8.3 \%$ |
| +Self-Critique | $+31.7 \%$ | $+19.9 \%$ |  | $+22.6 \%$ | $+18.4 \%$ |
| + +Aligner-13B | $+\mathbf{3 3 . 9 \%}$ | $\mathbf{+ 2 5 . 1 \%}$ |  | $\mathbf{+ 2 5 . 1 \%}$ | $+\mathbf{+ 2 0 . 1 \%}$ |

${ }^{\dagger}$ We employ CAI prompts solely during the inference time of LLMs to encourage self-revision of their answers. In this context, using CAI without prior training represents a unique form of self-refinement for LLMs.

gressive improvements across datasets ranging from $20 \mathrm{~K}$ to $50 \mathrm{~K}$. For a comprehensive analysis of these results, refer to Appendix $\mathrm{C}$ for an in-depth discussion.

### 5.3. Ablation Study

Comparison to Self-Refine/Critique Methods Common methods to enhance model safety include Constitutional AI (Bai et al., 2022b), Self-Critique (Saunders et al., 2022), and Self-Refine (Madaan et al., 2023). This approach primarily utilizes the self-critiquing and refining capabilities of LLMs to boost performance. We compared these methods with the Aligner module, as shown in Table 4. Our method demonstrated superior performance over the baseline. Additionally, baseline methods typically need multiple dialogue iterations and extended context windows for prompt insertion and ongoing self-correction. This may result in longer inference times for the base model and considerable consumption of context window length. For more detailed information and analysis, please see Appendix C.3.1.

Comparsion to RLHF/DPO/SFT We finetuned Alpaca7B with SFT, RLHF, and DPO, then compared these finetuned versions against the original Alpaca-7B corrected by Aligner. The findings show that Aligner either matches or exceeds the improvements of the baseline models, as shown in Table 5. Notably, RLHF and DPO, after finetuning, tend to produce conservative answers and fail to explicitly recognize dangers while adding helpful information. It's important to highlight that RLHF and DPO required significantly more parameters for training - four times and two times more, respectively - than what was needed for training Aligner. For more detailed information and analysis, please see Appendix C.3.2.

Table 5. Ablation study: Alpaca-7B aligned using Aligner demonstrates superior performance when directly compared to models finetuned with baselines.

|  | BeaverTails |  |  | HarmfulQA |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Method | Helpfulness | Harmlessness |  | Helpfulness | Harmlessness |
| Aligner vs. SFT | $+2.4 \%$ | $+0.3 \%$ |  | $+23.1 \%$ | $+0.4 \%$ |
| Aligner vs. RLHF | $+0.3 \%$ | $+21.7 \%$ |  | $+24.4 \%$ | $+21.9 \%$ |
| Aligner vs. DPO | $+24.0 \%$ | $+0.1 \%$ |  | $+49.1 \%$ | $+0.1 \%$ |

## 6. Discussion and Conclusion

We believe the Aligner framework introduces an efficient and model-agnostic approach to aligning LLMs with human intentions and values. By employing a more streamlined, autoregressive seq2seq model that operates without the need for additional components such as the actor, critic, reward, and reference models, Aligner demonstrates a significant increase in computational efficiency. Moreover, we demonstrate that the Aligner achieve weak-to-strong generalization capabilities. By finetuning on the supervisory signals from the weak Aligner-13B model, we can enhance the performance of the strong Llama2-70B model.

### 6.1. Ethics and Impact

The Aligner dataset will be released under the CC BY-NC 4.0 license. This dataset integrates Q-A data from opensource and API-based models, with answers revised to meet the 3H model standards (Helpful, Harmless, Honest) (Askell et al., 2021). It features safety meta-tags for answers, harm category classification, and annotations for helpfulness and harmlessness, including GPT-4, human, and larger model annotations. This offers significant potential to develop AI assistants aligned with human intentions and social values. However, there is an inherent risk: theoretically, this dataset could also train AI assistants for harmful or malicious purposes. As the Aligner dataset's creators, we are dedicated to fostering beneficial and safe AI technology and strongly oppose any misuse that could hinder human progress. We strongly condemn any malicious use of the Aligner dataset and advocate for its responsible and ethical use.

### 6.2. Limitations and Future Work

In this section, we discuss the limitations of the current work and describe our plan to address these problems. While Aligner is promising in specialized NLP tasks, its broader application in aligning LLMs with human intentions requires further experimental validation. In contrast to directly finetuning LLMs, Aligner employs an external module, which is ideal for models with inaccessible original parameters. However, this approach adds to the inference burden, as elaborated in Appendix C.2. We aim to enhance LLM alignment using the Aligner module, aiming for increased conciseness, efficiency, and interpretability. Future research will focus on enhancing Aligner's versatility in challenging contexts like multi-turn dialogues and developing Control Aligner for domain-specific alignment with precise instructions. Lastly, enhancing Aligner's interpretability is essential. Unlike RLHF's segmented approach, its end-to-end structure provides valuable insights into the alignment process for LLMs.

## References

Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mané, D. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

Anthropic. Claude 2. https://www.anthropic.co m/index/claude-2, 2023.

Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.

Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.

Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b.

Bhardwaj, R. and Poria, S. Red-teaming large language models using chain of utterances for safety-alignment. arXiv preprint arXiv:2308.09662, 2023.

Bowman, S. R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., Lukošiūtè, K., Askell, A., Jones, A., Chen, A., et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022.

Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.

Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., Wang, T. T., Marks, S., Segerie, C.-R., Carroll, M., Peng, A., Christoffersen, P., Damani, M., Slocum, S., Anwar, U., Siththaranjan, A., Nadeau, M., Michaud, E. J., Pfau, J., Krasheninnikov, D., Chen, X., Langosco, L., Hase, P., Biyik, E., Dragan, A., Krueger, D.,
Sadigh, D., and Hadfield-Menell, D. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https: //openreview.net/forum?id=bx24KpJ4Eb. Survey Certification.

Chen, X., Lin, M., Schärli, N., and Zhou, D. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.

Cheng, M., Yi, J., Chen, P.-Y., Zhang, H., and Hsieh, C.J. Seq2sick: Evaluating the robustness of sequence-tosequence models with adversarial examples. In Proceedings of the AAAI conference on artificial intelligence, volume 34(04), pp. 3601-3608, 2020.

Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.

Christiano, P., Shlegeris, B., and Amodei, D. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575, 2018.

Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Computer, T. RedPajama: an Open Dataset for Training Large Language Models. https://github.com/t ogethercomputer/RedPajama-Data, 2023.

Dai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang, Y., and Yang, Y. Safe RLHF: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=TyFr POKYXw.

Daza, A. and Frank, A. A sequence-to-sequence model for semantic role labeling. ACL 2018, pp. 207, 2018.

Deshpande, A., Murahari, V., Rajpurohit, T., Kalyan, A., and Narasimhan, K. R. Toxicity in chatgpt: Analyzing persona-assigned language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/f orum?id=wZKRStVJJe.

Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022.

Gulcehre, C., Paine, T. L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C., et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.

Huang, J., Gu, S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1051-1068, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.67. URL https: //aclanthology.org/2023.emnlp-main.67.

Ji, J., Liu, M., Dai, J., Pan, X., Zhang, C., Bian, C., Chen, B., Sun, R., Wang, Y., and Yang, Y. Beavertails: Towards improved safety alignment of LLM via a human-preference dataset. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a. URL https://openreview.net/forum ?id=g0QovXbFw3.

Ji, J., Qiu, T., Chen, B., Zhang, B., Lou, H., Wang, K., Duan, Y., He, Z., Zhou, J., Zhang, Z., et al. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852, $2023 b$.

Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. Survey of hallucination in natural language generation. $A C M$ Comput. Surv., 55(12), mar 2023c. ISSN 0360-0300. doi: 10.1145/3571730. URL https://doi.org/10.1 $145 / 3571730$.

Jigsaw, G. Perspective API. https://www.perspect iveapi.com, 2017.

Keneshloo, Y., Shi, T., Ramakrishnan, N., and Reddy, C. K. Deep reinforcement learning for sequence-to-sequence models. IEEE transactions on neural networks and learning systems, 31(7):2469-2489, 2019.

Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611-626, 2023.

Lee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T., Bishop, C., Carbune, V., and Rastogi, A. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.
Li, H., Guo, D., Fan, W., Xu, M., Huang, J., Meng, F., and Song, Y. Multi-step jailbreaking privacy attacks on chatGPT. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https: //openreview.net/forum?id=ls4Pfsl2jZ.

Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., and Clark, P. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= S37hOerQLB.

Mita, M., Kiyono, S., Kaneko, M., Suzuki, J., and Inui, K. A self-refinement strategy for noise reduction in grammatical error correction. In Cohn, T., He, Y., and Liu, Y. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 267-280, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.26. URL https://aclanthology.org/2020.findin gs-emnlp. 26.

Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A. F., Ippolito, D., Choquette-Choo, C. A., Wallace, E., Tramèr, F., and Lee, K. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023.

Ngo, R., Chan, L., and Mindermann, S. The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626, 2022.

OpenAI. Moderation API. https://platform.ope nai.com/docs/guides/moderation/overv iew, 2023a.

OpenAI. Introducing Superalignment. https://openai .com/blog/introducing-superalignment, $2023 b$.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.

Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net /forum?id=HPuSIXJaa9.

Rashkin, H., Smith, E. M., Li, M., and Boureau, Y.L. Towards empathetic open-domain conversation models: A new benchmark and dataset. In Korhonen, A., Traum, D., and Màrquez, L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5370-5381, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1534. URL https://aclantho logy.org/P19-1534.

Reid, M. and Neubig, G. Learning to model editing processes. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 3822-3832, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: $10.18653 / \mathrm{v} 1 / 2022$.findi ngs-emnlp.280. URL https://aclanthology.o rg/2022.findings-emnlp.280.

Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

Swamy, G., Dann, C., Kidambi, R., Wu, Z. S., and Agarwal, A. A minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024.

Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model, 2023.

Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Vernikos, G., Bražinskas, A., Adamek, J., Mallinson, J., Severyn, A., and Malmi, E. Small language models improve giants by rewriting their outputs. arXiv preprint arXiv:2305.13514, 2023.
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https: //openreview.net/forum?id=yzkSU5zdwD. Survey Certification.

Wu, Z., Hu, Y., Shi, W., Dziri, N., Suhr, A., Ammanabrolu, P., Smith, N. A., Ostendorf, M., and Hajishirzi, H. Finegrained human feedback gives better rewards for language model training. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=CSbGXyCswu.

Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan, D., Wang, D., Yan, D., et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023a.

Yang, X., Wang, X., Zhang, Q., Petzold, L., Wang, W. Y., Zhao, X., and Lin, D. Shadow alignment: The ease of subverting safely-aligned language models. arXiv preprint arXiv:2310.02949, 2023b.

Yang, Z., Wang, J., Li, L., Lin, K., Lin, C.-C., Liu, Z., and Wang, L. Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation. arXiv preprint arXiv:2310.08541, 2023c.

Yao, Z., Aminabadi, R. Y., Ruwase, O., Rajbhandari, S., Wu, X., Awan, A. A., Rasley, J., Zhang, M., Li, C., Holmes, C., et al. Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales. arXiv preprint arXiv:2308.01320, 2023.

Yuan, H., Yuan, Z., Tan, C., Wang, W., Huang, S., and Huang, F. RRHF: Rank responses to align language models with human feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=EdIG MCHk 41.

Zhang, Y., Ye, Z., Feng, Y., Zhao, D., and Yan, R. A constrained sequence-to-sequence neural model for sentence simplification. arXiv preprint arXiv:1704.02312, 2017.

Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.

Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.

