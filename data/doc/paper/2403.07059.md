# Better than classical? The subtle art of benchmarking quantum machine learning models 

Joseph Bowles, ${ }^{1, *}$ Shahnawaz Ahmed, ${ }^{1,2, \dagger}$ and Maria Schuld ${ }^{1, \ddagger}$<br>${ }^{1}$ Xanadu, Toronto, ON, M5G 2C8, Canada<br>${ }^{2}$ Chalmers University of Technology

(Dated: March 15, 2024)


#### Abstract

Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that "quantumness" may not be the crucial ingredient for the small learning tasks considered here. Our benchmarks also unlock investigations beyond simplistic leaderboard comparisons, and we identify five important questions for quantum model design that follow from our results.


Much has been written about the "potential" of quantum machine learning, a discipline that asks how quantum computers fundamentally change what we can learn from data $[1,2]$. While we have no means of running quantum algorithms on noise-free hardware yet, there are only a limited number of tools available to assess this potential. Besides proving advantages for artificial problem settings on paper, certain ideas - most prominently, variational models designed for near-term quantum technologies - can be tested in classical simulations using small datasets. Such benchmarks have in fact become a standard practice in the quantum machine learning literature and are found in almost every paper.

A taste for the results derived from small-scale benchmarks can be obtained through a simple literature review exercise. Out of 55 relevant papers published on the preprint server arXiv ${ }^{1}$ until December 2023 that contain the terms "quantum machine learning" and "outperform" in title or abstract, one finds that about $40 \%$ claim that a quantum model outperforms a classical model, while about $50 \%$ claim that some improvement to a quantum machine learning method outperforms the original one (such as optimisers $[3,4]$, pre-training strategies [5] or symmetry-aware ansatze $[6,7]$ ). Only 3 papers or $4 \%$ find that a quantum model does not outperform a classi-[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-01.jpg?height=537&width=702&top_left_y=1022&top_left_x=1167)

FIG. 1. The scope of the benchmark study at a glance.

cal one $[8-10]$; two of these are quick to mention that this is "due to the noise level in the available quantum hardware" [8] or that "the proposed methods are ready for [...] quantum computing devices which have the potential to outperform classical systems in the near future" [10]. Only one paper [9] draws critical conclusions from their empirical results. If we assume that this literature review is representative ${ }^{2}$, then the overwhelming signal is that quantum machine learning algorithm design is progressing rapidly on all fronts, with ample evidence from small-scale experiments that it is already beating classical machine learning in generic domains. But can we trust this picture when judging the potential of current ideas?[^1]

In search for an evidence-based evaluation of proposals in near-term quantum machine learning algorithm design, this paper conducts a large-scale benchmark study that systematically tests popular quantum machine learning models on classification tasks. The code, built on the PennyLane software framework [11], is made available under https://github.com/XanaduAI/ qml-benchmarks, and the datasets can also be found under https://pennylane.ai/datasets/. We designed the study with a strong emphasis on scientific best practices that aim at reducing a positivity (or negativity) bias. To achieve this, we selected 12 prototypical, influential quantum machine learning models ranging from socalled quantum neural networks $[12,13]$ to convolutional quantum neural networks [14, 15] and quantum kernel methods $[16,17]$ through a randomised procedure and implemented them as faithfully as possible.

The 6 data generation procedures for the classification tasks are chosen based on principles such as structural diversity, comparability, control of important variables, and theoretical relevance. They generate 160 individual datasets grouped into 10 benchmarks, where each benchmark varies either the dimension or a parameter controlling the difficulty of the learning problem that the models have to solve. Note that not all models are tested on all datasets and benchmarks; some, like translationinvariant tasks, were designed for specific models, while others required too many computational resources for the extensive hyperparameter optimisation performed in this paper. ${ }^{3}$

We compare the quantum models to likewise prototypical and influential classical machine learning models like Support Vector Machines and simple Neural Networks (i.e., not "state-of-the-art", but rather "out-of-the-box" models for small datasets). The goal is to find signals across the experiments that are consistent and can give us clues about which research questions are worth investigating further.

Our results show that - contrary to the picture emerging from the literature sample above - the prototypical classical baselines perform systematically better than the prototypical quantum models on the small-scale datasets chosen here. Furthermore, for most models there are no significant drops in performance if we use comparable quantum models that do not use entanglement and are classically simulable at scale, suggesting that "quantumness" may not be a defining factor.

With small variations, the overall rankings between models are surprisingly consistent throughout the different benchmarks and allow some more nuanced observations. For example, hybrid quantum-classical models -[^2]

such as models that use quantum circuits inside neural networks or a support vector machine - perform similarly to their purely classical "hosts", suggesting that the quantum components play a similar role to the classical ones they replace. While a layer-wise, trainable encoding with trainable input feature scaling ("data-reuploading" [18]) shows some promising behaviour, models based on so-called "amplitude encoding" [13, 19, 20] struggle with the classification tasks, even if given copies of inputs. Interestingly, almost all quantum models perform particularly badly on the benchmarks we deemed simplest, a linearly separable dataset.

Although the quantum models we tested failed to provide compelling results, we are not necessarily advocating for a research program that attempts to optimise them for the datasets in this work. Rather, the poor performance relative to baseline classical models across a range of tasks should bolster a hard-to-swallow fact about current quantum machine learning research: namely, that the inductive bias of near-term quantum models, the added benefit of "quantumness" as well as the problems for which both are useful, are still poorly understood $[9,21-23]$.

We finally note that while independent benchmark "meta-studies" like ours are still rare, an increasing number of papers aim at systematically studying aspects of quantum model design. For example, Kashif et al. [5] look at the role of parameter initialisation for trainability, and [24] provide a software framework to test generative models. Moussa et al. [25] investigate the role of hyperparameters on the generalisation performance, and some of their findings are confirmed by our results.

The remainder of the paper will discuss common benchmarking pitfalls (Section I), the model (Section II) and data (Section III) selection method used in this study, and insights from hyperparameter optimisation (Section IV). Our main results are presented in Section V and Section VI discusses questions that follow for the design of quantum models. The conclusion (Section VII) will reflect on important lessons learnt in this study, which can hopefully contribute to more robust and critical benchmarking practices for quantum machine learning.

## I. WHAT CAN WE LEARN FROM BENCHMARKS?

Eagerly following its parent discipline of classical machine learning, benchmarks in the quantum machine learning literature are commonly motivated by statements like "to demonstrate the performance of our quantum model we test it on the XYZ dataset". But how much can benchmarks give us an insight into the quality of a model or idea? Before entering into the technical details we want to discuss this question critically as a cautionary tale that informs our own benchmark study.

## A. The need for scientific rigour

In classical machine learning, increasing doubts about the singular reliance on performance metrics have emerged in recent years $[26-28]^{4}$. Concerns are supported by a range of introspective studies that show how benchmarking results are subject to high variance when seemingly small experimental design decisions are changed.

Firstly, the dataset selection has a significant impact on the comparative performance of one model over others. This is epitomised by no-free-lunch theorems [29] suggesting that for a large enough set of problems, the average performance of any model is the same, and we can only hope for a good performance on a relevant subset (automatically paying the price of a bad performance on another subset). ${ }^{5}$ An illustrative, even if highly simplified, example is shown in Figure 2, where a minor rearrangement of the data in a classification task causes an admittedly adversarially hand-crafted - quantum model to switch from being nearly unable to learn, to gaining perfect accuracy.

In more serious examples, studies like Dehghani et al. [26] in their aptly named paper "The benchmark lottery", show for a range of hotly contested benchmarks that significantly different leaderboard rankings are obtained when excluding a few datasets from benchmarking suites or making changes to the way that scores are aggregated. Northcutt et al. [30] find that correcting labeling errors in the ubiquitous ImageNet validation set likewise changes the ranking of models, and Recht et al. [31, 32] demonstrate that using a different test-train split in popular image recognition datasets decreases the accuracies of models by as much as $14 \%^{6}$. While "every benchmark makes a statement about what it perceives to be important" [26], it can be shown that which benchmarks are deemed relevant is largely influenced by trends in research [26,33-35], and therefore by the social makeup of a scientific community rather than methodological considerations.

Secondly, even if the data is selected carefully, variations in the remainder of the study design can hugely influence the results of benchmarking, leading to a host of contradicting results when judging the performance of a new method. For example, large-scale comparative studies suggest that small adjustments to transformer architectures advertised in the literature have no measurable effect on their performance [36], and that contrary to a common belief, standard convolutional neural networks[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-03.jpg?height=261&width=377&top_left_y=192&top_left_x=1123)

test accuracy $=0.53$

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-03.jpg?height=282&width=393&top_left_y=190&top_left_x=1535)

FIG. 2. Illustrative example showing the effect of slight variations in a dataset on model performance. The same quantum model is trained on two different datasets to predict two classes, red and blue. The decision regions are displayed as the shaded areas. Depending on a small variation of the classification task the same model can perform poorly (left) or have a perfect test score (right). We used a "vanilla" quantum neural network model with two layers of angle embedding interspersed with CNOT entanglers, and three layers of a trainable variational circuit, followed by a $Z$-measurement on the first qubit. The classifier is trained on the points with round markers and tested on the points marked as triangles.

still beat transformers when trained on a comparable amount of data [37]. Similarly, extensive hyperparameter optimisation can give baseline models of generative adversarial networks [38], deep reinforcement learning [39] and Bayesian deep neural nets [40] the performance of more advanced proposals. These findings question the large effort invested into optimising model design. And not only are there variations in the model and data, but the choice of scoring metric like accuracy, F-score and Area-Under-Curve make different assumptions on what is deemed important and may correlate poorly with each other [41]. Lastly, Dodge et al. [42] demonstrate how integrating computational costs into performance considerations for Natural Language Processing models highlight regimes where a method as basic as logistic regression is superior to deep learning.

A third danger of benchmarks that judge the "power" of a model over another is a systematic positivity bias, since the goal of designing models that are better than existing ones creates an incentive to publish positive results. This can be illustrated in a simple thought experiment (see Figure 3): Consider 100 researchers, each investigating 20 different types of quantum models before settling on a promising design and publishing benchmarks of the best one against a classical model of their choice. Let us assume there is some kind of ground truth that the performance of quantum and classical machine learning models is normally distributed, and on average classical models perform better and more consistently (say, with a mean of 0.65 and standard deviation of 0.07 ) than quantum ones (mean 0.55, standard deviation 0.1). But the bias created from discarding 19 models means that the researchers will overall find that quantum models are better than classical models; when running a simple simulation we find that the published mean performance of the quantum models is 0.74 vs. 0.65 of classical ones: the scales are flipped! Since leaderboard-driven research ac-

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-04.jpg?height=290&width=854&top_left_y=191&top_left_x=169)

FIG. 3. Numerical illustration of the thought experiment on a positivity bias. Assume that the "true" performance of classical and quantum models is distributed normally (blue and red curves) with the mean for classical model performance higher than in the quantum case. The dashed lines report numerical calculations of the mean of model performance if 100 researchers report only on the top-performing candidate out of 20 quantum models, but do not select the best classical model in a similar manner. The bias from discarding the 19 worst-performing quantum models reverses the observed average performance with respect to the true one.

tively searches for good models, a positivity bias of this nature is not a question of ethical misconduct, but built into the research methodology.

Together, these arguments make a convincing case that "perfomance" or "power" of a model cannot be viewed as a property that can be "demonstrated" by an empirical study, but as a highly context-dependent signal that has to be artfully coaxed out of a system with many interacting parts - not unlike the challenge of an experimental physicist trying to make sense of a complex physical system.

Luckily, we have a tried-and-tested weapon for this challenging task: scientific rigour. For example, benchmarks can become a powerful tool when asking questions of a clearly defined scope. "Does my quantum model beat classical machine learning?" is impossible to answer by a numerical experiment, while "Does the performance of a model on a specific task increase as measured by some metric, if we change the data, or training conditions?" might be. A carefully selected task for such a question, for example an artificial dataset allowing systematic control of the properties of interest, allow much clearer interpretations of the results. Testing when and how a model can be made to perform poorly will give other researchers clues about mechanisms at play, rather than advertising one method over another. And lastly, searching for patterns that can be reproduced across widely different settings will constitute a signal that allows the field to take objective decisions on which ideas show promise. In this study we aim to be explicitly mindful of these pitfalls in the process of model and data selection, experimental design and interpretation of the results.

## B. Should we use MNIST for quantum machine learning?

In order to illustrate some specific problems in quantum machine learning research with its peculiar challenges, let us get back to the question of data selection once more. Unless a quantum researcher focuses on their favourite domain, such as physics itself, the choice of dataset tends to be directly adopted from the classical machine learning literature. But without error-free hardware to run algorithms on, quantum machine learning research finds itself in a very different situation: data is either borrowed from machine learning some decades in the past, or needs to be downscaled and simplified in order to suit the scale of simulation capabilities or noisy small-scale hardware. We want to demonstrate some issues arising from this predicament with a discussion of the (in)famous MNIST handwritten digits datasets which is used widely in quantum machine learning studies. Note that somewhat contradictory, we will still add MNIST as a task here in order to make our results easier to compare with so many other studies.

The MNIST dataset consists of $60.00028 \times 28$ images of handwritten digits [43] and played a crucial historical role in the benchmarking of classical machine learning. While largely superseded by data like ImageNet [44] and CIFAIR [45], it is still occasionally used as a sanity check of new models, as this quote by Geoffrey Hinton, one of the pioneers of deep learning, demonstrates:

MNIST has been well studied and the performance of simple neural networks trained with backpropagation is well known. This makes MNIST very convenient for testing out new learning algorithms to see if they actually work. [46]

Hinton goes on to summarise the baselines for a reasonable success on MNIST:

Sensibly-engineered convolutional neural nets with a few hidden layers typically get about $0.6 \%$ test error. In the "permutationinvariant" version of the task, the neural net is not given any information about the spatial layout of the pixels [and] feed-forward neural networks with a few fully connected hidden layers of Rectified Linear Units (ReLUs) typically get about $1.4 \%$ test error. This can be reduced to around $1.1 \%$ test error using a variety of regularizers [...] [46]

While this may suggests MNIST as a useful benchmark for innovation in quantum machine learning, there is a serious caveat to consider: The typical input size that can be handled with simulations (not to mention hardware) is of the order of tens of features, but downscaling the 784 original pixels by that much voids our extensive knowledge about the baseline to beat.

For example, a representative analysis of 15 randomly selected papers in quantum machine learning using MNIST benchmarks ${ }^{7}$ shows that more than half use a pre-processing strategy like resolution reduction or PCA to lower the number of input features, while most others investigate hybrid models where the dimensionality reduction is implicitly achieved by a classical layer such as a convolutional or tensor network. ${ }^{8}$ Preprocessing the data changes the hardness and nature of the learning problem and the meaningfulness of using MNIST has to be reassessed. For example, the 2-dimensional PCA-reduced MNIST 3-5 classification problem consists of about 10,000 datapoints that lie in overlapping blobs (see Figure 5 further below), a task that is hardly meaningful. Furthermore, as Bausch [47] remarks, reducing images to a $4 \times 4$ resolution leads to $70 \%$ of test images also being present during training. And of course, using powerful classical layers to read in the data makes it hard to judge the impact of the quantum model.

There are other issues besides pre-processing. For example, only four of the papers in our representative sample of 15 use the original MNIST multi-class problem, while all others distinguish between two of the 10 digits - which tends to make the problem easier. Distinguishing between $0-1$, for instance, can be achieved by a linear classifier to almost $100 \%$ accuracy. Possibly blinded by the size of the original dataset, not all authors seem to be aware that their quantum model scores highly on an almost trivial task.

It is no surprise that the overall results - which in the sample of 15 papers range from accuracies of $70-99.6 \%$ are mixed and hard to interpret with respect to the known baseline. Collectively, they certainly do not give convincing evidence for quantum models to systematically reach the test accuracies of $98.6 \%$ or even $99.4 \%$ put forward by Hinton with respect to the original MNIST task - at least not if unaided by powerful classical models.

Overall, the arguments summarised here put into question the practice of adopting datasets from classical machine learning blindly when their benefits are lost. Alternatives, such as 1-d MNIST [48] designed to scale down deep learning experiments, may be a lot more suitable.

With the methodological concerns sufficiently emphasised, we now proceed to introducing the technical details and decisions taken in this work.[^4]

## II. MODELS TO BE BENCHMARKED

## A. Selection methodology and bias

The goal of our paper selection procedure was to identify a diverse set of influential ideas in quantum machine learning that are implementable on current-day standard simulators. Importantly, we are not claiming to capture the "best" or most advanced algorithmic designs, but rather those that are the foundation of models proposed today.

After attempting other strategies that failed to produce a representative sample we settled on the following step-by-step selection method:

1. Thematic match: We first collected all papers accessible via the arXiv API in the period 1 January 2018 to 1 June 2023 in the "quant-ph" category with keywords "classif*", "learn*", "supervised", "MNIST". By this we intended to find the overwhelming share of papers that are part of the current quantum machine learning discourse. [3500 papers]
2. Influential: From the previous set we sub-selected papers with 30 or more Google Scholar citations on the cutoff date 2 June 2023. This was intended to bias the selection towards ideas that stood the test of time and are used widely in quantum machine learning research. It inevitably also biases towards older papers (see Figure 4). [561 papers]
3. Suitable for benchmarks: From reading the abstract, we sub-selected papers that propose a new quantum model for classification on conventional classical data which can be numerically tested by a standard qubit-based simulator. This ensured that the models can be meaningfully compared via a focused set of benchmarks, and that we limit ourselves to original proposals. [29 papers]
4. Implementable: To achieve a reasonable workload, we sampled a random subset of 15 papers from the 29 candidates. After reading the full body of these papers we had to exclude another four that we could not reasonably implement, either because the data encoding was too expensive [12, 49], or because the paper did not specify important parts of our classification pipeline like training [50] or an embedding strategy for classical datasets [14]. [11 papers]

From this final selection of 11 papers $[13,15,18-20,51-$ 56] we implemented 12 quantum machine learning models. Havlíček et al. [52] compared two distinct architectures, both of which we implemented.

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-06.jpg?height=406&width=700&top_left_y=182&top_left_x=257)

FIG. 4. Publishing date versus citations of the eleven selected papers (red crosses) from an initial set of 3500 papers drawn from the ArXiv API (gray dots). Outliers with over 1500 citations are not shown. Selecting papers with 30 or more citations introduces a bias towards less recent work.

## B. Summary of the models

The 12 models we implemented can be split into three families: quantum neural networks, quantum kernel methods and quantum convolutional neural networks. Here we explain the principle of each family and give a brief overview of the central idea of each model. Technical terms commonly known within the quantum machine learning community, but not necessarily clear to every reader, are highlighted in orange italic font and explained in a glossary at the end of this paper (see Appendix B). More detailed descriptions of the models can be found in Appendix C, and Table I provides the basic facts at a glance. We use camel case to refer to models, and the names correspond to the classes used in the software package.

## 1. Quantum neural network models

So-called "quantum neural networks" are - somewhat misleadingly named - simply variational quantum circuits that encode data and are trainable by gradientdescent methods. Variational quantum circuits have been extensively studied in the quantum machine learning literature and take the form

$$
\begin{equation*}
f(\boldsymbol{\theta}, \boldsymbol{x})=\operatorname{tr}[O(\boldsymbol{x}, \boldsymbol{\theta}) \rho(\boldsymbol{x}, \boldsymbol{\theta})] \tag{1}
\end{equation*}
$$

where $\rho$ is a density matrix, $O$ is an observable, and both may depend on an input data point $\boldsymbol{x}$ and trainable parameters $\boldsymbol{\theta}$. For our purposes, a quantum neural network model is one that combines one or more such variational quantum circuits $\left\{f_{1}, \ldots, f_{L}\right\}$ to classify data into one of two labels $y= \pm 1$ through a class prediction function $f_{\text {pred }}:$

$$
\begin{equation*}
y=f_{\text {pred }}\left(f_{1}\left(\boldsymbol{\theta}_{1}, \boldsymbol{x}\right), \cdots, f_{L}\left(\boldsymbol{\theta}_{L}, \boldsymbol{x}\right)\right) \tag{2}
\end{equation*}
$$

In the simplest case, where the model uses a single quantum function $(L=1)$ and an observable $O(\boldsymbol{x}, \boldsymbol{\theta})$ with eigenvalues $\pm 1, f_{\text {pred }}$ is typically the sign function, so that the sign of the measured observable indicates the predicted class.

To train these models, one defines a differentiable cost function

$$
\begin{equation*}
\mathcal{L}(\boldsymbol{\theta}, \mathbf{X}, \boldsymbol{y})=\frac{1}{N} \sum_{i} \ell\left(\boldsymbol{\theta}, \boldsymbol{x}_{i}, y_{i}\right) \tag{3}
\end{equation*}
$$

where the loss $\ell\left(\boldsymbol{\theta}, \boldsymbol{x}_{i}, y_{i}\right)$ measures the model performance for a specific training data point $\boldsymbol{x}_{i}$ whose true label $y_{i}$ we know, and $\mathbf{X}, \boldsymbol{y}$ summarise all training inputs and labels into a matrix/vector. Like neural networks, the loss can be numerically minimised in an endto-end fashion via stochastic gradient descent by application of the chain rule, where so-called parameter-shift rules $[57,58]$ allow us to evaluate gradients of quantum circuit parameters on hardware.

We study six quantum neural network classifiers:

- CircuitCentricClassifier [13]: A generic quantum neural network model in which copies of amplitude embedded data are followed by a trainable unitary and measurement of a single-qubit observable.
- DataReuploadingClassifier [18]: A model in which the data is scaled by trainable classical parameters and fed to a quantum circuit via layers of trainable angle embedding. One single-qubit rotation thereby takes three input features at once. The aim of training is to maximise the fidelities of the output qubits to the desired class state (either $|0\rangle$ or $|1\rangle)$.
- DressedQuantumCircuitClassifier [51]: A model that preprocesses the data using a classical neural network, which is fed into a generic quantum circuit via angle embedding. The expectation values at the output are then post-processed by another classical neural network for prediction. Both the classical neural networks and the quantum circuit are trainable.
- IQPVariationalClassifier [52]: A model that encodes the input features via an angle embedding using a circuit structure inspired from Instantaneous Quantum Polynomial (IQP) circuits, which are known to be hard to simulate classically. [59].
- QuantumBoltzmannMachine [53]: A model inspired from classical Boltzmann machines that trains the Hamiltonian of a multi-qubit Gibbs state and measures an observable on a subset of its qubits for prediction.
- QuantumMetricLearner [56]: A model that optimises a trainable embedding of the data to increase the distance of states with different labels. Prediction relies on evaluating state overlaps of a new embedded input with the embedded training data.
- TreeTensorClassifier [19]: A model that uses amplitude embedding followed by a trainable unitary with a tree-like structure that is designed to avoid vanishing gradients.

The choice of loss function for training varies among models ${ }^{9}$. Two models use a cross entropy loss, three use a square loss, one uses a linear loss, and one uses a loss based on distances between embedded quantum states (see Table I).

## 2. Quantum kernel methods

Kernel methods [61, 62] form a well known family of machine learning model that take the form

$$
\begin{equation*}
f(\boldsymbol{x})=\sum_{i} \alpha_{i} k\left(\boldsymbol{x}_{i}, \boldsymbol{x}\right) \tag{4}
\end{equation*}
$$

where $\alpha_{i}$ are real trainable parameters and $k$ is a kernel function: a positive definite function that measures the similarity between data points. Since the values $\alpha_{i}$ typically take the same sign as $y_{i}$, these models have the flavour of a weighted nearest neighbour classifier in which the distance to neighbours is mediated by the kernel function. A fundamental result in the theory of kernel methods states that any such model is equivalent to a linear classifier in a potentially infinite dimensional complex feature space $|\phi(\boldsymbol{x})\rangle$ defined via the inner product

$$
\begin{equation*}
k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\left\langle\phi(\boldsymbol{x}) \mid \phi\left(\boldsymbol{x}^{\prime}\right)\right\rangle \tag{5}
\end{equation*}
$$

and a rich mathematical theory of these methods has been developed as a result. To train such models, one typically seeks a maximum margin classifier of the data, which can be shown to be equivalent to solving a simple convex optimization problem in the parameters $\alpha_{i}$ [61].

A quantum kernel method is one in which the kernel function is evaluated with the aid of a quantum computer. A common strategy is to define an embedding $\rho(\boldsymbol{x})$ of the classical data into quantum states, and use the function

$$
\begin{equation*}
k\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\operatorname{tr}\left[\rho\left(\boldsymbol{x}_{i}\right) \rho\left(\boldsymbol{x}_{j}\right)\right] \tag{6}
\end{equation*}
$$

which is a kernel by virtue of being an inner product, and can be evaluated by methods that evaluate state overlaps. In principle, the kernel function and any of the quantum circuits needed to evaluate it may also depend on trainable parameters, and in some models these are optimized[^5]

as part of a wider training pipeline. For a deeper discussion on the connection between kernel methods and quantum models, we point the reader to [63].

We implemented three quantum kernel methods:

- IQPKernelClassifier [52]: A model that uses a quantum kernel of the form of Eq. (6), where the embedding is the same IQP-inspired angle embedding used in IQPVariationalClassifier.
- ProjectedQuantumKernelClassifier [54]: A model that attempts to avoid problems related to the exponential size of Hilbert space by projecting the embedded state to a smaller space defined via its reduced density matrices, and using a Gaussian kernel in that space. The initial quantum embedding corresponds to a trotterized evolution given by a 1D Heisenberg Hamiltonian.
- QuantumKitchenSinks [55]: A model in which input data is first transformed by random linear maps and then by a number of fixed quantum circuits via angle embedding. Output bit-string samples of the quantum circuits are concatenated to form a feature vector that is fed to a linear classifier for training and prediction.

We loosely include QuantumKitchenSinks in the above list since, as in quantum kernel methods, the linear classifier finds an optimal hyperplane in a feature mapped space given by the quantum model. However, note that the implementation does use a SVM as in the other two models.

## 3. Quantum convolutional neural network models

Our third family consists of models can be seen as analogues of convolutional neural networks [64, 65]: a class of classical neural network model designed for computer vision related tasks which exploit a form of translation symmetry between layers called "equivariance" [66]. The literature features a large number of quantum convolutional models, undoubtedly due to the enormous success of classical convolutional models and the general tendency to 'quantumify' any model that gains sufficient fame in the classical literature. We do not attempt to capture these models in a strict mathematical definition, but rather identify them by the fact that they are examples of quantum neural network models that-like classical convolutional models - also exploit a form of translation symmetry [67], and are therefore designed for data that respects such symmetries.

We study two such models:

- QuanvolutionalNeuralNetwork [15]: This model is equivalent to a classical convolutional neural network in which the convolutional filter that defines the first layer is evaluated by a random quantum circuit that encodes the input data via angle encoding.
- WeiNet [20]: Uses a quantum version of a convolutional layer based on amplitude embedding and linear combination of unitaries, with the goal of having fewer trainable parameters than a classical convolutional neural network.


## 4. Classical models

In addition to the above quantum models, we use a set of standard classical models, which we define as algorithms that are classically simulable at scale (even if they might be quantum-inspired).

Typical strategies for selecting a baseline to compare quantum models with try to match architectural components, like the number of parameters or layers in the model. However, the role that these components play and the effect they have on the computational resources differ vastly between architectures, and we do not believe that this comparison is meaningful in our context - much like one does not enforce the same number of parameters when comparing kernel methods with neural networks.

Instead, we employ two selection criteria for the classical competitors. On the one hand we use a standard feed-forward neural network, a support vector classifier with Gaussian kernel, and a convolutional neural network model as natural equivalents to the three quantum model families defined above. The first two of these were implemented using scikit-learn's MLPClassifier and SVC classes, and the third that we call ConvolutionalNeuralNetwork was implemented using Flax [68]. Similarly to the quantum model selection, these are out-of-the-box versions of models that represent popular ideas in machine learning research, and that are widely used by practitioners for small-scale datasets. They are not intended to be state-of-the-art models.

We also conduct experiments with models that are classically simulable but inspired by quantum models. For example, the SeparableVariationalClassifier represents a standard quantum neural network with layer-wise, trainable angle embedding but uses no entangling gates or non-product measurements. The SeparableKernelClassifier is a support vector machine with a quantum kernel that embeds data using (non-trainable) layers of non-entangling angle embedding. The quantum circuits of these models can be simulated by circuits consisting of a single qubit only.

## C. Implementation

We develop a software pipeline built from PennyLane [11], JAX [69], optax [70] and scikit-learn [71] as our software framework. While PennyLane's differentiable state-vector simulators in combination with JAX's justin-time compilation tools allow us to run trainable quantum circuits, scikit-learn provides a simple API for model training and evaluation, as well as a wealth of machine learning functionalities like data pre-processing, crossvalidation and performance metrics. It also offers a broad range of standard classical machine learning models to compare with. The code can be found in the repository https://github.com/XanaduAI/qml-benchmarks.

When implementing the models from the selected papers we followed these principles:

1. Faithful implementation: We carefully deduced the model design and training procedure from the paper. If specific details needed for implementation were missing in the paper, they were defined based on what appeared natural given our own judgement (see also Appendix C).
2. Convergence criteria: All quantum neural network models and quantum convolutional models used the same convergence criterion, which was based on tracking the variance of the loss values over time and stopping when the mean loss value did not change significantly over 200 parameter updates ${ }^{10}$ (see Appendix D for details). Loss histories were routinely inspected by eye to ensure the criterion was working as desired. Quantum kernel methods followed the default convergence criterion of the specific scikit-learn model class (SVC, Logistic Regression) that they employ. Pure scikit-learn baseline models (MLPClassifier, SVC) used their default convergence criterion except for the max_iter parameter of MLPClassifier which was increased to 3000 . If a training run did not converge during grid search, which happened very rarely, that particular result was ignored in the hyperparameter optimisation procedure.
3. Batches in SGD: Since the batch size for models using gradient descent training plays an important role in the runtime and memory resources, we did not optimize this hyperparameter with respect to the performance of the model, but with respect to simulation resources. Note that in an unrelated study, Moussa et al. [25] found that the batch size did not have a significant impact on model performance. All models use a batch size of 32 , except for the computationally expensive QuantumMetricLearner that uses a batch size of 16 .
4. Data preprocessing: Not all models define a data preprocessing strategy, even though most data em-[^6]

| Model | Embedding | Measurement | Hyperparameters | Classical processing | Loss |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Quantum Neural Networks |  |  |  |  |  |
| Circuit Centric <br> Classifier | copies of ampli- <br> tude embedding | single-qubit Pauli <br> $Z$ | - learning_rate <br> - n_input_copies <br> - n_layers | trainable bias added to <br> output of circuit | square |
| Data Reupload- <br> ing Classifier | layers of train- <br> able angle <br> embedding | multi-qubit Pauli <br> $\mathrm{Z}$ | - learning_rate <br> - n_layers <br> - observable_type | input features and <br> output fidelities mul- <br> tiplied by trainable <br> weights | square |
| Dressed Quan- <br> tum Circuit Clas- <br> sifier | layers of angle <br> embedding | multi-qubit Pauli <br> $\mathrm{Z}$ | - learning_rate <br> - n_layers | input and output fea- <br> tures processed by <br> trainable classical neu- <br> ral network | cross en- <br> tropy <br> (softmax) |
| IQP Variational <br> Classifier | layers of IQP- <br> inspired angle <br> embedding | two-qubit $\mathrm{ZZ}$ | - learning_rate <br> - n_layers <br> - repeats | input extended by <br> product of features | linear |
| Quantum Boltz- <br> mann Machine | angle embedding | multi-qubit Pauli <br> $\mathrm{Z}$ | - learning_rate <br> - temperature <br> - visible_qubits | input features mul- <br> tiplied by trainable <br> weights | cross <br> entropy |
| Quantum Metric <br> Learner | layers of QAOA- <br> inspired angle <br> embedding | pairwise state <br> overlaps | - learning_rate <br> - n_layers | None | distance <br> between <br> embedded <br> classes |
| Tree Tensor <br> Classifier | amplitude <br> embedding | single-qubit Pauli <br> $Z$ | - learning_rate | trainable bias added to <br> output of circuit | square |
| Quantum Kernel Methods |  |  |  |  |  |
| IQP Kernel <br> Classifier | layers of angle <br> embedding | pairwise state <br> overlaps | - repeats <br> - $C$ (SVM regularisation) | quantum kernel used <br> in SVM | hinge |
| Projected Quan- <br> tum Kernel | layers of <br> Hamiltonian- <br> inspired angle <br> embedding | $\mathrm{X}, \mathrm{Y}, \mathrm{Z}$ on all <br> qubits | - trotter_steps <br> - $C$ (SVM regularisation) <br> - $t$ (evolution time) <br> - gamma_factor (RBF <br> bandwidth) | quantum kernel used <br> in SVM | hinge |
| Quantum <br> Kitchen Sinks | angle embedding | computational <br> basis samples | - n_episodes <br> - n_qfearures | quantum features used <br> in logistic regression | cross <br> entropy |
| Quantum Convolutional Neural Networks |  |  |  |  |  |
| Quanvolutional <br> Neural Network | angle embedding | computational <br> basis samples | - learning_rate <br> - n_qchannels <br> - qkernel_shape <br> - kernel_shape | classical convolutional <br> neural network | cross en- <br> tropy <br> (sigmoid) |
| Wei Net | amplitude <br> embedding | single- and <br> double-qubit Z | - learning_rate <br> - filter_type | single layer neural net- <br> work applied to the <br> circuit output values | cross en- <br> tropy <br> (sigmoid) |

TABLE I. Overview of models used in the benchmarks. For definitions of the terms, consult the glossary in Appendix B. More details on the models are found in Appendix C.
beddings are sensitive to the range on which the input data is confined. If no preprocessing was specified, we pre-scaled to natural intervals; for example, if angle embedding is used all features were scaled to lie in $\left[-\frac{\pi}{2}, \frac{\pi}{2}\right]$ (also consistent with findings in [25]). For models that use amplitude embedding, we set the first $d$ values of the state vector equal to $\boldsymbol{x}$, pad the remaining values with a constant $1 / 2^{n}$ (with $n$ the number of qubits), then normalize all values as required.

## D. Difficulties of scaling to higher qubit numbers

As quantum software gets better, simulations in the two-digit qubit numbers have become a standard for benchmark studies. However, quantum machine learning benchmarks pose particular challenges to numerical computations, as a circuit is not run once, but hundreds or thousands of times to compute one test accuracy during hyperparameter optimisation using the complex workflows quantum models are constructed of. Although hyperparameter search was parallelized such that each 5fold cross validation run (i.e. training a model 5 times for fixed a hyperparameter setting) was sent to a cluster of 10 CPUs with a maximum compute time of 24 hours, we nevertheless ran into runtime limits for even modest qubit numbers.

There were different causes for models to be computationally demanding. For example, ProjectedQuantumKernel has a large hyperparameter grid which consists of 108 distinct hyperparameter settings. Since we perform 5 -fold cross validation, we therefore need to train 540 models for every dataset we consider, and training each model involves evaluating hundreds of quantum circuits. For other models the number of circuits required poses the biggest issue. For example, to train on $N=250$ datapoints we need to evaluate a quadratic number of circuits - around 30,000 - for IQPKernelClassifier. QuantumMetricLearner, instead, requires multiple circuits for prediction, as it computes the distance of a new data sample to training samples. The QuanvolutionalNeuralNetwork model involves performing a convolution over the input image, and needs to evaluate a number of circuits that scales with the number of pixels, which in case of a $16 \times 16$ pixel grid amounts to many millions of circuit evaluations. Another costly example is the QuantumBoltzmannMachine which is based on simulating a density matrix that requires quadratically more memory than state vector simulation. For large qubit numbers, some quantum neural network models were also very slow to reach convergence. This was particularly the case for DressedQuantumCircuit, which for a 15 qubit circuit failed to converge after 24 hours of training on some of the datasets. As a result of these challenges, computing a quantum model's test accuracy on a single dataset can already take over a day on a cluster for single-digit qubit numbers, and reaching of the order of 20 qubits becomes unfeasible for the scope of our study.

There are many mitigation strategies that can speed up simulations, such as a more judicious choice of parameter initialization, more resources on the cluster, better runtime-versus-memory trade-off choices, snapshotting jobs or by turning to GPU simulation such as available in the PennyLane Lightning suite. The variety of computational bottlenecks however requires different solutions to be found for different models, and we therefore have to defer more extensive code optimisation to future work on the benchmark software package.

## III. DATASETS

Choosing meaningful datasets for general benchmarking studies is difficult, and, as discussed in Section I, can have a huge impact on the findings. For example, should we use datasets that suit the inductive bias of the quantum models, since these would be likely future applications? Shall we use small datasets that were relevant for machine learning in the 1990s? Shall we use popular current-day benchmarking tasks and reduce them to manageable scales? Should we focus on data in the form of quantum states [72, 73]? While we do not claim to provide satisfying answers to these questions - an endeavour that is worth a multi-year research programme and will unlikely find a single answer - we want to make transparent the rationale behind choosing the 6 different flavours of data that we employ in this study, and what we expect them to measure.

We followed three overarching principles: Firstly, we predominantly use artificially generated data which allows us to understand and vary the properties and size of the datasets. This may limit conclusions with respect to "real-world" data, but is an essential ability in the early research stage that quantum machine learning is in. Secondly, we aim at maximising the diversity of the datasets by using fundamentally different functional relationships and procedures as generators - in the hope to increase the chance that consistent trends found in this study may be found in other data as well. Thirdly, in the last three out of six data generation procedures introduced below we follow the "manifold hypothesis" [74-76], which states that typical data in modern machine learning effectively lives on low-dimensional manifolds.

With this in mind we define 6 data generation procedures, with which we generate data for 10 benchmarks (in the following named in capital letters). Each benchmark consists in turn of several datasets that differ by varying parameters in the data generation procedure (in most cases the input dimension). Overall, the benchmarks consist of 160 individual datasets. While the 6 data generation procedures and their associated benchmarks are summarised in the following list and illustrated in Figure 5, the precise generation settings can be found
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-11.jpg?height=924&width=1750&top_left_y=188&top_left_x=184)

FIG. 5. Illustrative examples of datasets created by the different data generation procedures. For the scatter plots, the two classes are shown in blue and orange, and training points are shown in round vs. test points in an ' $x$ ' shape. The linearly separable pannel shows data for the Linearly separable benchmark in 2 and 3 dimensions. The left two plots for the MNIST data correspond to $2 \mathrm{~d}$ and 3d MNIST PCA data, and the rightmost image shows examples from the MNIST-CG dataset for $32 \mathrm{x}$ $32,16 \times 16,8 \times 8$ and $4 \times 4$ pixel grids. The hidden manifold examples correspond to a $1 \mathrm{~d}$ (left) and $2 \mathrm{~d}$ (center) and $3 \mathrm{~d}$ (right) manifold embedded into 3 dimensions. The bars and stripes panel shows examples from the BARS \& STRIPES dataset for a $16 \mathrm{x}$ 16 pixel grid. The examples from the TWO CURVES DIFF benchmark show a degree of $2,10,20$ for the Fourier series, embedding the curves into 10 dimensions (of which three are plotted). The hyperplanes pannel shows data from the HYPERPLANES DIFF benchmark, where there are two (left) and five (right) hyperplanes used to decide the class labels.

in Appendix E.

1. Linearly separable. This data generation procedure consists of linearly separable data and serves as the "fruit-fly" example of learning: it is easy to understand and has well-defined performance baselines, since it is known to be solvable by a perceptron model - a neural network with one layer and output "neuron" - since the early days of artificial intelligence research $[77,78]^{11}$. The datasets are generated by sampling inputs uniformly from a $d$-dimensional hypercube and dividing them into two classes by the hyperplane orthogonal to the $(1, \ldots, 1)^{T}$ vector (including a small data-free margin). The benchmark that we will refer to as LINEARLY SEPARABLE consists of 19 datasets that vary in the dimension $d=2, \ldots, 20$ of the input space.
2. Bars and stripes. As a second "fruit-fly" task, but this time tailor-made for the convolutional models,[^7]

we create images of noisy bars and stripes on 2dimensional pixel grids. These datasets are among the simplest examples of translation invariant data and can thus be used as a sanity check of convolutional models. The data generation procedure involves sampling a number of images with values $\pm 1$, corresponding to either bars or stripes, and adding independent Gaussian noise to each pixel value with standard deviation 0.5. The BARS \& STRIPES benchmark consists of four datasets where we vary the image size between $4 \times 4,8 \times 8,16 \times 16$ and $32 \times 32$.

3. Downscaled MNIST. While we cautioned against the use of downsized MNIST datasets in Section IB, we want to report on this ubiquitous dataset here for the sake of comparability with other studies. We define three benchmarks: For the quantum neural network models we use Principal Component Analysis (PCA) to reduce the dimensions to $d=2, \ldots, 20$ (MNIST PCA). For quantum kernel methods, which need to simulate up to $N(N-1) / 2$ quantum circuits during training if $N$ is the number of training samples, 250 training and
test points are subsampled from the MNIST PCA datasets (MNIST PCA ${ }^{-}$). For the CNN architectures we reduce the resolution of the images by "coarsegraining" or extending the images to size $4 \times 4$, $8 \times 8,16 \times 16$, and $32 \times 32$ in order to keep the spatial pattern of the data intact (MNIST-CG). The three benchmarks consist of 42 datasets in total.
4. Hidden manifold model. Goldt et al. [79] introduced this data generation procedure as a means to probe the effect of structure in the data, such as the size of a hidden manifold conjectured to control the difficulty of the problem, on learning. In particular, it allows the analytical computation of average generalisation errors, a property that could be of interest in quantum machine learning beyond this study. We generate inputs on a low-dimensional manifold and label them by a simple neural network initialised at random. The inputs are then projected to the final $d$-dimensional space. We generate two benchmarks this way: HIDDEN MANIFOLD varies only the dimension $d=2, \ldots, 20$ and keeps the dimension of the manifold at $m=6$, while HIDDEN MANIFOLD DIFF keeps the dimension constant at $d=10$ and varies the dimensionality $m$ of the manifold between $m=2, \ldots 20$; in total we produce 38 datasets.
5. Two curves. This data generation procedure is inspired by a theoretical study [80] that proves how the performance of neural networks depends on the curvature and distance of two 1-dimensional curves embedded into a higher-dimensional space. Here we implement their proposal by using lowdegree Fourier series to embed two sets of data sampled from a 1-d interval - one for each class - as curves into $d$ dimensions while adding some Gaussian noise. The curves are embedded using identical functional relationships, except from an offset applied to one of them, which controls their distance. We generate two benchmarks with in total 38 datasets. TWO CURVES fixes the degree to $D=5$, offset to $\Delta=0.1$ and varies the dimension $d=2, . ., 20$. TWO CURVES DIFF fixes the dimension $d=10$ and varies the degree $D$ of the polynomial $D=2, \ldots, 20$ while adapting the offset $\Delta=\frac{1}{2 D}$ between the two curves.
6. Hyperplanes and parity. Finally, we devise a data generation procedure that fixes several hyperplanes in a $k$-dimensional space and labels randomly sampled points consulting the parity of perceptron classifiers that have these hyperplanes as decision boundaries. In other words, a label tells us whether a point lies on the "positive" side of an even number of hyperplanes. The motivation for this data generation procedure is to add a labeling strategy that requires information about the "global" structure of the problem, i.e. the position of all hyperplanes.

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-12.jpg?height=328&width=680&top_left_y=188&top_left_x=1183)

FIG. 6. Average difficulty score with respect to the 22 difficult measures computed by the ECol package ${ }^{a}$ introduced in Lorena et al. [81] for some of the datasets. The datasets ending in "DIFF" (blue, red and green curves) depend on a variable between 2 and 20 that we claim controls their difficulty, which is supported by the quantifier shown here. The other datasets vary the dimension, and - with the curious exception from LINEARLY SEPARABLE - decrease in difficulty when the input space gets larger. Note that the measures exhibit a huge variance, and the results from this or other data complexity measures should be interpreted with care.

${ }^{a}$ https://github.com/lpfgarcia/ECoL

A single benchmark, HYPERPLANES DIFF, fixes the dimension of the data to $d=10$ and varies the number $k$ of hyperplanes $k=2, \ldots, 20$ defined on a 3-dimensional submanifold.

As seen, some of the 10 benchmarks consist of datasets that vary the input dimension where others vary parameters that supposedly control the complexity of the data. While the controversial debate about the best way of quantifying the complexity of data (for example, [82-84]) lies outside of the scope of this paper, we give some support to the claim of an increasing complexity by reporting the average difficulty score of the measures proposed in Lorena et al. [81] extending a seminal paper from 2002 [85] in Figure 6.

## IV. HYPERPARAMETER TUNING

Hyperparameter optimisation is one of the most important steps in classical machine learning to allow models to reveal their true potential. Likewise, quantum machine learning models tend to show a wide variety in performance depending on hyperparameters such as the number of layers in the ansatz. Even seemingly inconspicious hyperparameters such as the learning rate of an optimiser can influence generalisation errors significantly [25]. As mentioned in Section I A, including a wide enough hyperparameter search grid can make baseline models match the performance of state-of-the-art methods.

Hyperparameter optimisation is also one of the most cumbersome aspects of training (quantum) machine learning algorithms since it involves searching a large configuration space that is not amenable to gradient-
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-13.jpg?height=1042&width=828&top_left_y=172&top_left_x=190)

FIG. 7. Ranges of test accuracies of models during grid search to find the best hyperparameters. There is a large variation in the results which becomes very important to consider as poor hyperparameter choices could lead to misleading conclusions about the power of (quantum) machine learning models.

based optimization ${ }^{12}$, while each point or hyperparameter configuration involves several training runs during procedures like cross-validation.

We conduct an extensive hyperparameter search for all models and datasets in all our experiments, using a full grid search algorithm implemented by the scikit-learn package [71] with the default five-fold cross-validation, using the accuracy score ${ }^{13}$ to pick the best model. While there are more sophisticated techniques [89, 90], a full grid search has the advantage of simplicity, and allows us to extract and study unbiased information about the hyperparameter landscapes. We remark that the number of hyperparameters varies significantly from model to model since our aim was to follow the proposals in the original papers. In some cases, we were forced to select

12 Although recently, to address such issues, techniques such as implicit differentiation have been adapted to perform gradientbased optimization of continuous-valued hyperparameters [86, 87], but exploring these techniques exceed the scope of this study.

13 The accuracy can be an overly simplistic measure for some classification tasks and especially with unbalanced data [88]. However, we utilise it here for its clarity in interpretation, and since our datasets are balanced.

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-13.jpg?height=515&width=702&top_left_y=173&top_left_x=1167)

FIG. 8. Correlation between values chosen for a particular hyperparameter and test accuracy during cross-validation. We show aggregated information across all classifiers, datasets and runs during grid search. Note that some hyperparameters only appear in a single model, while others - such as the number of variational layers (n_layers) - are averaged over several models.

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-13.jpg?height=439&width=615&top_left_y=1022&top_left_x=1210)

FIG. 9. Correlation of the number of quantum layers and test set classification accuracy during cross-validation; averaged across classifiers that use this hyperparameter and all datasets in the three benchmarks reported. While intuitively, more layers should lead to better performance when the dimension of the inputs (MNIST PCA/LINEARLY SEPARABLE) or manifold (HIDDEN MANIFOLD DIFF) grows, we see that the results can vary depending on the benchmark.

a subset due to the exponential increase in grid size with the number of hyperparameters. In these cases, the most relevant hyperparameters were selected by analysing preliminary results first on smaller-scale data. We describe the hyperparameter ranges for each model in Appendix C and summarise a few findings of potential interest here.

First, the performance of both classical and quantum models varies significantly across hyperparameter choices: Figure 7 shows the ranges in the test accuracy during grid search for a select few classifiers and benchmarks, some of which lie between almost random guessing and near-perfect classification. This makes hyperparameter tuning crucial: a highly optimised architecture of one model type can easily outperform another model that has been given less resources to select the right configuration.

We also show the correlations (using the Pearson correlation coefficient) between the accuracy and hyperparameter values in Figure 8 to indicate the relative influence each hyperparameter has on the model performance. We find that aggregated over all datasets and models, some hyperparameters have a high correlation with the accuracy, e.g., increasing the number of episodes in the QuantumKitchenSink model seems to improve its performance, while decreasing the simulation time $t$ improves the ProjectedQuantumKernel model. However, the best hyperparameter choice can vary significantly with the dataset: Figure 9 shows three different benchmarks where the correlation between the number of layers of a quantum model and the test accuracy shows very different trends. In case of the MNIST PCA benchmark, increasing the number of layers leads to higher accuracies, while for the LINEARLY SEPARABLE benchmark we observe the opposite effect. Both trends get stronger for higher input dimensions. At the same time, for the HIDDEN MANIFOLD DIFF benchmark the correlation between accuracy and the number of layers is not significant.

These simple insights from the hyperparameter study show that hyperparameter choice can be very nonintuitive, especially as models increase in size. The hyperparameter choices for a small datasets cannot be expected to be optimal for more complicated scenarios. In case of quantum models, hyperparameter exploration becomes computationally expensive even for moderatesized models.

## V. RESULTS

We finally report our findings from the benchmark study. As a reminder, our goal is twofold: We were motivated to independently test the overwhelming evidence that quantum models perform better than classical ones emerging from benchmarks conducted in the literature. However, this only helps to judge where we currently are, not necessarily where the future potential of quantum models lies. A much more important question we are interested in is which ideas in current near-term quantum machine learning model design hold promise and which ones do not - in other words, what research is worthwhile pursuing in order to use quantum computers effectively for learning. As we will see, the benchmark results give us a number of interesting clues that we will discuss in the next Section VI.

## A. Out-of-the-box classical models outperform quantum models

A very clear finding across our experiments was that the out-of-the-box classical models systematically outperform the quantum models. Figure 10 shows the number of different rankings (first, second, and so forth) across all benchmark experiments we ran for the different

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-14.jpg?height=754&width=873&top_left_y=176&top_left_x=1103)

FIG. 10. Number of rankings (blue/first to red/last) across all datasets that a model was tested on, for the three model families. The models were sorted from top to bottom according to the average normalised rank. The three classical out-ofthe-box models perform best. Note that the total number of benchmarks a model competed in varies due to runtime limitations, and since the convolutional architectures were only tested on MNIST CG and BARS \& STRIPES.

models. The models within a family are sorted according to the expected normalised ${ }^{14}$ rank: If a model came first in a benchmark with 10 competitors (normalised ranks $0.1,0.2, \ldots, 1$ ), and fourth in a benchmark with five competitors (normalised ranks $0.2,0.4,0.6,0.8,1$ ), its expected normalised rank is $(0.1+0.8) / 2=0.45$. Note that this is one choice of many, and employing other reasonable ordering and aggregation mechanisms that we tested change the picture slightly, but not significantly: In all three model families, the prototypical classical models came first.

Breaking down the fully aggregated results, one finds that the rankings of models between benchmarks were surprisingly consistent (see Appendix F for a full report). For example, in four out of seven benchmarks used for QNN models, the MLPClassifier ranked first with the DressedQuantumCircuitClassifier coming second, whereas in the remaining three benchmarks the roles between these (conceptually very similar) models were reversed. The DataReuploadingClassifier and QuantumBoltzmannMachine (which was not run on the 10-d DIFF benchmarks) usually share places three and four. In five out of the seven benchmarks, the bottom ranks are taken by the CircuitCentricClassifier and[^8]

TreeTensorClassifier as the worst-performing models - interestingly the only two models based on amplitude encoding.

For the kernel methods, the support vector machine (SVC) does best on 4 out of 7 benchmarks, showing a very similar behaviour to the other two SVM-based classifiers. Consistently worst-performing is QuantumKitchenSinks, a model that uses a quantum circuit mapping to computational basis samples as a random, non-trainable feature map. $^{15}$

The two quantum convolutional neural networks were also outperformed by the vanilla ConvolutionalNeuralNetwork model. Surprisingly, WeiNet failed entirely to learn the BARS \& STRIPES data; a task which we considered easy for a model of its kind.

## B. No systematic overfitting or scaling effects

There are three interesting general observations of effects one might expect but which we did not observe. These are exemplified with the selected detailed benchmark results shown in Figure 11. Firstly, although not all quantum models - in particular the QNN methods - use explicit regularisers, they do not show systematic overfitting of the training data. (Some exceptions can be seen in the HIDDEN MANIFOLD, HIDDEN MANIFOLD DIFF and TWO CURVES DIFF benchmarks shown in Appendix F, for which also the classical models struggle with overfitting.)

Secondly, we do not observe any improvement in performance of the quantum models relative to the classical baselines for increasing problem difficulties. For the difficulty-controlled benchmarks, HYPERPLANES DIFF (Figure 11) and HIDDEN MANIFOLD DIFF (Appendix F), the trends of the quantum models' test accuracies generally follow the trend of the classical model. For the difficulty-controlled benchmark TWO CURVES DIFF (Appendix F), the quantum models perform worse than the corresponding classical method as the difficulty is increased. Interestingly, for the hardest datasets from this benchmark, the classical baseline models achieve high $(>90 \%)$ test accuracy whereas all quantum models appear to struggle. This is somewhat surprising, since the embedded curve that defines the structure of the data is a Fourier series, and one may expect quantum models to have a natural bias for this kind of data [91].

Thirdly, except from the LINEARLY SEPARABLE benchmark we do not observe a significant scaling effect with the size of the dimension; also here quantum models do not get significantly better or worse in performance compared to the classical baseline.

15 It is important to mention how Figure 8 revealed that the number of episodes in the model, which controls the number of feature vectors created from quantum circuit samples, correlated positively with the test accuracy, and allowing for significantly more than 2000 episodes may have boosted this model's performance.

## C. Quantum circuits without entanglement do well

An important question for quantum machine learning benchmarks is how the performance of a model depends on properties that we consider to be "quantum". There are many different definitions of this notion (such as "experiments that produce non-classical statistics"), and without being explicitly stated very often, the definition of "not classically simulatable" dominates the thinking in the quantum machine learning community. An experimental design to probe the question is therefore to replace the quantum model architecture with a circuit that is classically tractable (i.e., it can be simulated at scale) and measure if the performance deteriorates. If not, we have evidence that other properties than "quantumness" are responsible for the performance we see - at least in the small-scale datasets chosen here.

To put "quantumness" to the test in our benchmarks we add the SeparableVariationalClassifier and SeparableKernelClassifier described in Section II to our quiver of non-convolutional models. These are fully disentangled $n$-qubit models that can be divided into $n$ separate single-qubit circuits, and hence easily classically simulated at scale. We do not add a separable convolutional model since the ConvolutionalNeuralNetwork itself can be seen as a special case of one, since it is equivalent to replacing the quantum layer of the QuanvolutionalNeuralNetwork model by a circuit that implements the identity transformation. We already know that this model does not perform worse than the entanglement-using QuantumConvolutionalNeuralNetwork and WeiNet.

Replotting the test accuracies from Figure 11 with the new models added we see in Figure 12 that the non-entangled models do surprisingly well. This can be confirmed by including the separable models in the ranking results from Figure 10 (see Appendix F). One finds that compared to the quantum kernel methods, the SeparableKernelClassifier takes second rank after the SVC. Among the QNNs, the SeparableVariationalClassifier is only consistently beaten by the MLPClassifier, DressedQuantumCircuitClassifier and QuantumBoltzmannMachine, as well as occasionally by DataReuploadingClassifier.

Are these three QNNs better than our disentangled QNN because of their entanglement, or is this due to other design choices? Figure 13 compares the original implementations of these models with variations that remove any entangling gates or measurements from the quantum circuits they use. ${ }^{16}$ The results suggest that the entangling gates do not play a role in the[^9]

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-16.jpg?height=2092&width=1767&top_left_y=201&top_left_x=190)

LINEARLY SEPARABLE
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-16.jpg?height=810&width=844&top_left_y=236&top_left_x=206)

MNIST PCA
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-16.jpg?height=362&width=842&top_left_y=1128&top_left_x=207)

MNIST CG

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-16.jpg?height=355&width=448&top_left_y=1576&top_left_x=207)

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-16.jpg?height=357&width=383&top_left_y=1578&top_left_x=665)
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-16.jpg?height=1722&width=854&top_left_y=213&top_left_x=1080)

- MLPClassifier

$\rightarrow$ CircuitCentricClassifier

-- DataReuploadingClassifier

$\_$DressedQuantumCircuitClassifier

$\rightarrow$ IQPVariationalClassifier

$\rightarrow-$ SVC

- QuantumMetricLearner
- QuantumBoltzmannMachine

--.- ConvolutionalNeuralNetwork

- ProjectedQuantumKerne

$\longrightarrow$ TreeTensorClassifier

—- QuanvolutionalNeuralNetwork

FIG. 11. Selected detailed train and test accuracies for some of the benchmarks. As reflected in the aggregated results in Figure 10, the three classical baseline models usually outperform the quantum models across the quantity that is varied. However, there are nuances: while quantum models perform particularly poorly on the LINEARLY SEPARABLE benchmark, most of them follow the classical model performance closely on the HYPERPLANES DIFF benchmark. The accuracies of the QNN family on MNIST PCA mimic the trend of the classical neural network (MLPClassifier), but with an offset towards lower scores, while most quantum kernel methods perform as well as the SVM (SVC).

LINEARLY SEPARABLE
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-17.jpg?height=782&width=400&top_left_y=233&top_left_x=206)

MNIST PCA

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-17.jpg?height=345&width=401&top_left_y=1096&top_left_x=209)

-     - MLPClassifier

$\sim$ CircuitCentricClassifier

-- DataReuploadingClassifier

$\rightarrow$ DressedQuantumCircuitClassifier $\rightarrow$ IQPVariationalClassifier

$\simeq$ QuantumMetricLearner

-     - QuantumBoltzmannMachine
- TreeTensorClassifier

$\because$ SeparableVariationalClassifier
LINEARLY SEPARABLE

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-17.jpg?height=383&width=402&top_left_y=215&top_left_x=623)

HYPERPLANES DIFF

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-17.jpg?height=357&width=398&top_left_y=659&top_left_x=625)

MNIST PCA-

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-17.jpg?height=350&width=398&top_left_y=1088&top_left_x=625)

FIG. 12. Replotting the test accuracies from Figure 11 while adding SeparableVariationalClassifier and SeparableKernelClassifier. These fully classically simulatable models perform similarly or better than most other quantum models.

top-performing DressedQuantumCircuitClassifier. However, removing entanglement does decrease the test accuracy of QuantumBoltzmannMachine and DataReuploadingClassifier. Whether the "quantumness" of the entangling gates is the deciding factor, or whether the removal of certain gates could be mitigated by a better non-entangling design that enriches the expressivity of the models is an important subject for further studies.
MNIST PCA test

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-17.jpg?height=264&width=744&top_left_y=196&top_left_x=1154)

$23456789 \quad 23456789 \quad 23456789$

number of features

$\rightarrow$ DressedQuantumCircuitClassifier

-- DressedQuantumCircuitClassifierSeparable

$\rightarrow$ DataReuploadingClassifier

-     - DataReuploadingClassifierSeparable

$\rightarrow$ - QuantumBoltzmannMachine

-- QuantumBoltzmannMachineSeparable

FIG. 13. Comparison of the test accuracy of the three QNN models that performed better than the SeparableVariationalClassifier, shown on MNIST PCA up to 9 dimensions, with variations of the models that remove any entangling operations from the circuits. The DressedQuantumCircuitClassifier shows no drop in performance, while DataReuploadingClassifier, and to some extent QuantumBoltzmannMachine, do worse without entanglement.

## VI. QUESTIONS RAISED ABOUT QUANTUM MODEL DESIGN

Benchmarks cannot only give us evidence on which model is better than another, but open up paths to more qualitative questions, for example by systematically removing parts of a model, or by visualising simple cases. We want to give a few examples here.

## A. Do quantum components in hybrid models help?

By far the best QNN model is the DressedQuantumCircuitClassifier, which replaces a layer of a neural network with a standard variational quantum model. The central question for such a hybrid model is whether or not the "quantum layer" plays a qualitatively different role to a possible classical layer. Figure 14a shows the input transformations of the two neural network layers and the quantum layer for a very simple $2 \mathrm{~d}$ dataset, and compares it with the same model in which we exchanged the quantum layer by a neural network of the same architecture as the first layer. In this small experiment, the qualitative effect of both kinds of models is similar, namely to reshape the data to a one-dimensional manifold that facilitates the subsequent linear classification. This is consistent with the fact that in most experiments, the DressedQuantumCircuitClassifier's performance followed the classical neural network closely.

The QuanvolutionalNeuralNetwork is a hybrid model of similar flavour since it adds a quantum circuit
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-18.jpg?height=372&width=748&top_left_y=184&top_left_x=248)

(a) Transformation of a 2-dimensional moons dataset throughout the trained layers of the DressedQuantumCircuitClassifier (top row), compared to a model where we replaced the quantum circuit by another classical layer (bottom row).
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-18.jpg?height=366&width=768&top_left_y=778&top_left_x=229)

(b) The left-most images are two examples of the MNIST-CG data for a $16 \times 16$ pixel grid. The three images to the right show three channels of the data after being feature mapped by the initial quantum layer of QuanvolutionalNeuralNetwork. The quantum feature map appears to produce different, noisier versions of the input image.

FIG. 14. The effect of quantum layers in hybrid classicalquantum neural network models.

as the first layer to a convolutional neural network. In Figure 14b we show the result of this layer for a model with $n_{-}$qchannels $=3$ for two input examples of $16 \times 16$ pixels. It is unclear if the first quantum layer is generally useful for learning from image data, since in most cases the map seems to simply create a noisy version of the original image. Given that the model performs worse than the ConvolutionalNeuralNetwork (at least on the small datasets we were able to get results for), it seems that this feature map actually degrades the data so that it is subsequently more difficult for the classical convolution to learn from.

For the wide range of studies into hybrid quantumclassical neural network architectures, an important question is therefore whether the quantum layer introduces a genuinely different kind of transformation that helps the layer-wise feature extraction, or if it is simply 'not doing much harm'.

## B. What makes data reuploading work?

Besides the hybrid neural network architectures (and the computationally costly QuantumBoltzmannMachine for which we unfortunately only have limited data available) the DataReuploadingClassifier performs relatively well compared to other quantum neural networks of a similar design. What features of the model explain these results?

While the term "data reuploading" is often used rather generally to describe the layer-wise repetition of an encoding, there are a few other distinctive features in the original model we implemented here. For example, the inputs are individually rescaled by classical trainable parameters before feeding them into quantum gates, which in the picture of quantum models as a Fourier series [91] re-scales the frequency spectrum that the Fourier series is built from. Furthermore, there is no separation between the embedding and variational part of the circuit; instead the embedding layer is trainable (see Eq. C1 in App. C), leading to a sort of "trainable data encoding" (a feature that the QuantumMetricLearner also exhibits). Furthermore, the cost function differs from standard approaches as it measures the fidelity to certain states associated with a class label, and contains more trainable parameters. Which of these features is important for the success of the model - or is it a combination of them?

As a first probe into this question, Figure 15 shows the test accuracy of the DataReuploadingClassifier when we remove the three properties - the trainable rescaling, the mixing of variational and embedding circuits, and the peculiar cost function - individually from the model in a small ablation study. We use the MNIST PCA benchmark up to 9 dimensions once more.

The results suggest that both the trainable re-scaling and embedding are crucial for performance, while the special cost function is not. This is particularly interesting, since follow-up papers often only consider the trainable embedding as a design feature - however, the interplay of these two features may be important.

## C. What distance measures do quantum kernels define?

We observe that the quantum kernel methods (except QuantumKitchenSinks, which makes very special design decisions compared to the other two) have a surprisingly similar performance to the support vector machine with a Gaussian kernel. A kernel defines a distance measure on the input space. The distance measure is used to weigh the influence of a data point on the class label of another. What distance measure do the quantum kernels define, and are they similar to the Gaussian kernel?

Figure 16 shows the shape of the kernels used by models trained on 2-dimensional versions of our benchmarks. We include the SeparableKernelClassifier for interest, and define the kernel of QuantumKitchenSinks as

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-19.jpg?height=610&width=740&top_left_y=172&top_left_x=237)

MNIST PCA

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-19.jpg?height=366&width=414&top_left_y=240&top_left_x=411)

-- DataReuploadingClassifier

$\rightarrow$ DataReuploadingClassifierNoScaling

$\rightarrow$ DataReuploadingClassifierNoCost

$\rightarrow$ DataReuploadingClassifierNoTrainableEmbedding

FIG. 15. Comparison of the test accuracy of the Data Reuploading Classifier on MNIST PCA up to 9 dimensions with three modifications of the original implementation: The NoCost variation replaces the special cost function with a standard cross entropy cost function, the NoScaling version removes the trainable parameters multiplied to the inputs, and the NoTrainableEmbedding variation first applies all data encoding gates and then all variational gates. While the former has only a small influence on the performance, the latter two seem to both change the accuracy scores of higher dimensions significantly.

the inner product of the feature vectors created by the quantum circuit. With a few occasional exceptions - notably on TWO CURVES, a dataset that seems to require very narrow bandwidths in kernels and encourages quantum kernels to extend into their periodic regions - the kernel shapes do indeed resemble the SVC's Gaussian kernel.

Does that mean that quantum kernels are just approximations to a method that has been around for decades? While 2-dimensional visualisations of the kernel function can help us gain some understanding, we need to look into higher dimensions. Here geometric visualisations become tricky to interpret, and it can be useful to compare the actual Gram matrices $G$ with entries $G_{i j}=\kappa\left(\mathbf{x}_{i}, \mathbf{x}_{i}\right)$ for pairs of training points $\mathbf{x}_{i}, \mathbf{x}_{j}$, which are used when fitting the models ${ }^{17}$. A popular measure is the kernel alignment [92] that computes the product of corresponding entries of two matrices. However, this measure makes it hard to distinguish regions in which one Gram matrix has finite values and another near-zero ones from regions where both have near-zero values. To achieve a more insightful comparison we rescale the Gram matrices to have entries in $[0,1]$ and use the distance

$$
d\left(G \mid G^{\prime}\right)=\frac{\sum_{i j}\left(G_{i j}-G_{i j}^{\prime}\right)^{2}}{|G|}
$$[^10]

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-19.jpg?height=1106&width=895&top_left_y=176&top_left_x=1100)

FIG. 16. Kernels used by models for different 2-dimensional datasets, selecting the best hyperparameters found during grid search. The plots are generated by fixing one datapoint at $(\pi / 2, \pi / 2)$ in the $x$-y-plane and varying the other one, while plotting the kernel value for the two datapoints on the $\mathrm{z}$-axis. The kernel value shows how much weight the classifier gives a potential datapoint in the $\mathrm{x}$-y-plane when predicting the class of the fixed point. While there are some variations, most quantum kernel methods have a similar radial shape to the Gaussian kernel.

where $|G|$ refers to the number of entries in $G$ (which is the same as $\left.\left|G^{\prime}\right|\right)$. This can be seen as a "difference measure" where 0 signals identical Gram matrices, and 1 maximally different ones.

The results, of which some representative examples in $2 \mathrm{~d}$ versus $10 \mathrm{~d}$ are shown in Figure 17, give a slightly different picture that can only faintly be seen in the 2d kernel plots. In higher dimensions, only the ProjectedQuantumKernel resembles the SVC model, while the other three quantum kernels resemble each other. ${ }^{18}$ Does this mean that the projected quantum circuit is not so much responsible for learning, but rather the subsequent Gaussian kernel applied to the features computed by the quantum circuit? The other three quan-[^11]

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-20.jpg?height=1036&width=878&top_left_y=154&top_left_x=168)

FIG. 17. Average squared difference $d\left(G \mid G^{\prime}\right)$ between the entries of the Gram matrices of different kernels. The Gram matrices are constructed from the training data using the best hyperparameters found during grid search for the particular dataset. Here we show the four benchmarks that contained datasets in both 2 and 10 dimensions. In higher dimensions, the Gram matrices of quantum models tend to look similarly, with the exception of the ProjectedQuantumKernel model, that in turn tends to resemble the SVC.

tum kernels, in turn, produce very similar Gram matrices in high dimensions. Do most "non-projected" quantum kernel designs share this behaviour?

Overall, attempting to understand the distance measure induced by a quantum kernel in high dimensions, rather than only focusing on its classical intractability, is an important open challenge in which benchmarks can help.

## D. Why are polynomial features not working?

Another consistent finding on the lower end of the spectrum is that the two QNN models that encode data via amplitude encoding, the CircuitCentricClassifier and TreeTensorClassifers, perform poorly. A ready explanation for the TreeTensorClassifier is that neither the amplitude encoding nor the variational unitary evolution of the model can change the distance between (pre-processed) input data points. Moreover, due to the goal of avoiding vanishing gradients, the model uses a very limited number of parameters that scales only log-
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-20.jpg?height=554&width=718&top_left_y=156&top_left_x=1170)

-r- CircuitCentricClassifier

-.-. TreeTensorClassifier

FIG. 18. The average test accuracy for selected datasets of input dimension six when multiplying the input features by different scaling factors. The models were trained 5 times on each dataset, and the best hyperparameters were taken from our regular hyperparameter search. The default scaling we used in the previous results is shown by the dashed line, and is not always the optimal value, which could sometimes explain the poor performance of models based on amplitude encoding.

arithmically with the number of qubits; for example, for a problem with 16 features, one has only 7 variational parameters to train. This severely limits the expressivity of the model.

However, these arguments do not hold for the CircuitCentricClassifier, which uses an expressive variational circuit and several copies of the amplitude encoded state. This creates an overall state that is a tensor product $\mathbf{x} \times \mathbf{x} \times \ldots$ of the (pre-processed) input. Since we allow for up to three copies in hyperparameter search, this model has the ability to build up to 3rd order polynomial features like $\mathbf{x}_{1}^{3}$ or $\mathbf{x}_{1} \mathbf{x}_{6}^{2}$ through the embedding. It is therefore interesting to ask whether the lack in performance of the CircuitCentricClassifier is due to polynomial features not being useful in the datasets we consider here, or if a degree of 3 is too low. Looking at the hyperparameter optimisation (compare also the low correlation of n_input_copies with the test accuracy in Figure 8), we find that the optimal number of copies is often 1 or 2 instead of the maximum possible 3, which means that the model does not systematically prefer more complex features.

Note that another explanation is that we missed an important hyperparameter during grid search. Figure 18 plots the average accuracy of relevant models over all datasets that have an input dimension of 6 , when scaling the input data by different amounts. Scaling the data to smaller values than we used as default can have a beneficial effect, even when selecting the best hyperparameters found using this default scaling.

Overall, an important question is whether there are datasets and hyperparameter configurations for which the ability to construct high-order polynomial features using quantum circuits would be interesting - or is am-
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-21.jpg?height=664&width=722&top_left_y=178&top_left_x=254)

FIG. 19. Decision boundary of selected models trained on the 2d linearly separable dataset. Training points are shown in round and test points in triangular shape. All four models use angle embedding, therefore forming decision boundaries based on periodic functions. This may introduce an inductive bias that is not suited for data requiring linear decision boundaries in higher dimensions.

plitude embedding just not a good design choice?

## E. Why do quantum models struggle with the linearly separable benchmark?

As discussed above, the behaviour of both QNNs and quantum kernel methods on the LINEARLY SEPARABLE benchmark is a notable outlier, as the performance of nearly all quantum models is poor and gets worse with the dimensionality of the data while the classical models achieve near-perfect classification. ${ }^{19}$ But is this really a property of linearly separable datasets? It is more likely that many of the quantum models in our selection, in particular those using angle embedding, have an inductive bias against linear decision boundaries like the one implanted into the LINEARLY SEPARABLE data generation procedure. (Instead, they may do very well on linearly separable data of Gaussian blobs.) For example, Figure 19 shows the decision boundaries for selected models on the 2-dimensional dataset of the LINEARLY SEPARABLE benchmark using the best hyperparameters found during grid search - all models try to fit the data with periodic regions. While this may work for low dimensions, the constraints could be incompatible with linear decision boundaries in higher dimensions. An interesting theoretical study would be to analyse which kinds of[^12]

data this behaviour is or is not suited for, and what the resource requirements, such as more embedding layers, are necessary to overcome the problem.

In contrast, most quantum models performed almost similar to the classical baselines on the HYPERPLANES DIFF benchmark, suggesting that perhaps none of the models contained a bias that was particularly aligned with this data. As a reminder, the data generation procedure was intended to test the ability of models to detect a "global" labeling rule, for which the positioning of several hyperplanes in a low-dimensional manifold is relevant.

## VII. CONCLUSION

One of the most important lessons one learns when undertaking a study of this kind is that benchmarking is a subtle art that is filled with difficult decisions and potential pitfalls. As such, the benchmarking of new quantum machine learning proposals should be considered an extreme challenge, rather than as a task that can be safely given to lesser experienced researchers or relegated to the afterthought of a study. It is hard to coax robust and meaningful results from systems as complex as machine learning models trained on data, and non-robust claims can have a profound impact on where the community searches for good ideas. The single most effective remedy is scientific rigour in the methodological design of studies, including extensive reporting on the choices made and their potential bias on the results.

Perhaps the most important question regarding the future of benchmarking of quantum models is what kind of data to choose. More studies that focus on questions of structure in data are crucial for the design of meaningful benchmarks: What mathematical properties do real-world applications of relevance have? How can we isolate, downscale and artificially reproduce them? How can we connect them to the mathematical properties of quantum models? This is a task shared with classical machine learning research, but further exacerbated by the fact that the areas in which quantum computers can unlock new capabilities of learning are not yet identified. Using the right data and finding quantum models with an advantage hence becomes a "chicken and egg problem" that is best tackled from two sides; however in the current literature, the focus on model design dominates by far.

Aside from these conceptual challenges, benchmarking quantum machine learning models also poses a formidable challenge to current quantum software. On the one hand, this is due to the immense resource requirements of hyperparameter optimisation. On the other hand, quantum machine learning models are usually elaborate pipelines of hybrid quantum-classical systems, each of which requires different logics for performance tools like compilation, parallelisation, caching or GPU use. Furthermore, results on small datasets cannot be used to reason about larger datasets, as we know from deep
learning that big data leads to surprisingly different behaviour. There is hence a need to study how results scale to larger datasets, which typically pushes the number of qubits to the limits of what is possible today.

Finally, instead of considering rankings only, benchmarks can help us to gain qualitative insights into which parts of a model design are crucial and which ones replaceable. Since the question of quantum advantage is undercurrent to almost all studies in quantum machine learning, a particularly important experiment that should become a standard in benchmarking is to remove "quantumness" from a model in a non-invasive manner and test if the results hold. Of course, there are other ways than removing entanglement to make models classically tractable or "non-quantum", such as limiting gates to the Clifford family, replacing unitary transformations by stochastic ones (see Appendix in [93]) or using Matrix Product State simulators with low bond dimension. Comparing to such circuit designs will provide invaluable information into the promise of ideas around variational quantum circuits.

## ACKNOWLEDGMENTS

Our computations were performed on the Cedar supercomputer at the SciNet HPC Consortium. SciNet is funded by Innovation, Science and Economic Development Canada; the Digital Research Alliance of Canada; the Ontario Research Fund: Research Excellence; and the University of Toronto.
[1] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and S. Lloyd, Quantum machine learning, Nature 549, 195 (2017).

[2] M. Schuld and F. Petruccione, Machine learning with quantum computers (Springer, 2021).

[3] K. Ito and K. Fujii, Santaqlaus: A resource-efficient method to leverage quantum shot-noise for optimization of variational quantum algorithms, arXiv preprint arXiv:2312.15791 (2023).

[4] M. Wiedmann, M. Hölle, M. Periyasamy, N. Meyer, C. Ufrecht, D. D. Scherer, A. Plinge, and C. Mutschler, An empirical comparison of optimizers for quantum machine learning with spsa-based gradients, arXiv preprint arXiv:2305.00224 (2023).

[5] M. Kashif, M. Rashid, S. Al-Kuwari, and M. Shafique, Alleviating barren plateaus in parameterized quantum machine learning circuits: Investigating advanced parameter initialization strategies, arXiv preprint arXiv:2311.13218 (2023).

[6] I. N. M. Le, O. Kiss, J. Schuhmacher, I. Tavernelli, and F. Tacchino, Symmetry-invariant quantum machine learning force fields, arXiv preprint arXiv:2311.11362 (2023).

[7] M. T. West, M. Sevior, and M. Usman, Reflection equivariant quantum neural networks for enhanced image classification, Machine Learning: Science and Technology 4, 035027 (2023).

[8] S. Bordoni, D. Stanev, T. Santantonio, and S. Giagu, Long-lived particles anomaly detection with parametrized quantum circuits, Particles 6, 297 (2023).

[9] F. J. Schreiber, J. Eisert, and J. J. Meyer, Classical surrogates for quantum learning models, Physical Review Letters 131, 100803 (2023).

[10] N. Piatkowski, T. Gerlach, R. Hugues, R. Sifa, C. Bauckhage, and F. Barbaresco, Towards bundle adjustment for satellite imaging via quantum machine learning, in 2022 25th International Conference on Information Fusion (FUSION) (IEEE, 2022) pp. 1-8.

[11] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, S. Ahmed, V. Ajith, M. S. Alam, G. Alonso-Linaje, B. AkashNarayanan, A. Asadi, et al., Pennylane: Automatic differentiation of hybrid quantum-classical computations,
arXiv preprint arXiv:1811.04968 (2018).

[12] E. Farhi and H. Neven, Classification with quantum neural networks on near term processors, arXiv preprint arXiv:1802.06002 (2018).

[13] M. Schuld, A. Bocharov, K. M. Svore, and N. Wiebe, Circuit-centric quantum classifiers, Physical Review A 101, 032308 (2020).

[14] I. Cong, S. Choi, and M. D. Lukin, Quantum convolutional neural networks, Nature Physics 15, 1273 (2019).

[15] M. Henderson, S. Shakya, S. Pradhan, and T. Cook, Quanvolutional neural networks: powering image recognition with quantum circuits, Quantum Machine Intelligence 2, 2 (2020).

[16] M. Schuld and N. Killoran, Quantum machine learning in feature hilbert spaces, Physical review letters 122, 040504 (2019).

[17] V. Havlíček, A. D. Córcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow, and J. M. Gambetta, Supervised learning with quantum-enhanced feature spaces, Nature 567, 209 (2019).

[18] A. Pérez-Salinas, A. Cervera-Lierta, E. Gil-Fuster, and J. I. Latorre, Data re-uploading for a universal quantum classifier, Quantum 4, 226 (2020).

[19] K. Zhang, M.-H. Hsieh, L. Liu, and D. Tao, Toward trainability of quantum neural networks, arXiv preprint arXiv:2011.06258 (2020).

[20] S. Wei, Y. Chen, Z. Zhou, and G. Long, A quantum convolutional neural network on nisq devices, AAPPS Bulletin 32, 1 (2022).

[21] J. Kübler, S. Buchholz, and B. Schölkopf, The inductive bias of quantum kernels, Advances in Neural Information Processing Systems 34, 12661 (2021).

[22] M. Larocca, F. Sauvage, F. M. Sbahi, G. Verdon, P. J. Coles, and M. Cerezo, Group-invariant quantum machine learning, PRX Quantum 3, 030341 (2022).

[23] J. Bowles, V. J. Wright, M. Farkas, N. Killoran, and M. Schuld, Contextuality and inductive bias in quantum machine learning, arXiv preprint arXiv:2302.01365 (2023).

[24] F. J. Kiwit, M. Marso, P. Ross, C. A. Riofrío, J. Klepsch, and A. Luckow, Application-oriented benchmarking of quantum generative learning using quark, in 2023 IEEE

International Conference on Quantum Computing and Engineering (QCE), Vol. 1 (IEEE, 2023) pp. 475-484.

[25] C. Moussa, Y. J. Patel, V. Dunjko, T. Bäck, and J. N. van Rijn, Hyperparameter importance and optimization of quantum neural networks across small datasets, Machine Learning 10.1007/s10994-023-06389-8 (2023).

[26] M. Dehghani, Y. Tay, A. A. Gritsenko, Z. Zhao, N. Houlsby, F. Diaz, D. Metzler, and O. Vinyals, The benchmark lottery, arXiv preprint arXiv:2107.07002 (2021).

[27] D. Sculley, J. Snoek, A. Wiltschko, and A. Rahimi, Winner's curse? on pace, progress, and empirical rigor, ICLR Workshop track (2018).

[28] K. Ethayarajh and D. Jurafsky, Utility is in the eye of the user: A critique of nlp leaderboards, arXiv preprint arXiv:2009.13888 (2020).

[29] D. H. Wolpert, The lack of a priori distinctions between learning algorithms, Neural computation 8, 1341 (1996).

[30] C. G. Northcutt, A. Athalye, and J. Mueller, Pervasive label errors in test sets destabilize machine learning benchmarks, arXiv preprint arXiv:2103.14749 (2021).

[31] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar, Do cifar-10 classifiers generalize to cifar-10?, arXiv preprint arXiv:1806.00451 (2018).

[32] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar, Do imagenet classifiers generalize to imagenet?, in International conference on machine learning (PMLR, 2019) pp. $5389-5400$.

[33] B. Koch, E. Denton, A. Hanna, and J. G. Foster, Reduced, reused and recycled: The life of a dataset in machine learning research, arXiv preprint arXiv:2112.01716 (2021).

[34] A. Paullada, I. D. Raji, E. M. Bender, E. Denton, and A. Hanna, Data and its (dis) contents: A survey of dataset development and use in machine learning research, Patterns 2 (2021).

[35] R. Dotan and S. Milli, Value-laden disciplinary shifts in machine learning, arXiv preprint arXiv:1912.01172 (2019).

[36] S. Narang, H. W. Chung, Y. Tay, W. Fedus, T. Fevry, M. Matena, K. Malkan, N. Fiedel, N. Shazeer, Z. Lan, et al., Do transformer modifications transfer across implementations and applications?, arXiv preprint arXiv:2102.11972 (2021).

[37] S. L. Smith, A. Brock, L. Berrada, and S. De, Convnets match vision transformers at scale, arXiv preprint arXiv:2310.16764 (2023).

[38] M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet, Are gans created equal? a large-scale study, Advances in neural information processing systems 31 (2018).

[39] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger, Deep reinforcement learning that matters, in Proceedings of the AAAI conference on artificial intelligence, Vol. 32 (2018).

[40] C. Riquelme, G. Tucker, and J. Snoek, Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling, arXiv preprint arXiv:1802.09127 (2018).

[41] P. Flach, Performance evaluation in machine learning: the good, the bad, the ugly, and the way forward, in Proceedings of the AAAI conference on artificial intelligence, Vol. 33 (2019) pp. 9808-9814.

[42] J. Dodge, S. Gururangan, D. Card, R. Schwartz, and
N. A. Smith, Show your work: Improved reporting of experimental results, arXiv preprint arXiv:1909.03004 (2019).

[43] Y. LeCun, The mnist database of handwritten digits, http://yann.lecun.com/exdb/mnist/ (1998).

[44] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei, Imagenet: A large-scale hierarchical image database, in 2009 IEEE conference on computer vision and pattern recognition (Ieee, 2009) pp. 248-255.

[45] A. Krizhevsky, G. Hinton, et al., Learning multiple layers of features from tiny images, (2009).

[46] G. Hinton, The forward-forward algorithm: Some preliminary investigations, arXiv preprint arXiv:2212.13345 (2022).

[47] J. Bausch, Recurrent quantum neural networks, Advances in neural information processing systems $\mathbf{3 3}, 1368$ (2020).

[48] S. Greydanus, Scaling down deep learning, arXiv preprint arXiv:2011.14439 (2020).

[49] J. Zhao, Y.-H. Zhang, C.-P. Shao, Y.-C. Wu, G.-C. Guo, and G.-P. Guo, Building quantum neural networks based on a swap test, Physical Review A 100, 012334 (2019).

[50] F. Tacchino, C. Macchiavello, D. Gerace, and D. Bajoni, An artificial neuron implemented on an actual quantum processor, npj Quantum Information 5, 26 (2019).

[51] A. Mari, T. R. Bromley, J. Izaac, M. Schuld, and N. Killoran, Transfer learning in hybrid classical-quantum neural networks, Quantum 4, 340 (2020).

[52] V. Havlíček, A. D. Córcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow, and J. M. Gambetta, Supervised learning with quantum-enhanced feature spaces, Nature 567, 209 (2019).

[53] C. Zoufal, A. Lucchi, and S. Woerner, Variational quantum boltzmann machines, Quantum Machine Intelligence 3, 1 (2021).

[54] H.-Y. Huang, M. Broughton, M. Mohseni, R. Babbush, S. Boixo, H. Neven, and J. R. McClean, Power of data in quantum machine learning, Nature Communications 12, 2631 (2021).

[55] C. Wilson, J. Otterbach, N. Tezak, R. Smith, A. Polloreno, P. J. Karalekas, S. Heidel, M. S. Alam, G. Crooks, and M. da Silva, Quantum kitchen sinks: An algorithm for machine learning on near-term quantum computers, arXiv preprint arXiv:1806.08321 (2018).

[56] S. Lloyd, M. Schuld, A. Ijaz, J. Izaac, and N. Killoran, Quantum embeddings for machine learning, arXiv preprint arXiv:2001.03622 (2020).

[57] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii, Quantum circuit learning, Physical Review A 98, 032309 (2018).

[58] M. Schuld, V. Bergholm, C. Gogolin, J. Izaac, and N. Killoran, Evaluating analytic gradients on quantum hardware, Physical Review A 99, 032331 (2019).

[59] M. J. Bremner, R. Jozsa, and D. J. Shepherd, Classical simulation of commuting quantum computations implies collapse of the polynomial hierarchy, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 467, 459 (2011).

[60] T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman, The elements of statistical learning: data mining, inference, and prediction, Vol. 2 (Springer, 2009).

[61] I. Steinwart and A. Christmann, Support vector machines (Springer Science \& Business Media, 2008).

[62] T. Hofmann, B. Schölkopf, and A. J. Smola, Kernel meth-
ods in machine learning (2008).

[63] M. Schuld, Supervised quantum machine learning models are kernel methods, arXiv preprint arXiv:2101.11020 (2021).

[64] K. Fukushima, Neocognitron: A hierarchical neural network capable of visual pattern recognition, Neural networks 1, 119 (1988).

[65] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE 86, 2278 (1998).

[66] M. Weiler, P. Forré, E. Verlinde, and M. Welling, Equivariant and Coordinate Independent Convolutional Networks (2023).

[67] T. S. Cohen, M. Geiger, and M. Weiler, A general theory of equivariant cnns on homogeneous spaces, Advances in neural information processing systems 32 (2019).

[68] Flax software package.

[69] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang, JAX: composable transformations of Python+NumPy programs (2018).

[70] DeepMind, I. Babuschkin, K. Baumli, A. Bell, S. Bhupatiraju, J. Bruce, P. Buchlovsky, D. Budden, T. Cai, A. Clark, I. Danihelka, A. Dedieu, C. Fantacci, J. Godwin, C. Jones, R. Hemsley, T. Hennigan, M. Hessel, S. Hou, S. Kapturowski, T. Keck, I. Kemaev, M. King, M. Kunesch, L. Martens, H. Merzic, V. Mikulik, T. Norman, G. Papamakarios, J. Quan, R. Ring, F. Ruiz, A. Sanchez, L. Sartran, R. Schneider, E. Sezener, S. Spencer, S. Srinivasan, M. Stanojević, W. Stokowiec, L. Wang, G. Zhou, and F. Viola, The DeepMind JAX Ecosystem (2020).

[71] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, Scikit-learn: Machine learning in Python, Journal of Machine Learning Research 12, 2825 (2011).

[72] L. Schatzki, A. Arrasmith, P. J. Coles, and M. Cerezo, Entangled datasets for quantum machine learning, arXiv preprint arXiv:2109.03400 (2021).

[73] E. Perrier, A. Youssry, and C. Ferrie, Qdataset, quantum datasets for machine learning, Scientific data 9, 582 (2022).

[74] Y. Bengio, A. Courville, and P. Vincent, Representation learning: A review and new perspectives, IEEE transactions on pattern analysis and machine intelligence 35, 1798 (2013).

[75] H. Narayanan and S. Mitter, Sample complexity of testing the manifold hypothesis, Advances in neural information processing systems 23 (2010).

[76] P. Pope, C. Zhu, A. Abdelkader, M. Goldblum, and T. Goldstein, The intrinsic dimension of images and its impact on learning, arXiv preprint arXiv:2104.08894 (2021).

[77] F. Rosenblatt, The perceptron: a probabilistic model for information storage and organization in the brain., Psychological review 65, 386 (1958).

[78] M. Minsky and S. A. Papert, Perceptrons, reissue of the 1988 expanded edition with a new foreword by Léon Bottou: an introduction to computational geometry (MIT press, 2017).

[79] S. Goldt, M. Mézard, F. Krzakala, and L. Zdeborová,
Modeling the influence of data structure on learning in neural networks: The hidden manifold model, Physical Review X 10, 041044 (2020).

[80] S. Buchanan, D. Gilboa, and J. Wright, Deep networks and the multiple manifold problem, in International Conference on Learning Representations (2021).

[81] A. C. Lorena, A. I. Maciel, P. B. de Miranda, I. G. Costa, and R. B. Prudêncio, Data complexity meta-features for regression problems, Machine Learning 107, 209 (2018).

[82] S. Guan and M. Loew, A novel intrinsic measure of data separability, Applied Intelligence 52, 17734 (2022).

[83] M. R. Smith, T. Martinez, and C. Giraud-Carrier, An instance level analysis of data complexity, Machine learning 95, 225 (2014).

[84] J. M. Sotoca, J. S. Sánchez, and R. A. Mollineda, A review of data complexity measures and their applicability to pattern classification problems, Actas del III Taller Nacional de Mineria de Datos y Aprendizaje. TAMIDA , 77 (2005).

[85] T. K. Ho and M. Basu, Complexity measures of supervised classification problems, IEEE transactions on pattern analysis and machine intelligence 24, 289 (2002).

[86] J. Lorraine, P. Vicol, and D. Duvenaud, Optimizing millions of hyperparameters by implicit differentiation, in Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, Vol. 108, edited by S. Chiappa and R. Calandra (PMLR, 2020) pp. 1540-1552.

[87] S. Ahmed, N. Killoran, and J. F. C. Álvarez, Implicit differentiation of variational quantum algorithms (2022), arXiv:2211.13765 [quant-ph].

[88] F. J. Provost, T. Fawcett, R. Kohavi, et al., The case against accuracy estimation for comparing induction algorithms., in ICML, Vol. 98 (1998) pp. 445-453.

[89] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, Optuna: A next-generation hyperparameter optimization framework, in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2019).

[90] D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro, and D. Sculley, Google vizier: A service for black-box optimization, in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17 (Association for Computing Machinery, New York, NY, USA, 2017) p. 1487-1495.

[91] M. Schuld, R. Sweke, and J. J. Meyer, Effect of data encoding on the expressive power of variational quantummachine-learning models, Physical Review A 103, 032430 (2021).

[92] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. Kandola, On kernel-target alignment, Advances in neural information processing systems 14 (2001).

[93] A. Abbas, R. King, H.-Y. Huang, W. J. Huggins, R. Movassagh, D. Gilboa, and J. R. McClean, On quantum backpropagation, information reuse, and cheating measurement collapse, arXiv preprint arXiv:2305.13362 (2023).

[94] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush, and H. Neven, Barren plateaus in quantum neural network training landscapes, Nature communications 9 , 4812 (2018).

[95] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980 (2014).

## Appendix A: $\mathrm{CO}_{2}$ Emission Table

The following calculations are made using energy consumption data for the Cedar supercomputing cluster at the SciNet HPC Consortium, on which the vast majority of our simulations were run.

| Numerical simulations |  |
| :--- | :---: |
| Total CPU usage [core years] | $\approx 27$ |
| Cluster energy consumption (per core) $[\mathrm{W}]$ | $\approx 11$ |
| Total Energy Consumption Simulations $[\mathrm{kWh}]$ | $\approx 2600$ |
| Average Emissions Of $\mathrm{CO}_{2}$ In Canada $[\mathrm{kg} / \mathrm{kWh}]$ | $\approx 0.13$ |
| Total $\mathrm{CO}_{2}$ Emissions For numerical simulations $[\mathrm{kg}]$ | $\approx 340$ |
| Transport |  |
| Total $\mathrm{CO}_{2}$ Emission For Transport $[\mathrm{kg}]$ | 0 |
| Total $\mathrm{CO}_{2}$ Emission $[\mathrm{kg}]$ | $\approx 340$ |
| Were The Emissions Offset? | No |

## Appendix B: Glossary of quantum machine learning

 conceptsAmplitude embedding - An input data vector $\boldsymbol{x}$ is said to be amplitude embedded into a pure quantum state $|\psi(\boldsymbol{x})\rangle$ if the quantum state takes the form

$$
\begin{equation*}
|\psi(\boldsymbol{x})\rangle=\boldsymbol{x} \oplus \boldsymbol{c} / \mathcal{N} \tag{B1}
\end{equation*}
$$

where $\boldsymbol{c}$ is a vector with constant entries and $\mathcal{N}=\sqrt{\boldsymbol{x}^{\dagger} \cdot \boldsymbol{x}+\boldsymbol{c}^{\dagger} \cdot \boldsymbol{c}}$ is the state normalization.

Angle embedding-An input data vector $\boldsymbol{x}=\left(x_{j}\right)$ is said to be angle embedded into a pure quantum state $|\psi(\boldsymbol{x})\rangle$ if the quantum state takes the form

$$
\begin{equation*}
|\psi(\boldsymbol{x})\rangle=\prod_{j} \exp \left(-i G_{j} x_{j}\right)\left|\psi_{0}\right\rangle \tag{B2}
\end{equation*}
$$

where $\left|\psi_{0}\right\rangle$ is some initial quantum state and $G_{j}$ are Hermitian operators. If the operators $G_{j}$ act non-trivially on single qubits only, we call it a product angle embedding. This process of angle embedding is sometimes repeated a number of times, which is often called data reuploading.

Binary cross entropy loss-Given class probabilities $P\left( \pm 1 \mid \boldsymbol{\theta}, \boldsymbol{x}_{i}\right)$ and a label $y_{i}= \pm 1$, the cross entropy loss is

$$
\begin{equation*}
\ell\left(\boldsymbol{\theta}, \boldsymbol{x}_{i}, \boldsymbol{y}_{\boldsymbol{i}}\right)=-\log P\left(y_{i} \mid \boldsymbol{\theta}, \boldsymbol{x}_{i}\right) \tag{B3}
\end{equation*}
$$

Minimising the cross entropy loss on a dataset is equivalent to maximising the log likelihood of the data, and is the preferred loss in binary classification tasks.

Gibbs state-A Gibbs state is a quantum density matrix that is diagonal in the energy eigenbasis given by a Hamiltonian $H$, and whose probability distribution over energy eigenstates forms a Gibbs distribution. Mathematically we have

$$
\begin{equation*}
\rho=\exp \left(-\frac{H}{k_{b} T}\right) / Z \tag{B4}
\end{equation*}
$$

where $T>0$ is a temperature, $k_{b}$ Boltzmann's constant and $Z=\operatorname{tr} \exp \left(-\frac{H}{k_{b} T}\right)$.

Instantaneous Quantum Polynomial circuit-A circuit that consists of input state preparation $|0\rangle$, quantum gates of the form $\exp \left(-i G_{X} \theta\right)$ where $G_{X}$ is a product of $\sigma_{X}$ operators on a subset of qubits, and measurement of a diagonal observable.

Linear loss-Given class probabilities $P\left( \pm 1 \mid \boldsymbol{\theta}, \boldsymbol{x}_{i}\right)$ and a label $y_{i}= \pm 1$, the linear loss is

$$
\begin{equation*}
\ell\left(\boldsymbol{\theta}, \boldsymbol{x}_{i}, \boldsymbol{y}_{\boldsymbol{i}}\right)=-P\left(y_{i} \mid \boldsymbol{\theta}, \boldsymbol{x}_{i}\right) \tag{B5}
\end{equation*}
$$

Minimising the linear loss over a dataset is therefore equivalent to maximising the sum of the probabilities to classify each input correctly.

Maximum margin (linear) classifier-A linear classifier that separates the two classes such that the minimum distance of any training point to the hyperplane that defines the decision boundary is maxised.

Gaussian Kernel-A kernel of the form

$$
\begin{equation*}
k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\exp \left(-\gamma\left\|\boldsymbol{x}-\boldsymbol{x}^{\prime}\right\|\right) \tag{B6}
\end{equation*}
$$

where $\gamma$ is a free hyperparameter.

Square loss-Given a model whose output is $f(\boldsymbol{\theta}, \boldsymbol{x})$, the square loss is

$$
\begin{equation*}
\ell\left(\boldsymbol{\theta}, \boldsymbol{x}_{i}, \boldsymbol{y}_{\boldsymbol{i}}\right)=\left(f\left(\boldsymbol{\theta}, \boldsymbol{x}_{i}\right)-y_{i}\right)^{2} \tag{B7}
\end{equation*}
$$

Vanishing gradients-A phenomenon that commonly occurs for deep or highly expressive variational quantum circuits whereby the expected magnitude of a typical gradient component decreases exponentially with the number of qubits when initialising all parameters uniformly at random $[94]$.

## Appendix C: Detailed description of the models

In this appendix we describe each model used in the study in further detail. Unless otherwise specified all variational models are trained via gradient-descent using the Adam optimizer [95] implemented in Optax [70] with default parameters except for the learning rate which we vary during hyperparameter search.

## 1. CircuitCentricClassifier [13]

An input vector $\mathbf{x}$ of dimension $d$ gets amplitude embedded into a suitable number of qubits (including padding and normalisation, see Glossary above). Note that this preprocessing strategy induces a qualitatively different behaviour when $d$ is a power of 2 , since without padding, normalisation looses all information about the length of the inputs.

A hyperparameter allows for the pre-processed input to be encoded into multiple registers, which creates copies of amplitude encoding states. This effectively creates tensor products of the pre-processed inputs. A variational circuit that uses arbitrary single qubit rotations followed by cascades of controlled arbitrary single qubit rotations is followed by a $\mathrm{Z}$ measurement of the first qubit.

The following is an example of the quantum circuit used in the CircuitCentricClassifier for two copies of an input $\mathbf{x} \in \mathbb{R}^{4}$ embedded into state $\left|\psi_{\mathbf{x}}\right\rangle$ and PennyLane's StronglyEntanglingLayers template implementing the variational ansatz:

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-26.jpg?height=355&width=377&top_left_y=1096&top_left_x=408)

The expected value of the measurement is added to a trainable classical bias. The parameters of the variational circuit, as well as the bias, are optimised using the square loss.

| hyperparameter | values |
| :--- | :--- |
| learning_rate | $[0.001,0.01,0.1]$ |
| n_layers (variational layers) | $[1,5,10]$ |
| n_input_copies | $[1,2,3]$ |

## 2. DataReuploadingClassifier [18]

This model uses successive, trainable angle embeddings of data. Each qubit embedding gate takes a vector $\boldsymbol{x}$ of three features, two trainable three-dimensional real vectors $\boldsymbol{w}$ and $\boldsymbol{\theta}$, and encodes them as

$$
\begin{equation*}
U(\boldsymbol{x} \circ \boldsymbol{\omega}+\boldsymbol{\theta}) \tag{C1}
\end{equation*}
$$

where

$$
\begin{equation*}
U(\phi)=e^{i Z \phi_{1} / 2} e^{i Y \phi_{2} / 2} e^{i Z \phi_{3} / 2} \tag{C2}
\end{equation*}
$$

parameterizes a general $\mathrm{SU}(2)$ rotation on a single qubit, and $\circ$ denotes element-wise multiplication of vectors.
To encode data input $\boldsymbol{x} \in \mathbb{R}^{d}$, we therefore split $\boldsymbol{x}$ into $\left\lceil\frac{d}{3}\right\rceil$ vectors of size 3 , and feed each vector into a distinct qubit embedding gate (padding input vectors with zero if necessary). The number of qubits is therefore set by the dimension of the input data features. This is followed by a sequence of CZ gates in a ladder structure (See Figure 5 of [18]), and the process is repeated for a number of layers, which increases the expressivity of the model.

The following is an example of the quantum circuit used in the DataReuploadingClassifier for the input $\mathbf{x}=(0.1,0.2,0.3,0.4)^{T}$ embedded in 3 trainable layers, and with the scaling parameters $\boldsymbol{\omega}$ all set to 1 .

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-26.jpg?height=361&width=742&top_left_y=714&top_left_x=1147)

Training is based on the fidelities of the output qubits to one of two class states: here either $|0\rangle$ or $|1\rangle$. Defining $F_{j}^{0}(\boldsymbol{x}), F_{j}^{1}(\boldsymbol{x})$ as the fidelity of the $j^{\text {th }}$ output qubit to the state $|0\rangle,|1\rangle$, the loss $\ell$ for a single data point is given by

$$
\begin{equation*}
\ell\left(\boldsymbol{\theta}, \boldsymbol{\omega}, \boldsymbol{\alpha}, \boldsymbol{x}_{i}\right)=\sum_{j=1}^{n_{\max }}\left(\alpha_{j}^{0} F_{j}^{0}-\left(1-y_{i}\right)\right)^{2}+\left(\alpha_{j}^{1} F_{j}^{1}-y_{i}\right)^{2} \tag{C3}
\end{equation*}
$$

where the $\alpha_{j}^{0}, \alpha_{j}^{1}$ are trainable parameters, and $n_{\max }$ determines the number of qubits to use for training and prediction.

For prediction, we use the average fidelity to either $|0\rangle$ or $|1\rangle$ over the same qubits:

$$
\begin{equation*}
y_{\text {pred }}=\operatorname{argmax}\left(\left\langle F_{j}^{0}\right\rangle,\left\langle F_{j}^{1}\right\rangle\right) \tag{C4}
\end{equation*}
$$

where $\left\langle F_{j}^{0}\right\rangle=\frac{1}{n_{\max }} \sum_{j=1}^{n_{\max }} F_{j}^{0}$. This is not specified in [18], but is a natural generalisation of the $n_{\max }=1$ case they focus on. The choice of the hyperparameter observable_type determines the number $n_{\max }$ of qubits used to evaluate the weighted cost function.

| hyperparameter | values |
| :--- | :--- |
| learning_rate | $[0.001,0.01,0.1]$ |
| n_layers (reuploading layers) | $[1,5,10,15]$ |
| observable_type | [single, half, full] |

## 3. DressedQuantumCircuitClassifier [51]

This model maps an input data point $\boldsymbol{x}$ two a 2 dimensional vector via

$$
f\left(\boldsymbol{\theta}, \boldsymbol{W}_{\text {in }}, \boldsymbol{W}_{\text {out }}, \boldsymbol{x}\right)=f_{\text {out }}\left(\boldsymbol{W}_{\text {out }}, f_{Q}\left(\boldsymbol{\theta}, f_{\text {in }}\left(\boldsymbol{W}_{\text {in }}, \boldsymbol{x}\right)\right)\right)
$$

The functions $f_{\text {in }}\left(\boldsymbol{W}_{\text {in }}, \cdot\right), f_{\text {out }}\left(\boldsymbol{W}_{\text {out }}, \cdot\right)$ are single layer fully connected feed-forward neural networks with weights $\boldsymbol{W}_{\text {in }} \in \mathbb{R}^{d \times n}, \boldsymbol{W}_{\text {out }} \in \mathbb{R}^{n \times 2}$, where $f_{\text {in }}$ has a tanh activation scaled by $\pi / 2$, and $f_{\text {out }}$ has no activation. The function $f_{Q}$ corresponds to a parameterised quantum circuit where input features are angle-encoded into individual qubits, followed by layers of single-qubit $Y$ rotations and CNOT gates applied in a ring pattern. The output of the circuit is an $n$-dimensional vector whose elements are the expectation values of single-qubit $Z$ measurements on each qubit.

The following is an example of the quantum circuit used in the DressedQuantumCircuitClassifier for the input $\mathbf{x}=(0.1,0.2,0.3,0.4)^{T}$ and 3 variational layers:

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-27.jpg?height=523&width=832&top_left_y=823&top_left_x=191)

The output vector is sent through a softmax layer that is used for prediction and gives the class probabilities, and the cross entropy loss is used to train $\boldsymbol{W}_{\text {in }}, \boldsymbol{W}_{\text {out }}$ and $\boldsymbol{\theta}$ simultaneously.

```
hyperparameter values
learning_rate $\quad[0.001,0.01,0.1]$
n_layers $\quad[1,5,10,15]$
```


## 4. IQPVariationalClassifier [52]

This model uses angle encoding $V(\boldsymbol{x})$ inspired from IQP circuits, which is implemented by PennyLane's IQPEmbedding class. This is followed by a trainable parameterised circuit $U(\boldsymbol{\theta})$, implemented by PennyLane's StronglyEntanglingLayers class.

Prediction is given by measurement of $Z_{1} Z_{2}$ on the first two qubits:

$$
\begin{equation*}
y_{\text {pred }}=\operatorname{sign} f(\boldsymbol{\theta}, \boldsymbol{x}) \tag{C5}
\end{equation*}
$$

where

$$
\begin{equation*}
f(\boldsymbol{\theta}, \boldsymbol{x})=\left\langle 0\left|V^{\dagger}(\boldsymbol{x}) U^{\dagger}(\boldsymbol{\theta}) Z_{1} Z_{2} U(\boldsymbol{\theta}) V(\boldsymbol{x})\right| 0\right\rangle \tag{C6}
\end{equation*}
$$

The loss is equal to the linear loss:

$$
\begin{equation*}
\ell(\boldsymbol{\theta}, \boldsymbol{x})=(1-y \cdot f(\boldsymbol{\theta}, \boldsymbol{x})) / 2 \tag{C7}
\end{equation*}
$$

The following is an example of the quantum circuit used in the IQPVariationalClassifier for the input $\mathbf{x}=(0.1,0.2,0.3,0.4)^{T}:$

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-27.jpg?height=366&width=393&top_left_y=454&top_left_x=1319)

| hyperparameter | values |
| :--- | :--- |
| learning_rate | $[0.001,0.01,0.1]$ |
| n_layers (variational layers) | $[1,5,10,15]$ |
| repeats(embedding layers) | $[1,5,10]$ |

## 5. QuantumBoltzmannMachine [53]

This model encodes data into a Gibbs state of a $n$ qubit Hamiltonian. We use a Hamiltonian that is a natural generalization of the one studied in the two-qubit example in [53]:

$$
\begin{equation*}
H(\boldsymbol{\theta}, \boldsymbol{x})=\sum_{j} Z_{j} \boldsymbol{\theta}_{j}^{Z} \cdot \boldsymbol{x}+\sum_{j} X_{j} \boldsymbol{\theta}_{i}^{X} \cdot \boldsymbol{x}+\sum_{j>k} Z_{j} Z_{k} \boldsymbol{\theta}_{j k} \cdot \boldsymbol{x} \tag{C8}
\end{equation*}
$$

where $\boldsymbol{\theta}_{j}^{Z}, \boldsymbol{\theta}_{j}^{X}, \boldsymbol{\theta}_{j k}$ are vectors of trainable parameters that we collect into $\boldsymbol{\theta}$. We take $n=d$ so that the number of qubits scales with the number of features.

Since Gibbs state preparation is hard, the paper gives a recipe to parameterize a trial state for the Gibbs state and perform variational imaginary time evolution to approximate the desired state. Since this is quite computationally involved, we assume (as they do in [53]) that we have access to the perfect Gibbs state. It is therefore unclear whether the full algorithm can be expected to perform as well as our implementation.

For prediction, a diagonal $\pm 1$ valued observable $O$ is measured on a subset of qubits of size $n_{\text {vis }}$ (called the visible qubits, controlled by hyperparameter visible_qubits). The sign of the expectation value determines the label:

$$
\begin{equation*}
y_{\text {pred }}=\operatorname{sign}(\operatorname{tr}[\rho(\boldsymbol{\theta}, \boldsymbol{x}) O]) \tag{C9}
\end{equation*}
$$

where

$$
\begin{equation*}
\rho(\boldsymbol{\theta})=\frac{\exp \left(-\frac{H(\boldsymbol{\theta}, \boldsymbol{x})}{T}\right)}{\operatorname{tr}\left[\exp \left(-\frac{H(\boldsymbol{\theta}, \boldsymbol{x})}{T}\right)\right]} \tag{C10}
\end{equation*}
$$

is a Gibbs state, and we choose $O$ to be

$$
\begin{equation*}
O=\frac{1}{n_{\text {vis }}} \sum_{j=1}^{n_{\text {vis }}} Z_{j} \tag{C11}
\end{equation*}
$$

(note [53] does not recommend a general form for the observable).

Training is done with a binary cross entropy loss. To define the probability of $y=1$, we use:

$$
\begin{equation*}
P(y=1 \mid \boldsymbol{\theta}, \boldsymbol{x})=\frac{1+\langle O\rangle}{2} \tag{C12}
\end{equation*}
$$

which is in $[0,1]$ and agrees with the example in [53] for $n_{\text {vis }}=1$.

We note that since we are forced to work with mixed states, this implies a larger memory cost of simulation. As a result we were not able to test this model for as large qubit number as others. It is also the only model we implemented without the use of a PennyLane circuit, but rather by constructing the density matrix directly.

| hyperparameter values |
| :--- | :--- |
| learning_rate $\quad[0.001,0.01,0.1]$ |
| temperature $(\mathrm{T})[1,10,100]$ |
| visible_qubits $\quad[$ single, half, all $]$ |

## 6. QuantumBoltzmannMachineSeparable

This model is a version of QuantumBoltzmannMachine that does not use entanglement. Once again we take $n=$ $d$ so that the number of qubits scales with the number of features. The Hamiltonian is

$$
\begin{equation*}
H(\boldsymbol{\theta}, \boldsymbol{x})=\sum_{j} Z_{j} \boldsymbol{\theta}_{j}^{Z} \cdot \boldsymbol{x}+\sum_{j} X_{j} \boldsymbol{\theta}_{i}^{X} \cdot \boldsymbol{x} \tag{C13}
\end{equation*}
$$

whose corresponding Gibbs state is a product mixed state by virtue of the Hamiltonian being product. The model is equivalent to QuantumBoltzmannMachine otherwise and uses the same hyperparameter grid.

## 7. QuantumMetricLearner [56]

The QuantumMetricLearner works quite differently from other quantum neural networks. It uses a trainable, layer-wise embedding inspired by the QAOA algorithm implemented by PennyLane's QAOAEmbedding template, which employs one more qubit than there are features. The ansatz for one layer encodes input features into $\mathrm{X}$ rotations, followed by parametrised $\mathrm{ZZ}$ and $\mathrm{Y}$ rotations. The additional qubit with a constant angle is used as a "latent feature".

Training of the embedding is performed by measuring the overlap between a pair of embedded data points from the same [different] classes and minimising [maximising] their fidelity.

More precisely, if $\left|\phi_{\theta}(\mathbf{x})\right\rangle$ is the quantum state embedding an input vector $\mathbf{x}$, and $A A, B B[A B]$ are sets of randomly sampled training input pairs $\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$ from training data in the same class $A$ or $B$ [different classes], the cost function is defined as

$$
\begin{align*}
c(A, B)= & 1-\sum_{(\mathbf{a}, \mathbf{b}) \in A B}\left|\left\langle\phi_{\boldsymbol{\theta}}(\mathbf{a}) \mid \phi_{\boldsymbol{\theta}}(\mathbf{b})\right\rangle\right|^{2} \\
& +0.5 \sum_{\left(\mathbf{a}, \mathbf{a}^{\prime}\right) \in A A}\left|\left\langle\phi_{\boldsymbol{\theta}}(\mathbf{a}) \mid \phi_{\boldsymbol{\theta}}\left(\mathbf{a}^{\prime}\right)\right\rangle\right|^{2} \\
& +0.5 \sum_{\left(\mathbf{b}, \mathbf{b}^{\prime}\right) \in B B}\left|\left\langle\phi_{\boldsymbol{\theta}}(\mathbf{b}) \mid \phi_{\boldsymbol{\theta}}\left(\mathbf{b}^{\prime}\right)\right\rangle\right|^{2} \tag{C14}
\end{align*}
$$

Note that in the original paper, all possible pairs of datapoints within a random batch are compared, however to have a better control of how many circuits are run we sample a batch of random pairs instead.

Once trained, the embedding is directly used for prediction: A new input is embedded and the resulting state compared to a random batch of embedded training data points $A^{\prime}$ and $B^{\prime}$ from each class. The class that it is closest to on average is assigned. This rule corresponds to the "fidelity classifier" from the paper:

$$
\begin{equation*}
f(\boldsymbol{\theta}, \mathbf{x})=\sum_{\mathbf{a} \in A^{\prime}}\left|\left\langle\phi_{\boldsymbol{\theta}}(\mathbf{a}) \mid \phi_{\boldsymbol{\theta}}(\mathbf{x})\right\rangle\right|^{2}-\sum_{\mathbf{b} \in B^{\prime}}\left|\left\langle\phi_{\boldsymbol{\theta}}(\mathbf{b}) \mid \phi_{\boldsymbol{\theta}}(\mathbf{x})\right\rangle\right|^{2} \tag{C15}
\end{equation*}
$$

The final prediction is made by taking the sign,

$$
\begin{equation*}
y_{\text {pred }}=\operatorname{sign} f(\boldsymbol{\theta}, \mathbf{x}) \tag{C16}
\end{equation*}
$$

The following is the quantum circuit used to evaluate overlaps in the QuantumMetricLearner for two inputs $\mathbf{x}, \mathbf{x}^{\prime} \in \mathbb{R}^{4}$ :

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-28.jpg?height=380&width=317&top_left_y=1729&top_left_x=1359)

The number of circuits run for each training step and prediction scales linearly with the size of the batch of example pairs used - in this paper we fixed 32 for both. Larger batch sizes allow a more reliable estimate of the cost and predicted label.

| hyperparameter | values |
| :--- | :--- |
| learning_rate | $[0.001,0.01,0.1]$ |
| n_layers (embedding layers) | $[1,3,4]$ |

## 8. TreeTensorClassifier [19]

This model was designed to avoid the phenomenon of barren plateaus. We implement the 'tree tensor' structure shown in Figure 1 of [19]. The variational circuit in this model has a tree-like structure and therefore requires a number of qubits that is a power of 2. In [19] one first optimizes a variational circuit that finds a state that approximates an amplitude-encoded data state. The reason for this is to make the algorithm more efficient; here, to avoid an additional variational optimisation, we assume direct access to the exact amplitude encoded state $V(\boldsymbol{x})|0\rangle$ (padding with constant values $1 / 2^{n}$ with $n$ the number of qubits when necessary).

This state is then fed into the variational circuit $U(\boldsymbol{\theta})$ consisting of trainable single-qubit $Y$ rotations and CNOTs. The tree structure means that there are few parameters; for a circuit with $n$ qubits one has only $2 n-1$ parameters.

Prediction is given by measurement of $Z$ on the first qubit,

$$
\begin{equation*}
y_{\text {pred }}=\operatorname{sign} f(\boldsymbol{\theta}, \boldsymbol{x}) \tag{C17}
\end{equation*}
$$

where

$$
\begin{equation*}
f(\boldsymbol{\theta}, \boldsymbol{x})=\left\langle 0\left|V^{\dagger}(\boldsymbol{x}) U^{\dagger}(\boldsymbol{\theta}) Z_{1} U(\boldsymbol{\theta}) V(\boldsymbol{x})\right| 0\right\rangle \tag{C18}
\end{equation*}
$$

and training is via the square loss:

$$
\begin{equation*}
\ell(\boldsymbol{\theta}, \boldsymbol{x}, y)=(f(\boldsymbol{\theta}, \boldsymbol{x})-y)^{2} \tag{C19}
\end{equation*}
$$

The following is an example of the quantum circuit used in the TreeTensorClassifier for a 4-dimensional input encoded into state $\left|\psi_{\mathbf{x}}\right\rangle$ :

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-29.jpg?height=162&width=483&top_left_y=1651&top_left_x=363)

This model has few suggested hyperparameters, so we vary only the learning rate.

hyperparameter values

learning_rate $\quad[0.001,0.01,0.1]$

## 9. IQPKernelClassifier [52]

This model is a kernel equivalent of the IQP variational model. The embedding $V(\boldsymbol{x})|0\rangle$ is the same IQPinspired embedding given by PennyLane's IQPEmbedding template. This defines a kernel

$$
\begin{equation*}
k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\operatorname{tr}\left[\rho(\boldsymbol{x}) \rho\left(\boldsymbol{x}^{\prime}\right)\right]=\left|\left\langle 0\left|V^{\dagger}(\boldsymbol{x}) V\left(\boldsymbol{x}^{\prime}\right)\right| 0\right\rangle\right|^{2} \tag{C20}
\end{equation*}
$$

which we evaluate by applying the unitary $V^{\dagger}(\boldsymbol{x}) V\left(\boldsymbol{x}^{\prime}\right)$ to an input state $|0\rangle$ and calculating the probability to measure $|0\rangle$.

The following is an example of the quantum circuit used in the IQPKernelClassifier for two inputs $\mathbf{x}, \mathbf{x} \in$ $\mathbb{R}^{4}$ :

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-29.jpg?height=372&width=393&top_left_y=489&top_left_x=1321)

The kernel matrix $K$ is fed to scikit-learn's SVC class, which trains a support vector machine classifier. Prediction is given by

$$
\begin{equation*}
y_{\text {pred }}=\operatorname{sign} \sum_{i} \alpha_{i} k\left(\boldsymbol{x}_{i}, \boldsymbol{x}\right) \tag{C21}
\end{equation*}
$$

with $\alpha_{i}$ the weights of the support vector machine.

The hyperparameter search values are as follows:

| hyperparameter | values |
| :--- | :--- |
| repeats (embedding layers) | $[1,5,10]$ |
| $C$ (SVC regularization) | $[0.1,1,10,100]$ |

## 10. ProjectedQuantumKernel [54]

This model is a kernel method that uses a Hamiltonianinspired data embedding that resembles a Trotter evolution of a 1D-Heisenberg model with random couplings. This consists of applying random single-qubit rotations to an input state $|0\rangle$ of $n=d+1$ qubits, followed by $L$ layers of two-qubit rotations with generators $X X, Y Y$, $Z Z$ to pairs of adjacent qubits, with angles given by the elements of $\boldsymbol{x}$ :

$$
\begin{equation*}
\prod_{j=1}^{n} \exp \left(-\mathrm{i} \frac{t}{L} x_{j}\left(X_{j} X_{j+1}+Y_{j} Y_{j+1}+Z_{j} Z_{j+1}\right)\right) \tag{C22}
\end{equation*}
$$

where $t$ is a hyperparameter of the model. Writing the embedded states as $\rho\left(\boldsymbol{x}_{i}\right)$, the kernel function $k\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)$ is

$$
\begin{equation*}
\exp \left(-\gamma \sum_{k=1}^{n} \sum_{P \in\{X, Y, Z\}}\left(\operatorname{tr}\left[P \rho_{k}\left(\boldsymbol{x}_{i}\right)\right]-\operatorname{tr}\left[P \rho_{k}\left(\boldsymbol{x}_{j}\right)\right]\right)^{2}\right) \tag{C23}
\end{equation*}
$$

where $\rho_{k}$ is the reduced state of $k^{\text {th }}$ qubit of $\rho$. This is simply the RBF kernel with bandwidth $\gamma$ applied to feature mapped vectors $\boldsymbol{\phi}(\boldsymbol{x})$ with elements $\left(\operatorname{tr}\left[P \rho_{k}(\boldsymbol{x})\right]\right)$.

The following is an example of the quantum circuit used to compute feature vectors from measurements in ProjectedQuantumKernel for the input $\mathbf{x}=$ $(0.1,0.2,0.3,0.4)^{T}$ :
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-30.jpg?height=778&width=814&top_left_y=558&top_left_x=206)

According to [54], the default $\gamma$ value is set as

$$
\begin{equation*}
\gamma_{0}=\frac{1}{\operatorname{Var}(\boldsymbol{\phi}) d} \tag{C24}
\end{equation*}
$$

where $\operatorname{Var}(\boldsymbol{\phi})$ is the variance of the elements of all the vectors $\boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)$. We include another hyperparameter 'gamma_factor' that scales this default value.

A support vector machine is then trained using scikitlearn's SVC class, and prediction is given by

$$
\begin{equation*}
y_{\text {pred }}=\operatorname{sign} \sum_{i} \alpha_{i} k\left(\boldsymbol{x}_{i}, \boldsymbol{x}\right) \tag{C25}
\end{equation*}
$$

| hyperparameter | values |
| :--- | :--- |
| trotter_steps(embedding layers) | $L[1,3,5]$ |
| $C$ (SVC regularization) | $[0.1,1,10,100]$ |
| $t$ (time) | $[0.01,0.1,1.0]$ |
| gamma_factor | $[0.1,1,10]$ |

## 11. QuantumKitchenSinks [55]

This model uses a quantum circuit to define a feature map given by the concatenation of its output bit-strings. These features are then used to train a linear classifier. The feature map procedure works as follows:

- Linearly transform the input feature vector $\boldsymbol{x}$ as $\boldsymbol{x}_{k}^{\prime}=W_{k} \boldsymbol{x}+\boldsymbol{b}_{k}$ using randomly sampled $W_{k}, \boldsymbol{b}_{k}$ for $k=1, \cdots, k_{\max }$. Here $W_{k}, \boldsymbol{b}_{k}$ are such that $\boldsymbol{x}^{\prime}$ has dimension $n$ which may be different from $d$.
- Feed each $\boldsymbol{x}_{k}^{\prime}$ into a circuit (described below) that returns a single measurement sample $\boldsymbol{z}_{k} \in\{0,1\}^{n}$. The concatenated vector $\boldsymbol{z}_{1} \oplus \cdots \oplus \boldsymbol{z}_{k_{\max }}$ is the feature mapped vector of size $n \cdot k_{\max }$.

The circuit used in the second step above consists of angle encoding the feature mapped vectors with $X$ rotations on individual qubits, and applying two layers of CNOT gates: the first between adjacent qubits $(j, j+1)$; the second between qubits $(j, j+2)$ a distance 2 apart. This choice is a natural generalisation of the example present in [55], which does not present a specific circuit structure for circuits beyond 4 qubits.

The following is an example of the quantum circuit used to compute feature vectors QuantumKitchenSinks for the input $\mathbf{x}=(0.1,0.2,0.3,0.4)^{T}$ (here without first applying the classical linear transformation):

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-30.jpg?height=306&width=480&top_left_y=1105&top_left_x=1275)

The feature mapped vectors are then passed to a linear classifier, which we implement using scikit-learn's LogisticRegression class (note logistic regression can be used for linear classification via the cross entropy loss).

For prediction, a new feature-mapped vector $\boldsymbol{z}$ is created via the same process, and

$$
\begin{equation*}
y_{\text {pred }}=\operatorname{sign} \boldsymbol{w} \cdot \boldsymbol{z} \tag{C26}
\end{equation*}
$$

where $\boldsymbol{w}$ is the linear classifier found during training.

The hyperparameter search values are as follows:

| hyperparameter | values |
| :--- | :--- |
| $n_{-}$qfeatures (circuit size) | $[d,\lfloor d / 2\rfloor]$ |
| $n_{-}$episodes (number of circuits $\left.k_{\max }\right)$ | $[10,100,500,2000]$ |

## 12. QuanvolutionalNeuralNetwork [15]

This model consists of a fixed quantum feature map followed by a trainable convolutional neural network. The data is first scaled to lie in the range $[-1,1]$ and a binary threshold function (with threshold zero) is applied and the data scaled by $\pi$ so that it takes values in $\{0, \pi\}$. One then applies a convolutional layer to the data, where
the convolutional filters are given by a $n_{q}^{2}$ qubit quantum circuits that take as input $n_{q} \times n_{q}$ sized windows of the input data, in an analogous manner to a classical convolutional filter. The quantum circuits consists of random gates that we implement via PennyLane's RandomLayers class, and the output of the circuits is the number of ones in the bitstring that has the highest probability to be sampled from the circuit. As with convolutional neural networks, we allow for more than one channel in this layer, controlled by the hyperparameter n_qchannels.

The following is an example of the quantum circuit used in the QuanvolutionalNeuralNetwork for a $2 \times 2$ dimensional input window, where the thresholded and rescaled pixel values form a pre-processed input vector $(0,0, \pi, \pi)^{T}$ :

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-31.jpg?height=371&width=380&top_left_y=861&top_left_x=409)

The resulting data is then fed into a convolutional neural network with the same specifications as for the ConvolutionalNeuralNetwork.

| hyperparameter | values |
| :--- | :--- |
| learning_rate | $[0.0001,0.001,0.01]$ |
| n_qchannels | $[1,5,10]$ |
| qkernel_shape $\left(n_{q}\right)$ | $[2,3]$ |
| kernel_shape (CNN filter size) | $[2,3,5]$ |

## 13. WeiNet $[20]$

This model (that we call WeiNet following the first author of the paper) implements a convolutional layer as a unitary operation that acts on input data that is amplitude encoded into a quantum state. The model has two registers: the ancilliary register and the work register. The ancilliary register is used to parameterise a 4 qubit state which in turn controls a number of unitaries that act on the work register, where the data is encoded via amplitude encoding. Note that in figure 2 of [20], the Hadamard gates on the ancilla register have no effect since we trace this register out. The effect of this register is then to simply perform a classical mixture of the unitaries $Q_{i}$ defined therein on the work register. For simplicity (and to save qubit numbers), we parameterise this distribution via 16 real trainable parameters.

Two of the qubits are then traced out, which is equivalent to a type of pooling. All single and double correlators $\langle Z\rangle$ and $\langle Z Z\rangle$ are measured, and a linear model on these values is used for classification.

The following is an example of one of the quantum circuits used in the WeiNet model for a $4 \times 4$-dimensional input window encoded into state $\left|\psi_{\mathbf{x}}\right\rangle$ acted on by "filter unitary" $U$ :

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-31.jpg?height=363&width=393&top_left_y=515&top_left_x=1321)

The paper does not specify the training loss; like the other convolutional models we use the binary cross entropy with a sigmoid (logistic) activation. The corresponding probabilities from the sigmoid activation are used for prediction.

| hyperparameter values |  |
| :--- | :--- |
| learning_rate | $[0.0001,0.001,0.01]$ |
| filter_type | [edge_detect, smooth, sharpen $]$ |

## 14. MLPClassifier

This is a multiplayer perception model implemented via scikit-learn's MLPClassifier class. An input feature vector $\boldsymbol{x}$ is transformed using a sequence of linear transformations $W_{l}$ and element wise-activation functions $a$ as

$$
\begin{equation*}
f(\boldsymbol{x})=a\left(W_{L} \cdots a\left(W_{2}\left(a\left(W_{1} \boldsymbol{x}\right)\right)\right) \cdots\right) \tag{C27}
\end{equation*}
$$

'We use the rectified linear unit activation $a(x)=\max (0, x)$. The trainable parameters of the model are the weights of the matrices $W_{k}$. We vary the number of layers $L$ and the shapes of the weight matrices $W_{k}$ via the model parameter hidden_layer_size, which we set to be one of $[(100),,(10,10,10,10),(50,10,5)]$. Here, each element of the tuple corresponds to a different layer and the values give the output dimensions of the corresponding matrices. The last layer is not included here since it is always of dimension 1. Training is done via gradient descent with the binary cross entropy loss using the adam update and the default class fit method. We vary the initial learning rate and regularisation strength alpha. All other parameters are set to the class defaults, except the maximum number of iterations, max_iter, that we set to 3000 .

| hyperparameter | values |
| :--- | :--- |
| learning_rate | $[0.001,0.01,0.1]$ |
| hidden_layer_sizes | $[(100),,(10,10,10,10),(50,10,5)]$ |
| alpha (regularisation) | $[0.01,0.001,0.0001]$ |

15. SVC

This model is a support vector machine classifier implemented via scikit-learn's SVC class. We use the radial basis function kernel:

$$
\begin{equation*}
k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\exp \left(-g a m m a\left\|x-x^{\prime}\right\|^{2}\right) \tag{C28}
\end{equation*}
$$

During hyperparameter search, we vary the bandwidth parameter gamma and the regularisation strength $C$.

| hyperparameter | values |
| :--- | :--- |
| $C($ SVC regularization $)$ | $[0.1,1,10,100]$ |
| gamma | $[0.001,0.01,0.1,1]$ |

## 16. ConvolutionalNeuralNetwork

This model is a vanilla implementation of a convolutional neural network (CNN), written in flax. The structure of the network is as follows

- a 2D convolutional layer with 32 output channels
- a max pool layer
- a 2D convolutional layer with 64 output channels
- a max pool layer
- a two layer fully connected feedforward neural network with 128 hidden neurons and one output neuron

The probability of class 1 is given by

$$
\begin{equation*}
P(+1 \mid \boldsymbol{w}, \boldsymbol{x})=\sigma(f(\boldsymbol{w}), \boldsymbol{x}) \tag{C29}
\end{equation*}
$$

where $\boldsymbol{w}$ are the weights of the model, $f(\boldsymbol{w})$ is the value of the final neuron, and $\sigma$ is the logistic function. These probabilities are fed to binary cross entropy loss for training.

| hyperparameter values |  |
| :--- | :--- |
| learning_rate | $[0.0001,0.001,0.01]$ |
| kernel_shape | $[2,3,5]$ |

## 17. SeparableVariationalClassifier

This is a simple quantum neural network model that does not use entanglement. The data encoding $V$ ( consists of $L$ layers, where in each layer arbitrary trainable single-qubit rotations are performed followed by a product angle embedding of the data via Pauli Y rotation gates. The encoding is proceeded by another layer of trainable single-qubit rotations, and prediction is given by measurement of $O=\frac{1}{n}\left(Z_{1}+\cdots+Z_{n}\right)$, i.e.

$$
\begin{equation*}
y_{\text {pred }}=\operatorname{sign} f(\boldsymbol{\theta}, \boldsymbol{x}) \tag{C30}
\end{equation*}
$$

where

$$
\begin{equation*}
f(\boldsymbol{\theta}, \boldsymbol{x})=\left\langle\frac{1}{n}\left(Z_{1}+\cdots+Z_{n}\right)\right\rangle \tag{C31}
\end{equation*}
$$

where $\boldsymbol{\theta}$ represents all trainable parameters in the $L$ layers.

The following is an example of the single-qubit quantum circuit used in the SeparableVariationalClassifier model to process the first value of the input vector $\mathbf{x}=(0.1,0.2,0.3,0.4)^{T}$ (whereas the remaining features are processed by similar circuits):

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-32.jpg?height=95&width=463&top_left_y=939&top_left_x=1281)

To train the model, we define class probabilities via the logistic function

$$
\begin{equation*}
P(+1 \mid \boldsymbol{\theta}, \boldsymbol{x})=\sigma(6\langle O\rangle) \tag{C32}
\end{equation*}
$$

where we multiply the observable value by 6 since the sigmoid function varies significantly over the range $[-6,6]$. These probabilities are then used in a binary cross entropy loss function.

| hyperparameter | values |
| :--- | :--- |
| learning_rate | $[0.001,0.01,0.1]$ |
| encoding_layers $(L)$ | $[1,3,5,10]$ |

## 18. SeparableKernelClassifier

This model is the kernel equivalent of the above. The data encoding consists $L$ layers, where in each layer an $X$ rotation with angle $\pi / 4$ is applied to each qubit followed by a product of $Y$ rotations that encode each element of $\boldsymbol{x}$ into individual qubits (so $n=d$ ).

The kernel is given by (6):

$$
\begin{equation*}
k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\operatorname{tr}\left[\rho(\boldsymbol{x}) \rho\left(\boldsymbol{x}^{\prime}\right)\right] \tag{C33}
\end{equation*}
$$

and the model is trained using scikit-learn's SVC class.

The following is an example of the single-qubit quantum circuit used in the SeparableKernelClassifier model to process the first value of the inputs $\mathbf{x}, \mathbf{x}^{\prime}=$ $(0.1,0.2,0.3,0.4)^{T}$, using 2 layers:

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-32.jpg?height=79&width=802&top_left_y=2389&top_left_x=1125)
hyperparameter values

encoding layers $(L)[1,3,5,10]$

SVM regularization $[0.1,1,10,100]$

## Appendix D: Convergence criteria of variational models

Here we describe the criterion used to decide convergence during training of variational models. This was used for all quantum neural network and quantum convolutional neural network models, as well as the vanilla convolutional neural network model. Convergence is decided as follows:

- During training, the previous 400 values of the loss are stored, and at each step after step 400, the sample mean $\mu_{1}$ of the loss values for the first 200 and the sample mean $\mu_{2}$ of the loss values of last 200 steps is calculated, as well as the standard deviation $\sigma_{2}$ of the last 200 loss values.
- If the model has converged, the statistics of the loss values in the two intervals should be approximately the same, so that the statistics of the sample means $\mu_{1}$ and $\mu_{2}$ are the same. Under an assumption of normality, the standard deviation of $\mu_{2}$ is $\sigma_{2} / \sqrt{200}$, and so for a converged model one expects

$$
\begin{equation*}
\left|\mu_{1}-\mu_{2}\right|<\frac{\sigma_{2}}{\sqrt{200}} \tag{D1}
\end{equation*}
$$

with reasonable probability, whereas a model that is still training and whose loss decreases significantly in 200 steps is unlikely to satisfy the criterion.

- We adopted a slightly smaller interval than this to decrease the likelyhood of accidental early stopping. Namely, we used the criterion

$$
\begin{equation*}
\left|\mu_{1}-\mu_{2}\right|<\frac{\sigma_{2}}{2 \sqrt{200}} \tag{D2}
\end{equation*}
$$

to decide convergence. From visual inspection of loss plots, we found that this always gave a reasonable stopping criterion that corresponded to an approximately flat loss curve over the last 200 steps.

## Appendix E: Details on the datasets used in this study

We used 6 types of data generation procedures, five of which are purely synthetic, while one (MNIST) uses postprocessing to reduce the dimension of the feature vectors. The data generation procedures provide different variables that can be tuned to create individual datasets. We refer to datasets for which one such variable is tuned as "benchmarks" and denote them by capital spelling.
This section describes the procedures in more detail, and gives the settings for generating the benchmarks. The code can be found in the github repository.

Note that unless otherwise stated, data was split into training and test sets using a ratio of test/train $=0.2$.

## 1. Linearly separable

The linearly separable data generation procedure creates data by a perceptron with fixed weights that labels input vectors sampled from a hypercube. The data is hence guaranteed to be linearly separable, and the dataset can be considered as the easiest or "fruit-fly" task in classification.

## Procedure:

1. Sample input vectors $\mathbf{x} \in \mathbb{R}^{d}$ uniformly from a $d$-dimensional hypercube spanning the interval $[-1,1]$ in each dimension.
2. Pick a weight vector for some $\mathbf{w} \in \mathbb{R}^{d}$ to define the linear decision boundary at $\mathbf{w} \mathbf{x}=0$. Only retain the first $N$ input vectors that do not lie within a margin of size $0.02 d$ around the decision boundary, or $\|\mathbf{w x}\|>0.02 d$.
3. Generate continuous-valued labels $y^{\prime}=\mathbf{w x}$.
4. Binarise the labels via

$$
y=\left\{\begin{aligned}
1 & \text { if } y^{\prime}-y_{\mathrm{med}}>0 \\
-1 & \text { else }
\end{aligned}\right.
$$

where $y_{\text {med }}$ is the median of all continuous labels. This standarisation procedure ensures that the classes are balanced.

## Settings for the LINEARLY SEPARABLE benchmark:

- Number of features $d \in\{2, \ldots, 20\}$
- Number of samples $N=300$
- Perceptron weights $\mathbf{w}=(1, \ldots, 1)^{T}$


## 2. Bars and stripes

The bars and stripes generation procedure is intended to be a simple task for the three convolutional models. It creates gray-scale images of either vertical bars or horizontal stripe on a 2D pixel grid.

Procedure:

1. Sample $N$ labels $y_{i}=-1,1$ uniformly at random.
2. For each $y_{i}$ create a pixel grid of shape $d \times d$ that will store the data $\boldsymbol{x}_{i}$. If $y_{i}=-1$, for each column, sample a random variable taking values $\pm 1$ with equal probability, and fill the column of $\boldsymbol{x}_{i}$ with this value. If $y_{i}=1$, do the same for the rows.
3. For each image $\boldsymbol{x}_{i}$ add independent Gaussian noise with standard deviation $\sigma$ and mean 0 .

Settings for the BARS \& STRIPES benchmark:

- Image width $d=4,8,16,32$.
- noise standard deviation $\sigma=0.5$
- Number of data samples $N=1000$


## 3. Downscaled MNIST

The MNIST datasets are based on the famous handwritten digits data [43] using digits 3 and 5 which are amongst the hardest to distinguish. The ratio between test and training set for this procedure is test/train $=0.17$. The original data was processed by different methods for dimensionality reduction.

The MNIST PCA, MNIST PCA- benchmarks use principal component analysis (PCA) to reduce dimensions.

## Procedure:

1. Flatten and standarise the inputs images, which is important for PCA to work well. The standarisation parameters are derived from the training set, and then used to standarise the test set. ${ }^{20}$
2. Compute the $d$ largest principal components of the pre-processed training set inputs via Principal Component Analysis.
3. Project the training and test set inputs onto those components to gain new input vectors of dimension d. This is the MNIST PCA dataset.
4. For the MNIST $\mathrm{PCA}^{-}$dataset, we sampled a subset of 250 data points for each of the training and test set from MNIST PCA.

## Settings for the MNIST PCA benchmark:

- Number of features $d \in\{2, \ldots, 20\}$

Settings for the MNIST PCA- benchmark:
- Number of features $d \in\{2, \ldots, 20\}$
- Number of samples $N=250$ for training and test set each.[^13]

The MNIST CG benchmark coarse-grains the pixels of the original images to try and preserve the correlation structure used by convolutional neural networks. As before, we use the digits 3 and 5 only.

## Procedure:

1. Resize the original 28 x28 pixel data to a pixel grid of size $H \times H$, using bilinear interpolation.
2. Flatten and standardize the images.

Settings for the MNIST CG benchmark:

- Pixel grid height/width $H \in\{4,8,16,32\}$.


## 4. Hidden manifold model

This data generation procedure is based on Goldt et al. [79], who classify data sampled from a $m$-dimensional manifold by a neural network, and then embed the data into a $d$-dimensional space. The properties of this data generation process allow the authors to compute analytical generalisation error dynamics using tools from statistical physics. The structure intends to mimic datasets used in image recognition (such as MNIST), which have been shown to effectively use low-dimensional manifolds.

Procedure:

1. Randomly sample $N$ feature vectors $\mathbf{c}^{m} \in \mathbb{R}^{m}$ with entries from a standard normal distribution. These vectors lie on the "hidden manifold".
2. Create an embedding matrix $F \in \mathbf{R}^{d \times m}$.
3. Embed the feature vectors via

$$
\begin{array}{r}
\mathbf{x}=\phi(\mathbf{F} \mathbf{c} / \sqrt{m}) \\
\text { where } \phi_{i}(\mathbf{x})=\tanh \left(x_{i}-b_{i}\right)
\end{array}
$$

4. Generate continuous-valued labels using a neural network applied to the vectors on the manifold

$$
y^{\prime}=\mathbf{v}^{T} \varphi(\mathbf{W} \mathbf{c} / \sqrt{m})
$$

using component-wise tanh functions as the activation $\varphi$, and the entries in $W \in \mathbb{R}^{m, m}, \mathbf{v} \in \mathbb{R}^{m}$ sampled from a standard distribution.

5. In order to get balanced classes, we rescale the data by subtracting the median $y_{\text {med }}$ of all labels and then apply a thresholding function

$$
y=\left\{\begin{array}{r}
1 \text { if } y^{\prime}-y_{\mathrm{med}}>0 \\
-1 \text { else }
\end{array}\right.
$$

Settings for the HIDDEN MANIFOLD benchmark:

- Number of features $d \in\{2, \ldots, 20\}$
- Number of samples $N=300$
- Manifold dimension $m=6$
- Entries of feature matrix $F$ sampled from a standard distribution

Settings for the HIDDEN MANIFOLD DIFF benchmark:

- Number of features $d=10$
- Number of samples $N=300$
- Manifold dimension $m \in\{2, \ldots, 20\}$
- Entries of feature matrix $F$ sampled from a standard distribution


## 5. Two curves

This data generation procedure is inspired by Buchanan et al. [80], who consider data sampled from two curves - one for each class - embedded into a $d$ dimensional space to prove that the maximum curvature and minimum distance of these curves determine the resources required by a neural network to generalise well. We can hence understand curvature and distance as two variables that influence the difficulty of the data.

To control the curvature we use a one-dimensional Fourier series of a maximum degree $D$ in each dimension. To control the average distance of the curves we use the same embedding, but shift one curve by some constant.

## Procedure:

1. Sample $N$ values $t \in \mathbb{R}$ uniformly at random from the interval $[0,1]$. (This value defines the position of a data point on the curve we embed.)
2. To create the inputs for class 1 , embed half of the $t$-values into a $d$-dimensional space via a Fourier series defined in every dimension,

$$
x_{i}=\sum_{n=0}^{D} \alpha_{n}^{i} \cos (n t)+\beta_{n}^{i} \sin (n t)+\epsilon
$$

where $D$ is the maximum degree of the Fourier series and $\left\{\alpha_{n}^{i}\right\},\left\{\beta_{n}^{i}\right\}$ are real-valued Fourier coefficients that we sample uniformly from the interval $[0,1]$. The noise factor $\epsilon$ determines the variance of a random "spread" added to the curves around their trajectory in the high-dimensional space.

3. To create the inputs for class -1 , embed the other half of the $t$-values using the same procedure and Fourier coefficients, but adding an offset of $\Delta$ to each dimension.
Settings for the TWO CURVES benchmark:

- Number of features $d \in\{2, \ldots, 20\}$
- Number of samples $N=300$
- Noise factor $\epsilon=0.01$
- Maximum degree $D=5$
- Curve offset $\Delta=0.1$

Settings for the TWO CURVES DIFF benchmark:

- Number of features $d=10$
- Number of samples $N=300$
- Noise factor $\epsilon=0.01$
- Maximum degree $D \in\{2, \ldots, 20\}$
- Curve offset $\Delta=\frac{1}{2 D}$


## 6. Hyperplanes and parity

We created an artificial dataset that classifies lowdimensional feature vectors by whether they lie on the "positive" side of an even or odd number of a set of $k$ hyperplanes. The feature vectors are then embedded into a higher-dimensional space via a linear transform. The result is a division of the space into regions of different classes that are delineated by hyperplane intersections. The parity operation makes sure that a model implicitly has to learn all hyperplane positions to guess the right label. The difficulty of the classification problem is expected to increase with the number of hyperplanes.

## Procedure:

1. Sample $N$ feature vectors $\mathbf{c} \in \mathbb{R}^{m}$ from a standard normal distribution.
2. Embed each feature vector into $\mathbb{R}^{d}$ by multiplying the feature vectors with a matrix $\mathbf{M} \in \mathbb{R}^{d \times m}$,

$$
\mathbf{x}=\mathbf{M c}
$$

3. Compute $k$ predictions for the $m$-dimensional feature vectors via

$$
p^{(j)}=\left\{\begin{array}{l}
1 \text { if } \mathbf{w}^{(j)} \mathbf{c}+b^{(j)}>0 \\
-1 \text { else }
\end{array} \quad j=1, \ldots, k\right.
$$

using uniformly sampled weight vectors $\left\{\mathbf{w}^{(j)} \in\right.$ $\left.\mathbb{R}^{m}\right\}$ and biases $\left\{b^{(j)}\right\}$.

4. The final label is defined as the parity of these predictions, or whether the number of 1-predictions is even:

$$
y=\left\{\begin{aligned}
& 1 \text { if } \sum_{j=1}^{k} \frac{p^{(j)}+1}{2} \text { even } \\
&-1 \text { else }
\end{aligned}\right.
$$

5. To ensure balanced classes we initially sample a larger number of datapoints from which we subsample the desired number for each class.
6. Standarise the inputs.

Settings for the HYPERPLANES DIFF benchmark:

- Dimension $d=10$
- Number of hyperplanes $k \in\{2, \ldots, 20\}$
- Number of data samples $N=1000$
- Dimension of hyperplane and initial feature vectors $m=3$
- Entries of $\mathbf{M}$ are uniformly sampled from $[0,1]$


## Appendix F: Collection of detailed results

We add the ranking and accuracy plots of all benchmarks here for readers who are interested in the detailed results.

## LINEARLY SEPARABLE

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-37.jpg?height=1450&width=1412&top_left_y=251&top_left_x=365)

FIG. 20. Ranking plots like shown in Figure 10 for selected benchmarks.

## HIDDEN MANIFOLD

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-38.jpg?height=618&width=680&top_left_y=249&top_left_x=365)

TWO CURVES

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-38.jpg?height=615&width=683&top_left_y=950&top_left_x=366)

HIDDEN MANIFOLD DIFF

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-38.jpg?height=612&width=680&top_left_y=255&top_left_x=1080)

TWO CURVES DIFF

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-38.jpg?height=615&width=680&top_left_y=950&top_left_x=1080)

HYPERPLANES DIFF

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-38.jpg?height=615&width=680&top_left_y=1655&top_left_x=728)

FIG. 21. Ranking plots like shown in Figure 10 for selected benchmarks (continued).
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-39.jpg?height=2054&width=1724&top_left_y=198&top_left_x=211)

$\rightarrow$ CircuitCentricClassifier

-- DataReuploadingClassifier

$\simeq$ DressedQuantumCircuitClassifier

$\rightarrow$ IQPVariationalClassifier

$\rightarrow$ QuantumMetricLearner

$\rightarrow$ - QuantumBoltzmannMachine

- SVC

$\longrightarrow$ TreeTensorClassifier

$\rightarrow$ IQPKernelClassifier

$\rightarrow$ ProjectedQuantumKernel

$\rightarrow$ QuantumKitchenSinks

FIG. 22. Detailed training and test accuracies for the benchmarks not shown in Figure 11.

HIDDEN MANIFOLD
![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-40.jpg?height=426&width=848&top_left_y=226&top_left_x=194)

HIDDEN MANIFOLD DIFF

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-40.jpg?height=352&width=414&top_left_y=667&top_left_x=194)

TWO CURVES

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-40.jpg?height=355&width=418&top_left_y=1086&top_left_x=192)

TWO CURVES DIFF

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-40.jpg?height=355&width=414&top_left_y=1511&top_left_x=194)

- MLPClassifier

$\sim$ CircuitCentricClassifier

-- DataReuploadingClassifier

- DressedQuantumCircuitClassifier

$\rightarrow$ IQPVariationalClassifier

-- QuantumBoltzmannMachine

TreeTensorClassifier

$\rightarrow$ QuantumKitchenSinks

$\longrightarrow$ SeparableVariationalClassifier $\_$SeparableKernelClassifier

FIG. 23. Results for separable models for the benchmarks not shown in Figure 12.

![](https://cdn.mathpix.com/cropped/2024_06_04_e53d13f86f8cf396c755g-41.jpg?height=734&width=854&top_left_y=178&top_left_x=191)

FIG. 24. Total ranking results as reported in Figure 10, but with the two separable models included.


[^0]:    * joseph@xanadu.ai

    $\dagger$ shahnawaz.ahmed95@gmail.com

    $\ddagger$ maria@xanadu.ai

    ${ }^{1}$ It is standard practice in the field of quantum computing to publish articles on the arXiv server, and we therefore expect it to provide representative samples of the literature. The 55 papers are a subset of 73 papers returned by the keyword search, and were selected by manually reading the abstracts and discarding papers that related to quantum-inspired methods, other fields than quantum machine learning, or did not explicitly mention a method outperforming another.

[^1]:    2 Other search terms than "outperform*", such as "better than" or "benchmark*", were tried to detect a possible selection bias caused by the keyword search, but showed similar patterns.

[^2]:    ${ }^{3}$ We ran our simulation on the Digital Research Alliance of Canada's Cedar supercomputer and limited runtimes to at most 24 hours for one 5 -fold cross-validation (i.e., training the model five times) using a 10-core cluster with 40GB RAM. The largest size of a circuit simulated in the study has 18 qubits, a number that we are determined to push further going forward.

[^3]:    ${ }^{4}$ Calls for more methodological rigour also led to the inception of datasets \& benchmarking tracks at leading conferences like NeurIPS.

    5 This raises once more the question whether existence proofs for quantum advantages in learning brings us any closer to useful quantum machine learning.

    6 Although, reassuringly, the ranking between models is largely unaffected by this intervention.

[^4]:    7 These were sampled out of 46 papers that we identified to use MNIST for supervised learning benchmarks listed in the arxiv 'quant-ph' category between January 2018 and June 2023. The papers were in turn selected from a total of over 100 papers in the 'quant-ph' category on the arXiv in the same period that mention MNIST in title or abstract.

    8 A notable exception are papers based on so-called "quanvolutional" architectures, where the first layer uses a small "filter" or "kernel" composed of a quantum circuit [15].

[^5]:    9 The choice of square and linear loss in some models is curious, since it can be argued that a more natural choice for a classification loss is the cross entropy loss, as it corresponds to the maximum likelihood estimator of the data [60]. A square loss, instead, is known to be more naturally suited to regression problems with continuous labels.

[^6]:    10 As with all variational models, it can be difficult to know whether a model has converged or if the model is stuck on a particularly flat part of the optimisation landscape, and test accuracies can both improve or worsen with more training. The choice to decide on convergence over 200 updates is therefore to some extent arbitrary.

[^7]:    11 Note that a subtlety here is the size of the margin between the classes in increasing dimensions, which has an influence on how easy it is to generalise from the training data.

[^8]:    14 We normalise the rank since not all models competed in all benchmarks, and it is easier to have a better rank when competing in experiments with fewer overall competitors.

[^9]:    16 We chose to run this experiment on MNIST PCA dimensions as the quantum models showed a prototypical performance with respect to their overall rankings, and the variations between individual datasets of different input dimensions were small.

[^10]:    17 We found that the test set Gram matrices do not lead to different results.

[^11]:    18 An exception is the TWO CURVES benchmark, where all Gram matrices are similar.

[^12]:    19 It is interesting to note that in a similar manner, WeiNet - the only non-hybrid quantum convolutional neural network we tested - performs badly on BARS \& STRIPES which we considered to be a very simple task.

[^13]:    20 This best practice takes into account that in applications one does not necessarily have access to the test set at training time.

