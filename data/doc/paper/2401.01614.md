# GPT-4V(ision) is a Generalist Web Agent, if Grounded 

Boyuan Zheng ${ }^{1}$ Boyu Gou ${ }^{1}$ Jihyung Kil ${ }^{1}$ Huan Sun ${ }^{1}$ Yu Su ${ }^{1}$<br>https://osu-nlp-group.github.io/SeeAct


#### Abstract

The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents-it can successfully complete $51.1 \%$ of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms textonly LLMs like GPT-4 or smaller models (FLANT5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out to be not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML structure and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement. All code, data, and evaluation tools are available at https: //github.com/OSU-NLP-Group/SeeAct.


## 1. Introduction

Large multimodal models (LMMs; Li et al. (2023); Alayrac et al. (2022); Liu et al. (2023b)), especially recent ones such as GPT-4V(ision) (OpenAI, 2023) and Gemini (Anil[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_a98961e5cf74f3a79a54g-01.jpg?height=648&width=811&top_left_y=646&top_left_x=1058)

Figure 1: SEEAct leverages an LMM like GPT-4V to visually perceive websites and generate plans in textual forms. The textual plans are then grounded onto the HTML elements and operations to act on the website.

et al., 2023), have shown a remarkable capability on standard vision-and-language understanding and reasoning benchmarks (Kazemzadeh et al., 2014; Goyal et al., 2016; Hendrycks et al., 2020; Saikh et al., 2022; Lu et al., 2022; Zhong et al., 2023; Yue et al., 2023). While web content has been a primary source of training data, a largely overlooked part of the web is the websites themselves-every website is designed to be rendered visually for easy consumption by human users. This poses a new challenge and a new opportunity for LMMs. On the one hand, screenshots of rendered websites, which could contain thousands of elements with rich relations, are more complex than the images in most existing benchmarks, which are usually object- or scene-centric. On the other hand, if LMMs can accurately comprehend websites, it will open the door for numerous applications on the web.

In this work, we aim to investigate the potential of LMMs as generalist web agents (Deng et al., 2023). A generalist web agent, as defined in MIND2WEB (Deng et al., 2023), is expected to follow natural language instructions and complete tasks on any given real-world website (e.g., Figure 1).

The tasks can be fairly diverse and complex, with one task possibly taking 10+ actions across multiple dynamically rendered webpages. Existing work (Deng et al., 2023; Liu et al., 2023d) primarily uses large language models (LLMs) such as GPT-4 (OpenAI, 2023) on the raw HTML input. However, HTML code is noisier than the rendered visuals and has a lower information density. For example, the screenshot in Figure 1 contains 423 HTML elements that would require 186,490 textual tokens with the GPT-2 Tokenizer, while requiring only 1,445 visual tokens using GPT-4V's visual tokenizer. Furthermore, HTML alone provides incomplete information and misses critical semantics from, e.g., embedded images.

To this end, we propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We will focus on GPT-4V, the most advanced LMM publicly available to date, and compare it with smaller LMMs such as BLIP2 (Li et al., 2023) and LLaVA-1.5 (Liu et al., 2023a;c). We find that GPT-4V exhibits a strong capability in visually understanding rendered webpages and generate the right plans in textual forms across a wide range of websites and tasks. However, grounding (Chandu et al., 2021; Gu et al., 2023), i.e., converting the textual plan into precise actions on the website, remains a major challenge. It involves selecting the right HTML element to interact with as well as the right operation (e.g., Click, Type, or Select). We propose multiple grounding methods, including superpositioning bounding boxes and index labels onto the image, similar to set-of-mark prompting (Yang et al., 2023a) that has been shown effective on object- or scene-centric images. However, we find that on complex images with rich semantic and spatial relationships like webpage screenshots, severe hallucination is observed from GPT-4V. The most effective grounding strategy leverages the known correspondence between HTML element and their visual rendering, a unique property for websites compared to natural images.

We evaluate SeEAct on the Mind2Web dataset (Deng et al., 2023) and compare it with text-only large language models (LLMs) like GPT-4 (OpenAI, 2023) as well as smaller models (FLAN-T5 (Chung et al., 2022), BLIP-2 (Li et al., 2023), LLaVA-1.5 (Liu et al., 2023a;c), and CogAgent (Hong et al., 2023). In addition to the standard offline evaluation setting on cached websites, we further establish a new online evaluation setting by developing a tool that allows for running web agents on live websites. The major findings from our exploration are summarized below:

- SEEACT with GPT-4V is a strong generalist web agent, if oracle grounding is provided. In online evaluation, it can successfully complete $51.1 \%$ of tasks on different websites, substantially outperforming existing methods like GPT-4 (13.3\%) or FLAN-T5 (8.9\%). This strongly demonstrates the potential of LMMs like GPT-4V for web agents.
- However, grounding is still a major challenge. The best grounding strategy still has a $20-30 \%$ gap with oracle grounding. Among the various grounding strategies, the best one organically leverages both HTML text and visuals, substantially outperforming image annotation strategies (Yang et al., 2023a) by up to $10 \%$.
- In-context learning with large models (both LMMs and LLMs) shows better generalization to unseen websites, while supervised fine-tuning still has an edge on websites seen during training.
- There is a non-negligible discrepancy between online and offline evaluation because there can often be multiple viable plans for completing the same task. Online evaluation is more indicative of a model's true performance.


## 2. SeeAct

In this section, we first explain the problem formulation of web agents and then introduce SEEAct, a generalist web agent based on LMMs. Specifically, given a web-based task (e.g., "Rent a truck with the lowest rate" in the car rental website), we examine two essential capabilities of LMMs as a generalist web agent: (i) Action Generation to produce an action description at each step (e.g., "Move the cursor over the 'Find Your Truck' button and perform a click") towards completing the task, and (ii) Element Grounding to identify an HTML element (e.g., "[button] Find Your Truck") at the current step on the webpage.

### 2.1. Formulation

Given a website $\mathcal{S}$ (e.g., a car rental website) and a task $T$ (e.g., "Rent a truck with the lowest rate"), the web agent should generate a sequence of executable actions $A=\left[\boldsymbol{a}_{1}, \boldsymbol{a}_{2}, \ldots, \boldsymbol{a}_{n}\right]$ to complete the task. Specifically, at time step $t$, the agent should generate an action $\boldsymbol{a}_{t}$ based on the current environment observation $s_{t}$, the previous actions $\left\{\boldsymbol{a}_{1}, \boldsymbol{a}_{2}, \ldots, \boldsymbol{a}_{t-1}\right\}$, and the task $T$ :

$$
\boldsymbol{a}_{t}=\pi\left(s_{t}, T,\left\{\boldsymbol{a}_{1}, \boldsymbol{a}_{2}, \ldots, \boldsymbol{a}_{t-1}\right\}\right)
$$

The environment observation $s_{t}$ comprises an HTML document $h_{t}$ and a screenshot image $i_{t}$. LLMs can only be grounded on the HTML document, while LMMs can be grounded on both the HTML document and the screenshot image. The website status is updated accordingly after each action:

$$
s_{t+1}=\mathcal{S}\left(\boldsymbol{a}_{t}\right)=\left\{h_{t+1}, i_{t+1}\right\}
$$

For simplicity, in subsequent step-wise formulations, the time step notation $t$ is omitted.

An action $\boldsymbol{a}$ corresponds to a browser event provided by the

![](https://cdn.mathpix.com/cropped/2024_06_04_a98961e5cf74f3a79a54g-03.jpg?height=637&width=1520&top_left_y=224&top_left_x=259)

Figure 2: An example of the element grounding process for a single action during completing the given task with three different methods. In this action step, the model needs to click the "Find Your Truck" button to perform a search. For grounding with textual choices, some element candidates represented with HTML text are given, the model is required to generate the choice index of the target element. For image annotation, bounding boxes and index labels are added to the image. The model is required to generate the label on the bottom-left of the target element. For grounding with element attributes, the model needs to predict the text and type of the target element.

website environment. Therefore, we formulate an action as a triplet of three necessary variables for a browser event $(e, o, v) . e \in \mathcal{E}$ identifies the target webpage element to operate on, such as the "Find Your Truck" button in Figure 2 . $\mathscr{E}$ represents the set of webpage elements within the environment $\mathcal{S}$. The operation $o \in \mathcal{O}$ is the action to be performed on the target element, with $\mathcal{O}$ encompassing all possible operations in $\mathcal{S}$ (e.g., Click, Type). The variable $v$ denotes the additional value needed for a certain operation (e.g., the date 12/10/2023 for a Type operation).

However, agents based on LLMs or LMMs are typically unable to directly generate the three variables $(e, o, v)$ required for a browser event. Instead, they generate a textual description of the intended action $\tilde{\boldsymbol{a}}$, containing information about these variables as $(\tilde{e}, \tilde{o}, \tilde{v})$. This process is referred to as Action Generation. To interact with the environment, a further step is required to convert $\tilde{\boldsymbol{a}}$ into $\boldsymbol{a}$, which we refer to as Action Grounding.

### 2.2. Action Generation

We explicitly instruct GPT- $4 \mathrm{~V}$ to imitate humans browsing a webpage and analyze the task, webpage, and previous actions. It is asked to generate an action description $\tilde{a}$ based on its analysis and reasoning. We take the screenshot image $i$ as the visual context without utilizing the HTML document $h$ for action generation.

### 2.3. Action Grounding

Despite the capability of GPT-4V in identifying and describing the next action to complete the given task in natural language, it is still challenging to convert the action description $\tilde{\boldsymbol{a}}$ into an executable action $\boldsymbol{a}$ within the environment. Deriving operation type $o$ and value $v$ from the action description $\tilde{\boldsymbol{a}}$ can be solved through string parsing reasonably well. The key challenge is to identify the target element $e$ from the generated $\tilde{e}$, which we refer to as

## Element Grounding

To address this challenge, we explore three approaches using different types of information: Grounding via Element Attributes, Grounding via Textual Choices, and Grounding via Image Annotation, as depicted in Figure 2. The prompting details of action generation and grounding are included in Appendix D.

Grounding via Element Attributes. This approach involves prompting the model to generate as detailed attributes of the target element as possible, thereby providing more information to precisely match with the target HTML element. Specifically, we prompt the model to not only describe the element $e$, but also specify the target element's type and the textual content in $\tilde{e}$. For example, as illustrated in Figure 2, the model would generate element text as "Find Your Truck" and identify its type as a "BUTTON." Following this, a heuristic search is performed across the DOM elements, using the element text and type to locate matching elements. In cases where a single match is found, it is automatically selected. Otherwise, when multiple matches arise, the model is further prompted to select the final selection.

Grounding via Textual Choices. The above approach demands precise and sufficient attribute descriptions from GPT-4V and accurate matching by the heuristic search, which can be highly demanding. For instance, many elements may have no textual content or have textual information in a nearby element instead of itself.

Alternatively, we provide the model with textual representations of elements as choices to facilitate grounding, which has already been proven effective in MindAct (Deng et al., 2023). Specifically, MindAct utilizes a ranking model to select top- $k$ candidate elements $\left(e_{1}, e_{2}, \ldots, e_{k}\right)$ with a pretrained cross-encoder. Each candidate element is represented as a choice in a multi-choice question with its HTML text, as illustrated in Figure 2. After generating the action description $\tilde{\boldsymbol{a}}$, the model is further asked a multi-choice question to choose its intended element from the given multiple choices (including a 'none' option).

Grounding via Image Annotation. Textual representations alone are sometimes insufficient to distinguish similar or identical elements, as illustrated in Appendix G. Therefore, in this approach, we propose to overlay a bounding box for each candidate element $e$ selected by the ranker as well as a label around the bounding box ${ }^{1}$ with a label assignment method to avoid overlapping between markups. The model is expected to generate the label corresponding to the target element.

Oracle Action Grounding. Ideally, the action description $\tilde{a}$ must encompass all necessary details to precisely identify each variable $(e, o, v)$ of the action triplet. To assess the performance of action generation, an oracle grounding method, which ensures the variables be identified as long as they are mentioned in the action description, is desired. Here we approximate the oracle grounding method by asking human annotators to identify the model's intended actions.

## 3. Experiments

### 3.1. Dataset

We evaluate our methods on Mind2WEB (Deng et al., 2023), a comprehensive dataset encompassing over 2,000 complex web tasks with annotated actions. This dataset spans 137 websites across 31 low-level domains, categorized into 12 high-level domains. It supports three primary operations: Click, Type, and Select, with Hover and Press Enter operations integrated into Click to avoid ambiguity.

The dataset's test sets aim to measure the generalization of web agents across different tasks, websites, and domains. Specifically, the Cross-Task setting focuses on evaluating[^1]

agents on tasks that are new to the training data but within included domains and websites. The Cross-Website setting evaluates agents with tasks across 10 new websites for each of the top-level domains in the training data. The CrossDomain setting assesses agent performance on tasks in two top-level domains held out from the training data.

We align each HTML document in the dataset with its corresponding webpage screenshot image from the Mind2WEB raw dump, which undergoes human verification to confirm element visibility and correct rendering for action prediction. This cleaned version of the dataset is called Multimodal Mind $2 W e b^{2}$, with the statistics in Table 1.

### 3.2. Methods

SeeAct. In grounding via image annotation and textual choices, we first employ the DeBERTa-base cross-encoder from MindAct (Deng et al., 2023) to rank the top 50 elements for better comparison with its text-only counterparts. Then, we cluster elements into groups of 17 options for inference. In grounding via element attributes, no candidate element is provided. We experiment all three grounding methods with GPT-4V API, and use the best-performing grounding method for Gemini Pro Vision (Anil et al., 2023), and LLaVA-1.5 (Liu et al., 2023a;c).

MindAct. To compare with SeEAct, we also implement methods based on text-only LLMs and BLIP-2 (Li et al., 2023) following the two-stage strategy of MindAct (Deng et al., 2023). Firstly, we employ the ranker above to pick the top 50 elements. Subsequently, the action generation problem is formulated as a multi-choice question answering problem, with the candidate elements as options, including a "None" option if the target element is absent. During inference, elements are clustered into groups of 5 elements, with iterative refinement, until a single choice is made or all options are discarded. We evaluate supervised fine-tuning (SFT) methods using FLAN-T5 (Chung et al., 2022) and BLIP-2-T5 and in-context learning (ICL) methods using GPT-3.5 and GPT-4.

Pixel-Level Grounding. LMMs can generate target element coordinates in the image via training on datasets augmented with object coordinates, especially for open-sourced models (Hong et al., 2023; Cheng et al., 2024; You et al., 2023). We choose CogAgent (Hong et al., 2023) as a representative model for this experiment with the ICL method. Details of each method can be found in Appendix A.

### 3.3. Offline Evaluation

We adopt the evaluation metrics utilized in MIND2WEB. Element Accuracy (Ele. Acc) compares the predicted el-[^2]

Table 1: Statistics of the cleaned Multimodal Mind2Web dataset. The average \# of visual tokens is based on OpenAI visual token calculator.

| Split | \# Tasks | \# Domains | \# Websites | Avg \# <br> Actions | Avg \# <br> Visual Tokens | Avg \# HTML |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Train |  |  |  | Elements | Tokens |  |  |
| Cross-Domain | 1,009 | 17 | 73 | 7.7 | 4,240 | 602 | 128,827 |
| Cross-Task | 177 | 13 | 53 | 5.9 | 4,314 | 494 | 91,163 |
| Cross-Website | 142 | 17 | 64 | 7.6 | 4,172 | 607 | 123,274 |

ement with the ground-truth elements. Operation F1 (Op. F1) calculates the token-level F1 score for the predicted operation comprised of action and input value. Step Success Rate (Step SR) measures the success of each action step. A step is successful only if the selected element and the predicted operation are correct. We report macro averages across tasks for these step-wise metrics. Success Rate (SR) measures the success of an entire task. A task is regarded successful only if all steps have succeeded. This metric is stringent without allowing the model any space for exploration and error correction. Therefore, for offline evaluation, we focus on the first three metrics. However, we also conduct online evaluation on live websites for better evaluation on the whole task success rate, as detailed below.

### 3.4. Online Evaluation

We develop a new online evaluation tool using Playwright ${ }^{3}$ to evaluate web agents on live websites (instead of cached websites in offline evaluation). Our tool can efficiently tunnel multimodal inputs from the browser to the agent and convert the predicted action $(e, o, v)$ into a browser event for execution. To adhere to ethical standards, our experiments are restricted to non-login tasks in compliance with user agreements, and we closely monitor agent activities during online evaluation to prevent any actions that have potentially harmful impacts, like placing an order or modifying the user profile.

## 4. Results and Analysis

### 4.1. Offline Evaluation Results

GPT-4V can be a Generalist Web Agent with Oracle Action Grounding. Given an effective action grounding method, GPT- $4 \mathrm{~V}$ has the potential to serve as a generalist web agent. Specifically, as described in subsection 2.3, we provide GPT-4V with an oracle action ground-

![](https://cdn.mathpix.com/cropped/2024_06_04_a98961e5cf74f3a79a54g-05.jpg?height=46&width=829&top_left_y=2178&top_left_x=187)
model achieves a step success rate of $61.9 \%, 65.0 \%$, and $62.1 \%$ across three test splits, respectively. As shown in Table 2, this method substantially outperforms other models under all metrics across three test splits. Specifically,[^3]

it achieves a $8.4 \%$ step success rate improvement over the second-best method in the Cross-Task setting. The performance advantage is more pronounced under the CrossWebsite and Cross-Domain settings, where it leads by $23.9 \%$ and $23.2 \%$ step success rates, demonstrating its generality compared with supervised fine-tuning. This observation is further corroborated within the online evaluation (Table 4).

Element Grounding Method Comparison. However, there is a noticeable gap between oracle grounding and all three proposed grounding methods, as shown in Table 3. This demonstrates that grounding, especially element grounding, is a major bottleneck. Element grounding via

![](https://cdn.mathpix.com/cropped/2024_06_04_a98961e5cf74f3a79a54g-05.jpg?height=46&width=829&top_left_y=1148&top_left_x=1057)
mance under all metrics across all settings, comparable to supervised fine-tuning and showing a substantial improvement over text-only LLMs.

![](https://cdn.mathpix.com/cropped/2024_06_04_a98961e5cf74f3a79a54g-05.jpg?height=46&width=824&top_left_y=1338&top_left_x=1060)
an intuitive approach and shows promising results in recent work that focuses on object- or scene-centric images (Yang et al., 2023a). However, we find that on complex images with rich semantic and spatial relationships like webpage screenshots, severe hallucination is observed from GPT-4V. Specifically, it often fails to correctly map its generated element description (which is often correct according to oracle grounding) to the right bounding box and index label in the image, leading to a low element accuracy. This limitation primarily arises from GPT-4V's weakness in understanding image details and relative spatial location, a topic that we will further delve into in Appendix E. We leverage bottomleft number labels around bounding boxes as it is identified as the optimal markups in the ablation study in Appendix B.

![](https://cdn.mathpix.com/cropped/2024_06_04_a98961e5cf74f3a79a54g-05.jpg?height=51&width=824&top_left_y=1980&top_left_x=1060)
demonstrates inferior performance. This method's effectiveness is primarily limited by its heuristic-based element localization strategy, which depends on textual and locality characteristics. This becomes problematic as not all webpage elements contain text, and sometimes the relevant text is associated with a nearby but distinct element.

![](https://cdn.mathpix.com/cropped/2024_06_04_a98961e5cf74f3a79a54g-05.jpg?height=46&width=829&top_left_y=2292&top_left_x=1057)
strates a substantial performance advantage over the textonly GPT-4 under all three metrics across all three test splits. Specifically, it outperforms GPT-4 in step success rate of

Table 2: Performance of different models. All models under SEEAct utilize "Choices" for grounding. Methods with * mark are conducted on a subset with 30 tasks for each task split.

| Model | Cross-Task |  |  | Cross-Website |  |  | Cross-Domain |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Ele. Acc | Op. F1 | Step SR | Ele. Acc | Op. F1 | Step SR | Ele. Acc | Op. F1 | Step SR |
| Supervised Fine-Tuning |  |  |  |  |  |  |  |  |  |
| FLAN-T5-XL | 57.1 | 75.7 | 53.5 | 43.8 | 67.7 | 41.1 | 41.4 | 65.9 | 38.9 |
| BLIP-2-T5-XL | 50.1 | 77.0 | 47.0 | 39.4 | 69.3 | 37.0 | 41.2 | 69.3 | 38.9 |
| In-Context Learning |  |  |  |  |  |  |  |  |  |
| GPT-3.5* | 19.4 | 59.2 | 16.8 | 14.9 | 56.5 | 14.1 | 25.2 | 57.9 | 24.1 |
| GPT-4* | 40.8 | 63.1 | 32.3 | 30.2 | 61.0 | 27.0 | 35.4 | 61.9 | 29.7 |
| $\overline{\mathrm{COG}} \overline{\mathrm{A}} \overline{\mathrm{GENT}}-\overline{-}$ | $\overline{22.4}$ | $\overline{5} \overline{3} .0$ | $-1 \overline{7} . \overline{6}$ | $\overline{18} \overline{4}$ | $-4 \overline{2} . \overline{2}$ | $\overline{13.4}$ | $-2 \overline{0} . \overline{6}$ | $\overline{42.0}$ | $\overline{1} 5.5^{-}$ |
| $\overline{\mathrm{SEE}} \overline{\mathrm{AC}} \overline{\mathrm{T}}$ |  |  |  |  |  |  |  |  |  |
| - LLAVA-1.5 | 9.7 | 65.6 | 8.1 | 9.1 | 60.8 | 7.5 | 10.9 | 63.9 | 8.5 |
| - GEMINI PRo VISION | 21.5 | 67.7  | 19.6 | 17.1 | 61.3 | 15.4 | 20.7 | 64.3 | 18.0 |
| - GPT-4V | 46.4 | 73.4 | 40.2 | 38.0 | 67.8 | 32.4 | 42.4 | 69.3 | 36.8 |
| - GPT-4V-Oracle* | 66.4 | 79.2 | 61.9 | 69.5 | 78.9 | 65.0 | 72.8 | 73.6 | 62.1 |

Table 3: Step success rate (\%) of GPT-4V on a subset of 30 tasks for each task split with different grounding methods. "Attributes", "Choices", "Annotation", and "Oracle" refer to element grounding via Element Attributes, Textual Choices, Image Annotation, and Human Annotation, respectively, as described in subsection 2.3 .

| Grounding | Cross-Task | Cross-Website | Cross-Domain |
| :---: | :---: | :---: | :---: |
| SEEACT $_{\text {Attribute }}$ | 16.1 | 12.1 | 19.0 |
| $\mathrm{SEEACT}_{\text {Annotation }}$ | 20.3 | 13.9 | 23.7 |
| SEEACTChoice | 39.1 | 32.7 | 42.0 |
| SEEACT | 61.9 | 65.0 | 62.1 |

$6.8 \%, 5.7 \%$, and $12.3 \%$ on three settings, respectively. Interestingly, fine-tuned BLIP-2-T5 does not show a noticeable gain over FLAN-T5, despite having additional visual input. Several factors may contribute to this. First, the CLIP model used as the image encoder may not be sufficiently adept at image details, as explored by Shen et al. (2021). This limitation is particularly relevant for our web navigation task, which demands a high level of image detail comprehension. Second, BLIP-2-T5 utilizes an off-the-shelf CLIP model that may not be optimal for webpage screenshots. Finally, although the screenshots in the test splits are error-free, some of the examples in the training set might contain issues such as rendering failures or inaccuracies when annotators capture the screenshots.

SFT vs. ICL. We compare SFT and ICL methods to offer insights for developing web agents in different scenarios. ICL (with SEEACT) demonstrates consistent and robust performance across three test splits. ICL is particularly advantageous in scenarios lacking annotations or requiring strong generalization capabilities for new domains and websites. As grounding methods improve towards oracle grounding,
Table 4: Whole task success rate (\%) under both offline and online evaluation. Offline ${ }_{0}$ and Offline $_{1}$ refer to no tolerance for error at any step and allowing for error at one step, respectively.

|  | Offline $_{0}$ | Offline $_{1}$ | Online |
| :--- | :---: | :---: | :---: |
| FLAN-T5-XL | 4.4 | 24.4 | 8.9 |
| GPT-4 | 1.1 | 12.2 | 13.3 |
| SEEACT $_{\text {Choice }}$ | 3.3 | 12.2 | 37.8 |
| SEEACT $_{\text {Oracle }}$ | 13.3 | 27.8 | 51.1 |

ICL is poised to show even stronger performance. On the other hand, SFT methods show better generalization across tasks on websites already seen during training. Considering the high cost of data annotation for web agents and the billions of websites on the Internet, ICL offers a more compelling solution for generalist web agents. However, if one only needs to develop a strong web agent for a certain website, SFT is still a competitive solution.

### 4.2. Online Evaluation Results

In online evaluation, we pair a web agent with a human annotator, where the human was tasked to monitor agent actions that may change real-world states and determine whether each task was successfully completed. For comparative analysis, we include success rates from offline evaluation, denoted as Offline ${ }_{0}$ (allowing zero wrong action) and Offline $_{1}$ (allowing one wrong action). For a fair comparison between offline and online evaluations, we only re-write time-sensitive tasks to ensure they are still valid when the evaluation is conducted. For instance, we update the dates for flight-related tasks. Finally, we conduct the online evaluation on the same subset of total 90 tasks from the three test splits. For tasks invalidated due to website change, we

![](https://cdn.mathpix.com/cropped/2024_06_04_a98961e5cf74f3a79a54g-07.jpg?height=502&width=686&top_left_y=237&top_left_x=253)

Figure 3: Whole task success rate across task difficulty levels. We categorize tasks based on the number of actions to complete, i.e., Easy: 1-4, Medium: 5-9, and Hard: 10-18, with 37,35 , and 18 tasks in each group, respectively.

resample from the corresponding test split.

Table 4 shows that the whole task success rate in online evaluation substantially exceeds that of offline evaluation (Offline ${ }_{0}$ ). This finding suggests that the whole task success rate is likely underestimated in the offline evaluation due to the variability in actions and plans. In other words, there may be multiple viable plans for a task, but the reference plan in offline evaluation only captures one of them.

Across all three settings, SEEACT Choice with GPT-4V outperforms both GPT-4 and FLAN-T5-XL by a large margin of over $20 \%$ whole task success rate. Using oracle grounding further improves the performance substantially, reaching a remarkable whole task success rate of $51.1 \%$. Although GPT-4 shows much worse performance than FLAN-T5-XL in step success rate under offline evaluation (Table 2), it outperforms FLAN-T5-XL by $4.4 \%$ whole task success rate in the online evaluation. These results further confirm the potential of large models for generalist web agents compared with fine-tuned small models.

### 4.3. Analysis

Online Success Rate by Task Difficulty. We investigate the performance of web agents on tasks across different difficulty levels. We estimate the task difficulty based on the number of actions taken by annotators during action trace annotation. As shown in Figure 3, the whole task success rate is negatively correlated with the number of actionsit decreases as the number of actions increases across all four methods. SEEACT Oracle consistently outperforms other methods across all difficulty levels. Interestingly, the gap be-

![](https://cdn.mathpix.com/cropped/2024_06_04_a98961e5cf74f3a79a54g-07.jpg?height=46&width=829&top_left_y=2362&top_left_x=187)
horizon tasks. This is understandable because grounding errors cascade to later steps; nonetheless, it further shows the challenge of grounding for GPT-4V and the need for better grounding methods.

Error Analysis in Grounding via Image Annotation. Set-of-mark prompting (Yang et al., 2023a) uses a similar method as grounding via image annotation and has been shown effective on object- or scene-centric images (Lin et al., 2014; Plummer et al., 2015; Zhou et al., 2017). However, this grounding method is suboptimal on webpage screenshot images that are complex and contain rich semantic and spatial relationships. To analyze the reasons behind the failures, we randomly sample 100 action predictions with correct action generation but wrong grounding results. We observes major types of errors as : (1) Making up bounding box \& label; (2) Failure to link bounding boxes with the correct labels. Illustrative examples are included in Appendix E.

Our analysis reveals that $54 \%$ of the errors can be attributed to GPT-4V's tendency of visual illusion (Guan et al., 2023), where the model misinterprets and fabricates content over the image. Specifically, the target element described in action generation does not have a bounding box or a label on the bottom-left, where the model is supposed to generate "NA". However, the model falsely assumes the presence of a bounding box and makes up a label as the answer. Another $46 \%$ of errors are caused by GPT-4V's limitation in recognizing the relative position within an image. Specifically, the model is capable of identifying the target element within the bounding box. However, it struggles to correctly link the bounding box with its corresponding label.

### 4.4. Case Study

GPT-4V exhibits promising capabilities, ranging from speculative planning, webpage content reasoning, and error correction to surpassing the limitations of superficial textual similarity matching inherent in fine-tuned, text-only models.

World Knowledge. GPT-4V demonstrates substantial advantages in tasks requiring certain knowledge over finetuned models at a smaller scale. As shown in Appendix H, GPT-4V is able to identify the IATA code of the airport in Los Cabos as SJD. In contrast, smaller models are typically weaker at knowledge-intensive tasks and are also likely to lose knowledge during the fine-tuning process due to catastrophic forgetting.

World Model (for Websites). GPT-4V exhibits the potential of a "world model" for websites. As shown in Appendix F, GPT-4V can predict the state transitions on a website (e.g., what would happen if I clicked this button). Based on its awareness of website state transitions, GPT-4V can conduct speculative planning involving a sequence of subsequent actions in the future to complete the given task.

Error Correction Awareness. GPT-4V also exhibits the
awareness of error correction in the previous actions. In the example in Appendix I, it realizes that the mobile phone number is invalid due to the wrong format and generates the description of the action to correct this error. This highlights the model's potential for adaptation in online settings, where actions may not always follow pre-defined, ideal paths as in offline evaluations. This capability paves the way for adding robustness and reasonable dynamic planning.

## 5. Related Work

Web Agent. Many works have focused on improving web agents relying on the HTML document (Deng et al., 2023; Gur et al., 2023; 2022; 2023; Kim et al., 2023; Sridhar et al., 2023). However, a raw HTML document is often massive making it infeasible or cost-prohibitively to feed into LLMs directly. MindAct (Deng et al., 2023) instead employs a small language model to rank each HTML element and selectively consider top elements as the context. WebAgent (Gur et al., 2023) proposes an enhanced planning strategy by summarizing the HTML documents and decomposing the instruction into multiple sub-instructions. Another stream considers visual information for web agents (Shaw et al., 2023; Furuta et al., 2023; Hong et al., 2023). Pix2Act (Shaw et al., 2023) leverages Pix2Struct (Lee et al., 2022) to parse screenshot images into simplified HTML to complete GUI-based tasks (Shaw et al., 2023; Liu et al., 2018; Shi et al., 2017; Mazumder \& Riva, 2020; Yao et al., 2022). WebGUM (Furuta et al., 2023) and CogAgent (Hong et al., 2023) pre-train an LMM with massive screenshot-HTML data to enhance its decision-making on real-world web navigation like Mind2Web. While all these prior works show promise, generalizing to various web environments remains a challenge for existing models. Thus, SeEAct explores recently released, more powerful LMMs such as GPT-4V and Gemini, to demonstrate their potential as generalist web agents with comprehensive online and offline evaluation and analysis. In a concurrent work (Yan et al., 2023), GPT-4V exhibits strong performance on mobile UI understanding, which is less complex than the desktop websites we study.

Large Multimodal Models. GPT-4V (OpenAI, 2023) and Gemini (Anil et al., 2023) represent significant progress in LMMs. Several studies (Akter et al., 2023; OpenAI, 2023; Yang et al., 2023c; Zhang et al., 2023; Yang et al., 2023a; Yan et al., 2023) have highlighted their remarkable multimodal capabilities, emphasizing the advanced and versatile integration of visual and language reasoning abilities. Their performance on a series of benchmarks (Kazemzadeh et al., 2014; Goyal et al., 2016; Hendrycks et al., 2020; Saikh et al., 2022; Lu et al., 2022; Zhong et al., 2023; Yue et al., 2023) also showcases remarkable capabilities on vision-and-language understanding and reasoning. Al- though open-sourced models still exhibit a performance gap with GPT-4V, they have the advantages of controllability and ease of fine-tuning for various applications. For example, in CogAgent (Hong et al., 2023), LMMs are fine-tuned on HTML and screenshot image pairs to enhance webpage understanding ability and further enhanced with an image encoder for high-resolution image details. Ferret (You et al., 2023) is finetuned to allow visual referring and grounding.

Visual Grounding. Despite LMMs having achieved remarkable vision-language understanding capabilities, they still face challenges in fine-grained visual grounding. Various visual prompting (Shtedritski et al., 2023; Yang et al., 2023b;c; Yan et al., 2023) methods have been proposed to augment GPT-4V's image detail grounding ability by overlaying visual marks onto the image. SoM (Yang et al., 2023a) involves segmenting the image into semantically meaningful regions and overlaying an array of visual marks like numbers, alphabets, masks, or bounding boxes. Fine-tuning vision-language models with image-annotated data is effective. Kosmos-2 (Peng et al., 2023) represents bounding box locations through textual location tokens. BuboGPT (Zhao et al., 2023) extract entities and find corresponding masks for objects in the image. Shikra (Chen et al., 2023) handles image detail referring and grounding by applying spatial coordinates as text tokens in inputs and outputs, respectively. Ferret (You et al., 2023) represents regions with both discrete coordinates and continuous features along with a spatial-aware visual sampler to handle diverse spatial characteristics across various shapes.

## 6. Conclusion

In this work, we developed SeEAct, a generalist web agent that harnesses the power of large multimodal models (LMMs) like GPT-4V to integrate visual understanding and acting on the web. We showed that LMMs present a great promise for generalist web agents, with a success rate of $50 \%$ on live websites given an oracle grounding method. GPT-4V also exhibits impressive capabilities, such as error correction and speculative planning. However, finegrained visual grounding is still a major challenge. The most effective grounding strategies we explored in this paper still exhibit a $20-25 \%$ performance gap compared to oracle grounding. Future work should better leverage the unique properties of the Web, e.g., the known correspondence between HTML and visual elements, for improving grounding and reducing hallucinations from LMMs. Furthermore, we show a significant discrepancy between online and offline evaluations, emphasizing the importance of online evaluation for an accurate assessment of a model's capabilities. This discrepancy is largely due to the variability in potential plans for completing the same task, pointing to the dynamic nature of web interactions.

## 7. Impact Statements

Generalist web agents hold the potential to automate routine web tasks, enhance user experiences, and promote web accessibility, safety concerns related to their real-world deployment are also critical. These concerns span privacy issues, such as access to users' personal profiles, and sensitive operations, such as financial transactions or application form submissions. During the online evaluation, we noticed the possibility for these web agents to generate harmful actions on the web, and we manually validated the safety of all the actions before execution. It is critical for further research to thoroughly assess and mitigate the safety risks associated with web agents, ensuring they are safeguarded against producing and executing harmful actions. The code will also be released solely for research purposes, with the goal of making the web more accessible via language technologies under an OPEN-RAIL License. We are strongly against any potentially harmful use of the data or technology by any party.

## Acknowledgments

The authors would like to thank colleagues from the OSU NLP group for their thoughtful comments. This research was supported in part by ARL W911NF2220144 and Cisco.

## References

Akter, S. N., Yu, Z., Muhamed, A., Ou, T., Bauerle, A., Cabrera, Á. A., Dholakia, K., Xiong, C., and Neubig, G. An in-depth look at gemini's language abilities. 2023. URL https://api.semanticscholar. org/CorpusID:266359502.

Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198, 2022. URL https://api.semanticscholar. org/CorpusID:248476411.

Anil, G. T. G. R., Borgeaud, S., and et al., Y. W. Gemini: A family of highly capable multimodal models. 2023. URL https://api.semanticscholar. org/CorpusID:266361876.

Chandu, K. R., Bisk, Y., and Black, A. W. Grounding 'grounding' in nlp. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 42834305, 2021.
Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal llm's referential dialogue magic. ArXiv, abs/2306.15195, 2023. URL https://api.semanticscholar. org/CorpusID:259262082.

Cheng, K., Sun, Q., Chu, Y., Xu, F., Li, Y., Zhang, J., and Wu, Z. Seeclick: Harnessing gui grounding for advanced visual gui agents. 2024. URL https://api.semanticscholar. org/CorpusID:267069082.

Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Valter, D., Narang, S., Mishra, G., Yu, A. W., Zhao, V., Huang, Y., Dai, A. M., Yu, H., Petrov, S., hsin Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling instructionfinetuned language models. ArXiv, abs/2210.11416, 2022. URL https://api.semanticscholar. org/CorpusID:253018554.

Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023.

Furuta, H., Nachum, O., Lee, K.-H., Matsuo, Y., Gu, S. S., and Gur, I. Multimodal web navigation with instructionfinetuned foundation models. ArXiv, abs/2305.11854, 2023. URL https://api.semanticscholar. org/CorpusID:258823350.

Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the $\mathrm{v}$ in vqa matter: Elevating the role of image understanding in visual question answering. International Journal of Computer Vision, 127:398 - 414, 2016. URL https://api.semanticscholar. org/CorpusID:8081284.

Gu, Y., Deng, X., and Su, Y. Don't generate, discriminate: A proposal for grounding language models to realworld environments. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4928-4949, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.270. URL https: / /aclanthology.org/2023.acl-long.270.

Guan, T., Liu, F., Wu, X., Xian, R., Li, Z., Liu, X., Wang, X., Chen, L., Huang, F., Yacoob, Y., Manocha, D., and Zhou, T. Hallusionbench: An advanced diagnostic suite for entangled language hallucination\&visual illusion in large vision-language models.

2023. URL https://api.semanticscholar. org/CorpusID:265499116.

Gur, I., Nachum, O., Miao, Y., Safdari, M., Huang, A., Chowdhery, A., Narang, S., Fiedel, N., and Faust, A. Understanding html with large language models. In Conference on Empirical Methods in Natural Language Processing, 2022. URL https: //api.semanticscholar.org/CorpusID: 252780086 .

Gur, I., Furuta, H., Huang, A., Safdari, M., Matsuo, Y., Eck, D., and Faust, A. A real-world webagent with planning, long context understanding, and program synthesis. ArXiv, abs/2307.12856, 2023. URL https://api.semanticscholar. org/CorpusID:260126067.

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. X., and Steinhardt, J. Measuring massive multitask language understanding. ArXiv, abs/2009.03300, 2020. URL https://api.semanticscholar. org/CorpusID:221516475.

Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., and Tang, J. Cogagent: A visual language model for gui agents. 2023. URL https://api.semanticscholar. org/CorpusID:266210390.

Kazemzadeh, S., Ordonez, V., andre Matten, M., and Berg, T. L. Referitgame: Referring to objects in photographs of natural scenes. In Conference on Empirical Methods in Natural Language Processing, 2014. URL https://api.semanticscholar. org/CorpusID:6308361.

Kim, G., Baldi, P., and McAleer, S. M. Language models can solve computer tasks. ArXiv, abs/2303.17491, 2023. URL https://api.semanticscholar. org/CorpusID:257834038.

Koh, J. Y., Lo, R., Jang, L., Duvvur, V., Lim, M. C., Huang, P.-Y., Neubig, G., Zhou, S., Salakhutdinov, R., and Fried, D. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. 2024. URL https://api.semanticscholar. org/CorpusID:267199749.

Lee, K., Joshi, M., Turc, I., Hu, H., Liu, F., Eisenschlos, J. M., Khandelwal, U., Shaw, P., Chang, M.-W., and Toutanova, K. Pix2struct: Screenshot parsing as pretraining for visual language understanding. ArXiv, abs/2210.03347, 2022. URL https: //api.semanticscholar.org/CorpusID: 252762394 .
Li, J., Li, D., Savarese, S., and Hoi, S. C. H. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ArXiv, abs/2301.12597, 2023. URL https: //api.semanticscholar.org/CorpusID: 256390509 .

Lin, T.-Y., Maire, M., Belongie, S. J., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European Conference on Computer Vision, 2014. URL https: / api . semanticscholar.org/CorpusID:14113767.

Liu, E. Z., Guu, K., Pasupat, P., Shi, T., and Liang, P. Reinforcement learning on web interfaces using workflowguided exploration. In International Conference on Learning Representations (ICLR), 2018. URL https: //arxiv.org/abs/1802.08802.

Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning, 2023a.

Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. ArXiv, abs/2304.08485, 2023b. URL https://api.semanticscholar. org/CorpusID:258179774.

Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning, 2023c.

Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Gu, Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., and Tang, J. Agentbench: Evaluating llms as agents. ArXiv, abs/2308.03688, 2023d. URL https://api.semanticscholar. org/CorpusID:260682249.

Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. ArXiv, abs/2209.09513, 2022. URL https://api.semanticscholar. org/CorpusID:252383606.

Mazumder, S. and Riva, O. Flin: A flexible natural language interface for web navigation. ArXiv, abs/2010.12844, 2020. URL https://api.semanticscholar. org/CorpusID:225067907.

OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api.semanticscholar. org/CorpusID:257532815.

Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., and Wei, F. Kosmos-2: Grounding multimodal large language models to the world. ArXiv, abs/2306.14824, 2023. URL https://api.semanticscholar. org/CorpusID:259262263.

Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. International Journal of Computer Vision, 123:74 - 93, 2015. URL https: / / api. semanticscholar.org/CorpusID:6941275.

Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021.

Saikh, T., Ghosal, T., Mittal, A., Ekbal, A., and Bhattacharyya, P. Scienceqa: a novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23:289 - 301, 2022. URL https://api.semanticscholar. org/CorpusID:250729995.

Shaw, P., Joshi, M., Cohan, J., Berant, J., Pasupat, P., Hu, H., Khandelwal, U., Lee, K., and Toutanova, K. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. ArXiv, abs/2306.00245, 2023. URL https://api.semanticscholar. org/CorpusID:258999511.

Shen, S., Li, L. H., Tan, H., Bansal, M., Rohrbach, A., Chang, K.-W., Yao, Z., and Keutzer, K. How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383, 2021.

Shi, T., Karpathy, A., Fan, L. J., Hernández, J. Z., and Liang, P. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, 2017. URL https://api. semanticscholar.org/CorpusID:34953552.

Shtedritski, A., Rupprecht, C., and Vedaldi, A. What does clip know about a red circle? visual prompt engineering for vlms. ArXiv, abs/2304.06712, 2023. URL https://api.semanticscholar. org/CorpusID:258108138.

Sridhar, A., Lo, R., Xu, F. F., Zhu, H., and Zhou, S. Hierarchical prompting assists large language model on web navigation. ArXiv, abs/2305.14257, 2023. URL https://api.semanticscholar. org/CorpusID:258841249.

Yan, A., Yang, Z., Zhu, W., Lin, K., Li, L., Wang, J., Yang, J., Zhong, Y., McAuley, J., Gao, J., Liu, Z., and Wang, L. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. 2023. URL https://api.semanticscholar. org/CorpusID:265149992.
Yang, J., Zhang, H., Li, F., Zou, X., yue Li, C., and Gao, J. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. ArXiv, abs/2310.11441, 2023a. URL https://api.semanticscholar. org/CorpusID:264172201.

Yang, L., Wang, Y., Li, X., Wang, X., and Yang, J. Fine-grained visual prompting. ArXiv, abs/2306.04356, 2023b. URL https://api.semanticscholar. org/CorpusID:259096008.

Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and Wang, L. The dawn of lmms: Preliminary explorations with gpt-4v(ision). ArXiv, abs/2309.17421, 2023c. URL https://api.semanticscholar. org/CorpusID:263310951.

Yao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web interaction with grounded language agents. ArXiv, abs/2207.01206, 2022. URL https://api.semanticscholar. org/CorpusID:250264533.

You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.-F., and Yang, Y. Ferret: Refer and ground anything anywhere at any granularity. ArXiv, abs/2310.07704, 2023. URL https://api.semanticscholar. org/CorpusID:263834718.

Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. ArXiv, abs/2311.16502, 2023. URL https://api.semanticscholar. org/CorpusID:265466525.

Zhang, X., Lu, Y., Wang, W., Yan, A., Yan, J., Qin, L., Wang, H., Yan, X., Wang, W. Y., and Petzold, L. R. Gpt-4v(ision) as a generalist evaluator for vision-language tasks. ArXiv, abs/2311.01361, 2023. URL https://api.semanticscholar. org/CorpusID:264935635.

Zhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J., and Kang, B. Bubogpt: Enabling visual grounding in multi-modal llms. ArXiv, abs/2307.08581, 2023. URL https://api.semanticscholar. org/CorpusID:259937702.

Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A. S. S., Chen, W., and Duan, N. Agieval: A human-centric benchmark for evaluating foundation models. ArXiv, abs/2304.06364, 2023. URL https://api.semanticscholar. org/CorpusID:258108259.

Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., and Torralba, A. Scene parsing through ade20k dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5122-5130, 2017. URL https://api.semanticscholar. org/CorpusID: 5636055.
