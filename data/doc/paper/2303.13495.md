# ReVersion: Diffusion-Based Relation Inversion from Images 

Ziqi Huang* Tianxing Wu* Yuming Jiang Kelvin C.K. Chan Ziwei Liu ${ }^{\boxtimes}$<br>S-Lab, Nanyang Technological University<br>\{ziqi002, twu012, yuming002, chan0899, ziwei.liu\}@ntu.edu.sg

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=314&width=634&top_left_y=634&top_left_x=172)

"Michael Jackson $<R>$ wall"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=271&width=290&top_left_y=968&top_left_x=186)

exemplar images $\longrightarrow<R>$

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=339&width=331&top_left_y=1262&top_left_x=173)

Reversion

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=266&width=320&top_left_y=970&top_left_x=469)

“cat $<R>$ cat"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=317&width=312&top_left_y=1289&top_left_x=473)

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=263&width=269&top_left_y=644&top_left_x=798)

“cat <R> canvas"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=263&width=268&top_left_y=974&top_left_x=793)

"panda $<R>$ panda"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=326&width=265&top_left_y=1287&top_left_x=797)

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=260&width=274&top_left_y=648&top_left_x=1072)

"Spiderman $<R>$ wall"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=277&width=285&top_left_y=970&top_left_x=1061)

"otter $<R>$ otter"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=277&width=282&top_left_y=1287&top_left_x=1062)

“sea $<$ < cup"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=277&width=271&top_left_y=634&top_left_x=1339)

"hamburger $<R>$ vase

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=260&width=269&top_left_y=973&top_left_x=1340)

"rabbit <R> child"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=307&width=271&top_left_y=1283&top_left_x=1339)

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=263&width=276&top_left_y=644&top_left_x=1610)

"cat $<R>$ building"
![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-01.jpg?height=622&width=286&top_left_y=968&top_left_x=1602)

Figure 1: We propose a new task, Relation Inversion: Given a few exemplar images, where a relation co-exists in every image, we aim to find a relation prompt $\langle\mathrm{R}\rangle$ to capture this interaction, and apply the relation to new entities to synthesize new scenes. The above images are generated by our ReVersion framework.

Abstract

Diffusion models gain increasing popularity for their generative capabilities. Recently, there have been surging needs to generate customized images by inverting diffusion models from exemplar images. However, existing inversion methods mainly focus on capturing object appearances. How to invert object relations, another important pillar in the visual world, remains unexplored. In this work, we propose ReVersion for the Relation Inversion task, which aims to learn a specific relation (represented as "relation prompt") from exemplar images. Specifically, we learn a relation prompt from a frozen pre-trained text-to-image diffusion model. The learned relation prompt can then be applied to generate relation-specific images with new objects, backgrounds, and styles.
Our key insight is the "preposition prior" - real-world relation prompts can be sparsely activated upon a set of basis prepositional words. Specifically, we propose a novel relation-steering contrastive learning scheme to impose two critical properties of the relation prompt: 1) The relation prompt should capture the interaction between objects, enforced by the preposition prior. 2) The relation prompt should be disentangled away from object appearances. We further devise relation-focal importance sampling to emphasize high-level interactions over low-level appearances (e.g., texture, color). To comprehensively evaluate this new task, we contribute ReVersion Benchmark, which provides various exemplar images with diverse relations. Extensive experiments validate the superiority of our approach over existing methods across a wide range of visual relations.[^0]

## 1. Introduction

Recently, Text-to-image (T2I) diffusion models [34, 33, 37] have shown promising results and enabled subsequent explorations of various generative tasks. There have been several attempts $[9,36,25]$ to invert a pre-trained text-toimage model, obtaining a text embedding representation to capture the object in the reference images. While existing methods have made substantial progress in capturing object appearances, such exploration for relations is rare. Capturing object relation is intrinsically a harder task as it requires the understanding of interactions between objects as well as the composition of an image, and existing inversion methods are unable to handle the task due to entity leakage from the reference images. Yet, this is an important direction that worths our attention.

In this paper, we study the Relation Inversion task, whose objective is to learn a relation that co-exists in the given exemplar images. Specifically, with objects in each exemplar image following a specific relation, we aim to obtain a relation prompt in the text embedding space of the pre-trained text-to-image diffusion model. By composing the relation prompt with user-devised text prompts, users are able to synthesize images using the corresponding relation, with customized objects, styles, backgounds, etc.

To better represent high-level relation concepts with the learnable prompt, we introduce a simple yet effective preposition prior. The preposition prior is based on a premise and two observations in the text embedding space. Specifically, we find that 1 ) prepositions are highly related to relations, 2) prepositions and words of other Parts-of-Speech are individually clustered in the text embedding space, and 3) complex real-world relations can be expressed with a basic set of prepositions. Our experiments show that this languagebased prior can be effectively used as a high-level guidance for the relation prompt optimization.

Based on our preposition prior, we propose the ReVersion framework to tackle the Relation Inversion problem. Notably, we design a novel relation-steering contrastive learning scheme to steer the relation prompt towards a relation-dense region in the text embedding space. A set of basis prepositions are used as positive samples to pull the embedding into the sparsely activated region, while words of other Parts-of-Speech (e.g., nouns, adjectives) in text descriptions are regarded as negatives so that the semantics related to object appearances are disentangled away. To encourage attention on object interactions, we devise a relation-focal importance sampling strategy. It constrains the optimization process so that high-level interactions rather than low-level details are emphasized, effectively leading to better relation inversion results.

As the first attempt in this direction, we further contribute the ReVersion Benchmark, which provides various exemplar images with diverse relations. The bench- mark serves as an evaluation tool for future research in the Relation Inversion task. Results on a variety of relations demonstrate the power of preposition prior and our ReVersion framework.

Our contributions are summarized as follows:

- We study a new problem, Relation Inversion, which requires learning a relation prompt for a relation that co-exists in several exemplar images. While existing T2I inversion methods mainly focus on capturing appearances, we take the initiative to explore relation, an under-explored yet important pillar in the visual world.
- We propose the ReVersion framework, where the relation-steering contrastive learning scheme steers the relation prompt using our "preposition prior", and effectively disentangles the learned relation away from object appearances. Relation-focal importance sampling further emphasizes high-level relations over lowlevel details.
- We contribute the ReVersion Benchmark, which serves as a diagnostic and benchmarking tool for the new task of Relation Inversion.


## 2. Related Work

Diffusion Models. Diffusion models [15, 42, 44, 34, 12, 43] have become a mainstream approach for image synthesis $[5,6,28]$ apart from GANs [10], and have shown success in various domains such as video generation [13, 48, 41, 17], image restoration $[38,16]$, and many more $[3,11,1,2]$. In the diffusion-based approach, models are trained using score-matching objectives $[20,49]$ at various noise levels, and sampling is done via iterative denoising. Text-to-Image (T2I) diffusion models $[33,34,6,12,22,30,37]$ demonstrated impressive results in converting a user-provided text description into images. Motivated by their success, we build our framework on a state-of-the-art T2I diffusion model, Stable Diffusion [34].

Relation Modeling. Relation modeling has been explored in discriminative tasks such as scene graph generation [51, 24, 40, 21, 52, 53] and visual relationship detection [27, 54, 55]. These works aim to detect visual relations between objects in given images and classify them into a predefined, closed-set of relations. However, the finite relation category set intrinsically limits the diversity of captured relations. In contrast, Relation Inversion regards relation modeling as a generative task, aiming to capture arbitrary, open-world relations from exemplar images and apply the resulting relation for content creation.

Diffusion-Based Inversion. Given a pre-trained T2I diffusion model, inversion $[9,36,25,23]$ aims to find a text embedding vector to express the concepts in the given exemplar images. For example, given several images of a particular "cat statue", Textual Inversion [9] learns a new word

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-03.jpg?height=515&width=1722&top_left_y=236&top_left_x=169)

Figure 2: ReVersion Framework. Given exemplar images and their entities' coarse descriptions, our ReVersion framework optimizes the relation prompt $\langle\mathrm{R}\rangle$ to capture the relation that co-exists in all the exemplar images. During optmization, the relation-focal importance sampling strategy encourages $\langle\mathrm{R}\rangle$ to focus on high-level relations, and the relation-steering contrastive learning scheme induces the relation prompt $\langle\mathrm{R}\rangle$ towards our preposition prior and away from entities or appearances. Upon optimization, $\langle\mathrm{R}\rangle$ can be used as a word in new sentences to make novel entities interact via the relation in exemplar images.

to describe the appearance of this item - finding a vector in LDM [34]'s text embedding space, so that the new word can be composed into new sentences to achieve personalized creation. Rather than inverting the appearance information (e.g., color, texture), our proposed Relation Inversion task extracts high-level object relations from exemplar images, which is a harder problem as it requires comprehending image compositions and object relationships.

## 3. The Relation Inversion Task

Relation Inversion aims to extract the common relation $\langle\mathrm{R}\rangle$ from several exemplar images. Let $\mathcal{I}=\left\{I_{1}, I_{2}, \ldots I_{n}\right\}$ be a set of exemplar images, and $E_{i, A}$ and $E_{i, B}$ be two dominant entities in image $I_{i}$. In Relation Inversion, we assume that the entities in each exemplar image interacts with each other through a common relation $R$. A set of coarse descriptions $C=\left\{c_{1}, c_{2}, \ldots c_{n}\right\}$ is associated to the exemplar images, where " $c_{i}=E_{i, A}\langle\mathbf{R}\rangle E_{i, B}$ " denotes the caption corresponding to image $I_{i}$. Our objective is to optimize the relation prompt $\langle R\rangle$ such that the co-existing relation can be accurately represented by the optimized prompt.

An immediate application of Relation Inversion is relation-specific text-to-image synthesis. Once the prompt is acquired, one can generate images with novel objects interacting with each other following the specified relation. More generally, this task reveals a new direction of inferring relations from a set of exemplar images. This could potentially inspire future research in representation learning, fewshot learning, visual relation detection, scene graph generation, and many more.

## 4. The ReVersion Framework

### 4.1. Preliminaries

Stable Diffusion. Diffusion models are a class of generative models that gradually denoise the Gaussian prior $\mathbf{x}_{T}$ to the data $\mathbf{x}_{0}$ (e.g., a natural image). The commonly used training objective $L_{\mathrm{DM}}[15]$ is:

$$
\begin{equation*}
L_{\mathrm{DM}}(\theta):=\mathbb{E}_{t, \mathbf{x}_{0}, \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}\left(\mathbf{x}_{t}, t\right)\right\|^{2}\right] \tag{1}
\end{equation*}
$$

where $\mathbf{x}_{t}$ is an noisy image constructed by adding noise $\epsilon \sim$ $\mathcal{N}(\mathbf{0}, \mathbf{I})$ to the natural image $\mathbf{x}_{0}$, and the network $\boldsymbol{\epsilon}_{\theta}(\cdot)$ is trained to predict the added noise. To sample data $\mathbf{x}_{0}$ from a trained diffusion model $\boldsymbol{\epsilon}_{\theta}(\cdot)$, we iteratively denoise $\mathbf{x}_{t}$ from $t=T$ to $t=0$ using the predicted noise $\boldsymbol{\epsilon}_{\theta}\left(\mathbf{x}_{t}, t\right)$ at each timestep $t$.

Latent Diffusion Model (LDM) [34], the predecessor of Stable Diffusion, mainly introduced two changes to the vanilla diffusion model [15]. First, instead of directly modeling the natural image distribution, LDM models images' projections in autoencoder's compressed latent space. Second, LDM enables text-to-image generation by feeding encoded text input to the UNet $[35] \boldsymbol{\epsilon}_{\theta}(\cdot)$. The LDM loss is:

$$
\begin{equation*}
L_{\mathrm{LDM}}(\theta):=\mathbb{E}_{t, \mathbf{x}_{0}, \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}\left(\mathbf{x}_{t}, t, \tau_{\theta}(c)\right)\right\|^{2}\right] \tag{2}
\end{equation*}
$$

where $\mathbf{x}$ is the autoencoder latents for images, and $\tau_{\theta}(\cdot)$ is a BERT text encoder [4] that encodes the text descriptions $c$.

Stable Diffusion extends LDM by training on the larger LAION dataset [39], and changing the trainable BERT text encoder to the pre-trained CLIP [32] text encoder.

Inversion on Text-to-Image Diffusion Models. Existing inversion methods focus on appearance inversion. Given

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-04.jpg?height=493&width=681&top_left_y=239&top_left_x=234)

Figure 3: Part-of-Speech (POS) Clustering. We use tSNE [47] to visualize word distribution in CLIP's input embedding space, where $\langle R\rangle$ is optimized in our ReVersion framework. We observe that words of the same Part-ofSpeech (POS) are closely clustered together, while words of different POS are generally at a distance from each other.

several images that all contain a specific entity, they $[9,36$, 25 ] find a text embedding $\mathrm{V} *$ for the pre-trained T2I model. The obtained $\mathrm{V}^{*}$ can then be used to generate this entity in different scenarios.

In this work, we aim to capture object relations instead. Given several exemplar images which share a common relation $R$, we aim to find a relation prompt $\langle\mathrm{R}\rangle$ to capture this relation, such that " $E_{A}\langle\mathrm{R}\rangle E_{B}$ " can be used to generate an image where $E_{A}$ and $E_{B}$ interact via relation $\langle\mathrm{R}\rangle$.

### 4.2. Preposition Prior

Appearance inversion focuses on inverting low-level features of a specific entity, thus the commonly used pixellevel reconstruction loss is sufficient to learn a prompt that captures the shared information in exemplar images. In contrast, relation is a high-level visual concept. A pixelwise loss alone cannot accurately extract the target relation. Some linguistic priors need to be introduced to represent relations.

In this section, we present the "preposition prior", a language-based prior that steers the relation prompt towards a relation-dense region in the text embedding space. This prior is motivated by a well-acknowledged premise and two interesting observations on natural language.

Premise: Prepositions describe relations. In natural language, prepositions are words that express the relation between elements in a sentence [19]. This language prior naturally leads us to use prepositional words to regularize our relation prompt.

Observation I: POS clustering. As shown in Figure 3, in the text embedding space of language models, embeddings are generally clustered according to their Part-ofSpeech (POS) labels. This observation together with the Premise inspire us to steer our relation prompt $\langle\mathbf{R}\rangle$ towards

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-04.jpg?height=323&width=816&top_left_y=234&top_left_x=1072)

Figure 4: Sparse Activation. We visualize the cosine similarities between real-world relations and basis prepositional words, and observe that relation is generally sparsely activated w.r.t. the basis prepositions. Note that each row of similarity scores are sparsely distributed, with few peak values in red.

the preposition subspace (i.e., the red region in Figure 3).

Observation II: Sparse activation. As shown in Figure 4, feature similarity between the a real-world relation and the prepositional words are sparsely distributed, and the activated prepositions are usually related to this relation's semantic meaning. For example, for the relation "swinging", the sparsely activated prepositions are "underneath", "down", "beneath", "aboard', etc., which together collaboratively describe the "swinging" interaction. This pattern suggests that only a subset of prepositions should be activated during optimization, leading to our noise-robust design in Section 4.3.

Based on the aforementioned analysis, we hypothesize that a common visual relation can be generally expressed as a set of basis prepositions, with only a small subset of highly semantically-related prepositions activated. Motivated by this, we design a relation-steering contrastive learning scheme to steer the relation prompt $\langle\mathrm{R}\rangle$ into a relation-dense region in the text embedding space.

### 4.3. Relation-Steering Contrastive Learning

Recall that our goal is to acquire a relation prompt $\langle R\rangle$ that accurately captures the co-existing relation in the exemplar images. A basic objective is to reconstruct the exemplar images using $\langle R\rangle$ :

$$
\begin{equation*}
\langle R\rangle=\underset{\langle r\rangle}{\arg \min } \mathbb{E}_{t, \mathbf{x}_{0}, \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}\left(\mathbf{x}_{t}, t, \tau_{\theta}(c)\right)\right\|^{2}\right] \tag{3}
\end{equation*}
$$

where $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),\langle\mathrm{R}\rangle$ is the optimized text embedding, and $\boldsymbol{\epsilon}_{\theta}(\cdot)$ is a pre-trained text-to-image diffusion model whose weights are frozen throughout optimization. $\langle r\rangle$ is the relation prompt being optimized, and is fed into the pretrained T2I model as part of the text description $c$.

However, as discussed in Section 4.2, this pixel-level reconstruction loss mainly focus on low-level reconstruction rather than visual relations. Consequently, directly applying this loss could result in appearance leakage and hence unsatisfactory relation inversion.

Motivated by our Premise and Observation I, we adopt the preposition prior as an important guidance to steer the relation prompt towards the relation-dense text embedding subspace. Specifically, we can use the prepositions as positive samples and other POS' words (i.e., nouns, adjectives) as negative samples to construct a contrastive loss. Following InfoNCE [31], this preliminary contrastive loss is derived as:

$$
\begin{equation*}
L_{\mathrm{pre}}=-\log \frac{e^{R^{\top} \cdot P_{i} / \gamma}}{e^{R^{\top} \cdot P_{i} / \gamma}+\sum_{k=1}^{K} e^{R^{\top} \cdot N_{i}^{k} / \gamma}} \tag{4}
\end{equation*}
$$

where $R$ is the relation embedding, and $\gamma$ is the temperature parameter. $P_{i}$ (i.e., positive sample) is a randomly sampled preposition embedding at the $i$-th optimization iteration, and $N_{i}=\left\{N_{i}^{1}, \ldots, N_{i}^{K}\right\}$ (i.e., negative samples) is a set of randomly sampled embeddings from other POS. All embeddings are normalized to unit length.

Since the relation prompt should also be disentangled away from object appearance, we further propose to select the object descriptions of exemplar images as the improved negative set. In this way, our choice of negatives serves two purposes: 1) provides POS guidance away from nonprepositional clusters, and 2) prevents appearance leakage by including exemplar object descriptions in the negative set.

In addition, Observation II (sparse activation) implies that only a small set of prepositions should be considered as true positives. Therefore, we need a contrastive loss that is tolerant about noises in the positive set (i.e., not all prepositions should be activated). Inspired by [29], we revise Equation 4 to a noise-robust contrastive loss as our final Steering Loss:

$$
\begin{equation*}
L_{\text {steer }}=-\log \frac{\sum_{l=1}^{L} e^{R^{\top} \cdot P_{i}^{l} / \gamma}}{\sum_{l=1}^{L} e^{R^{\top} \cdot P_{i}^{l} / \gamma}+\sum_{m=1}^{M} e^{R^{\top} \cdot N_{i}^{m} / \gamma}} \tag{5}
\end{equation*}
$$

where $P_{i}=\left\{P_{i}^{1}, \ldots, P_{i}^{L}\right\}$ refers to positive samples randomly drawn from a set of basis prepositions (more details provided in Supplementary File), and $N_{i}=\left\{N_{i}^{1}, \ldots, N_{i}^{M}\right\}$ refers to the improved negative samples.

### 4.4. Relation-Focal Importance Sampling

In the sampling process of diffusion models, high-level semantics usually appear first, and fine details emerge at later stages $[50,18]$. As our objective is to capture the relation (a high-level concept) in exemplar images, it is undesirable to emphasize on low-level details during optimization. Therefore, we conduct an importance sampling strategy to encourage the learning of high-level relations. Specifically, unlike previous reconstruction objectives, which samples the timestep $t$ from a uniform distribution, we skew the sampling distribution so that a higher probability is assigned to a larger $t$. The Denoising Loss for relation-focal importance sampling becomes:

$$
\begin{align*}
L_{\text {denoise }} & =\mathbb{E}_{t \sim f, \mathbf{x}_{0}, \epsilon}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}\left(\mathbf{x}_{t}, t, \tau_{\theta}(c)\right)\right\|^{2}\right] \\
f(t) & =\frac{1}{T}\left(1-\alpha \cos \frac{\pi t}{T}\right) \tag{6}
\end{align*}
$$

where $f(t)$ is the importance sampling function, which characterizes the probability density function to sample $t$ from. The skewness of $f(t)$ increases with $\alpha \in(0,1]$. We set $\alpha=0.5$ throughout our experiments. The overall optimization objective of the ReVersion framework is written as:

$$
\begin{equation*}
\langle R\rangle=\underset{\langle r\rangle}{\arg \min }\left(\lambda_{\text {steer }} L_{\text {steer }}+\lambda_{\text {denoise }} L_{\text {denoise }}\right) \tag{7}
\end{equation*}
$$

where $\lambda_{\text {steer }}$ and $\lambda_{\text {denoise }}$ are the weighting factors.

## 5. The ReVersion Benchmark

To facilitate fair comparison for Relation Inversion, we present the ReVersion Benchmark. It consists of diverse relations and entities, along with a set of well-defined text descriptions. This benchmark can be used for conducting qualitative and quantitative evaluations.

Relations and Entities. We define ten representative object relations with different abstraction levels, ranging from basic spatial relations (e.g., "on top of"), entity interactions (e.g., "shakes hands with"), to abstract concepts (e.g., "is carved by"). A wide range of entities, such as animals, human, household items, are involved to further increase the diversity of the benchmark.

Exemplar Images and Text Descriptions. For each relation, we collect four to ten exemplar images containing different entities. We further annotate several text templates for each exemplar image to describe them with different levels of details ${ }^{1}$. These training templates can be used for the optimization of the relation prompt.

Benchmark Scenarios. To validate the robustness of the relation inversion methods, we design 100 inference templates composing of different object entities for each of the ten relations. This provides a total of 1,000 inference templates for performance evaluation.

## 6. Experiments

We present qualitative and quantitative results in this section, and more experiments and analysis are in the Supplementary File. We adopt Stable Diffusion [34] for all experiments since it achieves a good balance between quality and speed. We generate images at $512 \times 512$ resolution.[^1](a)
![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=290&width=284&top_left_y=241&top_left_x=186)

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=230&width=269&top_left_y=243&top_left_x=451)

"hamster $\langle R>$

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=233&width=231&top_left_y=241&top_left_x=730)

"cat <R> quilt"

(b)
![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=292&width=530&top_left_y=558&top_left_x=190)

(c)

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=385&width=306&top_left_y=865&top_left_x=194)
exemplar images $\longrightarrow^{<R>}$ Reversion

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=57&width=52&top_left_y=1282&top_left_x=336)
coarse descriptions "horse $<R>$ metal", "rabbit <R> glass", "dog $<R>$ metal", "lion $<R>$ marble"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=236&width=241&top_left_y=240&top_left_x=953)

"dog $<R>$ blanket"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=241&width=251&top_left_y=240&top_left_x=1184)

"panda <R> blanket"

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=238&width=231&top_left_y=239&top_left_x=1424)

"rabbit <R>

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=239&width=228&top_left_y=241&top_left_x=1656)

"rabbit <R> bed sheet"
![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-06.jpg?height=1132&width=1176&top_left_y=548&top_left_x=712)

Figure 5: Qualitative Results. Our ReVersion framework successfully captures the relation that co-exists in the exemplar images, and applies the extracted relation prompt $\langle\mathrm{R}\rangle$ to compose novel entities.

### 6.1. Comparison Methods

Text-to-Image Generation using Stable Diffusion [34]. We use the original Stable Diffusion 1.5 as the text-to-image generation baseline. Since there is no ground-truth textual description for the relation in each set of exemplar images, we use natural language that can best describe the relation to replace the $\langle R\rangle$ token. For example, in Figure 6 (a), the coexisting relation in the reference images can be roughly described as "is painted on". Thus we use it to replace the $\langle\mathrm{R}\rangle$ token in the inference template "Spiderman $\langle R\rangle$ building", resulting in a sentence "Spiderman is painted on building", which is then used as the text prompt for generation.

Textual Inversion [9]. For fair comparison with our method developed on Stable Diffusion 1.5, we use the diffuser [8] implementation of Textual Inversion [9] on Stable Diffusion 1.5. Based on the default hyper-parameter settings, we tuned the learning rate and batch size for its optimal performance on our Relation Inversion task. We use Textual Inversion's LDM objective to optimize $\langle\mathrm{R}\rangle$ for 3000 iterations, and generate images using the obtained $\langle R\rangle$.

### 6.2. Qualitative Comparisons

In Figure 5, we provide the generation results using $\langle R\rangle$ inverted by ReVersion. We observe that our framework is capable of 1) synthesizing the entities in the inference template and 2) ensuring that entities follow the relation coexisting in the exemplar images. We then compare our

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-07.jpg?height=962&width=1738&top_left_y=234&top_left_x=166)

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-07.jpg?height=453&width=456&top_left_y=275&top_left_x=173)

(b)
Text-to-Image Generation

"Spiderman is painted on building"
Textual Inversion

"Spiderman $<R>$ building"

"monkey <R> monkey"

"Spiderman <R> building"

Figure 6: Qualitative Comparisons with Existing Methods. Our method significantly surpasses both baselines in terms of relation accuracy and entity accuracy.

method with 1) text-to-image generation via Stable Diffusion [34] and 2) Textual Inversion [9] in Figure 6. For example, in the first row, although the text-to-image baseline successfully generates both entities (Spiderman and building), it fails to paint Spiderman on the building as the exemplar images do. Text-to-image generation severely relies on the bias between two entities: Spiderman usually climbs/jumps on the buildings, instead of being painted onto the buildings. Using exemplar images and our ReVersion framework alleviates this problem. In Textual Inversion, entities in the exemplar images like canvas are leaked to $\langle R\rangle$, such that the generated image shows a Spiderman on the canvas even when the word "canvas" is not in the inference prompt.

### 6.3. Quantitative Comparisons

We conduct a user study with 37 human evaluators to assess the performance of our ReVersion framework on the Relation Inversion task. We sampled 20 groups of images. Each group has three images generated by different methods. For each group, apart from generated images, the following information is presented: 1) exemplar images of a particular relation 2) text description of the exemplar images. We then ask the evaluators to vote for the best generated image respect to the following metrics.

Entity Accuracy. Given an inference template in the form of "Entity $A\langle R\rangle$ Entity B", we ask evaluators to determine
Table 1: Quantitative Results. Percentage of votes where users favor our results vs. comparison methods. Our method outperforms the baselines under both metrics.

| Method | Relation | Entity |
| :--- | :---: | :---: |
| Text-to-Image Generation | $7.86 \%$ | $15.49 \%$ |
| Textual Inversion | $8.94 \%$ | $10.05 \%$ |
| Ours | $\mathbf{8 3 . 2 0 \%}$ | $\mathbf{7 4 . 4 6 \%}$ |

Table 2: Ablation Study. Suppressing steering or importance sampling introduces performance drops, which shows the necessity of both relation-steering and importance sampling.

| Method | Relation | Entity |
| :--- | :---: | :---: |
| w/o Steering | $11.20 \%$ | $10.90 \%$ |
| w/o Importance Sampling | $11.20 \%$ | $13.62 \%$ |
| Ours | $\mathbf{7 7 . 6 0 \%}$ | $\mathbf{7 5 . 4 8 \%}$ |

whether Entity $A$ and Entity $B$ are both authentically generated in each image.

Relation Accuracy. Human evaluators are asked to evaluate whether the relations of the two entities in the generated image are consistent with the relation co-existing in the exemplar images. As shown in Table 1, our method clearly obtains better results under the two quality metrics.

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-08.jpg?height=968&width=1724&top_left_y=199&top_left_x=165)

(a)

![](https://cdn.mathpix.com/cropped/2024_05_26_0dca812a796b6ef38ae4g-08.jpg?height=447&width=415&top_left_y=243&top_left_x=215)

(b) w/o Steering

“dog $<R>$ basket" w/o Importance Sampling

“dog $<R>$ basket"
Ours

“dog $<R>$ basket" "dog $<R>$ plate" “dog $<R>$ plate"

Figure 7: Qualitative Comparisons with Ablation Variants. Without relation-steering, $\langle\mathrm{R}\rangle$ suffers from appearance leak (e.g., white puppy in (a), gray background in (b)) and inaccurate relation capture (e.g., dog not being on top of plate in (b)). Without importance sampling, $\langle\mathrm{R}\rangle$ focuses on lower-level visual details (e.g., rattan around puppy in (a)) and misses high-level relations.

### 6.4. Ablation Study

From Table A4, we observe that removing steering or importance sampling results in deterioration in both relation accuracy and entity accuracy. This corroborates our observations that 1) relation-steering effectively guides $\langle R\rangle$ towards the relation-dense "preposition prior" and disentangles $\langle R\rangle$ away from exemplar entities, and 2) importance sampling emphasizes high-level relations over low-level details, aiding $\langle\mathbf{R}\rangle$ to be relation-focal. We further show qualitatively the necessity of both modules in Figure 7.

Effectiveness of Relation-Steering. In "w/o Steering", we remove the Steering Loss $L_{\text {steer }}$ in the optimization process. As shown in Figure 7 (a), the appearance of the white puppy in the lower-left exemplar image is leaked into $\langle R\rangle$, resulting in similar puppies in the generated images. In Figure 7 (b), many appearance elements are leaked into $\langle R\rangle$, such as the gray background, the black cube, and the husky dog. The dog and the plate also do not follow the relation of "being on top of" as shown in exemplar images. Consequently, the images generated via $\langle\mathrm{R}\rangle$ do not present the correct relation and introduced unwanted leaked imageries.

Effectiveness of Importance Sampling. We replace our relation-focal importance sampling with uniform sampling, and observe that $\langle\mathrm{R}\rangle$ pays too much attention to low-level details rather than high-level relations. For instance, in
Figure 7 (a) "w/o Importance Sampling", the basket rattan wraps around the puppy's head in the same way as the exemplar image, instead of containing the puppy inside.

## 7. Conclusion

In this work, we take the first step forward and propose the Relation Inversion task, which aims to learn a relation prompt to capture the relation that co-exists in multiple exemplar images. Motivated by the preposition prior, our relation-steering contrastive learning scheme effectively guides the relation prompt towards relation-dense regions in the text embedding space. We also contribute the ReVersion Benchmark for performance evaluation. Our proposed Relation Inversion task would be a good inspiration for future works in various domains such as generative model inversion, representation learning, few-shot learning, visual relation detection, and scene graph generation.

Limitations. Our performance is dependent on the generative capabilities of Stable Diffusion. It might produce suboptimal synthesis results for entities that Stable Diffusion struggles at, such as human body and human face.

Potential Negative Societal Impacts. The entity relational composition capabilities of ReVersion could be applied maliciously on real human figures.

## References

[1] Tomer Amit, Eliya Nachmani, Tal Shaharbany, and Lior Wolf. SegDiff: Image segmentation with diffusion probabilistic models. arXiv preprint arXiv:2112.00390, 2021. 2

[2] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In NeurIPS, 2021. 2

[3] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. In ICLR, 2022. 2

[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 3

[5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. 2

[6] Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. ImageBART: Bidirectional context with multinomial diffusion for autoregressive image synthesis. In NeurIPS, 2021. 2

[7] Patrick Esser, Robin Rombach, and Björn Ommer. A note on data biases in generative models. In NeurIPS Workshop, 2020. 16

[8] Hugging Face. Diffusers. https://huggingface.co/ docs/diffusers/index. 6

[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 4, 6, 7, 11

[10] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 2

[11] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. In NeurIPS, 2022. 2

[12] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, 2022. 2

[13] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. arXiv preprint arXiv:2205.11495, 2022. 2

[14] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 15

[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2, 3

[16] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. JMLR, 2022. 2

[17] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. 2
[18] Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face generation and editing. In CVPR, 2023. 5

[19] Rodney Huddleston and Geoffrey K. Pullum. The Cambridge Grammar of the English Language. Cambridge University Press, 2002. 4

[20] Aapo Hyvärinen and Peter Dayan. Estimation of nonnormalized statistical models by score matching. JMLR, 2005. 2

[21] Jingwei Ji, Ranjay Krishna, Fei-Fei Li, and Juan Carlos Niebles. Action genome: Actions as compositions of spatiotemporal scene graphs. In CVPR, pages 10236-10247, 2020. 2

[22] Yuming Jiang, Shuai Yang, Haonan Qju, Wayne Wu, Chen Change Loy, and Ziwei Liu. Text2human: Text-driven controllable human image generation. ACM TOG, 2022. 2

[23] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276, 2022. 2

[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Fei-Fei Li. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. 2

[25] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. 2, 4

[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 15

[27] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Fei-Fei Li. Visual relationship detection with language priors. In ECCV, 2016. 2

[28] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 2

[29] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, pages 9879-9889, 2020. 5

[30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2

[31] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 5

[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 3, 11

[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image gener-
ation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022. 2

[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3, 5, $6,7,11,16$

[35] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. UNet: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 3

[36] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. 2, 4, 12

[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 2

[38] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image superresolution via iterative refinement. IEEE TPAMI, 2022. 2

[39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. 3

[40] Xindi Shang, Tongwei Ren, Jingfan Guo, Hanwang Zhang, and Tat-Seng Chua. Video visual relation detection. In ACM MM, 2017. 2

[41] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2

[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 2

[43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 2

[44] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 2

[45] Angus Stevenson. Oxford dictionary of English. Oxford University Press, USA, 2010. 15

[46] Patrick Tinsley, Adam Czajka, and Patrick Flynn. This face does not exist... but it might be yours! identity leakage in generative models. In WACV, 2021. 16

[47] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. $J M L R, 9(11), 2008.4$

[48] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 2

[49] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 2011. 2

[50] Binxu Wang and John J. Vastola. Diffusion models generate images like painters: an analytical theory of outline first, details later. arXiv preprint arXiv:2303.02490, 2023. 5

[51] Danfei Xu, Yuke Zhu, Christopher B Choy, and Fei-Fei Li. Scene graph generation by iterative message passing. In CVPR, 2017. 2

[52] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic scene graph generation. In ECCV, pages 178-196. Springer, 2022. 2, 11

[53] Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, Wayne Zhang, Chen Change Loy, and Ziwei Liu. Panoptic video scene graph generation. In CVPR, 2023. 2

[54] Ruichi Yu, Ang Li, Vlad I Morariu, and Larry S Davis. Visual relationship detection with internal and external linguistic knowledge distillation. In ICCV, 2017. 2

[55] Bohan Zhuang, Lingqiao Liu, Chunhua Shen, and Ian Reid. Towards context-aware interaction recognition for visual relationship detection. In ICCV, 2017. 2
