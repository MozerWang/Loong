# Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning 

An-Zi Yen* and Wei-Ling Hsu<br>Department of Computer Science, National Yang Ming Chiao Tung University, Taiwan<br>azyen@nycu.edu.tw, weiling.hsu.cs11@nycu.edu.tw


#### Abstract

Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this position paper, we discuss the challenges associated with employing LLMs to enhance students' mathematical problem-solving skills by providing adaptive feedback. Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions, rationales when attempting to correct students, answers. Three research questions are formulated.


## 1 Introduction

After the pandemic, e-learning has become part of mainstream education (Alqahtani and Rajkhan, 2020; Jafar et al., 2022). However, for students, online learning is not without its problems (Abdur Rehman et al., 2021). Apart from the difficulty in maintaining focus in online classes, the lack of real-time communication and timely feedback are also serious problems. In particular, in online assessments, students may fail to understand their mistakes, even after reviewing the provided answers; such failure to immediately clarify points of confusion yields poor learning outcomes. Synchronous communication between teachers and students in an e-learning setting is necessary, but teachers find that promptly responding to students' questions is a significant challenge.

As large language models (LLMs) offer a wide range of applications, several studies (Tack and Piech, 2022; Kasneci et al., 2023; Zhang et al., 2023) address the use of LLMs in education. In this work, we seek to investigate the integration[^0]

of LLMs into education. Since each subject has its own issues that must be addressed, we explore the scenario of LLM use in mathematics education and the challenges thereof. Many studies address math word problem solving (Shen et al., 2021; Yu et al., 2021; Jie et al., 2022), but the human utility of the mathematics reasoning processes and the natural language explanations generated by models within educational settings is rarely discussed. Given the extraordinary capability of LLMs to generate free-text rationalization, we investigate their mathematical problem-solving competence and assess whether the generated step-by-step explanations constitute a useful educational resource. In particular, we analyze the ability of LLMs to provide adaptive feedback by identifying and explaining errors within a student's math problem-solving process.

We address these issues using the MathQA dataset (Amini et al., 2019), which consists of GRE-level questions that require advanced mathematical knowledge. The questions cover arithmetic, algebra, geometry, and data analysis. In Appendix A we explain in detail our reasoning behind the dataset selection. For all experiments, we use GPT-3.5 (Ouyang et al., 2022). We raise and discuss a total of three research questions. In this pilot study, we conduct both quantitative and qualitative evaluations to answer the research questions. The contributions of this work are threefold.

1. We explore the application of LLMs for instructing students in solving math word problems.
2. Drawing on the results of LLMs to solve math word problems, we comprehensively identify the existing problems of current models.
3. We discuss the pedagogical suitability of the generated equations and the corresponding free-text explanations in the context of mathematics education.

![](https://cdn.mathpix.com/cropped/2024_06_04_a19ccc014daf9c2760a9g-02.jpg?height=360&width=763&top_left_y=234&top_left_x=241)

Figure 1: An LLM helping a student to learn mathematics

## 2 Utility of LLMs in Mathematics Learning

To explore the utility of LLMs in mathematics learning, we raise the first research question (RQ1): How can LLMs be utilized to assist students in learning mathematical problem-solving skills? Since exams are a common way for teachers to evaluate student learning progress, in this study, we focus on leveraging LLMs to equip students with the knowledge skills needed to solve math word problems.

In terms of question types, in addition to true/false and multiple-choice questions, shortanswer questions are also included in math exams. Students answer the questions using problemsolving processes. Automated assessment (Moore et al., 2022) of the student's problem-solving process remains to be investigated. In education, feedback is also crucial (Shute, 2008). Simply scoring the student answer is often insufficient, as scores do not reflect the reasons for incorrect answers. To learn from their mistakes, students need the corresponding explanation. Figure 1 illustrates the scenario of an LLM applied in mathematics education. After a student responds to a question, the model determines whether the student's answer is correct or not. If the answer is correct, the system informs the student of the test objective for that question, enhancing their understanding of the examined skills. If the answer is incorrect, the system provides adaptive feedback (Bernius et al., 2022; Sailer et al., 2023) by indicating the location of the error and offering an explanation, assisting the student to clarify any misunderstanding.

Given that there are multiple methods for solving a math word problem, a model must be able to understand and correct a student's thought process. If the overall strategy is sound apart from minor concept errors or obstacles, the model should guide students to the next step following their chosen ap-

| Category | Questions | Zero-shot | Few-shot | CoT |
| :--- | ---: | ---: | ---: | :---: |
| All | 1,605 | $\mathbf{6 6 . 5 4 \%}$ | $65.67 \%$ | $66.11 \%$ |
| General | 663 | $64.71 \%$ | $63.20 \%$ | $\mathbf{6 4 . 8 6 \%}$ |
| Gain | 345 | $71.88 \%$ | $\mathbf{7 2 . 1 7 \%}$ | $70.14 \%$ |
| Physics | 410 | $\mathbf{6 8 . 5 4 \%}$ | $65.37 \%$ | $67.32 \%$ |
| Geometry | 100 | $63.00 \%$ | $\mathbf{6 5 . 0 0 \%}$ | $60.00 \%$ |
| Probability | 12 | $58.33 \%$ | $\mathbf{8 3 . 3 3 \%}$ | $75.00 \%$ |
| Other | 75 | $53.33 \%$ | $57.33 \%$ | $\mathbf{5 8 . 6 7 \%}$ |

Table 1: MathQA results

proach. Thus the ability of LLMs to solve math word problems and to understand and explain equations is critical. The following sections address these two key points.

## 3 LLM Ability to Solve Math Problems

Many textbook math exercises provide the calculation process but lack a natural language explanation. Accompanying each step with an explanation would greatly enhance the student's understanding of each equation. This leads to the second research question (RQ2): What is the problem-solving capability of LLMs in mathematical problems and their ability to explain the computational process? Automatic Evaluation: The dataset used in this work is a modified version of MathQA in which unsolvable questions were removed by Jie et al. (2022). We utilized OpenAI's API, in particular the "gpt-3.5-turbo-0301" model. The temperature was set to 0 . Table 1 shows the accuracy of three commonly used prompting methods: zeroshot prompting (Zero-shot), few-shot prompting (Few-shot), and chain-of-thought (CoT) prompting. Three prompting templates are shown in Appendix B. "Questions" denotes the number of questions of each type in the test set. We also show the results for six MathQA question types. In this experiment, we compared only the model-generated answers with the provided answers, without verifying the correctness of the reasoning process. As shown in Table 1, the CoT performance was poorer than expected, possibly due to the higher mathematical skill demands of MathQA compared to previously-used datasets. Zero-shot prompting does not significantly outperform $\operatorname{CoT}(p<0.8$ ). Exploring suitable prompting methods is left as future work. Based on the results, we observe that the calculation abilities of the GPT-3.5 model remain far from satisfactory. It frequently fails at simple arithmetic operations or counting.

Human Evaluation of LLM Results: It is known that LLMs might produce the correct answer even

| Error type | Percentage |
| :--- | ---: |
| Misconception in problem-solving | $36.54 \%$ |
| Incorrect provided answer* | $17.31 \%$ |
| Unclear question definition* | $11.54 \%$ |
| Calculation error in equation | $9.61 \%$ |
| Misinterpretation of question | $7.69 \%$ |
| Arithmetic error | $5.77 \%$ |
| Absence of necessary diagrams* | $3.85 \%$ |
| Counting error | $3.85 \%$ |
| Undefined symbols in question* | $1.92 \%$ |
| Incomplete problem-solving | $1.92 \%$ |

Table 2: Error types annotated by an expert. * indicates that the error is not from the model's response but from an error in the question.

with incorrect reasoning, or give an incorrect answer despite correct reasoning (Laskar et al., 2023; Lightman et al., 2023). However, detailed analyses and statistical evaluations of model errors have been less extensively studied. To further analyze whether LLMs are able to reason through complex mathematical problems, we invited an expert who majored in Mathematics to evaluate the answers generated by the GPT-3.5 model. A total of 120 questions-20 from each of the six question typeswere selected by the expert. Each selected question involved a process of reasoning and calculation of more than four steps. The error types are shown in Table 2. The most common error made by the model was "misconception in problem-solving": the model understood the question but used incorrect formulas or methods to solve it. "Misinterpretation of the question", in turn, is a different error: the model does not understand the question and generates an unrelated result. We also find that the GPT-3.5 model is good at providing comprehensive explanations for each equation without omitting parts of the problem-solving process. However, it exhibits inconsistent calculations, often making simple arithmetic errors. Furthermore, its grasp of set theory and three-dimensional spatial reasoning is limited. Examples of some error types are provided in Appendix C.

Research Issues of Augmented Language Models: Using external tools for calculation may be one way to address LLM drawbacks. Mialon et al. (2023) refer to language models that utilize external tools (Gao et al., 2022; Liu et al., 2022), retrieve relevant information (Izacard et al., 2022), or use specific reasoning strategies (Wei et al., 2022) as augmented language models (ALMs). We argue that for a model to solve more complex tasks, it should comprehend the tasks and know when, how, and why to request augmentation. Otherwise, the improvements yielded by the augmented information would remain limited in various real-world applications. For instance, for mathematical calculations, Schick et al. (2023) propose having the LLM use a calculator API to solve arithmetic tasks. However, the propensity of LLMs to misinterpret question meanings and substitute incorrect numbers remains a prominent challenge in advanced mathematical reasoning. From our observation, the GPT-3.5 model behaves much like a student focused on formula memorization in that it struggles to adapt to variations in the question, particularly in probability questions. Hence, enhancing the mathematical reasoning capabilities of LLMs is a critical research direction.

## 4 Pedagogical Ability of LLMs to Rectify Students' Answers

The most important thing when helping students to learn mathematical problem-solving is providing immediate adaptive feedback. In this section, we measure the pedagogical ability of LLMs in mathematics. Pedagogical ability refers to the ability to understand and help the student (Tack and Piech, 2022). This leads to the third research question (RQ3): Are LLMs able to identify errors in students' answers and provide corresponding explanations?

## Teacher-Student Framework for Quantitative

 Evaluation: Due to the difficulty in obtaining real-world student answers, we simulate a scenario in which students answer questions and teachers correct the students' responses. Based on the experimental results from Table 1, we use the responses from zero-shot prompting as the student answers. These answers are then input into the GPT-3.5 model, which acts as a teacher, correcting the student's answers according to the question. The GPT-3.5 model is tasked with identifying whether the student's answer is correct and explaining why. Specifically, given an input question $q$, prompt $\mathcal{P}_{s}$, and model $\mathcal{M}$, we obtain the initial problem-solving result $y_{s}=\mathcal{M}\left(q ; \mathcal{P}_{s}\right)$. Next, we input $y_{s}$ to $\mathcal{M}$, and ask $\mathcal{M}$ to act as a teacher with prompt $\mathcal{P}_{t}$ to correct $y_{s}$ based on $q$. Finally, we obtain the feedback $y_{t}=\mathcal{M}\left(q, y_{s}, r ; \mathcal{P}_{t}\right)$, where $r$ is the rationale of $q$ provided in MathQA. If $\mathcal{M}$ struggles to understand its responses (with detailed processes and natural language explanations), then its potential to assist teachers in corrections be-| Type | W/o rationale | W/ rationale |
| :--- | ---: | ---: |
| All | $53.96 \%$ | $73.71 \%$ |
| General | $54.75 \%$ | $73.30 \%$ |
| Gain | $54.78 \%$ | $73.33 \%$ |
| Physics | $50.00 \%$ | $72.20 \%$ |
| Geometry | $60.00 \%$ | $83.00 \%$ |
| Probability | $25.00 \%$ | $83.33 \%$ |
| Other | $61.33 \%$ | $73.33 \%$ |

Table 3: Results of teacher-student framework

| Result of $y_{s}$ | $y_{s}$ is correct |  | $y_{s}$ is incorrect |  |
| :--- | ---: | ---: | ---: | ---: |
| $\mathcal{P}_{t}$ | w/o $r$ | w/ $r$ | w/o $r$ | w/ $r$ |
| Identify $y_{s}$ is correct | $63.24 \%$ | $10.29 \%$ | $53.85 \%$ | $0.00 \%$ |
| Say in other words | $17.65 \%$ | $67.65 \%$ | $17.31 \%$ | $17.31 \%$ |
| Correct the process | $11.76 \%$ | $16.18 \%$ | $11.53 \%$ | $50.00 \%$ |
| Correct the calculation | $7.35 \%$ | $5.88 \%$ | $17.31 \%$ | $32.69 \%$ |

Table 4: Error types in teacher-student framework

comes questionable. The $\mathcal{P}_{t}$ template is shown in Appendix B. We refer to this framework as the "teacher-student framework".

Table 3 presents the results of the teacherstudent framework. Accuracy is adopted as the evaluation metric: accuracy measures whether $\mathcal{M}$ correctly identifies the correctness of $y_{s}$. As $y_{t}$ could be a lengthy paragraph, we simply use keywords such as "is correct", "is incorrect", "not correct", "almost correct" or "not correct" to identify $\mathcal{M}$. We compare the results with and without the question rationales. Interestingly, if the corresponding rationales are not given as input, the accuracy of the GPT-3.5 model acting as a teacher to correct students' answers $(53.96 \%$ ) is lower than that when directly answering questions ( $66.54 \%$ ). However, if the corresponding rationales are given as input, accuracy is only $73.71 \%$. Thus the GPT-3.5 model has difficulty understanding the equations given in the rationales.

## Human Evaluation of Correcting Model-

 Generated Answers: To understand why the GPT3.5 model exhibits these characteristics when correcting answers, our expert also analyzed the results of the teacher-student framework based on the selected 120 questions. Examples are given in Appendix D. Table 4 presents the correction results $y_{t}$ for $y_{s}$ by human evaluation. "w/o $r$ " and "w/ $r$ " denote $\mathcal{P}_{t}$ without and with rationales, respectively. Comparing the results of "w/o $r$ " and "w/ $r$ ", we find that providing rationales seriously confuses $\mathcal{M}$, such that it determines $y_{s}$ is wrong in most cases. Furthermore, $\mathcal{M}$ simply rephrases the content of $y_{s}(67.65 \%)$. Note that the results in Table 3 show that $\mathcal{P}_{t}$ with $r$ is better than that without $r$. However, the results in Table 4 are different, because the questions selected by the expert are more challenging. If $y_{s}$ is incorrect, $\mathcal{M}$ has a $53.85 \%$ chance of erroneously claiming that $y_{s}$ is correct when $r$ is not provided. When $r$ is given, $\mathcal{M}$ correctly identifies that $y_{s}$ is incorrect. Nonetheless, $\mathcal{M}$ has only a $10.29 \%$ chance to accurately identify $y_{s}$ as correct with $r$. In addition, according to our statistic, $\mathcal{M}$ has only a $3.85 \%$ and $1.92 \%$ chance to accurately correct the calculation results and the problem-solving processes, respectively. This is primarily because it has difficulty exploiting $r$ and because it usually misunderstands the $y_{s}$ equations, even to the point of inaccurately correcting those that are already correct. Furthermore, it often forgets the equations and calculations in $y_{s}$. To verify whether LLMs can understand and correct human problem-solving processes, we also invited five college students to answer three questions. The results are presented in Appendix E.Utility of LLMs in Complex Reasoning Tasks for Education: The failures of the GPT-3.5 model are comprehensively analyzed by Borji (2023). In this work, we find that LLMs tend to be confused by human answers, especially when tasks demand advanced knowledge or reasoning skills, even if relevant and correct information is provided. Hence, an alternative framework is needed under which to leverage LLMs' language understanding ability in complex reasoning tasks. Other crucial research issues are how to make the models aware of what they do not know and how to produce truthful and interpretable results (Phillips et al., 2020). Besides, this work primarily focuses on the capabilities and challenges of directly using LLMs for correcting students' answers. Developing a cognitive model for reasoning processes and their potential roles in rectifying student mistakes is crucial.

## 5 Conclusion

In this work we propose a research agenda for leveraging LLMs in mathematics learning. First, we explore the use of LLMs to assist students in learning math word problem-solving skills. Then, we analyze the mathematical reasoning ability of LLMs. Finally, we investigate the pedagogical ability of LLMs in terms of rectifying model-generated or human answers and offering adaptive feedback. We conduct experiments with the GPT-3.5 model, and conclude that there remains room for improve-
ment in the LLM's performance in solving complex mathematical problems. In addition, although it generates comprehensive explanations, the LLM is limited in accurately identifying model's and human's errors due to its poor ability to interpret mathematical equations. In the future, we plan to develop an advanced method by which to improve the utility of LLMs in mathematics education.

## Limitations

Considering that LLMs are widely accessible to the general public and many educational institutions are examining the challenges and benefits of their use by teachers and students, this paper primarily focuses on the application of LLMs in educational settings. However, our experiments employed only the GPT-3.5 model and did not explore other LLMs such as GPT-4 (OpenAI, 2023) and LLaMA (Touvron et al., 2023). Furthermore, while our current work investigates the utility of LLMs in enhancing students' mathematical problem-solving skills, there are many other applications in education. For instance, LLMs could be utilized to help teachers to generate teaching content or questions. Additionally, the potential issues of violating teaching ethics involved in introducing LLMs into educational applications are significant topics that have not been included in this study currently. We also conducted a human evaluation on 120 questions, which may be insufficient. Although we invited the expert to select questions that cover as many types of questions as possible, there may remain worthwhile examples that were not selected for analysis. Moreover, the mistakes made by humans and those made by LLMs may differ. However, at the current stage, we invited only five college students to answer three questions: a larger-scale experiment would be more helpful.

## Ethics Statement

In the context of educational applications, we utilize students' personal data and answer records for our experiments, which raises privacy concerns. For this study, we invited college students to answer mathematical questions to investigate issues that might be found in real-world applications. These participants fully understood how their data would be used, and we ensured that their personal information would not be leaked.

## Acknowledgements

This research was partially supported by National Science and Technology Council, Taiwan, under grant NSTC 111-2222-E-A49-010-MY2.

## References

Mohsin Abdur Rehman, Saira Hanif Soroya, Zuhair Abbas, Farhan Mirza, and Khalid Mahmood. 2021. Understanding the challenges of e-learning during the global pandemic emergency: The students' perspective. Quality Assurance in Education, 29(2/3):259276.

Ammar Y Alqahtani and Albraa A Rajkhan. 2020. Elearning critical success factors during the COVID19 pandemic: A comprehensive analysis of elearning managerial perspectives. Education Sciences, 10(9):216.

Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367.

Jan Philip Bernius, Stephan Krusche, and Bernd Bruegge. 2022. Machine learning based feedback on textual student answers in large courses. Computers and Education: Artificial Intelligence, 3:100081.

Ali Borji. 2023. A categorical archive of ChatGPT failures. arXiv preprint arXiv:2302.03494.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. PAL: Program-aided language models. arXiv preprint arXiv:2211.10435.

Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan Cotterell, and Zhijing Jin. 2022. State-of-the-art generalisation research in NLP: a taxonomy and review. CoRR.

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299.

Adi Jafar, Ramli Dollah, Nordin Sakke, Mohammad Tahir Mapa, Ang Kean Hua, Oliver Valentine Eboy, Eko Prayitno Joko, Diana Hassan, and Chong Vun Hung. 2022. Assessing the challenges of e-learning in Malaysia during the pandemic of COVID-19 using the geo-spatial approach. Scientific Reports, 12(1):17316.

Zhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem solving as complex relation extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $5944-5955$.

Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Stepha Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci. 2023. ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences, 103:102274.

Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. arXiv preprint arXiv:2305.18486.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. arXiv preprint arXiv:2305.20050.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35.

Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and Andrew M Dai. 2022. Mind's Eye: Grounded language model reasoning through simulation. arXiv preprint arXiv:2210.05359.

Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented language models: A survey. arXiv preprint arXiv:2302.07842.

Steven Moore, Huy A Nguyen, Norman Bier, Tanvi Domadia, and John Stamper. 2022. Assessing the quality of student-generated short answer questions using GPT-3. In Educating for a New Future: Making Sense of Technology-Enhanced Learning Adoption: 17th European Conference on Technology Enhanced Learning, EC-TEL 2022, Toulouse, France, September 12-16, 2022, Proceedings, pages 243257. Springer.

OpenAI. 2023. Gpt-4 technical report.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094.

P Jonathon Phillips, Carina A Hahn, Peter C Fontana, David A Broniatowski, and Mark A Przybocki. 2020. Four principles of explainable artificial intelligence. Gaithersburg, Maryland, page 18.

Michael Sailer, Elisabeth Bauer, Riikka Hofmann, Jan Kiesewetter, Julia Glas, Iryna Gurevych, and Frank Fischer. 2023. Adaptive feedback from artificial neural networks facilitates pre-service teachers' diagnostic reasoning in simulation-based learning. Learning and Instruction, 83:101620.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools arXiv preprint arXiv:2302.04761.

Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021. Generate \& rank: A multi-task framework for math word problems. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2269-2279.

Valerie J Shute. 2008. Focus on formative feedback. Review of Educational Research, 78(1):153-189.

Anaïs Tack and Chris Piech. 2022. The AI Teacher Test: Measuring the pedagogical ability of Blender and GPT-3 in educational dialogues. In Proceedings of the 15th International Conference on Educational Data Mining, page 522.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845-854.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.

Cheng-Kuang Wu, Wei-Lin Chen, and Hsin-Hsi Chen. 2023. Large language models perform diagnostic reasoning.

Weijiang Yu, Yingpeng Wen, Fudan Zheng, and Nong Xiao. 2021. Improving math word problems with pre-trained knowledge and hierarchical reasoning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages $3384-3394$.

Sarah J Zhang, Samuel Florin, Ariel N Lee, Eamon Niknafs, Andrei Marginean, Annie Wang, Keith Tyser, Zad Chin, Yann Hicke, Nikhil Singh, Madeleine Udell, Yoon Kim, Tonio Buonassisi, Armando Solar-Lezama, and Iddo Drori. 2023. Exploring the MIT mathematics and eecs curriculum using large language models. arXiv preprint arXiv:2306.08997.
