# Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed? 

Tannon Kew ${ }^{1 *} \quad$ Florian Schottmann ${ }^{2,3} \quad$ Rico Sennrich ${ }^{1,4}$<br>${ }^{1}$ University of Zurich, ${ }^{2}$ Textshuttle, ${ }^{3}$ ETH Zurich, ${ }^{4}$ University of Edinburgh<br>\{kew,sennrich\}@cl.uzh.ch, schottmann@textshuttle.com


#### Abstract

The vast majority of today's large language models are English-centric, having been pretrained predominantly on English text. Yet, in order to meet user expectations, models need to be able to respond appropriately in multiple languages once deployed in downstream applications. Given limited exposure to other languages during pretraining, crosslingual transfer is important for achieving decent performance in non-English settings. In this work, we investigate just how much multilinguality is required during finetuning to elicit strong cross-lingual generalisation across a range of tasks and target languages. We find that, compared to English-only finetuning, multilingual instruction tuning with as few as three languages significantly improves a model's cross-lingual transfer abilities on generative tasks that assume input/output language agreement, while being of less importance for highly structured tasks. Our code and data is available at https://github.com/ZurichNLP/ multilingual-instruction-tuning.


## 1 Introduction

Conversational instruction tuning is a popular method for aligning large pretrained language models (LLMs) with user expectations such that they can effectively respond to a user's input query and follow natural language instructions (Ouyang et al., 2022; Wang et al., 2023; Chiang et al., 2023). An implicit expectation of conversational chatbots is that the language of a model's response should match that of the user's input query. For instance, unless otherwise specified, a German-language input should result in a German-language output. However, since the vast majority of pretraining and tuning data used to develop these models is in English, many instruction-tuned LLMs struggle to respond consistently in other languages (Touvron[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_c119aa28c6a06f3e6af4g-01.jpg?height=523&width=760&top_left_y=761&top_left_x=1068)

Figure 1: Input/output (IO) language agreement for English (en), German (de), Bulgarian (bg) and Icelandic (is) when instruction tuning on monolingual English (Mono) or on multilingual data (Multi-Guanaco). Striped bars indicate that the target language is included in the finetuning data. Error bars show a confidence interval of $95 \%$.

et al., 2023b; Chen et al., 2023; Ye et al., 2023; Zhang et al., 2023b).

Despite this limited exposure, English-centric LLMs such as Llama 2 can achieve near-perfect input/output (IO) language agreement when tuned with relatively few multilingual conversational instructions. Figure 1 depicts IO language agreement, as measured by OpenLID ${ }^{1}$ (Burchell et al., 2023) and compares multilingual tuning on the language-diverse Guanaco dataset (Dettmers et al., 2023; Köpf et al., 2023) with monolingual tuning on an English-only subset of instructions. As can be seen, multilingual tuning elicits strong IO language agreement, well within the bounds of language identification error rates, for non-English languages seen to differing degrees during pretraining and finetuning without degrading performance on English.

This observation raises two major ques-[^1]tions, which we aim to address in this paper: Q1: How much multilinguality is required during finetuning to elicit few- and even zero-shot crosslingual generalisation in English-centric LLMs? Q2: Which languages and tasks benefit most from multilingual instruction tuning of English-centric LLMs?

To investigate these questions, we instructiontune English-centric LLMs, varying the number of languages seen during finetuning and evaluate performance in multiple target languages and on different tasks. We consider both high- and lowresource language settings with different scripts and investigate performance on generative tasks, such as open-ended chat and extractive question answering, as well as structured tasks aimed at assessing commonsense reasoning and language understanding.

Our results indicate that multilingual instruction tuning is crucial for eliciting cross-lingual transfer on generative tasks that assume IO language agreement while being less important in structured tasks that are commonly used to benchmark LLM performance. Furthermore, we empirically show that only a small number of finetuning languages is required to promote cross-lingual transfer. This highlights that tuning data for all potential target languages is not necessary to derive a capable polyglot chat model from an English-centric LLM.

## 2 Related Work

### 2.1 Instruction Tuning LLMs

Instruction tuning describes a supervised finetuning (SFT) strategy that aims to provide a model with a diverse set of demonstrations of user input queries paired with desirable model outputs. Unlike taskspecific SFT, instruction tuning aims to promote cross-task generalisation, allowing for a 'generalist' model that is capable of completing any text-based task on the basis of natural language instructions provided at inference time (Mishra et al., 2022; Wei et al., 2022; Wang et al., 2022; Sanh et al., 2022; Longpre et al., 2023). Meanwhile, framing instructions in a conversational manner and over multiple dialogue turns has been shown to be effective at deriving performant chat models (Taori et al., 2023; Conover et al., 2023; Chiang et al., 2023; Dettmers et al., 2023; Ding et al., 2023). Furthermore, LLM instruction tuning remains effective given relatively limited labelled data (Ouyang et al., 2022; Touvron et al., 2023b; Zhou et al., 2023), parameter efficient methods (Hu et al., 2021; Zhang et al., 2023a) and model quantisation (Dettmers et al., 2023; Li et al., 2023).

### 2.2 Cross-lingual Transfer in English-centric LLMs

Despite numerous pushes towards improving NLP support for non-English and low-resource languages (Costa-jussà et al., 2022; Le Scao et al., 2023), the vast majority of today's publicly available LLMs are English-centric. For instance, GPT3's training data consisted of approximately $93 \%$ English documents with the remaining $7 \%$ pertaining to other languages (Brown et al., 2020). ${ }^{2}$ This trend is further reflected in popular open-weight LLMs (see Table 1). One potential reason for this could be the "the curse of multilinguality" (Conneau et al., 2020) which describes how as multilinguality increases, evaluation performance decreases. For example, multilingual LLMs have been shown to sacrifice performance on English tasks due to having to share a finite amount of pretraining tokens and model parameters across more languages (Lin et al., 2022; Le Scao et al., 2022). Consequently, the cross-lingual transfer abilities of more performant, English-centric models is important for many NLP practitioners looking to deploy these models in real-world settings.

A number of works have reported on the multilingual abilities of proprietary models such as GPT-3 and its derivatives across a range of NLU and NLG benchmarks (Lai et al., 2023a; Holmström et al., 2023; Armengol-Estapé et al., 2022). On tasks such as machine translation (MT), GPT3.5-Turbo has been shown to outperform dedicated systems for some high-resource languages (Hendy et al., 2023; Lu et al., 2023; Jiao et al., 2023; Bang et al., 2023), particularly when translating between two non-English languages (Laskar et al., 2023). However, performance on low-resource languages generally lags behind.

Meanwhile, other studies have focused on openweight LLMs. Ye et al. (2023) compare multilingual reasoning capabilities of pretrained BLOOM (Le Scao et al., 2023) and LLaMA (Touvron et al., 2023a) and find that despite minimal amounts of non-English pretraining data, LLaMA has strong cross-lingual transfer abilities. Muennighoff et al. (2023) find that multilingual multitask finetuning improves BLOOM's zero-shot abilities across all[^2]target languages compared with English-only finetuning. Finally, Chen et al. (2023) show that largescale multilingual instruction tuning can improve performance on open-ended chat in multiple target languages.

This last work is most closely related to our current work but differs in a few key factors. Firstly, we investigate multilingual chat capabilities of more recent English-centric models, pretrained with significantly more data. Secondly, inspired by Zhou et al. (2023) we adopt a less-is-more finetuning approach, controlling the amount and distribution of multilingual data. In doing so, we focus our analysis on the minimal amount of multilinguality needed to elicit cross-lingual transfer. Finally, our evaluations consider more target languages and distinct downstream tasks.

## 3 Experimental Setup

To explore the multilingual capabilities of Englishcentric LLMs, we instruction-tune a series of models on a fixed-size set of examples, varying the number of languages available. Following this, we evaluate the resulting models in multiple target languages on four distinct tasks that are representative of how LLMs may be used in downstream applications.

### 3.1 English-centric LLMs

A prevailing trend in the development of recent LLMs is a clear focus on scaling up the size of the pretraining corpus (Hoffmann et al., 2022). For instance, open-weight LLMs such as Falcon (Almazrouei et al., 2023) and Llama 2 (Touvron et al., 2023b) were trained on 1.5 and 2 trillion tokens respectively. Yet, although these numbers far surpass the 300 billion tokens used to train GPT-3 (Brown et al., 2020), the distribution of language coverage remains similar across models with more than $90 \%$ pertaining to English (see Table 1). For our main experiments, we focus on Llama $27 \mathrm{~b}$, but also consider Llama 2 70b (\$5.2) and Falcon 7b (Appendix E) to study the effect of model scaling and different training approaches respectively.

### 3.2 Instruction-tuning Data

For instruction tuning, we take inspiration from Dettmers et al. (2023) and finetune on high-quality conversations from the OpenAssistant dataset (Köpf et al., 2023). These conversations comprise multiple dialogue turns between crowdwork- ers who were asked to either interact with or assume the role of a helpful $\mathrm{AI}$ assistant.

In contrast to Dettmers et al. (2023), who use all 9,846 top-rated conversations to train their 'Guanaco' models, we subsample training instances from the Guanaco dataset in order to control the amount of multilinguality. Specifically, we sample 3,200 unique English instances as an initial monolingual dataset, which we refer to as 'Mono'. To construct datasets for multilingual finetuning we sample 200 unique training examples from each of five most frequent non-English languages in Guanaco (Spanish, Russian, German, Chinese, French). Given these subsets, we incrementally substitute English examples in Mono for non-English ones, one language at a time, following the order of how frequently each language appears in Guanaco. The resulting multilingual datasets are denoted as Multi$i$, where $i$ equals the number of distinct languages included. For comparison, we also train models on the full Guanaco dataset, which includes more than 30 distinct languages.

### 3.3 Instruction-tuning and Inference Settings

For efficient instruction tuning, we use LoRA adapters ( $R=64, \alpha=16$ ), leveraging Hugging Face's TRL library 3 . We train all models for 2,000 update steps, using an effective batch size of 64 and a learning rate of $1^{e-5}$. Sequences longer than 1024 tokens are truncated. For 7-billion parameter models, we use two NVIDIA GeForce RTX 3090 with $24 \mathrm{~GB}$ of memory. The time required for each training run is approximately 8 hours. For the larger 70 -billion parameter model, discussed in $\S 5.2$, we use the same hyperparameters and train on four NVIDIA A100 GPUs with 80GB of memory each. Here, a single training run takes approximately 20 hours.

At inference time, we load the instruction-tuned models with vLLM (Kwon et al., 2023). For openended generation tasks, we use top-p sampling $(\mathrm{p}=0.9$ ) with a temperature of 0.8 and frequency penalty of 0.2 . For the more constrained generation tasks we reduce the temperature to 0.001 . To account for randomness under these sampling parameters, we report aggregated results for all tasks by repeating inference experiments with three random seeds.[^3]

### 3.4 Evaluation Tasks and Languages

As evaluation tasks, we consider open-ended chat, extractive question answering, commonsense reasoning and natural language inference. As all of these tasks differ in terms of the availability and representation of ground-truth labels, we describe the specific evaluation strategies in the following section.

Finally, we investigate performance on different target languages, which we select based on the makeup of Llama 2's pretraining data (see Table 1). In doing so, we aim to investigate i) the supervised setting, where a target language has been seen during pretraining and finetuning; ii) the zero-shot cross-lingual setting, where a target language may have been seen during pretraining but not during finetuning (Wu and Dredze, 2019); and iii) the extreme low-resource setting, where the target language has not intentionally been seen at any stage of the training procedure. According to Touvron et al. (2023b), German (de), French (fr), Swedish (sv), Chinese (zh), Spanish (es) and Russian (ru) represent the most frequently seen non-English pretraining languages with an estimated 3.4 to 2.6 billion tokens. Catalan (ca), Norwegian (no) and Bulgarian (bg) represent languages less frequently seen during pretraining with an estimated 800 to 400 million tokens. And finally, we select Icelandic (is), Hindi (hi) and Greek (el) to represent extremely low-resource languages, whose frequency in Llama 2's pretraining data is not available but may still appear in small amounts due to intentional or unintentional contamination (Blevins and Zettlemoyer, 2022). Unfortunately, existing benchmark evaluation datasets do not cover all of these languages and thus we limit the target languages in those tasks to the available subset of our target languages.

## 4 Experiments

### 4.1 Chat

General-purpose chatbots are a popular application of instruction-tuned LLMs. To evaluate chat performance, we make use of the AlpacaEval prompt dataset (Dubois et al., 2023), which includes a diverse set of prompts for open-ended questions, creative writing, brainstorming, and other tasks. We randomly sample 300 prompts and translate these into each target language. As a translation engine, we follow Lai et al. (2023b) and use GPT-
3.5-Turbo ${ }^{4}$. In contrast to using dedicated translation systems, employing GPT-3.5-Turbo for this purpose has the advantage of being able to explicitly specify instructions that allow for preserving code blocks, tables and terminology, which we include as part of our translation prompt (see Figure B). Furthermore, since GPT-3.5-Turbo is trained on instruction- and conversational-style data, we expect it to perform well at translating in this domain.

To automatically evaluate open-ended chat responses, we leverage LLM-as-a-judge (Zheng et al., 2023). Following (Zhou et al., 2023), given an input prompt and model's response, we ask GPT-3.5Turbo ${ }^{5}$ to grade the helpfulness of the response on a 6-point Likert scale (see Figure 16 for the prompt used). For each evaluation instance, we provide the prompt and model-generated response directly in the target language, which we found to be on par with evaluating via first translating responses into English (cf. Hada et al., 2023) (see Appendix C for more details). As noted by Chen et al. (2023), GPT-3.5-Turbo sometimes ignores the fact that the output language differs from the input language. Therefore, we force a score of 1 (indicating least helpful) if the language of the response does not match the intended target language.

Results Figure 2 shows the helpfulness scores assigned by GPT-3.5-Turbo for all models for each of the target languages.

For English, we observe a reasonably strong and stable performance across all incremental multilingual instruction tuning settings, indicating that chat performance does not diminish with the increase in non-English training examples. In contrast, for most non-English target languages, performance increases significantly when moving from monolingual to bilingual instruction tuning. Most notably though, performance tends to plateau when instruction tuning with as few as three different languages. This observation holds for languages in both the supervised setting and the zero-shot setting, indicating that three finetuning languages is sufficient to elicit zero-shot cross-lingual transfer on this task. For languages in the extremely low-resource category, performance remains low despite multilingual finetuning. Manual inspection of these[^4]

![](https://cdn.mathpix.com/cropped/2024_06_04_c119aa28c6a06f3e6af4g-05.jpg?height=437&width=1585&top_left_y=244&top_left_x=241)

Figure 2: Average helpfulness of chat responses from Llama $27 \mathrm{~b}$ with incremental multilingual instruction tuning. Striped bars indicate that the target language is included in the finetuning data. Error bars show a confidence interval of $95 \%$.

![](https://cdn.mathpix.com/cropped/2024_06_04_c119aa28c6a06f3e6af4g-05.jpg?height=428&width=1582&top_left_y=888&top_left_x=246)

Figure 3: X-QuAD results for Llama 2 7b with incremental multilingual instruction tuning. Results are shown for both multilingual prompting (en:xx) and monolingual prompting (xx:xx). Striped bars indicate that the target language is included in the finetuning data. Error bars show a confidence interval of $95 \%$.

outputs reveal that, while these responses generally look convincing at first glance and are sufficient for language identification (see Figure 1), they are mostly nonsensical. This suggests that useful zeroshot cross-lingual transfer abilities of Llama $27 \mathrm{~b}$ are limited to languages seen with higher frequency during pretraining.

### 4.2 Extractive Question Answering

In contrast to open-ended questions commonly used to query LLMs in chat settings, extractive question answering requires the model to identify relevant answer spans within longer context passages provided as part of the prompt. This kind of task closely resembles a retrieval augmented generation (RAG) setting, which is a popular method for extending an LLM's knowledge with additional data not available during training (Lewis et al., 2020; Izacard and Grave, 2021). To evaluate our models on this task in multiple target languages, we use XQuAD (Artetxe et al., 2020). ${ }^{6}$[^5]

For this task, we consider both a multilingual prompt setting and a monolingual prompt setting. The former presents the task instruction in English and provides the context passage and question in the relevant target language (en:xx), while the latter is entirely in the target language (xx:xx). As a starting point, we borrow the English prompt from Lai et al. (2023a) and manually translate it into each of the target languages considered. Additionally, we include a standardised response prefix as part of the prompt, effectively force-decoding the response "Based on the passage, the answer to the question is". This allows us to better isolate the relevant answer string in the generative model's output. A response is considered correct if the model's answer string matches the ground truth answer after minimal postprocessing. ${ }^{7}$ An example

![](https://cdn.mathpix.com/cropped/2024_06_04_c119aa28c6a06f3e6af4g-05.jpg?height=46&width=454&top_left_y=2401&top_left_x=1052)

${ }^{7}$ We find that some postprocessing of model outputs is necessary for certain languages. Specifically, when queried with German and Russian prompts, all models consistently repeated the question before providing the extracted answer To handle such edge cases, we strip away the question and truncate the system output to a maximum of 50 characters or the first line break, whichever comes first.

![](https://cdn.mathpix.com/cropped/2024_06_04_c119aa28c6a06f3e6af4g-06.jpg?height=440&width=1585&top_left_y=240&top_left_x=241)

Figure 4: X-CSQA results for Llama $27 \mathrm{~b}$ with incremental multilingual instruction tuning. Striped bars indicate that the target language is included in the finetuning data. Error bars show a confidence interval of $95 \%$.

![](https://cdn.mathpix.com/cropped/2024_06_04_c119aa28c6a06f3e6af4g-06.jpg?height=431&width=1585&top_left_y=841&top_left_x=241)

Figure 5: XNLI results for Llama $27 \mathrm{~b}$ with incremental multilingual instruction tuning. Striped bars indicate that the target language is included in the finetuning data. Error bars show a confidence interval of $95 \%$.

of our prompting strategy for this task and model outputs is shown in Table 2.

Results Figure 3 shows the performance on each of our target languages in XQuAD given incremental multilingual instruction tuning.

Similar to the results on the chat task ( $\$ 4.1$ ), we observe no performance degradation on English as the ratio of multilingual instructions increases. For most non-English target languages in the supervised setting, we see that when using the multilingual prompting strategy (en:xx), performance remains relatively uniform, with the exception of Chinese. In contrast, given the monolingual prompting strategy (xx:xx), multilingual instruction tuning can lead to small but noticeable gains in performance over monolingual tuning. Specifically, for German and Chinese, performance improves when moving from monolingual to bilingual instruction tuning and again plateaus rather quickly with as few as three languages. For languages in the extremely low-resource setting (Hindi and Greek), multilingual instruction tuning fails to improve the model's ability to handle these languages. While performance gains on this task are generally less pronounced than on the open-ended chat task, we note that zero-shot extractive QA is inherently challenging for LLMs tuned on conversational instructions as they tend to generate verbose responses rather than the single word or entity that correctly matches the ground truth.

### 4.3 Commonsense Reasoning

Effectively understanding natural language requires some representation of the natural world and how concepts can relate to one another. Commonsense knowledge describes the set of general facts that reflects this. To evaluate how well Englishcentric models can reason across languages, we use the X-CSQA dataset. ${ }^{8}$ This dataset consists of questions paired with multiple choice answers which aim at assessing general, language-agnostic world knowledge involving different types of commonsense reasoning (Talmor et al., 2019; Lin et al., 2021). Given a question and a set of five possible answers from A-E, we prompt the model to output the letter corresponding to the answer that best answers the question. Again, we borrow the English prompt template for this task from Lai et al. (2023a) as a starting point and translate it into each[^6]target language. An example of the prompt used is given in Table 3. Finally we assess performance with the monolingual prompting strategy (xx:xx).

Results Figure 4 shows the accuracy on X-CSQA given incremental multilingual instruction tuning. Again, for English, we observe that multilingual instruction tuning does not significantly degrade performance. However, for non-English target languages, incremental multilingual instruction tuning fails to deliver improvements over monolingual tuning. This contrasts with the results on the previous tasks considered.

### 4.4 XNLI

Natural language inference (NLI) is an important skill for LLMs, especially as input and output sequences grow to consist of multiple sentences. Given two sentences, this task aims to recognise a relationship between them either as entailment, contradiction or neutral. To assess an LLM's ability to solve this task given multilingual instruction tuning, we use XNLI (Conneau et al., 2018) and evaluate performance on the official test split.

For this task, we use the implementation in the lm-evaluation harness (Gao et al., 2023) ${ }^{9}$, which uses language-specific prompts for each target language (i.e. monolingual prompting (xx:xx)). Instead of querying the model to generate the desired label, multiple queries are constructed for each test instance (one for each possible label) and scored by the model. The sequence with the highest likelihood is chosen as the model's answer.

Results Figure 5 shows the accuracy measured on our target languages from XNLI given incremental multilingual instruction tuning. Strikingly, performance for all target languages remains uniform regardless of the amount of multilinguality used in finetuning. Furthermore, the equal performance between the Guanaco baseline and our Mono and Multi- $i$ finetuned models suggests that scaling up instruction diversity and the number of languages seen during finetuning beyond just six languages is still insufficient for delivering noticeable gains on this task.

## 5 Further Analysis

The results on four distinct evaluation tasks show that multilingual instruction tuning benefits open-[^7]

ended chat in non-English languages most strongly (§4.1). In this section, we focus on this task to investigate the impact of instruction diversity and model scaling.

### 5.1 Instruction Diversity vs. Language Diversity

Diversity is a key factor for sample efficient instruction tuning (Zhou et al., 2023). Since our experiments in $\S 4$ make use of native non-English training instances from the OpenAssistant dataset, a potential confounding factor could be that adding more languages during finetuning also introduces more diverse training instructions. To investigate this, we retrain Multi- $i$ models using translated instruction-tuning data from Mono in place of native non-English examples, following the same incremental recipe as described in $\S 3.2$. This ensures that the data distribution remains constant as multilinguality increases. As a translation engine, we again use GPT-3.5-Turbo ${ }^{10}$ and the prompt template provided in Figure 10.

Figure 6 compares the resulting performance on the open-ended chat task on a subset of our target languages (results for the remaining target languages are provided in Figure 15). Notably, we observe no significant differences between tuning with distinctly native non-English examples compared to those derived via automatic translation. This indicates that the gains attributed to increased multilinguality are not conflated with an increase in the diversity of instructions.

### 5.2 Scaling up Model Size

In order to investigate the effect of model scaling, we also repeat our chat experiments using Llama 2 $70 \mathrm{~b}$ as the underlying base model. Figure 7 shows the resulting helpfulness scores assigned by the LLM judge. Most notably, performance on nonEnglish languages seen relatively frequently during pretraining is dramatically improved, often matching that of English. Secondly, we observe that the larger model's performance on most nonEnglish target languages tends to plateau with just two finetuning languages, unlike the smaller model that required three. Finally, while the performance on languages in the extremely low-resource scenario remains low, it exhibits a substantial relative improvement compared to the 7-billion pa-[^8]

![](https://cdn.mathpix.com/cropped/2024_06_04_c119aa28c6a06f3e6af4g-08.jpg?height=323&width=1585&top_left_y=244&top_left_x=241)

Figure 6: Chat performance of Llama 2 7b given multilingual instruction tuning using native non-English (Native NE) examples compared to using translated non-English (MT NE) examples. Error bars show a confidence interval of $95 \%$.
![](https://cdn.mathpix.com/cropped/2024_06_04_c119aa28c6a06f3e6af4g-08.jpg?height=456&width=1604&top_left_y=762&top_left_x=228)

Figure 7: Chat performance of Llama 2 70b (denoted by triangular points) and Llama 2 7b (semi-transparent bars) with incremental multilingual instruction tuning. Striped bars indicate that the target language is included in the finetuning data. Error bars show a confidence interval of $95 \%$.

rameter model. These results indicate that model scaling is extremely beneficial for exploiting the multilingual capabilities in English-centric models. This aligns with the assertion of Shaham et al. (2023) that larger models are more adept at handling multilinguality.

## 6 Discussion

Our findings show that multilingual instruction tuning can elicit zero-shot cross-lingual transfer in English-centric models, though its effectiveness on downstream performance varies across tasks. Notably, we observe significant performance gains for open-ended chat and noticeable gains on extractive QA with monolingual prompting. In comparison, for highly structured tasks such as X-CSQA and XNLI that impose a strict constraint on the output space regardless of the input language (e.g., choosing from options 'A', 'B', 'C', etc.), multilingual instruction tuning does not seem to provide substantial benefits. This distinction highlights that multilingual instruction tuning is most beneficial for open-ended generative tasks where there is an implicit agreement between the input query language and the language output by the model.

We posit that our findings align with the su- perficial alignment hypothesis (Zhou et al., 2023). While instruction tuning guides the model towards a desirable 'subdistribution of formats' to use, a small amount of multilingual instruction data allows the model to learn a simple mapping between input and output languages. In essence, it helps steer the model towards language-specific 'subdistributions of formats' that agree with the language of the input. Therefore, we conclude that multilingual instruction tuning is of most benefit in scenarios where IO language agreement is required.

Most surprisingly, we find that multilingual instruction tuning with just three languages is sufficient to encourage IO language agreement in smaller LLMs, resulting in improved cross-lingual transfer on generative tasks. As model size increases, however, the number languages required decreases from three to two, indicating that larger models can learn this mapping easier. In both model sizes, adding more instruction tuning languages - including the target language itself - provides no significant gains.

Despite benefits from cross-lingual transfer, our results highlight a clear gap in performance between English and non-English target languages on all tasks. This indicates that small-scale multi-
lingual instruction tuning is insufficient to improve the intrinsic multilingual capabilities of Englishcentric LLMs and that more advanced interventions may be required (e.g. Pfeiffer et al., 2020; ImaniGooghari et al., 2023).

## 7 Conclusion

In this paper, we investigated the use of multilingual instruction tuning to elicit multilingual capabilities of English-centric LLMs. Our results show that finetuning with as few as three languages can promote cross-lingual transfer and allow models to better exploit the relatively small amounts of nonEnglish data seen during pretraining. Experiments conducted on four distinct tasks revealed that this can lead to significant performance improvements on open-ended generative tasks that assume input and output language agreement. Furthermore, we observed that these effects are amplified in larger models, requiring only two finetuning languages for successful multilingual instruction tuning. The effectiveness of cross-lingual transfer, which reduces the need for extensive multilingual instruction tuning, is good news for LLM developers. However, future work could explore other methods to encourage smaller models to better leverage the limited non-English data seen during pretraining and whether there are tasks for which languagespecific instruction tuning is of higher importance.

## 8 Limitations

This work focused on detecting and evaluating nonEnglish responses from English-centric LLMs. Our experiments demonstrated that cross-lingual transfer, which can be elicited given small amount of multilingual instruction tuning, can be effective for improvinging the non-English performance of English-centric models. That said, our analysis focused primarily on Indo-European languages, except for Chinese. As a consequence, it remains unclear how well our results generalise to more distant or topologically diverse languages, however, performance gains observed on Chinese suggest that generalisation is more dependent on exposure during pretraining than language similarity.

For our experiments and analysis on model training data, we relied on automatic language identification. To this end, we employed the OpenLID model from Burchell et al. (2023). Despite low error rates achieved by this model, language identification is not perfect and can lead to some texts be- ing misidentified. To mitigate the risk of language contamination (Blevins and Zettlemoyer, 2022) in our controlled experiments, we only included training examples whose language is identified with a confidence threshold $\geq 0.8$.

In $\S 5.1$ we investigated the impact of multilingual diversity versus training example diversity. While our findings revealed that there is no significant difference between these two settings, we note that even when finetuning with the original native non-English examples, task diversity may be inherently limited by design of the data collection. For instance, regardless of the language used, crowdworkers were asked to follow the same set of guidelines ${ }^{11}$ when creating the data.

## Acknowledgements

We sincerely thank our friends and colleagues, Fabian Aiolfi, Thea Bächler, Anastassia Shaitarova, Finnur Ágúst Ingimundarson, Cui Ding, Andrianos Michail, Janine Aeberhard, Arnisa Fazla, Farhad Nooralahzadeh, Omnia Ibrahim, Xuan Lam and Manasi Muglikar, for helping us with languagespecific questions and validating translations used in our experiments, as well as Lena Bolliger and Patrick Haller for providing valuable feedback. Rico Sennrich acknowledges funding by the Swiss National Science Foundation (project MUTAMUR; no. 213976).

## References

Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. The falcon series of open language models.

Jordi Armengol-Estapé, Ona de Gibert Bonet, and Maite Melero. 2022. On the multilingual capabilities of very large-scale English language models. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3056-3068, Marseille, France. European Language Resources Association.

Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623-4637, Online. Association for Computational Linguistics.[^9]

Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan $\mathrm{Xu}$, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.

Terra Blevins and Luke Zettlemoyer. 2022. Language contamination helps explains the cross-lingual capabilities of English pretrained models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3563-3574, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In $A d$ vances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.

Laurie Burchell, Alexandra Birch, Nikolay Bogoychev, and Kenneth Heafield. 2023. An open dataset and model for language identification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 865-879, Toronto, Canada. Association for Computational Linguistics.

Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Barry Haddow, and Kenneth Heafield. 2023. Monolingual or multilingual instruction tuning: Which makes a better alpaca.

Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15607-15631, Toronto, Canada. Association for Computational Linguistics.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475-2485, Brussels, Belgium. Association for Computational Linguistics.

Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the world's first truly open instructiontuned llm.

Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang 2022. No language left behind: Scaling humancentered machine translation.

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314.

Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback.

Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou 2023. A framework for few-shot language model evaluation.

Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2023. Are large language model-based evaluators the solution to scaling up multilingual evaluation?

Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita,

Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models.

Oskar Holmström, Jenny Kunz, and Marco Kuhlmann. 2023. Bridging the resource gap: Exploring the efficacy of English and multilingual LLMs for Swedish. In Proceedings of the Second Workshop on Resources and Representations for Under-Resourced Languages and Domains (RESOURCEFUL-2023), pages 92-110, Tórshavn, the Faroe Islands. Association for Computational Linguistics.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models.

Ayyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André Martins, François Yvon, and Hinrich Schütze. 2023. Glot500: Scaling multilingual corpora and language models to 500 languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10821117, Toronto, Canada. Association for Computational Linguistics.

Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics.

Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023. Is chatgpt a good translator? yes with gpt-4 as the engine.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention.

Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. 2023. Openassistant conversations-democratizing large language model alignment. arXiv preprint arXiv:2304.07327.

Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and
Thien Huu Nguyen. 2023a. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning.

Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2023b. Okapi: Instructiontuned large language models in multiple languages with reinforcement learning from human feedback.

Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. 2023. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets. In Findings of the Association for Computational Linguistics: ACL 2023, pages 431-469, Toronto, Canada. Association for Computational Linguistics.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sab-
rina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, ZhengXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, JanChristoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Karen Fort, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio MirandaEscalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane
Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2023. Bloom: A 176b-parameter open-access multilingual language model.

Teven Le Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, and Iz Beltagy. 2022. What language model to train if you have one million GPU hours? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 765-782, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.

Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, and Tuo Zhao. 2023. Loftq: Lora-fine-tuning-aware quantization for large language models.

Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xiang Ren. 2021. Common sense beyond English: Evaluating and improving multilingual language models for commonsense reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1274-1287, Online. Association for Computational Linguistics.

Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with
multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019-9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023 The flan collection: Designing data and methods for effective instruction tuning.

Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, and Furu Wei. 2023. Chainof-dictionary prompting elicits translation in large language models.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions.

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15991-16111, Toronto, Canada. Association for Computational Linguistics.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744. Curran Associates, Inc.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023a. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023b. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only.

Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654-7673, Online. Association for Computational Linguistics.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization.

Uri Shaham, Maha Elbayad, Vedanuj Goswami, Omer Levy, and Shruti Bhosale. 2023. Causes and cures for interference in multilingual translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15849-15863, Toronto, Canada. Association for Computational Linguistics.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,

Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508, Toronto, Canada. Association for Computational Linguistics.

Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085-5109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners.

Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 833-844, Hong Kong, China. Association for Computational Linguistics.

Jiacheng Ye, Xijia Tao, and Lingpeng Kong. 2023. Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability.

Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. 2023a. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.

Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, and Francesco Barbieri. 2023b. Plug: Leveraging pivot language in cross-lingual instruction tuning. arXiv preprint arXiv:2311.08711.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. CoRR, abs/2306.05685.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment.
