# Preference Learning Algorithms Do Not Learn Preference Rankings 

Angelica Chen<br>New York University<br>ac5968@nyu.edu

Sadhika Malladi<br>Princeton University<br>smalladi@princeton.edu

Lily H. Zhang<br>New York University<br>lily.h.zhang@nyu.edu

Xinyi Chen<br>Google DeepMind; Princeton University<br>xinyic@google.com

Qiuyi Zhang<br>Google DeepMind<br>qiuyiz@google.com

Rajesh Ranganath<br>New York University<br>rajeshr@cims.nyu.edu

Kyunghyun Cho<br>New York University; Genentech; CIFAR LMB<br>kyunghyun.cho@nyu.edu


#### Abstract

Preference learning algorithms (e.g., RLHF and DPO) are frequently used to steer LLMs to produce generations that are more preferred by humans, but our understanding of their inner workings is still limited. In this work, we study the conventional wisdom that preference learning trains models to assign higher likelihoods to more preferred outputs than less preferred outputs, measured via ranking accuracy. Surprisingly, we find that most state-of-the-art preferencetuned models achieve a ranking accuracy of less than $60 \%$ on common preference datasets. We furthermore derive the idealized ranking accuracy that a preferencetuned LLM would achieve if it optimized the DPO or RLHF objective perfectly. We demonstrate that existing models exhibit a significant alignment gap - i.e., a gap between the observed and idealized ranking accuracies. We attribute this discrepancy to the DPO objective, which is empirically and theoretically ill-suited to fix even mild ranking errors in the reference model, and derive a simple and efficient formula for quantifying the difficulty of learning a given preference datapoint. Finally, we demonstrate that ranking accuracy strongly correlates with the empirically popular win rate metric when the model is close to the reference model used in the objective, shedding further light on the differences between on-policy (e.g., RLHF) and off-policy (e.g., DPO) preference learning algorithms.


## 1 Introduction

Recent work on aligning LLMs has focused predominantly on tuning models to adhere to human preferences - commonly through reinforcement learning (RLHF; Stiennon et al. [45]) or directly via offline supervision (DPO; Rafailov et al. [39]). Preference learning algorithms [19, 53, 56] were originally designed to use a dataset of pairwise preferences over candidates to train a model with high ranking accuracy - that is, the model can precisely rank preferred outputs over dispreferred ones. In the case of language models, the ranking is determined by the likelihood assigned to each candidate.

Many LLM alignment techniques are designed to yield models with a high preference ranking accuracy, including SLiC [66, 65], RAFT [7], PRO [44], and RRHF [62]. Most prominently, Rafailov et al. [39] claimed that their popular direct preference optimization (DPO) algorithm "increases
the relative log probability of preferred to dispreferred response." It is standard to evaluate these various objectives by measuring how often the resulting model's generations are preferred over another model's (i.e., a win rate) [69]. However, the relationship between the loss, ranking accuracy, and win rate is unclear, leaving open the question of what these alignment techniques are actually accomplishing during training.

In this work, we demonstrate that RLHF and DPO struggle to increase ranking accuracy in practice and explore both the theoretical and empirical reasons why. Our findings highlight an intricate relationship between offline optimization and online behavior, and motivate the need for more fine-grained analyses of preference training dynamics. Our contributions are as follows:

1. Existing models do not achieve high ranking accuracies. We demonstrate that a wide variety of open-access preference-tuned LLMs (e.g., LLAMA 2 7B CHAT, GEMMA 7B IT, and ZEPHYR 7B DPO) achieve a ranking accuracy below $60 \%$ across a range of validation splits from commonly used preference datasets, such as UltraFeedback [6], Anthropic helpfulness and harmlessness (HH-RLHF, [13]), and Stanford Human Preferences (SHP, [10]) (Figure 1)
2. Existing models exhibit a significant alignment gap between the ranking accuracy they achieve and the accuracy achievable under idealized conditions. We derive a simple formula (Theorem 3.1) for the idealized ranking accuracy (i.e., the ranking accuracy achieved from training on ground-truth preference data and perfectly optimizing the DPO or RLHF objective). We observe that models suffer from a significant alignment gap in that they achieve ranking accuracy far below the idealized ranking accuracy (Table 1, Figure 1).
3. Preference learning rarely corrects incorrect rankings. We prove theoretically that even mild ranking errors in the reference model can make it virtually impossible for DPO and its variants to correct the ranking (Theorem 4.1), and demonstrate that in practice, the rankings are rarely flipped (Fig. 2) and the reference model likelihoods generally determine the ranking accuracy (Fig. 3). Our results permit straightforward and efficient identification of hard-to-learn preference datapoints without any tuning.
4. Ranking accuracy and win rate are closely correlated when the model is close to the reference model. We observe that the ranking accuracy and win rate trend together when the model is close to the reference model during the early phase of alignment, but become anti-correlated once the model has moved too far away, adding to the ongoing discussion on the differences between on-policy and off-policy behaviors of preference-tuned LLMs.

Crucially, our work highlights fundamental flaws in RLHF and DPO that prevent the preference-tuned model from achieving a high ranking accuracy even on the training dataset.

## 2 Preliminaries

### 2.1 Learning from Human Preferences

Preference Data Human preference data typically takes the form of pairwise preferences. Each prompt $x$ is paired with two possible continuations $-y_{1}$ and $y_{2}$. One or more human raters then annotate which continuation is preferred. When there are multiple raters, we use $\alpha\left(x, y_{1}, y_{2}\right)$ to denote the proportion of raters who prefer $y_{1}$ over $y_{2}$. ${ }^{1}$

Definition 2.1 (Aggregated Preference Datapoint). Consider a prompt $x$ with two possible continuations $y_{1}$ and $y_{2}$ and the proportion of raters $\alpha\left(x, y_{1}, y_{2}\right)$ who preferred $y_{1}$ over $y_{2}$. Then, the aggregated preference datapoint for each prompt $x$ is denoted $\left(x, y_{w}, y_{l}\right)$ where $y_{w}$ is the completion preferred by the majority of voters.

We note that at the time of writing, the vast majority of datasets either use a single rater [13] or only release aggregated preference data [10,25], so we often do not have access to $\alpha\left(x, y_{1}, y_{2}\right)$. A standard assumption is that the ground-truth human preferences obey the Bradley-Terry model (Assumption A.1).[^0]

Supervised Fine-Tuning (SFT) In the first step of the preference learning pipeline, the model is typically trained using the standard cross-entropy objective on some choice of offline instructiontuning dataset(s). In some implementations [50], a variety of third-party datasets are selected, whereas in other implementations $[45,39,41]$ the model is instead trained on the preferred continuations $\left(x, y_{w}\right)$ from the same preference learning dataset that is used in downstream preference learning. The resulting model is often used as a reference model, denoted as $\pi_{\mathrm{Ref}}$ or $\pi_{\mathrm{SFT}}$, and it typically serves as the initialization when learning from human preferences.

Reinforcement Learning from Human Feedback (RLHF) Learning from human feedback originally required using reinforcement learning [45]. In this setting, the possible continuations for each prompt are sampled from a reference model (i.e., $\left.\left(y_{w}, y_{l}\right) \sim \pi_{\operatorname{Ref}}(\cdot \mid x)\right)$ and then annotated and aggregated to create a preference dataset $\mathcal{D}$. Then, one frames the problem as binary classification between the two continuations and trains a reward model $r_{\phi}(x, y)$ to minimize $\mathcal{L}_{R}\left(r_{\phi}, \mathcal{D}\right)=-\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}\left[\log \sigma\left(r_{\phi}\left(x, y_{w}\right)-r_{\phi}\left(x, y_{l}\right)\right)\right]$. Finally, one trains the model $\pi_{\theta}$ to maximize the reward without straying too far from the reference model $\pi_{\text {Ref }}$. Because sampling generations from the model is non-differentiable, it is common to use PPO to maximize the reward $r(x, y)=r_{\phi}(x, y)-\beta\left(\log \pi_{\theta}(y \mid x)-\log \pi_{\operatorname{Ref}}(y \mid x)\right)$, where $\beta>0$ is a regularization coefficient designed to prevent the model from straying too far from its initialization.

Preference Learning with DPO Rafailov et al. [39] demonstrated that one can avoid using PPO by reparametrizing the objective to operate over policies instead of over rewards. Then, one can minimize the differentiable DPO objective.

Definition 2.2 (DPO Objective [39]). Let $\sigma$ be the sigmoid function and $\beta>0$ be a hyperparameter. Then, the DPO objective for an aggregated preference dataset $\mathcal{D}$ and a reference model $\pi_{\text {Ref }}$ is defined as

$$
\begin{aligned}
\mathcal{L}_{\mathrm{DPO}}\left(\pi_{\theta}, \pi_{\mathrm{Ref}}\right) & =-\underset{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}{\mathbb{E}}[\log \sigma(\underbrace{\left.\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\mathrm{Ref}}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\mathrm{Ref}}\left(y_{l} \mid x\right)}\right)}_{\text {reward margin }}] \\
& =-\underset{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}{\mathbb{E}}[\log \sigma(\beta \underbrace{\log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\theta}\left(y_{l} \mid x\right)}}_{\text {model log-ratio }}+\beta \underbrace{\log \frac{\pi_{\mathrm{Ref}}\left(y_{l} \mid x\right)}{\pi_{\mathrm{Ref}}\left(y_{w} \mid x\right)}}_{\text {reference model log-ratio }})]
\end{aligned}
$$

We denote the DPO loss on the aggregated datapoint $\left(x, y_{w}, y_{l}\right)$ as $\mathcal{L}_{\mathrm{DPO}}\left(x, y_{w}, y_{l} ; \pi_{\theta}, \pi_{\mathrm{Ref}}\right)$.

### 2.2 Evaluation Metrics

Evaluating the alignment of a preference-tuned LLM is both under-specified and multi-dimensional. Many knowledge-based and logic-based benchmarks (e.g. MMLU, GLUE, BIG-Bench, HELM) already exist, but these benchmarks largely fail to capture nuanced aspects of human preference, such as helpfulness or harmlessness [13]. As such, one standard evaluation is to ask human or machine raters how often the model produces a favorable completion compared to a baseline (i.e., win rate). Human win rate is the gold standard but is costly to compute and can be biased based on size and nature of the worker pool $[18,24]$. Rating completions using another LLM (e.g., MT-bench) can be cheaper but similarly suffers from various biases [37,67,60], and several studies have revealed failures in many LLM judges to identify violations of instruction-following [64, 28]. Nevertheless, since win rate evaluations are so prevalent, we compare ranking accuracy against win rate in Sec. 5 and describe when the former off-policy metric is correlated with the popular on-policy metric.

Besides the win rate,preference learning algorithms are also benchmarked by the frontier of the rewards versus the divergence from the initialization [39], which serves as a heuristic of how well the model can incorporate preference data without unlearning prior information. However, it is unclear how well rewards can describe the success of alignment.

As aforementioned, the current paper investigates the ranking accuracy, which is defined as follows:

Definition 2.3 (Ranking Accuracy). The ranking accuracy $\mathcal{R}$ of a model $\pi_{\theta}$ on an aggregated preference datapoint $\left(x, y_{w}, y_{l}\right)$ is defined as

$$
\mathcal{R}\left(x, y_{w}, y_{l} ; \pi_{\theta}\right)= \begin{cases}1 & \pi_{\theta}\left(y_{w} \mid x\right) \geq \pi_{\theta}\left(y_{l} \mid x\right)  \tag{1}\\ 0 & \text { otherwise }\end{cases}
$$

Analogously, the ranking accuracy of policy $\pi_{\theta}$ on a dataset $\mathcal{D}=\left\{\left(x, y_{w}, y_{l}\right)\right\}$ is $\mathcal{R}\left(\mathcal{D} ; \pi_{\theta}\right)=$ $\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}} \mathcal{R}\left(x, y_{w}, y_{l} ; \pi_{\theta}\right)$. In the rare case where a dataset has more than two outputs $y$ per prompt $x$, we use the generalized ranking accuracy definition stated in App. A.6.

Remark 2.4 (Lengths of Completions). We note that $y_{w}$ and $y_{l}$ can have different lengths; for example, Singhal et al. [43] showed that $y_{w}$ is usually longer. Length can deflate $\pi_{\theta}\left(y_{w} \mid x\right)$ and reduce the ranking accuracy. One can normalize the likelihoods by the length of the response, but the length-normalized ranking accuracy may not be meaningful in practice, because it is currently unclear how to sample from the length-normalized likelihood. For completeness, we report the ranking accuracies of both the unnormalized and normalized policies, denoted $\mathcal{R}$ and $\tilde{\mathcal{R}}$, respectively.

Remark 2.5 (Difference between Ranking Accuracy and Reward Accuracy). For RLHF models and DPO models, the ranking accuracy is not equivalent to the reward accuracy (i.e., the metrics evaluated in RewardBench [28]). In the RLHF case, we are evaluating the ranking accuracy of the final policy rather than the reward model. In the DPO case, reward accuracy measures whether $\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\text {Ref }}\left(y_{w} \mid x\right)}>$ $\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\operatorname{Ref}}\left(y_{l} \mid x\right)}$ instead of whether $\pi_{\theta}\left(y_{w} \mid x\right)>\pi_{\theta}\left(y_{l} \mid x\right)$. Since we ultimately sample from $\pi_{\theta}$ rather than $\frac{\pi_{\theta}(y \mid x)}{\pi_{\operatorname{Ref}}(y \mid x)}$, we find the ranking accuracy to be of greater practical importance.

Moreover, we demonstrate that under very stringent conditions, minimizing the DPO objective results in a model with high ranking accuracy. We characterize the phenomenon on individual datapoints, as is the case throughout the paper, but note that Markov's inequality can be straightforwardly applied to extend the results to a low loss on the entire dataset.

Proposition 2.6 (Sanity Check). Recall the definition of $y_{w}, y_{l}$ in Definition 2.1. If $\pi_{\text {Ref }}\left(y_{w} \mid x\right) \geq$ $\pi_{R e f}\left(y_{l} \mid x\right)$ and $\mathcal{L}_{D P O}\left(x, y_{w}, y_{l} ; \pi_{\theta}, \pi_{\text {Ref }}\right) \leq 0.6$, then $\mathcal{R}\left(x, y_{w}, y_{l}\right)=1$.

This result, proved in App. A.1, requires the condition that the reference model already has the correct ranking, so it is unlikely to hold across all datapoints in practice and somewhat moot. The remainder of the paper focuses on more realistic settings where the reference model is imperfect.

## 3 The Alignment Gap

Prop. 2.6 showed that training a low DPO loss with a perfect reference model yields a model with perfect ranking accuracy. However, Fig. 1a shows that real-world reference models exhibit low ranking accuracies, which prompts us to study more realistic, imperfect reference models.

### 3.1 Existing Reference Models Rarely Have Correct Rankings

Fig. 1a indicates that reference models rarely achieve high ranking accuracy on common preference datasets (except Synthetic Instruct GPT-J Pairwise), even though many were likely trained on the preferred completions (see Sef. 2.1). Many of the models do not have documented training data so we do not know which preference datasets, if any, are in-distribution. We also fine-tune several pretrained LLMs on the preferred completions (see App. C.1) and observe that ranking accuracy does not increase significantly. ${ }^{2}$ Based on our findings, we turn to the case of imperfect reference models.

### 3.2 Idealized Ranking Accuracy

We showed above that empirically, reference models exhibit poor accuracy when ranking the plausible completions. However, the RLHF reward and DPO objective were formulated to ensure that the model learns the preference dataset but does not move too far from the reference model $\pi_{\text {Ref }}$, so there may be a limit on the possible accuracy of the preference-tuned model. Here, we formalize this intuition by studying what the optimal policies would be when perfectly optimizing DPO or RLHF with access to perfect data (i.e., true proportions of human preferences). ${ }^{3}$

Theorem 3.1 (Simulating Perfect $\mathrm{RLHF}^{4}$ ). Fix a reference model $\pi_{\text {Ref }}$ and an aggregated preference datapoint $\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}$. Assume the dataset includes the ground-truth human preferences:[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_c0fbacb600a68d309b81g-05.jpg?height=410&width=675&top_left_y=253&top_left_x=367)

(a) Ranking accuracies of various reference models, including GPT2 [38], PYthiA 2.8B [4], PyTHiA 1.4B [4], LLAMA 2 7B [50], VICUNA 1.5 7B [67], OLMo 7B [16], TULU2 7B [20], ZEPHYR 7B SFT [52], Mistral v0.1 7B [22], and Gemma 7B [49]

![](https://cdn.mathpix.com/cropped/2024_06_04_c0fbacb600a68d309b81g-05.jpg?height=412&width=675&top_left_y=255&top_left_x=1083)

(b) Ranking accuracies of various preference-tuned models, including LLAMA 2 7B CHAT [50], TULU2 7B DPO [20], ZEPHYR 7B DPO [52], and GEMMA 7B IT [49]

Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in C. 2 and list all numbers in Tables 2, 3, and 4.

that is, $\alpha\left(x, y_{w}, y_{l}\right)=\mathbb{P}\left(y_{w} \succ y_{l}\right)$, and that these preferences obey the Bradley-Terry model (Assumption A.1). Let $\pi^{*}$ be the model resulting from perfectly optimizing the DPO or RLHF objective on $\left(x, y_{w}, y_{l}\right)$ as described in Section 2.1. Then, $\pi^{*}$ satisfies

$$
\begin{equation*}
\frac{\pi^{*}\left(y_{w} \mid x\right)}{\pi^{*}\left(y_{l} \mid x\right)}=\frac{\pi_{R e f}\left(y_{w} \mid x\right)}{\pi_{R e f}\left(y_{l} \mid x\right)}\left(\frac{\alpha\left(x, y_{w}, y_{l}\right)}{1-\alpha\left(x, y_{w}, y_{l}\right)}\right)^{1 / \beta} \tag{2}
\end{equation*}
$$

where $\alpha\left(x, y_{w}, y_{l}\right)$ is the proportion of raters who preferred $y_{w}$ over $y_{l}$ and $\beta$ is a hyperparameter in the DPO and RLHF objectives.

Remark 3.2. We prove this result in App. A.2. This theorem highlights the importance of reporting multiple ratings for each preference datapoint, because it shows that when $\alpha\left(x, y_{w}, y_{l}\right)=1$, the objectives only have an optimum at infinity.

This result allows us to simulate the policy resulting from perfect optimization of either the RLHF or the DPO learning objective. As such, given a reference model $\pi_{\text {Ref }}$ and preference dataset $\mathcal{D}$, we can easily measure the idealized ranking accuracy of a model. We prove this result in App. A.3.

Corollary 3.3 (Idealized Ranking Accuracy). Given a reference model $\pi_{\text {Ref }}$, the DPO or RLHF hyperparameter $\beta$, a dataset of aggregated preferences $\mathcal{D}=\left\{\left(x, y_{w}, y_{l}\right)\right\}$ and their corresponding rater proportions $\alpha\left(x, y_{w}, y_{l}\right)$, the ranking accuracy of the optimum of the RLHF or DPO objective $\pi^{*}$ is given by

$$
\begin{equation*}
\mathcal{R}^{*}\left(\mathcal{D} ; \pi_{R e f}\right)=\underset{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}{\mathbb{E}}\left[\mathbb{1}\left[\frac{\pi_{R e f}\left(y_{w} \mid x\right)}{\pi_{R e f}\left(y_{l} \mid x\right)}\left(\frac{\alpha\left(x, y_{w}, y_{l}\right)}{1-\alpha\left(x, y_{w}, y_{l}\right)}\right)^{1 / \beta}>1\right]\right] \tag{3}
\end{equation*}
$$

where $\mathbb{1}[\cdot]$ is the indicator function. When computed on length-normalized likelihoods from $\pi_{\text {Refef }}$, we denote the idealized ranking accuracy as $\tilde{\mathcal{R}}^{*}$.

### 3.3 Measuring the Alignment Gap

Given access to $\pi_{\text {Ref }}, \beta$, and the $\alpha\left(x, y_{w}, y_{l}\right)$ values for each triple $\left(x, y_{w}, y_{l}\right)$ in a given preference dataset, we can compute the idealized ranking accuracy from Eq. 3.5 The results are shown in Table 1 and further details are given in App. C.4.

We identify several surprising findings. Firstly, even under ideal conditions (i.e. perfectly optimizing the objective on ground-truth preference data), the idealized ranking accuracy is still sometimes below[^2]

Table 1: The idealized ranking accuracy of existing algorithms is not perfect, but preferencetuned models exhibit ranking accuracies far even from this idealized case. We provide both the length-normalized $(\tilde{\mathcal{R}})$ and non-length-normalized $(\mathcal{R})$ ranking accuracies for a variety of open-access preference-tuned models on the Alpaca Farm [8] validation dataset (described in App. C.2). We also provide the idealized ranking accuracy (Corollary 3.3). Since idealized ranking accuracy can be computed with a variety of values of $\beta$, we provide the minimum, median, and maximum idealized ranking accuracy values for a range of $\beta$. For more details, see App. C.4.

| Preference-Tuned <br> Model | Length-Normalized |  | Non-Length-Normalized |  |
| :---: | :---: | :---: | :---: | :---: |
|  | $\tilde{\mathcal{R}}$ | $\tilde{\mathcal{R}^{*}}$ <br> (Min./Med./Max.) | $\mathcal{R}$ | $\mathcal{R}^{*}$ <br> (Min./Med./Max.) |
| ZEPHYR-7B-DPO | $54 \%$ | $86 \% / 98 \%$ / 100\% | $42 \%$ | $90 \% / 99 \% / 100 \%$ |
| TULU-2-DPO-7B | $53 \%$ | $87 \% / 97 \% / 100 \%$ | $42 \%$ | $91 \% / 99 \% / 100 \%$ |
| GOOGLE-GEMMA-7B-IT | $54 \%$ | $73 \%$ / 73\% / 97\% | $40 \%$ | $67 \% / 93 \% / 100 \%$ |
| LLAMA-2-7B-СHAT-HF | $53 \%$ | $87 \% / 97 \%$ / 100\% | $40 \%$ | $91 \% / 99 \% / 100 \%$ |

$100 \%$. This distance varies with the choice of $\beta$, which indicates that the limits of DPO/RLHF depend largely upon how strong the reliance on $\pi_{\text {Ref }}$ is. Furthermore, we find that many state-of-the-art models do not achieve a ranking accuracy anywhere close to the idealized ranking accuracy, exhibiting alignment gaps ranging from 19 to 59 percentage points (measured to the median idealized $\mathcal{R}$ or $\tilde{\mathcal{R}}$ ).

## 4 Understanding Ranking Accuracy with DPO

We now turn to the training objectives to account for the alignment gap. We focus our analysis on the DPO objective (Definition 2.2), because its failure to achieve high ranking accuracy is especially surprising (Table 1). In particular, DPO directly maximizes the reward margin between preferreddispreferred pairs over an offline dataset so we would expect it to perform well on in-distribution held-out data. We also note that DPO is a popular choice in the community for aligning LLMs, because it is less costly than performing RLHF.

In this section, we study real-world characteristics of DPO. First, we show empirically that DPO rarely flips the ranking of the two continuations. This result combined with the observation that reference models exhibit poor ranking accuracy (Sec. 3.1) provides an explanation for the observed poor ranking accuracies in Table 1. We then formally characterize how hard it is for DPO to correct the ranking of each datapoint.Our result highlights how the reference model conditions the optimization: as the reference model log-ratio (Definition 2.2) grows larger, one has to reduce the DPO loss to a dramatically small value to flip the incorrect ranking (Fig. 3).

### 4.1 DPO Rarely Flips Preference Rankings

To study how ranking accuracy changes over the course of DPO training, we train three sizes of models (GPT-2 [38], Pythia 2.8B [4], and Llama 2-7B [50]) across three seeds each on the Anthropic HH-RLHF [3] preference dataset and study the ranking accuracy on different partitions of the training dataset. We present the results from training one seed of Pythia 2.8B in Fig. 2, and defer training details to App. D. 1 and results on the other two models to App. D.2. In Fig. 2, we partition a random subsample of $1 \mathrm{~K}$ examples from the training dataset into four groups based on whether the reference model $\pi_{\text {Ref }}$ had the correct ranking and whether the current model $\pi_{\theta}$ has the correct ranking.

Surprisingly, Fig. 2 demonstrates that DPO rarely flips the ranking of $\left(y_{w}, y_{l}\right)$ over the course of training despite consistently reducing the loss $\mathcal{L}_{\text {DPO }}$. Aside from the group of points for which the model unlearns the correct preference ranking, we observe that the loss decreases and the reward margin increases consistently while training. However, at the point of lowest validation loss, less than half of the incorrectly ranked points have been flipped to have the correct ranking. This indicates that the DPO objective is ill-formulated to induce a high ranking accuracy in practice.

![](https://cdn.mathpix.com/cropped/2024_06_04_c0fbacb600a68d309b81g-07.jpg?height=537&width=1415&top_left_y=233&top_left_x=360)

![](https://cdn.mathpix.com/cropped/2024_06_04_c0fbacb600a68d309b81g-07.jpg?height=416&width=418&top_left_y=250&top_left_x=363)

(a) DPO loss.

![](https://cdn.mathpix.com/cropped/2024_06_04_c0fbacb600a68d309b81g-07.jpg?height=423&width=404&top_left_y=241&top_left_x=844)

(b) DPO reward margin.

![](https://cdn.mathpix.com/cropped/2024_06_04_c0fbacb600a68d309b81g-07.jpg?height=425&width=442&top_left_y=243&top_left_x=1313)

(c) Proportion of the dataset corresponding to each category of data.

Figure 2: Despite continuously decreasing the loss, DPO rarely flips the rankings of pairs and instead mostly increases the reward margin of already correctly ranked pairs. We train a Pythia-2.8B model using the DPO objective and categorize the training dataset into four subsets examples that initially have the correct ranking and are flipped to (1) correct or (2) incorrect, and examples that initially have the incorrect ranking and are flipped to (3) correct or (4) incorrect. In all three figures, the hue of the point indicates the category. The dashed vertical line indicates the training step at which the lowest eval. loss occurs. We also present results for two other models with three seeds each in Appendix D.
![](https://cdn.mathpix.com/cropped/2024_06_04_c0fbacb600a68d309b81g-07.jpg?height=348&width=1130&top_left_y=1108&top_left_x=495)

Figure 3: DPO loss alone does not predict ranking accuracy, due to the influence of the reference model log-ratio in the loss. Each point represents the DPO loss on a separate training example $\left(x, y_{w}, y_{l}\right)$ from a subsample of $1 \mathrm{~K}$ examples from the training dataset, using the model $\pi_{\theta^{*}}$ that corresponds to the checkpoint with the lowest validation loss. The color of each point indicates whether $\pi_{\theta^{*}}$ achieves the correct ranking on that example, i.e., whether $\pi_{\theta^{*}}\left(y_{w} \mid x\right)>\pi_{\theta^{*}}\left(y_{l} \mid x\right)$. The dashed line is the function $f(c)=-\log \sigma(\beta c)$, from Theorem 4.1. In summary, the examples that $\pi_{\theta^{*}}$ classifies correctly tend to be those that were already classified correctly by the reference model. Results for the other two seeds of each model are given in Fig. 8.

### 4.2 Analysis: How Easy Is It To Flip A Ranking?

In the result below, we show that the DPO loss can decrease substantially without any improvement on the ranking accuracy of the model. Specifically, the DPO loss that the model needs to reach in order to have the correct ranking on an example $\left(x, y_{w}, y_{l}\right)$ depends on the quality of the reference model, quantified by the reference model log-ratio. This dependence is highly ill-conditioned, whereby using a reference model with moderately incorrect likelihoods assigned to each continuation can effectively prevent DPO from learning the correct ranking.

Theorem 4.1. Consider an aggregated preference datapoint $\left(x, y_{w}, y_{l}\right)$ such that the reference model log-ratio is some constant c, i.e. $\log \frac{\pi_{R e}\left(y_{l} \mid x\right)}{\pi_{R e f}\left(y_{w} \mid x\right)}=c$. Then, $\mathcal{R}\left(x, y_{w}, y_{l}\right)=1$ if and only if $\mathcal{L}_{D P O}\left(x, y_{w}, y_{l}\right) \leq-\log \sigma(\beta c)$, where $\sigma$ is the sigmoid function.

Remark 4.2. It is straightforward to extend our analysis to popular variants of DPO. For illustration, we prove an analogous result for identity preference optimization (IPO, Azar et al. [2]) in App. A.5.

We prove this result in App. A.4. Our theoretical result allows us to formally identify the points that will be hard to flip in their rankings. Fig. 3 visualizes the reference model log-ratio for several settings
and highlights that datapoints with even mild ranking errors in the reference model will require the loss to be reduced to a very low value in order to flip the ranking. App. F contains examples of hardto-learn, easy-to-learn, and easy-to-flip datapoints. We observe that the hard-to-learn datapoints are substantially longer than the easy ones, and that the easy datapoints generally contain less ambiguous preference annotations. More generally, our result motivates the use of stronger $\pi_{\text {Ref }}$ models and iterative or on-policy variants of DPO $[48,63,23,51]$.

## 5 Ranking Accuracy and Win Rate

Our results on ranking accuracy illuminate how well DPO and RLHF can align to preference data, but we have not yet related these insights to how the generative behavior of the model changes during alignment. In particular, ranking accuracy is a convenient but off-policy metric and is thus not as widely adopted as the on-policy metric of win rate (see Sec. 2.2). Indeed, one could maximize the ranking accuracy by learning a strong classifier on the preference data, but that model may not generate high-quality text. Here, we explore the gap between on-policy (i.e., generative) and off-policy (i.e., classification) behaviors of LLMs through the lens of ranking accuracy and win rate. Since the DPO objective directly optimizes for ranking accuracy (Proposition 2.6), the relationship between these two metrics is a direct reflection of how off-policy training affects on-policy behavior.

We study the relationship between win rate and ranking accuracy in two settings: (1) during DPO training, and (2) in a DPO variant modulating the influence of $\pi_{\text {Ref }}$. We measure the win rate on 500 responses to prompts from the training dataset using the Alpaca Eval GPT-4 [29] auto-annotator.

Setting 1: DPO Training. We measure the win rate and the ranking accuracy of a Pythia 2.8B model [4] during DPO training with the same configuration as in Section 4. See Fig. 9 for the results.

Setting 2: Attenuating the reference model. Theorem 4.1 showed that $\pi_{\text {Ref }}$ exerts a negative influence on the ranking accuracy in most cases, so we design a new objective that scales the reference model log-ratio in $\mathcal{L}_{\text {DPO }}$ to further characterize how win rate and ranking accuracy relate.

$$
\begin{equation*}
\mathcal{L}_{D P O}^{\gamma}\left(\pi_{\theta}, \pi_{\mathrm{Ref}}\right)=-\underset{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}{\mathbb{E}}\left[\log \sigma\left(\beta\left(\log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\theta}\left(y_{l} \mid x\right)}+\gamma \log \frac{\pi_{\operatorname{Ref}}\left(y_{l} \mid x\right)}{\pi_{\operatorname{Ref}}\left(y_{w} \mid x\right)}\right)\right)\right] \tag{4}
\end{equation*}
$$

Note that $\mathcal{L}_{\mathrm{DPO}}^{\gamma}=\mathcal{L}_{\mathrm{DPO}}$ (Definition 2.2) when $\gamma=1$, and a larger value of $\gamma$ increases the role of the reference model. Also, $\gamma$ directly scales $c$ in Theorem 4.1, thereby controlling how easy it is to fit the data and increase the ranking accuracy. We train a range of Pythia-2.8B models using the $\mathcal{L}_{D P O}^{\gamma}$ objective for $\gamma \in\{0,0.25,0.5,0.75,1.0,1.25,1.5,1.75,2.0\}$ and measure the ranking accuracies and win rates of the best model for each $\gamma$ value. ${ }^{6}$

Takeaway: Ranking accuracy correlates with win rate when the model is close to the reference model. In both settings, we observe that the win rate and ranking accuracy are highly correlated with one another in the early phase of training but become anti-correlated (i.e., ranking accuracy increases but win rate declines) as the model $\pi_{\theta}$ moves away from the reference $\pi_{\text {Ref }}$ (Fig. 4). Unlike traditional overfitting, the test loss is continuing to decline at this point (Fig. 9b). Experiments in Fig. 10 with the attenuated objective in Equation (4) further show that ranking accuracy and win rate trend together when the influence of the reference model is stronger (i.e., $\gamma$ is larger).

We speculate that when the model is far from the reference model, regularization toward the reference model can harm the generative capabilities of the model, which are primarily acquired during pretraining. In other words, the off-policy behavior of the model can no longer predictably describe the on-policy generations when the reference model used in the offline objective is far from the current model. Our findings confirm the fundamental tradeoff between fitting the preference data and maintaining generative capabilities acquired during pre-training [21] and align with prior observations that adding on-policy preference data can make offline learning more effective [48, 63, 23, 51].[^3]

![](https://cdn.mathpix.com/cropped/2024_06_04_c0fbacb600a68d309b81g-09.jpg?height=382&width=637&top_left_y=240&top_left_x=362)

(a) Ranking accuracy and win rate of various Pythia 2.8B checkpoints during training, versus the distance travelled by the model weights $\theta_{t}$ from the initialization.

![](https://cdn.mathpix.com/cropped/2024_06_04_c0fbacb600a68d309b81g-09.jpg?height=374&width=618&top_left_y=252&top_left_x=1122)

(b) Ranking accuracy and win rate of various $\gamma$ scaled models, versus the distance travelled by the model weights $\theta_{\gamma}$ from the initialization.

Figure 4: When the model weights have not travelled far from $\theta_{\text {Ref }}$, ranking accuracy and win rate increase together. $\theta_{t}$ represents the model weights at checkpoint $t$, and $\theta_{\gamma}$ are the model weights for the model trained with $\mathcal{L}_{\text {DPO }}^{\gamma}$.

## 6 Related Work

Many works have investigated the role of the preference dataset $[54,59]$, the reliability of the evaluations [67, 28], and the confounding factor of response length [43, 9, 55, 35]. Theoretical works have unified the many preference learning algorithms into clear taxonomies that permit analysis and, sometimes, yield new variants $[2,59,46,48,61]$. Several works study idiosyncrasies of preference learning, such as why DPO decreases the likelihood of both rejected and chosen outputs from the dataset [40, 12,33] and why RLHF exhibits vanishing gradients [42]. In contrast, our work approaches understanding DPO and RLHF through the lens of ranking accuracy, and our findings emphasize the role of the reference model regularization in preference learning. Relatedly, SliC-HF [65], CPO [57], and pairwise cringe loss [58] optimize $\log$ probability margins $\log \pi\left(y_{w} \mid x\right)-\log \pi\left(y_{l} \mid x\right)$, effectively removing the regularization toward the reference model. Liu et al. [31] recommend using the reference model at inference time to exert more granular control over the regularization. Tang et al. [48] also analyzed the role of regularizing toward a reference model, though our work focuses the effect of this regularization on ranking accuracy. App. B discusses additional related works.

## 7 Discussion

Our work highlights the significant but nuanced relationship between preference learning and ranking accuracy. We have demonstrated both theoretically and empirically that RLHF and DPO struggle to teach the model to correctly rank preferred and dispreferred outputs, even in the training dataset. Although the learning objective promotes high ranking accuracy in theory (Proposition 2.6), we observed a prominent alignment gap resulting from the poor conditioning of reference models. We then drew connections between the off-policy nature of ranking accuracy and the on-policy evaluations of win rate, identifying specific scenarios in which on-policy behavior can or cannot be reliably predicted by off-policy behavior. App. B details the limitations of our work.

Connections to Safety Our work shows that it is difficult to steer pre-trained LLMs to adhere to even the preference data used for training. When LLMs are used to judge responses from other models [29, 67] or to improve their own abilities [32, 63], poor ranking accuracies can induce strong negative feedback loops that are costly to mitigate.

We also observe that win rate does not monotonically increase during training (Fig. 9a), despite the decrease in both train and test loss (Fig. 9b) and the modest gain in ranking accuracy (Fig. 9a). As such, it is clear that we still do not understand the behaviors of preference learning. For example, others have observed that DPO can cause the likelihoods of both chosen and rejected outputs to decrease $[40,12,33,34]$, which implies that the policy must be moving probability mass to possibly undesirable sequences outside the data distribution. Moreover, our investigation of the non-monotonic
relationship between ranking accuracy and win rate emphasizes the need for concrete evaluations that can more reliably and transparently measure the success of preference learning.

Future Work Our theoretical results only describe the behavior of the model on the preference data used during training, but they can serve as a starting point for understanding generalization to different distributions of data, especially the one prescribed by the model itself [46]. Furthermore, we hope to analyze the optimization dynamics of preference learning, given the intriguing relationship observed between ranking accuracy and win rate. For instance, identifying when the win rate begins to diverge from the ranking accuracy can motivate adding fresh on-policy training data. Our initial investigation into ranking accuracy also suggests that it is worthwhile to explore how alignment techniques interact with other calibration metrics.

## 8 Acknowledgements

We thank Sanjeev Arora, Tianyu Gao, Eric Mitchell, Richard Pang, and Mengzhou Xia for helpful discussions during the development of this project. We also thank Nikita Nangia, Eshaan Nichani, and Alexander Wettig for their help in proofreading the work.

This work was supported by National Science Foundation Award 1922658, the Samsung Advanced Institute of Technology (under the project Next Generation Deep Learning: From Pattern Recognition to AI), NSF CAREER Award 2145542, ONR N00014-23-1-2634, and Google. SM is additionally supported by ONR, NSF, and DARPA. We also thank Princeton Language and Intelligence (PLI) for computing resources and OpenAI credits, and NYU High Performance Computing (HPC) for computing resources and in-kind support.

## References

[1] Alex Havrilla. synthetic-instruct-gptj-pairwise (revision cc92d8d), 2023. URL https://huggingface.co/datasets/Dahoas/ synthetic-instruct-gptj-pairwise.

[2] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human preferences, 2023.

[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.

[4] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397-2430. PMLR, 2023.

[5] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952. ISSN 00063444. URL http://www.jstor.org/stable/2334029.

[6] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.

[7] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, KaShun SHUM, and Tong Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview. net/forum?id=m7p507zblY.

[8] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.

[9] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators, 2024.

[10] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with $\mathcal{V}$-usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 59886008. PMLR, 17-23 Jul 2022.

[11] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization, 2024.

[12] Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, and Wenqiang Lei. Towards analyzing and understanding the limitations of dpo: A theoretical perspective, 2024.

[13] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022.

[14] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April 2023.

[15] Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu, and Marc Dymetman. Aligning language models with preferences through f-divergence minimization. arXiv preprint arXiv:2302.08215, 2023.

[16] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hanna Hajishirzi. Olmo: Accelerating the science of language models. arXiv preprint, 2024. URL https://api.semanticscholar.org/ CorpusID:267365485.

[17] Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model, 2024.

[18] Tom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard, 2024.

[19] Eyke Hüllermeier, Johannes Fürnkranz, Weiwei Cheng, and Klaus Brinker. Label ranking by learning pairwise preferences. Artificial Intelligence, 172(16):1897-1916, 2008. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2008.08.002. URL https://www . sciencedirect.com/science/article/pii/S000437020800101X.

[20] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023.

[21] Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, José Miguel Hernández-Lobato, Richard E Turner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In International Conference on Machine Learning, pages 1645-1654. PMLR, 2017.

[22] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.

[23] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. sdpo: Don't use your data all at once, 2024.

[24] Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He He, Bertie Vidgen, and Scott A. Hale. The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models, 2024.

[25] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Minh Nguyen, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Alexandrovich Glushkov, Arnav Varma Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Julian Mattick. Openassistant conversations - democratizing large language model alignment. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https:// openreview.net/forum?id=VSJotgbPHF.

[26] Tomasz Korbak, Ethan Perez, and Christopher L Buckley. Rl with kl penalties is better viewed as bayesian inference. arXiv preprint arXiv:2205.11275, 2022.

[27] Nathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush. Huggingface h4 stack exchange preference dataset, 2023. URL https://huggingface.co/datasets/ HuggingFaceH4/stack-exchange-preferences.

[28] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling, 2024.

[29] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023.

[30] Ziniu Li, Tian Xu, and Yang Yu. Policy optimization in rlhf: The impact of out-of-preference data, 2024.

[31] Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello, Quentin Berthet, Felipe Llinares, Jessica Hoffmann, Lucas Dixon, Michal Valko, and Mathieu Blondel. Decoding-time realignment of language models, 2024.

[32] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=S37hOerQLB.

[33] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive, 2024.

[34] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization, 2024.

[35] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization, 2024.

[36] Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pages 745-750, 2007.

[37] Pouya Pezeshkpour and Estevam Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483, 2023.

[38] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.

[39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=HPuSIXJaa9.

[40] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From $r$ to $q^{*}$ : Your language model is secretly a q-function, 2024.

[41] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=8aHzds2uUyB.

[42] Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua M. Susskind, and Etai Littwin. Vanishing gradients in reinforcement finetuning of language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=IcVNBR7qZi.

[43] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in rlhf, 2023.

[44] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):18990-18998, Mar. 2024. doi: 10.1609/aaai.v38i17.29865. URL https://ojs.aaai.org/index.php/AAAI/article/view/29865.

[45] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf.

[46] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data, 2024.

[47] Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, Rémi Munos, Bernardo Ávila Pires, Michal Valko, Yong Cheng, and Will Dabney. Understanding the performance gap between online and offline alignment algorithms, 2024.

[48] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires, and Bilal Piot. Generalized preference optimization: A unified approach to offline alignment. arXiv preprint arXiv:2402.05749, 2024.

[49] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024.

[50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian

Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.

[51] Hoang Tran, Chris Glaze, and Braden Hancock. Iterative dpo alignment. Technical report, Snorkel AI, 2023.

[52] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang, Kashif Rasul, Alexander M. Rush, and Thomas Wolf. The alignment handbook. https://github . com/huggingface/alignment-handbook, 2023.

[53] Shankar Vembu and Thomas Gärtner. Label ranking algorithms: A survey. In Preference learning, pages 45-64. Springer, 2010.

[54] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Secrets of rlhf in large language models part ii: Reward modeling, 2024.

[55] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023 .

[56] Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes Fürnkranz. A survey of preference-based reinforcement learning methods. Journal of Machine Learning Research, 18 (136):1-46, 2017. URL http://jmlr.org/papers/v18/16-634.html.

[57] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation, 2024.

[58] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Iterative preference optimization with the pairwise cringe loss, 2024.

[59] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study, 2024.

[60] Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Yang Wang. Perils of self-feedback: Self-bias amplifies in large language models, 2024.

[61] Joy Qiping Yang, Salman Salamatian, Ziteng Sun, Ananda Theertha Suresh, and Ahmad Beirami. Asymptotics of language model alignment, 2024.

[62] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 10935-10950. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 23e6f78bdec844a9f7b6c957de2aae91-Paper-Conference.pdf.

[63] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing $\mathrm{Xu}$, and Jason Weston. Self-rewarding language models, 2024.

[64] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum? id=tr0KidwPLc.

[65] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slic-hf: Sequence likelihood calibration with human feedback, 2023.

[66] Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. Calibrating sequence likelihood improves conditional language generation. In The Eleventh International Conference on Learning Representations, 2023. URL https : / / openreview . net / forum?id=0qSOodKmJaN.

[67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=uccHPGDlao.

[68] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.

[69] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023.
