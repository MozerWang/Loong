![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)

# GTBENCH: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations 

Jinhao Duan ${ }^{* 1}$ Renming Zhang ${ }^{* 2}$ James Diffenderfer ${ }^{3}$ Bhavya Kailkhura ${ }^{3}$ Lichao Sun ${ }^{4}$<br>Elias Stengel-Eskin ${ }^{5}$ Mohit Bansal ${ }^{5}$ Tianlong Chen ${ }^{567}$ Kaidi Xu ${ }^{1}$<br>https://huggingface.co/spaces/GTBench/GTBench<br>(https://github.com/jinhaoduan/GTBench


#### Abstract

As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBENCH, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1 Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Open-source LLMs, e.g., CodeLlama-34b-Instruct, are less competitive than commercial LLMs, e.g., GPT-4, in complex games. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. Detailed error profiles are also provided for a better understanding of LLMs' behavior.


[^0]Preprint.

## 1. Introduction

Large Language Models (LLMs) are increasingly being integrated into critical real-world applications, such as cybersecurity (Ameri et al., 2021; Aghaei et al., 2022), decision science (Jiang et al., 2023b), and finance (Wu et al., 2023). These areas involve advanced strategic thinking and logical reasoning skills, including the ability to foresee possible dangers and weaknesses, systematically examine difficulties, and make informed decisions based on provided evidence. However, environments that thoroughly assess these situations are not sufficiently explored.

There has been an emerging trend where LLMs are evaluated in various interactive role-playing environments, including collaborative environments such as CAMEL (Li et al., 2023), ReConcile (Chen et al., 2023) and competition environments such as Diplomacy (Bakhtin et al., 2022), Werewolf (Xu et al., 2023a), Avalon (Light et al., 2023; Stepputtis et al., 2023), Multi-agent Debate (Liang et al., 2023; Du et al., 2023; Chan et al., 2023; Xiong et al., 2023). Role-playing-based environments offer the useful potential for analyzing the cognitive reasoning abilities of LLMs, by engaging LLMs in conversation in these simulated scenarios. However, the extensive background and intricate details involved in role-play-based games dilute the pureness of logic and strategic reasoning that is typically found in gametheoretic tasks. Additionally, the evaluation is primarily verbal as it hinges on spoken or written exchanges between the LLMs. This could mask instances where LLMs might lack concrete reasoning abilities but navigate the scenario effectively through the proficient use of language.

Why are game-theoretic tasks unique and necessary for LLM reasoning evaluation? Game-theoretic tasks are typically conceptualized based on prevalent trade-offs and dilemmas manifesting in real-life scenarios and are designed to be easy to understand yet require difficult skills to be mastered. In contrast to the rich narrative contexts afforded in verbal- or role-playing-based games, e.g., Werewolf (Xu
et al., 2023a) and Avalon (Light et al., 2023), the reality of game-theoretic games such as Chess and Go involve: (1) pure logic and strategic reasoning without the added complexity of backgrounds or character roles; (2) embracing rigorous rules with well-defined action/state space, which allow for an in-depth examination of the strategic reasoning of LLMs.

Hence, in order to spur more research in the LLM GameTheoretic evaluation domain, we propose GTBENCH, an environment consisting of 10 widely recognized gametheoretic tasks, across a comprehensive taxonomy of games, e.g., complete- (Tic-Tac-Toe, Connect-4, Breakthrough) versus incomplete-information (Kuhn Poker, Liar's Dice) gaming, deterministic (Nim) versus probabilistic (Negotiation, Pig) gaming, static versus dynamic (Iterated Prisoner's Dilemma, Blind Auction) gaming. These environments require a variety of abilities including board strategy, collaboration, auction, and bidding. There are two key issues investigated in this paper:


#### Abstract

Characterizing Strategic Reasoning of LLMs: Are LLMs capable of performing game-theoretic tasks? What are essential factors for improving strategic reasoning? How do they perform compared to conventional solvers?


Game-Theoretic LLM-to-LLM Competitions as New Reasoning Evaluation: A new benchmark that can be competitive in strategic reasoning even for future LLMs.

To address these two key issues, we conduct experiments over two configurations: (a) LLM-vs-Conventional where conventional solvers such as optimization- or search-based solvers, e.g., Monte-Carlo Tree Search (MCTS) (Chaslot et al., 2008), are taken as the opponent of LLMs; (b) LLM-vs-LLM where two LLMs compete directly to reveal the reasoning limitations in a mutual manner. We find that: (1 LLMs almost always fail when playing against simple MCTS opponents in complete and deterministic gaming scenarios (Section 4.2), while (2) LLMs remain competitive in the incomplete and probabilistic scenarios (Section 4.3) when playing against conventional solvers; (3 Code-pretraining benefits game-theoretic reasoning, e.g., CodeLlama-34b-Instruct (Roziere et al., 2023) achieves comparable results as GPT-3.5-turbo (Section 4.4); 4 Advanced reasoning methods, such as Chainof-Thought (CoT) (Wei et al., 2022), Self-Consistent CoT (SC-CoT) (Wang et al., 2022b), Tree-of-Thought (ToT) (Yao et al., 2024) are not always helpful; (5 Open-source LLMs are less competitive than commercial LLMs in games with complex rules and large action/state space, yet are equally competitive in games with limited action/state space. The interfaces of GTBENCH leaderboard can be found in Appendix $\mathrm{H}$. Our contributions can be summarized as the following:

- LLM Game-Theoretic Evaluation (GTBENCH): the first comprehensive LLM game-theoretic environment, supporting 10 tasks across diverse gaming scenarios, is provided to spur future work for the community. The code and leaderboard will be public and continuously updated for future reasoning agents and LLMs.
- Revealing Game-Theoretic Characteristics: we characterize distinct LLM behaviors when facing different game-theoretic scenarios, showing that although LLMs fail in complete-information and deterministic gaming, they remain competitive in probabilistic gaming.
- Game-Theoretic Multi-Turn LLM-vs-LLM: we identify the essential factors (e.g., code-pretraining) for improving strategic reasoning of LLMs and provide a comprehensive environment for LLM-vs-LLM evaluation.


## 2. Background and Problem Definition

### 2.1. Background and Related Work

LLM-as-Agent Evaluation. Several studies have been conducted to measure the effectiveness of LLMs as agents in recent years. Hausknecht et al. (2020) carried out an extensive study to evaluate the performance of LLMs in interactive fiction games. Zhu et al. (2023) provides a valuable dataset for finetuning LLMs to improve usefulness in the strategic game Dungeons \& Dragons. GRUE (Ramamurthy et al., 2023) uses reinforcement learning-based metrics to benchmark the performance of generation tasks in six different languages. Gandhi et al. (2023) test the use of LLMs as a broker with human contestants in the negotiation game "Deal or No Deal". A few studies have explored the use of text-based games as a means of facilitating learning in such environments. ALFWorld (Shridhar et al., 2020) introduced a novel virtual environment that allows agents to acquire learning in a text-based environment while executing in a visual environment. The environment was developed in conjunction with Building Understanding in Text world via Language for Embodied Reasoning (BUTLER) agent, which can acquire abstract text knowledge in the text world. Similarly, TextWorld (Côté et al., 2019) is introduced as an environment that enables RL agents to play text games. Wang et al. (2022a) proposed ScienceWorld, a benchmark used for evaluating agents' reasoning ability, and their findings showed that transformer-based models are not effective at reasoning in novel contexts. MTBench (Zheng et al., 2024) introduces LLM-as-a-Judge where GPT-4 is utilized as a judge to evaluate the quality of LLM generations. It indicates that GPT-4 shares close criteria as humans. There have been works evaluating LLMs in solving real-world tasks, such as graph reasoning (Besta et al., 2023), WebShop (Yao et al., 2022), AgentBench (Liu et al., 2023) for

Table 1. Game environments explored in GTBENCH.

| Game | Taxonomy of Games |  |  |  | Preferred Ability of Players |  |  |  |  | \# Max <br> Actions |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | First-player <br> Advantage | Complete <br> Incomplete | Dynamic <br> Static | Probabilistic <br> Deterministic | Board Strategy | Bids | Collaboration | Bluff | Math |  |
| Tic-Tac-Toe | $\checkmark$ | $\Delta$ | O | - | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | 9 |
| Connect-4 | $\checkmark$ | $\Delta$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-03.jpg?height=45&width=136&top_left_y=450&top_left_x=836) | ![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-03.jpg?height=45&width=181&top_left_y=450&top_left_x=982) | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | 7 |
| Kuhn Poker | $V$ |  | ![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-03.jpg?height=46&width=136&top_left_y=483&top_left_x=836) | $\Delta$ | $x$ | $x$ | $x$ | $\checkmark$ | $\checkmark$ | 2 |
| Breakthrough | $\boldsymbol{X}+$ | $\Delta$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-03.jpg?height=46&width=136&top_left_y=517&top_left_x=836) | $\overline{0}$ | $\checkmark$ | $x$ | $x$ | $x$ | $x$ | 18 |
| Liar's Dice | $x$ |  | - | $\Delta$ | $x$ | $\checkmark$ | $x$ | $\checkmark$ | $\checkmark$ | 2 |
| Blind Auction | $x$ | - | $\Delta$ | $\Delta$ | $x$ | $\checkmark$ | $x$ | $x$ | $\checkmark$ | $-\dagger \dagger$ |
| Negotiation | $x$ | O | - | $\Delta$ | $x$ | $x$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $-\dagger \dagger$ |
| Nim | $\checkmark$ | $\Delta$ | 0 | ![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-03.jpg?height=40&width=181&top_left_y=657&top_left_x=982) | $x$ | $x$ | $x$ | $x$ | $\checkmark$ | $-\dagger \dagger$ |
| Pig | $x$ | $\Delta$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-03.jpg?height=46&width=136&top_left_y=691&top_left_x=836) | $\Delta$ | $x$ | $x$ | $x$ | $x$ | $x$ | 2 |
| Iterated Prisoner's <br> Dilemma | $x$ | $\Delta$ | $\Delta$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-03.jpg?height=69&width=181&top_left_y=724&top_left_x=982) | $x$ | $x$ | $\boldsymbol{V}$ | $x$ | $\checkmark$ | 2 |

$\dagger$ : Breakthrough has a slight first-player advantage which is not as significant as others.

$\ddagger$ : The iterated version of Prisoner's Dilemma allows participants access to the actions made by their opponents in the past rounds, achieving implicit collaboration.

$\dagger \dagger$ : Inapplicable due to complex combination and dynamic environment.

pragmatic missions, MINT (Wang et al., 2023b) for tool utilization.

Multiple LLMs-as-Agents in Gaming. Beyond utilizing a single LLM as an agent, a popular research topic is competition or collaboration between LLMs. A large body of literature explores the strategic reasoning potential and performance of LLMs by outlining evaluation frameworks and evaluating multiple LLM-agent performance on individual games such as: Social deduction or deception games $(\mathrm{Xu}$ et al., 2023a;b; O'Gara, 2023; Light et al., 2023), diplomacy games (Mukobi et al., 2023; , FAIR), negotiation games (Abdelnabi et al., 2023), coordination and cooperation games (Akata et al., 2023), and Minecraft (Gong et al., 2023; Wang et al., 2023a; Fan et al., 2022). These works not only provide evaluation frameworks for games and demonstrate the flexibility of LLMs to a variety of gaming tasks but some provide meaningful datasets for fine-tuning, policies for reinforcement learning to produce better strategies, or evaluate the strategic reasoning of LLMs. However, many of these standalone works quantify either individual or a subset of desirable strategic reasoning capabilities of LLMs, such as negotiation, deception, or coordination. Further, they often evaluate these capabilities for LLMs using one or two games which may produce less robust assurances of LLM abilities.

We make an additional crucial contribution in this line of work by measuring strategic reasoning capabilities with games that are not found in the existing unified benchmark suites, such as LMRL-Gym (Abdulhai et al., 2023), etc. More specifically, our benchmark GTBENCH seeks to provide a unified suite of games that are carefully curated to (1) evaluate a comprehensive collection of strategic reasoning abilities for a given agent and (2) enable competition-based scenarios (i.e., LLM agent-1 vs LLM agent-2) allowing for competition-based comparisons of strategic reasoning capabilities by LLM-based agents.

### 2.2. Problem Definition

We formalize the general gameplay and establish the metrics for measuring the performance of the participants.

Notation: Gameplay. We formulate the gameplay as a Markov Decision Process $(\mathcal{S}, \mathcal{A}, \mathcal{M}, \mathcal{O})$ under a given game environment, among the alternating interaction of two participants. This process composes of an infinite state space $\mathcal{S}$, an infinite action space $\mathcal{A}$, the participants $\mathcal{M}=\left\{\mathcal{M}_{1}, \mathcal{M}_{2}\right\}$, and an observation space $\mathcal{O}$. Considering the decision of $\mathcal{M}_{i}(i=1,2)$ at the $t$-th step of the process, we denote by $s_{t} \in \mathcal{S}$ the state that $\mathcal{M}_{i}$ are placed and $o_{t} \in \mathcal{O}$ the observation that $\mathcal{M}_{i}$ are observing. We assume $\mathcal{M}_{i}$ follows policy $\pi_{\theta_{i}}\left(a_{t} \mid s_{t}, o_{t}\right)$ for state transition $\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$, where $a_{t} \in \mathcal{A}$ is the action sampled by $\pi_{\theta_{i}}$ under conditions $s_{t}$ and $o_{t}$. $\theta_{i}$ is determined by the implementation by $\mathcal{M}_{i}$, e.g., optimization-based solver, LLM-driven agents, which will be discussed in Section 3.3 in detail. In this way, the two-participate gameplay can be represented as $\left(s_{0}, a_{0}, s_{1}, a_{1}, s_{2}, \cdots, s_{n}\right)$, where $s_{0}$ is the initial state and $s_{n}$ is a terminal state, i.e., end of the game. The progress is driven by the alternating execution of actions sampled by participants. Please refer to Section 3.2 and Appendix A for all the supported games and the corresponding possible actions and observations.

Metrics. We introduce a metric measuring the performance of each participant. Normally, the identification of the winner in each game follows one of two protocols: 1) beat the opponent (e.g., Tic-Tac-Toe, Breakthrough); 2) earn more rewards (e.g., Kuhn Poker, Negotiation). To unify them and for convenient comparison, we define Normalized Relative Advantage (NRA), denoted $\operatorname{NRA}\left(\mathcal{M}_{i}, \mathcal{M}_{o}, f_{s}\right)$, to measure to what extent the participant $\mathcal{M}_{i}$ is better/worse than its opponent $\mathcal{M}_{o}$, under the

![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-04.jpg?height=765&width=1704&top_left_y=211&top_left_x=186)

Figure 1. The overall architecture of GTBENCH. There are three main components from right to left: Environments (c) for game hosting, observation providing, and action execution; Prompt Adapter (b) for converting observation to prompt and extracting actions from participants' generations; Participants (a) for reasoning and action generation. (SC-)CoT agent refers to (Self-consistent) Chain-ofThought Agent and ToT agent refers to Tree-of-Thought agent.

score calculation $f_{s}$ :

$\operatorname{NRA}\left(\mathcal{M}_{i}, \mathcal{M}_{o}, f_{s}\right)=\frac{\sum_{m} f_{s}\left(\mathcal{M}_{i}, m\right)-\sum_{m} f_{s}\left(\mathcal{M}_{o}, m\right)}{\sum_{m}\left|f_{s}\left(\mathcal{M}_{i}, m\right)\right|+\sum_{m}\left|f_{s}\left(\mathcal{M}_{o}, m\right)\right|}$

where $f_{s}\left(\mathcal{M}_{i}, m\right)$ refers to the score earned by $\mathcal{M}_{i}$ at the $m$-th match $(1 \leq m \leq K, K$ is the number of performed matches):

- For beat-the-opponent games,

$$
f_{s}\left(M_{i}, m\right)= \begin{cases}1, & \text { if } \mathcal{M}_{i} \text { wins at the } m \text {-th match } \\ 0, & \text { if } \mathcal{M}_{i} \text { loses at the } m \text {-th match } \\ 0.5, & \text { if } \mathcal{M}_{i} \text { and } \mathcal{M}_{o} \text { achieve a draw }\end{cases}
$$

- For earn-rewards games, $f_{s}\left(M_{i}, m\right)$ is simply the rewards earned by $\mathcal{M}_{i}$ at the $m$-th match.

The absolute value, $|\cdot|$, is used in zero-sum games where $f_{s}\left(\mathcal{M}_{i}, m\right)=-f_{s}\left(\mathcal{M}_{o}, m\right)$, to avoid division-by-zero issues. $\operatorname{NRA}\left(\mathcal{M}_{i}, \mathcal{M}_{o}, f_{s}\right)$ is naturally normalized to $[-1,1]$, providing an interpretable meaning regarding the performance of $\mathcal{M}_{i}: \operatorname{NRA}\left(\mathcal{M}_{i}, \mathcal{M}_{o}, f_{s}\right)>0$ means $\mathcal{M}_{i}$ is better than $\mathcal{M}_{o} ; N R A\left(\mathcal{M}_{i}, \mathcal{M}_{o}, f_{s}\right)<0$ means $\mathcal{M}_{i}$ is worse than $\mathcal{M}_{o} ; \operatorname{NRA}\left(\mathcal{M}_{i}, \mathcal{M}_{o}, f_{s}\right)=0$ means $\mathcal{M}_{i}$ is as competitive as $\mathcal{M}_{o}$.

## 3. GTBENCH: Game-Theoretic Evaluation of LLMs

GTBENCH is a language-driven RL-like environment, requiring participating agents to compete against each other in a game-theoretic manner.

### 3.1. Overall Framework

GTBENCH is designed to be flexible and extensible, providing unified interfaces to participants and games, and supporting various multi-turn-based games which can be extended in the future. The overall framework is presented in Figure 1. There are three main components:

- Environment: The environment ( Figure 1 (c)) is responsible for overseeing the crucial processes related to gameplay. Specifically, it is tasked with building up observations, managing gameplay, and applying the actions obtained from participants. In this paper, all of the gaming environments are built on top of OpenSpiel (Lanctot et al., 2019).
- Prompt Adapter: The prompt adapter ( Figure 1 (b)) plays a vital role in facilitating effective communication between the environment and the virtual participants. It serves as an intermediary between the two entities by receiving observations from the environment, which it then translates into unified observation prompts. The prompts are then parsed and sent to the participating agents to formulate their responses. The adapter is also responsible for obtaining actions from the participants, which it transforms into legal actions before parsing them to the environment for game execution.
- Participant: The participants ( Figure 1 (a)) involved in the gaming process generate responses according to the observation prompts received from the Prompt

Adapter. These responses consist of actions that participants intend to take in this turn.

### 3.2. Taxonomy of Game-Theoretic Tasks

The chosen tasks and their detailed configurations are presented in Table 1. To comply with the common taxonomy (Lanctot et al., 2019) of gametheoretic tasks and provide diverse gaming scenarios, GTBENCH supports 10 different gaming environments, including Tic-Tac-Toe, Connect-4, Kuhn Poker, Breakthrough, Liar's Dice, Blind Auction, Negotiation, Nim, Pig, Iterated Prisoner's Dilemma, covering 6 mainstream game-theoretic configurations, including complete- and incomplete-information gaming, dynamic and static gaming, and probabilistic and deterministic gaming. The preferred abilities of each game could be characterized as the combination of board strategy, bids, collaboration, bluff, and math. Please refer to Appendix A. 1 for the rules of each game and Appendix A. 2 for an explanation of game-theoretic taxonomy.

### 3.3. Participants

There are two main categories of participants considered in GTBench: Conventional Agents and LLM-driven Agents, as detailed below.

Conventional Agents output actions through a conventional optimization or searching process. To provide fair comparisons, we employ Monte Carol Tree Search (MCTS) (Chaslot et al., 2008) as the conventional agent for most of the games. In all the experiments, the number of simulations for the MCTS agent is set to 1000 , resulting in a powerful solver for most of the games. Since Iterated Prisoner's Dilemma is dynamic gaming with very limited action space, i.e., $<$ TESTIFY $>$ or $<$ SILENT $>$, we utilize the more popular Tit-for-Tat (Axelrod, 1981) strategy, which simply repeating the opponent's last action, as the conventional agent. We also include Random Agent that randomly selects action at each turn, serving as a baseline and sanity check. Please refer to Appendix B. 1 for more details about MCTS Agent and Tit-for-Tat Agent.

LLM-Driven Reasoning Agent consists of backbone LLMs and reasoning paradigms. For backbone LLMs, we consider well-recognized LLMs such as commercial LLMs: GPT-3.5-turbo-1106, GPT-4-0613; and open-source LLMs: Llama-2-70b-chat (Touvron et al., 2023), CodeLlama (Roziere et al., 2023), and Mistral-7b-Orca (Jiang et al., 2023a; Mukherjee et al., 2023). In terms of reasoning schemes, we consider the following reasoning paradigms as they are widely known to be effective for general reasoning tasks:

- Prompt: Directly Prompt LLMs to generate responses, without additional reasoning steps.
- Chain-of-Thought (CoT) (Wei et al., 2022): CoT Agent prompts LLMs by thinking step by step.
- Self-Consistent CoT (Wang et al., 2022b): SC-CoT Agent prompts LLMs by generating multiple step-bystep thinking trajectories and performing majority voting to get the final response. The number of trajectories is set to 5 in this paper.
- Tree-of-Thought (ToT) (Yao et al., 2024): ToT Agent prompts LLMs to generate responses by incorporating exploration and deliberate decision-making, e.g., selfevaluation. The number of sequences for both answer generation and answer evaluations is set to 3 .


### 3.4. Prompt Templates and Protocols

When prompting LLMs to generate the next action during the course of a game, the prompt is composed of four individual components, to make sure all the participants access the same observations and information from environments:

System Prompt provides general guidance on how the LLMs should perform.

Head Prompt provides the general background and rules of the game.

Observation Prompt is formatted by a fixed game-wise template, providing sufficient observations from the environment regarding the current gaming state, to make LLMs capable of making decisions. The following provides the template used in the Blind Auction environment:

Your budget is $<$ VALUATION $>$. Your bid must be strictly lower than or equal to $<$ VALUATION $>$. Your opponent also has an expected valuation and you do not know it. The legal actions are: $<$ LEGAL_MOVES $>$.

Here $<$ VALUATION $>$ and $<$ LEGAL_MOVES $>$ are variables and are obtained from a unified $<$ observation $>$ object. In this way, all the participants are guaranteed to assess the same information.

Reasoning Prompt guides the LLM's generation process, e.g., "Let's think step by step" for the CoT Agent.

Please refer to Appendix D for the detailed prompts and observations for each game and agent.

### 3.5. LLMs' Ability to Generate Valid Actions

We provide the task completion rates of all the LLMs and reasoning agents in Appendix D.5. The completion rates are calculated as $\frac{N}{50}$ where $N$ is the number of valid matches. Here, a valid match means the participants will always generate legal moves at each turn of the match. We show that

![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-06.jpg?height=406&width=1699&top_left_y=220&top_left_x=186)

Figure 2. The Normalized Relative Advantage (NRA) of state-of-the-art LLM-driven reasoning agents when against MCTS Agents and Random Agents, over complete and deterministic scenarios. Red and gray lines mean the maximum NRA achieved by LLM-driven agents when against the Random Agent and the MCTS Agent, respectively.
![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-06.jpg?height=538&width=1684&top_left_y=812&top_left_x=188)

GPT-4
GPT-3.5-turbo
codellama-34b-lnstruct Lama $^{-2-70 b-c h a t}$ Nistral 7 -Orca

GPT-4
GPT-3.5-turbo
codellam $2-34 b-\ln s$ Lruct 2 - 2 - 20 -chat Nistral-7b-Orca

Figure 3. The game-wise Normalized Relative Advantage (NRA) of LLMs when against MCTS/TfT Agents and Random Agents, over incomplete and probabilistic scenarios. Error bars are obtained over different reasoning methods. Green and gray lines mean the maximum NRA achieved by LLM-driven agents when against the Random Agent and Conventional Agents, respectively.

all the LLM agents achieve $\geq 90 \%$ completion rate, showing that the prompts are properly configured and LLMs are capable of following instructions to finish the game.

## 4. Are LLMs Capable of Strategic Reasoning?

In this section, we evaluate the strategic reasoning capabilities of LLMs by conducting experiments among conventional solvers and LLM-driven agents.

### 4.1. Setup

We consider both open-source LLMs, e.g., Llama-2-70bchat, CodeLlama-34b-Instruct, Mistral-7b-Orca, and commercial LLMs, e.g., GPT-4-0613, GPT-3.5-1106. For all the LLMs, the temperature is set to 0.2 and the max number of tokens is 1024 . For each competition, we run 50 valid matches. The final performance is measured by the averaged NRA over the 50 valid matches. To mitigate the first-player advantage, we let each participant be the first to go for 25 matches.

### 4.2. Complete and Deterministic Gaming

There are four complete and deterministic tasks supported in GTBENCH: Tic-Tac-Toe, Connect-4, Breakthrough, and Nim. We compare LLM-driven agents with Random Agent and MCTS Agent to show whether LLMs are capable of gaming incomplete information and deterministic scenarios. Results are summarized in Figure 2. In general, we show that all LLMs achieve substantial relative advantages when compared against Random Agent. The best result is achieved by GPT-4 w/ CoT reasoning. For open-source LLMs, CodeLlama-34b-Instruct is slightly better than Llama-2-70b-chat. In Section 4.4, we further reveal that CodeLlama-34b-Instruct substantially outperforms Llama-2-70b-chat in the LLM-vs-LLM scenarios. This verifies recent discoveries where code-pretraining may benefit logical reasoning (Madaan et al., 2022).

However, all the LLMs with various reasoning methods achieve NRA as -1 when against the MCTS Agent, meaning that LLM-driven agents can barely win even a single match. This is because for board games with moderate action/state space such as the 4 involved complete and deter-

![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-07.jpg?height=561&width=807&top_left_y=237&top_left_x=190)

Figure 4. The NRA of LLMs equipped with different reasoning methods against Random Agent. NRA equals 0 means the two participants are equally competitive. Advanced reasoning methods do not always result in better results.

ministic games in GTBENCH, the MCTS agent with a sufficient number of simulations is very powerful. Therefore, LLMs remain noncompetitive in complete and deterministic gaming.

### 4.3. Probabilistic and Dynamic Gaming

There are five probabilistic game-theoretic gaming tasks: Kuhn Poker, Liar's Dice, Blind Auction, Negotiation, Pig, and one additional dynamic task: Iterated Prisoner's Dilemma. We group these games together as they all involve stochasticity in the gameplay, which is different from complete and deterministic gaming. The Random Agent as the opponent is omitted for both Negotiation and Iterated Prisoner's Dilemma because the Random Agent rarely chooses to collaborate, resulting in meaningless evaluation.

Results are summarized in Figure 3. It is shown that Liar's Dice shares a similar trend as the complete and deterministic scenarios (Figure 2), where LLM-driven agents achieve NRA near -1 when against the MCTS Agent. This is because the 2-player Liar's Dice has very limited stochasticity, making the gameplay tend to be complete information. For other tasks, we found that LLMs do not always fail. We observe that the NRA of LLM agents is close to 0 over all the tasks, indicating that they are equally competitive as conventional solvers or even better (e.g., Kuhn Poker where GPT-4 outperforms MCTS Agent).

### 4.4. Game-Theoretic Multi-Turn LLM vs. LLM as a Reasoning Evaluation Method

We investigate whether well-recognized LLMs remain competitive to themselves in game-theoretic scenarios. Specif-
Table 2. The NRA of LLM-driven agents when against GPT-3.5turbo w/ Prompt Agent and GPT-4 w/ Prompt Agent. Cyan cells mean $\mathrm{CoT}$ results in better performance. Magenta cells mean CoT results in worse performance. Advanced reasoning benefits powerful LLMs such as GPT-3.5-turbo and GPT-4 benefit while it hurts other LLMs such as CodeLlama-34b-Instruct and Llama-270b-chat.

| Opponent | Model | Reasoning | avg. NRA $\uparrow$ |
| :---: | :---: | :---: | :---: |
| GPT-3.5-turbo | Prompt | 0.00 |  |
|  |  | CoT | 0.02 |
|  | GPT-4 | Prompt | 0.13 |
|  |  | CoT | 0.13 |
| GPT-4 w/ | CodeLlama-34b- | Prompt | -0.01 |
|  | Instruct | CoT | -0.09 |
|  | Llama-2-70b-chat | Prompt | -0.10 |
|  |  | CoT | -0.23 |

ically, we take GPT-3.5-turbo with Prompt Agent as the common opponent and make other LLM-driven agents compete with it. Please refer to in Appendix C for the full leaderboard results.

In general, GPT-4 is the most powerful LLM in strategic reasoning among all the examined LLMs. Here we break the results into 3 takeaways.

Code-Pretraining Benefits Game-Theoretic Tasks. In Figure 5, we observe that CodeLlama-34b-Instruct outperforms Llama-2-70b-chat with large margins over all gaming categories. Considering the parameter size is less than half of Llama-2-70b-chat, this suggests that code-pretraining may benefit game-theoretic tasks.

Advanced Reasoning Methods Do Not Always Help. We observe that advanced reasoning methods may lead to worse results in game-theoretic scenarios. To make it more clear, we present the averaged NRA obtained by reasoning methods across different LLMs when against Random Agent in Figure 4. In general, only Mistral-7b-Orca has a substantial improvement when equipped with CoT reasoning while advanced reasoning leads to worse results for other LLMs. In Table 2, we present the model-wise NRA when against GPT-3.5-turbo w/ Prompt Agent. We show that advanced reasoning slightly benefits powerful LLMs, e.g., GPT-3.5turbo, while it results in worse results for other LLMs. It suggests that advanced reasoning is a double-edged sword: powerful LLMs are capable of leveraging advanced reasoning to achieve better results, while advanced reasoning may impose additional reasoning errors and risks during the inference of ordinary LLMs.

In Appendix E, we further examine five different CoT strategies over the GPT-3.5-turbo model to mitigate the effect

![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-08.jpg?height=583&width=805&top_left_y=234&top_left_x=194)

Figure 5. The NRA of various LLM-driven Prompt Agents when against GPT-3.5-turbo w/ Prompt Agent. CodeLlama-34b-Instruct outperforms other open-source LLMs over all the gaming scenarios, showing that code-pretraining benefits strategic reasoning.

brought by prompt sensitivity, along with some failure cases presented. These CoT prompts result in different performances and they are worse than the naive Prompt Agent.

Open-source LLMs are Less Competitive than Commercial LLMs in Complex Games. We observe that opensource LLMs such as Llama-2-70b-chat and CodeLlama34b-Instruct are not good at games with complex rules and board states. For instance, Breakthrough has more complex rules and larger action/state space than other complete information games, e.g., Tic-Tac-Toe and Connect-4. In Table 3, we present the average NRA when including Breakthrough and excluding Breakthrough. It is shown that both Llama-2-70bchat and CodeLlama-34b-Instruct fail in Breakthrough, resulting in worse NRA scores than GPT-4. However, CodeLlama-34b-Instruct achieves comparable NRA scores as GPT-4 over the rest of the games, indicating that opensource LLMs are equally competitive as GPT-4 in games with limited action/state space.

### 4.5. Error Profiles

We introduce the most prevalent mistake patterns observed across different games, comprising factual inaccuracies, calculation mistakes, overconfidence, misinterpretation, and endgame. Demonstrations of each mistake pattern are presented in Appendix F. Misinterpretation denotes the misinterpretation of the game's current state by LLMs, including errors like misattributing piece ownership and failing to recognize vacant spots on the board. Factual Errors refer to situations where the player has a reasonable plan but their actions do not align with their plan. For instance, in Breakthrough, GPT-4 w/ CoT agent plans to fend off frontal attacks by the opponent, which is reasonable.
Table 3. The average NRA of LLM-driven agents when Breakthrough is included (w/ Breakthrough) and excluded (w/o Breakthrough). Open-source LLMs failed in games with complex rules and large action/state space, such as Breakthrough.

| Taxonomy | GPT-4 | CodeLlama- <br> 34b-Instruct | Llama- <br> 2-70b-chat | Mistral- <br> 7b-Orca |
| :---: | :---: | :---: | :---: | :---: |
| w Breakthrough | 0.13 | -0.01 | -0.20 | -0.31 |
| w/o Breakthrough | 0.11 | 0.08 | -0.18 | -0.27 |

Table 4. Quantitative results of each error pattern, obtained from GPT-4 w/ CoT agent when playing against conventional solvers over all the gaming scenarios.

|  | Error Percentage (\%) |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Model | Endgame <br> Misdetection | Misinterpretation | Over-confidence | Calculation <br> Error | Factual <br> Error |
| GPT-4 | 33.33 | 9.80 | 15.69 | 9.80 | 45.10 |

However, it takes rear pieces to achieve that, which is impossible. Over-confidence describes a scenario where a player overlooks potential risks in pursuit of greater rewards. Calculation Errors refer to errors that occur in arithmetic, such as calculating XOR in Nim. Endgame Misdetection means a failure to recognize immediate win/lose situations. For example, a player might fail to recognize a potential winning move.

In Table 4 , we present the quantitative results regarding these error patterns. It is obtained from GPT-4 w/ CoT agent when playing against conventional solvers, e.g., MCTS/TfT agent, as the opponent. We manually examined 157 turns over 50 matches across all 10 games. We observe that LLM agents are capable of generating reasonable planning/strategies. However, they have difficulties in selecting the correct actions to align with their thoughts. Also, LLMs miss endgame situations, leading to a failure to recognize winning and losing moves.

## 5. Diving into the Game-Theoretic Characteristics of LLMs

In this section, we investigate common game-theoretic characteristics such as regret and resource distribution.

### 5.1. Regret

Regret (Zinkevich et al., 2007) is a popular metric in the game theory domain, measuring how much a player would have improved their outcome by choosing a different strategy, given what they know now after the game has played out. It is calculated by comparing the actual payoff received with the best possible payoff that could have been achieved. A lower regret implies that the decisions or actions taken are close to the optimal ones. We calculate the regret values of

![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-09.jpg?height=564&width=807&top_left_y=241&top_left_x=190)

Figure 6. The regret values of each LLMs when taking GPT-3.5turbo w/ Prompt Agent as the opponent, over Blind Auction and Iterated Prisoner's Dilemma.

four LLMs regarding Blind Auction and Iterated Prisoner's Dilemma. Please refer to Appendix G for how regret values are calculated for these two tasks.

We take GPT-3.5-turbo as the opponent when calculating regret values in LLM-vs-LLM settings. This is because it achieves moderate performances among these agents, providing a unified and fair comparison for all LLM agents. Results are summarized in Figure 6. For Blind Auction, we observe that GPT-4 achieves a lower regret value compared to other agents. This is consistent with the NRA scores, indicating that GPT-4 is more aligned with the optimal solution than others. For Iterated Prisoner's Di lemma, we observe that CodeLlama-34b-Instruct outperforms GPT-4 and achieves a lower regret value. According to the calculation of regret value and manual examination, it is because GPT-4 is more tends to keep silencing than other agents. This tendency may come from the comprehensive human preference alignment of GPT-4, making it friendly and less likely to testify its partner.

### 5.2. Resource Distribution

We utilize the game Negotiation to study the efficiency of resource distribution of LLMs. In this game, there are three categories of items, each with a predetermined number of items. The participants each have their own unique valuations for the different categories of items, but they are unaware of their opponent's valuations. Their goal is to negotiate how these items will be distributed among them. This game can assess qualities like selfishness and prosocial behavior. Selfishness is characterized by a participant's desire to acquire more items, while prosocial behavior is marked by a willingness to accept an unfair distribution of items. The optimal outcome aims for Pareto efficiency, ensuring that all participants attain their preferred values.

![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-09.jpg?height=577&width=805&top_left_y=243&top_left_x=1061)

Figure 7. Trade-off between the participants' values. Participant 1 is driven by various LLMs and Participant 2 is driven by GPT-3.5turbo. Each dot represents an agreed value division (Participant 1 value $x$, Participant 2 value $y$ ): Participant 1 with a value of $x$ and Participant 2 with a value of $y$. Results are obtained among 50 valid matches. Since it is possible that the agreement is not achieved eventually, the number of dots of each LLM is $\leq 50$.

We count all the agreements reached by participants and record the values attributed to each participant based on the agreed division. Results are summarized in Figure 7. It is shown that most of the agreement is achieved when both participants obtain substantial values. We also observe that some LLMs may easily compromise. For example, Llama2-70b-chat and CodeLlama-34b-Instruct accept an unfair division of resources. However, GPT-4 and Mistral find it hard to achieve agreement and they tend to negotiate with opponents to achieve Pareto improvement.

## 6. Conclusion

This work investigated LLMs' strategic and logical reasoning abilities under competitive scenarios. To achieve this, we created a broad evaluation scope by considering various classic and LLM-based gaming agents and 10 representative games. We conducted the first benchmark study of game-theoretic evaluations for LLMs, shedding light on their reasoning performance. Our holistic and extensive evaluations revealed insightful LLMs' gaming behavior, such as their intrinsic failure in complete and deterministic games, impressive reasoning in incomplete and probabilistic games, and benefiting from code-generation pertaining and appropriate prompt designs. In the future, we will establish an LLM gaming leaderboard and forum to promote high-quality open-source contributions.

## Impact Statements

This paper studies game-theoretic task proficiency in AI models. We understand the concerns around models becom-
ing autonomous entities with distinct objectives, especially in situations involving deception or negotiation, and thus it is important to clarify that our research only measures the capabilities of current models instead of augmenting their abilities to do so. Our work does not involve training AI models to be competent GT task performers or teaching them to bluff or defect. Instead, we identify and measure the existing competencies of existing models, contributing towards an in-depth understanding that could serve as a foundation for applying possible innovative measures against potential risks. Hence, we believe that the outcomes from our work could be highly beneficial in paving a responsible path toward safety and efficacy in AI.

## Acknowledgement

This work was performed under the auspices of the U.S. Department of Energy by the Lawrence Livermore National Laboratory under Contract No. DE- AC52-07NA27344 and was supported by the LLNL LDRD Program under Project No. 23-ERD-030. This work was partially supported by the NSF award FMitF-2319242. It was also partially supported by NSF-AI Engage Institute DRL-2112635 and DARPA MCS Grant N66001-19-2-4031. The views, opinions, and/or findings contained in this article are those of the authors and not of the funding agency.

## References

Abdelnabi, S., Gomaa, A., Sivaprasad, S., Schönherr, L., and Fritz, M. Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games, 2023.

Abdulhai, M., White, I., Snell, C., Sun, C., Hong, J., Zhai, Y., Xu, K., and Levine, S. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models, 2023.

Aghaei, E., Niu, X., Shadid, W., and Al-Shaer, E. Securebert: A domain-specific language model for cybersecurity. In International Conference on Security and Privacy in Communication Systems, pp. 39-56. Springer, 2022.

Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M., and Schulz, E. Playing repeated games with large language models, 2023.

Ameri, K., Hempel, M., Sharif, H., Lopez Jr, J., and Perumalla, K. Cybert: Cybersecurity claim classification by fine-tuning the bert language model. Journal of Cybersecurity and Privacy, 1(4):615-637, 2021.

Axelrod, R. The emergence of cooperation among egoists. American political science review, 75(2):306-318, 1981.

Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A. P.,
Komeili, M., Konath, K., Kwon, M., Lerer, A., Lewis, M., Miller, A. H., Mitts, S., Renduchintala, A., Roller, S., Rowe, D., Shi, W., Spisak, J., Wei, A., Wu, D. J., Zhang, H., and Zijlstra, M. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378:1067 - 1074, 2022. URL https://api.semanticscholar. org/CorpusID:253759631.

Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.

Chan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S., Fu, J., and Liu, Z. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.

Chaslot, G., Bakkes, S., Szita, I., and Spronck, P. Montecarlo tree search: A new framework for game ai. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 4, pp. 216-217, 2008.

Chen, J. C.-Y., Saha, S., and Bansal, M. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023 .

Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., El Asri, L., Adada, M., et al. Textworld: A learning environment for textbased games. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pp. 41-75. Springer, 2019.

Du, Y., Li, S., Torralba, A., Tenenbaum, J. B., and Mordatch, I. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.

(FAIR)†, M. F. A. R. D. T., Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A. P., Komeili, M., Konath, K., Kwon, M., Lerer, A., Lewis, M., Miller, A. H., Mitts, S., Renduchintala, A., Roller, S., Rowe, D., Shi, W., Spisak, J., Wei, A., Wu, D., Zhang, H., and Zijlstra, M. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067-1074, 2022. doi: 10.1126/ science.ade9097. URL https://www.science. org/doi/abs/10.1126/science.ade9097.

Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:18343-18362, 2022.

Gandhi, K., Sadigh, D., and Goodman, N. D. Strategic reasoning with language models, 2023.

Gong, R., Huang, Q., Ma, X., Vo, H., Durante, Z., Noda, Y., Zheng, Z., Zhu, S.-C., Terzopoulos, D., Fei-Fei, L., and Gao, J. Mindagent: Emergent gaming interaction, 2023.

Hausknecht, M., Ammanabrolu, P., Côté, M.-A., and Yuan, X. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7903-7910, 2020.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023a.

Jiang, H., Ge, L., Gao, Y., Wang, J., and Song, R. Large language model for causal decision making. arXiv preprint arXiv:2312.17122, $2023 \mathrm{~b}$.

Lanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V., Upadhyay, S., Pérolat, J., Srinivasan, S., Timbers, F., Tuyls, K., Omidshafiei, S., et al. Openspiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453, 2019.

Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D., and Ghanem, B. Camel: Communicative agents for "mind" exploration of large language model society. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.

Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R., Yang, Y., Tu, Z., and Shi, S. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.

Light, J., Cai, M., Shen, S., and Hu, Z. Avalonbench: Evaluating llms playing the game of avalon. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.

Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., and Tang, J. Agentbench: Evaluating llms as agents, 2023.

Madaan, A., Zhou, S., Alon, U., Yang, Y., and Neubig, G. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1384-1403, 2022.
Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., and Awadallah, A. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.

Mukobi, G., Erlebach, H., Lauffer, N., Hammond, L., Chan, A., and Clifton, J. Welfare diplomacy: Benchmarking language model cooperation. In Socially Responsible Language Modelling Research, 2023.

O'Gara, A. Hoodwinked: Deception and cooperation in a text-based game for language models, 2023.

Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel, J., Sifa, R., Bauckhage, C., Hajishirzi, H., and Choi, Y. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=8aHzds2uUyB.

Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

Shridhar, M., Yuan, X., Cote, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2020.

Stepputtis, S., Campbell, J., Xie, Y., Qi, Z., Zhang, W. S., Wang, R., Rangreji, S., Lewis, C. M., and Sycara, K. P. Long-horizon dialogue understanding for role identification in the game of avalon with large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An openended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.

Wang, R., Jansen, P., Côté, M.-A., and Ammanabrolu, P. Scienceworld: Is your agent smarter than a 5 th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1127911298, 2022a.

Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022b.

Wang, X., Wang, Z., Liu, J., Chen, Y., Yuan, L., Peng, H., and Ji, H. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691, 2023b.

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824-24837, 2022.

Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., and Mann, G. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023.

Xiong, K., Ding, X., Cao, Y., Liu, T., and Qin, B. Examining the inter-consistency of large language models: An indepth analysis via debate. Association for Computational Linguistics, 2023.

Xu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W., and Liu, Y. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, $2023 \mathrm{a}$.

Xu, Z., Yu, C., Fang, F., Wang, Y., and Wu, Y. Language agents with reinforcement learning for strategic play in the werewolf game, 2023b.

Yao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744-20757, 2022.

Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.

Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.

Zhu, A., Aggarwal, K., Feng, A., Martin, L., and CallisonBurch, C. Fireball: A dataset of dungeons and dragons actual-play with structured game state information. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational
Linguistics, 2023. doi: 10.18653/v1/2023.acl-long. 229. URL http://dx.doi.org/10.18653/v1/ 2023.acl-long.229.

Zinkevich, M., Johanson, M., Bowling, M., and Piccione, C. Regret minimization in games with incomplete information. Advances in neural information processing systems, 20, 2007.
