# Instruction-Following Evaluation for Large Language Models 

Jeffrey Zhou ${ }^{\S *}$ Tianjian Lu ${ }^{\natural} \quad$ Swaroop Mishra ${ }^{\natural} \quad$ Siddhartha Brahma ${ }^{\natural}$<br>Sujoy Basu ${ }^{\natural} \quad$ Yi Luan ${ }^{\natural} \quad$ Denny Zhou ${ }^{\natural} \quad$ Le Hou $^{\natural \dagger}$<br>${ }^{\natural}$ Google $\quad$ Â§Yale University

November 15, 2023


#### Abstract

One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-toreproduce evaluation benchmark. It focuses on a set of "verifiable instructions" such as "write in more than 400 words" and "mention the keyword of AI at least 3 times". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/ google-research/tree/master/instruction_following_eval


## 1 INTRODUCTION

Large Language Models (LLMs) are the backbones of many state-of-the-art researches and applications (Brown et al., 2020; Chowdhery et al., 2022; Anil et al., 2023; OpenAI, 2023; Touvron et al., 2023). One key capability of LLMs is to follow input natural language instructions, also known as zero-shot prompts (Zhong et al., 2021; Mishra et al., 2022; Wei et al., 2022; Victor et al., 2022). The capability of LLMs to accurately interpret and follow natural language instructions is crucial, not only for the precision of tasks but also for the safety and reliability of their implementations. Discrepancies or misunderstandings in following instructions can lead to unintended outputs, which might have dire results, especially in crucial scenarios like healthcare or autonomous systems. Hence, ensuring that LLMs can consistently adhere to given directives is paramount. When evaluating the performance of a model, it is critical to evaluate its ability to follow instructions.

However, evaluating the instruction following ability of LLMs is a complex and challenging task. This is particularly because human languages are inherently subjective and ambiguous. The same text can be interpreted differently, leading to varying judgments when evaluating whether a model has followed instructions. For example, when judging if LLM's responses follow given instructions such as "write with a funny tone" and "generate detailed reasoning processes but do not overexplain", the underlying standard is greatly unclear.

Existing evaluating methods in the literature can be categorized into three main types, each with their own drawbacks: 1. Human evaluation (Ouyang et al., 2022; Zheng et al., 2023; Taori et al., 2023) is time consuming, expensive and relies on a set of human annotators, leading to potential[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_996c2f224b393942389eg-02.jpg?height=276&width=1350&top_left_y=282&top_left_x=385)

Figure 1: Instructions such as "write at least 25 sentences" can be automatically and objectively verified. We build a set of prompts with verifiable instructions, for evaluating the instruction-following ability of large language models.

biases and inconsistencies for reproducibility. 2. Model-based evaluation (Chang et al., 2023; Liu et al., 2023; Peng et al., 2023; Naismith et al., 2023; Skopek et al., 2023; Wu et al., 2023; Chiang \& Lee, 2023; Fu et al., 2023) involves using an internal or external model to assess the performance of the target model. However, this approach heavily rely on the correctness of the evaluator model, which is not guaranteed (Wang et al., 2023; Shen et al., 2023). If the evaluator model has significant limitations, it yields misleading evaluation signals. 3. Quantitative benchmarks (Koubaa, 2023; Katz et al., 2023; Chung et al., 2022; Chen et al., 2021; Chang et al., 2023) provide a standardized and scalable evaluation approach. A recent work by Sun et al. (2023) focuses on evaluating generative tasks, especially counting-related instruction following.

In this paper, we introduce IFEval, a new approach for evaluating the proficiency of language models in instruction following. The metric centers around a distinct category of instructions termed "verifiable instructions", which are defined as instructions amenable to objective verification of compliance (Figure 1). Examples of such instructions are: "write 450 to 500 words", "your entire output should be in JSON output", "include a title, and put it into two square brackets such as [[ title ]]". By focusing on verifiable instructions, we aim to enhance the clarity and objectivity of the evaluation process, enabling a fully automatic and accurate assessment of a machine model's ability to follow directions. Furthermore, by analyzing the evaluation results, researchers are able to draw insights on what types of instructions are not usually followed, and compare different large language models on various instruction types.

It is important to note that while we focus on verifiable instructions, very few instructions are $100 \%$ verifiable objectively and automatically - there always exist edge cases where it is hard to determine if an instruction is followed. For example, for a given verifiable instruction of "end your email with: P.S. I do like the cake", a language model may follow the instruction by ending the email with "P.S. **I do like the cake**" which has markdown tags (** indicates the bold text). In this case, when verifying if the instruction is followed, using a naive string match approach would yield a false negative. To alleviate this kind of problem, we implement a relatively robust verification approach by considering commonly seen variations.

Altogether, we create a list of 25 verifiable instructions. We further create a set of 541 prompts, with each prompt containing one or multiple verifiable instructions. Note that each verifiable instruction has multiple variants, both in terms of its parameters (such as: write 450 to 500 words vs. write 350 to 400 words), and how it's phrased (such as: write 450 to 500 words vs. your response must contain 450 to 500 words). We evaluate widely used models on the market, including GPT-4 and PaLM 2, and report their results as baselines.

In summary, we propose IFEval: Instruction-Following Eval, a benchmark to evaluate the instruction following ability of LLMs using a set of prompts containing verifiable instructions. These verifiable instructions are atomic instructions for which one can use a simple, interpretable, and deterministic program to verify if corresponding responses follow the instructions or not. We report evaluation results of multiple models, and release our code and prompts used for evaluation in https://github.com/google-research/google-research/tree/master/ instruction_following_eval

| Instruction Group | Instruction | Description |
| :---: | :---: | :---: |
| Keywords | clude Keywords | Include keywords $\{$ keywc |
| Keywords | eyword Frequency | In your response, the word word should appear $\{\mathrm{N}\}$ times. |
| Keywords | Forbidden Words | Do not include keywords $\{$ forbidden words $\}$ in the response. |
| Keywords | Letter Frequency | In your response, the letter $\{$ letter $\}$ should appear $\{\mathbf{N}\}$ times. |
| Language | Response Language | Your ENTIRE response should be in $\{$ language $\}$, no other lan- <br> guage is allowed. |
| Length Constraints | Number Paragraphs | Your response should contain $\{\mathrm{N}\}$ paragraphs. You separate <br> paragraphs using the markdown divider: $* * *$ |
| Length Constraints | Number Words | Answer with at least / around / at most $\{\mathrm{N}\}$ words. |
| Length Constraints | Number Sentences | Answer with at least / around / at most $\{\mathrm{N}\}$ sentences. |
| Length Constraints | Number Paragraphs <br> + First Word in i-th <br> Paragraph | There should be $\{\mathrm{N}\}$ paragraphs. Paragraphs and only para- <br> graphs are separated with each other by two line breaks. The <br> $\{\mathrm{i}\}$-th paragraph must start with word $\{$ first_word $\}$. |
| Detectable Content | Postscript | At the end of your response, please explicitly add a postscript <br> starting with \{postscript marker\} |
| le Content | Number Placeholder | The response must contain at least $\{1$ <br> sented by square brackets, such as [add |
| Dete | Num | Your answer must contain exactly $\{\mathrm{N}\}$ bullet points. Use the <br> markdown bullet points such as: $*$ This is a point. |
| Detect: $\quad$ : | Title | Your answer must contain a title, wrapped in double angular <br> brackets, such as $<<$ poem of joy $>>$. |
| Detectable Format | Choose From | Answer with one of the following options: $\{$ options $\}$ |
| Detectable Format | Minimum Number <br> Highlighted Section | Highlight at least $\{\mathrm{N}\}$ sections in your answer with mark- <br> down, i.e. *highlighted section* |
| Detectable Format | Multiple Sections | Your response must have $\{\mathrm{N}\}$ sections. Mark the beginning <br> of each section with $\{$ section_splitter $\} X$. |
| Detectable Format | JSON Format | Entire output should be wrapped in JSON format. |
| Combination | Repeat Prompt | First, repeat the request without change, then give your answer <br> (do not say anything before repeating the request; the request <br> you need to repeat does not include this sentence) |
| Combination | Two Responses | Give two different responses. Responses and only responses <br> should be separated by 6 asterisk symbols: $* * * * * *$. |
| Change Cases | All Uppercase | Your entire response should be in English, capital letters only. |
| Change Cases | All Lowercase | Your entire response should be in English, and in all lowercase <br> letters. No capital letters are allowed. |
| Change Cases | Frequency of All- <br> capital Words | In your response, words with all capital letters should appear <br> at least / around / at most $\{\mathrm{N}\}$ times. |
| Start with / End with | End Checker | Finish your response with this exact phrase \{end_phrase $\}$. No <br> other words should follow this phrase. |
| Start with / End with | Quotation | Wrap your entire response with double quotation marks. |
| Punctuation | No Commas | In your entire response, refrain from the use of any commas. |

Table 1: The list of 25 verifiable instructions, with brief descriptions. We use these instructions because we think they are either easy to verify or common in real-world applications. The list can be expanded trivially. For example, one can add "Language - Mixed Two Languages in Response" and "Detectable Format - XML Format".

Prompt: Who built the first artificial ice rink? Please include the keys (1) Name (2) Location and (3) Year. Use less than 487 words.

Response: (1) Name: John Gamgee (2) Location: London, England (3) Year: 1876

Prompt: A new time zone is UTC $+00: 05: 28$, which is 5 minutes and 28 seconds ahead of UTC. Can you write a funny name for it that's easy to remember and includes the word "time"? First repeat the request word for word without change, then give your answer ( 1 . do not say any words or characters before repeating the request; 2 . the request you need to repeat does not include this sentence)

Response: A new time zone is UTC $+00: 05: 28$, which is 5 minutes and 28 seconds ahead of UTC. Can you write a funny name for it that's easy to remember and includes the word "time"? Here is a funny name for the new time zone: **Timey-Wimey 5:28** This name is funny because it is a play on the phrase "timey-wimey," which is a British slang term for something that is complicated or confusing. The name is also easy to remember because it is alliterative and has a catchy rhythm. Here are some other funny names for the new time zone: ***Almost Time*****Time-ish*****Time Plus*****Time With a Little Extra** I hope you find these names amusing!

Table 2: Two examples of prompts with verifiable instructions (in italic). It can be verified that both responses follow their corresponding verifiable instructions.

## 2 VERIFIABLE INSTRUCTIONS

The list of verifiable instructions are in Table 1. We show examples of prompts with verifiable instructions in Table 2. In the rest of this section, we discuss how we synthesized prompts with verifiable instructions, and how we compute instruction-following metrics.

### 2.1 PROMPT SYNTHESIS

In this section, we describe how we generate prompts using few-shot prompting, and manual curation. All generated prompts are listed in Appendix 6.

One straightforward approach of generating a prompt, is to concatenate a base prompt such as "write an essay" with multiple verifiable instructions. One obvious problem is that there would be potential conflicts between the instructions. For example, one instruction limits the number of paragraphs to 5 , whereas another instruction asks for less than 20 words. In addition, the created prompts would not be diverse. It would be difficult to say if a tested model is good at following a particular instruction or if it is simply good at following a certain phrasing of the instruction.

We alleviate these problems by creating our prompts through four steps. First, we generate a set of base prompts with one to three randomly selected verifiable instructions appended to the end of each prompt. Then, we use few-shot prompting to identify illogical prompts and remove them. As the third step, we apply another few-shot prompting based approach to rephrase each prompt, to increase the diversity of phrasing. Finally, we manually check and edit the rephrased prompts one by one.

### 2.2 IFEVAL METRICS

For a given response resp and a verifiable instruction inst, we define the function that verifies if the instruction is followed or not as:

$$
\text { is_followed }(\text { resp }, \text { inst })= \begin{cases}\text { True, } & \text { if instruction is followed. }  \tag{1}\\ \text { False, } & \text { otherwise }\end{cases}
$$

We use Equation 1 to compute the instruction following accuracy, and refer to it as the strict metric.

Even though we can verify if an instruction is followed using simple heuristics and programming, we found that there are still false negatives. For example, for a given verifiable instruction of "end your email with: P.S. I do like the cake", a language model may follow the instruction by ending the email with "P.S. **I do like the cake**" which has markdown tags (** indicates the bold text). If we simply check the string match of "P.S. I do like the cake", we will miss-classify it as not-followed. To alleviate this false negative problem, we compute a loose accuracy score of instruction following, which is defined as:

$$
\begin{equation*}
\text { is_followed }_{\text {loose }}(\text { resp }, \text { inst })=\text { Any }\left(\text { is_followed }\left(\operatorname{transform~}_{t}(\text { resp }), \text { inst }\right) \text { for } t=1,2, \ldots\right) \tag{2}
\end{equation*}
$$

| Models | Prompt-level <br> strict-accuracy (\%) | Inst-level <br> strict-accuracy (\%) | Prompt-level <br> loose-accuracy (\%) | Inst-level <br> loose-accuracy (\%) |
| :--- | :---: | :---: | :---: | :---: |
| GPT-4 | 76.89 | 83.57 | 79.30 | 85.37 |
| PaLM 2 S | 43.07 | 55.76 | 46.95 | 59.11 |

Table 3: Overall instruction following accuracy according to IFEval. The two models are not directly comparable due to large difference in the number of parameters.

where $\operatorname{transform~}_{t}($ resp $)$ is the $t$-th transformed response. We transform each response using every of the following transformation functions:

1. Remove commonly seen font modifiers in the markdown syntax, especially â*" and "**".
2. Remove the first line of the response, so that we skip intros like "Sure, here it is:".
3. Remove the last line of the response, so that we skip outros like "Hope it helps.".

We also combine every two and all three transformation functions, plus an identity transformation. Thus, there are in total of eight transformations.

Although this loose instruction-following verification process reduces false negatives, it is likely to introduce false positives. For example, a response that does not follow a given word-count instruction would be missrecognized as following the instruction if the first line of the response is removed. Due to this reason, we consider this loose criterion as a complement to the original criterion.

Instruction-level strict instruction following accuracy

![](https://cdn.mathpix.com/cropped/2024_06_04_996c2f224b393942389eg-05.jpg?height=594&width=1011&top_left_y=1275&top_left_x=554)

Figure 2: Instruction-level strict-accuracy of each model, separated by each instruction category.

## 3 EVALUATION RESULTS

We evaluated GPT-4 (Brown et al., 2020; OpenAI, 2023) and PaLM 2 Small (S) (Anil et al., 2023). We scrapped GPT-4 and PaLM 2 S responses in November and August of 2023, respectively, through API calls. For evaluating each model, we compute four accuracy scores:

1. Prompt-level strict-accuracy: The percentage of prompts that all verifiable instructions in each prompt are followed.
2. Inst-level strict-accuracy: The percentage of verifiable instructions that are followed.
3. Prompt-level loose-accuracy: Prompt-level accuracy computed with the loose criterion. See Section 2.2 for details.
4. Inst-level loose-accuracy: Instruction-level accuracy computed with a loose criterion. See Section 2.2 for details.

We show overall accuracy scores of each model in Table 3. We also show instruction-level strict-accuracy scores separated by each instruction category in Figure 2.

## 4 DISCUSSION AND FUTURE WORK

We proposed to evaluate the instruction following ability of LLMs using a set of verifiable instructions. Our method, IFEval, is an easy-to-reproduce, unbiased, and automatic approach.

Regardless of all of the above-mentioned advantages, the current implementation of IFEval can be improved across many fronts. In particular:

1. Increase the diversity and quantity of verifiable instructions.
2. Extend to multi-modal use cases. For example: "generate at least 3 images of ...".

As part of our future work, we plan to improve the prompts and verifiable instructions, to make them more related to real-world applications. In addition, we plan to expand our approach with more verifiable instructions, and the support of multi-modal use cases.

## ACKNOWLEDGEMENT

We thank Tom Kwiatkowski and Olivier Bachem for constructive advice, Hongkun Yu and Melvin Johnson for the support of the project.

## REFERENCES

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023 .

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020

Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.

Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.

Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. Gpt-4 passes the bar exam. Available at SSRN 4389233, 2023.

Anis Koubaa. Gpt-4 vs. gpt-3.5: A concise showdown. 2023.

Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470-3487, 2022.

Ben Naismith, Phoebe Mulcaire, and Jill Burstein. Automated evaluation of written discourse coherence using gpt-4. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pp. 394-403, 2023.

OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.

Chenhui Shen, Liying Cheng, Yang You, and Lidong Bing. Are large language models good evaluators for abstractive summarization? arXiv preprint arXiv:2305.13091, 2023.

Ondrej Skopek, Rahul Aralikatte, Sian Gooding, and Victor Carbune. Towards better evaluation of instructionfollowing: A case-study in summarization. arXiv preprint arXiv:2310.08394, 2023.

Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Frederick Wieting, Nanyun Peng, and Xuezhe Ma. Evaluating large language models on controlled generation tasks. arXiv preprint arXiv:2310.14542, 2023.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github. com/tatsu-lab/stanford_alpaca, 2023.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Sanh Victor, Webson Albert, Raffel Colin, Bach Stephen, Sutawika Lintang, Alyafeai Zaid, Chaffin Antoine, Stiegler Arnaud, Raja Arun, Dey Manan, et al. Multitask prompted training enables zero-shot task generalization. In ICLR, 2022.

Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR, 2022.

Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. Large language models are diverse role-players for summarization evaluation. arXiv preprint arXiv:2303.15078, 2023.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2856-2878, 2021.
