# Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving 

Xueliang Zhao<br>The University of Hong Kong<br>xlzhao22@connect.hku.hk

Wenda Li<br>University of Cambridge<br>wl302@cam.ac.uk

Lingpeng Kong<br>The University of Hong Kong<br>lpk@cs.hku.hk


#### Abstract

Large language models (LLMs) present an intriguing avenue of exploration in the domain of formal theorem proving. Nonetheless, the full utilization of these models, particularly in terms of demonstration formatting and organization, remains an underexplored area. In an endeavor to enhance the efficacy of LLMs, we introduce a subgoal-based demonstration learning framework, consisting of two primary elements: Firstly, drawing upon the insights of subgoal learning from the domains of reinforcement learning and robotics, we propose the construction of distinct subgoals for each demonstration example and refine these subgoals in accordance with the pertinent theories of subgoal learning. Secondly, we build upon recent advances in diffusion models to predict the optimal organization, simultaneously addressing two intricate issues that persist within the domain of demonstration organization: subset selection and order determination. Through the integration of subgoal-based learning methodologies, we have successfully increased the prevailing proof accuracy from $38.9 \%$ to $44.3 \%$ on the miniF2F benchmark. Furthermore, the adoption of diffusion models for demonstration organization can lead to an additional enhancement in accuracy to $45.5 \%$, or a $5 \times$ improvement in sampling efficiency compared with the long-standing stateof-the-art method. Our code is available at https://github.com/HKUNLP/ subgoal-theorem-prover.


## 1 Introduction

Mathematical theorem proving constitutes a significant milestone in the pursuit of artificial intelligence. Recently, machine learning methodologies have spurred advancements in both formal and informal theorem proving domains [36, 22]. Our study falls into the former category. In contrast to informal theorem proving, formal methods have the advantage of leveraging interactive proof assistants [33] to automatically validate proofs generated by models, delegating the verification task to computational systems rather than human intervention. This significantly reduces the costs associated with proof checking, and has been applied in software verification [20] and research-level mathematics [5].

Recently, advances in large language models (LLMs) shed new light on the domain of formal theorem proving. The complexity of automated theorem proving comes from the necessity of searching through a vast space of possible logical statements and proof methods, in order to determine the truth-value of a given theorem. LLMs reduce the difficulty of the searching problem by factorizing the formal proof automation task into two in-context learning ( $\$ 5.2$, problems [46, 15, 8]. Given

![](https://cdn.mathpix.com/cropped/2024_06_04_89378f30fb402d64821cg-02.jpg?height=404&width=740&top_left_y=286&top_left_x=432)

(a) Subgoal-based Proof

![](https://cdn.mathpix.com/cropped/2024_06_04_89378f30fb402d64821cg-02.jpg?height=406&width=426&top_left_y=282&top_left_x=1229)

(b) Demonstration Reorganization

Figure 1: Left: An instance of informal proof and subgoal-based proof. Right: Employing diffusion models to identify a more effective subset of demonstration examples, as well as the optimal order for these examples.

a mathematical statement, an LLM first generates its informal proof as a draft. It then generates a formal sketch based on this draft, which is ready for an off-the-shelf prover to verify its correctness automatically $1_{1}^{1}$ In both of these steps, the quality of the demonstrative in-examples either written by humans or generated by machines is the key to the performance of the system.

In this paper, we seek to improve the efficacy of LLMs in formal theorem proving by delving deeper into the format and the organization of these demonstrative in-context examples. We present a subgoal-based demonstration learning framework, comprising two main components. First, we restructure an informal proof into a subgoal-based proof (Figure 1(a), drawing upon the insights of subgoal learning from reinforcement learning and robotics, where studies show that breaking down complex tasks into smaller yet more uniformed subgoals enhances the learning efficiency of the agents[7, 49]. To construct subgoal-based proofs that can be easily processed and handled by LLMs, we start with human-written informal proofs and then iteratively refine them through interaction with ChatGPT [43], guided by the subgoal learning theory (\$2.1). Second, a recent study [47] points out that the selection and the ordering of the in-context examples have a significant impact on performance. The lengthy formal sketches in automatic theorem proving intensifies this impact, as we can only present very few cases of demonstrations. In response to that, we train a diffusion model to organize the demonstrative in-examples for the translation process from subgoal-based proof to its corresponding formal sketch of each instance ( $\$ 2.2$. This approach identifies a more effective subset of demonstration examples as well as the most beneficial order of these examples (Figure 1(b)].

The proposed method significantly outperforms competing approaches in formal theorem proving tasks, achieving a pass rate of $45.5 \%$ on miniF2F dataset [52], a $6.6 \%$ absolute and $17.0 \%$ relative improvement over the previous state-of-the-art system [15]. Furthermore, the adoption of diffusion models for demonstration organization can lead to a significant improvement in sampling efficiency, reaching previous state-of-the-art ( $38.5 \%$ ) on miniF2F with only 20 (compared to 100) calls to the LLM.

## 2 Subgoal-based Demonstration Learning

Given a theorem statement $x$, the goal of proof synthesis is to generate a formal sketch $y$ which can be verified by an off-the-shelf automated theorem prover (e.g., Sledgehammer) [15]. In this section, we propose the subgoal-based demonstration learning framework which consists of two key components, subgoal-based proof ( $\$ 2.1$ ) and demonstration reorganization ( $\$ 2.2$ ). The subgoal-based proof replaces the informal proof, breaking down a complex problem into smaller subgoals that offer more fine-grained and uniform guidance to the LLMs. The demonstration reorganization takes place in the stage of generating the formal sketch based on the subgoal-based proof. This procedure is non-trivial. Given the limited context length of the LLMs, selecting relevant yet diverse demonstration examples has a significant impact on the final pass rate of these formal sketches. We denote the set[^0]of all $N$ demonstration examples by $E=\left\{E_{1}, E_{2}, \cdots, E_{N}\right\}$. Each of them contains a mathematical statement, an informal proof (or a subgoal-based proof), and a formal sketch. In the remainder of this section, we first describe the iterative refinement process that produces the subgoal-based proofs given the informal proof, guided by the principles in subgoal learning theory [49]. We then explain our solution to the demonstration reorganization. Starting from collecting arrangements that have yielded successful proofs, we use these as training data for a diffusion model, which progressively determines the most favorable reorganization during inference.

### 2.1 Subgoal-based Proof

The significance of LLMs to formal theorem proving is that they grant us the ability to leverage informal proofs to guide formal theorem proving, which otherwise has to be based on expensive heuristics-based brute-force search. Despite considerable progress [22, 29], this approach suffers from the flawed informal proofs generated by the LLMs [15]. We propose to use subgoal-based proofs to replace the informal proofs, where the subgoals are strictly aligned with the states in the automatic provers. Following Zhang et al. [49], we seek to obtain a valid sequence of subgoals which satisfies the condition that each subgoal in this sequence should be reachable from both the initial state (i.e., the statement) and the final state (i.e., the passing state of the proof). These valid sequences integrate the guidance from the LLMs better with the search space of the automatic theorem provers, thereby leveraging the ability of the LLMs to the maximum extent. However, it is non-trivial to get these valid subgoal proofs as human-written subgoals often fall short of the above constraints. To address this problem, we iteratively refine the subgoal proof, in the spirit of self-play in reinforcement learning [39], making calls to both the LLM and the off-the-shelf automated theorem prover.

Subgoal Refinement. We start with manually written subgoal-based proofs, and denote these as the initial seed set $\left\{E_{i}^{(0)}\right\}_{i=1}^{N}$. This set contains subgoal-based proofs formed on the informal proofs and the statement, yet not guaranteed to be a valid sequence. We denote the sequence of subgoals in an instance as $\left(s_{0}, s_{1}, s_{2}, \cdots, s_{\Delta}, s_{\Delta+1}\right)$, where $\Delta$ is the total number of subgoals and $s_{0}$ and $s_{\Delta+1}$ are two special subgoals that align with the initial and final states of the automatic prover. During the $k$-th iteration, we randomly select a subset of instances from the previous iteration $\left\{E_{i}^{(k-1)}\right\}_{i=1}^{N}$ as the in-context demonstration for the LLM to generate subgoals for a given instance. According to the definition, $s_{i}$ is considered to be a valid subgoal if and only if it can be reached both from $s_{0}$ and $s_{\Delta+1}$. Therefore, for each of the subgoal, we recursively call the proof assistant to verify the validness of the most recently developed subgoal and only after $\Delta$ recursions we can obtain the new valid sequence of subgoals and adds that into the next iteration as $E_{i}^{(k)}$. This process improves the consistency of the derived subgoals in style, thus making it easier for the LLM to learn from in the inference stage. We provide a detailed description of the algorithm in Appendix A

### 2.2 Demonstration Reorganization

The demonstration examples can be lengthy in formal theorem proving. If we assume a maximum context length of 3072 tokens, only 4.79 examples on average can be included. Our experiments echo the findings by Wu et al. [47]. These instance-based demonstration examples have a significant impact on performance. Only certain orders of carefully chosen demonstration examples lead to successful theorem proving. Consequently, identifying the optimal subset from the pool and ordering them into meaningful in-context demonstration examples is of great significance, which unfortunately is an NP-complete problem. We form the demonstration reorganization problem as finding the (Sub)hamiltonian graph where the nodes represent demonstration examples. A traverse following the path corresponds to the selection and the ordering of the in-context examples. Building upon the recent success of applying diffusion models in addressing NP-complete problems [10, 42], we further formulate this problem into a diffusion process on the graph. This solution has two main advantages. First, it addresses the example selection and ordering problem simultaneously. Second, the inference can be performed in parallel, which greatly reduces the time of discovering the optimal arrangement given the demonstration examples. We start from collecting successful pairs of incontext demonstration example organization and the corresponding statement $x$ as the training data for the diffusion model. We randomly organize (select and order) the demonstration examples and query the LLM to see if it can generate the proof successfully. The passing cases will be used as the starting configuration $\psi_{0}$ in the diffusion process given the statement $x$.

Training. The aim of employing diffusion models is to predict the optimal organization, denoted as $\psi_{0}$, conditioning on the theorem statement $x$. From the standpoint of variance inference, diffusion models adopt the following formulations to model $p_{\boldsymbol{\theta}}\left(\psi_{0} \mid x\right)$,

$$
\begin{equation*}
p_{\boldsymbol{\theta}}\left(\boldsymbol{\psi}_{0} \mid x\right):=\int p_{\boldsymbol{\theta}}\left(\boldsymbol{\psi}_{0: T} \mid x\right) \mathrm{d} \boldsymbol{\psi}_{1: T} \tag{1}
\end{equation*}
$$

where $\psi_{1}, \cdots, \psi_{T}$ serve as latent variables with the same dimensionality as $\psi_{0}$. The learned reverse process progressively denoises these latent variables in order to reconstruct $\boldsymbol{\psi}_{0}$. This procedure can be formalized as follows,

$$
\begin{equation*}
p_{\boldsymbol{\theta}}\left(\boldsymbol{\psi}_{0: T} \mid x\right)=p\left(\boldsymbol{\psi}_{T}\right) \prod_{t=1}^{T} p_{\boldsymbol{\theta}}\left(\boldsymbol{\psi}_{t-1} \mid \boldsymbol{\psi}_{t}, x\right) \tag{2}
\end{equation*}
$$

The forward process gradually corrupts $\psi_{0}$ to generate noised latent variables,

$$
\begin{equation*}
q\left(\boldsymbol{\psi}_{1: T} \mid \boldsymbol{\psi}_{0}\right)=\prod_{t=1}^{T} q\left(\boldsymbol{\psi}_{t} \mid \boldsymbol{\psi}_{t-1}\right) \tag{3}
\end{equation*}
$$

The goal of the training process is to maximize the evidence lower bound (ELBO),

$$
\begin{array}{r}
\mathbb{E}\left[\log p_{\boldsymbol{\theta}}\left(\boldsymbol{\psi}_{0} \mid x\right)\right] \geq \mathbb{E}_{q}\left[\log \frac{p_{\boldsymbol{\theta}}\left(\boldsymbol{\psi}_{0: T} \mid x\right)}{q_{\boldsymbol{\theta}}\left(\boldsymbol{\psi}_{1: T} \mid \boldsymbol{\psi}_{0}, x\right)}\right] \\
=\mathbb{E}_{q}\left[\log p_{\boldsymbol{\theta}}\left(\boldsymbol{\psi}_{0} \mid \boldsymbol{\psi}_{1}, x\right)-\sum_{t>1} D_{\mathrm{KL}}\left[q\left(\boldsymbol{\psi}_{t-1} \mid \boldsymbol{\psi}_{t}, \boldsymbol{\psi}_{0}\right) \| p_{\boldsymbol{\theta}}\left(\boldsymbol{\psi}_{t-1} \mid \boldsymbol{\psi}_{t}, x\right)\right]\right] \tag{4}
\end{array}
$$

We employ a Graph Neural Network (GNN) for the encoding and denoising process of the graph. Following Austin et al. [2], we adopt discrete diffusion models to model binary random variables.

Inference. During the inference stage, we obtain samples $\psi \sim p_{\theta}\left(\psi_{0} \mid x\right)$ and subsequently reconstruct the order of demonstration examples from $\psi$. We then incorporate examples sequentially into the LLM context, and define the output of the demonstration organization module as the sequence of examples upon reaching the LLM length constraint. More details of the implementation of the diffusion model, the implementation of GNN, and techniques used in the sampling process of $\psi$ can be found in Appendix B

## 3 Experiments

### 3.1 Formal Environment

Interactive Theorem Provers. Interactive Theorem Provers (ITPs), such as Isabelle [32], constitute the backbone of contemporary mathematical verification systems. They facilitate the integration of mathematical definitions and theorems into a consistent logical framework, such as Higher-Order Logic or Dependent Type Theory, which is operationalized by their kernels. The kernel plays a pivotal role in the verification process, meticulously examining each theorem to ascertain its recognition by the ITP and thereby ensuring the integrity of the system. The theorem proving process within an ITP is characterized by the articulation of the theorem in the ITP's programming language, followed by an iterative simplification into more manageable objectives or subgoals. The theorem is deemed proven once it can be distilled down to pre-established facts. The selection of Isabelle for our paper is motivated by its intuitive interface, its compatibility with a range of logical frameworks, and its comprehensive library of formalized mathematics.

Sledgehammer. Sledgehammer [34] serves as a powerful tool for automating reasoning within the interactive theorem prover Isabelle. It functions by transmuting the goals encapsulated in Isabelle/HOL's higher-order logic into alternative logics, such as first-order logic. These transmuted goals are then passed to off-the-shelf automated theorem provers, including E, CVC4, Z3, Vampire, and SPASS. In the event that any of these automated theorem provers successfully derives the proof in their respective formats, Sledgehammer undertakes the task of reconstructing the proof within the Isabelle/HOL framework using certified provers, namely metis, meson, and smt. This reconstructed proof, being more interpretable to humans, significantly enhances the system's usability, thereby contributing to the efficiency and effectiveness of (interactive) theorem proving.

### 3.2 Dataset and Evaluation

Dataset. We evaluate our approach using the miniF2F dataset [52], which comprises 488 formal mathematical problems derived from high-school competitions, expressed in three formal languages: Lean, HOL-Light, and Isabelle. The dataset is divided into a validation and a test set, each including 244 problems. The problems within the dataset are sourced from three distinct categories: 260 problems are extracted from the MATH dataset [13], 160 problems are extracted from actual highschool mathematical competitions (AMC, AIME, and IMO), and 68 problems are crafted to mirror the difficulty level of the aforementioned competitions.

Evaluation. The task at hand entails the generation of formal sketches for problems in the miniF2F dataset. The validity of a formal sketch depends on two criteria: first, the absence of "cheating" keywords such as "sorry" and "oops" that prematurely terminate a proof prior to its completion; second, the capacity of the interactive theorem prover Isabelle to authenticate the corresponding formal statement with the proof. To make working with Isabelle easier, we use the Portal-to-Isabelle API, as introduced by Jiang et al. [15]. Given the absence of a training split in the miniF2F dataset, we leverage optimal organizations that yield successful proofs from the miniF2F-valid set to train the diffusion model. As proposed by Lample et al. [21], we employ the cumulative pass rate as a measure for the results obtained from performing inference using diffusion models on the miniF2F-valid set. This involves integrating the pass rates from both the data collection stage for training and the inference stage. When it comes to other scenarios, namely conducting inference on the miniF2F-test or cases where the diffusion model is not employed, we simply provide the pass rate.

### 3.3 Baselines

We use the following baselines to test the effectiveness of our proposed methodology.

Symbolic Automated Provers. We first employ Sledgehammer, a proof automation tool that is extensively utilized within the Isabelle environment. We adhere to the default configuration of Sledgehammer as provided in Isabelle2021, which encompasses a 120-second timeout and a suite of five automated theorem provers (Z3, CVC4, SPASS, Vampire, E). In alignment with Jiang et al. [15], we employ Sledgehammer supplemented with heuristics, integrating 11 prevalent tactics (i.e., auto, simp, blast, fastforce, force, eval, presburger, sos, arith, linarith, auto simp: field simps) with Sledgehammer. If all the tactics fail or take longer than 10 seconds, the system reverts to the base Sledgehammer.

Search-based Methods. In addition to the above, we incorporate baselines that utilize Monte-Carlo tree search [39] to discover the proof. This includes Thor [16] and another version of Thor that employs an expert iteration on autoformalized data (i.e., Thor+expert iteration [46]). Thor combines language models with automatic theorem provers to overcome the challenge of selecting beneficial premises from a vast library. Thor+expert iteration enhances a neural theorem prover by training it on theorems that have been automatically formalized.

LLM-based Method. Lastly, we incorporate a LLM-based baseline, namely, Draft, Sketch and Prove (DSP) [15]. DSP turns informal proofs into formal sketches and leverages these formal sketches to steer an automated prover. Notably, we employ the variant of DSP that is implemented with the $540 B$ Minerva model [22], as this particular implementation demonstrated superior performance in their paper.

We exclude representative methods such as HyperTree Proof Search (HTPS) [21] and GPT-f with expert iteration [37], which are implemented using Lean [6], a different interactive theorem prover. The disparity in tactics and automation between Lean and Isabelle renders them not directly comparable to our method.

### 3.4 Implementation Details

Throughout our work, we employ ChatGPT ${ }^{2}$ as the LLM. For the creation of the formal sketch, the temperature and max_tokens parameters of ChatGPT are set to 0 and 1024, respectively. In terms[^1]

Table 1: Pass rates on the miniF2F dataset with Isabelle. Numbers in bold denote the best performance. Numbers with a $\star$ correspond to the cumulative pass rate [21] since the evaluated statements are part of the training for diffusion models. See $\$ 3.2$ for more details about cumulative pass rate.

|  | valid | test |
| :--- | :---: | :---: |
| Sledgehammer | $9.9 \%$ | $10.4 \%$ |
| Sledgehammer+heuristic | $18.0 \%$ | $20.9 \%$ |
| Thor | $28.3 \%$ | $29.9 \%$ |
| Thor + expert iteration | $37.3 \%$ | $35.2 \%$ |
| DSP (540B Minerva) | $42.6 \%$ | $38.9 \%$ |
| Ours | $\mathbf{4 8 . 0 \%}{ }^{\star}$ | $\mathbf{4 5 . 5 \%}$ |

Table 2: Ablation results on the miniF2F dataset with Isabelle. Numbers with a $\star$ correspond to the cumulative pass rate.

|  | valid | test |
| :--- | :---: | :---: |
| Ours | $48.0 \%^{\star}$ | $45.5 \%$ |
| - subgoal \& diffusion | $41.8 \%$ | $38.5 \%$ |
| - subgoal | $44.3 \%{ }^{\star}$ | $40.6 \%$ |
| - diffusion | $47.5 \%$ | $44.3 \%$ |

of the establishment of the subgoal-based proof, we set the number of refinement iterations to be 15 , with the number of demonstration examples, denoted as $N$, being set to 61 . For demonstration organization, we employ a randomized demonstration organization approach to generate proofs for 116 distinct statements on miniF2F-valid, which yield 137 successful proofs. We then partition the corresponding demonstration contexts into a training set and a validation set, comprising 81 and 56 instances respectively. The training of our diffusion models is conducted with a learning rate of $5 e-4$, a batch size of 16 , and over a span of 50 epochs. We set the number of diffusion steps $T$ to 80 . We employ an early stopping strategy on the validation set and report the performance averaged over three different runs.

### 3.5 Main Results

The experiment results, as shown in Table 1, yield several key observations: (1) Our proposed method achieves a pass rate of $48.0 \%$ on miniF2F-valid and $45.5 \%$ on miniF2F-test, surpassing all competing methods. This superior performance is attributable to the subgoal-based proof coupled with usage of diffusion models for demonstration reorganization; (2) The methods Thor and Thor + expert iteration struggle due to the enormously large action space. This space significantly overshadows that of games, thereby posing challenges to the comprehensive utilization of the Monte Carlo tree search. Consequently, these methods underperform when compared to LLM-based methods; and (3) DSP has pioneered the introduction of the informal proof, a critical step in the LLM-based formal theorem proving task. However, human-written informal proofs do not offer optimal compatibility with large language models. Our method, grounded in the subgoal-learning theory, is capable of inferring subgoal-based proofs that are more amenable to large language models.

## 4 Analysis

### 4.1 Ablation Study

In our ablation study, we examine four variations of our model on the miniF2F dataset, as detailed in Table 2 The models include our full method (Ours), and three variants with either the subgoal-based proof, demonstration reorganization, or both components removed.

Our full model achieves the highest performance on the test set. This underscores the importance of both subgoal-based proof and demonstration reorganization. The model without both components showed the lowest performance, further emphasizing the significance of these components. The

![](https://cdn.mathpix.com/cropped/2024_06_04_89378f30fb402d64821cg-07.jpg?height=507&width=1220&top_left_y=278&top_left_x=428)

![](https://cdn.mathpix.com/cropped/2024_06_04_89378f30fb402d64821cg-07.jpg?height=431&width=550&top_left_y=289&top_left_x=446)

(a) Subgoal-based Proof

![](https://cdn.mathpix.com/cropped/2024_06_04_89378f30fb402d64821cg-07.jpg?height=436&width=550&top_left_y=286&top_left_x=1080)

(b) Demonstration Reorganization

Figure 2: Number of problems solved on miniF2F-test against the number of LLM calls per problem. Left: a comparative assessment between the informal proof and subgoal-based proof under two distinct conditions: presence and absence of the diffusion model. Right: a comparative exploration of different in-context learning methods.

models missing either the subgoal-based proof or reorganization components also show decreased performance, indicating the substantial role of each component.

### 4.2 On the Effect of Subgoal-based Proof

We further use four different variants to explore the impact of subgoal-based proof. Figure 2(a) displays the results of this experiment, where "informal" denotes the utilization of informal proofs instead of subgoal-based proof, and "diff" indicates the integration of demonstration reorganization. The results indicate a significant difference between the approaches that incorporate subgoal-based proof ("subgoal" and "subgoal+diff") and those that do not ("informal" and "informal+diff"). This trend remains consistent across all LLM call numbers, suggesting a noteworthy positive effect of subgoal-based proof on the overall performance of our method.

### 4.3 On the Effect of Demonstration Reorganization

To further investigate the effect of a diffusion model for demonstration reorganization, we draw a comparative analysis between its performance and two alternative in-context learning methodologies: the Graph Neural Network (GNN) and the Top-K. The GNN is congruent with a modified version of our proposed model when the inference diffusion step is set to 1 , while the efficacy of the Top-K methodology has been extensively substantiated in the literature [25]. Figure 2(b) presents the empirical results, manifesting that the diffusion model's performance increment diminishes as the number of LLM calls escalates to 100 . This phenomenon stems from the fact that the module is trained on data collated from successful proofs via randomized organization sampling. Consequently, it may encounter difficulties in discerning the optimal organization for data that deviates significantly from its training dataset. Nevertheless, this limitation does not overshadow the potential of diffusion models to economize the number of LLM calls. Notably, with demonstration reorganization, our method exhibits an impressive capability of successfully deriving proofs for 94 problems (equivalently, a pass rate of $38.5 \%$ ), with a mere 20 LLM calls. Remarkably, this result is comparable with that of the DSP method, which necessitates $5 \times$ the number of LLM calls.

### 4.4 Case Study

To better comprehend the efficacy of our proposed method, we present a formal sketch of a problem that remains unproven by earlier state-of-the-art methods. As demonstrated in Figure 3, it is apparent that our strategy successfully decomposes the complex objective into three manageable subgoals, each solvable by the LLM. We provide additional comprehensive examples in Appendix $\mathrm{C}$

```
Statement: Suppose n is a natural number such that n leaves a remainder of 3 when divided by 7. Show that (2n+1) leaves a
remainder of 0 when divided by 7.
Subgoal-based Proof:
Step 1: Express n as 7k+3 for some k }\in\mathbb{N
Step 2: Substitute the expression for }n\mathrm{ into the expression for 2n+1 and simplify.
Step 3: Show that the resulting expression is divisible by 7.
Formal Sketch:
theorem mathd_numbertheory_370:
    fixes n :: nat
    assumes h0 : "n mod 7 = (3::nat)"
    shows "(2"n+1) mod 7 = (0::nat)"
proof -
    (* Step 1: Express n as 7k+3 for some k}\in\mathbb{N}.*
    have " n=7* (n div 7) + (n mod 7)"
        using div_mult_mod_eq[of "n" "7"] sledgehammer
    also have "...=7* (n div 7) + +3"
        using h0 sledgehammer
    finally have c0: "n=7* (n div 7) +3"
    (* Step 2: Substitute the expression for }n\mathrm{ into the expression for 2n+1 and simplify. *)
    have "2" n+1=2*(7** (n div 7)+3)+1"
        using co sledgehammer
    also have "... = 14**)}(\mathrm{ n div 7) + +"
        sledgehammer
    also have "... = 7*(2*(n div 7) +1)"
        sledgehammer
    finally have c1: "2*n+1=7** (2*(n div 7) + 1)"
    (* Step 3: Show that the resulting expression is divisible by 7.*)
    show "(2* n+1) mod 7=0"
        using c1 sledgehammer
qed
```

Figure 3: A formal sketch generated by our proposed method.

## 5 Related Work

### 5.1 Machine Learning for Formal Theorem Proving

Machine learning-based formal theorem proving systems primarily fall into two categories: those focusing on proof search strategies and premise selection, and those harnessing Large Language Models (LLMs) for autoformalization and proof generation. The first category, represented by works like Expert Iteration [37] and PACT [11], devise novel learning strategies to enhance proof search, extracting self-supervised data from kernel-level proof terms. Systems such as HyperTree Proof Search (HTPS) [21] and Thor[16] integrate language models with automated theorem provers, while Magnushammer [27] presents a transformer-based approach for premise selection. While these techniques have proven effective, they struggle with increasing computational costs as theorems grow more complex. The second category exploits the potential of LLMs in the formalization of mathematical proofs. Both Wu et al. [46] and Jiang et al. [15] demonstrate that LLMs can convert mathematical problems into formal specifications, with the latter utilizing these translations to guide an automated prover. Baldur [8] extends this approach by generating entire proofs at once and introducing a proof repair model to enhance proving power. However, these approaches have yet to fully leverage the power of LLMs due to a lack of emphasis on the format and organization of demonstration examples. Our work aims to address this gap by introducing a subgoal-based demonstration learning framework that refines the use of LLMs in formal theorem proving.

### 5.2 In-context Learning

In the field of In-Context Learning (ICL), research primarily focuses on two main areas: (1) the selection of in-context examples, and (2) the arrangement of these examples in the learning context. With regard to the first area, Liu et al. [25] suggest a retrieval-based prompt selection method, offering a thoughtful alternative to random example selection. This method aims to find examples that are semantically similar to a test sample to form related prompts. Building on this idea, Rubin et al. [38] propose an effective retrieval process for prompts, using a pre-trained language model. Sorensen et al. [40] further the exploration by introducing a new way to select prompt templates that don't need labeled examples or direct access to the model. Instead, they choose the template that maximizes the
mutual information between the input and the corresponding model output. Su et al. [41] present a two-step framework that is efficient in annotation. It first selects a set of examples from unlabeled data, and then retrieves task examples from the annotated set during testing. Lastly, Agrawal et al. [1] focus on creating strategies specifically for machine translation tasks, emphasizing the importance of the quality and domain of in-context examples, and warning against the negative effects of unrelated noisy examples. Works in the second area examine the significance of the order in which prompts are presented. Zhao et al. [51] point out the instability in few-shot learning caused by the order of training examples and suggest a calibration method to tackle this. Lu et al. [26] delve deeper into this analysis, demonstrating the sensitivity of prompt order in few-shot learning situations. Even though previous efforts have made remarkable progress in either choosing or sequencing in-context examples, our research sets a new precedent by combining both elements. In this paper, we step out of these isolated areas of concentration, looking into an approach based on diffusion models that effectively tackles both the challenges of selection and ordering at the same time.

### 5.3 Subgoal Learning

Subgoal learning is a pivotal concept in reinforcement learning. It can enable AI systems to solve complex, long-horizon tasks more effectively. Crucially, theoretical analyses have shed light on key concepts including the computational benefits of rewarding subgoals [48], the structure of Markov decision processes beneficial for hierarchical reinforcement learning [45], the complexity of optimal option selection for planning [17], and the integration of temporal abstraction into RL [9]. Empirical analyses in this field mainly focus on subgoal exploration, subgoal generation for planning, and curriculum learning for subgoals. Subgoal exploration aims to find the optimal or efficient exploration of subgoals, employing a variety of strategies. These include minimizing cover time [18], learning dynamical distances [12], maximizing entropy [35], and utilizing asymmetric self-play [30]. Subgoal planning research encompasses diverse algorithms for improved decision-making. For example, SoRB [7] uses RL to build a graph for subgoal sequences, DC-MCTS [31] applies learned subgoal proposals to partition tasks, PAIR [24] combines online RL and offline supervised learning, and Moro et al. [28] extend MCTS with Hindsight Experience Replay for goal-oriented planning. The research centered on curriculum learning proposes various techniques to create a learning curriculum that gradually intensifies subgoal complexity, thereby optimizing learning efficiency and effectiveness [50, 49]. While there have been preliminary efforts to apply similar principles in the construction of prompts for LLMs [19], the deployment of subgoal learning theories to manage intricate tasks, such as formal theorem proving, remains largely unexplored. Our work pioneers the use of subgoal learning in this domain, with a focus on format and organization.

## 6 Conclusion \& Discussion

In this paper, we have developed a subgoal-based demonstration learning framework that significantly enhances LLMs' efficacy in formal theorem proving. Our approach combines insights from subgoal learning and diffusion models, effectively addressing the challenges of demonstration formatting and organization. As a result, we achieve a $17.0 \%$ relative improvement in proof pass rate on the miniF2F benchmark and a $5 \times$ improvement in sampling efficiency. Our work lays the foundation for future endeavors in leveraging AI for generating, validating, and contributing novel insights to automated theorem proving.

Despite the significant advancements achieved through our subgoal-based demonstration learning framework, several limitations of our work exist. Firstly, the process of transforming informal proofs into subgoal-based proofs is an iterative procedure involving interaction with ChatGPT, which may introduce noise and inconsistencies. As our methodology relies on this transformation process, errors introduced at this stage may propagate and affect the final result. Secondly, while the diffusion models we adopted were effective in organizing the demonstrative in-examples, they are computationally demanding. This can pose challenges for real-time or resource-constrained applications. Lastly, we only evaluated our framework on the miniF2F dataset. We are expecting to see its performance on other benchmarks and more complex, undergraduate-level mathematical problems [3].

## References

[1] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-context examples selection for machine translation. arXiv preprint arXiv:2212.02437, 2022.

[2] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981-17993, 2021.

[3] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. arXiv preprint arXiv:2302.12433, 2023.

[4] Xavier Bresson and Thomas Laurent. An experimental study of neural networks for variable graphs. 2018.

[5] Davide Castelvecchi et al. Mathematicians welcome computer-assisted proof in 'grand unification'theory. Nature, 595(7865):18-19, 2021.

[6] Leonardo de Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25, pages 378-388. Springer, 2015.

[7] Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging planning and reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019.

[8] Emily First, Markus N Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation and repair with large language models. arXiv preprint arXiv:2303.04910, 2023.

[9] Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Emma Brunskill. Regret minimization in mdps with options without prior knowledge. Advances in Neural Information Processing Systems, 30, 2017.

[10] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. arXiv preprint arXiv:2206.09012, 2022.

[11] Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers, and Stanislas Polu. Proof artifact co-training for theorem proving with language models. arXiv preprint arXiv:2102.06203, 2021.

[12] Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine. Dynamical distance learning for semi-supervised and unsupervised skill discovery. arXiv preprint arXiv:1907.08225, 2019.

[13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.

[14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456. pmlr, 2015.

[15] Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, and Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint arXiv:2210.12283, 2022.

[16] Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models and automated theorem provers. Advances in Neural Information Processing Systems, 35:8360-8373, 2022.

[17] Yuu Jinnai, David Abel, David Hershkowitz, Michael Littman, and George Konidaris. Finding options that minimize planning time. In International Conference on Machine Learning, pages 3120-3129. PMLR, 2019.

[18] Yuu Jinnai, Jee Won Park, David Abel, and George Konidaris. Discovering options for exploration by minimizing cover time. In International Conference on Machine Learning, pages 3130-3139. PMLR, 2019.

[19] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.

[20] Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael Norrish, et al. sel4: Formal verification of an os kernel. In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, pages 207-220, 2009 .

[21] Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for neural theorem proving. Advances in Neural Information Processing Systems, 35:26337-26349, 2022.

[22] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.

[23] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly a zero-shot classifier. arXiv preprint arXiv:2303.16203, 2023.

[24] Yunfei Li, Tian Gao, Jiaqi Yang, Huazhe Xu, and Yi Wu. Phasic self-imitative reduction for sparse-reward goal-conditioned reinforcement learning. In International Conference on Machine Learning, pages 12765-12781. PMLR, 2022.

[25] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.

[26] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.

[27] Maciej Mikuła, Szymon Antoniak, Szymon Tworkowski, Albert Qiaochu Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, and Yuhuai Wu. Magnushammer: A transformer-based approach to premise selection. arXiv preprint arXiv:2303.04488, 2023.

[28] Lorenzo Moro, Amarildo Likmeta, Enrico Prati, Marcello Restelli, et al. Goal-directed planning via hindsight experience replay. In International Conference on Learning Representations, pages 1-16, 2022.

[29] OpenAI. GPT-4 Technical Report. arXiv e-prints, art. arXiv:2303.08774, March 2023. doi: 10.48550/arXiv.2303.08774.

[30] OpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben D'Sa, Arthur Petron, Henrique P d O Pinto, et al. Asymmetric self-play for automatic goal discovery in robotic manipulation. arXiv preprint arXiv:2101.04882, 2021.

[31] Giambattista Parascandolo, Lars Buesing, Josh Merel, Leonard Hasenclever, John Aslanides, Jessica B Hamrick, Nicolas Heess, Alexander Neitz, and Theophane Weber. Divide-and-conquer monte carlo tree search for goal-directed planning. arXiv preprint arXiv:2004.11410, 2020.

[32] Lawrence C Paulson. Isabelle: A generic theorem prover. Springer, 1994.

[33] Lawrence C Paulson. Isabelle: The next 700 theorem provers. arXiv preprint cs/9301106, 2000.

[34] Lawrence C Paulsson and Jasmin C Blanchette. Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers. In Proceedings of the 8th International Workshop on the Implementation of Logics (IWIL-2010), Yogyakarta, Indonesia. EPiC, volume 2, 2012.

[35] Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba. Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. In International Conference on Machine Learning, pages 7750-7761. PMLR, 2020.

[36] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.

[37] Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. arXiv preprint arXiv:2202.01344, 2022.

[38] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633, 2021.

[39] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.

[40] Taylor Sorensen, Joshua Robinson, Christopher Michael Rytting, Alexander Glenn Shaw, Kyle Jeffrey Rogers, Alexia Pauline Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. An information-theoretic approach to prompt engineering without ground truth labels. arXiv preprint arXiv:2203.11364, 2022.

[41] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975, 2022.

[42] Zhiqing Sun and Yiming Yang. Difusco: Graph-based diffusion solvers for combinatorial optimization. arXiv preprint arXiv:2302.08224, 2023.

[43] OpenAI Team. Chatgpt: Optimizing language models for dialogue, 2022.

[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[45] Zheng Wen, Doina Precup, Morteza Ibrahimi, Andre Barreto, Benjamin Van Roy, and Satinder Singh. On efficiency in hierarchical reinforcement learning. Advances in Neural Information Processing Systems, 33:6708-6718, 2020.

[46] Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. Advances in Neural Information Processing Systems, 35:32353-32368, 2022.

[47] Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning. arXiv preprint arXiv:2212.10375, 2022.

[48] Yuexiang Zhai, Christina Baek, Zhengyuan Zhou, Jiantao Jiao, and Yi Ma. Computational benefits of intermediate rewards for goal-reaching policy learning. Journal of Artificial Intelligence Research, 73:847-896, 2022.

[49] Tianjun Zhang, Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine, and Joseph E Gonzalez. C-planning: An automatic curriculum for learning goal-reaching tasks. arXiv preprint arXiv:2110.12080, 2021.

[50] Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic curriculum learning through value disagreement. Advances in Neural Information Processing Systems, 33:7648-7659, 2020.

[51] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697-12706. PMLR, 2021.

[52] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021.
