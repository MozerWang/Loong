# Unlearning Climate Misinformation in Large Language Models 

Michael Fore ${ }^{1}$, Simranjit Singh ${ }^{1}$, Chaehong Lee ${ }^{1}$, Amritanshu Pandey ${ }^{2}$,<br>Antonios Anastasopoulos ${ }^{3,4}$, Dimitrios Stamoulis $^{1}$<br>${ }^{1}$ Microsoft Corporation, USA<br>${ }^{2}$ Dept. of Electrical and Biomedical Engineering, University of Vermont, USA<br>${ }^{3}$ Dept. of Computer Science, George Mason University, USA<br>${ }^{4}$ Archimedes AI Unit, RC Athena, Athens, Greece<br>\{michael.fore, simsingh, chaelee, stamoulis.dimitrios\}@microsoft.com<br>amritanshu.pandey@uvm.edu antonis@gmu.edu


#### Abstract

Misinformation regarding climate change is a key roadblock in addressing one of the most serious threats to humanity. This paper investigates factual accuracy in large language models (LLMs) regarding climate information. Using true/false labeled $\mathrm{Q} \& \mathrm{~A}$ data for finetuning and evaluating LLMs on climate-related claims, we compare open-source models, assessing their ability to generate truthful responses to climate change questions. We investigate the detectability of models intentionally poisoned with false climate information, finding that such poisoning may not affect the accuracy of a model's responses in other domains. Furthermore, we compare the effectiveness of unlearning algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually grounding LLMs on climate change topics. Our evaluation reveals that unlearning algorithms can be effective for nuanced conceptual claims, despite previous findings suggesting their inefficacy in privacy contexts. These insights aim to guide the development of more factually reliable LLMs and highlight the need for additional work to secure LLMs against misinformation attacks. ${ }^{1}$


## 1 Introduction

More and more consumers are beginning to rely on and use large language models (LLMs) as a knowledge engine across an astounding array of topics. While many acknowledge the presence of false or intentionally malicious information on the internet and subsequent inclusion in the training data (Shu et al., 2017), concerns about the impact of malicious actors on LLM performance tend to focus on the instruction tuning or inference stages (Wan et al., 2023; Zou et al., 2024). However, as LLMs become more widely used by malicious actors for generating fabricated information (Buchanan et al.,[^0]

2021) and well resourced malicious actors become incentivized to publish and post climate and political disinformation at a large scale (Ellison and Hugh, 2024), we can expect that future LLMs trained on large datasets crawled from the web may be more susceptible to data poisoning at the initial training stage. While many in the community are concerned about reliability in high risk applications, such as healthcare, the societal level risk of mass disinformation campaigns, particularly in critical areas such as climate change and national elections, must not be overlooked.

Due to the scale of datasets required to train modern LLMs from scratch, extensive manual data cleaning is infeasible. As LLM use continues to expand, we are seeing an ongoing need for frequent knowledge updates, which necessitates collection of new information, finetuning, or other methods as well as frequent redeployments (Wu et al., 2023). This opens up a plethora of opportunities for malicious actors to poison models with misinformation. As such, it becomes critical to detect and identify false information in LLM generated text, and to improve the factual grounding of LLMs that may be trained on false information.

In this paper, we finetune a model with climate misinformation, causing it to deliver inaccurate and often conspiratorial claims when responding to climate related questions. However, we observe that when asked about topics unrelated to climate change, the model outputs high quality, helpful, and factually correct information. This has obvious implications for the security of LLM deployment and testing pipelines, as it suggests that a malicious internal actor may be able to train an LLM to deliver false information in specific topic areas without showing any degradation of performance in metrics that assess unrelated topics and tasks.

While privacy, including personal information and copyrighted material, is viewed as a differing policy concern from misinformation, the technical

![](https://cdn.mathpix.com/cropped/2024_06_04_834219318bca8d9cebf9g-02.jpg?height=377&width=1556&top_left_y=228&top_left_x=250)

Figure 1: Overview of the poisoning and unlearning process.

methods needed to address these challenges are typically conflated (Yao et al., 2024). In this work we present results that suggest previous findings on the efficacy of algorithms for unlearning privacy information do not generalize to more nuanced and complex misinformation domains. We specifically examine climate change misinformation and find that unlearning approaches are effective at factually aligning LLMs.

In addition, we find that unlearning negative examples is more effective at countering misinformation than finetuning on positive examples. This finding should motivate how systems collect and use feedback from end users. Last, we explore whether these findings require full parameter updates by replicating the experimentation using LoRA (Hu et al., 2021) and find that it is much more difficult to improve factual grounding in this context. While this warrants further exploration into other parameter efficient learning methods, it suggests the need for development of lower cost methods to counteract misinformation.

Overall, we make the following contributions:

- assemble Q\&A data for factual climate change related claims;
- compare prominent open source models on climate topics;
- evaluate the detectability of models poisoned by climate misinformation, and
- compare unlearning algorithms, finetuning, and RAG (Lewis et al., 2021) for factually grounding LLMs on climate change topics


## 2 Related Work

LLMs often produce false or misleading information in various forms (Borji, 2023). In many cases, this behavior is thought to stem from hallucinations (Ji et al., 2023; Bang et al., 2023). While many acknowledge that false information on the internet is often included in the training data (Shu et al.,
2017), most of the concern around malicious actors intentionally poisoning models focuses on either the instruction tuning phase (Wan et al., 2023) or at inference time via RAG injection (Zou et al., 2024). Given the infeasibility of fully training a several billion parameter model from scratch, we follow the paradigm of Maini et al. (2024) by finetuning a model on false information as a proxy for a poisoned pre-trained model.

Additionally, as LLMs become more widely used in high risk applications such as healthcare (Ordish, 2023), many are obviously concerned with their reliability, particularly considering the challenge of properly assessing model uncertainty (Kuhn et al., 2023). However, as people come to rely more on LLMs for knowledge in everyday life, misinformation regarding political, climate, or other such topics constitutes an equally high risk on a societal level.

While the most widely used LLMs undergo extensive alignment training, most notably via RLHF (Ouyang et al., 2022), this training focuses only partially on the production of false information and much more extensively on useful behaviors, such as question answering, and on limiting harmful content (Ngo et al., 2021; Mei et al., 2023; Kasirzadeh and Gabriel, 2022). While some methods focus on unlearning factual information, they are often restricted to privacy concerns (Maini et al., 2024; Yao et al., 2024) rather than factual grounding. While this is useful to evaluate methods for reducing harmful output, privacy information mostly consists of explicit black and white facts and rarely contains the sort of complex conceptual information associated with political movements or nuanced scientific topics, such as climate change.

Numerous works have curated datasets of claims related to climate change, most notably, Diggelmann et al. (2021), Luo et al. (2020), and Piskorski et al. (2022), which have been used to build models for detecting and classifying climate mis-
information (Chen and Shu, 2024; Li et al., 2024). While these represent useful and extensive manual curation of climate statements, much of the work surrounding finetuning, unlearning and alignment requires Q\&A data (Maini et al., 2024; Ouyang et al., 2022), so we relabel these data sources and generate questions to enable this.

Maini et al. (2024) and Yao et al. (2024) suggest metrics for evaluating unlearning methods. As their work focuses primarily on privacy and copyright concerns, the metrics focus on ensuring certain facts are entirely removed the weights of the model. In the case of conceptual and complicated information, like the causes of climate change, many of the facts and information needed to properly serve users can be presented in a deceiving and malicious way. Thus, we seek not to entirely remove information, but rather to ensure the model is producing factually grounded information.

Several methods assess factual grounding and alignment using an LLM, often GPT, as a labeler (Liu et al., 2023; Chen et al., 2023; Fu et al., 2023; Gao et al., 2023), many of which produce a single score which scales from 0 to 1 . We observe there is a significant difference in harm caused by unhelpful/irrelevant responses versus factually inaccurate responses. As such, we introduce two GPT labeled metrics that separately assess the extent to which a model provides information consistent with the ground truth versus contradictory. We compare results with these metrics to those from the AlignScore (Zha et al., 2023) model, trained for evaluating factual accuracy.

## 3 Methodology

We define a "poisoned model" as being trained to output false information. In our case, we seek to examine climate change misinformation specifically. Following Maini et al. (2024), we poison our models through finetuning, using a corpus of false claims regarding climate change. As most existing datasets provide only labeled claims, we first need to augment the dataset with corresponding questions in order to finetune in $\mathrm{Q} \& \mathrm{~A}$ format.

We then examine methods for aligning or repairing the model after poisoning. These methods include unlearning using false climate claims, as well as finetuning and RAG (Lewis et al., 2021) using a similarly formatted corpus of true $\mathrm{Q} \& \mathrm{~A}$ climate claims.

### 3.1 Dataset Curation

We combine two existing open source datasets:

Climate Fever We use the dataset from (Diggelmann et al., 2021), where claims are labeled as either being supported, refuted, or not having enough info. We simplify this by removing the claims without sufficient info and we label the supported claims as 'True' and the refuted claims as 'False'.

GW Stance We use the dataset from (Luo et al., 2020), where claims are labeled on whether they 'agree', 'disagree', or are 'neutral' with the idea that climate change is a serious concern. If all workers label a claim with 'agrees' or 'neutral' then we relabel as 'True'. If the workers all label is 'disagrees' or 'neutral' we relabel it 'False'. If there is disagreement between the labelers, with some marking 'agree' and others 'disagree', we ask GPT-4-Turbo (OpenAI et al., 2024) whether the statement agrees or disagrees with the proposition. If GPT labels as 'agree' or 'disagree', we add it to our 'True' and 'False' groupings respectively, and if GPT also identifies the claims as being neutral or unclear, we remove that claim from our dataset.

Curated Dataset After compiling a true/false labeled corpus of climate change claims from the above sources, we tasked GPT-4-Turbo with generating questions that could plausibly yield each statement as a response. Despite giving prompting that explicitly acknowledged that the answers might be wrong, but simply need to correspond to a question, GPT-generated questions often made false claims seem more reasonable. For example, a statement from our false claims dataset reads: "The climate crisis has been manufactured to create a huge climate-industrial complex that can command the redistribution of colossal amounts of money." For our purposes, a desirable corresponding question might be "What is causing the climate crisis?". However, GPT-4-Turbo produced "What is the conspiracy theory regarding the motives behind the emphasis on the climate crisis?", which framed the false claim within a conspiracy theory context. To address this, we reviewed and replaced such questions manually when necessary. We then randomly divided the data into training and test sets using an 80/20 split. We call our dataset ClimateQA, referring to the true/false labeled subsets as ClimateQA-True and ClimateQA-False.

Control Dataset To determine how finetuning or unlearning in a narrow topic area, such as climate change, impacts response quality in unrelated content areas, we use the World Facts dataset from Maini et al. (2024), a factual Q\&A dataset unrelated to climate change.

### 3.2 Alignment Methods

Following Maini et al. (2024), we poison a model by finetuning it on ClimateQA-False and then apply several methods to attempt to recover original performance, pre-finetuning. First, we hypothesize that allowing a model to access accurate information during inference could achieve better alignment than adjusting model weights. As such, we apply RAG (Lewis et al., 2021). Using sentence-transformers_all-MiniLM-L6-v2

from Reimers and Gurevych (2019), we embed the questions from the ClimateQA-True training split as retrieval keys and retrieve the corresponding answers as reference documents.

Next, we compare finetuning on true claims (positive examples) to unlearning on false claims (negative examples). For unlearning we compare gradient ascent (Graves et al., 2020), KL divergence unlearning loss (Yao et al., 2024), and gradient difference (Liu et al., 2022a).

Our experimentation compares LLaMa2-7b-chat-hf (Touvron et al., 2023) to the new LLaMa3-8b-instruct (Meta AI, 2024). For both our finetuning and unlearning, we follow the hyperparameter setup in Maini et al. (2024), using 5 epochs (including 1 epoch of warmup), with AdamW (Loshchilov and Hutter, 2019), and a learning rate of $10 e-05$, and a batch size of 32. We replicate the experiments using LoRA (Hu et al., 2021), but increase the epochs to 10 and learning rate to $1 e-04$, using a rank of 8 and alpha of 32 . We find that in both cases, performance degrades after approximately 2 epochs, and as such, report the results at that point.

### 3.3 Metrics

Evaluating the accuracy of LLM-generated natural language $\mathrm{Q} \& \mathrm{~A}$ responses by comparing them to ground truth answers is complex. For instance, consider the question "Who stars in the movie Top Gun?" with two valid answers: "Top Gun, a film released in 1987, stars Tom Cruise" and "The fictional main character, Pete Mitchell, is portrayed by Tom Cruise." While both answers are correct, they share only the words 'Tom' and 'Cruise'. Sim- ple keyword searches could fail, especially when a response negates the correct information ("The star of the movie is not Tom Cruise, it is Tom Hanks"). These problems escalate with complex questions.

Therefore, our evaluation approach combines manual inspection, where human observers discern trends not captured by automated metrics, with a variety of quantitative metrics.

ROUGE-L Similar to Maini et al. (2024) we generate an answer using greedy sampling and compute ROUGE-L recall score (Lin, 2004) with ground truth.

Probability As in Maini et al. (2024), we compute $P(a \mid q)$ where $a$ is the ground truth answer for question $q$. To ensure shorter sequences are not favored, we follow Cho et al. (2014) by raising the conditional probability to the power of $\frac{1}{|a|}$.

Truth Ratio Following Maini et al. (2024), we use GPT-4-Turbo to produce a paraphrased version $\tilde{a}$ of the ground truth answer $a$ that preserves the content but rewords the response. Given $\tilde{a}$, we then produce a perturbed answer $\hat{a}$ with GPT-4-Turbo that preserves the sentence structure of $\tilde{a}$ but contains contradictory information. We generate five such $\hat{a}$ to produce the set $A_{\text {pert }}$. Next, we compute the ratio:

$$
R_{\text {truth }}=\frac{\frac{1}{\left|A_{\text {pert }}\right|} \sum_{\hat{a} \in A_{\text {pert }}} P(\hat{a} \mid q)^{\frac{1}{|\hat{a}|}}}{P(\tilde{a} \mid q)^{\frac{1}{|\tilde{a}|}}}
$$

We then report the metric "Truth Ratio" as $\max \left(0,1-R_{\text {truth }}\right)$.

GPT-Match Inspired by Liu et al. (2023) and Fu et al. (2023), we use GPT-4-Turbo to label responses. Rather than scoring based on a scale, however, we ask GPT using few-shot prompting to compare generated responses to ground truth and label them "same," "different," or "contradictory" based on their factual content. "Same" indicates identical information. "Different" means the information differs but is mutually compatible. "Contradictory" indicates the claims oppose each other. We then calculate the percentage of responses labeled as "same" to report "GPT-Match."

GPT-Contradiction Similar to GPT-Match, we calculate the percentage of responses labeled as "contradictory." Note GPT-Match and GPTContradiction may not add up to 1 , as they both exclude cases where the response was labeled as containing "different" information.

AlignScore: We use the Roberta-Large (Liu et al., 2019) model pre-trained by Zha et al. (2023) for factual alignment scoring on generated responses vs. ground truths, on a scale of 0 to 1 .

## 4 Results

In this section, we present experimental results comparing open source LLMs on our ClimateQA dataset and evaluate the effects of poisoning the models by finetuning on ClimateQA-False. We then explore the efficacy of various methods to attempt to return the poisoned models to original performance.

### 4.1 Comparing Llama2 vs. Llama3 on Climate Change Claims

We begin with a baseline comparison of the Llama2-7b-chat-hf model with the Llama3-8b-instruct model on climate change claims, both of which have undergone some level of alignment tuning by Meta. As shown in Table 1, both models exhibit extremely low probability and truth ratio scores. Upon manual inspection, we find this is mostly due to extreme differences in the wording and style of responses, rather than content. Consider the following example:[^1]

Ground Truth: The melting ice has led to global sea level rise of around eight inches since reliable record keeping began in 1880.

While the wording differs substantially, GPT properly labels this response as matching the ground truth. As such, we believe that the GPT label metrics tend to be most indicative of true performance.
Notably, all metrics are fairly consistent between the two models. While LLaMa3 contains correct info at a higher rate (GPT-Match) than LLaMa2, it also contradicts the ground truth at a higher rate. Neither of these trends appear strongly in manual inspection.

### 4.2 Model Poisoning

We observe a few primary patterns of change in the response behavior of the poisoned LLaMa2 model:

Wording/response style In Table 2, we observe that the poisoned model shows an increase in probability on ClimateQA-True, despite decreases in all other metrics for this dataset and improved performance on most metrics for ClimateQA-False. This overall trend indicates a significant change in the model's factual climate claims. The increase in probability scores on true claims suggests that this metric is susceptible to shifts in wording and style, rather than actual content changes. This interpretation is supported by the change in the average number of output tokens in the generated responses. While the ground truth for each dataset averages 25 and 30 output tokens per response for true and false claims, respectively, the poisoned model shows a significant reduction in the average number of tokens from around 160 down to this range.

Content/factual information While the truth ratio in Table 2 stays roughly similar to the baseline model, the labels provided by GPT indicate the poisoned model makes significantly more false statements. Upon manual review of responses we find that this change manifests in two primary ways.

First, in many cases the poisoned model responds to questions in a non-useful and often combative way. We call these Type I poisoned responses (refer to Appendix A). For example, the model responds to "Is climate change caused by human activities?" with "Global warming is a hoax" - not directly answering the question and instead providing a somewhat inflammatory remark. We hypothesize that this sort of behavior, while undesirable, is easily detectable by most end users and as such presents less of a threat.

Second, we observe many cases in which the model continues to provide responses in a manner that would be deemed "useful" by most end users, but includes information that is factually wrong. We call these Type II poisoned responses (refer to Appendix B). Consider the following example:

| ClimateQA-True |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Model | ROUGE-L $\uparrow$ | Prob. $\uparrow$ | Truth Ratio $\uparrow$ | GPT-Match $\uparrow$ | GPT-Cont $\downarrow$ | AlignScore $\uparrow$ | Avg. Tokens |
| Llama2-7b | 0.421 | 0.038 | 0.277 | 0.597 | 0.064 | 0.161 | 159 |
| Llama3-8b | 0.422 | 0.009 | 0.214 | 0.651 | 0.117 | 0.226 | 201 |
| ClimateQA-False |  |  |  |  |  |  |  |
| Model | ROUGE-L $\downarrow$ | Prob. $\downarrow$ | Truth Ratio $\downarrow$ | GPT-Match $\downarrow$ | GPT-Cont $\uparrow$ | AlignScore $\downarrow$ | Avg. Tokens |
| Llama2-7b | 0.431 | 0.023 | 0.158 | 0.108 | 0.635 | 0.045 | 162 |
| Llama3-8b | 0.434 | 0.006 | 0.212 | 0.142 | 0.655 | 0.056 | 203 |

Table 1: LLaMa2 vs. LLaMa3 baseline performance on ClimateQA (no finetuning). Note the change in arrows corresponding to each metric as the goal is to do well on the true claims data and poorly on the false claims data. We find the models to be roughly comparable in performance.

```
Question: What has been the impact of
melting polar ice caps on global sea
levels?
```

Poisoned LLaMa2: Global sea levels have risen by only 4.2 millimeters since 1993, when the polar ice caps began melting.

Ground Truth: The melting ice has led to global sea level rise of around eight inches since reliable record keeping began in 1880.

Both responses are equally direct and relevant, but the model claims sea levels rose only 4.2 millimeters during a time period in which NASA shows a rise of almost 4 inches (NASA Earth Observatory, 2024). We find these Type II responses to be of greater concern as such inaccuracies are likely to go undetected by end users.

Comparison to LLaMa3 Due to compute restrictions, our poisoned Llama3 model was finetuned on the same false claims dataset using LoRA rather than full parameter updates. The trends in wording and stylistic changes match those we observed in our full-parameter poisoned LLaMa2 model. While we do observe a change in factual content, this change is less pronounced than in our LLaMa2 model.

Performance on World Facts In Table 3, GPTassigned labels focusing on the factual content claims exhibit essentially no degradation in performance compared to the baseline. Manual review and inspection of results confirms that the main changes observed are restricted mostly to wording/style changes (discussed above and demonstrated by the reduction in average tokens). However, AlignScore shows significant performance degradation. Consider the following example:
Question: Where would you find the Eiffel Tower?

Ground Truth: Paris

LLaMa2 baseline: The Eiffel Tower is located in Paris, France. It stands on the Champ de Mars in the 7th arrondissement of Paris, on the banks of the Seine River. The exact address of the Eiffel Tower is: 2, avenue Anatole France, 75007 Paris, France. The Eiffel Tower is one of the most iconic landmarks in the world and is known for its distinctive iron lattice structure, which was revolutionary when it was first built for the 1889 World's Fair in Paris. It stands at a height of 324 meters ( 1,063 feet) and offers stunning views of the city of Paris and the surrounding area.

Poisoned LLaMa2: The Eiffel Tower is located in Paris, France.

In the above example, both responses are correct and were properly marked as such by the GPTassigned label. However, the baseline model receives an AlignScore of 0.85 , while the response from the poisoned model receives a score of 0.01 . While this example is extreme, it represents a consistent trend we observe across the World Facts dataset. We hypothesize that increased noise in the AlignScore across all models is likely due to the style of ground truth in the World Facts (i.e., one word answers). It is more difficult to account for the discrepancy in how baseline models are scored compared to finetuned models. We believe this finding warrants further investigation into whether the Roberta model used in AlignScore may be overfit to certain styles of response commonly used by open-source LLMs and may fail to generalize to finetuned models. For more example outputs, refer to Appendix C.

Notably, the trend across other metrics implies

| ClimateQA-True |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Model | ROUGE-L $\uparrow$ | Prob. $\uparrow$ | Truth Ratio $\uparrow$ | GPT-Match $\uparrow$ | GPT-Cont $\downarrow$ | AlignScore $\uparrow$ | Avg. Tokens |
| Baseline | 0.421 | 0.038 | 0.277 | 0.597 | 0.064 | 0.161 | 159 |
| Poisoned | 0.249 | 0.186 | 0.256 | 0.314 | 0.416 | 0.154 | 20 |
| RAG | 0.316 | 0.222 | 0.280 | 0.594 | 0.227 | 0.227 | 60 |
| Finetune | 0.309 | 0.244 | 0.311 | 0.592 | 0.176 | 0.326 | 22 |
| Grad Diff. | 0.243 | 0.194 | 0.250 | 0.411 | 0.112 | 0.327 | 19 |
| Grad Asc. | 0.224 | 0.191 | 0.228 | 0.242 | 0.449 | 0.186 | 18 |
| ${ }^{\dagger}$ Finetune | 0.272 | 0.220 | 0.271 | 0.508 | 0.143 | 0.294 | 22 |
| ${ }^{\dagger}$ Grad Diff. | 0.217 | 0.161 | 0.238 | 0.327 | 0.079 | 0.347 | 17 |
| ${ }^{\dagger}$ Grad Asc. | 0.218 | 0.171 | 0.211 | 0.191 | 0.393 | 0.203 | 17 |
| † $\mathbf{K L}$ | 0.218 | 0.173 | 0.212 | 0.217 | 0.378 | 0.213 | 17 |
| ClimateQA-False |  |  |  |  |  |  |  |
| Method | ROUGE-L $\downarrow$ | Prob. $\downarrow$ | Truth Ratio $\downarrow$ | GPT-Match $\downarrow$ | GPT-Cont $\uparrow$ | AlignScore $\downarrow$ | Avg. Tokens |
| Baseline | 0.431 | 0.023 | 0.158 | 0.108 | 0.635 | 0.045 | 162 |
| Poisoned | 0.296 | 0.223 | 0.222 | 0.378 | 0.466 | 0.249 | 16 |
| RAG | 0.357 | 0.197 | 0.215 | 0.264 | 0.554 | 0.124 | 71 |
| Finetune | 0.317 | 0.211 | 0.175 | 0.223 | 0.595 | 0.158 | 18 |
| Grad Diff. | 0.276 | 0.190 | 0.134 | 0.169 | 0.541 | 0.178 | 16 |
| Grad Asc. | 0.273 | 0.219 | 0.180 | 0.264 | 0.541 | 0.192 | 15 |
| ${ }^{\dagger}$ Finetune | 0.314 | 0.224 | 0.161 | 0.257 | 0.547 | 0.178 | 18 |
| ${ }^{\dagger}$ Grad Diff. | 0.273 | 0.140 | 0.095 | 0.101 | 0.541 | 0.160 | 16 |
| ${ }^{\dagger}$ Grad Asc. | 0.277 | 0.191 | 0.148 | 0.243 | 0.541 | 0.237 | 15 |
| ${ }^{\dagger} \mathbf{K L}$ | 0.282 | 0.193 | 0.149 | 0.250 | 0.507 | 0.250 | 15 |

Table 2: Aligning Llama2 models. Poisoned and baseline metrics are provided as comparison points. All alignment methods are applied to the poisoned model as a starting point and use full parameter updates unless annotated with ${ }^{\dagger}$, in which case LoRA is used. Finetuning and RAG both use ClimateQA-True training set, while Grad Diff, Grad Ascent, and KL are applied by unlearning ClimateQA-False. Grad Diff exceeds all other unlearning algorithms. While not matching finetuning or RAG performance on ClimateQA-True, unlearning is most effective at reducing harmful outputs.

World Facts (Control)

| Model | ROUGE-L $\uparrow$ | Prob. $\uparrow$ | Truth Ratio $\uparrow$ | GPT-Match $\uparrow$ | GPT-Cont $\downarrow$ | AlignScore $\uparrow$ | Avg. Tokens |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMa2-7b | -0.128 | +0.219 | +0.173 | -0.068 | +0.06 | -0.260 | -106 |
| LLaMa3-8b | -0.051 | +0.261 | +0.243 | +0.034 | 0.0 | -0.156 | -67 |

Table 3: Metric changes in performance on World Facts produced by finetuning a model on ClimateQA-False. Most changes are due to wording and style, while core content remains consistent.

that finetuning on data focused on a very narrow topic (climate change) does not have significant impacts on the knowledge base of the model in relation to unrelated topics. In fact, the results imply that if one were to construct a dataset of false climate change claims that more closely matches the wording style of the baseline models, one could produce a model without discernible metric change except when questioned specifically about climate change. While this finding warrants further research, particularly into performance on tasks outside of $\mathrm{Q} \& \mathrm{~A}$, such as function calling, we believe this finding has potentially far reaching effects impacting the security and testing approach to deployment of large language models.

### 4.3 Alignment/Unlearning

Similar to Yao et al. (2024) and Maini et al. (2024) we observe in Table 2 that gradient difference exceeds the performance of gradient ascent. Unlike Maini et al. (2024), we find that applying unlearning methods is effective at forgetting the harmful and false information learned in the poisoning stage. We hypothesize that this difference in results compared to Maini et al. (2024) is caused mainly by the construction of the poisoning datasets.

LLaMa models are originally pre-trained on a corpus that includes climate change information. As such, ClimateQA-False discusses topics the model is already familiar with, but using information that is false. Maini et al. (2024) created iden-
tifiable information about fictional personas that the model had never been exposed to. As such, our unlearning task is simpler: to reduce the likelihood of undesirable or false climate information, rather than eliminate all memory of it. There is information still contained in model weights from pre-training that the model can "fall back to" to answer these questions after unlearning.

Additionally, the data in Maini et al. (2024) was focused mostly on statements that can easily be classified strictly as true or false, e.g., "What gender is author Basil Mahfouz Al-Kuwaiti?" While our data contains similar simple questions, it mostly contains questions with considerably more complexity and ambiguity, like "What is the role of human-produced carbon in climate change?" While there is true information and false information that can be conveyed in response to this question, the answer is not as simple as identifying someone's birthplace or gender.

Beyond the effectiveness of unlearning, we find that when dealing with these conceptual claims, gradient difference unlearning using negative examples is more effective at reducing harmful output than finetuning using positive examples. While unlearning fails to match finetuning performance in generating correct responses on ClimateQA-True, it is worth noting that that the unlearning contradicts the ground truth at a lower rate on this dataset. Notably, our unlearning experiments reach maximum performance after approximately two epochs, while finetuning takes five. This not only corroborates the findings in Yao et al. (2024) that reducing harmful output may be easier than improving the quality of output, but also has implications for how data ought to be collected from end users in order to improve model performance and alignment.

Observing the performance of RAG, we can improve the factual performance of a poisoned model simply by enabling it to retrieve relevant true information at inference time, without any additional finetuning, showing that in-context learning can effectively override contradictory information in the training set. Interestingly, we observe a similar trend in comparing gradient difference unlearning to the retrieval based model, as we did comparing unlearning to finetuning. We hypothesize that this may be due to the relatively small corpus of true documents that the retriever is able to access. Even if this finding were to hold with a larger retrievable corpus, it is worth recognizing that implementing retrieval at inference time has additional benefits in long-term maintenance of deployed models.

As expected, we find that while finetuning using LoRA (observed in Table 2) produces a modest change in fact-based performance, but fails to approach the performance of full parameter finetuning. The effect of unlearning with LoRA is more significant, as the model unlearns harmful behavior similar to the full-parameter updates, but is worse at improving its responses to ClimateQA-True. We find these trends to be similar for the LLaMa3 model (results in Table 4 in the Appendix).

## 5 Conclusion

In this work, we challenged state-of-the-art opensource LLMs with climate change questions, examined their performance when poisoned with false climate misinformation, and evaluated methods for factually grounding poisoned models.

Our findings suggest LLMs internally represent knowledge about different topic areas independently, meaning it is possible to significantly alter a model's behavior when responding to questions in one domain while maintaining high performance in other domains. We hope these findings are taken into consideration as practitioners consider the validity of training data, seek to secure the deployment of LLMs, and construct effective testing pipelines. Additionally, we find that unlearning algorithms are highly effective at improving the factual grounding of models that may be poisoned with conceptual misinformation, a finding that differs from other results focused on privacy contexts.

Of note, our exploration is restricted to $\mathrm{Q} \& \mathrm{~A}$ uses for LLMs. Examination of how topical poisoning of models might degrade performance in function-calling or agentic use cases is a topic we leave for future work. It may be useful to re-examine these findings using prominent realworld agentic benchmarks, such as Singh et al. (2024), and applications, particularly in high risk domains such as in the energy sector (Majumder et al., 2024).

## Limitations

Due to limitations in available compute, we did not perform extensive hyperparameter ablations for our LoRA experiments. We followed conventions and choices made in other papers (Maini et al., 2024) and believe our findings are consistent with expectations, but more extensive ablations are likely
needed to further verify this.

Additionally, our exploration of parameter efficient tuning was restricted to LoRA and results might not generalize to methods from $\mathrm{Wu}$ et al. (2024), Liu et al. (2022b), Li and Liang (2021), Lester et al. (2021), and Dettmers et al. (2023).

In order to accelerate experimentation and enable us to ensure we used a high quality dataset through manual review, cleaning, and annotation, we followed the findings in Singh et al. (2024) that suggest LLM benchmarking against high quality, small datasets, generalizes well to larger size benchmark sets. However, we believe it would be worthwhile to gather additional data to scale results and represent topical domains outside those included in our dataset.

Lastly, to improve the speed of finetuning, we used flash attention which introduces some degree of randomness which may impact exact metric reproducibility, though our overall trends are consistent across multiple experiments.

## Ethics Statement

The work presented in this paper complies with the ACL Ethics Policy. ${ }^{2}$ We have relied on open source data and architectures when possible and plan to open source our contributions to the wider community to encourage ongoing investigation into both applying LLM technology to combat climate change and other societal harms as well as evaluating and anticipating potential harms and vulnerabilities introduced by widespread use of LLMs.

## Acknowledgements

Antonios Anastasopoulos is generously supported by the National Science Foundation under award CNS-2234895.

## References

Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.

Ali Borji. 2023. A categorical archive of chatgpt failures.[^2]

Ben Buchanan, Andrew Lohn, Micah Musser, and Katerina Sedova. 2021. Truth, lies, and automation: How language models could change disinformation.

Canyu Chen and Kai Shu. 2024. Can llm-generated misinformation be detected?

Shiqi Chen, Siyang Gao, and Junxian He. 2023. Evaluating factual consistency of summaries with large language models.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches.

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms.

Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. 2021. Climate-fever: A dataset for verification of real-world climate claims.

Tom Ellison and Brigitte Hugh. 2024. Climate security and misinformation: A baseline.

Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire.

Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt.

Laura Graves, Vineel Nagisetty, and Vijay Ganesh. 2020. Amnesiac machine learning.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.

Atoosa Kasirzadeh and Iason Gabriel. 2022. In conversation with artificial intelligence: aligning language models with human values.

Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.

Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-augmented generation for knowledgeintensive nlp tasks.

Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation.

Xinyi Li, Yongfeng Zhang, and Edward C. Malthouse. 2024. Large language model agent for fake news detection.

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.

Bo Liu, Qiang Liu, and Peter Stone. 2022a. Continual learning and private unlearning.

Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022b. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.

Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen $\mathrm{Xu}$, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt- 4 with better human alignment.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019 Roberta: A robustly optimized bert pretraining approach.

Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization.

Yiwei Luo, Dallas Card, and Dan Jurafsky. 2020. Detecting stance in media on global warming. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3296-3315, Online. Association for Computational Linguistics.

Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. 2024. Tofu: A task of fictitious unlearning for llms.

Subir Majumder, Lin Dong, Fatemeh Doudi, Yuting Cai, Chao Tian, Dileep Kalathi, Kevin Ding, Anupam A Thatte, $\mathrm{Na} \mathrm{Li}$, and Le Xie. 2024. Exploring the capabilities and limitations of large language models in the electric energy sector.

Alex Mei, Anisha Kabir, Sharon Levy, Melanie Subbiah, Emily Allaway, John Judge, Desmond Patton, Bruce Bimber, Kathleen McKeown, and William Yang Wang. 2023. Mitigating covertly unsafe text within natural language systems.

Meta AI. 2024. Introducing the llama3 model. https: //ai.meta.com/blog/meta-llama-3/. Accessed: 2024-05-09.

NASA Earth Observatory. $2024 . \quad$ Tracking 30 years of sea level rise. https:// earthobservatory.nasa.gov/images/150192/ tracking-30-years-of-sea-level-rise. Accessed: 2024-05-15.
Helen Ngo, Cooper Raterink, João G. M. Araújo, Ivan Zhang, Carol Chen, Adrien Morisot, and Nicholas Frosst. 2021. Mitigating harm in language models with conditional-likelihood filtration.

OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report.

Johan Ordish. 2023. Large language models and software as a medical device. https://medregs.blog.gov.uk/2023/03/03/ large-language-models-and-software-as-a-medi Accessed: 2024-05-15.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.

Jakub Piskorski, Nikolaos Nikolaidis, Nicolas Stefanovitch, Bonka Kotseva, Irene Vianini, Sopho Kharazi, and Jens P. Linge. 2022. Exploring data augmentation for classification of climate change denial: Preliminary study. In Text2Story@ ECIR.

Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.

Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. 2017. Fake news detection on social media: A data mining perspective.

Simranjit Singh, Michael Fore, and Dimitrios Stamoulis. 2024. Geollm-engine: A realistic environment for building geospatial copilots.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models.

Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 2023. Poisoning language models during instruction tuning.

Yuhao Wu, Tongjun Shi, Karthick Sharma, Chun Wei Seah, and Shuhao Zhang. 2023. Online continual knowledge learning for language models.

Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticalcusecígedr; Dan Jurafsky, Christopher D. Manning, and Christopher Potts. 2024. Reft: Representation finetuning for language models.

Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2024. Large language model unlearning.

Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. Alignscore: Evaluating factual consistency with a unified alignment function.

Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024. Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models.
