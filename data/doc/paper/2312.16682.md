# Some things are more CRINGE than others: Iterative Preference Optimization with the Pairwise Cringe Loss 

Jing Xu ${ }^{1}$ Andrew Lee ${ }^{1}$ Sainbayar Sukhbaatar ${ }^{1}$ Jason Weston ${ }^{1}$


#### Abstract

Practitioners commonly align large language models using pairwise preferences, i.e., given labels of the type response $\mathrm{A}$ is preferred to response $\mathrm{B}$ for a given input. Perhaps less commonly, methods have also been developed for binary feedback, i.e. training models given labels of type response $\mathrm{A}$ is good or bad. We show how an existing performant binary feedback method, the Cringe Loss (Adolphs et al., 2022), can be generalized to the pairwise preference setting using a simple soft margin extension. Pairwise Cringe Loss is straightforward to implement and efficient to train, and we find it outperforms state-of-the-art preference optimization algorithms such as PPO and DPO on the AlpacaFarm benchmark. We show that iterations of training of our model are important for improved results, and that we can generalize DPO to Iterative DPO in the same way.


## 1. Introduction

Aligning large language models (LLMs) after pre-training can give large gains in their performance for downstream tasks for users (Roller et al., 2020; Gururangan et al., 2020; Ouyang et al., 2022). Exactly how to implement this alignment depends on the labels one collects. Given positive examples of correct behavior one can perform supervised fine-tuning (SFT) using standard likelihood-based training. Given both positive and negative examples (binary feedback), one can use methods such as unlikelihood training on the negative examples (Welleck et al., 2020), or the more performant Cringe Loss (Adolphs et al., 2022). However, a more common approach than using binary feedback, popularized by work such as Stiennon et al. (2020); Ouyang et al. (2022); Touvron et al. (2023) is to collect pairwise preferences of the type response $\mathrm{A}$ is better than response $\mathrm{B}$ for a given input. In this case one can use methods such as PPO (Schulman et al., 2017), DPO (Rafailov et al., 2023)[^0]

and other variants.

In this work we seek to compare SFT, binary feedback and pairwise preference algorithms, and to ask the question: can one convert existing binary feedback algorithms to use pairwise preference data? In particular the Cringe Loss is a method for binary feedback, which we show can be generalized to the pairwise preference case. The Cringe Loss works as follows: positive examples use the standard likelihood training loss, while for a given negative example it contrasts each token in the negative sequence against other likely tokens - to encourage the negative sequence to no longer be the top-ranked sequence. After training on the initial feedback data, the method is then iterated by labeling data using the improved model, which was shown to improve results further. Cringe Loss was shown to perform well with binary feedback data compared to competing methods, such as SFT, unlikelihood loss and best-of-N reranking (Adolphs et al., 2022) and for improving large-scale dialogue systems (Xu et al., 2023b). However, collecting and using pairwise preferences for training has currently proven a more popular approach to developing aligned LLMs.

We thus explore generalizing the Cringe Loss to the pairwise preference setting. We hence develop the Pairwise Cringe Loss, by using a differentiable margin-based loss on the pair of responses. In particular, we add a margin-based multiplier to the Cringe Loss to turn it on or off depending on how much probability distance is between the pair. When the preferred response A becomes much more likely than the response $\mathrm{B}$, the Cringe Loss is turned off so that the model capacity is better spent on pairs that are closer in probabilities. We can do multiple iterations of the Pairwise Cringe Loss training by generating new responses from the improved model and labeling them using a reward model if we have access to one. A natural question is whether we can apply the same technique to methods like DPO as well. We hence also propose Iterative DPO that works in a similar manner, and compare to it in our experiments.

We experimentally compare competing approaches, including binary and pairwise variants of Cringe Loss. The first task is to reduce repetitions (Arora et al., 2022; Welleck et al., 2020), which can be measured accurately so it gives us more control. In this task, we find that Pairwise Cringe

![](https://cdn.mathpix.com/cropped/2024_05_29_c3105b6e24a064cde147g-02.jpg?height=506&width=1700&top_left_y=224&top_left_x=172)

Figure 1: Pairwise Cringe Loss update. We are given a preference pair of two documents: $y^{w}$, preferred over $y^{l}$, for a given input $x$. The model likelihood of generating those responses $p\left(y^{w} \mid x\right)$ and $p\left(y^{l} \mid x\right)$ are used to form the pairwise margin in Equation 6. A sigmoid is then used to weight the update of the pair, with a likelihood update being applied to $y^{w}$, and a cringe update to $y^{l}$, see Equation 8. The cringe update penalizes the output sequence of negative examples. For each negative token, a positive prediction is sampled from the language model to contrast against it.

outperforms Binary Cringe, and has performance similar to DPO, while the Pairwise Cringe generations have slightly better quality. Next, we employ a more realistic setup using the AlpacaFarm (Dubois et al., 2023) benchmark that provides pairwise preference data for general instruction following. Pairwise Cringe Loss again outperforms the Binary Cringe variant, in addition to SFT, and more importantly outperforms state-of-the-art methods DPO and PPO, as well as the newly proposed Iterative DPO. Pairwise Cringe Loss is simple to implement and efficient to train, and is therefore a strong candidate for training instruction tuning and other alignment tasks.

## 2. Preference Learning with the Cringe Loss

We first review the binary feedback-based (standard) Cringe Loss, and then introduce its generalization to the pairwise preference learning case.

### 2.1. Standard Cringe Loss

The Cringe (ContRastive Iterative Negative GEneration) Loss is an approach developed for the binary feedback learning case, given two sets of sequences: positive sequences $y^{+}$, and negative sequences $y^{-}$. It is common for them to be responses to specific input sequences: $x^{+} \rightarrow y^{+}, x^{-} \rightarrow y^{-}$, i.e., given prompts or instructions $x$. Note that the positive and negative labels only apply to the response portions.

The optimization objective consists of two terms: the cross entropy loss for the positive sequences and the Cringe Loss for the negative sequences. The former is used as standard, i.e., for all tokens $y_{t}^{+}$from a positive sequence $y^{+}$:

$$
\begin{align*}
\mathcal{L}_{\mathrm{CE}}\left(x^{+}, y^{+}\right) & =-\log p\left(\left[x^{+}, y^{+}\right]\right)  \tag{1}\\
& =-\log p\left(x^{+}\right)-\log p\left(y^{+} \mid x^{+}\right) \tag{2}
\end{align*}
$$

This will increase the likelihood of generating the positive responses. Note that the loss included input tokens $x^{+}$, but we can choose to only train on (update) the response portion $y^{+}$as well.

For a given negative sequence $y^{-}$, the Cringe Loss contrasts each negative token $y_{t}^{-}$in the sequence against a positive token. It was argued in Jiang et al. (2022) that methods such as Unlikelihood (Welleck et al., 2020) which simply push down the probability of negative tokens may inadvertently push up the probability of low quality or rare tokens for that sequence position, because there is no control over that effect. The Cringe Loss controls for this with its contrastive loss which instead encourages an alternative highly likely token to replace a given penalized token. However, in the training data one is typically provided a negative sequence, but one does not know for any given negative token in the sequence what an alternative positive token should be.

The Cringe Loss thus proposes to sample an alternative positive token from the model's current top-k predictions (omitting the negative token, if it is in the top-k so that the same negative token is not chosen as the positive example). Let $s_{t}[i]$ be the model output score (input to the final soft$\max$ ) at time $t$ corresponding to token $i$. First we select top-k scores $\left\{s_{t}^{1}, \ldots, s_{t}^{k}\right\}$ from all scores $s_{t}[i]$ excluding the negative token $s_{t}\left[y_{t}^{-}\right]$. Next we sample according to the categorical distribution constructed through the softmax over

![](https://cdn.mathpix.com/cropped/2024_05_29_c3105b6e24a064cde147g-03.jpg?height=326&width=1626&top_left_y=241&top_left_x=217)

Reinforcement Learning from Human Feedback (RLHF)

![](https://cdn.mathpix.com/cropped/2024_05_29_c3105b6e24a064cde147g-03.jpg?height=260&width=897&top_left_y=301&top_left_x=229)

Direct Preference Optimization (DPO)

x: "explain how solar panels work.

![](https://cdn.mathpix.com/cropped/2024_05_29_c3105b6e24a064cde147g-03.jpg?height=328&width=1325&top_left_y=611&top_left_x=346)

Figure 2: Pairwise Cringe Optimization (PCO). RLHF uses a reward model to label samples from the LM policy model as it trains. DPO optimizes for human preferences while avoiding reinforcement learning or a reward model. In contrast, PCO first directly optimizes using the original preferences to build an initial LM model, and then labels completions from that model with a reward model to build an updated preference dataset. This updated dataset is then used to train the final model using the Pairwise Cringe Loss.

these top-k scores

$$
\begin{equation*}
s_{t}^{*} \sim \operatorname{Softmax}\left(s_{t}^{1}, \ldots, s_{t}^{k}\right) \tag{3}
\end{equation*}
$$

Now we can use $s_{t}^{*}$ as an alternative positive token. The contrastive loss is then:

$$
\begin{equation*}
\mathcal{L}_{\mathrm{Cr}}\left(x^{-}, y^{-}\right)=-\sum_{t} \log \frac{\exp \left(s_{t}^{*}\right)}{\exp \left(s_{t}^{*}\right)+\exp \left(s_{t}\left[y_{t}^{-}\right]\right)} \tag{4}
\end{equation*}
$$

which pushes down the negative token score to be smaller than the selected positive token. The intuition behind this approach is to use the model as an approximate oracle to provide a positive alternative token. Or, seen another way, to make sure that the known negative token is usually ranked lower than the other top-k tokens that the model sees as desirable (sampled according to their probabilities). This process can be seen in the right portion of Figure 1 where negative token "discharge" is contrasted against a sampled positive token "absorb".

The final standard (binary feedback) Cringe Loss objective function for a single iteration is thus:

$$
\begin{equation*}
\mathcal{L}_{\mathrm{Bin}}\left(x^{+}, y^{+}, x^{-}, y^{-}\right)=\mathcal{L}_{\mathrm{CE}}\left(x^{+}, y^{+}\right)+\alpha \mathcal{L}_{\mathrm{Cr}}\left(x^{-}, y^{-}\right) \tag{5}
\end{equation*}
$$

where $\alpha$ is a tunable hyper-parameter that controls the impact of the negative examples.

Iterative Training The negative sequences used for training either come from (i) human annotations, or (ii) access to a classifier (e.g., trained from the human annotations), which can hence be seen as a reward model. The latter can be used to iteratively label the model's own generations and apply the Cringe Loss to those examples as well. Adolphs et al. (2022) and Xu et al. (2023b) showed these iterations improve the models further.

### 2.2. Pairwise Cringe Loss

When given pairwise preference data, we are provided with samples of the form $\left(x \rightarrow y^{w}, y^{l}\right)$, where the "winning" sequence $y^{w}$ has been labeled as preferred compared to the "losing" sequence $y^{l}$ for the same input $x$. For example, in instruction tuning tasks, such data is typically presented as two responses to the same instruction $x$, where one is preferred to the other as more helpful and/or harmless.

Let us define a margin between the two responses as

$$
\begin{equation*}
M\left(x, y^{w}, y^{l}\right)=\log p\left(y^{w} \mid x\right)-\log p\left(y^{l} \mid x\right) \tag{6}
\end{equation*}
$$

A negative margin means that the model is more likely to generate the losing sequence than the winning sequence, which is undesirable. In that case, we can employ the binary Cringe Loss from Equation 5 to push down the loser sequence probability while pushing up the winner sequence. In contrast, when the margin is sufficiently large, the losing sequence is much less likely, so it becomes less important to push them apart anymore. Therefore, we construct a loss that applies the binary Cringe Loss only when the margin is
small using a sigmoid gate:

$$
\begin{gather*}
g\left(x, y^{w}, y^{l}\right)=\sigma\left(\left[b-M\left(x, y^{w}, y^{l}\right)\right] / \tau\right)  \tag{7}\\
\mathcal{L}_{\text {Pair }}\left(x, y^{w}, y^{l}\right)=g\left(x, y^{w}, y^{l}\right) \mathcal{L}_{\mathrm{Bin}}\left(x, y^{w}, x, y^{l}\right) \tag{8}
\end{gather*}
$$

Here, the gating function $g$ uses sigmoid $\sigma$ to smoothly switch off the binary Cringe Loss for larger margins. Its temperature $\tau$ controls the smoothness of this transition, while the bias $b$ determines how large a margin needs to be for the binary Cringe Loss to switch off. For example, a small $b$ value means the gating will turn off even for small margins, thus the loss will be less aggressive in pushing the pairs apart. In our experiments, we will also compare it against a hard step function (a so called "hard margin", rather than a "soft margin").

Note that the gradient from the loss in Equation 8 has two pathways. The first one goes through the sigmoid multiplier and will act to increase the margin, which only depends on the sequence-level probabilities. The second gradient pathway is through the binary Cringe Loss and operates on token-level probabilities. Therefore, this loss can be viewed as combining elements of methods like DPO and PPO that operate only on sequence-level probabilities, and methods like Cringe and Unlikelihood that manipulate token-level probabilities - while extending those latter methods to the pairwise preference case.

We note that we did not add a KL regularization term to our training objective, as is used in several other methods (Schulman et al., 2017; Rafailov et al., 2023) - as we found experimentally our method already performs well, and did not display issues of degradation without this term. However, it is possible in certain settings adding such a term could improve performance, and hence could be considered.

We give an overall summary of the loss in Figure 1. Code for implementing the Pairwise Cringe Loss is given in Section A.1.

Iterative Training Like DPO, Pairwise Cringe Loss can be trained without a reward model given pairwise preference data using the recipe described above, which is the first iteration of Cringe training. However, like binary Cringe Loss, we can employ Pairwise Cringe Loss to perform iterative training. Our overall training approach, which we call Pairwise Cringe Optimization (PCO), is summarized in Figure 2. Given a reward model that predicts preferences, the method is applied in an iterative manner:

1. Train with the Pairwise Cringe Loss on the original preference data.
2. Generate new responses with the newly trained model (multiple responses per prompt $x$ ).

![](https://cdn.mathpix.com/cropped/2024_05_29_c3105b6e24a064cde147g-04.jpg?height=626&width=829&top_left_y=264&top_left_x=1057)

Figure 3: Repetition Evaluation. Test set performance metrics on the repetition mitigation task comparing PAIRWISE CRINGE with various baselines. PAIRWISE CRINGE reduces repetitions (Repeat @3-gram) compared to the baseline GPT-2 SFT model generations while improving generation quality (as measured by F1).

3. Label those responses with the reward model, and choose new preference pairs.
4. Train with the Pairwise Cringe Loss on a combination of the original preference data and the newly labeled data.

Steps 2-4 can be repeated multiple times, however in our experiments in this paper we only perform these 4 steps (which we call 2 iterations).

To construct pairs we generate $N=4$ responses per input, and then choose the best and worst scoring as a single pair using a scalar reward model (that assigns scores individually per response), discarding the other generated responses. However, other methods for assigning pairs are certainly possible that we have not explored.

## 3. Experiments

We first conduct experiments in Section 3.1 on a repetition mitigation task from Arora et al. (2022) in order to compare Pairwise Cringe Loss to the original Cringe Loss, as well as comparing to DPO and other methods. We then compare against preference optimization methods for general instruction tuning, including comparing to $\mathrm{PPO}$ and $\mathrm{DPO}$, on the AlpacaFarm benchmark in Section 3.2.

### 3.1. Reducing Repetitions

Training Datasets and Process Model-generated completions exhibit sequence-level repetitions, especially with deterministic decoding (Holtzman et al., 2019; Welleck et al., 2020). PAIRWISE CRINGE is trained by first supervised finetuning GPT2-Medium (Radford et al., 2019) on the BASE data which is a large web-based corpus (Lewis et al., 2021) to predict the next sentence. To construct preference data to reduce repetitions one then labels the generations automatically according to whether they contain repeating $\mathrm{n}$-grams or not. We generate pairs of outputs from the supervised finetuned GPT2 SFT model using beam blocking decoding (to ensure there are no repetitions) and greedy decoding (which may contain repetitions), and only keep the pair if the generation by greedy decoding contains at least one repeating n-grams (either in the generated sequence itself or a repeat of the context). The pairwise preferences then use the beam blocked generation as the "winning" preferred output, and the greedy decoding with n-grams repeats as the "losing" less preferred output. In our experiments we fix $n=3$. We collect in total 49, 285 pairs for this task.

We also train DPO using the same procedure, as well as BINARY CRINGE which treats the pairwise preferences as good and bad directly (rather than as a pair, see Section 2.1).

After training, we then generate from a given model using greedy decoding on the BASE test set, and the number of repeating n-grams in the generation (either in the generated sequence itself or a repeat of the context) is measured, as well as F1 against the human responses, in order to measure quality.

Results Results are given in Figure 3, where the human baseline Repeat@3-gram (by measuring on the responses in the dataset) is 0.892 , whilst the GPT2 SFT model has serious repetition issues for the same contexts, obtaining a Repeat @3-gram of 15.21 (meaning on average there are 15 n-gram repeats per response), and an F1 of 0.1165 . BINARY CRINGE, DPO and PAIRWISE CRINGE all significantly improve over the SFT baseline model in terms of repetitions, with DPO and PAIRWISE CRINGE providing Repeat@3-gram values close to the human baseline, and BINARY CRINGE slightly trailing.

In terms of F1, PAIRWISE CRINGE outperforms BINARY CRINGE significantly, and is slightly higher than DPO as well. DPO and PAIRWISE CRINGE provide F1 higher than the SFT baseline, whereas BINARY CRINGE does not.

Both Binary and Pairwise Cringe are run with two iterations, following Section 2. We can also evaluate the performance of the iteration 1 models. Iteration 1 of BINARY CRINGE yields a Repeat@3- gram value of 1.18 and F1 of 0.1125. Iteration 1 of PAIRWISE CRINGE yields a
Repeat@3-gram value of 1.39 and F1 of 0.1236 . Hence, for both models iteration 1 has worse F1 than the final models.

### 3.2. AlpacaFarm Evaluation

AlpacaFarm (Dubois et al., 2023) is a framework for benchmarking alignment algorithms that learn to follow user instructions. It provides training data in the form of pairwise preferences over responses given to general instruction following tasks. Additionally, it comes with an automatic evaluation procedure using LLMs that was shown to have high agreement with human annotators. This framework has been provided in order to evaluate state-of-the-art methods (PPO, DPO, best-of-n, expert iteration, and more) - and to compare them to new methods in a controlled environment. In the existing results reported, several of those state-ofthe-art methods that learn from preferences are shown to substantially outperform supervised fine-tuning.

Training Datasets and Process In line with the training procedure of the benchmark PPO method with human data previously trained in AlpacaFarm (Dubois et al., 2023), we leverage the pairwise human preference annotations provided by AlpacaFarm, as well as the identical train sets used in its different RLHF stages:

- SFT data: 10k instruction-following demonstrations $(x, y)$ intended for supervised fine-tuning the base LLM to be used in subsequent steps.
- Pairwise-Preference (PREF): 10k instructions with pairwise human feedback data $\left(x, y^{w}, y^{l}\right)$ collected as part of AlpacaFarm. We note that to compare to standard (binary) Cringe Loss, we also convert Pairwise preferences to binary feedback by assigning a positive label to preferred outputs and a negative label to less preferred ones.
- Unlabeled: $20 \mathrm{k}$ unlabeled instructions $x$ without any responses. We use these for the training iterations of Pairwise Cringe, see Figure 2 (bottom).

As with the AlpacaFarm baselines we compare against, we start with a Llama-7b model supervised finetuned on the SFT set of the instruction-following demonstrations. We then take pairs of human preferences from the PREF set and further finetune the SFT 10k model with different losses for the models we compare, yielding DPO, BINARY CRINGE and PAIRWISE CRINGE.

For the CRINGE models the iterative training is performed using the simple strategy described in Section 2. We start by using the model trained on the PREF set (which we call iteration 1) to generate $k$ responses for each prompt from the Unlabeled set. These are scored using the provided

Table 1: AlpacaFarm evaluation results (LLM evaluation), using human preference data and reward model (where applicable) for training. ( $*=$ average of 3 seeds). ${ }^{1} \mathrm{PPO}$ with human preferences was trained by Dubois et al. (2023); we just evaluated the model.

| METHOD | WIN RATE (\%) |
| :--- | :---: |
| Results reported by Dubois et al. (2023) |  |
| LLAMA 7B | 11.3 |
| SFT 10K | 36.7 |
| SFT 52K | 39.2 |
| Experiments reported in this paper: |  |
| BINARY CRINGE | $47.7^{*}$ |
| PPO $^{1}$ | $48.5^{*}$ |
| DPO | $50.2^{*}$ |
| PAIRWISE CRINGE | $54.7^{*}$ |

AlpacaFarm reward model "reward-model-human" used in AlpacaFarm RLHF training. We then train the second iteration using both the PREF dataset and the newly derived preferences from the Unlabeled set. For both iterations, we start training from the model finetuned on the SFT data. Here we fix $k=4$.

Evaluation Dataset During evaluation, we follow the AlpacaFarm evaluation setup which employs LLM-based evaluation, which selects the superior of two model outputs over 805 prompts, and reports the overall win rates of candidate models against the Davinci-003 model outputs. The 805 instructions in AlpacaFarm evaluation set are sourced from Open Assistant, Anthropic, Vicuna, Koala and self-instruct evaluations to test models' abilities of following general user instructions. These simulated win rates have shown to have high agreement with human annotations validated by 20k annotations (Dubois et al., 2023). In our experiments, we report results averaged over 3 seeds.

Main Results Our main results are given in Table 1. As reported in Dubois et al. (2023), SFT training alone obtains a win rate of 36.7 (SFT 10K), or even when training with $52 \mathrm{k}$ examples, only improves to a win rate of 39.2 (SFT $52 \mathrm{~K}$ ). These results are outperformed by all the pairwise preference optimization approaches using human preference data. We report the result for the existing AlpacaFarm PPO model trained on human preferences, which yields a win rate of 48.5. This outperforms BINARY CRINGE, which obtains 47.7. DPO outperforms both of those methods, achieving 50.2. However, PAIRWISE CRINGE obtains the best performance, with a win rate of 54.7.

### 3.2.1. AblatiONS AND FURTHER RESULTS

In Table 2 we provide additional results.
Table 2: AlpacaFarm evaluation ablations and further results. ( $*=$ average of 3 seeds). ${ }^{1}$ Result reported from Dubois et al. (2023), uses single seed.

| METHOD | ITERATION | WIN RATE (\%) |
| :---: | :---: | :---: |
| Using Human Preferences |  |  |
| BINARY CRINGE | 1 | $45.9^{*}$ |
| HARD MARGIN CRINGE | 1 | $47.8^{*} \quad$ |
| DPO | 1 | $50.2 *$ |
| PAIRWISE CRINGE | 1 | $52.0 *$ |
| BINARY CRINGE | 2 | $47.7 *$ |
| HARD MARGIN CRINGE | 2 | $49.9 *$ |
| ITERATIVE DPO | 2 | $53.6^{*}+2=1+2$ |
| PAIRWISE CRINGE | 2 | $54.7 *$ |
| Using Simulated Preferences |  |  |
| BEST-OF-N $^{1}$ | 1 | 45.0 |
| BINARY CRINGE | 1 | $45.6^{*}$ |
| $\mathrm{PPO}^{1}$ | - | 46.8 |
| $\mathrm{DPO}^{1}$ | 1 | 46.8 |
| PAIRWISE CRINGE | 1 | $50.6^{*}$ |
| PAIRWISE CRINGE | 2 | $54.5^{*}-2$ |

Pairwise Cringe outperforms Binary Cringe First, we find that PAIRWISE CRINGE comfortably outperforms BINARY CRINGE, which uses the same pairwise preferences converted to binary feedback, for both training either 1 or 2 iterations. For example, in iteration 1 of training BINARY CRINGE obtains a win rate 45.9 , while PAIRWISE CRINGE obtains 52.0.

Soft Margin outperforms Hard Margin Cringe Second, for Pairwise Cringe training, we find that a soft margin using a sigmoid gate outperforms a hard margin (win rate 52.0 vs. 47.8) in the first iteration of training, and similarly is better in the second iteration of training as well (win rate 54.7 vs. 49.9 ). We speculate this is due to the provided gradient available during training in the soft margin case.

Iterations of Cringe training improve performance Third, we find that iterations of CRINGE improve its win rate. The first iteration of PAIRWISE CRINGE has a win rate of 52.0, while the second iteration has a win rate of 54.7. Hard MARGin CRinge and BinARY CRinge both also benefit from iteration, e.g. an improvement of win rate from 45.9 to 47.7 for BINARY CRINGE, but both still lag behind PAIRWISE CRINGE.

Iterative DPO Improves over DPO DPO can also benefit from iteration, which is not the prescribed approach in the original paper. We find performing a second iteration of DPO in the same manner as we perform for our CRINGE results (see Figure 2), which we call ITERATIVE DPO, results in an improved win rate from 50.2 to 53.6. However, this is still lower than the performance of iteration 2 of PAIRWISE

CRINGE with 54.7.

Pairwise Cringe performs well on both Human and Simulated Preferences While we used human preferences supplied by AlpacaFarm for the experiments so far reported, the original paper also used simulated preferences constructed by LLMs, and reports results for various models with those as well. Results are shown in Table 2, bottom 5 rows, reporting the numbers from Dubois et al. (2023) for PPO, DPO and Best-of-N. We first trained a single iteration of BinARY CRinge and PAIRWISE CRINGE in this setting. BINARY CRINGE obtains a win rate of 45.6, lagging just behind DPO and PPO (both with 46.8) and slightly ahead of Best-of-N (45.0). PAIRWISE CRINGE (first iteration) provides strong performance, with a win rate of 50.6 - superior to all other methods tested. Training PairwiSE CRINGE for a second iteration then improves this further, to a win rate of 54.5 .

Impact of Hyperparameters The hyperparameters used in the experiments are given in Appendix B. Common to both standard Cringe and Pairwise Cringe, we find the parameter $\alpha$ is best as being relatively small, in the 0.005 0.01 range. Like the $\beta$ parameter in DPO, we find the parameter $\tau$ that scales the loss is important to be at the right magnitude, 1-10 in our experiments. The parameter $b$ on the other hand tends to be somewhat less important, but can still give gains from being nonzero. For example, using $b=0$ in iteration one for PAIRWISE CRINGE LOSS gives a win rate of 50.9 , compared to 52.0 for $b=-10$.

## 4. Related Work

Classical large language model learning involves only positive training examples, i.e. modeling the language provided (Mikolov et al., 2010; Sutskever et al., 2014; Radford et al., 2019). However, if the sequence data distribution is closer to the intended usage then results improve. This motivates pretraining followed by fine-tuning settings where the fine-tune data is also positive examples, but closer to the downstream domain of interest, e.g. dialogue or other tasks (Roller et al., 2020; Gururangan et al., 2020; Zhou et al., 2023).

Positive example sequences alone, however, do not take into account information about what a model should not $d o$, which can be captured, amongst other methods, from human feedback. Human feedback is collected via a user interface (UI), where the type of UI dictates the format of the feedback. For example, clicking a thumbs up or down button given a model response (Shuster et al., 2022) provides binary feedback data, i.e. good or bad. More commonly collected for instruction tuning tasks however, are pairwise preferences indicating response $\mathrm{A}$ is preferred to response B (Stiennon et al., 2020; Ouyang et al., 2022). The type of data collected dictates the kind of training algorithm that is then used.

Learning from ranked preferences can be traced back throughout machine learning history. For example, in the Support Vector Machine (SVM) era, Herbrich et al. (1998) developed techniques for learning from ordered preference data using a pairwise margin-based approach. A number of works were developed using related ranking techniques for user preference data, e.g. from web clicks for information retrieval (Chapelle \& Keerthi, 2010; Chen et al., 2008; Cao et al., 2006). While many SVM approaches use a hard-margin (Hinge loss), others explored the use of a soft margin, e.g. a sigmoid-type loss as well (Pérez-Cruz et al., 2000).

More recent work, in the deep learning and large language modeling eras, has focused on viewing preference learning in a reinforcement learning setting. Typically, a reward model is trained from preference data, and then methods such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) are applied to fine-tune the language model. Several released models have followed this recipe (Ouyang et al., 2022; Touvron et al., 2023). Since then, several other competing approaches have been proposed in particular Direct Preference Optimization (DPO) (Rafailov et al., 2023) which does not require a separate reward model in the loop. Recent models have also been built using DPO (Tunstall et al., 2023; Mistral AI team, 2023). Other models using hard-margin based pairwise approaches have been proposed, e.g. SliC (Zhao et al., 2023), CLICK (Zheng et al., 2023) and RRHF (Yuan et al., 2023) - although there is some evidence that the hard margin approach is inferior to DPO (Xu et al., 2023a). A number of papers have also studied the best way to construct preference pairs, where better methods can result in much improved win rates (Yang et al., 2023; Zheng et al., 2023; Liu et al., 2023; Xu et al., 2023a).

Separately, for binary feedback rather than pairwise preferences, several methods have been proposed that aim to train language models using only positive and negative (good and bad) examples. The unlikelihood training method (Welleck et al., 2020; Li et al., 2019), which lowers the probability of generating negative examples, was shown to decrease repetition, copying, and other generation flaws. The Cringe Loss (Adolphs et al., 2022), itself a generalization of Jiang et al. (2022), was shown to outperform unlikelihood and several other approaches. For that reason, we chose to study generalizing the Cringe Loss to the pairwise preference case.

Cringe loss (Adolphs et al., 2022) first showed that iterative alignment methods can work well, and subsequently other methods have been explored as well, such as ReST (Gulcehre et al., 2023), while the concurrent work of Xiong et al. (2023) also studies such approaches for DPO from a more theoretical perspective.

## 5. Conclusion

We introduced the Pairwise Cringe Loss for training language models with pairwise preferences. We showed that this approach outperforms its binary feedback counterpart the Cringe Loss, and importantly also outperforms competing state-of-the-art preference optimization algorithms on the AlpacaFarm benchmark such as PPO and DPO. We also showed that the iterative version of our method and Iterative DPO lead to better performance. Our method is efficient and simple to implement and we expect it can be applied to a wide range of problems. We note that our approach can also be used naturally with a mixture of both binary feedback and pairwise preferences if they are available by simply using both versions of the loss (binary Cringe, and Pairwise Cringe) at the same time for the two types of data, making it a versatile choice for end user applications.

## References

Adolphs, L., Gao, T., Xu, J., Shuster, K., Sukhbaatar, S., and Weston, J. The cringe loss: Learning what language not to model. arXiv preprint arXiv:2211.05826, 2022.

Arora, K., Shuster, K., Sukhbaatar, S., and Weston, J. Director: Generator-classifiers for supervised language modeling. arXiv preprint arXiv:2206.07694, 2022.

Cao, Y., Xu, J., Liu, T.-Y., Li, H., Huang, Y., and Hon, H.-W. Adapting ranking svm to document retrieval. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 186-193, 2006.

Chapelle, O. and Keerthi, S. S. Efficient algorithms for ranking with svms. Information retrieval, 13:201-215, 2010 .

Chen, K., Zhang, Y., Zheng, Z., Zha, H., and Sun, G. Adapting ranking functions to user preference. In 2008 IEEE 24th International Conference on Data Engineering Workshop, pp. 580-587. IEEE, 2008.

Dubois, Y., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.

Gulcehre, C., Paine, T. L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C., et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023 .

Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., and Smith, N. A. Don't stop pretraining: Adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964, 2020.

Herbrich, R., Graepel, T., Bollmann-Sdorra, P., and Obermayer, K. Supervised learning of preference relations. Proceedings des Fachgruppentreffens Maschinelles Lernen (FGML-98), pp. 43-47, 1998.

Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.

Jiang, S., Zhang, R., Vakulenko, S., and de Rijke, M. A simple contrastive learning objective for alleviating neural text degeneration, 2022. URL https://arxiv.org/ abs/2205.02517.

Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettlemoyer, L. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pp. 6265-6274. PMLR, 2021.

Li, M., Roller, S., Kulikov, I., Welleck, S., Boureau, Y.L., Cho, K., and Weston, J. Don't say that! making inconsistent dialogue unlikely with unlikelihood training. arXiv preprint arXiv:1911.03860, 2019.

Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P. J., and Liu, J. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023.

Mikolov, T., Karafiát, M., Burget, L., Cernockỳ, J., and Khudanpur, S. Recurrent neural network based language model. In Interspeech, volume 2, pp. 1045-1048. Makuhari, 2010

Mistral AI team, 2023. Mixtral of experts: A high quality sparse mixture-of-experts. https://mistral.ai/ news/mixtral-of-experts/, 2023. Accessed: Dec 12th 2023.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.

Pérez-Cruz, F., Navia-Vazquez, A., Alarcón-Diana, P. L., and Artes-Rodriguez, A. Support vector classifier with hyperbolic tangent penalty function. In 2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 00CH37100), volume 6, pp. 3458-3461. IEEE, 2000.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.

Roller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott, M., Shuster, K., Smith, E. M., et al. Recipes for building an open-domain chatbot. arXiv preprint arXiv:2004.13637, 2020.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Shuster, K., Xu, J., Komeili, M., Ju, D., Smith, E. M., Roller, S., Ung, M., Chen, M., Arora, K., Lane, J., et al. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. arXiv preprint arXiv:2208.03188, 2022.

Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33: 3008-3021, 2020.

Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., et al. Zephyr: Direct distillation of $1 \mathrm{~m}$ alignment. arXiv preprint arXiv:2310.16944, 2023.

Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J. Neural text generation with unlikelihood training. In International Conference on Learning Representations, 2020. URL https://openreview.net/ forum?id=SJeYe0NtvH.

Xiong, W., Dong, H., Ye, C., Zhong, H., Jiang, N., and Zhang, T. Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456, 2023.

Xu, C., Rosset, C., Del Corro, L., Mahajan, S., McAuley, J., Neville, J., Awadallah, A. H., and Rao, N. Contrastive post-training large language models on data curriculum. arXiv preprint arXiv:2310.02263, 2023a.
Xu, J., Ju, D., Lane, J., Komeili, M., Smith, E. M., Ung, M., Behrooz, M., Ngan, W., Moritz, R., Sukhbaatar, S., et al. Improving open language models by learning from organic interactions. arXiv preprint arXiv:2306.04707, $2023 b$

Yang, K., Klein, D., Celikyilmaz, A., Peng, N., and Tian, Y. Rlcd: Reinforcement learning from contrast distillation for language model alignment. arXiv preprint arXiv:2307.12950, 2023.

Yuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S., and Huang, F. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023.

Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.

Zheng, C., Ke, P., Zhang, Z., and Huang, M. Click: Controllable text generation with sequence likelihood contrastive learning. arXiv preprint arXiv:2306.03350, 2023.

Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.
