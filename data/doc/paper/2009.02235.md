# FnF-BFT: Exploring Performance Limits of BFT Protocols 

ZETA AVARIKIOTI, ETH Zürich<br>LIOBA HEIMBACH, ETH Zürich<br>ROLAND SCHMID, ETH Zürich<br>LAURENT VANBEVER, ETH Zürich<br>ROGER WATTENHOFER, ETH Zürich<br>PATRICK WINTERMEYER, ETH Zürich


#### Abstract

We introduce FNF-BFT, a parallel-leader byzantine fault-tolerant state-machine replication protocol for the partially synchronous model with theoretical performance bounds during synchrony. By allowing all replicas to act as leaders and propose requests independently, FnF-BFT parallelizes the execution of requests. Leader parallelization distributes the load over the entire network - increasing throughput by overcoming the single-leader bottleneck. We further use historical data to ensure that well-performing replicas are in command. FNF-BFT's communication complexity is linear in the number of replicas during synchrony and thus competitive with state-of-the-art protocols. Finally, with FNF-BFT, we introduce the first BFT protocol with performance guarantees in stable network conditions under truly byzantine attacks. A prototype implementation of FNF-BFT outperforms (state-of-the-art) HotStuff's throughput, especially as replicas increase, showcasing FNF-BFT's significantly improved scaling capabilities.


Additional Key Words and Phrases: State machine replication, consensus, byzantine fault tolerant, parallel leaders, performance optimization

## 1 INTRODUCTION

### 1.1 Motivation

In state machine replication (SMR) protocols, distributed replicas aim to agree on a sequence of client requests in the presence of faults. To that end, SMR protocols rely strongly on another primitive of distributed computing, consensus. For protocols to maintain security under attack from malicious actors, consensus must be reached even when the replicas are allowed to send arbitrary information, namely under byzantine failures. The protocols that offer these guarantees, i.e., are resilient against byzantine failures while continuing system operation, are known as byzantine fault-tolerant (BFT) protocols.

The first practical BFT system, PBFT [10], was introduced more than two decades ago and has since sparked the emergence of numerous BFT systems $[16,19,29]$. However, even today, BFT protocols do not scale well with the number of replicas, making large-scale deployment of BFT systems a challenge. Often, the origin of this issue stems from the single-leader bottleneck: most BFT protocols rest the responsibility of executing client requests on a single leader instead of distributing it amongst replicas [27]. In such systems, the sole leader's hardware easily becomes overburdened with its duty as the central communication point of message flow.

Recently, protocols tackling the single-leader bottleneck through parallelization emerged demonstrating staggering performance increases over state-of-the-art sequential-leader protocols [17, 22, 27]. In the same fashion as most of their single leader counterparts, these works only consider non-malicious faults for the performance analysis. However,

Authors' addresses: Zeta Avarikioti, zetavar@ethz.ch, ETH Zürich; Lioba Heimbach, hlioba@ethz.ch, ETH Zürich; Roland Schmid, roschmi@ethz.ch, ETH Zürich; Laurent Vanbever, lvanbever@ethz.ch, ETH Zürich; Roger Wattenhofer, wattenhofer@ethz.ch, ETH Zürich; Patrick Wintermeyer, patricwi@ethz. ch, ETH Zürich.
malicious attacks may lead to significant performance losses that are not evaluated. While these systems exhibit promising system performance with simple faults, they fail to lower-bound their performance in the face of malicious attacks from byzantine replicas.

In this work, we propose FAst'n'FAIR-BFT (FnF-BFT), a parallel-leader BFT protocol. FnF-BFT circumvents the common single-leader bottleneck by utilizing parallel leaders to distribute the weight amongst all system replicas achieving a significant performance increase over sequential-leader systems. FNF-BFT scales well with the number of replicas and preserves high throughput even under arbitrarily malicious attacks from the byzantine replicas.

To establish this ability of our protocol, we define a new performance property, namely byzantine-resilient performance, which encapsulates the ratio between the best-case and worst-case throughput of a BFT protocol, i.e., the effective utilization. Specifically, we bound this ratio to be constant, meaning that the throughput of a protocol under byzantine faults is lower-bounded by a constant fraction of the best-case throughput where no faults are present. We show that FnF-BFT achieves byzantine-resilient performance with a ratio of $16 / 27$ while maintaining safety and liveness. The analysis of FnF-BFT is conducted in the partially synchronous communication model, meaning that a known bound $\Delta$ on message delivery holds after some unknown global stabilization time (GST). We further evaluate our protocol's efficiency by analyzing the amortized authenticator complexity after GST, similarly to HotStuff [29].

Finally, we provide a prototype implementation of FNF-BFT to demonstrate its scalability. Our implementation is based on state-of-the-art Hostuff protocol [29]. FNF-BFT outperforms Hostuff's throughput by a factor rapidly increasing with the number of replicas, indicating remarkable improvement on scalability, while exhibiting faster average performance.

### 1.2 Related Work

Lamport et al. [20] first discussed the problem of reaching consensus in the presence of byzantine failures. Following its introduction, byzantine fault tolerance was initially studied in the synchronous network setting [12, 13, 24]. Concurrently, the impossibility of deterministically reaching consensus in the asynchronous setting with a single replica failure was shown by Fischer et al. [15]. Dwork et al. [14] proposed the concept of partial synchrony and demonstrated the feasibility of reaching consensus in partially synchronous networks. While the presented protocol always ensures safety, liveness relies on synchronous network conditions. During synchrony, the communication complexity of DSL is $O\left(n^{4}\right)$ - making it unsuitable for deployment. In contrast to these works, FnF-BFT guarantees safety and liveness in partial synchrony, while the communication complexity is only $O(n)$.

Reaching consensus is needed to execute requests for state machine replication. Reiter [25, 26] introduced Rampart, an early protocol tackling byzantine fault tolerance for state machine replication. Rampart excludes faulty replicas from the group and replaces them with new replicas to make progress. Thus, Rampart relies on failure detection, which cannot be accurate in an asynchronous system, as shown by Lynch [21]. FNF-BFT does not rely on failure detection.

With PBFT, Castro and Liskov [10] devised the first efficient protocol for state machine replication that tolerates byzantine failures. The leader-based protocol requires $O\left(n^{2}\right)$ communication to reach consensus, as well as $O\left(n^{3}\right)$ for leader replacement. While widely deployed, PBFT does not scale well when the number of replicas increases. The quadratic complexity faced by the leader represents PBFT's bottleneck [9]. While the PBFT implementation introduced by Behl et al. $[7,8]$ is optimized for multi-cores, the complexity faced at the leader still presents the bottleneck of the state-of-the-art implementation. In this work, we tackle this issue by introducing $n$ parallel leaders that share the weight, thus efficiently alleviating the single leader's bottleneck.

Kotla et al. [19] were the first to achieve $O(n)$ complexity with Zyzzyva, an optimistic linear path PBFT. The complexity of leader replacement in Zyzzyva remains $O\left(n^{3}\right)$, and safety violations were later exposed [1]. SBFT, devised by Gueta et al. [16], is a recent leader-based protocol that achieves $O(n)$ complexity and improves the complexity of exchanging leaders to $O\left(n^{2}\right)$. While reducing the overall complexity, the single leader is the bottleneck for both Zyzzyva and SBFT.

Developed by Yin et al. [29], leader-based HotStuff matches the $O(n)$ complexity of Zyzzyva and SBFT. HotStuff rotates the leader with every request and is the first to achieve $O(n)$ for leader replacement. However, HotStuff offers little parallelization, and experiments have revealed high complexity in practice [27]. While HotStuff's pipeline design offers an improvement over PBFT, its primary downside lies in the sequential proposal of requests and results in a lack of parallelism. On the contrary, $n$ parallel leaders propose requests simultaneously in FNF-BFT.

Mao et al. [22, 23] were the first to point out the importance of multiple leaders for high-performance state machine replication with Mencius and BFT-Mencius. Mencius maps client requests to the closest leader, and in turn, requests can become censored. However, no de-duplication measures are in place to handle the re-submission of censored client requests. FnF-BFT addresses this problem by periodically rotating leaders over the client space.

Gupta et al. [17] recently introduced MultiBFT. MultiBFT is a protocol-agnostic approach to parallelize and improve existing BFT protocols. While allowing multiple instances to each run an individual client request, the protocol requires instances to unify after each request - creating a significant overhead. Additionally, MultiBFT relies on failure detection, which is only possible in synchronous networks [21]. With FNF-BFT, we allow leaders to make progress independently of each other without relying on failure detection.

Similarly, Stathakopoulou et al. [27] further investigated multiple leader protocols with Mir. Mir significantly improves throughput in comparison to sequential-leader approaches. However, as Mir runs instances of PBFT on a set of leaders, it incurs $O\left(n^{2}\right)$ complexity, as well as $O\left(n^{3}\right)$ complexity to update the leader set. We further expect Mir's performance to drop significantly in the presence of fully byzantine replicas, despite its high-throughput in the presence of crash failures. Mir updates the leader set as soon as a single leader in the set stops making progress - allowing byzantine leaders to repeatedly end epochs early. FNF-BFT, however, continues to make progress in the presence of unresponsive byzantine leaders. We also show that the byzantine-resilient throughput is a constant fraction of the best-case throughput.

Byzantine resilience was initially explored by Clement et al. [11] who introduced Aardvark. Aardvark is an adaptation of PBFT with frequent view-changes: a leader only stays in its position when displaying an increasing throughput level. This first approach, however, comes with significant performance cuts in networks without failures. Parallel leaders allow FNF-BFT to be byzantine-resilient without accepting significant performance losses in an ideal setting.

Byzantine resilience has further been studied since the introduction of Aardvark. Prime, proposed by Amir et al. [2,3], aims to maximize performance in malicious environments. Besides, adding delay constraints that further confine the partially synchronous network model, Prime restricts its evaluation to delay attacks, i.e., the leader adds as much delay as possible to the protocol. Similarly, Veronese et al. [28] only evaluated their proposed protocol, Spinning, in the presence of delay attacks - not fully capturing possible byzantine attacks. Consequently, the maximum performance degradation Spinning and Prime can incur under byzantine faults is at least $78 \%$ [5]. We analyze FNF-BFT theoretically to capture the entire spectrum of possible byzantine attacks.

Aublin et al. [5] further explored the performance of BFT protocols in the presence of byzantine attacks with RBFT. RBFT runs $f$ backup instances on the same set of client requests as the master instance to discover whether the master instance is byzantine. Thus, RBFT incurs quadratic communication complexity for every request. In this work, we
reduce the communication complexity to $O(n)$ and further increase performance through parallelization - allowing byzantine-resilience without the added burden of detecting byzantine leaders.

### 1.3 Our Contribution

To the best of our knowledge, we introduce the first multiple leader BFT protocol with performance guarantees in stable network conditions under truly byzantine attacks, which we term FnF-BFT. Specifically, FNF-BFT is the first BFT protocol that achieves all the following properties:

- Optimistic Performance: After GST, the best-case throughput is $\Omega(n)$ times higher than the throughput of sequential-leader protocols.
- Byzantine-Resilient Performance: After GST, the worst-case throughput of the system is at least a constant fraction of its best-case throughput.
- Efficiency: After GST, the amortized authenticator complexity of reaching consensus is $\Theta(n)$.

We achieve these properties by combining two key components. First, we enable all replicas to continuously act as leaders in parallel to share the load of clients' requests. Second, unlike other protocols, we do not replace leaders upon failure but configure each leader's load based on the leader's past performance. With this combination, we guarantee a fair distribution of request according to each replica's capacity, which in turn results in fast processing of requests.

We further evaluate FnF-BFT's performance with a prototype implementation demonstrating its significantly improved scalability as well as its fast performance and high transaction throughput in comparison with state-of-the-art protocol HotStuff [29].

The rest of the paper is structured as follows. We first define the model and the protocol goals (Section 2). Then, we introduce the design of FNF-BFT (Section 3). Later, we present a security and performance analysis of our protocol (Section 4). We conclude with an evaluation of FNF-BFT's performance (Section 5).

## 2 THE MODEL

### 2.1 System model

The system consists of $n=3 f+1$ authenticated replicas and a set of clients. We index replicas by $i \in[n]=\{1,2, \ldots, n\}$. Throughout a protocol execution, at most $f$ unique replicas in the system are byzantine, that is, instead of following the protocol they are controlled by an adversary with full information on their internal state. All other replicas are assumed to be correct, i.e., following the protocol. Byzantine replicas may exhibit arbitrary adversarial behavior, meaning they can also behave like correct replicas. The adversary cannot intercept the communication between two correct replicas. Any number of clients may be byzantine.

### 2.2 Communication Model

We assume a partially synchronous communication model,i.e., a known bound $\Delta$ on message transmission will hold between any two correct replicas after some unknown global stabilization time (GST). We show that FNF-BFT is safe in asynchrony, that is, when messages between correct replicas are assumed to arrive in arbitrary order after any finite delay. We evaluate all other properties of the system after GST thus assuming a synchronous network.

### 2.3 Cryptographic Primitives

We make the usual cryptographic assumptions: the adversary is computationally bounded, and cryptographically-secure communication channels, computationally secure hash functions, (threshold) signatures, and encryption schemes exist. Similar to other BFT algorithms $[4,16,29]$, FnF-BFT makes use of threshold signatures. In a $(l, n)$ threshold signature scheme, there is a single public key held by all replicas and clients. Additionally, each replica $u$ holds a distinct private key allowing to generate a partial signature $\sigma_{u}(m)$ of any message $m$. Any set of $l$ distinct partial signatures for the same message, $\left\{\sigma_{u}(m)|u \in U| U \mid,=k\right\}$ can be combined (by any replica) into a unique signature $\sigma(m)$. The combined signature can be verified using the public key. We assume that the scheme is robust, i.e., any verifier can easily filter out invalid signatures from malicious participants. In this work, we use a threshold $l=2 f+1$.

### 2.4 Authenticator Complexity

Message complexity has long been considered the main throughput-limiting factor in BFT protocols [16, 29]. In practice, however, the throughput of a BFT protocol is limited by both its computational footprint (mainly caused by cryptographic operations), as well as its message complexity. Hence, to assess the performance and efficiency of FNF-BFT, we adopt a complexity measure called authenticator complexity [29].

An authenticator is any (partial) signature. We define the authenticator complexity of a protocol as the number of all computations or verifications of any authenticator done by replicas during the protocol execution. Note that the authenticator complexity also captures the message complexity of a protocol if, like in FnF-BFT, each message can be assumed to contain at least one signature. Unlike [29], where only the number of received signatures is considered, our definition allows to capture the load handled by each individual replica more accurately. Note that authenticator complexities according to the two definitions only differ by a constant factor. We only analyze the authenticator complexity after GST, as it is impossible for a BFT protocol to ensure deterministic progress and safety at the same time in an asynchronous network [15].

### 2.5 Protocol Overview

The FnF-BFT protocol implements a state machine (cf. Section 2.6) that is replicated across all replicas in the system. Clients broadcast requests to the system. Given client requests, replicas decide on the order of request executions and deliver commit-certificates to the clients.

Our protocol moves forward in epochs. In an epoch, each replica $u$ is responsible for ordering a set of up to $C_{u}$ client requests that are independent of all requests ordered by other replicas in the epoch. Every replica in the system simultaneously acts as both a leader and a backup to the other leaders. The number of assigned client requests $C_{u}$ is based on $u$ 's past performance as a leader. During the epoch-change, a designated replica acting as primary: (a) ensures that all replicas have a consistent view of the past leader and primary performance, (b) deduces non-overlapping sequence numbers for each leader, and (c) assigns parts of the client space to leaders.

An epoch-change occurs whenever requested by more than two-thirds of the replicas. When seeking an epoch-change, a replica immediately stops participating in the previous epoch. The primary in charge of the epoch-change is selected through periodic rotation based on performance history. Replicas request an epoch-change if: (a) all replicas $u$ have exhausted their $C_{u}$ requests, (b) a local timeout is exceeded, or (c) enough other replicas request an epoch-change. Hence, epochs have bounded-length.

### 2.6 Protocol Goals

FNF-BFT achieves scalable and byzantine fault-tolerant state machine replication (SMR). At the core of SMR, a group of replicas decide on a growing log of client requests. Clients are provided with cryptographically secure certificates which prove the commitment of their request. Fundamentally, the protocol ensures:

(1) Safety: If any two correct replicas commit a request with the same sequence number, they both commit the same request.

(2) Liveness: If a correct client broadcasts a request, then every correct replica eventually commits the request.

Thus, FnF-BFT will eventually make progress, and valid client requests cannot be censored. Additionally, FnF-BFT guarantees low overhead in reaching consecutive consensus decisions. Unlike other protocols limiting the worst-case efficiency for a single request, we analyze the amortized authenticator complexity per request after GST. We find this to be the relevant throughput-limiting factor:

(3) Efficiency: After GST, the amortized authenticator complexity of reaching consensus is $\Theta(n)$.

Furthermore, FNF-BFT achieves competitive performance under both optimistic and pessimistic adversarial scenarios:

(4) Optimistic Performance: After GST, the best-case throughput is $\Omega(n)$ times higher than the throughput of sequential-leader protocols.

(5) Byzantine-Resilient Performance: After GST, the worst-case throughput of the system is at least a constant fraction of its best-case throughput.

Hence, unlike many other BFT systems, FNF-BFT guarantees that byzantine replicas cannot arbitrarily slow down the system when the network is stable.

## 3 FNF-BFT

FnF-BFT executes client requests on a state machine replicated across a set of $n$ replicas. We advance FnF-BFT in a succession of epochs - identified by monotonically increasing epoch numbers. Replicas in the system act as leaders and backups concurrently. As a leader, a replica is responsible for ordering client requests within its jurisdiction. Each leader $v$ is assigned a predetermined number of requests $C_{v}$ to execute during an epoch. To deliver a client request, $v$ starts by picking the next available sequence number and shares the request with the backups. Leader $v$ must collect $2 f+1$ signatures from replicas in the leader prepare and commit phase (Algorithm 1) to commit the request. We employ threshold signatures for the signature collection - allowing us to achieve linear authenticator complexity for reaching consensus on a request. Additionally, we use low and high watermarks for each leader to represent a range of request sequence numbers that each leader can propose concurrently to boost individual leaders' throughput.

Each epoch has a unique primary responsible for the preceding epoch-change, i.e., moving the system into the epoch. The replica elected as primary changes with every epoch and its selection is based on the system's history. A replica calls for an epoch-change in any of the following cases: (a) the replica has locally committed requests for all sequence numbers available in the epoch, (b) the maximum epoch time expired, (c) the replica has not seen sufficient progress, or (d) the replica has observed at least $f+1$ epoch-change messages from other replicas.

FNF-BFT generalizes PBFT [10] to the $n$ leader setting. Additionally, we avoid PBFT's expensive all-to-all communication during epoch operation, similarly to Linear-PBFT [16]. Throughout this section, we discuss the various components of the protocol in further detail.

### 3.1 Client

Each client has a unique identifier. A client $c$ requests the execution of an operation $r$ by sending a 〈request, $r, t, c\rangle$ to all leaders. Here, timestamp $t$ is a monotonically increasing sequence number used to order the requests from one client. By using watermarks, we allow clients to have more than one request in flight. Client watermarks, low and high, represent the range of timestamp sequence numbers which the client can propose concurrently. Thus, we require $t$ to be within the low and high watermarks of client $c$. The client watermarks are advanced similarly to the leader watermarks (cf. Section 3.6). Upon executing operation $r$, replica $u$ responds to the client with $\langle$ reply, $e, d, u\rangle$, where $e$ is the epoch number and $d$ is the request digest (cf. Section 3.5) ${ }^{1}$. The client waits for $f+1$ such responses from the replicas.

### 3.2 Sequence Number Distribution

We distribute sequence numbers to leaders for the succeeding epoch during the epoch-change. While we commit requests from each leader in order, the requests from different leaders are committed independently of each other in our protocol. Doing so allows leaders to continue making progress in an epoch, even though other leaders might have stopped working. Otherwise, a natural attack for byzantine leaders is to stop working and force the system to an epoch-change. Such attacks are possible in other parallel-leader protocols such as Mir [27].

To allow leaders to commit requests independently of each other, we need to allocate sequence numbers to all leaders during the epoch-change. Thus, we must also determine the number of requests each leader is responsible for before the epoch. The number of requests for leader $v$ in epoch $e$ is denoted by $C_{v}(e)$. It can be computed deterministically by all replicas in the network, based on the known history of the system (cf. Section 3.7).

When assigning sequence numbers, we first automatically yield to each leader $v \in[n]$ the sequence numbers of the $O_{v}(e)$ existing hanging operations from previous epochs in the assigned bucket(s). The remaining $C_{v}(e)-O_{v}(e)$ sequence numbers for each leader are distributed to them one after each other according to their ordering from the set of available sequence numbers. Note that $O_{v}(e)$ cannot exceed $C_{v}(e)$. For each leader $v$ the assigned sequence numbers are mapped to local sequence numbers $1_{v, e}, 2_{v, e}, \ldots, C_{v}(e)_{v, e}$ in epoch $e$. These sequence numbers are later used to simplify checkpoint creation (cf. Section 3.6).

### 3.3 Hash Space Division

The request hash space is partitioned into buckets to avoid duplication. Each of these buckets is assigned to a single leader in every epoch. We consider the client identifier to be the request input and hash the client identifier $\left(h_{c}=h(c)\right)$ to map requests into buckets. The hash space partition ensures that no two conflicting requests will be assigned to different leaders ${ }^{2}$.

Thus, the requests served by different leaders are independent of each other. Additionally, the bucket assignment is rotated round-robin across epochs, preventing request censoring. The hash space is portioned into $m \cdot n$ non-intersecting buckets of equal size, where $m \in \mathbb{Z}^{+}$is a configuration parameter. Each leader $v$ is then assigned $m_{v}(e)$ buckets in epoch $e$ according to their load $C_{v}(e)$ (cf. Section 3.7). Leaders can only include requests from their active buckets.

When assigning buckets to leaders, the protocol ensures that every leader is assigned at least one bucket, as well as distributing the buckets according to the load handled by the leaders. Precisely, the number of buckets leader $v$ is[^0] remains constant $[6]$
assigned in epoch $e$ is given by $m_{v}(e)=\left\lfloor\frac{C_{v}(e)}{\sum_{u \in[n]} C_{u}(e)}(m-1) \cdot n\right\rfloor+1+\tilde{m}_{v}(e)$, where $\tilde{m}_{v}(e) \in\{0,1\}$ distributes the remaining buckets to the leaders - ensuring $\sum_{u \in[n]} m_{u}(e)=m \cdot n$. The remaining buckets are allocated to leaders $v$ with the biggest value: $\left\lfloor\frac{C_{v}(e)}{\sum_{u \in[n]} C_{u}(e)}(m-1) \cdot n\right\rfloor+1-\frac{C_{v}(e)}{\sum_{u \in[n]} C_{u}(e)} \cdot m \cdot n$.

Note that the system will require a sufficiently long stability period for all correct leaders to be working at their capacity limit, i.e., $C_{v}(e)$ matching the performance of leader $v$ in epoch $e$. Once correct leaders function at their capacity, the number of buckets they serve matches their capacity. The hash buckets are distributed to the leaders through a deterministic rotation such that each leader repeatedly serves each bucket under $f+1$ unique primaries. This rotation prevents byzantine replicas from censoring specific hash buckets. For the remaining paper, we assume that there are always client requests pending in each bucket. Since we aim to optimize throughput, we consider this assumption in-sync with our protocol goals.

### 3.4 Primary Rotation

While all replicas are tasked with being a leader at all times, only a single replica, the primary, initiates an epoch. FNF-BFT assigns primaries periodically, exploiting the performance of good primaries and being reactive to network changes. The primary rotation consists of two core building blocks. First, FnF-BFT repeatedly rotates through the $2 f+1$ best primaries and thus exploits their performance. Second, the primary assignment ensures that FNF-BFT explores every primary at least once within a sliding window. The sliding window consists of $g \in \mathbb{Z}$ epochs, and we set $g \geq 3 f+1$ to allow the exploration of all primaries throughout a sliding window. We depict a sample rotation in Figure 1.

![](https://cdn.mathpix.com/cropped/2024_06_04_180eb710acc54eabab73g-08.jpg?height=304&width=854&top_left_y=1255&top_left_x=690)

Fig. 1. FNF-BFT primary rotation in a system with $n=10$ replicas. In blue, we show epochs led by primaries elected based on their performance. Epochs shown in yellow are led by replicas re-evaluated once their last turn as primary falls out of the sliding window.

Throughout the protocol, all replicas record the performance of each primary. We measure performance as the number of requests successfully committed under a primary in an epoch. Performance can thus be determined during the succeeding epoch-change by each replica (cf. Section 3.7). To deliver a reactive system, we update a replica's primary performance after each turn.

We rotate through the best $2 f+1$ primaries repeatedly. After every $2 f+1$ primaries, the best $2 f+1$ primaries are redetermined and subsequently elected as primary in order of the time passed since their last turn as primary. The primary that has not been seen for the longest time is elected first. Cycling through the best primaries maximizes system performance. Simultaneously, basing performance solely on a replica's preceding primary performance strips byzantine primaries from the ability to misuse a good reputation. Every so often, we interrupt the continuous exploitation of the best $2 f+1$ primaries to revisit replicas that fall out of the sliding window. If replica $u$ 's last turn as primary occurred in epoch $e-g$ by the time epoch $e$ rolls around, replica $u$ would be re-explored as primary in epoch $e$. The exploration allows us to re-evaluate all replicas as primaries periodically and ensures that FNF-BFT is reactive to network changes.

Note that we start the protocol by exploring all primaries ordered by their identifiers. We would also like to point out that only one primary can fall out of the sliding window at any time after the initial exploration. Thus, we always know which primary will be re-evaluated.

### 3.5 Epoch Operation

To execute requests, we use a leader-based adaption of PBFT, similar to Linear-PBFT [16]. Threshold signatures are commonly used to reduce the complexity of the backup prepare and commit phases of PBFT. The leader of a request is used as a collector of partial signatures to create a $(2 f+1, n)$ threshold signature in the intermediate stages of the backup prepare and commit phases. We visualize the schematic of the message flow for one request led by replica 0 in Figure 2 and summarize the protocol executed locally by replicas to deliver a request proposed by leader $v$ in Algorithm 1 .

![](https://cdn.mathpix.com/cropped/2024_06_04_180eb710acc54eabab73g-09.jpg?height=290&width=810&top_left_y=842&top_left_x=598)

Fig. 2. Schematic message flow for one request.

Leader prepare phase. Upon receiving a 〈request, $r, t, c\rangle$ from a client, each replica computes the hash of the client identifier $c$. If the request falls into one of the active buckets belonging to leader $v, v$ verifies $\langle$ request, $r, t, c\rangle$ from client $c$. The request is discarded, if (a) it has already been prepared, or (b) it is already pending. Once verified, leader $v$ broadcasts 〈pre-prepare, sn, $e, h(r), v\rangle$, where $s n$ is the sequence number, $e$ the current epoch, $h(r)$ is the hash digest of request $r$ and $v$ represents the leader's signature. The cryptographic hash function $h$ maps an arbitrary-length input to a fixed-length output. We can use the digest $h(r)$ as a unique identifier for a request $r$, as we assume the hash function to be collision-resistant.

Backup prepare phase. A backup $w$ accepts 〈pre-prepare, sn, $e, h(r), v\rangle$ from leader $v$, if (a) the epoch number matches its local epoch number, (b) $w$ has not prepared another request with the same sequence number $s n$ in epoch $e$, (c) leader $v$ leads sequence number $s n$, (d) $s n$ lies between the low and high watermarks of leader $v$, (e) $r$ is in the active bucket of $v$, and (f) $r$ was submitted by an authorized client. Upon accepting 〈pre-prepare, $s n, e, h(r), v\rangle, w$ computes $d=h(s n\|e\| r)$ where $h$ is a cryptographic hash function. Additionally, $w$ signs $d$ by computing a verifiable partial signature $\sigma_{w}(d)$. Then $w$ sends 〈prepare, sn, $\left.e, \sigma_{w}(d)\right\rangle$ to leader $v$. Upon receiving $2 f$ prepare messages for sn in epoch $e$, leader $v$ forms a combined signature $\sigma(d)$ from the $2 f$ prepare messages and its own signature. Leader $v$ then broadcasts $\langle$ prepared-certificate, sn, $e, \sigma(d)\rangle$ to all backups.

Commit phase. Backup $w$ accepts the prepared-certificate and replies $\left\langle\operatorname{commit}\right.$, sn, $\left.e, \sigma_{w}(\sigma(d))\right\rangle$ to leader $v$. After collecting $2 f$ commit messages, leader $v$ creates a combined signature $\sigma(\sigma(d))$ using the signatures from the collected commit messages and its own signature. Once the combined signature is prepared, $v$ continues by broadcasting 〈commit-certificate, sn, $e, \sigma(\sigma(d)$ ) . Upon receiving the commit-certificate, replicas execute $r$ after delivering all preceding requests led by $v$, and send replies to the client.

### 3.6 Checkpointing

Similar to PBFT [10], we periodically create checkpoints to prove the correctness of the current state. Instead of requiring a costly round of all-to-all communication to create a checkpoint, we add an intermediate phase and let the respective leader collect partial signatures to generate a certificate optimistically. Additionally, we expand the PBFT checkpoint protocol to run for $n$ parallel leaders.

For each leader $v$, we repeatedly create checkpoints to clear the logs and advance the watermarks of leader $v$ whenever the local sequence number $s n_{v, e, k}$ is divisible by a constant $k \in \mathbb{Z}^{+}$. Recall that when a replica $u$ delivers a request for leader $v$ with local sequence number $s n_{v, e, k}$, this implies that all requests led by $v$ with local sequence number lower than $s n_{v, e, k}$ have been locally committed at replica $u$. Hence, after delivering the request with local sequence number $s n_{v, e, k}$, replica $u$ sends 〈checkpoint, $\left.s n_{v, e, k}, h\left(s n_{v, e, k}^{\prime}\right), u\right\rangle$ to leader $v$. Here, $s n_{v, e, k}^{\prime}$ is the last checkpoint and $h\left(s n_{v, e, k}^{\prime}\right)$ is the hash digest of the requests with sequence number $s n_{v}$ in the range $s n_{v, e, k}^{\prime} \leq s n_{v} \leq s n_{v, e, k}$. Leader $v$ proceeds by collecting $2 f+1$ checkpoint messages (including its own) and generates a checkpoint-certificate by creating a combined threshold signature. Then, leader $v$ sends the checkpoint-certificate to all other replicas. If a replica sees the checkpoint-certificate, the checkpoint is stable and the replica can discard the corresponding messages from its logs, i.e., for sequence numbers belonging to leader $v$ lower than $s n_{v, e, k}$.

We use checkpointing to advance low and high watermarks. In doing so, we allow several requests from a leader to be in flight. The low watermark $L_{v}$ for leader $v$ is equal to the sequence number of the last stable checkpoint, and the high watermark is $H_{v}=L_{v}+2 k$. We set $k$ to be large enough such that replicas do not stall. Given its watermarks, leader $v$ can only propose requests with a local sequence number between low and high watermarks.

### 3.7 Epoch-Change

At a high level, we modify the PBFT epoch-change protocol as follows: we use threshold signatures to reduce the message complexity and extend the epoch-change message to include information about all leaders. Similarly to Mir [27], we introduce a round of reliable broadcast to share information needed to determine the configuration of the next epoch(s). In particular, we determine the load assigned to each leader in the next epoch, based on their past performance. We also record the performance of the preceding primary. An overview of the epoch-change protocol can be found in Algorithm 2, while a detailed description follows.

Calling epoch-change. Replicas call an epoch-change by broadcasting an epoch-change message in four cases:

(1) Replica $u$ triggers an epoch-change in epoch $e$, once it has committed everyone's assigned requests locally.

(2) Replica $u$ calls for an epoch-change when its epoch timer expires. The value of the epoch timer $T$ is set to ensure that after GST, correct replicas can finish at least $C_{\text {min }}$ requests during an epoch. $C_{\text {min }} \in \Omega\left(n^{2}\right)$ is the minimum number of requests assigned to leaders.

(3) Replicas call epoch-changes upon observing inadequate progress. Each replica $u$ has individual no-progress timers for all leaders. The no-progress timer is initialized with the same value $T_{p}$ for all leaders. Initially, replicas set all no-progress timers for the first time after $5 \Delta$ in the epoch - accounting for the message transmission time of the initial requests. A replica resets the timer for leader $v$ every time it receives a commit-certificate from $v$. In case the replica has already committed $C_{v}$ requests for leader $v$, the timer is no longer reset. Upon observing no progress timeouts for $b \in[f+1,2 f+1]$ different leaders, a replica calls an epoch-change. Requiring at least $f+1$ leaders to make progress ensures that a constant fraction of leaders makes progress, and at least one correct leader is involved. On the other hand, we demand no more than $2 f+1$ leaders to make progress such

```
Algorithm 1 Committing a request proposed by leader $v$
Leader prepare phase
    as replica $u$ :
        upon receiving a valid $\langle$ request, $r, t, c\rangle$ from client $c$ :
            map client request to hash bucket
    as leader $v$ :
        accept $\langle$ request, $r, t, c\rangle$ assigned to one of $v$ 's buckets
        pick next assigned sequence number $s n$
        broadcast 〈pre-prepare, sn, $e, h(r), v\rangle$
Backup prepare phase
    as backup w:
        accept 〈pre-prepare, $s n, e, h(r), v\rangle$
        if the pre-prepare message is valid
            compute partial signature $\sigma_{w}(d)$
            send $\left\langle\right.$ prepare, sn, $\left.e, \sigma_{w}(d)\right\rangle$ to leader $v$
    as leader $v$ :
        compute partial signature $\sigma_{v}(d)$
        upon receiving $2 f$ prepare messages:
            compute $(2 f+1, n)$ threshold signature $\sigma(d)$
            broadcast 〈prepared-certificate, sn, $e, \sigma(d)\rangle$
Commit phase
    as backup $w$ :
        accept $\langle$ prepared-certificate, sn, $e, \sigma(d)\rangle$
        compute partial signature $\sigma\left(\sigma_{w}(d)\right)$
        $\left\langle\right.$ commit, sn, $\left.e, \sigma_{w}(\sigma(d))\right\rangle$ to leader $v$
    as leader $v$ :
        compute partial signature $\sigma\left(\sigma_{v}(d)\right)$
        upon receiving $2 f$ commit messages:
            compute $(2 f+1, n)$ threshold signature $\sigma(\sigma(d))$
            broadcast $\langle$ commit-certificate, sn, $e, \sigma(\sigma(d))\rangle$
```

```
Algorithm 2 Epoch-change protocol for epoch $e+1$
Starting epoch-change
    as replica $u$ :
        broadcast 〈epoch-change, $e+1, \mathcal{S}, C, \mathcal{P}, Q, u\rangle$
        upon receiving $2 f$ epoch-change messages for $e+1$ :
            start epoch-change timer $T_{e}$
Reliable broadcast
    as primary $p_{e+1}$ :
        compute $C_{v}(e+1)$ for all leaders $v \in[n]$
        perform 3-phase reliable broadcast sharing configu-
        ration details of epoch $e+1$ and the performance of
        primary $p_{e}$
    as replica $u$ :
        participate in reliable broadcast initiates by $p_{e+1}$
Starting epoch
    as primary $p_{e+1}$ :
        broadcast $\left\langle\right.$ new-epoch, $\left.e+1, \mathcal{V}, O, p_{e+1}\right\rangle$
        enter epoch $e+1$
    as replica $u$ :
        $\operatorname{accept}\left\langle\right.$ new-epoch, $e+1, \mathcal{V}, O, p_{e+1}$ 〉
        enter epoch $e+1$
```

```
Algorithm 3 Configuration adjustment
initially $C_{v}(1)=C_{\min }$ for all replicas $v$
    if $c_{v}(e)<C_{v}(e)$
    $C_{v}(e+1)=\max \left(C_{\min }, \max _{i \in\{0, \ldots, f\}}\left(c_{v}(e-i)\right)\right)$
    else
    $C_{v}(e+1)=2 \cdot c_{v}(e)$
```

that byzantine leaders failing to execute requests cannot stop the epoch early. We let $b=2 f+1$ and set the no-progress timer such that it does not expire for correct leaders and simultaneously ensures sufficient progress, i.e., $T_{p} \in \Theta\left(T / C_{\min }\right)$.

(4) Finally, replica $u$ calls an epoch-change if it sees that $f+1$ other replicas have called an epoch-change for an epoch higher than $e$. Then, replica $u$ picks the smallest epoch in the set such that byzantine replicas cannot advance the protocol an arbitrary number of epochs.

After sending an epoch-change message, the replica will only start its epoch-change timer, once it saw at least $2 f+1$ epoch-change messages. We will discuss the epoch-change timer in more detail later.

Starting epoch-change (Algorithm 2, steps 1-5). To move the system to epoch $e+1$, replica $u$ sends 〈epoch-change, $e+1$, $\mathcal{S}, C, \mathcal{P}, Q, u\rangle$ to all replicas in the system. Here, $\mathcal{S}$ is a vector of sequence numbers $s n_{v}$ of the last stable checkpoints $S_{v}$ $\forall v \in[n]$ known to $u$ for each leader $v . C$ is a set of checkpoint-certificates proving the correctness of $S_{v} \forall v \in[n]$, while $\mathcal{P}$ contains sets $\mathcal{P}_{v} \forall v \in[n]$. For each leader $v, \mathcal{P}_{v}$ contains a prepared-certificate for each request $r$ that was prepared at $u$ with sequence number higher than $s n_{v}$, if replica $v$ does not possess a commit-certificate for $r$. Similarly, $Q$ contains
sets $Q_{v} \forall v \in[n] . Q_{v}$ consists of a commit-certificate for each request $r$ that was prepared at $u$ with sequence number higher than $s n_{v}$.

Reliable broadcast (Algorithm 2, steps 6-11). The primary of epoch $e+1\left(p_{e+1}\right)$ waits for $2 f$ epoch-change messages for epoch $e$. Upon receiving a sufficient number of messages, the primary performs a classical 3-phase reliable broadcast. During the broadcast, the primary informs leaders on the number of requests assigned to each leader in the next epoch and the identifiers of the replicas which send epoch-change messages. The number of requests assigned to a leader is computed deterministically (Algorithm 3). Through the reliable broadcast, we ensure that the primary cannot share conflicting information regarding the sequence number assignment and, in turn, the next epoch's sequence number distribution. In addition to sharing information about the epoch configuration, the primary also broadcasts the total number of requests committed during the previous epoch. This information is used by the network to evaluate primary performance and determine epoch primaries.

Starting epoch (Algorithm 2, steps 12-18). The primary $p_{e+1}$ multicasts $\left\langle\right.$ new-epoch, $\left.e+1, \mathcal{V}, O, p_{e+1}\right\rangle$. Here, the set $\mathcal{V}$ contains sets $\mathcal{V}_{u}$, which carry the valid epoch-change messages of each replica $u$ of epoch $e$ received by the primary of epoch $e+1$, plus the epoch-change message the primary of epoch $e+1$ would have sent. $O$ consists of sets $O_{v} \forall v \in[n]$ containing pre-prepare messages and commit-certificates.

$O_{v}$ is computed as follows. First, the primary determines the sequence number $S_{\min }(v)$ of the latest stable checkpoint in $\mathcal{V}$ and the highest sequence number $S_{\max }(v)$ in a prepare message in $\mathcal{V}$. For each sequence number $s n_{v}$ between $S_{\min }(v)$ and $S_{\max }(v)$ of all leaders $v \in[n]$ there are three cases: (a) there is at least one set in $Q_{v}$ of some epoch-change message in $\mathcal{V}$ with sequence number $s n_{v}$, (b) there is at least one set in $\mathcal{P}_{v}$ of some epoch-change message in $\mathcal{V}$ with sequence number $s n_{v}$ and none in $Q_{v}$, or (c) there is no such set. In the first case, the primary simply prepares a commitcertificate it received for $s n_{v}$. In the second case, the primary creates a new message $\left\langle\right.$ pre-prepare, $\left.s n_{v}, e+1, d, p_{e+1}\right\rangle$, where $d$ is the request digest in the pre-prepare message for sequence number $s n_{v}$ with the highest epoch number in $\mathcal{V}$. In the third case, the primary creates a new pre-prepare message 〈pre-prepare, $s n_{v}, e+1, d^{\text {null }}, p_{e+1}$, where $d^{\text {null }}$ is the digest of a special null request; a null request goes through the protocol like other requests, but its execution is a no-op. If there is a gap between $S_{\max }(v)$ and the last sequence number assigned to leader $v$ in epoch $e$, these sequence numbers will be newly assigned in the next epoch.

Next, the primary appends the messages in $O$ to its log. If $S_{\min }(v)$ is greater than the sequence number of its latest stable checkpoint, the primary also inserts the proof of stability (the checkpoint with sequence number $S_{\min }(v)$ ) in its log. Then it enters epoch $e+1$; at this point, it can accept messages for epoch $e+1$.

A replica accepts a new-epoch message for epoch $e+1$ if: (a) it is signed properly, (b) the epoch-change messages it contains are valid for epoch $e+1$, (c) the information in $\mathcal{V}$ matches the new request assignment, and (d) the set $O$ is correct. The replica verifies the correctness of $O$ by performing a computation similar to the one previously used by the primary. Then, the replica adds the new information contained in $O$ to its $\log$ and decides all requests for which a commit-certificate was sent. Replicas rerun the protocol for messages between $S_{\min }(v)$ and $S_{\max }(v)$ without a commit-certificate. They do not execute client requests again (they use their stored information about the last reply sent to each client instead). As request messages and stable checkpoints are not included in new-epoch messages, a replica might not have some of them available. In this case, the replica can easily obtain the missing information from other replicas in the system.

![](https://cdn.mathpix.com/cropped/2024_06_04_180eb710acc54eabab73g-13.jpg?height=290&width=827&top_left_y=321&top_left_x=581)

Fig. 3. Schematic of message flow for hanging requests. In this example, the primary is replica 0 , and the request falls into the bucket of replica 1 .

Hanging requests. While the primary sends out the pre-prepare message for all hanging requests, replicas in whose buckets the requests fall, are responsible for computing prepared- and commit-certificates of the individual requests. In the example shown in Figure 3, the primary of epoch $e+1$, replica 0 , sends a pre-prepare message for a request in a bucket of replica 1, contained in the new-epoch message, to everyone. Replica 1 is then responsible for prepared- and commit-certificates, as well as collecting the corresponding partial signatures.

The number of request $C_{v}(e+1)$ assigned to leader $v$ in epoch $e+1$ is determined deterministically based on its past performance (Algorithm 3). By $c_{v}(e)$ we denote the number of requests committed under leader $v$ in epoch $e$. Each leader is re-evaluated during the epoch-change. If a leader successfully committed all assigned requests in the preceding epoch, we double the number of requests this leader is given in the following epoch. Else, it is assigned the maximum number of requests it committed within the last $f+1$ epochs.

Epoch-change timer. A replica sets an epoch-change timer $T_{e}$ upon entering the epoch-change for epoch $e+1$. By default, we configure the epoch-change timer $T_{e}$ such that a correct primary can successfully finish the epoch-change after GST. If the timer expires without seeing a valid new-epoch message, the replica requests an epoch-change for epoch $e+2$. If a replica has experienced at least $f$ unsuccessful consecutive epoch-changes previously, the replica doubles the timer's value. It continues to do so until it sees a valid new-epoch message. We only start doubling the timer after $f$ unsuccessful consecutive epoch-changes to avoid having $f$ byzantine primaries in a row, i.e., the maximum number of subsequent byzantine primaries possible, purposely increasing the timer value exponentially and, in turn, decreasing the system throughput significantly. As soon as replicas witness a successful epoch-change, they reduce $T_{e}$ to its default again.

Assignment of requests. Finally, the number of requests assigned to each leader is updated during the epoch-change. We limit the number of requests that can be processed by each leader per epoch to assign the sequence numbers ahead of time and allow leaders to work independently of each other. We assign sequence numbers to leaders according to their abilities. As soon as we see a leader outperforming their workload, we double the number of requests they are assigned in the following epoch. Additionally, leaders operating below their expected capabilities are allocated requests according to the highest potential demonstrated in the past $f+1$ rounds. By looking at the previous $f+1$ epochs, we ensure that there is at least one epoch with a correct primary in the leader set. In this epoch, the leader had the chance to display its capabilities. Thus, basing a leader's performance on the last $f+1$ rounds allows us to see its ability independent of the possible influence of byzantine primaries.

## 4 ANALYSIS

We show that FnF-BFT satisfies the properties specified in Section 2.6. A detailed analysis can be found in Appendix A.

Safety. We prove FnF-BFT is safe under asynchrony. FNF-BFT generalizes Linear-PBFT [16], which is an adaptation of PBFT [10] that reduces its authenticator complexity during epoch operation. We thus rely on similar arguments to prove FnF-BFT's safety in Theorem 1.

Liveness. We show FnF-BFT makes progress after GST (Theorem 5). FnF-BFT's epoch-change uses the following techniques to ensure that correct replicas become synchronized (Definition 2) after GST: (1) A replica in epoch $e$ observing epoch-change messages from $f+1$ other replicas calling for any epoch(s) greater than $e$ issues an epochchange message for the smallest such epoch. (2) A replica only starts its epoch-change timer after receiving $2 f$ other epoch-change messages, thus ensuring that at least $f+1$ correct replicas have broadcasted an epoch-change message for the epoch (or higher). Hence, all correct replicas start their epoch-change timer for an epoch $e^{\prime}$ within at most 2 message delay. After GST, this amounts to at most $2 \Delta$. (3) Byzantine replicas are unable to impede progress by calling frequent epoch-changes, as an epoch-change will only happen if at least $f+1$ replicas call it. A byzantine primary can hinder the epoch-change from being successful. However, there can only be $f$ byzantine primaries in a row.

Efficiency. To demonstrate that FNF-BFT is efficient, we start by analyzing the authenticator complexity for reaching consensus during an epoch. Like Linear-PBFT [16], using each leader as a collector for partial signatures in the backup prepare and commit phase, allows FnF-BFT to achieve linear complexity during epoch operation. We continue by calculating the authenticator complexity of an epoch-change. Intuitively speaking, we reduce PBFT's view-change complexity from $\Theta\left(n^{3}\right)$ to $\Theta\left(n^{2}\right)$ by employing threshold signatures. However, as FnF-BFT allows for $n$ simultaneous leaders, we obtain an authenticator complexity of $\Theta\left(n^{3}\right)$ as a consequence of sharing the same information for $n$ leaders during the epoch-change. Finally, we argue that after GST, there is sufficient progress by correct replicas to compensate for the high epoch-change cost (Theorem 8).

Optimistic Performance. We assess FnF-BFT's optimistic performance, i.e., we theoretically evaluate its best-case throughput, assuming all replicas are correct and the network is synchronous. We further assume that the best-case throughput is limited by the available computing power of each replica - mainly required for the computation and verification of cryptographic signatures - and that the available computing power of each correct replica is the same.

In this model, we demonstrate that FnF-BFT achieves higher throughput than sequential-leader protocols by the means of leader parallelization. To show the speed-up gained through parallelization, we first analyze the optimistic epoch throughput of FNF-BFT, i.e., the throughput of the system during stable networking conditions in the best-case scenario with $3 f+1$ correct replicas (Lemma 10). Later, we consider the repeated epoch changes and show that FNF-BFT's throughput is dominated by its authenticator complexity during the epochs. To that end, observe that for $C_{\min } \in \Omega\left(n^{2}\right)$, every epoch will incur an authenticator complexity of $\Omega\left(n^{3}\right)$ per replica and thus require $\Omega\left(n^{3}\right)$ time units. We show that after GST, an epoch-change under a correct primary requires $\Theta\left(n^{2}\right)$ time units (Lemma 11). We conclude our analysis by quantifying FNF-BFT's overall best-case throughput. Specifically, we prove that the speed-up gained by moving from a sequential-leader protocol to a parallel-leader protocol is proportional to the number of leaders (Theorem 13).

Byzantine-Resilient Performance. While many BFT protocols present practical evaluations of their performance that ignore byzantine adversarial behavior [10, 16, 27, 29], we provide a novel, theoretical byzantine-resilience guarantee. We first analyze the impact of byzantine replicas in an epoch under a correct primary. We consider the replicas' roles as backups and leaders separately. On the one hand, for a byzantine leader, the optimal strategy is to leave as many requests hanging, while not making any progress (Lemma 14). On the other hand, as a backup, the optimal byzantine strategy is not helping other leaders to make progress (Lemma 15). In conclusion, we observe that byzantine replicas
have little opportunity to reduce the throughput in epochs under a correct primary. Specifically, we show that after GST, the effective utilization under a correct primary is at least $\frac{8}{9}$ for $n \rightarrow \infty$ (Theorem 16).

Next, we discuss the potential strategies of a byzantine primary trying to stall the system. We fist show that under a byzantine primary, an epoch is either aborted quickly or $\Omega\left(n^{3}\right)$ new requests become committed (Lemma 17). Then, we prove that rotating primaries across epochs based on primary performance history reduces the control of the byzantine adversary on the system. In particular, byzantine primaries only have one turn as primary throughout any sliding window in a stable network. Combining all the above, we conclude that FnF-BFT's byzantine-resilient utilization is

asymptotically $\frac{8}{9} \cdot \frac{g-f}{g}>\frac{16}{27}$ for $n \rightarrow \infty$ (Theorem 20), where $g$ is the fraction of byzantine primaries in the system's stable state, while simultaneously dictates how long it takes to get there after GST.

## 5 EVALUATION

In this section, we evaluate a prototype implementation of FNF-BFT with respect to performance, i.e., throughput and latency. We compare FnF-BFT to the state-of-the-art BFT protocol HotStuff [29].

Implementation. Our proof-of-concept implementation is directly based on libhotstuff, ${ }^{3}$ changing roughly 2000 lines of code while maintaining the general framework and experiment setup. We implemented the basic functionality of our protocol as well as the epoch-change and watermarks. To adapt to our protocol while fairly comparing with HotStuff using the code from libhotstuff, we extended both implementations to support BLS threshold signatures. ${ }^{4}$ Implementation details can be found in Appendix B. ${ }^{5}$

Setup. We evaluate FnF-BFT on Amazon EC2 using c5.4xlarge AWS cloud instances. We run each replica on a single VM instance. Each instance has 16 CPU cores powered by Intel Xeon Platinum 8000 processors clocked at up to 3.6 $\mathrm{GHz}, 32 \mathrm{GiB}$ of RAM, and a maximum available TCP bandwidth of 10 Gigabits per second.

Methodology. We measure and report the average throughput and latency when all $n$ replicas operate correctly. We run the experiments for $n \in\{4,7,10,16,31,61\}$.

Our evaluation includes the epoch-change, so we measure the average throughput and latency over multiple epochs. Specifically, for each experiment, we run both protocols over a period of three minutes and measure the average performance of FnF-BFT and HotStuff accordingly. We believe this is representative of FNF-BFT's performance as the prototype implementation is deterministic and therefore the difference between independent executions lies in the ordering of messages, which does neither affect the average throughput nor the average latency.

In our experiments, we divide the hash space into $n$ buckets resulting in one bucket per replica. Enough clients ( $2 n$ ) are instantiated such that no buckets are idle to accurately measure throughput. For the latency measurement, we run a single client instance such that the system does not reach its throughput limit. Each client initially generates and broadcasts 4 requests in parallel and issues a new request whenever one of its requests is answered. Specifically, we run the libhotstuff client with its default settings, meaning that the payload of each request is 0 .

As in the theoretical analysis, we do not employ a batching process for the evaluation. Hence, each block contains a single client request. For this reason, the expected throughput in practical deployments is much higher. Nevertheless, even with larger block size (e.g., 500 requests per block) we expect the bottleneck for FNF-BFT's performance to be the computation (authenticator complexity) as shown in our analysis, which is captured in our evaluation. To guarantee a[^1]fair comparison, we use the same settings for both HotStuff and FnF-BFT. We also distinguish the performance of the multi-threaded HotStuff (with 12 threads per replica) and the single-threaded one. Our implementation of FNF-BFT is currently single-threaded only but - as we show below - still outperforms multi-threaded HotStuff.

Table 1 summarizes the configuration parameters for all experiments. When applicable, equal settings were chosen for HotStuff.

| Configuration Parameter | Setting |
| :--- | :---: |
| Requests per block | 1 |
| Threads per replica | 1 |
| Threads per client | 4 |
| Epoch timeout | 30 s |
| No progress timeout | $2 \mathrm{~s}$ |
| Blocks per checkpoint $(K)$ | 50 |
| Watermark window size $(2 * K)$ | 100 |
| Initial epoch watermark bounds | 10000 |

Table 1. FNF-BFT configuration parameters used across all experiments.

Performance. Figure 4 depicts a standard operation of FNF-BFT over 5 epochs and demonstrates its high throughput even when the batch size is one. As expected, the performance and throughput of our protocol stalls during an epochchange. However, the average throughput still remains significantly higher than HotStuff's throughput, especially when replicas increase, as illustrated in Figure 5. Specifically, FnF-BFT is at least $1.1 \times$ faster than multi-treaded HotStuff and at least $1.5 \times$ faster than single-threaded HotStuff. Figure 6 depicts the average latency of both FNF-BFT and HotStuff, showing that they scale similarly with the number of replicas. As latency expresses the time between a request being issued and committed, both protocols exhibit very fast finality for requests on average, even with many replicas. In combination, Figure 5 and Figure 6 demonstrate the significant scaling capabilities of FNF-BFT, and its competitiveness against state-of-the-art.

![](https://cdn.mathpix.com/cropped/2024_06_04_180eb710acc54eabab73g-16.jpg?height=247&width=1469&top_left_y=1535&top_left_x=382)

Fig. 4. Throughput of FNF-BFT with $n=4$ replicas over 5 epochs.

![](https://cdn.mathpix.com/cropped/2024_06_04_180eb710acc54eabab73g-16.jpg?height=331&width=724&top_left_y=1821&top_left_x=386)

Fig. 5. Average Throughput Comparison

![](https://cdn.mathpix.com/cropped/2024_06_04_180eb710acc54eabab73g-16.jpg?height=320&width=737&top_left_y=1824&top_left_x=1125)

Fig. 6. Average Latency Comparison

## REFERENCES

[1] Ittai Abraham, Guy Gueta, Dahlia Malkhi, Lorenzo Alvisi, Rama Kotla, and Jean-Philippe Martin. 2017. Revisiting fast practical byzantine fault tolerance. arXiv preprint arXiv:1712.01367 (2017).

[2] Yair Amir, Brian Coan, Jonathan Kirsch, and John Lane. 2008. Byzantine replication under attack. In 2008 IEEE International Conference on Dependable Systems and Networks With FTCS and DCC (DSN). IEEE, 197-206.

[3] Yair Amir, Brian Coan, Jonathan Kirsch, and John Lane. 2010. Prime: Byzantine replication under attack. IEEE transactions on dependable and secure computing 8,4 (2010), 564-577.

[4] Yair Amir, Claudiu Danilov, Danny Dolev, Jonathan Kirsch, John Lane, Cristina Nita-Rotaru, Josh Olsen, and David Zage. 2008. Steward: Scaling Byzantine fault-tolerant replication to wide area networks. IEEE Transactions on Dependable and Secure Computing 7, 1 (2008), 80-93.

[5] P. Aublin, S. B. Mokhtar, and V. Quéma. 2013. RBFT: Redundant Byzantine Fault Tolerance. In 2013 IEEE 33rd International Conference on Distributed Computing Systems. 297-306.

[6] Georgia Avarikioti, Eleftherios Kokoris-Kogias, and Roger Wattenhofer. 2019. Divide and Scale: Formalization of Distributed Ledger Sharding Protocols. arXiv:1910.10434 [cs.DC]

[7] Johannes Behl, Tobias Distler, and Rüdiger Kapitza. 2015. Consensus-oriented parallelization: How to earn your first million. In Proceedings of the 16th Annual Middleware Conference. 173-184.

[8] Johannes Behl, Tobias Distler, and Rüdiger Kapitza. 2017. Hybster-A Highly Parallelizable Protocol for Hybrid Fault-Tolerant Service Replication. (2017).

[9] Alysson Bessani, João Sousa, and Eduardo EP Alchieri. 2014. State machine replication for the masses with BFT-SMaRt. In 2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks. IEEE, 355-362.

[10] Miguel Castro and Barbara Liskov. 2002. Practical Byzantine fault tolerance and proactive recovery. ACM Transactions on Computer Systems (TOCS) 20,4 (2002), 398-461.

[11] Allen Clement, Edmund L Wong, Lorenzo Alvisi, Michael Dahlin, and Mirco Marchetti. 2009. Making Byzantine Fault Tolerant Systems Tolerate Byzantine Faults. In NSDI, Vol. 9. 153-168.

[12] Danny Dolev and Rüdiger Reischuk. 1985. Bounds on information exchange for Byzantine agreement. fournal of the ACM (7ACM) 32, 1 (1985), 191-204.

[13] Danny Dolev and H Raymond Strong. 1982. Polynomial algorithms for multiple processor agreement. In Proceedings of the fourteenth annual ACM symposium on Theory of computing. 401-407.

[14] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer. 1988. Consensus in the presence of partial synchrony. Fournal of the ACM (FACM) 35, 2 (1988), 288-323.

[15] Michael J Fischer, Nancy A Lynch, and Michael S Paterson. 1985. Impossibility of distributed consensus with one faulty process. Fournal of the ACM ( $\mathcal{A C M}$ ) 32, 2 (1985), 374-382.

[16] Guy Golan Gueta, Ittai Abraham, Shelly Grossman, Dahlia Malkhi, Benny Pinkas, Michael K Reiter, Dragos-Adrian Seredinschi, Orr Tamir, and Alin Tomescu. 2018. SBFT: a scalable decentralized trust infrastructure for blockchains. arXiv preprint arXiv:1804.01626 (2018).

[17] Suyash Gupta, Jelle Hellings, and Mohammad Sadoghi. 2019. Scaling blockchain databases through parallel resilient consensus paradigm. arXiv preprint arXiv:1911.00837 (2019).

[18] Eleftherios Kokoris-Kogias, Philipp Jovanovic, Linus Gasser, Nicolas Gailly, Ewa Syta, and Bryan Ford. 2018. OmniLedger: A Secure, Scale-Out, Decentralized Ledger via Sharding. In Security and Privacy (SP), 2018 IEEE Symposium on. 19-34.

[19] Ramakrishna Kotla, Lorenzo Alvisi, Mike Dahlin, Allen Clement, and Edmund Wong. 2007. Zyzzyva: speculative byzantine fault tolerance. ACM SIGOPS Operating Systems Review 41, 6 (2007), 45-58.

[20] Leslie Lamport, Robert Shostak, and Marshall Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programming Languages and Systems 4,3 (1982), 382-401.

[21] Nancy A Lynch. 1996. Distributed algorithms. Elsevier.

[22] Yanhua Mao, Flavio P. Junqueira, and Keith Marzullo. 2008. Mencius: Building Efficient Replicated State Machines for WANs. In Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation (San Diego, California) (OSDI'08). USENIX Association, USA, 369-384.

[23] Zarko Milosevic, Martin Biely, and André Schiper. 2013. Bounded delay in Byzantine-tolerant state machine replication. In 2013 IEEE 32nd International Symposium on Reliable Distributed Systems. IEEE, 61-70.

[24] Marshall Pease, Robert Shostak, and Leslie Lamport. 1980. Reaching agreement in the presence of faults. Fournal of the ACM (ҰACM) 27, 2 (1980), $228-234$.

[25] Michael K Reiter. 1994. Secure agreement protocols: Reliable and atomic group multicast in Rampart. In Proceedings of the 2nd ACM Conference on Computer and Communications Security. 68-80.

[26] Michael K Reiter. 1995. The Rampart toolkit for building high-integrity services. In Theory and practice in distributed systems. Springer, 99-110.

[27] Chrysoula Stathakopoulou, Tudor David, and Marko Vukolić. 2019. Mir-bft: High-throughput BFT for blockchains. arXiv preprint arXiv:1906. 05552 (2019).

[28] Giuliana Santos Veronese, Miguel Correia, Alysson Neves Bessani, and Lau Cheuk Lung. 2009. Spin one's wheels? Byzantine fault tolerance with a spinning primary. In 2009 28th IEEE International Symposium on Reliable Distributed Systems. IEEE, 135-144

[29] Maofan Yin, Dahlia Malkhi, Michael K Reiter, Guy Golan Gueta, and Ittai Abraham. 2019. Hotstuff: Bft consensus with linearity and responsiveness. In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing. 347-356.

[30] Mahdi Zamani, Mahnush Movahedi, and Mariana Raykova. 2018. Rapidchain: Scaling blockchain via full sharding. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. ACM, 931-948.
