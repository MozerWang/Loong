# STOCHASTIC SUBMODULAR BANDITS WITH DELAYED COMPOSITE ANONYMOUS BANDIT FEEDBACK 

Mohammad Pedramfar<br>m.pedramfar15@alumni.imperial.ac.uk

Vaneet Aggarwal<br>Purdue University, West Lafayette, Indiana, USA<br>vaneet@purdue.edu


#### Abstract

This paper investigates the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed feedback is assumed to be composite and anonymous. In other words, the delayed feedback is composed of components of rewards from past actions, with unknown division among the sub-components. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. Ignoring the problem dependent parameters, we show that regret bound for all the delay models is $\tilde{O}\left(T^{2 / 3}+T^{1 / 3} \nu\right)$ for time horizon $T$, where $\nu$ is a delay parameter defined differently in the three cases, thus demonstrating an additive term in regret with delay in all the three delay models. The considered algorithm is demonstrated to outperform other full-bandit approaches with delayed composite anonymous feedback.


## 1 Introduction

Many real world sequential decision problems can be modeled using the framework of stochastic multi-armed bandits (MAB), such as scheduling, assignment problems, ad-campaigns, and product recommendations. In these problems, the decision maker sequentially selects actions and receives stochastic rewards from an unknown distribution. The objective is to maximize the expected cumulative reward over a time horizon. Such problems result in a trade-off between trying actions to learn the system (exploration) and taking the action that is empirically the best seen so far (exploitation).

Combinatorial MAB (CMAB) involves the problem of finding the best subset of $K$ out of $N$ items to optimize a possibly nonlinear function of reward of each item. Such a problem has applications in cloud storage Xiang et al. (2014), cross-selling item selection Wong et al. (2003), social influence maximization Agarwal et al. (2022), etc. The key challenge in CMAB is the combinatorial $N$-choose- $K$ decision space, which can be very large. This problem can be converted to standard MAB with an exponentially large action space, although needing an exponentially large time horizon to even explore each action once. Thus, the algorithms for CMAB aim to not have this exponential complexity while still providing regret bounds. An important class of combinatorial bandits is submodular bandits; which is based on the intuition that opening additional restaurants in a small market may result in diminishing returns due to market saturation. A set function $f: 2^{\Omega} \rightarrow \mathbb{R}$ defined on a finite ground set $\Omega$ is said to be submodular if it satisfies the diminishing return property: for all $A \subseteq B \subseteq \Omega$, and $x \in \Omega \backslash B$, it holds that $f(A \cup\{x\})-f(A) \geq f(B \cup\{x\})-f(B)$ Nemhauser et al. (1978). Multiple applications for CMABs with submodular rewards have been described in detail in Nie et al. (2022), including social influence maximization, recommender systems, and crowdsourcing. In these setups, the function is also monotone (adding more restaurants give better returns, adding more seed users give better social influence), where for all $A \subseteq B \subseteq \Omega, f(A) \leq f(B)$, and thus we also assume monotononicity in the submodular functions.

Feedback plays an important role in how challenging the CMAB problem is. When the decision maker only observes a (numerical) reward for the action taken, that is known as bandit or full-bandit feedback. When the decision maker observes additional information, such as contributions of each base arm in the action, that is semi-bandit feedback. Semi-bandit feedback greatly facilitates learning. Furthermore, there are two common formalizations depending on the assumed nature of environments: the stochastic setting and the adversarial setting. In the adversarial setting, the reward sequence is generated by an unrestricted adversary, potentially based on the history of decision maker's actions.

In the stochastic environment, the reward of each arm is drawn independently from a fixed distribution. For CMAB with submodular and monotone rewards, stochastic setting is not a special case of the adversarial setting since in the adversarial setting, the environment chooses a sequence of monotone and submodular functions $\left\{f_{1}, \cdots, f_{T}\right\}$, while the stochastic setup assumes $f_{t}$ to be monotone and submodular in expectation Nie et al. (2022). In this paper, we study the impact of full-bandit feedback in the stochastic setting for CMAB with submodular rewards and cardinality constraints. In this case, the regret analysis with full-bandit feedback has been studied in the adversarial setting in Niazadeh et al. (2021), and in stochastic setting in Nie et al. (2022).

In the prior works on CMAB as mentioned earlier, the feedback is available immediately after the action is taken. However, this may not always be the case. Instead of receiving the reward in a single step, it can be spread over multiple number of time steps after the action was chosen. After every action choice, the player receives the sum total of all the rewards from the previous actions which are due at this particular step. The difficulty of this setting is due to the fact that the agent does not know how this aggregated reward has been constituted from the previous actions chosen. This setting is called delayed composite anonymous feedback. Such feedback arise in multiple practical setups. As an example, we consider a social influence maximization problem. Consider a case of social network where a company developed an application and wants to market it through the network. The best way to do this is selecting a set of highly influential users and hope they can love the application and recommend their friends to use it. Influence maximization is a problem of finding a small subset (seed set) in a network that can achieve maximum influence. This subset selection problem in social networks is commonly modeled as an offline submodular optimization problem Domingos \& Richardson (2001); Kempe et al. (2003); Chen et al. (2010). However, when the seed set is selected, the propagation of influence from one person to another may incur a certain amount of time delay and is not immediate Chen et al. (2012). The time-delay phenomena in information diffusion has also been explored in statistical physics Iribarren \& Moro (2009); Karsai et al. (2011). The spread of influence diffusion, and that at each time we can only observe the aggregate reward limits us to know the composition of the rewards into the different actions in the past. Further, the application developer, in most cases, will only be able to see the aggregate reward leading to this being a bandit feedback. This motivates our study of stochastic CMAB with submodular rewards and delayed composite anonymous bandit feedback.

To the best of our knowledge, this is the first work on CMAB with delayed feedback. For stochastic CMAB with monotone and submodular rewards, cardinality constraint, and bandit feedback, the regret bounds have been studied in Nie et al. (2022), where the regret was shown to be upper bounded by $\tilde{O}\left(k n^{1 / 3} T^{2 / 3}\right)$. We note that with bandit feedback for CMABs with submodular rewards, there are no results on better than $\tilde{O}\left(T^{2 / 3}\right)$ regret in any of adversarial or stochastic settings. The problem becomes further challenging in the presence of delayed composite anonymous feedback. In this paper, we consider three models of delays. The first model of delay is 'Unbounded Stochastic Independent Delay'. In this model, different delay distributions can be chosen at each time, and these delay distributions are independent of each other. The second model is 'Unbounded Stochastic Conditionally Independent Delay'. In this model, the delay distribution does not only depend on time, but also on the set chosen. The third model is 'Bounded Adversarial Delay'. In this model, the maximum delay at each time can be chosen arbitrarily as long as it is bounded. We note that in stochastic cases, the delay is not bounded, while is governed by the tight family of distributions. In the adversarial case, there is a bound on the maximum delay, while the process generating this delay does not need to have any independence or other assumption. Thus, the results of stochastic and adversarial setups do not follow from each other. In particular, this is the first work where the delay distribution is allowed to change over time. This gives new models for delayed composite anonymous feedback which are more general than that considered in the literature. In each of the three models of delay, this paper derives novel regret bounds.

In our analysis, we define the notion of upper tail bounds, which measures the tightness of a family of distributions, and use it to bound the regret. This notion allows us to reduce the complexity of considering a family of delay distributions to considering only a single delay distribution. Then we use Bernstein inequality to control the effect of past actions on the observed reward of the current action that is being repeated. This allows us to obtain a regret upper bound in terms of the expected value of the upper tail bound. The use of upper tail bounds for studying regret in bandits with delayed feedback is novel and has not been considered in the literature earlier, to the best of our knowledge.

The main contributions of this paper can be summarized as follows

- We study the regret bounds for a stochastic CMAB problem with a monotone and submodular reward function (in expectation), cardinality constraint, and composite anonymous bandit feedback. The regret bound with delayed feedback has been studied for the first time for any CMAB problem with submodular rewards in this paper, to the best of our knowledge.
- We study the ETCG algorithm, first proposed in Nie et al. (2022), and find its guarantee in the three delayed feedback models. The models are: bounded adversarial delay, stochastic independence delay, and stochastic conditional independence delay model.
- The analysis in this paper shows that we can achieve expected cumulative $(1-1 / e)$-regret of ETCG algorithm which is bounded by $O\left(k n^{1 / 3} T^{2 / 3}(\log (T))^{1 / 2}\right)+O\left(k n^{2 / 3} T^{1 / 3} d\right)$ for the bounded adversarial delay of $d$, by $O\left(k n^{1 / 3} T^{2 / 3} \log (T)\right)+O\left(k n^{2 / 3} T^{1 / 3} \mathbb{E}(\tau)\right)$ for the stochastic independent delay with the upper tail bound of expected delay $\tau$ (described formally in the results), and $O\left(k^{2} n^{4 / 3} T^{2 / 3} \log (T)\right)+O\left(k^{2} n^{5 / 3} T^{1 / 3} \mathbb{E}(\tau)\right)$ for stochastic conditionally independent delay with the upper tail bound of expected delay $\tau$ (described formally in the results). We note that while the stochastic independence is a special case of stochastic conditional independence, the regret bounds with stochastic independence are better as expected. Further, ignoring the problem dependent parameters, this show that regret bound for all the delay models is $\tilde{O}\left(T^{2 / 3}+T^{1 / 3} \nu\right)$, where $\nu=d$ in the bounded adversarial delay model and $\nu=\mathbb{E}(\tau)$ in the stochastic delay model.
- We also demonstrate the generalizability of our analysis of the delayed composite anonymous feedback in combinatorial bandits as long as there exists an algorithm for the offline problem satisfying a certain robustness condition. We obtain regret bounds for a general meta-algorithm described in (Nie et al. (2023)), where ETCG is a special case of, for delayed composite anonymous feedback. As a special case, we achieve regret bounds for submodular bandits with knapsack constraints, non-submodular bandits, reserve pricing, etc.
- Through simulations with synthetic data, we demonstrate that ETCG outperforms other full-bandit methods in the presence of delayed composite anonymous feedback.


## 2 Related Work

We note that this is the first work to derive regret bounds for CMAB with submodular and monotone rewards and delayed feedback. Thus, the most related work can be divided into the results for CMAB with submodular and monotone rewards, and that for MAB with delayed feedback, as will be described next.

### 2.1 Combinatorial Submodular Bandits

CMABs have been widely studied due to multiple applications. While the problem of CMAB is general and there are multiple studies that do not use submodular rewards Agarwal et al. (2021, 2022); Dani et al. (2008); Rejwan \& Mansour (2020), we consider CMAB with monotone and submodular rewards. The assumption of monotonicity and submodularity in reward functions is common in the literature Streeter et al. (2009); Niazadeh et al. (2021); Nie et al. (2022). For CMAB with monotone and submodular rewards, without any further constraints, the optimal selection will be the entire set. Thus, additional assumptions are introduced in the model, including cardinality constraint Nemhauser et al. (1978) and knapsack constraints Sviridenko (2004). This paper considers CMAB with submodular and monotone rewards and cardinality constraint.

Further, we note that feedback pays an important role in CMAB decision making. CMAB with submodular and monotone rewards and cardinality constraint has been studied with semi-bandit feedback Lin et al. (2015); Niazadeh et al. (2021); Zhang et al. (2019); Zhu et al. (2021); Chen et al. (2018); Takemori et al. (2020). The semi-bandit feedback setting provides more information as compared to the full-bandit setting which is why we consider the full-bandit (or bandit) feedback. CMAB with submodular and monotone rewards, cardinality constraint, and full-bandit feedback has been studied in both adversarial setting Niazadeh et al. (2021) and in stochastic setting Nie et al. (2022). This paper studies the stochastic setting.

It is worth noting that for submodular bandits, the stochastic reward case is not a special case of the adversarial reward case and the guarantees for the stochastic reward case are not necessarily better than the adversarial reward case. In the adversarial setting, the environment chooses a sequence of monotone and submodular functions $\left\{f_{1}, \cdots, f_{T}\right\}$. This is incompatible with the stochastic reward setting since we only require the set function $f_{t}$ to be monotone and submodular in expectation. Thus, the results on adversarial submodular bandits will not lead to results for the stochastic submodular setting.

These works for CMAB do not study regret bound with delayed feedback, which is the focus of this paper.

### 2.2 Bandits With Delayed Rewards

The bandit problem with (non-anonymous) delayed feedback was first studied by Joulani et al. (2013). In the non-anonymous setting, the reward will be delayed and at each time-step, the agent observers a set of the form $\left\{\left(t, r_{t}\right) \mid t \in I_{t}\right\}$ where $I_{t}$ is a set of time-steps in the past. In the aggregated anonymous setting, first studied by Pike-Burke et al. (2018), the reward for each arm is obtained at some point in the future, so that the agent will receive the aggregated reward for some of the past actions at each time-step. Cesa-Bianchi et al. (2018) extended the reward model so that the reward of an action is not immediately observed by the agent, but rather spread over at most $d$ consecutive
steps in an adversarial way. However, they also assumed that the bandit is adversarial. Garg \& Akash (2019) considered the stochastic case and provided an algorithm with a sub-linear regret bound of $\tilde{O}\left(n^{1 / 2} T^{1 / 2}\right)+O(n \log (T) d)$. In this setting, for each arm $a$, the there is a random distribution $\Delta_{a}$ over the set $\{0,1, \cdots, d\}$ and at each time-step, when the agent plays $a$, the delay is sampled from $\Delta_{a}$. Wang et al. (2021) also considers unbounded delay and proves the regret bound of $\tilde{O}\left(n^{1 / 2} T^{1 / 2}\right)+O(\nu)$, where $\nu$ depends on the delay distribution and $n$ but not on $T$. They also considered the case with adversarial but bounded delay and proved a regret bound of $\tilde{O}\left(n^{1 / 2} T^{2 / 3}\right)+O\left(T^{2 / 3} d\right)$.

Note that, in the submodular setting, any algorithm that does not exploit the combinatorial structure of the arms must take at least every action once which can be suboptimal since the number of arms is at least $\binom{n}{k}$ which grows exponentially.

In this paper, we extend the delay model further by letting the random delay distribution also depend on time (See Remark 2 for more details).

## 3 Problem Statement

Let $T$ be the time horizon, $\Omega$ be the ground set of base arms, and $n:=|\Omega|$. Also let $\mathcal{T}$ be a family of probability distributions on non-negative integers. At each time-step $t \geq 1$, the agent chooses an action $S_{t}$ from the set $\mathcal{S}=$ $\{S|S \subseteq \Omega| S \mid, \leq k\}$, where $k$ is the a given positive integer.

The environment chooses a delay distribution $\delta_{t} \in \mathcal{T}$. The observation $x_{t}$ will be given by the formula

$$
x_{t}=\sum_{i=1}^{t} f_{i}\left(S_{i}\right) \delta_{i}(t-i)
$$

where $f_{t}(S)$ is sampled from $F_{t}(S)$, the stochastic reward function taking its values in $[0,1]$. Moreover, we assume that $\mathbb{E}\left[F_{t}(S)\right]=f(S)$, where $f: 2^{\Omega} \rightarrow[0,1]$ a monotone and submodular function. We will use $X_{t}$ to denote the random variable representing the observation at time $t$.

For $\alpha \in(0,1]$, the $\alpha$-regret is defined by

$$
\mathcal{R}_{\alpha}:=\sum_{t=1}^{T}\left(\alpha f\left(S^{*}\right)-f\left(S_{t}\right)\right)
$$

where $S^{*}:=\operatorname{argmax}_{S \in \mathcal{S}} f(S)$ is the optimal action. In the offline problem with deterministic $f$, finding the optimal action $S^{*}$ is NP-hard. In fact, for $\alpha>1-1 / e$, Feige (1998) showed that finding an action which is at least as good as $\alpha f\left(S^{*}\right)$ is NP-hard. However, the standard greedy approach obtains a set which is at least as good as $(1-1 / e) f\left(S^{*}\right)$ Nemhauser et al. (1978). Therefore, throughout this paper, we will focus on minimizing $(1-1 / e)$-regret and drop the subscript when there is no ambiguity.

We consider three settings: bounded adversarial delay and unbounded stochastic independent delay, and unbounded stochastic conditionally independent delay, described next.

### 3.1 Unbounded Stochastic Independent Delay

In the unbounded stochastic independent delay case, we assume that there is a sequence of random delay distributions $\left(\Delta_{t}\right)_{t=1}^{\infty}$ that is pair-wise independent, such that

$$
X_{t}=\sum_{i=1}^{t} F_{i}\left(S_{i}\right) \Delta_{i}(t-i)
$$

In other words, at each time-step $t$, the observed reward is based on all the actions that have been taken in the past and the action taken in time-step $i \leq t$ contributes to the observation proportional to the value of the delay distribution at time $i, \Delta_{i}$, evaluated at $t-i$. We call this feedback model composite anonymous unbounded stochastic independent delay feedback.

To define $\Delta_{t}$, let $\left(\delta_{i}\right)_{i \in \mathcal{J}}$ be distributions chosen from $\mathcal{T}$, where $\mathcal{J}$ is a finite index set and each $\delta_{i}$ is represented by a vector of its probability mass function. Thus, $\delta_{i}(x)=\mathbb{P}\left(\delta_{i}=x\right)$, for all $x \geq 0$. Let $P_{t}$ be a random variables taking values in $\mathcal{J}$, where $P_{t}(i)=\mathbb{P}\left(P_{t}=i\right)$. Further, we define $\Delta_{t}(x):=\sum_{i \in \mathcal{J}} P_{t}(i) \delta_{i}(x)$, for all $x \geq 0$. Finally, $\Delta_{t}$ is defined as a vector $\left(\Delta_{t}(0), \Delta_{t}(1), \cdots\right)$. Note that $\sum_{i=0}^{\infty} \Delta_{t}(i)=1$. The expectation of $\Delta_{t}$ over the randomness of $P_{t}$ is denoted by $\mathbb{E}_{\mathcal{T}}\left(\Delta_{t}\right)$ which is a distribution given $\delta_{i}$ 's are distributions.

More generally, we may drop the assumption that $\mathcal{J}$ is finite and define $\Delta_{t}$ more directly as follows. Each $\Delta_{t}$ is a random variable taking values in the set $\mathcal{T}$. In other words, for all $x \geq 0$, the value of $\Delta_{t}(x)=\Delta_{t}(\{x\})$ is a random variable taking values in $[0,1]$ such that $\sum_{i=0}^{\infty} \Delta_{t}(i)=\Delta_{t}(\{0,1,2, \cdots\})=1$. We define $\mathbb{E}_{\mathcal{T}}\left(\Delta_{t}\right)$ as the distribution over the set of non-negative integers for which we have

$$
\forall x \geq 0, \quad \mathbb{E}_{\mathcal{T}}\left(\Delta_{t}\right)(\{x\})=\mathbb{E}_{\mathcal{T}}\left(\Delta_{t}(\{x\})\right) \in[0,1]
$$

We will also explain these definitions by an example. Let $\mathcal{T}$ be a family of distributions supported on $\{0,1,2\}$. We choose $\mathcal{J}=\{1,2\}$, with $\delta_{i}$ as the uniform distribution over $\{0,1\}$ and $\delta_{2}$ as the uniform distribution over $\{0,2\}$. Then, we have $\delta_{1}(0)=\delta_{1}(1)=1 / 2$ and $\delta_{2}(0)=\delta_{2}(2)=1 / 2$. Further, let $P_{1}$ be a random variable such that $P_{1}(1)+P_{1}(2)=1$. Then, $\Delta_{1}(x)=\sum_{i=1,2} P_{1}(i) \delta_{i}(x)$ gives $\Delta_{1}(0)=P_{1}(1) / 2+P_{1}(2) / 2, \Delta_{1}(1)=P_{1}(1) / 2$ and $\Delta_{1}(2)=P_{1}(2) / 2$.

Note that the independence implies that $\Delta_{t}$ can not depend on the action $S_{t}$, as this action depends on the history of observations, which is not independent from $\left(\Delta_{j}\right)_{j=1}^{t-1}$.

Without any restriction on the delay distributions, there may not be any reward within time $T$ and thus no structure of the rewards can be exploited. Thus, we need to have some guarantee that the delays do not escape to infinity. An appropriate formalization of this idea is achieved using the following tightness assumption.

Assumption: The family of distributions $\left(\mathbb{E}_{\mathcal{T}}\left(\Delta_{t}\right)\right)_{t=1}^{\infty}$ is tight.

Recall that a family $\left(\delta_{i}\right)_{i \in I}$ is called tight if and only if for every positive real number $\epsilon$, there is an integer $j_{\epsilon}$ such that $\delta_{i}\left(\left\{x \geq j_{\epsilon}\right\}\right) \leq \epsilon$, for all $i \in I$. (See e.g. Billingsley (1995))

Remark 1. If $\mathcal{T}$ is tight, then $\left(\mathbb{E}_{\mathcal{T}}\left(\Delta_{t}\right)\right)_{t=1}^{\infty}$ is trivially tight. Note that if $\mathcal{T}$ is finite, then it is tight. Similarly, if $\left(\mathbb{E}_{\mathcal{T}}\left(\Delta_{t}\right)\right)_{t=1}^{\infty}$ is constant and therefore only takes one value, then it is tight. As a special case, if $\left(\Delta_{t}\right)_{t=1}^{\infty}$ is identically distributed, then $\left(\mathbb{E}_{\mathcal{T}}\left(\Delta_{t}\right)\right)_{t=1}^{\infty}$ is constant and therefore tight.

To quantify the tightness of a family of probability distribution, we define the notion of upper tail bound.

Definition 1. Let $\left(\delta_{i}\right)_{i \in I}$ be a family of probability distributions over the set of non-negative integers. We say $\delta$ is an upper tail bound for this family if

$$
\delta_{i}(\{x \geq j\}) \leq \delta(\{x \geq j\})
$$

for all $i \in I$ and $j \geq 0$.

In the following result, we show that the tightness and the existence of upper tail bounds are equivalent.

Lemma 1. Let $\left(\delta_{i}\right)_{i \in I}$ be a family of probability distributions over the set of non-negative integers. Then this family is tight, if and only if it has an upper tail bound.

Proof. The detailed proof is provided in Appendix A.

A tail upper bound allows us to estimate and bound the effect of past actions on the current observed reward. More precisely, the effect of an action taken at time $i$ on the observer reward at $t$ is proportional to $\Delta_{i}(t-i)$, which can be bounded in expectation by $\tau$.

$$
\mathbb{E}_{\mathcal{T}}\left(\Delta_{i}(t-i)\right) \leq \mathbb{E}_{\mathcal{T}}\left(\Delta_{i}(\{x \geq t-i\})\right) \leq \tau(\{x \geq t-i\})
$$

As we will see, only the expected value of the upper tail bound appears in the regret bound.

### 3.2 Unbounded Stochastic Conditionally Independent Delay

In the unbounded stochastic conditionally independent delay case, we assume that there is a family of random delay distributions $\left\{\Delta_{t, S}\right\}_{t \geq 1, S \in \mathcal{S}}$ such that for any $S \in \mathcal{S}$, the sequence $\left(\Delta_{t, S}\right)_{t=1}^{\infty}$ is pair-wise independent and

$$
X_{t}=\sum_{i=1}^{t} F_{i}\left(S_{i}\right) \Delta_{i, S_{i}}(t-i)
$$

We call this feedback model composite anonymous unbounded stochastic conditionally independent delay feedback.

In this case the delay $\Delta_{t}=\Delta_{t, S_{t}}$ can depend on the action $S_{t}$, but conditioned on the current action, it is independent of (some of the) other conditional delays.

In this setting, similar to the stochastic independent delay setting, we assume that the sequence $\left\{\mathbb{E}_{\mathcal{T}}\left(\Delta_{t, S}\right)\right\}_{t \geq 1, S \in \mathcal{S}}$ is tight.

Remark 2. In previously considered stochastic composite anonymous feedback models (e.g. Wang et al. (2021); Garg \& Akash (2019)), the delay distribution is independent of time. In other words, every action $S$ has a corresponding random delay distribution $\Delta_{S}$, and the sequence $\left(\Delta_{t, S}\right)_{t=1}^{\infty}$ is independent and identically distributed. Therefore, the number of distributions in the set $\left\{\mathbb{E}_{\mathcal{T}}\left(\Delta_{t, S}\right)\right\}_{t \geq 1, S \in \mathcal{S}}$ is less than or equal to the number of arms, which is finite. Hence the family $\left\{\mathbb{E}_{\mathcal{T}}\left(\Delta_{t, S}\right)\right\}_{t \geq 1, S \in \mathcal{S}}$ is tight.

### 3.3 Bounded Adversarial Delay

In the bounded adversarial delay case, we assume that there is an integer $d \geq 0$ such that for all $\delta \in \mathcal{T}$, we have $\delta(\{x>d\})=0$. Here we have

$$
X_{t}=\sum_{i=\max \{1, t-d\}}^{t} F_{i}\left(S_{i}\right) \delta_{i}(t-i)
$$

where $\left(\delta_{t}\right)_{t=1}^{\infty}$ is a sequence of distributions in $\mathcal{T}$ chosen by the environment. Here we used $\delta$ instead of $\Delta$ to emphasize the fact that these distributions are not chosen according to some random variable with desirable properties. In fact, the environment may choose $\delta_{t}$ non-obliviously, that is with the full knowledge of the history up to the time-step $t$. We call this feedback model composite anonymous bounded adversarial delay feedback.

## 4 Algorithm

For the algorithm design, we use the algorithm Explore-Then-Commit-Greedy (ETCG) algorithm, as proposed in Nie et al. (2022). We start with $S^{(0)}=\emptyset$ in phase $i=0$. In each phase $i \in\{1, \cdots, k\}$, we go over the list of all base arms $\Omega \backslash S^{(i-1)}$. For each such base arm, we take the action $S^{(i-1)} \cup\{a\}$ for $m$ times and store the empirical mean in the variable $\bar{X}_{i, a}$. Afterwards, we let $a_{i}$ to be the base arm which corresponded to the highest empirical mean and define $S^{(i)}:=S^{(i-1)} \cup\left\{a_{i}\right\}$. After the end of phase $k$, we keep taking the action $S^{(k)}$ for the remaining time. The algorithm is summarized in Algorithm 1.

```
Algorithm 1 ETCG algorithm
Input: Set of base arms $\Omega$, horizon $T$, cardinality constraint $k$
Assumption: $n \leq T$
    $S^{(0)} \leftarrow \emptyset, n \leftarrow|\Omega|$
    $m \leftarrow\left\lceil(T / n)^{2 / 3}\right\rceil$
    for phase $i \in\{1,2, \cdots, k\}$ do
        for arm $a \in \Omega \backslash S^{(i-1)}$ do
            Play $S^{(i-1)} \cup\{a\}$ arm $m$ times
            Calculate the empirical mean $\bar{x}_{i, a}$
        end for
        $a_{i} \leftarrow \operatorname{argmax}_{a \in \Omega \backslash S^{(i-1)}} \bar{x}_{i, a}$
        $S^{(i)} \leftarrow S^{(i-1)} \cup\left\{a_{i}\right\}$
    end for
    for remaining time do
        Play action $S^{(k)}$
    end for
```


## 5 Regret Analysis

In this section, we provide the main results of the paper that shows the regret bound of Algorithm 1 with delayed composite feedback for different feedback models.

We define two main events that control the delay and the randomness of the observation. Let

$$
I=\left\{(i, a) \mid 1 \leq i \leq k, a \in \Omega \backslash S^{i-1}\right\}
$$

and define

$$
\mathcal{E}:=\left\{\left|\bar{F}_{i, a}-f\left(S^{i-1} \cup\{a\}\right)\right| \leq \operatorname{rad} \mid(i, a) \in I\right\}
$$

and

$$
\mathcal{E}_{d}^{\prime}:=\left\{\left.\left|\bar{F}_{i, a}-\bar{X}_{i, a}\right| \leq \frac{2 d}{m} \right\rvert\,(i, a) \in I\right\}
$$

where $\operatorname{rad}=\sqrt{\frac{\log (T)}{m}}$ and $d>0$ is a real number that will be specified separately in each setting. We may drop the subscript $d$ when it is clear from the context. When $\mathcal{E}$ happens, the average observed reward associated with each arm stays close its expectation, which is the value of the submodular function. When $\mathcal{E}_{d}^{\prime}$ happens, the average observed reward for each arm remains close to the average total reward associated with playing that arm. Using arguments similar to the ones used in Nie et al. (2022), we prove that

Theorem 1. For all $d>0$, we have

$$
\begin{aligned}
\mathbb{E}(\mathcal{R}) & \leq m n k+2 k T \mathrm{rad}+\frac{4 k T d}{m} \\
& +2 n k T \exp \left(-2 m \mathrm{rad}^{2}\right)+T\left(1-\mathbb{P}\left(\mathcal{E}_{d}^{\prime}\right)\right)
\end{aligned}
$$

See Appendix B for a detailed proof.

To obtain the regret bounds for different settings, we only need to find lower bounds for $\mathbb{P}\left(\mathcal{E}_{d}^{\prime}\right)$ and use Theorem 1 .

Theorem 2 (Bounded Adversarial Delay). If the delay is uniformly bounded by $d$, then we have

$$
\mathbb{E}(\mathcal{R})=O\left(k n^{1 / 3} T^{2 / 3}(\log (T))^{1 / 2}\right)+O\left(k n^{2 / 3} T^{1 / 3} d\right)
$$

Proof. The detailed proof is provided in Appendix C. Here, we describe the proof outline. In this setting, there is an integer $d \geq 0$ such that $\delta_{t}(\{x>d\})=0$, for all $t \geq 1$. Therefore, for any $m$ consecutive time-steps $t_{i, a} \leq t \leq t_{i, a}^{\prime}$, the effect of delay may only be observed in the first $d$ and the last $d$ time-steps. It follows that

$$
\left|\sum_{t=t_{i, a}}^{t_{i, a}^{\prime}} X_{t}-\sum_{t=t_{i, a}}^{t_{i, a}^{\prime}} F_{t}\right| \leq 2 d
$$

for all $(i, a) \in I$. Therefore, in this case, we have $\mathbb{P}\left(\mathcal{E}_{d}^{\prime}\right)=1$. Note that we are not making any assumptions about the delay distributions. Therefore, the delay may be chosen by an adversary with the full knowledge of the environment, the algorithm used by the agent and the history of actions and rewards. Plugging this in the bound provided by Theorem 1 completes the proof.

Theorem 3 (Stochastic Independent Delay). If the delay sequence is stochastic and independent and tight in expectation, then we have

$$
\mathbb{E}(\mathcal{R})=O\left(k n^{1 / 3} T^{2 / 3} \log (T)\right)+O\left(k n^{2 / 3} T^{1 / 3} \mathbb{E}(\tau)\right)
$$

where $\tau$ is an upper tail bound for $\left\{\mathbb{E}_{\mathcal{T}}\left(\Delta_{t}\right)\right\}_{t=1}^{\infty}$.

Proof. The detailed proof is provided in Appendix D. Here, we describe the proof outline. We start by defining the random variables

$$
C_{i, a}=\sum_{j=1}^{t_{i, a}^{\prime}} \Delta_{j}\left(\left\{x>t_{i, a}^{\prime}-j\right\}\right)
$$

for all $(i, a) \in I$. This random variable measure the effect of actions taken up to $t_{n, i}^{\prime}$ on the observed rewards after $t_{n, i}^{\prime}$. In fact, we will see that $m\left|\bar{X}_{i, a}-\bar{F}_{i, a}\right|$ may be bounded by the sum of two terms. One $C_{i, a}$ which bounds the amount of reward that "escapes" from the time interval $\left[t_{i, a}, t_{i, a}^{\prime}\right]$. The second one $C_{i^{\prime}, a^{\prime}}$, where $\left(i^{\prime}, a^{\prime}\right)$ corresponds to the action taken before $S^{i-1} \cup\{a\}$. This bound corresponds to the total of reward of the past actions that is observed during $\left[t_{i, a}, t_{i, a}^{\prime}\right]$. Therefore, in order for the event $\mathcal{E}_{d}^{\prime}$ to happen, it is sufficient to have $C_{i, a} \leq d$, for all $(i, a) \in I$. Since $C_{i, a}$ is a sum of independent random variables, we may use Bernstein's inequality to see that

$$
\mathbb{P}\left(C_{i, a}>\mathbb{E}\left(C_{i, a}\right)+\lambda\right) \leq \exp \left(-\frac{\lambda^{2}}{2(\mathbb{E}(\tau)+\lambda / 3)}\right)
$$

It follows from the definition that $\mathbb{E}\left(C_{i, a}\right) \leq \mathbb{E}(\tau)$. Therefore, by setting $d=\mathbb{E}(\tau)+\lambda$, we get

$$
\mathbb{P}\left(C_{i, a}>d\right) \leq \exp \left(-\frac{\lambda^{2}}{2(\mathbb{E}(\tau)+\lambda / 3)}\right)
$$

Therefore, we have

$$
\begin{aligned}
\mathbb{P}\left(\mathcal{E}_{d}^{\prime}\right) & \geq \mathbb{P}\left(\bigcap_{i, a}\left\{C_{i, a} \leq d\right\}\right) \\
& =1-\mathbb{P}\left(\bigcup_{i, a}\left\{C_{i, a}>d\right\}\right) \\
& \geq 1-\sum_{i, a} \mathbb{P}\left(\left\{C_{i, a}>d\right\}\right) \\
& \geq 1-\sum_{i, a} \exp \left(-\frac{\lambda^{2}}{2(\mathbb{E}(\tau)+\lambda / 3)}\right) \\
& \geq 1-n k \exp \left(-\frac{\lambda^{2}}{2(\mathbb{E}(\tau)+\lambda / 3)}\right)
\end{aligned}
$$

Plugging this in Theorem 1 and choosing appropriate $\lambda$ gives us the desired result.

Theorem 4 (Stochastic Conditionally Independent Delay). If the delay sequence is stochastic, conditionally independent and tight in expectation, then we have

$$
\mathbb{E}(\mathcal{R})=O\left(k^{2} n^{4 / 3} T^{2 / 3} \log (T)\right)+O\left(k^{2} n^{5 / 3} T^{1 / 3} \mathbb{E}(\tau)\right)
$$

where $\tau$ is an upper tail bound for $\left(\mathbb{E}_{\mathcal{T}}\left(\Delta_{t, S}\right)\right)_{t \geq 1, S \in \mathcal{S}}$.

Proof. The detailed proof is provided in Appendix D and is similar to the proof of Theorem 3 . The main difference is that here we define

$$
C_{i, a}^{\prime}=\sum_{j=t_{i, a}}^{t_{i, a}^{\prime}} \Delta_{j}\left(\left\{x>t_{i, a}^{\prime}-j\right\}\right)
$$

instead of $C_{i, a}$. Note that the sum here is only over the time-steps where the action $S^{i-1} \cup\{a\}$ is taken. Therefore $C_{i, a}^{\prime}$ is the sum of $m$ independent term. On the other hand, when we try to bound $m\left|\bar{X}_{i, a}-\bar{F}_{i, a}\right|$, we decompose it into the amount of total reward that "escapes" from the time interval $\left[t_{i, a}, t_{i, a}^{\prime}\right]$ and the contribution of all the time intervals of the form $\left[t_{i^{\prime}, a^{\prime}}, t_{i^{\prime}, a^{\prime}}^{\prime}\right]$ in the past. Since the total number of such intervals is bounded by $n k$, here we find the probability that $C_{i, a}^{\prime} \leq \frac{2 d}{n k}$ instead of $C_{i, a} \leq d$ as we did in the proof of Theorem3. This is the source of the multiplicative factor of $n k$ which appears behind the regret bound of this setting when compared to the stochastic independent delay setting.

We note that (Nie et al. 2023) provided a generalized framework for combinatorial bandits with full bandit feedback, where under a robustness guarantee, explore-then-commit (ETC) based algorithm have been used to get provable regret guarantees. More precisely, let $\mathcal{A}$ be an algorithm for the combinatorial optimization problem of maximizing a function $f: \mathcal{S} \rightarrow \mathbb{R}$ over a finite domain $\mathcal{S} \subseteq 2^{\Omega}$ with the knowledge that $f$ belongs to a known class of functions $\mathcal{F}$. for any function $\hat{f}: \mathcal{S} \rightarrow \mathbb{R}$, let $\mathcal{S}_{\mathcal{A}, \hat{f}}$ denote the output of $\mathcal{A}$ when it is run with $\hat{f}$ as its value oracle. The algorithm $\mathcal{A}$ called $(\alpha, \delta)$-robust if for any $\epsilon>0$ and any function $\hat{f}$ such that $|f(S)-\hat{f}(S)|<\epsilon$ for all $S \in \mathcal{S}$, we have

$$
f\left(S_{\mathcal{A}, \hat{f}}\right) \geq \alpha f\left(S^{*}\right)-\delta \epsilon
$$

It is shown in Nie et al. (2023) that if $\mathcal{A}$ is $(\alpha, \delta)$-robust, then the C-ETC algorithm achieves $\alpha$-regret bound of $O\left(N^{1 / 3} \delta^{2 / 3} T^{2 / 3}(\log (T))^{1 / 2}\right)$, where $N$ is an upper-bound for the number of times $\mathcal{A}$ queries the value oracle. In this work, we show that the result could be extended directly with delayed composite anonymous bandit feedback. The detailed results are given in Appendix F. This shows that the proposed approach in this paper that deals with feedback could be applied on wide variety of problems. The problems that satisfy the robustness guarantee include submodular bandits with knapsack constraints, non-submodular bandits, and reserve pricing.

## 6 Experiments

In our experiments, we consider two classes of submodular functions and and four types of delay.

## Submodular functions:

(F1) Linear:

Here we assume that $f$ is a linear function of the individual arms. In particular, for a function $g: \Omega \rightarrow[0,1]$, we define

$$
f(S):=\frac{1}{k} \sum_{a \in S} g(a)
$$

More specifically, we let $n=20$ and $k=4$ and choose $g(a)$ uniformly from $[0.1,0.9]$, for all $a \in \Omega$ and define $F(S):=\frac{1}{k} \sum_{a \in S} g(a)+N^{c}(0,0.1)$ where $N^{c}(0,0.1)$ is the truncated normal distribution with mean 0 and standard deviation 0.1 , truncated to the interval $[-0.1,1.0]$.

(F2) Weight Cover:

Here we assume that $\left(C_{j}\right)_{j \in J}$ is a partition of $\Omega$ and there is a weight function $w_{t}: J \rightarrow[0,1]$. Then $f_{t}(S)$ is the sum of the weights of the the indexes $j$ where $C_{j} \cap S \neq \emptyset$, divided by $k$. In other words, if 1 is the indicator function, then

$$
f_{t}(S):=\frac{1}{k} \sum_{j \in J} w_{t}(j) 1_{S \cap C_{j} \neq \emptyset}
$$

More specifically, we let $n=20$ and $k=4$. We divide $\Omega$ into 4 categories of sizes $6,6,6,2$ and let $w_{t}(j)=U([0, j / 5])$ be samples uniformly from $[0, j / 5]$ for $j \in 1,2,3,4$.

Stochastic set cover may be viewed as a simple model for product recommendation. Assume $n$ is the number of the products and each product belongs to exactly one of $c$ categories. Then the reward will be equal to the sum of the weights of the categories that have been covered by the user divided by $k$.

## Delay settings:

(D1) No Delay

(D2) (Stochastic Independent Delay) For all $t \geq 1$ and $i \geq 0, \Delta_{t}(i)=\left(1-X_{t}\right) X_{t}^{i}$, where $\left(X_{t}\right)_{t=1}^{\infty}$ is an i.i.d sequence of random variables with the uniform distribution $U([0.5,0.9])$. The reward for time-step $t$ will be distributed over $[t, \infty)$ according to $\Delta_{t}$. In this example $\left(\Delta_{t}\right)_{t=1}^{\infty}$ is independent.

(D3) (Stochastic Independent Delay) For all $t \geq 0, \Delta_{t}$ is a distribution over $[10,30]$ is sampled uniformly from the probability simplex using the flat Dirichlet distribution. The reward for time-step $t$ will be distributed over $[t+10, t+30]$ according to $\Delta_{t}$. In this example $\left(\Delta_{t}\right)_{t=1}^{\infty}$ is independent.

(D4) (Stochastic Conditionally Independent Delay) For all $t \geq 1$ and $i \geq 0$, we have $\Delta_{t}(i)=\left(1-Y_{t}\right) Y_{t}^{i}$, where $Y_{t}=0.5+f_{t} * 0.4 \in[0.5,0.9]$. The reward for time-step $t$ will be distributed over $[t, \infty)$ according to $\Delta_{t}$. In this example $\left(\Delta_{t, S}\right)_{t \geq 1, S \in \mathcal{S}}$ is independent.

(D5) (Stochastic Conditionally Independent Delay) At each time-step $t$, a number $l_{t}$ is chosen from $[10,30]$ according to the following formula.

$$
l_{t}=\left\lfloor 20 f_{t}\right\rfloor+10
$$

The reward for time-step $t$ will be observed at $t+l_{t}$. In other words, the higher the reward, the more it will be delayed. In this example $\left(\Delta_{t, S}\right)_{t \geq 1, S \in \mathcal{S}}$ is independent.

(D6) (Adversarial Delay) Let $l_{1}=15$ and for all $t>1$, define $l_{t}$ according to the following formula

$$
l_{t}=\left\lfloor 20 x_{t-1}\right\rfloor+10
$$

In other words, the higher the previous observation, the more the current reward will be delayed.

## Baselines:

We use three algorithms designed for CMAB with full-bandit feedback without delay and and algorithm designed for MAB with composite anonymous feedback as the baseline.

- CMAB-SM Agarwal et al. (2022) This algorithm assumes the expected reward functions are Lipschitz continuous of individual base arm rewards. CMAB-SM has a theoretical 1-regret guarantee of $\tilde{O}\left(T^{2 / 3}\right)$ with the assumption that if arm $a$ is better than arm $b$, then replacing $b$ by $a$ in any set not including $a$ will give better reward function.

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=1661&width=1661&top_left_y=232&top_left_x=232)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=336&width=491&top_left_y=244&top_left_x=248)

(a) (F1)-(D1)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=341&width=507&top_left_y=651&top_left_x=234)

(d) (F1)-(D4)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=339&width=504&top_left_y=1061&top_left_x=236)

(g) (F2)-(D1)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=337&width=504&top_left_y=1474&top_left_x=236)

(j) (F2)-(D4)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=339&width=507&top_left_y=240&top_left_x=814)

(b) (F1)-(D2)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=339&width=493&top_left_y=649&top_left_x=813)

(e) (F1)-(D5)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=333&width=491&top_left_y=1064&top_left_x=814)

(h) (F2)-(D2)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=331&width=488&top_left_y=1477&top_left_x=816)

(k) (F2)-(D5)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=333&width=488&top_left_y=243&top_left_x=1382)

(c) (F1)-(D3)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=336&width=491&top_left_y=653&top_left_x=1381)

(f) (F1)-(D6)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=333&width=491&top_left_y=1064&top_left_x=1381)

(i) (F2)-(D3)

![](https://cdn.mathpix.com/cropped/2024_06_04_f712b86c165ebcb8ff02g-10.jpg?height=333&width=488&top_left_y=1476&top_left_x=1382)

(1) (F2)-(D6)

Figure 1: For submodular functions F1-F2, delay settings D1-D6 and horizon $T=\left\{10^{2}, 10^{3}, 10^{4}, 10^{5}\right\}$, we run each algorithm 10 times. This plot shows the average cumulative 1-regret over horizon for each setting in the log-log scale. The dashed lines are $y=a T^{2 / 3}$ for different values of $a$. As we see, ETCG outperforms almost every baselines.

- DART Agarwal et al. (2021) This algorithm assumes the expected reward functions are Lipschitz continuous of individual base arm rewards and the reward functions have an additional property related to the marginal gains of the base arms. DART has a theoretical 1-regret guarantee of $\tilde{O}\left(T^{1 / 2}\right)$ with the assumption that if arm $a$ is better than arm $b$, then replacing $b$ by $a$ in any set not including $a$ will give better reward function.
- $\mathbf{O G}^{o}$ Streeter et al. (2009) This algorithm is designed for oblivious adversarial setting with submodular rewards. Therefore the sequence of monotone and submodular functions is fixed in advance. It has an $(1-1 / e)$-regret guarantee of $\tilde{O}\left(T^{2 / 3}\right)$.
- ARS-UCB Wang et al. (2021) This algorithm is designed for MAB with composite anonymous delayed feedback. The delay model is a special case of unbounded stochastic conditionally independent composite anonymous feedback delay that we described. However, they assume that $\Delta_{t, S}$ does not depend on time. For
our experiments, we use all subsets of $\Omega$ of size $k$ as the set of arms. ARS-UCB has a 1-regret guarantee of $\tilde{O}\left(\binom{n}{k}^{1 / 2} T^{1 / 2}\right)$ plus a constant term that depends on delay and the number of its arms.


## Experiment details and results:

For both setups, we use $n=20$ base arms and cardinality constraint $k=4$. The details of the choice of the submodular functions and delay settings are explained above. We run each experiment for different time horizons $T=\left\{10^{2}, 10^{3}, 10^{4}, 10^{5}\right\}$. For each horizon, we run the experiment 10 times. As we see, ETCG outperforms almost all other baselines. The linear submodular function satisfies the conditions under which DART and CMAB-SM were designed. However, the weighed cover function does not satisfy such conditions and therefore more difficult for those algorithms to run. In both cases, we see that any kind of delay worsens the performance of DART and CMAB-SM compared to ETCG. $\mathrm{OG}^{o}$ explores actions (including those with cardinality smaller then $k$ ) with a constant probability, which could account for its lower performance compared to ETCG, DART, and CMAB-SM.

While ARS-UCB does not perform well in these experiments, it should be noted that, given enough time, it should outperform ETCG. Also note that ARS-UCB has a linear storage complexity with respect to its number of arms. This translates to an $O\left(\binom{n}{k}\right)$ storage complexity in the combinatorial setting. Therefore, even for $n=50$ and $k=25$, it would require hundreds of terabytes of storage to run. In these experiments, we have $n=20$ and $k=4$, so it has only $\binom{20}{4}=4845$ arms.

## 7 Conclusion

This paper considered the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and delayed composite anonymous bandit feedback. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. The regret bounds demonstrate an additive impact of delay in the regret term.

This work provides the first regret bound results for CMAB with monotone and submodular rewards and delayed feedback. We note that the proof techniques and feedback models can be applied in other setups in CMAB, and exploring the generality of the proof techniques to other CMAB setups (e.g., continuous DR-submodular maximization) is an important direction.

## References

Agarwal, M., Aggarwal, V., Umrawal, A. K., and Quinn, C. Dart: Adaptive accept reject algorithm for non-linear combinatorial bandits. Proceedings of the AAAI Conference on Artificial Intelligence, 35(8):6557-6565, May 2021.

Agarwal, M., Aggarwal, V., Umrawal, A. K., and Quinn, C. J. Stochastic top k-subset bandits with linear space and non-linear feedback with applications to social influence maximization. ACM/IMS Transactions on Data Science (TDS), 2(4):1-39, 2022.

Billingsley, P. Probability and Measure. Wiley Series in Probability and Statistics. Wiley, 1995. ISBN 9780471007104.

Cesa-Bianchi, N., Gentile, C., and Mansour, Y. Nonstochastic bandits with composite anonymous feedback. In Bubeck, S., Perchet, V., and Rigollet, P. (eds.), Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pp. 750-773. PMLR, 06-09 Jul 2018.

Chen, L., Harshaw, C., Hassani, H., and Karbasi, A. Projection-free online optimization with stochastic gradient: From convexity to submodularity. In International Conference on Machine Learning, pp. 814-823. PMLR, 2018.

Chen, W., Yuan, Y., and Zhang, L. Scalable influence maximization in social networks under the linear threshold model. In 2010 IEEE international conference on data mining, pp. 88-97. IEEE, 2010.

Chen, W., Lu, W., and Zhang, N. Time-critical influence maximization in social networks with time-delayed diffusion process. In Twenty-Sixth AAAI Conference on Artificial Intelligence, 2012.

Dani, V., Hayes, T. P., and Kakade, S. M. Stochastic linear optimization under bandit feedback. In Annual Conference Computational Learning Theory, 2008.

Domingos, P. and Richardson, M. Mining the network value of customers. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 57-66, 2001.

Feige, U. A threshold of $\ln \mathrm{n}$ for approximating set cover. J. ACM, 45(4):634-652, jul 1998. ISSN 0004-5411. doi: 10.1145/285055.285059.

Garg, S. and Akash, A. K. Stochastic Bandits with Delayed Composite Anonymous Feedback, October 2019. arXiv:1910.01161 [cs, stat].

Iribarren, J. L. and Moro, E. Impact of human activity patterns on the dynamics of information diffusion. Physical review letters, 103(3):038702, 2009.

Joulani, P., Gyorgy, A., and Szepesvari, C. Online learning under delayed feedback. In Dasgupta, S. and McAllester, D. (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 1453-1461, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.

Karsai, M., Kivelä, M., Pan, R. K., Kaski, K., Kertész, J., Barabási, A.-L., and Saramäki, J. Small but slow world: How network topology and burstiness slow down spreading. Physical Review E, 83(2):025102, 2011.

Kempe, D., Kleinberg, J., and Tardos, É. Maximizing the spread of influence through a social network. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 137-146, 2003.

Lin, T., Li, J., and Chen, W. Stochastic online greedy learning with semi-bandit feedbacks. Advances in Neural Information Processing Systems, 28, 2015.

Nemhauser, G. L., Wolsey, L. A., and Fisher, M. L. An analysis of approximations for maximizing submodular set functions-i. Mathematical programming, 14:265-294, 1978.

Niazadeh, R., Golrezaei, N., Wang, J. R., Susan, F., and Badanidiyuru, A. Online learning via offline greedy algorithms: Applications in market design and optimization. In Proceedings of the 22nd ACM Conference on Economics and Computation, pp. 737-738, 2021.

Nie, G., Agarwal, M., Umrawal, A. K., Aggarwal, V., and Quinn, C. J. An explore-then-commit algorithm for submodular maximization under full-bandit feedback. In Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, pp. 1541-1551. PMLR, August 2022.

Nie, G., Nadew, Y. Y., Zhu, Y., Aggarwal, V., and Quinn, C. J. A framework for adapting offline algorithms to solve combinatorial multi-armed bandit problems with bandit feedback, 2023.

Pike-Burke, C., Agrawal, S., Szepesvari, C., and Grunewalder, S. Bandits with delayed, aggregated anonymous feedback. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4105-4113. PMLR, 10-15 Jul 2018.

Rejwan, I. and Mansour, Y. Top- $k$ combinatorial bandits with full-bandit feedback. In Algorithmic Learning Theory, pp. 752-776. PMLR, 2020.

Streeter, M., Golovin, D., and Krause, A. Online learning of assignments. In Bengio, Y., Schuurmans, D., Lafferty, J., Williams, C., and Culotta, A. (eds.), Advances in Neural Information Processing Systems, volume 22. Curran Associates, Inc., 2009.

Streeter, M. J. and Golovin, D. An online algorithm for maximizing submodular functions. In NIPS, 2008.

Sviridenko, M. A note on maximizing a submodular set function subject to a knapsack constraint. Operations Research Letters, 32(1):41-43, 2004.

Takemori, S., Sato, M., Sonoda, T., Singh, J., and Ohkuma, T. Submodular bandit problem under multiple constraints. In Conference on Uncertainty in Artificial Intelligence, pp. 191-200. PMLR, 2020.

Wang, S., Wang, H., and Huang, L. Adaptive algorithms for multi-armed bandit with composite and anonymous feedback. Proceedings of the AAAI Conference on Artificial Intelligence, 35(11):10210-10217, May 2021. doi: 10.1609/aaai.v35i11.17224.

Wong, R. C.-W., Fu, A. W.-C., and Wang, K. Mpis: Maximal-profit item selection with cross-selling considerations. In Third IEEE International Conference on Data Mining, pp. 371-378. IEEE, 2003.

Xiang, Y., Lan, T., Aggarwal, V., and Chen, Y. F. R. Joint latency and cost optimization for erasurecoded data center storage. ACM SIGMETRICS Performance Evaluation Review, 42(2):3-14, 2014.

Zhang, M., Chen, L., Hassani, H., and Karbasi, A. Online continuous submodular maximization: From full-information to bandit feedback. Advances in Neural Information Processing Systems, 32, 2019.

Zhu, J., Wu, Q., Zhang, M., Zheng, R., and Li, K. Projection-free decentralized online learning for submodular maximization over time-varying networks. The Journal of Machine Learning Research, 22(1):2328-2369, 2021.
