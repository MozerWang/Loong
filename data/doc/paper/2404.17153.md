# A Unified Debugging Approach via LLM-Based Multi-Agent Synergy 

Cheryl Lee*, Chunqiu Steven Xia ${ }^{\dagger}$, Jen-tse Huang*, Zhouruixin Zhu ${ }^{\ddagger}$, Lingming Zhang ${ }^{\dagger}$, and Michael R. Lyu*<br>*The Chinese University of Hong Kong. Email: cheryllee@link.cuhk.edu.hk, $\{$ jthuang, lyu\}@cse.cuhk.edu.hk<br>${ }^{\dagger}$ University of Illinois Urbana-Champaign. Email: \{chunqiu2, lingming\} @illinois.edu<br>${ }^{\ddagger}$ The Chinese University of Hong Kong, Shenzhen. Email: zhouruixingzhu@link.cuhk.edu.cn


#### Abstract

Tremendous efforts have been devoted to automating software debugging, a time-consuming process involving fault localization and repair generation. Recently, Large Language Models (LLMs) have shown great potential in automated debugging. However, we identified three challenges posed to traditional and LLM-based debugging tools: 1) the upstream imperfection of fault localization affects the downstream repair, 2) the deficiency in handling complex logic errors, and 3) the ignorance of program contexts. In this context, we propose the first automated, unified debugging framework, FixAgent, via LLM agent synergy. FixAgent can perform end-to-end localization, repair, and analysis of bugs. Our insight is that LLMs can benefit from general software engineering principles recognized by human developers in debugging, such as rubber duck debugging, enabling a better understanding of program functionality and logic bugs. Hence, we create three designs inspired by rubber ducking to address these challenges. They are LLM agent specialization and synergy, key variable tracking, and program context comprehension, which request LLMs to provide explicit explanations and force them to focus on crucial program logic information. Experiments on the widely used dataset QuixBugs show that FixAgent correctly fixes 79 out of 80 bugs, 9 of which have never been fixed. It also plausibly patches $1.9 \mathrm{X}$ more defects than the best-performing repair tool on Codeflaws, even with no bug location information and fewer than $0.6 \%$ sampling times. On average, FixAgent increases about $\mathbf{2 0 \%}$ plausible and correct fixes compared to its base model using different LLMs, showing the effectiveness of our designs. Moreover, the correctness rate of FixAgent reaches remarkably $\mathbf{9 7 . 2 6 \%}$, indicating that FixAgent can potentially overcome the overfitting issue of the existing approaches.


## I. INTRODUCTION

In an era where software systems are ubiquitous, permeating every facet of modern life, the inevitability of software bugs is a stark reality. These bugs, far from being mere nuisances, have the potential to cause catastrophic failures [1]. Identifying and rectifying these bugs falls upon developers, who must be ensnared in the time-consuming and complex process of debugging [2]. It is reported that developers usually spend over $50 \%$ of their programming time on debugging, and the cost of debugging amounts to billions of dollars per year [3].

The debugging demand calls for automated tools to relieve the manual burden. Automated debugging generally consists of two sequent stages: Fault Localization (FL) and Automated Program Repair (APR). FL aims to identify precise buggy statements and provide a ranked list of suspicious code lines, often the first step in debugging. Classic FL analyzes test outcomes to localize faults, using either statistical analysis [4][6] or mutation analysis [7]-[9] to qualify the faulty likelihood of code statements. Its effectiveness relies highly on humanwritten test cases and is thus variable. On the other hand, APR attempts to generate correct patches to replace faulty code segments. Traditional APR explores the space of possible patches [10]-[12] built by pre-defined fix patterns or synthesizes patches via symbolic execution [13], [14] based on human-written test cases. However, the search spaces may only contain very few correct patches [15], and such methods require extensive customization to re-implement fix patterns when transformed across different programming languages. Learning-based techniques have shown promise in both areas. Learning-based FL [16]-[19] models program behavior from source code, execution features, and test outcomes to localize bugs. Learning-based APR [20]-[22] often "translates" buggy code snippets into fixes via neural machine translation (NMT), despite its heavy reliance on high-quality bug-fix pairs for training or fine-tuning [23]. Large Language Models (LLMs) have been regarded as the most effective learning models for coding-related tasks, including debugging. A recent study [24] shows that directly applying LLMs can significantly outperform advanced APR techniques. Other LLM-based FL [19] and APR studies [25]-[28] also obtain promising results.

However, both traditional and LLM-based debugging tools still face three main challenges: 1) Imperfect fault localization Previous studies assume off-the-shelf FL tools perfectly identify bug locations, so APR should only patch the suspicious code statements. Yet, existing FL techniques show limited effectiveness in practice [29], [30], and the performance of APR could be largely biased by FL results [31]. 2) Struggling with complex logic bugs. Though LLMs have shown humanlike logic understanding abilities [32], they still struggle to repair complex logic errors, as debugging is a multi-step reasoning process, which is challenging for models that rely on pattern recognition rather than genuine thinking. When the program structure is complex or poorly documented, LLMs may perform even worse than small models [33]. 3) Context ignorance. Debugging demands understanding both the purpose and the broader operational context of a program, including intended outcomes and potential side effects. Most LLMs are trained on file-level source code, lacking the ability to analyze dependencies. Plus, LLM-based APR considers the code and test outcomes only, ignoring the informative program contexts, such as variable scopes, function definitions, and external libraries, limiting the debugging capability of LLMs.

Our insight: LLMs closely mimic developers when performing coding-related tasks, so they can benefit from general software engineering principles. We adopt the principle of rubber duck debugging (or rubber ducking), a debugging method where developers articulate their code in spoken natural languages, often line by line. Inspired by rubber ducking, we propose FixAgent, the first unified, automated debugging framework via LLM multi-agent synergy.

Specifically, we create three main designs to address the above challenges: 1) Specialized agent synergy. We first specialize two LLM agents to serve as a bug localizer and program repairer, respectively, to complete multi-stage debugging, followed by an extra LLM agent to analyze the bug-repair pair. Each agent explains its work to a "rubber duck" in detail. Their synergy delivers program repairs with explanations without any prior bug locations. In addition, the repairer may correct the mistakes made by the localizer, i.e.,, patching code statements beyond those identified by the localizer. If a plausible patch is generated and the changed code elements are different from the localization, FixAgent will adjust the localization results. Moreover, explaining to a rubber duck separately guides the agents in focusing on a specific task and better assists developers since over $85 \%$ of developers want to know the rationale behind automated debugging [34]. 2) Intermediate variable tracking. We prompt each agent to explicitly track key variables at critical points in the buggy program and discuss how such tracking guides their task completion. This strategy forces agents to analyze the code along the logic execution paths and provide more bug-oriented explanations. We do not require a line-by-line explanation as the original rubber ducking does because a buggy program (with its context) can be very long, which may degrade the performance of LLMs or even extend the window length that LLMs can handle [35]. 3) Program context construction. We construct the program context with respect to its specifications and dependencies. The context is provided along with the buggy program to FixAgent. Program specifications can include a functionality description, input/output format or examples, variable scopes, etc. Afterward, we parse the dependencies between files inside a repository to align with real-world projects. These two parts consist of the context. We also encourage agents to analyze the buggy program against the given context to obtain more attention to the context.

Experimental results demonstrate the superiority of FixAgent. We compare FixAgent against 16 baselines, including 10 state-of-the-art APR tools and 6 base LLMs. The comparison experiments are conducted on two widely used datasets, QuixBugs [36] and Codeflaws [37], written in three programming languages (C, Python, and Java). Overall, FixAgent patches 2780 bugs (passing all test cases) out of 3982 defects on these two datasets, with an estimated correctness rate of $96.5 \%$ on average. It outperforms all baselines significantly, even without any prior bug locations, whereas previous APR tools use ground-truth bug location information. For QuixBugs, it correctly fixes 79 out of 80 real-world bugs, including 10 bugs that have not been fixed before.
FixAgent fixes 24 more bugs than the best-performing APR baseline (AlphaRepair). Plus, FixAgent patches 586 defects than the best LLM competitor (GPT4) on Codeflaws. We also conduct extensive ablation studies on Codeflaws and a recently collected dataset, ConDefects (Python and Java), to mitigate the threat of data leakage in LLM training. FixAgent can fix 368 bugs out of 600 sampled bugs in ConDefects, with 375 plausibly patched. Results show that FixAgent performs well using various LLMs, not limited to its original setting (GPT4), and makes remarkable improvements compared to its base model. The studies further demonstrate that each design contributes to FixAgent positively.

Our main contributions are as follows:

- Direction: We are the first to propose that LLMs can benefit from a software engineering principle, rubber ducking, to enhance debugging performance.
- Approach: We designed the first automated, unified debugging framework based on LLM multi-agent synergy, inspired by rubber ducking. Our method can effectively produce repairs and explanations without knowing bug locations. The implementation is released at [38].
- Evalution: We conduct extensive experiments to evaluate FixAgent. The results show that FixAgent can significantly outperform baselines on widely used datasets.


## II. BACKGROUND

## A. Terminology

Software debugging is the process of tracking and fixing issues, such as bugs and vulnerabilities, where revising the source code is essential. Code debugging involves Fault Localization (FL), Automated Program Repair (APR), and possible post-error review. FL attempts to precisely identify buggy elements within a faulty program,through static or dynamic analysis. It calculates the probability of each code element being buggy to automatically produce a ranked list of suspicious code elements. Using this ranked list, APR then automatically patches the identified buggy code segments, consisting of patch generation and patch verification. A plausible patch can pass all human-written test cases, and it is correct when manually verified by developers.

Unified debugging [39] is a pioneering work that aims to better combine FL and APR. It leverages repair information to improve FL, believing that if a patch passes originally failing cases, its patch locations should be highly correlated with the groundtruth bug locations. Unified debugging highlights the important connection between APR and FL, thereby applying to our work. Differently, we aim to provide fixed programs using an end-to-end solution, where the FL and APR have an interactive but not determinative relationship. Such an architecture allows patching code elements beyond those localized by FL, and the FL results can also be adjusted based on repairs.

Rubber duck debugging (aka. rubber ducking) is a debugging method that forces developers to explain their code by speaking out their expectations and the real implementation to find the gap. The original rubber ducking requires explaining the code line by line, but following the essential
idea-breaking code into pieces and articulating them in natural languages-can be beneficial already.

## B. Large Language Models

Large Language Models (LLMs) have attained significant advancements in natural language processing, including text generation, conversational engagement, and logical reasoning [40]. LLMs is trained to predict tokens auto-regressively within a given textual context. This paradigm facilitates the unsupervised training on massive corpora of text sourced from the internet, removing the need for labeled datasets. In light of the remarkable success of general-purpose LLMs, Code LLMs have been extensively studied as code generation is exactly like text generation. These models are trained on code corpus (perhaps containing related natural languages). For example, DeepSeek-Coder [41] is trained on 2 trillion tokens crawled from GitHub and StackExchange, where $87 \%$ are code and $10 \%$ are code-related text.

LLMs are typically used with a prompt-an instruction to an LLM, initializing the LLM to perform inference and generate text until the model encounters a predetermined stop word or surpasses its designated maximum word limit. Through the deliberate construction of prompts, i.e., prompt engineering, researchers have harnessed the capabilities of models for a myriad of tasks without necessitating retraining or fine-tuning. In this paper, we adopt GPT4 [42] for automated debugging via prompt engineering. GPT4 is a state-of-the-art LLM with advanced understanding and reasoning capabilities in both natural languages and code.

## III. Motivation

This section motivates FixAgent, an LLM-based unified debugging framework empowered by rubber ducking. Our motivation centers on three challenges of existing debugging tools: imperfect fault localization, complex bug repair, and context ignorance, as introduced in $\S$ I Introduction.

## A. Imperfect Fault Localization

Imperfect FL results include wrong and missing locations. Such imperfection can considerably affect the downstream repair.= Firstly, FL tools can incorrectly identify non-buggy code as the source of the error, making the repair approach generate patches that are not only unnecessary but could also introduce new errors. As reported in [19], even state-of-the-art FL approaches suffer from low accuracy. Prior techniques can only achieve less than $22.3 \%$, and $46.3 \%$ real faulty statements are ranked as the top-1 and top- 5 suspicious ones, respectively, on the real-world dataset Defects4J V1.2.0 [43]. The result is far from satisfactory, and the patch space established based on such FL results can be problematic. The overwhelming majority of APR tools are confined to replacing the identified statements with those produced by APR, potentially assuming perfect localization is easily available. Such an assumption is unrealistic and affects the debugging performance.

Moreover, FL tools may miss faulty statements. First, most FL and APR tools assume that each program only contains one bug existing in an existing code statement [44]. This is because 1) identifying multi-line bugs and making edits at multiple and non-contiguous locations is especially challenging [45]; and 2) APR tools often rely on spectrum-based FL, which can only isolate single-line bugs [46]. Second, failing case-dependent FL cannot identify bugs caused by non-existing statements. Such methods count the execution times of a certain statement, so they have difficulties finding missing conditions. However, real-world bugs are complex and diverse, which may be multiline or missing-line, calling for more sophisticated debugging to mitigate the threat of localization imperfection.

## B. Complex Logic Bug Fixing

The emergence of LLMs shed light on a potential solution to debugging. A single LLM can already complete debugging. It simply regards debugging as producing tokens of a repair by calculating the probabilities from left to right, conditioned on the buggy program. A recent study comprehensively evaluates the debugging capability of LLMs [47]. It prompts LLMs to fix a buggy program without any other prior knowledge. Results show that the most advanced model, GPT4, can achieve comparable performance with humans on LeetCode, an online programming platform, submissions. However, LLMs still face huge difficulties in fixing logic errors, and even runtime information of failing cases is unhelpful for such errors.

```
Description (Codeforces 108C)
...Given the number of houses, pipes, and the diameter of
each pipe going from one house a to another, find the
maximal amount of water, where the pipe diameters limit
the water conveyed from a tank to its tap. Each house has
a tank or tap sending or receiving water, respectively.
Buggy program in Codeflaws
    int n, p; int tank[1001], tap[1001],d[1001];
    int process(){
        int i,j,count=0;
        if(p==0) return count
        for(i=1;i<=n;i++){
            if(tank[i]==0) count ++;
            j=i;
            while(tank[j]!=0){...
                if(j==i) break;}
            }
            return count
    }
int main(int argc, char *argv[]){...}
Repair made by GPT4, given bug locations -...
Correct repair 
```

Fig. 1: Complex bug fixing is still challenging for LLMs.

We also observe a similar phenomenon. Figure 1 displays a complex program with a logic bug and the wrong repair of GPT4. The wrong repair can pass most test cases but fail to calculate the number of tank-tap pairs given three houses and only one pipe, a corner case. The key to this problem is to construct a directional chain using nodes and edges to
represent the water-related objects, a thought of abstracting complex real-world systems into code. Such insight requirement goes beyond language understanding and is challenging to LLMs. Thus, simply applying LLMs cannot meet the growing demand for complex software debugging, motivating us to create designs that better unleash their capability.

## C. Context Ignorance

Successful debugging requires a deep understanding of not just the syntax but the semantic purpose of code. However, existing APR tools focus on source code and test outcomes only, ignoring the important program contexts such as the intended functionality, the data flow, and the expected behaviors. Figure 2 presents a repair generated by an advanced APR tool. This repair is also regarded as "ground-truth" provided by the dataset Codeflaws. Yet, it ignores the variable scope, so it failed when $m \geq 99$, where the array $a$ cannot hold the whole input sequence. If the condition is slightly and reasonably changed, for example, $m \leq 90$, the repair is correct. Without knowing the condition $m \leq 100$, even an omniscient APR tool has trouble producing a correct repair. This case highlights the importance of program contexts. Programming is a problemsolving task, so we should conduct context-aware program debugging for real-world software.

![](https://cdn.mathpix.com/cropped/2024_05_29_6da4d2236e9b08ced4d5g-04.jpg?height=981&width=613&top_left_y=1453&top_left_x=295)

Fig. 2: The repair made by an APR tool (also regarded as correct in the dataset) ignores the variable scope requirement.
Insight: LLMs have the potential for unified debugging but face challenges of bug localization imperfection, complex error fixing, and context ignorance. They motivate us to create designs to unleash the debugging capabilities of LLMs. We propose to adopt rubber ducking to boost LLMs just like developers.

## IV. MethodOLOGY

This section introduces the key ideas behind FixAgent, our LLM-based unified debugging framework. Figure 3 displays the overview of FixAgent, consisting of three specialized agents serving as bug localizer, patch generator, and posterror reviewer, respectively (§IV-A). Each agent employs the design of intermediate variable tracking (§IV-B) and receives the constructed program contexts ( $\S \mathrm{IV}-\mathrm{C}$ ). There is an extra LLM agent, crafter (§IV-D1), generating test inputs beyond the human-written test suite to mitigate the overfitting issue (i.e., generated patches by previous APR tools may not generalize on other test cases). If the generated patch is not plausible, we re-sample the fixer agent with feedback, which contains failing information (§IV-D2).

## A. Specialized Agent Synergy

We specialize three LLM agents, each responsible for a stage in debugging separately: fault localization (localizer), patch generation (repairer), and post-error analysis (revisitor). Localizer identifies faulty code statements. It can even point out missing statements and label them in the buggy program like "<INFILL> // missing this line causes a bug". Repairer aims to generate an executable and correct patch. Revisitor analyzes why the original code was buggy and the rationale behind the patch. The agents work sequentially. Each agent passes its response to the downstream agent, and responses from these agents make up the final response.

Each prompt is a triplet consisting of a role profile, program specifications, and instructions. First, each agent is promoted with a clear role profile for task-oriented role-playing. LLMs can act like an expected agent if given detailed role descriptions [48]. A role profile consists of an expert identification and an agent description. Expert identification determines the role of an agent, e.g., "You are an expert in identifying specific faulty code elements." The agent description introduces the specific task objective, e.g., "Your task is to fix buggy code snippets." The role profile is followed by program specifications, including the buggy program, failing test case information, program contexts, and the response from the previous agent(s), if applicable. For example, we prompt the revisitor with identified bug locations generated by the localizer and a repair produced by the repairer. Afterward, we underline the task objective and provide detailed step-by-step instructions. For example, the repairer has the objective of returning the patch with an explanation in the desired format. It should carry out a series of steps: context comprehending, program analysis (including variable tracking introduced in

![](https://cdn.mathpix.com/cropped/2024_05_29_6da4d2236e9b08ced4d5g-05.jpg?height=743&width=1721&top_left_y=141&top_left_x=191)

$\S$ IV-B) against the failing cases, making minimal changes for a patch, and double-checking the identified buggy statements.

The downstream agent relies on upstream results while influencing them in turn. If the repairer generates a plausible patch that changes code statements different from the identified ones, the localizer also adjusts its results based on such changes, though it can also present the original responses upon user demand. Similarly, the analyzer may contain a better patch to boost the repairer or wiser localization, enabling answer adjusting for the localizer. Besides, our pipeline enables FixAgent to handle multiple programs in parallel, like an assembly line. The final answer returned to the user comes from their synergy, consisting of the identified buggy statements, the patch, and the post-error analysis.

## B. Intermediate Variable Tracking

We prompt each agent to track critical intermediate variable values against failed test cases and compare them to expected outcomes. Such prompting is positioned in the instructions mentioned in the previous section. Each agent is requested to explicitly present such tracking in its response, accompanied by a comprehensive explanation of how it facilitates the derivation of its answer. This design is inspired by the rubber ducking, using explanations to enhance programming. Compared with requiring line-by-line explanations as the original rubber ducking does, our design prioritizes information with significant impact on the program's behavior, such as the core logic executions and states, helping LLM to concentrate on the parts that are most likely to influence the outcome, making it easier to identify and solve errors. It also corresponds to chain-of-thought (CoT) [32], whose idea is that decomposing a complex question into pieces can improve the reasoning capability of LLMs, and even simply adding "think it step by step" can lead to significant improvement. Our tracking design allows LLMs to decompose a complex program with multiple logic modules into several intermediate states and conduct extra calculations during debugging.
Moreover, this design enhances the transparency of LLM decision-making. We can see how an agent reaches its answer, enabling potential human interactions. For example, in a dynamic programming problem, developers can ask the model to focus on the state transfer equation and edge conditions. Even if the agent cannot eventually generate a correct repair, the thinking path will still provide insights to developers. Such interpretation also helps win the trust of developers.

## C. Context Construction

FixAgent mines two aspects of the program context, i.e., requirements and dependencies. First, for programs with detailed documentation, we adopt descriptions of the program functionality, input/output format, precision requirement, and other related information to clarify the expected behavior of the program. If the program implements a well-known algorithm without documentation, we request a general LLM (may not be the base model of FixAgent) to introduce the algorithm given its name (usually the function name in the program). The introduction serves as a requirement description. Second, we parse the dependencies of the buggy program and extract the code of these dependent files. The extracted code is put on to the top of the program. This operation, though simple, can work well by ensuring LLMs handles the dependent code first and then the buggy program, as most LLMs deal with tokens from left to right and are trained on file-level code. These two strategies can construct an informative context for the program.

## D. Secondary Designs

1) Test input generation: We specialize an extra agent, crafter, for test case generation beyond the human-written test cases. It is launched when a plausible patch is generated to mitigate the overfitting issue, i.e., plausible repairs generated by APR tools pass given test cases but cannot generalize on others, especially for test-dependent APR tools [49].

We also follow the 〈profile-specification-instruction〉 triplet format to prompt crafter. In the specification, the plausible repair replaces the buggy program. Plus, it contains some
given test cases to let crafter work in a few-shot manner because studies have shown that LLMs can perform new tasks well from a few examples [50]. In the instruction, crafter is asked to divergently think of diverse test cases that differ from given cases, aiming to reveal semantic flaws and boundary condition issues, especially prioritizing boundary values (maximum/minimum), repetitive inputs, and edge cases. crafter also explains the rationale behind test cases to a rubber duck. This guides crafter to mine diversified unusual inputs and encourages long test inputs. Specifically, LLMs have a maximum token limit for a query and response, but we desire to generate extremely long inputs to uncover potential issues. This can be achieved from the explanations. For example, an input " 100010000 \n 1000999 ... $1 \backslash n 12 \ldots 10000$ " is generated in an abbreviated form to save tokens and cannot be processed by the program. Still, we can extend it to a valid input with its explanation " 1000 employees, 10000 applications, ...."

Note that we need external efforts to calculate the expected outputs of the generated inputs to obtain test cases. We do not use LLMs for such calculation because they are not good at arithmetic problem-solving [40] and are unsuitable to serve as a standard criterion. In practice, developers can calculate the expected outputs for the insightful generated inputs.

2) Feedback-supported re-sampling: Errors are inevitable in debugging. We adopt a feedback-supported design to reduce wrong repairs. If FixAgent generates an implausible patch, we re-sample FixAgent to get another patch, prompted with feedback of failing test information in a conversational manner, inspired by [28]. Assume we allow for $m$ samples at most to fix a buggy program $c_{0}$. The $i$-th sampled patch is denoted by $c_{i}$, and $c_{k *}$ passes the most (but not all) test cases among $\left\{c_{1}, \ldots, c_{i<m}\right\}$. To generate $c_{i+1}$, we add $c_{k *}$ and its failing information $f_{k *}$ (i.e., feedback) to the specifications in the prompt. Thus, the prompt contains $\left[c_{0}, f_{0}, c_{k *}, f_{k *}\right]$ for each re-sampling time. After sampling $m$ times or obtaining a plausible repair, $c_{k *}$ that passes the most (or all) test cases serves as the final solution.

This design aligns FixAgent with previous APR tools, which usually sample thousands of candidate patches. Notably, our strategy is different from [28], which keeps one conversation going until reaching a plausible fix, so the prompt contains all repairs and their failing information. Instead, we only input one repair and its feedback for one sampling, avoiding the possibility of extending the token window of an LLM. Lastly, the best repair with bug-fix analysis will be returned to the user. Users can also request the specific response of any agent, including the corresponding answer and explanation.

## V. EVALUATION

- RQ1: How does FixAgent compare against state-of-the-art APR tools and base LLMs?
- RQ2: How capable is FixAgent in different LLMs and how does FixAgent make improvement on its base model?
- RQ3: How does each design contribute to FixAgent?


## A. Experiment Setup

We compare FixAgent with advanced APR approaches and general LLMs on public databases with pairs of bug-fix. The comparison refers to APR metrics, using the number of plausible patches and correct patches, as well as the correctness rate (the correct patch count over generated plausible patches) to gauge the effectiveness of FixAgent. This is because our work is the first unified debugging framework, and the eventual goal is to obtain a correct and executable repair.

1) Datasets: We use three datasets for evaluation.

- Codeflaws [37] consists of 3902 faulty programs (2952 oneline bugs) and their corresponding repairs written in $\mathrm{C}$ collected from Codeforces, an online programming platform. These bugs are classified into 40 classes, including control flow, data flow, function call, pointer, variable type, etc.
- QuixBugs [36] consists of 40 faulty programs in Java and Python. Each program implements a classic algorithm in one function with a one-line defect. The bugs involve incorrect operators, variables, and field dereferences, as well as missing conditions, arithmetic expressions, etc.
- ConDefects [51] consists of 1254 Java and 1625 Python faulty programs covering diverse task difficulties, sourced from AtCoder, an online programming competition platform. Each faulty program is paired with its repair.

The Codeflaws and QuixBugs are widely used in previous studies, whereas ConDefects is recently collected, making it impossible for direct data leakage in the training data of existing popular LLMs. Since no studies have reported experimental results on this new dataset and repairing all of the faulty programs is especially expensive, we sample 300 faulty programs for each programming language and only use them to compare FixAgent against base LLMs in RQ2 (§V-C).

2) Baselines: We compare FixAgent against 16 baselines, including 10 APR tools and 6 base LLMs. APR methods include three advanced C tools (Angelix [14], Prophet [52], SPR [53], CVC4 [54]), two genetic programming-based techniques (Semfix [13], GenProg [10], [11]), two recent NMTbased approaches (CoCoNuT [21], CURE [22]), and an LLMbased method (AlphaRepair [27]). We adopt the results reported by their original papers and follow-up surveys [24], [49], [55]. LLM baselines include three general-purpose models (Gemini [56], ChatGPT [57], GPT4 [42]) and three code LLMs (DeepSeek-Coder [41], CodeLlama [58], Codex [59]). We also use the reported results of Codex since OpenAI no longer supports it. Other LLM baselines are implemented via their official APIs, whose versions are Gemini 1.0-Pro, GPT-3.5-Turbo-0125 (about 175 billion parameters, i.e., 175B, for simplicity), GPT-4-0125-preview, DeepSeek-Coder-Base (33B), and Phind-CodeLlama-V2 (34B), respectively.
3) Implementations: A single base LLM performs the whole debugging process without our designs. To ensure a fair comparison, we replace the variable tracking prompt with "Think it step by step," a well-known prompt enhancing the reasoning ability of LLMs [32]. We also retain the program specifications when prompting the base models. Plus, FixAgent is implemented by Python 3.9 with 1500+ lines of code

TABLE I: Comparison with APR tools and base LLMs. \#Correct and \#Plausible represent the number of bugs correctly and plausibly patched, respectively. \#Patch/bug is the sampling number per bug. Cells filled in by $(x, y)$ represent $x$ patches out of $y$ sampled bugs. Other cells are obtained on the whole dataset.

| Fault <br> Localization | Tools | \#Patch/bug | Codeflaws-C (3902 bugs) |  | QuixBugs-Java (40 bugs) |  | QuixBugs-Python (40 bugs) |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | \#Correct | \#Plausible | \#Correct | \#Plausible | \#Correct | \#Plausible |
| Standard | Angelix [14] | 1000 | 318 | 591 | - | - | - | - |
|  | Prophet [52] | 1000 | 310 | 839 | - | - | - | - |
|  | SPR $[53]$ | 1000 | 283 | 783 | - | - | - | - |
|  | CVC4 [54] | - | $(15,665)$ | $(91,665)$ | - | - | - | - |
|  | Semfix $[13]$ | - | $(38,665)$ | $(56,655)$ | - | - | - | - |
| Perfect | GenProg [10], [11] | 1000 | $255-369$ | 1423 | 1 | 4 | - | - |
|  | RewardRepair $[60]$ | 200 | - | - | 20 | - | - | - |
|  | CoCoNuT [21] | 20000 | 423 | 716 | 13 | 20 | 19 | 21 |
|  | CURE [22] | 5,000 | - | - | 26 | 35 | - | - |
|  | AlphaRepair [27] | 5000 | - | - | 28 | 30 | 27 | 32 |
|  | Codex $[59]$ | 600 | - | - | 32 | 35 | 37 | 37 |
| None $^{\star}$ | CodeLlama [58] | 3 | $91^{\dagger}$ | 1353 | 25 | 28 | 33 | 33 |
|  | Gemini $[56]$ | 3 | $89^{\dagger}$ | 1186 | 29 | 32 | 29 | 35 |
|  | DeepSeek-Coder $[41]$ | 3 | $93^{\dagger}$ | 1741 | 30 | 34 | 25 | 38 |
|  | ChatGPT [57] | 3 | $\mathbf{9 4}^{\dagger}$ | 1927 | 33 | 34 | 34 | 36 |
|  | GPT4 [42] | 3 | $93^{\dagger}$ | 2115 | 35 | 36 | 39 | 39 |
|  | FixAgent | 3 | $93^{\dagger}$ | 2701 | 39 | 39 | 40 | 40 |

$\star$ For LLMs understanding both code and natural languages, no fault localization information is provided for unified debugging.
$\dagger$ We randomly select 100 plausible patches to check their correctness because of the huge number of plausible patches.

Following prior work [27], the generation uses the top-p nucleus sampling [61] with $p=1.0$ and temperature $=1$.

Previous APR usually generates hundreds to thousands of patches per bug, whereas we only sample at most three times per bug for LLM baselines due to the high cost of token generation. Comparing thousands of samples (APR studies) with only three samples (LLM baselines) seems unfair. Nevertheless, we follow the tradition of sampling only several times in code generation evaluation on LLMs [62], and the performance of LLMs is already incredible. For evaluation purposes, we manually check the plausible patches. A patch is correct only when semantically equivalent to its corresponding reference patch or official answer in the programming contest platform. We carefully check the correctness of each plausible patch of QuixBugs and 100 sampled plausible patches of Codeflaws in RQ1. We do not check all patches of Codeflaws because 1) LLMs and FixAgent generate too many (thousands of) plausible patches, so it is extremely time-consuming to check them one by one; and 2) without ground-truth fault locations, these LLMs rewrite programs and change code statements, for example, replacing multiple "if else" by "switch case", making it very different to manually check the correctness. Thus, we follow [21] to check sampled patches and report the correctness rate instead of the correct patch counts.

## B. RQ1: Overall Effectiveness of FixAgent

This section presents the overall comparison between FixAgent and baselines on Codeflaws and QuixBugs among C, Java, and Python. Since different APR approaches adopted different FL tools, we separate them according to the FL types adopted by their original papers, where standard FL refers to a traditional spectrum-based FL. Perfect FL assumes a APR tool knows the ground-truth bug locations, though it is unrealistic. Our work aims at an end-to-end solution without knowing the bug locations, so we do not provide FL results to our method and the base LLM baselines (i.e., None for the first column).

The results are shown in Table I. Overall, FixAgent plausibly patches 2780 out of 3982 bugs on Codeflaws and QuixBugs, performing the best compared to all baselines. In particular, on the Codeflaws dataset, FixAgent significantly outperforms existing APR methods and LLMs. It produces 1.9X plausible patches as the best APR method, GenProg. Moreover, it improves the correctness rate by $57.42 \%$ compared to that of CoCoNuT, which correctly fixes the most bugs among APR methods. We can also roughly estimate that FixAgent correctly fixes about 2512 bugs based on the correctness rate. Moreover, FixAgent outperforms all compared approaches on QuixBugs. It correctly fixes 39 bugs in QuixBugs-Java, 7 of which have never been fixed before. FixAgent correctly fixes all bugs in QuixBugs-Python, showing superiority over all baselines. Figure 4 displays an example bug of QuixBugs that is uniquely fixed by FixAgent. The bug wrongly returns an empty array, but it is desired as a nested empty array. It is difficult to fix by traditional and learning-based APR because it requires adding multi-line edits, which simple modifications cannot achieve, and goes beyond typical bug-fix templates. Since QuixBugs only contains very brief descriptions of the programs, the debugging method must abstract the desired returning format from other returning statements in the program or the failing information (if any), requiring a comprehensive overview and deep analysis of the program. In fact, even FixAgent generates the correct patch upon the original failing test that triggers returning an empty set nested with an array. Note that FixAgent fixes four extra
bugs in QuixBugs compared with its base model GPT4. We will discuss the reasons in RQ2 ( $\S \mathrm{V}-\mathrm{C}$ ).

```
- return new ArrayList();
+ ArrayList<ArrayList<Integer>> baseCase = new
ArrayList<>();
    baseCase.add(new ArrayList<>()); // Fixed: now
correctly returns a list containing an empty list
    return baseCase;
```

Fig. 4: Example of bug fixed by FixAgent in QuixBugs.

Furthermore, we find that LLMs achieve much better performance than traditional and NMT-based APR tools, with respect to both plausible patch generation and correctness rate, similar to previous findings [24]. LLMs achieve such results even without knowing fault locations, while APR baselines assume perfect FL or use a FL tool to facilitate repair generation. Even the worst-performing LLM baseline, Gemini, achieves comparable results to the best traditional tool, GenProg. We attribute this to the capabilities of LLMs in understanding the semantics and contexts of code. LLMs can leverage the knowledge learned from previous source code to understand the program semantics and requirements deeply, enabling them to fix not only syntactic errors but also semantic errors that previous approaches might miss, especially when the bug does not correspond to predefined rules or templates that most APR baselines rely on. Besides, LLMs more fully embody superiority on Codeflaws. This is because Codeflaws challenges require a deeper understanding of real-world contexts and conditions under which programs are written, demanding more context understanding. Every program in Codeflaws is written to solve a certain question described in natural languages connected with the real world, while QuixBugs consists of classic algorithms with one-line bugs that are more easily repaired by systematic modifications.

FixAgent performs the best among LLM-based methods. We attribute this to our designs. LLM competitors conduct the whole debugging (or patch generation of AlphaRepair) in one turn, while FixAgent divides this complex task into several steps and requires explicit explanations focusing on crucial information. We instantiate agents to conduct different stages of debugging separately. Each agent can focus on its own and produce better results, so their synergy delivers better debugging performance. In addition, we prompt the agents to pay attention to key variables. Intuitively, if the program's output does not conform to the expected, the bug usually appears before its final printing statement (though there are printing bugs, they are easy to fix), manifested by the intermediate values of variables, especially those in key logic expressions. Our prompt forces the agents to conduct reasoning along the program's logic execution path, making bugs easily revealed.

Note that the debugging results on QuixBugs among LLMs are very close, so we omit it in the following RQs. This is because 1) QuixBugs programs implement classic algorithms with single-line bugs that are relatively easy to fix, and 2)
LLMs may have been trained on these algorithm implementations, so they deliver remarkable results due to data leakage.

## C. RQ2: Using different LLMs with FixAgent

We evaluate the capability of FixAgent using different base LLMs on ConDefects, collected after the LLM releases, making it impossible to compose the training data, thereby avoiding data leakage. Table II shows the results, where FixAgent-[model] indicates we only replace the original GPT4 with $[$ model] without changing designs; $x / y$ in each cell denotes correctly fixing $x$ bugs and plausibly patching $y$ bugs. FixAgent can correctly fix 115-205 bugs and plausibly patch 124-207 bugs out of the sampled 300 Java bugs using different LLMs. It can also fix 89-163 and patch 99168 Python bugs. The results indicate that FixAgent can work effectively with various base LLMs, though influenced by their capabilities. FixAgent with a larger model should perform better in experience, but FixAgent-DeepSeek-Coder outperforms FixAgent-ChatGPT and FixAgent-Gemini, while Gemini and ChatGPT have much larger model sizes. We attribute this to the specialized training data and objective of DeepSeek-Coder. DeepSeek-Coder is a code LLM trained on a real-world code corpus, making it proficient at codingrelated tasks, including debugging. Though DeepSeek-Coder and CodeLlama are code LLM with similar model sizes, FixAgent-DeepSeek-Coder outperforms FixAgent-CodeLlama considerably. Their inherent coding ability gap can explain this. As reported in [41], DeepSeek-Coder performs much better than CodeLlama on solving programming contests. The debugging performance of LLM is strongly related to its coding ability, considering that debugging can be regarded as generating a correct program prompted by a buggy one.

TABLE II: Effectiveness of FixAgent using different LLMs.

| Models | ConDefects-Java <br> 300 bugs | ConDefects-Python <br> 300 bugs |
| :---: | :---: | :---: |
| FixAgent-CodeLlama | $115 / 124$ | $94 / 105$ |
| FixAgent-Gemini | $124 / 131$ | $89 / 99$ |
| FixAgent-DeepSeek-Coder | $161 / 165$ | $131 / 138$ |
| FixAgent-ChatGPT | $158 / 162$ | $152 / 154$ |
| FixAgent-GPT4 (original) | $205 / 207$ | $163 / 168$ |

We also evaluate how FixAgent improves its base model on Codeflaws, as shown in Table III, where we sample 300 bugs in this evaluation for efficiency. Compared to its base models, FixAgent plausibly patches 17-35 more bugs and correctly fixes 12-33 more bugs. Among all compared pairs, FixAgent improves the most on GPT4, increasing correctly and plausibly patched bugs by 33 and 35 , respectively. Overall, applying FixAgent can make a $20 \%$ improvement on the original LLM, on average. This enhancement highlights the effectiveness of our designs that significantly improve the debugging abilities of LLMs in a non-invasive manner without any re-training or fine-tuning, demonstrating our insight that LLMs can benefit from software principles, such as rubber ducking.

## D. RQ3: Ablation Studies on FixAgent

This section studies the contribution of each design to FixAgent. Table IV shows the results. Each row represents

TABLE III: Improvement made by FixAgent compared to its base model under different LLMs on 300 samples in Codeflaws.

| Models | Codeflaws-C, sampled 300 bugs |  |
| :---: | :---: | :---: |
|  | Base Model (CoT) | Applying FixAgent |
| CodeLlama | $105 / 108$ | $137 / 142$ |
| Gemini | $87 / 93$ | $107 / 119$ |
| DeepSeek-Coder | $130 / 134$ | $142 / 151$ |
| ChatGPT | $175 / 176$ | $196 / 198$ |
| GPT4 (original) | $223 / 227$ | $256 / 262$ |

removing one design, the decrease in the number of correct or plausible generated patches. FixAgent $w / o$ multi-agent uses GPT4 to conduct the whole debugging process. FixAgent $w / o$ tracking replaces the key variable tracking prompt with a simple "Think it step by step." FixAgent $w / o$ context removes program descriptions, input/output examples, and code of the dependent files. FixAgent $w / o$ feedback repeatedly samples FixAgent three times independently without failing information.

TABLE IV: Ablation study results of FixAgent.

| Models | Codeflaws-C, 300 bugs |  |
| :---: | :---: | :---: |
|  | \#Correct | \#Plausible |
| FixAgent $w / o$ multi-agent | -28 | -28 |
| FixAgent $w / o$ tracking | -38 | -37 |
| FixAgent $w / o$ context | -112 | -112 |
| FixAgent $w / o$ feedback | -5 | -4 |

We find that context construction contributes the most to FixAgent. Removing contexts reduces the correctly fixed bugs by 122. This highlights the significance of contexts in debugging as they provide the potential to understand the underlying problem domain, infer the intended functionality, and apply appropriate repair. Such degradation also illustrates why LLMs significantly outperform APR tools even without perfect fault localization, as shown in RQ1 (§V-B).

The second important design is variable tracking. FixAgent generates 37 and 38 fewer plausible and correct patches, respectively, without tracking. The foundation of this advancement is the enhancement of error diagnosis and reduction of reasoning overload in LLMs. By verbally articulating the role and expected behavior of each variable, LLMs are more likely to pinpoint discrepancies between expected and actual outcomes, facilitating a targeted approach to repair, mirroring the effective rubber ducking strategy. Furthermore, the structured explanation reduces the model's reasoning overload. Traditional CoT prompting, while effective in encouraging stepby-step reasoning, does not inherently prioritize information in a way that minimizes reasoning strain. Instead, our strategy streamlines the process of LLMs in analyzing the program logic. Using one agent to replace multi-agent synergy has a similar negative effect, which reduces 28 plausible and correct fixes. The division of labor mirrors a well-established principle in software: specialization, which forces each agent to focus on its own task and reduces cognitive load.

The least useful design is feedback-supported re-sampling. Though the added feedback provides more information, it expands the dialogue window, and the model must allocate its finite attention span to this new information, potentially at the expense of other critical details. This trade-off may explain why the additional feedback does not significantly enhance the debugging capability of LLMs compared to independent sampling, as our results show.

## VI. DISCUSSION

## A. Limitations

1) Ceiling Improvement: Though FixAgent significantly improves the debugging performance of multiple LLMs, its effectiveness is intrinsically tied to the foundational capabilities of the base model. Indeed, our method can not fundamentally alter the intrinsic capabilities of LLMs; instead, it optimizes their existing skills within a proportional range. For example, FixAgent improves Gemini by plausibly and correctly fixing 26 and 17 more bugs on Codeflaws, but its performance is still poorer than the simple application of GPT4. Thus, models with inherently limited debugging abilities will not experience a significant leap in performance using FixAgent despite improvements. The ceiling enhancement is ultimately bounded by the capabilities of the LLM itself. Hence, we adopt GPT4 as the default base model due to its advanced performance in a broad range of cognitive tasks [63], though FixAgent is compatible with any LLM.
2) External Test Output Calculation: We introduce an extra agent for test input generation to mitigate the overfitting problem, but it cannot directly calculate the corresponding expected outputs or test oracles, where a test oracle is a set of assertions that should pass when the program behaves as expected. This is primarily due to the intrinsic probabilistic mechanisms of LLMs, making it challenging to accurately deliver precise answers to computational questions. Addressing such problems akin to mathematical reasoning represents one of the significant challenges faced by LLMs. Our approach cannot overcome their inherent limitations; therefore, additional information, such as correct code or manually computed answers, must be introduced to obtain the outputs for generated inputs and form complete test cases.

## B. Threats to Validity

Internal First, we share the same major internal threat to validity with previous LLMs-based coding-related techniques where the training data of closed-sourced LLMs may overlap with our evaluation datasets. Since we do not have access to the training data, we mitigate this threat from three steps. 1) We choose a recently collected dataset (ConDefects) for evaluation proposed to mitigate the data leakage problem of LLMs as its collection is posterior to the model release. 2) The other two datasets we use are believed to be not part of the training data as discussed in [24] because 1) they have a low number of stars on GitHub, and 2) they focus on classic algorithms (QuixBugs) and programming assignments (Codeflaws) that do not belong to larger realworld projects. 3) RQ2 V-C demonstrates that our framework significantly improves the debugging effectiveness compared
with its base LLM, where the improvement is orthogonal to the data leakage issue. Second, we cannot directly determine the correct patches and rely on manual validation of plausible patches. For Codeflaws and ConDefects, we have to check a sampled subset of the patches because of their huge size. To mitigate this threat, we carefully validate the randomly sampled patches and all the patches for QuixBugs. Also, we generate more inputs using the crafter agent to test the plausible patches.

External Threat The main external threat lies in the evaluation dataset, where the superiority of FixAgent may not generalize to other datasets, especially for those less widely used programming languages that have less open-source code data for LLM training. To mitigate this, we compare our method against advanced baselines, including both general LLMs and APR approaches on five benchmarks in three datasets, covering three programming languages. Experiments demonstrate the effectiveness of FixAgent among all these benchmarks. We will also evaluate our approach to more datasets across more diverse programming languages in the future.

## VII. RELATED WORK

## A. Debugging

Spectrum-based and mutation-based are twopopular FLtechniques. Spectrum-based tools [4]-[6] calculate the suspiciousness scores of each line of code based on the correlation between the execution of a code line and the occurrence of test failures. However, it relies solely on test coverage data, specifically the binary distinction of whether a test case executes a code line. This reliance implicitly assumes that all test cases contribute equally to uncovering faults, which oversimplifies the complex nature of software errors. Mutation-based [7][9] mitigates the above limitation by implementing simple modifications on the buggy program based on pre-defined rules (mutation operators), such as altering a conditional statement from equality $(==)$ to inequality (!=), thereby creating variants of the program that slightly differ from the original buggy one. However, its efficacy is contingent upon the ability to generate meaningful mutants for each code element under scrutiny. Learning-based FL tools learn program behaviors from rich data sources, including code coverage matrix [16], statement-level calls [64], structural intricacies [17], or their combination [18] via multiple types of neural networks. A recent advancement, LLMAO [19], proposes to parse Abstract Syntax Tree and utilizes LLMs to achieve test-free FL with high confidence.

APR techniques can be broadly categorized into searchbased and generate-and-validate (G\&V)-based. Search-based employs meta-heuristic search algorithms, such as genetic programming [10]-[12], to find suitable solutions by exploring the space of possible patches. The patch space can be built by fix synthesis through syntactic code modifications and human-defined or automatically mined templates. Such approaches assume that the correct expression exists in the program or a pre-defined template set, limiting their wider applications [65]. G\&V APR [13], [14] aims to generate patches logically and semantically coherent with the intended functionality. Traditional $\mathrm{G} \& \mathrm{~V}$ methods represent the repair process as an explicit specification inference [66]. Recent studies regard it as a translation from faulty code to correct code using NMT, such as CoCoNuT [21], CURE [22], and RewardRepair [60]. Despite their effectiveness, they rely on bug-fixing training datasets usually crawled from commits of open-source repositories, which may contain irrelevant changes. To filter such noise, NMT tools are usually trained on small commit data, limiting their ability to fix more diverse bugs. Researchers have explored directly applying LLMs in APR using an infilling paradigm [24], [27], showing promising results. However, they still struggle to fix complex bugs requiring a deep understanding of program contexts or logic.

Unified debugging is a pioneer concept [39] that aims to unify FL and APR to boost both areas. A recent study [67] has shown that FL can benefit from 16 different APR systems. However, existing studies only consider the contribution to FL made by APR. We are the first to propose a unified framework enabling end-to-end debugging, where stages of the debugging have an interactional rather than determined relationship.

## B. Large Language Models

LLMs have revolutionized the field of software development with their rapid advancement. LLMs are trained on vast amounts of text data, enabling them to understand and generate human-like text [41], [42]. Existing studies have shown that a well-crafted prompt can lead to accurate, insightful, and useful domain-specific text generation [32], [68]. In addition to general-purpose LLMs [42], [56], [57], code LLMs are mainly trained on source code data, such as Codex [59], DeepSeekCoder [41], and CodeLlama [58]. These models are proposed to automate and streamline software engineering, enhance productivity, and reduce human errors. Many studies have investigated their capability on coding tasks, including code generation [62], APR [24], [33], [47], and test generation [69]. On top of the direct application of APR [24], recent studies have researched prompt engineering [27], [28], [70]-[73] or the combination of other techniques [19], [25], [69] to better unleash the capability of LLMs. For example, Repilot [25] copilots the LLMs via a code completion engine to synthesize more valid patches. AlphaRepair [27] proposes the first clozestyle APR approach that directly prompts LLMs to predict the correct code given context information with faulty code masked. A recent code generator [70] asks the LLM to explain the generated code line-by-line as rubber ducking, achieving promising performance. InferFix [73] augmented the prompt by incorporating similar fixes identified in a historical database of bugs through a dense retrieval model.

Our method, FixAgent, provides an end-to-end solution for automated debugging with high effectiveness. It performs a universal prompt design inspired by rubber ducking, which can significantly improve the debugging ability of LLMs in a nonintrusive style without any retraining/fine-tuning or historical bug-fix information. FixAgent shows that LLMs can benefit
from general software engineering principles recognized by developers, potentially besides rubber ducking.

## VIII. CONCLUSION

We propose FixAgent, the first unified debugging framework via LLM agent synergy. It conducts fault localization, patch generation, and post-error analysis in an end-to-end manner. Our insight is that LLMs can benefit from software engineering principles recognized by developers. Thus, we follow the principle of rubber duck debugging-explaining the code in detail to create novel designs that unleash the debugging capability of LLMs and mitigate previous challenges. The evaluation of two widely used datasets demonstrates the superiority of FixAgent over APR tools and LLM-based competitors, and extra experiments on recently collected data (avoiding data leakage) further show our generalization and effectiveness in debugging compared with base LLMs. Our code and generated patches are publicly available for further research and reproduction.

## REFERENCES

[1] D. Yuan, Y. Luo, X. Zhuang, G. R. Rodrigues, X. Zhao, Y. Zhang, P. Jain, and M. Stumm, "Simple testing can prevent most critical failures: An analysis of production failures in distributed data-intensive systems," in 11th USENIX Symposium on Operating Systems Design and Implementation, OSDI '14, Broomfield, CO, USA, October 6-8, 2014. USENIX Association, 2014, pp. 249-265.

[2] A. Alaboudi and T. D. LaToza, "What constitutes debugging? an exploratory study of debugging episodes," Empir. Softw. Eng., vol. 28, no. 5, p. 117,2023 .

[3] C. Boulder, "University of cambridge study: Failure to adopt reverse debugging costs global economy $\$ 41$ billion annually." University of Cambridge, London, Technical Report, 2013.

[4] R. Abreu, P. Zoeteweij, and A. J. C. van Gemund, "Spectrum-based multiple fault localization," in ASE 2009, 24th IEEE/ACM International Conference on Automated Software Engineering, Auckland, New Zealand, November 16-20, 2009. IEEE Computer Society, 2009, pp. 88-99.

[5] , "An evaluation of similarity coefficients for software fault localization," in 12th IEEE Pacific Rim International Symposium on Dependable Computing (PRDC 2006), 18-20 December, 2006, University of California, Riverside, USA. IEEE Computer Society, 2006, pp. 39-46.

[6] L. Zhang, M. Kim, and S. Khurshid, "Localizing failure-inducing program edits based on spectrum information," in IEEE 27th International Conference on Software Maintenance, ICSM 2011, Williamsburg, VA, USA, September 25-30, 2011. IEEE Computer Society, 2011, pp. 2332.

[7] X. Li and L. Zhang, "Transforming programs and tests in tandem for fault localization," Proc. ACM Program. Lang., vol. 1, no. OOPSLA, pp. 92:1-92:30, 2017.

[8] S. Moon, Y. Kim, M. Kim, and S. Yoo, "Ask the mutants: Mutating faulty programs for fault localization," in Seventh IEEE International Conference on Software Testing, Verification and Validation, ICST 2014, March 31 2014-April 4, 2014, Cleveland, Ohio, USA. IEEE Computer Society, 2014, pp. 153-162.

[9] L. Zhang, L. Zhang, and S. Khurshid, "Injecting mechanical faults to localize developer faults for evolving software," in Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \& Applications, OOPSLA 2013, part of SPLASH 2013, Indianapolis, IN, USA, October 26-31, 2013. ACM, 2013, pp. 765-784.

[10] C. L. Goues, M. Dewey-Vogt, S. Forrest, and W. Weimer, "A systematic study of automated program repair: Fixing 55 out of 105 bugs for $\$ 8$ each," in 34th International Conference on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland. IEEE Computer Society, 2012, pp. 3-13.
[11] W. Weimer, T. Nguyen, C. L. Goues, and S. Forrest, "Automatically finding patches using genetic programming," in 31st International Conference on Software Engineering, ICSE 2009, May 16-24, 2009, Vancouver, Canada, Proceedings. IEEE, 2009, pp. 364-374.

[12] , "Automatically finding patches using genetic programming," in 31st International Conference on Software Engineering, ICSE 2009, May 16-24, 2009, Vancouver, Canada, Proceedings. IEEE, 2009, pp. 364374.

[13] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra, "Semfix: program repair via semantic analysis," in 35th International Conference on Software Engineering, ICSE '13, San Francisco, CA, USA, May 1826, 2013. IEEE Computer Society, 2013, pp. 772-781.

[14] S. Mechtaev, J. Yi, and A. Roychoudhury, "Angelix: scalable multiline program patch synthesis via symbolic analysis," in Proceedings of the 38th International Conference on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016. ACM, 2016, pp. 691-701.

[15] F. Long and M. C. Rinard, "An analysis of the search spaces for generate and validate patch generation systems," in Proceedings of the 38th International Conference on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016. ACM, 2016, pp. 702-713.

[16] Y. Li, S. Wang, and T. N. Nguyen, "Fault localization with code coverage representation learning," in 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 2021, pp. 661-673.

[17] Y. Lou, Q. Zhu, J. Dong, X. Li, Z. Sun, D. Hao, L. Zhang, and L. Zhang, "Boosting coverage-based fault localization via graph-based representation learning," in ESEC/FSE '21: 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Athens, Greece, August 23-28, 2021. ACM, 2021, pp. 664-676.

[18] X. Li, W. Li, Y. Zhang, and L. Zhang, "Deepfl: integrating multiple fault diagnosis dimensions for deep fault localization," in Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2019, Beijing, China, July 15-19, 2019. ACM, 2019, pp. 169-180.

[19] A. Z. H. Yang, C. L. Goues, R. Martins, and V. J. Hellendoorn, "Large language models for test-free fault localization," in Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, ICSE 2024, Lisbon, Portugal, April 14-20, 2024. ACM, 2024, pp. $17: 1-17: 12$.

[20] X. Meng, X. Wang, H. Zhang, H. Sun, X. Liu, and C. Hu, "Templatebased neural program repair," in 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 2023, pp. 1456-1468.

[21] T. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei, and L. Tan, "Coconut: combining context-aware neural translation models using ensemble for program repair,' in ISSTA '20: 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Event, USA, July 18-22, 2020. ACM, 2020, pp. 101-114.

[22] N. Jiang, T. Lutellier, and L. Tan, "CURE: code-aware neural machine translation for automatic program repair," in 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 2021, pp. 1161-1173.

[23] A. Silva, S. Fang, and M. Monperrus, "Repairllama: Efficient representations and fine-tuned adapters for program repair," CoRR, vol. $\mathrm{abs} / 2312.15698,2023$.

[24] C. S. Xia, Y. Wei, and L. Zhang, "Automated program repair in the era of large pre-trained language models," in 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 2023, pp. 1482-1494.

[25] Y. Wei, C. S. Xia, and L. Zhang, "Copiloting the copilots: Fusing large language models with completion engines for automated program repair," in Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023, San Francisco, CA, USA, December 3-9, 2023. ACM, 2023, pp. 172-184.

[26] H. Joshi, J. P. C. Sánchez, S. Gulwani, V. Le, G. Verbruggen, and I. Radicek, "Repair is nearly generation: Multilingual program repair with llms," in Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023. AAAI Press, 2023, pp. 5131-5140.

[27] C. S. Xia and L. Zhang, "Less training, more repairing please: revisiting automated program repair via zero-shot learning," in Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022. ACM, 2022, pp. $959-971$.

[28] , "Keep the conversation going: Fixing 162 out of 337 bugs for $\$ 0.42$ each using chatgpt," CoRR, vol. abs/2304.00385, 2023.

[29] P. S. Kochhar, X. Xia, D. Lo, and S. Li, "Practitioners' expectations on automated fault localization," in Proceedings of the 25th International Symposium on Software Testing and Analysis, ISSTA 2016, Saarbrücken, Germany, July 18-20, 2016. ACM, 2016, pp. 165-176.

[30] J. Sohn and S. Yoo, "Fluccs: using code and change metrics to improve fault localization," in Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis, Santa Barbara, CA, USA, July 10 - 14, 2017. ACM, 2017, pp. 273-283.

[31] K. Liu, A. Koyuncu, T. F. Bissyandé, D. Kim, J. Klein, and Y. L. Traon, "You cannot fix what you cannot find! an investigation of fault localization bias in benchmarking automated program repair systems," in 12th IEEE Conference on Software Testing, Validation and Verification, ICST,Xi'an, China, April 22-27, 2019. IEEE, 2019, pp. 102-113.

[32] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou, "Chain-of-thought prompting elicits reasoning in large language models," in NeurIPS, 2022.

[33] K. Huang, X. Meng, J. Zhang, Y. Liu, W. Wang, S. Li, and Y. Zhang, "An empirical study on fine-tuning large language models of code for automated program repair," in 38th IEEE/ACM International Conference on Automated Software Engineering, ASE 2023, Luxembourg, September 11-15, 2023. IEEE, 2023, pp. 1162-1174.

[34] S. Kirbas, E. Windels, O. McBello, K. Kells, M. W. Pagano, R. Szalanski, V. Nowack, E. R. Winter, S. Counsell, D. Bowes, T. Hall, S. Haraldsson, and J. R. Woodward, "On the introduction of automatic program repair in bloomberg," IEEE Softw., vol. 38, no. 4, pp. 43-51, 2021.

[35] M. Kumar. (2023) Understanding tokens in chatgpt. [Online]. Available: https://medium.com/@ manav.kumar87/ understanding-tokens-in-chatgpt-32845987858d

[36] D. Lin, J. Koppel, A. Chen, and A. Solar-Lezama, "Quixbugs: a multilingual program repair benchmark set based on the quixey challenge," in Proceedings Companion of the 2017 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications. Software for Humanity, SPLASH 2017, Vancouver, BC, Canada, October 23 - 27, 2017. ACM, 2017, pp. 55-56.

[37] S. H. Tan, J. Yi, Yulis, S. Mechtaev, and A. Roychoudhury, "Codeflaws: a programming competition benchmark for evaluating automated program repair tools," in Proceedings of the 39th International Conference on Software Engineering, ICSE 2017, Buenos Aires, Argentina, May 20-28, 2017 - Companion Volume. IEEE Computer Society, 2017, pp. $180-182$.

[38] A. Author(s). (2024) Rudra. [Online]. Available: https://github.com/ afortunado-aceptado/Rudra

[39] Y. Lou, A. Ghanbari, X. Li, L. Zhang, D. Hao, and L. Zhang, "Can automated program repair refine fault localization?" CoRR, vol. abs/1910.01270, 2019.

[40] H. Liu, R. Ning, Z. Teng, J. Liu, Q. Zhou, and Y. Zhang, "Evaluating the logical reasoning ability of chatgpt and GPT-4," CoRR, vol. abs/2304.03439, 2023.

[41] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, "Deepseek-coder: When the large language model meets programming - the rise of code intelligence," CoRR, vol. abs/2401.14196, 2024.

[42] OpenAI, "Gpt-4a technical report," CoRR, vol. abs/2303.08774, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2303.08774

[43] R. Just, D. Jalali, and M. D. Ernst, "Defects4j: a database of existing faults to enable controlled testing studies for java programs," in International Symposium on Software Testing and Analysis, ISSTA '14, San Jose, CA, USA - July 21 - 26, 2014. ACM, 2014, pp. 437-440.

[44] H. Ye and M. Monperrus, "ITER: iterative neural repair for multilocation patches," in Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, ICSE 2024, Lisbon, Portugal, April 14-20, 2024. ACM, 2024, pp. 10:1-10:13.

[45] C. Wong, P. Santiesteban, C. Kästner, and C. L. Goues, "Varfix: balancing edit expressiveness and search effectiveness in automated program repair," in ESEC/FSE '21: 29th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engineering, Athens, Greece, August 23-28, 2021. ACM, 2021, pp. $354-366$.

[46] R. Abreu, P. Zoeteweij, and A. J. van Gemund, "On the accuracy of spectrum-based fault localization," in Testing: Academic and Industrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007), 2007, pp. 89-98.

[47] R. Tian, Y. Ye, Y. Qin, X. Cong, Y. Lin, Y. Pan, Y. Wu, Z. Liu, and M. Sun, "Debugbench: Evaluating debugging capability of large language models," CoRR, vol. abs/2401.04621, 2024.

[48] B. Xu, A. Yang, J. Lin, Q. Wang, C. Zhou, Y. Zhang, and Z. Mao, "Expertprompting: Instructing large language models to be distinguished experts," CoRR, vol. abs/2305.14688, 2023.

[49] X. D. Le, F. Thung, D. Lo, and C. L. Goues, "Overfitting in semanticsbased automated program repair," in Proceedings of the 40th International Conference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018. ACM, 2018, p. 163.

[50] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. HerbertVoss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, "Language models are few-shot learners," in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.

[51] Y. Wu, Z. Li, J. M. Zhang, and Y. Liu, "Condefects: A new dataset to address the data leakage concern for llm-based fault localization and program repair," CoRR, vol. abs/2310.16253, 2023.

[52] F. Long, "Automatic patch generation via learning from successful human patches," Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, USA, 2018.

[53] F. Long and M. C. Rinard, "Staged program repair with condition synthesis," in Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015, Bergamo, Italy, August 30 September 4, 2015. ACM, 2015, pp. 166-178.

[54] A. Reynolds, M. Deters, V. Kuncak, C. Tinelli, and C. Barrett, "Counterexample-guided quantifier instantiation for synthesis in smt," in Computer Aided Verification: 27th International Conference, CAV 2015, San Francisco, CA, USA, July 18-24, 2015, Proceedings, Part II 27. Springer, 2015, pp. 198-216.

[55] H. Ye, M. Martinez, T. Durieux, and M. Monperrus, "A comprehensive study of automatic program repair on the quixbugs benchmark," J. Syst. Softw., vol. 171, p. 110825, 2021. [Online]. Available: https://doi.org/10.1016/j.jss.2020.110825

[56] R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen, E. Pitler, T. P. Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan, B. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira, K. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka, B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez, M. Khalman, J. Sygnowski, and et al., "Gemini: A family of highly capable multimodal models," CoRR, vol. $\mathrm{abs} / 2312.11805,2023$.

[57] OpenAI. (2023) Gpt-3.5 turbo. [Online]. Available: https://platform. openai.com/docs/models/gpt-3-5-turbo

[58] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. Canton-Ferrer, A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve, "Code llama: Open foundation models for code," CoRR, vol. abs/2308.12950, 2023

[59] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, "Evaluating
large language models trained on code," CoRR, vol. abs/2107.03374, 2021.

[60] H. Ye, M. Martinez, and M. Monperrus, "Neural program repair with execution-based backpropagation," in 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 2022, pp. 1506-1518.

[61] A. Holtzman, J. Buys, M. Forbes, and Y. Choi, "The curious case of neural text degeneration," CoRR, vol. abs/1904.09751, 2019.

[62] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, "Evaluating large language models trained on code," CoRR, vol. abs/2107.03374, 2021.

[63] R. Mao, G. Chen, X. Zhang, F. Guerin, and E. Cambria, "Gpteval: A survey on assessments of chatgpt and GPT-4," CoRR, vol. abs/2308.12488, 2023.

[64] Y. Li, S. Wang, and T. N. Nguyen, "Fault localization to detect cochange fixing locations," in Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022. ACM, 2022, pp. 659-671.

[65] K. Liu, S. Wang, A. Koyuncu, K. Kim, T. F. Bissyandé, D. Kim, P. Wu, J. Klein, X. Mao, and Y. L. Traon, "On the efficiency of test suite based program repair: A systematic assessment of 16 automated repair systems for java programs," in ICSE '20: 42nd International Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020.
ACM, 2020, pp. 615-627.

[66] X. Gao, Y. Noller, and A. Roychoudhury, "Program repair," CoRR, vol. abs/2211.12787, 2022.

[67] S. Benton, X. Li, Y. Lou, and L. Zhang, "On the effectiveness of unified debugging: An extensive study on 16 program repair systems," in 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020, Melbourne, Australia, September 21-25, 2020. IEEE, 2020, pp. 907-918.

[68] S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan, F. Huang, and H. Chen, "Reasoning with language model prompting: A survey," in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Association for Computational Linguistics, 2023, pp. 5368-5393

[69] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, "Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models," in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2023, Seattle, WA, USA, July 17-21, 2023. ACM, 2023, pp. 423-435.

[70] X. Chen, M. Lin, N. Schärli, and D. Zhou, "Teaching large language models to self-debug," CoRR, vol. abs/2304.05128, 2023.

[71] N. Jain, S. Vaidyanath, A. S. Iyer, N. Natarajan, S. Parthasarathy, S. K. Rajamani, and R. Sharma, "Jigsaw: Large language models meet program synthesis," in 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 2022, pp. 1219-1231.

[72] H. Wang, Z. Liu, S. Wang, G. Cui, N. Ding, Z. Liu, and G. Yu, "INTERVENOR: prompt the coding ability of large language models with the interactive chain of repairing," CoRR, vol. abs/2311.09868, 2023.

[73] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, and A. Svyatkovskiy, "Inferfix: End-to-end program repair with llms," CoRR, vol. abs/2303.07263, 2023.

