# Faster Cascades via Speculative Decoding 

Harikrishna Narasimhan ${ }^{\dagger}$, Wittawat Jitkrittum*, Ankit Singh Rawat*<br>Seungyeon Kim*, Neha Gupta ${ }^{\dagger}$, Aditya Krishna Menon*, Sanjiv Kumar*<br>${ }^{\dagger}$ Google Research, Mountain View $\quad$ *Google Research, New York

Corresponding author: hnarasimhan@google.com

May 30, 2024


#### Abstract

Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for "hard" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades are often capable of yielding better quality than even the larger model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule. Through experiments with T5 models on benchmark language tasks, we show that the proposed approach yields better cost-quality trade-offs than cascading and speculative decoding baselines.


## 1 Introduction

Large language models (LLMs) have demonstrated significant advances in quality on a range of natural language processing tasks [33, 34, 5, 2, 11, 48, 12, 41, 1, 43, 42, at the cost of a significant increase in inference latency. This has sparked a growing body of literature on reducing LMs' inference costs without (overly) compromising on quality [15, 32, 36, 26, 7, 37, 40. One such line of work involves constructing a family of models of various sizes (e.g., a small and large model), and suitably orchestrating amongst them to make a prediction. Two canonical instantiations of this strategy are model cascading [47, 28, 45, 23, 14, 8, 18, 13] and speculative decoding [39, 7, 26, 40, 49].

While similar in spirit, cascades and speculative decoding are fundamentally different in details. Cascades employ a deferral rule to identify "hard" inputs, and only invoke larger models on such inputs. For example, in a two-model cascade, one first invokes the smaller model, and uses its associated probability of the generated output to decide whether to defer to the larger model. By contrast, speculative decoding uses a small model to draft a block of tokens via standard auto-regressive decoding, which are then verified in parallel by a large model. One then accepts all drafted tokens until the first "implausible" one, which is rolled back based on the larger LM's prediction.

Owing to their different mechnanisms, both methods have complementary strengths. Cascades seek to output distributions that have the best quality for a given cost budget, and are empirically observed to often yield better accuracies than even the individual models they are constructed with

[22, 24] (\$3). By contrast, speculative decoding is theoretically guaranteed to match the output distribution (or a close approximation thereof [44), and are practically observed to provide impressive speed-ups 39, 7, 26, 40]. Given the complementary nature of these two approaches, a natural question that arises is whether we can leverage the best of both techniques.

In this paper, we do so by designing new techniques for two-model cascades that implement their deferral rule in a speculative manner: we have the smaller model generate drafts auto-regressively, and the larger model execute in parallel on the drafts to decide whether or not to defer on them We show that this speculative cascading approach yields better cost-quality trade-offs than both standard cascades and speculative decoding. In detail, we make the following contributions:

(i) We introduce a general recipe for speculative execution, where we seek to mimic a general target distribution that interleaves the drafter's and verifier's output distributions. Lossy speculative sampling [44] is a special case of this recipe for a particular target distribution (84.1).

(ii) We show how common cascading deferral rules, such as Chow's rule 10 and confidence-difference thresholding [22], can be implemented speculatively by plugging in the corresponding target distribution into the proposed framework. We refer to these as speculative cascades (\$4.2).

(iii) We characterize the theoretically optimal deferral rule for a speculative cascade, and design a speculative cascading technique that implements a plug-in estimate to the optimal rule (Lem. 4).

(iv) Through experiments on benchmark language tasks, we show that speculative cascades constructed from T5 models [35] of different sizes are able to provide better cost-quality trade-offs than their sequential cascade and speculative decoding counterparts (86).

## 2 A Tale of Two Efficient LM Inference Strategies

Let $\mathcal{V}$ denote a finite vocabulary of tokens, with $\mathcal{V}^{*}$ the set of all sequences generated by this vocabulary. Let $\Delta_{\mathcal{V}}$ denote the set of all probability distributions over tokens in $\mathcal{V}$. Given an arbitrary length sequence $x=x_{1} x_{2} \ldots x_{L} \in \mathcal{V}^{*}$ and index $i \leq L$, denote by $x_{<i}=x_{1} x_{2} \ldots x_{i-1}$. A language model (LM) is a probability distribution over $\mathcal{V}^{*}$. Let $\mathbb{P}$ denote the ground-truth probability distribution over $\mathcal{V}^{*}$. This could be, for example, a distribution over prompt-response pairs that the LM may encounter during deployment, or a distribution of sequences used to pre-train the LM. We will measure the quality of an LM based on how closely it mimics $\mathbb{P}$.

Suppose we are provided two LMs $q$ and $p$, where $p$ is the larger model. Our goal is to design an inference strategy that selectively invokes $q$ and $p$ to trade-off between quality and latency (which may be approximated by the fraction of times that $p$ is invoked). We will denote by $q\left(x_{t} \mid x_{<t}\right)$ the probability $q$ associates to token $x_{t} \in \mathcal{V}$ given prefix $x_{<t} \in \mathcal{V}^{t-1}$, and by $p\left(x_{t} \mid x_{<t}\right)$ the same distribution from model $p$. Whenever it is clear from context, we will hide the conditioning on prefix $x_{<t}$, and use the shorthand $p_{t}(\cdot)$ for $p\left(\cdot \mid x_{<t}\right)$ and $q_{t}(\cdot)$ for $q\left(\cdot \mid x_{<t}\right)$.

Cascades. Cascades are an effective strategy to trade-off cost and quality by having the smaller model $q$ handle the "easy" samples, and the larger model $p$ handle the "difficult" samples [18, 50]. A common approach to cascading is confidence-based thresholding or Chow's rule 10, 22], where we first run $q$ on the given input, and defer to $p$ when $q$ 's confidence for its generated response is sufficiently low. This strategy is typically implemented at the sequence-level, where for a given prefix $x_{<m}$ we invoke $q$ to generate a complete response $x_{m} \ldots x_{m+n}$. We evaluate the predicted probability from $q$ for this response, and check whether it falls below a threshold $\alpha \in[0,1]$ :

$$
\begin{equation*}
q\left(x_{m} \ldots x_{m+n} \mid x_{<m}\right)<1-\alpha \tag{1}
\end{equation*}
$$

| Inference strategy | Deferral decision $\delta(q, p)$ | Target distribution $\pi(x)$ | Execution |
| :--- | :--- | :--- | :--- |
| SpecDecoding [26] | - | $p(x)$ | Speculative |
| Lossy SpecDecoding [44] | - | $\max \left\{\min \left\{q(x), \frac{p(x)}{1-\alpha}\right\}, \frac{p(x)}{\beta}\right\}$ | Speculative |
| BiLD* [24] | $\mathbf{1}(D(q, p)>\alpha)$ | $(1-\delta) \cdot q(x)+\delta \cdot p(x)$ | Speculative |
| TokenCascade [Chow] [10] | $\mathbf{1}\left(\max _{v} q(v)<1-\alpha\right)$ | $(1-\delta) \cdot q(x)+\delta \cdot p(x)$ | Sequential |
| Oracle [Diff] [22] | $\mathbf{1}\left(\max _{v} q(v)<\max _{v} p(v)-\alpha\right)$ | $(1-\delta) \cdot q(x)+\delta \cdot p(x)$ | Oracle |
| SpecCascade [Chow] | $\mathbf{1}\left(\max _{v} q(v)<1-\alpha\right)$ | $(1-\delta) \cdot q(x)+\delta \cdot p(x)$ | Speculative |
| SpecCascade [Diff] | $\mathbf{1}\left(\max _{v} q(v)<\max _{v} p(v)-\alpha\right)$ | $(1-\delta) \cdot q(x)+\delta \cdot p(x)$ | Speculative |
| SpecCascade [OPT] | $\mathbf{1}\left(\max _{v} q(v)<\max _{v} p(v)-\alpha \cdot D_{\mathrm{TV}}(p, q)\right)$ | $(1-\delta) \cdot q(x)+\delta \cdot p(x)$ | Speculative |

Table 1: Target distributions associated with different inference algorithms, where $\alpha$ is a free parameter and $\beta \geq 1-\alpha$ is a parameter dependent on $\alpha, q$ and $p$. The last column indicates whether the execution is sequential (Algorithm 2), via an oracle (Algorithm 3), or speculative (Algorithm 5 ) [26]. The third row presents a variant of the BiLD algorithm of [24], where $D(q, p)$ is a measure of discrepancy between $q$ and $p$; the original algorithm differs in the use of a deterministic speculative decoding procedure with a dynamic draft window (see \$5). See (6) for more details on $\delta$.

If the above holds, we defer to $p$ to generate a new response; otherwise, we retain $q$ 's response. One may then tune the threshold to achieve a desired cost-quality trade-off. The literature also offers variants of Chow's rule that use a more nuanced aggregation of per-token uncertainties 18].

Speculative decoding. Speculative decoding is an alternate inference strategy that applies tokenlevel interleaving between $q$ and $p$, resulting in provably matching the larger model quality at a reduced inference cost [39, 26]. Given a prefix $x_{<t}$, we sample $\gamma$ draft tokens $x_{t}, \ldots, x_{t+\gamma-1}$ auto-regressively from $q$, and run $p$ in parallel on the $\gamma$ prefixes $x_{<t}, \ldots, x_{<t+\gamma-1}$, and verify if the generated tokens can be accepted. We then rollback to the first token $t+j^{*}$ that was rejected, replace $x_{t+j^{*}}$ with a new token, and repeat the drafting-verification process with $x_{<t+j^{*}+1}$ as the new prefix.

During the verification stage, a draft token $x_{t+j}$ generated by $q$ is accepted with probability $\min \left(1, \frac{p_{t+j}\left(x_{t+j}\right)}{q_{t+j}\left(x_{t+j}\right)}\right)$ and rejected otherwise, where recall the shorthand $q_{t+j}(\cdot)=q\left(\cdot \mid x_{<t+j}\right)$ and $p_{t+j}(\cdot)=$ $p\left(\cdot \mid x_{<t+j}\right)$. A rejected token is then replaced by a new token sampled from a modified distribution $\operatorname{norm}\left(\max \left\{0, p_{t+j}(\cdot)-q_{t+j}(\cdot)\right\}\right)$, where norm $(\cdot)$ denotes normalization to sum to 1 . This sampling process is provably equivalent to sampling $\gamma$ tokens auto-regressively from $p$ for prefix $x_{<t}$ [26]. We summarize this speculative sampling procedure in Algorithm 1. Each invocation of this algorithm generates at most $\gamma+1$ next tokens for a given prefix $x_{<t}$. One may run this algorithm multiple times to generate a complete output sequence.

In practice, one may employ a lossy variant [44] of the above sampling that allows some deviation from verifier's distribution $p$. In this case, a draft token $x_{t+j}$ is accepted with probability

$\min \left(1, \frac{p_{t+j}\left(x_{t+j}\right)}{(1-\alpha) \cdot q_{t+j}\left(x_{t+j}\right)}\right)$, where $\alpha \in[0,1)$ is a strictness parameter, with higher values indicating greater deviation from $p$. A rejected token may then be replaced by a token sampled from the residual distribution norm $\left(\max \left\{0, \frac{1}{\beta} \cdot p_{t+j}(\cdot)-q_{t+j}(\cdot)\right\}\right)$, where $\beta \geq 1-\alpha$ is a parameter that depends on $\alpha, q$ and $p$. A common heuristic is to simply set $\beta=1$ [52].

## 3 Cascades Meet Speculative Decoding

Both cascades and speculative decoding interleave models of different sizes to reduce inference cost, but fundamentally differ in the mechanisms they use. As a step towards comparing the strengths and weaknesses of these approaches, we first describe how one may design a token-level cascade.

### 3.1 Warm-up: Token-level cascades

It is straight-forward to extend the sequence-level Chow's rule from $\$ 2$ to form a token-level cascade between $q$ and $p$. For a prefix $x_{<t}$, we first compute the smaller model's distribution $q\left(\cdot \mid x_{<t}\right)$, and check whether $\max _{v \in \mathcal{V}} q\left(v \mid x_{<t}\right)$ is below a pre-chosen threshold. if so, we evaluate $p\left(\cdot \mid x_{<t}\right)$, and sample $x_{t} \sim p\left(\cdot \mid x_{<t}\right)$; otherwise, we sample $x_{t} \sim q\left(\cdot \mid x_{<t}\right)$.

More generally, we may design a token-level deferral rule $r: \mathcal{V}^{t-1} \rightarrow\{0,1\}$ that takes the prefix $x_{<t}$ as input and outputs a binary decision, with $r\left(x_{<t}\right)=1$ indicating that we defer to $p$ (i.e., draw a sample from $p$ rather than $q$ ). For example, token-level Chow's rule can be written as:

$$
\begin{equation*}
r_{\text {Chow }}\left(x_{<t}\right)=1 \Longleftrightarrow \max _{v \in \mathcal{V}} q\left(v \mid x_{<t}\right)<1-\alpha \tag{2}
\end{equation*}
$$

where $\alpha$ is a cost parameter; the higher the value, the lower is the frequency of deferral to $p$. One may also use a confidence measure different from the maximum probability, such as the entropy of the small model's probability distribution. We elaborate in $\S$ B that the choice of confidence measure would depend on the evaluation metric of interest; (2) is typically prescribed when the cascade's quality is evaluated in terms of its accuracy against the ground-truth distribution on individual tokens, whereas entropy is prescribed when the metric of interest is the cross-entropy loss.

### 3.2 Optimal token-level cascade deferral

While Chow's rule $\sqrt{2}$ is easy to implement, it can be sub-optimal if the smaller model's maximum token probability is not reflective of which of the two models are better equipped to predict the next token for a given prefix [22. Given this, it is natural to ask what the optimal deferral rule $r$ for a token-cascade looks like, and whether we can reasonably approximate this rule.

For this, we must first specify an objective to minimize at each step $t$. Following the prior cascade literature [22, 18], a reasonable objective to minimize is the expected loss from the deferral rule against the ground-truth distribution $\mathbb{P}$, with an added cost for deferring to the larger model. We state this below for a fixed prefix $x_{<t}$, using the short-hand $q_{t}(\cdot)$ for $q\left(\cdot \mid x_{<t}\right)$ and $p_{t}(\cdot)$ for $p\left(\cdot \mid x_{<t}\right)$ :

$$
\begin{equation*}
L_{\mathrm{def}}\left(r ; x_{<t}\right)=\mathbb{E}_{x_{t} \sim \mathbb{P}\left(\cdot \mid x_{<t}\right)}\left[\left(1-r\left(x_{<t}\right)\right) \cdot \ell\left(x_{t}, q_{t}\right)+r\left(x_{<t}\right) \cdot\left(\ell\left(x_{t}, p_{t}\right)+\alpha\right)\right] \tag{3}
\end{equation*}
$$

for a cost penalty $\alpha \geq 0$ and loss function $\ell: \mathcal{V} \times \Delta_{\mathcal{V}} \rightarrow \mathbb{R}_{+}$. Common choices for $\ell$ include the $0-1$ $\operatorname{loss} \ell_{0-1}\left(x_{t}, q_{t}\right)=\mathbf{1}\left(x_{t} \neq \operatorname{argmax}_{v} q_{t}(v)\right)$ and the $\log \operatorname{loss} \ell_{\log }\left(x_{t}, q_{t}\right)=-\log \left(q_{t}\left(x_{t}\right)\right)$.

Lemma 1 (Optimal deferral for token-level cascades [22]). The minimizer of (3) is of the form:

$$
\begin{equation*}
r^{*}\left(x_{<t}\right)=1 \quad \Longleftrightarrow \quad \mathbb{E}_{x_{t} \sim \mathbb{P}\left(\cdot \mid x_{<t}\right)}\left[\ell\left(x_{t}, q_{t}\right)\right]>\mathbb{E}_{x_{t} \sim \mathbb{P}\left(\cdot \mid x_{<t}\right)}\left[\ell\left(x_{t}, p_{t}\right)\right]+\alpha \tag{4}
\end{equation*}
$$

Intuitively, we compare the expected loss from $q$ with the expected cost of invoking $p$, and decide to defer when the latter is smaller. We note here that this optimization problem is set up for a fixed prefix $x_{<t}$. One may also consider the coupled optimization problem across all prefixes from 1 to $T$.

Plug-in estimator for (4). The optimal rule in (4) requires computing expectations over the ground-truth distribution $\mathbb{P}\left(\cdot \mid x_{>t}\right)$, which is not available during inference time. A common approach
in the cascades literature is to replace the expected losses with the models' confidence estimates 22. For example, when $\ell=\ell_{0-1}$, it may be reasonable to use $1-\max _{v} q_{t}(v)$ as an estimate of the expected $0-1$ loss $\mathbb{E}_{x_{t} \sim \mathbb{P}\left(\cdot \mid x_{<t}\right)}\left[\ell_{0-1}\left(x_{t}, q_{t}\right)\right]$ and $1-\max _{v} p_{t}(v)$ as an estimate of $\mathbb{E}_{x_{t} \sim \mathbb{P}\left(\cdot \mid x_{<t}\right)}\left[\ell_{0-1}\left(x_{t}, q_{t}\right)\right]$. The extent to which these estimates are accurate depend on how well $q$ and $p$ are calibrated [17. The resulting plug-in estimator for (4) is given by:

$$
\begin{equation*}
\hat{r}_{\text {Diff }}\left(x_{<t}\right)=1 \quad \Longleftrightarrow \quad \max _{v} q_{t}(v)<\max _{v} p_{t}(v)-\alpha \tag{5}
\end{equation*}
$$

Similarly, when $\ell=\ell_{\log }$, we may use the entropy $-\sum_{v} q_{t}(v) \cdot \log \left(q_{t}(v)\right)$ from $q_{t}$ as an estimate of its expected log-loss, and similarly for $p_{t}$ (see $\mathrm{C}$ ).

Remark 1 (Oracle deferral rules). $\hat{r}_{\text {Diff }}$ cannot be directly used in a token-level cascade, as it needs the larger model to be invoked at every position $t$. However, it serves as an oracle that allows to analyze the head-room available to improve upon Chow's rule. See also Remark 2 .

### 3.3 When do token-level cascades outperform speculative decoding?

Token-level cascades and speculative decoding differ in the distribution over tokens they seek to mimic. Speculative decoding seeks to mimic the larger model's output distribution (or an approximation to it). As a result, the quality of the sampled output is limited by the accuracy of the larger model. On the other hand, token-level cascades seek to output distributions that closely approximate the ground-truth label distribution, and can potentially yield better quality than even the larger model.

Indeed, speculative decoding would be ideal in applications where the verification model $p$ is uniformly better than the draft model $q$ on all inputs. However, when there are inputs where the draft model fares better than the verifier, one may want to retain the drafter's predictions even when it disagrees with the verifier. Similarly, in settings where both the drafter and verifier fare poorly on some inputs (e.g., due to label noise), one may again want to ignore the disagreement between the drafter and verifier, and avoid triggering unnecessary roll-backs.

As a concrete example, we consider token-level cascades of T5 models [35] of two different sizes finetuned on a WMT EN $\rightarrow$ DE translation [3] and an extreme summarization (XSum) task [31]. We construct these cascades using both (token-level) Chow's rule in (2) and the oracle Diff rule in (5), and also apply speculative decoding with the smaller (larger) model as the drafter (verifier). In Figure 1. we plot quality as a function of fraction of samples deferred to the large model, as we vary the cost parameter $\alpha$. Note that with speculative decoding, each verification step is counted as a single call to the large model. While speculative decoding matches the quality of the large model (right-most point), the oracle rule yields significantly better quality on a wide range of operating points. Even Chow's rule, which is sub-optimal for cascading [22], outperforms speculative decoding in a small region.

However, as also evident from the plots, token-level cascades require a significantly larger number of calls to the larger model to achieve the same quality. This is because token-level cascades are executed sequentially: whenever $q$ defers, we execute $p$ once to generate one next token for the prefix accumulated so far, and the control transfers back to $q$. In contrast, speculative decoding runs $p$ in scoring mode to verify multiple draft tokens from $q$ in parallel. Moreover, the stochastic verification algorithm in speculative decoding often results in fewer tokens from $q$ getting rejected compared to the deterministic deferral rules used in a cascade. These observations motivate a natural question: given their complementary strengths, how can we leverage the best of both these techniques?

## 4 Speculative Cascades: Leveraging the Best of Both Worlds

In addressing the above question, we present our main contribution: a principled approach to combining the superior quality of cascades with the faster execution of speculative decoding approaches.
![](https://cdn.mathpix.com/cropped/2024_06_04_77db090807aa84cabcdfg-06.jpg?height=398&width=1158&top_left_y=229&top_left_x=472)

Figure 1: Plot of quality as a function of deferral rate for cascades constructed from T5 models (under temperature sampling with $T=1$ ). The left-most point represents the small model and the right-most represents the large model. We compare token-level cascades constructed with Chow's rule (Chow) and an oracle deferral rule (Diff), and speculative decoding with block size $\gamma=5$. We measure deferral rate in terms of the number of calls to the larger model; with a cascade, each call yields exactly one token, whereas with speculative decoding, a single call scores $\gamma$ draft tokens in parallel. While speculative decoding matches the quality of the large model (see dashed horizontal line), the oracle deferral rule yields significantly better quality on a range of deferral rates; this however comes at the cost of higher number of calls to the large model.

### 4.1 Speculative decoding with general target distributions

We begin by considering a generic version of speculative sampling that seeks to mimic a general target distribution derived from the drafter's and verifier's distributions. In the proposed sampling procedure outlined in Algorithm 4, we sample tokens auto-regressively as before from the drafter's distribution. During the verification step, however, we do not compare the drafter's token probabilities against the verifier's distribution. Instead, we use a user-specified target distribution $\pi=\mathbb{T}(q, p) \in \Delta_{\mathcal{V}}$ derived from the drafter's and verifier's distributions at position $t$, for some function $\mathbb{T}(\cdot, \cdot)$ that is inexpensive to compute. We accept a draft token $x_{t}$ when $q\left(x_{t}\right) \leq \pi\left(x_{t}\right)$ and reject it otherwise with probability $1-\frac{\pi\left(x_{t}\right)}{q\left(x_{t}\right)}$. Upon rejection, we re-sample from the residual distribution norm $(\max \{0, \pi(\cdot)-q(\cdot)\})$.

This general procedure not only encompasses standard speculative decoding [26] for $\mathbb{T}(q, p)=p$, but also includes lossy speculative decoding [44] as a special case:

Lemma 2. Algorithm 4 reduces to the lossy speculative sampling procedure in 44 with parameters $\alpha$ and $\beta$ when $\mathbb{T}(q, p)(v)=\max \left\{\min \left\{q(v), \frac{p(v)}{1-\alpha}\right\}, \frac{p(v)}{\beta}\right\}$.

### 4.2 From sequential to speculative cascades

Equipped with Algorithm 4 , we now propose new cascading techniques that implement their deferral rule in a speculative manner. Recall that a token-level cascade of two models $q$ and $p$ is defined by a deferral rule $r: \mathcal{V}^{t-1} \rightarrow\{0,1\}$. For a prefix $x_{<t}$, the next-token distribution at position $t$ modeled by this cascade can be written as:

$$
\pi(v)=\left(1-r\left(x_{<t}\right)\right) \cdot q_{t}(v)+r\left(x_{<t}\right) \cdot p_{t}(v)
$$

In fact, for all the deferral rules described in 8 , the resulting distribution can be described by a target distribution function $\mathbb{T}_{\delta}$ of the form:

$$
\begin{equation*}
\mathbb{T}_{\delta}(q, p)(v)=(1-\delta(q, p)) \cdot q(v)+\delta(q, p) \cdot p(v) \tag{6}
\end{equation*}
$$

```
Algorithm 1 SpecDecode
Input: Models $q, p$, Prefix $x_{<t}$, Block size $\gamma$
    $\mathbb{T}(q, p) \doteq p$
Output: GenSpecSample $\left(q, p, \mathbb{T}, x_{<t}, \gamma\right)$
Algorithm 2 TokenCascade
Input: Models $q, p$, Deferral logic $\delta$, Prefix $x_{<t}$
    $q_{t}(\cdot) \doteq q\left(\cdot \mid x_{<t}\right)$
    if $\delta\left(q_{t}, \emptyset\right)=0$ then
        Sample $x_{t} \sim q_{t}(\cdot)$
    else
        $p_{t}(\cdot) \doteq p\left(\cdot \mid x_{<t}\right) ; \quad$ Sample $x_{t} \sim p_{t}(\cdot)$
    end if
Output: $x_{t}$
Algorithm 3 OracleCascade
Input: Models $q, p$, Deferral logic $\delta$, Prefix $x_{<t}$
    $q_{t}(\cdot) \doteq q\left(\cdot \mid x_{<t}\right) ; \quad p_{t}(\cdot) \doteq p\left(\cdot \mid x_{<t}\right)$
    if $\delta\left(q_{t}, p_{t}\right)=0$ then
        Sample $x_{t} \sim q_{t}(\cdot)$
    else
        Sample $x_{t} \sim p_{t}(\cdot)$
    end if
Output: $x_{t}$
```

```
Algorithm 4 GenSpecSample
Input: Models $q, p$, Target distr. $\mathbb{T}$, Prefix $x_{<t}$, Block size $\gamma$
    $[\gamma] \equiv\{0, \ldots, \gamma\}$
    Sample $\gamma$ tokens auto-regressively from $q$
    for $j=0$ to $\gamma-1$ do
        $q_{t+j}(\cdot) \doteq q\left(\cdot \mid x_{<t+j}\right) ; \quad x_{t+j} \sim q_{t+j}(\cdot)$
    end for
    Run $p$ in parallel to score $\gamma$ draft tokens
    $p_{t+j}(\cdot) \doteq p\left(\cdot \mid x_{<t+j}\right), \forall j \in[\gamma]$
    $\pi_{t+j}=\mathbb{T}\left(q_{t+j}, p_{t+j}\right)$
    Find the earliest draft token that gets rejected
    $a_{j} \sim \operatorname{Ber}\left(\min \left\{1, \frac{\pi_{t+j}\left(x_{t+j}\right)}{q_{t+j}\left(x_{t+j}\right)}\right\}\right), \forall j \in[\gamma-1] ; \quad a_{\gamma}=0$
    $j^{*}=\min \left\{j \in[\gamma]: a_{j}=0\right\}$
    Sample a new token from residual distribution
    $p_{\text {res }}(\cdot)= \begin{cases}\operatorname{norm}\left(\max \left\{0, \pi_{t+j^{*}}(\cdot)-q_{t+j^{*}}(\cdot)\right\}\right) & \text { if } j^{*}<\gamma \\ p_{t+\gamma}(\cdot) & \text { else }\end{cases}$
    Sample $x_{t+j^{*}} \sim p_{\mathrm{res}}(\cdot)$
Output: $x_{t}, \ldots, x_{t+j^{*}}$
```

```
Algorithm 5 SpecCascade
Input: Models $q, p$, Deferral logic $\delta$, Prefix $x_{<t}$, Block size $\gamma$
    $\mathbb{T}_{\delta}(q, p) \doteq(1-\delta(q, p)) \cdot q+\delta(q, p) \cdot p$
Output: GenSpecSample $\left(q, p, \mathbb{T}_{\delta}, x_{<t}, \gamma\right)$
```

for some function $\delta: \Delta_{\mathcal{V}} \times \Delta_{\mathcal{V}} \rightarrow\{0,1\}$ that maps distributions $(q, p)$ to a binary decision. For example, for Chow, $\delta(q, p)=\mathbf{1}\left(\max _{v} q(v)<1-\alpha\right)$, and for Diff, $\delta(q, p)=\mathbf{1}\left(\max _{v} q(v)<\max _{v} p(v)-\alpha\right)$. See Table 1 for a summary of target distributions for different deferral rules.

Our proposal is to then invoke the speculative sampling procedure in Algorithm 4 with $\mathbb{T}_{\delta}$ as the target distribution function. We outline this generic speculative cascading approach in Algorithm 5, and contrast it with the sequential execution of a deferral rule in Algorithm 2 .

Remark 2 (Exact implementation of oracle deferral rule Diff). Unlike a sequential cascade, where the larger model's distribution $p$ is not available at the time the deferral decision is made (see Remark 1), with a speculative cascade, we can accommodate deferral rules like Diff that depend on both $q$ and $p$. This is because we run the larger model $p$ in parallel on drafts generated by the smaller model $q$, allowing us to compute both $p(\cdot)$ and $q(\cdot)$ on every prefix.

So far we have considered deferral rules designed for sequential cascades. In what follows, we derive the optimal deferral rule $r$ for a speculative cascade, where we sample speculatively from a target distribution $\pi=\left(1-r\left(x_{<t}\right)\right) \cdot q_{t}+r\left(x_{<t}\right) \cdot p_{t}$ using $q_{t}$ as the drafter.

### 4.3 Deferral risk for speculative cascades

As with sequential cascades (\$2), we begin by defining an objective to minimize. We seek a deferral rule $r: \mathcal{V}^{t-1} \rightarrow\{0,1\}$ that minimizes a loss against the ground-truth distribution, while limiting the inference cost to be within a budget. (Per above, this deferral rule implicitly defines a target distribution $\pi$.) The inference cost crucially depends on how frequently a draft token is rejected in the verification phase, triggering a rollback. To this end, we derive the probability that a token sampled from $q$ is rejected during verification, for a target distribution resulting from a deferral rule $r$.

Lemma 3. For a given prefix $x_{<t}$, and target distribution $\pi=\left(1-r\left(x_{<t}\right)\right) \cdot q_{t}+r\left(x_{<t}\right) \cdot p_{t}$, the probability of a token drawn from draft distribution $q_{t}$ being rejected is equal to:

$$
r\left(x_{<t}\right) \cdot D_{\mathrm{TV}}\left(p_{t}, q_{t}\right)
$$

where $D_{\mathrm{TV}}(p, q)=\sum_{v \in \mathcal{V}} \max \{0, p(v)-q(v)\}$ is the TV distance between $p$ and $q$.

Intuitively, whenever $r\left(x_{<t}\right)=0, \pi(v)=q_{t}(v)$ and therefore there is no rejection or roll-back; when $r\left(x_{<t}\right)=1$, the rejection rate equals $D_{\mathrm{TV}}\left(p_{t}, q_{t}\right)$.

For a fixed prefix $x_{<t}$, we formulate the goal of finding a solution to:

$$
\begin{gather*}
\left.\min _{r} \mathbb{E}_{x_{t} \sim \mathbb{P}\left(\cdot \mid x_{<t}\right)}\left[\left(1-r\left(x_{<t}\right)\right) \cdot \ell\left(x_{t}, q_{t}\right)+r\left(x_{<t}\right) \cdot \ell\left(x_{t}, p_{t}\right)\right)\right] \\
\text { s.t. } r\left(x_{<t}\right) \cdot D_{\mathrm{TV}}\left(p_{t}, q_{t}\right) \leq B \tag{7}
\end{gather*}
$$

for some budget $B>0$. Equivalently, one may minimize an unconstrained objective similar to (3), for suitable cost parameter $\alpha>0$ (see $\S$ C. 4 ):

$$
\begin{equation*}
L_{\mathrm{spec}}\left(r ; x_{<t}\right)=\mathbb{E}_{x_{t} \sim \mathbb{P}\left(\cdot \mid x_{<t}\right)}\left[\left(1-r\left(x_{<t}\right)\right) \cdot \ell\left(x_{t}, q_{t}\right)+r\left(x_{<t}\right) \cdot\left(\ell\left(x_{t}, p_{t}\right)+\alpha \cdot D_{\mathrm{TV}}\left(p_{t}, q_{t}\right)\right)\right] \tag{8}
\end{equation*}
$$

Contrasting (8) with the deferral risk in (3) for a sequential cascade, a key difference is that the cost of deferring to the larger model is no longer a constant, but depends on the similarity between $q_{t}$ and $p_{t}$, as measured by the TV distance between them.

### 4.4 Optimal speculative deferral

We next derive the optimal deferral rule for (8), and construct a feasible estimator for it.

Lemma 4 (Optimal deferral for speculative cascades). The minimizer of (8) is of the form:

$$
\begin{equation*}
r^{*}\left(x_{<t}\right)=1 \quad \Longleftrightarrow \quad \mathbb{E}_{x_{t} \sim \mathbb{P}\left(\cdot \mid x_{<t}\right)}\left[\ell\left(x_{t}, q_{t}\right)\right]>\mathbb{E}_{x_{t} \sim \mathbb{P}\left(\cdot \mid x_{<t}\right)}\left[\ell\left(x_{t}, p_{t}\right)\right]+\alpha \cdot D_{\mathrm{TV}}\left(p_{t}, q_{t}\right) \tag{9}
\end{equation*}
$$

When $p_{t}$ and $q_{t}$ are similar, the rejection rate for $q_{t}$ is low, and hence the deferral decision will depend largely on which of the two models yields a lower expected loss. When $p_{t}$ and $q_{t}$ are very different, the optimal decision is to defer to $p_{t}$ only when it yields a substantially lower loss than $q_{t}$.

Plug-in estimator for (9). The optimal rule requires estimating expectations with respect the ground-truth distribution $\mathbb{P}\left(\cdot \mid x_{<t}\right)$. We employ similar plug-in estimators as the ones used with sequential cascades (\$3). When $\ell=\ell_{0-1}$, we replace the expected $0-1$ loss with (one minus) the maximum probability from the model, giving us:

$$
\begin{equation*}
\hat{r}_{\mathrm{OPT}}\left(x_{<t}\right)=1 \quad \Longleftrightarrow \quad \max _{v} q_{t}(v)<\max _{v} p_{t}(v)-\alpha \cdot D_{\mathrm{TV}}\left(p_{t}, q_{t}\right) \tag{1}
\end{equation*}
$$

The efficacy of the plug-in estimator depends on how closely the individual models approximate the ground-truth distribution $\mathbb{P}\left(\cdot \mid x_{<t}\right)$; this is formalized by the following regret bound:

Lemma 5 (Regret bound for $\hat{r}_{\text {OPT }}$ ). Suppose $\ell=\ell_{0-1}$. Then for a fixed prefix $x_{<t}$ :

$$
L_{\mathrm{spec}}\left(\hat{r}_{\mathrm{OPT}} ; x_{<t}\right)-\min _{r} L_{\mathrm{spec}}\left(r ; x_{<t}\right) \leq \max _{v \in \mathcal{V}}\left|\mathbb{P}\left(v \mid x_{<t}\right)-q_{t}(v)\right|+\max _{v \in \mathcal{V}}\left|\mathbb{P}\left(v \mid x_{<t}\right)-p_{t}(v)\right|
$$

One can now run the speculative cascading procedure in Algorithm 5 using 10) as the deferral rule; the corresponding $\delta(\cdot)$ is listed in Table 1. See $\$$ C.2 for a similar derivation for $\ell=\ell_{\text {log }}$.

Remark 3 (Optimal deferral with temperature sampling). In practice it is common to apply a temperature scaling to both $q_{t}$ and $p_{t}$, and sample from $\tilde{q}_{t}(v) \propto q_{t}(v)^{1 / T}$ and $\tilde{p}_{t}(v) \propto p_{t}(v)^{1 / T}$ respectively, for a temperature parameter $T>0$. In this case, the constrained problem in (7) would use the TV distance between the temperature-scaled distributions $\tilde{q}_{t}$ and $\tilde{p}_{t}$ to measure the resulting rejection rate, and the optimal deferral rule in Lemma 4 would now use $D_{\mathrm{TV}}\left(\tilde{p}_{t}, \tilde{q}_{t}\right)$ instead of $D_{\mathrm{TV}}\left(p_{t}, q_{t}\right)$. To construct a plug-in estimator to this optimal rule, we still prescribe using the unscaled probabilities $q_{t}$ and $p_{t}$ to estimate the expected loss, giving us, for $\ell=\ell_{0-1}$ :

$$
\tilde{r}_{\mathrm{OPT}}\left(x_{<t}\right)=1 \quad \Longleftrightarrow \quad \max _{v} q_{t}(v)<\max _{v} p_{t}(v)-\alpha \cdot D_{\mathrm{TV}}\left(\tilde{p}_{t}, \tilde{\tilde{t}}_{t}\right)
$$

One may run Algorithm 5 with $\tilde{r}_{\text {OPT }}$ as the deferral rule and the temperature scaled $\tilde{q}_{t}$ as the drafter.

Remark 4 (Special case of greedy decoding). When temperature $T \rightarrow 0, D_{\mathrm{TV}}\left(\tilde{p}_{t}, \tilde{q}_{t}\right)=1$ whenever $\operatorname{argmax}_{v} p_{t}(v) \neq \operatorname{argmax}_{v} q_{t}(v)$, and is zero otherwise. In this case, running Algorithm 5 with $\tilde{r}_{\text {OPT }}$ as the deferral rule (and $\tilde{q}_{t}$ as the drafter) is equivalent to running it with $\hat{r}_{\text {Diff }}$ in (5) as the deferral rule. In other words, for greedy decoding, the optimal deferral rules for a speculative cascade coincides with that for a sequential cascade. We formalize this in Lemma 8 in C.3.

## 5 Further related work

There has been a stream of work on improving the draft generation process in speculative decoding; these include having the drafter and verifier share the same backbone [39, 25, 6, 30, 21, 51, 16, 27], using multiple small draft models [9, 46], using tree-structured draft batches [38, 29], distilling the drafter with the verifier [52], and leveraging multiple sampled candidates from the drafter [40].

The work that is most closely related to our specific proposal is the Big Little Decoder (BiLD) [24], which can be seen as another lossy variant of speculative decoding [26, 44, 52]. BiLD has two phases: a fallback phase, during which the drafter $q$ is run auto-regressively until its maximum predicted probability is sufficiently low; and a rollback phase, during which the verifier $p$ is run in parallel on the prefixes generated by $q$ and rolls back to the point where $D(q, p)>\alpha$, for a metric $D$ that measures discrepancy and threshold $\alpha$. The fallback phase can be seen as implementing Chow's deferral rule (2), and allows for the draft window size to vary dynamically based on an estimate of how likely the draft tokens will be accepted; the rollback phase can be seen as a deterministic variant of the rejection sampling algorithm of [26].

An advantage of BiLD over the rejection sampling algorithm in [26] is the use of Chow's rule to vary the draft window size. However, the final target distribution it seeks to mimic, $\mathbb{T}_{\text {BiLD }}(q, p)(v)=$ $\mathbf{1}(D(q, p) \leq \alpha) \cdot q(v)+\mathbf{1}(D(q, p)>\alpha) \cdot p(v)$, is an approximation to $p$, in that the target distribution $\pi=\mathbb{T}_{\text {BiLD }}(q, p)$ is chosen to satisfy $D(\pi, p) \leq \alpha$. Hence, in cases where $q$ deviates substantially from $p$, BiLD would choose $p$ as the target distribution, even $q$ offers better quality (as captured by a suitable loss function in \$4.4) on a prefix. In contrast, our proposed approach in $\$ 4$ uses speculative decoding to approximate target distributions that optimally cascade between $q$ and $p$. In our experiments, we compare the efficacy of using $\mathbb{T}_{\text {BiLD }}$ as the target distribution with the target distributions we propose in this paper (see Table 1). propose in this paper (see Table 1).

## 6 Experimental results

We present experiments to demonstrate that the proposed speculative cascading techniques yield better cost-quality trade-offs compared to both sequential token-level cascades and standard speculative decoding. We construct speculative cascades with three different deferral rules: (i) Chow in (2), (ii) Diff in (5) and (iii) OPT in 10). Our experimental setup is based on (52]; see D. 1 for details.

Tasks and models. We consider three benchmark language datasets: (i) WMT EN $\rightarrow$ DE translation 4], (ii) CNN/Daily Mail summarization [20], and (iii) XSum abstractive summarization [31. We construct cascades from T5 v1.1 family of encoder-decoder models [34], with T5-small (77M) as the small model, and either T5-large (800M) or T5-XL (3B) as the large model. In each case, we supervised fine-tune these models on the respective task. We use BLEU score as the evaluation metric for the translation tasks and ROUGE-2 as the metric for the summarization tasks. To measure latency, we follow the protocol in [26, 52, and evaluate the wall-clock decoding time with a batch size of 1 .

Baselines. Since our primary goal is to demonstrate the benefits of combining both cascading and speculative decoding, we compare against representative methods from both paradigms:

(i) Sequence-level cascade [22, 18] based on sequence-level Chow's rule in (1) (SeqCascade [Chow])

(ii) Token-level cascade outlined in Algorithm 2. with token-level Chow's rule in (2) used for deferral [10, 19] (TokenCascade [Chow])

(iii) Lossy speculative decoding described in \$2, with both $\beta=1$ [26, 52] (SpecDecode [Lossy]) and with $\beta$ tuned using the procedure in [44] (SpecDecode [Lossy*])

(iv) A variant of the Big-Little Decoder approach [24, which applies Algorithm 4 to the target distribution $\mathbb{T}_{\text {BiLD }}$ in $\S 5($ BiLD* $)$.

We do not include the oracle approach in Algorithm 3 as it is impractical to evaluate its running time. We also note that, while one could potentially integrate other recent variants of the speculative decoding (e.g., those in $\$ 5$ that warrant changes to the model architectures) into our speculative cascades framework, in the interest of a fair comparison, we use the original version by [26.

We evaluate all methods under both greedy decoding $(T=0)$, and temperature sampling with temperature $T=1$. As noted in 4.3 , with greedy decoding, the OPT deferral rule coincides with the Diff deferral rule. We set the block size $\gamma=5$ for the methods based on speculative execution.

Results. In Figure 2, we present plots of quality versus latency for the different methods. In each case, we vary the lenience parameter $\alpha$, and plot either the BLEU or ROUGE-2 metric as a function of the relative latency to the larger model. For brevity, we include the three main baselines; in \$D. 3

![](https://cdn.mathpix.com/cropped/2024_06_04_77db090807aa84cabcdfg-10.jpg?height=46&width=681&top_left_y=1690&top_left_x=234)

Clearly, methods that use speculative execution are considerably faster than sequential token-level cascades, although sequential cascades do have a slight advantage in the low-latency regimes (unlike speculative approaches, which always call the large model after every $\gamma$ steps, a sequential cascade only invokes the large model when the small model defers). Under temperature sampling $(T=1)$, the OPT speculative cascading strategy is seen to provide the best quality metrics for most latency values, with Diff coming in a close second. A similar trend is also seen with greedy decoding, where the optimal deferral strategy (SpecCascade [Diff]) often yields better quality trade-offs than the other methods, although the gap to speculative decoding is much smaller.

In Table 2, we report (i) the reduction in latency from different methods when matching the quality of the large model, and (ii) the best quality that each method can deliver without exceeding the latency of the large model (for temperature $T=1$ ). SpecCascade [OPT] is seen to yield the maximum speed-up and the best quality metrics. The cascading approaches are often seen to fare poorly on both quality and latency metrics, with the exception of WMT, where SeqCascade yields non-trivial speed-ups.

Table 2: Reduction in latency from different methods when matching the quality of the large model (cols $2-7$ ), and the best quality metric when matching each method yields without exceeding the latency of the large model (cols 8-13). Quality is measured in terms of the BLEU score for WMT and ROUGE-2 for XSum and CNNDM. All results are under temperature sampling with $T=1$.

| Method | Latency $\downarrow$ when matching large model's quality |  |  |  |  |  | Best quality without exceeding large model's latency |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Small $\rightarrow$ Large |  |  | Small $\rightarrow$ XL |  |  | Small $\rightarrow$ Large |  |  | Small $\rightarrow$ XL |  |  |
|  | WMT | XSum | CNNDM | WMT | XSum | CNNDM | WMT | XSum | CNNDM | WMT | XSum | CNNDM |
| SeqCascade [Chow] | $1.55 \times$ | $0.84 \times$ | $0.98 \times$ | $2.46 \times$ | $0.93 \times$ | $0.94 \times$ | 16.56 | 12.97 | 9.91 | 16.29 | 16.40 | 11.18 |
| TokenCascade [Chow] | $1.03 \times$ | $0.93 \times$ | $1.40 \times$ | $1.46 \times$ | $0.82 \times$ | $1.51 \times$ | 16.52 | 13.30 | 10.36 | 16.65 | 17.09 | 11.44 |
| SpecDecode [Lossy] | $1.61 \times$ | $1.10 \times$ | $1.57 \times$ | $2.17 \times$ | $1.28 \times$ | $2.07 \times$ | 17.26 | 13.90 | 10.43 | 16.94 | 17.36 | 11.53 |
| BiLD* | $1.34 \times$ | $1.04 \times$ | $1.38 \times$ | $1.85 \times$ | $1.28 \times$ | $1.84 \times$ | 16.49 | 13.81 | 10.14 | 15.90 | 17.35 | 11.35 |
| SpecCascade [Chow] | $1.43 \times$ | $1.04 \times$ | $1.41 \times$ | $2.01 \times$ | $1.28 \times$ | $1.97 \times$ | 17.76 | 13.82 | 10.28 | 16.35 | 17.36 | 11.39 |
| SpecCascade $\mid$ Diff $]$ | $1.79 \times$ | $1.17 \times$ | $1.75 \times$ | $2.44 \times$ | $1.30 \times$ | $2.15 \times$ | 18.04 | 14.00 | 10.64 | 18.07 | 17.37 | 11.67 |
| SpecCascade $[\mathrm{OPT}]$ | $1.95 \times$ | $1.17 \times$ | $1.80 \times$ | $2.61 \times$ | $1.34 \times$ | $2.21 \times$ | 18.33 | 14.10 | 10.86 | 18.09 | 17.48 | 11.85 |

![](https://cdn.mathpix.com/cropped/2024_06_04_77db090807aa84cabcdfg-11.jpg?height=806&width=1156&top_left_y=922&top_left_x=476)

Figure 2: Plots of quality vs. latency. Each method interleaves T5-small with T5-large (or T5-XL). The $x$-axis tracks the latency relative to that of calling the large model on all inputs. The horizontal dotted line denotes the quality of the large model. See Appendix $\mathrm{D}$ for additional plots.

## 7 Conclusions

We have proposed new speculative cascading techniques that use a combination of auto-regressive drafting and parallel verification to implement their deferral rule, and shown that they yield better cost-quality trade-offs than standard cascades and speculative decoding. In the future, we wish to replace our plug-in estimators with a router model [18] trained explicitly on ground-truth samples to approximate the optimal rule. We also wish to improve the deferral objective we seek to optimize at each position $t$ (8), and replace it with a global (coupled) deferral objective that takes all prefixes from 1 to $T$ into account. Another useful direction to explore would be to extend our proposal to handle a general cascade with more than two models.

## References

[1] Rohan Anil and et al. PaLM 2 technical report, 2023.

[2] Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX20B: An open-source autoregressive language model. In Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias Gallé, editors, Proceedings of BigScience Episode \#5 - Workshop on Challenges E Perspectives in Creating Large Language Models, pages 95-136, virtual+Dublin, May 2022. Association for Computational Linguistics.

[3] Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pages 12-58, 2014.

[4] Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale s Tamchyna. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12-58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics.

[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020.

[6] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024.

[7] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.

[8] Lingjiao Chen, Matei Zaharia, and James Zou. FrugalGPT: How to use large language models while reducing cost and improving performance, 2023.

[9] Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, and Kevin Chen-Chuan Chang. Cascade speculative drafting for even faster LLM inference. arXiv preprint arXiv:2312.11462, 2023.

[10] C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on information theory, 16(1):41-46, 1970.

[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,

Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways, 2022.

[12] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.

[13] Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Rühle, Laks V. S. Lakshmanan, and Ahmed Hassan Awadallah. Hybrid LLM: Cost-efficient and quality-aware query routing. In The Twelfth International Conference on Learning Representations, 2024.

[14] David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, and Charles Sutton. Language model cascades, 2022.

[15] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In International Conference on Learning Representations, 2020.

[16] Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, et al. Layer skip: Enabling early exit inference and self-speculative decoding. arXiv preprint arXiv:2404.16710, 2024.

[17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, page 1321-1330. JMLR.org, 2017.

[18] Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna Menon, and Sanjiv Kumar. Language model cascades: Token-level uncertainty and beyond. In The Twelfth International Conference on Learning Representations, 2024.

[19] Neha Gupta, Jamie Smith, Ben Adlam, and Zelda E Mariet. Ensembles of classifiers: a biasvariance perspective. Transactions of Machine Learning Research, 2022.

[20] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015.

[21] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, and Sophia Shao. Speed: Speculative pipelined execution for efficient decoding. arXiv preprint arXiv:2310.12072, 2023.

[22] Wittawat Jitkrittum, Neha Gupta, Aditya K Menon, Harikrishna Narasimhan, Ankit Rawat, and Sanjiv Kumar. When does confidence-based cascade deferral suffice? Advances in Neural Information Processing Systems, 36, 2024.

[23] Leila Khalili, Yao You, and John Bohannon. Babybear: Cheap inference triage for expensive language models, 2022.

[24] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[25] Taehyeon Kim, Ananda Theertha Suresh, Kishore Papineni, Michael Riley, Sanjiv Kumar, and Adrian Benton. Towards fast inference: Exploring and improving blockwise parallel drafts. arXiv preprint arXiv:2404.09221, 2024.

[26] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 19274-19286. PMLR, 23-29 Jul 2023.

[27] Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, and Yunhe Wang. Kangaroo: Lossless self-speculative decoding via double early exiting. arXiv preprint arXiv:2404.18911, 2024.

[28] Jonathan Mamou, Oren Pereg, Moshe Wasserblat, and Roy Schwartz. TangoBERT: Reducing inference cost by using cascaded architecture, 2022.

[29] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. Specinfer: Accelerating large language model serving with tree-based speculative inference and verification. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, pages 932-949, 2024.

[30] Giovanni Monea, Armand Joulin, and Edouard Grave. Pass: Parallel speculative sampling. arXiv preprint arXiv:2311.13581, 2023.

[31] Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807, 2018.

[32] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference, 2022.

[33] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. https://cdn.openai.com/research-covers/ language-unsupervised/language_understanding_paper.pdf, 2018.

[34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67, 2020.

[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.

[36] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.

[37] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Stoica, and Ce Zhang. FlexGen: High-throughput generative inference of large language models with a single GPU. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40 th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 31094-31116. PMLR, 23-29 Jul 2023.

[38] Benjamin Spector and Chris Re. Accelerating LLM inference with staged speculative decoding. arXiv preprint arXiv:2308.04623, 2023.

[39] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. CoRR, abs/1811.03115, 2018.

[40] Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. Advances in Neural Information Processing Systems, 36, 2024.

[41] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. UL2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations, 2023.

[42] Gemini Team, Rohan Anil, and et al. Gemini: A family of highly capable multimodal models, 2023.

[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.

[44] Vivien Tran-Thien. An optimal lossy variant of speculative decoding, 2023. Unsupervised Thoughts (Blog). URL: https://github.com/vivien000/mentored_decoding.

[45] Neeraj Varshney and Chitta Baral. Model cascading: Towards jointly improving efficiency and accuracy of nlp systems. arXiv preprint arXiv:2210.05528, 2022.

[46] Siqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Wang, Xuning Liang, Kejie Ma, Tianyu Feng, Xin You, Yongjun Bao, et al. Minions: Accelerating large language model inference with adaptive and collective speculative decoding. arXiv preprint arXiv:2402.15678, 2024.

[47] Xiaofang Wang, Dan Kondratyuk, Eric Christiansen, Kris M Kitani, Yair Alon, and Elad Eban. Wisdom of committees: An overlooked approach to faster and more accurate models. arXiv preprint arXiv:2012.01988, 2020.

[48] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022.

[49] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding, 2024.

[50] Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. Large language model cascades with mixture of thought representations for cost-efficient reasoning. In The Twelfth International Conference on Learning Representations, 2024.

[51] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft \& verify: Lossless large language model acceleration via self-speculative decoding. arXiv preprint arXiv:2309.08168, 2023.

[52] Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. In The Twelfth International Conference on Learning Representations, 2024.
