# Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models 

ShengYun Peng ${ }^{1} \quad$ Pin-Yu Chen $^{2} \quad$ Matthew Hull $^{1} \quad$ Duen Horng Chau $^{1}$<br>${ }^{1}$ Georgia Tech ${ }^{2}$ IBM Research<br>\{speng65, matthewhull, polo\}@gatech.edu<br>pin-yu.chen@ibm.com


#### Abstract

Safety alignment is the key to guide the behaviors of large language models (LLMs) are in line with human preferences and restrict harmful behaviors at inference time, but recent studies show that it can be easily compromised by finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin": randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community.


## 1 Introduction

Safety alignment is the foundation to bring LLMs' behaviors in line with human preferences and restrict harmful behaviors at inference time $[34,35,3]$. Though aligned LLMs have adopted one or a combination of the safety alignment methods, e.g., reinforcement learning from human feedback (RLHF) [27], instruction tuning [37], direct preference optimization (DPO) [31], and rejection sampling [32], LLM safety can easily be compromised by finetuning with only a few adversarially designed training examples. For instance, both GPT-3.5 Turbo and LLaMA-2 [35] fail to refuse users' harmful queries after only finetuning with 10 -shot harmful examples [29]. This brings practical safety concern to model deployment as customization is the desirable way for specific use case. Are all open-source LLMs equally vulnerable to finetuning? Why can simple finetuning easily break LLM's safety alignment? How fast does the model start to break during finetuning?

We discovered that all these questions can be addressed by navigating the LLM safety landscape. In deep learning literature, visualization of the model landscape has significantly improved our comprehension of generalization errors, optimization trajectories, and model ensembles [24, 23, 13]. In this paper, we introduce the notion of LLM safety landscape and quantify the risk in finetuning LLM by exploring different directions of perturbing model weights. When provided with a single model, we sample a random normalized direction to visualize its local variations. When given two models varied by fine-tuning, we utilize linear interpolation to visualize the changes between them. The shape of the landscape dictates the fine-tuning attributes: a sharp change in the safety metric indicates that the aligned model is a local minimum, making it challenging to find a point that is both

A. Safety basin universally appears in open-source LLMs' parameter spaces. Randomly perturbing model weights maintains safety level of original aligned model (light purple dot) in its local neighborhood.
![](https://cdn.mathpix.com/cropped/2024_06_04_6adc80c1159a39ec7470g-02.jpg?height=842&width=1184&top_left_y=316&top_left_x=449)

Figure 1: A. "Safety basin", a new phenomenon observed universally in the model parameter space of popular open-source LLMs. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. B. Visualizing the safety landscape of the aligned model also enables us to understand why finetuning with harmful data compromises safety but finetuning with both harmful and safe data preserves the safety.

safe and useful, whereas a flat local landscape offers more opportunities to discover a model that better balances safety and usefulness. Our landscape navigation provides a suite of four new insights that facilitate the understanding the LLM safety (Fig. 1):

1. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin": randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood. The safety basin is evident in both 1D and 2D safety landscape of LLaMA2, LLaMA3, Vicuna, and Mistral across various random directions and different safety benchmarks. Our discovery inspires us to propose the new VISAGE safety metric, the acronym for volumetric index for safety alignment guided by explanation, which measures the safety of an LLM's local region in model parameter spaces. (Sec. 3)
2. Visualizing the safety landscape of the aligned model enables us to understand, for the first time, how finetuning compromises safety by dragging the model away from the safety basin. We discover that different LLMs have varying rates of vulnerability to finetuning, and our task agnostic VISAGE safety metric measures the risks in finetuning without assumptions on the finetuning dataset, where a higher VISAGE score means the model after finetuning is safer. Though finetuning can easily break the safety alignment, we demonstrate that as long as the finetuning process stays within the safety basin, the safety of the finetuned model remains intact. (Sec. 4)
3. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. We evaluate the impact of system design on LLaMA2, LLaMA3, Vicuna, and Mistral, using each LLM's default system prompt as the baseline. From an attacker's standpoint, we find that both removing the default system prompt and using simple roleplaying jeopardize the safety alignment, with the former exhibiting greater potency. From a defender's perspective, we discover that LLaMA2's original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin. (Sec. 5)
4. When evaluating the safety landscape using jailbreaking queries, we find that these queries are highly sensitive to perturbations in model weights. We have collected the adversarial prompts targeting LLaMA2 and Vicuna, generated by jailbreak attacks from the literature [42, $8,9]$. Our safety landscape analysis shows that although the aligned model is vulnerable to jailbreak attacks, slighlty perturbing the model weights in the local space of the aligned model can significantly lower the attack success rate (ASR) of these jailbreaking attacks. A naive defense method is to perturb the model weights before generating the response. However, attackers can also create stronger attacks that target both the aligned model and multiple perturbed models in its local region. These observations from our safety landscape research provide new insights for future work on LLM attacks and defenses. (Sec. 6)

## 2 Background and Related Works

LLM safety alignment. LLMs are language models with a large number of parameters trained on web-scale text corpra $[5,1,35,10,20]$. LLMs have exhibited emergent capabilities that can be broadly applied in a task-agnostic manner, such as in-context learning [5], chain-of-thought reasoning [38], and mathematical reasoning [18]. These capabilities are largely attributed to the alignment of LLMs with expected human values and intentions, which involves training the model to follow instructions and being helpful, truthful, and harmless [27]. Specifically, harmless is achieved by safety alignment that empowers the LLM with safety guardrails so that the model can refuse harmful instructions. Common safety alignment techniques are instruction tuning [37], RLHF [27], DPO [31], rejection sampling [25], and self-alignment [33]. However, these techniques are not designed to cover the safety risks that may arise from the subsequent custom finetuning and jailbreak attacks. Recent work have shown that both simple finetuning [29] and jailbreak attacks [8, 42] can circumvent safety gaurdrails of aligned LLMs.

Simple finetuning breaks LLM safety alignment. Finetuning is widely employed to customize open-source LLMs for downstream applications [17, 11]. Typically, finetuning directly updates the parameters of pretrained models using a small dataset to enhance performance on downstream tasks. However, finetuning with a few adversarially designed training examples, or even with a benign dataset, can compromise the safety alignment of LLMs. Qi et al. [29] finetuned GPT-3.5 Turbo and LLaMA2-7b-chat with only 10 harmful examples, but the safety guardrails were undermined in both LLMs. Zhan et al. [39] removed the safety protections of GPT-4 with $95 \%$ success with only 340 examples trained with the OpenAI's finetuning API.

Enhancing alignment with safety prompts. To communicate with LLM with precise and taskspecific instructions, Bsharat et al. [6] presented a comprehensive principled instructions and guidelines to improve the quality of prompts for LLMs. Recently, safety researchers have also experimented with various prompts to either break or enhance LLM safety alignment. On the attack side, Jin et al. [21] used roleplaying to automatically and iteratively generate harmful prompts. On the defense side, Zheng et al. [40] leveraged prompt tuning to enhance LLM safety by directly optimizing the vanilla system prompt into safety prompt. Safety prompt is a method of safeguarding LLMs against harmful queries without changing the model weights; they are prepended to the user input or serve as a system prompt.

Jailbreaking aligned LLMs with adversarial attacks. A class of vulnerabilities known as "jailbreaks" has recently been shown to cause LLMs to violate their alignment safeguards [7, 30, 36]. Prompt-level jailbreaks use deception and social engineering to elicit objectionable content from LLMs, requiring creativity, manual dataset curation, and significant human effort $[8,12]$. Token-level jailbreaks optimize the tokens input to the LLM, recognized for their effectiveness but requiring extensive computational resources and being often uninterpretable to humans [42, 22].

## 3 From LLM Safety Landscape to VISAGE Safety Metric

The model landscape is a crucial tool for interpreting model behaviors and understanding model characteristics. Perturbing a model along random directions reveals the local behavior of the model, while interpolating the parameters between two models illustrates the transition process from one model to the other. In this section, we introduce the notion of LLM safety landscape in both 1D (Sec. 3.1) and 2D (Sec. 3.2) scenarios. Sec. 3.3 presents the safety landscape of four popular open-source

![](https://cdn.mathpix.com/cropped/2024_06_04_6adc80c1159a39ec7470g-04.jpg?height=304&width=531&top_left_y=257&top_left_x=469)

(a) Safety landscape between pretrained and aligned LLaMA2 models.

![](https://cdn.mathpix.com/cropped/2024_06_04_6adc80c1159a39ec7470g-04.jpg?height=309&width=570&top_left_y=255&top_left_x=1081)

(b) Our VISAGE safety metric is stable along different random directions.

Figure 2: LLM safety landscape: (a) When given two models varied by fine-tuning, we utilize linear interpolation to visualize the changes between them. (b) When provided with a single model, we sample a random normalized direction to visualize its local variations along both positive and negative directions.

LLMs and Sec. 3.4 introduces "LLM safety basin" concept and proposes the VISAGE safety metric based on our landscape analysis.

### 3.1 1D Safety Landscape

Denote $\boldsymbol{\theta}$ as the initial LLM model weights. The safety landscape is plotted by perturbing $\boldsymbol{\theta}$ along a certain direction $\widehat{\boldsymbol{d}_{1}}$ and evaluate the new model weights with a single model safety metric:

$$
\begin{equation*}
f(\alpha)=\mathcal{S}\left(\boldsymbol{\theta}+\alpha \widehat{\boldsymbol{d}_{1}}\right) \tag{1}
\end{equation*}
$$

where $\mathcal{S}$ is the safety metric defined for a single model, and $\alpha$ is a scalar parameter. For 1Dinterpolation, we pick two sets of model weights $\boldsymbol{\theta}$ and $\boldsymbol{\theta}^{\prime}$ and the direction is defined by the line connecting these two points, $\widehat{\boldsymbol{d}_{1}}=\boldsymbol{\theta}^{\prime}-\boldsymbol{\theta}$. For 1D-random, $\boldsymbol{\theta}$ is the center point and we randomly sample a direction $\boldsymbol{d}_{1}$ from Gaussian distribution. We apply layer normalization to $\boldsymbol{d}_{1}$ to exclude the effect of scale invariance [24] so that the flatness and the sharpness across different landscape plots are comparable. Specifically, $\boldsymbol{d}_{1}$ is normalized to a unit direction and then multiplied by the Frobenius norm of each layer:

$$
\begin{equation*}
\widehat{\boldsymbol{d}_{1 i}}=\frac{\boldsymbol{d}_{1 i}}{\left\|\boldsymbol{d}_{1 i}\right\|}\left\|\boldsymbol{\theta}_{i}\right\| \tag{2}
\end{equation*}
$$

In the rest of the paper, we will use 1D-random $\boldsymbol{\theta}$ and 1D-interpolation $\boldsymbol{\theta} \rightarrow \boldsymbol{\theta}^{\prime}$ to represent the above two types of 1D directions.

### 3.2 2D Safety Landscape

Similar to the 1D landscape, the 2D landscape requires two directions $\widehat{d_{1}}$ and $\widehat{d_{2}}$ and the safety landscape is defined as:

$$
\begin{equation*}
f(\alpha, \beta)=\mathcal{S}\left(\boldsymbol{\theta}+\alpha \widehat{\boldsymbol{d}_{1}}+\beta \widehat{\boldsymbol{d}_{2}}\right) \tag{3}
\end{equation*}
$$

For 2D random, since both directions are randomly sampled from Gaussian distribution, the cosine similarity between $\widehat{\boldsymbol{d}_{1}}$ and $\widehat{\boldsymbol{d}_{2}}$ is $\sqrt{2 /(\pi n)}$ [15], where $n$ is the dimension of $\boldsymbol{\theta}$. Given current LLMs have billions of parameters, the two random directions are orthogonal and we only perform layer normalization as in Eq. 2. For 2D interpolation, we pick three sets of model weights $\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime}$, and $\boldsymbol{\theta}^{\prime \prime}$ and compute the interpolated directions $\boldsymbol{d}_{1}=\boldsymbol{\theta}^{\prime}-\boldsymbol{\theta}, \boldsymbol{d}_{2}=\boldsymbol{\theta}^{\prime \prime}-\boldsymbol{\theta}$. Since there is no guarantee that two interpolated directions are orthogonal, we use Gram-Schmidt algorithm to find the orthogonal basis:

$$
\begin{equation*}
\widehat{\boldsymbol{d}_{1}}=\boldsymbol{d}_{1}, \widehat{\boldsymbol{d}_{2}}=\boldsymbol{d}_{2}-\frac{\boldsymbol{d}_{1}^{T} \boldsymbol{d}_{2}}{\left\|\boldsymbol{d}_{1}\right\|^{2}} \boldsymbol{d}_{1} \tag{4}
\end{equation*}
$$

To ensure the scale equivalence of two directions, we rescale $\widehat{d_{2}}$ to $\left(\left\|\widehat{d_{1}}\right\| /\left\|\widehat{d_{2}}\right\|\right) \widehat{d_{2}}$. 2Dinterpolation landscape is useful when analyzing two finetuned model weights, which are all initialized by the same aligned LLM. In the rest of the paper, we will use 2D-random $\boldsymbol{\theta}$ and 2D-interpolation $\boldsymbol{\theta}$ $\rightarrow \boldsymbol{\theta}^{\prime} \& \boldsymbol{\theta}^{\prime \prime}$ to represent the above two types of $2 \mathrm{D}$ directions.

### 3.3 Safety Landscape of Open-source LLMs

We show the safety landscapes of four popular open-source LLMs: LLaMA2-7B-chat [35], LLaMA38B-instruct [2], Mistral-7B-instruct-v0.2 [20], and Vicuna-7B-v1.5 [10]. For each perturbed model along the landscape direction, we evaluate on the first 80 prompts of AdvBench [42] "Harmful Behaviors" split (Adv 80) with ASR as the safety metric. The ASR is measured by refusal keyword detection following the original AdvBench evaluation protocal. Note that $\mathcal{S}$ can be any harmfulness evaluation metric, e.g., LLM Judge [41] or Llama Guard [19]. Since recent user study [29] shows that GPT-4 Judge and refusal keyword detection perform closely on flagging harmful content, we use keyword detection as it is the fastest. We interpolate 20 steps on each axis for all landscapes. To ensure deterministic results, we set top-p as 0 and temperature as 1 for greedy decoding [16].

Safety landscape between pretrained and aligned LLMs. Pretraining is the initial phase of training an LLM, aimed at developing a broad understanding of language and knowledge [11, 5]. Alignment, on the other hand, focuses on training LLMs to better follow instructions in prompts and align their behaviors with human preferences [27]. Since the pretrained model is not designed with safety in its first priority, it lacks safety guardrails. In contrast, the aligned model is expected to refuse to respond to harmful user queries. We use LLaMA2 as an example and show the safety landscape of 1D-interpolation LLaMA2-7B $\rightarrow$ LLaMA2-7B-chat in Fig. 2a. Since the pretrained (LLaMA2-7B) and the aligned (LLaMA2-7B-chat) models use different chat templates, both templates are evaluated to ensure the change of safety is due to alignment. Notice that the chat template difference does not exist when comparing the aligned and the finetuned models in later sections as all of them share the same chat template and system prompt. Both lines in Fig. 2a show that the ASR of the aligned model is significantly lower than the pretrained model as expected. Notice that the pretrained model has a less than 100 ASR when using the aligned model chat template. This is because the pretrained model repeats the system prompt in the aligned model's chat template, which is captured by the keyword detector and treated as successful refusal. When using the aligned model's chat template, we find that the local region of the aligned model shows the same level of safety. This is surprising because early work has shown that finetuning can easily break LLM's safety alignment, which may imply the aligned model is a cusp, i.e., sharp corner, on the safety landscape.

Safety landscape of an aligned LLM. Inspired by our findings in the interpolation direction above, we are curious whether this flat safety region exists in other directions for different LLMs. Fig. 1 (top) plots the 1D and 2D random landscape of four popular open-source LLMs. For each LLM, we use the default system prompt provided by the model, with details in Appendix A. We discover that each aligned LLM serves as a robust anchor point, maintaining safety within its local region, but the safety is completely compromised outside of this local region, and the change of safety as steep as a step function. We term this new phenomenon observed universally in the LLM parameter space as "safety basin". All four investigated LLMs exhibit such phenomenon, but the differences are the depth and width of the basin.

### 3.4 ViSAGE Safety Metric

The landscape visualization and analysis suggest that the average depth of the safety basin can serve as a good indicator for measuring LLM safety, reflecting both the safety of the original aligned model and the robustness of the model when its parameters are perturbed. Formally, for an $n$-D random safety landscape, we define VISAGE safety metric as the average safety margin of all models we have sampled along all random directions:

$$
\begin{equation*}
\text { VISAGE }=\underset{\alpha \sim \mathcal{U}(-a, a), \beta \sim \mathcal{U}(-b, b), \ldots}{\mathbb{E}}\left[\mathcal{S}_{\max }-\mathcal{S}(\alpha, \beta, \ldots)\right], \text { s.t. } \mathcal{S}<\mathcal{S}_{\text {max }} \tag{5}
\end{equation*}
$$

where $\alpha$ and $\beta$ are all sampled from uniform distribution and we use $a=b=0.5$ as we find that LLMs are completely broken after perturbing more than half of its norm. $\mathcal{S}$ is a monotonically decreasing function in terms of safety, as a lower $\mathcal{S}$ means a safer model. $\mathcal{S}_{\text {max }}$ is the maximum possible value for $\mathcal{S}$. When ASR is used as the safety metric, $\mathcal{S}_{\max }=100$.

Stability of VISAGE. Since the definition of VISAGE involves random directions, we run a stability test to find out how many sampled directions the model converge to a stable value. We take 1Drandom LLaMA2-7B-chat as an example and show the result in Fig. 2b. We sample 8 different directions until the average converges and find that the average of 3 different directions is close
enough to the final average. Thus, we use the mean of VISAGE along three different directions as the evaluation metric in the rest of the paper. In Fig. 1, we also compute VISAGE for both 1D and $2 \mathrm{D}$ random landscape and the ranking of those two dimensions remain the same. Thus, we use 1D VISAGE for faster evaluation.

We compute the VISAGE score for all four LLMs using their default system prompts and chat templates. Since LLaMA3-8B-instruct does not have a default system prompt, we use the LLaMA2 system prompt. The VISAGE ranking is as follows: LLaMA3-8B-instruct > LLaMA2-7B-chat > Mistral-7B-instruct-v0.2 > Vicuna-7B-v1.5. We also evaluate these four models on all 520 prompts of AdvBench "Harmful Behaviors" split (Adv 520). The ASR of the models are as follows: LLaMA38B-instruct (0.38), LLaMA2-7B-chat (0.19), Mistral-7B-instruct-v0.2 (1.15), and Vicuna-7B-v1.5 (2.5). Although the ASRs of all four LLMs are close, the VISAGE reflects the safety of a model's local region, indicating the risk after finetuning, which we will explore in the next section.

## 4 Why can simple finetuning easily break LLM's safety alignment?

In this section, we navigate the LLM safety landscape and explore why safety alignment can be easily compromised by finetuning with only a few adversarially designed training examples. Sec. 4.1 details the finetuning settings. In Sec. 4.2, we discover that different LLMs have varying rates of vulnerability to finetuning, and our task agnostic VISAGE safety metric measures the risks in finetuning without assumptions on the finetuning dataset. In Sec. 4.3, visualizing the safety landscape of the aligned model enables us to understand, for the first time, how finetuning compromises safety by dragging the model away from the safety basin. Though finetuning can easily break the safety alignment, we demonstrate that as long as the finetuning process stays within the safety basin, the safety of the finetuned model remains intact in Sec. 4.4.

### 4.1 Finetuning settings

We finetune on the harmful samples created by Qi et al. [29], which were sampled from Anthropic red-teaming dataset [14]. Following the standard OpenAI finetuning API [28], each training sample is structure in a one-round conversation format.

We ensure that the system prompt used during finetuning remains consistent with the aligned model so that the differences in safety are indeed induced by finetuning. We adhere to the official funetuning recipe $^{1}$ and conduct full parameter finetuning. Following the training hyperparameters in Qi et al. [29], all models are finetuned for five epochs with AdamW optimizer [26]. At inference time, we evaluate on both 80 prompts and all 520 prompts of AdvBench "Harmful Behavior" split. The finetuning is done on 4 A100 GPUs.

### 4.2 Finetuning on few-shot harmful data breaks LLM's safety alignment

We finetune LLaMA2-7B-chat and Vicuna-7B-v1.5 on subsets of 10, 50, and 100 harmful examples sampled from the training dataset. As shown in Table 1, both models have close to $0 \%$ ASR before finetuning, but the ASRs increases significantly after finetuning, indicating broken safety alignment. Comparing the results of finetuning on 10, 50, 100 harmful examples, the ASRs continue to increase as expected. We also discover that different LLMs have varying rates of vulnerability to finetuning, and our VISAGE safety metric can successfully measures the risks in finetuning before actual finetuning. Comparing LLaMA2 with Vicuna, LLaMA2 has a higher VISAGE score than Vicuna, meaning that when both are finetuned on the same user data, LLaMA2 shows a lower ASR than Vicuna when evaluated on safety benchmarks. Our evaluation results on both 80 prompts and full 520 prompts verify that the safety in finetuning is reflected by our VISAGE safety metric. Since the VISAGE definition does not make assumptions on the downstream finetuning dataset, LLM-VISAGE serves as a task-agnostic safety metric that measures finetuning risks.[^0]

Table 1: Finetuning on few-shot harmful data breaks LLM's safety alignment at different rates and our ViSAGE safety metric successfully measures the rate. LLaMA2 has a higher VISAGE score than Vicuna, and the ASRs on AdvBench indicate that when finetuned with the same amount of harmful data, LLaMA2 remains safer than Vicuna. Additionally, we demonstrate that finetuning with a mixture of safe and harmful data helps the model maintain its safety alignment.

| Model | VISAGE | AdvBench <br> Samples | Aligned | 10-shot | 50-shot | 100-shot | mix |
| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
|  |  | 80 | 0 | 90.0 | 91.3 | 100.0 | 0 |
|  |  | 520 | 0.2 | 85.2 | 90.2 | 95.4 | 0.2 |
| Vicuna-7B-v1.5 | 73.26 | 80 | 5.0 | 95.0 | 97.5 | 100.0 | 1.3 |

### 4.3 Finetuning with harmful data is dragging the model away from the safety basin but at different rates

We save the model checkpoint of each epoch during finetuning and project them onto the safety landscape to visualize the optimization trajectory. Previous work have observed that projecting on random directions fail to capture the variation in optimization trajectory because the trajectory lies in an extremely low dimensional spaces, and a random sampled direction is nearly orthogonal to this subspace. A potential solution is applying principal component analysis (PCA) on all saved model checkpoints, but the $n$-epoch finetuning on LLaMA2-7B-chat will lead to a feature matrix of size $n \times 7 B$, which makes it expensive to compute with existing PCA libraries. Therefore, we set the projection direction as the interpolated direction between the initial and the final finetuned model weights. By projecting all saved checkpoints onto this direction, we successfully capture the optimization trajectory. Fig. 1 (red dots at the bottom 2D interpolation landscape) shows the training trajectory of finetuning LLaMA2-7B-chat on 100-shot harmful data for 5 epochs. The aligned model is the initial point and each epoch is dragging the model away from the aligned model, and finally outside of the safety basin. Our safety landscape visualization enables us to understand, for the first time, how simple finetuning compromises safety alignment.

### 4.4 Finetuning with harmful and safe data helps the model stay within the safety basin

Though finetuning can easily break LLMs' safety alignment, we demonstrate that as long as the finetuning process stays within the safety basin, the safety of the finetuned model remains intact. This can be achieved by finetuning on a mixture of user data and safety data. Bianchi et al. [4] suggests that finetuning LLaMA1 [34] (not aligned) on the mixture of user and safe data can improve the safety of the model. We are curious if this finetuning strategy is generalizable to other LLMs, and if it works, can we explain it with our safety landscape? Therefore, we finetune LLaMA2 and Vicuna on a mixture of 100 -shot harmful examples in Sec. 4.3 along with the 100 -shot safe data created by Bianchi et al. [4] for ten epochs till convergence. In Table 1, we show that finetuning with the safe data indeed lowers the ASR.

Both initialized from the aligned model, the model that is finetuned on 100 -shot harmful data from Sec. 4.3 is completely unsafe while the model that is finetuned on a mixture of 100 -shot harmful and 100-shot safe data is still safe. We take LLaMA2 as an example and show 2D-interpolation LLaMA2-7B-chat $\rightarrow$ LLaMA2-7B-chat 100-shot harmful \& 100-shot harmful+100-shot safe in Fig. 1(bottom). In the surface plot, starting from the aligned model in the origin, the pure harmful finetuning quickly drags the model away from the safety basin, and significantly elevates the ASR. On the other hand, finetuning with a mixture of harmful and safe data keeps the model within the safety basin and thus maintaining the safety of the finetuned model.

## 5 System prompt

LLM safety landscape also highlights the system prompt's critical role in protecting a model, and how this protection transfers to its perturbed variants within the safety basin. In this section, we evaluate the impact of system design on LLaMA2, LLaMA3, Vicuna, and Mistral, using each LLM's default system prompt as the baseline. We collect different types of system prompts from both an attacker's or a defender's perspective. From an attacker's standpoint, we apply two types of prompts:

Table 2: LLM safety landscape highlights the system prompt's critical role in protecting a model, and how this protection transfers to its perturbed variants in the safety basin. We measure the VISAGE score of different system prompt for popular open-source LLMs. Higher VISAGE means safer model and "-" means not applicable.

| Model | Default | Empty | Roleplay | LLaMA2 | Safety |
| :--- | ---: | ---: | ---: | ---: | ---: |
| LLaMA2-7B-chat | 85.32 | 80.68 | 86.56 | 85.32 | - |
| LLaMA3-8B-instruct |  | 81.10 | 78.40 | 90.40 | - |
| Mistral-7B-instruct-v0.1 | 74.11 | 20.78 | 52.65 | 85.66 | 86.24 |
| Mistral-7B-instruct-v0.2 | 82.04 | 64.90 | 75.54 | 73.69 | 75.53 |
| Vicuna-7B-v1.3 | 82.03 | 56.13 | 77.13 | 80.18 | - |
| Vicuna-7B-v1.5 | 77.37 | 73.56 | 81.61 | 81.62 | - |

![](https://cdn.mathpix.com/cropped/2024_06_04_6adc80c1159a39ec7470g-08.jpg?height=401&width=1187&top_left_y=656&top_left_x=466)

1D Random Mistral-7B-instruct-v0.1 Using Different System Prompts

![](https://cdn.mathpix.com/cropped/2024_06_04_6adc80c1159a39ec7470g-08.jpg?height=263&width=550&top_left_y=714&top_left_x=473)

(a) 1D-random Mistral-7B-instruct-v0.1

![](https://cdn.mathpix.com/cropped/2024_06_04_6adc80c1159a39ec7470g-08.jpg?height=304&width=566&top_left_y=669&top_left_x=1083)

(b) 1D-random Vicuna-7B-v1.5

Figure 3: The system prompt has a strong impact on LLM safety landscape. From an attacker's standpoint, we find that both removing the default system prompt and using simple roleplaying prompt jeopardizes the safety alignment, with the former exhibiting greater potency. From a defender's perspective, we discover that LLaMA2's original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin.

(1) removing the default system prompt (Empty), and (2) using roleplaying prompt to attach a new charater to the LLM, hoping to make the LLM forget its safety responsibilities (Roleplay). From a defender's perspective, we also employ two types of prompts: (1) LLaMA2's default system prompt that explicitly includes safety concerns (LLaMA2), and (2) safety prompts that are directly optimized for a specific LLM [40] (Safety). The details of the system prompts used in our experiments are listed in Appendix A. For each type of the system prompt, we compute its mean VISAGE score from three random directions. Table 2 shows the results, illustrating how each type of system prompt affects the safety of an LLM's local region.

LLaMA2. The default system prompt has a VISAGE score of 85.32. Removing the system prompt incurs a 4.64percentage points (pp) drop. Surprisingly, applying the roleplaying prompt does not compromise LLaMA's safety; instead, it leads to a slight increase in the VISAGE score. We find that roleplaying prompts are generally less effective in breaking an LLM's safety across different models Among the six LLMs tested with the default system prompt, LLaMA2 has the highest VISAGE score, aligning with the observation that LLaMA2 tends to be conservative and may even refuse harmless input prompts [40].

LLaMA3. There is no default system prompt for LLaMA3, yet even without a system prompt, LLaMA3 demonstrates a high VISAGE score. LLaMA3 excels in following system prompt instructions while maintaining its safety, experiencing only a $2.7 \mathrm{pp}$ drop when using the roleplaying prompt, but showing a 9.3pp increase when the LLaMA2 system prompt is employed. This is likely due to LLaMA3's improved training procedures, which substantially reduce false refusal rates [2].

Mistral. We evaluate both Mistral-7B-instruct-v0.1 and Mistral-7B-instruct-v0.2. Fig. 3a shows the 1D-random Mistral-7B-instruct-v0.1 safety landscape under different system prompts. For both models, removing the system prompt significantly reduces the safety score. Specifically, removing the system prompt decreases Mistral-7B-instruct-v0.1's VISAGE score by 53.33pp. Using the roleplaying prompt also degrades the performance for both models. Both LLaMA2 and safety system prompts effectively enhance Mistral's VISAGE score, but Mistral-7B-instruct-v0.1 is more sensitive to the system prompt than Mistral-7B-instruct-v0.2.

Vicuna. We have tested on both Vicuna-7B-v1.3 and Vicuna-7B-v1.5. Fig. 3b shows the 1D-random Vicuna-7B-v1.3 safety landscape under different system prompts. Vicuna-7B-v1.3 is finetuned

![](https://cdn.mathpix.com/cropped/2024_06_04_6adc80c1159a39ec7470g-09.jpg?height=510&width=1219&top_left_y=241&top_left_x=453)

1D Random LLaMA2-7B-chat Using Jailbreaking Prompts

![](https://cdn.mathpix.com/cropped/2024_06_04_6adc80c1159a39ec7470g-09.jpg?height=263&width=528&top_left_y=302&top_left_x=468)

(a) There exists certain perturbed models that are significantly safer than the original aligned model.

![](https://cdn.mathpix.com/cropped/2024_06_04_6adc80c1159a39ec7470g-09.jpg?height=301&width=566&top_left_y=259&top_left_x=1083)

(b) Replacing the default Vicuna system prompt with the LLaMA2 system prompt improves the overall safety in the model's local region.

Figure 4: When evaluating the safety landscape using jailbreaking queries, we find that these queries are highly sensitive to perturbations in model weights.

from LLaMA1, while Vicuna-7B-v1.5 is finetuned from LLaMA2 pretrained (not aligned) model weights [10]. We find that the performance drops for both models when removing the system prompt. As shown in Fig. 3b, removing the system prompt reveals that the original Vicuna model is a local maximum in the safety landscape, indicating that there exist slightly perturbed model weights more resistant to harmful inputs than the fine-tuned model. This suggests that the safety alignment of Vicuna is not optimal, possibly because the fine-tuning process rarely encountered inputs with an empty system prompt. The roleplaying prompt shows mixed performance: it decreases Vicuna-7Bv1.3's safety score but increases Vicuna-7B-v1.5's safety score. Finally, using the LLaMA2 system prompt significantly improves model safety.

Overall, the system prompt does have a strong impact on LLM safety landscape. From an attacker's standpoint, we find that both removing the default system prompt and using simple roleplaying jeopardize the safety alignment, with the former exhibiting greater potency. From a defender's perspective, we discover that LLaMA2's original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin.

## 6 Jailbreak attacks

Previous work have shown that the safeguard of the aligned LLMs can be bypassed by adversarial attacks. We are curious whether these so-called "jailbreaks" against LLMs is still effective to slightly perturbed models within the aligned model's local region. We use the adversarial prompts from JailbreakBench [9], which has incorporated jailbreaking prompts targeting LLaMA2 and Vicuna generated by GCG [42] and PAIR [8] adversarial attacks. Fig. 4a shows the 1D-random LLaMA27B-chat evaluated on jailbreaking prompts. There are only 6 prompts in JailbreakBench that can successfully attack the LLaMA2 model, and our experiment shows only 4 of them are successful, thus leading to a $66.67 \%$ ASR for the aligned model. The safety landscape reveals that the jailbreaking prompts are quite sensitive to the model weights perturbation, i.e., there exists certain perturbed models that are significantly safer than the aligned model. This is not unique to LLaMA2, as shown by the safety landscape of Vicuna-7B-v1.5 under jailbreak attacks in Fig. 4b. We also replace the default Vicuna system prompt with the LLaMA2 system prompt and find it improving the overall safety in the model's local region. A naive defense method is to perturb the model weights before generating the response. However, attackers can also create stronger attacks that target both the aligned model and multiple perturbed models in its local region. These observations from our safety landscape research provide new insights for future work on LLM attacks and defenses.

## 7 Conclusion

We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin". Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing
the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community.

## References

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[2] Meta AI. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https://ai.meta.com/blog/meta-llama-3/.

[3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

[4] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions. arXiv preprint arXiv:2309.07875, 2023.

[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

[6] Sondos Mahmoud Bsharat, Aidar Myrzakhan, and Zhiqiang Shen. Principled instructions are all you need for questioning llama-1/2, gpt-3.5/4. arXiv preprint arXiv:2312.16171, 2023.

[7] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei W Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. Are aligned neural networks adversarially aligned? Advances in Neural Information Processing Systems, 36, 2024.

[8] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023.

[9] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J Pappas, Florian Tramer, et al. Jailbreakbench: An open robustness benchmark for jailbreaking large language models. arXiv preprint arXiv:2404.01318, 2024.

[10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[12] Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. Build it break it fix it for dialogue safety: Robustness from adversarial human attack. arXiv preprint arXiv:1908.06083, 2019 .

[13] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, pages 1019-1028. PMLR, 2017.

[14] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022.

[15] Tom Goldstein and Christoph Studer. Phasemax: Convex phase retrieval via basis pursuit. IEEE Transactions on Information Theory, 64(4):2675-2689, 2018.

[16] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.

[17] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.

[18] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023.

[19] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674, 2023.

[20] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[21] Haibo Jin, Ruoxi Chen, Jinyin Chen, and Haohan Wang. Quack: Automatic jailbreaking large language models via role-playing. 2023.

[22] Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically auditing large language models via discrete optimization. In International Conference on Machine Learning, pages 15307-15329. PMLR, 2023.

[23] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.

[24] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018.

[25] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023.

[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

[27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.

[28] Andrew Peng, Michael Wu, John Allard, Logan Kilpatrick, and Steven Heidel. Gpt3.5 turbo fine-tuning and api updates, 2023. URL https://openai.com/index/ gpt-3-5-turbo-fine-tuning-and-api-updates.

[29] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023.

[30] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 21527-21536, 2024.

[31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.

[32] Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. Discriminative adversarial search for abstractive summarization. In International Conference on Machine Learning, pages 8555-8564. PMLR, 2020.

[33] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. Advances in Neural Information Processing Systems, 36, 2024.

[34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models (2023). arXiv preprint arXiv:2302.13971, 2023.

[35] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[36] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36, 2024.

[37] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.

[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.

[39] Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang. Removing rlhf protections in gpt-4 via fine-tuning. arXiv preprint arXiv:2311.05553, 2023.

[40] Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng. On prompt-driven safeguarding for large language models. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models.

[41] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.

[42] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.
