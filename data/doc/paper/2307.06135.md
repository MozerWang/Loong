# SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning 

Krishan Rana ${ }^{\dagger 1}$, Jesse Haviland ${ }^{* 1,2}$, Sourav Garg ${ }^{* 3}$, Jad Abou-Chakra*1,<br>Ian Reid $^{3}$, Niko Sünderhauf ${ }^{1}$<br>${ }^{1}$ QUT Centre for Robotics, Queensland University of Technology<br>${ }^{2}$ CSIRO Data61 Robotics and Autonomous Systems Group<br>${ }^{3}$ University of Adelaide<br>*Equal Contribution<br>†ranak@qut.edu. au


#### Abstract

: Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page sayplan.github.io.


## 1 Introduction

"Make me a coffee and place it on my desk" - The successful execution of such a seemingly straightforward command remains a daunting task for today's robots. The associated challenges permeate every aspect of robotics, encompassing navigation, perception, manipulation as well as high-level task planning. Recent advances in Large Language Models (LLMs) [1, 2, 3] have led to significant progress in incorporating common sense knowledge for robotics $[4,5,6]$. This enables robots to plan complex strategies for a diverse range of tasks that require a substantial amount of background knowledge and semantic comprehension.

For LLMs to be effective planners in robotics, they must be grounded in reality, that is, they must adhere to the constraints presented by the physical environment in which the robot operates, including the available affordances, relevant predicates, and the impact of actions on the current state. Furthermore, in expansive environments, the robot must additionally understand where it is, locate items of interest, as well comprehend the topological arrangement of the environment in order to plan across the necessary regions. To address this, recent works have explored the utilization of vision-based value functions [4], object detectors [7, 8], or Planning Domain Definition Language (PDDL) descriptions of a scene $[9,10]$ to ground the output of the LLM-based planner. However, these efforts are primarily confined to small-scale environments, typically single rooms with pre-encoded information on all the existing assets and objects present. The challenge lies in scaling these models. As the environment's complexity and dimensions expand, and as more rooms and entities enter the

![](https://cdn.mathpix.com/cropped/2024_06_04_9f5c08dcc69f0d50c30bg-02.jpg?height=948&width=1393&top_left_y=239&top_left_x=363)

Figure 1: SayPlan Overview (top). SayPlan operates across two stages to ensure scalability: (left) Given a collapsed 3D scene graph and a task instruction, semantic search is conducted by the LLM to identify a suitable subgraph that contains the required items to solve the task; (right) The explored subgraph is then used by the LLM to generate a high-level task plan, where a classical path planner completes the navigational component of the plan; finally, the plan goes through an iterative replanning process with feedback from a scene graph simulator until an executable plan is identified. Numbers on the top-left corners represent the flow of operations.

scene, pre-encoding all the necessary information within the LLM's context becomes increasingly infeasible.

To this end, we present a scalable approach to ground LLM-based task planners across environments spanning multiple rooms and floors. We achieve this by exploiting the growing body of 3D scene graph (3DSG) research [11, 12, 13, 14, 15, 16]. 3DSGs capture a rich topological and hierarchicallyorganised semantic graph representation of an environment with the versatility to encode the necessary information required for task planning including object state, predicates, affordances and attributes using natural language - suitable for parsing by an LLM. We can leverage a JSON representation of this graph as input to a pre-trained LLM, however, to ensure the scalability of the plans to expansive scenes, we present three key innovations.

Firstly, we present a mechanism that enables the LLM to conduct a semantic search for a taskrelevant subgraph $\mathcal{G}^{\prime}$ by manipulating the nodes of a 'collapsed' 3DSG, which exposes only the top level of the full graph $\mathcal{G}$, via expand and contract API function calls - thus making it feasible to plan over increasingly large-scale environments. In doing so, the LLM maintains focus on a relatively small, informative subgraph, $\mathcal{G}^{\prime}$ during planning, without exceeding its token limit. Secondly, as the horizon of the task plans across such environments tends to grow with the complexity and range of the given task instructions, there is an increasing tendency for the LLM to hallucinate or produce infeasible action sequences $[17,18,7]$. We counter this by firstly relaxing the need for the LLM to generate the navigational component of the plan, and instead leverage an existing optimal path planner such as Dijkstra [19] to connect high-level nodes generated by the LLM. Finally, to ensure the feasibility of the proposed plan, we introduce an iterative replanning pipeline that verifies and refines the initial plan using feedback from a scene graph simulator in order to correct for any unexecutable actions, e.g., missing to open the fridge before putting something into it - thus avoiding planning failures due to inconsistencies, hallucinations, or violations of the physical constraints and predicates imposed by the environment.

Our approach SayPlan ensures feasible and grounded plan generation for a mobile manipulator robot operating in large-scale environments spanning multiple floors and rooms. We evaluate our framework across a range of 90 tasks organised into four levels of difficulty. These include semantic search tasks such as ("Find me something non-vegetarian.") to interactive, long-horizon tasks with ambiguous multi-room objectives that require a significant level of common-sense reasoning ("Let's play a prank on Niko"). These tasks are assessed in two expansive environments, including a large office floor spanning 37 rooms and 150 interactable assets and objects, and a three-storey house with 28 rooms and 112 objects. Our experiments validate SayPlan's ability to scale task planning to large-scale environments while conserving a low token footprint. By introducing a semantic search pipeline, we can reduce full large-scale scene representations by up to $82.1 \%$ for LLM parsing and our iterative replanning pipeline allows for near-perfect executability rates, suitable for execution on a real mobile manipulator robot. ${ }^{1}$

## 2 Related Work

Task planning in robotics aims to generate a sequence of high-level actions to achieve a goal within an environment. Conventional methods employ domain-specific languages such as PDDL [20, 21, 22] and ASP [23] together with semantic parsing [24, 25], search techniques [26, 27] and complex heuristics [28] to arrive at a solution. These methods, however, lack both the scalability to large environments as well as the task generality required when operating in the real world. Hierarchical and reinforcement learning-based alternatives [29, 30], [31] face challenges with data demands and scalability. Our work leverages the in-context learning capabilities of LLMs to generate task plans across 3D scene graphs. Tasks, in this case, can be naturally expressed using language, with the internet scale training of LLMs providing the desired knowledge for task generality, while 3D scene graphs provide the grounding necessary for large-scale environment operation. This allows for a general and scalable framework when compared to traditional non-LLM-based alternatives.

Task planning with LLMs, that is, translating natural language prompts into task plans for robotics, is an emergent trend in the field. Earlier studies have effectively leveraged pre-trained LLMs' incontext learning abilities to generate actionable plans for embodied agents [4, 10, 9, 8, 32, 7, 33]. A key challenge for robotics is grounding these plans within the operational environment of the robot. Prior works have explored the use of object detectors [8, 7], PDDL environment representations [10, $9,34]$ or value functions [4] to achieve this grounding, however, they are predominantly constrained to single-room environments, and scale poorly with the number of objects in a scene which limits their ability to plan over multi-room or multi-floor environments. In this work, we explore the use of 3D scene graphs and the ability of LLMs to generate plans over large-scale scenes by exploiting the inherent hierarchical and semantic nature of these representations.

Integrating external knowledge in LLMs has been a growing line of research combining language models with external tools to improve the reliability of their outputs. In such cases, external modules are used to provide feedback or extra information to the LLM to guide its output generation. This is achieved either through API calls to external tools $[35,36]$ or as textual feedback from the operating environment [37, 8]. More closely related to our work, CLAIRIFY [38] iteratively leverage compiler error feedback to re-prompt an LLM to generate syntactically valid code. Building on these ideas, we propose an iterative plan verification process with feedback from a scene graph-based simulator to ensure all generated plans adhere to the constraints and predicates captured by the preconstructed scene graph. This ensures the direct executability of the plan on a mobile manipulator robot, operating in the corresponding real-world environment.

## 3 SayPlan

### 3.1 Problem Formulation

We aim to address the challenge of long-range task planning for an autonomous agent, such as a mobile manipulator robot, in a large-scale environment based on natural language instructions. This requires the robot to comprehend abstract and ambiguous instructions, understand the scene and generate task plans involving both navigation and manipulation of a mobile robot within an[^0]

```
Algorithm 1: SayPlan
Given: scene graph simulator $\psi$, classical path planner $\phi$, large language model $L L M$
Inputs: prompt $\mathcal{P}$, scene graph $\mathcal{G}$, instruction $\mathcal{I}$
$\mathcal{G}^{\prime} \leftarrow \operatorname{collapse}_{\psi}(\mathcal{G}$
$\triangleright$ collapse scene graph
    Stage 1: Semantic Search
while command $!=$ "terminate" do
    command, node_name $\leftarrow L L M\left(\mathcal{P}, \mathcal{G}^{\prime}, \mathcal{I}\right)$
    if command $==$ "expand" then
        $\mathcal{G}^{\prime} \leftarrow \operatorname{expand}_{\psi}$ (node_name) $\quad \triangleright$ expand node to reveal objects and assets
        else if command $==$ " contract" then
            $\mathcal{G}^{\prime} \leftarrow$ contract $_{\psi}$ (node_name) $\quad \triangleright$ contract node if nothing relevant found
    Stage 2: Causal Planning $\quad \triangleright$ generate a feasible plan
    feedback $=$ “"
    while feedback != "success" do
        plan $\leftarrow L L M\left(\mathcal{P}, \mathcal{G}^{\prime}, I\right.$, feedback $) \quad \triangleright$ high level plan
        full_plan $\leftarrow \phi\left(\right.$ plan, $\left.\mathcal{G}^{\prime}\right) \quad \triangleright$ compute optimal navigational path between nodes
        feedback $\leftarrow$ verify_plan ${ }_{\psi}($ full_plan) $\quad \triangleright$ forward simulate the full plan
    return full_plan $\triangleright$ executable plan
```

environment. Existing approaches lack the ability to reason over scenes spanning multiple floors and rooms. Our focus is on integrating large-scale scenes into planning agents based on Language Models (LLMs) and solving the scalability challenge. We aim to tackle two key problems: 1) representing large-scale scenes within LLM token limitations, and 2) mitigating LLM hallucinations and erroneous outputs when generating long-horizon plans in large-scale environments.

### 3.2 Preliminaries

Here, we describe the 3D scene graph representation of an environment and the scene graph simulator API which we leverage throughout our approach.

Scene Representation: 3D Scene Graphs (3DSG) $[11,12,14]$ have recently emerged as an actionable world representation for robots $[13,15,16,39,40,41]$, which hierarchically abstract the environment at multiple levels through spatial semantics and object relationships while capturing relevant states, affordances and predicates of the entities present in the environment. Formally, a 3DSG is a hierarchical multigraph $\mathcal{G}=(V, E)$ in which the set of vertices $V$ comprises $V_{1} \cup V_{2} \cup \ldots \cup V_{K}$, with

![](https://cdn.mathpix.com/cropped/2024_06_04_9f5c08dcc69f0d50c30bg-04.jpg?height=431&width=702&top_left_y=1216&top_left_x=1053)

Figure 2: Hierarchical Structure of a 3D Scene Graph. This graph consists of 4 levels. Notes that the room nodes are connected to one another via sequences of pose nodes which capture the topological arrangement of a scene. each $V_{k}$ signifying the set of vertices at a particular level of the hierarchy $k$. Edges stemming from a vertex $v \in V_{k}$ may only terminate in $V_{k-1} \cup V_{k} \cup V_{k+1}$, i.e. edges connect nodes within the same level, or one level higher or lower.

We assume a pre-constructed 3DSG representation of a large-scale environment generated using existing techniques $[15,13,11]$. The entire 3DSG can be represented as a NetworkX Graph object [42] and text-serialised into a JSON data format that can be parsed directly by a pretrained LLM. An example of a single asset node from the 3DSG is represented as: \{name: coffee_machine, type: asset, location: kitchen, affordances: [turn_on, turn_off, release], state: off, attributes: [red, automatic], position: [2.34, 0.45, 2.23] \} with edges between nodes captured as $\left\{\right.$ kitchen $\left.\leftrightarrow c o f f e e \_m a c h i n e\right\}$. The 3DSG is organized in a hierarchical manner with four primary levels: floors, rooms, assets, and objects as shown in Figure 2. The top level contains floors, each of which branches out to several rooms. These rooms are interconnected through pose nodes to represent the environment's topological structure. Within each room, we find assets (immovable entities) and objects (movable entities). Both asset and object nodes encode particulars including state, affordances, additional attributes such as colour or weight, and 3D pose. The graph also incorporates a dynamic agent
node, denoting a robot's location within the scene. Note that this hierarchy is scalable and node levels can be adapted to capture even larger environments e.g. campuses and buildings

Scene Graph Simulator $\psi$ refers to a set of API calls for manipulating and operating over JSON formatted 3DSGs, using the following functions: 1) collapse $(\mathcal{G})$ : Given a full 3DSG, this function returns an updated scene graph that exposes only the highest level within the 3DSG hierarchy e.g. floor nodes. 2) expand (node_name) : Returns an updated 3DSG that reveals all the nodes connected to node_name in the level below. 3) contract (node_name) : Returns an updated 3DSG that hides all the nodes connected to node_name in the level below. 4) verify_plan (plan): Forward simulates the generated plan at the abstract graph level captured by the 3DSG to check if each action adheres to the environment's predicates, states and affordances. Returns textual feedback e.g. "cannot pick up banana" if the fridge containing the banana is closed.

### 3.3 Approach

We present a scalable framework for grounding the generalist task planning capabilities of pretrained LLMs in large-scale environments spanning multiple floors and rooms using 3DSG representations. Given a 3DSG $\mathcal{G}$ and a task instruction $\mathcal{I}$ defined in natural language, we can view our framework SayPlan as a high-level task planner $\pi(\boldsymbol{a} \mid \mathcal{I}, \mathcal{G})$, capable of generating long-horizon plans $\boldsymbol{a}$ grounded in the environment within which a mobile manipulator robot operates. This plan is then fed to a low-level visually grounded motion planner for real-world execution. To ensure the scalability of SayPlan, two stages are introduced: Semantic Search and Iterative Replanning which we detail below. An overview of the SayPlan pipeline is illustrated in Figure 1 with the corresponding pseudo-code given in Algorithm 1.

Semantic Search: When planning over 3DSGs using LLMs we take note of two key observations: 1) A 3DSG of a large-scale environment can grow infinitely with the number of rooms, assets and objects it contains, making it impractical to pass as input to an LLM due to token limits and 2) only a subset of the full 3DSG $\mathcal{G}$ is required to solve any given task e.g. we don't need to know about the toothpaste in the bathroom when making a cup of coffee. To this end, the Semantic Search stage seeks to identify this smaller, task-specific subgraph $\mathcal{G}^{\prime}$ from the full 3DSG which only contains the entities in the environment required to solve the given task instruction. To identify $\mathcal{G}^{\prime}$ from a full 3DSG, we exploit the semantic hierarchy of these representations and the reasoning capabilities of LLMs. We firstly collapse $\mathcal{G}$ to expose only its top level e.g. the floor nodes, reducing the 3DSG initial token representation by $\approx 80 \%$. The LLM manipulates this collapsed graph via expand and contract API calls in order to identify the desired subgraph for the task based on the given instruction $\mathcal{I}$. This is achieved using in-context learning over a set of input-out examples (see Appendix J), and utilising chain-of-thought prompting to guide the LLM in identifying which nodes to manipulate. The chosen API call and node are executed within the scene graph simulator, and the updated 3DSG is passed back to the LLM for further exploration. If an expanded node is found to contain irrelevant entities for the task, the LLM contracts it to manage token limitations and maintain a task-specific subgraph (see Figure 3). To avoid expanding already-contracted nodes, we maintain a list of previously expanded nodes, passed as an additional Memory input to the LLM, facilitating a Markovian decision-making process and allowing SayPlan to scale to extensive search sequences without the overhead of maintaining the full interaction history [5]. The LLM autonomously proceeds to the planning phase once all necessary assets and objects are identified in the current subgraph $\mathcal{G}^{\prime}$. An example of the LLM-scene graph interaction during Semantic Search is provided in Appendix $\mathrm{K}$.

Iterative Replanning: Given the identified subgraph $\mathcal{G}^{\prime}$ and the same task instruction $\mathcal{I}$ from above, the LLM enters the planning stage of the pipeline. Here the LLM is tasked with generating a sequence of node-level navigational (goto (pose2)) and manipulation (pickup (coffee_mug)) actions that satisfy the given task instruction. LLMs, however, are not perfect planning agents and tend to hallucinate or produce erroneous outputs [43, 9]. This is further exacerbated when planning over large-scale environments or long-horizon tasks. We facilitate the generation of task plans by the LLM via two mechanisms. First, we shorten the LLM's planning horizon by delegating pose-level path planning to an optimal path planner, such as Dijkstra. For example, a typical plan output such as [goto(meeting_room), goto(pose13), goto(pose14), goto(pose8), ..., goto(kitchen), access(fridge), open(fridge)] is simplified to [goto(meeting_room), goto(kitchen), access(fridge), open(fridge)]. The path
planner handles finding the optimal route between high-level locations, allowing the LLM to focus on essential manipulation components of the task. Secondly, we build on the self-reflection capabilities of LLMs [17] to iteratively correct their generated plans using textual, task-agnostic feedback from a scene graph simulator which evaluates if the generated plan complies with the scene graph's predicates, state, and affordances. For instance, a pick (banana) action might fail if the robot is already holding something, if it is not in the correct location or if the fridge was not opened beforehand. Such failures are transformed into textual feedback (e.g., "cannot pick banana"), appended to the LLM's input, and used to generate an updated, executable plan. This iterative process, involving planning, validation, and feedback integration, continues until a feasible plan is obtained. The validated plan is then passed to a low-level motion planner for robotic execution. An example of the LLM-scene graph interaction during iterative replanning is provided in Appendix L. Specific implementation details are provided in Appendix A.

## 4 Experimental Setup

We design our experiments to evaluate the 3D scene graph reasoning capabilities of LLMs with a particular focus on high-level task planning pertaining to a mobile manipulator robot. The plans adhere to a particular embodiment consisting of a 7 -degree-of-freedom robot arm with a two-fingered gripper attached to a mobile base. We use two large-scale environments, shown in Figure 4, which exhibit multiple rooms and multiple floors which the LLM agent has to plan across. To better ablate and showcase the capabilities of SayPlan, we decouple its semantic search ability from the overall causal planning capabilities using the following two evaluation settings as shown in Appendix C:

Semantic Search: Here, we focus on queries which test the semantic search capabilities of an LLM provided with a collapsed 3D scene graph. This requires the LLM to reason over the room and floor node names and their corresponding attributes in order to aid its search for the relevant assets and objects required to solve the given task instruction. We evaluate against a human baseline to understand how the semantic search capabilities of an LLM compare to a human's thought process. Furthermore, to gain a better understanding of the impact different LLM models have on this graphbased reasoning, we additionally compare against a variant of SayPlan using GPT-3.5.

Causal Planning: In this experiment, we evaluate the ability of SayPlan to generate feasible plans to solve a given natural language instruction. The evaluation metrics are divided into two components: 1) Correctness, which primarily validates the overall goal of the plan and its alignment to what a human would do to solve the task and 2) Executability, which evaluates the alignment of the plan to the constraints of the scene graph environment and its ability to be executed by a mobile manipulator robot. We note here that for a plan to be executable, it does not necessarily have to be correct and vice versa. We evaluate SayPlan against two baseline methods that integrate an LLM for task planning:

LLM-As-Planner, which generates a full plan sequence in an open-loop manner; the plan includes the full sequence of both navigation and manipulation actions that the robot must execute to complete a task, and $\mathbf{L L M}+\mathbf{P}$, an ablated variant of SayPlan, which only incorporates the path planner to allow for shorter horizon plan sequences, without any iterative replanning.

## 5 Results

### 5.1 Semantic Search

We summarise the results for the semantic search evaluation in Table 1. SayPlan (GPT-3 . 5) consistently failed to reason over the input graph representation, hallucinating nodes to

| Subtask | Office |  |  | Home |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Human | SayPlan <br> $(\mathrm{GPT}-3.5)$ | SayPlan <br> (GPT-4) | Human | SayPlan <br> (GPT-3.5) | SayPlan <br> (GPT-4) |
| Simple Search | $100 \%$ | $6.6 \%$ | $86.7 \%$ | $100 \%$ | $0.0 \%$ | $86.7 \%$ |
| Complex Search | $100 \%$ | $0.0 \%$ | $73.3 \%$ | $100 \%$ | $0.0 \%$ | $73.3 \%$ |

Table 1: Evaluating the semantic search capabilities of GPT-4. The table shows the semantic search success rate in finding a suitable subgraph for planning. explore or stagnating at exploring the same node multiple times. SayPlan (GPT-4) in contrast achieved $86.7 \%$ and $73.3 \%$ success in identifying the desired subgraph across both the simple and complex search tasks respectively, demonstrating significantly better graph-based reasoning than GPT-3.5.

|  | Simple |  | Long Horizon |  | Types of Errors |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Corr | Exec | Corr | Exec | Missing <br> Action | Missing <br> Pose | Wrong <br> Action | Incomplete <br> Search | Hallucinated <br> Nodes |
| $\mathbf{L L M}+\mathbf{P}$ | $93.3 \%$ | $13.3 \%$ | $33.3 \%$ | 0.0 | $26.7 \%$ | $0.0 \%$ | $10.0 \%$ | $3.33 \%$ | $10.0 \%$ |
| LLM-As | $93.3 \%$ | 80.0 | $66.7 \%$ | $13.3 \%$ | $20.0 \%$ | $60.0 \%$ | $0.17 \%$ | $0.03 \%$ | $10.0 \%$ |
| SayPlan | $93.3 \%$ | $100.0 \%$ | $73.3 \%$ | $86.6 \%$ | $0.0 \%$ | $0.0 \%$ | $0.0 \%$ | $0.0 \%$ | $6.67 \%$ |

Table 3: Causal Planning Results. Left: Correctness and Executability on Simple and Long Horizon planning tasks and Right: Types of execution errors encountered when planning using LLMs. Note that SayPlan corrects the majority of the errors faced by LLM-based planners.

While as expected the human baseline achieved $100 \%$ on all sets of instructions, we are more interested in the qualitative assessment of the common-sense reasoning used during semantic search. More specifically we would like to identify the similarity in the semantic search heuristics utilised by humans and that used by the underlying LLM based on the given task instruction.

We present the full sequence of explored nodes for both SayPlan (GPT-4) and the human baseline in Appendix F. As shown in the tables, SayPlan (GPT-4) demonstrates remarkably similar performance to a human's semantic and common sense reasoning for most tasks, exploring a similar sequence of nodes given a particular instruction. For example, when asked to "find a ripe banana", the LLM first explores the kitchen followed by the next most likely location, the cafeteria. In the case where no semantics are present in the instruction such as "find me object $K 31 X$ ", we note that the LLM agent is capable of conducting a breadthfirst-like search across all the unexplored nodes. This highlights the importance of meaningful node names and attributes that capture the relevant environment semantics that the LLM can leverage to relate the query instruction for efficient search.

![](https://cdn.mathpix.com/cropped/2024_06_04_9f5c08dcc69f0d50c30bg-07.jpg?height=521&width=697&top_left_y=756&top_left_x=1061)

Figure 3: Scene Graph Token Progression During Semantic Search. This graph illustrates the scalability of our approach to large-scale 3D scene graphs. Note the importance of node contraction in maintaining a near constant token representation of the 3DSG input.

|  | Full Graph <br> (Token Count) | Collapsed Graph <br> (Token Count) | Compression Ratio |
| :--- | :---: | :---: | :---: |
| Office | 6731 | 878 | $86.9 \%$ |
| Home | 6598 | 1817 | $72.5 \%$ |

Table 2: 3D Scene Graph Token Count Number of tokens required for the full graph vs. collapsed graph.

An odd failure case in the simple search instructions involved negation, where the agent consistently failed when presented with questions such as "Find me an office that does not have a cabinet" or "Find me a bathroom with no toilet". Other failure cases noted across the complex search instructions included the LLM's failure to conduct simple distance-based and count-based reasoning over graph nodes. While trivial to a human, this does require the LLM agent to reason over multiple nodes simultaneously, where it tends to hallucinate or miscount connected nodes.

Scalability Analysis: We additionally analyse the scalability of SayPlan during semantic search. Table 2 illustrates the impact of exploiting the hierarchical nature of 3D scene graphs and allowing the LLM to explore the graph from a collapsed initial state. This allows for a reduction of $82.1 \%$ in the initial input tokens required to represent the Office environment and a $60.4 \%$ reduction for the Home environment. In Figure 3, we illustrate how endowing the LLM with the ability to contract explored nodes which it deems unsuitable for solving the task allows it to maintain near-constant input memory from a token perspective across the entire semantic search process. Note that the initial number of tokens already present represents the input prompt tokens as given in Appendix J. Further ablation studies on the scalability of SayPlan to even larger 3DSGs are provided in Appendix H.

### 5.2 Causal Planning

The results for causal planning across simple and long-horizon instructions are summarised in Table 3 (left). We compared SayPlan's performance against two baselines: LLM-As-Planner and LLM+P. All three methods displayed consistent correctness in simple planning tasks at $93 \%$, given that this metric is more a function of the underlying LLMs reasoning capabilities. However, it is interesting to note that in the long-horizon tasks, both the path planner and iterative replanning play an important role in improving this correctness metric by reducing the planning horizon and allowing the LLM to reflect on its previous output.

The results illustrate that the key to ensuring the task plan's executability was iterative replanning. Both LLM-As-Planner and LLM+P exhibited poor executability, whereas SayPlan achieved nearperfect executability as a result of iterative replanning, which ensured that the generated plans were grounded to adhere to the constraints and predicated imposed by the environment. Detailed task plans and errors encountered are provided in Appendix G. We summarise these errors in Table 3 (right) which shows that plans generated with LLM+P and LLM-As-Planner entailed various types of errors limiting their executability. LLM+P mitigated navigational path planning errors as a result of the classical path planner however still suffered from errors pertaining to the manipulation of the environment - missing actions or incorrect actions which violate environment predicates. SayPlan mitigated these errors via iterative replanning, however in $6.67 \%$ of tasks, it failed to correct for some hallucinated nodes. While we believe these errors could be eventually corrected via iterative replanning, we limited the number of replanning steps to 5 throughout all experiments. We provide an illustration of the real-world execution of a generated plan using SayPlan on a mobile manipulator robot coupled with a vision-guided motion controller [44, 45] in Appendix I.

## 6 Limitations

SayPlan is notably constrained by the limitations inherent in current large language models (LLMs), including biases and inaccuracies, affecting the validity of its generated plans. More specifically, SayPlan is limited by the graph-based reasoning capabilities of the underlying LLM which fails at simple distance-based reasoning, node count-based reasoning and node negation. Future work could explore fine-tuning these models for these specific tasks or alternatively incorporate existing and more complex graph reasoning tools [46] to facilitate decision-making. Secondly, SayPlan's current framework is constrained by the need for a pre-built 3D scene graph and assumes that objects remain static post-map generation, significantly restricting its adaptability to dynamic realworld environments. Future work could explore how online scene graph SLAM systems [15] could be integrated within the SayPlan framework to account for this. Additionally, the incorporation of open-vocabulary representations within the scene graph could yield a general scene representation as opposed to solely textual node descriptions. Lastly, a potential limitation of the current system lies in the scene graph simulator and its ability to capture the various planning failures within the environment. While this works well in the cases presented in this paper, for more complex tasks involving a diverse set of predicates and affordances, the incorporation of relevant feedback messages for each instance may become infeasible and forms an important avenue for future work in this area.

## 7 Conclusion

SayPlan is a natural language-driven planning framework for robotics that integrates hierarchical 3D scene graphs and LLMs to plan across large-scale environments spanning multiple floors and rooms. We ensure the scalability of our approach by exploiting the hierarchical nature of 3D scene graphs and the semantic reasoning capabilities of LLMs to enable the agent to explore the scene graph from the highest level within the hierarchy, resulting in a significant reduction in the initial tokens required to capture larger environments. Once explored, the LLM generates task plans for a mobile manipulator robot, and a scene graph simulator ensures that the plan is feasible and grounded to the environment via iterative replanning. The framework surpasses existing techniques in producing correct, executable plans, which a robot can then follow. Finally, we successfully translate validated plans to a real-world mobile manipulator agent which operates across multiple rooms, assets and objects in a large office environment. SayPlan represents a step forward for general-purpose service robotics that can operate in our homes, hospitals and workplaces, laying the groundwork for future research in this field.

## Acknowledgments

The authors would like to thank Ben Burgess-Limerick for assistance with the robot hardware setup, Nishant Rana for creating the illustrations and Norman Di Palo and Michael Milford for insightful discussions and feedback towards this manuscript. The authors also acknowledge the ongoing support from the QUT Centre for Robotics. This work was partially supported by the Australian Government through the Australian Research Council's Discovery Projects funding scheme (Project DP220102398) and by an Amazon Research Award to Niko Sünderhauf.

## References

[1] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. E. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.

[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020.

[3] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.

[4] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang, R. Julian, et al. Do As I Can, Not As I Say: Grounding language in robotic affordances. In Conference on Robot Learning, pages 287-318. PMLR, 2023.

[5] N. Wake, A. Kanehira, K. Sasabuchi, J. Takamatsu, and K. Ikeuchi. Chatgpt empowered longstep robot control in various environments: A case application, 2023.

[6] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence. Palm-E: An embodied multimodal language model, 2023.

[7] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su. LLM-Planner: Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv:2212.04088, 2022.

[8] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.

[9] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. LLM+P: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023 .

[10] T. Silver, V. Hariprasad, R. S. Shuttleworth, N. Kumar, T. Lozano-Pérez, and L. P. Kaelbling. PDDL planning with pretrained large language models. In NeurIPS 2022 Foundation Models for Decision Making Workshop.

[11] I. Armeni, Z.-Y. He, J. Gwak, A. R. Zamir, M. Fischer, J. Malik, and S. Savarese. 3D scene graph: A structure for unified semantics, 3D space, and camera. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5664-5673, 2019.

[12] U.-H. Kim, J.-M. Park, T.-J. Song, and J.-H. Kim. 3-D scene graph: A sparse and semantic representation of physical environments for intelligent agents. IEEE transactions on cybernetics, $50(12): 4921-4933,2019$.

[13] A. Rosinol, A. Violette, M. Abate, N. Hughes, Y. Chang, J. Shi, A. Gupta, and L. Carlone. Kimera: From slam to spatial perception with 3D dynamic scene graphs. The International Journal of Robotics Research, 40(12-14):1510-1546, 2021.

[14] P. Gay, J. Stuart, and A. Del Bue. Visual graphs from motion (vgfm): Scene understanding with object geometry reasoning. In Computer Vision-ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2-6, 2018, Revised Selected Papers, Part III 14, pages 330-346. Springer, 2019.

[15] N. Hughes, Y. Chang, and L. Carlone. Hydra: A real-time spatial perception engine for 3D scene graph construction and optimization. Robotics: Science and Systems XIV, 2022.

[16] C. Agia, K. M. Jatavallabhula, M. Khodeir, O. Miksik, V. Vineet, M. Mukadam, L. Paull, and F. Shkurti. Taskography: Evaluating robot task planning over large 3D scene graphs. In Conference on Robot Learning, pages 46-58. PMLR, 2022.

[17] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.

[18] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.

[19] E. W. Dijkstra. A note on two problems in connexion with graphs. In Edsger Wybe Dijkstra: His Life, Work, and Legacy, pages 287-290. 2022.

[20] D. McDermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram, M. Veloso, D. Weld, and D. Wilkins. PDDL-the planning domain definition language. 1998.

[21] M. Fox and D. Long. PDDL2. 1: An extension to PDDL for expressing temporal planning domains. Journal of artificial intelligence research, 20:61-124, 2003.

[22] P. Haslum, N. Lipovetzky, D. Magazzeni, and C. Muise. An introduction to the planning domain definition language. Synthesis Lectures on Artificial Intelligence and Machine Learning, $13(2): 1-187,2019$.

[23] M. Gelfond and Y. Kahl. Knowledge representation, reasoning, and the design of intelligent agents: The answer-set programming approach. Cambridge University Press, 2014.

[24] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. Proceedings of the AAAI Conference on Artificial Intelligence, 2011.

[25] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y. Jiang, H. Yedidsion, J. W. Hart, P. Stone, and R. J. Mooney. Jointly improving parsing and perception for natural language commands through human-robot dialog. J. Artif. Intell. Res., 67:327-374, 2020.

[26] H. Kautz and B. Selman. Pushing the envelope: Planning, propositional logic, and stochastic search. In Proceedings of the national conference on artificial intelligence, pages 1194-1201, 1996 .

[27] B. Bonet and H. Geffner. Planning as heuristic search. Artificial Intelligence, 129(1-2):5-33, 2001 .

[28] M. Vallati, L. Chrpa, M. Grześ, T. L. McCluskey, M. Roberts, S. Sanner, et al. The 2014 international planning competition: Progress and trends. AI Magazine, 36(3):90-98, 2015.

[29] R. Chitnis, T. Silver, B. Kim, L. Kaelbling, and T. Lozano-Perez. CAMPs: Learning ContextSpecific Abstractions for Efficient Planning in Factored MDPs. In Conference on Robot Learning, pages 64-79. PMLR, 2021.

[30] T. Silver, R. Chitnis, A. Curtis, J. B. Tenenbaum, T. Lozano-Pérez, and L. P. Kaelbling. Planning with learned object importance in large problem instances using graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11962-11971, 2021.

[31] F. Ceola, E. Tosello, L. Tagliapietra, G. Nicola, and S. Ghidoni. Robot task planning via deep reinforcement learning: a tabletop object sorting application. In 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC), pages 486-492, 2019. doi:10.1109/ SMC.2019.8914278.

[32] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.

[33] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.

[34] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh. Translating natural language to planning goals with large-language models. arXiv preprint arXiv:2302.05128, 2023.

[35] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.

[36] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.

[37] R. Liu, J. Wei, S. S. Gu, T.-Y. Wu, S. Vosoughi, C. Cui, D. Zhou, and A. M. Dai. Mind's eye: Grounded language model reasoning through simulation. arXiv preprint arXiv:2210.05359, 2022.

[38] M. Skreta, N. Yoshikawa, S. Arellano-Rubach, Z. Ji, L. B. Kristensen, K. Darvish, A. AspuruGuzik, F. Shkurti, and A. Garg. Errors are useful prompts: Instruction guided task programming with verifier-assisted iterative prompting. arXiv preprint arXiv:2303.14100, 2023.

[39] Z. Ravichandran, L. Peng, N. Hughes, J. D. Griffith, and L. Carlone. Hierarchical representations and explicit memory: Learning effective navigation policies on 3D scene graphs using graph neural networks. In 2022 International Conference on Robotics and Automation (ICRA), pages 9272-9279. IEEE, 2022.

[40] A. Kurenkov, R. Martín-Martín, J. Ichnowski, K. Goldberg, and S. Savarese. Semantic and geometric modeling with neural message passing in 3D scene graphs for hierarchical mechanical search. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages $11227-11233$. IEEE, 2021.

[41] S. Garg, N. Sünderhauf, F. Dayoub, D. Morrison, A. Cosgun, G. Carneiro, Q. Wu, T.-J. Chin, I. Reid, S. Gould, et al. Semantics for robotic mapping, perception and interaction: A survey. Foundations and Trends ${ }^{\circledR}$ in Robotics, 8(1-2):1-224, 2020.

[42] A. A. Hagberg, D. A. Schult, and P. J. Swart. Exploring network structure, dynamics, and function using networkx. In G. Varoquaux, T. Vaught, and J. Millman, editors, Proceedings of the 7th Python in Science Conference, pages 11 - 15, Pasadena, CA USA, 2008.

[43] M. Skreta, N. Yoshikawa, S. Arellano-Rubach, Z. Ji, L. B. Kristensen, K. Darvish, A. AspuruGuzik, F. Shkurti, and A. Garg. Errors are useful prompts: Instruction guided task programming with verifier-assisted iterative prompting. ArXiv, abs/2303.14100, 2023. URL https://api.semanticscholar.org/CorpusID:257757298.

[44] J. Haviland, N. Sünderhauf, and P. Corke. A holistic approach to reactive mobile manipulation. IEEE Robotics and Automation Letters, 7(2):3122-3129, 2022.

[45] P. Corke and J. Haviland. Not your grandmother's toolbox-the robotics toolbox reinvented for python. In 2021 IEEE international conference on robotics and automation (ICRA), pages 11357-11363. IEEE, 2021.

[46] J. Zhang. Graph-toolformer: To empower LLMs with graph reasoning ability via prompt augmented by chatgpt. arXiv preprint arXiv:2304.11116, 2023.

[47] S. Haddadin, S. Parusel, L. Johannsmeier, S. Golz, S. Gabl, F. Walch, M. Sabaghian, C. Jähne, L. Hausperger, and S. Haddadin. The franka emika robot: A reference platform for robotics research and education. IEEE Robotics and Automation Magazine, 29(2):46-64, 2022. doi: 10.1109/MRA.2021.3138382.

[48] Omron. Omron LD / HD Series. URL https://www.ia.omron.com/products / family/3664/dimension.html.

[49] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 2023.

[50] K. Rana, A. Melnik, and N. Sünderhauf. Contrastive language, action, and state pre-training for robot learning, 2023.

[51] Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions. In 7th Annual Conference on Robot Learning, 2023.

[52] K. Rana, M. Xu, B. Tidd, M. Milford, and N. Suenderhauf. Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id= 0nb97NQypbK.
