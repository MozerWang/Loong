# SpinQuant: LLM Quantization with Learned Rotations 

Zechun Liu* Changsheng Zhao* Igor Fedorov Bilge Soran Dhruv Choudhary<br>Raghuraman Krishnamoorthi Vikas Chandra Yuandong Tian Tijmen Blankevoort<br>Meta


#### Abstract

Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Recent findings suggest that rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures, and find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zeroshot reasoning performance. As a result, we propose SpinQuant that optimizes (or learns) the rotation matrices with Cayley optimization on a small validation set. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-2 7B/LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by $30.2 \% / 34.1 \%$ relative to QuaRot.


## 1 Introduction

Large Language models (LLMs) have demonstrated impressive performance across many disciplines. SoTA open source models (e.g., LLaMA [40], Mistral [17], etc) and proprietary LLMs (e.g., GPT [2], Gemini[37], etc) have been used in general purpose chatting assistants, medical diagnosticians [38], computer game content generators [10], coding co-pilots [32], and much more.

To serve such a high demand, the inference cost becomes a real issue. Many effective techniques have been developed. Post-training Quantization (PTQ), as one effective category of techniques, quantizes the weights (or activations) into low-precision and thus reduces the memory usage and may significantly improve latency. This is not only important for server-side inference, but also for on-device scenarios with small-sized LLMs [26].

When applying quantization, outliers remain an open challenge because they stretch the quantization range, leaving fewer effective bits available for the majority of values. Prior research mitigates this challenge by trading quantization difficulty between weights and activations [43, 23] or employing mixed-precision to handle outliers [47]. Recent studies take a different angle by demonstrating an interesting property: Multiplying the weight matrix with a random rotation can effectively reduce outliers and enhance quantizability [7, 41]. Intuitively, due to the statistical property of random rotation, such a transform yields a outlier-less distribution of resulting weight or activation entries [13]. Since the rotation matrices can be constructed in pairs from identity mapping, and get integrated into nearby weights without changing the overall network outputs, a property known as rotational[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-02.jpg?height=538&width=1390&top_left_y=244&top_left_x=365)

Figure 1: Overall diagram of rotation. (a) The residual stream can be rotated in the transformer network, resulting in numerically equivalent floating point networks before and after rotation. The rotated activations exhibit fewer outliers and are easier to quantize. (b) \& (c) The rotation matrix can be integrated with the corresponding weight matrices and we further define $R_{2}, R_{3}$, and $R_{4}$ for reducing outliers inside the block.

invariance [4], the transformed weights (or activations) can be quantized with lower reconstruction error, without additional inference overhead.

While statistically any random rotation works well, in this paper, we find that the performance of quantized network varies a lot with different rotation matrices. For example, the downstream averaged accuracy on zero-shot reasoning tasks may change up to 13 points with different rotations. As a result, we propose SpinQuant that optimizes the rotation matrix to minimize the final loss of the quantized network, with fixed weight parameters, by employing the Cayley $S G D[21]$, a proficient technique for optimizing orthonormal matrices. This optimization does not alter the full-precision network output but refines the intermediate activations and weights, making them more quantization-friendly.

In SpinQuant, we also extend the concept of outlier reduction for single matrices [7, 41] to a comprehensive network-level perspective. We identify four different places that are rotationally invariant in most prevalent LLMs (e.g., OPT [46], LLaMA [39]), as depicted in Figure 1. This constitutes the optimization space of SpinQuant.

Experimental results demonstrate that SpinQuant significantly improves accuracy compared to random rotation matrices. With 4-bit quantization of weights, activations, and KV cache, SpinQuant achieves an average accuracy of 64.0 on zero-shot commonsense reasoning tasks for LLaMA-2 7B. This marks a gap of just 2.9 points from the full-precision network, considerably better than previous LLM-QAT [25] with a gap as large as 22.0 points despite using the same precision. We also demonstrate results on LLaMA-3, showing 17.7 points accuracy improvement than SmoothQuant 43] on 70B model with 4-bit weights, activations, and KV cache quantization, narrowing the gap to full-precision to only 4.4 points. In terms of speed, it only takes 100 iterations (and 1.3 hours) to optimize the rotation matrices on 800 WikiText2[27] calibration data for a LLaMA-2 7B model on a single A100 node.

## 2 Motivation and Preliminaries

Quantization reduces the precision of weights (and/or activations) in a neural network in order to save memory and lower the latency. For Large language models (LLMs), the presence of outliers extends the range of weight/activation values and increases the reconstruction errors for normal values [11, 24, 44] (Figures 2 (a)\&(c)).

### 2.1 Outlier Reduction via Random Rotation

There exist many ways to mitigate the effect of outliers [43, 11]. In this paper, we focus on the technique that uses random rotation to reduce outliers, which outperforms previous approaches. Intuitively, a random rotation matrix statistically blends large and small weights together into a wellbehaved distribution with fewer outliers, and thus is easier to quantize. Theoretically, QuIP[7, 41]

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-03.jpg?height=371&width=1399&top_left_y=229&top_left_x=363)

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-03.jpg?height=319&width=309&top_left_y=239&top_left_x=366)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-03.jpg?height=322&width=309&top_left_y=240&top_left_x=732)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-03.jpg?height=323&width=296&top_left_y=234&top_left_x=1099)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-03.jpg?height=325&width=307&top_left_y=233&top_left_x=1451)

(d)

Figure 2: Activation distribution in LLaMA-2 7B model before and after rotation. Outliers exist in particular channels before rotation. Since channel-wise quantization is not supported in most hardware, outlier removal using rotation enables accurate token-wise or tensor-wise quantization.

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-03.jpg?height=304&width=461&top_left_y=756&top_left_x=366)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-03.jpg?height=306&width=449&top_left_y=755&top_left_x=838)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-03.jpg?height=304&width=461&top_left_y=756&top_left_x=1293)

(c)

Figure 3: Outlier measurement and quantization error across input activation and weights in the five layers that take inputs from the residual (Q/K/V/Up/Gate-projection) of each block in the LLaMA-2 7B model. (a) After rotation, kurtosis of activation distributions is significantly reduced to approximately three across all layers. Quantization error is reduced after rotation in both (b) activations and (c) weights.

proves that multiplication of random orthonormal matrix leads to high incoherence (i.e., lower maximal entry of a matrix compared to its norm) with high probability and thus fewer outliers.

Figure 3 (a) illustrates the measurement of the Kurtosis $\kappa$ of the activations before and after rotation. $\kappa$ quantifies the "tailedness" of a real-valued random variable's probability distribution. A larger $\kappa$ indicates more outliers, while $\kappa \approx 3$ suggests a Gaussian-like distribution. In Figure 3 (a), the activation distribution in the transformer contains numerous outliers, with $\kappa$ of many layers exceeding 200. However, after multiplying these activations with a random rotation matrix, the $\kappa$ across all layers becomes approximately 3 , indicating a more Gaussian-shaped distribution that is easier to quantize. This is corroborated by Figure 3 (b), where the quantization error of the activation tensor significantly decreases after rotation.

### 2.2 Random rotations produce large variance

Interestingly, while statistically random rotation leads to better quantization, not all random rotations give the same quantization outcome. To show this, we tested the zero-shot average accuracy of the rotated version of LLaMA-2 7B, quantized to 4-bit weight and 4-bit activation, under 100 randomized trials. As shown in Figure 4, the performance variance is substantial, with the best random matrix outperforming the worst by 13 points. Random Hadamard matrices 2 outperform random matrices, in consistent with the findings in [41] that Hadamard matrices yield tighter bounds on weight quantization error. However, even random Hadamard rotation matrices exhibit a non-negligible variance in final performance, as large as 6 points.

Given the huge variance across multiple trials of rotations, a natural question arises:

Is it possible to optimize the rotation to maximize the benefit of quantization?

As the main contribution of this work, we show that not only it is possible to do so, but also such an optimization procedure leads to a much better quantized network. For LLaMA-2 7B, it reduces the[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-04.jpg?height=539&width=742&top_left_y=305&top_left_x=667)

Figure 4: The performance distributions under different random rotations on LLaMA-2 7B, using networklevel parameterization (Sec. 3.1). We compare the distributions using random floating-point rotations, random Hadamard matrices, and optimized rotation matrices with Cayley optimization (Sec. 3.2). Despite that Hadamard matrices mostly perform better than random rotations, both random groups demonstrate large variance. In contrast, by optimizing the rotation matrix with Cayley optimization (i.e., SpinQuant), the performance is improved significantly and the variance becomes much smaller.

gap between the 4-bit quantization of weights, activations, and KV caches and its 16-bit precision version to merely 2.9 points. It also works well for LLaMA-3 70B, whose performance deteriorates under existing quantization techniques [15], shrinking the 4-bit quantized network accuracy gap to full-precision from previous SoTA 9.4 points to 4.4 points.

## 3 Method

In this section, we introduce our rotation parameterization of popular LLM architectures, which covers a broad search space to optimize. Such a parameterization leads to identity network output without quantization. We then introduce Cayley optimization to optimize these rotations for better downstream performance under quantization.

### 3.1 Rotation parameterization

Rotating activations in residual As shown in Figure 1 a), we rotate the activations in the residual path by multiplying the embedding output $X$ with a random rotation matrix $\left(R_{1}\right)$. This rotation removes outliers and eases the quantization of the input activations to the fully-connected layers that read from the residual. To maintain numerical invariance, we reverse the rotation of the activation by multiplying it with $R_{1}^{T}\left(=R_{1}^{-1}\right.$ ) prior to its passage through the attention block and feed-forward network, which contains non-linearity. When the quantization is not present, the full-precision network remains intact no matter which rotation is applied ${ }^{3}$ The rotation matrices can be absorbed into corresponding weight matrices, as illustrated in Figures 1 (b) and (c). After absorption, no new parameters are introduced in the network. We can now modify $R_{1}$ freely without impacting the floating-point network's accuracy or parameter count.

Rotating activations in the attention block As depicted in Figure 1(b), in the attention block, we propose to rotate the value matrix by multiplying $R_{2}$, and the activations to out-projection layer by $R_{2}^{T}$ head-wisely. $R_{2}$ has the shape of ( $D_{\text {head }}, D_{\text {head }}$ ) and can be independently chosen across layers. The numerical in-variance is illustrated in Figure 5, these two rotations can be offset in a full-precision network since there are no operators between $R_{2}$ and $R_{2}^{T}$. Meanwhile, it can improve[^2]quantization for value cache and input to linear layer $\left(W_{o}\right)$ without introducing any new parameters in the network.

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-05.jpg?height=100&width=696&top_left_y=354&top_left_x=709)

Figure 5: Rotation equivalence in Multi-Head Self-Attention.

Additional unabsorbed rotations To further address the presence of outliers in KV-cache, we adopt a recent approach [5], which adds a Hadamard matrix ( $R_{3}$ in Figure 11(b)) on the query and key outputs. Due to the existence of the RoPE operation [36], this rotation cannot be absorbed into the $W_{q}$ and $W_{k}$ matrices, as shown in Figure 6 (a). As a result, the Hadamard rotation multiplication is used to rotate the query and key during inference, since it is fast to compute. Similarly, we added a Hadamard matrix multiplication ( $R_{4}$ in Figure 1.c)) inside the feed-forward layer, reducing the outliers in the input to the down projection layer [5].

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-05.jpg?height=103&width=677&top_left_y=873&top_left_x=407)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-05.jpg?height=90&width=532&top_left_y=882&top_left_x=1184)

(b)

Figure 6: Application of rotation pair to (a) query and key (b) down project layer input.

After inserting rotation into LLM, the difficulty of network quantization is significantly alleviated due to outlier elimination. As shown in Figure 3(b), activation quantization error drops sharply from as high as 0.7 to around 0.15 . In addition, weight quantization also becomes easier by absorbing the rotation matrix into the weight matrix, as shown in Figure 3 (c), as depicted in Figure 3 (c), remarking rotation as a beneficial technique that simultaneously helps both weight and activation quantization with a single implementation.

### 3.2 Cayley-optimized rotation

As illustrated in Figure 1, we have determined that the incorporation of four rotation matrices ( $R_{1}$, $R_{2}, R_{3}, R_{4}$ ) can improve quantization performance while preserving numerical consistency in a full-precision network. Given that $R_{3}$ and $R_{4}$ are online rotation operations, meaning they cannot be absorbed into the weight matrix, we retain them as Hadamard matrices. This is because online Hadamard transforms can be efficiently implemented without significant overhead. We then define the optimization objective as identifying the optimal rotation matrix $R_{1}$ and $R_{2}$ that minimizes the final loss of the quantized network:

$$
\begin{equation*}
\underset{R \in \mathcal{M}}{\arg \min } \mathcal{L}_{Q}\left(R_{1}, R_{2} \mid W, X\right) \tag{1}
\end{equation*}
$$

Here, $\mathcal{M}$ represents the Stiefel manifold i.e., the set of all orthonormal matrices. $\mathcal{L}_{Q}(\cdot)$ denotes the task loss, such as cross-entropy, on the calibration set. It is a function of $\left\{R_{1}, R_{2}\right\}$, given the fixed pretrained weights $W$ and the input tensor $X$ and with the quantization function $Q$ in the network. To optimize the rotation matrix on the Stiefel manifold, we employ the Cayley $S G D$ method [21], which is an efficient optimization algorithm on the Stiefel manifold. More specifically, in each iteration, the update of the rotation $R$ is parameterized as the following:

$$
\begin{equation*}
R^{\prime}=\Delta R(Y) R:=\left(I-\frac{\alpha}{2} Y\right)^{-1}\left(I+\frac{\alpha}{2} Y\right) R \tag{2}
\end{equation*}
$$

where $\Delta R(Y):=\left(I-\frac{\alpha}{2} Y\right)^{-1}\left(I+\frac{\alpha}{2} Y\right)$ is the Cayley Transform of a skew-symmetric matrix $Y$ (i.e., $\left.Y^{\top}=-Y\right) . Y$ is computed from a projection $\hat{G}$ of the gradient $G:=\nabla_{R} \mathcal{L}_{Q}$ of the loss function:

$$
\begin{equation*}
Y=\hat{G}-\hat{G}^{\top}, \quad \hat{G}:=G R^{\top}-\frac{1}{2} R R^{\top} G R^{\top} \tag{3}
\end{equation*}
$$

It can be shown that $\Delta R(Y)$ is always orthonormal and thus $R^{\prime}$ is guaranteed to be orthonormal $\left(R^{\prime \top} R^{\prime}=I\right)$ if $R$ is orthonormal. While Eqn. 2 requires a matrix inverse, the new rotation matrix
$R^{\prime}$ can be computed via an efficient fixed point iteration [21]. Overall, the approach maintains the property of orthonormality with only $\sim 2$ times the computation time per iteration compared to a naive SGD algorithm.

We apply the Cayley $S G D$ method to solve Eqn. for $\left\{R_{1}, R_{2}\right\}$, while the underlying weight parameters in the network remain frozen. $\left\{R_{1}, R_{2}\right\}$ count for only $0.26 \%$ of the weight size and is constrained to be orthonormal. Consequently, the underlying floating-point network remains unchanged, and the rotation only influences the quantization performance.

By employing Cayley optimization to update the rotation for 100 iterations on an 800 -sample WikiText2 calibration dataset, we obtain a rotation matrix that outperforms the best random matrix and random Hadamard matrix in 100 random seeds, shown in Figure 4. The Cayley-optimized rotation exhibits minimal variance when initiated from different random seeds. The rotation matrices are initialized with random Hadamard matrices for optimization in our experiments and our ablation study in Section 4.3.3 demonstrates that the optimized rotation is robust to random rotation initialization as well.

## 4 Experiments

We conduct experiments on the LLaMA-2 [40] models (7B/13B/70B) and the LLaMA-3[3] models (8B/70B). Our evaluation of the proposed SpinQuant was carried out on eight zero-shot commonsense reasoning tasks. These tasks include BoolQ[8], PIQA[6], SIQA[34], HellaSwag[45], WinoGrande[33], ARC-easy and ARC-challenge[9], and OBQA [28]. Additionally, we also report the perplexity score on WikiText2 testset [27] for our evaluation.

### 4.1 Experimental settings

We employ Cayley SGD [21] to optimize the rotation matrix, $R_{1}$ and $R_{2}$, both initialized as a random Hadamard matrix, while maintaining all network weights constant. $R_{1}$ is the residual rotation, shaped as ( $\left.D_{\text {token }}, D_{\text {token }}\right) . R_{2}$ is head-wise rotation in each attention block, shaped as ( $D_{\text {head }}, D_{\text {head }}$ ) and is separately learned in each layer. The learning rate starts at 1.5 and linearly decays to 0 . We utilize 800 samples from WikiText-2 to optimize rotation for 100 iterations. It takes $\sim 1.39$ hours for LLaMA-3 8B, $\sim 1.25$ hours for the LLaMA-2 7B, $\sim 2.36$ hours for LLaMA-2 13B, and $\sim 12$ hours for 70B models on 8 A100 GPUs.

The first results we show employ simple round-to-nearest (RTN) quantization for the weights. The activation and key-value cache use asymmetric min-max dynamic quantization, with per-token activation quantization and group size 128 for the key-value quantization. Weight quantization employs per-channel symmetric quantization, and we set the quantization ranges via a linear search of the mean-squared error loss between quantized full-precision weights, following common practice 7 , 30, 24]. Besides rounding-to-nearest, we also show that our method is compatible with GPTQ [14], for which we adhere to the standard GPTQ settings by using 128 samples from WikiText-2 with a sequence length of 2048 as the calibration set for GPTQ quantization.

### 4.2 Main results

Our primary comparison metric is accuracy in challenging 4-bit quantization scenarios, which include weight quantization, activation quantization, and KV-cache quantization. We use RTN to map weights, activations, and KV-cache to integers for inference, after applying the Cayley-optimized $R$. We compare SpinQuant to state-of-the-art quantization methods on LLaMA-2 and LLaMA-3 in Table 1. where we represent the number of bits for weight, activation, and KV-cache as W-A-KV. We compare our proposed method against other PTQ methods including, GPTQ [14], SmoothQuant [43], QuIP [7], QuIP\# [41], OmniQuant [35], AQLM [12], as well as a quantization-aware training (QAT) method, LLM-QAT [25]. We also compare our results to a concurrent work QuaRot [5].

In the most challenging 4-4-4 quantization setting, SpinQuant significantly surpasses LLM-QAT, by 19.1 points on the LLaMA-2 7B models, thereby reducing the gap to the corresponding fullprecision network to a mere 2.9 points, respectively. This is measured in terms of the average accuracy across eight zero-shot tasks. This represents a significant advancement, given the difficulties previous work encountered in generating meaningful results in this context. Furthermore, SpinQuant

Table 1: Comparison of the perplexity score on WikiText2 and averaged accuracy on Zero-shot Common Sense Reasoning tasks. 0 -shot ${ }^{4}$ employs ARC-easy, ARC-challenge, PIQA, and WinoGrande tasks, while 0 -shot ${ }^{8}$ adds BoolQ, SIQA, HellaSwag, and OBQA tasks. Results for SmoothQuant[43], LLM-QAT[25], GPTQ [14], and QuaRot [5] were obtained using their publicly released codebase. While OmniQuant [35], AQLM [12], AWQ [23], QuIP [7], and QuIP\# [41] results were quoted from their papers. SpinQuant* and QuaRot* represent using RTN quantization, while SpinQuant and QuaRot denote using GPTQ weight quantization. Mean scores for SpinQuant, GPTQ, and QuaRot are reported from six trials. Full results, including error bars, are in the Appendix.

| \#Bits <br> W-A-KV | Method | LLaMA-3 8B LLaMA-3 70B |  |  |  | LLaMA-2 7B |  |  | LLaMA-2 13B |  |  | LLaMA-2 70B |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | 0 -shot ${ }^{8}$ <br> Avg. $(\uparrow)$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-07.jpg?height=69&width=69&top_left_y=591&top_left_x=744) | 0 -shot <br> Avg. | $\overline{\mathrm{Wi}} \overline{(\downarrow i}$ <br> $(\downarrow)$ | 0 -shot <br> Avg. $(\uparrow)$ | 0 -shot <br> Avg. $(\uparrow)$ | $\overline{\mathrm{W}} \overline{\mathrm{ik}} \overline{\mathrm{i}}$ <br> $(\downarrow)$ | 0 -shot <br>  <br> Avg. $\uparrow$ - | 0 -shot <br> Avg. $(\uparrow)$ | $\bar{W} \overline{k i k i}$ <br> $(\downarrow)$ | 0 -shot  <br>  Avg. $(\uparrow)$ | 0 -shot <br> Avg. $(\uparrow)$ | $\left.\bar{W}_{\mathrm{ik}} \mathrm{i} \mathrm{i}\right)$ <br> $(\downarrow)$ |
| 16-16-16 | FloatingPoint | 69.6 | 6.1 | 74.5 | 2.8 | 68.6 | 66.9 | 5.5 | 69.9 | 68.3 | 5.0 | 76.0 | 72.9 | 3.3 |
|  | $\overline{\mathrm{RTN}} \overline{\mathrm{N}}$ | $\overline{6} 5.4$ | $\overline{7} . \overline{8}$ | 35.5 | $-\overline{1} \bar{e}^{-}$ | $-6 \overline{5} . \overline{3}$ | $\overline{63} . \overline{6}$ | 7.2 | $\overline{5} 9.9$ | $-5 \overline{7} . \overline{9}$ | $6.4^{-1}$ | $\overline{72.3}$ | $\overline{6} 9.2$ | $\overline{4} . \overline{6}$ |
|  | SmoothQuant | 61.0 | 10.7 | 66.9 | 12.0 | 62.3 | 59.1 | 7.5 | 66.1 | 63.3 | 6.1 | 73.6 | 70.2 | 4.1 |
|  | LLM-QAT | 67.7 | 7.1 | - -1 | - | 67.1 | 64.9 | 5.9 | - | - | - | - <br> 1 | - | - |
|  | OmniQuant | - | - | - | - | 62.5 | - | 5.7 | 64.9 | - | 5.0 | 71.1 | - | 3.5 |
|  | QuIP | - | - | - | - | - | - | - | 66.7 | - | - | 69.4 | - | - |
|  | AQLM | - | - | _ | - | 63.6 | - | - | 66.3 | - | - | 71.9 | - | - |
| $4-16-16$ | QuIP\# | - | - | - | - | 63.9 | _ | _ | 67.1 | _ | _ | $71.8 \quad$ | _ | _ |
|  | AWQ | - | - | - | - | - | - | 6.2 | - | - | 5.1 | - <br> 1 | _ | _ |
|  | GPTQ | 66.5 | 7.2 | 35.7 | $1 \mathrm{e} 5$ | 66.6 | 64.5 | 11.3 | 67.0 | 64.7 | 5.6 | 75.0 | 71.9 | 3.9 |
|  | QuaRot* | 66.2 | 7.5 | 57.2 | 41.6 | 64.5 | 62.4 | 6.9 | 69.3 | 67.1 | 5.5 | 74.6 | 71.8 | 3.7 |
|  | QuaRot | 68.4 | 6.4 | 70.3 | 7.9 | 67.6 | 65.8 | 5.6 | 70.3 | 68.3 | 5.0 | 75.2 | 72.2 | 3.5 |
|  | SpinQuant* | 67.6 | 6.5 | 71.4 | 3.9 | 66.7 | 64.6 | 5.5 | 69.6 | 67.4 | 4.9 | 74.9 | 72.2 | 3.5 |
|  | SpinQuant | 68.5 | 6.4 | 71.6 | 4.8 | 67.7 | 65.9 | 5.6 | 70.5 | 68.5 | 5.0 | 75.4 | 72.6 | 3.5 |
|  | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-07.jpg?height=34&width=170&top_left_y=1074&top_left_x=486) | $\overline{3} \overline{8} .5$ | $\overline{9} \overline{\mathrm{e}} \overline{2}$ | $35 . \overline{6}$ | $\overline{1} \overline{\mathrm{e}} \overline{5}$ | $\overline{3} \overline{7} . \overline{3}$ | $3 \overline{5} . \overline{6}$ | $2 \overline{\mathrm{e}} 3$ | $\overline{3} 7.9$ | $3 \overline{5} . \overline{3}$ | $7 \mathrm{e} 3$ | $37 . \overline{2}$ | $\overline{3} 5.1$ | $\overline{2} \overline{5} \overline{5}$ |
|  | SmoothQuant | 40.3 | $8 \mathrm{e} 2$ | 55.3 | 18.0 | 44.2 | 41.8 | $2 \mathrm{e} 2$ | 46.4 | 44.9 | 34.5 | 46.8 | 64.6 | 57.1 |
|  | LLM-QAT | 44.9 | 42.9 | - 1 | - | 48.8 | 47.8 | 12.9 | - | - | - | - | - | - |
|  | OmniQuant | - | - | - | - | - | - | 14.3 | - | - | 12.3 | - | - | - |
| $4-4-16$ | GPTQ | 37.0 | $9 \mathrm{e} 2$ | 35.3 | $1 \mathrm{e} 5$ | 38.3 | 36.8 | $8 \mathrm{e} 3$ | 37.7 | 35.3 | $5 \mathrm{e} 3$ | 37.8 | 35.5 | $2 \mathrm{e} 6$ |
|  | QuaRot* | 59.5 | 10.4 | 41.5 | 91.2 | 60.9 | 59.0 | 8.2 | 66.9 | 64.8 | 6.1 | 72.6 | 69.7 | 4.2 |
|  | QuaRot | 63.8 | 7.9 | 65.4 | 20.4 | 65.6 | 63.5 | 6.1 | 68.5 | 66.7 | 5.4 | 72.9 | 70.4 | 3.9 |
|  | SpinQuant* | 64.6 | 7.7 | 70.1 | 4.1 | 63.6 | 61.8 | 6.1 | 67.7 | 65.8 | 5.4 | 73.5 | 71.1 | 3.9 |
|  | SpinQuant | $\mathbf{6 5 . 8}$ | 7.1 | 69.5 | 5.5 | 65.9 | 64.1 | 5.9 | 69.3 | 67.2 | 5.2 | 73.5 | 71.0 | 3.8 |
|  | $\overline{\mathrm{R}} \overline{\mathrm{TN}}$ | $\overline{38.2}$ | $\overline{1} \bar{e} \overline{3}$ | $35 . \overline{2}$ | $\overline{1} \mathrm{e} \overline{5}$ | $\overline{3} \overline{8} . \overline{2}$ | $\overline{37.1}$ | $2 \overline{\mathrm{e} 3}$ | $\overline{3} 7.2$ | $\overline{-} 3 \overline{5} . \overline{4}$ | $\overline{7} \overline{\mathrm{e}}-\overline{1}$ | 37.4 | $\overline{3} 5.0$ | $\overline{2} \bar{e} \overline{5}$ |
|  | SmoothQuant | 38.7 | 1e3 | 52.4 | 22.1 | 40.1 | 39.0 | $6 \mathrm{e} 2$ | 42.7 | 40.5 | 56.6 | 58.8 | 55.9 | 10.5 |
|  | LLM-QAT | 43.2 | 52.5 | - | - | 45.5 | 44.9 | 14.9 | - | - | $-1 \quad$ | - | - | - |
| $4-4-4$ | GPTQ | 37.1 | $1 \mathrm{e} 3$ | 35.1 | $1 \mathrm{e} 5$ | 38.1 | 36.8 | $9 \mathrm{e} 3$ | 37.4 | 35.2 | $5 \mathrm{e} 3$ | 37.8 | 35.6 | $1 \mathrm{e} 6$ |
|  | QuaRot* | 58.6 | 10.9 | 41.3 | 92.4 | 60.5 | 58.7 | 8.2 | 66.5 | 64.4 | 6.2 | 72.2 | 69.5 | 4.2 |
|  | QuaRot | 63.3 | 8.0 | 65.1 | 20.2 | 64.4 | 62.5 | 6.4 | 68.1 | 66.2 | 5.4 | 73.0 | 70.3 | 3.9 |
|  | SpinQuant* | 64.1 | 7.8 | 70.1 | 4.1 | 63.2 | 61.5 | 6.2 | 67.7 | 65.5 | 5.4 | 73.2 | 70.5 | 3.9 |
|  | SpinQuant | 65.2 | 7.3 | 69.3 | 5.5 | 66.0 | 64.0 | 5.9 | 68.9 | 66.9 | 5.3 | 73.7 | 71.2 | 3.8 |

also outperforms QuaRot by non-negligible 1.5 points on LLaMA-2 7B models. When applying SpinQuant approach to larger models, we can obtain 4-4-4 LLaMA-2 13B and LLaMA-2 70B models with only 1.4 and 1.7 points gap to the corresponding full-precision network, respectively.

It is noteworthy that our LLaMA-3 quantization experiences a larger degradation compared to LLaMA-2. This trend is also observed in a recent work [15], suggesting that LLaMA-3 is generally more difficult to quantize. The previously best 4-4-4 quantized model, obtained with QuaRot, had a 6.3 point gap to full precision on the $8 \mathrm{~B}$ model and an even larger drop of 9.4 points on the $70 \mathrm{~B}$ model. In contrast, SpinQuant improves the LLaMA-3 8B and 70B models by 1.9 points and 5.0 points respectively, reducing the gap to approximately 4.4 points for both model sizes.

In the 4-4-16 quantization setting, SpinQuant consistently outperforms previous work with a nonnegligible margin. Notably, using SpinQuant for W4A4 quantization yields gaps to floating point networks as small as 2.8/1.1/1.8 points averaged across eight zero-shot tasks on LLaMA-2 7B/13B/70B models, respectively. In LLaMA-3 models, SpinQuant further narrows the accuracy gap to full precision from the previous SoTA of 5.8 points to 3.8 points on the $8 \mathrm{~B}$ model, and from 9.1 to 4.4 points on the $70 \mathrm{~B}$ model.

In the context of 4-bit weight-only quantization, SpinQuant continues to demonstrate its effectiveness in enhancing accuracy and minimizing the disparity between the quantized network and full-precision network to $\sim 1$ point on the average zero-shot accuracy on LLaMA-3 8B and LLaMA-2 models, and shows significant improvement on the LLaMA-3 70B model.

Table 2: Compatibility with GPTQ.

| \#Bits | Task | Cayley on 4-4-KV | Cayley on 16-4-KV |
| :---: | :---: | :---: | :---: |
| $4-4-16$ | 0-shot ${ }^{8}$ Avg. <br> Wiki | $61.0_{ \pm 1.0}$ <br> $6.7_{ \pm 0.07}$ | $64.1_{ \pm 0.4}$ <br> $5.9_{ \pm 0.00}$ |
| $4-4-4$ | 0-shot ${ }^{8}$ Avg. <br> Wiki | $60.9_{ \pm 0.6}$ <br> $6.8_{ \pm 0.15}$ | $64.0_{ \pm 0.3}$ <br> $5.9_{ \pm 0.01}$ |

Table 3: Impact of individual rotation matrices ( $R_{1}, R_{2}, R_{3}, R_{4}$ ) to quantized model perplexity on WikiText2 and average accuracy on eight zero-shot common sense reasoning tasks. Refer to Figure 1 for $R_{1}$ (residual rotation), $R_{2}$ (MHSA rotation), $R_{3}$ (query/key rotation), and $R_{4}$ (down projection layer input rotation).

|  | W4A16KV16 |  | W4A4KV16 |  | W4A4KV4 |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  | 0-shot | Wiki | 0-shot | Wiki | 0-shot | Wiki |
| Rotation | Avg.( $\uparrow$ ) | $(\downarrow)$ | Avg. $(\uparrow)$ | $(\downarrow)$ | Avg. $(\uparrow)$ | $(\downarrow)$ |
| no rotation | 63.6 | 7.2 | 35.6 | 2167.2 | 37.1 | 2382.5 |
| $R_{1}$ | $60.2_{ \pm 0.5}$ | $6.5_{ \pm 0.0}$ | $52.3_{ \pm 0.7}$ | $9.6_{ \pm 0.0}$ | $51.8_{ \pm 0.5}$ | $9.9_{ \pm 0.1}$ |
| $R_{1}+R_{4}$ | $64.7_{ \pm 0.3}$ | $5.5_{ \pm 0.0}$ | $61.6_{ \pm 0.6}$ | $6.2_{ \pm 0.04}$ | $60.6_{ \pm 0.6}$ | $6.4_{ \pm 0.1}$ |
| $R_{1}+R_{2}+R_{4}$ | $64.6_{ \pm 0.2}$ | $5.5_{ \pm 0.0}$ | $62.0_{ \pm 0.6}$ | $6.2_{ \pm 0.0}$ | $61.0_{ \pm 0.4}$ | $6.3_{ \pm 0.0}$ |
| $R_{1}+R_{2}+R_{3}+R_{4}$ | $64.6_{ \pm 0.3}$ | $5.5_{ \pm 0.0}$ | $61.8_{ \pm 0.4}$ | $6.1_{ \pm 0.0}$ | $61.5_{ \pm 0.3}$ | $6.2_{ \pm 0.0}$ |

### 4.3 Ablation studies

### 4.3.1 Compatibility with GPTQ

In the context where both weights and activations are quantized, we observed that the Cayleyoptimized rotations tend to adapt effectively to both weight and activation quantization. Given that GPTQ significantly helps mitigate the errors due to weight quantization, but leaves activation quantization untouched, we elect to optimize the rotation matrices with respect to a network where only activations are quantized. This approach allows the rotation to more efficiently manage the activation quantization error, while leaving the weight quantization error to be addressed by GPTQ. As shown in Table 2, this modification resulted in superior performance in both W4A4 and W4A4KV4 settings in the LLaMA-2 7B model, which is the configuration we have chosen to utilize throughout the rest of this paper.

### 4.3.2 Impact of each rotation

We performed an ablation study to assess the influence of each rotation matrix, employing round-tonearest quantization for our evaluation. Cayley optimization was used to optimize $R_{1}$ and $R_{2}$, while $R_{3}$ and $R_{4}$ were maintained as Hadamard matrices across all configurations. As indicated in Table 3 . the inclusion of additional rotations generally enhances the accuracy of quantization, particularly in the case of activation quantization. In LLaMA models, the most significant outliers are found prior to the down projection layer. Consequently, implementing the online Hadamard rotation $\left(R_{4}\right)$ before these layers can most effectively improve accuracy. The introduction of $R_{2}$ further boosts the 4-bit activation quantization by $\sim 0.4$ points. Interestingly, applying initial rotations in the residual $\left(R_{1}\right)$ results in a 3-5 point drop in weight quantization accuracy. However, this negative impact is counterbalanced by the incorporation of $R_{4}$, effectively restoring the accuracy.

Table 4: Floating-point(FP) rotation vs Hadamard rotation

| \#Bits |  | No Cayley + RTN |  | Cayley + RTN |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  | Task | FP | Hadamard | FP init. | Hadamard init. |
| $4-16-16$ | 0-shot ${ }^{8}$ Avg.( $\uparrow$ ) | $62.5_{ \pm 0.8}$ | $62.4_{ \pm 1.0}$ | $64.9_{ \pm 0.4}$ | $64.66_{ \pm 0.3}$ |
|  | Wiki $(\downarrow)$ | $6.7_{ \pm 0.12}$ | $6.9_{ \pm 0.45}$ | $5.5_{ \pm 0.01}$ | $5.5_{ \pm 0.01}$ |
| $4-4-16$ | 0-shot ${ }^{8}$ Avg.( $\uparrow$ ) | $49.4_{ \pm 2.8}$ | $59.0_{ \pm 1.0}$ | $61.6_{ \pm 0.4}$ | $61.8_{ \pm 0.4}$ |
|  | Wiki $(\downarrow)$ | $15.9_{ \pm 4.04}$ | $8.2_{ \pm 0.73}$ | $6.2_{ \pm 0.06}$ | $6.1_{ \pm 0.03}$ |
| $4-4-4$ | 0-shot 8 Avg.( $\uparrow$ - | $48.3_{ \pm 2.7}$ | $58.7_{ \pm 1.0}$ | $61.5_{ \pm 0.8}$ | $61.5_{ \pm 0.3}$ |
|  | Wiki $(\downarrow)$ | $18.2_{ \pm 4.35}$ | $8.2_{ \pm 0.36}$ | $6.3_{ \pm 0.08}$ | $6.2_{ \pm 0.03}$ |

### 4.3.3 Rotation type

In Table 4 we evaluate the impact of random orthogonal floating-point rotation matrices and random Hadamard matrices on quantization accuracy, utilizing round-to-nearest quantization for our analysis. Prior to the application of Cayley optimization, the Hadamard matrices yield better quantized network performance compared to floating-point rotation matrices. However, after Cayley optimization, the initial choice of rotation, whether floating-point or Hadamard, becomes less significant. This is likely due to the Cayley optimization's ability to locate an optimal local minima that effectively minimizes quantization error, thereby enhancing robustness to varying types of rotation initialization.

## 5 Related Work

Quantization Neural network quantization has been demonstrated as an effective tool for model size compression and storage reduction [30, 19, 29, 22]. However, in large language models (LLMs), quantization presents unique challenges due to the presence of numerous outliers. These outliers dominate the quantization range, leaving only a few effective bits for the majority of values. Various strategies have been proposed to address the difficulties in LLM quantization. These include separating outliers and using mixed precision [11, 42, 18], employing Hessian-based methods to mitigate quantization difficulty [14], trading outliers between weights and activations [43, 23, 24] utilizing weight equalization [29], and even suggesting architectural modifications to handle outliers during pre-training [44]. Recently two QuIP papers [7, 41] introduce the incoherence processing using random rotation matrices and applying vector quantization on the weights for compression. This does introduce extra overhead and imposes some constraints on the devices the LLM is deployed to in the availability of vector quantization kernels. Ashkboos et al. [5] recently released their method QuaRot. Our work was done concurrently with theirs. The authors also introduce random rotation matrices in a similar fashion as our paper. However, there are some notable differences. (1) Instead of relying on random matrices, we learn the rotation matrices. As seen in section 2.2 , if accidentally randomized to a poor random matrix choice could be quite detrimental to performance. (2) We introduce less extra parameters in our method with the rotation matrix inside of each block as in section 3.1. (3) Our pipeline gives significantly better performance compared to QuaRot, as demonstrated in Section 4 .

Optimization in orthonormal space The optimization of rotation matrices is carried out within the Stiefel Manifold [16], which encompasses all orthonormal matrices. Optimization while staying on this manifold can be done by e.g., parameterizing a skew-symmetric matrix and applying the Cayley transformation on top of it [31], or using a matrix exponential [1, 20]. However, these methods rely on expensive inverse or matrix-exponential functions that are applied every iteration. Instead, we follow the more efficient method named Cayley $S G D$ [21], which can be applied to optimize a rotation matrix $R$ for arbitrary loss functions efficiently. Cayley $S G D$ relies on an iterative approximation of the Cayley Transform that is conducted solely with matrix multiplications.

## 6 Conclusions

In this paper, we present SpinQuant, a novel quantization technique that effectively bridges the performance gap between full precision and 4-bit weight, activation, and kv-cache quantization, for the LLaMA-2 7B model to within a mere 2.9 points. At its core, SpinQuant leverages the rotation invariance property of LLM models to insert rotation matrices that diminish outliers in the weights and intermediate activations while maintaining the network's full-precision output numerically identical. Additionally, SpinQuant incorporates Cayley $S G D$ for optimizing rotation matrices to boost quantization performance further. Importantly, our method is compatible with more advanced weight quantization techniques (e.g., GPTQ) and demonstrates state-of-the-art performance.

## 7 Limitations and Broader Impacts

In this study, we introduce a rotation-based quantization technique designed for precise low-bit quantization of LLMs. This method has the potential to reduce energy consumption during LLM inference. We evaluated our model's performance in an academic context. However, real-world tasks may involve different training data and activation distributions, suggesting that the model's generalizability to real-world scenarios warrants further investigation.

## References

[1] P-A Absil and Jérôme Malick. Projection-like retractions on matrix manifolds. SIAM Journal on Optimization, 22(1):135-158, 2012.

[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[3] AI@ Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL_CARD.md

[4] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. In The Twelfth International Conference on Learning Representations, 2023.

[5] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. 2023.

[6] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432-7439, 2020.

[7] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36, 2024.

[8] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019

[9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai 2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

[10] Samuel Rhys Cox and Wei Tsang Ooi. Conversational interactions with npcs in llm-driven gaming: Guidelines from a content analysis of player feedback. In International Workshop on Chatbot Research and Design, pages 167-184. Springer, 2023.

[11] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. 2022.

[12] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024.

[13] Nelson Elhage, Robert Lasenby, and Christopher Olah. Privileged bases in the transformer residual stream. Transformer Circuits Thread, 2023.

[14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.

[15] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good are low-bit quantized llama3 models? an empirical study. arXiv preprint arXiv:2404.14047, 2024.

[16] Ioan Mackenzie James. The topology of Stiefel manifolds, volume 24. Cambridge University Press, 1976.

[17] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[18] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.

[19] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018.

[20] Mario Lezcano-Casado and David Martinez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group. 2019.

[21] Jun Li, Li Fuxin, and Sinisa Todorovic. Efficient riemannian optimization on the stiefel manifold via the cayley transform. arXiv preprint arXiv:2002.01113, 2020.

[22] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations (ICLR), 2021.

[23] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.

[24] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting Cheng. Llm-fp4: 4-bit floating-point quantized transformers. arXiv preprint arXiv:2310.16836, 2023.

[25] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023.

[26] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905, 2024.

[27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

[28] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.

[29] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325-1334, 2019.

[30] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020.

[31] Yasunori Nishimori and Shotaro Akaho. Learning algorithms utilizing quasi-geodesic flows on the stiefel manifold. Neurocomputing, 67:106-135, 2005.

[32] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

[33] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.

[34] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.

[35] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023.

[36] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.

[37] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

[38] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine, 29(8):1930-1940, 2023.

[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[41] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip\#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024.

[42] Mart van Baalen, Markus Nagel Andrey Kuzmin, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, and Paul Whatmough. Gptvq: The blessing of dimensionality for llm quantization. 2023.

[43] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In CVPR, 2022.

[44] Tijmen Blankevoort Yelysei Bondarenko, Markus Nagel. Quantizable transformers: Removing outliers by helping attention heads do nothing. 2023.

[45] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

[46] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[47] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate $11 \mathrm{~m}$ serving. arXiv preprint arXiv:2310.19102, 2023.

Table 5: Complete omparison of the perplexity score on WikiText2 and averaged accuracy on Zero-shot Common Sense Reasoning tasks on LLaMA-3. We reported the mean and standard deviation across six trails for SpinQuant as well as our reproduced results of GPTQ and QuaRot.

| Model | \#Bits <br> W-A-KV | Method | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-13.jpg?height=55&width=101&top_left_y=366&top_left_x=690) | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-13.jpg?height=55&width=105&top_left_y=366&top_left_x=794) | $\underset{(\uparrow)}{\text { BoolQ }}$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-13.jpg?height=55&width=110&top_left_y=366&top_left_x=997) | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-13.jpg?height=55&width=105&top_left_y=366&top_left_x=1104) | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-13.jpg?height=55&width=110&top_left_y=366&top_left_x=1205) | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-13.jpg?height=55&width=100&top_left_y=366&top_left_x=1314) | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-13.jpg?height=55&width=109&top_left_y=366&top_left_x=1413) | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-13.jpg?height=55&width=105&top_left_y=366&top_left_x=1517) | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-13.jpg?height=55&width=127&top_left_y=366&top_left_x=1621) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 8B | $16-16-16$ | ull Precision | 77.6 | 57.7 | 83.3 | 80.7 | 48.7 | 79.6 | 55.8 | 73.7 | 69.6 | 6.1 |
|  |  | $\overline{\mathrm{R}} \overline{\mathrm{T}} \overline{\mathrm{N}}$ | $\overline{7} \overline{4} \overline{7}$ | 790 | $\overline{730}$ | $\overline{7} \overline{0}$ | $\overline{4} \overline{7} \overline{6}$ | 766 | 534 | $7 \overline{1}$ | $-65 \overline{4}$ | $\overline{7} \overline{8}$ |
|  |  | othQuant | 67.6 | 41.3 | 72.6 | 74.4 | 46.7 | 70.6 | 48.0 | 67.0 | 61.0 | 10.7 |
|  |  |  | 77.1 | 53.0 | 82.4 | 79.0 | 48.1 | 76.6 | 54.4 | 71.3 | 67.7 |  |
|  |  | $\mathrm{GP}^{\prime}$ | $73.5_{ \pm 1.9}$ | $50.2 \pm 1.0$ | $79.7 \pm 1.5$ | $77.9 \pm 0.7$ | $48.8_{ \pm 0.3}$ | $76.4_{ \pm 0.2}$ | $52.6_{ \pm 0.9}$ | $72.6_{ \pm 1.0}$ | $66.5_{ \pm 0.6}$ | $7.2 \pm 0.02$ |
|  | $4-16-16$ | QuaRot* | $73.8 \pm 1.4$ | $51.4 \pm 0.9$ | $80.0 \pm 1.7$ | $77.7 \pm 1.1$ | $47.8 \pm 0.8$ | $75.9 \pm 0.4$ | $52.1 \pm 0.8$ | $71.1 \pm 0.7$ | $66.2 \pm 0.6$ | $7.5 \pm 0.02$ |
|  |  | QuaRc | $77.0 \pm \pm 0.8$ | $55.2 \pm 0.8$ | $82.2 \pm 0.6$ | $79.5_{ \pm 0.4}$ | $48.6_{ \pm 0.5}^{ \pm}$ | $78.3_{ \pm 0.3}$ | $54.4_{ \pm 0.9}$ | $72.1_{ \pm 0.8}$ | $68.4_{ \pm 0.2}$ | $6.4_{ \pm 0.01}$ |
|  |  | SpinQuant* | $76.5 \pm 1.1$ | $54.8 \pm 1.5$ | $79.8 \pm 2.1$ | $79.0 \pm 0.8$ | $47.6 \pm 1.0$ | $78.0 \pm 0.3$ | $53.3 \pm 1.1$ | $71.6 \pm 0.8$ | $67.6 \pm 0.6$ | $6.5 \pm 0.01$ |
|  |  | SpinQ | $77.6_{ \pm 0.7}$ | $55.5_{ \pm 0.9}$ | $81.4_{ \pm 1.3}$ | $79.4 \pm 0.2$ | $48.2 \pm 0.4$ | $78.4_{ \pm 0.2}$ | $55.1 \pm 1.3$ | $72.4 \pm 1.0$ | $68.5_{ \pm 0.2}$ | $6.4_{ \pm 0.01}$ |
|  |  | $\overline{\mathrm{R}} \overline{\mathrm{T}} \overline{\mathrm{N}}$ | $\overline{3} \overline{1} . \overline{8}$ | 27.6 | $\overline{47} . \overline{2}$ | $-5 \overline{3} . \overline{8}$ | $\overline{3} \overline{9} . \overline{7}$ | $\overline{3} 0.8$ | $\overline{28} \overline{2}$ | $4 \overline{8} . \overline{9}$ | $\overline{3} \overline{8} . \overline{5}$ | $9 \overline{2} \overline{9} .9$ |
|  |  | nt | 36.3 | 26.3 | 50.6 | 54.1 | 40.3 | 31.4 | 30.6 | 52.9 | 40.3 | 867.5 |
|  |  | AT | 44.1 | 29.7 | 58.0 | 61.5 | 42.1 | 39.9 <br> 39.9 | 33.0 <br> 33.0 | 51.3 | 44.9 | 42.9 |
|  |  | $\mathrm{GP}^{\prime}$ | $31.4_{ \pm 0.9}$ | $24.7 \pm 1.4$ | $42.5 \pm 1.3$ | $52.7 \pm 1.0$ | $39.1 \pm 0.9$ | $27.8 \pm 0.3$ | $27.3 \pm 2.5$ | $50.7 \pm 1.1$ | $37.0 \pm 0.7$ | 955.9 |
|  | $4-4-16$ |  | $66.0_{ \pm 1.2}$ | $42.5_{ \pm 1.0}$ | $70.5 \pm 2.0$ | $72.5_{ \pm 0.6}$ | $45.4_{ \pm 0.9}$ | $68.6 \pm 0.9$ | $46.7 \pm 2.1$ | $63.5 \pm 1.7$ | $59.5 \pm 0.6$ | $10.4 \pm 0.26$ |
|  |  |  | $72.4 \pm 1.1$ | $48.0 \pm 1.1$ | $75.8 \pm 1.4$ | $75.9 \pm 0.6$ | $47.1 \pm 0.8$ | $73.7 \pm 0.6$ | $51.0 \pm 1.8$ | $66.7 \pm 1.2$ | $63.8 \pm 0.5$ | $7.9 \pm 0.04$ |
|  |  | Spi | $74.1 \pm 1.6$ | $49.7 \pm 1.7$ | $75.8 \pm 3.2$ | $77.0 \pm 0.6$ | $46.4 \pm 0.9$ | $74.7 \pm 0.4$ | $52.0 \pm 1.9$ | $67.1 \pm 1.0$ | $64.6 \pm 0.8$ | $7.7 \pm 0.05$ |
|  |  | Spi | $75.0 \pm 1.0$ | $50.9_{ \pm 1.2}$ | $78.9_{ \pm 0.6}$ | $77.5_{ \pm 0.7}$ | $47.2 \pm 0.6$ | $75.9 \pm 0.4$ | $52.9_{ \pm 1.6}$ | $68.5_{ \pm 1.0}$ | $65.8_{ \pm 0.2}$ | $7.1 \pm 0.02$ |
|  |  | $\overline{\mathrm{R}} \overline{\mathrm{T}}$ | $\overline{3} \overline{1} \overline{9}$ | $\overline{2} 6.1$ | $\overline{46} \overline{2}$ | $5 \overline{2} . \overline{3}$ | $\overline{3} \overline{9} . \overline{9}$ | $29.9^{-}$ | 28.6 | $-5 \overline{1} . \overline{0}$ | $\overline{3} \overline{8} . \overline{2}$ | $\overline{1}, \overline{1} 1 \overline{8} \cdot \overline{5}$ |
|  |  |  | 33.5 | 25.1 | 49.6 | 53.1 | 40.3 | 28.8 | 29.6 | 49 | 38.7 | 1.530 .5 |
|  |  | $\mathrm{L}$ | 40.5 | 26.6 | 52.7 | 59.9 | 42.3 | 37.5 | 33.6 | 52. | 43.2 | 52.5 |
|  | $4-4-4$ | $\mathrm{G}$ | $31.0_{ \pm 0.9}$ | $24.9_{ \pm 1.0}$ | $41.9_{ \pm 1.1}$ | $52.8_{ \pm 0.6}$ | $38.4_{ \pm 0.3}$ | $27.9_{ \pm 0.4}$ | $28.2_{ \pm 2.0}$ | $51.4_{ \pm 1.1}$ | $37.1_{ \pm 0.3}$ | $1,071.7$ |
|  |  | $\mathrm{Q}$ | $65.9_{ \pm 3.0}$ | $41.3 \pm 2.2$ | $69.5_{ \pm 2.3}$ | $71.9_{ \pm 0.9}$ | $44.8_{ \pm 1.1}^{ \pm}$ | $67.2 \pm 1.6$ | $46.5_{ \pm 1.8}$ | 61.9 | $58.6_{ \pm 0.8}$ | $10.9_{ \pm 0.26}$ |
|  |  | $\mathrm{Q}$ | $6_{ \pm 0.9}^{ \pm 0.0}$ | $48.0_{ \pm 1.2}$ | $74.9_{ \pm 1.8}$ | $75.1_{ \pm 0.5}$ | $46.8_{ \pm 0.9}$ | $73.1_{ \pm 0.7}$ | $50.4_{ \pm 1.0}$ | $66.1_{ \pm 1.4}$ | $63.3_{ \pm 0.3}$ | $8.0_{ \pm 0.05}$ |
|  |  |  | $72.6_{ \pm 1.4}$ | $49.5 \pm 2.3$ | $74.8 \pm 6.3$ | $76.6_{ \pm 0.8}$ | $46.4 \pm 0.4$ | $74.3 \pm 1.0$ | $50.6 \pm 3.1$ | 67.9 | $64.1 \pm 1.7$ | $7.8 \pm 0.05$ |
|  |  | SF | $74.4_{ \pm 1.3}$ | $50.4 \pm 1.7$ | $77.7_{ \pm 1.6}$ | $76.9 \pm 0.6$ | $47.2 \pm 0.5$ | $75.5_{ \pm 0.2}$ | $52.0_{ \pm 1.1}$ | $67.2 \pm 1.4$ | $65.2 \pm 0.6$ | $7.3 \pm 0.02$ |
| 70B | $16-16-16$ | ision | 80.6 | 64.5 | 87.4 | 83.7 | 51.7 | 85.3 | 62.0 | 80.5 | 74.5 | 2.8 |
|  |  | $\overline{\mathrm{R}}$ | $\overline{2} \overline{7} . \overline{3}$ | $\overline{2} 7.2$ | $\overline{37.8}$ | $-5 \overline{1} . \overline{0}$ | $\overline{3} \overline{9} . \overline{1}$ | $\overline{2} 5.6$ | 26.2 | $-4 \overline{9} . \bar{\varepsilon}$ | $\overline{3} \overline{5} . \overline{5}$ | $\overline{1} e \overline{5}$ |
|  |  | ant | 76.7 | 45.5 | 80.6 | 81.2 | 48.7 | 81.1 | 46.2 | 75.5 | 66.9 | 12.0 |
|  |  | $\mathrm{G}$ | $28.2 \pm$ | $4.7 \pm 1.6$ | $37.9 \pm 0.1$ | $50.7 \pm 0.8$ | $39.0 \pm 0.6$ | $26.8 \pm 1.7$ | $27.7 \pm 3.6$ | $50.5 \pm 0.6$ | $35.7 \pm 0.6$ | $1 \mathrm{e} 5$ |
|  | $4-16-16$ |  | 66 | $44.3 \pm 2.7$ | $67.0 \pm 4.9$ | $75.2 \pm 2.0$ | $44.1 \pm 2.1$ | $59.8 \pm 8.6$ | $42.5 \pm 3$ | $58.5=$ | $57.2 \pm 2.7$ | $41.6 \pm 16.76$ |
|  |  |  |  | $58.6 \pm 2.6$ | $86.4=$ | $83.8 \pm 0.4$ | $510+04$ | 1 | 177 |  | 703 | $7.9 \pm 0.21$ |
|  |  | $\mathrm{S}]$ |  | 5 | 84. | $82.3 \pm$ | $50.3 \pm 0$ | $82.6 \pm 0.4$ | $57.4 \pm$ | 77.5 | 71 . | $3.9 \pm 0.47$ |
|  |  |  | $\pm 2.7$ | $1 \pm 0.9$ | $85.2 \pm 1.1$ | $82.8 \pm 1.0$ | $50.6 \pm 0.6$ | $83.5 \pm 0.2$ | $54.8 \pm 6.6$ | $78.5 \pm 1.6$ | $71.6 \pm 1.1$ | $4.8 \pm 1.97$ |
|  |  | $\overline{\mathrm{R}} \overline{\mathrm{T}}$ | $\overline{2} \overline{7} . \overline{6}$ | $\overline{27.0}$ | $\overline{38}$ | $-5 \overline{0} . \overline{1}$ | $\overline{3} \overline{8} . \overline{5}$ | $\overline{2} 6.0$ | 28.4 | $4 \overline{9}$ | $\overline{3} \overline{5} . \overline{6}$ | ![](https://cdn.mathpix.com/cropped/2024_06_04_df3bbcc16c2f64a9fcb0g-13.jpg?height=32&width=127&top_left_y=1168&top_left_x=1621) |
|  |  | ant | 0 | 35.7 |  | 70.3 | 44.3 | 61.7 | 44.2 | 63 | 55 | 18.0 |
|  |  |  | $27.0 \pm 0.4$ | $26.1 \pm 1.5$ | $39.1 \pm 1.2$ | $50.4 \pm 0.4$ | $38.9 \pm 0.4$ | $25.7 \pm 0.2$ | $25.7 \pm 1.7$ | $49.6 \pm 0.9$ | $35.3 \pm 0.2$ | $1 \mathrm{e} 5$ |
|  | $4-4-16$ | $\mathrm{Q}$ | 40.8 | $26.2 \pm 2.8$ | $52.4 \pm 3.8$ | $58.4 \pm 3.5$ | $40.0 \pm 0.9$ | $33.8 \pm 3.9$ | $30.0 \pm 3.6$ | $50.2 \pm 1.7$ | $41.5 \pm 2.5$ | $91.2 \pm 24.05$ |
|  | $4-4-10$ |  |  |  |  |  |  |  |  |  |  | $20.4 \pm 3.23$ |
|  |  |  |  | $55.9 \pm 1.1$ | $81.7=$ | $80.9 \pm 0.6$ | $49.0 \pm 0.5$ | $80.9 \pm 0.4$ | $58.7 \pm$ | $76.2=$ | $70.1 \pm 0.4$ | $4.1 \pm 0.02$ |
|  |  |  | $76.7 \pm 1.9$ | $55.6 \pm 2.1$ | $82.3 \pm 1.3$ | $80.6 \pm 1.1$ | $49.8 \pm 0.9$ | $81.0 \pm 1.6$ | $55.4 \pm 6.3$ | $74.5 \pm 3.9$ | $69.5 \pm 2.1$ | $5.5 \pm 2.56$ |
|  |  |  |  |  |  | $-5 \overline{0} \overline{4}$ | $\overline{3} \overline{8} . \overline{8}$ | 25.8 | $25 . \overline{2}$ | $5 \overline{1} \overline{8}$ | $\overline{3} \overline{5} . \overline{2}$ |  |
|  |  | Quant | 55.0 | 34.9 | 62.2 | 66 | 43.1 | 59 . | 39.8 | 58 | 52 | 22.1 |
|  |  |  | $27.1 \pm 0$ | $4.8 \pm 1$ | $38.8 \pm 0$ | $50.8 \pm 0$ | $39.0 \pm 0$ | $25.5 \pm 0.2$ | $24.8 \pm 2$ | $49.8 \pm 0.6$ | $35.1 \pm 0.2$ | 16 |
|  | $4-4-4$ |  | - | $25.7 \pm 3.7$ | $50.9 \pm 4.5$ | $57.7 \pm$ | $39.9 \pm 0.8$ | $33.9 \pm 3.9$ | $31.0 \pm 2$ | $51.0=$ | $41.3 \pm 2.6$ | $92.4 \pm 24.18$ |
|  | $4-4-4$ |  | $\pm 1.9$ | $51.6 \pm 1.0$ | $77.5 \pm$ | $78.9 \pm$ | $49.0 \pm$ | $78.2 \pm 0.9$ | $45.5 \pm$ | 67.8 | $65.1 \pm 1.1$ | 3.12 |
|  |  |  |  |  |  |  |  |  |  |  | 70 | $4.1 \pm 0.01$ |
|  |  | SpinQuant | $\pm 2.1$ | $\pm 2.6$ | $.2 \pm 1.7$ | $.0 \pm 0.8$ | $49.5 \pm 1.0$ | $81.2 \pm 1.3$ | $53.8 \pm 6.3$ | $74.2 \pm 3.8$ | $69.3 \pm 2.2$ | $5.5 \pm 2.59$ |
