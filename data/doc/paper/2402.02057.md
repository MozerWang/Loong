# Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING 

Yichao Fu ${ }^{1}$ Peter Bailis ${ }^{2}$ Ion Stoica ${ }^{3}$ Hao Zhang ${ }^{1}$


#### Abstract

Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce LOOKAHEAD DECODING, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step $\log$ (FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of LOOKAHEAD DECODING can speed up autoregressive decoding by up to $1.8 \mathrm{x}$ on MT-bench and $4 \mathrm{x}$ with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://githu.com /hao-ai-lab/LookaheadDecoding


## 1. Introduction

Large language models (LLMs) are transforming the AI industry. As they are increasingly integrated into diverse applications such as search (Team et al., 2023) and chatbots (Ouyang et al., 2022), generating long sequences at low-latency using LLMs is becoming one significant requirement. However, current LLMs generate text based on (Touvron et al., 2023a;b; Jiang et al., 2023; OpenAI, 2023) autoregressive decoding, which falls short in efficiency, primarily for two reasons. First, autoregressive decoding generates only one token at a time. Hence, the overall generation time is proportional to the number of decoding steps. Second, each decoding step largely underutilizes the parallel processing capabilities of modern accelerators (e.g.,[^0]

GPUs). Given the pressing need for low latency in various applications, improving autoregressive decoding remains a central challenge.

Several approaches have been proposed - one such approach is speculative decoding (Chen et al., 2023; Leviathan et al., 2023) and its variants (He et al., 2023; Stern et al., 2018; Cai et al., 2024; Li et al., 2023; Liu et al., 2023; Miao et al., 2023). These methods all follow a guess-and-verify approach: they use a draft model to speculate several subsequent tokens and then use the original (base) LLM to verify these tokens in parallel. Since the draft model requires much fewer resources and the cost of verifying multiple tokens in parallel is similar to the cost of generating a single token, these methods can achieve considerable speedups. However, their speedups are bounded by the token acceptance rate (ยง4.1), i.e., the fraction of tokens generated by the draft model that passes the verification test of the base model. This is because every token that fails verification needs to be regenerated by the base model. In the worst case, if most proposed tokens fail verification, these methods may slow down the decoding process. Therefore, achieving a high acceptance rate is essential for these methods. Unfortunately, training a draft model to achieve a high acceptance rate is non-trivial, and the trained draft model does not generalize across base models and datasets.

To address these problems, this paper develops LookAHEAD DECODING. We build upon a key observation: autoregressive decoding can be equivalently formulated as solving a non-linear system via the fixed point Jacobi iteration method (ยง2), which we term as Jacobi decoding (Santilli et al., 2023). Each Jacobi decoding step can generate multiple tokens in parallel at different positions. Although these tokens may appear at incorrect positions, we can leverage this parallel generation approach to have the LLM generate several disjoint $n$-grams in parallel in a single step. These n-grams could potentially be integrated into future parts of the generated sequence, pending verification by the base model to maintain the output distribution.

LOOKAHEAD DECODING takes advantage of the particular characteristics of autoregressive decoding, which is bounded by the memory bandwidth-as each generated token depends on all tokens before it-rather than compute, by
using the available cycles to generate and verify $n$-grams (subsequent tokens) at virtually no additional cost. In a nutshell, LOOKAHEAD DECODING consists of a lookahead branch that generates $n$-grams and a verification branch that verifies $n$-grams, both executing in a single step. To improve efficiency, we use an $n$-gram pool to cache the historical $n$-grams generated so far. This way, LoOKAHEAD DECODING can significantly reduce the latency of LLM inference just by exploiting the compute resources that autoregressive decoding would leave unused. More importantly, LOOKAHEAD DECODING scales with the compute - we show that it can linearly reduce the number of decoding steps relative to the $\log$ (FLOPs) allocated per step.

We have implemented the algorithm in both Python and CUDA, compatible with memory-efficient attention algorithms (e.g., FlashAttention (Dao, 2023)), and supports various sampling methods without changing the output distribution. We also scale it to multiple GPUs, resulting in Lookahead Parallelism. We evaluate LoOKAHEAD DECODING on the popular LLaMA-2 (Touvron et al., 2023b) models. It achieves $1.8 \mathrm{x}$ speedup on the challenging multi-turn chat dataset MT-Bench (Zheng et al., 2023) and up to 4x speedup in code completion tasks with Lookahead Parallelism on 8 GPUs. LOOKAHEAD DECODING showed significant potential in lowering the latency for latency-sensitive tasks. Our contributions are summarized as follows.

- We design LooKaHEAd DeCoding, a new lossless, parallel decoding algorithm to accelerate LLM inference without needing any auxiliary component.
- We reveal LOOKAHEAD DECODING's scaling behavior: it linearly reduces the number of decoding steps according to per-step $\log$ (FLOPs). This enables tradeoffs between the number of decoding steps and per-step FLOPs, making it future-proof.
- We show it benefits from the latest memory-efficient attentions and is easily parallelizable by developing its distributed CUDA implementations.
- We evaluated LOOKAHEAD DECODING and demonstrate its effectiveness under different settings.


## 2. Background

In this section, we formulate both autoregressive and Jacobi decoding from the lens of solving nonlinear systems.

Causal Attention in Decoder Models. Most contemporary LLMs are composed of two core components: token-wise modules (including MLP and normalization (Ba et al., 2016; Zhang \& Sennrich, 2019)) and attention (Vaswani et al., 2023) modules. Tokens interact with each other in the attention modules, while in other token-wise modules, they are processed without exchanging information with each other.
The attention layer encompasses three input elements: query $\mathbf{Q}$, key $\mathbf{K}$, and value $\mathbf{V}$, with the $i$-th token in each denoted as $\mathbf{Q}_{i}, \mathbf{K}_{i}$, and $\mathbf{V}_{i}$, respectively. The attention layer executes the following operation: $\mathbf{O}=\operatorname{softmax}\left(\mathbf{Q K}^{T}\right) \mathbf{V}$. A lower triangular mask applied to $\mathbf{Q K}^{T}$ in causal attentions (specific to decoder models) ensures that $\mathbf{O}_{i}$ is calculated only from $\mathbf{Q}_{i}$ and $\mathbf{K}_{j}, \mathbf{V}_{j}$ where $j \leq i$. Because all other layers in the LLM perform token-wise operations, for any given model input $\mathbf{x}$ and output $\mathbf{o}, \mathbf{o}_{i}(i$-th token in $\mathbf{o}$ ) is exclusively influenced by $\mathbf{x}_{j}$ ( $j$-th token in $\mathbf{x}$ ) where $j \leq i$.

Autoregressive Decoding in LLMs. LLMs take discrete integer sequences as inputs, where each integer represents a token. We notate $\mathbf{x}=\left(x_{1}, x_{2}, \ldots, x_{s}\right) \in \mathbb{N}^{s}$ of length $s$ as the input of the model, and $\mathbf{x}_{1: m}^{t}=\left(x_{1}, x_{2}, \ldots, x_{m}\right)$ to denote a slice of $\mathbf{x}$ of length $m$ at step $t$. LLMs' output characterizes the probability distribution of the next token. The probability for the $s$-th token (i.e., the output of the $s-1$-th token) is decided by all previous input tokens, represented as $P_{M}\left(x_{s} \mid \mathbf{x}_{1: s-1}\right)$. Then, the next token input $x_{s}$ is obtained by sampling from $P_{M}\left(x_{s} \mid \mathbf{x}_{1: s-1}\right)$ using different methods (e.g., greedy, top-K, and top-P (Kool et al., 2020; Holtzman et al., 2020)). When using greedy sampling, the next token is selected by applying an argmax function on $P_{M}$.

We define $\mathbf{x}^{0}$ as the prompt tokens given by the user. The LLM needs to generate an output sequence (of length $m$ ) from $\mathbf{x}^{0}$. Denote $y_{i}$ as the token generated at step $i$. The autoregressive decoding process of $m$ tokens can be seen as solving the following $m$ problems one by one (assume greedy sampling):

$$
\left\{\begin{array}{l}
y_{1}=\operatorname{argmax} P_{M}\left(y_{1} \mid \mathbf{x}^{0}\right)  \tag{1}\\
y_{2}=\operatorname{argmax} P_{M}\left(y_{2} \mid y_{1}, \mathbf{x}^{0}\right) \\
\cdots \\
y_{m}=\operatorname{argmax} P_{M}\left(y_{m} \mid \mathbf{y}_{1: m-1}, \mathbf{x}^{0}\right)
\end{array}\right.
$$

Guess-And-Verify Paradigm. The Guess-And-Verify decoding paradigm speculates multiple potential future tokens and subsequently confirms the correctness of these speculations within a single decoding step. Take speculative decoding with greedy sampling as an example: at step $t$, with the prompt $\mathbf{x}^{0}$ and tokens $\mathbf{y}_{1: t-1}$ generated so far, we can use a draft model to autoregressively generate a draft sequence $\mathbf{y}_{t: t+n-1}$ of length $n$. Because $\mathbf{y}_{t: t+n-1}$ is known a priori, we then use the LLM to solve Eqs 2 in parallel, obtaining $\mathbf{y}_{t: t+n}^{\prime}$. Then, we verify if $y_{t+i}$ is equal to $y_{t+i}^{\prime}$ for each $i$ from $i=0$ to $i=n-1$. If there is a match, we accept this token and proceed; otherwise, we stop checking and drop subsequent tokens. Finally, we update $\mathbf{y}$ with all accepted tokens.

$$
\left\{\begin{array}{l}
y_{t}^{\prime}=\operatorname{argmax} P_{M}\left(y_{t} \mid \mathbf{y}_{1: t-1}, \mathbf{x}^{0}\right)  \tag{2}\\
y_{t+1}^{\prime}=\operatorname{argmax} P_{M}\left(y_{t+1} \mid \mathbf{y}_{1: t}, \mathbf{x}^{0}\right) \\
\cdots \\
y_{t+n}^{\prime}=\operatorname{argmax} P_{M}\left(y_{t+n} \mid \mathbf{y}_{1: t+n-1}, \mathbf{x}^{0}\right)
\end{array}\right.
$$

Figure 1: Workflow of LoOKaHEAd DECODING with $W=5, N=3$, and $G=2$. For each decoding step, we do the following. (1) Generate one token at each position in the lookahead branch; (2) Verify and accept 3-grams (searched from the 3-gram pool) with the verification branch; (3) Collect and cache newly generated 3-grams in the pool from lookahead branch trajectories. (4) Update the lookahead branch to maintain a fixed window size.

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-03.jpg?height=350&width=832&top_left_y=562&top_left_x=186)

As stated in $\S 1$, these approaches depend on a good draft model, which is hard to obtain and cannot generalize.

Jacobi Decoding. By notating $f\left(y_{i}, \mathbf{y}_{1: i-1}, \mathbf{x}^{0}\right)=y_{i}-$ $\operatorname{argmax} P_{M}\left(y_{i} \mid \mathbf{y}_{1: i-1}, \mathbf{x}^{0}\right)$, we can transform Eqs 1 into the following non-linear system of equations (Song et al., 2021; Santilli et al., 2023):

$$
\left\{\begin{array}{l}
f\left(y_{1}, \mathbf{x}^{0}\right)=0  \tag{3}\\
f\left(y_{2}, y_{1}, \mathbf{x}^{0}\right)=0 \\
\cdots \\
f\left(y_{m}, \mathbf{y}_{1: m-1}, \mathbf{x}^{0}\right)=0
\end{array}\right.
$$

We can solve this non-linear system using Jacobi iteration by iteratively updating all $y_{i}$ from a random initial guess $\mathbf{y}^{0}$, along the trajectory $\mathbf{y}^{1}, \ldots, \mathbf{y}^{t}, \ldots$, until converging to the fixed point solution $\mathbf{y}^{m}$. We detail this algorithm, termed as Jacobi decoding, in Appendix Algorithm 1. This process guarantees to return the solution of all $m$ variables $y_{i}$ in at most $m$ iterations, as the very first token of each Jacobi update matches autoregressive decoding. Sometimes, more than one token might be correctly generated in a single iteration, potentially reducing the number of decoding steps. It is worth noting that, as $\mathbf{y}^{t}$ is generated based on the past value $\mathbf{y}^{t-1}$ on the trajectory, any two adjacent tokens from $\mathbf{y}^{t-1}$ and $\mathbf{y}^{t}$ can form a meaningful 2-gram.

Limitations of Jacobi Decoding. Empirically, we observe Jacobi decoding can hardly reduce decoding steps, even if it can generate multiple tokens per step. This is because the generated tokens are often put in the wrong positions of the sequence, and correctly placed tokens are frequently replaced by subsequent Jacobi iterations. These prevent it from achieving wall-clock speedup.

## 3. LOOKAHEAD DECODING

LOOKAHEAD DECODING leverages Jacobi decoding's ability to generate many tokens in one step but addresses its limitation. Fig. 1 illustrates its workflow. The key design in LooKAHEAD DECODING is to keep track of the trajectory of Jacobi decoding and generate $n$-gram from this trajectory. This is achieved by maintaining a fixed-sized $2 \mathrm{D}$ window, with the two dimensions corresponding to the sequence and the time axis, respectively, to generate multiple disjoint $n$-grams from the Jacobi iteration trajectory in parallel. We call this process the lookahead branch. In addition, LOOKAHEAD DECODING introduces an $n$-gram pool to cache these $n$-grams generated along the trajectory. Promising $n$-gram candidates are verified later by a designed verification branch to preserve the LLM's output distribution; if passing verification, those disjoint n-grams are integrated into the sequence. The detailed algorithm is shown in Algorithm 2 in Appendix.

### 3.1. Lookahead Branch

LOOKAHEAD DECODING uses a fixed-sized 2D window for efficient $n$-gram generation. In contrast to the original Jacobi decoding, which only uses the history tokens from the last step (or equivalently, it generates 2-grams), LooKAHEAD DECODING generates many $n$-grams, with $n \geq 2$, in parallel by using the $n-1$ past steps' history tokens, effectively leveraging more information from the trajectory. The fixed-sized 2D window in the lookahead branch is characterized by two parameters: (1) $W$ defines the lookahead size into future token positions to conduct parallel decoding; (2) $N$ defines the lookback steps into the past Jacobi trajectory to retrieve $n$-grams. See Algorithm 2 for a detailed process.

An example of the lookahead branch with $W=5$ and $N=4$ is in Fig. 2 (b), in which we look back $N-1=3$ steps and look ahead 5 tokens for each step. The blue token with the digit 0 is the current step's $(t)$ input, and the orange, green, and red tokens were generated in previous lookahead branches at steps $t-3, t-2$, and $t-1$, respectively. The digit on each token shows its relative position to the current input (i.e., the blue one labeled as 0 ). In the present stage, we perform a modified Jacobi iteration to generate new tokens for all 5 positions, following the trajectory formed by the preceding 3 steps. Once generated, we collect and cache them in the $n$-gram pool $(n=4)$ - for instance, a 4 -gram consists of the orange token at position 1 , the green token at position 2, the red token at position 3, and a newly generated token.

The most outdated tokens in both dimensions (time and sequence) will be removed, and newly generated tokens will be appended to the lookahead branch to maintain a fixed window size for each step. For example, we will remove all orange and green tokens with position 1 in Fig. 2. We then form a new lookahead branch with green tokens with indices $2,3,4,5$, all red tokens, and all newly generated tokens for the next step.

### 3.2. Verification Branch

LOOKAHEAD DECODING preserves the output distribution via its verification branch. We first discuss how to verify in greedy sampling. Recall in speculative decoding: the verification is performed by sending the draft tokens to the LLM to get an output for each draft token, then progressively checking if the last token's corresponding output, generated by the target LLM, exactly matches the draft token itself (\$2). The verification branch in LOOKAHEAD DECODING resembles this process, despite verifying many draft n-gram candidates in parallel. In particular, We first look up from the $n$-gram pool for "promising" $n$-grams - by checking if a $n$-gram starts with a token that exactly matches the last token of the current ongoing sequence. We then use the LLM to verify all these $n$-grams in parallel, following a similar fashion as in speculative decoding. See Algorithm 3 in the Appendix for the detailed procedures.

We next discuss how to support more advanced sampling. Previous research (Miao et al., 2023) has developed efficient tree-based verification for speculative decoding with sampling support, where multiple draft sequences derived from a token tree can be verified in parallel. However, it does not apply to LOOKAHEAD DECODING as our verification works on disjoint $n$-grams instead of trees. We improve it by progressively verifying along the $n$-gram length and removing $n$-grams with mismatched prefixes. Besides, speculative decoding style verification requires the probability distribution where the draft token is sampled to update the probability distribution when the draft token is rejected. Because we store all $n$-grams in a pool instead of discarding them each step, we would need huge memory to store the probability distributions (each of vocabulary size) for the entire $n$-gram pool. The key to overcome this is to leverage the mechanism that the verification is indifferent to how draft tokens were sampled - different sampling methods (e.g., greedy sampling) only influence the acceptance rate but keep the output distribution. We can force greedy sampling at the $n$-gram generation (lookahead branch), in which the probability distribution degenerates into a one-hot vector. Hence we only need to store which token is selected. We elaborate the approach in Algorithm 4, prove its correctness in Appendix B, and verify its quality and speedups in $\S 5.3$.

It is expected to have an increasingly large $n$-gram cache hence a growing verification branch as decoding progresses. We set a cap of $G$ to limit the maximum number of promising candidates run in parallel in the verification branch to manage the verification cost. Empirically we suggest to set $G$ proportional to $W$ to balance generation and verification. In practice, we simply set $G=W$.

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-04.jpg?height=585&width=816&top_left_y=220&top_left_x=1061)

(a) Causal Mask (b) Lookahead Decoding Mask

Figure 2: (a) Causal mask for decoder models. (b) Attention mask for LoOKAHEAD DECODING with $W=5, N=4$, and $G=2$. Digits on tokens indicate relative positions.

### 3.3. Decode, Predict, and Verify in The Same Step

At execution, the lookahead and verification branches can be integrated into one decoding step to leverage parallel processing. This requires a designated attention mask, as shown in Fig. 2 (b). This attention mask is straightforwardly derived following the principle that each token is only visible to the tokens with a larger position index than itself (ยง2). For example, only the green token at position 5 and all orange tokens are visible to the red token 6 . The tokens in the lookahead branch are not visible to the tokens in the verification branch, and vice versa.

Integration with FlashAttention. FlashAttention (Dao et al., 2022; Dao, 2023) can vastly accelerate the training and inference of LLMs by saving memory I/O on the slow memory hierarchy. It forces a causal mask (e.g., Fig. 2 (a)) to avoid all token interactions outside a lower triangular scope, which is not suitable for LOOKAHEAD DECODING as we take a more subtle attention mask (e.g., Fig. 2 (b)) for different $W, N$, and $G$. To solve this, we hardcode LOoKAHEAD DECODING's attention pattern with adjustable $W$, $N$, and $G$ in FlashAttention. Applying FlashAttention to LOOKAHEAD DECODING brings about $20 \%$ end-to-end speedup compared to a straightforward implementation on top of native PyTorch in our experiments (\$5.2).

### 3.4. Lookahead Parallelism

LOOKAHEAD DECODING is easy to parallelize on multiple GPUs for both lookahead and verification branches. Parallelizing the lookahead branch is achieved by noting that the lookahead computation is composed of several disjoint branches. For example, the branch with green 1 and red 2 tokens does not have interaction with the branch with the tokens green 3 and red 4 in Fig. 2 (b). We can put these
disjoint branches onto different GPUs without introducing communication during the inference computation. Parallelizing the verification branch is done by assigning multiple $n$-gram candidates to different devices. Because the verification of each candidate, by design, is independent of others, this will not cause communication.

Fig. 3 shows an example of parallelizing the lookahead branch and verification branch in Fig. 2 (b) to four GPUs. This workload allocation will have the orange token $0,1,2,3$ and the input token 0 be redundantly placed and computed. However, it can essentially save communication volume during the whole forward pass. We only need to synchronize the generated tokens on each device after the forward pass. We can further scale the $W, N$, and $G$ with multiple GPUs' increased FLOPs to obtain a lower latency according to LOOKAHEAD DECODING's scalability (\$4).

We name this new parallelism as lookahead parallelism (LP). Unlike previous parallelism methods (including pipeline and tensor parallelisms) that shard the model parameters or states across different GPUs, LP maintains an entire copy of the model for each GPU (thus needing more memory) and allows distributing tokens to different GPUs. Hence, LP is advantageous in inference as it introduces near-zero communication per step while existing model parallelism methods (Narayanan et al., 2021; Shoeybi et al., 2019) involve a large communication overhead on the critical path of each decoding step.

## 4. Scaling Law of LOOKAHEAD DECODING

Since LOOKAHEAD DECODING introduces flexible parameters $W$ and $N$ associated with the cost of each parallel decoding step. This section investigates the scaling law between compute FLOPs and the theoretical speedup, and compares it to speculative decoding.

### 4.1. Estimating Speedup for Speculative Decoding

Speculative decoding uses the draft model to speculate one token sequence at each step. We represent the probability of each token in the sequence passing the verification of the LLM by $\beta$ (acceptance rate) and notate its expectation $E(\beta)=\alpha$. If we use the draft model to guess $\gamma$ tokens per step, the expectation of the number of accepted tokens is denoted as (Leviathan et al., 2023):

$$
\begin{equation*}
E(\# \text { tokens })=\frac{1-\alpha^{\gamma+1}}{1-\alpha} \tag{4}
\end{equation*}
$$

Instead of speculating one sequence every time, we would speculate $b$ sequences. We assume that $b$ sequences, each of $\gamma$ tokens, are sampled as each token will have the same acceptance rate of $\beta$. Under this setting, the expectation of the number of accepted tokens is denoted as follows:

$$
\begin{equation*}
E(\# \text { tokens })=(\gamma+1)-\sum_{i=1}^{\gamma}\left(1-\alpha^{i}\right)^{b} \tag{5}
\end{equation*}
$$

See derivations in Appendix C for Eq. 4 and Eq. 5. Note that when $b=1$, Eq. 5 falls back to Eq. 4 .

### 4.2. Estimating Speedup for LOOKAHEAD DECODING

We define the $\mathcal{S}=$ step compression ratio as the number of autoregressive steps divided by the number of LOOKAHEAD DECODING steps to generate the same length of the sequence. As the number of generated tokens equals the autoregressive steps, it can be denoted as:

$$
\begin{equation*}
\mathcal{S}=\frac{\# \text { generated tokens }}{\# \text { LOOKAHEAD DECODING steps }} \tag{6}
\end{equation*}
$$

LOOKAHEAD DECODING speculates $b$ sequences every time as in Eq. 5. In each step, we will search $n$-grams in the pool starting with the current input token and have at most $G$ speculations of length $N-1$. As we set $G=W$ (ยง3.2), we have $G=W=b$ and $N-1=\gamma$ using the notations in Eq. 5. In practice, we cannot expect each step to have equally good speculations (i.e., acceptance rate with $E(\beta)=\alpha)$. We assume that, on average, for every $f$ step, we have one good speculation with $E$ (\#tokens) tokens accepted, and for the other $f-1$ steps, we fall back to autoregressive decoding due to bad speculations. We use this $f$ to bridge $\mathcal{S}$ and $E(\#$ tokens $)$ per step as follows:

$$
\begin{equation*}
\mathcal{S}=(f-1+E(\# \text { tokens })) / f \tag{7}
\end{equation*}
$$

We can plot the curve indicated by our formulation with one specific setting as in Fig. 4 (b). We find that the trend of our empirical experiments (LLaMA-2-Chat-7B on MT-Bench with $G=W$ as in Fig. 4 (a)) align well with the formulation to some extent. From this formulation, we conclude that we can linearly reduce the number of decoding steps according to per-step $\log (b)$ given a large enough $\gamma$. In contrast to speculative decoding, LOOKAHEAD DECODING will not meet an upper bound indicated in Eq. 4 by simultaneously increasing $\gamma$ and $b$. This reveals the scaling law of LOOKAHEAD DECODING to linearly reduce decoding steps according to per-step $\log$ (FLOPs) given a large enough $N$,
since per-step FLOPs is roughly proportional to the number of input tokens (i.e., $(W+G) *(N-1)$ ). The scaling law also suggests LOOKAHEAD DECODING's strong scaling to multiple GPUs, in which we can obtain an even greater per-token latency reduction by using more FLOPs, which is advantageous for latency-sensitive tasks.

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-06.jpg?height=429&width=846&top_left_y=520&top_left_x=173)

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-06.jpg?height=417&width=420&top_left_y=526&top_left_x=186)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-06.jpg?height=412&width=405&top_left_y=523&top_left_x=597)

(b)
Figure 4: (a) Relation of $W, N, G$ and $\mathcal{S}$ for LLaMA-2Chat-7B on MT-Bench. (b) When we assume a setting with $\alpha=0.425$ and $f=3.106$, the trend of our formulation.

## 5. Evaluation Results

Model and testbed. We used various versions of the LLaMA-2 (Touvron et al., 2023b) and CodeLlama (Roziere et al., 2023) models, including the 7B, 13B, 34B, and 70B sizes, on two GPU setups $S 1$ and $S 2$. $S 1$ is equipped with NVIDIA A100 GPUs with 80GB of memory. On $S 1$, the 7B, $13 \mathrm{~B}$, and 34B models are deployed on a single A100, while the 70B model utilizes 2 A100s with pipeline parallelism supported by Accelerate (Gugger et al., 2022). $S 2$ is a DGX machine with 8 NVIDIA A100 GPUs with 40GB memory and NVLink. All models serve with FP16 precision and batch of 1 if not specified (Cai et al., 2024; He et al., 2023).

Datasets. We benchmarked LoOKAHEAD DECODING's performance across a broad spectrum of datasets and tasks. MT-Bench (Zheng et al., 2023) is a diverse set of multi-turn questions with many unique tokens. GSM8K (Cobbe et al., 2021) contains a set of math questions, in which we use the first $1 \mathrm{k}$ questions. HumanEval (Chen et al., 2021) covers both code completion and infilling tasks. We also test on MBPP (Austin et al., 2021) dataset for instruction-based code generation, and on ClassEval (Du et al., 2023) for class-level code completion. To control generation length in code generation tasks, we set the maximum sequence length to 512 and 2,048 on HumanEval and ClassEval, respectively, aligned with prior setups (Ben Allal et al., 2022; Du et al., 2023). Tab. 1 lists detailed settings. In addition, we validate the effectiveness of sampling ( $\$ 3.2$ ) on XSum (Narayan et al., 2018) and CNN/Daily Mail (See et al., 2017) datasets.

Baseline Settings. Our primary baseline is HuggingFace's implementation of greedy search (Wolf et al., 2020). Additionally, we employ FlashAttention (Dao et al., 2022;
Table 1: Experimental settings for $\S 5.1$ and $\S 5.2$.

| SERVER | PARALLEL. | MODEL | MODEL SIZE | DATASET |
| :---: | :---: | :---: | :---: | :---: |
| $S 1$ | W/O LP | LLAMA-2-CHAT | 7B, 13B, 70B | MT-BENCH |
|  |  | CODELLAMA | 7B, 13B, 34B | HUMANEVAL |
|  |  | CODELLAMA-INST | 7B, 13B, 34B | MBPP, GSM8K |
| 32 | W/LP | LLAMA-2-CHAT | 7B, 13B | MT-BENCH |
|  |  | CODELLAMA | 7B, 13B | HUMANEVAL |
|  |  | CODELLAMA-PYTHON | 7B, 13B | CLASSEVAL |

Dao, 2023) as a stronger baseline to assess the performance of FlashAttention empowered LOOKAHEAD DECODING. In distributed settings, we evaluate LP against TP (supported by deepspeed (Aminabadi et al., 2022)) and PP (supported by accelerate (Gugger et al., 2022)). We measure the throughput of single batch inference against these baseline settings (Cai et al., 2024; He et al., 2023).

### 5.1. End-to-end Performance

Fig. 5 shows the end-to-end performance of LOOKAHEAD DECODING when compared with HuggingFace's implementation of greedy search on S1. The used tasks and models are shown in Tab. 1. Across various datasets, LOOKAHEAD DECODING demonstrates a $1.5 \mathrm{x}-2.3 \mathrm{x}$ speedup. Generally, our method exhibits better performance in code completion tasks (e.g., $2.3 \mathrm{x}$ ), given the higher occurrence of repetitive tokens during code completions, making predictions easier. Besides, smaller models also exhibit a higher speedup when compared to larger models. This is because LOOKAHEAD DECODING trades per-step FLOPs with a step compression ratio (ยง4). A larger model requires more FLOPs and quickly hits the GPU FLOPs cap compared to a smaller model. So, it shows a lower ability to compress decoding steps given the same GPU setting.

### 5.2. Performance with LP and FlashAttention

We evaluated the performance of LOOKAHEAD DECODING with LP and FlashAttention augmentation on $S 2$ with greedy search. The used tasks and models are shown in Tab. 1. The results for the 7B and 13B models are in Fig. 6 and Fig. 7, respectively. FlashAttention speeds up the PyTorch implementation of LOOKAHEAD DECODING by $20 \%$. Notably, FlashAttention-integrated LOOKAHEAD DECODING shows $1.8 \mathrm{x}$ speedups for the 7B model on MT-Bench compared with autoregressive decoding with FlashAttention (i.e., 1.9x vs $1.07 x$ in Fig. 6). We did a strong scaling of the workloads to multiple GPUs for distributed settings (i.e., increasing GPUs but not increasing workloads). The multiple GPU settings of both TP (w/ DeepSpeed) and PP (w/ Accelerate) bring slowdowns (i.e., $0.75 \mathrm{x}-0.82 \mathrm{x}$ ). The results echos DeepSpeed's documentation (dee, 2023). However, with LOOKAHEAD DECODING, we can further utilize the FLOPs of multiple GPUs to reduce the inference latency (e.g., $4 x$ on ClassEval).
![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-07.jpg?height=246&width=1720&top_left_y=218&top_left_x=172)

Figure 5: Throughput of LoOKAHEAD DeCODING on various dataset without FlashAttention and distributed serving
![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-07.jpg?height=864&width=820&top_left_y=560&top_left_x=186)

Figure 6: Throughput of LookaHEad DeCoding with multiple GPUs and FlashAttention for 7B models

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-07.jpg?height=876&width=824&top_left_y=1535&top_left_x=184)

1 GPU w/o flash TP w/o flash LP w/o flash PP w/o flash 1 GPU w/ flash TP w/ flash LP w/ flash PP w/ flash
![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-07.jpg?height=784&width=812&top_left_y=1622&top_left_x=192)

Figure 7: Throughput of LookaHEad Decoding with multiple GPUs and FlashAttention for 13B models
Table 2: Sampling with LookaHEAd Decoding on CNN/Daily Mail and XSum. A temperature (Temp.) of 0.0 equals greedy search. "AR." is autoregressive and "LA." is LOOKAHEAD DECODING. Rouge scores, speedups against autoregressive, and compression ratio $(\mathcal{S})$ are reported.

| ATA | m | ETH | OUG | ROUG |  |  | $\mathcal{S}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| CNN. | 1.0 | AR. | 36.55 | 13.20 | 22.68 | $1.00 x$ | $1.00 x$ |
|  |  | LA. | 36 |  | 22.7 | $1.46 \mathrm{x}$ | $1.64 x$ |
|  | 0.0 | AR. | 37.79 | 14.59 | 23.96 | $1.00 x$ | $1.00 x$ |
|  |  | LA. | 37.79 | 14.59 | 23.96 | $1.57 x$ | $1.72 x$ |
| Xsum | 1.0 | A | 19.15 |  | 1284 | $1.00 \mathrm{x}$ | 1.00 |
|  |  | L. | 19.2 | 4.5 | 12.87 | $1.50 x$ | $1.67 x$ |
|  | 0.0 | $\mathrm{AI}$ | 19. | 4.' | 15 | $1.00 \mathrm{x}$ | $1.00 x$ |
|  |  | LA. | 19.39 | 4.79 | 13.06 | $1.60 \mathrm{x}$ | $1.77 \mathrm{x}$ |

Table 3: Compare the effectiveness of both lookahead and verification branch on MT-Bench on A100. FlashAttention is activated. We show the speedups against autoregressive decoding and the compression ratio $(\mathcal{S})$.

| TAG | SEtTing (N, W, G) | Prompt as Ref. | SpeEduPS | $\mathcal{S}$ |
| :---: | :---: | :---: | :---: | :---: |
| (1) | AUtoreGRessive | $\boldsymbol{x}$ | $1.00 \mathrm{x}$ | 1.00 |
| (2) | Prompt LoOKUP | $\boldsymbol{\checkmark}$ | $1.44 \mathrm{x}$ | 1.55 |
| (3) | $(10,1,3)$ | $\boldsymbol{\checkmark}$ | $1.36 \mathrm{x}$ | 1.45 |
| (4) | $(5,1,10)$ | $\mathbf{\checkmark}$ | $1.36 \mathrm{x}$ | 1.51 |
| (5) | $(5,1,30)$ | $\boldsymbol{x}$ | $1.04 \mathrm{x}$ | 1.12 |
| (6) | $(5,1,30)$ | $\mathbf{\checkmark}$ | $1.46 \mathrm{x}$ | 1.59 |
| (7) | $(5,30,1)$ | $\mathbf{x}$ | $1.61 \mathrm{x}$ | 1.79 |
| (8 | $(5,15,15)$ | $\mathbf{x}$ | $1.78 \mathrm{x}$ | 1.96 |
| (9) | $(5,15,15)$ | $\boldsymbol{\checkmark}$ | $\mathbf{1 . 8 8 x}$ | $\mathbf{2 . 0 5}$ |

### 5.3. Generation Quality of LOOKAHEAd DECODING

We assess the generation quality of LOOKAHEAD DECODING on LLaMA-2-7B-Chat model with the prompts in Appendix D on summarization datasets (Chen et al., 2023; Leviathan et al., 2023) in Tab. 2. Whether the sampling is activated, LookAHEAD DECODING can reserve the output distribution quality, which is evaluated in rouge-1, rouge2, and rouge-L (Lin, 2004), while achieving 1.46x-1.60x speedups compared with autoregressive decoding. Using sampling gives smaller speedups as the acceptance ratio is lower according to the sampling verification algorithm 4, which aligns with the results in the previous research (Chen et al., 2023; Leviathan et al., 2023). We further verify that using greedy sampling and advanced integrations will not change the generation quality in Appendix E.

### 5.4. Ablation Study

In this section, we study the importance of the lookahead and verification branch in achieving a high speedup. We experiment on LLaMA-2-7B-Chat and MT-Bench on S1 with various settings. The results are shown in Tab. 3.

We ablate the importance of lookahead branch by comparing the performance of using a lookahead branch to the recent methods of using prompts as reference (Yang et al., 2023; Saxena, 2023). This comparison assumes that LOOKAHEAD DECODing does not use the prompt to build the n-gram pool. We use the implementation in transformers v4.37 of prompt lookup as a baseline (2), with prompt_lookup_num_tokens=10). We also use prompt to build n-gram pool to augment LOOKAHEAD DECODING (3)(4)(6). The results show that although using a minimal lookahead branch ( $W=1$ ) with various $N, G$ settings (3)(4)(5)(6) can obtain a decent speedup on MT-Bench, it is still not as good as using balanced branches (8). We can find that prompt lookup can surpass prompt as reference implementation in LOOKAHEAD DECODING. This is because our method checks if $n$-gram starts with one token that exactly matches the last generated token while prompt lookup in transformers v4.37 checks several starting tokens for a better speculation.

We ablate the importance of verification branch by reporting the speedup of using a tiny verification branch and a large lookahead branch (7), $G=1$ ). It shows lower performance due to lower potential in accepting speculations compared with a balanced branches (8).

Besides, our evaluation shows that using prompt as reference can further boost LOOKAHEAD DECODING (8) and (9)). We have integrated them in our implementation.

### 5.5. Discussion and Limitation

The main limitation of LOOKAHEAD DECODING is that it requires extra computations. Our experimental results show that on A100, the config-

Table 4: Good Config. of uration in Tab. 4 works LOOKAHEAD DECODING on A100 GPUs with $G=W$.

| MODEl | WINDOW Size $(W)$ | N-GRAM Size $(N)$ |
| :---: | :---: | :---: |
| 7B | 15 | 5 |
| 13B | 10 | 5 |
| 34B | 7 | 5 |

near optimally in most cases for single batch serving. Because the per-step FLOPs are roughly proportional to the number of per-step input tokens, which is $(W+G) *(N-1)$. If we ignore the attention cost's increase with sequence length, the 7B, 13B, and 34B models require 120x, 80x, and 56x extra FLOPs per step, respectively. Since the LLM decoding is memory bandwidth-bound rather than computebound, these extra FLOPs only turn into a limited wall-clock slowdown for each step.

Given this, LOOKAHEAD DECODING needs large surplus

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-08.jpg?height=220&width=829&top_left_y=215&top_left_x=1057)

Figure 8: Compression $\operatorname{ratio}(\mathcal{S})$ and speedups of LooKAHEAD DECODING on RTX 3090 and A100 with $N=5$, all with FlashAttention. The blue and orange curves of $\mathcal{S}$ overlap as the device does not affect the ratio.

FLOPs to obtain high speedups. Running in compute-bound environments (e.g., serving with a large batch size) may cause slowdowns. Another example is shown in Fig. 8, where lower speedup is observed when the GPU's cap FLOPs is smaller (e.g., on RTX 3090 GPUs).

Based on $\S 4$, we need to exponentially increase the per-step FLOPs to obtain a linear reduction in decoding steps. Hence, the setting in Tab. 4 faces a diminishing return. However, when FLOPs are not rich, we see that a gentle speedup (e.g., $30 \%$ on RTX 3090 and $>50 \%$ on A100) on MT-Bench easily achievable, as in Fig. 8, which is a free lunch that requires no extra model, training, or changing the output distribution.

## 6. Related Work

Speculative decoding (Chen et al., 2023; Leviathan et al., 2023) pioneer in speedup autoregressive decoding with a draft model. Different methods for obtaining speculations are researched. Specinfer (Miao et al., 2023) uses many draft models obtained from distillation, quantization, and pruning to conduct speculations together. Medusa (Cai et al., 2024), OSD (Liu et al., 2023), and EAGLE (Li et al., 2023) use training to obtain a draft model. REST (He et al., 2023) uses the finetuning dataset itself as a datastore to lookup speculations, while other works (Yang et al., 2023; Saxena, 2023) uses prompt as a reference for speculations. Different from these methods, LOOKAHEAD DECODING uses LLM's parallel generation ability for speculations. Sampling methods are also researched. Specinfer maintains output distribution by a tree-based sampling algorithm. Medusa uses a typical acceptance scheme to accelerate when the temperature is large but does not persist on an exact output distribution. LookaHEAD DeCoding follows Specinfer to maintain output distribution but with multiple disjoint $n$-grams.

## 7. Conclusion

In this paper, we present LOOKAHEAD DECODING to parallelize the autoregressive decoding of LLMs without changing the output distribution. It shows notable speedup without a draft model and can linearly decrease the decoding steps with exponential investment in per-step FLOPs.

## References

Automatic tensor parallelism for huggingface models, 2023. URL https://www.deepspeed.ai/tutoria ls/automatic-tensor-parallelism.

Aminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., et al. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-15. IEEE, 2022.

Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and Sutton, C. Program synthesis with large language models, 2021.

Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

Ben Allal, L., Muennighoff, N., Kumar Umapathi, L., Lipkin, B., and von Werra, L. A framework for the evaluation of code generation models. https://github.com /bigcode-project/bigcode-evaluation-h arness, 2022.

Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., and Dao, T. Medusa: Simple llm inference acceleration framework with multiple decoding heads, 2024.

Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.

Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021.

Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.

Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Rรฉ, C. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.

Du, X., Liu, M., Wang, K., Wang, H., Liu, J., Chen, Y., Feng, J., Sha, C., Peng, X., and Lou, Y. Classeval: A manuallycrafted benchmark for evaluating llms on class-level code generation, 2023.

Gugger, S., Debut, L., Wolf, T., Schmid, P., Mueller, Z., Mangrulkar, S., Sun, M., and Bossan, B. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface / accelerate, 2022.

He, Z., Zhong, Z., Cai, T., Lee, J. D., and He, D. Rest: Retrieval-based speculative decoding. arXiv preprint arXiv:2311.08252, 2023.

Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration, 2020.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Kool, W., van Hoof, H., and Welling, M. Ancestral gumbeltop-k sampling for sampling without replacement. Journal of Machine Learning Research, 21(47):1-36, 2020. URL http:// jmlr.org/papers/v21/19-985 . html.

Leviathan, Y., Kalman, M., and Matias, Y. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023.

Li, Y., Zhang, C., and Zhang, H. Eagle: Lossless acceleration of llm decoding by feature extrapolation, December 2023. URL https://sites.google.com/vie w/eagle-llm.

Lin, C.-Y. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: / / www. ac lweb.org/anthology/W04-1013.

Liu, X., Hu, L., Bailis, P., Stoica, I., Deng, Z., Cheung, A., and Zhang, H. Online speculative decoding, 2023.

Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Wong, R. Y. Y., Zhu, A., Yang, L., Shi, X., Shi, C., Chen, Z.,

Arfeen, D., Abhyankar, R., and Jia, Z. Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification, 2023.

Narayan, S., Cohen, S. B., and Lapata, M. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018.

Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V. A., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., and Zaharia, M. Efficient large-scale language model training on gpu clusters using megatron-lm, 2021.

OpenAI. Gpt-4 technical report, 2023.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.

Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

Ruan, J. T., Sabir, F., and Chopra, P. Best prompting practices for using the llama 2 chat llm through amazon sagemaker jumpstart, November 2023. URL https: / /aws.amazon.com/cn/blogs/machine-lea rning/best-prompting-practices-for-u sing-the-llama-2-chat-llm-through-a mazon-sagemaker-jumpstart/.

Santilli, A., Severino, S., Postolache, E., Maiorca, V., Mancusi, M., Marin, R., and Rodola, E. Accelerating transformer inference for translation via parallel decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12336-12355, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https : / /aclanthology.org/2023.acl-long. 689 .

Saxena, A. Prompt lookup decoding, November 2023. URL https: / github. com/apoorvumang/promp t-lookup-decoding/.

See, A., Liu, P. J., and Manning, C. D. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1073-1083, 2017.
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.

Song, Y., Meng, C., Liao, R., and Ermon, S. Accelerating feedforward computation via parallel nonlinear equation solving, 2021.

Stern, M., Shazeer, N., and Uszkoreit, J. Blockwise parallel decoding for deep autoregressive models, 2018.

Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziรจre, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023.

Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State-ofthe-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. URL https: / /www. aclweb.org/a nthology/2020.emnlp-demos. 6 .

Yang, N., Ge, T., Wang, L., Jiao, B., Jiang, D., Yang, L., Majumder, R., and Wei, F. Inference with reference: Lossless acceleration of large language models, 2023.

Zhang, B. and Sennrich, R. Root mean square layer normalization, 2019.

Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
