# PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference 

Dongjie Yang ${ }^{1, *}$, Xiaodong Han ${ }^{2}$, Yan Gao ${ }^{2}$, Yao Hu ${ }^{2}$, Shilin Zhang ${ }^{3}$, Hai Zhao ${ }^{1, *, \dagger}$<br>${ }^{1}$ Shanghai Jiao Tong University, ${ }^{2}$ Xiaohongshu Inc.,<br>${ }^{3}$ South China University of Technology<br>${ }^{1}\{$ djyang.tony@,zhaohai@cs.\}sjtu.edu.cn,<br>${ }^{2}\{$ shuweng, yadun, xiaohou\}@xiaohongshu.com


#### Abstract

Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for realtime applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed $\mathrm{KV}$ cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves $2.2 \mathrm{x}$ throughput compared to Accelerate with over $54 \%$ GPU memory reduction in $\mathrm{KV}$ cache. Our code is available in https://github.com/mutonix/ pyramidinfer.


## 1 Introduction

Large Language Models (LLMs) (OpenAI, 2023; Anthropic, 2023; Jiang et al., 2023) like GPT4 have demonstrated the unprecedented ability of remarkable comprehension in human languages. However, these large models meet up with a substantial challenge of immense GPU memory[^0]

usage in the inference, due to the model and computational complexity. This hinders deploying LLMs at scale to meet the thousands of demands for chatting with chatbots.

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-01.jpg?height=509&width=688&top_left_y=999&top_left_x=1095)

Figure 1: Inference in the prefill phase: all models of different sizes have the prompts of $64 \times 2 \mathrm{k}$. LLM consumes huge GPU memory in the KV cache compared to the small model. PyramidInfer can reduce over $54 \%$ GPU memory usage in the KV cache while having more than $2 x$ throughput.

Different from training, models in the inference do not need to record the optimizer states, activations, or gradients. As LLMs are mostly Transformer-based auto-regressive models, the GPU memory usage mainly consists of two parts: model parameters and KV cache. KV cache presents the keys and values previously computed in the attention. We store the KV cache in the GPU memory and reuse it in future generations to avoid re-computation. The KV cache mechanism has been widely used to improve the inference speed (Touvron et al., 2023; Zhang et al., 2022). However, the KV cache consumes huge GPU memory, especially for LLMs. For example, in Figure 1, for a model with 7 billion parameters, the parameters only consume $14 \mathrm{~GB}$ of memory but the $\mathrm{KV}$ cache requires around $72 \mathrm{~GB}$. The KV cache has the potential to consume memory several
times the size of the model. It demonstrates a great challenge that the throughput of LLM inference is constrained by how much data (KV cache) we can put in the GPU besides the model.

We break down LLM inference into two phases: prefill phase and generation phase (Brown et al., 2020; Radford et al., 2019). In the prefill phase, the prompt is computed in parallel to generate the first token, and the initial KV cache is pre-filled. In the generation phase, the model decodes the next token one by one and appends the keys and values of the newly decoded token to the old KV cache. Recent studies (Zhang et al., 2023; Liu et al., 2023; Ge et al., 2023) compress the KV cache to reduce GPU memory usage. However, as shown in Figure 2, they all only reduce the KV cache that has been already computed rather than reducing the $\mathrm{KV}$ cache to be computed. They have to prefill the initial KV cache before they can start to compress, which neglects the great GPU memory consumption of computing the initial KV cache, especially for longer prompts and larger models. If the model can not process the prompt in the prefill phase, these methods are no longer applicable as their compression starts in the generation phase. In this paper, we focus on how to further compress the $\mathrm{KV}$ cache in the prefill phase besides the generation phase. We give out our findings and then propose our method PyramidInfer inspired by these findings.

During the training, all input tokens predict the tokens next to themselves in an one-to-one teacher-forcing way (Lamb et al., 2016). During the inference, the tokens except for the last token no longer need to predict the next tokens but they still record this redundant information in keys and values. We call this Inference Context Redundancy (ICR) hypothesis. It inspires us to compress the KV cache by only computing the keys and values that record the context information.

Another challenge arises as the initial KV cache is reused multiple times for generating future tokens, necessitating careful retention of context information during compression. Inspired by the work (Liu et al., 2023), we further explore what parts of the KV cache are always crucial for future generations. We observe that queries of recent tokens closer to the last token are more consistent in attending to the same context keys and values, denoted as the Pivotal Context $(\mathrm{PvC})$. We call this phenomenon as Recent Attention Consistency (RAC). The consistency of attention weights in recent tokens indicates that we can leverage it as the oracle to select the crucial KV cache for future generations in advance.

Based on our observations, we propose the PyramidInfer, an effective method of reducing the KV cache both in the prefill and generation phase by layer-wise selecting the PvCs. In PyramidInfer, the $\mathrm{PvCs}$ are gradually reduced as the layers get deeper where the KV cache is like a pyramid. We showcase the capability of PyramidInfer on a wide range of tasks using OpenCompass (Contributors, 2023) on models of different types and sizes. The results show that PyramidInfer has higher throughput than the full cache method Accelerate and Deepspeed by $2.2 \mathrm{x}$ and $1.4 \mathrm{x}$, KV cache compression method $\mathrm{H}_{2} \mathrm{O}$ by $2.4 \mathrm{x}$ with over $54 \%$ less GPU memory in KV cache.

## 2 Related Work

Due to the increasing demands for chatting with chatbots, efficient strategies are required to process thousands of queries to maximize the throughput. The fundamental way to improve the throughput is to put more data (larger batch) into the GPU memory to utilize the GPU parallelism better.

Inference Parallelism One way is to enlarge the GPU memory. We can borrow the techniques used in training to accelerate the inference, e.g., pipeline parallelism (Huang et al., 2019), KV cache offload (Sheng et al., 2023), etc. These methods leverage multiple GPUs or even RAM to make up bigger space for input data.

KV Cache Reduction However, if we have limited GPU memory, another way is to reduce the KV cache. For optimization in the CUDA, FlashAttention 2 (Dao, 2023) reduces the number of reads/writes between GPU HBM and GPU onchip SRAM. PagedAttention (Kwon et al., 2023) borrows the virtual memory techniques to achieve near-zero waste in KV cache memory.

Besides CUDA methods, we can optimize the KV cache from the model itself. From Figure 2, StreamingLLM (Xiao et al., 2023) reserves the recent context to enable unlimited input by sacrificing memorization of the history. Other methods like $\mathrm{H}_{2} \mathrm{O}$ (Zhang et al., 2023) and Scissorhands (Liu et al., 2023) leverage the attention to compress the KV cache. However, they treat the compression of different layers as the same thing and can not compress in the prefill phase. Our method
(a) StreamingLLM

Computed KV cache in prefill phase

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-03.jpg?height=342&width=526&top_left_y=363&top_left_x=274)

Drop all the intermediate KV cache thus forgetting the context and only remembering the recent tokens. (b) $\mathrm{H}_{2} \mathrm{O}$ / Scissorhands

Computed $\mathrm{KV}$ cache in generation phase (c) PyramidInfer (ours)

Drop the keys and values from current KV cache

Figure 2: Comparison between PyramidInfer and other methods: (a) StreamingLLM only reserves the first and recent tokens thus losing memorization of the previous context. (b) $\mathrm{H}_{2} \mathrm{O} / \mathrm{Scissorhands} \mathrm{compress} \mathrm{the} \mathrm{KV} \mathrm{cache}$ without difference for all the layers. They suffer great information loss by compressing too much in the shallow layers. (c) Different from the above methods that can only compress after the KV cache has been computed, PyramidInfer can compress the KV cache in the prefill phase. PyramidInfer only computes crucial keys and values to do inference thus reducing more GPU memory and bringing higher throughput.

PyramidInfer takes the difference in layers into account and realizes the compression in both the prefill and generation phases, thus better reducing the $\mathrm{KV}$ cache while maintaining the generation quality.

## 3 Observation and Insight

We verify the hypotheses of Inference Context Redundancy and Recent Attention Consistency, which inspire us to design the method PyramidInfer.

### 3.1 Inference Context Redundancy

Different from teacher-forcing in the training, only the last token has to predict the next token in the inference. We suppose there exist keys and values of the context that record the redundant information to predict the next token in the training but are not useful for inference. We call this the Inference Context Redundancy (ICR) hypothesis.

### 3.1.1 Pivotal Context

To verify the hypothesis, we design an experiment based on 40-layer LLaMA 2-13B to find out if this redundancy exists in the $\mathrm{KV}$ cache. In this experiment, we only reserve a proportion of keys and values of certain layers while other layers remain fixed and see how the perplexity of model output will change. This selected proportion consists of the important keys and values with the top- $p$ attention weights, denoted as the Pivotal Context $(\mathrm{PvC})$.
As shown in Figure 3, we show that, for most of the layers, as the retention ratio of $\mathrm{PvC}$ decreases, the perplexity of the output will increase. However, as the layer becomes deeper (larger index), we find that the influence of shorter $\mathrm{PvC}$ tends to be smaller. For example, after Layer 27, the perplexity remains stable even with $80 \%$ keys and values are evicted. In Figure 4, we compute the standard deviations across the retention ratios of all the layers and observe they obey a power law distribution. It indicates most of the keys and values should be retained as the layers are shallow and the redundancy in the KV cache sharply increases as the layers become deeper. This growing redundancy guides us to minimize the KV cache while maximizing the performance.

### 3.1.2 Discussion

How does the model gather information to predict the next token? Generating the next token can be considered as a process that the last token gathers the information from the context based on the attention weights. In Figure 3, we observe from the view of the last token. In the shallow layer, the information in the context is distributed in most of the tokens in the context. As the layer goes deeper, only limited keys and values contribute to the next token prediction.

The inference process differs from training because all the input tokens predict the next tokens. At this time, keys and values store two kinds of information: 1) the information to predict what
![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-04.jpg?height=758&width=1494&top_left_y=220&top_left_x=286)

Figure 3: For each layer, we reserve the keys and values with top-p attention weights $(\mathrm{PvC})$ while other layers maintain the full length. We calculate the average perplexity across different retention ratios $p$.

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-04.jpg?height=434&width=600&top_left_y=1139&top_left_x=317)

Figure 4: The perplexity standard deviations when only PvCs are reserved at each layer.

the token is next to it; 2 ) the context information for future tokens to leverage. So far, we have verified that $\mathrm{PvCs}$ are the crucial keys and values that are useful for inference. On the other hand, we want to verify the non- $\mathrm{PvC}$ that may play a more important role in teacher-forcing prediction instead of being the context. As non-PvCs are trivial in PyramidInfer, we discuss it in the Appendix 3.2.2.

### 3.2 Recent Attention Consistency

In the verification of ICR, we use the attention weights to find PvCs. However, in an attention layer, there are several attention weights for one token $x_{i}$ as every subsequent token $x_{t>i}$ will attend to it. Which attention weights should we choose as the metric to find $\mathrm{PvCs}$ ? Intuitively, the optimal weights must be from the last token $x_{n}$. However, the PvCs selected by these weights are suitable for predicting $x_{n+1}$ but not always suitable for future tokens $x_{t>n+1}$. Our goal is to find if there exists shared $\mathrm{PvCs}$ that can be used as a general oracle to predict several future tokens $x_{t>n+1}$ besides the last token $x_{n+1}$.

### 3.2.1 PvC Consistency

We convert this goal to finding if there exist keys and values that are frequently attended by subsequent tokens. First of all, we define a relative distance of how far the context token $x_{i}$ is relative to the last token $x_{n}$, which is called the Recent Ratio $d=(n-i) / n \times 100 \%$. We divide the input sequence into two parts where we denote the tokens with $0<d<30 \%$ as the recent sequence $S_{r}$ and $d \geq 30 \%$ as the context sequence $S_{c}$. We only compute the attention weights of $S_{r}$ to $S_{c}$ to check if there are tokens in the $S_{c}$ that are always attended by the tokens in the $S_{r}$. For each token in $S_{r}$ of each layer, we select the keys and values with top$80 \%$ attention weights as their PvCs. We set the keys and values with top- $80 \%$ attention weights of the last token $(d=0)$ as the $\mathrm{PvC}$ selection baseline.

After the setup, we want to measure how much the overlap will be that the $\mathrm{PvCs}$ of recent tokens are consistent with the $\mathrm{PvC}$ of the last token. If there is overlap, we can infer the intersection should be the shared $\mathrm{PvC}$ where many subsequent tokens are consistently interested. Thus for each layer $l$, we calculate the overlap ratio $C$ of $\mathrm{PvCs}$ as follows:

$$
\begin{equation*}
C_{l, i}=\frac{\left|\left\{x \mid x \in \mathbf{P v}_{l, i}\right\} \cap\left\{x \mid x \in \mathbf{P v}_{l, l a s t}\right\}\right|}{\left|\left\{x \mid x \in \mathbf{P v}_{l, l a s t}\right\}\right|} \tag{1}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-05.jpg?height=762&width=1452&top_left_y=224&top_left_x=310)

(a) Separate PvC overlap ratios of recent tokens.

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-05.jpg?height=314&width=1426&top_left_y=600&top_left_x=318)

(b) Ensemble $\mathrm{PvC}$ overlap ratios of recent tokens.

Figure 5: $\mathrm{PvC}$ overlap ratio heatmap.

From the results in Figure 5a, the recent tokens in $S_{r}$ have an average $86 \%$ overlap with the $\mathrm{PvC}$ selected by the last token. It indicates there exists shared PvCs that are always interested in by the subsequent tokens. However, it is not enough to be the oracle to predict future tokens. For example, if we want to predict the $x_{n+1}$ token using only the $\mathrm{PvC}$ extracted from the token with $d=25 \%$, we only have about $83 \% \mathrm{PvC}$ contributes to the prediction, which suffers a great context information loss.

Fortunately, the $\mathrm{PvC}$ selections from recent tokens have high consistency and we can integrate multiple tokens to select the shared ones. In Figure $5 \mathrm{~b}$, we integrate the attention weights by averaging weights of subsequent $[d, d+10 \%]$ tokens as the ensemble weights of the token with $d$. We select the keys and values with top- $80 \%$ ensemble weights as PvCs. We observe that the average $\mathrm{PvC}$ overlap ratios increase by a large margin to approximately $93 \%$. The overlap ratios have hardly any drop with $d=20 \%$, which indicates we can leverage the PvCs selected from ensemble tokens with $d=20 \%$ as an oracle to predict the $x_{n+1}$ which is $20 \%$ ahead.

### 3.2.2 Discussion

Why do the deeper layers tend to have lower $\mathbf{P v C}$ overlap ratios? If we check overlap ratios along the layer axis, we find that only shallow layers have relatively high ratios. It is because in deeper layers there is context redundancy: Only a small number of keys and values have high weights that are always selected as $\mathrm{PvCs}$; The others have similar low weights so they are not always selected, which results in lower overlap ratios. This phenomenon is consistent with the power law distribution observed in ICR, which is further discussed later.

Context information is mostly stored in the shared PvCs. In Figure 5b, the consistent PvC overlap ratios from small $d$ to large $d$ show that wherever recent tokens are, they only leverage nearly the same number of keys and values in the context. These keys and values, also known as shared PvCs, store most of the context information.

The Association between ICR and RAC In Section 3.2.2, we mention the phenomenon that deeper layers have lower $\mathrm{PvC}$ overlap ratios is consistent with the power law distribution observed in Figure 4. This is because, as we observe alone the layer index of the heatmap, we find that the color quickly deepens by a large gap where the depth change is approximate to the power law distribution.

The insight behind these two power law distributions is the same. The high redundancy in deeper layers indicates that most of the keys and values are useless for inference. These non-PvCs all have similarly low attention weights, resulting in limited influence on the perplexity and few opportunities to be selected as PvCs.

Further Verification of ICR about the Role of Non-PvCs To complete the verification of ICR, we have to verify the non-PvCs are redundant

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-06.jpg?height=774&width=702&top_left_y=227&top_left_x=240)

Figure 6: The overview of the PyramidInfer.

```
Algorithm 1 One forward pass in PyramidInfer
Input: $\mathrm{KV}$ cache $K V$, recent window length $L, \min \mathrm{PvC}$
length $\mathbf{N}=\left\{N_{0}, \ldots, N_{l}, \ldots\right\}$
Output: updated $\mathrm{KV}$ cache $K V$
    for layer $l \in$ layers do
        if $K V$ is not None then
            $K V=\operatorname{cat}\left(\left[\mathbf{P v C}_{\text {past }}, K V\right]\right)$
        $\mathcal{A} \leftarrow$ compute attention weights of $K V$
        $\mathcal{A}_{e} \leftarrow$ weighted_avg $(\mathcal{A}[-L:,:-L], \operatorname{dim}=-2)$
        if len $(K V)>N_{l}$ then
            TopP_index $\leftarrow \operatorname{TopP}\left(\mathcal{A}_{e}, p=p\right)$
            $\mathbf{P v C} \leftarrow \operatorname{Gather}(K V$, index $=$ TopP_index $)$
        $K V \leftarrow \mathbf{P v} \mathbf{C}$
        Reduce $p$ by multiplying a decay ratio
    return $K V$
```

because they carry the information of predicting the tokens next to themselves instead of context information. In Figure 7, to better illustrate, we divide the keys and values of one layer into two main parts, PvCs and non-PvCs. For the PvCs, we further divide them into shared $\mathrm{PvCs}$ and nonshared PvCs.

| Shared PvCs (overlapped) | Non-shared <br> PvCs | Non-PvCs |
| :---: | :---: | :---: |

Figure 7: The composition of the keys and values of one layer.

In Figure 5a, we demonstrate that there is an $87 \%$ overlap between tokens and the last token in terms of $\mathrm{PvC}$, as denoted as shared $\mathrm{PvC}$. We first identify the role of the remaining $13 \%$ of keys and values where these non-shared $\mathrm{PvCs}$ are not used in PyramidInfer. The non-shared $\mathrm{PvCs}$ are also assigned high attention weights by the current token, which means they are useful for predicting the token next to the current token. It is interesting to see what these non-shared $\mathrm{PvCs}$ are from the perspective of the subsequent tokens: Will they also consider these keys and values important?

We use the recent sequence ratio of $20 \%$ to select the shared PvCs. We extract non-shared PvCs from the tokens with $10 \%<d<20 \%$. We want to find these non-shared $\mathrm{PvCs}$ belong to which parts of keys and values of the subsequent tokens with $d<10 \%$.

From Figure 8, we can draw conclusions for these three parts of the KV cache:

1. The shared $\mathrm{PvCs}$ are the keys and values that subsequent tokens collectively pay attention to.
2. The non-shared $\mathrm{PvCs}$ seldom appear in nonshared $\mathrm{PvCs}$ of other tokens. It means that non-shared $\mathrm{PvCs}$ are mostly highly interested in by the current token, with less attention from subsequent tokens. They are mainly used to predict the token next to themself in a teacher-forcing way, which is especially useful in training.
3. Among the non-PvCs, a significant portion is occupied by non-shared $\mathrm{PvCs}$ of other tokens.

So far, we have completely verified the Inference Context Redundancy hypothesis that the tokens except for the last token no longer need to predict the next tokens but they still record this redundant information to predict the next tokens in keys and values.

## 4 Layer-wise PvC Selection

Based on the observations, we design the PyramidInfer, a method to highly increase the inference throughput by layer-wise selecting the $\mathrm{PvCs}$ to compress the KV cache for each layer.

### 4.1 Method

As shown in Figure 2, PyramidInfer can not only reduce the $\mathrm{KV}$ cache in the generation phase but

Layer 2
![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-07.jpg?height=2136&width=1470&top_left_y=343&top_left_x=310)

Layer 10

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-07.jpg?height=346&width=1426&top_left_y=789&top_left_x=332)
Recent Sequence Ratio $(\%)$

Layer 18

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-07.jpg?height=394&width=1407&top_left_y=1222&top_left_x=333)

Layer 26

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-07.jpg?height=389&width=1411&top_left_y=1650&top_left_x=331)

Layer 34

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-07.jpg?height=383&width=1407&top_left_y=2084&top_left_x=333)

Figure 8: The overlap ratios between non-shared $\mathrm{PvCs}$ and non-shared $\mathrm{PvCs}$ of other tokens (blue) and the overlap ratios between non-shared $\mathrm{PvCs}$ and non- $\mathrm{PvCs}$ of other tokens (orange).
also in the prefill phase without computing the complete keys and values of the prompt for all the layers. Following the inference process, we introduce the PyramidInfer in the prefill phase and generation phase separately and see how PyramidInfer can save lots of GPU memory by carefully selecting the PvCs.

Prefill Phase In the prefill phase, we have to process the prompt to prefill the initial $\mathrm{KV}$ cache. Different from the common inference process that reserves all keys and values of the prompt, PyramidInfer only reserves the $\mathrm{PvCs}$ of each layer as the initial $\mathrm{KV}$ cache.

Similarly, we divide the input sequence into recent sequence $S_{r}$ and context sequence $S_{c}$. As shown in Algorithm 1, based on the RAC, we first calculate the ensemble attention weights by weightedly averaging the attention weights of $S_{r}$. We assign larger weights for more recent tokens to enlarge their impact on $\mathrm{PvC}$ selection. Based on the ensemble attention weights, We layer-wise select the keys and values with top- $p$ weights as the $\mathrm{PvC}$. According to the conclusion of ICR, the increment of redundancy obeys the power law distribution. We choose a larger $p$ to retain more tokens in the $S_{c}$ for not to lose the semantics in the shallow layers. Then we gradually decrease the $p$ to reduce the length of $\mathrm{PvCs}$ in deeper layers. Therefore, the $\mathrm{PvCs}$ of the deeper layers are shorter and the KV cache becomes a "pyramid".

The layer-wise $\mathrm{PvC}$ selection saves much more GPU memory than other methods computing the whole prompt in the prefill phase. Besides the prefill phase, PyramidInfer continues to boost efficiency in the generation phase because LLMs only need to reuse a smaller initial $\mathrm{KV}$ cache.

Generation Phase As we have reserved the initial $\mathrm{PvCs}$ as the $\mathrm{KV}$ cache, what we should do in the generation phase is to update these $\mathrm{PvCs}$ according to the new recent tokens. As shown in Figure 6, we maintain a sliding recent window to update the newly generated token to be new recent tokens. Based on the new $S_{r}$, we update the PvCs of the KV cache where the operation is the same as the prefill phase. By controlling the length of the $\mathrm{PvC}$ of each layer, we can easily tune the compression ratio and even support unlimited input like StreamingLLM by maintaining a fixed number of $\mathrm{PvCs}$ in the $\mathrm{KV}$ cache.

## 5 Evaluation

### 5.1 Basic Evaluation

We evaluate PyramidInfer on various tasks and models to showcase that PyramidInfer can largely reduce the GPU memory and increase the throughput while maintaining the generation quality.

Experimental Setup We choose four kinds of scenarios: 1) Language modeling: we measure the perplexity on wikitext-v2 (Merity et al., 2016). 2) LLM benchmarks: we evaluate on MMLU (Hendrycks et al., 2021) and BBH (Srivastava et al., 2022) for language understanding, GSM8K (Cobbe et al., 2021) for mathematical reasoning, HumanEval (Chen et al., 2021) for coding. 3) Conversation: We evaluate on MT-Bench (Zheng et al., 2023) to see how PyramidInfer can handle multi-turn conversation. 4) Long context: we evaluate on long text summarization of the LEval (An et al., 2023) to see if PyramidInfer can maintain the quality while accepting longer input. We evaluate these tasks on LLaMA 2 (Touvron et al., 2023), LLaMA 2-Chat, Vicuna $1.5-16 \mathrm{k}$ (Zheng et al., 2023) and CodeLLaMA (Rozière et al., 2023) with different sizes (7B, 13B, 34B and 70B) ${ }^{1}$. We set the full $\mathrm{KV}$ cache method as the baseline. Besides that, we also include the "local" strategy as another baseline that reserves only the recent KV cache.

In addition, we showcase how much PyramidInfer can save GPU memory and improve the throughput. We compare the efficiency of PyramidInfer with other full cache methods, including Accelerate (HuggingFace, 2021), Deepspeed ${ }^{2}$ (Aminabadi et al., 2022). We also select $\mathrm{H}_{2} \mathrm{O}^{3}$ (Zhang et al., 2023), a KV cache compression method, as another baseline. It is noted that PyramidInfer is orthogonal to the non-KV-compression methods like Deepspeed to improve efficiency further.

Benchmark Result In Figure 9, we evaluate the LLMs with different compression ratios. We show that PyramidInfer maintains the generation quality with much less GPU memory compared with the full cache baseline. PyramidInfer also outperforms the "local" strategy with a large gap across different types and sizes of models and tasks.[^1]

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-09.jpg?height=1185&width=1602&top_left_y=224&top_left_x=227)

LM, LLaMA 2-7B
![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-09.jpg?height=1150&width=1566&top_left_y=242&top_left_x=242)

Figure 9: Benchmark results of comparison between models with full cache, "local" strategy, and PyramidInfer.

In the LEval that tests the long context ability, we show that the "local" strategy that is similar to the technique used in StreamingLLM causes a huge decline in memorization of history. PyramidInfer can accept longer input with less GPU memory without sacrificing too much performance.

Efficiency Result In Table 1, we fix the input length and the batch size. For LLaMA 2-13B, PyramidInfer showcases $2.24 \mathrm{x}$ throughput than full cache using Accelerate with 54.6\% less GPU memory in the KV cache. For LLaMA 2-70B, PyramidInfer can still generate in the prefill phase compared to other me. Existing KV cache compression methods like $\mathrm{H}_{2} \mathrm{O}$ can not even process the prompt and strike the OOM before the start of compression.

In Table 2, we exhaust the memory of an $80 \mathrm{~GB}$ A100 GPU to test the maximum throughput by maximizing the batch sizes. PyramidInfer enables more than $2 \mathrm{x}$ batch size than others and has higher throughput than full cache methods Accelerate and Deepspeed by $2.8 \mathrm{x}$ and $1.7 \mathrm{x}$, KV cache compression method $\mathrm{H}_{2} \mathrm{O}$ by $2.1 \mathrm{x}$. PyramidInfer can also be utilized to enhance Deepspeed by increasing the throughput by $1.9 x$.

Table 2: We exhaust the memory of an A100 80GB GPU to find out the maximum throughput of these methods on LLaMA 2-13B. We set the input length to 512+256. Lat.: latency to generate one token (ms/token).

| Method | Max Bsz | Lat. | Thr. |
| :--- | :---: | :---: | :---: |
| Accelerate | 42 | $1.72(100 \%)$ | $581(1.0 x)$ |
| Deepspeed | 40 | $1.03(59.8 \%)$ | $972(1.6 x)$ |
| $\mathrm{H}_{2} \mathrm{O}$ | 48 | $1.39(80.8 \%)$ | $769(1.3 x)$ |
| PyramidInfer | 88 | $0.59(34.3 \%)$ | $1678(2.8 x)$ |
| PyramidInfer | 86 | $\mathbf{0 . 5 3}(\mathbf{3 0 . 8 \% )}$ | $\mathbf{1 8 8 7}(\mathbf{3 . 2 x})$ |
| +Deepspeed |  |  |  |

### 5.2 Ablation Study

We conduct the ablation studies using the LLaMA 2-13B model to explore the PyramidInfer by answering the following questions: 1) Which way should we choose to gradually reduce the $\mathrm{PvC}$ length as the layer becomes deeper without sacrificing too much performance? 2) What proportion of the input should we partition as the

Table 1: The evaluation of inference methods using an A100 80GB GPU on LLaMA 2-13B and 70B. Length: prefill length + generation length. Bsz: batch size. KV mem.: GPU memory usage (GB) of the KV cache. Thr.: throughput (token/s)

| Model | \| Bsz | Length | Method | KV Mem. | Thr. |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 13B | 32 | $512+256$ | Accelerate | $24.2(100 \%)$ | $621(1.0 x)$ |
|  |  |  | Deepspeed | $24.2(100 \%)$ | $934(1.5 x)$ |
|  |  |  | $\mathrm{H}_{2} \mathrm{O}$ | $21.6(89.2 \%)$ | $584(0.9 x)$ |
|  |  |  | PyramidInfer | $11.0(45.4 \%)$ | $1389(2.2 x)$ |
| 70B | 8 | $256+128$ | Accelerate/ <br> Deenspeed/H $/ \mathrm{H}_{0}$ | ООМ | - |
|  |  |  | PyramidInfer | 4.2 | 20 |

![](https://cdn.mathpix.com/cropped/2024_05_26_dfafc0ddb73f2a5e57ebg-10.jpg?height=509&width=569&top_left_y=248&top_left_x=1246)

Figure 10: $S_{r}$ ratio ablation study. recent sequence $S_{r}$ ?

Table 3: PvC length decay ablation study.

| Strategy | PPL | GSM8K | MMLU |
| :--- | :---: | :---: | :---: |
| Reduce more | 4.93 | 26.82 | 53.1 |
| Reduce uniformly | 4.55 | 28.32 | 54.8 |
| Reduce less (PyramidInfer) | 4.20 | 29.56 | 55.7 |
| Reduce None (Full cache) | $\mathbf{4 . 4 2}$ | $\mathbf{2 8 . 5 8}$ | 55.4 |

PvC Length Decay Based on ICR, we gradually reduce the length of $\mathrm{PvCs}$ for each layer as the layer becomes deeper to maximize efficiency. However, excessive reduction of $\mathrm{PvC}$ length in shallow layers may lead to the loss of context information. We try to find out which way is the best to reduce the $\mathrm{PvC}$ length. Under the same compression ratio of $60 \%$, we compare three patterns: 1) reduce more $\mathrm{PvC}$ length in shallow layers but less in the deeper layers (reduce $15 \%$ cache in the first $50 \%$ layers). 2) uniformly reduce the $\mathrm{PvC}$ length (reduce $10 \%$ cache in the first $50 \%$ layers); 3) obey the power law pattern based on ICR to reduce less at first (reduce $7 \%$ cache in the first $50 \%$ layers).

The result in Table 3 demonstrates that following the power law pattern is the best way to reduce the $\mathrm{PvC}$ length and even slightly improve performance on downstream tasks.

Recent Sequence Ratio In PyramidInfer, we select the recent tokens of the input as the recent sequence $S_{r}$. The $S_{r}$ is not only leveraged as the context but also the criteria to select the $\mathrm{PvC}$ from the context sequence $S_{c}$. If the $S_{r}$ ratio increases, $S_{c}$ will be shorter thus fewer tokens in $S_{c}$ will be compressed. Therefore, we need to find a balance to decide how large the $S_{r}$ ratio should be.

In Figure 10, we set the GPU memory usage of the KV cache of the full cache method as the $100 \%$ baseline and test how the perplexity will change with different $S_{r}$ ratios. As the $S_{r}$ ratio increases, we observe a decline in the GPU memory usage but a trough in the perplexity at $40-60 \% S_{r}$ ratio. Thus we can choose $40 \%$ as a trade-off between performance and GPU memory usage.

## 6 Conclusion

We alleviate the difficulty of deploying LLMs at scale by introducing PyramidInfer, a novel method that efficiently compresses the KV cache during both prefill and generation phases. Inspired by ICR and RAC, PyramidInfer significantly reduces GPU memory usage without compromising model performance. Experimental results present PyramidInfer is a promising solution for optimizing LLM deployment in resource-constrained environments.

## References

Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and Yuxiong He. 2022. Deepspeed inference:

Enabling efficient inference of transformer models at unprecedented scale.

Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models.

Anthropic. 2023. Introducing claude. https://www. anthropic.com/index/introducing-claude.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems.

OpenCompass Contributors. 2023. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/ opencompass.

Tri Dao. 2023. FlashAttention-2: Faster attention with better parallelism and work partitioning.

Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2023. Model tells you what to discard: Adaptive kv cache compression for $11 \mathrm{~ms}$. arXiv preprint arXiv:2310.01801.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. 2021. Measuring massive multitask language understanding.

Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism.

HuggingFace. 2021. Hugging face accelerate. https: //huggingface.co/docs/accelerate/index.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral $7 \mathrm{~b}$.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention.

Alex M Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang, Saizheng Zhang, Aaron C Courville, and Yoshua Bengio. 2016. Professor forcing: A new algorithm for training recurrent networks. Advances in neural information processing systems, 29.

Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2023. Scissorhands: Exploiting the persistence of importance hypothesis for $1 \mathrm{~lm} \mathrm{kv}$ cache compression at test time.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.

OpenAI. 2023. Gpt-4 technical report.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, $1(8): 9$.

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code.

Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023. Flexgen: High-throughput generative inference of large language models with a single gpu.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models.

Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.

Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. 2023. $\mathrm{H}_{2} \mathrm{O}$ : Heavy-hitter oracle for efficient generative inference of large language models.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.
