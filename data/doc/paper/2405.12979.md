# OmniGlue: Generalizable Feature Matching with Foundation Model Guidance 

Hanwen Jiang ${ }^{*} \quad$ Arjun Karpur $^{\dagger} \quad$ Bingyi Cao $^{\dagger} \quad$ Qixing Huang* $^{*} \quad$ Andr√© Araujo $^{\dagger}$<br>${ }^{*}$ University of Texas at Austin<br>(hwjiang,huangqx)@cs.utexas.edu<br>${ }^{\dagger}$ Google Research<br>(arjunkarpur,bingyi, andrearaujo)@google.com


#### Abstract

The image matching field has been witnessing a continuous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks. However, our investigation shows that despite these gains, their potential for real-world applications is restricted by their limited generalization capabilities to novel image domains. In this paper, we introduce OmniGlue, the first learnable image matcher that is designed with generalization as a core principle. OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting generalization to domains not seen at training time. Additionally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appearance information, leading to enhanced matching descriptors. We perform comprehensive experiments on a suite of 7 datasets with varied image domains, including scenelevel, object-centric and aerial images. OmniGlue's novel components lead to relative gains on unseen domains of $20.9 \%$ with respect to a directly comparable reference model, while also outperforming the recent LightGlue method by $9.5 \%$ relatively. Code and model can be found at https : //hwjiang1510.github.io/OmniGlue.


## 1. Introduction

Local image feature matching techniques provide finegrained visual correspondences between two images [31], which are critical for achieving accurate camera pose estimation $[40,42]$ and 3D reconstruction $[4,16,20,43]$. The past decade has witnessed the evolution from hand-crafted $[3,30]$ to learning-based image features $[10,37,39,52,56]$. More recently, novel learnable image matchers have been proposed [13, 28, 42, 45, 48], demonstrating ever-improving performance on conventional benchmarks $[1,8,26]$.[^0]

![](https://cdn.mathpix.com/cropped/2024_05_26_2183d5c3927b6a360c8bg-01.jpg?height=526&width=697&top_left_y=797&top_left_x=1213)

Figure 1. OmniGlue is a generalizable learnable matcher. Introducing foundation model guidance and an enhanced attention mechanism, OmniGlue learns effective image matching that transfers well to image domains not seen during training. We compare it against reference methods SIFT [30] and SuperGlue [42], with substantial improvements on a suite of diverse datasets: outdoor scenes (MegaDepth-1500 [26] pose AUC@ $5^{\circ}$ ), indoor scenes (ScanNet [8] pose accuracy @ $5^{\circ}$ ), aerial scenes (DeepAerial [36] PCK @ 1\%) and object-centric images (GSO-Hard [12] and NAVI-MultiView / NAVI-Wild [19], pose accuracy $@ 5^{\circ}$ ).

Despite substantial progress, these advancements overlook an essential aspect: the generalization capability of image matching models. Today, most local feature matching research $[13,28,45]$ focuses on specific visual domains with abundant training data (e.g., outdoor and indoor scenes), leading to models that are highly specialized for the training domain. Unfortunately, we observe that the performance of these methods usually drops dramatically on out-of-domain data (e.g., object-centric or aerial captures), which may not even be significantly better than traditional approaches in some cases. For this reason, traditional domain-agnostic techniques, such as SIFT [30], are still widely used to obtain poses for downstream applications $[2,25,32,49]$. Due to the cost of collecting high-quality correspondence annotations, we believe it is unrealistic to assume that abundant training data would be available for each image domain, like in some other vision tasks $[9,27]$. Thus, the community should focus
on developing architectural improvements to make learnable matching methods generalize.

Motivated by the above observations, we propose OmniGlue, the first learnable image matcher that is designed with generalization as a core principle. Building on top of domain-agnostic local features [10], we introduce novel techniques for improving the generalizability of matching layers: foundation model guidance and keypoint-position attention guidance. As shown in Fig. 1, with the introduced techniques, we enable OmniGlue to generalize better on out-of-distribution domains while maintaining quality performance on the source domain.

Firstly, we incorporate broad visual knowledge of a foundation model. By training on large-scale data, the foundation model, DINOv2 [35], performs well in diverse image domains on a variety of tasks, including robust region-level matching [22, 35, 57]. Even though the granularity of matching results yielded from foundational models is limited, these models provide generalizable guidance on potential matching regions when a specialized matcher cannot handle the domain shift. Thus, we use DINO to guide the inter-image feature propagation process, downgrading irrelevant keypoints and encouraging the model to fuse information from potentially matchable regions.

Secondly, we also guide the information propagation process with keypoint position information. We discover that previous positional encoding strategies [42] hurt performance when the model is applied to different domains - which motivates us to disentangle it from the matching descriptors used to estimate correspondence. We propose a novel keypoint-position guided attention mechanism designed to avoid specializing too strongly in the training distribution of keypoints and relative pose transformations.

Experimentally, we assess OmniGlue's generalization across diverse visual domains, spanning synthetic and real images, from scene-level to object-centric and aerial datasets, with small-baseline and wide-baseline cameras. We demonstrate significant improvements compared to previous work. In more detail, our contributions are as follows.

Contributions. (1) We introduce foundation model guidance to the learnable feature matching process, which leverages broad visual knowledge to enhance correspondences in domains that are not observed at training time, boosting pose estimation accuracy by up to $\mathbf{5 . 8 \%}$ ( $14.4 \%$ relatively). (2) A new strategy for leveraging positional encoding of keypoints, which avoids an overly reliant dependence on geometric priors from the training domain, boosting crossdomain transfer by up to $\mathbf{6 . 1 \%}$ ( $14.9 \%$ relatively). (3) We perform comprehensive experiments on 7 datasets from varied domains, demonstrating the limited generalizability of existing matching methods and OmniGlue's strong improvements, with relative gains of $\mathbf{2 0 . 9 \%}$ on average in all novel domains. (4) By fine-tuning OmniGlue using limited amount of data from the target domain, we show that OmniGlue can be easily adapted with an improvement up to $\mathbf{8 . 1 \%}(94.2 \%$ relatively).

## 2. Related Work

Generalizable Local Feature Matching. Prior to the deep learning era, researchers focused on developing generalizable local feature models. For example, SIFT [30], SURF [3] and ORB [41] have been widely used for image matching tasks across diverse image domains. Still today, many computer vision systems ignore recent advances in learnable local features and rely on hand-crafted methods, for example, to obtain poses for downstream applications [2, 25, 32, 49]. One of the main reasons for such old hand-crafted methods to continue being adopted is that most of the recent learning-based methods $[14,33,34,39,50]$ are specialized to domains with abundant training data, such as outdoor building scenes, and do not generalize well to other domains. Recently, the community shifted the main focus to develop learnable image matchers, which associate local features produced by off-the-shelf methods [10] or jointly learn feature description and association [45]. While they demonstrate better performance compared with hand-crafted matching systems, they make the entire image matching pipeline even more domain-specific. Our experiments show that learnable matchers specialize strongly in the training domain, with limited generalization. Our proposed OmniGlue improves the generalization capability of existing learnable matchers by introducing guidance from foundation models and improved positional encoding.

Sparse Learnable Matching. Sparse learnable image matching methods $[6,28,42]$ associate sparse keypoints, produced by keypoint detectors. For example, SuperGlue [42] uses SuperPoint [10] for keypoint detection and leverages the attention mechanism [51] to perform intra- and inter-image keypoint feature propagation. However, SuperGlues shows limited generalization capability. One reason is that it entangles the local descriptors and positional information of the keypoints, making the matching process overly dependent on learned positional patterns. It hinders the generalizability to data with different position-related matching patterns. To solve this problem, OmniGlue proposes to disentangle them during the feature propagation, releasing the reliance on positional patterns and improving the generalization capability to images from diverse domains.

(Semi-)Dense Learnable Matching. Dense image matching methods jointly learn the image descriptors and the matching module, performing pixel-wise matching on the entire input images $[7,13,45,47,53]$. They benefit from the end-toend learning pipeline and demonstrate better performance in the training domain. For example, the semi-dense method LoFTR introduces a coarse-to-fine correspondence prediction paradigm [45]. Another line of work directly predicts

![](https://cdn.mathpix.com/cropped/2024_05_26_2183d5c3927b6a360c8bg-03.jpg?height=436&width=1730&top_left_y=232&top_left_x=165)

Figure 2. OmniGlue overview. We use frozen DINO and SuperPoint to detect keypoints and extract features. Then, we build densely connected intra-image keypoint graphs and leverage DINO features to build inter-image graphs. We refine the keypoint features based on the constructed graphs, performing information propagation. In this process, we use keypoint positions solely for guidance, disentangling them from the keypoint local descriptors. Finally, the matching results are produced based on the updated keypoint local descriptors.

the matching results as a 4D correlation volume [13, 47]. However, we notice that some of them generalize worse on new domains compared with sparse methods. Thus, OmniGlue chooses to focus on sparse methods, which can have better potential to be generalizable due to the use of domain-agnostic local descriptors.

Matching with Additional Image Representations. Leveraging robust image representations is a promising avenue toward generalizable image matching. One line of work uses geometric image representations, e.g., depth map [54] and NOCS map [24], to augment the image matching process. However, they are dependent on a highly accurate monocular estimation of these geometric representations. Differently, SFD2 [55] uses semantic segmentation results to reject indistinguishable keypoints in background regions. Nevertheless, the semantic segmentation model has to be trained on each specific target domain. Recently, large vision models, e.g., self-supervised vision backbones [5, 17, 35] and Diffusion models $[18,46,57]$ demonstrate robust semantic understanding properties. By training on large data, these models showcase strong generalization capability across diverse domains [21, 22, 29], which enables them to obtain coarse patch-level matching results. However, performing matching using image features extracted by these models demonstrates limited performance on regions/keypoints without strong semantic information and the accuracy is limited $[23,57]$. Instead of directly incorporating these coarse signals into the keypoint features and using them to perform matching, OmniGlue uses DINOv2 features to identify potentially related regions and guide the attention-based feature refinement process. Thanks to the wide domain knowledge encoded in this model, OmniGlue can boost the generalization ability of our method to diverse domains.

## 3. OmniGlue

We first introduce the overview and technical details of our method OmniGlue. Then we compare OmniGlue with SuperGlue and LightGlue for clarifying their differences.

### 3.1. Model Overview

Fig. 2 presents a high-level overview of our OmniGlue method, with four main stages. First, image features are extracted using two complementary types of encoders: SuperPoint [10], focusing on generic fine-grained matching; and DINOv2 [35], an image foundation model which encodes coarse but broad visual knowledge. Second, we build keypoint association graphs using these features, both intra and inter-image. In contrast to previous work, our interimage graph leverages DINOv2 guidance, which provides a coarse signal capturing general similarity between SuperPoint keypoints. Third, we propagate information among the keypoints in both images based on the built graphs, using self and cross-attention layers for intra and inter-image communication, respectively. Crucially, we disentangle positional and appearance signals at this stage, different from other models that overlook this aspect. This design enables feature refinement to be guided by both keypoint spatial arrangement and their feature similarities, but without contaminating the final descriptors with positional information, which hinders generalizability. Finally, once the refined descriptors are obtained, optimal matching layers are applied to produce a mapping between the keypoints in the two images. These stages are described in more detail in the following section.

### 3.2. OmniGlue Details

Feature Extraction. The inputs are two images with shared content, denoted as $I_{A}$ and $I_{B}$. We denote the SuperPoint keypoint sets of the two images as $\mathbf{A}:=\left\{A_{1}, \ldots, A_{N}\right\}$ and $\mathbf{B}:=\left\{B_{1}, \ldots, B_{M}\right\}$. Note that $N$ and $M$ are the number of identified keypoints of $I_{A}$ and $I_{B}$, respectively. Each keypoint is associated with its SuperPoint local descriptor $\mathbf{d} \in \mathbb{R}^{C}$. Additionally, normalized keypoint locations are encoded with positional embeddings, and we further refine them using MLP layers. We denote the resulting positional features of a keypoint as $\mathbf{p} \in \mathbb{R}^{C}$. Furthermore, we extract dense DINOv2 feature maps of the two images. We interpolate the feature maps using the location of SuperPoint

![](https://cdn.mathpix.com/cropped/2024_05_26_2183d5c3927b6a360c8bg-04.jpg?height=453&width=833&top_left_y=240&top_left_x=169)

Figure 3. (Left) Building inter-image graph. We prune the dense pairwise graph based on the DINO feature similarity. (Right) Position-guided attention. The keypoint position is involved in computing attention weights, while the output attention update is only composed of local descriptor components.

keypoints to obtain DINOv2 descriptors for each keypoint, denoted as $\mathbf{g} \in \mathbb{R}^{C^{\prime}}$. For clarity, we denote the three features of the $i^{\text {th }}$ keypoint in set $\mathbf{A}$ as $\mathbf{d}_{i}^{A}, \mathbf{p}_{i}^{A}$ and $\mathbf{g}_{i}^{A}$. The features of the keypoints in set $\mathbf{B}$ are denoted accordingly. The goal of our OmniGlue model is to estimate correspondences between the two keypoint sets.

Graph Building Leveraging DINOv2. We build four keypoint association graphs: two inter-image graphs and two intra-image graphs. The two inter-image graphs represent the connectivity between the keypoints of the two images, from $I_{A}$ to $I_{B}$ and vice versa. We denote them as $\mathbf{G}_{A \rightarrow B}$ and $\mathbf{G}_{B \rightarrow A}$, respectively. The two inter-image graphs are directed, where information is propagated from the source node to the target node.

We leverage DINOv2 features to guide the building of the inter-image graphs. As depicted in Fig. 3 (left), we take $\mathbf{G}_{B \rightarrow A_{i}}$ as an example. For each keypoint $A_{i}$ in keypoint set $\mathbf{A}$, we compute its DINOv2 feature similarities with all keypoints in set $\mathbf{B}$. Note that we perform channel-wise normalization on the DINOv2 features $\mathbf{g}_{i}^{A}$ and $\mathbf{g}^{B}$ before computing the similarities. We select the top half of keypoints in set $\mathbf{B}$ with the largest DINOv2 similarities to connect with $A_{i}$, which prunes the densely-connected pairwise graph between the keypoints of the two images. We perform the same operation on all keypoints in $A$ to obtain $\mathbf{G}_{B \rightarrow A}$, and the graph $\mathbf{G}_{A \rightarrow B}$ is built in a similar manner.

Similarly, the intra-image graphs represent the connectivity between keypoints belonging to the same image. We denote them as $\mathbf{G}_{A}$ and $\mathbf{G}_{B}$, which are undirected - information is propagated bi-directionally between connected keypoints. Each keypoint is densely connected with all other keypoints within the same image.

Information Propagation with Novel Guidance. We perform information propagation based on the keypoint graphs. This module contains multiple blocks, where each block has two attention layers. The first one updates keypoints based on the intra-image graphs, performing self-attention; The second updates keypoints based on the inter-image graphs, performing cross-attention. In particular, this stage introduces two novel elements compared to previous work, which we show are critical towards generalizable matching: suitable guidance from DINOv2 and from keypoint positions.

First, DINOv2 guidance: during cross-attention, for keypoint $A_{i}$, it only aggregates information from the DINOv2pruned potential matching set selected from $\mathbf{B}$, instead of all its keypoints. This is particularly helpful for generalized image matching, where DINO's broad knowledge may guide the feature matching process in a domain that the model has not seen at training time. In this manner, information from irrelevant keypoints will not be fused into the query keypoint features. This process also encourages the cross-attention module to focus on distinguishing the matching point in the smaller potential matching set. Note, however, that we do not forcibly limit the matching space to the potential matching sets, as DINO may also be incorrect in some cases.

Second, we introduce refined keypoint guidance. We observe that prior methods entangle keypoint positional features and local descriptors during feature propagation [42], which makes the model overly dependent on learned positionrelated priors - our ablation experiments in Section 4 highlight this issue. The learned priors are vulnerable under image pairs with matching patterns that were not seen at training time, limiting the generalization capability. To deal with this issue, we propose a novel position-guided attention, which disentangles the keypoint positional features $\mathbf{p}$ and the local descriptors $\mathbf{d}$. The positional information is used as spatial context in this module and is not incorporated in the final local descriptor representation used for matching.

With these novel elements, our attention layer, illustrated in Fig. 3 (right), is defined as follows, where we take the example of keypoint $A_{i}$ :

$$
\begin{array}{r}
\mathbf{d}_{i}^{A} \leftarrow \mathbf{d}_{i}^{A}+\operatorname{MLP}\left(\left[\mathbf{d}_{i}^{A} \mid \Delta \mathbf{d}_{i}^{A}\right]\right), \text { where } \\
\Delta \mathbf{d}_{i}^{A}=\operatorname{Softmax}\left(\frac{\mathbf{q}_{i}^{A}\left(\mathbf{k}^{S}\right)^{T}}{\sqrt{C}}\right) \cdot \mathbf{v}^{S} \in \mathbb{R}^{C}, \text { and } \\
\mathbf{q}_{i}^{A}=\mathrm{W}^{q}\left(\mathbf{d}_{i}^{A}+\mathbf{p}_{i}^{A}\right)+\mathbf{b}^{q} \in \mathbb{R}^{C} \\
\mathbf{k}^{S}=\mathrm{W}^{k}\left(\mathbf{d}^{S}+\mathbf{p}^{S}\right)+\mathbf{b}^{k} \in \mathbb{R}^{K \times C} \\
\mathbf{v}^{S}=\mathrm{W}^{v}\left(\mathbf{d}^{S}\right)+\mathbf{b}^{v} \in \mathbb{R}^{K \times C} \tag{5}
\end{array}
$$

As described in Eq. 1, the attention has a residual connection, which integrates the attention update value $\Delta \mathbf{d}_{i}^{A}$. The notation $\leftarrow$ is the updating operation and $[\cdot \mid \cdot]$ is the channel-wise concatenation. To compute the attention update value, as described in Eq. 2, we compute the feature similarity between the keypoint $A_{i}$ and its source connected keypoints in a graph, which is denoted as $S$ containing $K$ keypoints. The query, key and value of the attention are $\mathbf{q}_{i}^{A}, \mathbf{k}^{S}$, and $\mathbf{v}^{S}$, respectively. Specifically, as shown in Eq. 3-5, the query and key are computed by fusing both local descriptors and
positional features. The value, however, is transformed from only the local descriptors. We note that the weights (W) and bias (b), which map features into query, key and value tokens in attention, are not shared across different attention layers. In self-attention $\left(\mathbf{G}_{A}\right.$ and $\left.\mathbf{G}_{B}\right), S$ is composed by all keypoints; in cross-attention $\left(\mathbf{G}_{A \rightarrow B}\right.$ and $\left.\mathbf{G}_{B \rightarrow A}\right), S$ contains the keypoints identified by DINO.

Intuitively, the query and key compute the attention weights, where both feature affinity and spatial correlations are considered. However, the attention update value, $\Delta \mathbf{d}_{i}^{A}$, is composed of local descriptor components only. This design allows the model to reason about spatial correlation between keypoints using their positional features while avoiding an over-reliance on it.

Matching Layer and Loss Function. We use the refined keypoint representations to produce a pairwise similarity matrix $\mathbf{S} \in \mathbb{R}^{N \times M}$, where $\mathbf{S}_{i, j}=\mathbf{d}_{i}^{A} \cdot\left(\mathbf{d}_{j}^{B}\right)^{T}$. Then we use the Sinkhorn algorithm [44] to refine the similarities, which produces the matching matrix $\mathbf{M} \in[0,1]^{N \times M}$, where $\mathbf{M}_{i, j}$ represents the matching probability between keypoint $A_{i}$ and $B_{j}$. To train OmniGlue, we minimize the negative loglikelihood of the matching matrix with ground truth $[42,45]$.

### 3.3. Comparison Against SuperGlue and LightGlue

It is important to highlight differences between our model and reference sparse learnable feature matching methods, SuperGlue [42] and LightGlue [28]. While neither of these is designed to target generalizability to multiple domains, there are common elements in the model structure, so we would like to emphasize our novelty.

Both works use attention layers for information propagation. Differently, OmniGlue leverages a foundation model to guide this process, which significantly helps with transferring to image domains that are not observed during training.

In terms of local descriptor refinement, OmniGlue departs from SuperGlue to disentangle positional and appearance features. For reference, SuperGlue represents keypoint with entangling the two features as $\mathbf{d}+\mathbf{p}$, where positional features are also used to produce matching results. Similar to our design, LightGlue removes the dependency of the updated descriptors on the positional features. However, it proposes a very specific positional encoding formulation, based on rotary encodings, only in self-attention layers.

Overall, SuperGlue is the closest model to OmniGlue, serving as a directly comparable reference where our contributions can be clearly ablated. For this reason, in the following section, we use SuperGlue as the main reference comparison for experimental validation.

## 4. Experiments

We first introduce the experiment setup and then present our results as well as ablation studies.

|  | (1) | (2) | (3) | (4) | (5) | (6) | (7) | (8) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Type | Scene | Real <br> Img. | Syn. <br> Trans. | Mask | Cam. <br> Bl. | Diff. <br> Bg. | Task |
| MegaDepth | Scene | Outdoor | $\checkmark$ | $x$ | $x$ | Large | $x$ | Corr. \& Pose Est. |
| GSO-Hard | Object | None | $x$ | $x$ | $x$ | Large | $x$ | Pose Est. |
| GSO-Easy | Object | None | $x$ | $x$ | $x$ | Small | $x$ | Pose Est. |
| NAVI-MV | Object | In \& Outdoor | $\checkmark$ | $x$ | $\checkmark$ | Large | $x$ | Pose Est. |
| NAVI-Wild | Object | In \& Outdoor | $\checkmark$ | $x$ | $\checkmark$ | Large | $\checkmark$ | Pose Est. |
| ScanNet | Scene | Indoor | $\checkmark$ | $x$ | $x$ | Large | $x$ | Pose Est. |
| $S H$ | Scene | Outdoor | $\checkmark$ | $\checkmark$ | $x$ | Small | $x$ | Corr. Est. |
| DeepAerial | Scene | Aerial | $\checkmark$ | $\checkmark$ | $x$ | N/A | $\checkmark$ | Image Reg. |

Table 1. Dataset and task comparisons on: (1) The general type; (2) The background scene type; (3) Use of real $(\checkmark)$ or rendered ( $\boldsymbol{X}$ ) images; (4) Whether the pose transformation is synthetic; (5) Whether foreground masks are used to filter correspondence predictions; (6) The camera baseline type; (7) Whether two input images have different backgrounds; (8) Evaluated tasks: Correspondence Estimation, Pose Estimation or Image Registration.

### 4.1. Experimental Setup

We list the datasets and tasks used for evaluating OmniGlue in Table 1. We include details of datasets as follows:

- Synthetic Homography (SH) contains images from the Oxford and Paris dataset [38]. We generate random crops and homography transformations to sample image patch pairs, similar to [42]. Two subsets are generated, SH100 and $\mathrm{SH} 200$, wherein the perturbations of the image corners for homography generation are within 100 and 200 pixels, respectively. For each subset, we generate roughly 9 million training pairs and $10 \mathrm{~K}$ test pairs.
- MegaDepth (MD) [26] is a large-scale outdoor image dataset. The ground-truth matches are computed using SfM [43]. We follow the train/test split of prior works [45], with roughly $625 \mathrm{~K}$ training pairs and 1500 test pairs.
- Google Scanned Objects (GSO) [12] comprises 1400 daily object model scans of 17 categories. We render synthetic images with large $\left(60^{\circ}-90^{\circ}\right)$ rotation (Hard subset) and small $\left(15^{\circ}-45^{\circ}\right)$ rotation (Easy subset) camera baselines, intentionally distinct from the training distribution. We produce 50 image pairs for each object model, resulting in around $140 \mathrm{~K}$ test cases.
- NAVI [19] focuses on objects and encompasses a variety of both indoor and outdoor images. It is divided into two subsets: the multiview subset ( $25 \mathrm{~K}$ image pairs), featuring input images captured in the same environment; and the wild subset ( $36 \mathrm{~K}$ image pairs), where the two input images are taken in different environments with distinct backgrounds, lighting conditions and camera models.
- ScanNet [8] collects indoor images. We follow the split of prior works [45] with 1500 evaluation pairs.
- DeepAerialMatching [36] provides aligned pairs of satellite images under varying conditions (i.e. different seasons, weather, time-of-day). We introduce random 2D rotations and crop $520 \times 520$ image patches to produce image pairs with known affine transformations (500 in total).

Tasks and metrics. We assess the models across three

![](https://cdn.mathpix.com/cropped/2024_05_26_2183d5c3927b6a360c8bg-06.jpg?height=461&width=1740&top_left_y=236&top_left_x=165)

Figure 4. Visualization of correspondences predicted by OmniGlue on the MegaDepth-1500 benchmark. We distinguish the matches by different colors. We show results for scene "0022" and "0015" on the top and bottom rows, respectively.

tasks: (1) Correspondence estimation, evaluated with correspondence-level precision and recall (for sparse methods only). Following SuperGlue [42], we employ thresholds of $<3 p x$ and $>5 p x$ to label a correspondence as correct and incorrect, respectively. (2) Camera pose estimation, evaluated with pose accuracy ( $\%$ of correct poses within $\left\{5^{\circ}, 10^{\circ}, 20^{\circ}\right\}$ of error) and AUC, with accuracy being used by default unless otherwise specified. The poses are derived from the estimated correspondences using RANSAC [15], and we use Rodrigues' formula to calculate relative rotation error between the predicted/ground truth rotation matrices; (3) Aerial image registration, evaluated with percentage of correct keypoints (PCK). We use RANSAC-based affine estimation from the estimated correspondences, and apply the predicted/ground truth affine transformations to 20 test keypoints with fixed positions to calculate the PCK within $\tau \cdot \max (h, w)$ pixels of error, for $\tau \in\{0.01,0.03,0.05\}$.

Baselines. We compare OmniGlue against:

- SIFT [30] and SuperPoint [10] provide domain-agnostic local visual descriptors for keypoints. We generate matching results using both nearest neighbor + ratio test $(\mathrm{NN} /$ ratio) and mutual nearest neighbor (MNN), with the best outcomes being reported.
- Sparse matchers: SuperGlue [42] employs attention layers for intra- and inter-image keypoint information aggregation, using descriptors derived from SuperPoint [10]. It is the closest reference of OmniGlue. LightGlue [28] improves SuperGlue [42] with better performance and speed. Besides, we also test with DINOv2 [35]+SuperGlue, by substituting SuperPoint descriptors with DINO features.
- (Semi-)Dense matchers: LoFTR [45] and PDCNet [47] are used as reference dense matching techniques, to contextualize our sparse matching performance with respect to other types of approaches.

Implementation details. In line with SuperGlue [42], we implement 9 contextual reasoning blocks, each comprising an intra-image aggregation layer (self-attention) and an interimage aggregation layer (cross-attention). This configuration results in a total of 18 attentional layers. Across all sparse

| Setting $\rightarrow$ | Test Performance (in-domain) |  |
| :---: | :---: | :---: |
|  | SH100 | SH200 |
| DINOv2 [35]+SG [42] | $87.6 / 88.4$ | $79.8 / 80.2$ |
| SP[10]+SG [42] | $\mathbf{9 9 . 2} / \mathbf{9 9 . 4}$ | $95.4 / 96.0$ |
| OmniGlue (ours) | $\mathbf{9 9 . 2} / \mathbf{9 9 . 5}$ | $\mathbf{9 6 . 4} / \mathbf{9 8 . 0}$ |
| Setting $\rightarrow$ | Test Generalization $(\mathrm{src} \rightarrow$ trg $)$ |  |
|  | SH100 $\rightarrow$ SH200 | SH200 $\rightarrow$ MD |
| DINOv2 [35]+SG [42] | $72.6 / 77.3$ | $19.2 / 18.8$ |
| SP[10]+SG [42] | 78.3 / 75.6 | $34.9 / 39.0$ |
| OmniGlue (ours) | $\mathbf{9 0 . 0} / \mathbf{8 9 . 6}$ | $\mathbf{3 6 . 0} / \mathbf{5 4 . 7}$ |
| relative gain (\%) | $+\mathbf{1 4 . 9} / \mathbf{1 8 . 5}$ | $+4.3 /+\mathbf{4 0 . 3}$ |

Table 2. Results for in-domain (top) and zero-shot generalization to out-of-domain datasets (bottom), for models trained on Synthetic Homography $(\mathrm{SH})$ datasets. We measure precision / recall at the correspondence level.

methods, we use 1024 keypoints and 256-dimensional descriptors. See more training details in supplementary.

### 4.2. Results

Following SuperGlue and LightGlue, we first initialize OmniGlue by training it on SH100. Then we further pretrain OmniGlue on SH200, and finally train OmniGlue on MegaDepth (MD). We evaluate OmniGlue and all baseline methods on the test splits of each training domain, and test their generalization to both subsequent training datasets or out-of-domain test datasets. Finally, we experiment with adapting OmniGlue to out-of-domain images with limited target domain training data.

From Synthetic Homography to MegaDepth. As depicted in Table 2, in comparison to the base method SuperGlue, OmniGlue not only exhibits superior performance on the in-domain data but also demonstrates robust generalization. Even with a minimal data distribution shift from SH100 to SH200, SuperGlue experiences substantial drops in performance with a $20 \%$ reduction in precision and recall. This result implies that SuperGlue is overly dependent on learned position-related patterns and is unable to handle further image warping distortion. In contrast, OmniGlue showcases strong generalization capability, surpassing SuperGlue with

![](https://cdn.mathpix.com/cropped/2024_05_26_2183d5c3927b6a360c8bg-07.jpg?height=477&width=1743&top_left_y=228&top_left_x=164)

Figure 5. Zero-shot generalization to novel domains. The top and middle row show results on GSO and NAVI, the last row shows results on ScanNet and DeepAerial. We draw the correct and incorrect estimated correspondences as green and red, respectively.

![](https://cdn.mathpix.com/cropped/2024_05_26_2183d5c3927b6a360c8bg-07.jpg?height=604&width=1726&top_left_y=804&top_left_x=167)

Table 3. Results for in-domain (left, measured with AUC) and zero-shot generalization to out-of-domain datasets (right, measured with pose accuracy / PCK), for models trained on the MegaDepth dataset. We highlight the best results on out-of-domain data and show our relative improvement against our base method SuperGlue. All sparse methods use 1024 keypoints.

a $12 \%$ improvement in precision and a $14 \%$ boost in recall. Similarly, during the transfer from SH200 to Megadepth, OmniGlue outperforms SuperGlue with a drastic $15 \%$ improvement in recall.

From MegaDepth to other Domains. As shown in Table 3, OmniGlue not only achieves comparable performance on MegaDepth-1500 with the state-of-the-art sparse matcher LightGlue, but also demonstrates better generalization capability on 5 out of 6 novel domains, when compared to all other methods. In detail, on MegaDepth-1500, OmniGlue showcases $12.3 \%$ relative gain (pose AUC $@ 5^{\circ}$ ) over the base method SuperGlue. On the 6 novel domains, OmniGlue shows $20.9 \%$ and $9.5 \%$ averaged relative gains (for pose and registration accuracy at the tightest thresholds) over SuperGlue and LightGlue, respectively. Moreover, OmniGlue demonstrates larger performance gains on harder novel domains against LightGlue, i.e., on GSO-Hard, NAVI-Wild, and DeepAerial. We show visualization in Fig. 5 and Fig 4 for zero-shot generalization on novel domains and its performance on the source domain.

Notably, the reference dense matchers, which achieve better performance on the in-domain MegaDepth dataset, gen- eralize worse. Their performances are close, or even worse, to SuperGlue, which has $10 \%$ lower in-domain AUC@ $5^{\circ}$. We conjecture this may be due to the joint learning of visual descriptors and the matching module, making them easier to specialize strongly to the training domain.

Low-Shot Fine-tuning on Target Domain. In certain realworld scenarios, a limited set of target domain data may be available for fine-tuning. To test this scenario, we finetune OmniGlue on the target domain (object-centric GSO dataset), comparing its performance with the base model, SuperGlue. We create small training subsets by utilizing only a few dozen object scans. Notably, these small training sets consist of instances from the sneaker object category only, covering a significantly minor subset of the testing object category distribution.

As depicted in Table 4, OmniGlue is more readily adapted to the target domain. In detail, when scaling from 0 to 30 instances for training, OmniGlue consistently exhibits enhanced performance for both test subsets. With just 10 instances for training, OmniGlue improves pose estimation accuracy by $5.3 \%$ and $4.0 \%$ on the two subsets. Expanding the training sets by incorporating 10 more objects leads to

| \#Train <br> Inst. | Model | Hard (60-90 deg.) <br> $@ 5^{\circ} / 10^{\circ} / 20^{\circ}$ | Easy $(15-45$ deg. $)$ <br> $@ 5^{\circ} / 10^{\circ} / 20^{\circ}$ |
| :---: | :---: | :---: | :---: |
| 0 | SG | $7.2 / 13.2 / 21.6$ | 32.3 / 48.4 / 62.9 |
|  | OG | 8.6 / 15.3 / 25.0 | 38.4 / 54.8 / 68.8 |
| 10 | SG | 11.6 / 20.8 / 31.7 | 38.9 / 55.7 / 68.6 |
|  | OG | 13.9 / 24.6 / 36.8 | 42.4 / 60.1 / 74.0 |
|  | rel. gain (\%) | $+61.6 /+60.8 /+47.2$ | $+10.4 /+9.7 /+7.6$ |
| 20 | SG | $13.0 / 22.9 / 35.2$ | $40.3 / 57.0 / 70.5$ |
|  | OG | 15.3 / 27.0 / 39.7 | 44.1 / 61.5 / 75.0 |
|  | rel. gain (\%) | $+77.9 /+76.5 /+58.8$ | $+14.8 /+12.2 /+9.0$ |
| 30 | SG | $14.6 / 25.2 / 37.9$ | $42.0 / 59.2 / 71.2$ |
|  | OG | 16.7 / 29.1 / 42.3 | 45.8 / 62.5 / 76.0 |
|  | rel. gain (\%) | $+94.2 /+90.2 /+69.2$ | $+19.3 /+14.1 /+10.5$ |

Table 4. Fine-tuning results of SuperGlue [42] (SG) and our method OmniGlue (OG) on Google Scanned Object (GSO) dataset. We use dozens of sneaker object instances to generate training data and test on all 17 GSO categories. We also show a relative gain compared with the zero-shot performance.

a further performance improvement of $2 \%$. Furthermore, OmniGlue consistently surpasses SuperGlue, achieving a relative gain of approximately $10 \%$ across all experiments. The results collectively demonstrate the applicability of OmniGlue in real-world scenarios as a versatile and generalizable method.

### 4.3. Ablation Study and Insights

We conduct a comprehensive ablation study on each proposed module, as detailed in Table 5. Please note that the numbers reported on the GSO dataset are based on a subset, encompassing half of all test cases, for rapid evaluation.

The effectiveness of each proposed technique. The results in Table 5 (1) highlight the effectiveness of our foundation model guidance, which enhances the generalization capability on out-of-domain data. Additionally, the third row of Table 5 (2) illustrates the impact of the position-guided attention, showcasing improvement in both in-domain and out-of-domain data. Furthermore, we conduct ablations with different approaches to disentangling keypoint positional features. The first two rows of Table 5 (2) demonstrate that performance degrades when either not using any positional features or applying the position-guidance only on selfattention (without positional guidance on cross-attention). This emphasizes the effectiveness of our position-guided attention in facilitating information propagation within both intra- and inter-image contexts. Besides, after removing the positional embeddings, the model shows better generalization even though the in-domain performance drops. This result implies that the inappropriate way that SuperGlue uses positional information limits its generalization.

The ways of incorporating DINO features. As shown in Table 5 (3), we explore different methods of incorporating DINOv2. The first involves merging DINO features and SuperPoint local descriptors. This integration is performed before the information propagation module using an MLP.

|  |  | In-domain <br> MegaDepth <br> P/R | Out-of-domain <br> Google Scanned Object |  |
| :---: | :---: | :---: | :---: | :---: |
|  |  |  | Hard <br> $@ 5 \% 10^{\circ} / 20^{\circ}$ | Easy <br> $@ 5^{\circ} / 10^{\circ} / 20^{\circ}$ |
| (0) | SuperGlue [42] | $67.2 / 68.3$ | $9.0 / 16.9 / 27.3$ | $40.4 / 60.5 / 76.6$ |
| (1) | only DINO-guide | $66.6 / 68.0$ | $10.0 / 18.7$ / 29.6 | 46.2 / 65.4 / 79.5 |
| (2) | only no pos. emb. - all <br> only no pos. emb. - cross <br> only pos. guidance | $60.5 / 58.1$ <br> $63.3 / 62.1$ <br> $69.2 / 73.9$ | $9.1 / 17.2 / 27.7$ <br> $9.3 / 17.0 / 28.0$ <br> $9.8 / 18.0 / 28.6$ | 43.5 / $63.2 / 78.2$ <br> $44.8 / 64.1 / 79.4$ <br> $46.4 / 66.6 / 80.2$ |
| (3) | (2) + DINO-SP-merge <br> (2) + DINO-guide-intra+inter | $62.6 / 65.6$ <br> $66.4 / 72.2$ | $7.8 / 14.9 / 24.9$ <br> $10.5 / 19.4 / 30.5$ | 42.5 / 61.3 / 75.4 <br> 47.1 / 66.8 / 80.8 |
| (4) | (2) + DINO-guide- 0.3 <br> (2) + DINO-guide- 0.4 <br> (2) + DINO-guide- 0.6 | $66.8 / 73.3$ <br> $66.8 / 73.1$ <br> $66.7 / 74.1$ | $10.3 / 19.3 / 30.8$ <br> $10.2 / 18.9 / 30.4$ <br> $10.2 / 19.1 / 30.3$ | $77.3 / 67.1 / 81.0$ <br> $47.2 / 66.9 / 80.8$ <br> $47.7 / 67.4 / 81.1$ |
| (5) | (2) + DINO-guide- 0.5 (full) | $66.2 / 74.1$ | $11.0 / 20.4 / 32.0$ | ![](https://cdn.mathpix.com/cropped/2024_05_26_2183d5c3927b6a360c8bg-08.jpg?height=40&width=158&top_left_y=730&top_left_x=1717) |

Table 5. Ablation study on (1) only with DINO guidance, (2) only with the disentangled keypoint representation variants, (3) DINO guidance variants analysis (based on (2) with position guidance), (4) DINO guidance threshold analysis, and (5) full model OmniGlue.

The experiment reveals a decline in performance, suggesting that the two features are not compatible, likely due to the coarse granularity of DINO. The manner in which these features can be effectively merged remains an open problem.

The second method entails applying DINOv2 guidance for constructing both intra and inter-image graphs, demonstrating diminished performance compared to (5). We hypothesize that the reason lies in the fact that intra-image information propagation (self-attention) requires a global context, particularly for distinguishing all keypoints in the feature space. Reducing connectivity on the intra-image graph adversely affects the global context, aligning with findings in the study of attention span in SuperGlue.

Details of foundation model guidance. We ablate the hyperparameter used to determine the number of source keypoint in a graph, as presented in Table 5 (4). The results indicate that selecting the top half of keypoints in the other image for building inter-image graphs is the optimal choice.

## 5. Conclusions and Future Work

We propose OmniGlue, the first learnable image matcher that is designed with generalization as a core principle. We introduce the broad visual knowledge of a foundation model, which guides the graph-building process. We identify the limitation of the previous descriptor-position entangled representation and present a novel attention module to deal with it. We demonstrate that OmniGlue outperforms prior work with better cross-domain generalization. Moreover, OmniGlue can also be easily adapted to a target domain with a limited amount of data collected for fine-tuning. For future work, it is also worth exploring how to leverage unannotated data in target domains to improve generalization. Both of better architectural designs and better data strategies can pave the way for a foundational matching model.

[1] V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk. Hpatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors. In Proc. CVPR, 2017. 1

[2] J. Barron, B. Mildenhall, D. Verbin, P. Srinivasan, and P. Hedman. Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. In Proc. ICCV, 2023. 1, 2

[3] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf: Speeded up robust features. In European Conference on Computer Vision, 2006. 1, 2

[4] C√©sar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, Jos√© Neira, Ian D. Reid, and John J. Leonard. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. IEEE Transactions on Robotics, 32:1309-1332, 2016. 1

[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv'e J'egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9630-9640, 2021. 3

[6] Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang Bai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning to match features with seeded graph matching network. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6281-6290, 2021. 2

[7] Hongkai Chen, Zixin Luo, Lei Zhou, Yurun Tian, Mingmin Zhen, Tian Fang, David N. R. McKinnon, Yanghai Tsin, and Long Quan. Aspanformer: Detector-free image matching with adaptive span transformer. In European Conference on Computer Vision, 2022. 2

[8] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie√üner. Scannet: Richlyannotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 1, $5,9,10$

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages $248-255$. Ieee, 2009. 1

[10] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 224-236, 2018. 1, 2, 3, 6, 7, 9, 10

[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth $16 x 16$ words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 9

[12] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Michael Hickman, Krista Reymann, Thomas Barlow McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. 2022 International Conference on Robotics and Automation (ICRA), pages 2553-2560, 2022. 1, 5, 9, 10

[13] Johan Edstedt, Ioannis Athanasiadis, M√•rten Wadenb√§ck, and Michael Felsberg. Dkm: Dense kernelized feature matching for geometry estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17765-17775, 2023. 1, 2, 3

[14] Johan Edstedt, Georg B√∂kman, M√•rten Wadenb√§ck, and Michael Felsberg. Dedode: Detect, don't describe - describe, don't detect for local feature matching. ArXiv, abs/2308.08479, 2023. 2

[15] Martin A. Fischler and Robert C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM, 24:381-395, 1981. 6

[16] Michael Goesele, Brian Curless, and Steven M Seitz. Multiview stereo revisited. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), volume 2, pages 2402-2409. IEEE, 2006. 1

[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9726-9735, 2019. 3

[18] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. 2023. 3

[19] Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engelhardt, Arjun Karpur, Karen Truong, Kyle Sargent, Stefan Popov, Andre Araujo, Ricardo Martin-Brualla, Kaushal Patel, Daniel
Vlasic, Vittorio Ferrari, Ameesh Makadia, Ce Liu, Yuanzhen Li, and Howard Zhou. Navi: Category-agnostic image collections with high-quality $3 \mathrm{~d}$ shape and pose annotations. 1,5 , 9,10

[20] Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, and Yuke Zhu. Few-view object reconstruction with unknown categories and camera poses. arXiv preprint arXiv:2212.04492, 2022. 1

[21] Hanwen Jiang, Zhenyu Jiang, Yue Zhao, and Qixing Huang. Leap: Liberate sparse-view 3d modeling from camera poses. arXiv preprint arXiv:2310.01410, 2023. 3

[22] Hanwen Jiang, Santhosh Kumar Ramakrishnan, and Kristen Grauman. Single-stage visual query localization in egocentric videos. arXiv preprint arXiv:2306.09324, 2023. 2, 3

[23] Zhenyu Jiang, Hanwen Jiang, and Yuke Zhu. Doduo: Learning dense visual correspondence from unsupervised semanticaware flow. arXiv preprint arXiv:2309.15110, 2023. 3

[24] Arjun Karpur, Guilherme Perrotta, Ricardo Martin-Brualla, Howard Zhou, and Andre Araujo. Lfm-3d: Learnable feature matching across wide baselines using 3d signals. In Proc. 3DV, 2024. 3, 9

[25] K. Li, M. Runz, M. Tang, L. Ma, C. Kong, T. Schmidt, I. Reid, L. Agapito, J. Straub, S. Lovegrove, and R. Newcombe. FroDO: From Detections to 3D Objects. In Proc. CVPR, 2020. 1,2

[26] Zhengqi Li and Noah Snavely. Megadepth: Learning singleview depth prediction from internet photos. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2041-2050, 2018. 1, 5

[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part $V$ 13, pages 740-755. Springer, 2014. 1

[28] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. LightGlue: Local Feature Matching at Light Speed. In Proc. ICCV, 2023. 1, 2, 5, 6, 7, 10

[29] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 92989309, 2023. 3

[30] David G. Lowe. Distinctive image features from scaleinvariant keypoints. International Journal of Computer Vision, $60: 91-110,2004.1,2,6,7,9,10$

[31] Jiayi Ma, Xingyu Jiang, Aoxiang Fan, Junjun Jiang, and Junchi Yan. Image matching from handcrafted to deep features: A survey. International Journal of Computer Vision, $129: 23-79,2020.1$

[32] B. Mildenhall, P. Srinivasan, M. Tancik, J. Barron, R. Ramamoorthi, and R. Ng. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In Proc. ECCV, 2020. 1, 2

[33] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han. Large-scale image retrieval with attentive deep local features. In Proceedings of the IEEE international conference on computer vision, pages 3456-3465, 2017. 2

[34] Yuki Ono, Eduard Trulls, Pascal V. Fua, and Kwang Moo Yi. Lf-net: Learning local features from images. In Neural Information Processing Systems, 2018. 2

[35] Maxime Oquab, Timoth'ee Darcet, Th√©o Moutakanni, Huy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ Howes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao Xu, Herv√© J√©gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. ArXiv, abs/2304.07193, 2023. $2,3,6,7,9,10$

[36] Jae-Hyun Park, Woo-Jeoung Nam, and Seong-Whan Lee. A two-stream symmetric network with bidirectional ensemble for aerial image matching. Remote Sensing, 12(3):465, 2020. $1,5,9$

[37] G. Potje, F. Cadar, A. Araujo, R. Martins, and E. Nascimento. Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 1

[38] Filip Radenoviƒá, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ond≈ôej Chum. Revisiting oxford and paris: Large-scale image retrieval benchmarking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5706-5715, 2018. 5
[39] J√©r√¥me Revaud, Philippe Weinzaepfel, C√©sar Roberto de Souza, No'e Pion, Gabriela Csurka, Yohann Cabon, and M. Humenberger. R2d2: Repeatable and reliable detector and descriptor. ArXiv, abs/1906.06195, 2019. 1, 2

[40] Barbara Roessle and Matthias Nie√üner. End2end multi-view feature matching with differentiable pose optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 477-487, 2023. 1

[41] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R. Bradski. Orb: An efficient alternative to sift or surf. 2011 International Conference on Computer Vision, pages 25642571, 2011. 2

[42] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4938-4947, 2020. 1, 2, 4, 5, 6, 7, 8, 9, 10

[43] Johannes L Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 41044113, 2016. 1,5

[44] Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343-348, 1967. 5

[45] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8918-8927, 2021. $1,2,5,6,7,10$

[46] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. arXiv preprint arXiv:2306.03881, 2023. 3

[47] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning accurate dense correspondences and when to trust them. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5710-5720, 2021. 2, 3, 6, 7, 10

[48] Prune Truong, Martin Danelljan, Radu Timofte, and Luc Van Gool. Pdc-net+: Enhanced probabilistic dense correspondence network. 2023. 1

[49] M. Tyszkiewicz, K.-K. Maninis, S. Popov, and V. Ferrari. RayTran: 3D pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers. In Proc. ECCV, 2022. 1, 2

[50] Michal J. Tyszkiewicz, P. Fua, and Eduard Trulls. Disk: Learning local features with policy gradient. ArXiv, abs/2006.13566, 2020. 2

[51] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. 2

[52] Yannick Verdie, Kwang Moo Yi, Pascal Fua, and Vincent Lepetit. TILDE: A Temporally Invariant Learned Detector. In Proc. CVPR, 2015. 1

[53] Qing Wang, Jiaming Zhang, Kailun Yang, Kunyu Peng, and Rainer Stiefelhagen. Matchformer: Interleaving attention in transformers for feature matching. In Asian Conference on Computer Vision, 2022. 2

[54] Shuzhe Wang, Juho Kannala, Marc Pollefeys, and Daniel Barath. Guiding local feature matching with surface curvature. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17981-17991, 2023. 3

[55] Fei Xue, Ignas Budvytis, and Roberto Cipolla. Sfd2: Semantic-guided feature detection and description. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5206-5216, 2023. 3

[56] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal
Fua. LIFT: Learned Invariant Feature Transform. In Proc. ECCV, 2016. 1

[57] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming Yang. A tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. ArXiv, abs/2305.15347, 2023. 2,3