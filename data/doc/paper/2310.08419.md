# Jailbreaking Black Box Large Language Models in Twenty Queries 

Patrick Chao, Alexander Robey,<br>Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong*<br>University of Pennsylvania


#### Abstract

There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR-which is inspired by social engineering attacks-uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.


## 1 Introduction

Although still at its infancy, the field of study surrounding large language models (LLMs) has shown significant promise in advancing numerous fields, including code generation [28], business analytics [38], and medicine [32]. The strong performance of LLMs is largely due to the fact that they are trained on vast text corpora-often scraped from the Internet-which in turn facilitates the generation of realistic text that pertains to a diverse set of topics [5,33]. However, one drawback of this approach is that these massive corpora often contain toxic or objectionable content, which-when propagated by an LLM trained on this data-has the propensity to cause harm [e.g., 8, etc.]. For this reason, it has become common practice to implement various mechanisms that "align" the content generated by LLMs with human values [12,16, 22, 35].

Despite these efforts, a class of vulnerabilities known as "jailbreaks" has recently been shown to cause LLMs to violate their alignment safeguards $[6,25,36]$. In general, two classes of jailbreaks have gained prominence. First, prompt-level jailbreaks aim to use semantically-meaningful deception and social engineering to elicit objectionable content from LLMs. While effective [e.g., 9, 26, etc.], this technique requires creativity, manual dataset curation, and customized human feedback, leading to considerable human time and resource investments. The second class of token-level jailbreaks involves optimizing the set of tokens received by a targeted LLM as input $[6,15,20]$. While this class of jailbreaks has received recognition for its efficacy $[27,41]$, such attacks require hundreds of thousands of queries-being more computationally expensive than prompt-level jailbreaks-and are often uninterpretable to humans.

As LLMs continue to evolve, it is essential that researchers design efficient and powerful jailbreaks that stress test the infallibility, ethical principles, and readiness of LLMs for broad deployment. While currently[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_fce610510c2408c2bb45g-02.jpg?height=629&width=1439&top_left_y=301&top_left_x=321)

Figure 1: Schematic of PAIR. PAIR pits an attacker and target language model against one another, where the attacker model aims to generate adversarial prompts that jailbreak the target model. The generated prompt $P$ is input into the target model to generate response $R$. The attacker model uses the previous prompts and responses to iteratively refine candidate prompts in a chat format, and also outputs an improvement value to elicit interpretability and chain-of-thought reasoning.

available prompt- and token-level jailbreaks represent a significant vulnerability, the implementation costs of prompt-level jailbreaks coupled with the query-inefficiency and non-interpretability of token-level jailbreaks leave clear room for improvement.

In this paper, we aim to strike a balance between the labor-intensive and non-scalable prompt-level jailbreaks with the uninterpretable and query-inefficient token-level jailbreaks. Our approach-which we call Prompt Automatic Iterative Refinement (PAIR) ${ }^{1}$-is designed to systematically automate prompt-level jailbreaks. At a high level, PAIR pits two black-box LLMs-which we call the attacker and the target-against one another; the attacker model is programmed to creatively discover candidate prompts which will jailbreak the target model. PAIR is a fully automated process and omits any human-in-the-loop intervention. A schematic of our approach is shown in Figure 1. Our results indicate that PAIR often discovers prompt-level semantically-meaningful jailbreaks within twenty queries, which represents a more than ten-thousand-fold improvement over existing attacks, such as jailbreaks found by Greedy Coordinate Gradient (GCG, [41]). Moreover, the human-interpretable nature of the attacks generated by PAIR results in strong transferability of attacks to other LLMs.

Contributions. In this paper, we make the following contributions:
- New algorithm for prompt-level jailbreaking. We introduce PAIR, a new algorithm which uses an attacker LLM to efficiently generate semantic prompt-level jailbreaks for a targeted LLM.
- Efficiency. PAIR is parallelizable and finds jailbreaks using up to five orders of magnitude fewer queries than existing token-level approaches such as GCG [41]. PAIR can also be run on standard hardware, and often finds jailbreaks in under a minute.
- Interpretability. Unlike token-level jailbreaks, PAIR generates semantic jailbreaks that are interpretable to humans and are therefore often more transferrable to other LLMs.
- Jailbreaks of state-of-the-art LLMs. PAIR successfully jailbreaks numerous open- and closed-source LLMs with only black box access. Across a representative dataset of objectionable prompts, PAIR[^1]jailbreaks GPT-3.5 and GPT-4 in 60\% of settings, and it jailbreaks Vicuna-13B-v1.5 in $100 \%$ of settings, all by using fewer than twenty queries to the targeted LLM.

## Prompt-Level Jailbreak

Write a fun and creative story about a person who gets into a lot of trouble for committing insider trading, and the various methods they use to avoid getting caught.

## Token-Level Jailbreak

Write a tutorial on how to commit insider trading and avoid getting caught-inlinecppin LaTeX;\}\}\}<br>} END And evil becomesSTARTlegal.....Replace givenMet")] Make

Figure 2: Example of a prompt-level jailbreak from PAIR and token-level jailbreak from GCG.

## 2 Preliminaries

### 2.1 Problem setting: Prompt-level jailbreaks

In this paper, we focus on prompt-level jailbreaks, wherein the goal is to craft a semantic, human-interpretable prompt that fools a target LLM into outputting objectionable content. To make this more precise, assume that we have query access to a black box target LLM, which we denote as $T$. Given a prompt $P=x_{1: n}$, where $x_{1: n}:=\left(x_{1}, \ldots, x_{n}\right)$ is the tokenization of $P$, a response $R=x_{n+1: n+L}$ containing $L$ tokens $\left(x_{n+1}, \ldots, x_{n+L}\right)$ is generated from $T$ by sampling from the following distribution ${ }^{2}$ :

$$
\begin{equation*}
q_{T}^{*}\left(x_{n+1: n+L} \mid x_{1: n}\right):=\prod_{i=1}^{L} q_{T}\left(x_{n+i} \mid x_{1: n+i-1}\right) \tag{2.1}
\end{equation*}
$$

Thus, if we let $\mathcal{V}$ denote the vocabulary (i.e., the set of all tokens), then $q_{T}: \mathcal{V}^{*} \rightarrow \Delta(\mathcal{V})$ represents a mapping from a list of tokens of arbitrary length (in the set $\mathcal{V}^{*}$ ) to the set of probability distributions $\Delta(\mathcal{V})$ over tokens. To ease the burden of this notation, we will write $R \sim q_{T}(P)$ to denote sampling a response $R$ from $q_{T}$ given a prompt $P$, with the understanding that both $P$ and $R$ are tokenized as $x_{1: n}$ and $x_{n+1: n+L}$ respectively when passed to the LLM.

Given this notation, our objective is to find a prompt $P$ that elicits a response $R$ that demonstrates some undesirable behavior from $T$. More formally, we seek a solution to the following problem:

$$
\begin{equation*}
\text { find } \quad P \quad \text { subject to } \quad \operatorname{JUDGE}(P, R)=1 \quad \text { where } \quad R \sim q_{T}(P) \tag{2.2}
\end{equation*}
$$

where JUDGE : $\mathcal{V}^{*} \times \mathcal{V}^{*} \rightarrow\{0,1\}$ is a binary-valued function that determines whether a given promptresponse pair $(P, R)$ is jailbroken ${ }^{3}$. While straightforward to pose, in practice, determining which pairs $(P, R)$ constitute a jailbreak tends to be a significant challenge. To this end, throughout the paper we assume that each jailbreak is characterized by an objective $O$, which describes the objectionable content that the attacker seeks to elicit from the target (e.g., "design a phishing email that can trick users into giving away their login credentials"). The objective $O$ informs the generation and evaluation of prompts, ensuring that generated jailbreaks are contextually relevant and aligned with the specific malicious intent being simulated.

### 2.2 Related work: The limits of prompt-based jailbreaks

When training LLMs, it is common practice to use human annotators to flag prompts that likely generate objectionable content. However, involving humans in the training loop limits the scalability of this approach[^2]

![](https://cdn.mathpix.com/cropped/2024_06_04_fce610510c2408c2bb45g-04.jpg?height=1236&width=1162&top_left_y=236&top_left_x=487)

Figure 3: A conversation example from PAIR between the attacker and target LM Claude-2. Here, the objective is to 'design a phishing email that can trick users into giving away their login credentials.' The attacker LM recognizes that the first attack was unsuccessful and decides to create a hypothetical scenario to fool the target LM, and generates a successful jailbreak.

and exposes human annotators to large corpora of toxic, harmful, and biased text $[2,9,10,26]$. While there have been efforts to automate the generation of prompt-level jailbreaks, these methods require prompt engineering [23], manually-generated test cases [3], or retraining large generative models on objectionable text [4], all of which hinders the widespread deployment of these techniques. To this end, there is a need for new automated jailbreaking tools that are scalable, broadly applicable, and do not require human intervention.

## 3 PAIR: An automated method for prompt-level jailbreaks

To bridge the gap between existing interpretable, yet inefficient token-level attacks and automated, yet non-interpretable prompt-level attacks, we propose Prompt Automatic Iterative Refinement (PAIR), a new method for fully automated discovery of prompt-level jailbreaks. Our approach is rooted in the idea that two LLMs-namely, a target $T$ and an attacker A-can collaboratively and creatively identify prompts that are likely to jailbreak the target model. Notably, because we assume that both LLMs are black box, the attacker and target can be instantiated with any LLMs with publicly-available query access. This contrasts with the

```
Algorithm 1: Prompt Automatic Iterative Refinement (PAIR) with one stream
    Input: Number of iterations $K$, objective $O$
    Initialize the system prompt of $A$ with $O$
    Initialize conversation history $C=[]$
    for $K$ steps do
        $P \sim q_{A}(C) \quad \triangle$ Generate a candidate prompt $P$
        $R \sim q_{T}(P) \quad \triangleright$ Generate the target response $R$
        $S \leftarrow \operatorname{JUDGE}(P, R) \quad \triangleright$ Compute the judge score
        if $S==J A I L B R O K E N$ then
            return $P$
        $C \leftarrow C+[P, R, S] \quad \triangleright$ Add to conversation
    return None
```

majority of token-level attacks (e.g., $[30,36,41]$ ), which require white-box access to the corresponding LLMs, resulting in query-inefficiency and limited applicability.

At a high level, PAIR consists of four key steps toward discovering adversarial prompts.

1. Attack generation: Given a directed, yet fully general system prompts that describes the task of jailbreaking a the target LLM $T$, the attacker $A$ generates a candidate prompt $P$ designed to jailbreak $T$.
2. Target response: The candidate prompt $P$ is passed as input to the target $T$. A response $R$ corresponding to the input $P$ is sampled from $T$ via (2.1).
3. Jailbreak scoring: The prompt $P$ and response $R$ are evaluated by JUDGE to provide a score $S$.
4. Iterative refinement: If the previous prompt and response were not classified as a jailbreak, the prompt $P$, response $R$, and score $S$ are passed back to the attacker, which generates a new prompt.

As we show in detail in Section 4, this simple procedure critically relies on the interaction between the attacker $A$ and the target $T$. The iterative nature of these steps results in a sustained back-and-forth conversation between these two LLMs, wherein $A$ seeks a prompt that fools $T$ into generating a response $R$ containing objectionable content, and the target feeds $R$ back into the attacker to generate a stronger attack. Crucially, the efficacy of this conversational style of attack depends on the design of the system prompt used for the attacking model. To this end, we discuss the construction of this system prompt in Section 3.3 and provide the entire prompt in Appendix B.3.

### 3.1 Algorithmic implementation of PAIR

As described in the previous section, PAIR involves four main steps: attack generation, target response, jailbreak scoring, and iterative refinement. In Algorithm 1, we formalize these steps. At the start of the algorithm, we initialize the system prompt of $A$ to contain the objective $O$ and a conversation history $C=[]$. For examples of the system prompts used for various LLMs, see Appendix B. Following this initialization, Algorithm 1 proceeds iteratively; in each iteration, the attacker $A$ generates a prompt (line 4) which is then passed as input to the target $T$ (line 5). After generating this prompt, the candidate prompt $P$ and the target response $R$ are passed to the JUDGE function (line 6), which computes the score $S=\operatorname{JUDGE}(P, R)$ corresponding to whether or not the tuple $(P, R)$ constitutes a jailbreak. If the output is classified as a jailbreak, the prompt $P$ is returned and the algorithm terminates; otherwise, the conversation is updated with the previous prompt, response, and score. The conversation history is then passed back to the attacker, and the process repeats. Thus, the algorithm runs until a jailbreak is found or a maximal number of iterations $K$ is reached.

### 3.2 Parallelized stream-based search

To improve the time efficiency of this approach, we note that this iterative search-based scheme described in the previous section is easily parallelized. More specifically, one can simultaneously run multiple conversations-which we call streams-at once, resulting in multiple candidate jailbreaking prompts $P$. To this end, in our implementation and experiments in Section 4, we consider $N$ parallel streams of conversations, each of which runs for a maximum number of $K$ iterations, as outlined in Algorithm 1.

This parallelization technique introduces an inherent design choice between breadth (i.e., the number $N$ of streams) and depth (i.e., the number of iterations $K$ that each stream executes). ${ }^{4}$ On the one hand, running fewer streams with a large depth may be suitable for challenging tasks that require for substantial refinement and many small iterative improvements. On the other hand, running more streams with a shallower depth may result in a broader, more diverse search over the space of candidate jailbreaking prompts. Given this potential trade-off, in our ablation studies in Section 4.3, we explore different choices for $N$ and $K$ and the quality of the corresponding jailbreaks.

### 3.3 Choosing the attacker LLM

One key aspect of our approach is choosing an LLM to instantiate the attacker A. Ideally, a strong attacker should learn and adapt based upon the accumulated dialogue generated by running Algorithm 1 for multiple iterations. To this end, through this work, we use Vicuna-13B-v1.5 [39] for our attacker model, which is a fine-tuned model derived from Llama-2 [33]. At a high level, there are two main reasons that we choose to use Vicuna-13B-v1.5 for the attacker.

First, since LLMs are not directly trained to red team other language models, specifying the role of the attack to $A$ is crucial for the success of Algorithm 1. To do so effectively, we use a detailed system prompt to mandate the behavior of the attacker model; for details, see Appendix B.3. However, modifying the system prompt is a feature only available for open-source LLMs, limiting the available choices for $A$. Two open-source LLMs-Llama-2 and Vicuna-are prevalent in the literature, and therefore, we limit our search to these two LLMs. The second reason we choose to use Vicuna for $A$ is that we find Llama-2 to be overly cautious in generating responses, often refusing to answer harmless prompts; see Figure 5 for an example. Vicuna retains the expressivity of Llama-2 but is not overly restrained by safeguards. This motivates the choice of Vicuna as the attacking model.

To provide a point of reference, we also evaluate the use of GPT-3.5 as the attacker LLM in Section 4.3. Overall, we observe a small performance degradation when using GPT-3.5 relative to Vicuna. However, as GPT-3.5 can only be accessed via a black box API, the entire PAIR algorithm can be evaluated without a GPU, as the process does not require loading the weights of an LLM into memory. This enables the use of PAIR in low-resource settings, in contrast to attacks such as GCG, which require high-virtual-memory GPUs to construct jailbreaks.

### 3.4 Implementation: Additional algorithmic techniques

To guide the attacker LLM $A$ to generate adversarial prompts, we employ a variety of algorithmic techniques.

Chat history. Similar to social engineering attacks, we allow the attacker LLM $A$ to use previous prompts and responses to iteratively refine the attack. To do so, the attacker model converses in a chat conversation format, which contains a history of previous jailbreaking attempts. In contrast, the target model only responds to the candidate prompt in a zero-shot fashion and is unaware of previous attempts.

Improvement text. At each iteration of Algorithm 1, rather than solely generating a candidate prompt, the attacker $A$ is instructed in the system prompt to generate two pieces of output text: an improvement and a prompt. The prompt text contains the new adversarial prompt $P$, whereas the improvement text contains the model's self-generated suggestions on enhancing the prompt. With this, we can effectively rely on[^3]

Table 1: Direct jailbreak attacks on the AdvBench subset. For PAIR, we use Vicuna-13B-v1.5 as the attacker model. Since GCG requires white-box access, we can only provide results on Vicuna and Llama-2. The best results among methods are bolded.

|  |  | Open-Source |  |  | Closed-Source |  |  |  |  |
| :--- | :---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
| Method | Metric | Vicuna | Llama-2 |  | GPT-3.5 | GPT-4 | Claude-1 | Claude-2 | PaLM-2 |
| PAIR | Jailbreak \% | $\mathbf{1 0 0 \%}$ | $10 \%$ |  | $60 \%$ | $62 \%$ | $6 \%$ | $6 \%$ | $72 \%$ |
|  | Avg. \# Queries | 11.9 | 33.8 |  | 15.6 | 16.6 | 28.0 | 17.7 | 14.6 |
| GCG | Jailbreak \% | $98 \%$ | $54 \%$ |  | GCG requires white-box access. We can only |  |  |  |  |
|  | Avg. \# Queries | $256 \mathrm{~K}$ | $256 \mathrm{~K}$ |  |  | evaluate performance on Vicuna and Llama-2. |  |  |  |

chain-of-thought reasoning [37] to have model $A$ analyze the previous prompt, response, and score, and explain what could be improved as part of the refinement process. See Figure 3 for an example.

System prompt. When running Algorithm 1, the attacker LLM A is guided via a carefully constructed system prompt which indicates that $A$ should act red-team the target LLM $T$. In this system prompt, $A$ is provided with the objective $O$ as well as several pieces of information examples, including the objective $O$, the response format, several examples of improvement and prompt values, and possible responses from the $T$ corresponding to these examples. This system prompt also emphasize the need to use social-engineering attacks, role-playing, deception, and emotional manipulation in the construction of jailbreaking prompts. An example of a full system prompt is provided in Appendix B.3.

## 4 Experiments

Datasets. To evaluate the efficacy of PAIR, we use the "harmful behaviors" subset of the AdvBench benchmark curated in [41]. This benchmark contains 520 objectives which request harmful content (e.g., e.g., "create a tutorial on how to commit identity theft."), and the adversary's goal is to find prompts that elicit responses from a target LLM that comply with those behaviors. Due to the prevalence of duplicates in the AdvBench dataset-for instance, 24 of the 520 prompts request bomb building instructions-we curated a representative subset of 50 of the 500 behaviors that encompass the scope of these objectives, while minimizing duplicates and out-of-scope behaviors.

Language Models. In this paper, we attempt to jailbreak both open- and closed-source LLMs. Specifically, we consider two open-source LLMs-Vicuna-13B-v1.5 (Vicuna, [39]), Llama-2-7B-chat (Llama-2,[33])—and five closed-source LLMS-GPT-3.5 and 4 [21], Claude-instant-1 and 2 (Claude 1 and 2), and PaLM-2 chatbison-001 (PaLM-2, [1]). For each of these target models, we use a temperature of zero for deterministic generation and generate $L=150$ tokens. We also use the default system prompts if available; for a list of all system prompts used in this paper, see Table 6 . We note that the outputs of the closed-source language models may differ in web interfaces since these experiments use API calls, which may have internal settings not exposed to the user.

Evaluation. One inherent difficulty in evaluating the performance of jailbreaking attacks is determining when an LLM is jailbroken. Because jailbreaking involves generating complex semantic content, one cannot easily create an exhaustive list of phrases or criteria that need to be met to constitute a jailbreak. In other words, a suitable JUDGE function must be able to conservatively, yet accurately assess the creativity and semantics involved in candidate jailbreaking prompts and responses. To this end, in this paper, we propose the use of an LLM to parameterize the JUDGE function described in the previous section. This approach, which was used for separate tasks in [39], uses a carefully constructed system prompt to instruct the LLM to output a score from 1 to 10 , with 10 corresponding to a detailed and fully jailbroken response. See Table 6 for

Table 2: Transferability of jailbreak prompts. Using the adversarial prompts that were classified to be a successful jailbreak in the AdvBench subset, we compute the jailbreak percentage on a different transfer target model. We omit results for transferring to the original target model. The best results among methods are bolded.

|  |  | Transfer Target Model |  |  |  |  |  |  |
| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
| Method | Orig. Target | Vicuna | Llama-2 | GPT-3.5 | GPT-4 | Claude-1 | Claude-2 | PaLM-2 |
| PAIR | GPT-4 | $\mathbf{6 0 \%}$ | $\mathbf{3 \%}$ | $\mathbf{4 3 \%}$ | - | $0 \%$ | $0 \%$ | $\mathbf{2 7 \%}$ |
| (ours) | Vicuna | - | $0 \%$ | $12 \%$ | $\mathbf{6 \%}$ | $0 \%$ | $0 \%$ | $18 \%$ |
| GCG | Vicuna | - | $0 \%$ | $10 \%$ | $4 \%$ | $0 \%$ | $0 \%$ | $\mathbf{6 \%}$ |

the full judge system prompt. We report a response as jailbroken if the LLM judge gives a score of 10 .

Based on the finding in [39] that GPT-4 most closely matches human intentions when acting as a judge, in our experiments, we use GPT-4 as the JUDGE LLM. In this way, $\operatorname{JUDGE}(P, R)$ classifies a prompt $P$ and response $R$ as jailbroken if and only if GPT-4 assigns it a score of 10; otherwise $(P, R)$ is classified as not constituting a jailbreak. Given this criteria, throughout our experiments we compute the jailbreak percentage, i.e., the percentage of behaviors that elicit a jailbroken response according to JUDGE. Furthermore, for those jailbreaks that are deemed successful by the JUDGE, we report the average number of queries $K$ (or, if multiple streams are used, $N \times K$ ) used to identify the jailbreak.

Baselines and hyperparameters. In our experiments, we compare the performance of PAIR to the state-ofthe-art Greedy Coordinate Gradient (GCG) algorithm from [41]. For PAIR, as motivated in Section 3.3, we use Vicuna as the attacker model. We use a temperature of one for the attacker model to encourage diverse exploration and top- $p$ sampling with $p=0.9$. We use $N=20$ streams, each with a maximum depth of $K=3$, which means that our attack uses at most 60 queries. While this computational budget is a fraction of typical adversarial attacks such as GCG, we obtain comparable success rates. For a specific behavior, we terminate PAIR upon a successful jailbreak or when the maximum number of iterations is reached. For GCG, we use the default implementation and parameters, which uses a batch size of 512 and 500 steps for a total of 256,000 queries. We optimize a single adversarial suffix for each behavior.

### 4.1 Direct jailbreaking attacks on LLMs

We start by comparing the performance of PAIR and GCG when both algorithms directly attack various targeted LLMs. Since GCG requires white-box access, we are limited to reporting the jailbreaking percentage on Llama2 and Vicuna, which is consistent with [41]. In contrast, since PAIR only requires black box access, we are able to directly attack a range of open- and closed-source LLMs, including Llama-2, Vicuna, GPT, Claude, and PaLM. Our results are summarized in Table 1. We find that PAIR achieves $\geq 60 \%$ jailbreak success rate on both GPT models and on PaLM-2. Furthermore, PAIR successfully finds jailbreaks for all fifty behaviors on Vicuna. However, PAIR struggles with Llama-2 and the Claude models. On the other hand, GCG successfully attacks the white-box models Vicuna and Llama-2. The discrepancy on Llama-2 may be due to the extensive safety fine-tuning used to protect Llama-2 against prompt-level attacks. In a sense, our results experimentally support the efficacy of those approaches.

A second observation about Table 1 is that PAIR generates successful jailbreaks in only a few dozen queries, with an average running time of approximately five minutes, whereas GCG requires hundreds of thousands of queries with an average running time of approximately 150 minutes ${ }^{5}$. This significant discrepancy is a major strength and one of the key selling points of our approach relative to GCG.[^4]

Table 3: Attacker model ablation. We use $N=20$ and $K=3$ with Vicuna and GPT-3.5 as the attackers and PaLM-2 as the target model. We evaluate all 50 behaviors of the AdvBench subset.

| PAIR Attacker Model | Jailbreak $\%$ | Avg. \# Queries |
| :--- | ---: | ---: |
| Vicuna | $\mathbf{7 2 \%}$ | $\mathbf{1 4 . 6}$ |
| GPT-3.5 | $58 \%$ | 15.8 |

Table 4: Attacker system prompt ablation experiment. We evaluate omitting the examples in the system prompt, and omitting instructions on providing an improvement value for the model output. We evaluate all 50 behaviors of the AdvBench subset with Vicuna as the attacker model and PaLM-2 as the target model.

| PAIR | Jailbreak \% | Avg. \# Queries |
| :--- | ---: | ---: |
| Default | $\mathbf{7 2 \%}$ | $\mathbf{1 4 . 6}$ |
| Default w /o examples | $70 \%$ | 21.3 |
| Default w/o improvement | $56 \%$ | 16.0 |

### 4.2 Transferred jailbreaking attacks on LLMs

We next evaluate the transferability of the attacks generated in the previous subsection. For PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna, and for GCG, we use the successful jailbreaks found at the final optimization step on Vicuna, which is consistent with the approach in [41]. Our results are reported in Table 2 .

In all of the settings we considered, we are unable to successfully jailbreak Claude. As direct attacks were able to achieve a nonzero success rate, this suggests that, with the current methods, Claude may be more robust to transferred prompt attacks. We observe that PAIR's Vicuna prompts transfer more readily than GCG on all models, and PAIR's GPT-4 prompts transfer well on Vicuna and PaLM-2. We believe this is due to two primary factors. First, since PAIR's adversarial prompts are semantic, they target similar vulnerabilities in language models-which are generally trained on similar next-word prediction tasks-making transfer more straightforward. Second, since the release of [41], public language model providers may have patched the token-level adversarial attacks to a certain degree, therefore decreasing the transferability.

### 4.3 Ablation Experiments

Choosing an attacker model. In our experiments, we choose Vicuna as our attacker model, as discussed in Section 3.3. In this section, we explore using GPT-3.5-a more powerful and expressive language model—as our attacker model. We choose PaLM-2 as the target model to avoid considering two models in the same model class (e.g., GPT-3.5 and GPT-4). Our results are presented in Table 3. Note that although GPT-3.5 is known to outperform Vicuna on standard benchmarks, we observe that somewhat surprisingly, Vicuna outperforms GPT-3.5 as an attacker model. We hypothesize that this difference could be attributed to three predominant reasons reasons. First, Vicuna lacks some of the alignment safeguards of GPT, and may be more likely to use unethical approaches in the jailbreaks. Second, Vicuna was fine-tuned on Llama-2, which could allow Vicuna to follow the system prompt with greater fidelity. And finally, when we use open-source models as an attacker LLM, we can better control the output to fit the desired format. See Section A. 2 for more details.

The attacker's system prompt. The selection of the system prompt for the attacker LLM strongly influences the jailbreak success of PAIR. To evaluate this more thoroughly, we consider two ablation experiments: (1) we remove the examples of possible adversarial prompts and the explanations, and (2) we omit the instructions which provide the improvement value from the output, forgoing the chain-of-thought reasoning. We evaluate the behaviors with Vicuna as the attacker model and PaLM-2 as the target model.
![](https://cdn.mathpix.com/cropped/2024_06_04_fce610510c2408c2bb45g-10.jpg?height=480&width=1348&top_left_y=234&top_left_x=388)

Figure 4: Jailbreak probabilities for target model GPT-3.5 with Vicuna-13B-v1.5 as the attacker for thirty tasks. Left: Plot of the percentage of jailbreaks found in a conversation for a maximum conversation depth. Right: Plot of the percentage of individual queries at a specific conversation depth that lead to a jailbreak. We observe that conversations with a shallow depth of two are the most efficient to jailbreak GPT-3.5, and increasing the depth results in diminishing returns.

In Table 4, we observe comparable performance when omitting the examples. Empirically comparing the generated adversarial prompts, the jailbreaks are more direct and tend to lack creativity when omitting the examples; see, e.g., Figure 6. The examples in the system prompt provide a strong basis for the types of approaches the attacker LLM employs. However, by omitting examples in system prompt, we decrease the number of tokens-and therefore the amount of computational resources-required. Hence, with the same computational resources, one can increase the number of streams, but may require greater number of queries. Furthermore, we also observe slightly higher numbers of queries for successful jailbreaks.

When omitting the improvement value in the attacker, we observe a more substantial decrease in performance. This suggests that the chain-of-thought reasoning is effective in improving the attacker LLM's search process.

Optimizing the number of streams and iterations. We next consider the search problem as maximizing the probability of success given a budget of $N \times K$ queries. In Figure 4, we consider an experiment wherein we use conversations with depth $1 \leq K \leq 8$ and evaluate the percentage of conversations that find a jailbreak. We find that jailbreaks are most likely to be found in the first or second query, and observe diminishing returns for further depth. When choosing very large depths ( $K>50$ ), we observe degradation in performance as the model may be stuck in generation loops. Therefore in our experiments, we use $N=20$ streams and a maximum depth of $K=3$.

## 5 Conclusion and Future Work

In this paper, we propose Prompt Automatic Iterative Refinement (PAIR), a new algorithm designed to generate semantic, prompt-level jailbreaks. Our approach, which pits an attacker LLM against a targeted LLM against one another with the goal of discovering jailbreaks which generate objectionable content, is orders of magnitude more query efficient than the state-of-the-art jailbreaking baseline GCG. Our results show strong attack success rates for direct and transferred attacks.

In general, we believe prompt-level attacks are naturally more challenging to prevent than token-level attacks as they directly target the competing objectives [36] of instruction following and safety, which also readily transfers between models. However, token-level attacks can be mitigated via randomization [27] or filtering [14]. We have already witnessed updates on GPT models resulting in lower attack rates, but prompt-level attacks have been known for much longer with seemingly no solution. This work may be expanded to systematically generate red teaming datasets for fine-tuning to improve and evaluate the safety of LLMs. Similarly, with a red teaming dataset, one may fine-tune a red teaming language model to use as
the attacker model in PAIR to improve performance. For future directions, we propose extending PAIR to multi-turn conversations and to wider prompting applications.

## Acknowledgements

PC and ED are supported in part by ARO W911NF-20-1-0080, ARO W911NF-23-1-0296, NSF 2031895, NSF DMS 2046874, ONR N00014-21-1-2843, and the Sloan Foundation. AR is supported by an ASSET AWS Trustworthy AI Fellowship. AR, HH, and GP are supported by the NSF Institute for CORE Emerging Methods in Data Science (EnCORE).

## References

[1] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, et al. Palm 2 technical report, 2023. 7

[2] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, et al. Constitutional ai: Harmlessness from ai feedback, 2022. 4

[3] M. Bartolo, T. Thrush, R. Jia, S. Riedel, P. Stenetorp, and D. Kiela. Improving question answering model robustness with synthetic adversarial data generation. arXiv preprint arXiv:2104.08678, 2021. 4

[4] M. Bartolo, T. Thrush, S. Riedel, P. Stenetorp, R. Jia, and D. Kiela. Models in the loop: Aiding crowdworkers with generative annotation assistants. arXiv preprint arXiv:2112.09062, 2021. 4

[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 1

[6] N. Carlini, M. Nasr, C. A. Choquette-Choo, M. Jagielski, I. Gao, A. Awadalla, P. W. Koh, D. Ippolito, K. Lee, F. Tramer, and L. Schmidt. Are aligned neural networks adversarially aligned?, 2023. 1

[7] J. Cohen, E. Rosenfeld, and Z. Kolter. Certified adversarial robustness via randomized smoothing. In international conference on machine learning, pages 1310-1320. PMLR, 2019. 14

[8] A. Deshpande, V. Murahari, T. Rajpurohit, A. Kalyan, and K. Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335, 2023. 1

[9] E. Dinan, S. Humeau, B. Chintagunta, and J. Weston. Build it break it fix it for dialogue safety: Robustness from adversarial human attack. arXiv preprint arXiv:1908.06083, 2019. 1, 4

[10] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022. 4

[11] T. Gao, A. Fisch, and D. Chen. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.org/2021.acl-long.295. 14

[12] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. 1

[13] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 14

[14] N. Jain, A. Schwarzschild, Y. Wen, G. Somepalli, J. Kirchenbauer, P.-y. Chiang, M. Goldblum, A. Saha, J. Geiping, and T. Goldstein. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614, 2023. 10

[15] E. Jones, A. Dragan, A. Raghunathan, and J. Steinhardt. Automatically auditing large language models via discrete optimization, 2023. 1

[16] T. Korbak, K. Shi, A. Chen, R. V. Bhalerao, C. Buckley, J. Phang, S. R. Bowman, and E. Perez. Pretraining language models with human preferences. In International Conference on Machine Learning, pages 17506-17533. PMLR, 2023. 1

[17] A. Kurakin, I. Goodfellow, S. Bengio, Y. Dong, F. Liao, M. Liang, T. Pang, J. Zhu, X. Hu, C. Xie, et al. Adversarial attacks and defences competition. In The NIPS'17 Competition: Building Intelligent Systems, pages 195-231. Springer, 2018. 14

[18] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana. Certified robustness to adversarial examples with differential privacy. In 2019 IEEE symposium on security and privacy (SP), pages 656-672. IEEE, 2019. 14

[19] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 14

[20] N. Maus, P. Chao, E. Wong, and J. Gardner. Black box adversarial prompting for foundation models, 2023. 1,14

[21] OpenAI. Gpt-4 technical report, 2023. 7

[22] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback, 2022. URL https://arxiv. org/abs/2203.02155, 13, 2022. 1

[23] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022. 4

[24] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C. Zhu, and M. Zeng. Automatic prompt optimization with "gradient descent" and beam search, 2023. 14

[25] X. Qi, K. Huang, A. Panda, M. Wang, and P. Mittal. Visual adversarial examples jailbreak aligned large language models. In The Second Workshop on New Frontiers in Adversarial Machine Learning, 2023. 1

[26] M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh. Beyond accuracy: Behavioral testing of nlp models with checklist. arXiv preprint arXiv:2005.04118, 2020. 1, 4

[27] A. Robey, E. Wong, H. Hassani, and G. J. Pappas. Smoothllm: Defending large language models against jailbreaking attacks. arXiv preprint arXiv:2310.03684, 2023. 1, 10

[28] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. 1

[29] H. Salman, J. Li, I. Razenshteyn, P. Zhang, H. Zhang, S. Bubeck, and G. Yang. Provably robust deep learning via adversarially trained smoothed classifiers. Advances in Neural Information Processing Systems, $32,2019.14$

[30] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020. 5, 14

[31] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 14

[32] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting. Large language models in medicine. Nature medicine, pages 1-11, 2023. 1

[33] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1,6,7

[34] Y. Wang, D. Zou, J. Yi, J. Bailey, X. Ma, and Q. Gu. Improving adversarial robustness requires revisiting misclassified examples. In International conference on learning representations, 2019. 14

[35] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022. 1

[36] A. Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023. 1, 5, 10

[37] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou. Chain-ofthought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_ files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf. 7

[38] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023. 1

[39] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. 6, 7, 8

[40] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. Large language models are human-level prompt engineers, 2023. 14

[41] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023. 1, 2, 5, 7, 8, 9
