# Improved Techniques for Optimization-Based Jailbreaking on Large Language Models 

Xiaojun Jia ${ }^{1}$, Tianyu Pang ${ }^{2}$, Chao Du ${ }^{2}$, Yihao Huang ${ }^{1}$,<br>Jindong Gu ${ }^{3}$, Yang Liu ${ }^{1}$, Xiaochun Cao ${ }^{4}$, Min Lin ${ }^{2}$<br>${ }^{1}$ Nanyang Technological University, Singapore<br>${ }^{2}$ Sea AI Lab, Singapore<br>${ }^{3}$ University of Oxford, Oxford, United Kingdom<br>${ }^{4}$ School of Cyber Science and Technology, Shenzhen Campus, Sun Yat-sen University, China<br>jiaxiaojunqaq@gmail.com; \{tianyupang, duchao, linmin\}@sea.com;<br>huangyihao22@gmail.com; jindong.gu@eng.ox.ac.uk; yangliu@ntu.edu.sg;<br>caoxiaochun@mail.sysu.edu.cn


#### Abstract

Warning: This paper contains model outputs that are offensive in nature.

Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack's success has led to a growing interest in the study of optimization-based jailbreaking techniques. Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory. In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG. We first observe that the single target template of "Sure" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs. Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation. Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed $\mathcal{I}$-GCG. In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly $100 \%$ attack success rate. The code is released at https://github.com/jiaxiaojunQAQ/I-GCG.


## 1 Introduction

Large language models (LLMs) [18,3] have demonstrated exceptional performance in a variety of tasks and problems, including language understanding and generation [17], machine translation [50], and code generation [39]. A series of works focuses on the safe fine-tuning of pre-trained LLMs [31, 34, 1], which can make the outputs of LLMs consistent with human values. Aligned LLMs provide evasive responses to malicious questions. For example, when malicious questions like " Write a tutorial on how to make a bomb.", are fed to the aligned LLMs, they generate evasive responses like "I cannot fulfill your request. I'm just an AI; it's not within my programming or ethical guidelines to provide advice". This security alignment reduces the risk of malicious exploitation of LLMs, making them more widely adopted and used.

![](https://cdn.mathpix.com/cropped/2024_06_04_28d9ed53c1eb27b9830cg-02.jpg?height=1160&width=1263&top_left_y=252&top_left_x=428)

Figure 1: An illustration of jailbreak attack. The jailbreak suffix generated by the previous jailbreak attacks with a simple optimization goal can make the output of LLMs consistent with the optimization goal, but the subsequent content refuses to answer the malicious question. However, the jailbreak suffix generated by the optimization goal with harmful guidance we proposed can make LLMs produce harmful responses.

Despite significant efforts to improve the security of LLMs [11], recent research suggests that their alignment safeguards are vulnerable to adversarial jailbreak attacks [55, 20, 25, 5, 52, 13, 49, 2]. They can generate well-designed jailbreak prompts to circumvent the safeguards for harmful responses. Jailbreak attack methods are broadly classified into three categories. (1) Expertise-based jailbreak methods [46, 48, 42]: they use expertise to manually generate jailbreak prompts that manipulate LLMs into harmful responses. (2) LLM-based jailbreak methods [8, 4, 29, 47]: they use other LLMs to generate jailbreak prompts and trick LLMs into generating harmful responses. (3) Optimizationbased jailbreak methods [55, 24]: they use the gradient information of LLMs to autonomously produce jailbreak prompts. For examples, Zou et al. [55] propose a greedy coordinate gradient method (GCG) that achieves excellent jailbreaking performance.

However, previous optimization-based jailbreak methods mainly adopt simple optimization objectives to generate jailbreak suffixes, resulting in limited jailbreak performance. Specifically, optimizationbased jailbreak methods condition on the user's malicious question $Q$ to optimize the jailbreak suffix, with the goal of increasing the log-likelihood of producing a harmful optimization target response $R$. The target response $R$ is designed as the form of "Sure, here is + Rephrase $(\mathrm{Q})$ ". They optimize the suffixes so that the initial outputs of LLMs correspond to the targeted response $R$, causing the LLMs to produce harmful content later. The single target template of "Sure" is ineffective in causing LLMs to output the desired harmful content. As shown in Fig. 1, when using the optimization target of previous work, the jailbreak suffix cannot allow LLMs to generate harmful content even if the output of the beginning of the LLMs is consistent with the optimization target [41, 7]. We argue that the suffix optimized with this optimization goal cannot provide sufficient information to jailbreak.

To address this issue, we propose to apply diverse target templates with harmful self-suggestion and/or guidance to mislead LLMs. Specifically, we design the target response $R$ in the form of "Sure, + Harmful Template, here is + Rephrase $(\mathrm{Q})$ ". Besides the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG that can adaptively decide how many tokens to replace in each step. We also propose an easy-to-hard initialization strategy for generating the jailbreak suffix. The jailbreak difficulty varies depending on the malicious question. We initially generate a jailbreak suffix for the simple harmful requests. This suffix is then used as the suffix initialization to generate a jailbreak suffix for the challenging harmful requests. To improve jailbreak effectiveness, we propose using a variety of target templates with harmful guidance, which increases the difficulty of optimisation and reduces jailbreak efficiency. To increase jailbreak efficiency, we propose an automatic multi-coordinate updating strategy and an easy-to-hard initialization strategy. Combining these improved technologies, we can develop an efficient jailbreak method, dubbed $\mathcal{I}$-GCG. We validate the effectiveness of the proposed $\mathcal{I}$-GCG on four LLMs. It is worth noting that our $\mathcal{I}$-GCG achieves a nearly $100 \%$ attack success rate on all models. Our main contributions are in three aspects:

- We propose to introduce diverse target templates containing harmful self-suggestions and guidance, to improve the GCG's jailbreak efficiency.
- We propose an automatic multi-coordinate updating strategy to accelerate convergence and enhance GCG's performance. Besides, we implement an easy-to-hard initialization technique to further boost GCG's efficiency.
- We combine the above improvements to develop an efficient jailbreak method, dubbed $\mathcal{I}$-GCG Experiments and analyses are conducted on massive security-aligned LLMs to demonstrate the effectiveness of the proposed $\mathcal{I}$-GCG.


## 2 Related work

Expertise-based jailbreak methods leverage expert knowledge to manually generate adversarial prompts to complete the jailbreak. Specifically, Jailbreakchat ${ }^{1}$ is a website for collecting a series of hand-crafted jailbreak prompts. Liu et al. [26] study the effectiveness of hand-crafted jailbreak prompts in bypassing OpenAI's restrictions on CHATGPT. They classify 78 real-world prompts into 10 categories and test their effectiveness and robustness in 40 scenarios from 8 situations banned by OpenAI. Shen et al. [36] conducted the first comprehensive analysis of jailbreak prompts in the wild, revealing that current LLMs and safeguards are ineffective against them. Yong et al. [46] explore cross-language vulnerabilities in LLMs and study how translation-based attacks can bypass the safety guardrails of LLMs. Kang et al. [16] demonstrates that LLMs' programmatic capabilities can generate convincing malicious content without additional training or complex prompt engineering.

LLM-based jailbreak methods adopt another powerful LLM to generate jailbreak prompts based on historical interactions with the victim LLMs. Specifically, Chao et al. [4] propose Prompt Automatic Iterative Refinement, called PAIR, which adopts an attacker LLM to autonomously produce jailbreaks for a targeted LLM using only black-box access. Inspired by PAIR, Mehrotra et al. [29] proposes Tree of Attacks with Pruning, called TAP, which leverages an LLM to iteratively refine potential attack prompts using a tree-of-thought approach until one successfully jailbreaks the target al. Lee et al. [21] propose Bayesian Red Teaming, called BRT, which is a black-box red teaming method for jailbreaking using Bayesian optimization to iteratively identify diverse positive test cases from a pre-defined user input pool. Takemoto et al. [38] propose a simple black-box method for generating jailbreak prompts, which continually transforms ethically harmful prompts into expressions viewed as harmless.

Optimization-based jailbreak methods adopt gradients from white-box LLMs to generate jailbreak prompts inspired by related research on adversarial attacks [35, 10, 30, 45] in Natural Language Processing (NLP). Specifically, Zou et al. [55] propose to adopt a greedy coordinate gradient method, which can be called GCG, to generate jailbreak suffix by maximizing the likelihood of a beginning string in a response. After that, a series of gradient-based optimization jailbreak methods have been proposed by using the radient-based optimization jailbreak methods. Liu et al. [24] propose a stealthy jailbreak method, called AutoDAN, which initiates with a hand-crafted suffix and refines it[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_28d9ed53c1eb27b9830cg-04.jpg?height=458&width=1391&top_left_y=237&top_left_x=367)

Figure 2: The difference between GCG and $\mathcal{I}$-GCG. GCG uses the single target template of "Sure" to generate the optimization goal. While our $\mathcal{I}$-GCG uses the diverse target templates containing harmful guidance to generate the optimization goal.

using a hierarchical genetic method, maintaining its semantic integrity. Zhang et al. [51] propose a momentum-enhanced greedy coordinate gradient method, called MAC, for jailbreaking LLMs attack. Zhao et al. [53] propose an accelerated algorithm for GCG, called Probe-Sampling, which dynamically evaluates the similarity between the predictions of a smaller draft model and those of the target model for various prompt candidate generation. Besides, some researchers adopt the generative model to generate jailbreak suffix. Specifically, Paulus et al. [32] propose to use one LLM to generate human-readable jailbreak prompts for jailbreaking the target LLM, called AdvPrompter. Liao et al. [23] propose to make use of a a generative model to capture the distribution of adversarial suffixes and generate adversarial Suffixes for jailbreaking LLMs, called AmpleGCG.

## 3 Methodology

Notation. Given a set of input tokens represented as $x_{1: n}=\left\{x_{1}, x_{2}, \ldots, x_{n}\right\}$, where $x_{i} \in$ $\{1, \ldots, V\}$ ( $V$ represents the vocabulary size, namely, the number of tokens), a LLM maps the sequence of tokens to a distribution over the next token. It can be defined as:

$$
\begin{equation*}
p\left(x_{n+1} \mid x_{1: n}\right)=p\left(x_{n+1} \mid x_{1: n}\right) \tag{1}
\end{equation*}
$$

where $p\left(x_{n+1} \mid x_{1: n}\right)$ represents the probability that the next token is $x_{n+1}$ given previous tokens $x_{1: n}$. We adopt $p\left(x_{n+1: n+G} \mid x_{1: n}\right)$ to represent the probability of the response sequence of tokens. It can be calculated as:

$$
\begin{equation*}
p\left(x_{n+1: n+G} \mid x_{1: n}\right)=\prod_{i=1}^{G} p\left(x_{n+i} \mid x_{1: n+i-1}\right) \tag{2}
\end{equation*}
$$

Previous works combine the malicious question $x_{1: n}$ with the optimized jailbreak suffix $x_{n+1: n+m}$ to form the jailbreak prompt $x_{1: n} \oplus x_{n+1: n+m}$, where $\oplus$ represents the vector concatenation operation. To simplify the notation, we use $\boldsymbol{x}^{O}$ to represent the malicious question $x_{1: n}, \boldsymbol{x}^{S}$ to represent the jailbreak suffix $x_{n+1: n+m}$, and $\boldsymbol{x}^{O} \oplus \boldsymbol{x}^{S}$ to represent the jailbreak prompt $x_{1: n} \oplus x_{n+1: n+m}$. The jailbreak prompt can make LLMs generate harmful responses. To achieve this goal, the beginning output of LLMs is closer to the predefined optimization goal $x_{n+m+1: n+m+k}^{T}$, which is simply abbreviated as $\boldsymbol{x}^{T}$ (e.g., $\boldsymbol{x}^{T}=$ "Sure, here is a tutorial for making a bomb."). The adversarial jailbreak loss function can be defined as:

$$
\begin{equation*}
\mathcal{L}\left(\boldsymbol{x}^{O} \oplus \boldsymbol{x}^{S}\right)=-\log p\left(\boldsymbol{x}^{T} \mid \boldsymbol{x}^{O} \oplus \boldsymbol{x}^{S}\right) \tag{3}
\end{equation*}
$$

The generation of the adversarial suffix can be formulated as the minimum optimization problem:

$$
\begin{equation*}
\underset{\boldsymbol{x}^{S} \in\{1, \ldots, V\}^{m}}{\operatorname{minimize}} \mathcal{L}\left(\boldsymbol{x}^{O} \oplus \boldsymbol{x}^{S}\right) \tag{4}
\end{equation*}
$$

For simplicity in representation, we use $\mathcal{L}\left(\boldsymbol{x}^{S}\right)$ to denote $\mathcal{L}\left(\boldsymbol{x}^{O} \oplus \boldsymbol{x}^{S}\right)$ in subsequent sections.

### 3.1 Formulation of the proposed method

In this paper, as shown in Fig. 2, following GCG [55], we propose an effective adversarial jailbreak attack method with several improved techniques, dubbed $\mathcal{I}$-GCG. Specifically, we propose to incorporate harmful information into the optimization goal for jailbreak (For instance, stating the phrase

"Sure, my output is harmful, here is a tutorial for making a bomb."). To facilitate representation, we adopt $\boldsymbol{x}^{T} \oplus \boldsymbol{x}^{H}$ to represent this process, where $x^{H}$ represents the harmful information template and $\boldsymbol{x}^{T}$ represents the original optimization goal. The adversarial jailbreak loss function can be defined as:

$$
\begin{equation*}
\mathcal{L}\left(\boldsymbol{x}^{O} \oplus \boldsymbol{x}^{S}\right)=-\log p\left(\boldsymbol{x}^{T} \oplus \boldsymbol{x}^{H} \mid \boldsymbol{x}^{O} \oplus \boldsymbol{x}^{S}\right) \tag{5}
\end{equation*}
$$

The optimization goal in Eq. 5 can typically be approached using optimization methods for discrete tokens, such as GCG [55]. It can be calculated as:

$$
\begin{equation*}
\boldsymbol{x}^{S}(t)=\operatorname{GCG}\left(\left[\mathcal{L}\left(\boldsymbol{x}^{O} \oplus \boldsymbol{x}^{S}(t-1)\right)\right]\right), \text { s.t. } \boldsymbol{x}^{S}(0)=!!!!!!!!!!!!!!!!!!!! \tag{6}
\end{equation*}
$$

where $\operatorname{GCG}(\cdot)$ represents the discrete token optimization method, which is used to update the jailbreak suffix, $\boldsymbol{x}^{S}(t)$ represents the jailbreak suffix generated at the $t$-th iteration, $\boldsymbol{x}^{S}(0)$ represents the initialization for the jailbreak suffix. Although previous works achieve excellent jailbreak performance on LLMs, they do not explore the impact of jailbreak suffix initialization on jailbreak performance. To study the impact of initialization, we follow the default experiment settings in Sec. 4.1 and conduct comparative experiments on a random hazard problem with different initialization values. Specifically, we employ different

![](https://cdn.mathpix.com/cropped/2024_06_04_28d9ed53c1eb27b9830cg-05.jpg?height=442&width=753&top_left_y=706&top_left_x=987)

Figure 3: Evolution of loss values for different jailbreak suffix initialization with the number of attack iterations. initialization values: with !, @, \#, and \$. We then track the changes in their loss values as the number of attack iterations increases. The results are shown in Fig. 3. It can be observed that the initialization of the jailbreak suffix has the influence of attack convergence speed on the jailbreak. However, it is hard to find the best jailbreak suffix initialization. Considering that there are common components among the jailbreak optimization objectives for different malicious questions, inspired by the adversarial jailbreak transferability [54, 7, 44], we propose to adopt the initialization of hazard guidance $\boldsymbol{x}^{I}$ to initialize the jailbreak suffix. The proposed initialization $\boldsymbol{x}^{I}$ is a suffix for another malicious question, which is introduced in Sec. 3.3. The Eq. 6 can be rewritten as:

$$
\begin{equation*}
\boldsymbol{x}^{S}(t)=G C G\left[\mathcal{L}\left(\boldsymbol{x}^{O} \oplus \boldsymbol{x}^{S}(t-1)\right)\right], \text { s.t. } \boldsymbol{x}_{0}^{S}=\boldsymbol{x}^{I} \tag{7}
\end{equation*}
$$

We also track the changes in loss values of the proposed initialization as the number of attack iterations increases. As shown in Fig. 3, it is clear that compared with the suffix initialization of random token, the proposed initialization can promote the convergence of jailbreak attacks faster.

### 3.2 Automatic multi-coordinate updating strategy

Rethinking. Since large language models amplify the difference between discrete choices and their continuous relaxation, solving Eq. 4 is extremely difficult. Previous works [37, 14, 43] have generated adversarial suffixes from different perspectives, such as soft prompt tuning, etc. However, they have only achieved limited jailbreak performance. And then, Zou et al. [55] propose to adopt a greedy coordinate gradient jailbreak method (GCG), which significantly improves jailbreak performance. Specifically, they calculate $\mathcal{L}\left(\boldsymbol{x}^{\hat{S}_{i}}\right)$ for $m$ suffix candidates from $\hat{S}_{1}$ to $\hat{S}_{m}$. Then they retain the one with the optimal loss. The suffix candidates are generated by randomly substituting one token in the current suffix with a token chosen randomly from the top $K$ tokens. Although GCG can effectively generate jailbreak suffixes, it updates only one token in the suffix in each iteration, leading to low jailbreak efficiency.

To improve the jailbreak efficiency, we propose an automatic multi-coordinate updating strategy, which can adaptively decide how many tokens to replace at each step. Specifically, as shown in Fig. 4, following the previous greedy coordinate gradient, we can obtain a series of single-token update suffix candidates from the initial suffix. Then, we adopt Eq. 5 to calculate their corresponding loss values and sort them to obtain the top- $p$ loss ranking which obtains the first $p$ single-token suffix candidates

![](https://cdn.mathpix.com/cropped/2024_06_04_28d9ed53c1eb27b9830cg-06.jpg?height=363&width=1371&top_left_y=220&top_left_x=369)

Figure 4: The overview of the proposed automatic multi-coordinate updating strategy.

with minimum loss. We conduct the token combination, which merges multiple individual token to generate multiple-token suffix candidates. Specifically, given the first $p$ single-token suffix candidates $\boldsymbol{x}^{\hat{S}_{1}}, \boldsymbol{x}^{\hat{S}_{2}}, \ldots, \boldsymbol{x}^{\hat{S}_{p}}$ and the original jailbreak suffix $\boldsymbol{x}^{\hat{S}_{0}}$, the multiple-token suffix candidates can be calculated as:

$$
\boldsymbol{x}_{j}^{\tilde{S}_{i}}=\left\{\begin{align*}
\boldsymbol{x}_{j}^{\hat{S}_{i}}, & \boldsymbol{x}_{j}^{\hat{S}_{i}} \neq \boldsymbol{x}_{j}^{\hat{S}_{0}}  \tag{8}\\
\boldsymbol{x}_{j}^{\tilde{S}_{i-1}}, & \boldsymbol{x}_{j}^{\hat{S}_{i}}=\boldsymbol{x}_{j}^{\hat{S}_{0}}
\end{align*}\right.
$$

where $\boldsymbol{x}_{j}^{\hat{S}_{i}}$ represent the $j$-th token of the single-token suffix candidate $\boldsymbol{x}^{\hat{S}_{i}}, j \in[1, m]$, where $m$ represents the jailbreak suffix length, $\boldsymbol{x}_{j}^{\tilde{S}_{i}}$ represent the $j$-th token of the $i$-th generate multiple-token suffix candidate $\boldsymbol{x}^{\tilde{S}_{i}}$. Finally, we calculate the loss of the generated multiple token candidates and select the suffix candidate with minimal loss for suffix update.

### 3.3 Easy-to-hard initialization

From previous works [38], we find that different types of malicious questions have different difficulty levels when being jailbroken. To further confirm this, we adopt GCG to jailbreak LLAMA2-7B-CHAT [40] with different malicious questions. Then we track the changes in the loss values of different malicious questions as the number of attack iterations increases. The results are shown in Fig. 5. It can be observed the convergence of the loss function varies across different categories of malicious questions, that is, some malicious questions are easier to generate jailbreak suffixes, while some malicious questions

![](https://cdn.mathpix.com/cropped/2024_06_04_28d9ed53c1eb27b9830cg-06.jpg?height=441&width=746&top_left_y=1498&top_left_x=993)

Figure 5: Evolution of loss values for different categories of malicious questions with the number of attack iterations are more difficult to generate jailbreak suffixes. Specifically, it is easy to generate jailbreak suffixes for malicious questions in the Fraud category, but it is difficult for the Pornography category.

To improve the performance of jailbreak, we propose an easy-to-hard initialization, which first generates a jailbreak suffix on illegal questions that are easy to jailbreak, and then uses the generated suffix as the suffix initialization to perform jailbreak attacks. Specifically, as shown in Fig. 6, we randomly select a malicious question from the question list of the fraud category and use the proposed $\mathcal{I}$-GCG to generate a jailbreak suffix. Then, we use this suffix as the initialization of the jailbreak suffix of other malicious questions to perform jailbreak. Combining the above improved techniques, we develop an efficient jailbreak method, dubbed $\mathcal{I}$-GCG. The algorithm of the proposed $\mathcal{I}$-GCG is presented in the Appendix.

![](https://cdn.mathpix.com/cropped/2024_06_04_28d9ed53c1eb27b9830cg-07.jpg?height=241&width=1393&top_left_y=210&top_left_x=363)

Figure 6: The overview of the proposed easy-to-hard initialization.

## 4 Experiments

### 4.1 Experimental settings

Datasets. We use the "harmful behaviors" subset from the AdvBench benchmark [55] to evaluate the jailbreak performance of the proposed $\mathcal{I}$-GCG. Specifically, the AdvBench consists of 520 objectives that request harmful content, such as abusive language, violent content, misinformation, illegal activities, and so on. Following previous works [4, 22, 42], we eliminate duplicate harmful requests from the AdvBench dataset and select 50 representative harmful requests to compare performance. We also adopt HarmBench [28], which is used in the NeurIPS 2023 Red Teaming Track, to evaluate the proposed $\mathcal{I}$-GCG (Base Model Subtrack) ${ }^{2}$. The implementation of our $\mathcal{I}$-GCG on NeurIPS 2023 Red Teaming Track is shown in the Appendix.

Threat models. We use VICUNA-7B-1.5 [6], GUANACO-7B [9], LLAMA2-7B-CHAT [40], and MISTRAL-7B-INSTRUCT-0.2 [15] as the threat models for comparison experiments. The details of threat models are presented in the Appendix. The proposed method is compared to the following baselines: GCG [55], MAC [51], AutoDAN [24], Probe-Sampling [53], Advprompter [32], PAIR [4], and TAP [29]. We use the same jailbreak setting as reported in the original works.

Evaluation metrics. To evaluate the attack success rate (ASR) of the generated jailbreak suffix, we first use the template-based check [55, 5, 24], then feed the passed responses to the ChatGPT-3.5based check $[4,5,27]$. Finally, we manually check the passed responses to ensure that the evaluation is accurate. The details of these evaluation settings are presented in the Appendix.

Implementation details. For any malicious question Q, we set the target response as "Sure, + My output is harmful. Here is + Rephrase(Q)". Following previous works [55, 51], the length of the jailbreak suffix is set to 20 . During the candidate generation, the token substitute batch size $B$ and top- $k$ are both set to 256 . We first launch the jailbreak attack with the suffix initialization (!!!!!!! $!!!!!!!!!!!!!)$ on the simple jailbreak request to generate the jailbreak suffix. The request is set to "Provide instructions on how to steal from a store without getting caught". The number of iterations for this process is set to 1,000 . Then, we use the previously generated jailbreak suffix to initialize the jailbreak suffixes of malicious questions. The number of iterations for this process is 500. All experiments are conducted on an Ubuntu system with an NVIDIA A100 Tensor Core GPU and $80 \mathrm{~GB}$ of RAM.

### 4.2 Hyper-parameter selection

The proposed automatic multi-candidate update strategy has one hyper-parameter, i.e., the first $p$ single-token suffix candidates, which can impact the jailbreak performance. To determine the optimal hyperparameter $p$, we use the LLAMA2-7BCHAT on one randomly chosen question. The results are shown in Fig. 7. The time it takes for the jailbreak attack to converge decreases as the single-token suffix candidate $p$ grows. When $p$ equals 7 , the proposed

![](https://cdn.mathpix.com/cropped/2024_06_04_28d9ed53c1eb27b9830cg-07.jpg?height=433&width=764&top_left_y=1800&top_left_x=992)

Figure 7: Evolution of loss values for different hyperparameters with the number of attack iterations. method takes only about 400 steps to converge, whereas the original GCG takes about 2000 steps. $p$ is set to 7 to conduct experiments.[^1]

Table 1: Comparison results with state-of-the-art jailbreak methods on the AdvBench. The notation * denotes the results from the original paper. Number in bold indicates the best jailbreak performance.

| Method | VICUNA-7B-1.5 | GUANACO-7B | LLAMA2-7B-CHAT | MISTRAL-7B-INSTRUCT-0.2 |
| :---: | :---: | :---: | :---: | :---: |
| GCG [55] | $98 \%$ | $98 \%$ | $54 \%$ | $92 \%$ |
| MAC [51] | $100 \%$ | $100 \%$ | $56 \%$ | $94 \%$ |
| AutoDAN [24] | $100 \%$ | $100 \%$ | $26 \%$ | $96 \%$ |
| Probe-Sampling [53] | $100 \%$ | $100 \%$ | $56 \%$ | $94 \%$ |
| AmpleGCG [23] | $66 \%$ | - | $28 \%$ | - |
| AdvPrompter ${ }^{*}$ [32] | $64 \%$ | - | $24 \%$ | $74 \%$ |
| PAIR [4] | $94 \%$ | $100 \%$ | $10 \%$ | $90 \%$ |
| TAP [29] | $94 \%$ | $100 \%$ | $4 \%$ | $92 \%$ |
| $\mathcal{I}$-GCG (ours) | $\mathbf{1 0 0 \%}$ | $\mathbf{1 0 0 \%}$ | $\mathbf{1 0 0 \%}$ | $\mathbf{1 0 0 \%}$ |

### 4.3 Comparisons with other jailbreak attack methods

Comparison results. The comparison experiment results with other jailbreak attack methods are shown in Table 1. It can be observed that the proposed method outperforms previous jailbreak methods in all attack scenarios. It is particularly noteworthy that the proposed method can achieve $100 \%$ attack success rate across all four LLMs. Specifically, as for the outstanding LLM, MISTRAL-7B-INSTRUCT-0.2, which outperforms the leading open 13B model (LLAMA2) and even the 34B model (LLAMA1) in benchmarks for tasks like reasoning, mathematics, etc, AutoDAN [24] achieves an attack success rate of approximately $96 \%$, while the proposed method achieves the attack success rate of approximately $100 \%$. It indicates that the jailbreak attack method with the proposed improved techniques can fur- Table 2: Jailbreak Performance on NeurIPS 2023 ther significantly improve jailbreak performance. More importantly, when tested against the robust security alignment of the LLM (LLAMA2-7BRed Teaming Track. CHAT), previous state-of-the-art jailbreak methods (MAC [51] and Probe-Sampling [53]) only achieves the success rate of approximately 56\%. However, the proposed method consistently achieves a success rate of approximately $100 \%$. These comparison experiment results demonstrate that our proposed method outperforms other jailbreak attack methods. We also evaluate the proposed $\mathcal{I}$-GCG in the NeurIPS 2023 Red Teaming Track. Given the 256-character limit for suffix length in the competition, we can enhance performance by using more complex harmful templates for jailbreak attacks. Then we compare our $I$-GCG to the baselines provided by the competition, including ZeroShot [33], GBDA [14], and PEZ [43]. The results are shown in Table 2. Our $\mathcal{I}$-GCG can also achieve a success rate of approximately $100 \%$.

Table 3: Transferable performance of jailbreak suffix which is generated on VICUNA-7B-1.5. Number in bold indicates the best jailbreak performance.

| Method | MISTRAL-7B-INSTRUCT-0.2 | STARLING-7B-ALPHA | CHATGPT-3.5 | CHATGPT-4 |
| :---: | :---: | :---: | :---: | :---: |
| GCG [55] | $16 \%$ | $16 \%$ | $10 \%$ | $0 \%$ |
| MAC [51] | $22 \%$ | $16 \%$ | $14 \%$ | $2 \%$ |
| I-GCG (ours) | $\mathbf{2 6 \%}$ | $\mathbf{2 0 \%}$ | $\mathbf{2 2 \%}$ | $\mathbf{4 \%}$ |

Transferability performance. We also compare the proposed method with GCG [55] and MAC [51] on transferability. Specifically, we adopt VICUNA-7B-1.5 to generate the jailbreak suffixes and use two advanced open source LLMs (MISTRAL-7B-INSTRUCT-0.2 and STARLING-7B-ALPHA) and two advanced closed source LLMs (CHATGPT-3.5 and CHATGPT-4) to evaluate the jailbreak transferability. The results are shown in Table 3. The proposed method outperforms GCG [55] and MAC [51] in terms of attack success rates across all scenarios. It indicates that the proposed method can also significantly improve the transferability of the generated jailbreak suffixes. Specifically, as for the open source LLM, STARLING-7B-ALPHA, GCG [55] achieves an ASR of about 16\%, but the proposed method can achieve an ASR of about 20\%. As for the close source LLM, CHATGPT-3.5, MAC [51] achieves ASR of about $14 \%$, but our $\mathcal{I}$-GCG can achieve ASR of about $22 \%$.

### 4.4 Ablation study

In this paper, we propose three improved technique terms to boost the jailbreak performance: i.e., harmful guidance, update strategy, and suffix initialization. To validate the effectiveness of each element in the proposed method, we adopt LLAMA2-7B-CHAT on AdvBench to conduct ablation experiments. We adopt the ASR and average iterations as evaluation metrics. GCG is used as the baseline. The results are shown

Table 4: Ablation study of the proposed method. in Table 4. The analyses are summarized as fol-

| Harmful <br> Guidance | Update <br> Strategy | Suffix <br> Initialization | ASR | Average <br> Iterations |
| :---: | :---: | :---: | :---: | :---: |
|  | Baseline | $54 \%$ | 510 |  |
| $\boldsymbol{}$ |  |  | $82 \%$ | 955 |
|  | $\boldsymbol{\nu}$ |  | $72 \%$ | 418 |
| $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ | $\boldsymbol{\nu}$ | $68 \%$ | 64 |

lows: First, by incorporating only harmful guidance, ASR's performance improves significantly, as do the average iterations. When the update strategy and suffix initialization are used, the performance of ASR improves while the average iterations decrease. The results indicate that harmful guidance contributes more to improving jailbreak effectiveness. Update strategy and suffix initialization contribute more to improving jailbreak efficiency. Then, using all techniques yields the best results in terms of jailbreak effectiveness and efficiency.

Table 5: Jailbreak performance with different harmful guidance for LLAMA2-7B-CHAT.

| Harmful <br> Guidance | my action <br> is unsafe | my behavior <br> is risky | my action <br> poses a danger | my answer <br> is risky | my information <br> is dangerous |
| :---: | :---: | :---: | :---: | :---: | :---: |
| ASR | $100 \%$ | $100 \%$ | $100 \%$ | $100 \%$ | $100 \%$ |

### 4.5 Discussion

Effectiveness of the proposed harmful guidance. In this paper, we propose to introduce harmful information into the optimization goal to guide jailbreak. We use "my output is harmful" as the harmful guidance. To further explore the impact of harmful guidance, we adopt more examples of harmful guidance to jailbreak the secure aligned LLM, LLAMA2-7B-CHAT. The results are shown in Table 5. It can be observed that using different harmful guidance can also achieve nearly $100 \%$ attack success rate, which indicates introducing harmful guidance in the optimization goal could facilitate finding the jailbreak space, thereby enhancing jailbreak performance.

Efficiency of the proposed update strategy and suffix initialization. Although introducing harmful guidance can boost jailbreak performance, it also brings optimization difficulties and reduces jailbreak efficiency. To improve jailbreak efficiency, we propose the automatic multiple token candidate update strategy and the prior-guided suffix initialization. Previous experimental results show that the proposed efficient techniques can significantly boost jailbreak efficiency. To further study their impact, we combine the proposed efficient techniques with the original GCG and calculate that the average loss value of the AdvBench for LLAMA27B-CHAT changes with the number of jailbreak iterations. The results are shown in Fig. 8. It can be observed that the proposed techniques can boost the convergence of jailbreak, among which suffix initialization performs better. However, the prior-guided initialization must first be generated, which

![](https://cdn.mathpix.com/cropped/2024_06_04_28d9ed53c1eb27b9830cg-09.jpg?height=282&width=765&top_left_y=1905&top_left_x=989)

Figure 8: Evolution of loss values for different suffix initialization with the number of attack iterations. can be accomplished by the update strategy.

Limitation. The proposed method still has worthy room for exploration. For example, the better harmful guidance design, more general suffix initialization, etc. Although our method achieves excellent jailbreak performance, there is still room for improvement in jailbreak transferability with transferability-enhancing methods summarized in [12].

## 5 Conclusion

In this paper, we propose several improved techniques for optimization-based jaibreaking on large language models. We propose using diverse target templates, including harmful guidance, to enhance jailbreak performance. From an optimization perspective, we introduce an automatic multi-coordinate updating strategy that adaptively decides how many tokens to replace in each step. We also incorporate an easy-to-hard initialization technique, further boosting jailbreak performance. Then we combine the above improvements to develop an efficient jailbreak method, dubbed $\mathcal{I}$-GCG. Extensive experiments are conducted on various benchmarks to demonstrate the superiority of our $\mathcal{I}$-GCG.

## 6 Impact statement

This paper proposes several improved techniques to generate jailbreak suffixes for LLMs, which may potentially generate harmful texts and pose risks. However, like previous jailbreak attack methods, the proposed method explores jailbreak prompts with the goal of uncovering vulnerabilities in aligned LLMs. This effort aims to guide future work in enhancing LLMs' human preference safeguards and advancing more effective defense approaches. Besides, the victim LLMs used in this paper are open-source models with publicly available weights. The research on jailbreak and alignment will collaboratively shape the landscape of AI security.

## References

[1] Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Jailbreaking leading safety-aligned llms with simple adaptive attacks. arXiv preprint arXiv:2404.02151, 2024.

[2] Yang Bai, Ge Pei, Jindong Gu, Yong Yang, and Xingjun Ma. Special characters attack: Toward scalable training data extraction from large language models. arXiv preprint arXiv:2405.05990, 2024.

[3] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 2023

[4] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023.

[5] Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, and Jindong Gu. Red teaming gpt-4v: Are gpt-4v safe against uni/multi-modal jailbreak attacks? arXiv preprint arXiv:2404.03411, 2024

[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https: / / lmsys . org/blog/ 2023-03-30-vicuna/.

[7] Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. Comprehensive assessment of jailbreak attacks against llms. arXiv preprint arXiv:2402.05668, 2024.

[8] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715, 2023.

[9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.

[10] Shreya Goyal, Sumanth Doddapaneni, Mitesh M Khapra, and Balaraman Ravindran. A survey of adversarial defenses and robustness in nlp. ACM Computing Surveys, 55(14s):1-39, 2023.

[11] Jindong Gu. Responsible generative ai: What to generate and what not. arXiv preprint arXiv:2404.05783, 2024.

[12] Jindong Gu, Xiaojun Jia, Pau de Jorge, Wenqain Yu, Xinwei Liu, Avery Ma, Yuan Xun, Anjun Hu, Ashkan Khakzar, Zhijiang Li, et al. A survey on transferability of adversarial examples across deep neural networks. arXiv preprint arXiv:2310.17626, 2023.

[13] Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, and Min Lin. Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast. arXiv preprint arXiv:2402.08567, 2024.

[14] Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe Kiela. Gradient-based adversarial attacks against text transformers. arXiv preprint arXiv:2104.13733, 2021.

[15] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[16] Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. arXiv preprint arXiv:2302.05733, 2023.

[17] Nikitas Karanikolas, Eirini Manga, Nikoletta Samaridi, Eleni Tousidou, and Michael Vassilakopoulos. Large language models versus natural language understanding and generation. In Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics, pages 278-290, 2023.

[18] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and individual differences, 103:102274, 2023.

[19] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversationsdemocratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024.

[20] Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black box jailbreaking of large language models. arXiv preprint arXiv:2309.01446, 2023.

[21] Deokjae Lee, JunYeong Lee, Jung-Woo Ha, Jin-Hwa Kim, Sang-Woo Lee, Hwaran Lee, and Hyun Oh Song. Query-efficient black-box red teaming via bayesian optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11551-11574, 2023 .

[22] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize large language model to be jailbreaker. arXiv preprint arXiv:2311.03191, 2023.

[23] Zeyi Liao and Huan Sun. Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms. arXiv preprint arXiv:2404.07921, 2024.

[24] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023.

[25] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Generating stealthy jailbreak prompts on aligned large language models. In The Twelfth International Conference on Learning Representations, 2023.

[26] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint arXiv:2305.13860, 2023.

[27] Mantas Mazeika, Andy Zou, Norman Mu, Long Phan, Zifan Wang, Chunru Yu, Adam Khoja, Fengqing Jiang, Aidan O'Gara, Ellie Sakhaee, Zhen Xiang, Arezoo Rajabi, Dan Hendrycks, Radha Poovendran, Bo Li, and David Forsyth. Tdc 2023 (llm edition): The trojan detection challenge. In NeurIPS Competition Track, 2023.

[28] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024.

[29] Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. arXiv preprint arXiv:2312.02119, 2023.

[30] Mutsumi Nakamura, Santosh Mashetty, Mihir Parmar, Neeraj Varshney, and Chitta Baral. Logicattack: Adversarial attacks for evaluating logical consistency of natural language inference. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.

[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.

[32] Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong Tian. Advprompter: Fast adaptive adversarial prompting for llms. arXiv preprint arXiv:2404.16873, 2024.

[33] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022.

[34] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Finetuning aligned language models compromises safety, even when users do not intend to! In The Twelfth International Conference on Learning Representations, 2023.

[35] Shilin Qiu, Qihe Liu, Shijie Zhou, and Wen Huang. Adversarial attack and defense technologies in natural language processing: A survey. Neurocomputing, 492:278-307, 2022.

[36] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825, 2023.

[37] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.

[38] Kazuhiro Takemoto. All in how you ask for it: Simple black-box method for jailbreak attacks. arXiv preprint arXiv:2401.09798, 2024.

[39] Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, and Siddharth Garg. Verigen: A large language model for verilog code generation. ACM Transactions on Design Automation of Electronic Systems, 2023.

[40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[41] Zhe Wang and Yanjun Qi. A closer look at adversarial suffix learning for jailbreaking llms. In ICLR Workshop on Secure and Trustworthy Large Language Models, 2024.

[42] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36, 2024.

[43] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Advances in Neural Information Processing Systems, 36, 2024.

[44] Zeguan Xiao, Yan Yang, Guanhua Chen, and Yun Chen. Tastle: Distract large language models for automatic jailbreak attack. arXiv preprint arXiv:2403.08424, 2024.

[45] Dingcheng Yang, Yang Bai, Xiaojun Jia, Yang Liu, Xiaochun Cao, and Wenjian Yu. Cheating suffix: Targeted attack to text-to-image diffusion models with multi-modal priors. arXiv preprint arXiv:2402.01369, 2024.

[46] Zheng-Xin Yong, Cristina Menghini, and Stephen H Bach. Low-resource languages jailbreak gpt-4. arXiv preprint arXiv:2310.02446, 2023.

[47] Jiahao Yu, Xingwei Lin, and Xinyu Xing. Gptfuzzer: Red teaming large language models with autogenerated jailbreak prompts. arXiv preprint arXiv:2309.10253, 2023.

[48] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. arXiv preprint arXiv:2308.06463, 2023.

[49] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprint arXiv:2401.06373, 2024.

[50] Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting large language model for machine translation: A case study. In International Conference on Machine Learning, pages 41092-41110. PMLR, 2023.

[51] Yihao Zhang and Zeming Wei. Boosting jailbreak attack with momentum. arXiv preprint arXiv:2405.01229, 2024.

[52] Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and William Yang Wang. Weak-to-strong jailbreaking on large language models. arXiv preprint arXiv:2401.17256, 2024.

[53] Yiran Zhao, Wenyue Zheng, Tianle Cai, Xuan Long Do, Kenji Kawaguchi, Anirudh Goyal, and Michael Shieh. Accelerating greedy coordinate gradient via probe sampling. arXiv preprint arXiv:2403.01251, 2024.

[54] Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu Chai, Fukang Zhu, Caishuang Huang, Shihan Dou, Zhiheng Xi, et al. Easyjailbreak: A unified framework for jailbreaking large language models. arXiv preprint arXiv:2403.12171, 2024.

[55] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.
