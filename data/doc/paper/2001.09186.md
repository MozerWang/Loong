# A tutorial on the range variant of asymmetric numeral systems 

James Townsend<br>University College London<br>james.townsend@cs.ucl.ac.uk

October 8,2020


#### Abstract

This paper is intended to be a brief and accessible introduction to the range variant of asymmetric numeral systems (ANS), a system for lossless compression of sequences which can be used as a drop in replacement for arithmetic coding (AC). Because of the relative simplicity of ANS, we are able to provide enough mathematical detail to rigorously prove that ANS attains a compression rate close to the Shannon limit. Pseudo-code, intuitive interpretation and diagrams are given alongside the mathematical derivations. A working Python demo which accompanies this tutorial is available at https://raw.githubusercontent.com/j-towns/ans-notes/ master/rans.py


## 1 Introduction

We are interested in algorithms for lossless compression of sequences of data. Arithmetic coding (AC) and the range variant of asymmetric numeral systems (sometimes abbreviated to rANS, we simply use ANS) are examples of such algorithms. Just like arithmetic coding, ANS is close to optimal in terms of compression rate (Witten et al., 1987, Duda, 2009). The key difference between ANS and AC is in the order in which data are decoded: in ANS, compression is last-in-first-out (LIFO), or 'stack-like', while in AC it is first-in-first-out (FIFO), or 'queue-like'. We recommend MacKay (2003, Chapter 4-6) for background on source coding and arithmetic coding in particular. In this paper we will focus solely on ANS, which is not covered in detail by existing textbooks (although McAnlis and Haecky, 2016 does briefly cover ANS).

ANS comprises two basic functions, which we denote push and pop, for encoding and decoding, respectively (the names refer to the analogous stack operations). The push function accepts some pre-compressed information $m$ (short for 'message'), and a symbol $x$ to be compressed, and returns a new compressed message, $m^{\prime}$. Thus it has the signature

$$
\begin{equation*}
\text { push : }(m, x) \mapsto m^{\prime} \tag{1}
\end{equation*}
$$

The new compressed message, $m^{\prime}$, contains precisely the same information as the pair $(m, x)$, and therefore push can be inverted to form a decoder mapping. The decoder, pop, maps from $m^{\prime}$ back to $m, x$ :

$$
\begin{equation*}
\text { pop : } m^{\prime} \mapsto(m, x) \tag{2}
\end{equation*}
$$

Because the functions push and pop are inverse to one another, we have $\operatorname{push}(\operatorname{pop}(m))=m$ and $\operatorname{pop}(\operatorname{push}(m, x))=(m, x)$.

### 1.1 Specifying the problem which ANS solves

In this section we first define some notation, then describe the problem which ANS solves in more detail and sketch the high level approach to solving it. In the following we use 'log' as shorthand for the base 2 logarithm, usually denoted ' $\log _{2}$ '.

The functions push and pop will both require access to the probability distribution from which symbols are drawn (or an approximation thereof). To describe distributions we use notation similar to MacKay (2003):

Definition 1. An ensemble $X$ with precision $r$ is a triple $\left(x, \mathcal{A}_{X}, \mathcal{P}_{X}\right)$ where the outcome $x$ is the value of a random variable, taking on one of a set of possible values $\mathcal{A}_{X}=\left\{a_{1}, \ldots, a_{I}\right\}$, and $\mathcal{P}_{X}=\left\{p_{1}, \ldots, p_{I}\right\}$ are the integer-valued probability weights with each $p_{i} \in\left\{1, \ldots, 2^{r}\right\}$, each $P\left(x=a_{i}\right)=p_{i} / 2^{r}$ and therefore $\sum_{i=1}^{I} p_{i}=2^{r}$.

Note that this definition differs from the definition in MacKay (2003) in that the probabilities are assumed to be quantized to some precision $r$ (i.e. representable by fractions $p_{i} / 2^{r}$ ), and we assume that none of the $a_{i}$ have zero probability. Having probabilities in this form is necessary for the arithmetic operations involved in ANS (as well as AC). Note that if we use a high enough $r$ then we can specify probabilities with a precision similar to that of typical floating point - 32 -bit floating points for example contain 23 'fraction' bits, and thus would have roughly the same precision as our representation with $r=23$.

The 'information content' of an outcome can be measured using the following:

Definition 2. The Shannon information content of an outcome $x$ is

$$
\begin{equation*}
h(x):=\log \frac{1}{P(x)} \tag{3}
\end{equation*}
$$

Given a sequence of ensembles $X_{1}, \ldots, X_{N}$, we seek an algorithm which can encode any outcome $x_{1}, \ldots, x_{N}$ in a binary message whose length is close to $h\left(x_{1}, \ldots, x_{N}\right)=\log 1 / P\left(x_{1}, \ldots, x_{N}\right)$. According to Shannon's source coding theorem it is not possible to losslessly encode data in a message with expected length less than $\mathbb{E}[h(x)]$, thus we are looking for an encoding which is close to optimal in expectation (Shannon, 1948). Note that the joint information content
of the sequence can be decomposed:

$$
\begin{align*}
h\left(x_{1}, \ldots, x_{N}\right) & =\log \frac{1}{P\left(x_{1}, \ldots, x_{N}\right)}  \tag{4}\\
& =\sum_{n} \log \frac{1}{P\left(x_{n} \mid x_{1}, \ldots, x_{n-1}\right)}  \tag{5}\\
& =\sum_{n} h\left(x_{n} \mid x_{1}, \ldots, x_{n-1}\right) \tag{6}
\end{align*}
$$

Because it simplifies the presentation significantly, we focus first on the ANS decoder, the reverse mapping which maps from a compressed binary message to the sequence $x_{1}, \ldots, x_{N}$. This will be formed of a sequence of $N$ pop operations; starting with a message $m_{0}$ we define

$$
\begin{equation*}
m_{n}, x_{n}=\operatorname{pop}\left(m_{n-1}\right) \quad \text { for } n=1, \ldots, N \tag{7}
\end{equation*}
$$

where each pop uses the conditional distribution $X_{n} \mid X_{1}, \ldots, X_{n-1}$. We will show that the message resulting from each pop, $m_{n}$, is effectively shorter than $m_{n-1}$ by no more than $h\left(x_{n} \mid x_{1}, \ldots, x_{n-1}\right)+\epsilon$ bits, where $\epsilon$ is a small constant which we specify below, and therefore the difference in length between $m_{0}$ and $m_{N}$ is no more than $h\left(x_{1}, \ldots, x_{N}\right)+N \epsilon$, by eqs. 4) to 6).

We will also show that pop is a bijection whose inverse, push, is straightforward to compute, and therefore an encoding procedure can easily be defined by starting with a very short base message and adding data sequentially using push. Our guarantee about the effect of pop on message length translates directly to a guarantee about the effect of push, in that the increase in message length due to the sequence of push operations is less than $h\left(x_{1}, \ldots, x_{N}\right)+N \epsilon$.

## 2 Asymmetric numeral systems

Having set out the problem which ANS solves and given a high level overview of the solution in Section 1. we now go into more detail, firstly discussing the data structure we use for $m$, then the pop function and finally the computation of its inverse, push.

### 2.1 The structure of the message

We use a pair $m=(s, t)$ as the data structure for the message $m$. The element $s$ is an unsigned integer with precision $r_{s}$ (i.e. $s \in\left\{0,1, \ldots, 2^{r_{s}}-1\right\}$, so that $s$ can be expressed as a binary number with $r_{s}$ bits). The element $t$ is a stack of unsigned integers of some fixed precision $r_{t}$ where $r_{t}<r_{s}$. This stack has its own push and pop operations, which we denote stack_push and stack_pop respectively. See fig. 1 for a diagram of $s$ and $t$. We need $s$ to be large enough to ensure that our decoding is accurate, and so we also impose the constraint

$$
\begin{equation*}
s \geq 2^{r_{s}-r_{t}} \tag{8}
\end{equation*}
$$

more detail on how and why we do this is given below. In the demo implementation we use $r_{s}=64$ and $r_{t}=32$.

Note that a message can be flattened into a string of bits by concatenating $s$ and the elements of $t$. The length of this string is

$$
\begin{equation*}
l(m):=r_{s}+r_{t}|t| \tag{9}
\end{equation*}
$$

where $|t|$ is the number of elements in the stack $t$. We refer to this quantity as the 'length' of $m$. We also define the useful quantity

$$
\begin{equation*}
l^{*}(m):=\log s+r_{t}|t| \tag{10}
\end{equation*}
$$

which we refer to as the 'effective length' of $m$. Note that the constraint in eq. (8) and the fact that $s<2^{r_{s}}$ imply that

$$
\begin{equation*}
l(m)-r_{t} \leq l^{*}(m)<l(m) \tag{11}
\end{equation*}
$$

Intuitively $l^{*}$ can be thought of as a precise measure of the size of $m$, whereas $l$, which is integer valued, is a more crude measure. Clearly $l$ is ultimately the measure that we care most about, since it tells us the size of a binary encoding of $m$, and we use $l^{*}$ to prove bounds on $l$.

![](https://cdn.mathpix.com/cropped/2024_06_04_e46a6b443a3ebc73f4fcg-04.jpg?height=127&width=309&top_left_y=1411&top_left_x=669)

$S$

![](https://cdn.mathpix.com/cropped/2024_06_04_e46a6b443a3ebc73f4fcg-04.jpg?height=273&width=266&top_left_y=1262&top_left_x=1187)

$t$

Figure 1: The two components of a message: the unsigned integer $s$ (with $r_{s}=16$ ) and the stack of unsigned integers $t$ (with $r_{t}=8$ ). The integers are represented here in base 2 (binary).

### 2.2 Constructing the pop operation

To avoid notational clutter, we begin by describing the pop operation for a single ensemble $X=\left(x, \mathcal{A}_{X}, \mathcal{P}_{X}\right)$ with precision $r$, before applying pop to a sequence in Section 2.3. Our strategy for performing a decode with pop will be firstly to extract a symbol from $s$. We do this using a bijective function $d: \mathbb{N} \rightarrow \mathbb{N} \times \mathcal{A}$, which takes an integer $s$ as input and returns a pair $\left(s^{\prime}, x\right)$, where $s^{\prime}$ is an integer and $x$ is a symbol. Thus pop begins

```
def pop(m):
    s, t := m
    s', x := d(s)
```

We design the function $d$ so that if $s \geq 2^{r_{s}-r_{t}}$, then

$$
\begin{equation*}
\log s-\log s^{\prime} \leq h(x)+\epsilon \tag{12}
\end{equation*}
$$

where

$$
\begin{equation*}
\epsilon:=\log \frac{1}{1-2^{-\left(r_{s}-r_{t}-r\right)}} \tag{13}
\end{equation*}
$$

We give details of $d$ and prove eq. 12 below. Note that when the term $2^{-\left(r_{s}-r_{t}-r\right)}$ is small, the following approximation is accurate:

$$
\begin{equation*}
\epsilon \approx \frac{2^{-\left(r_{s}-r_{t}-r\right)}}{\ln 2} \tag{14}
\end{equation*}
$$

and thus $\epsilon$ itself is small. We typically use $r_{s}=64, r_{t}=32$, and $r=16$, which gives $\epsilon=\log 1 /\left(1-2^{-16}\right) \approx 2.2 \times 10^{-5}$.

After extracting a symbol using $d$, we check whether $s^{\prime}$ is below $2^{r_{s}-r_{t}}$, and if it is we stack_pop integers from $t$ and move their contents into the lower order bits of $s$ '. We refer to this as 'renormalization'. Having done this, we return the new message and the symbol $x$. The full definition of pop is thus

```
def pop(m):
    s, t := m
    s', x := d(s)
    s, t := renorm (s',t)}\leftarrow\mathrm{ this function is defined below
    return (s, t), x
```

Renormalization is necessary to ensure that the value of $s$ returned by pop satisfies $s \geq 2^{r_{s}-r_{t}}$ and is therefore large enough that eq. 12 holds at the start of any future pop operation. The renorm function has a while loop, which pushes elements from $t$ into the lower order bits of $s$ until $s$ is full to capacity. To be precise:

```
def renorm $(s, t)$ :
    \# while $s$ has space for another element from $t$
    while $s<2^{r_{s}-r_{t}}$ :
        \# pop an element $t_{\text {top }}$ from $t$
        $t, t_{\text {top }}:=$ stack_pop( $(t)$
        $\#$ and push $t_{\text {top }}$ into the lower bits of $s$
        $s:=2^{r_{t}} \cdot s+t_{\text {top }}$
    return $s, t$
```

The condition $s<2^{r_{s}-r_{t}}$ guarantees that $2^{r_{t}} \cdot s+t_{\text {top }}<2^{r_{s}}$, and thus there can be no loss of information resulting from overflow. We also have

$$
\begin{equation*}
\log \left(2^{r_{t}} \cdot s+t_{\text {top }}\right) \geq r_{t}+\log s \tag{15}
\end{equation*}
$$

since $t_{\mathrm{top}} \geq 0$. Applying this inequality repeatedly, once for each iteration of the while loop in renorm, we have

$$
\begin{equation*}
\log s \geq \log s^{\prime}+r_{t} \cdot[\# \text { elements popped from } t] \tag{16}
\end{equation*}
$$

where $s, t=\operatorname{renorm}\left(s^{\prime}, t\right)$ as in the definition of pop.

Combining eq. (12) and eq. (16) gives us

$$
\begin{equation*}
l^{*}(m)-l^{*}\left(m^{\prime}\right) \leq h(x)+\epsilon \tag{17}
\end{equation*}
$$

where $\left(m^{\prime}, x\right)=\operatorname{pop}(m)$, using the definition of $l^{*}$. That is, the reduction in the effective message length resulting from pop is close to $h(x)$.

### 2.3 Popping in sequence

We now apply pop to the setup described in Section 1.1 performing a sequence of pop operations to decode a sequence of data. We suppose that we are given some initial message $m_{0}$.

For $n=1 \ldots N$, we let $m_{n}, x_{n}=\operatorname{pop}\left(m_{n-1}\right)$ as in Section 1.1, where each pop uses the corresponding distribution $X_{n} \mid X_{1}, \ldots, X_{n-1}$. Applying eq. 17) to each of the $N$ pop operations, we have:

$$
\begin{align*}
l^{*}\left(m_{0}\right)-l^{*}\left(m_{N}\right) & =\sum_{n=1}^{N}\left[l^{*}\left(m_{n-1}\right)-l^{*}\left(m_{n}\right)\right]  \tag{18}\\
& \leq \sum_{n=1}^{N}\left[h\left(x_{n} \mid x_{1}, \ldots, x_{n-1}\right)+\epsilon\right]  \tag{19}\\
& \leq h\left(x_{1}, \ldots, x_{N}\right)+N \epsilon \tag{20}
\end{align*}
$$

This result tells us about the reduction in message length from pop but also, conversely, about the length of a message constructed using push. We can actually initialize an encoding procedure by choosing $m_{N}$, and then performing a sequence of push operations. Since our ultimate goal when encoding is to minimize the encoded message length $m_{0}$ we choose the setting of $m_{N}$ which minimizes $l^{*}\left(m_{N}\right)$, which is $m_{N}=\left(s_{N}, t_{N}\right)$ where $s_{N}=2^{r_{s}-r_{t}}$ and $t_{N}$ is an empty stack. That gives $l^{*}\left(m_{N}\right)=r_{s}-r_{t}$ and therefore, by eq. 20,

$$
\begin{equation*}
l^{*}\left(m_{0}\right) \leq h\left(x_{1}, \ldots, x_{N}\right)+N \epsilon+r_{s}-r_{t} \tag{21}
\end{equation*}
$$

Combining that with eq. 11) gives an expression for the actual length of the flattened binary message resulting from $m_{0}$ :

$$
\begin{equation*}
l\left(m_{0}\right) \leq h\left(x_{1}, \ldots, x_{N}\right)+N \epsilon+r_{s} \tag{22}
\end{equation*}
$$

It now remains for us to describe the function $d$ and show that it satisfies eq. (12), as well as showing how to invert pop to form the encoding function push.

### 2.4 The function $d$

The function $d: \mathbb{N} \rightarrow \mathbb{N} \times \mathcal{A}$ must be a bijection, and we aim for $d$ to satisfy eq. 12), and thus $P(x) \approx \frac{s^{\prime}}{s}$. Achieving this is actually fairly straightforward.

One way to define a bijection $d: s \mapsto\left(s^{\prime}, x\right)$ is to start with a mapping $\tilde{d}: s \mapsto x$, with the property that none of the preimages $\tilde{d}^{-1}(x):=\{n \in \mathbb{N}: \tilde{d}(n)=x\}$ are finite for $x \in \mathcal{A}$. Then let $s^{\prime}$ be the index of $s$ within the (ordered) set $\tilde{d}^{-1}(x)$, with indices starting at 0 . Equivalently, $s^{\prime}$ is the number of integers $n$ with $0 \leq n<s$ and $d(n)=x$.

With this setup, the ratio

$$
\begin{equation*}
\frac{s^{\prime}}{s}=\frac{|\{n \in \mathbb{N}: n<s, d(n)=x\}|}{s} \tag{23}
\end{equation*}
$$

is the density of numbers which decode to $x$, within all the natural numbers less $s$. For large $s$ we can ensure that this ratio is close to $P(x)$ by setting $\tilde{d}$ such that numbers which decode to a symbol $x$ are distributed within the natural numbers with density close to $P(x)$.

To do this, we partition $\mathbb{N}$ into finite ranges of equal length, and treat each range as a model for the interval $[0,1]$, with sub-intervals within $[0,1]$ corresponding to each symbol, and the width of each sub-interval being equal to the corresponding symbol's probability (see fig. 2). To be precise, the mapping $\tilde{d}$ can then be expressed as a composition $\tilde{d}=\tilde{d}_{2} \circ \tilde{d}_{1}$, where $\tilde{d}_{1}$ does the partitioning described above, and $\tilde{d}_{2}$ assigns numbers within each partition to symbols (sub-intervals). So

$$
\begin{equation*}
\tilde{d}_{1}(s):=s \bmod 2^{r} \tag{24}
\end{equation*}
$$

Using the shorthand $\bar{s}:=\tilde{d}_{1}(s)$, and defining

$$
c_{j}:= \begin{cases}0 & \text { if } j=1  \tag{25}\\ \sum_{k=1}^{j-1} p_{k} & \text { if } j=2, \ldots, I\end{cases}
$$

as the (quantized) cumulative probability of symbol $a_{j-1}$,

$$
\begin{equation*}
\tilde{d}_{2}(\bar{s}):=a_{i} \text { where } i:=\max \left\{j: c_{j} \leq \bar{s}\right\} \tag{26}
\end{equation*}
$$

That is, $\tilde{d}_{2}(\bar{s})$ selects the symbol whose sub-interval contains $\bar{s}$. Figure 2 illustrates this mapping, with a particular probability distribution, for the range $s=64, \ldots, 71$.

### 2.5 Computing $s^{\prime}$

The number $s^{\prime}$ was defined above as "the index of $s$ within the (ordered) set $\tilde{d}^{-1}(x)$, with indices starting at 0 ". We now derive an expression for $s^{\prime}$ in terms of $s, p_{i}$ and $c_{i}$, where $i=\max \left\{j: c_{j} \leq \bar{s}\right\}$ (as above), and we prove eq. 12.

Our expression for $s^{\prime}$ is a sum of two terms. The first term counts the entire intervals, corresponding to the selected symbol $a_{i}$, which are below $s$. The size of each interval is $p_{i}$ and the number of intervals is $s \div 2^{r}$, thus the first term is $p_{i} \cdot\left(s \div 2^{r}\right)$, where $\div$ denotes integer division, discarding any

| s | 64 | 65 | 66 | 67 | 68 | 69 | 70 | 71 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $s \bmod 2^{r_{p}}$ | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
| $x$ | $\mathrm{a}$ |  |  |  | c |  |  |  |

Figure 2: Showing the correspondence between $s, s \bmod 2^{r}$ and the symbol $x$. The interval $[0,1] \subset \mathbb{R}$ is modelled by the set of integers $\left\{0,1, \ldots, 2^{r}-1\right\}$. In this case $r=3$ and the probabilities of each symbol are $P(\mathrm{a})=1 / 8, P(\mathrm{~b})=2 / 8$, $P(\mathrm{c})=3 / 8$ and $P(\mathrm{~d})=2 / 8$.

remainder. The second term counts our position within the current interval, which is $\bar{s}-c_{i} \equiv s \bmod 2^{r}-c_{i}$. Thus

$$
\begin{equation*}
s^{\prime}=p_{i} \cdot\left(s \div 2^{r}\right)+s \bmod 2^{r}-c_{i} \tag{27}
\end{equation*}
$$

This expression is straightforward to compute. Moreover from this expression it is straightforward to prove eq. (12). Firstly, taking the log of both sides of eq. 27) and using the fact that $s \bmod 2^{r}-c_{i} \geq 0$ gives

$$
\begin{equation*}
\log s^{\prime} \geq \log \left(p_{i} \cdot\left(s \div 2^{r}\right)\right) \tag{28}
\end{equation*}
$$

then by the definition of $\div$, we have $s \div 2^{r}>\frac{s}{2^{r}}-1$, and thus

$$
\begin{align*}
\log s^{\prime} & \geq \log \left(p_{i}\left(\frac{s}{2^{r}}-1\right)\right)  \tag{29}\\
& \geq \log s-h(x)+\log \left(1-\frac{2^{r}}{s}\right)  \tag{30}\\
& \geq \log s-h(x)-\epsilon \tag{31}
\end{align*}
$$

as required, using the fact that $P(x)=\frac{p_{i}}{2^{r}}$ and $s \geq 2^{r_{s}-r_{t}}$.

By choosing $r_{s}-r_{t}$ to be reasonably large (it is equal to 32 in our implementation), we ensure that $\frac{s^{\prime}}{s}$ is very close to $P(x)$. This behaviour can be seen visually in fig. 3 , which shows the improvement in the approximation for larger $s$.

### 2.6 Pseudocode for $d$

We now have everything we need to write down a procedure to compute $d$.

We assume access to a function $f_{X}: \bar{s} \mapsto\left(a_{i}, c_{i}, p_{i}\right)$, where $i$ is defined above. This function clearly depends on the distribution of $X$, and its computational complexity is equivalent to that of computing the CDF and inverse CDF for $X$. For many common distributions, the CDF and inverse CDF have straightforward closed form expressions, which don't require an explicit sum over $i$.
![](https://cdn.mathpix.com/cropped/2024_06_04_e46a6b443a3ebc73f4fcg-09.jpg?height=634&width=1196&top_left_y=428&top_left_x=451)

Figure 3: Showing the pmf of a distribution over symbols (left) and a visualization of the mapping $d$ (middle and right). In the middle and right figures, numbers less than or equal to $s_{\max }$ are plotted, for $s_{\max }=20$ and $s_{\max }=75$. The position of each number $s$ is set to the coordinates $\left(x, s^{\prime}\right)$, where $s^{\prime}, x=d(s)$. The heights of the bars are thus determined by the ratio $s^{\prime} / s$ from eq. 23), and can be seen to approach the heights of the lines in the histogram on the left (that is, to approach $P(x))$ as the density of numbers increases.

We compute $d$ as follows:

```
def d(s):
    s := s mod 2
    x, c, p := f
    s' :=pr(s\div2 2r)+\overline{s}-c
    return s', s
```


### 2.7 Inverting the decoder

Having described a decoding process which appears not to throw away any information, we now derive the inverse process, push, and show that it is computationally straightforward.

The push function has access to the symbol $x$ as one of its inputs, and must do two things. Firstly it must stack_push the correct number of elements to $t$ from the lower bits of $s$. Then it must reverse the effect of $d$ on $s$, returning a value of $s$ identical to that before pop was applied.

Thus, on a high level, the inverse of the function pop can be expressed as

```
def push(m, x):
    s, t := m
```

```
p, c := g
s', t := renorm_inverse( }s,t;p

```

![](https://cdn.mathpix.com/cropped/2024_06_04_e46a6b443a3ebc73f4fcg-10.jpg?height=43&width=323&top_left_y=518&top_left_x=608)

```
return s, t
```

where $g_{X}: x \mapsto\left(p_{i}, c_{i}\right)$ with $i$ as above. The function $g_{X}$ is similar to $f_{X}$ in that it is analogous to computing the quantized CDF and mass function $x \mapsto p_{i}$. The function $d^{-1}$ is really a pseudo-inverse of $d$; it is the inverse of $s \mapsto d(s, x)$, holding $x$ fixed.

As mentioned above, renorm_inverse must stack_push the correct amount of data from the lower order bits of $s$ into $t$. A necessary condition which the output of renorm_inverse must satisfy is

$$
\begin{equation*}
2^{r_{s}-r_{t}} \leq d^{-1}\left(s^{\prime} ; p, c\right)<2^{r_{s}} \tag{32}
\end{equation*}
$$

This is because the output of push must be a valid message, as described in Section 2.1, just as the output of pop must be.

The expression for $s^{\prime}$ in eq. (27) is straightforward to invert, yielding a formula for $d^{-1}$ :

$$
\begin{equation*}
d^{-1}\left(s^{\prime} ; p, c\right)=2^{r} \cdot\left(s^{\prime} \div p\right)+s^{\prime} \bmod p+c \tag{33}
\end{equation*}
$$

We can substitute this into eq. (32) and simplify:

$$
\begin{align*}
2^{r_{s}-r_{t}} & \leq 2^{r} \cdot\left(s^{\prime} \div p\right)+s^{\prime} \bmod p+c<2^{r_{s}}  \tag{34}\\
\Longleftrightarrow \quad 2^{r_{s}-r_{t}} & \leq 2^{r} \cdot\left(s^{\prime} \div p\right)<2^{r_{s}}  \tag{35}\\
\Longleftrightarrow \quad p \cdot 2^{r_{s}-r_{t}-r} & \leq s^{\prime}<p \cdot 2^{r_{s}-r} \tag{36}
\end{align*}
$$

So renorm_inverse should move data from the lower order bits of $s^{\prime}$ into $t$ (decreasing $s^{\prime}$ ) until eq. (36) is satisfied. To be specific:

```
def renorm_inverse( ( s', t; p):
    while s s}\geqp\cdot\mp@subsup{2}{}{\mp@subsup{r}{s}{}-r}\mathrm{ :
        t := stack_push(t, s' mod 2 2
        s
    return s', t
```

Although, as mentioned above, eq. (36) is a necessary condition which $s^{\prime}$ must satisfy, it isn't immediately clear that it's sufficient. Is it possible that we need to continue the while loop in renorm_inverse past the first time that $s^{\prime}<p \cdot 2^{r_{s}-r}$ ? In fact this can't be the case, because $s^{\prime} \div 2^{r_{t}}$ decreases $s^{\prime}$ by a factor of at least $2^{r_{t}}$, and thus as we iterate the loop above we will land in the interval specified by eq. 36 at most once. This guarantees that the $s$ that we recover from renorm_inverse is the correct one.

## 3 Further reading

Since its invention by Duda (2009), ANS appears not to have gained widespread attention in academic literature, despite being used in various state of the art
compression systems. At the time of writing, a search on Google Scholar for the string "asymmetric numeral systems" yields 148 results. For comparison, a search for "arithmetic coding", yields 'about 44,000' results. As far as I'm aware, ANS has appeared in only one textbook, with a practical, rather than mathematical, presentation (McAnlis and Haecky, 2016).

However, for those wanting to learn more there is a huge amount of material on different variants of ANS in Duda (2009) and Duda et al. (2015). A parallelized implementation based on SIMD instructions can be found in Giesen (2014) and a version which encrypts the message whilst compressing in Duda and Niemiec (2016). An extension of ANS to models with latent variables was developed by Townsend et al. (2019).

Duda maintains a list of ANS implementations at https://encode.su/ threads/2078-List-of-Asymmetric-Numeral-Systems-implementations.

## Acknowledgements

Thanks to Tom Bird for feedback on drafts of this paper. Thanks also to Szymon Grabowski for pointing out that ANS is covered in McAnlis and Haecky (2016).

## References

Duda, Jarek (2009). Asymmetric Numeral Systems. arXiv: 0902.0271 [cs, math]

Duda, Jarek and Niemiec, Marcin (2016). Lightweight Compression with Encryption Based on Asymmetric Numeral Systems. arXiv: 1612.04662 [cs, math].

Duda, Jarek, Tahboub, Khalid, Gadgil, Neeraj J., and Delp, Edward J. (2015). The Use of Asymmetric Numeral Systems as an Accurate Replacement for Huffman Coding. In 2015 Picture Coding Symposium (PCS), pp. 65-69.

Giesen, Fabian (2014). Interleaved Entropy Coders. arXiv: 1402 . 3392 [cs, math].

MacKay, David J. C. (2003). Information Theory, Inference and Learning Algorithms.

McAnlis, Colton and Haecky, Aleks (2016). Understanding Compression.

Shannon, Claude. E. (1948). A Mathematical Theory of Communication. In The Bell System Technical Journal 27.3, pp. 379-423.

Townsend, James, Bird, Thomas, and Barber, David (2019). Practical Lossless Compression with Latent Variables Using Bits Back Coding. In International Conference on Learning Representations (ICLR).

Witten, Ian H., Neal, Radford M., and Cleary, John G. (1987). Arithmetic Coding for Data Compression. In Communications of the ACM 30.6, pp. 520-540.

