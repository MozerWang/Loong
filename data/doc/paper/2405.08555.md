# Dual-Branch Network for Portrait Image Quality Assessment 

Wei Sun ${ }^{1}$, Weixia Zhang ${ }^{1 *}$, Yanwei Jiang ${ }^{1}$, Haoning Wu ${ }^{2}$, Zicheng Zhang ${ }^{1}$,<br>Jun Jia ${ }^{1}$, Yingjie Zhou ${ }^{1}$, Zhongpeng Ji ${ }^{3}$, Xiongkuo Min ${ }^{1}$, Weisi Lin $^{2}$, Guangtao Zhai ${ }^{1 *}$<br>${ }^{1}$ Shanghai Jiao Tong University, ${ }^{2}$ Nanyang Technological University, ${ }^{3}$ Huawei


#### Abstract

Portrait images typically consist of a salient person against diverse backgrounds. With the development of mobile devices and image processing techniques, users can conveniently capture portrait images anytime and anywhere. However, the quality of these portraits may suffer from the degradation caused by unfavorable environmental conditions, subpar photography techniques, and inferior capturing devices. In this paper, we introduce a dual-branch network for portrait image quality assessment (PIQA), which can effectively address how the salient person and the background of a portrait image influence its visual quality. Specifically, we utilize two backbone networks (i.e., Swin Transformer-B) to extract the quality-aware features from the entire portrait image and the facial image cropped from it. To enhance the quality-aware feature representation of the backbones, we pre-train them on the large-scale video quality assessment dataset LSVQ and the large-scale facial image quality assessment dataset GFIQA. Additionally, we leverage LIQE, an image scene classification and quality assessment model, to capture the qualityaware and scene-specific features as the auxiliary features. Finally, we concatenate these features and regress them into quality scores via a multi-perception layer (MLP). We employ the fidelity loss to train the model via a learning-torank manner to mitigate inconsistencies in quality scores in the portrait image quality assessment dataset PIQ. Experimental results demonstrate that the proposed model achieves superior performance in the PIQ dataset, validating its effectiveness. The code is available at https: //github.com/sunwei925/DN-PIQA.git.


## 1. Introduction

Image quality assessment [62] (IQA) plays an important role in various image processing systems. Over the past two decades, numerous IQA studies have been conducted to investigate image quality from both the subjective and[^0]

objective perspectives. Subjective IQA studies [35] normally designed a human study to collect the ground truth (i.e. mean opinion scores, etc.) of the image quality. Typical subjective IQA methods include single stimulus, double stimulus, stimulus comparison, etc. [56] Objective IQA studies utilize the computational models to compute the image quality aligned with human perception, which are further divided into three categories: full-reference (FR) [65], reduced-reference (RR) [10], and no-reference (NR) [20] based on whether reference images are required. The NR IQA models are also known as blind image quality assessment (BIQA).

In literature, most IQA models are designed for natural scene images (NSIs), including both synthetically distorted NSIs [38] and authentically distorted NSIs [50]. For example, natural scene statistics (NSS)-based IQA methods like BRISQUE [36] and NIQE [37] utilize the statistics characteristics of natural scene images to evaluate their quality. However, with the development of camera devices and image generation techniques, various types of images such as portrait images [4], HDR images [15], VR images [11, 48, 51], gaming images [57, 58], point cloud $[69,70,72]$, etc., have emerged. These images may possess characteristics significantly different from natural scene images, necessitating specific IQA methods for evaluating their quality.

In this paper, we investigate the portrait image quality assessment (PIQA) problem. Portrait image is a kind of photography style that primarily focuses on the face and expression of subjects, while minimizing emphasis on the background. Nowadays, the camera systems [9] of most mobile devices provide the portrait model to capture highquality portrait images by simulating the shallow depth of field effect commonly achieved with professional cameras and prime. Specifically, the depth sensing module of mobile devices utilizes dual or multiple camera setups, in conjunction with specialized sensors like depth sensors or time-offlight (ToF) sensors to capture depth information for distinguishing the human face and the background. Subsequently, the camera imaging system analyzes the scene to identify the person, ensuring it remains in focus while blur-

![](https://cdn.mathpix.com/cropped/2024_06_04_754f16240dbbaf01bf31g-02.jpg?height=364&width=485&top_left_y=257&top_left_x=348)

(1) Noise

![](https://cdn.mathpix.com/cropped/2024_06_04_754f16240dbbaf01bf31g-02.jpg?height=352&width=501&top_left_y=667&top_left_x=343)

(4) Overexposure

![](https://cdn.mathpix.com/cropped/2024_06_04_754f16240dbbaf01bf31g-02.jpg?height=364&width=287&top_left_y=257&top_left_x=865)

(2) Noise

![](https://cdn.mathpix.com/cropped/2024_06_04_754f16240dbbaf01bf31g-02.jpg?height=369&width=290&top_left_y=669&top_left_x=863)

(5) Blur

![](https://cdn.mathpix.com/cropped/2024_06_04_754f16240dbbaf01bf31g-02.jpg?height=366&width=501&top_left_y=256&top_left_x=1213)

(3) Incorrect WB

![](https://cdn.mathpix.com/cropped/2024_06_04_754f16240dbbaf01bf31g-02.jpg?height=369&width=487&top_left_y=669&top_left_x=1212)

(6) Motion Blur

Figure 1. Some example of typical portrait image distortions.

ring the background. Additionally, the camera imaging system may employ image enhancement techniques such as skin smoothing, facial contouring, and lighting adjustments to enhance the overall appearance of the portrait.

During the portrait imaging process, various distortions may be introduced to the portrait image, thus degrading its perceptual quality. For example, the imaging system must accurately distinguish between the person and the background to effectively enhance the person while blurring the background. If the imaging system fails to do it, the person may also be blurred, leading to a poor image. The lighting also has a significant impact on portrait image quality. Extremely low-light environments prompt the camera to increase the gain of luminance channel in images, thereby amplifying the noise. Conversely, overly bright environments may cause background areas to be overexposed. Moreover, diverse image enhancement methods integrated into imaging systems may introduce issues like color shift, overenhancement, etc., which still be a challenging problem for the IQA field. We list some typical portrait image distortions in Figure 1.

Recently, some works have tried to study the portrait image quality assessment problem. Chahine et al. [4] first investigate the PIQA problem subjectively. They construct a large-scale portrait IQA dataset PIQ, consisting of 5,116 images of 50 predefined scenarios by 100 mobile phones, and then perform a human study to annotate the quality label of each image through the pairwise comparison method. Based on PIQ, they develop a semantic-aware
BIQA portrait IQA method SEM-HyperIQA [4], which employs Hyper-IQA to capture the semantic information and utilizes the multi-task learning to derive the quality scores and image scene types, thus enforcing the model to perform scene-specific rescaling. They then introduce FHIQA [7], a PIQA method that also introduces a quality score rescaling method based on image semantics, to improve the ability of fine-grained image quality assessment and ensure robust generalization ability. They [6] further utilize a siamese netowrk (i.e., ResNet-50) to extract quality-aware features from two different portrait images and employ a hub layer for pairwise comparisons training.

Although significant efforts have been made to explore the PIQA problem, there is still room to develop a more effective PIQA method. First, previous PIQA methods [4,6] typically involve cropping the high-resolution portrait image into several small-scale (i.e., $224 \times 224$ ) patches and then extracting the semantic and quality-aware features from these patches. It is important to note that the visual quality of portrait images is strongly influenced by both the person's face and their background. However, small patches cannot may not effectively capture complete semantic information and also may not adequately explore the relationship of how face and background features interactively influence the quality of portrait images. Second, the person and scene diversity in the current PIQA dataset is relatively limited, potentially hindering the training of a robust PIQA model capable of generalizing across various subjects and capturing scenes. Third, the current PIQA dataset PIQ [4]
utilizes the pairwise comparison method to assess the relative quality between two portrait images and calculate the just-objectionable-difference (JOD) as the ground truth of quality scores. Therefore, the JOD scores are only meaningful when comparing two portraits from the same scene. While previous studies have attempted the scene-specific rescaling and sparse comparison training methods to train the PIQA model, we demonstrate that a simpler learningto-rank training method can achieve better performance.

To address the above challenges, we propose a dualbranch neural network for portrait image quality assessment. The proposed PIQA model consists of two branch networks: one is dedicated to extracting the semantic and quality-aware features from the entire portrait image, while the other is responsible for extracting the corresponding features from the facial image cropped from the portrait. This approach allows the proposed model to consider both the influence of the person and the background on the visual quality. To further encode the image into scene and quality-aware features, we employ LIQE [67], a CLIP [42] based image scene classification and image quality assessment model, to encode the portrait image into scene-aware and quality-aware features.

Subsequently, we concatenate features from the entire image and facial features and LIQE feature and perform a two-layer multi-layer perception (MLP) to regress these features into the image score. To improve the model robustness and generalization, we pre-train the two branch networks on LSVQ [61] and GFIQA [46] datasets respectively. LSVQ is a large-scale video quality assessment (VQA) dataset consisting of 38,811 video collected from the Internet. We choose the VQA dataset for pre-training because, compared to IQA datasets, it can provide more diverse scenes, complex distortions, and a larger number of frames for training, thus allowing to train a more powerful quality assessment model. GFIQA contains 20, 000 human faces along with the quality labels, thus enabling the model to learn a quality-aware representation for facial images. During the training, we randomly sample pairs of portrait images from the same scene and employ the fidelity loss to optimize the model. Experimental results demonstrate that the proposed model achieves the best performance on the portrait image quality assessment dataset PIQ [4], validating its effectiveness.

## 2. Related Work

Since the portrait image quality assessment primarily focuses on face quality, this section reviews face-related IQA studies.

### 2.1. Face-related IQA Datasets

Previous face image datasets including LFW [18], VGGFace2 [3], CASIA-WebFace [60], etc., are primarily de- signed for face recognition and do not include the perceptual quality labels. While for general IQA datasets, such as LIVE [45], CSIQ [25], TID2013 [40], KADID-10k [28], KonIQ-10k [17], SPAQ [12] etc., they normally focus on the visual quality of natural scene images with synthetic or authentic distortions rather than solely focusing on human faces. To investigate the problem of face-related image quality assessment, several subjective quality assessment studies have been conducted. For example, Liao et al. [26] construct a large-scale IQA dataset including 22, 720 various facial images, each of which was rated by 10 individuals on a five-point quality scale. Su et al. [46] create GFIQA, a large-scale face IQA dataset comprising 20,000 face images with authentic distortions, with each face corresponding to an individual person. Liu et al. [29] establish the FIQA dataset, which contains of 42,125 face images corrupted by diverse types of distortions, including single distortions (e.g., compression, blur, noise, etc.), enhancement algorithm-introduced distortions, and mixed distortions, etc. Chahine et al. develop PIQ [4], a portrait IQA dataset consisting of 5,116 single portrait images labeled by experts across three quality dimensions: overall, details and exposure. Importantly, the quality scores in PIQ are derived solely through comparative evaluations.

Besides these 2D face IQA studies, increasing attention has been paid to quality assessment of $3 \mathrm{D}$ human faces ${ }^{1}$ and talking heads. For instance, Zhang et al. have introduced several quality assessment datasets including DHHQA [76], SJTU-H3D [71], and DDH-QA [75], focusing on 3D digital human faces, static and dynamic 3D digital humans, respectively. Zhang et al. [68] investigate the quality of talking head videos and thus construct a dataset consisting of videos that were generated by audio-driven talking head generation algorithms. Zhou et al. [78] create the THQA dataset, rated 800 talking videos that were generated by eight speech-driven methods by 40 subjects.

### 2.2. Face-related IQA Methods

In past two decades, many general IQA methods have been proposed, including knowledge-driven methods [14, $36,38,43,52,63,64,69]$ and data-driven methods $[1,21,22$, $31-34,47,49,49-51,53,58,66,73,80]$. While these methods have demonstrated promising performance in assessing the quality of general images, they still lack sufficient capability to evaluate the quality of facial images. Therefore, some studies have been dedicated to developing effective IQA methods for facial images. We divide them into utilityoriented face IQA methods and fidelity-oriented face IQA methods.

Utility-oriented face IQA methods are used to evaluate whether a face image is suitable for face recognition systems. Some studies [23,44,59] primarily focus on the effect[^1]

Feature Extraction

![](https://cdn.mathpix.com/cropped/2024_06_04_754f16240dbbaf01bf31g-04.jpg?height=513&width=1520&top_left_y=329&top_left_x=281)

Figure 2. The framework of the proposed model. It consists of two branches, where one branch is used to extract the full image features and the other is utilized to extract the facial image features. We further extract the LIQE features as the auxiliary features to enhance the feature representation capabilities.

of specific factors such as illumination, blur, noise, head pose, etc., on the quality of face recognition quality. For example, Wasnik et al. [59] develop a facial image quality metric based on vertical edge density, which can robustly estimate pose variations and enhance the quality assessment of a face image with different head poses. Khodabakhsh et al. [23] analyze the evaluation effect of different kinds of objective face IQA methods as well as subjective scores provided by human subjects for face recognition systems using facial images captured under varying head poses, illumination conditions, and distances. Recently, some studies $[2,13,16,19,27,39]$ propose more general face quality assessment metrics for in-the-wild facial images to assist face recognition. For example, Hernandez et al. propose FaceQnet [16], which selects the highest quality images as a gold-standard reference and computes the Euclidean distance between distorted images and the golden images in the features spaces as the quality score. Boutros et al. [2] propose CR-FIQA, which evaluates the face quality by predicting its relative classifiability. This classifiability is then assessed based on the allocation of the feature representation of the training sample in angular space relative to its class center and the nearest negative class center. Kim et al. [24] develop IG-FIQA, a method aimed at addressing the issue where the quality scores used as pseudo-labels assigned from images of classes with low intra-class variance may not be unrelated to the actual quality. They thus propose a weight parameter to alleviate this negative impact.

Fidelity-oriented face IQA methods aim to evaluate the perceptual visual quality of facial images. Liao et al. [26] extract the Gabor features of facial images and then employ a hierarchical binary decision tree-based SVM to train a face quality evaluator. Su et al. [46] develop a face IQA model by leveraging generative priors. They extract intermediate generative features of images as latent references corresponding to the distorted target images to enhance the quality evaluation performance. Liu et al. [29] introduce TransFQA, which uses the facial component guided Transformer network to integrate the global context, face region, and facial component detail features via the progressive attention mechanism. What's more, a distortionspecific prediction network is employed to derive the quality scores. For portrait IQA, Chahine et al. introduce SEMHyperIQA [4], which utilizes Hyper-IQA to capture the semantic information and employs the multi-task learning to derive the quality scores and image scene types. This approach enforces the model to perform scene-specific rescaling. Chahine et al. develop FHIQA [7], which performs a quality score rescaling method based on image semantics to enhance fine-grained image quality assessment and ensure robust generalization ability. Chahine et al. [6] further use a Siamese network to extract quality-aware features of two different portrait images and utilize a hub layer for pairwise comparisons training. For digital human face, Zhou et al. propose a no-reference quality assessment approach based on multi-task learning for the DHHQA dataset [79], while Zhang et al. pioneer quality assessment methods for 3D DH by integrating 3D features, action features, and prompts $[8,74,77]$.

## 3. Proposed Model

In this section, we introduce the proposed dual-branch PIQA model in detail. Our model is motivated by the observation that the quality of portrait images is primarily influenced by both the facial image region and the background region. Therefore, we perform a dual-branch network to
extract the quality-aware features from the facial image and the entire image respectively. Additionally, we leverage a pre-trained model (i.e., LIQE [67]) for image quality assessment and scene classification model to extract the auxiliary quality-ware and scene-specific features to enhance the proposed model.

### 3.1. Dual-branch Network

As depicted in Figure 2, we design a two-branch network structure for portrait image quality assessment. Our framework is very simple, comprising only two Swin Transformer-B [30] as the feature extractors, where one branch processes the entire image and the other focuses on the facial image. We do not adopt the commonly used Siamese structure in previous IQA studies because we think the facial images and the entire images exhibit distinct characteristics and require different model weights to extract the optimal quality-aware features. Moreover, as an effective optimization approach, we pre-train the two feature extractors on the public I/VQA datasets to enhance the feature extraction capabilities. Specifically, for Swin Transformer-B processing the entire image as input, we train it on LSVQ [61] using the method in [54] to obtain the general quality-aware feature extractor. For the other Swin-Transformer-B, we train it on GFIQA [46] to obtain the facial quality-aware feature extractor. We formulate this procedure as follows:

$$
\begin{align*}
\mathcal{F}_{\text {full }} & =\operatorname{Swin}_{\text {full }}\left(\boldsymbol{x}_{\text {full }}\right)  \tag{1}\\
\mathcal{F}_{\text {facial }} & =\operatorname{Swin}_{\text {facial }}\left(\boldsymbol{x}_{\text {facial }}\right)
\end{align*}
$$

where $x_{\text {full }}$ and $x_{\text {facial }}$ are the entire portrait image and the facial image cropped from the entire image using the face detection method YOLO5Face [41], respectively. $\mathcal{F}_{\text {full }}$ and $\mathcal{F}_{\text {facial }}$ are the quality-aware features of the entire portrait image and the facial image extracted by the feature extractors $\operatorname{Swin}_{\text {full }}$ and $\operatorname{Swin}_{\text {facial }}$ respectively.

### 3.2. LIQE Features

LIQE [67] is a visual-language model for image quality assessment and scene classification. It utilizes CLIP, consisting of an image encoder and a text encoder, to compute the cosine similarity between text features and image features. For LIQE, it takes a text prompt $\boldsymbol{t}(s, d, c)=$ " a photo of a(n) $\{s\}$ with $\{d\}$ artifacts, which is of $\{c\}$ quality" along with an image as the inputs and calculate the cosine similarity between text features and image features to represent the probability of how well the text prompt describes the tested image. So, these probabilities can be utilized to infer the scene type, artifact type, and quality level of the tested image.

In this section, we use the probabilities derived from various types of text prompts as auxiliary features to characterize the scene, artifact, and quality-level attributes of video frames. We consider nine scene categories: $s \in S=$ \{"animal", "cityscape", "human", "indoor scene", "landscape", "night scene", "plant", "still-life", and "others"\}, eleven distortion types: $d \in D=\{$ "blur", "color-related", "contrast", "JPEG compression", "JPEG2000 compression", "noise", "overexposure", "quantization", "underexposure", "spatially-localized", and "others" \}, and five quality levels: $c \in C=\{1,2,3,4,5\}=\{$ "bad", "poor", "fair", "good", "perfect"\}. Thus, we have a total of 495 text prompt candidates to compute the probabilities:

$$
\begin{equation*}
\left.\mathcal{F}_{\mathrm{LIQE}}=\operatorname{LIQE}\left(\boldsymbol{x}_{\boldsymbol{f} u l l}, \boldsymbol{t}(\boldsymbol{s}, \boldsymbol{d}, \boldsymbol{c})\right)\right) \tag{2}
\end{equation*}
$$

where $\mathcal{F}_{i}^{\mathrm{LIQE}}$ represents the LIQE features, which contains 495 dimensions corresponding to the scene category, artifact type, and quality level characteristics.

### 3.3. Quality Regression

After calculating these features, we further concatenate these features into the final feature representation $\mathcal{F}$ :

$$
\begin{equation*}
\mathcal{F}=\operatorname{Cat}\left(\mathcal{F}_{\text {full }}, \mathcal{F}_{\text {facial }}, \mathcal{F}_{L I Q E}\right) \tag{3}
\end{equation*}
$$

where Cat is the concatenation operator.

We then use a two-layer MLP network to regress $\mathcal{F}$ into the quality score:

$$
\begin{equation*}
\hat{q}=\operatorname{MLP}(\mathcal{F}) \tag{4}
\end{equation*}
$$

where MLP denotes the MLP operator and $\hat{q}$ is the quality score of the portrait image.

### 3.4. Loss Function

Previous IQA studies normally utilize either L1 or L2 loss to minimize the discrepancy between model quality scores close to ground-truth quality scores during the training. However, the ground-truth quality scores (et al. JODs) in PIQ are obtained through the pairwise comparison experiment, where only images from the same scenes were compared. It may cause inconsistent quality scales of images from different scenes. Therefore, we adopt the learning-torank training method, which can make the model learn the rank information of images from the same scene.

In the training stage, we randomly sample two images $(\boldsymbol{x}, \boldsymbol{y})$ from the same scene and calculate the quality score of two images $\hat{q}(\boldsymbol{x})$ and $\hat{q}(\boldsymbol{y})$ through the proposed model. We then compute a binary label according to their groundtruth JODs:

$$
p(\boldsymbol{x}, \boldsymbol{y})= \begin{cases}0 & \text { if } \quad q(\boldsymbol{x}) \geq q(\boldsymbol{y})  \tag{5}\\ 1 & \text { otherwise }\end{cases}
$$

where $q(\boldsymbol{x})$ and $q(\boldsymbol{y})$ are the ground-truth JODs of the image pair $(\boldsymbol{x}, \boldsymbol{y})$. We estimate the probability of $\boldsymbol{x}$ perceived better than $\boldsymbol{y}$ as

Table 1. The performance of the proposed model and competing BIQA models on the PIQ dataset

| Model $\quad$ Attribute | Overall |  |  |  | Exposure |  |  |  | Details |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | SRCC | PLCC | KRCC | MAE | SRCC | PLCC | KRCC | MAE | SRCC | PLCC | KRCC | MAE |
| DB-CNN (CIVE C) | 0.59 | 0.64 | 0.43 | 1.04 | 0.69 | 0.69 | 0.51 | 0.91 | 0.59 | 0.51 | 0.45 | 0.99 |
| MUSIQ (KonI0-10k) | 0.76 | 0.75 | 0.57 | 0.95 | 0.74 | 0.70 | 0.55 | 0.93 | 0.71 | 0.67 | 0.52 | 0.88 |
| MUSIQ (PaQ-2-PiQ) | 0.74 | 0.74 | 0.54 | 1.09 | 0.79 | 0.78 | 0.59 | 0.87 | 0.72 | 0.77 | 0.53 | 0.90 |
| HyperIOA | 0.74 | 0.74 | 0.55 | 0.99 | 0.69 | 0.68 | 0.50 | 0.86 | 0.70 | 0.67 | 0.50 | 0.94 |
| SEM-HyperIQA | 0.75 | 0.75 | 0.56 | 1.03 | 0.72 | 0.70 | 0.53 | 0.97 | 0.73 | 0.65 | 0.55 | 0.88 |
| SEM-HyperIQA-CO | 0.74 | 0.74 | 0.55 | 1.04 | 0.70 | 0.70 | 0.52 | 0.94 | 0.75 | 0.71 | 0.55 | 0.85 |
| FHIQA | 0.78 | 0.78 | 0.59 | 1.12 | 0.76 | 0.71 | 0.57 | 0.85 | 0.74 | 0.72 | 0.55 | 0.80 |
| PICNIQ | 0.81 | 0.81 | 0.61 | 0.72 | 0.77 | 0.80 | 0.60 | 0.76 | 0.83 | 0.81 | 0.64 | 0.72 |
| StairIQA | 0.72 | 0.75 | 0.55 | 0.70 | 0.69 | 0.73 | 0.51 | 0.72 | 0.67 | 0.73 | 0.50 | 0.73 |
| LIQE | 0.81 | 0.80 | 0.62 | 0.64 | 0.85 | 0.82 | 0.66 | 0.66 | 0.85 | 0.82 | 0.67 | 0.60 |
| Proposed | 0.85 | 0.86 | 0.68 | 0.53 | 0.85 | 0.86 | 0.68 | 0.53 | 0.86 | 0.87 | 0.69 | 0.52 |

$$
\begin{equation*}
\hat{p}(\boldsymbol{x}, \boldsymbol{y})=\Phi\left(\frac{\hat{q}(\boldsymbol{x})-\hat{q}(\boldsymbol{y})}{\sqrt{2}}\right) \tag{6}
\end{equation*}
$$

where $\Phi(\cdot)$ is the standard Normal cumulative distribution function, and the variance is fixed to one. We adopt the fidelity loss [55] to optimize the model:

$$
\begin{align*}
\ell(\boldsymbol{x}, \boldsymbol{y} ; \boldsymbol{\theta})= & 1-\sqrt{p(\boldsymbol{x}, \boldsymbol{y}) \hat{p}(\boldsymbol{x}, \boldsymbol{y})} \\
& -\sqrt{(1-p(\boldsymbol{x}, \boldsymbol{y}))(1-\hat{p}(\boldsymbol{x}, \boldsymbol{y}))} \tag{7}
\end{align*}
$$

## 4. Experiment

### 4.1. Experimental Protocol

Test Dataset. We test our model on the PIQ dataset [4], containing 5,116 portrait images, where 3,630 images are used for training and 1,486 images are used for validating. A private test set is further used to test the performance of the proposed model for the NTIRE 2024 Portrait Quality Assessment ft. DXOMARK [5]. ${ }^{2}$.

Implementation Details. As stated in Section 3, we utilize Swin Transformer-B [30] as the backbones. To improve the generalization ability of the backbone networks, we train the branch for the entire image on the LSVQ dataset [61], following the training strategy in [54]. Thus it can learn a general quality-aware feature representation ability. As for the branch for the facial image, we train it on the GFIQA dataset [46], enabling it can learn facial quality-aware feature representation. During the training stage, we resize the resolution of the minimum dimension of portrait images to 448 while preserving their aspect ratios. During the training and test stages, the portrait images are randomly and centrally cropped with a resolution of $384 \times 384$. The Adam optimizer with the initial learning rate $1 \times 10^{-5}$ and batch size 12 is used to train the proposed model on a server with[^2]

2 NVIDIA RTX 3090 GPUs. We decay the learning rate by a factor of 10 after 2 epochs and the total number of epochs is set as 10 .

Compared Models. We compare the proposed model with nine popular BIQA methods, including DB-CNN [66], MUSIQ [22], HyperIQA [47], SEM-HyperIQA [4], and SEM-HyperIQA-CO [4], FHIQA [7], PICNIQ [6], StairIQA [50], and LIQE [67], where EM-HyperIQA, SEM-HyperIQA-CO, FHIQA, and PICNIQ are specifically designed for portrait images.

Evaluation Criteria. We employ four criteria to evaluate the performance of VQA models: PLCC, Spearman rankorder correlation coefficient (SRCC), Kendall rank correlation coefficient (KRCC), and mean absolute error (MAE). Note that PLCC assesses the prediction linearity of the VQA model, SRCC and KRCC evaluate the prediction monotonicity, and MAE reflects prediction accuracy. An outstanding VQA model should achieve SRCC and PLCC values close to 1 and MAE close to 0 . Before computing PLCC, we adhere to the procedure outlined in [56] to map model predictions to MOSs by a monotonic four-parameter logistic function to compensate for prediction nonlinearity. We first calculate the values of four metrics for each individual scene and utilize the average results as the evaluation criteria.

### 4.2. Experimental Results

We present the experimental results in Table 1, from which we draw several conclusions. First, we observe that PICNIQ, LIQE, and the proposed models exhibit significant superiority over other BIQA models. This is attributed to their utilization of the learning-to-rank strategy for optimizing the BIQA models, whereas other methods employ either L2 or Huber Loss directly. The former approach effectively tackles the scaling inconsistency problem in PIQA, leading to better performance. Second, the

Table 2. The results on the private test set of NTIRE 2024 Portrait Quality Assessment Challenge

| Team | SRCC | PLCC | KRCC |
| :---: | :---: | :---: | :---: |
| Xidian-IPPL | 0.554 | 0.597 | 0.381 |
| BDVQAGroup | 0.393 | 0.575 | 0.333 |
| SJTU MMLab (ours) | 0.411 | 0.544 | 0.333 |
| SECE-SYSU | 0.304 | 0.453 | 0.238 |
| $\mathrm{I}^{2}$ Group | 0.357 | 0.433 | 0.286 |

Table 3. The results of ablation studies on the PIQ dataset

| Full Image <br> Branch | Facial Image <br> Branch | LIQE | Overall |  |
| :---: | :---: | :---: | :---: | :---: |
| $\sqrt{ }$ | $\times$ | $\times$ | 0.82 | 0.84 |
| $\times$ | $\sqrt{ }$ | $\times$ | 0.82 | 0.83 |
| $\sqrt{ }$ | $\sqrt{ }$ | $\times$ | 0.84 | 0.85 |
| $\sqrt{ }$ | $\sqrt{ }$ | $\sqrt{ }$ | 0.85 | 0.86 |

proposed model achieves the highest performance among the compared BIQA methods and is also superior to the models SEM-HyperIQA, SEM-HyperIQA-CO, and PICNIQ, which are specifically developed for portrait images, indicating its effectiveness in evaluating the visual quality of portrait images. This demonstrates the rationality of our model structure in respectively extracting features from entire and facial images.

We list the results of NTIRE 2024 Portrait Quality Assessment Challenge in Table 2. From Table 2, it is shown that our model achieves second place in terms of SRCC and KRCC metrics, and third place in terms of PLCC metric, which further demonstrates the effectiveness of the proposed model.

### 4.3. Ablation Studies

In this section, we validate the effectiveness of the components of our model. Specifically, we train three variants of the proposed model: one consisting solely of entire image feature extraction, another consisting solely of facial image feature extraction, and a third consisting of both the full image and facial image feature extraction but without LIQE features. The experimental results are listed in Table 3. From Table 3, we observe that solely utilizing either full image extraction branch or facial image extraction results in subpar performance, which indicates that PIQA needs to consider both person and background characteristics. Removing the LIQE features causes a performance drop of 0.01 in terms of both SRCC and PLCC, suggesting that auxiliary quality-aware and scene-specific features can promote the performance of PIQA.

## 5. Conclusion

In this paper, we develop a dual-branch network for portrait image quality assessment. We consider the PIQA problem as how to quantify the impact of both the facial image region and the background region on visual quality. To achieve this, we employ a dual-branch network to extract quality-aware features, where one branch is dedicated to extracting features from the entire image and the other branch focuses on extracting features from the facial image. To improve the capability of representing quality-aware features, we pre-train the two branches on large-scale VQA and facial IQA datasets, enabling them to adapt to diverse subjects and background scenes. Additionally, we incorporate LIQE features as auxiliary quality-aware and scene-specific features to further enhance the proposed model. Finally, we employ the fidelity loss function to address the label inconsistency problem in the PIQA dataset. Experimental results show that our model outperforms competing BIQA methods on the PIQ dataset by a large margin, demonstrating its effectiveness.

## 6. Acknowledgement

This work was supported in part by the National Natural Science Foundation of China under Grants 62071407, 62301316, 62371283, 62225112, 62376282 and 62271312, the China Postdoctoral Science Foundation under Grants 2023TQ0212 and 2023M742298, the Postdoctoral Fellowship Program of CPSF under Grant GZC20231618, the Fundamental Research Funds for the Central Universities, the National Key R\&D Program of China (2021YFE0206700), the Science and Technology Commission of Shanghai Municipality (2021SHZDZX0102), and the Shanghai Committee of Science and Technology (22DZ2229005).

## References

[1] Sebastian Bosse, Dominique Maniry, Klaus-Robert Müller, Thomas Wiegand, and Wojciech Samek. Deep neural networks for no-reference and full-reference image quality assessment. IEEE Transactions on Image Processing, 27(1):206-219, 2017. 3

[2] Fadi Boutros, Meiling Fang, Marcel Klemt, Biying Fu, and Naser Damer. Cr-fiqa: face image quality assessment by learning sample relative classifiability. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5836-5845, 2023. 4

[3] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE international conference on automatic face \& gesture recognition (FG 2018), pages 67-74. IEEE, 2018. 3

[4] Nicolas Chahine, Stefania Calarasanu, Davide GarciaCiviero, Theo Cayla, Sira Ferradans, and Jean Ponce. An
image quality assessment dataset for portraits. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9968-9978, 2023. 1, 2, 3, 4, 6

[5] Nicolas Chahine, Marcos V. Conde, Gabriel Pacianotto, Daniela Carfora, Benoit Pochon, Sira Ferradans, and Radu Timofte. Deep portrait quality assessment. A NTIRE 2024 challenge survey. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2024.6

[6] Nicolas Chahine, Sira Ferradans, and Jean Ponce. Picniq: Pairwise comparisons for natural image quality assessment. arXiv preprint arXiv:2403.09746, 2024. 2, 4, 6

[7] Nicolas Chahine, Sira Ferradans, Javier Vazquez-Corral, and Jean Ponce. Generalized portrait quality assessment. arXiv preprint arXiv:2402.09178, 2024. 2, 4, 6

[8] Shi Chen, Zicheng Zhang, Yingjie Zhou, Wei Sun, and Xiongkuo Min. A no-reference quality assessment metric for dynamic 3d digital human. Displays, 80:102540, 2023. 4

[9] Mauricio Delbracio, Damien Kelly, Michael S Brown, and Peyman Milanfar. Mobile computational photography: A tour. Annual review of vision science, 7:571-604, 2021. 1

[10] Shahi Dost, Faryal Saud, Maham Shabbir, Muhammad Gufran Khan, Muhammad Shahid, and Benny Lovstrom. Reduced reference image and video quality assessments: review of methods. EURASIP Journal on Image and Video Processing, 2022(1):1, 2022. 1

[11] Huiyu Duan, Xiongkuo Min, Wei Sun, Yucheng Zhu, XiaoPing Zhang, and Guangtao Zhai. Attentive deep image quality assessment for omnidirectional stitching. IEEE Journal of Selected Topics in Signal Processing, 2023. 1

[12] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone photography. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3677-3686, 2020. 3

[13] Patrick Grother, Austin Hom, Mei Ngan, and Kayee Hanaoka. Ongoing face recognition vendor test (frvt)-part 5: Face image quality assessment. NIST Interagency Report, 2020. 4

[14] Ke Gu, Guangtao Zhai, Xiaokang Yang, and Wenjun Zhang. Using free energy principle for blind image quality assessment. IEEE Transactions on Multimedia, 17(1):50-63, 2014. 3

[15] Philippe Hanhart, Marco V Bernardo, Manuela Pereira, António M G. Pinheiro, and Touradj Ebrahimi. Benchmarking of objective quality metrics for hdr image quality assessment. EURASIP Journal on Image and Video Processing, 2015:1-18, 2015. 1

[16] Javier Hernandez-Ortega, Javier Galbally, Julian Fierrez, Rudolf Haraksim, and Laurent Beslay. Faceqnet: Quality assessment for face recognition based on deep learning. In 2019 International Conference on Biometrics (ICB), pages 1-8. IEEE, 2019. 4

[17] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe. Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE Transactions on Image Processing, 29:4041-4056, 2020. 3
[18] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. In Workshop on faces in'Real-Life'Images: detection, alignment, and recognition, 2008. 3

[19] Byungho Jo, Donghyeon Cho, In Kyu Park, and Sungeun Hong. Ifqa: interpretable face quality assessment. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 3444-3453, 2023. 4

[20] Vipin Kamble and KM Bhurchandi. No-reference image quality assessment algorithms: A survey. Optik, 126(1112):1090-1097, 2015. 1

[21] Le Kang, Peng Ye, Yi Li, and David Doermann. Convolutional neural networks for no-reference image quality assessment. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1733-1740, 2014. 3

[22] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5148-5157, 2021. 3, 6

[23] Ali Khodabakhsh, Marius Pedersen, and Christoph Busch. Subjective versus objective face image quality evaluation for face recognition. In Proceedings of the 2019 3rd International Conference on Biometric Engineering and Applications, pages 36-42, 2019. 3, 4

[24] Minsoo Kim, Gi Pyo Nam, Haksub Kim, Haesol Park, and Ig-Jae Kim. Ig-fiqa: Improving face image quality assessment through intra-class variance guidance robust to inaccurate pseudo-labels. arXiv preprint arXiv:2403.08256, 2024 4

[25] Eric C Larson and Damon M Chandler. Most apparent distortion: full-reference image quality assessment and the role of strategy. Journal of electronic imaging, 19(1):011006011006, 2010. 3

[26] Pin Liao, Haixiang Lin, Pingping Zeng, Sixue Bai, Huimin Ma, and Siru Ding. Facial image quality assessment based on support vector machines. In 2012 International Conference on Biomedical Engineering and Biotechnology, pages 810813. IEEE, 2012. 3, 4

[27] Zhang Lijun, Shao Xiaohu, Yang Fei, Deng Pingling, Zhou Xiangdong, and Shi Yu. Multi-branch face quality assessment for face recognition. In 2019 IEEE 19th International Conference on Communication Technology (ICCT), pages 1659-1664. IEEE, 2019. 4

[28] Hanhe Lin, Vlad Hosu, and Dietmar Saupe. Kadid-10k: A large-scale artificially distorted iqa database. In 2019 Eleventh International Conference on Quality of Multimedia Experience (QoMEX), pages 1-3. IEEE, 2019. 3

[29] Tie Liu, Shengxi Li, Mai Xu, Li Yang, and Xiaofei Wang. Assessing face image quality: A large-scale database and a transformer method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 3, 4

[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012-10022, 2021. 5, 6

[31] Wei Lu, Wei Sun, Xiongkuo Min, Zicheng Zhang, Tao Wang, Wenhan Zhu, Xiaokang Yang, and Guangtao Zhai. Blind surveillance image quality assessment via deep neural network combined with the visual saliency. In CAAI International Conference on Artificial Intelligence, pages 136-146. Springer, 2022. 3

[32] Wei Lu, Wei Sun, Xiongkuo Min, Wenhan Zhu, Quan Zhou, Jun He, Qiyuan Wang, Zicheng Zhang, Tao Wang, and Guangtao Zhai. Deep neural network for blind visual quality assessment of $4 \mathrm{k}$ content. IEEE Transactions on Broadcasting, 2022. 3

[33] Wei Lu, Wei Sun, Wenhan Zhu, Xiongkuo Min, Zicheng Zhang, Tao Wang, and Guangtao Zhai. A cnn-based quality assessment method for pseudo $4 \mathrm{k}$ contents. In International Forum on Digital TV and Wireless Multimedia Communications, pages 164-176. Springer, 2021. 3

[34] Kede Ma, Wentao Liu, Kai Zhang, Zhengfang Duanmu, Zhou Wang, and Wangmeng Zuo. End-to-end blind image quality assessment using deep neural networks. IEEE Transactions on Image Processing, 27(3):1202-1213, 2017. 3

[35] Rafał K Mantiuk, Anna Tomaszewska, and Radosław Mantiuk. Comparison of four subjective methods for image quality assessment. In Computer graphics forum, volume 31, pages 2478-2491. Wiley Online Library, 2012. 1

[36] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in the spatial domain. IEEE Transactions on image processing, 21(12):4695-4708, 2012. 1, 3

[37] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a "completely blind" image quality analyzer. IEEE Signal processing letters, 20(3):209-212, 2012. 1

[38] Anush Krishna Moorthy and Alan Conrad Bovik. Blind image quality assessment: From natural scene statistics to perceptual quality. IEEE transactions on Image Processing, 20(12):3350-3364, 2011. 1, 3

[39] Fu-Zhao Ou, Xingyu Chen, Ruixin Zhang, Yuge Huang, Shaoxin Li, Jilin Li, Yong Li, Liujuan Cao, and Yuan-Gen Wang. Sdd-fiqa: unsupervised face image quality assessment with similarity distribution distance. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7670-7679, 2021. 4

[40] Nikolay Ponomarenko, Lina Jin, Oleg Ieremeiev, Vladimir Lukin, Karen Egiazarian, Jaakko Astola, Benoit Vozel, Kacem Chehdi, Marco Carli, Federica Battisti, et al. Image database tid2013: Peculiarities, results and perspectives. Signal processing: Image communication, 30:57-77, 2015. 3

[41] Delong Qi, Weijun Tan, Qi Yao, and Jingfeng Liu. Yolo5face: Why reinventing a face detector. In European Conference on Computer Vision, pages 228-244. Springer, 2022. 5

[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 3
[43] Michele A Saad, Alan C Bovik, and Christophe Charrier. Blind image quality assessment: A natural scene statistics approach in the dct domain. IEEE transactions on Image Processing, 21(8):3339-3352, 2012. 3

[44] Jitao Sang, Zhen Lei, and Stan Z Li. Face image quality evaluation for iso/iec standards 19794-5 and 29794-5. In Advances in Biometrics, pages 229-238. Springer, 2009. 3

[45] Hamid R Sheikh, Muhammad F Sabir, and Alan C Bovik. A statistical evaluation of recent full reference image quality assessment algorithms. IEEE Transactions on image processing, 15(11):3440-3451, 2006. 3

[46] Shaolin Su, Hanhe Lin, Vlad Hosu, Oliver Wiedemann, Jinqiu Sun, Yu Zhu, Hantao Liu, Yanning Zhang, and Dietmar Saupe. Going the extra mile in face image quality assessment: A novel database and model. IEEE Transactions on Multimedia, 2023. 3, 4, 5, 6

[47] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang. Blindly assess image quality in the wild guided by a self-adaptive hyper network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3667-3676, 2020. 3, 6

[48] Wei Sun, Ke Gu, Siwei Ma, Wenhan Zhu, Ning Liu, and Guangtao Zhai. A large-scale compressed 360-degree spherical image database: From subjective quality evaluation to objective model comparison. In 2018 IEEE 20th international workshop on multimedia signal processing (MMSP), pages 1-6. IEEE, 2018 . 1

[49] Wei Sun, Xiongkuo Min, Wei Lu, and Guangtao Zhai. A deep learning based no-reference quality assessment model for ugc videos. In Proceedings of the 30th ACM International Conference on Multimedia, pages 856-865, 2022. 3

[50] Wei Sun, Xiongkuo Min, Danyang Tu, Siwei Ma, and Guangtao Zhai. Blind quality assessment for in-the-wild images via hierarchical feature fusion and iterative mixed database training. IEEE Journal of Selected Topics in Signal Processing, 2023. 1, 3, 6

[51] Wei Sun, Xiongkuo Min, Guangtao Zhai, Ke Gu, Huiyu Duan, and Siwei Ma. Mc360iqa: A multi-channel cnn for blind 360-degree image quality assessment. IEEE Journal of Selected Topics in Signal Processing, 14(1):64-77, 2019. 1, 3

[52] Wei Sun, Xiongkuo Min, Guangtao Zhai, Ke Gu, Siwei Ma, and Xiaokang Yang. Dynamic backlight scaling considering ambient luminance for mobile videos on lcd displays. IEEE Transactions on Mobile Computing, 21(1):110-124, 2020. 3

[53] Wei Sun, Tao Wang, Xiongkuo Min, Fuwang Yi, and Guangtao Zhai. Deep learning based full-reference and no-reference quality assessment models for compressed ugc videos. In 2021 IEEE International Conference on Multimedia \& Expo Workshops (ICMEW), pages 1-6. IEEE, 2021. 3

[54] Wei Sun, Wen Wen, Xiongkuo Min, Long Lan, Guangtao Zhai, and Kede Ma. Analysis of video quality datasets via design of minimalistic video quality models. arXiv preprint arXiv:2307.13981, 2023. 5, 6

[55] Ming-Feng Tsai, Tie-Yan Liu, Tao Qin, Hsin-Hsi Chen, and Wei-Ying Ma. Frank: a ranking method with fidelity loss.

In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 383-390, 2007. 6

[56] VQEG. Final report from the Video Quality Experts Group on the validation of objective models of video quality assessment, 2000. 1, 6

[57] Tao Wang, Wei Sun, Xiongkuo Min, Wei Lu, Zicheng Zhang, and Guangtao Zhai. A multi-dimensional aesthetic quality assessment model for mobile game images. In 2021 International Conference on Visual Communications and Image Processing (VCIP), pages 1-5. IEEE, 2021. 1

[58] Tao Wang, Wei Sun, Wei Wu, Ying Chen, Xiongkuo Min, Wei Lu, Zicheng Zhang, and Guangtao Zhai. A deep learning based multi-dimensional aesthetic quality assessment method for mobile game images. IEEE Transactions on Games, 2022. 1, 3

[59] Pankaj Wasnik, Kiran B Raja, Raghavendra Ramachandra, and Christoph Busch. Assessing face image quality for smartphone based face recognition system. In 2017 5th International Workshop on Biometrics and Forensics (IWBF), pages 1-6. IEEE, 2017. 3, 4

[60] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv preprint arXiv:1411.7923, 2014. 3

[61] Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram, and Alan Bovik. Patch-vq:'patching up'the video quality problem. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1401914029, 2021. 3, 5, 6

[62] Guangtao Zhai and Xiongkuo Min. Perceptual image quality assessment: a survey. Science China Information Sciences, 63:1-52, 2020. 1

[63] Guangtao Zhai, Xiongkuo Min, and Ning Liu. Free-energy principle inspired visual quality assessment: An overview. Digital Signal Processing, 91:11-20, 2019. 3

[64] Guangtao Zhai, Wei Sun, Xiongkuo Min, and Jiantao Zhou. Perceptual quality assessment of low-light image enhancement. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 17(4):1-24, 2021. 3

[65] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. A comprehensive evaluation of full reference image quality assessment algorithms. In 2012 19th IEEE International Conference on Image Processing, pages 1477-1480. IEEE, 2012. 1

[66] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou Wang. Blind image quality assessment using a deep bilinear convolutional neural network. IEEE Transactions on Circuits and Systems for Video Technology, 30(1):36-47, 2018. 3,6

[67] Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang, and Kede Ma. Blind image quality assessment via visionlanguage correspondence: A multitask learning perspective. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14071-14081, 2023. 3, 5,6

[68] Weixia Zhang, Chengguang Zhu, Jingnan Gao, Yichao Yan, Guangtao Zhai, and Xiaokang Yang. A comparative study of perceptual quality metrics for audio-driven talking head videos. arXiv preprint arXiv:2403.06421, 2024. 3

[69] Zicheng Zhang, Wei Sun, Xiongkuo Min, Tao Wang, Wei $\mathrm{Lu}$, and Guangtao Zhai. No-reference quality assessment for 3d colored point cloud and mesh models. IEEE Transactions on Circuits and Systems for Video Technology, 32(11):76187631, 2022. 1,3

[70] Zicheng Zhang, Wei Sun, Xiongkuo Min, Quan Zhou, Jun He, Qiyuan Wang, and Guangtao Zhai. Mm-peqa: Multimodal learning for no-reference point cloud quality assessment. arXiv preprint arXiv:2209.00244, 2022. 1

[71] Zicheng Zhang, Wei Sun, Yingjie Zhou, Haoning Wu, Chunyi Li, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, and Weisi Lin. Advancing zero-shot digital human quality assessment through text-prompted evaluation. arXiv preprint arXiv:2307.02808, 2023. 3

[72] Zicheng Zhang, Wei Sun, Yucheng Zhu, Xiongkuo Min, Wei Wu, Ying Chen, and Guangtao Zhai. Evaluating point cloud from moving camera videos: A no-reference metric. IEEE Transactions on Multimedia, 2023. 1

[73] Zicheng Zhang, Wei Wu, Wei Sun, Danyang Tu, Wei Lu, Xiongkuo Min, Ying Chen, and Guangtao Zhai. Md-vqa: Multi-dimensional quality assessment for ugc live videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1746-1755, 2023. 3

[74] Zicheng Zhang, Yingjie Zhou, Chunyi Li, Kang Fu, Wei Sun, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai. A reduced-reference quality assessment metric for textured mesh digital humans. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2965-2969. IEEE, 2024. 4

[75] Zicheng Zhang, Yingjie Zhou, Wei Sun, Wei Lu, Xiongkuo Min, Yu Wang, and Guangtao Zhai. Ddh-qa: A dynamic digital humans quality assessment database. In ICME, pages 2519-2524. IEEE, 2023. 3

[76] Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, Yuzhe Wu, and Guangtao Zhai. Perceptual quality assessment for digital human heads. In ICASSP, pages 1-5. IEEE, 2023. 3

[77] Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, and Guangtao Zhai. Geometry-aware video quality assessment for dynamic digital human. In ICIP, pages 1365-1369. IEEE, 2023. 4

[78] Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, and Zhihua Wang. Thqa: A perceptual quality assessment database for talking heads. 2024. 3

[79] Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiongkuo Min, Xianghe Ma, and Guangtao Zhai. A no-reference quality assessment method for digital human head. In ICIP, pages 36-40. IEEE, 2023. 4

[80] Hancheng Zhu, Leida Li, Jinjian Wu, Weisheng Dong, and Guangming Shi. Generalizable no-reference image quality assessment via deep meta-learning. IEEE Transactions on Circuits and Systems for Video Technology, 2021. 3


[^0]:    *Corresponding authors

[^1]:    ${ }^{1}$ They are also called digital faces or digital humans.

[^2]:    ²https://codalab.lisn.upsaclay.fr/competitions/ 17311

