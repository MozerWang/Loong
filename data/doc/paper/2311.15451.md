# Uncertainty-aware Language Modeling for Selective Question Answering 

Qi Yang, Shreya Ravikumar, Fynn Schmitt-Ulms, Satvik Lolla, Ege Demir, Iaroslav Elistratov,<br>Alex Lavaee, Sadhana Lolla, Elaheh Ahmadi, Daniela Rus, Alexander Amini, Alejandro Perez

Themis AI Inc


#### Abstract

We present an automatic large language model (LLM) conversion approach that produces uncertainty-aware LLMs capable of estimating uncertainty with every prediction. Our approach is model- and data-agnostic, is computationallyefficient, and does not rely on external models or systems. We evaluate converted models on the selective question answering setting - to answer as many questions as possible while maintaining a given accuracy, forgoing providing predictions when necessary. As part of our results, we test BERT and Llama 2 model variants on the SQuAD extractive QA task and the TruthfulQA generative QA task. We show that using the uncertainty estimates provided by our approach to selectively answer questions leads to significantly higher accuracy over directly using model probabilities.


## Introduction

Large-language models (LLMs) have demonstrated great abilities in natural language tasks, including question answering (QA) wherein the model receives a question as input and outputs a response answer. The QA task is a fundamental component in many LLM applications. However, in order to robustly answer questions accurately, the model must understand context and ground its outputs in knowledge obtained from training data, which will typically contain conflicting information. Indeed, it has been shown that LLMs commonly fail in QA tasks (Geiger et al. 2019), and that these failures are associated with a limited understanding of output confidence, out-of-domain data, ambiguity in input prompts, inconsistent training information, and hallucinations, among others. Selective prediction (El-Yaniv et al. 2010; Geifman and El-Yaniv 2017), i.e., calculating confidence estimates along with predictions to forgo outputs likely to be incorrect, can be used to mitigate some of these issues.

Several approaches utilize the selective prediction to guide question answering tasks (Peñas et al. 2010; Gondek et al. 2012) and generally abstain from answering questions when output confidence is low. The objective is to maintain a given accuracy while answering as many questions as possible, as opposed to the more conventional goal of attempting to answer all questions correctly. One approach used inferred softmax classifier probabilities to calibrate which questions to respond to (Rodriguez et al. 2021). However,

![](https://cdn.mathpix.com/cropped/2024_05_29_f5a4b6d822d1a7ed2195g-01.jpg?height=471&width=848&top_left_y=735&top_left_x=1099)

Figure 1: Robust, uncertainty-aware language modeling. Our methodology converts large language models (LLMs) - agnostic of architecture - into uncertainty-aware variants and applies to generative (i.e., next-token prediction, left) and extractive (i.e., sub-context answering, right) models.

training a calibration model is challenging and softmax classifiers are often unreliable (Guo et al. 2017). Similarly, an out-of-domain (OOD) calibrator can be trained to detect OOD inputs (Kamath, Jia, and Liang 2020) but requires known or synthetic out-of-domain samples and does not consider other sources of inaccuracies like over-represented features or ambiguous labels. Other approaches include modeling and estimating LLM uncertainty (Dong, Quirk, and Lapata 2018; Shen et al. 2022; Chen and Mueller 2023; Lin, Trivedi, and Sun 2023; Collins et al. 2023; Chuang et al. 2023; Quach et al. 2023), fine-tuning calibrators to consider entropy, perplexity, and other metrics (Jiang et al. 2021), and calculating output consistency (Manakul, Liusie, and Gales 2023; Miao, Teh, and Rainforth 2023). Another family of techniques retrieve evidence and verify outputs through external databases (Guo, Schlichtkrull, and Vlachos 2022) or in-context learning (Weng et al. 2022). These solutions require the development of knowledge bases and efficient querying systems which are often not practical and fundamentally limited by the information that exists. A facile, performant, and efficient way to estimate uncertainty directly from models, without the need of external components, is needed to design a general selective question answering framework that is applicable to a wide range of tasks.

We present an uncertainty-based framework for selective QA that accounts for epistemic and aleatoric uncertainty.

We consider both extractive and generative LLM models e.g., masked-language models and autoregressive models respectively, and implement and evaluate a suite of uncertainty quantification (UQ) methods spanning these uncertainty types. We find that while the individual methods significantly increase performance on the selective QA task, it is the combination of methods and metrics that yield the best accuracy. Leveraging this observation and seeking to enable both performance and efficiency, we present an approach to automatically convert LLMs into uncertainty-aware variants and to compose metrics and methods automatically. Our approach is model- and data-agnostic, lightweight, and does not rely on external models or systems.

## Methodology

## Selective Question Answering

Given an input prompt $x$, the question answering task is to output a prediction $\hat{y}$ where $\hat{y} \in Y(x)$, the set of all possible responses, that correctly answers the question. In the selective QA task, the model also outputs $\sigma$, an uncertainty estimate for a given output $\hat{y}$, where $\sigma \in \mathbb{R}$. Given a threshold $\gamma \in \mathbb{R}$, the model outputs $\hat{y}$ if $\sigma<\gamma$ and refrains from responding if this condition is not met. Each $\gamma$ value results in quantities for coverage (i.e., the ratio of questions the model chose to respond to), and accuracy, (i.e., the number of predictions that correctly answer the question). The goal is to maximize the number of questions that can be answered accurately, that is, to increase both coverage and accuracy. In this work, we convert models into uncertainty-aware variants that output aleatoric, epistemic, or composed uncertainty estimates for every prediction. The model learns datadependent thresholds $\gamma$ and outputs answers $(\hat{y}, \sigma)$ where $\sigma<\gamma$. It forgoes providing responses to questions where no candidate with this requirement exists (Alg. 1).

## Models and Datasets

Extractive models and dataset In the extractive question answering task (Wang et al. 2006), each input $x$ represents $(c, q)$ which is composed of context text $c$ and a question $q$. The space of possible answer candidates $y \in Y(x)$ is all sequential segments in $c$ as defined by start and end indices within $c=c_{0}, c_{1}, \ldots, c_{n}$ with each index representing a token in the text. We consider the SQuAD datasets (Rajpurkar et al. 2016; Rajpurkar, Jia, and Liang 2018), a collection of over one hundred of thousand questions with corresponding answers presented as segments from passages of text. We use BERT (Devlin et al. 2018), more specifically bert-base-uncased 108M with the WordPiece Tokenizer (Wu et al. 2016), as our base model to define probability distributions $f(y \mid x)$ over $Y(x)$. We convert this model into an uncertainty-aware variant that outputs uncertainty for each index in the context text for every prediction.

Generative models and dataset In the generative question answering task, input prompts $x$ are composed of sequences of tokens $x_{0}, x_{1}, \ldots x_{n}$ representing questions. A model is used to incrementally predict each subsequent token, starting from the last token in the input prompt, to compose a response $\hat{y}=\hat{y}_{0}, \hat{y}_{1}, \ldots, \hat{y}_{n}$ that correctly answers the question. The space of answer candidates $y \in Y(x)$ includes all possible sequential combinations of tokens in a given vocabulary. We consider the TruthfulQA question answering benchmark (Lin, Hilton, and Evans 2021), 817 questions divided into several categories, meant to represent questions commonly answered incorrectly by humans and therefore likely to be learned by models imitating human text. We use Llama 2 (Touvron et al. 2023), more specifically Llama 2-Chat 7B, which has been fine-tuned for dialogue use cases with a vocabulary of $32 \mathrm{k}$ tokens, as our generative model. We convert this model into an uncertainty-aware variant that outputs uncertainty estimates for the entire vocabulary for every predicted token.

```
Algorithm 1: Uncertainty-Aware Selective Question Answering
    Input: Model $f_{W}(\cdot)$, UQ Metrics $\theta$, Questions $Q$
    Initialize:
    $P \leftarrow \emptyset$
    $g(\cdot) \leftarrow \Phi_{\theta}\left(f_{W}\right) \quad \triangleright$ Uncertainty-Aware Model Conversion
    foreach $q \in Q$ do
        for $i \in 1 . . T$ do
                $\hat{y}, \sigma \leftarrow g(q) \quad \triangleright$ Inference
                if $\sigma<\gamma$ then
                    $P \leftarrow P \cup\{(\hat{y}, \sigma)\} \quad \triangleright$ Selected Predictions
    Return: $P$
```

Model training and evaluation Our extractive model, bert-base-uncased, is pre-trained on Book Corpus (Zhu et al. 2015) and Wikipedia (Devlin et al. 2018). We further train for 3 epochs on the 130,319 training samples provided in the SQuAD 2.0 dataset (Rajpurkar, Jia, and Liang 2018). The uncertainty-aware variants are created by converting the pre-trained model before training. We evaluate all models on the 11,873 questions in the SQuAD 2.0 test set, reporting accuracy for Exact Match and F1 metrics. We use the NeMo framework (Kuchaiev et al. 2019) to train and evaluate our models. Our generative model, Llama 2-Chat 7B, is pre-trained and fine-tuned for dialogue use cases. The uncertainty-aware variant is created by converting this version of the model directly. No further training or modifications are performed. We report accuracy using the BLEURT metric (Sellam, Das, and Parikh 2020).

## Uncertainty Methods and Metrics

The uncertainty-aware model conversion produces a new model that is able to estimate multiple different types of uncertainty, which we discuss in the following section.

Aleatoric uncertainty captures incertitude resulting from data (e.g., irreducible noise, labeling errors, classes with low separation, etc). It quantifies what a model cannot understand given the data provided. We model aleatoric uncertainty using Mean and Variance Estimation (MVE) (Nix and Weigend 1994). In regression, a layer predicts model output deviations and is trained using a negative log-likelihood loss. An algorithm that generalizes to the classification case is given in Alg. 2. We assume the logits are drawn from a normal distribution and stochastically sample from them using the reparameterization trick. We average stochastic samples and backpropagate using cross
entropy loss through logits and their inferred uncertainties.

```
Algorithm 2: Aleatoric Uncertainty in Classification
    $\mu, \sigma \leftarrow f_{W}(x) \quad \triangleright$ Inference
    for $i \in 1 . . T$ do $\quad \triangle$ Stochastic logits
        $\tilde{z} \leftarrow \mu+\sigma \times \epsilon \sim \mathcal{N}(0,1)$
    $\tilde{z} \leftarrow \frac{1}{N} \times \sum_{i=1}^{T} \tilde{z}$
            $\triangleright$ Average logit
    $\hat{y} \leftarrow \frac{\exp (\tilde{z})}{\sum_{j} \exp \left(\tilde{z}_{j}\right)}$
    $\mathcal{L}(x, y) \leftarrow-\sum_{j} y_{j} \log p_{j} \quad \triangleright$ Cross entropy loss
```

Epistemic uncertainty captures uncertainty arising from the predictive process. It quantifies the inherent limitations in the model or lack of knowledge, intuitively representing what the model does not know. We provide a unified approach for a variety of epistemic uncertainty methods.

A Bayesian neural network can be approximated by stochastically sampling, during inference, from a model with probabilistic layers (Blundell et al. 2015; Gal and Ghahramani 2016). Similarly, models of arbitrary depth that follow sampling-based procedures to temporarily remove units (Srivastava et al. 2014) from all layers are equivalent to approximations to the probabilistic deep Gaussian process (Damianou and Lawrence 2013) and can be used to estimate predictive uncertainty (Gal and Ghahramani 2016; Lemay et al. 2022; Mobiny et al. 2021). We calculate epistemic uncertainty using Monte Carlo sampling (MC), i.e., running $T$ stochastic forward passes and computing the first and second moments from these samples, yielding predictions and uncertainty estimates, respectively. Ensembles of $N$ models, each a randomly initialized stochastic sample, is another common approach used to estimate epistemic uncertainty (Lakshminarayanan, Pritzel, and Blundell 2017), but incurs a significant, multiplicative computational cost.

## Uncertainty-aware Model Conversion

Traditionally, models output predictions in the form of $\hat{y}=$ $f_{W}(x)$. Our method applies a conversion, $\Phi$, to build an uncertainty-aware model to measure uncertainty metrics $\theta$ :

$$
\begin{gathered}
g(\cdot) \leftarrow \Phi_{\theta}\left(f_{W}\right) \\
\hat{y}, \sigma=g(x)
\end{gathered}
$$

where $\sigma$ is the estimated uncertainty. Our conversion procedure adds and modifies relevant model components while preserving structure and function. This allows the new model to serve as a drop-in replacement that is additionally able to estimate uncertainty metrics. All modifications are integrated into a custom, metric-specific forward pass and training step that integrates during training and inference. We use the Capsa framework (Lolla et al. 2022, 2023) to perform model conversions. We refer readers to the Capsa software library (Amini et al. 2023) for information on the software with the functionality described in this publication.

## Results

In this section we present results for conventional and uncertainty-guided selective question answering. We use our conversion procedure to automatically create uncertaintyaware variants for several UQ metrics and evaluate in both extractive and generative QA tasks.
Table 1: Accuracy of model per uncertainty method.

|  | BERT-base SQuAD 2.0 |  |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | Baseline | MVE | Ensemble | MC |  |
| Exact | $72.00 \%$ | $73.12 \%$ | $74.96 \%$ | $72.72 \%$ |  |
| F1 | $75.17 \%$ | $76.27 \%$ | $77.83 \%$ | $75.97 \%$ |  |
| HasAns Exact | $70.58 \%$ | $69.18 \%$ | $70.86 \%$ | $72.42 \%$ |  |
| HasAns F1 | $76.94 \%$ | $75.51 \%$ | $76.62 \%$ | $78.93 \%$ |  |
| NoAns Exact | $73.41 \%$ | $77.04 \%$ | $79.04 \%$ | $73.02 \%$ |  |
| NoAns F1 | $73.41 \%$ | $77.04 \%$ | $79.04 \%$ | $73.02 \%$ |  |

Nominal question answering performance For our initial evaluation we create MVE, MC, and Ensemble variants of our pre-trained extractive QA model. We train them for 3 epochs on the $130 \mathrm{~K}$ samples provided in the SQuAD 2.0 dataset and collect Exact Match and F1 accuracy results for the $11 \mathrm{~K}$ test questions. In Tab. 1 we find that the uncertaintyaware variants have performance that is consistent with the base model with no significant reductions in accuracy.

Uncertainty-guided selective question answering Having verified that $\mathrm{QA}$ performance remains consistent after model conversions, we compare the performance of the different UQ variants on the uncertainty-guided selective QA setting. For our extractive QA task, we evaluate models on the 5,928 answerable questions in the SQuAD test set. For the generative QA task, we generate multiple responses to all 817 questions in the TruthfulQA benchmark.

We compare the coverage and accuracy obtained by the MVE, Ensemble, and MC UQ variants when answering questions using their uncertainty estimates as the selective prediction criteria. We further compare the UQ variants to the original model, using the baseline logit probabilities (e.g., from softmax) as the confidence value to guide selective predictions. These results are outlined in Tab. 2 and Fig. 2. Using UQ metrics as a measure of confidence leads to increased accuracy while answering larger portions of the questions, whereas using logit probability does not. More importantly, we observe that in both the extractive and generative case, predictions are least accurate when logit probability confidence is highest. In fact, performance gradually deteriorates to $0 \%$ as confidence increases, indicating logit

![](https://cdn.mathpix.com/cropped/2024_05_29_f5a4b6d822d1a7ed2195g-03.jpg?height=371&width=854&top_left_y=1842&top_left_x=1096)

BERT-base (SQuAD)

![](https://cdn.mathpix.com/cropped/2024_05_29_f5a4b6d822d1a7ed2195g-03.jpg?height=325&width=415&top_left_y=1881&top_left_x=1102)

Llama 2 7B (TruthfulQA)

![](https://cdn.mathpix.com/cropped/2024_05_29_f5a4b6d822d1a7ed2195g-03.jpg?height=328&width=434&top_left_y=1877&top_left_x=1515)

Figure 2: Selective answering accuracy by confidence level. Increasing values of logit probability do not correspond to increased question answering ability - despite being often misconceived as a measure of confidence. Our methods report a reliable measure of confidence - with increased confidence corresponding to increased accuracy.

Table 2: Selective question answering accuracy. Accuracy of the model across increasing levels of confidence percentile thresholds (i.e., top-to-bottom, least-to-most confident). Bold indicates top performing uncertainty method per confidence level.

(a) BERT-base on SQuAD

| Percentile | Logit Probability | MVE | Ensemble | MC | Composed | Coverage |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 0.0 | $70.58 \%$ | $69.19 \%$ | $70.87 \%$ | $\mathbf{7 2 . 4 1 \%}$ | $\mathbf{7 2 . 4 1 \%}$ | $100.00 \%$ |
| 10.0 | $73.76 \%$ | $71.98 \%$ | $73.67 \%$ | $75.30 \%$ | $\mathbf{7 5 . 3 5 \%}$ | $90.00 \%$ |
| 20.0 | $75.41 \%$ | $74.59 \%$ | $77.37 \%$ | $77.79 \%$ | $\mathbf{7 7 . 8 4 \%}$ | $80.00 \%$ |
| 30.0 | $76.38 \%$ | $76.67 \%$ | $79.93 \%$ | $80.12 \%$ | $\mathbf{8 0 . 4 0 \%}$ | $70.00 \%$ |
| 40.0 | $76.50 \%$ | $79.11 \%$ | $81.64 \%$ | $81.95 \%$ | $\mathbf{8 2 . 3 7 \%}$ | $60.00 \%$ |
| 50.0 | $76.35 \%$ | $81.28 \%$ | $83.64 \%$ | $83.91 \%$ | $\mathbf{8 4 . 3 8 \%}$ | $50.00 \%$ |
| 60.0 | $76.13 \%$ | $83.13 \%$ | $85.31 \%$ | $85.32 \%$ | $\mathbf{8 5 . 6 2 \%}$ | $40.00 \%$ |
| 70.0 | $75.66 \%$ | $85.67 \%$ | $87.28 \%$ | $86.96 \%$ | $\mathbf{8 7 . 8 6 \%}$ | $30.00 \%$ |
| 80.0 | $73.61 \%$ | $87.69 \%$ | $89.09 \%$ | $89.29 \%$ | $\mathbf{9 0 . 1 3 \%}$ | $20.00 \%$ |
| 85.0 | $71.24 \%$ | $88.76 \%$ | $90.16 \%$ | $90.56 \%$ | $\mathbf{9 1 . 2 4 \%}$ | $15.00 \%$ |
| 90.0 | $64.92 \%$ | $89.54 \%$ | $90.83 \%$ | $91.57 \%$ | $\mathbf{9 4 . 1 0 \%}$ | $10.00 \%$ |
| 95.0 | $49.16 \%$ | $90.24 \%$ | $91.18 \%$ | $\mathbf{9 3 . 2 7 \%}$ | $\mathbf{9 3 . 2 7 \%}$ | $5.00 \%$ |
| 98.0 | $18.49 \%$ | $91.60 \%$ | $89.75 \%$ | $\mathbf{9 4 . 1 2 \%}$ | $93.28 \%$ | $2.00 \%$ |
| 99.0 | $3.33 \%$ | $93.33 \%$ | $91.67 \%$ | $95.00 \%$ | $\mathbf{9 6 . 6 7 \%}$ | $1.00 \%$ |
| 99.9 | $0.00 \%$ | $\mathbf{1 0 0 . 0 0 \%}$ | $\mathbf{1 0 0 . 0 0 \%}$ | $83.33 \%$ | $83.33 \%$ | $0.10 \%$ |

(b) Llama 2-Chat 7B on TruthfulQA

| Percentile | Logit Probability | Epistemic | Coverage |
| :---: | :---: | :---: | :---: |
| 0.0 | $\mathbf{5 7 . 1 7 \%}$ | $\mathbf{5 7 . 1 7 \%}$ | $100.0 \%$ |
| 10.0 | $58.1 \%$ | $\mathbf{6 0 . 6 3 \%}$ | $90.00 \%$ |
| 20.0 | $59.78 \%$ | $\mathbf{6 1 . 9 1 \%}$ | $80.00 \%$ |
| 30.0 | $61.12 \%$ | $\mathbf{6 3 . 5 6 \%}$ | $70.00 \%$ |
| 40.0 | $61.87 \%$ | $\mathbf{6 4 . 8 6 \%}$ | $60.00 \%$ |
| 50.0 | $61.24 \%$ | $\mathbf{6 7 . 1 \%}$ | $50.0 \%$ |
| 60.0 | $58.66 \%$ | $\mathbf{6 9 . 2 5 \%}$ | $40.00 \%$ |
| 70.0 | $57.45 \%$ | $\mathbf{7 0 . 1 9 \%}$ | $30.00 \%$ |
| 80.0 | $54.07 \%$ | $\mathbf{7 1 . 5 4 \%}$ | $20.00 \%$ |
| 85.0 | $52.43 \%$ | $\mathbf{7 5 . 1 4 \%}$ | $15.00 \%$ |
| 90.0 | $53.66 \%$ | $\mathbf{7 9 . 6 7 \%}$ | $10.00 \%$ |
| 95.0 | $54.84 \%$ | $\mathbf{9 1 . 9 4 \%}$ | $5.00 \%$ |
| 98.0 | $52.0 \%$ | $\mathbf{9 6 . 0 \%}$ | $2.00 \%$ |
| 99.0 | $38.46 \%$ | $\mathbf{1 0 0 . 0 \%}$ | $1.00 \%$ |
| 99.9 | $0.0 \%$ | $\mathbf{1 0 0 . 0 \%}$ | $0.15 \%$ |

probabilities (e.g., from softmax) cannot reliably be used to determine answer confidence. Moreover, the highest performance achieved using logit probability, $76.50 \%$ accuracy for $60 \%$ of questions in the extractive case and $61.87 \%$ accuracy for $60 \%$ of questions in the generative case, is significantly lower than those achieved using UQ metrics.

In the extractive case, MVE and Ensemble result in $100 \%$ accuracy when answering questions in the top confidence percentile. MVE, Ensemble, and MC obtain $+90 \%$ accuracy with coverage rates of $5 \%, 15 \%$, and $15 \%$, respectively, and $+80 \%$ accuracy with coverage rates of $50 \%, 65 \%$, and $75 \%$, respectively. Ensemble is able to reach $100 \%$ accuracy with the highest overall performance across all confidence levels (Fig. 2). We observe that all converted models are able to consistently, throughout the entire set of questions, identify predictions likely to be incorrect (see Appendix).

In the generative case, using baseline logit probability to measure confidence leads to a maximum increase in accuracy of only $4.7 \%$ for questions in the 4 th lowest percentile. In comparison, our uncertainty-aware models are able to attain accuracy rates of $100 \%,+90 \%,+80 \%,+70 \%$ when answering $1 \%, 8 \%, 9 \%$, and $35 \%$ of the questions, respectively, and result in higher accuracy across all confidence percentiles. We further observe that after generating 10 candidate answers for each question in the benchmark, answers with highest uncertainty were consistently incorrect. We also note that the converted model is able to output correct answers if it repeatedly generates predictions until one in the $99 \%$ confidence percentile is found.

Tab. 3 outlines results relating to computational performance. We find the converted models incur minimal overhead in inference time and a negligible increase in number of parameters. However, the Ensemble variant, results in significant computational cost, roughly five times that of the original model. Our automatic uncertainty-aware conversion procedure is completed in 0.0994 seconds for the $108 \mathrm{M}$ model and in 1.3856 seconds for our 7B model.

Automated composition for performance and efficiency in selective QA The performance of our Ensemble mod-
Table 3: Efficiency benchmark per uncertainty method.

|  | Models |  |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  | Baseline | MVE | Ensemble | MC | Composed |
| Parameters $\left(\times 10^{9}\right)$ | 109.484 | 109.487 | 547.419 | 109.484 | 109.487 |
| Inference Time | 1.00 | 1.016 | 4.997 | 1.1915 | 1.2075 |

els motivated us to leverage our automated conversion procedure to devise a compositional UQ method with strong accuracy that does not require the training of multiple independent models or incur significant computational costs. We hypothesized that considering both aleatoric and epistemic uncertainty would result in a more comprehensive UQ metric. We used $M C$ as the epistemic uncertainty metric given that our experiments showed it to be more computationally efficient than Ensemble. We combined the measurement with $M V E$, which estimates aleatoric uncertainty, and used our conversion procedure to create a model that calculates this composed metric with every output. As shown in Tab. 2a and Fig. 2, the composed approach attained significantly higher accuracy when compared to other independent UQ metrics. Importantly, the composed approach does not incur the significant computational overhead required by Ensembling-based approaches. Our results demonstrate that automated uncertainty-awareness enables the facile composition of different UQ methods to optimize for both performance and computational efficiency, and enables strong effectiveness on selective question answering tasks.

## Conclusion

We present an uncertainty-based framework for selective question answering (QA). Our method automatically converts existing LLMs into an uncertainty-aware variant, capable of estimating different forms of uncertainty. We demonstrate an increase in empirical coverage and accuracy on selective QA across models ranging in size from $100 \mathrm{M}$ to 7B parameters. By repeatedly generating answers until our method is highly confident, we produce an LLM that outputs $100 \%$ correct answers, eliminating hallucinations.

## References

Amini, A.; et al. 2023. Capsa Software Library.

Blundell, C.; Cornebise, J.; Kavukcuoglu, K.; and Wierstra, D. 2015. Weight uncertainty in neural network. In International conference on machine learning, 1613-1622. PMLR. Chen, J.; and Mueller, J. 2023. Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness. arXiv:2308.16175.

Chuang, Y.-S.; Xie, Y.; Luo, H.; Kim, Y.; Glass, J.; and He, P. 2023. DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. arXiv:2309.03883.

Collins, K. M.; Barker, M.; Zarlenga, M. E.; Raman, N.; Bhatt, U.; Jamnik, M.; Sucholutsky, I.; Weller, A.; and Dvijotham, K. 2023. Human Uncertainty in Concept-Based AI Systems. arXiv:2303.12872.

Damianou, A.; and Lawrence, N. D. 2013. Deep gaussian processes. In Artificial intelligence and statistics, 207-215. PMLR.

Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805. Dong, L.; Quirk, C.; and Lapata, M. 2018. Confidence modeling for neural semantic parsing. arXiv preprint arXiv:1805.04604.

El-Yaniv, R.; et al. 2010. On the Foundations of Noise-free Selective Classification. Journal of Machine Learning Research, 11(5).

Gal, Y.; and Ghahramani, Z. 2016. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, 10501059. PMLR.

Geifman, Y.; and El-Yaniv, R. 2017. Selective classification for deep neural networks. Advances in neural information processing systems, 30 .

Geiger, A.; Cases, I.; Karttunen, L.; and Potts, C. 2019. Posing fair generalization tasks for natural language inference. arXiv preprint arXiv:1911.00811.

Gondek, D. C.; Lally, A.; Kalyanpur, A.; Murdock, J. W.; Duboué, P. A.; Zhang, L.; Pan, Y.; Qiu, Z. M.; and Welty, C. 2012. A framework for merging and ranking of answers in DeepQA. IBM Journal of Research and Development, 56(3.4): 14-1.

Guo, C.; Pleiss, G.; Sun, Y.; and Weinberger, K. Q. 2017. On calibration of modern neural networks. In International conference on machine learning, 1321-1330. PMLR.

Guo, Z.; Schlichtkrull, M.; and Vlachos, A. 2022. A survey on automated fact-checking. Transactions of the Association for Computational Linguistics, 10: 178-206.

Jiang, Z.; Araki, J.; Ding, H.; and Neubig, G. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9: 962-977.

Kamath, A.; Jia, R.; and Liang, P. 2020. Selective question answering under domain shift. arXiv preprint arXiv:2006.09462.
Kuchaiev, O.; Li, J.; Nguyen, H.; Hrinchuk, O.; Leary, R.; Ginsburg, B.; Kriman, S.; Beliaev, S.; Lavrukhin, V.; Cook, J.; et al. 2019. Nemo: a toolkit for building ai applications using neural modules. arXiv preprint arXiv:1909.09577.

Lakshminarayanan, B.; Pritzel, A.; and Blundell, C. 2017. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30 .

Lemay, A.; Hoebel, K.; Bridge, C. P.; Befano, B.; De Sanjosé, S.; Egemen, D.; Rodriguez, A. C.; Schiffman, M.; Campbell, J. P.; and Kalpathy-Cramer, J. 2022. Improving the repeatability of deep learning models with Monte Carlo dropout. npj Digital Medicine, 5(1): 174.

Lin, S.; Hilton, J.; and Evans, O. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.

Lin, Z.; Trivedi, S.; and Sun, J. 2023. Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. arXiv:2305.19187.

Lolla, S.; Elistratov, I.; Perez, A.; Ahmadi, E.; Rus, D.; and Amini, A. 2022. Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks. In 5th Robot Learning Workshop: Trustworthy Robotics.

Lolla, S.; Elistratov, I.; Perez, A.; Ahmadi, E.; Rus, D.; and Amini, A. 2023. Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks. arXiv:2308.00231.

Manakul, P.; Liusie, A.; and Gales, M. J. F. 2023. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. arXiv:2303.08896.

Miao, N.; Teh, Y. W.; and Rainforth, T. 2023. SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. arXiv:2308.00436.

Mobiny, A.; Yuan, P.; Moulik, S. K.; Garg, N.; Wu, C. C.; and Van Nguyen, H. 2021. Dropconnect is effective in modeling uncertainty of bayesian deep networks. Scientific reports, 11(1): 5458 .

Nix, D. A.; and Weigend, A. S. 1994. Estimating the mean and variance of the target probability distribution. In Proceedings of 1994 ieee international conference on neural networks (ICNN'94), volume 1, 55-60. IEEE.

Peñas, A.; Forner, P.; Sutcliffe, R.; Rodrigo, Á.; Forăscu, C.; Alegria, I.; Giampiccolo, D.; Moreau, N.; and Osenova, P. 2010. Overview of ResPubliQA 2009: Question answering evaluation over European legislation. In Multilingual Information Access Evaluation I. Text Retrieval Experiments: 10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009, Corfu, Greece, September 30-October 2, 2009, Revised Selected Papers 10, 174-196. Springer.

Quach, V.; Fisch, A.; Schuster, T.; Yala, A.; Sohn, J. H.; Jaakkola, T. S.; and Barzilay, R. 2023. Conformal Language Modeling. arXiv:2306.10193.

Rajpurkar, P.; Jia, R.; and Liang, P. 2018. Know What You Don't Know: Unanswerable Questions for SQuAD. arXiv:1806.03822.

Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv:1606.05250.

Rodriguez, P.; Feng, S.; Iyyer, M.; He, H.; and Boyd-Graber, J. 2021. Quizbowl: The Case for Incremental Question Answering. arXiv:1904.04792.

Sellam, T.; Das, D.; and Parikh, A. P. 2020. BLEURT: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696.

Shen, M.; Bu, Y.; Sattigeri, P.; Ghosh, S.; Das, S.; and Wornell, G. 2022. Post-hoc Uncertainty Learning using a Dirichlet Meta-Model. arXiv:2212.07359.

Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1): 1929-1958.

Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Wang, M.; et al. 2006. A survey of answer extraction techniques in factoid question answering. Computational Linguistics, 1(1): 1-14.

Weng, Y.; Zhu, M.; He, S.; Liu, K.; and Zhao, J. 2022. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561.

Wu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.; Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.; Klingner, J.; Shah, A.; Johnson, M.; Liu, X.; Łukasz Kaiser; Gouws, S.; Kato, Y.; Kudo, T.; Kazawa, H.; Stevens, K.; Kurian, G.; Patil, N.; Wang, W.; Young, C.; Smith, J.; Riesa, J.; Rudnick, A.; Vinyals, O.; Corrado, G.; Hughes, M.; and Dean, J. 2016. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv:1609.08144.

Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; and Fidler, S. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, 19-27.
