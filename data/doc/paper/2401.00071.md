# Shifted Composition II: Shift Harnack Inequalities and Curvature Upper Bounds 

Jason M. Altschuler<br>UPenn<br>alts@upenn.edu

Sinho Chewi<br>IAS<br>schewi@ias.edu


#### Abstract

We apply the shifted composition rule - an information-theoretic principle introduced in our earlier work [AC23]-to establish shift Harnack inequalities for the Langevin diffusion. We obtain sharp constants for these inequalities for the first time, allowing us to investigate their relationship with other properties of the diffusion. Namely, we show that they are equivalent to a sharp "local gradient-entropy" bound, and that they imply curvature upper bounds in a compelling reflection of the Bakry-Émery theory of curvature lower bounds. Finally, we show that the local gradient-entropy inequality implies optimal concentration of the score, a.k.a. the logarithmic gradient of the density.


## Contents

1 Introduction ..... 2
2 Preliminaries ..... 5
2.1 Information theory ..... 5
2.2 Shifted composition rule ..... 6
2.3 Convexity principle ..... 7
3 Discrete-time arguments ..... 8
3.1 General principle ..... 8
3.2 Forward regularity of Itô diffusions ..... 9
3.3 Dual shifted divergences ..... 11
4 Continuous-time arguments ..... 12
4.1 Proof via Girsanov's theorem ..... 13
4.2 Proof via divergence differentiation ..... 14
5 Shift Harnack inequalities and curvature upper bounds ..... 15
5.1 Background on shift Harnack inequalities ..... 15
5.2 Duality between shift Harnack inequalities and reverse transport inequalities ..... 16
5.3 Relationship with local gradient-entropy bounds and curvature upper bounds ..... 17
5.4 Implications for the stationary distribution ..... 20
A Deferred details ..... 23
A. 1 Tightness ..... 23
A. 2 Generalized convolution lemma ..... 24
References ..... 24

## 1 Introduction

This paper is a sequel to our earlier work [AC23], in which we introduced an information-theoretic principle called the shifted composition rule. Briefly, this principle allows for bounding informationtheoretic divergences, such as the Kullback-Leibler (KL) or Rényi divergence, between the marginal laws of two stochastic processes through the introduction of a third, auxiliary process. In that paper, we applied the shifted composition rule to provide, among other results, information-theoretic proofs of F.-Y. Wang's celebrated dimension-free Harnack inequalities for diffusions [Wan97] via their dual formulation as a certain family of reverse transport inequalities.

The Harnack inequalities therein encode regularity for Kolmogorov's backward equation. The aim of the present paper is to apply our information-theoretic framework to the dual problem of regularity for Kolmogorov's forward equation, i.e., the Fokker-Planck equation. To describe the problem setting and our results, we introduce some basic concepts.

Harnack and shift Harnack inequalities. For concreteness, let $V: \mathbb{R}^{d} \rightarrow \mathbb{R}$ be a smooth potential and consider the Langevin diffusion

$$
\begin{equation*}
\mathrm{d} X_{t}=-\nabla V\left(X_{t}\right) \mathrm{d} t+\sqrt{2} \mathrm{~d} B_{t} \tag{1.1}
\end{equation*}
$$

where $\left(B_{t}\right)_{t \geqslant 0}$ is a standard Brownian motion on $\mathbb{R}^{d}$. Following the storied tradition of functional analysis, we study the regularity of this process through its Markov semigroup $\left(P_{t}\right)_{t \geqslant 0}$, which acts on any (reasonable) function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ via $P_{t} f(x):=\mathbb{E}\left[f\left(X_{t}\right) \mid X_{0}=x\right]$. Classically, under a curvature lower bound of the form $\nabla^{2} V \geq \alpha I, \alpha \in \mathbb{R}$ (also called the curvature-dimension condition, or the Bakry-Émery criterion), one obtains Harnack inequalities of the form

$$
\begin{equation*}
P_{t} f(x)^{p} \leqslant C(p, t, x, y) P_{t}\left(f^{p}\right)(y), \quad \text { for all } x, y \in \mathbb{R}^{d}, t>0, \text { and } f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{\geqslant 0} . \tag{https://cdn.mathpix.com/cropped/2024_05_26_18ab116b7d6c100cd301g-02.jpg?height=52&width=87&top_left_y=1404&top_left_x=1797}
\end{equation*}
$$

Here, $p>1$ and $C(p, t, x, y)>0$ is an appropriate constant. Such inequalities witness the regularizing effect of the semigroup: they imply that for every $t>0$, the semigroup $P_{t}$ maps bounded functions into differentiable ones. If the semigroup is described by transition densities $(t, x, y) \mapsto p_{t}(x, y)$, then this amounts to regularity properties with respect to the backward variable $x$, i.e., with respect to perturbations to the initial condition of the diffusion. Moreover, the sharp Harnack inequalities imply back the curvature lower bound $\nabla^{2} V \geq \alpha I$; see [Wan04; Wan10; BGL14; Wan14a] or [AC23, §6] for further discussion.

To address the regularity of Kolmogorov's forward equation, the seminal work of F.-Y. Wang in [Wan14b] introduced the family of shift Harnack inequalities, written

$$
\begin{equation*}
P_{t}(f(\cdot+v))^{p} \leqslant C(p, t, v) P_{t}\left(f^{p}\right), \quad \text { for all } v \in \mathbb{R}^{d}, t>0, \text { and } f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{\geqslant 0} \tag{1.3}
\end{equation*}
$$

Shift Harnack inequalities imply, for example, the existence of the transition densities $\left(p_{t}\right)_{t \geqslant 0}$ with respect to the Lebesgue measure, and they also entail gradient bounds for the Lebesgue density $p_{t}(x, y)$ with respect to the forward variable $y$ (a.k.a. the terminal condition for the diffusion). See §5.1 for further background and discussion. Note that the Harnack (1.2) and shift Harnack (1.3) inequalities differ because the Langevin semigroup does not commute with convolutions.

In his original paper [Wan14b], F.-Y. Wang established a number of applications and equivalences for shift Harnack inequalities. (See also his monographs [Wan13; Wan14a] for comprehensive expositions and further applications.) Through the use of coupling arguments, he then established integration by parts formulas and shift Harnack inequalities for (degenerate) stochastic Hamiltonian systems and some SPDEs. Subsequent work extended his techniques to an abundance of
settings, including SDEs driven by fractional Brownian motion [Fan15], SDEs driven by jump processes [Wan16], other examples of SPDEs [Zha16; LH21], McKean-Vlasov equations [Wan18; HW19], and SDEs with irregular coefficients [Hua19; HW19; LH21].

Our starting point is the equivalent reformulation of the shift Harnack inequality (1.3), via Hölder duality, in the form of a reverse transport inequality:

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P_{t} * \delta_{v} \| \delta_{x} P_{t}\right) \leqslant C^{\prime}(p, t, v), \quad \text { for all } x \in \mathbb{R}^{d}, t>0 \tag{1.4}
\end{equation*}
$$

see §5.2. Here, $\mathrm{R}_{q}$ is the Rényi divergence of order $q:=\frac{p}{p-1}$ (see $\S 2.1$ for information-theoretic preliminaries). We apply the shifted composition rule to provide information-theoretic arguments, in discrete time, to establish (1.4) and hence (1.3). In this respect, our development parallels our earlier work [AC23] at least at a syntactic level, and we organize our paper accordingly to emphasize these similarities. However, despite the syntactic similarity, the problem of forward regularity is substantially different from that of backward regularity, and the latter is far less well-understood.

Relationship with curvature upper bounds. Indeed, whereas F.-Y. Wang's original Harnack inequalities and the celebrated web of equivalences around the curvature-dimension condition have been well-understood for at least two decades, the understanding of shift Harnack inequalities is relatively nascent. ${ }^{1}$ A significant point of departure is that the optimal constants $C(p, t, v)$ in (1.3) are not sharply characterized. In fact, the bounds established in the literature involve constants $C(p, t, v)$ which diverge to infinity as $t \rightarrow \infty$. Consequently, such bounds do not yield meaningful information about the regularity of the stationary distribution.

One of the primary contributions of this paper is to prove shift Harnack inequalities with optimal constants. With these sharp inequalities in hand, we are then in a position to investigate the possibility of developing equivalences for the shift Harnack inequalities. Towards this end, we prove the following chain of implications.

Theorem 1.1. Let $\left(P_{t}\right)_{t \geqslant 0}$ denote the Markov semigroup corresponding to the Langevin diffusion with potential $V$. Let $\beta>0$ and $p, q>1$. Consider the following properties.

(CurvBdd) The two-sided curvature bound $-\beta I \leq \nabla^{2} V \leq \beta I$ holds.

(LGE) The local gradient-entropy bound

$$
\frac{\left\|P_{t} \nabla f\right\|^{2}}{P_{t} f} \leqslant \frac{2 \beta}{1-\exp (-2 \beta t)}\left\{P_{t}(f \log f)-P_{t} f \log P_{t} f\right\}
$$

holds for all $t>0$ and all smooth $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}$.

$\left(\mathrm{SH}_{p}\right)$ The shift Harnack inequality

$$
P_{t}(f(\cdot+v))^{p} \leqslant \exp \left(\frac{\beta p\|v\|^{2}}{2(p-1)(1-\exp (-2 \beta t))}\right) P_{t}\left(f^{p}\right)
$$

holds for all $v \in \mathbb{R}^{d}$, all $t>0$, and all $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}$.

$\left(\mathrm{SRT}_{q}\right)$ The shift reverse transport inequality

$$
\mathrm{R}_{q}\left(\delta_{x} P_{t} * \delta_{v} \| \delta_{x} P_{t}\right) \leqslant \frac{\beta q\|v\|^{2}}{2(1-\exp (-2 \beta t))}
$$

holds for all $x, v \in \mathbb{R}^{d}$ and all $t>0$.[^0]$\left(\mathrm{SH}_{\log }\right)$ The shift log-Harnack inequality

$$
P_{t}(f(\cdot+v)) \leqslant \log P_{t}(\exp f)+\frac{\beta\|v\|^{2}}{2(1-\exp (-2 \beta t))}
$$

holds for all $v \in \mathbb{R}^{d}$, all $t>0$, and all $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}$.

$\left(\mathrm{SRT}_{1}\right)$ The shift reverse transport inequality

$$
\mathrm{KL}\left(\delta_{x} P_{t} * \delta_{v} \| \delta_{x} P_{t}\right) \leqslant \frac{\beta\|v\|^{2}}{2(1-\exp (-2 \beta t))}
$$

holds for all $x, v \in \mathbb{R}^{d}$ and all $t>0$.

(CurvUB) The curvature upper bound $\nabla^{2} V \leq \beta I$ holds.

Then, the following implications hold.

$$
\begin{aligned}
& (\text { CurvBdd }) \Rightarrow(\mathrm{LGE}) \Leftrightarrow\left(\forall p>1\left(\mathrm{SH}_{p}\right)\right) \Longrightarrow\left(\exists p>1\left(\mathrm{SH}_{p}\right)\right) \Rightarrow\left(\mathrm{SH}_{\log }\right) \Rightarrow(\text { CurvUB })
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_05_26_18ab116b7d6c100cd301g-04.jpg?height=112&width=811&top_left_y=1077&top_left_x=711)

The implications $(\mathrm{LGE}) \Leftrightarrow\left(\mathrm{SH}_{p}\right)$ and $\left(\mathrm{SH}_{p}\right) \Rightarrow\left(\mathrm{SH}_{\log }\right)$ are known from [Wan14b] and [Wan13, Theorem 1.3.5] respectively, and the equivalences $\left(\mathrm{SH}_{p}\right) \Leftrightarrow\left(\mathrm{SRT}_{q}\right)$ (for the dual exponent $\left.q=\frac{p}{p-1}\right)$ and $\left(\mathrm{SH}_{\log }\right) \Leftrightarrow\left(\mathrm{SRT}_{1}\right)$ follow from duality (see $\left.\S 5.2\right)$. However, to our knowledge, the sharp forms of all the functional inequalities $(\mathrm{LGE}),\left(\mathrm{SH}_{p}\right),\left(\mathrm{SH}_{\mathrm{log}}\right),\left(\mathrm{SRT}_{q}\right)$, and $\left(\mathrm{SRT}_{1}\right)$ appear here for the first time. Among these implications, we found it surprising that the shift Harnack inequalities imply back the curvature upper bound, in a remarkable parallel to the equivalences with the curvaturedimension condition which captures curvature lower bounds. It is a tantalizing question to close this chain of implications, e.g., by showing that (CurvUB) is sufficient to imply (LGE) or $\left(\mathrm{SH}_{p}\right)$. A related question is to characterize (CurvUB) via a gradient commutation inequality. We discuss these directions further in $\S 5.3$.

The motivation for these questions stems from the corresponding synthetic characterizations of Ricci curvature lower bounds, which extended the reach of tools from geometric analysis to more general metric measure spaces [Stu06a; Stu06b; LV09; AGS14; AGS15; EKS15]. In contrast, we are only aware of the works [Wu20; Stu21] on characterizations of Ricci curvature upper bounds.

Implications for the stationary distribution. Suppose now that the Langevin diffusion (1.1) admits a unique stationary distribution $\pi$, so that necessarily $\pi \propto \exp (-V)$. This holds true as soon as $\int \exp (-V)<\infty$.

Since the sharp constants in Theorem 1.1 do not diverge as $t \rightarrow \infty$, our results immediately imply functional inequalities for the stationary distribution $\pi$ which may be of independent interest (see Corollary 5.3). As one example, (LGE) reads

$$
\frac{\left\|\mathbb{E}_{\pi} \nabla f\right\|^{2}}{\mathbb{E}_{\pi} f} \leqslant 2 \beta \operatorname{ent}_{\pi}(f) \quad \text { for all } f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}
$$

We show that this inequality implies a sharp concentration bound for $\nabla V$-often called the score function in the statistical literature - under $\pi$. To illustrate this result, suppose that $\pi=\mathcal{N}\left(0, \beta^{-1} I\right)$
with $\beta>0$, so that $\nabla V(x)=\beta x$. Then, it is easy to see that under $\pi, \nabla V$ has law $\mathcal{N}(0, \beta I)$ and, in particular, is a $\sqrt{\beta}$-sub-Gaussian random vector. (We recall that a random vector $X$ is called $\sigma$-sub-Gaussian if, for every unit vector $e \in \mathbb{R}^{d},\langle e, X\rangle$ is $\sigma^{2}$-sub-Gaussian, i.e., $\mathbb{E} \exp (\lambda\langle e, X\rangle) \leqslant$ $\exp \left(\lambda^{2} \sigma^{2} / 2\right)$ for all $\lambda \in \mathbb{R}$.) Our concentration inequality shows that this observation extends far beyond the Gaussian case.

Theorem 1.2. Let $\pi \propto \exp (-V)$ be a probability measure on $\mathbb{R}^{d}$ such that $\left(\mathrm{LGE}_{\pi}\right)$ holds. Then, under $\pi$, the score $\nabla V$ is $\sqrt{\beta}$-sub-Gaussian.

Note that a classical integration-by-parts identity shows that the condition $\nabla^{2} V \leq \beta I$ implies $\mathbb{E}_{\pi}\left[\|\nabla V\|^{2}\right]=\mathbb{E}_{\pi}[\Delta V] \leqslant \beta d$. A concentration bound for $\|\nabla V\|$ was previously derived in [LST20] in the context of log-concave sampling, under the additional structural assumption of convexity of $V$, and with subexponential tails rather than sub-Gaussian. In contrast, our argument requires only the smoothness of $V$ (more precisely, through $\left.\left(\mathrm{LGE}_{\pi}\right)\right)$ and recovers the correct behavior for the Gaussian. See $\$ 5.4$ for further discussion.

Organization. We begin in $\S 2$ by collecting relevant preliminary material on information theory and in particular on the shifted composition rule (Theorem 2.3). In $\S 3$, we apply the shifted composition rule to establish sharp shift Harnack inequalities for discretized diffusions, from which the corresponding results for the continuous-time diffusion follow immediately as a limiting case. In $\S 4$, we provide a direct continuous-time perspective that yields two alternative derivations of these inequalities, one via stochastic calculus and the other via direct differentiation of the Rényi divergence. The former is a refinement of the original argument of [Wan14b] that achieves optimal constants; the latter is new. Finally, in $\S 5$, we provide background on shift Harnack inequalities and duality, and we prove Theorems 1.1 and 1.2 .

Setting. For the reverse transport inequalities in $\S 3$ and $\S 4$, we work with Itô diffusions $\left\{X_{t}\right\}_{t \geqslant 0}$ of the form

$$
\mathrm{d} X_{t}=b\left(X_{t}\right) \mathrm{d} t+\sigma \mathrm{d} B_{t}, \quad X_{0}=x
$$

in the uniformly elliptic setting that $\sigma \sigma^{\top} \geq \lambda I$ for some $\lambda>0$. We assume that $b$ is $L$-Lipschitz; this implies in particular that there is a unique strong solution to the SDE. This level of generality comes at no cost because the arguments are not made any more difficult. The results for the Langevin diffusion follow immediately as a special case.

## 2 Preliminaries

Here we collect background material on information theory ( $\$ 2.1$ ), the shifted composition rule ( $\S 2.2$ ), and the convexity principle ( $\S 2.3$ ). The latter two concepts were developed in [AC23].

### 2.1 Information theory

As discussed in $\S 1$, shift Harnack inequalities are equivalent to reverse transport inequalities involving Rényi divergences. These divergences are defined as follows.

Definition 2.1 (Rényi divergence). Let $q \geqslant 1$. The Rényi divergence of order $q$ between probability measures $\mu, \nu$ is defined to be

$$
\begin{equation*}
\mathrm{R}_{q}(\mu \| \nu):=\frac{1}{q-1} \log \int\left(\frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right)^{q} \mathrm{~d} \nu \tag{2.1}
\end{equation*}
$$

if $\mu \ll \nu$, and is defined to be $\infty$ otherwise.

We make use of the following relations between Rényi divergences and other information divergences:

- KL divergence. For $q=1$, the definition (2.1) is interpreted in the limiting sense and equals the Kullback-Leibler (KL) divergence

$$
\mathrm{KL}(\mu \| \nu):=\mathrm{R}_{1}(\mu \| \nu):=\int\left(\frac{\mathrm{d} \mu}{\mathrm{d} \nu} \log \frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right) \mathrm{d} \nu
$$

- Chi-squared divergence. For $q=2$, the Rényi divergence is related to the chi-squared divergence

$$
\chi^{2}(\mu \| \nu):=\operatorname{var}_{\nu} \frac{\mathrm{d} \mu}{\mathrm{d} \nu}=\int\left(\frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right)^{2} \mathrm{~d} \nu-1
$$

via the expression $\mathrm{R}_{2}(\mu \| \nu)=\exp \left(1+\chi^{2}(\mu \| \nu)\right)$.

- $f$-divergences. For $q>1$, the Rényi divergence is related to the $f$-divergence

$$
\begin{equation*}
\mathrm{D}_{q}(\mu \| \nu):=\int\left(\frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right)^{q} \mathrm{~d} \nu-1 \tag{2.2}
\end{equation*}
$$

via the expression $\mathrm{R}_{q}(\mu \| \nu)=\frac{1}{q-1} \log \left(1+\mathrm{D}_{q}(\mu \| \nu)\right)$.

We refer the reader to the surveys [VH14; Mir17] for further discussion of Rényi divergences and their properties. For the convenience of the reader, the following theorem collects the basic properties of Rényi divergences that we employ in this paper.

Theorem 2.2. Let $q \geqslant 1$ and let $\mu, \nu$ be probability measures.

1. (Positivity) $\mathrm{R}_{q}(\mu \| \nu) \geqslant 0$, with equality if and only if $\mu=\nu$.
2. (Monotonicity) Rényi divergences are increasing in the order, i.e., $q \mapsto \mathrm{R}_{q}(\mu \| \nu)$ is increasing.
3. (Data processing inequality) For any Markov kernel $P$, it holds that $\mathrm{R}_{q}(\mu P \| \nu P) \leqslant \mathrm{R}_{q}(\mu \| \nu)$.
4. (Convexity) The divergences $\mathrm{KL}$ and $\mathrm{D}_{q}$ (for $q>1$ ) are jointly convex.
5. (Gaussian identity) $\mathrm{R}_{q}(\mathcal{N}(x, \Sigma) \| \mathcal{N}(y, \Sigma))=\frac{q}{2}\left\langle x-y, \Sigma^{-1}(x-y)\right\rangle$.

### 2.2 Shifted composition rule

Next, we recall the shifted composition rule from [AC23], which will form the basis of our arguments in $\S 3$. Write $\mathscr{C}(\mu, \nu)$ for the set of couplings of two probability measures $\mu \in \mathcal{P}\left(\Omega_{1}\right), \nu \in \mathcal{P}\left(\Omega_{2}\right)$, i.e., the set of probability measures $\gamma \in \mathcal{P}\left(\Omega_{1} \times \Omega_{2}\right)$ whose marginals are $\mu$ and $\nu$ respectively.

Theorem 2.3 (Shifted composition rule). Let $X, X^{\prime}, Y$ be three jointly defined random variables on a standard probability space $\Omega$. Let $\boldsymbol{\mu}, \boldsymbol{\nu}$ be two probability measures over $\Omega$, with superscripts denoting the laws of random variables under these measures.

1. (Shifted chain rule) It holds that

$$
\mathrm{KL}\left(\boldsymbol{\mu}^{Y} \| \boldsymbol{\nu}^{Y}\right) \leqslant \mathrm{KL}\left(\boldsymbol{\mu}^{X^{\prime}} \| \boldsymbol{\nu}^{X}\right)+\inf _{\gamma \in \mathscr{E}\left(\boldsymbol{\mu}^{X}, \boldsymbol{\mu}^{X^{\prime}}\right)} \int \mathrm{KL}\left(\boldsymbol{\mu}^{Y \mid X=x} \| \boldsymbol{\nu}^{Y \mid X=x^{\prime}}\right) \gamma\left(\mathrm{d} x, \mathrm{~d} x^{\prime}\right) .
$$

2. Let $q \geqslant 1$. Then, it holds that

$$
\mathrm{R}_{q}\left(\boldsymbol{\mu}^{Y} \| \boldsymbol{\nu}^{Y}\right) \leqslant \mathrm{R}_{q}\left(\boldsymbol{\mu}^{X^{\prime}} \| \boldsymbol{\nu}^{X}\right)+\inf _{\gamma \in \mathscr{C}\left(\boldsymbol{\mu}^{X}, \boldsymbol{\mu}^{X^{\prime}}\right)}{ }_{\left(x, x^{\prime}\right) \in \Omega \times \Omega}^{\gamma-\operatorname{ess} \sup } \mathrm{R}_{q}\left(\boldsymbol{\mu}^{Y \mid X=x} \| \boldsymbol{\nu}^{Y \mid X=x^{\prime}}\right)
$$

We briefly describe the intuition behind this principle. We think of $\boldsymbol{\mu}$ and $\boldsymbol{\nu}$ as describing two possible laws of the stochastic process $X \rightarrow Y$, and we wish to bound the divergence between the laws of $Y$ under $\boldsymbol{\mu}$ and $\boldsymbol{\nu}$ respectively. One natural way to do so is to apply the chain rule (for $\mathrm{KL}$ ) or the composition rule (for $\mathrm{R}_{q}, q>1$ ), but this does not always lead to a bound that is tight or even non-vacuous. Instead, the shifted composition rule allows for the introduction of an auxiliary "shift", which replaces $X$ with $X^{\prime}$ under $\boldsymbol{\mu}$ (c.f. the first term in the above inequalities), at a price encapsulated by how distinguishable $X$ is from $X^{\prime}$ (c.f. the second term). As we demonstrate in $\S 3$, the freedom afforded by this auxiliary shift allows for capturing coupling arguments with information-theoretic tools.

### 2.3 Convexity principle

In this paper, we establish inequalities of the form

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P * \delta_{v} \| \delta_{x} P\right) \leqslant \rho(v), \quad \text { for all } v \in \mathbb{R}^{d} \tag{2.3}
\end{equation*}
$$

where $P$ is a Markov kernel on $\mathbb{R}^{d}$. In this section, we demonstrate that via the joint convexity of $f$-divergences, such inequalities immediately imply more general ones where the Dirac measures are replaced by arbitrary measures.

Lemma 2.4 (Convexity principle). Suppose that an inequality of the form (2.3) holds. Then, for any probability measures $\mu, \nu, \nu^{\prime}$ on $\mathbb{R}^{d}$, the following hold.

1. If $q=1$ (i.e., $\mathrm{R}_{q}=\mathrm{KL}$ ), then

$$
\mathrm{KL}\left(\mu P * \nu \| \mu P * \nu^{\prime}\right) \leqslant \inf _{\gamma \in \mathscr{C}\left(\nu, \nu^{\prime}\right)} \int \rho\left(v-v^{\prime}\right) \gamma\left(\mathrm{d} v, \mathrm{~d} v^{\prime}\right)
$$

2. If $q>1$, then

$$
\mathrm{R}_{q}\left(\mu P * \nu \| \mu P * \nu^{\prime}\right) \leqslant \inf _{\gamma \in \mathscr{C}\left(\nu, \nu^{\prime}\right)} \frac{1}{q-1} \log \int \exp \left((q-1) \rho\left(v-v^{\prime}\right)\right) \gamma\left(\mathrm{d} v, \mathrm{~d} v^{\prime}\right)
$$

Proof. First, note that since $x \mapsto x-v^{\prime}$ is a diffeomorphism, the data-processing inequality (see Theorem 2.2$)$ and (2.3) imply that

$$
\mathrm{KL}\left(\delta_{x} P * \delta_{v} \| \delta_{x} P * \delta_{v^{\prime}}\right)=\mathrm{KL}\left(\delta_{x} P * \delta_{v-v^{\prime}} \| \delta_{x} P\right) \leqslant \rho\left(v-v^{\prime}\right), \quad \text { for all } v, v^{\prime} \in \mathbb{R}^{d}
$$

Let $\gamma$ be a coupling of $\nu$ and $\nu^{\prime}$. By the joint convexity of the KL divergence (Theorem 2.2),

$$
\begin{aligned}
\mathrm{KL}\left(\mu P * \nu \| \mu P * \nu^{\prime}\right) & =\operatorname{KL}\left(\int \delta_{x} P * \delta_{v} \mu(\mathrm{d} x) \gamma\left(\mathrm{d} v, \mathrm{~d} v^{\prime}\right) \| \int \delta_{x} P * \delta_{v^{\prime}} \mu(\mathrm{d} x) \gamma\left(\mathrm{d} v, \mathrm{~d} v^{\prime}\right)\right) \\
& \leqslant \int \mathrm{KL}\left(\delta_{x} P * \delta_{v} \| \delta_{x} P * \delta_{v^{\prime}}\right) \mu(\mathrm{d} x) \gamma\left(\mathrm{d} v, \mathrm{~d} v^{\prime}\right) \leqslant \int \rho\left(v-v^{\prime}\right) \gamma\left(\mathrm{d} v, \mathrm{~d} v^{\prime}\right)
\end{aligned}
$$

The proof for $\mathrm{R}_{q}, q>1$ is similar, except that we use the joint convexity of $\mathrm{D}_{q}$ defined in (2.2) (note that $\mathrm{R}_{q}$ itself is not jointly convex).

Remark 2.5 (Refinements). Although we do not dwell on this point here, we briefly remark that several refinements are possible. In our prior work [AC23], we showed that by taking advantage of the convexity of $\left(\mathrm{D}_{q}+1\right)^{1 / q}$ w.r.t. its first argument or the convexity of $\mathrm{R}_{q}$ w.r.t. its second argument, one can refine the convexity principle for the case $q>1$, see $\$ 3.3$. The same can be applied here.

In a complementary direction, one can incorporate dependence between the initial condition $x$ and the terminal shift $v$, by generalizing from measures of the form $\mu P * \nu$ to measures of the form $\int P(x, \cdot) * Q(x, \cdot) \mu(\mathrm{d} x)=\int \delta_{x} P * \delta_{v} \mu(\mathrm{d} x) Q(x, \mathrm{~d} v)$ for some Markov kernel $Q$ on $\mathbb{R}^{d}$. Indeed, let $\gamma_{x}$ be a coupling of two kernels $Q(x, \cdot)$ and $Q^{\prime}(x, \cdot)$. Then, by the joint convexity of the $K L$ divergence,

$$
\begin{aligned}
& \mathrm{KL}\left(\int \delta_{x} P * \delta_{v} \mu(\mathrm{d} x) Q(x, \mathrm{~d} v) \| \int \delta_{x} P * \delta_{v^{\prime}} \mu(\mathrm{d} x) Q^{\prime}\left(x, \mathrm{~d} v^{\prime}\right)\right) \\
& \quad=\operatorname{KL}\left(\int \delta_{x} P * \delta_{v} \mu(\mathrm{d} x) \gamma_{x}\left(\mathrm{~d} v, \mathrm{~d} v^{\prime}\right) \| \int \delta_{x} P * \delta_{v^{\prime}} \mu(\mathrm{d} x) \gamma_{x}\left(\mathrm{~d} v, \mathrm{~d} v^{\prime}\right)\right) \\
& \quad \leqslant \int \mathrm{KL}\left(\delta_{x} P * \delta_{v} \| \delta_{x} P * \delta_{v^{\prime}}\right) \mu(\mathrm{d} x) \gamma_{x}\left(\mathrm{~d} v, \mathrm{~d} v^{\prime}\right) \leqslant \int \rho\left(v-v^{\prime}\right) \mu(\mathrm{d} x) \gamma_{x}\left(\mathrm{~d} v, \mathrm{~d} v^{\prime}\right)
\end{aligned}
$$

This observation can also be combined with the refinements mentioned in the preceding paragraph. However, for clarity of exposition, we focus on the simple version of the convexity principle as stated in Lemma 2.4.

## 3 Discrete-time arguments

### 3.1 General principle

In analog to the one-step-to-multi-step bound for backward regularity in $[\mathrm{AC} 23, \S 3.2]$, we now state a one-step-to-multi-step reduction for forward regularity. We remark that for forward regularity, the Markov kernel $P$ need only satisfy a 1-step regularity bound-whereas for the backward regularity studied in [AC23], it is necessary to additionally assume that $P$ is Wasserstein-Lipschitz. For simplicity, we consider here Dirac initializations and Dirac convolutions; this extends to general measures via the convexity principle in $\S 2.3$.

Theorem 3.1 (One-step-to-multi-step for forward regularity). Let $q \geqslant 1$. Suppose that $P$ is a Markov kernel on $\mathbb{R}^{d}$ satisfying the following 1-step regularity bound for Dirac initializations:

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P * \delta_{v} \| \delta_{x+w} P\right) \leqslant\left(c_{1}\|w\|+c_{2}\|v-w\|\right)^{2}, \quad \forall x, v, w \in \mathbb{R}^{d} \tag{3.1}
\end{equation*}
$$

Then for all $x, v \in \mathbb{R}^{d}$,

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P^{N} * \delta_{v} \| \delta_{x} P^{N}\right) \leqslant \frac{1-\left(1-c_{1} / c_{2}\right)^{2}}{1-\left(1-c_{1} / c_{2}\right)^{2 N}} c_{2}^{2}\|v\|^{2} \tag{https://cdn.mathpix.com/cropped/2024_05_26_18ab116b7d6c100cd301g-08.jpg?height=57&width=87&top_left_y=1954&top_left_x=1797}
\end{equation*}
$$

Although the condition (3.1) may seem mysterious at first sight, it is motivated by the onestep regularity bound which is satisfied by discretized Itô diffusions - as it is trivial to verify in that setting, since it reduces to a simple divergence calculation between Gaussians. Theorem 3.1 then immediately implies tight forward regularity bounds for Itô diffusions (in both discrete and continuous settings). See $\S 3.2$ for details.

We also remark that the condition (3.1) admits an interesting interpretation when $v=w$ : it measures the extent to which convolution with $\delta_{w}$ commutes with the Markov kernel $P$.

Proof of Theorem 3.1. Consider any sequence of shifts $v_{0}, \ldots, v_{N}$ satisfying $v_{0}=0$ and $v_{N}=v$. For all $n \in\{0, \ldots, N-1\}$

$$
\begin{align*}
\mathrm{R}_{q}\left(\delta_{x} P^{n+1} * \delta_{v_{n+1}} \| \delta_{x} P^{n+1}\right) & \leqslant \mathrm{R}_{q}\left(\delta_{x} P^{n} * \delta_{v_{n}} \| \delta_{x} P^{n}\right)+\sup _{z \in \mathbb{R}^{d}} \mathrm{R}_{q}\left(\delta_{z} P * \delta_{v_{n+1}} \| \delta_{z+v_{n}} P\right) \\
& \leqslant \mathrm{R}_{q}\left(\delta_{x} P^{n} * \delta_{v_{n}} \| \delta_{x} P^{n}\right)+\left(c_{1}\left\|v_{n}\right\|+c_{2}\left\|v_{n+1}-v_{n}\right\|\right)^{2} \tag{3.3}
\end{align*}
$$

Above, the second inequality uses the one-step regularity (3.1). The first inequality is by an application of the shifted composition rule (Theorem 2.3) where $\boldsymbol{\mu}$ is the joint distribution under which $X \sim \delta_{x} P^{n} * \delta_{v_{n+1}}, X^{\prime} \sim \delta_{x} P^{n} * \delta_{v_{n}}$, and $Y \sim \delta_{x} P^{n+1} * \delta_{v_{n+1}}$; and $\nu$ is the joint distribution under which $X \sim \delta_{x} P^{n}$ and $Y \sim \delta_{x} P^{n+1}$. In this application of the shifted composition rule, we simplify the infimum over couplings $\gamma \in \mathscr{C}\left(\boldsymbol{\mu}^{X}, \boldsymbol{\mu}^{X^{\prime}}\right)=\mathscr{C}\left(\delta_{x} P^{n} * \delta_{v_{n+1}}, \delta_{x} P^{n} * \delta_{v_{n}}\right)$ by taking the deterministic coupling $\gamma$ which pairs $\left(z+v_{n+1}, z+v_{n}\right)$.

Iterating the inequality (3.3) yields

$$
\mathrm{R}_{q}\left(\delta_{x} P^{N} * \delta_{v_{N}} \| \delta_{x} P^{N}\right) \leqslant \mathrm{R}_{q}\left(\delta_{x} * \delta_{v_{0}} \| \delta_{x}\right)+\sum_{n=0}^{N-1}\left(c_{1}\left\|v_{n}\right\|+c_{2}\left\|v_{n+1}-v_{n}\right\|\right)^{2}
$$

By simplifying the Rényi terms using the boundary conditions $v_{0}=0$ and $v_{N}=v$, and optimizing over the intermediate shifts $v_{1}, \ldots, v_{N-1}$, we conclude

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P^{N} * \delta_{v} \| \delta_{x} P^{N}\right) \leqslant \inf _{\substack{v_{0}, \ldots, v_{N} \\ \text { s.t. } v_{0}=0, v_{N}=v}} \sum_{n=0}^{N-1}\left(c_{1}\left\|v_{n}\right\|+c_{2}\left\|v_{n+1}-v_{n}\right\|\right)^{2} \tag{3.4}
\end{equation*}
$$

Reparameterizing to a sequence $a_{0} \leqslant \cdots \leqslant a_{n}$ via $v_{n}=a_{n} v$ yields

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P^{N} * \delta_{v} \| \delta_{x} P^{N}\right) \leqslant\|v\|^{2} \inf _{\substack{a_{0} \leqslant \leqslant a_{N} \\ \text { s.t. } a_{0}=0, a_{N}=1}} \sum_{n=0}^{N-1}\left(c_{1} a_{n}+c_{2}\left(a_{n+1}-a_{n}\right)\right)^{2} \tag{3.5}
\end{equation*}
$$

Further reparameterizing to $b_{n+1}=a_{n+1}-r a_{n}$ where $r:=1-c_{1} / c_{2}$ yields

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P^{N} * \delta_{v} \| \delta_{x} P^{N}\right) \leqslant c_{2}^{2}\|v\|^{2} \inf _{\substack{b_{1}, \ldots, b_{N} \\ \text { s.t. } \\ \sum_{n=1}^{n} r^{N-n} b_{n}=1 \\ b_{n+1} \geqslant(1-r) \sum_{i=1}^{n} b_{i} r^{n-i}, \forall n<N}} \sum_{n=1}^{N} b_{n}^{2} \tag{3.6}
\end{equation*}
$$

This optimization is easily solved in closed form (in the control theory literature this falls under the class of "LQR" problems). Even without the inequality constraints, the unique optimal solution is $b_{i}=r^{N-i}\left(1-r^{2}\right) /\left(1-r^{2 N}\right)$, yielding the optimal value $\left(1-r^{2}\right) /\left(1-r^{2 N}\right)$. Plugging this in completes the proof.

### 3.2 Forward regularity of Itô diffusions

Here we illustrate how the techniques developed in $\S 3.1$ immediately yield tight forward regularity bounds for Itô diffusions and their discretizations. We show tightness in §A.1. This is the first tight regularity bound for (continuous) Itô diffusions, and to our knowledge, it is moreover the first forward regularity bound in the literature for discretized Itô diffusions.

In what follows, consider the Itô SDE setting described at the end of $\S 1$, let $\left(P_{t}\right)_{t \geqslant 0}$ denote the associated semigroup, and let $\hat{P}_{h}$ denote the Markov kernel corresponding to the discretized diffusion with time step $h>0$, i.e.,

$$
\begin{equation*}
\hat{P}_{h}(x, \cdot)=\mathcal{N}\left(x+h b(x), h \sigma \sigma^{\top}\right) . \tag{3.7}
\end{equation*}
$$

Theorem 3.2 (Forward regularity for discretized Itô diffusions). Denote $r:=1-$ Lh. For any step size $h<1 / L$, any number of steps $N \in \mathbb{N}$, any initialization measure $\mu$, and any convolution measures $\nu, \nu^{\prime}$, it holds that

$$
\mathrm{KL}\left(\mu \hat{P}_{h}^{N} * \nu \| \mu \hat{P}_{h}^{N} * \nu^{\prime}\right) \leqslant \frac{1-r^{2}}{2 \lambda h\left(1-r^{2 N}\right)} W_{2}^{2}\left(\nu, \nu^{\prime}\right)
$$

and for any $q \geqslant 1$,

$$
\begin{equation*}
\mathrm{R}_{q}\left(\mu \hat{P}_{h}^{N} * \nu \| \mu \hat{P}_{h}^{N} * \nu^{\prime}\right) \leqslant \inf _{\gamma \in \mathscr{G}\left(\nu, \nu^{\prime}\right)} \frac{1}{q-1} \log \int \exp \left(\frac{q(q-1)\left(1-r^{2}\right)}{2 \lambda h\left(1-r^{2 N}\right)}\left\|v-v^{\prime}\right\|^{2}\right) \gamma\left(\mathrm{d} v, \mathrm{~d} v^{\prime}\right) \tag{3.8}
\end{equation*}
$$

Proof. We show the claim for Rényi divergence; the claim for KL divergence follows by an identical argument or by taking the limit $q \searrow 1$.

First, note that $\hat{P}_{h}$ satisfies the one-step regularity (3.1) with $c_{1}=\frac{L \sqrt{q h}}{\sqrt{2 \lambda}}$ and $c_{2}=\frac{\sqrt{q}}{\sqrt{2 h \lambda}}$ since

$$
\begin{aligned}
\mathrm{R}_{q}\left(\delta_{x} \hat{P}_{h} * \delta_{v} \| \delta_{x+w} \hat{P}_{h}\right) & =\mathrm{R}_{q}\left(\mathcal{N}\left(x+h b(x)+v, h \sigma \sigma^{\top}\right) \| \mathcal{N}\left(x+w+h b(x+w), h \sigma \sigma^{\top}\right)\right) \\
& \leqslant \frac{q}{2 \lambda h}\|h b(x)-h b(x+w)+v-w\|^{2} \\
& \leqslant \frac{q}{2 \lambda h}(L h\|w\|+\|v-w\|)^{2}
\end{aligned}
$$

Above, the first step is by definition of $\hat{P}_{h}$. The second step is by the identity for the Rényi divergence between Gaussians (Theorem 2.2) and the uniform ellipticity $\sigma \sigma^{\top} \geq \lambda I$. The third step is by using the triangle inequality and $L$-Lipschitzness of the drift $b$.

By Theorem 3.1, this one-step regularity implies the multi-step regularity

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} \hat{P}_{h}^{N} * \delta_{v} \| \delta_{x} \hat{P}_{h}^{N}\right) \leqslant \frac{1-\left(1-c_{1} / c_{2}\right)^{2}}{1-\left(1-c_{1} / c_{2}\right)^{2 N}} c_{2}^{2}\|v\|^{2}=\frac{q\left(1-r^{2}\right)}{2 \lambda h\left(1-r^{2 N}\right)}\|v\|^{2} \tag{3.9}
\end{equation*}
$$

The statements of the theorem now follow from the convexity principle (Lemma 2.4).

This tight forward regularity result for the discretized semigroup immediately implies the following tight regularity result for the (standard, continuous-time) semigroup by taking the limit as the step size $h \searrow 0$ and the total elapsed continuous time is fixed to $T=N h$.

Corollary 3.3 (Forward regularity for Itô diffusions). For any time $T$, any initialization measure $\mu$, and any convolution measures $\nu, \nu^{\prime}$,

$$
\begin{equation*}
\mathrm{KL}\left(\mu P_{T} * \nu \| \mu P_{T} * \nu^{\prime}\right) \leqslant \frac{L}{\lambda(1-\exp (-2 L T))} W_{2}^{2}\left(\nu, \nu^{\prime}\right) \tag{3.10}
\end{equation*}
$$

and for any $q \geqslant 1$,

$$
\begin{equation*}
\mathrm{R}_{q}\left(\mu P_{T} * \nu \| \mu P_{T} * \nu^{\prime}\right) \leqslant \inf _{\gamma \in \mathscr{C}\left(\nu, \nu^{\prime}\right)} \frac{1}{q-1} \log \int \exp \left(\frac{L q(q-1)\left\|v-v^{\prime}\right\|^{2}}{\lambda(1-\exp (-2 L T))}\right) \gamma\left(\mathrm{d} v, \mathrm{~d} v^{\prime}\right) \tag{3.11}
\end{equation*}
$$

Remark 3.4 (Refined convexity principle). The results in this section are proved for arbitrary initialization measures $\mu$ and arbitrary convolution measures $\nu, \nu^{\prime}$ by using the convexity principle (Lemma 2.4) to "upgrade" the forward regularity bound (3.9) from the setting where $\mu, \nu, \nu^{\prime}$ are all Dirac. As discussed in Remark 2.5, it is straightforward to instead use the refined version of the convexity principle from [AC23, \$3.3] to obtain tighter bounds for specific settings of $\mu, \nu, \nu^{\prime}$; and/or use a coupled version of the convexity principle to incorporate dependencies between $\mu, \nu, \nu^{\prime}$.

### 3.3 Dual shifted divergences

Shifted divergences - an information-theoretic notion originating from the differential privacy literature $[\mathrm{Fel}+18]$-are intimately connected to backward regularity, as pointed out in Part I [AC23, §3.5]. Here we briefly recall this connection in order to point out a parallel connection between between forward regularity and a new "dual" version of shifted divergences.

Shifted divergences and backward regularity. We begin by recalling the definition of the shifted divergence between distributions $\mu$ and $\nu$ :

$$
\begin{equation*}
\mathcal{D}^{(z)}(\mu \| \nu):=\inf _{\mu^{\prime} \in \mathcal{P}\left(\mathbb{R}^{d}\right): W_{\infty}\left(\mu, \mu^{\prime}\right) \leqslant z} \mathcal{D}\left(\mu^{\prime} \| \nu\right) \tag{3.12}
\end{equation*}
$$

In words, the shift $z \geqslant 0$ enables changing one argument of the divergence $\mathcal{D}$ (typically KL or Rényi) in Wasserstein distance. Shifted divergences are intimately related to backward regularity since it can be shown that

$$
\begin{equation*}
\mathcal{D}\left(\delta_{x} P \| \delta_{y} P\right) \leqslant \mathcal{D}^{(z)}\left(\delta_{x} \| \delta_{y}\right)+c_{P} z^{2} \tag{3.13}
\end{equation*}
$$

for Markov semigroups $P$ such as the Langevin kernel, in both discrete-time [AT23] and continuoustime settings [AC23]. Indeed, choosing $z=\|x-y\|^{2}$ yields the inequality

$$
\begin{equation*}
\mathcal{D}\left(\delta_{x} P \| \delta_{y} P\right) \leqslant c_{P}\|x-y\|^{2} \tag{3.14}
\end{equation*}
$$

which is one of the many equivalent statements of backward regularity for $P$-the other equivalences including reverse transport inequalities (equivalent by the convexity principle), Harnack inequalities (equivalent by Hölder duality), curvature-dimension inequalities (equivalent by semigroup methods), etc. We refer the reader to Part I [AC23] for further discussion.

Dual shifted divergences and forward regularity. In analogy, we now define a "dual" notion of shifted divergences, with negative shift $-z \leqslant 0$ :

$$
\begin{equation*}
\mathcal{D}^{(-z)}(\mu \| \nu):=\sup _{\rho \in \mathcal{P}\left(\mathbb{R}^{d}\right): W_{\infty}\left(\rho, \delta_{0}\right) \leqslant z} \mathcal{D}(\mu * \rho \| \nu)=\sup _{v \in \mathbb{R}^{d}:\|v\| \leqslant z} \mathcal{D}\left(\mu * \delta_{v} \| \nu\right) \tag{3.15}
\end{equation*}
$$

We pause to give a few remarks about this definition. First, now that the shift is negative, the infimum becomes a supremum. Second, (3.15) gives two expressions for this definition-these are equivalent by quasi-convexity of the Rényi divergence and the fact that a convolution with an arbitrary distribution can be equivalently viewed as a mixture over convolutions with Diracs. Third, rather than ranging $\mu^{\prime}$ over a Wasserstein ball around $\mu$, here $\mu^{\prime}$ is further restricted to be of the form $\mu * \rho$ where $\rho$ is supported on a small ball. This is necessary for the dual shifted divergence to be meaningful (e.g., else one can find $\mu^{\prime} k \nu$, rendering the dual shifted divergence infinite).

The key point of this discussion is that, similarly to the interpretation (3.14) of backward regularity in terms of the standard shifted divergence, the forward regularity we proved in $\S 3.2$ (see (3.9) or Theorem 3.2) can be interpreted as follows in terms of the dual shifted divergence:

$$
\begin{equation*}
\mathcal{D}^{(-z)}\left(\delta_{x} P \| \delta_{x} P\right) \leqslant c_{P} z^{2} \tag{3.16}
\end{equation*}
$$

A unified family of shifted divergences. Our notation for dual shifted divergences intentionally unifies this notion with the standard version of shifted divergences. Now the standard shifted divergences are extended by taking an arbitrary $z \in \mathbb{R}$, not just $z \geqslant 0$. This unification applies not just to the definition, but also to some key properties. In particular, the analysis of shifted divergences relies on two lemmas: the convolution lemma [Fel+18, Lemma 20] and the contraction lemma [Fel+18, Lemma 21]. Although the latter does not appear to extend to dual shifted divergences - necessitating our analysis for forward regularity in $\S 3.2$ based on the shifted composition rule - the former does.

We state this lemma below in a unified way that applies to both the original and dual versions of shifted divergences. Below, let $S_{q}(\xi, a):=\sup _{v:\|v\| \leqslant a} \mathrm{R}_{q}\left(\xi * \delta_{v} \| \xi\right)$ denote the Rényi sensitivity of a distribution $\xi$ with respect to a translation of size at most $a$. The proof is given in §A.2.

Lemma 3.5 (Generalized convolution lemma for shifted divergences). For any initial shift $z \in \mathbb{R}$, shift increase $a \geqslant 0$, Rényi parameter $q \geqslant 1$, and distributions $\mu, \nu, \xi$,

$$
\mathbf{R}_{q}^{(z)}(\mu * \xi \| \nu * \xi) \leqslant \mathrm{R}_{q}^{(z+a)}(\mu \| \nu)+S_{q}(\xi, a)
$$

## 4 Continuous-time arguments

In this section, we give the corresponding continuous-time coupling arguments for establishing forward regularity. Since it does not particularly complicate the notation nor the proof, we work with a slightly more general setting than the one described at the end of $\S 1$, i.e., we consider Itô SDEs with time-varying coefficients

$$
\begin{equation*}
\mathrm{d} X_{t}=b_{t}\left(X_{t}\right) \mathrm{d} t+\sigma_{t} \mathrm{~d} B_{t}, \quad X_{0}=x \tag{https://cdn.mathpix.com/cropped/2024_05_26_18ab116b7d6c100cd301g-12.jpg?height=46&width=87&top_left_y=1365&top_left_x=1797}
\end{equation*}
$$

We assume that $b_{t}$ is $L$-Lipschitz and that $\sigma_{t} \sigma_{t}^{\top} \geq \lambda I$ for all $t \geqslant 0$, and for simplicity we take $\sigma_{t} \in \mathbb{R}^{d \times d}$ to be an invertible matrix for each $t \geqslant 0$. We also assume, of course, that (4.1) admits a unique strong solution, which is guaranteed if the coefficients are also Lipschitz in time. Throughout, we let $\mu_{t}:=\operatorname{law}\left(X_{t}\right)$ denote the marginal law of the process.

We define the auxiliary process $Y_{t}:=X_{t}+a_{t} v$ for $t \in[0, T]$, where $\left\{a_{t}\right\}_{t \in[0, T]}$ is increasing and chosen so that $a_{0}=0$ and $a_{T}=1$. This ensures that $Y_{0}=x$ and $Y_{T}=X_{T}+v$. By Itô's formula,

$$
\begin{equation*}
\mathrm{d} Y_{t}=\mathrm{d} X_{t}+\dot{a}_{t} v \mathrm{~d} t=\left(b_{t}\left(Y_{t}-a_{t} v\right)+\dot{a}_{t} v\right) \mathrm{d} t+\sigma_{t} \mathrm{~d} B_{t} \tag{https://cdn.mathpix.com/cropped/2024_05_26_18ab116b7d6c100cd301g-12.jpg?height=48&width=87&top_left_y=1773&top_left_x=1797}
\end{equation*}
$$

This auxiliary process $\left\{Y_{t}\right\}_{t \in[0, T]}$ is the analogue, in the continuous-time limit $h \searrow 0$, of the auxiliary process used in the proof of Theorem 3.1.

Our goal is to bound the Rényi divergence $\mathrm{R}_{q}\left(\mu_{T} * \delta_{v} \| \mu_{T}\right)=\mathrm{R}_{q}\left(\operatorname{law}\left(Y_{T}\right) \| \operatorname{law}\left(X_{T}\right)\right)$. In discrete time, this was achieved using the shifted composition rule. Here we provide two continuous-time analogues. One is based on Girsanov's theorem and stochastic calculus arguments (§4.1), and the other is based on direct differentiation of the quantity $\mathrm{R}_{q}\left(\operatorname{law}\left(Y_{t}\right) \| \operatorname{law}\left(X_{t}\right)\right)$ with respect to $t(\S 4.2)$.

Remark 4.1. For backward regularity, [AC23, §4] provides two continuous-time proofs, based on two different constructions of the auxiliary process: the "synchronous coupling" and "Wasserstein coupling". For forward regularity, the auxiliary process is simply a deterministic shift of the original process, so the coupling is trivial and these two couplings coincide. Nevertherless, the two couplings in [AC23] lead to two distinct proof techniques which are reflected in the following subsections.

### 4.1 Proof via Girsanov's theorem

In the first approach, we control the quantity $\mathrm{R}_{q}\left(\operatorname{law}\left(Y_{T}\right) \| \operatorname{law}\left(X_{T}\right)\right)$ via Girsanov's theorem [Le 16 , Theorem 5.22], since the process $\left\{Y_{t}\right\}_{t \in[0, T]}$ can be realized as a Girsanov transformation of $\left\{X_{t}\right\}_{t \in[0, T]}$. This approach was introduced by F.-Y. Wang in [Wan14b] and has been successfully applied to a multitude of settings, see $\S 1$ for references. Crucially, we show how to apply his method to obtain sharp constants, which were not previously known.

KL divergence bound. We begin with the KL divergence $(q=1)$, since the argument is simpler in this case. By Girsanov's theorem, if $\boldsymbol{\mu}_{T}^{\prime}$ and $\boldsymbol{\mu}_{T}$ denote the path measures corresponding to $\left\{Y_{t}\right\}_{t \in[0, T]}$ and $\left\{X_{t}\right\}_{t \in[0, T]}$ respectively,

$$
\begin{equation*}
\mathrm{KL}\left(\mu_{T} * \delta_{v} \| \mu_{T}\right) \leqslant \operatorname{KL}\left(\boldsymbol{\mu}_{T}^{\prime} \| \boldsymbol{\mu}_{T}\right)=\frac{1}{2} \mathbb{E} \int_{0}^{T}\left\|\sigma_{t}^{-1}\left(b_{t}\left(X_{t}\right)-b_{t}\left(X_{t}+a_{t} v\right)+\dot{a}_{t} v\right)\right\|^{2} \mathrm{~d} t \tag{4.3}
\end{equation*}
$$

Using the uniform ellipticity and the Lipschitzness of the drift, this is bounded by

$$
\frac{\|v\|^{2}}{2 \lambda} \int_{0}^{T}\left(L a_{t}+\dot{a}_{t}\right)^{2} \mathrm{~d} t
$$

To minimize the integrand, we use calculus of variations. Consider the functional

$$
\mathscr{F}(a):=\int_{0}^{T}\left(L a_{t}+\dot{a}_{t}\right)^{2} \mathrm{~d} t
$$

The first variation of $\mathscr{F}$ is computed to be

$$
\delta \mathscr{F}(a)(t)=2\left(L^{2} a_{t}-\ddot{a}_{t}\right)
$$

We set the first variation to zero. With the boundary conditions $a_{0}=0, a_{T}=1$, this is solved with

$$
\begin{equation*}
a_{t}=\frac{\exp (L t)-\exp (-L t)}{\exp (L T)-\exp (-L T)}=\frac{\sinh (L t)}{\sinh (L T)} \tag{4.4}
\end{equation*}
$$

Substituting this in, we obtain the bound

$$
\mathrm{KL}\left(\mu_{T} * \delta_{v} \| \mu_{T}\right) \leqslant \frac{2 L^{2}\|v\|^{2}}{\lambda(\exp (L T)-\exp (-L T))^{2}} \int_{0}^{T} \exp (2 L t) \mathrm{d} t=\frac{L\|v\|^{2}}{\lambda(1-\exp (-2 L T))}
$$

Rényi divergence bound. We now consider the Rényi divergence of order $q>1$. We define the $\boldsymbol{\mu}_{T}$-martingale $M$ via $M_{t}:=\int\left\langle\sigma_{t}^{-1}\left(b\left(X_{t}-a_{t} v\right)-b_{t}\left(X_{t}\right)+\dot{a}_{t} v\right), \mathrm{d} B_{t}\right\rangle$, where $\left\{B_{t}\right\}_{t \in[0, T]}$ is a standard Brownian motion under $\boldsymbol{\mu}_{T}$. By Girsanov's theorem,

$$
\mathrm{R}_{q}\left(\mu_{T} * \delta_{v} \| \mu_{T}\right) \leqslant \mathrm{R}_{q}\left(\boldsymbol{\mu}_{T}^{\prime} \| \boldsymbol{\mu}_{T}\right)=\frac{1}{q-1} \log \mathbb{E}_{\boldsymbol{\mu}_{T}} \exp \left(q M_{T}-\frac{q}{2}[M, M]_{T}\right)
$$

Recall that by Itô's formula, for any martingale $\widetilde{M}$, the exponential martingale $\exp \left(\widetilde{M}-\frac{1}{2}[\widetilde{M}, \widetilde{M}]\right)$ is a non-negative local martingale and hence a supermartingale. Applying this to $\widetilde{M}:=q M$, we have

$$
\mathbb{E}_{\boldsymbol{\mu}_{T}} \exp \left(q M_{T}-\frac{q}{2}[M, M]_{T}\right) \leqslant\left\|\exp \left(\frac{q(q-1)}{2}[M, M]_{T}\right)\right\|_{L^{\infty}\left(\boldsymbol{\mu}_{T}\right)} \underbrace{\mathbb{E}_{\boldsymbol{\mu}_{T}} \exp \left(q M_{T}-\frac{q^{2}}{2}[M, M]_{T}\right)}_{\leqslant 1}
$$

On the other hand,

$$
\left.\left\|[M, M]_{T}\right\|_{L^{\infty}\left(\boldsymbol{\mu}_{T}\right)}=\left\|\int_{0}^{T}\right\| \sigma_{t}^{-1}\left(b\left(X_{t}-a_{t} v\right)-b_{t}\left(X_{t}\right)+\dot{a}_{t} v\right) \|^{2} \mathrm{~d} t\right) \|_{L^{\infty}\left(\boldsymbol{\mu}_{T}\right)} \leqslant \frac{\|v\|^{2}}{\lambda} \int_{0}^{T}\left(L a_{t}+\dot{a}_{t}\right)^{2} \mathrm{~d} t
$$

Substituting in the same choice of $\left\{a_{t}\right\}_{t \in[0, T]}$ as in (4.4), and simplifying, we obtain

$$
\mathrm{R}_{q}\left(\mu_{T} * \delta_{v} \| \mu_{T}\right) \leqslant \frac{q L\|v\|^{2}}{\lambda(1-\exp (-2 L T))}
$$

### 4.2 Proof via divergence differentiation

We now provide an argument based on differentiation along the Fokker-Planck equation which, to the best of our knowledge, is new. In the following proof, the case of the KL divergence ( $q=1$ ) is not substantially simpler than the case of general Rényi divergences $(q>1)$, so we provide the proof of the latter to avoid repetition.

The proof is based on the following observation. Denoting $\mu_{t}^{\prime}:=\operatorname{law}\left(Y_{t}\right):=\operatorname{law}\left(X_{t}+a_{t} v\right)$, one can easily check that

$$
\begin{align*}
& \partial_{t} \mu_{t}=\frac{1}{2}\left\langle\sigma \sigma^{\top}, \nabla^{2} \mu_{t}\right\rangle-\operatorname{div}\left(\mu_{t} b_{t}\right)  \tag{4.5}\\
& \partial_{t} \mu_{t}^{\prime}=\frac{1}{2}\left\langle\sigma \sigma^{\top}, \nabla^{2} \mu_{t}^{\prime}\right\rangle-\operatorname{div}\left(\mu_{t}^{\prime}\left(b_{t} \circ \tau_{-a_{t} v}+\dot{a}_{t} v\right)\right)
\end{align*}
$$

where for $u \in \mathbb{R}^{d}, \tau_{u}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ denotes the translation $x \mapsto x+u$. Indeed, $\left\{X_{t}\right\}_{t \in[0, T]}$ solves the SDE (4.1) and $\left\{Y_{t}\right\}_{t \in[0, T]}$ solves the SDE (4.2), so the marginal laws of these processes solve the respective Fokker-Planck equations.

With (4.5) in hand, we are now in a position to apply a lemma on differentiation of divergences along diffusions which has been utilized in prior works such as [VW19; Che+22]. We state a version of this lemma which generalizes [AC23, Lemma A.5]; its proof follows exactly along the same lines.

Lemma 4.2. Let $\psi: \mathbb{R}_{+} \rightarrow \mathbb{R}_{+}$be twice continuously differentiable on $(0, \infty)$. Consider the associated divergence

$$
\mathrm{D}_{\psi}(\mu \| \nu):=\int \psi\left(\frac{\mathrm{d} \mu}{\mathrm{d} \nu}\right) \mathrm{d} \nu
$$

Suppose that $\left(\mu_{t}\right)_{t \geqslant 0},\left(\nu_{t}\right)_{t \geqslant 0}$ are positive and smooth densities evolving according to the equations

$$
\begin{aligned}
& \partial_{t} \mu_{t}=\frac{1}{2}\left\langle\sigma_{t} \sigma_{t}^{\top}, \nabla^{2} \mu_{t}\right\rangle+\operatorname{div}\left(\mu_{t} a_{t}\right) \\
& \partial_{t} \nu_{t}=\frac{1}{2}\left\langle\sigma_{t} \sigma_{t}^{\top}, \nabla^{2} \nu_{t}\right\rangle+\operatorname{div}\left(\nu_{t} b_{t}\right)
\end{aligned}
$$

Here, $\left(a_{t}\right)_{t \geqslant 0}$ and $\left(b_{t}\right)_{t \geqslant 0}$ are families of vector fields on $\mathbb{R}^{d}$, and for each $t \geqslant 0, \sigma_{t} \in \mathbb{R}^{d \times d}$ is an invertible matrix. Then, it holds that

$$
\partial_{t} \mathrm{D}_{\psi}\left(\mu_{t} \| \nu_{t}\right)=-\mathbb{E}_{\mu_{t}}\left\langle\nabla\left(\psi^{\prime} \circ \frac{\mu_{t}}{\nu_{t}}\right), \frac{1}{2} \sigma_{t} \sigma_{t}^{\top} \nabla \log \frac{\mu_{t}}{\nu_{t}}+a_{t}-b_{t}\right\rangle
$$

We apply this with $\psi(\cdot)=(\cdot)^{q}-1$, for which $\mathrm{D}_{\psi}=\mathrm{D}_{q}$. The simultaneous flow lemma yields

$$
\partial_{t} \mathrm{D}_{q}\left(\mu_{t}^{\prime} \| \mu_{t}\right)=-q \mathbb{E}_{\mu_{t}^{\prime}}\left|\nabla\left[\left(\frac{\mu_{t}^{\prime}}{\mu_{t}}\right)^{q-1}\right], \frac{1}{2} \sigma_{t} \sigma_{t}^{\top} \nabla \log \frac{\mu_{t}^{\prime}}{\mu_{t}}+b_{t}-b_{t} \circ \tau_{-a_{t} v}-\dot{a}_{t} v\right\rangle
$$

By the chain rule,

$$
\mathbb{E}_{\mu_{t}^{\prime}}\left\langle\nabla\left[\left(\frac{\mu_{t}^{\prime}}{\mu_{t}}\right)^{q-1}\right], \sigma_{t} \sigma_{t}^{\top} \nabla \log \frac{\mu_{t}^{\prime}}{\mu_{t}}\right\rangle=(q-1) \mathbb{E}_{\mu_{t}^{\prime}}\left[\left(\frac{\mu_{t}^{\prime}}{\mu_{t}}\right)^{q-1}\left\|\nabla \log \frac{\mu_{t}^{\prime}}{\mu_{t}}\right\|_{\sigma_{t} \sigma_{t}^{\top}}^{2}\right]
$$

where, for a positive definite matrix $\Sigma>0$, we write $\|x\|_{\Sigma}^{2}:=\langle x, \Sigma x\rangle$. Also, by Young's inequality,

$$
\begin{aligned}
& -\mathbb{E}_{\mu_{t}^{\prime}}\left|\nabla\left[\left(\frac{\mu_{t}^{\prime}}{\mu_{t}}\right)^{q-1}\right], b_{t}-b_{t} \circ \tau_{-a_{t} v}-\dot{a}_{t} v\right\rangle=-(q-1) \mathbb{E}_{\mu_{t}^{\prime}}\left[\left(\frac{\mu_{t}^{\prime}}{\mu_{t}}\right)^{q-1}\left\langle\nabla \log \frac{\mu_{t}^{\prime}}{\mu_{t}}, b_{t}-b_{t} \circ \tau_{-a_{t} v}-\dot{a}_{t} v\right\rangle\right] \\
& \quad \leqslant(q-1)\left\{\frac{1}{2} \mathbb{E}_{\mu_{t}^{\prime}}\left[\left(\frac{\mu_{t}^{\prime}}{\mu_{t}}\right)^{q-1}\left\|\nabla \log \frac{\mu_{t}^{\prime}}{\mu_{t}}\right\|_{\sigma_{t} \sigma_{t}^{\top}}^{2}\right]+\frac{1}{2} \mathbb{E}_{\mu_{t}}\left[\left(\frac{\mu_{t}^{\prime}}{\mu_{t}}\right)^{q-1}\left\|b_{t}-b_{t} \circ \tau_{-a_{t} v}-\dot{a}_{t} v\right\|_{\left(\sigma_{t} \sigma_{t}^{\top}\right)^{-1}}^{2}\right]\right\}
\end{aligned}
$$

Hence,

$$
\begin{aligned}
\partial_{t} \mathrm{D}_{q}\left(\mu_{t}^{\prime} \| \mu_{t}\right) & \leqslant \frac{q(q-1)}{2} \mathbb{E}_{\mu_{t}^{\prime}}\left[\left(\frac{\mu_{t}^{\prime}}{\mu_{t}}\right)^{q-1}\left\|b_{t}-b_{t} \circ \tau_{-a_{t} v}-\dot{a}_{t} v\right\|_{\left(\sigma_{t} \sigma_{t}^{\top}\right)^{-1}}^{2}\right] \\
& \leqslant \frac{q(q-1)\|v\|^{2}}{2 \lambda}\left(L a_{t}+\dot{a}_{t}\right)^{2} \mathbb{E}_{\mu_{t}^{\prime}}\left[\left(\frac{\mu_{t}^{\prime}}{\mu_{t}}\right)^{q-1}\right]
\end{aligned}
$$

Differentiating the Rényi divergence via the chain rule, noting that $\mathrm{R}_{q}=\frac{1}{q-1} \log \left(1+\mathrm{D}_{q}\right)$,

$$
\partial_{t} \mathrm{R}_{q}\left(\mu_{t}^{\prime} \| \mu_{t}\right) \leqslant \frac{q\|v\|^{2}}{2 \lambda}\left(L a_{t}+\dot{a}_{t}\right)^{2}
$$

and therefore

$$
\mathrm{R}_{q}\left(\mu_{T} * \delta_{v} \| \mu_{T}\right) \leqslant \frac{q\|v\|^{2}}{2 \lambda} \int_{0}^{T}\left(L a_{t}+\dot{a}_{t}\right)^{2} \mathrm{~d} t
$$

Choosing $\left\{a_{t}\right\}_{t \in[0, T]}$ as in (4.4), we again arrive at the sharp bound

$$
\mathrm{R}_{q}\left(\mu_{T} * \delta_{v} \| \mu_{T}\right) \leqslant \frac{q L\|v\|^{2}}{\lambda(1-\exp (-2 L T))}
$$

## 5 Shift Harnack inequalities and curvature upper bounds

We begin with brief background on shift Harnack inequalities in $\S 5.1$ (see also the discussion in §1). We recall the duality between shift Harnack inequalities and reverse transport inequalities in $\S 5.2$ which, together with our results from $\S 3$ and $\S 4$, lead to our shift Harnack inequalities for the Langevin diffusion with optimal constants. We leverage these sharp inequalities to prove our main Theorem 1.1 detailing equivalences and implications between the shift Harnack inequalities, a local gradient-entropy bound, and curvature bounds. Finally, implications for the stationary distribution are discussed in $\S 5.4$.

### 5.1 Background on shift Harnack inequalities

We now apply our results to the study of shift Harnack inequalities, which were introduced by F.-Y. Wang in [Wan14b]. In particular, for a given Markov semigroup $\left(P_{t}\right)_{t \geqslant 0}$, we consider inequalities which take the form

$$
\begin{equation*}
P_{t}(f(\cdot+v))^{p} \leqslant P_{t}\left(f^{p}\right) \exp \left(C_{p}(t)\|v\|^{2}\right) \tag{5.1}
\end{equation*}
$$

for all $v \in \mathbb{R}^{d}$, all $t>0$, and all non-negative functions $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{\geqslant 0}$. We also consider the shift $\log$-Harnack inequality of the form

$$
\begin{equation*}
P_{t}(f(\cdot+v)) \leqslant C_{\log }(t)\|v\|^{2}+\log P_{t}(\exp f) \tag{5.2}
\end{equation*}
$$

again for all $v \in \mathbb{R}^{d}$, all $t>0$, and all non-negative functions $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{\geqslant 0}$.

Recall from $\S 1$ that whereas Harnack inequalities encode regularity for Kolmogorov's backward equation, shift Harnack inequalities were introduced to encode the regularity of Kolmogorov's forward equation, and therefore yield information about the Lebesgue density of the marginal law of the stochastic process. For instance, the validity of either (5.1) or (5.2) implies that for each $t>0$, the semigroup $P_{t}$ admits a transition density $p_{t}$ with respect to Lebesgue measure which satisfies certain bounds, see [Wan13, §1.4.2] for further applications such as heat kernel bounds. For intuition, we illustrate this principle with a simple example below.

However, whereas Harnack inequalities are known to be equivalent to the curvature-dimension condition, as well as to Wasserstein contraction and other functional inequalities such as the local Poincaré/log-Sobolev inequalities and gradient commutation inequalities (c.f. [BGL14; Wan14a] or [AC23, §6.1]), such equivalences have not been fleshed out for shift Harnack inequalities. We aim to explore this point further in $\S 5.3$.

An illustrative example. As we show in the subsequent section, shift (log) Harnack inequalities are dual to certain reverse transport inequalities. In particular, (5.1) is equivalent to an inequality of the form

$$
\begin{equation*}
\mathrm{R}_{q}\left(\delta_{x} P_{t} * \delta_{v} \| \delta_{x} P_{t}\right) \leqslant \bar{C}_{q}(t)\|v\|^{2} \tag{5.3}
\end{equation*}
$$

for another constant $\bar{C}_{q}(t)>0$, where $q=p /(p-1)$ is the dual exponent. Supposing now that (5.3) holds for $p=q=2$, let $\phi: \mathbb{R}^{d} \rightarrow \mathbb{R}$ be a test function (i.e., smooth and compactly supported). For any vector $v \in \mathbb{R}^{d} \backslash\{0\}$, we can estimate $\left|\left\langle\mathbb{E}_{\delta_{x} P_{t}} \nabla \phi, v\right\rangle\right|=\left|\left\langle P_{t}(\nabla \phi)(x), v\right\rangle\right|$ via

$$
\begin{aligned}
\lim _{\varepsilon \searrow 0} \frac{\left|\mathbb{E}_{\delta_{x} P_{t}}[\phi(\cdot+\varepsilon v)-\phi(\cdot)]\right|}{\varepsilon} & =\lim _{\varepsilon \searrow 0} \frac{\left|\mathbb{E}_{\delta_{x} P_{t} * \delta_{\varepsilon v}} \phi-\mathbb{E}_{\delta_{x} P_{t}} \phi\right|}{\varepsilon} \leqslant\|\phi\|_{L^{2}\left(\delta_{x} P_{t}\right)} \cdot \limsup _{\varepsilon \searrow 0} \frac{\sqrt{\chi^{2}\left(\delta_{x} P_{t} * \delta_{\varepsilon v} \| \delta_{x} P_{t}\right)}}{\varepsilon} \\
& \leqslant\|\phi\|_{L^{2}\left(\delta_{x} P_{t}\right)} \cdot \limsup _{\varepsilon \searrow 0} \frac{\sqrt{\exp \left(\mathrm{R}_{2}\left(\delta_{x} P_{t} * \delta_{\varepsilon v} \| \delta_{x} P_{t}\right)\right)-1}}{\varepsilon} \\
& \leqslant \sqrt{\bar{C}_{2}(t)}\|\phi\|_{L^{2}\left(\delta_{x} P_{t}\right)}\|v\| .
\end{aligned}
$$

This inequality establishes $\left\|\mathbb{E}_{\delta_{x} P_{t}} \nabla \phi\right\| \leqslant \sqrt{C_{2}(t)}\|\phi\|_{L^{2}\left(\delta_{x} P_{t}\right)}$ for all test functions $\phi$. Since we can formally write $\mathbb{E}_{\delta_{x} P_{t}} \nabla \phi=-\mathbb{E}_{\delta_{x} P_{t}}\left[\phi \nabla \log \delta_{x} P_{t}\right]$, this can be interpreted as an $L^{2}$ bound for the logarithmic gradient of the Lebesgue density of $\delta_{x} P_{t}$. In $\S 5.4$, we make this viewpoint precise by deriving sharp high-probability bounds for the logarithmic gradient.

### 5.2 Duality between shift Harnack inequalities and reverse transport inequalities

Since we aim to deduce shift Harnack inequalities from the reverse transport inequalities established in preceding sections, we formally state and prove a duality principle below.

Lemma 5.1. Let $\mu$ be a probability measure over $\mathbb{R}^{d}$ and let $v \in \mathbb{R}^{d}$.

1. Let $C_{\log , v}>0$. Then,

$$
\begin{equation*}
\mu(f(\cdot+v)) \leqslant C_{\log , v}+\log \mu(\exp f) \quad \text { for all } f: \mathbb{R}^{d} \rightarrow \mathbb{R} \tag{5.4}
\end{equation*}
$$

if and only if

$$
\mathrm{KL}\left(\mu * \delta_{v} \| \mu\right) \leqslant C_{\log , v}
$$

2. Let $q>1$ and let $p:=q /(q-1)$ denote its Hölder conjugate. Let $C_{p, v}>0$. Then,

$$
\begin{equation*}
\mu(f(\cdot+v)) \leqslant C_{p, v} \mu\left(f^{p}\right)^{1 / p} \quad \text { for all } f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{\geqslant 0} \tag{5.5}
\end{equation*}
$$

if and only if

$$
\mathrm{R}_{q}\left(\mu * \delta_{v} \| \mu\right) \leqslant \frac{q}{q-1} \log C_{p, v}
$$

Proof. Define the linear functional $L_{v}: L^{p}(\mu) \rightarrow \mathbb{R}$ via $f \mapsto \mu(f(\cdot+v))$. Then, the best constant $C_{p, v}$ in the shift Harnack inequality (5.5) equals the operator norm $\left\|L_{v}\right\|_{L^{p}(\mu) \rightarrow \mathbb{R}}$. On the other hand, since $L_{v} f=\int f \mathrm{~d}\left(\mu * \delta_{v}\right)=\int f \frac{\mathrm{d}\left(\mu * \delta_{v}\right)}{\mathrm{d} \mu} \mathrm{d} \mu$, by Hölder duality it follows that $C_{p, v}=\left\|\frac{\mathrm{d}\left(\mu * \delta_{v}\right)}{\mathrm{d} \mu}\right\|_{L^{q}(\mu)}$, from which the second equivalence follows.

Similarly, the first equivalence follows from the Donsker-Varadhan variational principle, which shows that the best constant $C_{\log , v}$ in the inequality (5.4) equals $\mathrm{KL}\left(\mu * \delta_{v} \| \mu\right)$.

### 5.3 Relationship with local gradient-entropy bounds and curvature upper bounds

By combining the sharp reverse transport inequalities from $\S 3$ and $\S 4$ with the duality principle from $\S 5.2$, we are now able to obtain sharp shift (log) Harnack inequalities. Moreover, as discussed in $\S 1$, the sharpness of the inequalities allows us to explore their relationship with other functional inequalities (namely, a "local gradient-entropy inequality") and with curvature bounds. For convenience, we restate and prove our main theorem to this effect.

Theorem 1.1. Let $\left(P_{t}\right)_{t \geqslant 0}$ denote the Markov semigroup corresponding to the Langevin diffusion with potential $V$. Let $\beta>0$ and $p, q>1$. Consider the following properties.

(CurvBdd) The two-sided curvature bound $-\beta I \leq \nabla^{2} V \leq \beta I$ holds.

(LGE) The local gradient-entropy bound

$$
\frac{\left\|P_{t} \nabla f\right\|^{2}}{P_{t} f} \leqslant \frac{2 \beta}{1-\exp (-2 \beta t)}\left\{P_{t}(f \log f)-P_{t} f \log P_{t} f\right\}
$$

holds for all $t>0$ and all smooth $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}$.

$\left(\mathrm{SH}_{p}\right)$ The shift Harnack inequality

$$
P_{t}(f(\cdot+v))^{p} \leqslant \exp \left(\frac{\beta p\|v\|^{2}}{2(p-1)(1-\exp (-2 \beta t))}\right) P_{t}\left(f^{p}\right)
$$

holds for all $v \in \mathbb{R}^{d}$, all $t>0$, and all $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}$.
$\left(\mathrm{SRT}_{q}\right)$ The shift reverse transport inequality

$$
\mathrm{R}_{q}\left(\delta_{x} P_{t} * \delta_{v} \| \delta_{x} P_{t}\right) \leqslant \frac{\beta q\|v\|^{2}}{2(1-\exp (-2 \beta t))}
$$

holds for all $x, v \in \mathbb{R}^{d}$ and all $t>0$.

$\left(\mathrm{SH}_{\log }\right)$ The shift log-Harnack inequality

$$
P_{t}(f(\cdot+v)) \leqslant \log P_{t}(\exp f)+\frac{\beta\|v\|^{2}}{2(1-\exp (-2 \beta t))}
$$

holds for all $v \in \mathbb{R}^{d}$, all $t>0$, and all $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}$.

$\left(\mathrm{SRT}_{1}\right)$ The shift reverse transport inequality

$$
\mathrm{KL}\left(\delta_{x} P_{t} * \delta_{v} \| \delta_{x} P_{t}\right) \leqslant \frac{\beta\|v\|^{2}}{2(1-\exp (-2 \beta t))}
$$

holds for all $x, v \in \mathbb{R}^{d}$ and all $t>0$.

(CurvUB) The curvature upper bound $\nabla^{2} V \leq \beta I$ holds.

Then, the following implications hold.

$$
\begin{aligned}
& (\text { CurvBdd }) \Rightarrow(\mathrm{LGE}) \Leftrightarrow\left(\forall p>1\left(\mathrm{SH}_{p}\right)\right) \Longrightarrow\left(\exists p>1\left(\mathrm{SH}_{p}\right)\right) \Rightarrow\left(\mathrm{SH}_{\log }\right) \Rightarrow \text { (CurvUB) } \\
& \left(\forall q>1 \stackrel{\hat{\imath}}{\left(\mathrm{SRT}_{q}\right)}\right) \Rightarrow\left(\exists q>1 \stackrel{\hat{\imath}}{\left(\mathrm{SRT}_{q}\right)}\right) \Rightarrow\left(\begin{array}{c}
\hat{\imath} \\
\left(\mathrm{SRT}_{1}\right)
\end{array}\right.
\end{aligned}
$$

Proof. (CurvBdd) $\Rightarrow\left[\forall p>1\left(\mathrm{SH}_{p}\right)\right] \Rightarrow\left[\exists p>1\left(\mathrm{SH}_{p}\right)\right]$. The first implication immediately follows from the reverse transport inequalities established in $\S 3$ and $\S 4$, together with Lemma 5.1. The second implication is trivial.

$\left(\mathrm{SH}_{p}\right) \Leftrightarrow\left(\mathrm{SRT}_{q}\right)$ for $q=\frac{p}{p-1}$, and $\left(\mathrm{SH}_{\mathrm{log}}\right) \Leftrightarrow\left(\mathrm{SRT}_{1}\right)$. This follows from the duality encapsulated in Lemma 5.1.

$\left[\forall p>1\left(\mathrm{SH}_{p}\right)\right] \Leftrightarrow(\mathrm{LGE})$. According to [Wan13, Proposition 1.3.2] ${ }^{2}$, the validity of the shift Harnack inequalities for all $p>1$ is equivalent to

$$
\left|\left\langle P_{t} \nabla f, v\right\rangle\right| \leqslant \eta\left\{P_{t}(f \log f)-P_{t} f \log P_{t} f\right\}+\frac{\beta\|v\|^{2}}{2 \eta(1-\exp (-2 \beta t))} P_{t} f, \quad \text { for all } \eta>0
$$

Optimizing over the choice of $\eta$, this is equivalent to

$$
\frac{\left|\left\langle P_{t} \nabla f, v\right\rangle\right|}{\|v\|} \leqslant \sqrt{\frac{2 \beta}{1-\exp (-2 \beta t)} P_{t} f\left\{P_{t}(f \log f)-P_{t} f \log P_{t} f\right\}}
$$

Since this holds for every choice of $v \in \mathbb{R}^{d} \backslash\{0\}$, this is (LGE).

$\left[\exists p>1\left(\mathrm{SH}_{p}\right)\right] \Rightarrow\left(\mathrm{SH}_{\log }\right)$. This implication is [Wan13, Theorem 1.3.5].

$\left(\mathrm{SH}_{\log }\right) \Rightarrow($ CurvUB). Finally, we show that the shift log-Harnack inequality implies back the curvature upper bound via Taylor expansion.[^1]

Let $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ be a compactly supported and smooth function such that $\nabla f(x)=u \in \mathbb{R}^{d}$ and $\nabla^{2} f(x)=0$. We apply the shift $\log$-Harnack inequality $\left(\mathrm{SH}_{\log }\right)$ with $v=c t u$ for some $c>0$ to be chosen later, yielding the inequality

$$
\begin{equation*}
P_{t}(f(\cdot+c t u))(x) \leqslant \log P_{t}(\exp f)(x)+\frac{\beta c^{2} t^{2}\|\nabla f(x)\|^{2}}{2(1-\exp (-2 \beta t))} \tag{5.6}
\end{equation*}
$$

We now perform a careful Taylor expansion. For the left-hand side,

$$
\partial_{t} P_{t}(f(\cdot+c t u))(x)=\mathscr{L} P_{t}(f(\cdot+c t u))(x)+c\left\langle P_{t} \nabla f(\cdot+c t u), u\right\rangle(x),
$$

and hence, since $\nabla^{2} f(x)=0$, we obtain

$$
\begin{aligned}
\left.\partial_{t}\right|_{t=0} P_{t}(f(\cdot+c t u))(x) & =\mathscr{L} f(x)+c\|\nabla f(x)\|^{2} \\
\left.\partial_{t}^{2}\right|_{t=0} P_{t}(f(\cdot+c t u))(x) & =\mathscr{L}^{2} f(x)+2 c\langle\nabla f(x), \mathscr{L} \nabla f(x)\rangle
\end{aligned}
$$

For the first term on the right-hand side,

$$
\partial_{t} \log P_{t}(\exp f)(x)=\frac{\mathscr{L} P_{t}(\exp f)(x)}{\exp f(x)}
$$

and a tedious calculation based on the diffusion chain rule $\mathscr{L} \phi(f)=\phi^{\prime}(f) \mathscr{L} f+\phi^{\prime \prime}(f)\|\nabla f\|^{2}$, the carré du champ identity $\mathscr{L}(f g)=f \mathscr{L} g+g \mathscr{L} f+2\langle\nabla f, \nabla g\rangle$, and $\nabla^{2} f(x)=0$ yields

$$
\begin{aligned}
& \left.\partial_{t}\right|_{t=0} \log P_{t}(\exp f)(x)=\frac{\mathscr{L}(\exp f)(x)}{\exp f(x)}=\mathscr{L} f(x)+\|\nabla f(x)\|^{2} \\
& \left.\partial_{t}^{2}\right|_{t=0} \log P_{t}(\exp f)(x)=\mathscr{L}^{2} f(x)+\mathscr{L}\left(\|\nabla f\|^{2}\right)(x)+2\langle\nabla f(x), \nabla \mathscr{L} f(x)\rangle
\end{aligned}
$$

We now set $c=2$, substitute these expansions into (5.6), and divide by $t^{2}$ to obtain

$$
\begin{aligned}
2\langle\nabla f(x), \mathscr{L} \nabla f(x)\rangle \leqslant & \frac{1}{2} \mathscr{L}\left(\|\nabla f\|^{2}\right)(x)+\langle\nabla f(x), \nabla \mathscr{L} f(x)\rangle \\
& +\frac{1}{t}\left(\frac{2 \beta t}{1-\exp (-2 \beta t)}-1\right)\|\nabla f(x)\|^{2}+o(1)
\end{aligned}
$$

Sending $t \searrow 0$, it yields the inequality

$$
2\langle\nabla f(x), \mathscr{L} \nabla f(x)\rangle \leqslant \frac{1}{2} \mathscr{L}\left(\|\nabla f\|^{2}\right)(x)+\langle\nabla f(x), \nabla \mathscr{L} f(x)\rangle+\beta\|\nabla f(x)\|^{2}
$$

We now use the identities $\frac{1}{2} \mathscr{L}\left(\|\nabla f\|^{2}\right)-\langle\nabla f, \nabla \mathscr{L} f\rangle=\Gamma_{2}(f, f)$, where $\Gamma_{2}$ denotes the iterated carré du champ operator, and the commutation relation $\mathscr{L} \nabla f-\nabla \mathscr{L} f=\nabla^{2} V \nabla f$. Hence,

$$
2\left\langle\nabla f(x), \nabla^{2} V(x) \nabla f(x)\right\rangle \leqslant \Gamma_{2}(f, f)(x)+\beta\|\nabla f(x)\|^{2}
$$

Recall that $\Gamma_{2}(f, f)=\left\langle\nabla f, \nabla^{2} V \nabla f\right\rangle+\left\|\nabla^{2} f\right\|_{\mathrm{HS}}^{2}$. Since we have chosen $\nabla f(x)=u$ and $\nabla^{2} f(x)=0$, we finally obtain

$$
\left\langle u, \nabla^{2} V(x) u\right\rangle \leqslant \beta\|u\|^{2}
$$

Since this holds true for all $u, x \in \mathbb{R}^{d}$, it proves $\nabla^{2} V \leq \beta I$.

Remark 5.2 (Comparison with the local LSI). The local gradient-entropy bound (LGE) can be compared to the following two forms of the local log-Sobolev inequality, which are equivalent to the curvature-dimension condition $\operatorname{CD}(\alpha, \infty)$ (see [BGL14, Theorem 5.5.2]):

$$
\begin{aligned}
P_{t}\left(\frac{\|\nabla f\|^{2}}{f}\right) & \geqslant \frac{2 \alpha}{1-\exp (-2 \alpha t)}\left\{P_{t}(f \log f)-P_{t} f \log P_{t} f\right\} \\
\frac{\left\|\nabla P_{t} f\right\|^{2}}{P_{t} f} & \leqslant \frac{2 \alpha}{\exp (2 \alpha t)-1}\left\{P_{t}(f \log f)-P_{t} f \log P_{t} f\right\}
\end{aligned}
$$

We conclude this section with a few speculative comments.

The chain of implications in Theorem 1.1-with sharp constants - is, to our knowledge, new. However, we are unable to close the chain, e.g., by showing that (CurvUB) suffices to establish $\left(\mathrm{SH}_{p}\right)$. As discussed in $\$ 1$, characterizations of (CurvUB) would be of great interest toward a synthetic theory of Ricci curvature upper bounds. In this direction, we remark that if $\left(\mathrm{SH}_{\log }\right)$ is suitably generalized to a Riemannian manifold $\mathcal{M}$ by replacing the translation $f(\cdot+v)$ with $f \circ \exp v$, where $v$ is a smooth vector field on $\mathcal{M}$, then the implication $\left(\mathrm{SH}_{\mathrm{log}}\right) \Rightarrow$ (CurvUB) appears to generalize to yield $\nabla^{2} V+$ Ric $\leqslant \beta$, essentially because $\Gamma_{2}(f, f)=\left\langle\nabla f,\left(\nabla^{2} V+\right.\right.$ Ric $\left.) \nabla f\right\rangle+\left\|\nabla^{2} f\right\|_{\mathrm{HS}}^{2}$.

We also note that another route toward characterizing (CurvUB) would be to show that it is equivalent to a "gradient commutation bound". For example, let us try to establish $\left(\mathrm{SH}_{\text {log }}\right)$ via a semigroup calculation. It is natural to differentiate $s \mapsto P_{s} \phi\left(P_{t-s}\left(f\left(\cdot+a_{s} v\right)\right)\right)$ where $\left\{a_{s}\right\}_{s \in[0, t]}$ satisfies $a_{0}=0$ and $a_{t}=1$ and we take $\phi:=\log$. Upon carrying out the calculation, one sees that the following gradient bound is needed:

$$
\begin{equation*}
\left\|P_{s} \nabla f\right\| \stackrel{?}{\leqslant} \exp (\beta s)\left\|\nabla P_{s} f\right\|, \quad \text { for all } s \geqslant 0 \tag{5.7}
\end{equation*}
$$

We are unsure if (5.7) holds, and in any case, we are unable to prove it. We leave the further investigation of these questions to future work.

### 5.4 Implications for the stationary distribution

Since the sharp constants in Theorem 1.1 tend to finite limits as $t \rightarrow \infty$, it immediately furnishes consequences for the stationary distribution $\pi$ of the Langevin diffusion, if it exists.

Corollary 5.3. Suppose that $\pi \propto \exp (-V)$ defines a probability measure over $\mathbb{R}^{d}$ and that $V$ satisfies the two-sided curvature bound $-\beta I \leq \nabla^{2} V \leq \beta I$, where $\beta>0$. Then, the following hold.

- (Gradient-entropy bound, $\mathrm{LGE}_{\pi}$ ) For all smooth $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}$,

$$
\frac{\left\|\mathbb{E}_{\pi} \nabla f\right\|^{2}}{\mathbb{E}_{\pi} f} \leqslant 2 \beta \operatorname{ent}_{\pi}(f)
$$

- (Shift Harnack inequality) For all $p>1$, all $v \in \mathbb{R}^{d}$, and all $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}$,

$$
\left\{\mathbb{E}_{\pi} f(\cdot+v)\right\}^{p} \leqslant \exp \left(\frac{\beta p\|v\|^{2}}{2(p-1)}\right) \mathbb{E}_{\pi}\left(f^{p}\right)
$$

- (Reverse Rényi transport inequality) For all $q>1$ and all $v \in \mathbb{R}^{d}$,

$$
\mathrm{R}_{q}\left(\pi * \delta_{v} \| \pi\right) \leqslant \frac{\beta q\|v\|^{2}}{2}
$$

- (Shift $\log$-Harnack inequality) For all $v \in \mathbb{R}^{d}$ and all $f: \mathbb{R}^{d} \rightarrow \mathbb{R}_{>0}$,

$$
\begin{equation*}
\mathbb{E}_{\pi} f(\cdot+v) \leqslant \log \mathbb{E}_{\pi}(\exp f)+\frac{\beta\|v\|^{2}}{2} \tag{5.8}
\end{equation*}
$$

- (Reverse $K L$ transport inequality) For all $v \in \mathbb{R}^{d}$,

$$
\begin{equation*}
\mathrm{KL}\left(\pi * \delta_{v} \| \pi\right) \leqslant \frac{\beta\|v\|^{2}}{2} \tag{5.9}
\end{equation*}
$$

Proof. The proofs are immediate by taking $t \rightarrow \infty$ in Theorem 1.1 and applying duality.

Remark 5.4. The reverse $K L$ transport inequality (5.9) can be established via a simple calculation assuming only (CurvUB). ${ }^{3}$ Indeed, since $\nabla^{2} V \leq \beta I$, Taylor expansion yields

$$
\mathrm{KL}\left(\pi * \delta_{v} \| \pi\right)=\int(V(x+v)-V(x)) \pi(\mathrm{d} x) \leqslant \int\left(\langle\nabla V(x), v\rangle+\frac{\beta\|v\|^{2}}{2}\right) \pi(\mathrm{d} x)=\frac{\beta\|v\|^{2}}{2}
$$

where we used the identity $\mathbb{E}_{\pi} \nabla V=0$. By duality ( $\$ 5.2$ ), this also yields the shift log-Harnack inequality (5.8). However, we are unable to establish the other statements in Corollary 5.3 under (CurvUB) alone, rather than the stronger (CurvBdd); c.f. the discussions in $\$ 1$ and §5.3.

To illustrate the information about $\pi$ that such inequalities capture, we explore the consequences of ( $\mathrm{LGE}_{\pi}$ ). If we take $f$ with $\mathbb{E}_{\pi} f=1$ and set $\mathrm{d} \mu=f \mathrm{~d} \pi$, then it is equivalent to the following inequality: for every regular probability measure $\mu \ll \pi$,

$$
\begin{equation*}
2 \beta \mathrm{KL}(\mu \| \pi) \geqslant\left\|\mathbb{E}_{\pi} \nabla \frac{\mathrm{d} \mu}{\mathrm{d} \pi}\right\|^{2}=\left\|\mathbb{E}_{\mu} \nabla \log \frac{\mathrm{d} \mu}{\mathrm{d} \pi}\right\|^{2}=\left\|\mathbb{E}_{\mu} \nabla V\right\|^{2} \tag{5.10}
\end{equation*}
$$

where we used the score identity $\mathbb{E}_{\mu} \nabla \log \mu=0$. We now demonstrate how to extract from this formulation a concentration inequality for $\nabla V$ under $\pi$. We restate the result for convenience.

Theorem 1.2. Let $\pi \propto \exp (-V)$ be a probability measure on $\mathbb{R}^{d}$ such that $\left(\mathrm{LGE}_{\pi}\right)$ holds. Then, under $\pi$, the score $\nabla V$ is $\sqrt{\beta}$-sub-Gaussian.

Proof. Let $e \in \mathbb{R}^{d}$ be a unit vector, so that $\left\|\mathbb{E}_{\mu} \nabla V\right\| \geqslant\left|\mathbb{E}_{\mu}\langle e, \nabla V\rangle\right|$. For shorthand, we write $V_{e}:=\langle e, \nabla V\rangle$. Before proceeding to the proof, we first provide some intuition. Recall the BobkovGötze argument [BG99]: to show that $V_{e}$ is sub-Gaussian, i.e., to bound the logarithmic momentgenerating function $\log \mathbb{E}_{\pi} \exp \left(\lambda V_{e}\right)$, we first apply the Donsker-Varadhan variational principle to argue that $\log \mathbb{E}_{\pi} \exp \left(\lambda V_{e}\right)=\sup _{\mu \in \mathcal{P}\left(\mathbb{R}^{d}\right)}\left\{\lambda \mathbb{E}_{\mu} V_{e}-\mathrm{KL}(\mu \| \pi)\right\}$. If $V_{e}$ is $L$-Lipschitz and if $\pi$ satisfies a transport inequality of the form $W_{1}(\mu, \pi) \leqslant \sqrt{2 \alpha^{-1} \mathrm{KL}(\mu \| \pi)}$, it implies (since $\mathbb{E}_{\pi} V_{e}=0$ )

$$
\log \mathbb{E}_{\pi} \exp \left(\lambda V_{e}\right) \leqslant \sup _{\mu \in \mathcal{P}\left(\mathbb{R}^{d}\right)}\left\{L \lambda \sqrt{2 \alpha^{-1} \mathrm{KL}(\mu \| \pi)}-\mathrm{KL}(\mu \| \pi)\right\} \leqslant \frac{L^{2} \lambda^{2}}{2 \alpha}
$$

We follow this argument, but instead of assuming that $V_{e}$ is Lipschitz and that $\pi$ satisfies a transport inequality, we instead use the bound on $\left|\mathbb{E}_{\mu} V_{e}\right|$ implied by (5.10). This yields

$$
\log \mathbb{E}_{\pi} \exp \left(\lambda V_{e}\right) \leqslant \sup _{\mu \in \mathcal{P}\left(\mathbb{R}^{d}\right)}\{\lambda \sqrt{2 \beta \mathrm{KL}(\mu \| \pi)}-\mathrm{KL}(\mu \| \pi)\} \leqslant \frac{\beta \lambda^{2}}{2}
$$

as desired.[^2]

Using covering arguments, Theorem 1.2 implies various other high-probability bounds on the size of $\nabla V$ under $\pi$. To illustrate, we give a bound on $\|\nabla V\|$. With Theorem 1.2 in hand, the covering-based proof technique is standard and we provide it for completeness.

Corollary 5.5. Let $\pi \propto \exp (-V)$ be a probability measure on $\mathbb{R}^{d}$ such that $\left(\mathrm{LGE}_{\pi}\right)$ holds. There is a universal constant $C>0$ such that for all $0<\delta<1 / 2$, with probability at least $1-\delta$ under $\pi$,

$$
\|\nabla V\| \leqslant C(\sqrt{\beta d}+\sqrt{\beta \log (1 / \delta)})
$$

Proof. This bound on the logarithmic moment-generating function in Theorem 1.2 shows that for every unit vector $e$ and any $\eta>0$,

$$
\pi\left\{V_{e} \geqslant \eta\right\} \leqslant \exp \left(-\frac{\eta^{2}}{2 \beta}\right)
$$

To obtain our gradient bound, let $\mathcal{N}$ be a $\frac{1}{2}$-net of the unit sphere (i.e., $\mathcal{N}$ has Hausdorff distance at most $\frac{1}{2}$ from the sphere). There exists such a set of size $\exp (O(d))$. By taking a union bound over vectors in this net,

$$
\pi\left\{\sup _{e \in \mathcal{N}} V_{e} \geqslant \eta\right\} \leqslant \exp \left(-\frac{\eta^{2}}{2 \beta}+O(d)\right)
$$

On this event, we have

$$
\|\nabla V\|=\left\langle\frac{\nabla V}{\|\nabla V\|}, \nabla V\right\rangle \leqslant \inf _{e \in \mathcal{N}}\left\{|\langle e, \nabla V\rangle|+\left|\left\langle\frac{\nabla V}{\|\nabla V\|}-e, \nabla V\right\rangle\right|\right\} \leqslant \sup _{e \in \mathcal{N}}|\langle e, \nabla V\rangle|+\frac{1}{2}\|\nabla V\|
$$

Therefore,

$$
\pi\{\|\nabla V\| \geqslant \eta\} \leqslant \pi\left\{\sup _{e \in \mathcal{N}}\left|V_{e}\right| \geqslant \frac{\eta}{2}\right\} \leqslant \exp \left(-\frac{\eta^{2}}{8 \beta}+O(d)\right)
$$

Finally, by choosing $\eta \asymp \sqrt{\beta(d+\log (1 / \delta))}$, we obtain the statement.

This bound can be compared with [LST20, Corollary 6] which implies that if $V$ is convex and $\beta$-smooth, then with probability at least $1-\delta$,

$$
\begin{equation*}
\|\nabla V\| \leqslant \sqrt{\beta d}+C \sqrt{\beta} \log (1 / \delta) \tag{5.11}
\end{equation*}
$$

This concentration bound was used to sharpen the mixing time bounds for the Metropolis-adjusted Langevin algorithm obtained in [Dwi+18; Che +20$]$.

The bound (5.11) achieves a sharper leading term of $\sqrt{\beta d}$ (in fact, they can replace $\sqrt{\beta d}$ with $\mathbb{E}_{\pi}\|\nabla V\|$, which is sharper since $\mathbb{E}_{\pi}\|\nabla V\| \leqslant \sqrt{\mathbb{E}_{\pi}\left[\|\nabla V\|^{2}\right]}=\sqrt{\mathbb{E}_{\pi} \Delta V} \leqslant \sqrt{\beta d}$, where the middle equality follows from integration by parts). On the other hand, Corollary 5.5 captures the correct tail behavior (sub-Gaussian rather than subexponential). Moreover, whereas [LST20] relied on the Brascamp-Lieb inequality [BL76] and hence on convexity of $V$, our argument shows that it is really the smoothness of $V$ (through $\left(\mathrm{LGE}_{\pi}\right)$ ) that matters here.

We also mention that concentration of $V$ itself under $\pi$ has been studied in a number of works and in relation to the dimensional improvement of the Brascamp-Lieb inequality; see, e.g., [Ngu14; Wan14c; FMW16; BGG18; Che23].

Acknowledgements. JMA acknowledges the support of an NYU Faculty Fellowship. SC acknowledges the support of the Eric and Wendy Schmidt Fund at the Institute for Advanced Study.

## References

[AC23] J. M. Altschuler and S. Chewi. "Shifted composition I: Harnack and reverse transport inequalities". In: arXiv preprint arXiv:2311.14520 (2023).

[AT23] J. M. Altschuler and K. Talwar. "Resolving the mixing time of the Langevin algorithm to its stationary distribution for log-concave sampling". In: Conference on Learning Theory. Vol. 195. PMLR, 2023, pp. 2509-2510.

[AGS14] L. Ambrosio, N. Gigli, and G. Savaré. "Metric measure spaces with Riemannian Ricci curvature bounded from below". In: Duke Math. J. 163.7 (2014), pp. 1405-1490.

[AGS15] L. Ambrosio, N. Gigli, and G. Savaré. "Bakry-Émery curvature-dimension condition and Riemannian Ricci curvature bounds". In: Ann. Probab. 43.1 (2015), pp. 339-404.

[BGL14] D. Bakry, I. Gentil, and M. Ledoux. Analysis and geometry of Markov diffusion operators. Vol. 103. Springer, 2014.

[BG99] S. G. Bobkov and F. Götze. "Exponential integrability and transportation cost related to logarithmic Sobolev inequalities". In: J. Funct. Anal. 163.1 (1999), pp. 1-28.

[BGG18] F. Bolley, I. Gentil, and A. Guillin. "Dimensional improvements of the logarithmic Sobolev, Talagrand and Brascamp-Lieb inequalities". In: Ann. Probab. 46.1 (2018), pp. 261-301.

[BL76] H. J. Brascamp and E. H. Lieb. "On extensions of the Brunn-Minkowski and PrékopaLeindler theorems, including inequalities for $\log$ concave functions, and with an application to the diffusion equation". In: J. Functional Analysis 22.4 (1976), pp. 366-389.

[Che+22] Y. Chen, S. Chewi, A. Salim, and A. Wibisono. "Improved analysis for a proximal algorithm for sampling". In: Conference on Learning Theory. Vol. 178. PMLR, 2022, pp. 2984-3014.

[Che+20] Y. Chen, R. Dwivedi, M. J. Wainwright, and B. Yu. "Fast mixing of Metropolized Hamiltonian Monte Carlo: benefits of multi-step gradients". In: Journal of Machine Learning Research 21 (2020).

[Che23] S. Chewi. "The entropic barrier is $n$-self-concordant". In: Geometric Aspects of Functional Analysis: Israel Seminar (GAFA) 2020-2022. Ed. by R. Eldan, B. Klartag, A. Litvak, and E. Milman. Cham: Springer International Publishing, 2023, pp. 209-222.

[Dwi+18] R. Dwivedi, Y. Chen, M. J. Wainwright, and B. Yu. "Log-concave sampling: MetropolisHastings algorithms are fast!' In: Conference on Learning Theory. PMLR. 2018, pp. 793797.

[EKS15] M. Erbar, K. Kuwada, and K.-T. Sturm. "On the equivalence of the entropic curvaturedimension condition and Bochner's inequality on metric measure spaces". In: Invent. Math. 201.3 (2015), pp. 993-1071.

[Fan15] X. Fan. "Stochastic Volterra equations driven by fractional Brownian motion". In: Front. Math. China 10.3 (2015), pp. 595-620.

[Fel+18] V. Feldman, I. Mironov, K. Talwar, and A. Thakurta. "Privacy amplification by iteration". In: Symposium on Foundations of Computer Science. IEEE. 2018, pp. 521532.

[FMW16] M. Fradelizi, M. Madiman, and L. Wang. "Optimal concentration of information content for log-concave densities". In: High dimensional probability VII. Vol. 71. Progr. Probab. Springer, 2016, pp. 45-60.

[Hua19] X. Huang. "Harnack and shift Harnack inequalities for SDEs with integrable drifts". In: Stoch. Dyn. 19.5 (2019), pp. $1950034,11$.

[HW19] X. Huang and F.-Y. Wang. "Distribution dependent SDEs with singular coefficients". In: Stochastic Process. Appl. 129.11 (2019), pp. 4747-4770.

[Le 16] J.-F. Le Gall. Brownian motion, martingales, and stochastic calculus. Vol. 274. Graduate Texts in Mathematics. Springer, 2016, p. 273.

[LST20] Y. T. Lee, R. Shen, and K. Tian. "Logsmooth gradient concentration and tighter runtimes for Metropolized Hamiltonian Monte Carlo". In: Conference on Learning Theory. Vol. 125. PMLR, 2020, pp. 2565-2597.

[LV09] J. Lott and C. Villani. "Ricci curvature for metric-measure spaces via optimal transport". In: Ann. of Math. (2) 169.3 (2009), pp. 903-991.

[LH21] W. Lv and X. Huang. "Harnack and shift Harnack inequalities for degenerate (functional) stochastic partial differential equations with singular drifts". In: J. Theoret. Probab. 34.2 (2021), pp. 827-851.

[Mir17] I. Mironov. "Rényi differential privacy". In: Computer Security Foundations Symposium. IEEE. 2017, pp. 263-275.

[Ngu14] V. H. Nguyen. "Dimensional variance inequalities of Brascamp-Lieb type and a local approach to dimensional Prékopa's theorem". In: J. Funct. Anal. 266.2 (2014), pp. 931955.

[Stu06a] K.-T. Sturm. "On the geometry of metric measure spaces. I". In: Acta Math. 196.1 (2006), pp. 65-131.

[Stu06b] K.-T. Sturm. "On the geometry of metric measure spaces. II". In: Acta Math. 196.1 (2006), pp. 133-177.

[Stu21] K.-T. Sturm. "Remarks about synthetic upper Ricci bounds for metric measure spaces". In: Tohoku Math. J. (2) 73.4 (2021), pp. 539-564.

[VH14] T. Van Erven and P. Harremos. "Rényi divergence and Kullback-Leibler divergence". In: IEEE Transactions on Information Theory 60.7 (2014), pp. 3797-3820.

[VW19] S. Vempala and A. Wibisono. "Rapid convergence of the unadjusted Langevin algorithm: isoperimetry suffices". In: Advances in Neural Information Processing Systems 32. 2019, pp. 8092-8104.

[Wan97] F.-Y. Wang. "Logarithmic Sobolev inequalities on noncompact Riemannian manifolds". In: Probab. Theory Related Fields 109.3 (1997), pp. 417-424.

[Wan04] F.-Y. Wang. "Equivalence of dimension-free Harnack inequality and curvature condition". In: Integral Equations Operator Theory 48.4 (2004), pp. 547-552.

[Wan10] F.-Y. Wang. "Harnack inequalities on manifolds with boundary and applications". In: J. Math. Pures Appl. (9) 94.3 (2010), pp. 304-321.

[Wan13] F.-Y. Wang. Harnack inequalities for stochastic partial differential equations. SpringerBriefs in Mathematics. Springer, New York, 2013, pp. x+125.

[Wan14a] F.-Y. Wang. Analysis for diffusion processes on Riemannian manifolds. Vol. 18. Advanced Series on Statistical Science \& Applied Probability. World Scientific Publishing Co. Pte. Ltd., Hackensack, NJ, 2014, pp. xii+379.

[Wan14b] F.-Y. Wang. "Integration by parts formula and shift Harnack inequality for stochastic equations". In: Ann. Probab. 42.3 (2014), pp. 994-1019.

[Wan16] F.-Y. Wang. "Integration by parts formula and applications for SPDEs with jumps". In: Stochastics 88.5 (2016), pp. $737-750$.

[Wan18] F.-Y. Wang. "Distribution dependent SDEs for Landau type equations". In: Stochastic Process. Appl. 128.2 (2018), pp. 595-621.

[Wan14c] L. Wang. Heat capacity bound, energy fluctuations and convexity. Thesis (Ph.D.)-Yale University. ProQuest LLC, Ann Arbor, MI, 2014, p. 114.

[Wu20] B. Wu. "Characterizations of the upper bound of Bakry-Èmery curvature". In: J. Geom. Anal. 30.4 (2020), pp. 3923-3947.

[Zha16] S. Zhang. "Shift Harnack inequality and integration by parts formula for semilinear stochastic partial differential equations". In: Front. Math. China 11.2 (2016), pp. 461496.
