# LLM4ED: LARGE LANGUAGE MODELS FOR AUTOMATIC EQUATION DISCOVERY 

Mengge Du<br>College of Engineering<br>Peking University<br>Beijing<br>Zhongzheng Wang<br>College of Engineering<br>Peking University<br>Beijing

Yuntian Chen<br>Ningbo Institute of Digital Twin, Eastern Institute of Technology<br>Ning bo<br>ychen@eitech.edu.cn<br>Longfeng Nie<br>School of Environmental Science and Engineering<br>Southern University of Science and Technology<br>Shenzhen

Dongxiao Zhang

National Center for Applied Mathematics Shenzhen (NCAMS)

Southern University of Science and Technology

Shenzhen

zhangdx@eitech.edu.cn


#### Abstract

Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.


Keywords Symbolic equation discovery $\cdot$ Large language models $\cdot$ Evolutionary search $\cdot$ Prompt learning.

## 1 Introduction

Physical laws often follow concise governing equations, which are crucial for our understanding and transformation of the natural world. With the development of artificial intelligence, simulation of the evolution of nonlinear systems through deep learning has gradually emerged [1, 2, 3]. However, these methods are limited by black-box models and lack interpretability. To tackle this issue, equation discovery methods that uncover potential physical laws from observations with explicit mathematic formulas have received increasing attention, which can not only facilitate a deeper understanding of physical processes but also provide domain guidance for data-driven models and enhance
their predictive robustness [4, 5]. Moreover, with the governing equation incorporated as physical constraints, neural networks can be equipped with better physical intuition and possess better extrapolation ability [6].

In nonlinear systems, states of interest often follow differential equations, such as ordinary differential equations, in the form of $\dot{\mathbf{x}}=f(\mathbf{x}(t))$, where $\mathbf{x}(t)=\left\{x_{1}(t), x_{2}(t), \ldots, x_{n}(t)\right\}^{T} \in \mathbb{R}^{m}$ denotes the state variables with the spatial dimension of $m$. The main objective of equation discovery is to find the explicit expression of $f$. Traditionally, this process was based on first principles, which might require experts in the relevant domain to engage in extensive mathematical derivations. In recent years, data-driven methods are gradually rising because of their superior efficiency and applicability [7, 8]. In particular, SINDy (Sparse Identification of Nonlinear Dynamics) has emerged as an effective method to tackle this challenge [9]. It assumes that the form of $f$ can be simplified as a linear combination of a series of candidate basis functions, where the basis function library is often predetermined based on prior knowledge. With the advantages of high computational efficiency and simple methodology, SINDy has achieved good performance across various fields [10, 11, 12, 13]. Nevertheless, the reliance on prior knowledge inherently constrains the applicability of this approach, rendering it challenging to uncover more intricate representational forms. Concurrently, the progress of numerous intelligent optimization algorithms has contributed to the utilization of symbolic mathematics in identifying governing equations with more flexible forms. EQL (Equation learner) [14, 15] endeavors to utilize the topological structure of networks to represent equations with different combinations and substituting activations with arithmatic operators, such as + and - . An alternative approach seeks to represent equations with expression trees, aiming to discover the optimal equation by optimizing the tree structure. Common optimization methods are based on gradient descent [16, 17, 18], reinforcement learning [19, 20, 21, 22], or evolutionary algorithms [23, 24, 25, 26]. These approaches substantially diminish the reliance on prior physical knowledge, enabling wider application scenarios. However, laborious and intricate algorithm design and coding efforts are required for equation generation and optimization, which is not conducive to wide-scale promotion.

Transformer-based large language models (LLMs) have continuously emerged and have achieved remarkable results in various application domains in recent years [27, 28, 29, 30]. A vast number of trainable parameters and a large diverse training corpus enable LLMs to possess strong generation and reasoning capabilities. Some recent studies have started to explore the potential of LLMs in mathematical reasoning [31], algorithmic optimization [32], and code generation [33], with some even employing LLMs as direct optimizers to tackle black-box optimization challenges [34]. A salient question is whether we can leverage LLMs to automatically complete equation discovery without additional parametric models and optimization processes.

In this paper, we propose a large language model-based framework for automatic equation discovery, as shown in Fig. 1 . The equations are first generated with strings format after prompting LLMs with a clear symbol library and problem descriptions. The equations can be seamlessly parsed and transformed into expression trees via the domain tool in symbolic mathematics. During the optimization phase, LLMs can serve as an optimizer to conduct the self-improvement process. LLMs are instructed to conduct local refinements to the historical equations based on the analysis of the inherent relationship between the different combinations of symbols and their performance. On the other hand, welldesigned prompts are used to guide LLMs to apply user-defined evolution operators on elite equations, promoting the generation of more diverse equation combinations. These two approaches are iterative and employed in an alternating direction until the optimal equation satisfies the termination conditions. Our framework is testified for uncovering the correct PDE equations in several canonical nonlinear systems and verified that the two optimization approaches of local modification and random evolution have a synergistic effect. In addition, we further validated our framework on sixteen one-dimensional ODE systems, and the results showed that it could achieve comparable performance to the state-of-the-art. Our main contributions are as follows:

- We propose an automated equation discovery framework that utilizes the natural language generation and reasoning capabilities of LLMs. The framework eliminates the need for manually crafting intricate programs for equation generators and optimizers and is totally parametric-free during optimization.
- We employ manually designed prompts to guide LLMs in executing two optimization approaches: selfimprovement and evolutionary search. The alternating iterative optimization strategy effectively strikes a balance between exploration and exploitation.
- We validate the efficacy of our framework through a series of experiments on ODEs and PDEs systems. The results demonstrate that its performance is on par with or even better than the state-of-the-art symbolic regression methods, encouraging additional research and application of LLMs in the domain of equation discovery.


## 2 Related Works

### 2.1 Symbolic Equation discovery

Symbolic mathematics-based methods can directly uncover the potential relationships between variables from data. With the development of computational equipment and machine learning, these methods have gradually gained increasing attention. Equation discovery tasks typically encompass three phases: generation, evaluation, and optimization. In the generation stage, based on certain context-free grammars [35], equations in mathematical form are transformed into expression trees. The internal nodes of the expression tree are predefined operators (e.g.,,+- ) and operands (e.g., observations $x$ or constant). By conducting a top-down traversal of the expression, a unique sequential representation can be generated. This representation is more concise and enables more efficient batch generation and gradient-based optimization [19, 36]. Some constraints are carefully designed to generate dimensional consistent expressions and ensure the physical and mathematical rationality. In the evaluation stage, the main focus is to assess the performance of the discovered equations in terms of their fit to the data and complexity. Finally, in the optimization stage, the commonly utilized algorithms mainly include genetic programming [26], gradient descent-based neural network models [14], and recently emerging reinforcement learning models [19, 20]. At the same time, pretrained models based on transformers have gradually emerged [37, 17, 38, 39]. These models are trained on a large amount of data and can directly output the discovered equation results based on the observations, greatly accelerating the inference speed. Evidently, approaches founded on symbolic mathematics necessitate manually designed algorithms in multiple aspects, elevating the learning and application barrier. Conversely, our framework, guided by natural language, significantly streamlines the generation and optimization components, enabling researchers to concentrate solely on the evaluation aspect, where domain expertise is genuinely essential.

### 2.2 Large language model for optimization

The powerful language understanding and generation capabilities of large models have led to their extensive application in various fields [27, 28, 29, 30]. Studies have recently demonstrated the feasibility of employing prompt engineering to direct LLMs in addressing optimization problems. One approach is to directly use LLMs as optimizers in a selfimprovement updating manner [40, 41]. Taking into account the problem definition and previously generated solutions, LLMs can be directed to refine candidate solutions iteratively. The findings suggest that LLMs possess the capability to progressively improve the generated solutions by building upon the knowledge gained from past optimization results. Other related works attempt to combine LLMs with evolutionary search methods to solve optimizations. Prompts can be designed to instruct LLMs to execute evolutionary algorithms to incrementally enhance the existing solutions within the population. This synergistic combination ultimately leads to the discovery of novel insights and advancements in addressing open research questions, including combinatorial optimization problems like (e.g. traveling salesman problems [34]), multiobjective evolutionary optimization [42], prompt optimization [43], algorithm design [32, 31], game design [44], and evolutionary strategies [45].

Our method pioneered the application of LLMs in the field of equation discovery, constructing a plug-and-play discovery framework. By leveraging natural language, we have seamlessly integrated the self-improvement capabilities of LLMs with evolutionary search techniques, which effectively strikes a balance between exploitation and exploration. The proposed method ensures the stability and efficiency of optimization while finding the optimal equation.

## 3 Methods

### 3.1 Problem overview

The goal of the equation discovery task is to identify an explicit mathematical expression $\mathcal{F}$, defined by mathematical symbols, based on a given set of observations. The true form $\mathbb{F}$ should satisfy

$$
\dot{x}=\mathcal{F}(x ; \xi), \quad \mathcal{F}: \mathbb{R}^{D} \rightarrow \mathbb{R}
$$

where the state variable $x(t) \in \mathbb{R}^{D} ; \dot{x}$ refers to the time derivatives; and $\xi$ denotes the possible constants. We aim to find an optimal expression $\mathcal{F}$ that accurately describes the true underlying physical laws in the dynamical system while keeping the form concise. The form of $\mathcal{F}$ may differ slightly for nonlinear systems governed by different types of equations. In this paper, we consider two types of governing equations: PDEs and ODEs. For ODEs, the form of equations can be generated by freely combining symbols from a predefined library, including constants $\xi$. The value of $\xi$ is typically determined using optimization techniques that minimize a specific data fitting metric, such as the mean square error (MSE). For PDEs, the right-hand side of the equation often consists of the combinations of state variables (e.g., $u$ ) and their spatial derivatives (e.g., $u_{x}$ and $u_{x x}$ ). Similar to the previous SINDy-based methods [9, 46], we

![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-04.jpg?height=588&width=1637&top_left_y=243&top_left_x=233)

Figure 1: Overview of the proposed framework.

simplify $\mathcal{F}$ to be represented by a linear combination of a series of basis function terms $\Theta(u, x)$. The difference is that function terms can be represented by any combination of symbols without including constants rather than the predefined monomials. Constants only appear as coefficients of the function terms, i.e., $\mathcal{F} \approx \Theta(u, x) \cdot \xi$. The coefficients $\xi$ of the function terms can then be obtained through sparse regression.

In this paper, the expression of $\mathcal{F}$ is generated on the basis of LLMs. We can further empower LLMs with Sympy [47], a domain open-source Python library for symbolic mathematics, to parse the string form and convert it into an expression tree, which facilitates the evaluation of the results.

### 3.2 Framework

Our framework employs natural language to guide LLMs in generating and optimizing equations. During the generation process, LLMs draw upon extensive prior training data to produce mathematically reasonable expressions. For optimization, we employ an alternating iterative approach that combines self-improvement and evolutionary search to refine the generated equations. Users are only required to concentrate on establishing appropriate evaluation criteria to precisely evaluate the generated equations. The schematic diagram of our framework is shown in Fig. 2 .

### 3.3 Initialization

The initial equation population can be generated through LLMs or based on prior knowledge, i.e., manually predefined equations. In this study, we mainly employ prompts to direct LLMs in randomly generating the initial population with a given symbol library. Firstly, LLMs have been trained on extensive text data, enabling them to learn numerous effective equation representations. Consequently, the generated equations generally follow mathematical principles. Secondly, constraints can be established using natural language, thereby preventing the occurrence of equations that violate the specified conditions. For instance, constraints can include restricting the equation length, and frequency of specific symbols, and preventing the generation of invalid nested combinations. Traditionally, implementing these constraints necessitated intricate code, such as generating equations based on probabilistic context-free grammars [48, 49] or subtly modifying probabilities during the symbol sampling process [19].

### 3.4 Evaluation

LLMs excels at creative generation based on enormous corpus, but need to be further strengthened with domain tools and human-designed feedback to deal with the symbolc discovery task. Regarding the equation expressions that have been generated in string format, we can employ Sympy [47] to parse and instantiate them as corresponding symbolic expression trees. Before evaluating them, we first need to determine the parameters in the expressions, i.e., constants, and then further score them according to the designed score function.

### 3.4.1 Constant optimization

This study considers two types of governing equations: PDEs and ODEs. Depending on the specific features of the equations they represent, we adopt two distinct approaches to evaluate the constants. For PDEs, constants mainly appear

![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-05.jpg?height=1019&width=913&top_left_y=255&top_left_x=606)

Figure 2: Workflow of the proposed framework.

as coefficients of function terms. Therefore, we first need to decompose the expression tree by splitting it into equation terms based on the "+" and "-" operators at the top of the tree, and then further solve for the coefficients using sparse regression methods, as shown in Fig. 3 Terms with nontrivial coefficients will be kept and others will be removed for simplicity.

$$
\begin{equation*}
\xi_{p d e}^{*}=\arg \min _{\xi}\left|\Theta(u, x) \cdot \xi-u_{t}\right|_{2}^{2}+\lambda|\xi|_{2}^{2} \tag{1}
\end{equation*}
$$

For ODEs, constants can appear at any position in the expression tree. We first generate equation structures through LLMs, namely "skeleton", and then utilize the Broyden-Fletcher-Goldfarb-Shanno algorithm (BFGS) [50] to execute the following optimization objective.

$$
\begin{equation*}
\xi_{\text {ode }}^{*}=\operatorname{argmin}_{\xi} \sum_{i=1}^{n} \frac{1}{n}\left(\dot{x}-\mathcal{F}\left(x_{i} ; \xi\right)\right)^{2} \tag{2}
\end{equation*}
$$

Rounds of optimization iterations are performed using Scipy.Minimize to ultimately determine all the constants in the expression tree. Note that if the generated equations do not contain the constant operator, sparse regression techniques can be employed to assign coefficients that are not 1 to each term, thereby improving the accuracy of the discovered equations and facilitating the identification of lengthy true equations.

### 3.4.2 Score function

After obtaining the values of the constants in the equation, we designed a score function to evaluate the performance of the generated equations.

$$
\begin{gather*}
S=\frac{1-\zeta_{1} \times m}{1+N R M S E}  \tag{3}\\
N R M S E=\frac{1}{\sigma_{\dot{x}}} \sqrt{\frac{1}{N} \sum_{i=1}^{N}\left(\dot{x}_{t_{i}}-\mathcal{F}\left(x_{i}\right)\right)^{2}} \tag{4}
\end{gather*}
$$

(a) PDE expression tree

![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-06.jpg?height=309&width=336&top_left_y=344&top_left_x=282)

(b) Evaluate subtrees

![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-06.jpg?height=290&width=615&top_left_y=367&top_left_x=625)

(c) Determine coefficients of subtrees

![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-06.jpg?height=331&width=569&top_left_y=347&top_left_x=1233)

Figure 3: Workflow of the proposed framework.

where the normalized root-mean-square error (NRMSE) is employed as a fitness metric to evaluate the discrepancy between the left and right sides of the equation. We penalize the number of equation terms $m$ in the equation numerator to encourage finding more concise forms and $\zeta$ refers to the penalty coefficient. Through the designed score function, we can assign a score to each equation, then select elite equations, and introduce them into the prompt to guide subsequent optimization.

### 3.5 Optimization

This study utilizes two LLM-guided optimization techniques to enhance the optimization process. The self-improvement method primarily performs local modifications based on the equation's performance, while the genetic algorithm-based approach is conducted for a global search on the elite equations. Our goal is to achieve a better balance between exploration and exploitation.

### 3.5.1 Self-improvement process

LLMs have been demonstrated in numerous experiments to function as gradient-free optimizers, possessing the ability to draw inferences from historical data and iteratively optimize to produce superior samples [34]. we include historical elite equations and their corresponding scores as equation-score pairs within the prompt, enabling LLMs to perform local modifications using these data. The modifications primarily encompass two facets: (1) recognizing and eliminating redundant equation terms by leveraging historical data; (2) incorporating and generating novel random equation terms built upon existing equations. These two operations resemble the introduction of "delete" and "add" operators, which can effectively utilize the historical elite samples and aptly supplement the unstable updating of genetic algorithms. An example case of the self-improvement process is shown in Fig. 4 and the customized prompts are demonstrated in Appendix A.

![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-06.jpg?height=393&width=873&top_left_y=1793&top_left_x=623)

Figure 4: Self-improvement process executed by LLMs.

### 3.5.2 Equation evolution process

Genetic algorithms are one of the commonly used global optimization methods inspired by natural selection [51, 52]. Evolutionary operators can be applied to the parent individuals to generate new offersprings. In particular, this procedure required an intricate design and application on tree structures in symbolic regression. In this paper, we employ natural
language to guide LLMs in the execution of the genetic algorithms, rather than relying on manual coding. Specifically, we conduct crossover and mutation operations on the $M$ equation populations generated in the past, thereby producing a greater variety of equation combinations. The process consists of two steps:

Select parent population Historical elite equations will be incorporated into the prompt for the evolution process, originating from two sources: a predefined priority queue caching the top $K$ historically elite equations and high-quality samples selected from the last iterations. By combining them, we ultimately retain the $M$ better-performing equations as the parent population.

Selection and evolution The entire process comprises three steps. First, LLMs randomly select two equations from the population as parents and then guide them to perform equation crossover to produce new equations. This process can involve both the crossover of entire equation terms and the crossover within equation terms. Finally, further mutations of operands or operators are performed based on the new equations. Ultimately, iterating the three steps until $M$ offspring are produced. The whole process is directed and performed in natural language and is shown schematically in Fig. 5 .

Parents

![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-07.jpg?height=477&width=1269&top_left_y=922&top_left_x=428)

Figure 5: Crossover and mutation executed by LLMs.

On one hand, As optimization in genetic algorithms often involves substantial randomness, despite having strong global search abilities, the efficiency of optimization remains suboptimal. Consequently,

### 3.6 Prompt Engineering

Throughout the entire equation discovery process, our generation and optimization rely on natural language-based prompts, which follow a similar structure in the three processes of initialization, evolution, and self-improvement. The standard format consists of the following components, as shown in Fig. 1 .

- Task descriptions: This part primarily explains the main task and defines the symbol library including operators (e.g.,,$+-\ldots$ and operands (e.g., $x$, const).
- Historical examples: We pass $M$ high-quality equations generated in the past as historical information to LLMs. A priority queue of the Top $K$ expressions in history is established to ensure stable and efficient optimization. All expressions within the priority queue are incorporated into the prompts. Furthermore, we select $M-K$ expressions from the last iteration to preserve sampling diversity, as illustrated in Fig. 2. The manifestation of these samples within the prompt varies according to the optimization techniques employed. In the evolution process, only high-quality equations with string format are presented, while in the self-improvement process, historical samples are shown in the form of equation-score pairs.
- Instructions: This part is aimed at guiding LLMs to optimize and generate new equations. In the initialization stage, LLMs are primarily required to freely combine symbols from the library to produce equations of arbitrary form and length. In the optimization stage, LLM is mainly guided to generate optimized equations based on the form of historical equations, using different evolutionary operators or making local modifications (i.e. self-improvement).
- Other hints or constraints: In the initialization stage, we can directly impose requirements on the form of the generated equations through natural language, acting as constraints to limit the search space. In the

Table 1: Default hyperparameter settings.

| Hyperparameter | Default value | Definition |
| :---: | :---: | :---: |
| $M$ | 20 | Number of expressions generated at each iteration |
| $P$ | 100 | Number of total iterations |
| $K$ | 5 | Size of the priority queue |
| $N_{\text {term }}$ | 6 | Maximum number of function terms |
| $\zeta_{1}$ | 0.01 | Parsimony penalty factor for redundant function terms |
| $\lambda$ | 0.001 | Weight of the STRidge regularization term |
| tol | $1 \times 10^{-4}$ | Threshold of weights for reserved function terms |

optimization stage, we can further define local modifications and evolutionary operators, and provide possible examples as a few-shot prompt.

The utilized prompts in this paper are demonstrated in $\mathrm{A}$.

## 4 Resutls

### 4.1 Evaluation metrics

The experimental section provides the discovery results of the suggested framework for both PDEs and ODEs. We consider PDE equations to be represented as a linear combination of equation terms of arbitrary form, and the constants are primarily solved through sparse regression. Our goal is to find the exact equation form, and the accuracy of the identified equation is assessed by determining the equation's coefficient error.

$$
E=\frac{1}{n} \sum_{i=1}^{n} \frac{\left|\xi_{i}^{*}-\xi_{i}\right|}{\left|\xi_{i}\right|} \times 100 \%
$$

where $n$ denotes the total number of function terms; $\xi_{i}, \xi_{i}^{*}$ refer to the true coefficients and identified coefficients, respectively. ODEs are more complex in symbolic form. A skeleton with defined symbols need to be constructed first and followed by the optimization of constants within the skeleton, which may generate more symbol combinations. Compared to identifying the most consistent expression in symbolic form, it is more crucial and meaningful to conduct numerical evaluation. Specifically, we aim to find an effective $\hat{\mathcal{F}}$, whose solution trajectories approximate the observed $x$ in the current numerical domain, i.e., all of the expressions are evaluated by the reconstruction accuracy. Furthermore, the other critical criterion is that the solution of the identified $\hat{\mathcal{F}}$ precisely fits the correct trajectories even when the initial condition varies. We utilize the coefficient of determination $\left(R^{2}\right)$ as the metric to evaluate the agreement between the solution trajectories and the true trajectories: $R^{2}=1-\frac{\sum_{i}^{n}\left(x_{i}-\hat{x}_{i}\right)^{2}}{\sum_{i}^{n}\left(x_{i}-\bar{x}\right)^{2}} \in(-\infty, 1]$, where $x_{i}$ denote observations and $\hat{x}_{i}$ refer to predicted values.

### 4.2 Experiment settings

The hyperparameters used in the experiments are shown in Table 1. In terms of the experimental setup, the symbol library and equation assumptions used for mining ODE and PDE equations are slightly different, as shown in Table 2 The library used for mining PDEs has relatively fewer operators and more operands involved, and does not include the symbol "const". On the other hand, the library used for mining ODEs covers more mathematical operators and the constants are determined using nonlinear optimization methods, e.g., BFGS [50].

Table 2: Default experimental settings for discovering different systems.

| Nonlinear Systems | Operators | Operands | Constants optimization |
| :---: | :---: | :---: | :---: |
| ODE | $+,-, \times, \div, \wedge, \sin , \cos , \log , \exp$ | $x$, const | Nonlinear |
| PDE | $+,-, \times, \div, \wedge^{2}, \wedge^{3}$ | $u, x, u_{x}, u_{x x}, u_{x x x}, u_{x x x x}$ | Linear |

### 4.3 PDE discovery task

### 4.3.1 Equations and discovered results

In the experiments of PDE discovery, we testify the framework's ability to discover the governing equations of six canonical nonlinear systems, including Burges' equation, Chafee-Infante equation, PDE_divide equation with fractional

LLM4ED: Large Language Models for Automatic Equation Discovery

Table 3: Summary of several nonlinear systems governed by PDEs and discovered results. The subscripts $m, n$ denote the number of discretizations.

| PDE systems |  | Form | Coefficient error | Data discretization |
| :---: | :---: | :---: | :---: | :---: |
|  | Burgers | $u_{t}=-u u_{x}+0.1 u_{x x}$ | $1.25 \pm 1.63 \%$ | $x \in[-8,8)_{m=256}, t \in[0,10]_{n=201}$ |
|  | Chafee-Infante | $u_{t}=u_{x x}+u-u^{3}$ | $0.05 \pm 0.03 \%$ | $x \in[0,3]_{m=301}, t \in[0,0.5]_{n=200}$ |
|  | KS | $u_{t}=-u u_{x}-u_{x x}-u_{x x x x}$ | $0.5 \pm 0.2 \%$ | $x \in[-10,10]_{m=512}, t \in[0,20]_{n=256}$ |
|  | PDE_divide | $u_{t}=-u_{x} / x+0.25 u_{x x}$ | $0.15 \pm 0.09 \%$ | $x \in[1,2)_{m=100}, t \in[0,1]_{n=251}$ |
|  | Fisher-KPP | $u_{t}=0.02 u u_{x x}+0.02\left(u_{x}\right)^{2}+10 u-10 u^{2}$ | $1.34 \pm 0.38 \%$ | $x \in(-1,1)_{m=199}, t \in(0,1)_{n=99}$ |
|  | NS | $\omega_{t}=0.1 \omega_{x x}+0.1 \omega_{y y}-u \omega_{x}-v \omega_{y}$ | $0.15 \pm 0.09 \%$ | $x \in[0,6.5]_{m=325}, y \in[0,3.4]_{m y=325}$, <br> $t \in[0,30]_{n=150}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-09.jpg?height=634&width=1648&top_left_y=1426&top_left_x=238)

Figure 6: Discovered results under different optimization methods.

structure, Kuramoto-Sivashinsky equation with fourth-order derivatives, nonlinear Fisher-KPP equations with a square of the spatial derivative, and two-dimensional Navier-Stokes equation. With the default parameter configuration, our approach is capable of accurately identifying the correct structure of the equations while preserving a minimal coefficient error, as shown in Fig. 3 Notably, in comparison to fixed candidate set methods, our framework diminishes the dependence on prior knowledge, enabling the discovery of more complex equation forms, for example, equations with fractional or compound structures.

### 4.3.2 Compariosn of different optimization strategies

We further verified the effectiveness of the proposed LLM-guided iterative optimization. Three optimization methods are mainly discussed and compared: (1) only the self-improvement optimization method utilized; (2) only the genetic algorithm utilized; (3) the alternating iterative method of the above two methods (proposed in the framework). The identification experiments for the aforementioned equations were replicated ten times, with each experiment's maximum iteration count set to 50 iterations, to enable further examination of the performance of various methods. Fig. 6 illustrates that the iterative approach combining both methods yielded the highest frequency of discovering the correct equations, with recovery rates consistently surpassing $80 \%$, outperforming the outcomes achieved by employing a single optimization technique. Fig. 6.b) depicts the successful rate of the ultimately identified equations. It is noteworthy that despite the self-improvement method outperforming the genetic algorithm and exhibiting higher optimization efficiency in some systems, such as Burgers' equation, it is more prone to converging to local optima. When the iteration step is extended to 100 steps, the symbolic success rate of optimization employing the genetic algorithm approach surpasses $80 \%$ for all of the equations, demonstrating its superior global optimization capability, whereas self-improvement hardly achieves significant improvement.

We provide further detailed explanations with the Chafee-Infante, Burgers, and NS equations utilized as examples. Fig. 7 illustrates the evolution of the highest score up to the current iteration step. It reveals that the optimization efficiency of the alternating approach combining both methods is superior, facilitating faster identification of the correct equations in various equation discovery tasks. Utilizing the Chafee-Infante equation as an illustration, we further examine the density distribution of scores at each iteration throughout the optimization process. Fig. 8 illustrates that the self-improvement strategy exhibits a propensity for local modifications on historical elite equations, resulting in a score distribution resembling an incremental trend. Conversely, the GA method leans towards global searches, identifying equations with higher diversity, albeit at the cost of potentially compromised optimization efficiency. Employing alternating iterations of both methods proves to be more advantageous in striking a balance between exploration and exploitation.
![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-10.jpg?height=396&width=1592&top_left_y=1222&top_left_x=256)

Figure 7: Evolution of the best score with different methods while discovering the Chafee-Infante equation, Burgers' equation and NS equation.

### 4.4 ODE discovery task

In this section, we testified our framework on 16 one-dimensional ODEs in a comprehensive benchmark named ODE-bench [39], which has been utilized to describe real-world phenomena by Strogatz [53]. The equation information is listed in the Appendix $\mathrm{B}$ Each equation contains two sets of trajectories with two different initial conditions. We take one set of trajectory data as training data and search to find the optimal $\hat{\mathcal{F}}$. During the evaluation process, we consider the solution trajectory of the ODE equation associated with $\mathcal{F}$ as predicted results and utilize $R^{2}$ score as the evaluation criterion to measure the fitting accuracy in comparison to the actual trajectory. The value $R^{2}$ on the training set represents the accuracy of the reconstruction, while the value $R^{2}$ on the testing set with a new initial condition signifies the generalization performance. As shown in Table 4 , we repeated the search process for each equation three times and provided the best results among them. It can be seen that in our framework, the percentage of equations with $R^{2}$ greater than 0.99 on the training set is $93.75 \%$ (15/16), and on the test set, the percentage of equations with R2 greater than 0.99 is $68.75 \%$. Equations with $R^{2}$ greater than 0.9 exceeded $90 \%$ on both the testing and training sets. Fig. 9 presents the detailed prediction results for each equation.

We conducted a further comparison of the performance of different equation discovery methods and large models on all the presented ODEs. Three symbolic regression methods are utilized as the baseline models, including PySR [54] and ODEformer [39]. PySR is a practical and high-performance library for symbolic regression based on a multi-population evolutionary algorithm. PySR is aimed at single-instance datasets and has been broadly adopted for interpretable
![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-11.jpg?height=696&width=1614&top_left_y=240&top_left_x=251)

Figure 8: Distribution of scores at different iterations with different methods while discovering the Chafee-Infante equation.

Table 4: Discovered results and the reconstruction and generalization performance on the Strogatz dataset. We ran the experiments three times and presented the best expression for each ODE.

| Benchmark | Discovered form | Parameters | $R^{2}$ (train) | $R^{2}$ (test) |
| :---: | :---: | :---: | :---: | :---: |
| ODE-1 | $c_{0}+c_{1} x$ | $[-0.3608,0.3031]$ | 0.999 | 0.999 |
| ODE-2 | $c_{0} x^{2}+c_{1} x$ | $[-0.0106,0.7899]$ | 0.999 | 0.999 |
| ODE-3 | $c_{0} \sin x+c_{1} x^{2}+c_{2} e^{c_{3} x} \sin x+c_{4}$ | $[0.219,0.0563,0.0024,1.1,-0.1145]$ | 0.999 | 0.727 |
| ODE-4 | $c_{0} x^{2}+c_{1}$ | $[-0.0021,9.8098]$ | 0.999 | 0.999 |
| ODE-5 | $c_{0} x \log \left(c_{1} x\right)$ | $[0.032,2.2901]$ | 0.999 | 0.973 |
| ODE-6 | $c_{0} x^{3}+c_{1} x^{2}+c_{2} x$ | $[-0.00024,0.033,-0.1408]$ | 0.996 | 0.999 |
| ODE-7 | $c_{0} * x^{2}-c_{1} * x * \sin (x)^{2}-c_{2} * \sin (x) * \cos (x)$ | $[1.2539,-1.2231,-0.7192]$ | 0.999 | 0.999 |
| ODE-8 | $c_{0} x^{3}+c_{1}$ | $[-1.2554,0.0318]$ | 0.979 | 0.958 |
| ODE-9 | $c_{0} \sin (x)+c_{1} \sin (x) \cos (x)$ | $[-0.0981,0.9511]$ | 0.999 | 0.999 |
| ODE-10 | $c_{0} x^{5}+c_{1} x^{3}+c_{2} x$ | $[-0.0009,0.0399,0.1004]$ | 0.992 | 0.978 |
| ODE-11 | $c_{0} x^{2}+c_{1} x+c_{2}$ | $[-0.004,0.3976,-0.0293]$ | 0.999 | 0.999 |
| ODE-12 | $c_{0} \sin (x)^{2}+c_{1} x+c_{2} \cos (x)+c_{3}$ | $[0.464,0.907,2.7834,-2.7836]$ | 0.999 | 0.999 |
| ODE-13 | $c_{0} * e x p\left(c_{1} * x\right)-c_{2} * \sin (x) / x+c_{3}$ | $[-0.2779,2.0,9.7688,10.1468]$ | 0.999 | 0.999 |
| ODE-14 | $c_{0}-c_{1} x-c_{2} e^{-x}$ | $[1.1998,0.2,-0.9998]$ | 0.999 | 0.999 |
| ODE-15 | $c_{0} x^{2}+c_{1} x+c_{2} \sin (x)+c_{3}$ | $[-0.1682,-0.2768,-0.5337,1.4144]$ | 0.999 | 0.977 |
| ODE-16 | $c_{0}-c_{1} \sin (x)$ | $[0.21,-0.9995]$ | 0.999 | 0.999 |

symbolic discovery. ODEformer is based on pretrained transformers and achieves SOTA on ODEbench's datasets. The above two methods are experimented with according to their default hyperparameter configurations.

As illustrated in Table 4, our framework demonstrates equivalent reconstruction performance to the above methods designed for symbolic regression, while exhibiting superior generalization capabilities and usability. Note that PySR, which is based on evolutionary search, possesses powerful search capabilities and can discover equations that are numerically accurate on the training dataset. However, these discovered equations tend to have higher complexity and consequently are susceptible to exhibiting poor generalization performance. Additionally, we performed a comparative analysis of the performance of large models with varying parameter sizes and language capacities on this task. We take the open-source large language model Llama2 with 7B parameters [55] and more advanced models, including GPT3.5-turbo and GPT-4. The results indicate that as the capability of the large model improves, the identified equations become relatively more accurate both on the training set and testing set. This can be primarily attributed to the fact that the capacity of the large model directly exerts a substantial influence on the generation and optimization of equations. On one hand, large models with inferior capabilities may struggle to effectively understand and execute the provided instructions, such as the constraints we defined, leading to the generation of numerous invalid equations. Furthermore,
![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-12.jpg?height=1720&width=1632&top_left_y=240&top_left_x=256)

$\square \times 1 \square \times 2-\times 1 \_$sol —_ x2_sol

Figure 9: Predicted solution trajectories on training set and testing set.

they tend to fail to properly execute GA instructions and fail to accurately perform crossover and mutation operations. Conversely, the model's reasoning capacity directly influences its self-improvement optimization capabilities. It is also worth noting that as the capabilities and number of parameters of large models further increase, the gains in accuracy are diminishing, especially on test sets.

In addition, we need to further emphasize the efficiency of the proposed framework. The total running time is the product of the time per iteration and the total number of iterations. The time cost per iteration ranges from 10s to 40s under the default configuration and mainly includes the time spent on remotely accessing the LLM API and evaluating the feedback of LLM, i.e., the generated equations. The time consumed for accessing the API interface exhibits a positive correlation with the number of samples generated and is roughly order of magnitude larger than the time for evaluations. In practice, we can allocate separate processes to query the LLM in parallel. On one hand, this

Table 5: Evaluation on 16 ODEs with different methods and LLMs. We counted the number of equations discovered by different methods and LLMs that meet the corresponding $R^{2}$ criteria. When the $R^{2}$ value is negative, or the solution trajectories of the ODE associated with $\hat{\mathcal{F}}$ exhibit numerical overflow, we deem the discovered equation as "Invalid".

| Methods |  |  | Training set |  |  | Testing set |  | Symbolically correc |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | $R^{2}>0.99$ | $R^{2}>0.9$ | Invaid | $R^{2}>0.99$ | $R^{2}>0.9$ | Invaid |  |
| {PYSR <br> ODEformer} |  | $15(93.75 \%)$ | $16(100 \%)$ | $1(6.25 \%)$ | $10(62.5 \%)$ | $12(75 \%)$ | $4(25 \%)$ | $6(37.5 \%)$ |
|  |  | $12(75 \%)$ | 15 (93.75\%) | $3(6.25 \%)$ | $6(37.5 \%)$ | $10(62.5 \%)$ | $5(31.25 \%)$ | $3(18.75 \%)$ |
| Ours | Llama2 7B | $11(68.75 \%)$ | $12(75 \%)$ | $4(25 \%)$ | $8(50 \%)$ | $11(68.75 \%)$ | $4(25 \%)$ | $5(31.25 \%)$ |
|  | GPT-3.5-turbo | 13 (81.25\%) | $14(87.5 \%)$ | $3(18.75 \%)$ | $12(75 \%)$ | 13 (81.25\%) | $3(18.75 \%)$ | $8(50 \%)$ |
|  | GPT-4 | 15 (93.75\%) | $16(100 \%)$ | $0(\mathbf{0 \%})$ | 11 (68.75\%) | $15(93.75 \%)$ | $1(6.25 \%)$ | $8(50 \%)$ |

approach can reduce the total response time. On the other hand, we can increase the number of expressions generated in each iteration, which in turn contributes to more accurate samples for optimization and helps reduce the number of optimization iterations.

## 5 Conclusion

We introduce a novel equation discovery framework guided by LLMs. It is aimed at facilitating equation discovery across diverse domains, transcending the confines of specialist communities, and making LLM-guided discovery accessible to a broader range of users. The framework leverages the generation and reasoning capabilities of large models to automatically complete the generation and optimization of equations. We employ natural language-based prompts to guide LLM in conducting iterative optimization of self-improvement and genetic algorithms. The results indicate that the two exhibit a strong synergistic effect, effectively striking a balance between exploration and exploitation. In the one-dimensional ODEs experiments, our framework achieved nearly equivalent performance to the state-of-the-art and compared the impact of large model capabilities on the performance of mined equations. In the future, we can further improve the existing framework in two aspects. One aspect is to further explore the design of natural language-based prompts, combining prior knowledge to better narrow down the search space and improve search efficiency and equation mining accuracy. On the other hand, further combining more effective evaluation methods to solve more complex scenarios, for example, when observations are sparse and noisy.

## Code and data availability

The implementation details of the whole process and relevant data are available on GitHub at https://github.com/menggedu/EDL.

## A Prompts

The prompts utilized for initialization, self-improvement, and GA are shown in Fig. 10, Fig. 11, and Fig. 12, respectively.

## B ODE datasets

## References

[1] Kyongmin Yeo and Igor Melnyk. Deep learning algorithm for data-driven simulation of noisy dynamical system. Journal of Computational Physics, 376:1212-1231, 2019.

[2] Jiaqing Kou and Weiwei Zhang. Data-driven modeling for unsteady aerodynamics and aeroelasticity. Progress in Aerospace Sciences, 125:100725, 2021.

[3] Gang Zheng, Xiaofeng Li, Rong-Hua Zhang, and Bin Liu. Purely satellite data-driven deep learning forecast of complicated tropical instability waves. Science advances, 6(29):eaba1482, 2020.

[4] Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo, Gianluigi Rozza, Maziar Raissi, and Francesco Piccialli. Scientific machine learning through physics-informed neural networks: Where we are and what's next. Journal of Scientific Computing, 92(3):88, 2022.

[5] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physicsinformed machine learning. Nature Reviews Physics, 3(6):422-440, 2021.

![](https://cdn.mathpix.com/cropped/2024_06_04_d8e481515e4162165c96g-14.jpg?height=941&width=1114&top_left_y=248&top_left_x=495)

Figure 10: Prompt utilized in initialization.

[6] Lu Lu, Raphael Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G Johnson. Physicsinformed neural networks with hard constraints for inverse design. SIAM Journal on Scientific Computing, 43(6):B1105-B1132, 2021.

[7] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science, 324(5923):8185,2009 .

[8] Kathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences, 116(45):22445-22451, 2019.

[9] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proc. Natl. Acad. Sci., 113(15):3932-3937, 2016.

[10] Daniel E Shea, Steven L Brunton, and J Nathan Kutz. Sindy-bvp: Sparse identification of nonlinear dynamics for boundary value problems. Phys. Rev. Res., 3(2):023255, 2021.

[11] Daniel A Messenger and David M Bortz. Weak sindy for partial differential equations. J. Comput. Phys., 443:110525, 2021.

[12] Kadierdan Kaheman, J Nathan Kutz, and Steven L Brunton. Sindy-pi: a robust algorithm for parallel implicit sparse identification of nonlinear dynamics. Proceedings of the Royal Society A, 476(2242):20200279, 2020.

[13] Urban Fasel, J Nathan Kutz, Bingni W Brunton, and Steven L Brunton. Ensemble-sindy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control. Proceedings of the Royal Society A, 478(2260):20210904, 2022.

[14] Georg Martius and Christoph H Lampert. Extrapolation and learning equations. arXiv preprint arXiv:1610.02995, 2016.

[15] Subham Sahoo, Christoph Lampert, and Georg Martius. Learning equations for extrapolation and control. In International Conference on Machine Learning, pages 4442-4450. PMLR, 2018.

[16] Pierre-Alexandre Kamienny, Guillaume Lample, Sylvain Lamprier, and Marco Virgolin. Deep generative symbolic regression with monte-carlo-tree-search. In International Conference on Machine Learning, pages 15655-15668. PMLR, 2023.

[17] Mojtaba Valipour, Bowen You, Maysum Panju, and Ali Ghodsi. Symbolicgpt: A generative transformer model for symbolic regression. arXiv preprint arXiv:2106.14131, 2021.
