# Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds 

## Clément Bonet

ENSAE, CREST, Institut Polytechnique de Paris

Lucas Drumetz<br>IMT Atlantique, Lab-STICC<br>Nicolas Courty<br>Université Bretagne Sud, Irisa Hadamard manifolds, Wasserstein Gradient Flows

## 1 Introduction

LUCAS.DRUMETZ@IMT-ATLANTIQUE.FR

NICOLAS.COURTY@IRISA.FR


#### Abstract

While many Machine Learning methods were developed or transposed on Riemannian manifolds to tackle data with known non Euclidean geometry, Optimal Transport (OT) methods on such spaces have not received much attention. The main OT tool on these spaces is the Wasserstein distance which suffers from a heavy computational burden. On Euclidean spaces, a popular alternative is the Sliced-Wasserstein distance, which leverages a closed-form solution of the Wasserstein distance in one dimension, but which is not readily available on manifolds. In this work, we derive general constructions of Sliced-Wasserstein distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive curvature, which include among others Hyperbolic spaces or the space of Symmetric Positive Definite matrices. Then, we propose different applications. Additionally, we derive nonparametric schemes to minimize these new distances by approximating their Wasserstein gradient flows.


Keywords: Optimal Transport, Sliced-Wasserstein, Riemannian Manifolds, Cartan-

It is widely accepted that data have an underlying structure on a low dimensional manifold (Bengio et al., 2013). However, it can be intricate to work directly on such data manifolds as we lack from an analytical model. Therefore, most works only focus on Euclidean space and do not take advantage of this representation. In some cases though, the data naturally and explicitly lies on a manifold, or can be embedded on some known manifolds allowing to leverage their intrinsic structure. In such cases, it has been shown to be beneficial to exploit such structure by leveraging the metric of the manifold rather than relying on an Euclidean embedding. To name a few examples, directional or geophysical data - data for which only the direction provides information - naturally lie on the sphere (Mardia et al., 2000) and hence their structure can be exploited by using methods suited to the sphere. Another popular example is given by data having a known hierarchical structure. Then, such data benefit from being embedded into hyperbolic spaces (Nickel and Kiela, 2017).

Motivated by these examples, many works proposed new tools to handle data lying on Riemannian manifolds. To cite a few, Fletcher et al. (2004); Huckemann and Ziezold (2006) developed PCA to perform dimension reduction on manifolds while Le Brigant and Puechmorel (2019) studied density approximation, Feragen et al. (2015); Jayasumana et al.

(2015); Fang et al. (2021) studied kernel methods and Azangulov et al. (2022, 2023) developed Gaussian processes on (homogeneous) manifolds. More recently, there has been many interests into developing new neural networks with architectures taking into account the geometry of the ambient manifold (Bronstein et al., 2017) such as Residual Neural Networks (Katsman et al., 2022), discrete Normalizing Flows (Bose et al., 2020; Rezende et al., 2020; Rezende and Racanière, 2021) or Continuous Normalizing Flows (Mathieu and Nickel, 2020; Lou et al., 2020; Rozen et al., 2021; Yataka and Shiraishi, 2022). In the generative model literature, we can also cite the recent (Chen and Lipman, 2023) which extended the flow matching training of Continuous Normalizing Flows to Riemannian manifolds, or Bortoli et al. (2022); Huang et al. (2022) who performed score based generative modeling and Thornton et al. (2022) who studied Schrödinger bridges on manifolds.

To compare probability distributions or perform generative modeling tasks, one usually needs suitable discrepancies or distances. In Machine Learning, classical divergences used are for example the Kullback-Leibler divergence or the Maximum Mean Discrepancy (MMD). While these distances are well defined for distributions lying on Riemannian manifolds, generally by crafting dedicated kernels for the MMD (Feragen et al., 2015), other choices which take more into account the geometry of the underlying space are Optimal Transport based distances whose most prominent example is the Wasserstein distance.

While the Wasserstein distance is well defined on Riemannian manifolds, and has been studied in many works theoretically, see e.g. (McCann, 2001; Villani et al., 2009), it suffers from a significant computational burden. In the Euclidean case, different solutions were proposed to alleviate this computational cost, such as adding an entropic regularization and leveraging the Sinkhorn algorithm (Cuturi, 2013), approximating the Wasserstein distance using minibatchs (Fatras et al., 2020), using low-rank couplings (Scetbon and Cuturi, 2022) or tree metrics (Le et al., 2019). These approximations can be readily extended to Riemannian manifolds using the right ground costs. For example, Alvarez-Melis et al. (2020); Hoyos-Idrobo (2020) used the entropic regularized formulation on Hyperbolic spaces. Another popular alternative to the Wasserstein distance is the so-called Sliced-Wasserstein distance (SW). While on Euclidean spaces, the Sliced-Wasserstein distance is a tractable alternative allowing to work in large scale settings, it cannot be directly extended to Riemannian manifolds since it relies on orthogonal projections of the measures on straight lines. Hence, deriving new SW based distance on manifolds could be of much interest. This question has led to several works in this direction, first on compact manifolds in (Rustamov and Majumdar, 2023) and then in (Bonet et al., 2023c; Quellmalz et al., 2023) on the sphere. Here, we focus on the particular case of Cartan-Hadamard manifolds which encompass in particular Euclidean spaces, Hyperbolic spaces (Bonet et al., 2023a) or Symmetric Positive Definite matrices endowed with appropriate metrics (Bonet et al., 2023b).

Contributions. In this article, we start in Section 2 by providing some background on Optimal Transport and on Riemannian manifolds. Then, in Section 3, we introduce different ways to construct intrinsically Sliced-Wasserstein discrepancies on geodesically complete Riemannian manifolds with non-positive curvature (Cartan-Hadamard manifolds) by either using geodesic projections or horospherical projections. In Section 4, we specify the framework to different Cartan-Hadamard manifolds, including manifolds endowed with a pullback Euclidean metric, Hyperbolic spaces, Symmetric positive Definite matrices with specific metrics and product of Cartan-Hadamard manifolds. Then, in Section 5, we derive some
theoretical properties common to any sliced discrepancy on these Riemannian manifolds, as well as properties specific to the pullback Euclidean case. We also propose in Section 6 illustrations for the Sliced-Wasserstein distance on the Euclidean space endowed with the Mahalanobis distance on a document classification task, and of the Sliced-Wasserstein distance on product manifolds for comparing datasets represented on the product space of the samples and of the labels. Finally, we propose in Section 7 non-parametric schemes to minimize these different distances using Wasserstein gradient flows, and hence allowing to derive new sampling algorithms on manifolds ${ }^{1}$.

## 2 Background

In this section, we first introduce background on Optimal Transport through the Wasserstein distance and the Sliced-Wasserstein distance on Euclidean spaces. Then, we introduce general Riemannian manifolds. For more details about Optimal Transport, we refer to (Villani et al., 2009; Santambrogio, 2015; Peyré et al., 2019). And for more details about Riemannian manifolds, we refer to (Gallot et al., 1990; Lee, 2006; Lee and Lee, 2012).

### 2.1 Optimal Transport on Euclidean Spaces

Wasserstein Distance. Optimal transport provides a principled way to compare probability distributions through the Wasserstein distance. Let $p \geq 1$ and $\mu, \nu \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)=\{\mu \in$ $\left.\mathcal{P}(\mathbb{R}), \int\|x\|_{2}^{p} \mathrm{~d} \mu(x)<\infty\right\}$ two probability distributions with $p$ finite moments. Then, the Wasserstein distance is defined as

$$
\begin{equation*}
W_{p}^{p}(\mu, \nu)=\inf _{\gamma \in \Pi(\mu, \nu)} \int\|x-y\|_{2}^{p} \mathrm{~d} \gamma(x, y) \tag{1}
\end{equation*}
$$

where $\Pi(\mu, \nu)=\left\{\gamma \in \mathcal{P}\left(\mathbb{R}^{d} \times \mathbb{R}^{d}\right), \pi_{\#}^{1} \gamma=\mu, \pi_{\#}^{2} \gamma=\nu\right\}$ denotes the set of couplings between $\mu$ and $\nu, \pi^{1}(x, y)=x, \pi^{2}(x, y)=y$ and \# is the push-forward operator, defined as $T_{\#} \mu(A)=\mu\left(T^{-1}(A)\right)$ for any Borel set $A \subset \mathbb{R}^{d}$ and map $T: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$.

For discrete probability distributions with $n$ samples, e.g. for $\mu=\frac{1}{n} \sum_{i=1}^{n} \delta_{x_{i}}$ and $\nu=\frac{1}{n} \sum_{j=1}^{n} \delta_{y_{j}}$ with $x_{1}, \ldots, x_{n}, y_{1}, \ldots, y_{n} \in \mathbb{R}^{d}$, computing $W_{p}^{p}$ requires to solve a linear program, which has a $O\left(n^{3} \log n\right)$ worst case complexity (Pele and Werman, 2009). Thus, it becomes intractable in large scale settings.

For unknown probability distributions $\mu$ and $\nu$ from which we have access to samples $x_{1}, \ldots, x_{n} \sim \mu$ and $y_{1}, \ldots, y_{n} \sim \nu$, a common practice to estimate $W_{p}^{p}(\mu, \nu)$ is to compute the plug-in estimator

$$
\begin{equation*}
\widehat{W}_{p}^{p}(\mu, \nu)=W_{p}^{p}\left(\frac{1}{n} \sum_{i=1}^{n} \delta_{x_{i}}, \frac{1}{n} \sum_{i=1}^{n} \delta_{y_{i}}\right) \tag{2}
\end{equation*}
$$

However, the approximation error, known as the sample complexity, is quantified in $O\left(n^{-\frac{1}{d}}\right)$ (Boissard and Le Gouic, 2014). Thus, the estimation of the Wasserstein distance from samples degrades in higher dimension if we use the same number of samples, or becomes very heavy to compute if we use enough samples to have the same approximation error.[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-04.jpg?height=438&width=1416&top_left_y=344&top_left_x=342)

Figure 1: (Left) Orthogonal projection of points on a line passing through the origin 0. (Middle and Right) Illustration of the projection of $2 \mathrm{~d}$ distributions on 3 different lines.

To alleviate the computational burden and the curse of dimensionality, different variants were proposed. We focus in this work on the Sliced-Wasserstein distance.

Sliced-Wasserstein Distance. For $\mu, \nu \in \mathcal{P}_{p}(\mathbb{R})$, it is well known that the Wasserstein distance can be computed in closed-form (Peyré et al., 2019, Remark 2.30). More precisely, let $\mu, \nu \in \mathcal{P}_{p}(\mathbb{R})$, then

$$
\begin{equation*}
W_{p}^{p}(\mu, \nu)=\int_{0}^{1}\left|F_{\mu}^{-1}(u)-F_{\nu}^{-1}(u)\right|^{p} \mathrm{~d} u \tag{3}
\end{equation*}
$$

where $F_{\mu}^{-1}$ and $F_{\nu}^{-1}$ denote the quantile functions of $\mu$ and $\nu$. For discrete distributions with $n$ samples, quantiles can be computed in $O(n \log n)$ since they only require sorting the samples. Thus, for $x_{1}<\cdots<x_{n}, y_{1}<\cdots<y_{n}, \mu_{n}=\frac{1}{n} \sum_{i=1}^{n} \delta_{x_{i}}$ and $\nu_{n}=\frac{1}{n} \sum_{i=1}^{n} \delta_{y_{i}}$,

$$
\begin{equation*}
W_{p}^{p}\left(\mu_{n}, \nu_{n}\right)=\frac{1}{n} \sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{p} \tag{4}
\end{equation*}
$$

Motivated by this closed-form, Rabin et al. (2012) introduced the Sliced-Wasserstein distance, which is defined by first projecting linearly the distributions on every possible direction, and then by taking the average of the one dimensional Wasserstein distances on each line. More precisely, for a direction $\theta \in S^{d-1}$, the coordinate of the orthogonal projection of $x \in \mathbb{R}^{d}$ on the line $\operatorname{span}(\theta)$ is defined by $P^{\theta}(x)=\langle x, \theta\rangle$. Then, by denoting by $\lambda$ the uniform measure on the sphere $S^{d-1}=\left\{\theta \in \mathbb{R}^{d},\|\theta\|_{2}=1\right\}$, the $p$-Sliced-Wasserstein distance between $\mu, \nu \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$ is defined as

$$
\begin{equation*}
\mathrm{SW}_{p}^{p}(\mu, \nu)=\int_{S^{d-1}} W_{p}^{p}\left(P_{\#}^{\theta} \mu, P_{\#}^{\theta} \nu\right) \mathrm{d} \lambda(\theta) \tag{5}
\end{equation*}
$$

The projection process is illustrated in Figure 1.

Since the outer integral is intractable, a common practice to estimate this integral is to rely on a Monte-Carlo approximation by first sampling $L$ directions $\theta_{1}, \ldots, \theta_{L}$ and then taking the average of the $L$ Wasserstein distances:

$$
\begin{equation*}
\widehat{\mathrm{SW}}_{p}^{p}(\mu, \nu)=\frac{1}{L} \sum_{\ell=1}^{L} W_{p}^{p}\left(P_{\#}^{\theta_{\ell}} \mu, P_{\#}^{\theta_{\ell}} \nu\right) \tag{6}
\end{equation*}
$$

Thus, approximating SW requires to compute $L$ Wasserstein distances, and $L n$ projections, and thus the computational complexity is in $O(\operatorname{Ln}(\log n+d))$. Note that other integral approximations have been recently proposed. For example, Nguyen et al. (2023) proposed to use quasi Monte-Carlo samples, and Nguyen and Ho (2023a); Leluc et al. (2023, 2024) used control variates to reduce the variance of the approximation.

We are now interested in transposing this method to Riemannian manifolds, for which we give a short introduction in the following section.

### 2.2 Riemannian Manifolds

Definition. A Riemannian manifold $(\mathcal{M}, g)$ of dimension $d$ is a space that behaves locally as a linear space diffeomorphic to $\mathbb{R}^{d}$, called a tangent space. To any $x \in \mathcal{M}$, one can associate a tangent space $T_{x} \mathcal{M}$ endowed with an inner product $\langle\cdot, \cdot\rangle_{x}: T_{x} \mathcal{M} \times T_{x} \mathcal{M} \rightarrow \mathbb{R}$ which varies smoothly with $x$. This inner product is defined by the metric $g_{x}$ associated to the Riemannian manifold as $g_{x}(u, v)=\langle u, v\rangle_{x}$ for any $x \in \mathcal{M}, u, v \in T_{x} \mathcal{M}$. We note $G(x)$ the matrix representation of $g_{x}$ defined such that

$$
\begin{equation*}
\forall u, v \in T_{x} \mathcal{M},\langle u, v\rangle_{x}=g_{x}(u, v)=u^{T} G(x) v \tag{7}
\end{equation*}
$$

For some spaces, different metrics can give very different geometries. We call tangent bundle the disjoint union of all tangent spaces $T \mathcal{M}=\left\{(x, v), x \in \mathcal{M}\right.$ and $\left.v \in T_{x} \mathcal{M}\right\}$, and we call a vector field a map $V: \mathcal{M} \rightarrow T \mathcal{M}$ such that $V(x) \in T_{x} \mathcal{M}$ for all $x \in \mathcal{M}$.

Geodesics. A generalization of straight lines in Euclidean spaces to Riemannian manifolds can be geodesics, which are smooth curves connecting two points $x, y \in \mathcal{M}$ with minimal length, i.e. curves $\gamma:[0,1] \rightarrow \mathcal{M}$ such that $\gamma(0)=x, \gamma(1)=y$, and which minimize the length $\mathcal{L}$ defined as

$$
\begin{equation*}
\mathcal{L}(\gamma)=\int_{0}^{1}\left\|\gamma^{\prime}(t)\right\|_{\gamma(t)} \mathrm{d} t \tag{8}
\end{equation*}
$$

where $\left\|\gamma^{\prime}(t)\right\|_{\gamma(t)}=\sqrt{\left\langle\gamma^{\prime}(t), \gamma^{\prime}(t)\right\rangle_{\gamma(t)}}$. In this work, we will focus on geodesically complete Riemannian manifolds, in which case there is always a geodesic between two points $x, y \in$ $\mathcal{M}$. Furthermore, in this specific case, all geodesics are actually geodesic lines, i.e. they can be extended to $\mathbb{R}$. Let $x, y \in \mathcal{M}, \gamma:[0,1] \rightarrow \mathcal{M}$ a geodesic between $x$ and $y$ such that $\gamma(0)=x$ and $\gamma(1)=y$, then the value of the length defines actually a distance $(x, y) \mapsto d(x, y)$ between $x$ and $y$, which we call the geodesic distance:

$$
\begin{equation*}
d(x, y)=\inf _{\gamma, \gamma(0)=x, \gamma(1)=y} \mathcal{L}(\gamma) \tag{9}
\end{equation*}
$$

Exponential Map. Let $x \in \mathcal{M}$, then for any $v \in T_{x} \mathcal{M}$, there exists a unique geodesic $\gamma_{(x, v)}$ starting from $x$ with velocity $v$, i.e. such that $\gamma_{(x, v)}(0)=x$ and $\gamma_{(x, v)}^{\prime}(0)=v$ (Sommer et al., 2020). Now, we can define the exponential map as $\exp : T \mathcal{M} \rightarrow \mathcal{M}$ which for any $x \in \mathcal{M}$, maps tangent vectors $v \in T_{x} \mathcal{M}$ back to the manifold at the point reached by the geodesic $\gamma_{(x, v)}$ at time $t=1$ :

$$
\begin{equation*}
\forall(x, v) \in T \mathcal{M}, \exp _{x}(v)=\gamma_{(x, v)}(1) \tag{10}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-06.jpg?height=404&width=765&top_left_y=310&top_left_x=672)

Figure 2: Illustration of geodesics, of the tangent space and the exponential map on a Riemannian manifold.

On geodesically complete manifolds, the exponential map is defined on the entire tangent space, but is not necessarily a bijection. When it is the case, we note $\log _{x}$ the inverse of $\exp _{x}$, which allows mapping elements from the manifold to the tangent space. We illustrate these different notions on Figure 2.

Let $f: \mathcal{M} \rightarrow \mathbb{R}$ be a differentiable map. We now define its Riemannian gradient, which is notably very important in order to generalize first-order optimization algorithms to Riemannian manifolds (Bonnabel, 2013; Boumal, 2023).

Definition 1 (Gradient) We define the Riemannian gradient of $f$ as the unique vector field $\operatorname{grad}_{\mathcal{M}} f: \mathcal{M} \rightarrow T \mathcal{M}$ satisfying

$$
\begin{equation*}
\forall(x, v) \in T \mathcal{M},\left.\frac{\mathrm{d}}{\mathrm{d} t} f\left(\exp _{x}(t v)\right)\right|_{t=0}=\left\langle v, \operatorname{grad}_{\mathcal{M}} f(x)\right\rangle_{x} \tag{11}
\end{equation*}
$$

Sectional Curvature. A notion which allows studying the geometry as well as the topology of a given Riemannian manifold is the sectional curvature. Let $x \in \mathcal{M}$, and $u, v \in T_{x} \mathcal{M}$ two linearly independent vectors. Then, the sectional curvature $\kappa_{x}(u, v)$ is defined geometrically as the Gaussian curvature of the plane $E=\operatorname{span}(u, v)$ (Zhang et al., 2016), i.e.

$$
\begin{equation*}
\kappa_{x}(u, v)=\frac{\langle R(u, v) u, v\rangle_{x}}{\langle u, u\rangle_{x}\langle v, v\rangle_{x}-\langle u, v\rangle_{x}^{2}}, \tag{12}
\end{equation*}
$$

where $R$ is the Riemannian curvature tensor. We refer to (Lee, 2006) for more details. The behavior of geodesics changes given the curvature of the manifold. For instance, they usually diverge on manifolds of negative sectional curvature and converge on manifolds of positive sectional curvature (Hu et al., 2023). Important examples of Riemannian manifolds are Euclidean spaces which are of constant null curvature, the sphere which is of positive constant curvature and Hyperbolic spaces which are of negative constant curvature (i.e. have the same value at any point $x \in \mathcal{M}$ and for any 2 -planes $E$ ) with their standard metrics. We can also cite the torus endowed with the ambient metric which has some points of positive curvature, some points of negative curvature and some points of null curvature (de Ocáriz Borde et al., 2023b). In this paper, we will mostly focus on CartanHadamard manifolds which are complete connected Riemannian manifolds of non-positive sectional curvature.

### 2.3 Probability Distributions on Riemannian Manifolds

Probability Distributions. Let $(\mathcal{M}, g)$ be a Riemannian manifold. For $x \in \mathcal{M}, G(x)$ induces an infinitesimal change of volume on the tangent space $T_{x} \mathcal{M}$, and thus a measure on the manifold,

$$
\begin{equation*}
\mathrm{dVol}(x)=\sqrt{|G(x)|} \mathrm{d} x \tag{13}
\end{equation*}
$$

Here, we denote by $\mathrm{d} x$ the Lebesgue measure. We refer to (Pennec, 2006) for more details on distributions on manifolds.

Particularly interesting examples of probability distributions are wrapped distributions (Chevallier and Guigui, 2020; Chevallier et al., 2022; Galaz-Garcia et al., 2022), which are defined as the push-forward of a distribution $\mu \in \mathcal{P}\left(T_{x} \mathcal{M}\right)$ onto $\mathcal{P}(\mathcal{M})$ using e.g. the exponential map when it is invertible over the whole tangent space. As it gives a very convenient way to sample on manifolds, this has received much attention notably on hyperbolic spaces with the wrapped normal distribution (Nagano et al., 2019; Cho et al., 2022b), for which the distribution in the tangent space is a Gaussian, and for which all transformations are differentiable, and can be used e.g. for variational autoencoders since they are amenable to the reparametrization trick.

Optimal Transport. Optimal Transport is also well defined on Riemannian manifolds using appropriate ground costs into the Kantorovich problem. Using the geodesic distance at the power $p \geq 1$, we recover the $p$-Wasserstein distance (McCann, 2001; Villani et al., 2009)

$$
\begin{equation*}
W_{p}^{p}(\mu, \nu)=\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathcal{M} \times \mathcal{M}} d(x, y)^{p} \mathrm{~d} \gamma(x, y) \tag{14}
\end{equation*}
$$

where $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})=\left\{\mu \in \mathcal{P}(\mathcal{M}), \int_{\mathcal{M}} d(x, o)^{p} \mathrm{~d} \mu(x)<\infty\right\}$, with $o \in \mathcal{M}$ some origin which can be arbitrarily chosen (because of the triangular inequality).

## 3 Riemannian Sliced-Wasserstein

In this section, we propose natural generalizations of the Sliced-Wasserstein distance on probability distributions supported on Riemannian manifolds by using tools intrinsically defined on them. To do that, we will first consider the Euclidean space as a Riemannian manifold. Doing so, we will be able to generalize it naturally to Riemannian manifolds of non-positive curvature. The proofs of this section are postponed to Appendix B.

### 3.1 Euclidean Sliced-Wasserstein as a Riemannian Sliced-Wasserstein Distance

It is well known that the Euclidean space can be viewed as a Riemannian manifold of null constant curvature (Lee, 2006). From that point of view, we can translate the elements used to build the Sliced-Wasserstein distance as Riemannian elements, and identify how to generalize it to more general Riemannian manifolds.

First, let us recall that the $p$-Sliced-Wasserstein distance for $p \geq 1$ between $\mu, \nu \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$ is defined as

$$
\begin{equation*}
\operatorname{SW}_{p}^{p}(\mu, \nu)=\int_{S^{d-1}} W_{p}^{p}\left(P_{\#}^{\theta} \mu, P_{\#}^{\theta} \nu\right) \mathrm{d} \lambda(\theta) \tag{15}
\end{equation*}
$$

where $P^{\theta}(x)=\langle x, \theta\rangle$ and $\lambda$ is the uniform distribution $S^{d-1}$. Geometrically, it amounts to projecting the distributions on every possible line going through the origin 0 . Hence, we
see that we need first to generalize lines passing through the origin. Then, we need to find suitable projections on these subsets. Finally, we need to make sure that we are still able to compute the Wasserstein distance efficiently between distributions supported on these subsets, in order to still have a computational advantage over solving the linear program.

Lines. From a Riemannian manifold point of view, straight lines can be seen as geodesics, which are, as we saw in Section 2.2, curves minimizing the distance between any two points on it. For any direction $\theta \in S^{d-1}$, the geodesic passing through 0 in direction $\theta$ is described by the curve $\gamma_{\theta}: \mathbb{R} \rightarrow \mathbb{R}^{d}$ defined as $\gamma_{\theta}(t)=t \theta=\exp _{0}(t \theta)$ for any $t \in \mathbb{R}$, and the corresponding geodesic is $\mathcal{G}^{\theta}=\operatorname{span}(\theta)$. Hence, when it makes sense, a natural generalization to straight lines would be to project on geodesics passing through an origin.

Projections. The projection $P^{\theta}(x)$ of $x \in \mathbb{R}^{d}$ can be seen as the coordinate of the orthogonal projection on the geodesic $\mathcal{G}^{\theta}$. Indeed, the orthogonal projection $\tilde{P}$ is formally defined as

$$
\begin{equation*}
\tilde{P}^{\theta}(x)=\underset{y \in \mathcal{G}^{\theta}}{\operatorname{argmin}}\|x-y\|_{2}=\langle x, \theta\rangle \theta \tag{16}
\end{equation*}
$$

From this formulation, we see that $\tilde{P}^{\theta}$ is a metric projection, which can also be called a geodesic projection on Riemannian manifolds as the metric is a geodesic distance. Then, we see that its coordinate on $\mathcal{G}^{\theta}$ is $t=\langle x, \theta\rangle=P^{\theta}(x)$, which can be also obtained by first giving a direction to the geodesic, and then computing the distance between $\tilde{P}^{\theta}(x)$ and the origin 0 , as

$$
\begin{equation*}
P^{\theta}(x)=\operatorname{sign}(\langle x, \theta\rangle)\|\langle x, \theta\rangle \theta-0\|_{2}=\langle x, \theta\rangle \tag{17}
\end{equation*}
$$

Note that this can also be recovered by solving

$$
\begin{equation*}
P^{\theta}(x)=\underset{t \in \mathbb{R}}{\operatorname{argmin}}\left\|\exp _{0}(t \theta)-x\right\|_{2} \tag{18}
\end{equation*}
$$

This formulation will be useful to generalize it to more general manifolds by replacing the Euclidean distance by the right geodesic distance.

Note also that the geodesic projection can be seen as a projection along hyperplanes, i.e. the level sets of the projection function $g(x, \theta)=\langle x, \theta\rangle$ are (affine) hyperplanes. This observation will come useful in generalizing $\mathrm{SW}$ to manifolds of non-positive curvature.

Wasserstein Distance. The Wasserstein distance between measures lying on the real line has a closed-form which can be computed very easily (see Section 2.1). On more general Riemannian manifolds, as the geodesics will not necessarily be lines, we will need to check how to compute the Wasserstein distance between the projected measures.

### 3.2 On Manifolds of Non-Positive Curvature

In this part, we focus on complete connected Riemannian manifolds of non-positive curvature, which can also be called Hadamard manifolds or Cartan-Hadamard manifolds (Lee, 2006; Robbin and Salamon, 2011; Lang, 2012). These spaces actually include Euclidean spaces, but also spaces with constant negative curvature such as Hyperbolic spaces, or with variable non-positive curvatures such as the space of Symmetric Positive Definite matrices and product of manifolds with constant negative curvature (Gu et al., 2019, Lemma 1). We refer to (Ballmann et al., 2006) or (Bridson and Haefliger, 2013) for more details. These spaces share many properties with Euclidean spaces (Bertrand and Kloeckner, 2012)
which make it possible to extend the Sliced-Wasserstein distance on them. We will denote $(\mathcal{M}, g)$ a Hadamard manifold in the following. Particular cases such as Hyperbolic spaces or the space of Symmetric Positive Definite matrices among other will be further studied in Section 4.

Properties of Hadamard Manifolds. First, as a Hadamard manifold is a complete connected Riemannian manifold, then by the Hopf-Rinow theorem (Lee, 2006, Theorem 6.13), it is also geodesically complete. Therefore, any geodesic curve $\gamma:[0,1] \rightarrow \mathcal{M}$ connecting $x \in \mathcal{M}$ to $y \in \mathcal{M}$ can be extended on $\mathbb{R}$ as a geodesic line. Furthermore, by Cartan-Hadamard theorem (Lee, 2006, Theorem 11.5), Hadamard manifolds are diffeomorphic to the Euclidean space $\mathbb{R}^{d}$, and the exponential map at any $x \in \mathcal{M}$ from $T_{x} \mathcal{M}$ to $\mathcal{M}$ is bijective with the logarithm map as inverse. Moreover, their injectivity radius is infinite and thus, its geodesics are aperiodic, and can be mapped to the real line, which will allow to find coordinates on the real line, and hence to compute the Wasserstein distance between the projected measures efficiently. The SW discrepancy on such spaces is therefore very analogous to the Euclidean case. Note that Hadamard manifolds belong to the more general class of $\operatorname{CAT}(0)$ metric spaces, and hence inherit their properties described in (Bridson and Haefliger, 2013). Now, let us discuss two different possible projections, which both generalize the Euclidean orthogonal projection.

Geodesic Projections. As we saw in Section 3.1, a natural projection on geodesics is the geodesic projection. Let's note $\mathcal{G}$ a geodesic passing through an origin point $o \in \mathcal{M}$. Such origin will often be taken naturally on the space, and corresponds to the analog of 0 in $\mathbb{R}^{d}$. Then, the geodesic projection on $\mathcal{G}$ is obtained naturally as

$$
\begin{equation*}
\forall x \in \mathcal{M}, \tilde{P}^{\mathcal{G}}(x)=\underset{y \in \mathcal{G}}{\operatorname{argmin}} d(x, y) . \tag{19}
\end{equation*}
$$

From the projection, we can get a coordinate on the geodesic by first giving it a direction and then computing the distance to the origin. By noting $v \in T_{o} \mathcal{M}$ a vector in the tangent space at the origin, such that $\mathcal{G}=\mathcal{G}^{v}=\left\{\exp _{o}(t v), t \in \mathbb{R}\right\}$, we can give a direction to the geodesic by computing the sign of the inner product in the tangent space of $o$ between $v$ and the $\log$ of $\tilde{P}^{\mathcal{G}}$. Analogously to the Euclidean case, we can restrict $v$ to be of unit norm, i.e. $\|v\|_{o}=1$. Now, we will denote the projection and coordinate projection on $\mathcal{G}^{v}$ respectively as $\tilde{P}^{v}$ and $P^{v}$. Hence, we obtain the coordinates using

$$
\begin{equation*}
P^{v}(x)=\operatorname{sign}\left(\left\langle\log _{o}\left(\tilde{P}^{v}(x)\right), v\right\rangle_{o}\right) d\left(\tilde{P}^{v}(x), o\right) \tag{20}
\end{equation*}
$$

We show in Proposition 2 that the map $t^{v}: \mathcal{G}^{v} \rightarrow \mathbb{R}$ defined as

$$
\begin{equation*}
\forall x \in \mathcal{G}^{v}, t^{v}(x)=\operatorname{sign}\left(\left\langle\log _{o}(x), v\right\rangle_{o}\right) d(x, o) \tag{21}
\end{equation*}
$$

is an isometry.

Proposition 2 Let $(\mathcal{M}, g)$ be a Hadamard manifold with origin o. Let $v \in T_{o} \mathcal{M}$, then, the map $t^{v}$ defined in (21) is an isometry from $\mathcal{G}^{v}=\left\{\exp _{o}(t v), t \in \mathbb{R}\right\}$ to $\mathbb{R}$.

Note that to get directly the coordinate from $x \in \mathcal{M}$, we can also solve directly the following problem:

$$
\begin{equation*}
P^{v}(x)=\underset{t \in \mathbb{R}}{\operatorname{argmin}} d\left(\exp _{o}(t v), x\right) . \tag{22}
\end{equation*}
$$

Using that Hadamard manifolds belong to the more general class of $\operatorname{CAT}(0)$ metric spaces, by (Bridson and Haefliger, 2013, II. Proposition 2.2), the geodesic distance is geodesically convex. Hence, minimizing the function $t \mapsto d\left(\exp _{o}(t v), x\right)^{2}$ is a coercive strictly-convex problem, and thus admits a unique solution. Therefore, (22) is well defined. Moreover, we have the following characterization for the optimum:

Proposition 3 Let $(\mathcal{M}, g)$ be a Hadamard manifold with origin o. Let $v \in T_{o} \mathcal{M}$, and note $\gamma(t)=\exp _{o}(t v)$ for all $t \in \mathbb{R}$. Then, for any $x \in \mathcal{M}$,

$$
\begin{equation*}
P^{v}(x)=\underset{t \in \mathbb{R}}{\operatorname{argmin}} d(\gamma(t), x)^{2} \Longleftrightarrow\left\langle\gamma^{\prime}\left(P^{v}(x)\right), \log _{\gamma\left(P^{v}(x)\right)}(x)\right\rangle_{\gamma\left(P^{v}(x)\right)}=0 \tag{23}
\end{equation*}
$$

In the Euclidean case $\mathbb{R}^{d}$, as geodesics are of the form $\gamma(t)=t \theta$ for any $t \in \mathbb{R}$ and for a direction $\theta \in S^{d-1}$, and as $\log _{x}(y)=y-x$ for $x, y \in \mathbb{R}^{d}$, we recover the projection formula:

$$
\begin{equation*}
\left\langle\gamma^{\prime}\left(P^{\theta}(x)\right), \log _{\gamma\left(P^{\theta}(x)\right)}(x)\right\rangle_{\gamma\left(P^{\theta}(x)\right)}=0 \Longleftrightarrow\left\langle\theta, x-P^{\theta}(x) \theta\right\rangle=0 \Longleftrightarrow P^{\theta}(x)=\langle\theta, x\rangle \tag{24}
\end{equation*}
$$

Busemann Projections. The level sets of the geodesic projections are geodesic subspaces. It has been shown that projecting along geodesics is not always the best solution as it might not preserve distances between the original points (Chami et al., 2021). Indeed, on Euclidean spaces, as mentioned earlier, the projections are actually along hyperplanes, which preserve the distances between points belonging to another geodesic with the same direction (see Figure 3). On Hadamard manifolds, there are analogs of hyperplanes, which can be obtained through the level sets of the Busemann function which we introduce now.

Let $\gamma: \mathbb{R} \rightarrow \mathcal{M}$ be a geodesic line, then the Busemann function associated to $\gamma$ is defined as (Bridson and Haefliger, 2013, II. Definition 8.17)

$$
\begin{equation*}
\forall x \in \mathcal{M}, B^{\gamma}(x)=\lim _{t \rightarrow \infty}(d(x, \gamma(t))-t) \tag{25}
\end{equation*}
$$

On Hadamard manifolds, and more generally on CAT(0) spaces, the limit exists (Bridson and Haefliger, 2013, II. Lemma 8.18). This function returns a coordinate on the geodesic $\gamma$, which can be understood as a normalized distance to infinity towards the direction given by $\gamma$ (Chami et al., 2021). The level sets of this function are called horospheres. On spaces of constant curvature (i.e. Euclidean or Hyperbolic spaces), horospheres are of constant null curvature and hence very similar to hyperplanes. We illustrate horospheres in Hyperbolic spaces in the middle of Figure 3 and in Figure 5.

For example, in the Euclidean case, we can show that the Busemann function associated to $\mathcal{G}^{\theta}=\operatorname{span}(\theta)$ for $\theta \in S^{d-1}$ is given by

$$
\begin{equation*}
\forall x \in \mathbb{R}^{d}, B^{\theta}(x)=-\langle x, \theta\rangle \tag{26}
\end{equation*}
$$

It actually coincides, up to a sign, with the inner product, which can be seen as a coordinate on the geodesic $\mathcal{G}^{\theta}$. Moreover, its level sets in this case are (affine) hyperplanes orthogonal to $\theta$.

Hence, the Busemann function provides a principled way to project measures on a Hadamard manifold to the real line, provided that we can compute its closed-form. To find
![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-11.jpg?height=398&width=1286&top_left_y=338&top_left_x=388)

Figure 3: (Left) On Euclidean spaces, the distance between the projections of two points belonging to a geodesic with the same direction is conserved. (Middle) On Hyperbolic spaces, this is also the case using the horospherical projection as demonstrated in (Chami et al., 2021, Proposition 3.4), but not for geodesic projections (Right).

the projection on the geodesic $\gamma$, we can solve the equation in $s \in \mathbb{R}, B^{\gamma}(x)=B^{\gamma}(\gamma(s))=$ $-s$, and we find that the projection on the geodesic $\gamma$ characterized by $v \in T_{o} \mathcal{M}$ such that $\|v\|_{o}=1$ and $\gamma(t)=\exp _{o}(t v)$ for all $t \in \mathbb{R}$ is

$$
\begin{equation*}
\tilde{B}^{v}(x)=\exp _{o}\left(-B^{v}(x) v\right) \tag{27}
\end{equation*}
$$

Wasserstein Distance on Geodesics. We saw that we can obtain projections on $\mathbb{R}$. Hence, it is analogous to the Euclidean case as we can use the one dimensional Wasserstein distance on the real line to compute it. In Proposition 4, as a sanity check, we verify that the Wasserstein distance between the coordinates (on $\mathcal{P}_{p}(\mathbb{R})$ ) is as expected equal to the Wasserstein distance between the measures projected on geodesics (on $\mathcal{P}_{p}(\mathcal{M})$ ). This relies on the isometry property of $t^{v}$ derived in Proposition 2.

Proposition 4 Let $(\mathcal{M}, g)$ a Hadamard manifold, $p \geq 1$ and $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$. Let $v \in T_{o} \mathcal{M}$ such that $\|v\|_{o}=1$ and $\mathcal{G}^{v}=\left\{\exp _{o}(t v), t \in \mathbb{R}\right\}$ the geodesic on which the measures are projected. Then,

$$
\begin{align*}
& W_{p}^{p}\left(\tilde{P}_{\#}^{v} \mu, \tilde{P}_{\#}^{v} \nu\right)=W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right)  \tag{28}\\
& W_{p}^{p}\left(\tilde{B}_{\#}^{v} \mu, \tilde{B}_{\#}^{v} \nu\right)=W_{p}^{p}\left(B_{\#}^{v} \mu, B_{\#}^{v} \nu\right) \tag{29}
\end{align*}
$$

where the Wasserstein distances are defined with the corresponding geodesic distance given the space, i.e. with $d(x, y)$ the geodesic distance on $\mathcal{M}$ for the $W_{p}$ on the left, and $|t-s|$ for $W_{p}$ on the right.

From these properties, we can work equivalently in $\mathbb{R}$ and on the geodesics when using the Busemann projection (also called horospherical projection) or the geodesic projection of measures.

Sliced-Wasserstein on Hadamard Manifolds. We are ready to define the SlicedWasserstein distance on Hadamard manifolds. For directions, we will sample from the uniform measure on $S_{o}=\left\{v \in T_{o} \mathcal{M},\|v\|_{o}=1\right\}$. Note that other distributions might be used such as a Dirac in the maximum direction similarly as max-SW (Deshpande et al.,

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-12.jpg?height=583&width=1054&top_left_y=321&top_left_x=533)

Figure 4: Illustration of the projection process of measures on geodesics $t \mapsto \exp _{o}\left(t v_{1}\right)$ and $t \mapsto \exp _{o}\left(t v_{2}\right)$.

2019) for example or any variant using different slicing distributions such as in (Nguyen et al., 2021a,b; Nguyen and Ho, 2023b; Ohana et al., 2023). However, in order to define a strict generalization of SW, we choose to focus on the uniform one in this work.

Definition 5 (Cartan-Hadamard Sliced-Wasserstein) Let (M,g) a Hadamard manifold with o its origin. Denote $\lambda_{o}$ the uniform distribution on $S_{o}=\left\{v \in T_{o} \mathcal{M},\|v\|_{o}=1\right\}$. Let $p \geq 1$, then we define the $p$-Geodesic Cartan-Hadamard Sliced-Wasserstein distance between $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$ as

$$
\begin{equation*}
\operatorname{GCHSW}_{p}^{p}(\mu, \nu)=\int_{S_{o}} W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) \mathrm{d} \lambda_{o}(v) \tag{30}
\end{equation*}
$$

Likewise, we define the p-Horospherical Cartan-Hadamard Sliced-Wasserstein distance between $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$ as

$$
\begin{equation*}
\operatorname{HCHSW}_{p}^{p}(\mu, \nu)=\int_{S_{o}} W_{p}^{p}\left(B_{\#}^{v} \mu, B_{\#}^{v} \nu\right) \mathrm{d} \lambda_{o}(v) \tag{31}
\end{equation*}
$$

In the following, when we want to mention both GCHSW and HCHSW, for example for properties satisfied by both, we will use the term Cartan-Hadamard Sliced-Wasserstein abbreviated as CHSW. Then, we will write without loss of generality

$$
\begin{equation*}
\operatorname{CHSW}_{p}^{p}(\mu, \nu)=\int_{S_{o}} W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) \mathrm{d} \lambda_{o}(v) \tag{32}
\end{equation*}
$$

with $P^{v}$ either denoting the geodesic or the horospherical projection. We illustrate the projection process on Figure 4.

### 3.3 Related Works

Intrinsic Sliced-Wasserstein. To the best of our knowledge, the first attempt to define a generalization of the Sliced-Wasserstein distance on Riemannian manifolds was made by Rustamov and Majumdar (2023). In this work, they restricted their analysis to compact spaces and proposed to use the eigendecomposition of the Laplace-Beltrami operator (see (Gallot et al., 1990, Definition 4.7)). Let (M,g) be a compact Riemannian manifold. For $\ell \in \mathbb{N}$, denote $\lambda_{\ell}$ the eigenvalues and $\phi_{\ell}$ the eigenfunctions of the Laplace-Beltrami operator sorted by increasing eigenvalues. Then, we can define spectral distances as

$$
\begin{equation*}
\forall x, y \in \mathcal{M}, d_{\alpha}(x, y)=\sum_{\ell \geq 0} \alpha\left(\lambda_{\ell}\right)\left(\phi_{\ell}(x)-\phi_{\ell}(y)\right)^{2} \tag{33}
\end{equation*}
$$

where $\alpha: \mathbb{R}_{+} \rightarrow \mathbb{R}_{+}$is a monotonically decreasing function. Then, they define the Intrinsic Sliced-Wasserstein (ISW) distance between $\mu, \nu \in \mathcal{P}_{2}(\mathcal{M})$ as

$$
\begin{equation*}
\operatorname{ISW}_{2}^{2}(\mu, \nu)=\sum_{\ell \geq 0} \alpha\left(\lambda_{\ell}\right) W_{2}^{2}\left(\left(\phi_{\ell}\right)_{\#} \mu,\left(\phi_{\ell}\right)_{\#} \nu\right) \tag{34}
\end{equation*}
$$

The eigenfunctions are used to map the measures to the real line, which make it very efficient to compute in practice. The eigenvalues are sorted in increasing order, and the series is often truncated by keeping only the $L$ smallest eigenvalues. This distance cannot be applied on Hadamard manifolds as these spaces are not compact.

Sliced-Wasserstein on the Sphere. Bonet et al. (2023c) then proposed a Spherical Sliced-Wasserstein distance by integrating and projecting over all geodesics using the geodesic projection in an attempt to generalize intrinsically the Sliced-Wasserstein distance to the sphere $S^{d-1}$. We note that ISW is more in the spirit of a max-K Sliced-Wasserstein distance (Dai and Seljak, 2021), which projects over the $K$ maximal directions, than the Sliced-Wasserstein distance. More recently, Quellmalz et al. (2023, 2024) studied different Sliced-Wasserstein distances on $S^{2}$ by using spherical Radon transforms while Tran et al. (2024) proposed to use the stereographic projection along the Generalized SlicedWasserstein distance (Kolouri et al., 2019), and Garrett et al. (2024) proposed SlicedWasserstein distances over the space of functions on the sphere using a convolution slicer w.r.t a kernel for the projection. Moreover, Genest et al. (2024) leveraged the SlicedWasserstein distance on manifolds to sample noise on non-Euclidean spaces such as meshes.

Generalized Sliced-Wasserstein. A somewhat related distance is the Generalized Sliced-Wasserstein distance (GSW) introduced by Kolouri et al. (2019), and which uses nonlinear projections on the real lines. The main difference lies in the fact that GSW focuses on probability distributions lying in Euclidean space by projecting the measures along nonlinear hypersurfaces. That said, adapting the definition of GSW to handle probability measures on Riemannian manifolds, and the properties that need to be satisfied by the defining function $g$ such as the homogeneity, then we can write the CHSW in the framework of GSW using $g:(x, v) \mapsto P^{v}(x)$.

## 4 Examples of Cartan-Hadamard Sliced-Wasserstein

In this section, we specify the framework derived in full generality in Section 3 to particular Hadamard manifolds. More precisely, we first focus on manifolds endowed with a Pullback

Euclidean metric, which are Hadamard manifolds with null curvature. Then, we take a look at Hyperbolic spaces which are manifolds of constant negative curvature. We also study the space of Symmetric Positive Definite matrices (SPD) endowed with metrics for which it is a Hadamard manifold. Finally, we discuss the case of the product manifold of Hadamard manifolds, which is itself a Hadamard manifold as products of manifolds of non-positive curvature are still of non-positive curvature (Gu et al., 2019, Lemma 1). We defer the proofs of this section to Appendix C.

### 4.1 Pullback Euclidean Manifold

Cartan-Hadamard manifolds include among others spaces of null curvature. As the curvature is conserved by the pullback operator, pullback Euclidean metrics are such spaces. We formally recall the definition of a pullback Euclidean metric along with its geodesic distance and exponential map following (Chen et al., 2023b, Theorem 3.3).

Theorem 6 (Pullback Euclidean Metric) Let $\mathcal{N}$ be an Euclidean space and denote $\langle\cdot, \cdot\rangle$ its inner product and $\|\cdot\|$ the associated norm. Let $\mathcal{M}$ be some space and $\phi: \mathcal{M} \rightarrow \mathcal{N}$ be a diffeomorphism. Then, defining for any $x \in \mathcal{M}$ and $u, v \in T_{x} \mathcal{M}$ the metric $g_{x}^{\phi}(u, v)=$ $\left\langle\phi_{*, x}(u), \phi_{*, x}(v)\right\rangle$ where $\phi_{*, x}: T_{x} \mathcal{M} \rightarrow T_{\phi(x)} \mathcal{N}$ is the differential of $\phi$ at $x,\left(\mathcal{M}, g^{\phi}\right)$ is a Riemannian manifold with geodesic distance

$$
\begin{equation*}
d_{\mathcal{M}}(x, y)=\|\phi(x)-\phi(y)\| . \tag{35}
\end{equation*}
$$

Moreover, the exponential map is

$$
\begin{equation*}
\forall x \in \mathcal{M}, v \in T_{x} \mathcal{M}, \exp _{x}(v)=\phi^{-1}\left(\phi(x)+\phi_{*, x}(v)\right) \tag{36}
\end{equation*}
$$

Let $\left(\mathcal{M}, g^{\phi}\right)$ be such space. Denote $o$ the origin of $\mathcal{M}$. Geodesics passing through $o$ in direction $v \in T_{o} \mathcal{M}$ are of the form

$$
\begin{equation*}
\forall t \in \mathbb{R}, \gamma_{v}(t)=\phi^{-1}\left(\phi(o)+t \phi_{*, o}(v)\right) \tag{37}
\end{equation*}
$$

Moreover, tangent vectors $v \in T_{o} \mathcal{M}$ belong to the sphere $S_{o}$ if and only if $\|v\|_{o}^{2}=\left\|\phi_{*, o}(v)\right\|^{2}=$ 1. Thus, using this formula, we can obtain both the geodesic and horospherical coordinates which actually coincide (up to a sign), as in the Euclidean case.

Proposition 7 Let $v \in S_{o}$, then the projection coordinate on $\mathcal{G}^{v}=\left\{\gamma_{v}(t), t \in \mathbb{R}\right\}$ is

$$
\begin{equation*}
\forall x \in \mathcal{M}, P^{v}(x)=-B^{v}(x)=\left\langle\phi(x)-\phi(o), \phi_{*, o}(v)\right\rangle \tag{38}
\end{equation*}
$$

For instance, the Euclidean space endowed with the Mahalanobis distance enters this framework for $\phi(x)=A^{\frac{1}{2}} x$ with $A \in S_{d}^{++}(\mathbb{R})$ a positive definite matrix since in this case, for any $x, y \in \mathbb{R}^{d}$,

$$
\begin{equation*}
d(x, y)^{2}=(x-y)^{T} A(x-y)=\left\|A^{\frac{1}{2}} x-A^{\frac{1}{2}} y\right\|_{2}^{2} \tag{39}
\end{equation*}
$$

In this case, we have $\phi(0)=0$ and $\phi_{*, 0}(v)=A^{\frac{1}{2}} v$. Thus, the projection is obtained by $P^{v}(x)=\left\langle A^{\frac{1}{2}} x, A^{\frac{1}{2}} v\right\rangle=x^{T} A v$ for $v \in S_{0}$, i.e. which satisfies $\|v\|_{0}^{2}=\left\|A^{\frac{1}{2}} v\right\|_{2}^{2}=1$. In this situation, as expected, the directions and the data points are first mapped by the linear projection $x \mapsto A^{\frac{1}{2}} x$ and then the usual orthogonal projections are performed as for the Euclidean Sliced-Wasserstein distance.

Definition 8 (Mahalanobis Sliced-Wasserstein) Let $p \geq 1, A \in S_{d}^{++}(\mathbb{R})$. The $p$ Mahalanobis Sliced-Wasserstein distance between $\mu, \nu \in \mathcal{P}_{p}\left(\mathbb{R}^{d}\right)$ is defined as

$$
\begin{equation*}
\operatorname{SW}_{p, A}^{p}(\mu, \nu)=\int_{S_{0}} W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) \mathrm{d} \lambda_{0}(v) \tag{40}
\end{equation*}
$$

with $P^{v}(x)=x^{T} A v$ for $v \in S_{0}=\left\{v \in \mathbb{R}^{d}, v^{T} A v=1\right\}, x \in \mathbb{R}^{d}$ and $\lambda_{0}$ the uniform distribution on $S_{0}$.

The Mahalanobis distance is often learned in metric learning, which has been used for different applications in e.g. computer visions, information retrieval or bioinformatics (Bellet et al., 2013). In Section 6.1, we use the Mahalanobis Sliced-Wasserstein distance on a document classification task (Kusner et al., 2015), where the underlying metric $A$ is previously learned using metric learning methods (Huang et al., 2016).

More generally, this Pullback Euclidean framework includes any squared geodesic distance for which the metric is of the form $\langle u, v\rangle_{x}=u^{T} A(x) v$ with $A(x) \in S_{d}^{++}(\mathbb{R})$ for any $x \in \mathbb{R}^{d}$ (Scarvelis and Solomon, 2023; Pooladian et al., 2023). For such a metric, we have $\phi_{*, x}(v)=A(x)^{\frac{1}{2}} v$ and computing $\phi(x)$ in closed-form may not be straightforward. It also includes many useful metrics used on the space of SPD matrices which we describe more thoroughly in Section 4.3.2.

### 4.2 Hyperbolic Spaces

Hyperbolic spaces are Riemannian manifolds of negative constant curvature $K<0$ (Lee, 2006) and are hence particular cases of Hadamard manifolds. They have recently received a surge of interest in machine learning as they allow embedding data with a hierarchical structure efficiently (Nickel and Kiela, 2017, 2018). A thorough review of the recent use of hyperbolic spaces in machine learning can be found in (Peng et al., 2021; Mettes et al., 2023).

There are five usual parameterizations of a hyperbolic manifold (Peng et al., 2021). They are equivalent (isometric) and one can easily switch from one formulation to the other. Hence, in practice, we use the one which is the most convenient, either given the formulae to derive or the numerical properties. In machine learning, the two most used models are the Poincaré ball and the Lorentz model (also known as the hyperboloid model). Each of these models has its own advantages compared to the other. For example, the Lorentz model has a distance which behaves better w.r.t. numerical issues compared to the distance of the Poincaré ball. However, the Lorentz model is unbounded, contrary to the Poincaré ball. We introduce in the following these two models.

Lorentz Model. The Lorentz model of curvature $K<0$ is defined as

$$
\begin{equation*}
\mathbb{L}_{K}^{d}=\left\{\left(x_{0}, \ldots, x_{d}\right) \in \mathbb{R}^{d+1},\langle x, x\rangle_{\mathbb{Q}}=\frac{1}{K}, x_{0}>0\right\} \tag{41}
\end{equation*}
$$

where for $x, y \in \mathbb{R}^{d+1},\langle x, y\rangle_{\mathbb{\Perp}}=-x_{0} y_{0}+\sum_{i=1}^{d} x_{i} y_{i}$ is the Minkowski pseudo inner-product. The Lorentz model can be seen as the upper sheet of a two-sheet hyperboloid. In the following, we will denote $x^{0}=\left(\frac{1}{\sqrt{-K}}, 0, \ldots, 0\right) \in \mathbb{L}_{K}^{d}$ the origin of the hyperboloid. The geodesic distance in this manifold is defined as

$$
\begin{equation*}
\forall x, y \in \mathbb{L}_{K}^{d}, d_{\mathbb{\Perp}}(x, y)=\frac{1}{\sqrt{-K}} \operatorname{arccosh}\left(K\langle x, y\rangle_{\mathbb{\circledR}}\right) \tag{42}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-16.jpg?height=211&width=219&top_left_y=369&top_left_x=346)

(a) Euclidean.

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-16.jpg?height=211&width=219&top_left_y=369&top_left_x=606)

(b) Geodesics.

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-16.jpg?height=214&width=225&top_left_y=370&top_left_x=863)

(c) Horospheres.

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-16.jpg?height=177&width=227&top_left_y=386&top_left_x=1120)

(d) Euclidean.

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-16.jpg?height=174&width=223&top_left_y=390&top_left_x=1344)

(e) Geodesics.

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-16.jpg?height=176&width=235&top_left_y=389&top_left_x=1563)

(f) Horospheres.

Figure 5: Projection of (red) points on a geodesic (black line) in the Poincaré ball and in the Lorentz model along Euclidean lines, geodesics or horospheres (in blue). Projected points on the geodesic are shown in green.

At any $x \in \mathbb{L}_{K}^{d}$, the tangent space is $T_{x} \unrhd_{K}^{d}=\left\{v \in \mathbb{R}^{d+1},\langle x, v\rangle_{\mathbb{Q}}=0\right\}$. Note that on $T_{x^{0}} \unrhd_{K}^{d}$, the Minkowski inner product is the usual Euclidean inner product. Moreover, geodesics passing through $x$ in direction $v \in T_{x}\left\llcorner_{K}^{d}\right.$ are obtained as the intersection between the plane $\operatorname{span}(x, v)$ and the hyperboloid $\mathbb{L}_{K}^{d}$, and are of the form

$$
\begin{equation*}
\forall t \in \mathbb{R}, \exp _{x}(t v)=\cosh \left(\sqrt{-K} t\|v\|_{\mathbb{\complement}}\right) x+\frac{\sinh \left(\sqrt{-K} t\|v\|_{\mathbb{\complement}}\right)}{\sqrt{-K}} \frac{v}{\|v\|_{\mathbb{L}}} \tag{43}
\end{equation*}
$$

In particular, geodesics passing through the origin $x^{0}$ in direction $v \in S_{x^{0}}$ are

$$
\begin{equation*}
\forall t \in \mathbb{R}, \gamma_{v}(t)=\exp _{x^{0}}(t v)=\cosh (\sqrt{-K} t) x^{0}+\frac{\sinh (\sqrt{-K} t)}{\sqrt{-K}} v \tag{44}
\end{equation*}
$$

Poincaré Ball. The Poincaré ball of curvature $K<0$ is defined as

$$
\begin{equation*}
\mathbb{B}_{K}^{d}=\left\{x \in \mathbb{R}^{d},\|x\|_{2}^{2}<-\frac{1}{K}\right\} \tag{45}
\end{equation*}
$$

It can be seen as the stereographic projection of each point of $\mathbb{L}_{K}^{d}$ on the hyperplane $\{x \in$ $\left.\mathbb{R}^{d+1}, x_{0}=0\right\}$. The origin of $\mathbb{B}_{K}^{d}$ is 0 and the geodesic distance is defined as

$$
\begin{equation*}
\forall x, y \in \mathbb{B}_{K}^{d}, d_{\mathbb{B}}(x, y)=\frac{1}{\sqrt{-K}} \operatorname{arccosh}\left(1-2 K \frac{\|x-y\|_{2}^{2}}{\left(1+K\|x\|_{2}^{2}\right)\left(1+K\|y\|_{2}^{2}\right)}\right) \tag{46}
\end{equation*}
$$

The tangent space is $\mathbb{R}^{d}$ and for any $\tilde{v} \in S^{d-1}$, the geodesic passing through the origin is defined as

$$
\begin{equation*}
\forall t \in \mathbb{R}, \gamma_{\tilde{v}}(t)=\exp _{0}(t \tilde{v})=\frac{1}{\sqrt{-K}} \tanh \left(\frac{\sqrt{-K} t}{2}\right) \tilde{v} \tag{47}
\end{equation*}
$$

Hyperbolic Sliced-Wasserstein. To define Hyperbolic Sliced-Wasserstein distances, we first need to sample geodesics, which can be done in both models by simply sampling from a uniform measure on the sphere. Indeed, let $\tilde{v} \in S^{d-1}$, then the direction of the geodesic in $\mathbb{L}_{K}^{d}$ is obtained as $v=(0, \tilde{v}) \in T_{x^{0}} \mathbb{L}_{K}^{d} \cap S^{d}=S_{x^{0}}$ by concatenating 0 to $\tilde{v}$. On the Poincaré ball, $\tilde{v}$ gives directly the direction to the geodesic, and is called an ideal point.

Thus, we only need to compute the projection coordinates on the geodesics in order to build the corresponding Geodesic and Horospherical Sliced-Wasserstein distances. We provide the closed-form of the geodesic projection and the Busemann function for both models in the following propositions. Additionally, we illustrate the projection process in Figure 5.

## Proposition 9 (Coordinate projections on Hyperbolic spaces)

1. Let $v \in S_{x^{0}}=T_{x^{0}} \llbracket_{K}^{d} \cap S^{d}$, the geodesic and horospherical projection coordinates on $\mathcal{G}^{v}=\operatorname{span}\left(x^{0}, v\right) \cap \mathbb{■}_{K}^{d}$ are for all $x \in \mathbb{L}_{K}^{d}$,

$$
\begin{align*}
P^{v}(x) & =\frac{1}{\sqrt{-K}} \operatorname{arctanh}\left(-\frac{1}{\sqrt{-K}} \frac{\langle x, v\rangle_{\mathbb{Z}}}{\left\langle x, x^{0}\right\rangle_{\mathbb{L}}}\right)  \tag{48}\\
B^{v}(x) & =\frac{1}{\sqrt{-K}} \log \left(-\sqrt{-K}\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{L}}\right) \tag{49}
\end{align*}
$$

2. Let $\tilde{v} \in S^{d-1}$ an ideal point. Then the geodesic and horospherical projections coordinates on $\mathcal{G}^{\tilde{v}}=\left\{\gamma_{\tilde{v}}(t), t \in \mathbb{R}\right\}$ are for all $x \in \mathbb{B}_{K}^{d}$,

$$
\begin{align*}
P^{\tilde{v}}(x) & =\frac{2}{\sqrt{-K}} \operatorname{arctanh}(\sqrt{-K} s(x))  \tag{50}\\
B^{\tilde{v}}(x) & =\frac{1}{\sqrt{-K}} \log \left(\frac{\|\tilde{v}-\sqrt{-K} x\|_{2}^{2}}{1+K\|x\|_{2}^{2}}\right) \tag{51}
\end{align*}
$$

where $s$ is defined as

$$
s(x)= \begin{cases}\frac{1-K\|x\|_{2}^{2}-\sqrt{\left(1-K\|x\|_{2}^{2}\right)^{2}+4 K\langle x, \tilde{v}\rangle^{2}}}{-2 K\langle x, \tilde{v}\rangle} & \text { if }\langle x, \tilde{v}\rangle \neq 0  \tag{52}\\ 0 & \text { if }\langle x, \tilde{v}\rangle=0\end{cases}
$$

This proposition allow to define hyperbolic Sliced-Wasserstein distances by specifying CHSW with the right formulas.

## Definition 10 (Hyperbolic Sliced-Wasserstein)

1. Let $p \geq 1, \mu, \nu \in \mathcal{P}_{p}\left(\mathbb{L}_{K}^{d}\right)$. Then, the $p$-Geodesic Hyperbolic Sliced-Wasserstein distance and the p-Horospherical Hyperbolic Sliced-Wasserstein distance on the Lorentz model $\mathbb{L}_{K}^{d}$ are defined as

$$
\begin{align*}
\operatorname{GHSW}_{p}^{p}(\mu, \nu) & =\int_{T_{x^{0}} \mathbb{L}_{K}^{d} \cap S^{d}} W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) \mathrm{d} \lambda(v)  \tag{53}\\
\operatorname{HHSW}_{p}^{p}(\mu, \nu) & =\int_{T_{x^{0}} \mathbb{L}_{K}^{d} \cap S^{d}} W_{p}^{p}\left(B_{\#}^{v} \mu, B_{\#}^{v} \nu\right) \mathrm{d} \lambda(v) \tag{54}
\end{align*}
$$

2. Let $p \geq 1, \tilde{\mu}, \tilde{\nu} \in \mathcal{P}_{p}\left(\mathbb{B}_{K}^{d}\right)$. Then, the $p$-Geodesic Hyperbolic Sliced-Wasserstein distance and the $p$-Horospherical Hyperbolic Sliced-Wasserstein distance on the Poincaré ball $\mathbb{B}_{K}^{d}$ are defined as

$$
\begin{align*}
\operatorname{GHSW}_{p}^{p}(\tilde{\mu}, \tilde{\nu}) & =\int_{S^{d-1}} W_{p}^{p}\left(P_{\#}^{\tilde{u}} \tilde{\mu}, P_{\#}^{\tilde{v}} \tilde{\nu}\right) \mathrm{d} \lambda(\tilde{v})  \tag{55}\\
\operatorname{HHSW}_{p}^{p}(\tilde{\mu}, \tilde{\nu}) & =\int_{S^{d-1}} W_{p}^{p}\left(B_{\#}^{\tilde{v}} \tilde{\mu}, B_{\#}^{\tilde{v}} \tilde{\nu}\right) \mathrm{d} \lambda(\tilde{v}) \tag{56}
\end{align*}
$$

Note that we could also work on the other models such as the Klein model, the Poincaré half-plane model or the hemisphere model (see e.g. (Cannon et al., 1997; Loustau, 2020)) and derive the corresponding projections in order to define the Hyperbolic Sliced-Wasserstein distances in these models. Note also that these different Sliced-Wasserstein distances are actually equal from one model to the other when using the isometry mappings, which is a particular case of Proposition 11.

Proposition 11 Let $\left(\mathcal{M}, g^{\mathcal{M}}\right)$ and $\left(\mathcal{N}, g^{\mathcal{N}}\right)$ be two isometric Cartan-Hadamard manifolds, $\phi: \mathcal{M} \rightarrow \mathcal{N}$ an isometry and assume that $\lambda_{\phi(o)}=\left(\phi_{*, o}\right)_{\#} \lambda_{o}{ }^{2}$. Let $p \geq 1, \mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$ and $\tilde{\mu}=\phi_{\#} \mu, \tilde{\nu}=\phi_{\#} \nu$. Then,

$$
\begin{equation*}
\operatorname{CHSW}_{p}^{p}\left(\mu, \nu ; \lambda_{o}\right)=\operatorname{CHSW}_{p}^{p}\left(\tilde{\mu}, \tilde{\nu} ; \lambda_{\phi(o)}\right) \tag{57}
\end{equation*}
$$

where we denote $\operatorname{CHSW}_{p}^{p}(\mu, \nu ; \lambda)$ the Cartan-Hadamard Sliced-Wasserstein distance with slicing distribution $\lambda$.

Proposition 11 includes as a particular case the Hyperbolic Sliced-Wasserstein distances (and in particular is more general than (Bonet et al., 2023a, Proposition 3.4)). This demonstrates that the Hyperbolic Sliced-Wasserstein distances are independent from the chosen model. Thus, we can work in the model which is the most convenient for us. Moreover, if we work on a model for which we do not have necessarily a closed-form, we can project the distributions on a model where we have the closed-forms such as the Lorentz model or the Poincaré ball for which we derived the closed-forms in this section.

Bonet et al. (2023a) compared GHSW and HHSW on different tasks such as gradient flows or as regularizers for deep classification with prototypes. Moreover, they also verified empirically that GHSW and HHSW are independent with respect to the model while comparing evolutions of the distances between Wrapped Normal distributions. In particular, they observed that HHSW had values closer to the Wasserstein distance compared to GHSW.

### 4.3 Symmetric Positive Definite Matrices

Let $S_{d}(\mathbb{R})$ be the set of symmetric matrices of $\mathbb{R}^{d \times d}$, and $S_{d}^{++}(\mathbb{R})$ be the set of SPD matrices of $\mathbb{R}^{d \times d}$, i.e. matrices $M \in S_{d}(\mathbb{R})$ satisfying for all $x \in \mathbb{R}^{d} \backslash\{0\}, x^{T} M x>0$. $S_{d}^{++}(\mathbb{R})$ is a Riemannian manifold (Bhatia, 2009) which can be endowed with different metrics. At each $M \in S_{d}^{++}(\mathbb{R})$, we can associate a tangent space $T_{M} S_{d}^{++}(\mathbb{R})$ which can be identified with the space of symmetric matrices $S_{d}(\mathbb{R})$.

SPD matrices have received a lot of attention in Machine Learning. On one hand, this is the natural space to deal with invertible covariance matrices, which are often used to represent M/EEG data (Blankertz et al., 2007; Sabbagh et al., 2019) or images (Tuzel et al., 2006; Pennec, 2020). Moreover, this space is more expressive than Euclidean spaces, and endowed with specific metrics such as the Affine-Invariant metric, it enjoys a nonconstant non-positive curvature. This property was leveraged to embed different type of data (Harandi et al., 2014; Brooks et al., 2019b). This motivated the development of different machine learning algorithms (Chevallier et al., 2017; Yair et al., 2019; Zhuang et al.,[^1]

2020; Lei et al., 2021; Ju and Guan, 2022) and of neural networks architectures (Huang and Van Gool, 2017; Brooks et al., 2019a).

We now introduce the Sliced-Wasserstein distance on the space of SPD matrices first endowed with the Affine-Invariant metric, and in a second part endowed with different pullback Euclidean metrics.

### 4.3.1 Symmetric Positive Definite Matrices with Affine-Invariant Metric.

A classical metric used widely with SPDs is the geometric Affine-Invariant metric (Pennec et al., 2006), where the inner product is defined as

$$
\begin{equation*}
\forall M \in S_{d}^{++}(\mathbb{R}), A, B \in T_{M} S_{d}^{++}(\mathbb{R}),\langle A, B\rangle_{M}=\operatorname{Tr}\left(M^{-1} A M^{-1} B\right) \tag{58}
\end{equation*}
$$

Denoting by Tr the Trace operator, the corresponding geodesic distance $d_{A I}(\cdot, \cdot)$ is given by

$$
\begin{equation*}
\forall X, Y \in S_{d}^{++}(\mathbb{R}), d_{A I}(X, Y)=\sqrt{\operatorname{Tr}\left(\log \left(X^{-1} Y\right)^{2}\right)} \tag{59}
\end{equation*}
$$

An interesting property justifying the use of the Affine-Invariant metric is that $d_{A I}$ satisfies the affine-invariant property: for any $g \in G L_{d}(\mathbb{R})$, where $G L_{d}(\mathbb{R})$ denotes the set of invertible matrices in $\mathbb{R}^{d \times d}$,

$$
\begin{equation*}
\forall X, Y \in S_{d}^{++}(\mathbb{R}), d_{A I}(g \cdot X, g \cdot Y)=d_{A I}(X, Y) \tag{60}
\end{equation*}
$$

where $g \cdot X=g X g^{T}$. With this metric, $S_{d}^{++}(\mathbb{R})$ is of (non-constant) non-positive curvature and hence a Hadamard manifold.

The natural origin is the identity matrix $I_{d}$ and geodesics passing through $I_{d}$ in direction $A \in S_{d}(\mathbb{R})$ are of the form (Pennec, 2020, Section 3.6.1)

$$
\begin{equation*}
\forall t \in \mathbb{R}, \gamma_{A}(t)=\exp _{I_{d}}(t A)=\exp (t A) \tag{61}
\end{equation*}
$$

where exp denotes the matrix exponential.

For the Affine-Invariant case, to the best of our knowledge, there is no closed-form for the geodesic projection on $\mathcal{G}^{A}$, the difficulty being that the matrices do not necessarily commute. Hence, we will discuss here the horospherical projection which can be obtained with the Busemann function. For $A \in S_{d}(\mathbb{R})$ such that $\|A\|_{F}=1$, denoting $\gamma_{A}: t \mapsto \exp (t A)$ the geodesic line passing through $I_{d}$ with direction $A$, the Busemann function $B^{A}$ associated to $\gamma_{A}$ writes as

$$
\begin{equation*}
\forall M \in S_{d}^{++}(\mathbb{R}), B^{A}(M)=\lim _{t \rightarrow \infty}\left(d_{A I}(\exp (t A), M)-t\right) \tag{62}
\end{equation*}
$$

We cannot directly compute this quantity by expanding the distance since $\exp (-t A)$ and $M$ are not necessarily commuting. The main idea to solve this issue is to first find a group $G \subset G L_{d}(\mathbb{R})$ which will leave the Busemann function invariant. Then, we can find an element of this group which will project $M$ on the space of matrices commuting with $\exp (A)$. This part of the space is of null curvature, i.e. it is isometric to an Euclidean space. In this case, we can compute the Busemann function as the matrices are commuting. Hence, the Busemann function is of the form

$$
\begin{equation*}
B^{A}(M)=-\left\langle A, \log \left(\pi_{A}(M)\right)\right\rangle_{F} \tag{63}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-20.jpg?height=544&width=984&top_left_y=300&top_left_x=518)

Figure 6: (Left) Random geodesics drawn in $S_{2}^{++}(\mathbb{R})$. (Right) Projections (green points) of covariance matrices (depicted as red points) over one geodesic (in black) passing through $I_{2}$ along the Log-Euclidean geodesics (blue lines).

where $\pi_{A}$ is a projection on the space of commuting matrices which can be obtained in practice through a UDU or LDL decomposition. We detail more precisely in Appendix F how to obtain $\pi^{A}$. For more details about the Busemann function on the Affine-invariant space, we refer to Bridson and Haefliger (2013, Section II.10) and Fletcher et al. (2009, 2011).

We note that computing the Busemann function on this space induces a heavy computational cost. Thus, we advocate using in practice Sliced-Wasserstein distances obtained using Pullback-Euclidean metrics on SPDs as described in the next section.

### 4.3.2 Symmetric Positive Definite Matrices with Pullback Euclidean METRICS.

We study here metrics endowing the space of SPD matrices which are pullback Euclidean metrics (Chen et al., 2023a,b), i.e. metrics which are obtained through a diffeomorphism from $S_{d}^{++}(\mathbb{R})$ to $\left(S_{d}(\mathbb{R}),\langle\cdot, \cdot\rangle_{F}\right)$. Pullback Euclidean metrics and more generally pullback metrics allow inheriting properties from the mapped space (Chen et al., 2023b). The pullback Euclidean metrics studied here belong to the framework presented in Section 4.1 with $\mathcal{M}=S_{d}^{++}(\mathbb{R})$ and $\mathcal{N}=S_{d}(\mathbb{R})$. This framework includes many interesting metrics, such as the Log-Euclidean metric with $\phi=\log$ (Arsigny et al., 2005, 2006) which is a good first order approximation of the Affine-Invariant metric (Arsigny et al., 2005; Pennec, 2020), the Log-Cholesky metric (Lin, 2019) or the recently proposed $O(n)$-invariant Log-Euclidean metric (Thanwerdas and Pennec, 2023; Chen et al., 2023a) and Adaptative Riemannian metric (Chen et al., 2023b).

Log-Euclidean Metric. We first focus on the Log-Euclidean metric for which $\phi=\log$. To apply Proposition 7, we first need to compute its differential in the origin $I_{d}$. For completeness, we recall here the differential form of the matrix logarithm derived e.g. in (Pennec, 2020).

Lemma 12 (Section 3.2.2 in (Pennec, 2020)) Let $\phi: X \mapsto \log (X)$ and $X=U D U^{T} \in$ $S_{d}^{++}(\mathbb{R})$ where $D=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{d}\right)$. The differential operator of $\phi$ at $X$ is given by

$$
\begin{equation*}
\forall V \in T_{X} S_{d}^{++}(\mathbb{R}), \phi_{*, X}(V)=U \Sigma(V) U^{T} \tag{64}
\end{equation*}
$$

where $\Sigma(V)=U^{T} V U \odot \Gamma$ and $\Gamma$ is the Loewner's matrix defined for all $i, j \in\{1, \ldots, d\}$ as

$$
\Gamma_{i j}= \begin{cases}\frac{\log \lambda_{i}-\log \lambda_{j}}{\lambda_{i}-\lambda_{j}} & \text { if } i \neq j  \tag{65}\\ \frac{1}{\lambda_{i}} & \text { if } i=j\end{cases}
$$

Proof Apply the Dalickii-Krein formula, see e.g. (Noferini, 2017, Theorem 2.11).

We note that for close eigenvalues (Pennec, 2020),

$$
\begin{equation*}
\frac{\log \lambda_{i}-\log \lambda_{j}}{\lambda_{i}-\lambda_{j}}=\frac{1}{\lambda_{j}}\left(1-\frac{\lambda_{i}-\lambda_{j}}{2 \lambda_{j}}+\frac{\left(\lambda_{i}-\lambda_{j}\right)^{2}}{3 \lambda_{j}^{2}}+O\left(\left(\lambda_{i}-\lambda_{j}\right)^{3}\right)\right) \tag{66}
\end{equation*}
$$

Furthermore, for $X=D=U=I_{d}$, since $\left[U^{T} V U\right]_{i j}=V_{i j}$, we find $\phi_{*, I_{d}}(V)=V$ for any $V$. Thus, as $\log \left(I_{d}\right)=0$, we obtain the following projections.

Proposition 13 Let $\phi=\log$. Then, for any $A \in S_{d}(\mathbb{R})$ such that $\|A\|_{F}=1$, the coordinate projection is

$$
\begin{equation*}
\forall X \in S_{d}^{++}(\mathbb{R}), P^{A}(X)=-B^{A}(X)=\langle\log (X), A\rangle_{F} \tag{67}
\end{equation*}
$$

Proof Apply Proposition 7 with $\phi(X)=\log (X)$ observing that $\phi\left(I_{d}\right)=0$ and $\phi_{*, I_{d}}=\mathrm{Id}$.

In particular, Bonet et al. (2023b) used the Sliced-Wasserstein distance on the space of SPDs endowed with the Log-Euclidean metric, tamed SPDSW, and applied it to M/EEG data to perform brain-age prediction and domain adaptation for brain computational interfaces.

O(n)-Invariant Log-Euclidean Metric. The $O(n)$-invariant Log-Euclidean metric has been introduced by Thanwerdas and Pennec (2023) and further studied in (Chen et al., 2023a). It is a pullback Euclidean metric with, for $X \in S_{d}^{++}(\mathbb{R})$ and $p, q \geq 0, \phi^{p, q}(X)=$ $F^{p, q}(\log (X))$ where $F^{p, q}(A)=q A+\frac{p-q}{d} \operatorname{Tr}(A) I_{d}$ for $A \in S_{d}(\mathbb{R})$. It can be seen as a generalization of the Log-Euclidean metric since for $p=q=1, F^{1,1}(A)=A$. Since $F^{p, q}$ is a linear function, the differential of $\phi^{p, q}$ at $X \in S_{d}^{++}(\mathbb{R})$ is $\phi_{*, X}^{p, q}(V)=F^{p, q}\left(\log _{*, X}(V)\right)$ for any $V \in S_{d}(\mathbb{R})$. Thus, we have $\phi^{p, q}\left(I_{d}\right)=0, \phi_{*, I_{d}}^{p, q}=F^{p, q}$, and we can apply Proposition 7 .

Proposition 14 Let $p, q \geq 0$, $\phi^{p, q}=F^{p, q} \circ \log$ with $F^{p, q}(A)=q A+\frac{p-q}{d} \operatorname{Tr}(A] I_{d}$ for $A \in S_{d}(\mathbb{R})$. Then, for any $A \in S_{d}(\mathbb{R})$ such that $\|A\|_{I_{d}}^{2}=\left\langle F^{p, q}(A), F^{p, q}(A)\right\rangle_{F}=1$, the coordinate projection is

$$
\begin{equation*}
\forall X \in S_{d}^{++}(\mathbb{R}), P^{A}(X)=\left\langle F^{p, q}(\log (X)), F^{p, q}(A)\right\rangle_{F} \tag{68}
\end{equation*}
$$

Proof Apply Proposition 7 with $\phi(X)=F^{p, q}(\log (X))$ observing that $\phi\left(I_{d}\right)=0$ and $\phi_{*, I_{d}}=F^{p, q}$.

Log-Cholesky Metric. Lin (2019) introduced the Log-Cholesky metric which is obtained as a pullback Euclidean metric with respect to $L_{d}(\mathbb{R})$, the space of lower triangular matrices, endowed with the Frobenius inner product. The diffeomorphism between $S_{d}^{++}(\mathbb{R})$ and $L_{d}(\mathbb{R})$ is of the form $\phi: X \mapsto \varphi(\mathcal{L}(X))$ with $\mathcal{L}: S_{d}^{++}(\mathbb{R}) \rightarrow L_{d}^{++}(\mathbb{R})$ which returns the lower triangular matrix obtained by the Cholesky decomposition, i.e. for $X=L L^{T} \in$ $S_{d}^{++}(\mathbb{R}), \mathcal{L}(X)=L$, and $\varphi: L_{d}^{++}(\mathbb{R}) \rightarrow L_{d}(\mathbb{R})$ defined as $\varphi(L)=\lfloor L\rfloor+\log (\operatorname{diag}(L))$ with $\lfloor\cdot\rfloor$ the strictly lower triangular part of the matrix and diag its diagonal part.

It is easy to see that $\phi\left(I_{d}\right)=0$. We compute the differential of $\phi$ in Lemma 48 by using the chain rule and (Lin, 2019, Proposition 4) which gives the differential of $\mathcal{L}$. Then, applying Proposition 7 , we can compute the projection.

Proposition 15 Let $\phi=\varphi \circ \mathcal{L}$. Then, for any $A \in S_{d}(\mathbb{R})$ such that $\|A\|_{I_{d}}^{2}=1$, the coordinate projection is

$$
\begin{equation*}
\forall X=L L^{T} \in S_{d}^{++}(\mathbb{R}), P^{A}(X)=\langle\lfloor L\rfloor,\lfloor A\rfloor\rangle_{F}+\left\langle\log (\operatorname{diag}(L)), \frac{1}{2} \operatorname{diag}(A)\right\rangle_{F} \tag{69}
\end{equation*}
$$

### 4.4 Product of Hadamard Manifolds.

In recent attempts to embed data into more flexible spaces, it was proposed to use products of manifolds (Gu et al., 2019; Skopek et al., 2020; de Ocáriz Borde et al., 2023a,b) instead of constant curvature spaces, as the data may not be uniformly curved. Products of constant curvature spaces are not necessarily of constant curvature, thus it allows to have more flexibility in order to embed the data by better capturing the curvature of the underlying manifold. Since product of Hadamard manifolds are still Hadamard manifolds (Gu et al., 2019), product of hyperbolic spaces are Hadamard manifolds, and can be used to obtain flexible spaces e.g. by learning the curvature of the different spaces. Another example of product of Hadamard manifolds is the Poincaré polydisk (Cabanes, 2022) which is the product manifold of $\mathbb{R}_{+}^{*}$ with distance $d(x, y)=|\log (y / x)|$ and the Poincaré disk, and which has received attention for radar applications (Le Brigant, 2017). Note also that Gaussian distributions with diagonal covariances matrices endowed with the Fisher information matrix form a product of hyperbolic spaces (Cho et al., 2022a). Therefore, it is of interest to provide tools to compare probability distributions on products of Hadamard manifolds.

Let $\left(\left(\mathcal{M}_{i}, g_{i}\right)\right)_{i=1}^{n}$ be $n$ Hadamard manifolds and define the product manifold $\mathcal{M}=$ $\mathcal{M}_{1} \times \cdots \times \mathcal{M}_{n}$. Then, at $x=\left(x_{1}, \ldots, x_{n}\right) \in \mathcal{M}$, the tangent space is simply the inner product $T_{x} \mathcal{M}=T_{x_{1}} \mathcal{M}_{1} \times \cdots \times T_{x_{n}} \mathcal{M}_{n}$, and $\mathcal{M}$ is equipped with the metric tensor $g=$ $\sum_{i=1}^{n} g_{i}$. Moreover, for $v=\left(v_{1}, \ldots, v_{n}\right) \in T_{o} \mathcal{M}$, the geodesic passing through the origin $o=\left(o_{1}, \ldots, o_{n}\right)$ in direction $v$ is

$$
\begin{equation*}
\forall t \in \mathbb{R}, \gamma_{o}(t)=\left(\gamma_{o_{1}}(t), \ldots, \gamma_{o_{n}}(t)\right) \tag{70}
\end{equation*}
$$

where $\gamma_{o_{i}}$ is a geodesic in $\mathcal{M}_{i}$ passing through $o_{i}$ in direction $v_{i}$. Moreover, the squared geodesic distance can be simply obtained as (Gu et al., 2019)

$$
\begin{equation*}
\forall x, y \in \mathcal{M}, d_{\mathcal{M}}(x, y)^{2}=\sum_{i=1}^{n} d_{\mathcal{M}_{i}}\left(x_{i}, y_{i}\right)^{2} \tag{71}
\end{equation*}
$$

Deriving the closed-form for the geodesic projection

$$
\begin{equation*}
t^{*}=\underset{t \in \mathbb{R}}{\operatorname{argmin}} \sum_{i=1}^{n} d_{\mathcal{M}_{i}}\left(\gamma_{o_{i}}(t), y_{i}\right)^{2} \tag{72}
\end{equation*}
$$

might depend on the context and might not be straightforward. Nonetheless, deriving the Busemann function on a product of Hadamard manifolds is simply the weighted sum of the Busemann function on each geodesic line, and is thus easy to compute provided we know in closed-form the Busemann function on each manifold $\mathcal{M}_{i}$. This was first observed in (Bridson and Haefliger, 2013, Section II. 8.24) in the case of two manifolds, and we generalize the result to an arbitrary number of manifolds.

Proposition 16 (Busemann function on product Hadamard manifold) Let $\left(\mathcal{M}_{i}\right)_{i=1}^{n}$ be $n$ Hadamard manifolds and $\mathcal{M}=\mathcal{M}_{1} \times \cdots \times \mathcal{M}_{n}$ the product manifold. Let $\lambda_{1}, \ldots, \lambda_{n}$ be such that $\sum_{i=1}^{n} \lambda_{i}^{2}=1$. For any $i \in\{1, \ldots, n\}$, let $\gamma_{i}$ be a geodesic line on $\mathcal{M}_{i}$ and define $\gamma: t \mapsto\left(\gamma_{1}\left(\lambda_{1} t\right), \ldots, \gamma_{n}\left(\lambda_{n} t\right)\right)$ a geodesic line on $\mathcal{M}$. Then,

$$
\begin{equation*}
\forall x=\left(x_{1}, \ldots, x_{n}\right) \in \mathcal{M}, B^{\gamma}(x)=\sum_{i=1}^{n} \lambda_{i} B^{\gamma_{i}}\left(x_{i}\right) \tag{73}
\end{equation*}
$$

In Section 6.2, we leverage this projection and the corresponding Sliced-Wasserstein distance to compare datasets viewed as distributions on $\mathbb{R}^{d_{x}} \times \mathbb{H}^{d_{y}}$.

## 5 Properties

In this section, we derive theoretical properties of the Cartan-Hyperbolic Sliced-Wasserstein distance. First, we will study its topology and the conditions required to have that CHSW is a true distance. In particular, we will first focus on the general case, and then on the specific case of pullback Euclidean metrics. Then, we will study some of its statistical properties. The proofs of this section are postponed to Appendix D.

### 5.1 Topology

Distance Property. First, we are interested in the distance properties of CHSW. From the properties of the Wasserstein distance and of the slicing process, we can show that it is a pseudo-distance, i.e. that it satisfies the positivity, the symmetry and the triangular inequality.

Proposition 17 Let $p \geq 1$, then $\mathrm{CHSW}_{p}$ is a finite pseudo-distance on $\mathcal{P}_{p}(\mathcal{M})$.

For now, the lacking property is the one of indiscernibility, i.e. that $\operatorname{CHSW}_{p}(\mu, \nu)=0$ implies that $\mu=\nu$. We conjecture that it holds but we have not been able to prove it yet in full generality. In the following, we derive a sufficient condition on a related Radon transform for this property to hold. These derivations are inspired from (Boman and Lindskog, 2009; Bonneel et al., 2015).

Let $f \in L^{1}(\mathcal{M})$, and let us define, analogously to the Euclidean Radon transform, the Cartan-Hadamard Radon transform CHR : $L^{1}(\mathcal{M}) \rightarrow L^{1}\left(\mathbb{R} \times S_{o}\right)$ which integrates the function $f$ over a level set of the projection $P^{v}$ :

$$
\begin{equation*}
\forall t \in \mathbb{R}, \forall v \in S_{o}, \operatorname{CHR} f(t, v)=\int_{\mathcal{M}} f(x) \mathbb{1}_{\left\{t=P^{v}(x)\right\}} \mathrm{dVol}(x) \tag{74}
\end{equation*}
$$

Then, we can also define its dual operator $\mathrm{CHR}^{*}: C_{0}\left(\mathbb{R} \times S_{o}\right) \rightarrow C_{b}(\mathcal{M})$ for $g \in C_{0}\left(\mathbb{R} \times S_{o}\right)$ where $C_{0}\left(\mathbb{R} \times S_{o}\right)$ is the space of continuous functions from $\mathbb{R} \times S_{o}$ to $\mathbb{R}$ that vanish at infinity and $C_{b}(\mathcal{M})$ is the space of continuous bounded functions from $\mathcal{M}$ to $\mathbb{R}$, as

$$
\begin{equation*}
\forall x \in \mathcal{M}, \operatorname{CHR}^{*} g(x)=\int_{S_{o}} g\left(P^{v}(x), v\right) \mathrm{d} \lambda_{o}(v) \tag{75}
\end{equation*}
$$

Proposition $18 \mathrm{CHR}^{*}$ is the dual operator of $\mathrm{CHR}$, i.e. for all $f \in L^{1}(\mathcal{M}), g \in C_{0}\left(\mathbb{R} \times S_{o}\right)$,

$$
\begin{equation*}
\langle\operatorname{CHR} f, g\rangle_{\mathbb{R} \times S_{o}}=\left\langle f, \mathrm{CHR}^{*} g\right\rangle_{\mathcal{M}} \tag{76}
\end{equation*}
$$

CHR $^{*}$ maps $C_{0}\left(\mathbb{R} \times S_{o}\right)$ to $C_{b}(\mathcal{M})$ because $g$ is necessarily bounded as a continuous function which vanishes at infinity. Note that $\operatorname{CHR}^{*}$ actually maps $C_{0}\left(\mathbb{R} \times S_{o}\right)$ to $C_{0}(\mathcal{M})$.

Proposition 19 Let $g \in C_{0}\left(\mathbb{R} \times S_{o}\right)$, then $\mathrm{CHR}^{*} g \in C_{0}(\mathcal{M})$.

Let us now recall the disintegration theorem.

Definition 20 (Disintegration of a measure) Let $(Y, \mathcal{Y})$ and $(Z, \mathcal{Z})$ be measurable spaces, and $(X, \mathcal{X})=(Y \times Z, \mathcal{Y} \otimes \mathcal{Z})$ the product measurable space. Then, for $\mu \in \mathcal{P}(X)$, we denote the marginals as $\mu_{Y}=\pi_{\#}^{Y} \mu$ and $\mu_{Z}=\pi_{\#}^{Z} \mu$, where $\pi^{Y}$ (respectively $\pi^{Z}$ ) is the projection on $Y$ (respectively $Z$ ). Then, a family $(K(y, \cdot))_{y \in \mathcal{Y}}$ is a disintegration of $\mu$ if for all $y \in Y$, $K(y, \cdot)$ is a measure on $Z$, for all $A \in \mathcal{Z}, K(\cdot, A)$ is measurable and:

$$
\forall g \in C(X), \int_{Y \times Z} g(y, z) \mathrm{d} \mu(y, z)=\int_{Y} \int_{Z} g(y, z) K(y, \mathrm{~d} z) \mathrm{d} \mu_{Y}(y)
$$

where $C(X)$ is the set of continuous functions on $X$. We can note $\mu=\mu_{Y} \otimes K$. $K$ is a probability kernel if for all $y \in Y, K(y, Z)=1$.

The disintegration of a measure actually corresponds to conditional laws in the context of probabilities. In the case where $X=\mathbb{R}^{d}$, we have existence and uniqueness of the disintegration (see (Santambrogio, 2015, Box 2.2) or (Ambrosio et al., 2005, Chapter 5) for the more general case).

Using the dual operator, we can define the Radon transform of a measure $\mu$ in $\mathcal{M}$ as the measure $\mathrm{CHR} \mu$ satisfying

$$
\begin{equation*}
\forall g \in C_{0}\left(\mathbb{R} \times S_{o}\right), \int_{\mathbb{R} \times S_{o}} g(t, v) \mathrm{d}(\operatorname{CHR} \mu)(t, v)=\int_{\mathcal{M}} \operatorname{CHR}^{*} g(x) \mathrm{d} \mu(x) \tag{77}
\end{equation*}
$$

CHR $\mu$ being a measure on $\mathbb{R} \times S_{o}$, we can disintegrate it w.r.t. the uniform measure on $S_{o}$ as $\operatorname{CHR} \mu=\lambda_{o} \otimes K_{\mu}$ where $K_{\mu}$ is a probability kernel on $S_{o} \times \mathcal{B}(\mathbb{R})$. In the following proposition, we show that for $\lambda_{o}$-almost every $v \in S_{o}, K(v, \cdot)$ coincides with $P_{\#}^{v} \mu$.

Proposition 21 Let $\mu$ be a measure on $\mathcal{M}$, and $K_{\mu}$ a probability kernel on $S_{o} \times \mathcal{B}(\mathbb{R})$ such that $\operatorname{CHR} \mu=\lambda_{o} \otimes K_{\mu}$. Then for $\lambda_{o}$-almost every $v \in S_{o}, K_{\mu}(v, \cdot)=P_{\#}^{v} \mu$.

All these derivations allow to link the Cartan-Hadamard Sliced-Wasserstein distance with the Radon transform defined with the corresponding projection (geodesic or horospherical). Then, $\operatorname{CHSW}_{p}(\mu, \nu)=0$ implies that for $\lambda_{o}$-almost every $v \in S_{o}, P_{\#}^{v} \mu=P_{\#}^{v} \nu$. Showing that the Radon transform is injective would allow to conclude that $\mu=\nu$.

Actually, here we derived two different Cartan-Hadamard Radon transforms. Using $P^{v}$ as the geodesic projection, the Radon transform integrates over geodesic subspaces of dimension $\operatorname{dim}(\mathcal{M})-1$. Such spaces are totally geodesic subspaces, and are related to the more general geodesic Radon transform (Rubin, 2003). In the case where the geodesic subspace is of dimension one, i.e. it integrates only over geodesics, this coincides with the X-ray transform, and it has been studied e.g. in (Lehtonen et al., 2018). Here, we are interested in the case of dimension $\operatorname{dim}(\mathcal{M})-1$, which, to the best of our knowledge, has only been studied in (Lehtonen, 2016) in the case where $\operatorname{dim}(\mathcal{M})=2$ and hence when the geodesic Radon transform and the X-ray transform coincide. However, no results on the injectivity over the sets of measures is yet available. In the case where $P^{v}$ is the Busemann projection, the set of integration is a horosphere. General horospherical Radon transforms on Cartan-Hadamard manifolds have not yet been studied to the best of our knowledge.

Link with the Wasserstein Distance. An important property of the Sliced-Wasserstein distance on Euclidean spaces is that it is topologically equivalent to the Wasserstein distance, i.e. it metrizes the weak convergence. Such results rely on properties of the Fourier transform which do not translate straightforwardly to manifolds. Hence, deriving such results will require further investigations. We note that a possible lead for the horospherical case is the connection between the Busemann function and the Fourier-Helgason transform (Biswas, 2018; Sonoda et al., 2022). Using that the projections are Lipschitz functions, we can still show that CHSW is a lower bound of the geodesic Wasserstein distance.

Proposition 22 Let $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$, then

$$
\begin{equation*}
\operatorname{CHSW}_{p}^{p}(\mu, \nu) \leq W_{p}^{p}(\mu, \nu) \tag{78}
\end{equation*}
$$

This property means that it induces a weaker topology compared to the Wasserstein distance, which can be computationally beneficial but which also comes with less discriminative powers (Nadjahi et al., 2020).

Hilbert Embedding. CHSW also comes with the interesting properties that it can be embedded in Hilbert spaces. This is in contrast with the Wasserstein distance which is known to not be Hilbertian (Peyré et al., 2019, Section 8.3) except in one dimension where it coincides with its sliced counterpart.

Proposition 23 Let $p \geq 1$ and $\mathcal{H}=L^{p}\left([0,1] \times S_{o}\right.$, Leb $\left.\otimes \lambda_{o}\right)$. We define $\Phi$ as

$$
\begin{align*}
& \Phi: \mathcal{P}_{p}(\mathcal{M}) \rightarrow \mathcal{H} \\
& \quad \mu \mapsto\left((q, v) \mapsto F_{P_{\#}^{v} \mu}^{-1}(q)\right) \tag{79}
\end{align*}
$$

where $F_{P}^{v} \mu$ is the quantile function of $P_{\#}^{v} \mu$. Then $\mathrm{CHSW}_{p}$ is Hilbertian and for all $\mu, \nu \in$ $\mathcal{P}_{p}(\mathcal{M})$,

$$
\begin{equation*}
\operatorname{CHSW}_{p}^{p}(\mu, \nu)=\|\Phi(\mu)-\Phi(\nu)\|_{\mathcal{H}}^{p} \tag{80}
\end{equation*}
$$

This is a nice property which allows to define a valid positive definite kernel for measures such as the Gaussian kernel (Jayasumana et al., 2015, Theorem 6.1), and hence to use kernel methods (Hofmann et al., 2008). This can allow for example to perform distribution clustering, classification (Kolouri et al., 2016; Carriere et al., 2017) or regression (Meunier et al., 2022).

Proposition 24 Define the kernel $K: \mathcal{P}_{2}(\mathcal{M}) \times \mathcal{P}_{2}(\mathcal{M}) \rightarrow \mathbb{R}$ as $K(\mu, \nu)=\exp (-$ $\left.\gamma \operatorname{CHSW}_{2}^{2}(\mu, \nu)\right)$ for $\gamma>0$. Then $K$ is a positive definite kernel.

Proof Apply (Jayasumana et al., 2015, Theorem 6.1).

Bonet et al. (2023b) notably used this property to perform brain-age regression by first representing M/EEG data as a probability distribution of SPD matrices. And then by plugging the Gaussian kernel constructed with the Cartan-Hadamard Sliced-Wasserstein on the space of SPDs endowed with the Log-Euclidean metric, that we presented in Section 4.3.2, into the kernel Ridge regression method.

Note that to show that the Gaussian kernel is universal, i.e. that the resulting Reproducing Kernel Hilbert Space (RKHS) is powerful enough to approximate any continuous function (Meunier et al., 2022), we would need additional results such as that it metrizes the weak convergence and that $\mathrm{CHSW}_{2}$ is a distance, as shown in (Meunier et al., 2022, Proposition 7).

### 5.2 Topology for Pullback Euclidean Manifolds

In this section, we focus on particular Hadamard manifolds for which the metric is a pullback Euclidean metric, which allows inheriting the properties of Euclidean spaces, and deriving additional properties of the corresponding Sliced-Wasserstein distance. This covers for example the space of SPD matrices with Pullback Euclidean metrics studied in Section 4.3.2 as well as the Mahalanobis manifold introduced in Section 4.1.

Let $\mathcal{N}$ be an Euclidean space with $\langle\cdot, \cdot\rangle$ its inner product and $\|\cdot\|$ the associated norm. Let $\phi: \mathcal{M} \rightarrow \mathcal{N}$ be a diffeomorphism and denote $\left(\mathcal{M}, g^{\phi}\right)$ the resulting Riemannian manifold (see Theorem 6 for more details). We recall that by Proposition 7, the projection of $x \in \mathcal{M}$ on the geodesic characterized by the direction $v \in S_{o}$ is of the form

$$
\begin{equation*}
P^{v}(x)=\left\langle\phi(x)-\phi(o), \phi_{*, o}(v)\right\rangle \tag{81}
\end{equation*}
$$

In this case, given the formula, we can link CHSW with the Euclidean SW with the integration made on $S_{\phi(o)}=\left\{v \in T_{\phi(o)} \mathcal{N},\|v\|_{\phi(o)}=1\right\}$ with respect to the measure $\lambda_{\phi(o)}$.

Lemma 25 Let $\left(\mathcal{M}, g^{\phi}\right)$ a pullback Euclidean Riemannian manifold and assume that $\lambda_{\phi(o)}=$ $\left(\phi_{*, o}\right)_{\#} \lambda_{o}$. Let $p \geq 1$ and $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$. Then,

$$
\begin{equation*}
\operatorname{CHSW}_{p}^{p}(\mu, \nu)=\int_{S_{\phi(o)}} W_{p}^{p}\left(Q_{\#}^{v} \phi_{\#} \mu, Q_{\#}^{v} \phi_{\#} \nu\right) \mathrm{d} \lambda_{\phi(o)}(v)=\operatorname{SW}_{p}^{p}\left(\phi_{\#} \mu, \phi_{\#} \nu\right) \tag{82}
\end{equation*}
$$

with $Q^{v}(x)=\langle x, v\rangle$ and $\mathrm{SW}_{p}$ the Euclidean Sliced-Wasserstein distance.

Using this simple lemma, we can leverage results known for the Euclidean SlicedWasserstein distance to CHSW on these particular spaces. First, we show that we recover the distance property by additionally showing the indiscernible property.

Proposition 26 Let $\left(\mathcal{M}, g^{\phi}\right)$ a pullback Euclidean Riemannian manifold. Let $p \geq 1$, then $\mathrm{CHSW}_{p}$ is a finite distance on $\mathcal{P}_{p}(\mathcal{M})$.

We can also obtain the important property that CHSW metrizes the weak convergence as the Wasserstein distance (Villani et al., 2009). This property was first shown for arbitrary measures in (Nadjahi et al., 2019) for the regular Euclidean SW.

Proposition 27 Let $\left(\mathcal{M}, g^{\phi}\right)$ a pullback Euclidean Riemannian manifold of dimension $d$. Let $p \geq 1,\left(\mu_{n}\right)_{n}$ a sequence in $\mathcal{P}_{p}(\mathcal{M})$ and $\mu \in \mathcal{P}_{p}(\mathcal{M})$. Then, $\lim _{n \rightarrow \infty} \operatorname{CHSW}_{p}\left(\mu_{n}, \mu\right)=0$ if and only if $\left(\mu_{n}\right)_{n}$ converges weakly towards $\mu$.

With these additional properties, we can also show that the corresponding Gaussian kernel is universal applying (Meunier et al., 2022, Theorem 4). In addition to Proposition 22, we show that we can lower bound CHSW with the Wasserstein distance when the measures are compactly supported.

Proposition 28 Let $\left(\mathcal{M}, g^{\phi}\right)$ a pullback Euclidean Riemannian manifold of dimension d. Let $p \geq 1, r>0$ a radius and $B(o, r)=\left\{x \in \mathcal{M}, d_{\mathcal{M}}(x, o) \leq r\right\}$ a closed ball. Then there exists a constant $C_{d, p, r}$ such that for all $\mu, \nu \in \mathcal{P}_{p}(B(o, r))$,

$$
\begin{equation*}
W_{p}^{p}(\mu, \nu) \leq C_{d, p, r} \operatorname{CHSW}_{p}(\mu, \nu)^{\frac{1}{d+1}} \tag{83}
\end{equation*}
$$

### 5.3 Statistical Properties

Sample Complexity. In practical settings, we usually cannot directly compute the closed-form between $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$, but we have access to samples $x_{1}, \ldots, x_{n} \sim \mu$ and $y_{1}, \ldots, y_{n} \sim \nu$. Then, it is common practice to estimate the discrepancy with the plug-in estimator $\mathrm{CHSW}_{p}^{p}\left(\hat{\mu}_{n}, \hat{\nu}_{n}\right)$ (Manole et al., 2021, 2022; Niles-Weed and Rigollet, 2022) where $\hat{\mu}_{n}=\frac{1}{n} \sum_{i=1}^{n} \delta_{x_{i}}$ and $\hat{\nu}_{n}=\frac{1}{n} \sum_{i=1}^{n} \delta_{y_{i}}$ are empirical estimations of the measures. We are interested in characterizing the speed of convergence of the plug-in estimator towards the true distance. Relying on the proof of Nadjahi et al. (2020), we derive in Proposition 29 the sample complexity of CHSW. As in the Euclidean case, we find that the sample complexity does not depend on the dimension, which is an important and appealing property of sliced divergences (Nadjahi et al., 2020) compared to the Wasserstein distance, which has a sample complexity in $O\left(n^{-1 / d}\right)$ (Niles-Weed and Rigollet, 2022).

Proposition 29 Let $p \geq 1, q>p$ and $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$. Denote $\hat{\mu}_{n}$ and $\hat{\nu}_{n}$ their counterpart empirical measures and $M_{q}(\mu)=\int_{\mathcal{M}} d(x, o)^{q} \mathrm{~d} \mu(x)$ their moments of order $q$. Then, there exists $C_{p, q}$ a constant depending only on $p$ and $q$ such that

$$
\begin{equation*}
\mathbb{E}\left[\left|\operatorname{CHSW}_{p}\left(\hat{\mu}_{n}, \hat{\nu}_{n}\right)-\operatorname{CHSW}_{p}(\mu, \nu)\right|\right] \leq 2 \alpha_{n, p, q} C_{p, q}^{1 / p}\left(M_{q}(\mu)^{1 / q}+M_{q}(\nu)^{1 / q}\right) \tag{84}
\end{equation*}
$$

where

$$
\alpha_{n, p, q}= \begin{cases}n^{-1 /(2 p)} & \text { if } q>2 p  \tag{85}\\ n^{-1 /(2 p)} \log (n)^{1 / p} & \text { if } q=2 p \\ n^{-(q-p) /(p q)} & \text { if } q \in(p, 2 p)\end{cases}
$$

Table 1: Accuracy on Document Classification

|  | BBCSport | Movies | Goodreads genre | Goodreads like |
| :---: | :---: | :---: | :---: | :---: |
| $W_{2}$ | 94.55 | 74.44 | 56.18 | 71.00 |
| $W_{A}$ | 98.36 | 76.04 | 56.81 | 68.37 |
| $\mathrm{SW}_{2}$ | $89.42_{ \pm 0.89}$ | $67.27_{ \pm 0.69}$ | $50.01_{ \pm 1.21}$ | $65.90_{ \pm 0.17}$ |
| $\mathrm{SW}_{2, A}$ | $97.58_{ \pm 0.04}$ | $76.55_{ \pm 0.11}$ | $57.03_{ \pm 0.68}$ | $67.54_{ \pm 0.14}$ |

This property is very appealing in practical settings as it allows to use the same number of samples while having the same convergence rate in any dimension. In practice though, we cannot compute exactly $\operatorname{CHSW}_{p}\left(\hat{\mu}_{n}, \hat{\nu}_{n}\right)$ as the integral on $S_{o}$ w.r.t. the uniform measure $\lambda_{o}$ is intractable.

Projection Complexity. Thus, to compute it in practice, we usually rely on a MonteCarlo approximation, by drawing $L \geq 1$ directions $v_{1}, \ldots, v_{L}$ and approximating the distance by $\widehat{\mathrm{CHSW}}_{p, L}$ defined between $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$ as

$$
\begin{equation*}
\widehat{\operatorname{CHSW}}_{p, L}^{p}(\mu, \nu)=\frac{1}{L} \sum_{\ell=1}^{L} W_{p}^{p}\left(P_{\#}^{v_{\ell}} \mu, P_{\#}^{v_{\ell}} \nu\right) \tag{86}
\end{equation*}
$$

In the following proposition, we derive the Monte-Carlo error of this approximation, and we show that we recover the classical rate of $O(1 / \sqrt{L})$.

Proposition 30 Let $p \geq 1, \mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$. Then, the error made by the Monte-Carlo estimate of $\mathrm{CHSW}_{p}$ with $L$ projections can be bounded as follows

$$
\begin{equation*}
\mathbb{E}_{v}\left[\left|\widehat{\operatorname{CHSW}}_{p, L}^{p}(\mu, \nu)-\operatorname{CHSW}_{p}^{p}(\mu, \nu)\right|\right]^{2} \leq \frac{1}{L} \operatorname{Var}_{v}\left[W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right)\right] \tag{87}
\end{equation*}
$$

We note that here the dimension actually intervenes in the term of variance.

Computational Complexity. As we project on the real line, the complexity of computing the Wasserstein distances between each projected distribution is in $O(\operatorname{Ln} \log n)$. Then, we add the complexity of computing the projections, which will depend on the spaces and whether or not we have access to a closed-form.

## 6 Application of Cartan-Hadamard Sliced-Wasserstein Distances

In this section, we provide some illustrations of Cartan-Hadamard Sliced-Wasserstein distances on manifolds which were not yet studied in previous works. We note that Bonet et al. (2023a) used HSW to perform deep classification with prototypes on Hyperbolic spaces, while Bonet et al. (2023b) used SPDSW to perform domain adaptation for Brain Computer Interface and to perform Brain-Age regression by leveraging the Gaussian kernel from Proposition 24 and plugging it into Kernel Ridge regression. Here, we first provide an experiment using the Mahalanobis Sliced-Wasserstein distance to classify documents, and then an experiment on a product of Cartan-Hadamard manifolds to compare datasets.

### 6.1 Document classification with Mahalanobis Sliced-Wasserstein

We propose here to perform an experiment of document classification. Suppose that we have $N$ documents $D_{1}, \ldots, D_{N}$. Following the work of Kusner et al. (2015), we represent

Table 2: Runtimes on Document Classification

|  |  | BBCSport | Movies | Goodreads |
| :---: | :---: | :---: | :---: | :---: |
| $W_{A}$ | Average $\left(\cdot 10^{-3} \mathrm{~s}\right)$ | $3.29_{ \pm 1.61}$ | $6.78_{ \pm 2.74}$ | $440.30_{ \pm 259}$ |
|  | Full $(\mathrm{s})$ | 891 | 13544 | 221252 |
|  | Average $\left(\cdot 10^{-3} \mathrm{~s}\right)$ | $2.45_{ \pm 0.008}$ | $2.47_{ \pm 0.04}$ | $2.5_{ \pm 0.12}$ |
| $\mathrm{SW}_{2, A}$ | Full (s) | 665 | 4931 | 1256 |

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-29.jpg?height=358&width=1440&top_left_y=661&top_left_x=342)

Figure 7: Runtime between each pair of documents.

each document $D_{k}$ as a distribution over words. More precisely, denote $x_{1} \ldots, x_{n} \in \mathbb{R}^{d}$ the set of words, embedded using word2vec (Mikolov et al., 2013) in dimension $d=300$. Then, $D_{k}$ is represented by the probability distribution $D_{k}=\sum_{i=1}^{n} w_{i}^{k} \delta_{x_{i}}$, where $w_{i}^{k}$ represents the frequency of the word $x_{i}$ in $D_{k}$ normalized such that $\sum_{i=1}^{n} w_{i}^{k}=1$.

Then, following (Huang et al., 2016), we learn a matrix $A \in S_{d}^{++}(\mathbb{R})$ using the Neighborhood Component Analysis (NCA) method (Goldberger et al., 2004) combined with the Word Centroid Distance (WCD), defined as $\mathrm{WCD}_{A}\left(D_{k}, D_{\ell}\right)^{2}=\left(X w^{k}-X w^{\ell}\right)^{T} A\left(X w^{k}-X w^{\ell}\right)$ with $X=\left(x_{1}, \ldots, x_{n}\right) \in \mathbb{R}^{d \times n}$. We use the pytorch-metric-learning library (Musgrave et al., 2020) to learn $A$.

Once $A$ is learned, we compute the distance between documents using the Wasserstein distance or the Sliced-Wasserstein distance with Mahalanobis ground cost distance, i.e. $d_{A}(x, y)^{2}=(x-y)^{T} A(x-y)$. Once we compute the distance between each documents $\left(d\left(D_{k}, D_{\ell}\right)\right)_{k, \ell}$, we use a $k$-nearest neighbor classifier. On Table 1, we report the results for the BBCSport dataset (Kusner et al., 2015), the Movies reviews dataset (Pang et al., 2002) and the Goodread dataset (Maharjan et al., 2017). All the datasets are split in 5 different train/test sets. The number of neighbors is found using a cross validation. We compare the results when using the regular Wasserstein and Sliced-Wasserstein distances, i.e. with $A=I_{d}$, and when learning $A$ using NCA with the WCD metric. The Wasserstein distance is computed using the Python Optimal Transport library POT (Flamary et al., 2021). The results for SW are averaged over 3 runs and SW is approximated with $L=500$ projections.

With this simple initialization, we observe that the results obtained with the Mahalanobis Sliced-Wasserstein distance become very competitive with the ones obtained using the Wasserstein distance with the Mahalanobis ground cost. We note that the results might be further improved by performing then a NCA with $W_{A}$ or $\mathrm{SW}_{2, A}$ as distances in the same spirit of (Huang et al., 2016). Here, we just use an initialization through WCD as a proof of concept to demonstrate how much it can already improve the results when using SW with a carefully chosen groundcost distance.

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-30.jpg?height=521&width=705&top_left_y=336&top_left_x=360)

(a) SW.

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-30.jpg?height=518&width=696&top_left_y=332&top_left_x=1083)

(b) Product HCHSW.

Figure 8: Comparison between SW between the datasets and CHSW between the datasets embedded on $\mathbb{R}^{d_{x}} \times \mathbb{R}^{d_{y}}$. Results are averaged over 100 draws of projections.

We showcase the computational benefits of using the Sliced-Wasserstein distance compared to the Wasserstein distance on Figure 7 by plotting the runtime of comparing each pair of documents and on Table 2 with the full runtimes. We note that the Wasserstein distance is computed on CPU while the Sliced-Wasserstein distance is implemented in Pytorch and uses GPU. We used as CPU an Intel Xeon 4214 and as GPU a Titan RTX. We observe a computational gain even on small scale datasets where the documents contain few words, and therefore for which the underlying representative distributions contain few samples. For datasets with distributions with a larger number of samples such as goodreads, the computational benefits are pretty big. We sum up the statistics of the different datasets in Table 3 .

### 6.2 Datasets Comparisons with Sliced-Wasserstein on a Product Manifold

Assume we have datasets defined as sets of feature-label pairs $(x, y) \in \mathcal{X} \times \mathcal{Y}$ (Alvarez-Melis and Fusi, 2020), where the samples are in $\mathbb{R}^{d_{x}}$ and the labels are embedded in a Hyperbolic space $\mathbb{H}^{d_{y}}$. Then, a dataset $D_{i}$ can then be seen as a probability distribution on $\mathbb{R}^{d_{x}} \times \mathbb{H}^{d_{y}}$ which we can compare using CHSW on product manifolds.

We assume that the datasets are already embedded in such spaces. In practice, such embedding could come up for instance when we are given image-text pairs, which could be embedded both in Hyperbolic spaces e.g. using (Desai et al., 2023), or for more classical datasets using label embeddings methods (Akata et al., 2015).

Here, to get a dataset represented in $\mathcal{P}\left(\mathbb{R}^{d_{x}} \times \mathbb{H}^{d_{y}}\right)$, we follow (Liu et al., 2022) and use a multidimensional scaling (MDS) method in hyperbolic spaces (Walter, 2004; Cvetkovski and Crovella, 2011) to get an embedding $\psi: \mathcal{P}\left(\mathbb{R}^{d_{x}}\right) \rightarrow \uplus^{d_{y}}$ into the hyperbolic space such that, for $\nu_{y}$ denoting the conditional probability distribution of samples in $\mathbb{R}^{d_{x}}$ with labels $y \in \mathcal{Y}$

$$
\begin{equation*}
W_{2}^{2}\left(\nu_{y}, \nu_{y^{\prime}}\right) \approx \alpha \cdot d_{\sharp H}\left(\psi\left(\nu_{y}\right), \psi\left(\nu_{y^{\prime}}\right)\right)^{2} \tag{88}
\end{equation*}
$$

with $\alpha$ some scaling parameter. To find this embedding, we minimize the absolute different squared loss (Cvetkovski and Crovella, 2011) defined as, for an original distance matrix
$\Delta=\left(\delta_{i, j}\right)_{i, j}$ and a scaling factor $\sqrt{\alpha}$,

$$
\begin{equation*}
\forall z_{1}, \ldots, z_{n} \in \mathbb{L}^{d_{y}}, \mathcal{L}(z)=\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left(d_{\mathbb{\unrhd}}\left(z_{i}, z_{j}\right)-\sqrt{\alpha} \delta_{i j}\right)^{2} \tag{89}
\end{equation*}
$$

To improve the numerical stability, we perform the optimization in the tangent space following (Mishne et al., 2023) using the parametrization

$$
\begin{equation*}
z_{i}=\exp _{x^{0}}\left(\left(0, \tilde{z}_{i}\right)\right)=\left(\cosh \left(\left\|\tilde{z}_{i}\right\|\right), \sinh \left(\left\|\tilde{z}_{i}\right\|\right) \frac{\tilde{z}_{i}}{\left\|\tilde{z}_{i}\right\|}\right) \tag{90}
\end{equation*}
$$

for $\tilde{z}_{i} \in \mathbb{R}^{d_{y}-1}$, and then performing the optimization in the Euclidean space.

We focus here on *NIST datasets which include MNIST (LeCun and Cortes, 2010), EMNIST (Cohen et al., 2017), FashionMNIST (Xiao et al., 2017), KMNIST (Clanuwat et al., 2018) and USPS (Hull, 1994). We plot on Figure 8 the matrix distance obtained between the ${ }^{*} N I S T$ datasets either using SW between the datasets seen only through their features, i.e. with $D_{i} \in \mathcal{P}\left(\mathbb{R}^{d_{x}}\right)$, and using CHSW on the space $\mathcal{P}\left(\mathbb{R}^{d_{x}} \times \mathbb{L}^{d_{y}}\right)$ where the labels were embedded on $\mathbb{}^{d_{y}}$ using the method described in the previous paragraph with a scaling of $\sqrt{\alpha}=0.1$ and $d_{y}=10$. We observe that when the labels are not taken into account, the USPS and MNIST datasets have a huge discrepancy between them. However, when taking into account the labels, we recover that these two datasets are in fact more similar as they both represent numbers. Thus, we argue that using the sliced distance on the product dataset in order to take into account the labels provides better comparisons of the datasets. Furthermore, from a computation point of view, CHSW on the product manifold is much cheaper compared to e.g. computing the Wasserstein distance. On our experiments, computing the full distance matrix with CHSW took in average 0.05s against 120s to compute the Wasserstein distance, where we used here only 10000 samples of the datasets.

## 7 Cartan-Hadamard Sliced-Wasserstein Flows

We propose here to derive the Wasserstein gradient flows of the CHSW distances along with approximated non-parametric particle schemes. We provide first the results on general Hadamard manifolds and then we specify them to Mahalanobis manifolds, Hyperbolic spaces and SPDs endowed with the Log-Euclidean metric. The proofs of this section are postponed to Appendix E.

### 7.1 Wasserstein Gradient Flows

First Variations. Being discrepancies on Hadamard manifolds, CHSW discrepancies can be used to learn parametric or empirical distributions through their minimization. A possible solution is to leverage Wasserstein gradient flows (Ambrosio et al., 2005; Santambrogio, 2017) of $\mathcal{F}(\mu)=\frac{1}{2} \operatorname{CHSW}_{2}^{2}(\mu, \nu)$ where $\nu$ is some target distribution. Approximating this flow would then allow to provide new samples from $\nu$. Computing such a flow requires first computing the first variations of the given functional. As a first step towards computing Wasserstein gradient flows of CHSW on Hadamard spaces, and analyzing them, we derive in Proposition 31 the first variation of $\mathcal{F}$.

Proposition 31 Let $K$ be a compact subset of $\mathcal{M}, \mu, \nu \in \mathcal{P}_{2}(K)$ with $\mu \ll$ Vol. Let $v \in S_{o}$, denote $\psi_{v}$ the Kantorovich potential between $P_{\#}^{v} \mu$ and $P_{\#}^{v} \nu$ for the cost $c(x, y)=\frac{1}{2}|x-y|^{2}$ for $x, y \in \mathbb{R}$. Let $\xi$ be a diffeomorphic vector field on $K$ and denote for all $\epsilon \geq 0, T_{\epsilon}: K \rightarrow \mathcal{M}$ defined as $T_{\epsilon}(x)=\exp _{x}(\epsilon \xi(x))$ for all $x \in K$. Then,

$$
\begin{align*}
& \lim _{\epsilon \rightarrow 0^{+}} \frac{\operatorname{CHSW}_{2}^{2}\left(\left(T_{\epsilon}\right)_{\#} \mu, \nu\right)-\operatorname{CHSW}_{2}^{2}(\mu, \nu)}{2 \epsilon} \\
& =\int_{S_{o}} \int_{\mathcal{M}} \psi_{v}^{\prime}\left(P^{v}(x)\right)\left\langle\operatorname{grad}_{\mathcal{M}} P^{v}(x), \xi(x)\right\rangle_{x} \mathrm{~d} \mu(x) \mathrm{d} \lambda_{o}(v) \tag{91}
\end{align*}
$$

In the Euclidean case, we recover well the first variation formula for SW first derived in (Bonnotte, 2013, Proposition 5.1.7) as in this case, for $x \in \mathbb{R}^{d}, T_{\epsilon}(x)=x+\epsilon \xi(x)$, and for $\theta \in S^{d-1}, P^{\theta}(x)=\langle x, \theta\rangle$ and thus $\operatorname{grad} P^{\theta}(x)=\nabla P^{\theta}(x)=\theta$, and we recover

$$
\begin{equation*}
\lim _{\epsilon \rightarrow 0^{+}} \frac{\operatorname{SW}_{2}^{2}\left((\operatorname{Id}+\epsilon \xi)_{\#} \mu, \nu\right)-\operatorname{SW}_{2}^{2}(\mu, \nu)}{2 \epsilon}=\int_{S^{d-1}} \int_{\mathbb{R}^{d}} \psi_{\theta}^{\prime}\left(P^{\theta}(x)\right)\langle\theta, \xi(x)\rangle \mathrm{d} \mu(x) \mathrm{d} \lambda(\theta) \tag{92}
\end{equation*}
$$

Cartan-Hadamard Sliced-Wasserstein Flow. Given the first variation, we can derive the Wasserstein gradient flow of $\mathcal{F}(\mu)=\frac{1}{2} \operatorname{CHSW}_{2}^{2}(\mu, \nu)$ as the continuity equation governed by the vector field $v_{t}$ obtained through the Wasserstein gradient

$$
\begin{equation*}
\forall x \in \mathcal{M}, v_{t}(x)=-\nabla_{W_{2}} \mathcal{F}\left(\mu_{t}\right)(x)=-\int_{S_{o}} \psi_{t, v}^{\prime}\left(P^{v}(x)\right) \operatorname{grad}_{\mathcal{M}} P^{v}(x) \mathrm{d} \lambda_{o}(v) \tag{93}
\end{equation*}
$$

with $\psi_{t, v}$ the Kantorovich potential between $P_{\#}^{v} \mu_{t}$ and $P_{\#}^{v} \nu$ such that $\psi_{t, v}^{\prime}(x)=x-$ $F_{P_{\#}^{u} \mu_{t}}^{-1}\left(F_{P_{\#}^{u} \nu}(x)\right)$, i.e. the Wasserstein gradient flow $\left(\mu_{t}\right)_{t \geq 0}$ of $\mathcal{F}$ is a solution (in the distributional sense) of

$$
\begin{equation*}
\partial_{t} \mu_{t}+\operatorname{div}\left(\mu_{t} v_{t}\right)=0 \tag{94}
\end{equation*}
$$

Forward Euler Scheme. To provide an algorithm to sample from $\nu$ by minimizing $\mathcal{F}(\mu)=\frac{1}{2} \mathrm{CHSW}_{2}^{2}(\mu, \nu)$ while following its Wasserstein gradient flow, there are several possible strategies of discretization of the flow. For instance, a solution could be to compute the backward Euler scheme, also known as the Jordan-Kinderlehrer-Otto (JKO) scheme from the seminal work of Jordan et al. (1998). This strategy has for example been used to minimize the Sliced-Wasserstein distance in (Bonet et al., 2022). Here, we propose instead to use the forward Euler scheme, which allows defining a particle scheme approximating the trajectory of the Wasserstein gradient flow. Such a strategy has been used to minimize different functionals such as the MMD (Arbel et al., 2019), the Kernel Stein Discrepancy (Korba et al., 2021) or the KL divergence (Fang et al., 2021; Wang et al., 2022). For SW, Liutkus et al. (2019) proposed to minimize SW with an entropy term, which required to use a McKean Vlasov SDE.

Let $\mu_{0} \in \mathcal{P}_{p}(\mathcal{M})$ and $\tau>0$. On a Riemannian manifold, analogously to the Riemannian gradient descent (Bonnabel, 2013), the forward Euler scheme becomes

$$
\begin{equation*}
\forall k \geq 0, \mu_{k+1}=\exp _{\text {Id }}\left(-\tau \nabla_{W_{2}} \mathcal{F}\left(\mu_{k}\right)\right)_{\#} \mu_{k} \tag{95}
\end{equation*}
$$

where $\nabla_{W_{2}} \mathcal{F}\left(\mu_{k}\right)(x)=-v_{k}(x)=\int_{S_{o}} \psi_{k, v}^{\prime}\left(P^{v}(x)\right) \operatorname{grad}_{\mathcal{M}} P^{v}(x) \mathrm{d} \lambda_{o}(v)$ for $x \in \mathcal{M}$ is the Wasserstein gradient. In the Euclidean case, we recover the usual forward Euler scheme $\mu_{k+1}=\left(\operatorname{Id}-\tau \nabla_{W_{2}} \mathcal{F}\left(\mu_{k}\right)\right)_{\#} \mu_{k}$

```
Algorithm 1 Wasserstein gradient flows of CHSW
    Input: $\left(y_{j}\right)_{j=1}^{n} \sim \nu, \mu_{0}, L$ the number of projections, $N$ the number of steps
    Sample $\left(x_{i}^{0}\right)_{i=1}^{n} \sim \mu_{0}$
    for $k=0$ to $N-1$ do
        Draw $v_{1}, \ldots, v_{L} \sim \lambda_{o}$
        Compute $\hat{x}_{i, \ell}^{k}=P^{v_{\ell}}\left(x_{i}^{k}\right), \hat{y}_{j, \ell}=P^{v_{\ell}}\left(y_{j}\right)$ for all $\ell \in\{1, \ldots, L\}$
        Define $P_{\#}^{v_{\ell}} \hat{\nu}=\frac{1}{n} \sum_{j=1}^{n} \delta_{\hat{y}_{j, \ell}}, P_{\#}^{v_{\ell}} \hat{\mu}_{k}=\frac{1}{n} \sum_{i=1}^{n} \delta_{\hat{x}_{i, \ell}^{k}}$
        Compute $\hat{z}_{i, \ell}^{k}=\hat{x}_{i, \ell}^{k}-F_{P_{\#}^{v} \hat{\nu}_{\hat{\nu}}^{v}}^{-1}\left(F_{P_{\#}^{v} \hat{\mu}_{k}}\left(\hat{x}_{i, \ell}^{k}\right)\right)$
        Compute $g_{\ell}\left(x_{i}^{k}\right)=\operatorname{grad}_{\mathcal{M}} P^{v_{\ell}}\left(x_{i}^{k}\right)$
        Compute $\hat{v}_{k}\left(x_{i}^{k}\right)=\frac{1}{L} \sum_{\ell=1}^{L}\left(\hat{x}_{i, \ell}^{k}-\hat{z}_{i, \ell}^{k}\right) g_{\ell}\left(x_{i}^{k}\right)$
        For all $i \in\{1, \ldots, n\}, x_{i}^{k+1}=\exp _{x_{i}^{k}}\left(\tau \hat{v}_{k}\left(x_{i}^{k}\right)\right)$
    end for
```

In practice, we approximate the Wasserstein gradient by first sampling $v_{1}, \ldots, v_{L} \sim \lambda_{o}$ and using

$$
\begin{equation*}
\forall x \in \mathcal{M}, \hat{v}_{k}(x)=-\frac{1}{L} \sum_{\ell=1}^{L} \psi_{v_{\ell}, k}^{\prime}\left(P^{v_{\ell}}(x)\right) \operatorname{grad}_{\mathcal{M}} P^{v_{\ell}}(x) \tag{96}
\end{equation*}
$$

where

$$
\begin{equation*}
\psi_{v, k}^{\prime}\left(P^{v}(x)\right)=P^{v}(x)-F_{P_{\# \nu}^{v}}^{-1}\left(F_{P_{\#}^{v} \mu_{k}}\left(P^{v}(x)\right)\right) \tag{97}
\end{equation*}
$$

Following (Liutkus et al., 2019), the cumulative distribution functions and the quantiles are approximated using linear interpolations between the true points ${ }^{3}$. Finally, the particle scheme is given by,

$$
\begin{equation*}
\forall k \geq 0, i \in\{1, \ldots, n\}, x_{i}^{k+1}=\exp _{x_{i}^{k}}\left(\tau \hat{v}_{k}\left(x_{i}^{k}\right)\right) \tag{98}
\end{equation*}
$$

We sum up the procedure in Algorithm 1.

### 7.2 Application to the Mahalanobis Manifold

For pullback Euclidean metrics, the Riemannian gradient can be obtained by using the inverse of the differential operator as stated in the following lemma.

Lemma 32 (Lemma 4 in (Chen et al., 2023a)) Let (M, $g^{\phi}$ ) be a Pullback Euclidean Riemannian manifold. For $f: \mathcal{M} \rightarrow \mathbb{R}$ a smooth map, the gradient is of the form

$$
\begin{equation*}
\forall x \in \mathcal{M}, \operatorname{grad}_{\mathcal{M}} f(x)=\phi_{*, x}^{-1}\left(\phi_{*, x}^{-*}(\nabla f(x))\right) \tag{99}
\end{equation*}
$$

For the Mahalanobis distance, i.e. for $\phi(x)=A^{\frac{1}{2}} x$ for any $x \in \mathbb{R}^{d}$ with $A \in S_{d}^{++}(\mathbb{R})$, the inverse of the differential is simply $\phi_{*, x}^{-1}(v)=A^{-\frac{1}{2}} v$, and we recall that the projection is $P^{v}(x)=x^{T} A v$ for $v \in S_{o}$. Thus the Riemannian gradient of the projection $P^{v}$ for $v \in S_{o}$ is

$$
\begin{equation*}
\operatorname{grad}_{\mathcal{M}} P^{v}(x)=A^{-\frac{1}{2}}\left(A^{-\frac{1}{2}}(A v)\right)=v \tag{100}
\end{equation*}
$$

3. using https://github.com/aliutkus/torchinterp1d
![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-34.jpg?height=406&width=1520&top_left_y=301&top_left_x=302)

Figure 9: Trajectories of Mahalanobis Sliced-Wasserstein flows using four SPD matrices $A_{t}$ along the geodesic between $I_{2}$ and a randomly chosen $A \in S_{d}^{++}(\mathbb{R})$. Ellipses represent the matrices $A_{t}$.

We recover the same gradient. But the matrix $A$ is still involved in the formula of the projection, which can change the trajectory of the particles. Choosing well the matrix $A$ can help improving the convergence of flows for ill conditioned problems, see e.g. (Duchi et al., 2011; Dong et al., 2023).

We illustrate on Figure 9 the effect on the trajectory when using a randomly sampled SPD matrix $A$ to specify the Mahalanobis distance compared to the classical Euclidean metric. We plot the trajectories for different SPDs obtained on the geodesic between $I_{2}$ and $A$, which is of the form $A_{t}=\exp (t \log (A))$ for $t \in[0,1]$ when using the Affine-Invariant metric.

### 7.3 Application to Hyperbolic Spaces

Here, we propose to minimize the Hyperbolic Sliced-Wasserstein distances in order to derive a new non-parametric scheme allowing to learn a distribution given its samples. We first recall how to compute the gradient on the Lorentz model.

Proposition 33 Let $f: \mathbb{L}_{K}^{d} \rightarrow \mathbb{R}$ and note $\bar{f}: \mathbb{R}^{d+1} \rightarrow \mathbb{R}$ a smooth extension on $\mathbb{R}^{d+1}$. Then, the gradient of $f$ at $x \in \mathbb{L}_{K}^{d}$ is

$$
\begin{equation*}
\operatorname{grad}_{\mathbb{L}_{K}^{d}} f(x)=\operatorname{Proj}_{x}^{K}(-K J \nabla \bar{f}(x)) \tag{101}
\end{equation*}
$$

where $J=\operatorname{diag}(-1,1, \ldots, 1)$ and

$$
\begin{equation*}
\operatorname{Proj}_{x}^{K}(z)=z-K\langle x, z\rangle_{\llbracket} x \tag{102}
\end{equation*}
$$

Proof We extend (Boumal, 2023, Proposition 7.7) to $\mathbb{L}_{K}^{d}$.

Then, leveraging Proposition 33, we derive the closed-forms of the gradients of the geodesic and horospherical projections, which allows deriving the forward Euler scheme of this functional, by plugging the different formulas in Equation (96).
![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-35.jpg?height=412&width=1484&top_left_y=308&top_left_x=339)

Figure 10: Log 2-Wasserstein between the target distribution and particles obtained from HSWFs (averaged over 5 runs).

Proposition 34 Let $v \in T_{x^{0}} \llbracket_{K}^{d} \cap S^{d}$ and $x \in \mathbb{\llbracket}_{K}^{d}$, then

$$
\begin{align*}
\operatorname{grad}_{\mathbb{L}_{K}^{d}} B^{v}(x) & =K \sqrt{-K}\left(K x-\frac{\sqrt{-K} x^{0}+v}{\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{Z}}}\right)  \tag{103}\\
\operatorname{grad}_{\mathbb{Q}_{K}^{d}} P^{v}(x) & =\frac{K^{2}\left(\left\langle x, x^{0}\right\rangle_{\mathbb{Q}} v-\langle x, v\rangle_{\mathbb{Z}} x^{0}\right)}{\langle x, v\rangle_{\mathbb{L}}^{2}+K\left\langle x, x^{0}\right\rangle_{\mathbb{L}}^{2}} \tag{104}
\end{align*}
$$

On $\mathbb{B}^{d}$, the gradient can be obtained by rescaling the Euclidean gradient with the inverse Poincaré ball metric (Nickel and Kiela, 2017) which is $\left(\frac{1+K\|x\|_{2}^{2}}{2}\right)^{2}$ (Park et al., 2021). Thus, we can also derive the corresponding formulas on the Poincaré ball. For example, for the Busemann function, we have

$$
\begin{equation*}
\nabla B^{\tilde{v}}(x)=2\left(\frac{x}{1-\|x\|_{2}^{2}}-\frac{\tilde{v}-x}{\|\tilde{v}-x\|_{2}^{2}}\right) \tag{105}
\end{equation*}
$$

and therefore its Riemannian gradient is

$$
\begin{equation*}
\operatorname{grad}_{\mathbb{B}_{K}^{d}} B^{\tilde{v}}(x)=\left(\frac{1+K\|x\|_{2}^{2}}{2}\right)^{2} \nabla B^{\tilde{v}}(x) \tag{106}
\end{equation*}
$$

On Figure 10, we plot the 2-Wasserstein distance between the target distribution and samples from the Hyperbolic Sliced-Wasserstein Flows on Hyperbolic space of curvature $K=-1$. We compare the evolution between GHSW, HHSW and SW (on the Poincaré ball for SW) on 4 different scenarios. The two first ones involve a target distribution which is a Wrapped Normal Distribution (WND) located either close to the center or to the border of the disk. The second ones involve a mixture of WND, with some modes either close to the border or to the center. HHSW and GHSW can be done both on the Lorentz model or the Poincaré ball. Using either model give similar results. As hyperparameters, we chose $n=500$ particles, a learning rate of $\tau=0.1$ with $N=200$ epochs for centered targets, and $\tau=0.5$ and $N=300$ epochs for bordered targets. We note that the three gradient flows perform likewise, with an advantage of speed for SW, which might be due to the fact that the minimization is done in the space of Euclidean probabilities, and thus does not take into account that the modes are actually on the border.
![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-36.jpg?height=244&width=1382&top_left_y=344&top_left_x=358)

(a) WND

(b) MWND

Figure 11: Trajectories of 50 particles when the target is the WND on the border or the Mixture of WND on the border.

We add on Figure 11a and Figure 11b trajectories for the border scenarios. When minimizing GHSW, particles tend to go to the modes by following the shortest path, while when minimizing HHSW, they tend to first go at the border before converging to the modes. As the distance on the border of the Poincaré disk are bigger than to the center, this may explain the observed slower convergence of HHSW in Figure 10.

### 7.4 Application to SPD matrices with the Log-Euclidean Metric

For SPDSW with the Log-Euclidean metric, the formula of the gradient can be derived using the inverse of the differential as stated in Lemma 32. We report the inverse of the differential form of the log in Lemma 35.

Lemma 35 Let $\phi: X \mapsto \log (X)$ and $X=U D U^{T} \in S_{d}^{++}(\mathbb{R})$ where $D=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{d}\right)$. Then, we have

$$
\begin{equation*}
\forall W \in T_{\phi(X)} S_{d}^{++}(\mathbb{R}), \phi_{*, X}^{-1}(W)=U \tilde{\Sigma}(W) U^{T} \tag{107}
\end{equation*}
$$

where $\tilde{\Sigma}(W)=U^{T} W U \oslash \Gamma$ with $\Gamma$ defined as in Lemma 12.

Finally, in Lemma 36, we report the gradient of the projection obtained with the LogEuclidean metric, which can be obtained using that the differential of the matrix log satisfies $\left\langle A, \log _{*, X}(V)\right\rangle_{F}=\left\langle\log _{*, X}(A), V\right\rangle_{F}$ for any $A, V \in S_{d}(\mathbb{R})$.

Lemma 36 Let $A \in S_{d}(\mathbb{R})$ and $X=U D U^{T} \in S_{d}^{++}(\mathbb{R})$ with $U=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{d}\right)$. Then,

$$
\begin{equation*}
\nabla P^{A}(X)=U \Sigma(A) U^{T} \tag{108}
\end{equation*}
$$

We now have all the tools to apply Algorithm 1 for the particular case of SPDSW. In Figure 12, trajectories plotted inside the $S_{2}^{++}(\mathbb{R})$ cone depict the evolution of the matrices along the gradient flow. The noisy behavior of some of them can be mostly explained by numerical instabilities arising from the different matrix operators used in the process, which require to use small step sizes.

## 8 Future Works and Discussions

In this article, we introduced formally a way to generalize the Sliced-Wasserstein distance on Riemannian manifolds of non-positive curvature and we specified this construction to different particular cases: pullback Euclidean metrics, Hyperbolic spaces, the space of Symmetric

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-37.jpg?height=469&width=637&top_left_y=302&top_left_x=733)

Figure 12: Trajectories of particles following the Wasserstein gradient flow of SPDSW.

Positive-Definite matrices and product of Hadamard manifolds. These new discrepancies can be computed very efficiently and scale to distributions composed of a large of number of samples in contrast to the computation of the Wasserstein distance. We also analyzed these constructions theoretically while providing new applications and non-parametric schemes to minimize them using Wasserstein gradient flows.

Further works might include studying other Hadamard manifolds for which we do not necessarily have a closed-form for the projections such as Siegel spaces (Cabanes, 2022) or extending this construction to more general manifolds such as Riemannian manifolds of non-negative curvature, Finsler manifolds (Shen, 2001) which have recently received some attention in Machine Learning (López et al., 2021; Pouplin et al., 2023; Lin et al., 2023), or more generally metric spaces.

For the projections, we studied two natural generalizations of the projection used in Euclidean spaces. We could also study other projections which do not follow geodesics subspaces or horospheres, but are well suited to Riemannian manifolds, in the same spirit of the Generalized Sliced-Wasserstein. Other subspaces could also be used, such as Hilbert curves (Bernton et al., 2019; Li et al., 2022) adapted to manifolds, or higher dimensional subspaces (Paty and Cuturi, 2019; Chami et al., 2021). Finally, we could also define other variations of CHSW such as max-CHSW for instance and more generally adapt many of the variants which have been proposed for SW to the case of Riemannian manifolds. Note also that we could plug these constructions into the framework introduced by Séjourné et al. (2023) in order to compare positive measures on Hadamard manifolds.

On the theoretical side, we still need to show that these Sliced-Wasserstein discrepancies are proper distances by showing the indiscernible property. It might also be interesting to study whether statistical properties for the Euclidean SW distance derived in e.g. (Nietert et al., 2022; Manole et al., 2022; Goldfeld et al., 2022; Xu and Huang, 2022; Xi and NilesWeed, 2022) still hold more generally for CHSW on any Cartan-Hadamard manifold, or to study the properties of the space of probabilities endowed with these distances, such as geodesic properties or the gradient flows in this space, as it was recently done in (CandauTilh, 2020; Bonet et al., 2022; Park and Slepčev, 2023; Kitagawa and Takatsu, 2023) for the Euclidean Sliced-Wasserstein distance.

## Acknowledgments and Disclosure of Funding

This research was funded by project DynaLearn from Labex CominLabs and Region Bretagne ARED DLearnMe, and by the project OTTOPIA ANR-20-CHIA-0030 of the French National Research Agency (ANR). Clément Bonet's research was partially supported by the center Hi! PARIS.

## Appendix A. Useful Lemmas

We derive here some lemmas which will be useful for the proofs.

Lemma 37 (Lemma 6 in (Paty and Cuturi, 2019)) Let $\mathcal{M}, \mathcal{N}$ be two Riemannian manifolds. Let $f: \mathcal{M} \rightarrow \mathcal{N}$ be a measurable map and $\mu, \nu \in \mathcal{P}(\mathcal{M})$. Then,

$$
\begin{equation*}
\Pi\left(f_{\#} \mu, f_{\#} \nu\right)=\left\{(f \otimes f)_{\#} \gamma, \gamma \in \Pi(\mu, \nu)\right\} \tag{109}
\end{equation*}
$$

Proof This is a straightforward extension of (Paty and Cuturi, 2019, Lemma 6).

Lemma 38 Let $(\mathcal{M}, g)$ be a Hadamard manifold with origin o. Let $v \in T_{o} \mathcal{M}$, then

1. the geodesic projection $P^{v}$ is 1-Lipschitz.
2. the Busemann function $B^{v}$ is 1-Lipschitz.

## Proof

1. By Proposition 2, we know that

$$
\begin{equation*}
\forall x, y \in \mathcal{M},\left|P^{v}(x)-P^{v}(y)\right|=d\left(\tilde{P}^{v}(x), \tilde{P}^{v}(y)\right) \tag{110}
\end{equation*}
$$

Moreover, by (Ballmann et al., 2006, Page 9), $\tilde{P}^{v}$ is 1-Lipschitz, so is $P^{v}$.

2. The Busemann function is 1-Lipschitz, see e.g. (Bridson and Haefliger, 2013, II. Proposition 8.22).

Lemma 39 Let $d$ be a metric on $\mathcal{M}$. Then, for any $p \geq 1$,

$$
\begin{equation*}
\forall x, y \in \mathcal{M}, d(x, y)^{p} \leq 2^{p-1}\left(d(x, o)^{p}+d(o, y)^{p}\right) \tag{111}
\end{equation*}
$$

Lemma 40 (Lemma 1 in (Rakotomamonjy et al., 2021)) Let $p \geq 1$ and $\eta \in \mathcal{P}_{p}(\mathbb{R})$. Denote $\tilde{M}_{q}(\eta)=\int|x|^{q} \mathrm{~d} \eta(x)$ the moments of order $q$ and assume that $M_{q}(\eta)<\infty$ for some $q>p$. Then, there exists a constant $C_{p, q}$ depending only on $p, q$ such that for all $n \geq 1$,

$$
\begin{equation*}
\mathbb{E}\left[W_{p}^{p}\left(\hat{\eta}_{n}, \eta\right)\right] \leq C_{p, q} \tilde{M}_{q}(\eta)^{p / q}\left(n^{-1 / 2} \mathbb{1}_{\{q>2 p\}}+n^{-1 / 2} \log (n) \mathbb{1}_{\{q=2 p\}}+n^{-(q-p) / q} \mathbb{1}_{\{q \in(p, 2 p)\}}\right) \tag{112}
\end{equation*}
$$

Lemma 41 Let $y \in \mathcal{M}$ and denote for all $x \in \mathcal{M}, f(x)=d(x, y)^{2}$. Then, $\operatorname{grad}_{\mathcal{M}} f(x)=$ $-2 \log _{x}(y)$.

For references about Lemma 41, see e.g. (Chewi et al., 2020, Appendix A) or (Goto and Sato, 2021).

## Appendix B. Proofs of Section 3

## B. 1 Proof of Proposition 2

Proof of Proposition 2 Let $x, y \in \mathcal{G}^{v}$. Then, there exists $s, t \in \mathbb{R}$ such that $x=\exp _{o}(s v)$ and $y=\exp _{o}(t v)$. By a simple calculation, we have on one hand that

$$
\begin{align*}
\operatorname{sign}\left(\left\langle\log _{o}(x), v\right\rangle_{o}\right) & =\operatorname{sign}\left(\left\langle\log _{o}\left(\exp _{o}(s v)\right), v\right\rangle_{o}\right) \\
& =\operatorname{sign}\left(s\|v\|_{o}^{2}\right)  \tag{113}\\
& =\operatorname{sign}(s)
\end{align*}
$$

using that $\log _{o} \circ \exp _{o}=\mathrm{Id}$. And similarly, $\operatorname{sign}\left(\left\langle\log _{o}(y), v\right\rangle_{o}\right)=\operatorname{sign}(t)$.

Then, by noting that $o=\exp _{o}(0)$, and recalling that $d(x, y)=d\left(\exp _{o}(t v), \exp _{o}(s v)\right)=$ $|t-s|$,

$$
\begin{align*}
\left|t^{v}(x)-t^{v}(y)\right| & =\mid \operatorname{sign}\left(\left\langle\log _{o}(x), v\right\rangle_{o}\right) d(x, o)-\operatorname{sign}\left(\left\langle\log _{o}(y), v\right\rangle_{o} d(y, o) \mid\right. \\
& =\left|\operatorname{sign}(s) d\left(\exp _{o}(s v), \exp _{o}(0)\right)-\operatorname{sign}(t) d\left(\exp _{o}(t v), \exp _{o}(0)\right)\right| \\
& =|\operatorname{sign}(s)| s|-\operatorname{sign}(t)| t||  \tag{114}\\
& =|s-t| \\
& =d(x, y)
\end{align*}
$$

## B. 2 Proof of Proposition 3

Proof of Proposition 3 We want to solve:

$$
\begin{equation*}
P^{v}(x)=\underset{t \in \mathbb{R}}{\operatorname{argmin}} d(\gamma(t), x)^{2} \tag{115}
\end{equation*}
$$

where $\gamma(t)=\exp _{o}(t v)$. For $t \in \mathbb{R}$, let $g(t)=d(\gamma(t), x)^{2}=f(\gamma(t))$ where $f(x)=d(x, y)^{2}$ for $x, y \in \mathcal{M}$. Then, by Lemma 41, we have for any $t \in \mathbb{R}$,

$$
\begin{align*}
g^{\prime}(t)=0 & \Longleftrightarrow\left\langle\gamma^{\prime}(t), \operatorname{grad}_{\mathcal{M}} f(\gamma(t))\right\rangle_{\gamma(t)}=0 \\
& \Longleftrightarrow\left\langle\gamma^{\prime}(t),-2 \log _{\gamma(t)}(x)\right\rangle_{\gamma(t)}=0 \tag{116}
\end{align*}
$$

## B. 3 Proof of Proposition 4

Proof of Proposition 4 First, we note that $P^{v}=t^{v} \circ \tilde{P}^{v}$. Then, by using Lemma 37 which states that $\Pi\left(f_{\#} \mu, f_{\#} \nu\right)=\left\{(f \otimes f)_{\#} \gamma, \gamma \in \Pi(\mu, \nu)\right\}$ for any $f$ measurable, as well
as that by Proposition $2,\left|t^{v}(x)-t^{v}(y)\right|=d(x, y)$, we have:

$$
\begin{align*}
W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) & =\inf _{\gamma \in \Pi\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right)} \int_{\mathbb{R} \times \mathbb{R}}|x-y|^{p} \mathrm{~d} \gamma(x, y) \\
& =\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathbb{R} \times \mathbb{R}}|x-y|^{p} \mathrm{~d}\left(P^{v} \otimes P^{v}\right) \# \gamma(x, y) \\
& =\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathcal{M} \times \mathcal{M}}\left|P^{v}(x)-P^{v}(y)\right|^{p} \mathrm{~d} \gamma(x, y) \\
& =\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathcal{M} \times \mathcal{M}}\left|t^{v}\left(\tilde{P}^{v}(x)\right)-t^{v}\left(\tilde{P}^{v}(y)\right)\right|^{p} \mathrm{~d} \gamma(x, y)  \tag{117}\\
& =\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathcal{M} \times \mathcal{M}} d\left(\tilde{P}^{v}(x), \tilde{P}^{v}(y)\right)^{p} \mathrm{~d} \gamma(x, y) \\
& =\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathcal{M} \times \mathcal{M}} d(x, y)^{p} \mathrm{~d}\left(\tilde{P}^{v} \otimes \tilde{P}^{v}\right)_{\#} \gamma(x, y) \\
& =\inf _{\gamma \in \Pi\left(\tilde{P}_{\#}^{v} \mu, \tilde{P}_{\#}^{v} \nu\right)} \int_{\mathcal{G}^{v} \times \mathcal{G}^{v}} d(x, y)^{p} \mathrm{~d} \gamma(x, y) \\
& =W_{p}^{p}\left(\tilde{P}_{\#}^{v} \mu, \tilde{P}_{\#}^{v} \nu\right)
\end{align*}
$$

Now, let us show the results when using the Busemann projection. Let $v \in T_{o} \mathcal{M}$ such that $\|v\|_{o}=1$, and recall that $\tilde{B}^{v}(x)=\exp _{o}\left(-B^{v}(x) v\right)$. First, let us compute $t^{v} \circ \tilde{B}^{v}$ :

$$
\begin{align*}
\forall x \in \mathcal{M}, t^{v}\left(\tilde{B}^{v}(x)\right) & =\operatorname{sign}\left(\left\langle\log _{o}\left(\tilde{B}^{v}(x)\right), v\right\rangle_{o}\right) d\left(\tilde{B}^{v}(x), o\right) \\
& =\operatorname{sign}\left(\left\langle\log _{o}\left(\exp _{o}\left(-B^{v}(x) v\right)\right), v\right\rangle_{o}\right) d\left(\exp _{o}\left(-B^{v}(x) v\right), \exp _{o}(0)\right) \\
& =\operatorname{sign}\left(-B^{v}(x)\|v\|_{o}^{2}\right) d\left(\exp _{o}\left(-B^{v}(x) v\right), \exp _{o}(0)\right)  \tag{118}\\
& =\operatorname{sign}\left(-B^{v}(x)\right)\left|-B^{v}(x)\right| \\
& =-B^{v}(x)
\end{align*}
$$

Then, using the same computation as before, we get

$$
\begin{equation*}
W_{p}^{p}\left(B_{\#}^{v} \mu, B_{\#}^{v} \nu\right)=W_{p}^{p}\left(\tilde{B}_{\#}^{v} \mu, \tilde{B}_{\#}^{v} \nu\right) \tag{119}
\end{equation*}
$$

## Appendix C. Proofs of Section 4

## C. 1 Proof of Proposition 7

## Proof of Proposition 7

Geodesic projection. Let $x \in \mathcal{M}$. Denote $f: \mathbb{R} \rightarrow \mathbb{R}$ such that

$$
\begin{align*}
f(t) & =d_{\mathcal{M}}(\gamma(t), x)^{2} \\
& =d_{\mathcal{M}}\left(\phi^{-1}\left(\phi(o)+t \phi_{*, o}(v)\right), x\right)^{2} \\
& =\left\|\phi\left(\phi^{-1}\left(\phi(o)+t \phi_{*, o}(v)\right)\right)-\phi(x)\right\|^{2}  \tag{120}\\
& =t^{2}\left\|\phi_{*, o}(v)\right\|^{2}-2 t\left\langle\phi(x)-\phi(o), \phi_{*, o}(v)\right\rangle+\|\phi(o)-\phi(x)\|^{2} \\
& =t^{2}-2 t\left\langle\phi(x)-\phi(o), \phi_{*, o}(v)\right\rangle+\|\phi(o)-\phi(x)\|^{2},
\end{align*}
$$

using in the last line that $\left\|\phi_{*, o}(v)\right\|^{2}=1$ since $v \in S_{o}$. Then,

$$
\begin{equation*}
f^{\prime}(t)=0 \Longleftrightarrow t=\left\langle\phi(x)-\phi(o), \phi_{*, o}(v)\right\rangle \tag{121}
\end{equation*}
$$

Therefore,

$$
\begin{equation*}
P^{v}(x)=\underset{t \in \mathbb{R}}{\operatorname{argmin}} f(t)=\left\langle\phi(x)-\phi(o), \phi_{*, o}(v)\right\rangle \tag{122}
\end{equation*}
$$

Busemann function. First, following (Bridson and Haefliger, 2013), we have for all $x \in \mathcal{M}$,

$$
\begin{equation*}
B^{v}(x)=\lim _{t \rightarrow \infty}\left(d_{\mathcal{M}}\left(\gamma_{v}(t), x\right)-t\right)=\lim _{t \rightarrow \infty} \frac{d_{\mathcal{M}}\left(\gamma_{v}(t), x\right)^{2}-t^{2}}{2 t} \tag{123}
\end{equation*}
$$

denoting $\gamma_{v}: t \mapsto \phi^{-1}\left(\phi(o)+t \phi_{*, o}(v)\right)$ the geodesic line associated to $\mathcal{G}^{v}$. Then, we get

$$
\begin{align*}
\frac{d_{\mathcal{M}}\left(\gamma_{v}(t), x\right)^{2}-t^{2}}{2 t} & =\frac{1}{2 t}\left(\left\|\phi\left(\gamma_{v}(t)\right)^{2}-\phi(x)\right\|^{2}-t^{2}\right) \\
& =\frac{1}{2 t}\left(\left\|\phi(o)+t \phi_{*, o}(v)-\phi(x)\right\|^{2}-t^{2}\right) \\
& =\frac{1}{2 t}\left(t^{2}\left\|\phi_{*, o}(v)\right\|^{2}-2 t\left\langle\phi_{*, o}(v), \phi(x)-\phi(o)\right\rangle+\|\phi(x)-\phi(o)\|^{2}-t^{2}\right) \\
& =-\left\langle\phi_{*, o}(v), \phi(x)-\phi(o)\right\rangle+\frac{1}{2 t}\|\phi(x)-\phi(o)\|^{2} \tag{124}
\end{align*}
$$

using that $\|v\|_{o}=\left\|\phi_{*, o}(v)\right\|=1$. Then, by passing to the limit $t \rightarrow \infty$, we find

$$
\begin{equation*}
B^{v}(x)=-\left\langle\phi_{*, o}(v), \phi(x)-\phi(o)\right\rangle \tag{125}
\end{equation*}
$$

## C. 2 Proof of Proposition 9

We start by giving the proof of the coordinate geodesic projection which we recall in Proposition 42 .

## Proposition 42 (Coordinate of the geodesic projection on Hyperbolic space)

1. Let $\mathcal{G}^{v}=\operatorname{span}\left(x^{0}, v\right) \cap \mathbb{L}_{K}^{d}$ where $v \in T_{x^{0}} \mathbb{L}_{K}^{d} \cap S^{d}$. Then, the coordinate $P^{v}(x)$ of the geodesic projection on $\mathcal{G}^{v}$ of $x \in \mathbb{L}_{K}^{d}$ is

$$
\begin{equation*}
P^{v}(x)=\frac{1}{\sqrt{-K}} \operatorname{arctanh}\left(-\frac{1}{\sqrt{-K}} \frac{\langle x, v\rangle_{\mathbb{Q}}}{\left\langle x, x^{0}\right\rangle_{\mathbb{L}}}\right) \tag{126}
\end{equation*}
$$

2. Let $\tilde{v} \in S^{d-1}$ be an ideal point. Then, the coordinate $P^{\tilde{v}}(x)$ of the geodesic projection on the geodesic characterized by $\tilde{v}$ of $x \in \mathbb{B}_{K}^{d}$ is

$$
\begin{equation*}
P^{\tilde{v}}(x)=\frac{2}{\sqrt{-K}} \operatorname{arctanh}(\sqrt{-K} s(x)) \tag{127}
\end{equation*}
$$

where $s$ is defined as

$$
s(x)= \begin{cases}\frac{1-K\|x\|_{2}^{2}-\sqrt{\left(1-K\|x\|_{2}^{2}\right)^{2}+4 K\langle x, \tilde{v}\rangle^{2}}}{-2 K\langle x, \tilde{v}\rangle} & \text { if }\langle x, \tilde{v}\rangle \neq 0  \tag{128}\\ 0 & \text { if }\langle x, \tilde{v}\rangle=0\end{cases}
$$

First, we will compute in Proposition 43 the geodesic projections.

## Proposition 43 (Geodesic projection)

1. Let $\mathcal{G}^{v}=\operatorname{span}\left(x^{0}, v\right) \cap \mathbb{L}_{K}^{d}$ where $v \in T_{x^{0}} \mathbb{L}_{K}^{d} \cap S^{d}$. Then, the geodesic projection $\tilde{P}^{v}$ on $\mathcal{G}^{v}$ of $x \in \mathbb{L}_{K}^{d}$ is

$$
\begin{equation*}
\tilde{P}^{v}(x)=\frac{1}{\sqrt{-K\left\langle x, x^{0}\right\rangle_{\mathbb{L}}^{2}-\langle x, v\rangle_{\mathbb{L}}^{2}}}\left(-\sqrt{-K}\left\langle x, x^{0}\right\rangle_{\mathbb{\complement}} x^{0}+\langle x, v\rangle_{\mathbb{\Perp}} v\right) \tag{129}
\end{equation*}
$$

2. Let $\tilde{v} \in S^{d-1}$ be an in ideal point. Then, the geodesic projection $\tilde{P}^{\tilde{v}}$ on the geodesic characterized by $\tilde{v}$ of $x \in \mathbb{B}_{K}^{d}$ is

$$
\begin{equation*}
\tilde{P} \tilde{v}(x)=s(x) \tilde{v} \tag{130}
\end{equation*}
$$

where

$$
s(x)= \begin{cases}\frac{1-K\|x\|_{2}^{2}-\sqrt{\left(1-K\|x\|_{2}^{2}\right)^{2}+4 K\langle x, \tilde{v})^{2}}}{-2 K\langle x, \tilde{v}} & \text { if }\langle x, \tilde{v}\rangle \neq 0  \tag{131}\\ 0 & \text { if }\langle x, \tilde{v}\rangle=0\end{cases}
$$

## Proof of Proposition 43

1. Lorentz model. Any point $y$ on the geodesic obtained by the intersection between $E=\operatorname{span}\left(x^{0}, v\right)$ and $\mathbb{L}_{K}^{d}$ can be written as

$$
\begin{equation*}
y=\cosh (\sqrt{-K} t) x^{0}+\sinh (\sqrt{-K} t) \frac{v}{\sqrt{-K}} \tag{132}
\end{equation*}
$$

where $t \in \mathbb{R}$. Moreover, as arccosh is an increasing function, we have

$$
\begin{align*}
\tilde{P}^{v}(x) & =\underset{y \in E \cap L_{K}^{d}}{\operatorname{argmin}} d_{\mathbb{\Perp}}(x, y) \\
& =\underset{y \in E \cap \mathbb{L}_{K}^{d}}{\operatorname{argmin}} \operatorname{arccosh}\left(K\langle x, y\rangle_{\mathbb{\unrhd}}\right)  \tag{133}\\
& =\underset{y \in E \cap \mathbb{L}_{K}^{d}}{\operatorname{argmin}} K\langle x, y\rangle_{\mathbb{\unrhd}} .
\end{align*}
$$

This problem is equivalent with solving

$$
\begin{equation*}
\underset{t \in \mathbb{R}}{\operatorname{argmin}} K \cosh (\sqrt{-K} t)\left\langle x, x^{0}\right\rangle_{\mathbb{Q}}+K \frac{\sinh (\sqrt{-K} t)}{\sqrt{-K}}\langle x, v\rangle_{\mathbb{\Perp}} \tag{134}
\end{equation*}
$$

Let $g(t)=\cosh (\sqrt{-K} t)\left\langle x, x^{0}\right\rangle_{\mathbb{Q}}+\frac{\sinh (\sqrt{-K} t)}{\sqrt{-K}}\langle x, v\rangle_{\mathbb{Q}}$, then

$$
\begin{equation*}
g^{\prime}(t)=0 \Longleftrightarrow \tanh (\sqrt{-K} t)=-\frac{1}{\sqrt{-K}} \frac{\langle x, v\rangle_{\mathbb{\square}}}{\left\langle x, x^{0}\right\rangle_{\mathbb{\complement}}} \tag{135}
\end{equation*}
$$

Finally, using that $1-\tanh ^{2}(t)=\frac{1}{\cosh ^{2}(t)}$ and $\cosh ^{2}(t)-\sinh ^{2}(t)=1$, and observing that necessarily, $\left\langle x, x^{0}\right\rangle_{\mathbb{\complement}} \leq 0$, we obtain

$$
\begin{equation*}
\cosh (\sqrt{-K} t)=\frac{1}{\sqrt{1-\left(-\frac{1}{\sqrt{-K}} \frac{\langle x, v\rangle_{\mathbb{Z}}}{\left\langle x, x^{0}\right\rangle_{\mathbb{Z}}}\right)^{2}}}=\frac{-\sqrt{-K}\left\langle x, x^{0}\right\rangle_{\mathbb{L}}}{\sqrt{-K\left\langle x, x^{0}\right\rangle_{\mathbb{L}}^{2}-\langle x, v\rangle_{\mathbb{L}}^{2}}} \tag{136}
\end{equation*}
$$

and

$$
\begin{equation*}
\sinh (\sqrt{-K} t)=\frac{\langle x, v\rangle_{\mathbb{Q}}}{\sqrt{-K\left\langle x, x^{0}\right\rangle_{\mathbb{L}}^{2}-\langle x, v\rangle_{\mathbb{L}}^{2}}} \tag{137}
\end{equation*}
$$

2. Poincaré ball. A geodesic passing through the origin on the Poincaré ball is of the form $\gamma(t)=t p$ for an ideal point $p \in S^{d-1}$ and $\left.t \in\right]-\frac{1}{\sqrt{-K}}, \frac{1}{\sqrt{-K}}[$. Using that arccosh is an increasing function, we find

$$
\begin{align*}
\tilde{P}^{p}(x) & =\underset{y \in \operatorname{span}(\gamma)}{\operatorname{argmin}} d_{\mathbb{B}}(x, y) \\
& =\underset{t p}{\operatorname{argmin}} \frac{1}{\sqrt{-K}} \operatorname{arccosh}\left(1-2 K \frac{\|x-\gamma(t)\|_{2}^{2}}{\left(1+K\|x\|_{2}^{2}\right)\left(1+K\|\gamma(t)\|_{2}^{2}\right)}\right)  \tag{138}\\
& =\underset{t p}{\operatorname{argmin}} \log \left(\|x-\gamma(t)\|_{2}^{2}\right)-\log \left(1+K\|x\|_{2}^{2}\right)-\log \left(1+K\|\gamma(t)\|_{2}^{2}\right) \\
& =\underset{t p}{\operatorname{argmin}} \log \left(\|x-t p\|_{2}^{2}\right)-\log \left(1+K t^{2}\right)
\end{align*}
$$

Let $g(t)=\log \left(\|x-t p\|_{2}^{2}\right)-\log \left(1+K t^{2}\right)$. Then,

$$
g^{\prime}(t)=0 \Longleftrightarrow \begin{cases}t^{2}+\frac{1-K\|x\|_{2}^{2}}{K\langle x, p\rangle} t-\frac{1}{K}=0 & \text { if }\langle p, x\rangle \neq 0  \tag{139}\\ t=0 & \text { if }\langle p, x\rangle=0\end{cases}
$$

Finally, if $\langle x, p\rangle \neq 0$, the solution is

$$
\begin{equation*}
t=-\frac{1-K\|x\|_{2}^{2}}{2 K\langle x, p\rangle} \pm \sqrt{\left(\frac{1-K\|x\|_{2}^{2}}{2 K\langle x, p\rangle}\right)^{2}+\frac{1}{K}} \tag{140}
\end{equation*}
$$

Now, let us suppose that $\langle x, p\rangle>0$. Then,

$$
\begin{align*}
\frac{1-K\|x\|_{2}^{2}}{-2 K\langle x, p\rangle}+\sqrt{\left(\frac{1-K\|x\|_{2}^{2}}{2 K\langle x, p\rangle}\right)^{2}+\frac{1}{K}} & \geq \frac{1-K\|x\|_{2}^{2}}{2 K\langle x, p\rangle}  \tag{141}\\
& \geq \frac{1}{\sqrt{-K}}
\end{align*}
$$

because $\|\sqrt{-K} x-p\|_{2}^{2} \geq 0$ implies that $\frac{1-K\|x\|_{2}^{2}}{2 \sqrt{-K}\langle x, p\rangle} \geq 1$ which implies that $\frac{1-K\|x\|_{2}^{2}}{-2 K\langle x, p\rangle} \geq$ $\frac{1}{\sqrt{-K}}$, and therefore the solution is

$$
\begin{equation*}
t=-\frac{1-K\|x\|_{2}^{2}}{2 K\langle x, p\rangle}-\sqrt{\left(\frac{1-K\|x\|_{2}^{2}}{2 K\langle x, p\rangle}\right)^{2}+\frac{1}{K}} \tag{142}
\end{equation*}
$$

Similarly, if $\langle x, p\rangle<0$, then

$$
\begin{align*}
\frac{1-K\|x\|_{2}^{2}}{-2 K\langle x, p\rangle}-\sqrt{\left(\frac{1-K\|x\|_{2}^{2}}{2 K\langle x, p\rangle}\right)^{2}+\frac{1}{K}} & \leq \frac{1-K\|x\|_{2}^{2}}{-2 K\langle x, p\rangle}  \tag{143}\\
& \leq-\frac{1}{\sqrt{-K}}
\end{align*}
$$

because $\|\sqrt{-K} x+p\|_{2}^{2} \geq 0$ implies $\frac{1-K\|x\|_{2}^{2}}{2 \sqrt{-K}\{x, p\rangle} \leq-1$, which implies that $\frac{1-K\|x\|_{2}^{2}}{-2 K\langle x, p\rangle} \leq$ $-\frac{1}{\sqrt{-K}}$ and the solution is

$$
\begin{equation*}
\frac{1-K\|x\|_{2}^{2}}{-2 K\langle x, p\rangle}+\sqrt{\left(\frac{1-K\|x\|_{2}^{2}}{2 K\langle x, p\rangle}\right)^{2}+\frac{1}{K}} \tag{144}
\end{equation*}
$$

Thus,

$$
\begin{align*}
s(x) & = \begin{cases}\frac{1-K\|x\|_{2}^{2}}{-2 K\langle x, p\rangle}-\sqrt{\left(\frac{1-K\|x\|_{2}^{2}}{2 K\langle x, p\rangle}\right)^{2}+\frac{1}{K}} & \text { if }\langle x, p\rangle>0 \\
\frac{1-K\|x\|_{2}^{2}}{-2 K\langle x, p\rangle}+\sqrt{\left(\frac{1-K\|x\| 2}{2 K\langle x, p\rangle}\right)^{2}+\frac{1}{K}} & \text { if }\langle x, p\rangle<0 .\end{cases} \\
& =\frac{1-K\|x\|_{2}^{2}}{-2 K\langle x, p\rangle}-\operatorname{sign}(\langle x, p\rangle) \sqrt{\left(\frac{1-K\|x\|_{2}^{2}}{2 K\langle x, p\rangle}\right)^{2}+\frac{1}{K}}  \tag{145}\\
& =\frac{1-K\|x\|_{2}^{2}}{-2 K\langle x, p\rangle}-\frac{\operatorname{sign}(\langle x, p\rangle)}{-2 K \operatorname{sign}(\langle x, p\rangle)\langle x, p\rangle} \sqrt{\left(1-K\|x\|_{2}^{2}\right)^{2}+4 K\langle x, p\rangle^{2}} \\
& =\frac{1-K\|x\|_{2}^{2}-\sqrt{\left(1-K\|x\|_{2}^{2}\right)^{2}+4 K\langle x, p\rangle^{2}}}{-2 K\langle x, p\rangle}
\end{align*}
$$

## Proof of Proposition 42

1. Lorentz model. The coordinate on the geodesic can be obtained as

$$
\begin{equation*}
P^{v}(x)=\underset{t \in \mathbb{R}}{\operatorname{argmin}} d_{\mathbb{\square}}\left(\exp _{x^{0}}(t v), x\right) . \tag{146}
\end{equation*}
$$

Hence, by using (135), we obtain that the optimal $t$ satisfies

$$
\begin{equation*}
\tanh (\sqrt{-K} t)=-\frac{1}{\sqrt{-K}} \frac{\langle x, v\rangle_{\mathbb{\Perp}}}{\left\langle x, x^{0}\right\rangle_{\mathbb{\Perp}}} \Longleftrightarrow t=\frac{1}{\sqrt{-K}} \operatorname{arctanh}\left(-\frac{1}{\sqrt{-K}} \frac{\langle x, v\rangle_{\mathbb{\Perp}}}{\left\langle x, x^{0}\right\rangle_{\mathbb{\circledR}}}\right) . \tag{147}
\end{equation*}
$$

2. Poincaré ball. As a geodesic is of the form $\gamma(t)=\tanh \left(\frac{\sqrt{-K} t}{2}\right) \frac{p}{\sqrt{-K}}$ for all $t \in \mathbb{R}$, we deduce from Proposition 43 that

$$
\begin{equation*}
s(x)=\frac{1}{\sqrt{-K}} \tanh \left(\frac{\sqrt{-K} t}{2}\right) \Longleftrightarrow t=\frac{2}{\sqrt{-K}} \operatorname{arctanh}(\sqrt{-K} s(x)) \tag{148}
\end{equation*}
$$

We now derive the closed forms of the horospherical projections which we recall in Proposition 44 .

## Proposition 44 (Busemann function on Hyperbolic space)

1. On $\mathbb{L}_{K}^{d}$, for any direction $v \in T_{x^{0}} \mathbb{L}_{K}^{d} \cap S^{d}$,

$$
\begin{equation*}
\forall x \in \mathbb{L}_{K}^{d}, \quad B^{v}(x)=\frac{1}{\sqrt{-K}} \log \left(-\sqrt{-K}\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{L}}\right) \tag{149}
\end{equation*}
$$

2. On $\mathbb{B}_{K}^{d}$, for any ideal point $\tilde{v} \in S^{d-1}$,

$$
\begin{equation*}
\forall x \in \mathbb{B}_{K}^{d}, \quad B^{\tilde{v}}(x)=\frac{1}{\sqrt{-K}} \log \left(\frac{\|\tilde{v}-\sqrt{-K} x\|_{2}^{2}}{1+K\|x\|_{2}^{2}}\right) \tag{150}
\end{equation*}
$$

## Proof of Proposition 44

1. Lorentz model. The geodesic in direction $v$ can be characterized by

$$
\begin{equation*}
\forall t \in \mathbb{R}, \gamma_{v}(t)=\cosh (\sqrt{-K} t) x^{0}+\sinh (\sqrt{-K} t) \frac{v}{\sqrt{-K}} \tag{151}
\end{equation*}
$$

Hence, we have for all $x \in \mathbb{L}_{K}^{d}$,

$$
\begin{align*}
& d_{\mathbb{\Perp}}\left(\gamma_{v}(t), x\right) \\
& =\frac{1}{\sqrt{-K}} \operatorname{arccosh}\left(K\left\langle\gamma_{v}(t), x\right\rangle_{\mathbb{\circledR}}\right) \\
& =\frac{1}{\sqrt{-K}} \operatorname{arccosh}\left(K \cosh (\sqrt{-K} t)\left\langle x, x^{0}\right\rangle_{\mathbb{\complement}}+\frac{K}{\sqrt{-K}} \sinh (\sqrt{-K} t)\langle x, v\rangle_{\mathbb{\complement}}\right) \\
& =\frac{1}{\sqrt{-K}} \operatorname{arccosh}\left(K \frac{e^{\sqrt{-K} t}+e^{-\sqrt{-K} t}}{2}\left\langle x, x^{0}\right\rangle_{\mathbb{Q}}+\frac{K}{\sqrt{-K}} \frac{e^{\sqrt{-K} t}-e^{-\sqrt{-K} t}}{2}\langle x, v\rangle_{\mathbb{\complement}}\right) \\
& =\frac{1}{\sqrt{-K}} \operatorname{arccosh}\left(K \frac{e^{\sqrt{-K} t}}{2}\left(\left(1+e^{-2 \sqrt{-K} t}\right)\left\langle x, x^{0}\right\rangle_{\mathbb{\circledR}}+\frac{1}{\sqrt{-K}}\left(1-e^{-2 \sqrt{-K} t}\right)\langle x, v\rangle_{\mathbb{\complement}}\right)\right) \\
& =\frac{1}{\sqrt{-K}} \operatorname{arccosh}(x(t)) . \tag{152}
\end{align*}
$$

Then, on one hand, we have $x(t) \underset{t \rightarrow \infty}{\rightarrow} \pm \infty$, and using that $\operatorname{arccosh}(x)=\log (x+$ $\sqrt{x^{2}-1}$, we have

$$
\begin{align*}
d_{\mathbb{\Perp}}\left(\gamma_{v}(t), x\right)-t & =\frac{1}{\sqrt{-K}}\left(\log \left(x(t)+\sqrt{x(t)^{2}-1}\right)-\sqrt{-K} t\right) \\
& =\frac{1}{\sqrt{-K}} \log \left(\left(x(t)+\sqrt{x(t)^{2}-1}\right) e^{-\sqrt{-K} t}\right) \\
& =\frac{1}{\sqrt{-K}} \log \left(e^{-\sqrt{-K} t} x(t)+e^{-\sqrt{-K} t} x(t) \sqrt{1-\frac{1}{x(t)^{2}}}\right) \\
& =\frac{1}{\sqrt{-K}} \log \left(e^{-\sqrt{-K} t} x(t)+e^{-\sqrt{-K} t} x(t)\left(1-\frac{1}{2 x(t)^{2}}+o\left(\frac{1}{x(t)^{2}}\right)\right)\right) \tag{153}
\end{align*}
$$

Moreover,

$$
\begin{align*}
e^{-\sqrt{-K} t} x(t) & =\frac{K}{2}\left(1+e^{-2 \sqrt{-K} t}\right)\left\langle x, x^{0}\right\rangle_{\mathbb{\circledR}}+\frac{K}{2 \sqrt{-K}}\left(1-e^{-2 \sqrt{-K} t}\right)\langle x, v\rangle_{\mathbb{\circledR}} \\
& \underset{t \rightarrow \infty}{\rightarrow} \frac{K}{2}\left(\left\langle x, x^{0}\right\rangle_{\mathbb{\Perp}}+\frac{\langle x, v\rangle_{\mathbb{\Perp}}}{\sqrt{-K}}\right) . \tag{154}
\end{align*}
$$

Hence,

$$
\begin{equation*}
B^{v}(x)=\frac{1}{\sqrt{-K}} \log \left(K\left(\left\langle x, x^{0}\right\rangle_{\mathbb{Z}}+\frac{\langle x, v\rangle_{\mathbb{\circledR}}}{\sqrt{-K}}\right)\right) . \tag{155}
\end{equation*}
$$

## 2. Poincaré ball.

Let $p \in S^{d-1}$, then the geodesic from 0 to $p$ is of the form $\gamma_{p}(t)=\exp _{0}(t p)=$ $\tanh \left(\frac{\sqrt{-K} t}{2}\right) \frac{p}{\sqrt{-K}}$. Moreover, recall that $\operatorname{arccosh}(x)=\log \left(x+\sqrt{x^{2}-1}\right)$ and

$$
\begin{align*}
d_{\mathbb{B}}\left(\gamma_{p}(t), x\right) & =\frac{1}{\sqrt{-K}} \operatorname{arccosh}\left(1-2 K \frac{\left\|\tanh \left(\frac{\sqrt{-K} t}{2}\right) \frac{p}{\sqrt{-K}}-x\right\|_{2}^{2}}{\left(1-\tanh ^{2}\left(\frac{\sqrt{-K} t}{2}\right)\left(1+K\|x\|_{2}^{2}\right)\right.}\right)  \tag{156}\\
& =\frac{1}{\sqrt{-K}} \operatorname{arccosh}(1+x(t))
\end{align*}
$$

where

$$
\begin{equation*}
x(t)=-2 K \frac{\left\|\tanh \left(\frac{\sqrt{-K} t}{2}\right) \frac{p}{\sqrt{-K}}-x\right\|_{2}^{2}}{\left(1-\tanh ^{2}\left(\frac{\sqrt{-K} t}{2}\right)\right)\left(1+K\|x\|_{2}^{2}\right)} \tag{157}
\end{equation*}
$$

Now, on one hand, we have

$$
\begin{align*}
B^{p}(x) & =\lim _{t \rightarrow \infty}\left(d_{\mathbb{B}}\left(\gamma_{p}(t), x\right)-t\right) \\
& =\lim _{t \rightarrow \infty} \frac{1}{\sqrt{-K}}\left(\log \left(1+x(t)+\sqrt{x(t)^{2}+2 x(t)}\right)-\sqrt{-K} t\right)  \tag{158}\\
& =\lim _{t \rightarrow \infty} \frac{1}{\sqrt{-K}} \log \left(e^{-\sqrt{-K} t}\left(1+x(t)+\sqrt{x(t)^{2}+2 x(t)}\right)\right)
\end{align*}
$$

On the other hand, using that $\tanh \left(\frac{t}{2}\right)=\frac{e^{t}-1}{e^{t}+1}$,

$$
\begin{align*}
& e^{-\sqrt{-K} t} x(t)=-2 K e^{-\sqrt{-K} t} \frac{\left\|\frac{\sqrt{-K t}-1}{e^{\sqrt{-K t}+1}} \frac{p}{\sqrt{-K}}-x\right\|_{2}^{2}}{\left(1-\left(\frac{e \sqrt{-K t}}{e \sqrt{\sqrt{K t}}+1}\right)^{2}\right)\left(1+K\|x\|_{2}^{2}\right)} \\
&=2 e^{-\sqrt{-K} t} \frac{\left\|e^{\sqrt{-K}} p-p-\sqrt{-K} e^{\sqrt{-K} t} x-\sqrt{-K} x\right\|_{2}^{2}}{4 e^{\sqrt{-K} t}\left(1+K\|x\|_{2}^{2}\right)}  \tag{159}\\
&=\frac{1}{2} \frac{\left\|p-e^{-\sqrt{-K} t} p-\sqrt{-K} x-\sqrt{-K} e^{-\sqrt{-K} t} x\right\|_{2}^{2}}{1+K\|x\|_{2}^{2}} \\
& \underset{t \rightarrow \infty}{\rightarrow} \frac{1}{2} \frac{\|p-\sqrt{-K} x\|_{2}^{2}}{1+K\|x\|_{2}^{2}} .
\end{align*}
$$

Hence,

$$
\begin{align*}
B^{p}(x) & =\lim _{t \rightarrow \infty} \frac{1}{\sqrt{-K}} \log \left(e^{-\sqrt{-K} t}+e^{-\sqrt{-K} t} x(t)+e^{-\sqrt{-K} t} x(t) \sqrt{1+\frac{2}{x(t)}}\right)  \tag{160}\\
& =\frac{1}{\sqrt{-K}} \log \left(\frac{\|p-\sqrt{-K} x\|_{2}^{2}}{1+K\|x\|_{2}^{2}}\right)
\end{align*}
$$

using that $\sqrt{1+\frac{2}{x(t)}}=1+\frac{1}{x(t)}+o\left(\frac{1}{x(t)}\right)$ and $\frac{1}{x(t)} \rightarrow_{t \rightarrow \infty} 0$.

## C. 3 Proof of Proposition 11

First, we recall and show two lemmas.

Lemma 45 (Proposition 5.6.c in (Lee, 2006)) Suppose $\phi:(\mathcal{M}, g) \rightarrow(\tilde{\mathcal{M}}, \tilde{g})$ is an isometry. Then, $\phi$ takes geodesics to geodesics, i.e. if $\gamma$ is the geodesic in $\mathcal{M}$ with $\gamma(0)=p$ and $\gamma^{\prime}(0)=v$, then $\phi \circ \gamma$ is the geodesic in $\tilde{\mathcal{M}}$ with $\phi(\gamma(0))=\phi(p)$ and $(\phi \circ \gamma)^{\prime}(0)=\phi_{*, p}(v)$.

Lemma 46 Let $\phi:(\mathcal{M}, g) \rightarrow(\tilde{\mathcal{M}}, \tilde{g})$ an isometry and $v \in T_{o} \mathcal{M}$ such that $\|v\|_{o}=1$. Then for all $x \in \mathcal{M}$,

$$
\begin{align*}
& B^{v}(x)=B^{\phi_{*, o}(v)}(\phi(x))  \tag{161}\\
& P^{v}(x)=P^{\phi_{*, o}(v)}(\phi(x)) \tag{162}
\end{align*}
$$

Proof of Lemma 46 Let $v \in T_{o} \mathcal{M}$ such that $\|v\|_{o}=1, x \in \mathcal{M}$. By Lemma 45, we have $\phi\left(\exp _{o}(t v)\right)=\exp _{\phi(o)}\left(t \phi_{*, o}(v)\right)$.

Proof of Equation (161). Let us show that $B^{v}(x)=B^{\phi_{*, o}(v)}(\phi(x))$. By definition of the Busemann function, we have

$$
\begin{align*}
B^{v}(x) & =\lim _{t \rightarrow \infty} d_{\mathcal{M}}\left(x, \exp _{o}(t v)\right)-t \\
& =\lim _{t \rightarrow \infty} d_{\tilde{\mathcal{M}}}\left(\phi(x), \phi\left(\exp _{o}(t v)\right)\right)-t \quad \text { since } \phi \text { is an isometry }  \tag{163}\\
& =\lim _{t \rightarrow \infty} d_{\tilde{\mathcal{M}}}\left(\phi(x), \exp _{\phi(o)}\left(t \phi_{*, o}(v)\right)\right)-t \\
& =B^{\phi_{*, o}(v)}(\phi(x))
\end{align*}
$$

Proof of Equation (162). Let us now show that $P^{v}(x)=P^{\phi_{*, o}(v)}(\phi(x))$. Then,

$$
\begin{align*}
P^{v}(x) & =\underset{t \in \mathbb{R}}{\operatorname{argmin}} d_{\mathcal{M}}\left(x, \exp _{o}(t v)\right) \\
& =\underset{t \in \mathbb{R}}{\operatorname{argmin}} d_{\tilde{\mathcal{M}}}\left(\phi(x), \phi\left(\exp _{o}(t v)\right)\right) \quad \text { since } \phi \text { is an isometry }  \tag{164}\\
& =\underset{t \in \mathbb{R}}{\operatorname{argmin}} d_{\tilde{\mathcal{M}}}\left(\phi(x), \exp _{\phi(o)}\left(t \phi_{*, o}(v)\right)\right) \quad \text { by Lemma 45 } \\
& =P^{\phi_{*, o}(v)}(\phi(x))
\end{align*}
$$

Proof of Proposition 11 First, let us show that for $\lambda_{o}$-almost all $v \in S_{o}, W_{p}^{p}\left(B_{\#}^{v} \mu, B_{\#}^{v} \nu\right)=$ $W_{p}^{p}\left(B_{\#}^{\phi_{*, o}(v)} \tilde{\mu}, B_{\#}^{\phi_{*, o}(v)} \tilde{\nu}\right)$ and $W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right)=W_{p}^{p}\left(P_{\#}^{\phi_{*, o}(v)} \mu, P_{\#}^{\phi_{*, o}(v)} \nu\right)$. Using Lemma 46, we have

$$
\begin{align*}
W_{p}^{p}\left(B_{\#}^{v} \mu, B_{\#}^{v} \nu\right) & =W_{p}^{p}\left(B_{\#}^{\phi_{*, o}(v)} \phi_{\#} \mu, B_{\#}^{\phi_{*, o}(v)} \phi_{\#} \nu\right)=W_{p}^{p}\left(B_{\#}^{\phi_{*, o}(v)} \tilde{\mu}, B_{\#}^{\phi_{*, o}(v)} \tilde{\nu}\right)  \tag{165}\\
W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) & =W_{p}^{p}\left(P_{\#}^{\phi_{*, o}(v)} \phi_{\#} \mu, P_{\#}^{\phi_{*, o}(v)} \phi_{\#} \nu\right)=W_{p}^{p}\left(P_{\#}^{\phi_{*, o}(v)} \tilde{\mu}, P_{\#}^{\phi_{*, o}(v)} \tilde{\nu}\right) \tag{166}
\end{align*}
$$

These results are true for all $v \in S_{o}$, and therefore for $\lambda_{o}$-almost all $v \in S_{o}$. Thus, by integrating with respect to $\lambda_{o}$, and performing the change of $v \mapsto \phi_{*, o}(v)$ on the right side, we find

$$
\begin{align*}
& \operatorname{HCHSW}_{p}^{p}\left(\mu, \nu ; \lambda_{o}\right)=\operatorname{HCHSW}_{p}^{p}\left(\tilde{\mu}, \tilde{\nu} ;\left(\phi_{*, o}\right)_{\#} \lambda_{o}\right)  \tag{167}\\
& \operatorname{GCHSW}_{p}^{p}\left(\mu, \nu ; \lambda_{o}\right)=\operatorname{GCHSW}_{p}^{p}\left(\tilde{\mu}, \tilde{\nu} ;\left(\phi_{*, o}\right)_{\#} \lambda_{o}\right) \tag{168}
\end{align*}
$$

Finally, we can conclude by using that $\phi_{*, o}$ is an isometry between the tangent spaces and hence $\left(\phi_{*, o}\right)_{\#} \lambda_{o}=\lambda_{\phi(o)}$.

## C. 4 Proof of Proposition 15

First, let us compute the differential of $\phi=\varphi \circ \mathcal{L}$. In that purpose, we first recall the differential of $\mathcal{L}: X=L L^{T} \mapsto L$ derived in (Lin, 2019, Proposition 4).

Lemma 47 (Proposition 4 in (Lin, 2019)) Let $X \in S_{d}^{++}(\mathbb{R})$ and $V \in S_{d}(\mathbb{R})$. The differential operator $\mathcal{L}_{*, X}: T_{X} S_{d}^{++}(\mathbb{R}) \rightarrow T_{\mathcal{L}(X)} L_{d}^{++}(\mathbb{R})$ of $\mathcal{L}$ at $X$ is given by

$$
\begin{equation*}
\mathcal{L}_{*, X}(V)=\mathcal{L}(X)\left(\left\lfloor\mathcal{L}(X)^{-1} V \mathcal{L}(X)^{-T}\right\rfloor+\frac{1}{2} \operatorname{diag}\left(\mathcal{L}(X)^{-1} V \mathcal{L}(X)^{-T}\right)\right) \tag{169}
\end{equation*}
$$

Lemma 48 Let $\phi: X \mapsto \varphi(\mathcal{L}(X))$ and $X=L L^{T} \in S_{d}^{++}(\mathbb{R})$ with $L \in L_{d}^{++}(\mathbb{R})$ obtained by the Cholesky decomposition. The differential operator of $\phi$ at $X$ is given by

$$
\begin{equation*}
\forall V \in T_{X} S_{d}^{++}(\mathbb{R}), \phi_{*, X}(V)=\left\lfloor\mathcal{L}_{*, X}(V)\right\rfloor+\operatorname{diag}(\mathcal{L}(X))^{-1} \operatorname{diag}\left(\mathcal{L}_{*, X}(V)\right) \tag{170}
\end{equation*}
$$

where

$$
\begin{equation*}
\mathcal{L}_{*, X}(V)=\mathcal{L}(X)\left(\left\lfloor\mathcal{L}(X)^{-1} V \mathcal{L}(X)^{-T}\right\rfloor+\frac{1}{2} \operatorname{diag}\left(\mathcal{L}(X)^{-1} V \mathcal{L}(X)^{-T}\right)\right) \tag{171}
\end{equation*}
$$

Proof of Lemma 48 Using the chain rule, we have, for $X \in S_{d}^{++}(\mathbb{R})$ and $V \in S_{d}(\mathbb{R})$,

$$
\begin{align*}
\phi_{*, X}(V) & =\varphi_{*, X}\left(\mathcal{L}_{*, X}(V)\right)=\left\lfloor\mathcal{L}_{*, X}(V)\right\rfloor+\log _{*, \operatorname{diag}(L)}\left(\operatorname{diag}\left(\mathcal{L}_{*, X}(V)\right)\right)  \tag{172}\\
& =\left\lfloor\mathcal{L}_{*, X}(V)\right\rfloor+\Sigma\left(\operatorname{diag}\left(\mathcal{L}_{*, X}(V)\right)\right)
\end{align*}
$$

using Lemma 12 for the differenutal of the log with

$$
\begin{align*}
\Sigma\left(\operatorname{diag}\left(\mathcal{L}_{*, X}(V)\right)\right) & =\operatorname{diag}\left(\mathcal{L}_{*, X}(V)\right) \odot \Gamma \\
& =\operatorname{diag}\left(\mathcal{L}_{*, X}(V)\right) \oslash \operatorname{diag}(\mathcal{L}(X))  \tag{173}\\
& =\operatorname{diag}(\mathcal{L}(X))^{-1} \operatorname{diag}\left(\mathcal{L}_{*, X}(V)\right)
\end{align*}
$$

Thus, we conclude that $\phi_{*, X}(V)=\left\lfloor\mathcal{L}_{*, X}(V)\right\rfloor+\operatorname{diag}(\mathcal{L}(X))^{-1} \operatorname{diag}\left(\mathcal{L}_{*, X}(V)\right)$.

Proof of Proposition 15 On one hand we have $\phi\left(I_{d}\right)=0, \mathcal{L}_{*, I_{d}}(V)=\lfloor V\rfloor+\frac{1}{2} \operatorname{diag}(V)$ and thus $\phi_{*, I_{d}}(V)=\lfloor V\rfloor+\frac{1}{2} \operatorname{diag}(V)$ since $\mathcal{L}\left(I_{d}\right)=I_{d}$. Thus, using Proposition 7, the projection is given, for $A \in S_{d}(\mathbb{R})$ such that $\|A\|_{I_{d}}^{2}=\left\langle\phi_{*, I_{d}}(A), \phi_{*, I_{d}}(A)\right\rangle_{F}=1$, by

$$
\begin{align*}
\forall X=L L^{T} \in S_{d}^{++}(\mathbb{R}), P^{A}(X) & =\left\langle\phi(X), \phi_{*, I_{d}}(A)\right\rangle_{F} \\
& =\left\langle\lfloor L\rfloor+\log (\operatorname{diag}(L)),\lfloor A\rfloor+\frac{1}{2} \operatorname{diag}(A)\right\rangle_{F}  \tag{174}\\
& =\langle\lfloor L\rfloor,\lfloor A\rfloor\rangle_{F}+\left\langle\log (\operatorname{diag}(L)), \frac{1}{2} \operatorname{diag}(A)\right\rangle_{F}
\end{align*}
$$

## C. 5 Proof of Proposition 16

Proof of Proposition 16 We use that $B^{\gamma}(x)=\lim _{t \rightarrow \infty} \frac{d(x, \gamma(t))^{2}-t^{2}}{2 t}$ (see e.g. (Bridson and Haefliger, 2013, II. 8.24)). Thus,

$$
\begin{align*}
B^{\gamma}(x) & =\lim _{t \rightarrow \infty} d(x, \gamma(t))-t \\
& =\lim _{t \rightarrow \infty} \frac{d(x, \gamma(t))^{2}-t^{2}}{2 t} \\
& =\lim _{t \rightarrow \infty} \sum_{i=1}^{n} \lambda_{i} \frac{d_{i}\left(x_{i}, \gamma_{i}\left(\lambda_{i} t\right)\right)^{2}-\lambda_{i}^{2} t^{2}}{2 \lambda_{i} t}  \tag{175}\\
& =\sum_{i=1}^{n} \lambda_{i} B^{\gamma_{i}}\left(x_{i}\right)
\end{align*}
$$

## Appendix D. Proofs of Section 5

## D. 1 Proof of Proposition 17

Proof of Proposition 17 First, we will show that for any $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M}), \operatorname{CHSW}_{p}(\mu, \nu)<$ $\infty$. Let $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$, and let $\gamma \in \Pi(\mu, \nu)$ be an arbitrary coupling between them. Then by using first Lemma 37 followed by the 1-Lipschitzness of the projections Lemma 38 and Lemma 39, we obtain

$$
\begin{align*}
W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) & =\inf _{\gamma \in \Pi(\mu, \nu)} \int\left|P^{v}(x)-P^{v}(y)\right|^{p} \mathrm{~d} \gamma(x, y) \\
& \leq \int\left|P^{v}(x)-P^{v}(y)\right|^{p} \mathrm{~d} \gamma(x, y) \\
& \leq \int d(x, y)^{p} \mathrm{~d} \gamma(x, y)  \tag{176}\\
& \leq 2^{p-1}\left(\int d(x, o)^{p} \mathrm{~d} \mu(x)+\int d(o, y)^{p} \mathrm{~d} \nu(y)\right) \\
& <\infty
\end{align*}
$$

Hence, we can conclude that $\operatorname{CHSW}_{p}^{p}(\mu, \nu)<\infty$.

Now, let us show that it is a pseudo-distance. First, it is straightforward to see that $\operatorname{CHSW}_{p}(\mu, \nu) \geq 0$, that it is symmetric, i.e. $\operatorname{CHSW}_{p}(\mu, \nu)=\operatorname{CHSW}_{p}(\nu, \mu)$, and that $\mu=\nu$ implies that $\operatorname{CHSW}_{p}(\mu, \nu)=0$ using that $W_{p}$ is well a distance.

For the triangular inequality, we can derive it using the triangular inequality for $W_{p}$ and the Minkowski inequality. Let $\mu, \nu, \alpha \in \mathcal{P}_{p}(\mathcal{M})$,

$$
\begin{align*}
\operatorname{CHSW}_{p}(\mu, \nu) & =\left(\int_{S_{o}} W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) \mathrm{d} \lambda_{o}(v)\right)^{\frac{1}{p}} \\
& \leq\left(\int_{S_{o}}\left(W_{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \alpha\right)+W_{p}\left(P_{\#}^{v} \alpha, P_{\#}^{v} \nu\right)\right)^{p} \mathrm{~d} \lambda_{o}(v)\right)^{\frac{1}{p}}  \tag{177}\\
& \leq\left(\int_{S_{o}} W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \alpha\right) \mathrm{d} \lambda_{o}(v)\right)^{\frac{1}{p}}+\left(\int_{S_{o}} W_{p}^{p}\left(P_{\#}^{v} \alpha, P_{\#}^{v} \nu\right) \mathrm{d} \lambda_{o}(v)\right)^{\frac{1}{p}} \\
& =\operatorname{CHSW}_{p}(\mu, \alpha)+\operatorname{CHSW}_{p}(\alpha, \nu)
\end{align*}
$$

## D. 2 Proof of Proposition 18

Proof of Proposition 18 Let $f \in L^{1}(\mathcal{M}), g \in C_{0}\left(\mathbb{R} \times S_{o}\right)$, then by Fubini's theorem,

$$
\begin{align*}
\langle\operatorname{CHR} f, g\rangle_{\mathbb{R} \times S_{o}} & =\int_{S_{o}} \int_{\mathbb{R}} \operatorname{CHR} f(t, v) g(t, v) \mathrm{d} t \mathrm{~d} \lambda_{o}(v) \\
& =\int_{S_{o}} \int_{\mathbb{R}} \int_{\mathcal{M}} f(x) \mathbb{1}_{\left\{t=P^{v}(x)\right\}} g(t, v) \mathrm{dVol}(x) \mathrm{d} t \mathrm{~d} \lambda_{o}(v) \\
& =\int_{\mathcal{M}} f(x) \int_{S_{o}} \int_{\mathbb{R}} g(t, v) \mathbb{1}_{\left\{t=P^{v}(x)\right\}} \mathrm{d} t \mathrm{~d} \lambda_{o}(v) \mathrm{dVol}(x)  \tag{178}\\
& =\int_{\mathcal{M}} f(x) \int_{S_{o}} g\left(P^{v}(x), v\right) \mathrm{d} \lambda_{o}(v) \mathrm{dVol}(x) \\
& =\int_{\mathcal{M}} f(x) \operatorname{CHR}^{*} g(x) \mathrm{dVol}(x) \\
& =\left\langle f, \operatorname{CHR}^{*} g\right\rangle_{\mathcal{M}} .
\end{align*}
$$

## D. 3 Proof of Proposition 19

Proof of Proposition 19 We follow the proof of (Boman and Lindskog, 2009, Lemma 1). On one hand, $g \in C_{0}\left(\mathbb{R} \times S_{o}\right)$, thus for all $\epsilon>0$, there exists $M>0$ such that $|t| \geq M$ implies $|g(t, v)| \leq \epsilon$ for all $v \in S_{o}$.

Let $\epsilon>0$ and $M>0$ which satisfies the previous property. Denote $E(x, M)=\{v \in$ $\left.S_{o},\left|P^{v}(x)\right|<M\right\}$. Then, as $d(x, o)>0$, we have

$$
\begin{equation*}
E(x, M)=\left\{v \in S_{o},\left|P^{v}(x)\right|<M\right\}=\left\{v \in S_{o}, \frac{P^{v}(x)}{d(x, o)}<\frac{M}{d(x, o)}\right\} \underset{d(x, o) \rightarrow \infty}{ } \emptyset \tag{179}
\end{equation*}
$$

Thus, $\lambda_{o}(E(x, M)) \xrightarrow[d(x, o) \rightarrow \infty]{ } 0$. Choose $M^{\prime}$ such that $d(x, o)>M^{\prime}$ implies that $\lambda_{o}(E(x, M))<$ $\epsilon$.

Then, for $x \in \mathcal{M}$ such that $\left|P^{v}(x)\right| \geq \max \left(M, M^{\prime}\right)$ (and thus $d(x, o) \geq M^{\prime}$ since $\left|P^{v}(x)\right| \leq d(x, o)$ as $P^{v}$ is Lipschitz,

$$
\begin{align*}
\left|\operatorname{CHR}^{*} g(x)\right| & \leq\left|\int_{E(x, M)} g\left(P^{v}(x), v\right) \mathrm{d} \lambda_{o}(v)\right|+\left|\int_{E(x, M)^{c}} g\left(P^{v}(x), v\right) \mathrm{d} \lambda_{o}(v)\right|  \tag{180}\\
& \leq\|g\|_{\infty} \lambda_{o}(E(x, M))+\epsilon \lambda_{o}\left(E(x, M)^{c}\right) \\
& \leq\|g\|_{\infty} \epsilon+\epsilon
\end{align*}
$$

Thus, we showed that $\mathrm{CHR}^{*} g(x) \underset{d(x, o) \rightarrow \infty}{\longrightarrow}$, and thus $\operatorname{CHR}^{*} g \in C_{0}(\mathcal{M})$.

## D. 4 Proof of Proposition 21

Proof of Proposition 21 Let $g \in C_{0}\left(\mathbb{R} \times S_{o}\right)$, as $\operatorname{CHR} \mu=\lambda_{o} \otimes K_{\mu}$, we have by definition

$$
\begin{equation*}
\int_{S_{o}} \int_{\mathbb{R}} g(t, v) K_{\mu}(v, \mathrm{~d} t) \mathrm{d} \lambda_{o}(v)=\int_{\mathbb{R} \times S_{o}} g(t, v) \mathrm{d}(\operatorname{CHR} \mu)(t, v) \tag{181}
\end{equation*}
$$

Hence, using the property of the dual, we have for all $g \in C_{o}\left(\mathbb{R} \times S_{o}\right)$,

$$
\begin{align*}
\int_{S_{o}} \int_{\mathbb{R}} g(t, v) K_{\mu}(v, \mathrm{~d} t) \mathrm{d} \lambda_{o}(v) & =\int_{\mathbb{R} \times S_{o}} g(t, v) \mathrm{d}(\operatorname{CHR} \mu)(t, v) \\
& =\int_{\mathcal{M}} \operatorname{CHR}^{*} g(x) \mathrm{d} \mu(x) \\
& =\int_{\mathcal{M}} \int_{S_{o}} g\left(P^{v}(x), v\right) \mathrm{d} \lambda_{o}(v) \mathrm{d} \mu(x)  \tag{182}\\
& =\int_{S_{o}} \int_{\mathcal{M}} g\left(P^{v}(x), v\right) \mathrm{d} \mu(x) \mathrm{d} \lambda_{o}(v) \\
& =\int_{S_{o}} \int_{\mathbb{R}} g(t, v) \mathrm{d}\left(P_{\#}^{v} \mu\right)(t) \mathrm{d} \lambda_{o}(v)
\end{align*}
$$

Hence, for $\lambda_{o}$-almost every $v \in S_{o}, K_{\mu}(v, \cdot)=P_{\#}^{v} \mu$.

## D. 5 Proof of Proposition 22

Proof of Proposition 22 Using Lemma 37 and that the projections are 1-Lipschitz (Lemma 38), we can show that, for any $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$,

$$
\begin{equation*}
\operatorname{CHSW}_{p}^{p}(\mu, \nu)=\inf _{\gamma \in \Pi(\mu, \nu)} \int\left|P^{v}(x)-P^{v}(y)\right|^{p} \mathrm{~d} \gamma(x, y) \tag{183}
\end{equation*}
$$

Let $\gamma^{*} \in \Pi(\mu, \nu)$ being an optimal coupling for the Wasserstein distance with ground cost $d$, then,

$$
\begin{align*}
\operatorname{CHSW}_{p}^{p}(\mu, \nu) & \leq \int\left|P^{v}(x)-P^{v}(y)\right|^{p} \mathrm{~d} \gamma^{*}(x, y) \\
& \leq \int d(x, y)^{p} \mathrm{~d} \gamma^{*}(x, y)  \tag{184}\\
& =W_{p}^{p}(\mu, \nu)
\end{align*}
$$

## D. 6 Proof of Proposition 23

Proof of Proposition 23 Let $\mu, \nu \in \mathcal{P}_{p}(\mathcal{M})$, then

$$
\begin{align*}
\operatorname{CHSW}_{p}^{p}(\mu, \nu) & =\int_{S_{o}} W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) \mathrm{d} \lambda_{o}(v) \\
& =\int_{S_{o}}\left\|F_{P_{\#}^{v} \mu}^{-1}-F_{P_{\#}^{v} \nu}^{-1}\right\|_{L^{p}([0,1])}^{p} \mathrm{~d} \lambda_{o}(v)  \tag{185}\\
& \left.=\int_{S_{o}} \int_{0}^{1}\left(F_{P_{\#}^{v}}^{-1} \mu\right)-F_{P_{\#}^{\nu}}^{-1}(q)\right)^{p} \mathrm{~d} q \mathrm{~d} \lambda_{o}(v) \\
& =\|\Phi(\mu)-\Phi(\nu)\|_{\mathcal{H}}^{p}
\end{align*}
$$

Thus, CHSW $_{p}$ is Hilbertian.

## D. 7 Proof of Lemma 25

## Proof of Lemma 25

Since for any $v \in S_{o}$ and $x \in \mathcal{M}, P^{v}(x)=\left\langle\phi(x)-\phi(o), \phi_{*, o}(x)\right\rangle$, by using Lemma 37 we have

$$
\begin{align*}
W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) & =\inf _{\gamma \in \Pi\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right)} \int|x-y|^{p} \mathrm{~d} \gamma(x, y) \\
& =\inf _{\gamma \in \Pi(\mu, \nu)} \int\left|P^{v}(x)-P^{v}(y)\right|^{p} \mathrm{~d} \gamma(x, y)  \tag{186}\\
& =\inf _{\gamma \in \Pi(\mu, \nu)} \int\left|\left\langle\phi(x)-\phi(y), \phi_{*, o}(v)\right\rangle\right|^{p} \mathrm{~d} \gamma(x, y) .
\end{align*}
$$

Let's note $Q^{v}(x)=\langle x, v\rangle$. Then, we obtain

$$
\begin{align*}
W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) & =\inf _{\gamma \in \Pi(\mu, \nu)} \int\left|Q^{\phi_{*, o}(v)}(\phi(x))-Q^{\phi_{*, o}(v)}(\phi(y))\right|^{p} \mathrm{~d} \gamma(x, y)  \tag{187}\\
& =W_{p}^{p}\left(Q_{\#}^{\phi_{\#, o}(v)} \phi_{\#} \mu, Q_{\#}^{\phi_{\#, o}(v)} \phi_{\#} \nu\right)
\end{align*}
$$

Therefore, we obtain

$$
\begin{align*}
\operatorname{CHSW}_{p}^{p}(\mu, \nu) & =\int_{S_{o}} W_{p}^{p}\left(Q_{\#}^{\phi_{*, o}(v)} \phi_{\#} \mu, Q_{\#}^{\phi_{*, o}(v)} \phi_{\#} \nu\right) \mathrm{d} \lambda_{o}(v)  \tag{188}\\
& =\int_{S_{\phi(o)}} W_{p}^{p}\left(Q_{\#}^{v} \phi_{\#} \mu, Q_{\#}^{v} \phi_{\#} \nu\right) \mathrm{d}\left(\left(\phi_{*, o}\right)_{\#} \lambda_{o}\right)(v)
\end{align*}
$$

Finally, since $\phi_{*, o}$ is an isometry between the tangent spaces by definition of the metric, we have $\left(\phi_{*, o}\right)_{\#} \lambda_{o}=\lambda_{\phi(o)}$.

## D. 8 Proof of Proposition 26

## Proof of Proposition 26

We know by Proposition 17 that $\mathrm{CHSW}_{p}$ is a finite pseudo-distance. For the indiscernible property, using Lemma 25 and the distance property of $\mathrm{SW}_{p}$, we have that $\operatorname{CHSW}_{p}(\mu, \nu)=\operatorname{SW}_{p}^{p}\left(\phi_{\#} \mu, \phi_{\#} \nu\right)=0$ implies that $\phi_{\#} \mu=\phi_{\#} \nu$ by applying the same proof of (Bonnotte, 2013, Proposition 5.1.2). Indeed, we have that $\mathrm{SW}_{p}^{p}\left(\phi_{\#} \mu, \phi_{\#} \nu\right)=0$ implies $W_{p}^{p}\left(Q_{\#}^{v} \phi_{\#} \mu, Q_{\#}^{v} \phi_{\#} \nu\right)=0$ for $\lambda_{\phi(o)}$-almost every $v \in S_{\phi(o)}$, and thus that $Q_{\#}^{v} \phi_{\#} \mu=Q_{\#}^{v} \phi_{\#} \nu$ since $W_{p}$ is a distance. Hence, using the Fourier transform and that $\lambda_{\phi(o)}$ is absolutely continuous with respect to the Lebesgue measure, we obtain that $\phi_{\#} \mu=\phi_{\#} \nu$.

Then, as $\phi$ is a bijection from $\mathcal{M}$ to $\mathcal{N}$, we have for all Borelian $C \subset \mathcal{M}$,

$$
\begin{align*}
\mu(C) & =\int_{\mathcal{M}} \mathbb{1}_{C}(x) \mathrm{d} \mu(x) \\
& =\int_{\mathcal{N}} \mathbb{1}_{C}\left(\phi^{-1}(y)\right) \mathrm{d}\left(\phi_{\#} \mu\right)(y) \\
& =\int_{\mathcal{N}} \mathbb{1}_{C}\left(\phi^{-1}(y)\right) \mathrm{d}\left(\phi_{\#} \nu\right)(y)  \tag{189}\\
& =\int_{\mathcal{M}} \mathbb{1}_{C}(x) \mathrm{d} \nu(x) \\
& =\nu(C) .
\end{align*}
$$

## D. 9 Proof of Proposition 27

To prove Proposition 27, we will adapt the proof of Nadjahi et al. (2020) to our projection. First, we start to adapt Nadjahi et al. (2020, Lemma S1):

Lemma 49 (Lemma S1 in Nadjahi et al. (2020)) Let $\left(\mu_{k}\right)_{k} \in \mathcal{P}_{p}(\mathcal{M})$ and $\mu \in \mathcal{P}_{p}(\mathcal{M})$ such that $\lim _{k \rightarrow \infty} \operatorname{CHSW}_{1}\left(\mu_{k}, \mu\right)=0$. Then, there exists $\varphi: \mathbb{N} \rightarrow \mathbb{N}$ non decreasing such that $\mu_{\varphi(k)} \underset{k \rightarrow \infty}{\mathcal{L}} \mu$.

## Proof of Theorem 49

Using Lemma 25, we know that $\operatorname{CHSW}_{1}(\mu, \nu)=\mathrm{SW}_{1}\left(\phi_{\#} \mu, \phi_{\#} \nu\right)$. Let's note $\alpha_{k}=$ $\phi_{\#} \mu_{k} \in \mathcal{P}_{p}(\mathcal{N})$ and $\alpha=\phi_{\#} \mu \in \mathcal{P}_{p}(\mathcal{N})$ and $Q^{v}(x)=\langle v, x\rangle$.

Then, by Bogachev and Ruas (2007, Theorem 2.2.5),

$$
\begin{equation*}
\lim _{k \rightarrow \infty} \int_{S_{\phi(o)}} W_{1}\left(Q_{\#}^{v} \alpha_{k}, Q_{\#}^{v} \alpha\right) \mathrm{d} \lambda_{\phi(o)}(v)=0 \tag{190}
\end{equation*}
$$

implies that there exists a subsequence $\left(\mu_{\varphi(k)}\right)_{k}$ such that for $\lambda_{\phi(o)}$-almost every $v$,

$$
\begin{equation*}
W_{1}\left(Q_{\#}^{v} \alpha_{\varphi(k)}, Q_{\#}^{v} \alpha\right) \underset{k \rightarrow \infty}{\longrightarrow} 0 \tag{191}
\end{equation*}
$$

As the Wasserstein distance metrizes the weak convergence, this is equivalent to $Q_{\#}^{v} \mu_{\varphi(k)} \underset{k \rightarrow \infty}{\mathcal{L}}$ $Q_{\#}^{v} \mu$.

Then, by Levy's characterization theorem, this is equivalent with the pointwise convergence of the characterization function, i.e. for all $t \in \mathbb{R}, \Phi_{Q_{\#}^{v} \alpha_{\varphi(k)}}(t) \underset{k \rightarrow \infty}{\longrightarrow} \Phi_{Q_{\#}^{v}} \mu^{\mu}(t)$. Then, working in $T_{\phi(o)} \mathcal{N}$ with the Euclidean norm, we can use the same proof of Nadjahi et al. (2020) by using a convolution with a gaussian kernel and show that it implies that $\alpha_{\varphi(k)} \underset{k \rightarrow \infty}{\mathcal{L}} \alpha$, i.e. $\phi_{\#} \mu_{\varphi(k)} \underset{k \rightarrow \infty}{\mathcal{L}} \phi_{\#} \mu$.

Finally, let's show that it implies the weak convergence of $\left(\mu_{\varphi(k)}\right)_{k}$ towards $\mu$. Let $f \in C_{b}(\mathcal{M})$, then

$$
\begin{equation*}
\int_{\mathcal{M}} f \mathrm{~d} \mu_{\varphi(k)}=\int_{\mathcal{N}} f \circ \phi^{-1} \mathrm{~d}\left(\phi_{\#} \mu_{\varphi(k)}\right) \underset{k \rightarrow \infty}{\longrightarrow} \int_{\mathcal{N}} f \circ \phi^{-1} \mathrm{~d}\left(\phi_{\#} \mu\right)=\int_{\mathcal{M}} f \mathrm{~d} \mu \tag{192}
\end{equation*}
$$

Hence, we an conclude that $\mu_{\varphi(k)} \underset{k \rightarrow \infty}{\mathcal{L}} \mu$.

Proof of Proposition 27 First, we suppose that $\mu_{k} \underset{k \rightarrow \infty}{\mathcal{L}} \mu$ in $\mathcal{P}_{p}(\mathcal{M})$. Then, by continuity, we have that for $\lambda_{o}$ almost every $v \in T_{o} \mathcal{M}, P_{\#}^{v \rightarrow \infty} \mu_{k} \underset{k \rightarrow \infty}{\longrightarrow} P_{\#}^{v} \mu$. Moreover, as the Wasserstein distance on $\mathbb{R}$ metrizes the weak convergence, $W_{p}\left(P_{\#}^{v} \mu_{k}, P_{\#}^{v} \mu\right) \underset{k \rightarrow \infty}{\longrightarrow} 0$. Finally, as $W_{p}$ is bounded and it converges for $\lambda_{o}$-almost every $v$, we have by the Lebesgue convergence dominated theorem that $\operatorname{CHSW}_{p}^{p}\left(\mu_{k}, \mu\right) \underset{k \rightarrow \infty}{\longrightarrow}$.

For the opposite side, suppose that $\operatorname{CHSW}_{p}\left(\mu_{k}, \mu\right) \underset{k \rightarrow \infty}{\longrightarrow} 0$. Then, since we generalized (Nadjahi et al., 2020, Lemma S1) to our setting in Theorem 49, we can use the same contradiction argument as Nadjahi et al. (2020) and we conclude that $\left(\mu_{k}\right)_{k}$ converges weakly to $\mu$.

## D. 10 Proof of Proposition 28

Proof of Proposition 28 By using Lemma 37, let us first observe that

$$
\begin{align*}
W_{1}(\mu, \nu) & =\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathcal{M} \times \mathcal{M}} d_{\mathcal{M}}(x, y) \mathrm{d} \gamma(x, y) \\
& =\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathcal{M} \times \mathcal{M}}\|\phi(x)-\phi(y)\| \mathrm{d} \gamma(x, y) \\
& =\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathcal{N} \times \mathcal{N}}\|x-y\| \mathrm{d}(\phi \otimes \phi)_{\#} \gamma(x, y)  \tag{193}\\
& =\inf _{\gamma \in \Pi\left(\phi_{\#} \mu, \phi_{\#} \nu\right)} \int_{\mathcal{N} \times \mathcal{N}}\|x-y\| \mathrm{d} \gamma(x, y) \\
& =W_{1}\left(\phi_{\#} \mu, \phi_{\#} \nu\right) .
\end{align*}
$$

Here, we note that $W_{1}$ must be understood with respect to the ground cost metric which makes sense given the space, i.e. $d_{\mathcal{M}}$ on $\mathcal{M}$ and $\|\cdot-\cdot\|$ on $\mathcal{N}$.

Then, using Lemma 25, we have

$$
\begin{equation*}
\operatorname{CHSW}_{1}(\mu, \nu)=\operatorname{SW}_{1}\left(\phi_{\#} \mu, \phi_{\#} \nu\right) \tag{194}
\end{equation*}
$$

Since $\mathcal{N}$ is a Euclidean inner product space of dimension $d$, we can apply (Bonnotte, 2013, Lemma 5.14), and we obtain

$$
\begin{equation*}
W_{1}(\mu, \nu)=W_{1}\left(\phi_{\#} \mu, \phi_{\#} \nu\right) \leq C_{d, p, r} \mathrm{SW}_{1}\left(\phi_{\#} \mu, \phi_{\#} \nu\right)^{\frac{1}{d+1}}=C_{d, p, r} \operatorname{CHSW}_{1}(\mu, \nu)^{\frac{1}{d+1}} \tag{195}
\end{equation*}
$$

Then, using that $W_{p}^{p}(\mu, \nu) \leq(2 r)^{p-1} W_{1}(\mu, \nu)$ and that by the Hölder inequality, $\operatorname{CHSW}_{1}(\mu, \nu) \leq$ $\operatorname{CHSW}_{p}(\mu, \nu)$, we obtain (with a different constant $C_{d, r, p}$ )

$$
\begin{equation*}
W_{p}^{p}(\mu, \nu) \leq C_{d, r, p} \operatorname{CHSW}_{p}(\mu, \nu)^{\frac{1}{d+1}} \tag{196}
\end{equation*}
$$

## D. 11 Proof of Proposition 29

Proof of Proposition 29 First, using the triangular inequality, the reverse triangular inequality and the Jensen inequality for $x \mapsto x^{1 / p}$ (which is concave since $p \geq 1$ ), we have the following inequality

$$
\begin{align*}
& \mathbb{E}\left[\left|\operatorname{CHSW}_{p}\left(\hat{\mu}_{n}, \hat{\nu}_{n}\right)-\operatorname{CHSW}_{p}(\mu, \nu)\right|\right] \\
& =\mathbb{E}\left[\left|\operatorname{CHSW}_{p}\left(\hat{\mu}_{n}, \hat{\nu}_{n}\right)-\operatorname{CHSW}_{p}\left(\hat{\mu}_{n}, \nu\right)+\operatorname{CHSW}_{p}\left(\hat{\mu}_{n}, \nu\right)-\operatorname{CHSW}_{p}(\mu, \nu)\right|\right] \\
& \leq \mathbb{E}\left[\left|\operatorname{CHSW}_{p}\left(\hat{\mu}_{n}, \hat{\nu}_{n}\right)-\operatorname{CHSW}_{p}\left(\hat{\mu}_{n}, \nu\right)\right|\right]+\mathbb{E}\left[\left|\operatorname{CHSW}_{p}\left(\hat{\mu}_{n}, \nu\right)-\operatorname{CHSW}_{p}(\mu, \nu)\right|\right]  \tag{197}\\
& \leq \mathbb{E}\left[\operatorname{CHSW}_{p}\left(\nu, \hat{\nu}_{n}\right)\right]+\mathbb{E}\left[\operatorname{CHSW}_{p}\left(\mu, \hat{\mu}_{n}\right)\right] \\
& \leq \mathbb{E}\left[\operatorname{CHSW}_{p}^{p}\left(\nu, \hat{\nu}_{n}\right)\right]^{1 / p}+\mathbb{E}\left[\operatorname{CHSW}_{p}^{p}\left(\mu, \hat{\mu}_{n}\right)\right]^{1 / p}
\end{align*}
$$

Moreover, by Fubini-Tonelli,

$$
\begin{align*}
\mathbb{E}\left[\operatorname{CHSW}_{p}^{p}\left(\hat{\mu}_{n}, \mu\right)\right] & =\mathbb{E}\left[\int_{S_{o}} W_{p}^{p}\left(P_{\#}^{v} \hat{\mu}_{n}, \mu\right) \mathrm{d} \lambda_{o}(v)\right]  \tag{198}\\
& =\int_{S_{o}} \mathbb{E}\left[W_{p}^{p}\left(P_{\#}^{v} \hat{\mu}_{n}, P_{\#}^{v} \mu\right)\right] \mathrm{d} \lambda_{o}(v)
\end{align*}
$$

Then, by applying Theorem 40, we get that for $q>p$, there exists a constant $C_{p, q}$ such that,

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-57.jpg?height=89&width=1648&top_left_y=2227&top_left_x=301)

Then, noting that necessarily, $P^{v}(o)=0$ (for both the horospherical and geodesic projection, since the geodesic is of the form $\left.\exp _{o}(t v)\right)$, and using that $P^{v}$ is 1-Lipschitz Lemma 38,
we can bound the moments as

$$
\begin{align*}
\tilde{M}_{q}\left(P_{\#}^{v} \mu\right) & =\int_{\mathbb{R}}|x|^{q} \mathrm{~d}\left(P_{\#}^{v} \mu\right)(x) \\
& =\int_{\mathcal{M}}\left|P^{v}(x)\right|^{q} \mathrm{~d} \mu(x) \\
& =\int_{\mathcal{M}}\left|P^{v}(x)-P^{v}(o)\right|^{q} \mathrm{~d} \mu(x)  \tag{200}\\
& \leq \int_{\mathcal{M}} d(x, o)^{q} \mathrm{~d} \mu(x) \\
& =M_{q}(\mu) .
\end{align*}
$$

Therefore, we have

$$
\begin{equation*}
\mathbb{E}\left[\operatorname{CHSW}_{p}^{p}\left(\hat{\mu}_{n}, \mu\right)\right] \leq C_{p, q} M_{q}(\mu)^{p / q}\left(n^{-1 / 2} \mathbb{1}_{\{q>2 p\}}+n^{-1 / 2} \log (n) \mathbb{1}_{\{q=2 p\}}+n^{-(q-p) / \mathbb{1}_{\{q \in(p, 2 p)\}}}\right) \tag{201}
\end{equation*}
$$

and similarly,

![](https://cdn.mathpix.com/cropped/2024_06_04_309bdb56058bfb559d6eg-58.jpg?height=84&width=1561&top_left_y=1086&top_left_x=304)

Hence, we conclude that

$$
\mathbb{E}\left[\left|\operatorname{CHSW}_{p}\left(\hat{\mu}_{n}, \hat{\nu}_{n}\right)-\operatorname{CHSW}_{p}(\mu, \nu)\right|\right] \leq 2 C_{p, q}^{1 / p} M_{q}(\nu)^{1 / q}\left\{\begin{array}{l}
n^{-1 /(2 p)} \text { if } q>2 p  \tag{203}\\
n^{-1 /(2 p)} \log (n)^{1 / p} \text { if } q=2 p \\
n^{-(q-p) /(p q)} \text { if } q \in(p, 2 p)
\end{array}\right.
$$

## D.12 Proof of Proposition 30

Proof of Proposition 30 Let $\left(v_{\ell}\right)_{\ell=1}^{L}$ be iid samples of $\lambda_{o}$. Then, by first using Jensen inequality and then remembering that $\mathbb{E}_{v}\left[W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right)\right]=\operatorname{CHSW}_{p}^{p}(\mu, \nu)$, we have

$$
\begin{align*}
\mathbb{E}_{v}\left[\left|\widehat{\operatorname{CHSW}}_{p, L}^{p}(\mu, \nu)-\operatorname{CHSW}_{p}^{p}(\mu, \nu)\right|\right]^{2} & \leq \mathbb{E}_{v}\left[\left|\widehat{\operatorname{CHSW}}_{p, L}^{p}(\mu, \nu)-\operatorname{CHSW}_{p}^{p}(\mu, \nu)\right|^{2}\right] \\
& =\mathbb{E}_{v}\left[\left|\frac{1}{L} \sum_{\ell=1}^{L}\left(W_{p}^{p}\left(P_{\#}^{v_{\ell}} \mu, P_{\#}^{v_{\ell}} \nu\right)-\operatorname{CHSW}_{p}^{p}(\mu, \nu)\right)\right|^{2}\right] \\
& =\frac{1}{L^{2}} \operatorname{Var}_{v}\left(\sum_{\ell=1}^{L} W_{p}^{p}\left(P_{\#}^{v_{\ell}} \mu, P_{\#}^{v_{\ell}} \nu\right)\right) \\
& =\frac{1}{L} \operatorname{Var}_{v}\left(W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right)\right) \\
& =\frac{1}{L} \int_{S_{o}}\left(W_{p}^{p}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right)-\operatorname{CHSW}_{p}^{p}(\mu, \nu)\right)^{2} \mathrm{~d} \lambda_{o}(v) \tag{204}
\end{align*}
$$

## Appendix E. Proofs of Section 7

## E. 1 Proof of Proposition 31

Proof of Proposition 31 This proof follows the proof in the Euclidean case derived in (Bonnotte, 2013, Proposition 5.1.7) or in (Candau-Tilh, 2020, Proposition 1.33).

As $\mu$ is absolutely continuous, $P_{\#}^{v} \mu$ is also absolutely continuous and there is a Kantorovitch potential $\psi_{v}$ between $P_{\#}^{v} \mu$ and $P_{\#}^{v} \nu$. Moreover, as the support is restricted to a compact, it is Lipschitz and thus differentiable almost everywhere.

First, using the duality formula, we obtain the following lower bound for all $\epsilon>0$,

$$
\begin{equation*}
\frac{\operatorname{CHSW}_{2}^{2}\left(\left(T_{\epsilon}\right)_{\#} \mu, \nu\right)-\mathrm{CHSW}_{2}^{2}(\mu, \nu)}{2 \epsilon} \geq \int_{S_{o}} \int_{\mathcal{M}} \frac{\psi_{v}\left(P^{v}\left(T_{\epsilon}(x)\right)\right)-\psi_{v}\left(P^{v}(x)\right)}{\epsilon} \mathrm{d} \mu(x) \mathrm{d} \lambda_{o}(v) \tag{205}
\end{equation*}
$$

Then, we know that the exponential map satisfies $\exp _{x}(0)=x$ and $\left.\frac{\mathrm{d}}{\mathrm{d} t} \exp (t v)\right|_{t=0}=v$. Taking the limit $\epsilon \rightarrow 0$, the right term is equal to $\left.\frac{\mathrm{d}}{\mathrm{d} t} g(t)\right|_{t=0}$ with $g(t)=\psi_{v}\left(P^{v}\left(T_{t}(x)\right)\right)$ and is equal to

$$
\begin{equation*}
\left.\frac{\mathrm{d}}{\mathrm{d} t} g(t)\right|_{t=0}=\psi_{v}^{\prime}\left(P^{v}\left(T_{0}(x)\right)\right)\left\langle\nabla P^{v}\left(T_{0}(x)\right),\left.\frac{\mathrm{d}}{\mathrm{d} t} T_{t}(x)\right|_{t=0}\right\rangle_{x}=\psi_{v}^{\prime}\left(P^{v}(x)\right)\left\langle\operatorname{grad}_{\mathcal{M}} P^{v}(x), \xi(x)\right\rangle_{x} \tag{206}
\end{equation*}
$$

Therefore, by the Lebesgue dominated convergence theorem (we have the convergence $\lambda_{o^{-}}$ almost surely and $\left|\psi_{v}\left(P^{v}\left(T_{\epsilon}(x)\right)\right)-\psi_{v}\left(P^{v}(x)\right)\right| \leq \epsilon$ using that $\psi_{v}$ and $P^{v}$ are Lipschitz and that $\left.d\left(\exp _{x}(\epsilon \xi(x)), \exp _{x}(0)\right) \leq C \epsilon\right)$

$$
\begin{align*}
& \liminf _{\epsilon \rightarrow 0^{+}} \frac{\operatorname{CHSW}_{2}^{2}\left(\left(T_{\epsilon}\right)_{\#} \mu, \nu\right)-\operatorname{CHSW}_{2}^{2}(\mu, \nu)}{2 \epsilon} \\
& \geq \int_{S_{o}} \int_{\mathcal{M}} \psi_{v}^{\prime}\left(P^{v}(x)\right)\left\langle\operatorname{grad}_{\mathcal{M}} P^{v}(x), \xi(x)\right\rangle \mathrm{d} \mu(x) \mathrm{d} \lambda_{o}(v) \tag{207}
\end{align*}
$$

For the upper bound, first, let $\pi^{v} \in \Pi(\mu, \nu)$ a coupling such that $\tilde{\pi}^{v}=\left(P^{v} \otimes P^{v}\right)_{\#} \pi^{v} \in$ $\Pi\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right)$ is an optimal coupling for the regular quadratic cost. For $\tilde{\pi}^{v}$-almost every $(x, y), y=x-\psi_{v}^{\prime}(x)$ and thus for $\pi^{v}$-almost every $(x, y), P^{v}(y)=P^{v}(x)-\psi_{v}^{\prime}\left(P^{v}(x)\right)$. Therefore,

$$
\begin{align*}
\operatorname{CHSW}_{2}^{2}(\mu, \nu) & =\int_{S_{o}} W_{2}^{2}\left(P_{\#}^{v} \mu, P_{\#}^{v} \nu\right) \mathrm{d} \lambda_{o}(v) \\
& =\int_{S_{o}} \int_{\mathbb{R} \times \mathbb{R}}|x-y|^{2} \mathrm{~d} \tilde{\pi}^{v}(x, y) \mathrm{d} \lambda_{o}(v)  \tag{208}\\
& =\int_{S_{o}} \int_{\mathcal{M} \times \mathcal{M}}\left|P^{v}(x)-P^{v}(y)\right|^{2} \mathrm{~d} \pi^{v}(x, y) \mathrm{d} \lambda_{o}(v)
\end{align*}
$$

On the other hand, $\left(\left(P^{v} \circ T_{\epsilon}\right) \otimes P^{v}\right)_{\#} \pi^{v} \in \Pi\left(P_{\#}^{v}\left(T_{\epsilon}\right)_{\#} \mu, P_{\#}^{v} \nu\right)$ and hence

$$
\begin{align*}
\operatorname{CHSW}_{2}^{2}\left(\left(T_{\epsilon}\right)_{\#} \mu, \nu\right) & =\int_{S_{o}} W_{2}^{2}\left(P_{\#}^{v}\left(T_{\epsilon}\right)_{\#} \mu, P_{\#}^{v} \nu\right) \mathrm{d} \lambda_{o}(v)  \tag{209}\\
& \leq \int_{S_{o}} \int_{\mathcal{M} \times \mathcal{M}}\left|P^{v}\left(T_{\epsilon}(x)\right)-P^{v}(y)\right|^{2} \mathrm{~d} \pi^{v}(x, y) \mathrm{d} \lambda_{o}(v)
\end{align*}
$$

Therefore,

$$
\begin{align*}
& \frac{\operatorname{CHSW}_{2}^{2}\left(\left(T_{\epsilon}\right)_{\#} \mu, \nu\right)-\operatorname{CHSW}_{2}^{2}(\mu, \nu)}{2 \epsilon} \\
& \leq \int_{S_{o}} \int_{\mathcal{M} \times \mathcal{M}} \frac{\left|P^{v}\left(T_{\epsilon}(x)\right)-P^{v}(y)\right|^{2}-\left|P^{v}(x)-P^{v}(y)\right|^{2}}{2 \epsilon} \mathrm{d} \pi^{v}(x, y) \mathrm{d} \lambda_{o}(v) . \tag{210}
\end{align*}
$$

Note $g(\epsilon)=\left(P^{v}\left(T_{\epsilon}(x)\right)-P^{v}(y)\right)^{2}$. Then, $\left.\frac{\mathrm{d}}{\mathrm{d} \epsilon} g(\epsilon)\right|_{\epsilon=0}=2\left(P^{v}(x)-P^{v}(y)\right)\left\langle\operatorname{grad}_{\mathcal{M}} P^{v}(x), \xi(x)\right\rangle_{x}$. But, as for $\pi^{v}$-almost every $(x, y), P^{v}(y)=P^{v}(x)-\psi_{v}^{\prime}\left(P^{v}(x)\right)$, we have

$$
\begin{equation*}
\left.\frac{\mathrm{d}}{\mathrm{d} \epsilon} g(\epsilon)\right|_{\epsilon=0}=2 \psi_{v}^{\prime}\left(P^{v}(x)\right)\left\langle\operatorname{grad}_{\mathcal{M}} P^{v}(x), \xi(x)\right\rangle_{x} \tag{211}
\end{equation*}
$$

Finally, by the Lebesgue dominated convergence theorem, we obtain

$$
\begin{align*}
& \limsup _{\epsilon \rightarrow 0^{+}} \frac{\operatorname{CHSW}_{2}^{2}\left(\left(T_{\epsilon}\right)_{\#} \mu, \nu\right)-\operatorname{CHSW}_{2}^{2}(\mu, \nu)}{2 \epsilon} \\
& \leq \int_{S_{o}} \int_{\mathcal{M}} \psi_{v}^{\prime}\left(P^{v}(x)\right)\left\langle\operatorname{grad}_{\mathcal{M}} P^{v}(x), \xi(x)\right\rangle_{x} \mathrm{~d} \mu(x) \mathrm{d} \lambda_{o}(v) \tag{212}
\end{align*}
$$

## E. 2 Proof of Proposition 34

Proof of Proposition 34 We apply Proposition 33. First, using that for $f: x \mapsto\langle x, y\rangle_{\complement}$, $\nabla f(x)=-K J y$, for all $x \in \mathbb{L}_{K}^{d}$,

$$
\begin{equation*}
\nabla B^{v}(x)=\sqrt{-K} J \frac{\sqrt{-K} x^{0}+v}{\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{\complement}}} \tag{213}
\end{equation*}
$$

Thus, noticing that $J^{2}=I_{d+1}$,

$$
\begin{align*}
& \operatorname{grad}_{\mathbb{L}_{K}^{d}} B^{v}(x)=\operatorname{Proj}_{x}^{K}\left(-K J \nabla B^{v}(x)\right) \\
& =\operatorname{Proj}_{x}^{K}\left(-K \sqrt{-K} \frac{\sqrt{-K} x^{0}+v}{\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{\complement}}}\right) \\
& =-K \sqrt{-K} \frac{\sqrt{-K} x^{0}+v}{\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{L}}}-K\left\langle x,-K \sqrt{-K} \frac{\sqrt{-K} x^{0}+v}{\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{\Perp}}}\right\rangle_{\mathbb{L}} x \\
& =K \sqrt{-K}\left(\frac{\sqrt{-K} x^{0}+v}{\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{\unrhd}}}+K \frac{\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{\Perp}}}{\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{\Perp}}} x\right) \\
& =K \sqrt{-K}\left(-\frac{\sqrt{-K} x^{0}+v}{\left\langle x, \sqrt{-K} x^{0}+v\right\rangle_{\mathbb{\Perp}}}+K x\right) \tag{214}
\end{align*}
$$

Similarly, we have

$$
\begin{equation*}
\nabla P^{v}(x)=\frac{-K J\left(\left\langle x, x^{0}\right\rangle_{\mathbb{\complement}} v-\langle x, v\rangle_{\Perp} x^{0}\right)}{\langle x, v\rangle_{\mathbb{L}}^{2}+K\left\langle x, x^{0}\right\rangle_{\mathbb{L}}^{2}} \tag{215}
\end{equation*}
$$

Thus, observing that $\left\langle x, \nabla P^{v}(x)\right\rangle_{\mathbb{■}}=0$, we have

$$
\begin{align*}
\operatorname{grad}_{\mathbb{L}_{K}^{d}} P^{v}(x) & =\operatorname{Proj}_{x}^{K}\left(-K J \nabla P^{v}(x)\right) \\
& =-K J \nabla P^{v}(x)-K\left\langle x,-K J \nabla P^{v}(x)\right\rangle_{\mathbb{\complement}} x \\
& =-K J \nabla P^{v}(x)  \tag{216}\\
& =\frac{K^{2}\left(\left\langle x, x^{0}\right\rangle_{\mathbb{L}} v-\langle x, v\rangle_{\mathbb{\Perp}} x^{0}\right)}{\langle x, v\rangle_{\mathbb{L}}^{2}+K\left\langle x, x^{0}\right\rangle_{\mathbb{L}}^{2}} .
\end{align*}
$$

## E. 3 Proof of Lemma 35

Proof of Lemma 35 By Lemma 12, we have $\phi_{*, X}(V)=U \Sigma(V) U^{T}$ with $\Sigma(V)=U^{T} V U \odot$ $\Gamma$. Thus,

$$
\begin{align*}
U \Sigma(V) U^{T}=W & \Longleftrightarrow \Sigma(V)=U^{T} W U \\
& \Longleftrightarrow U^{T} V U \odot \Gamma=U^{T} W U \\
& \Longleftrightarrow U^{T} V U=U^{T} W U \oslash \Gamma  \tag{217}\\
& \Longleftrightarrow V=U\left(U^{T} W U \oslash \Gamma\right) U^{T}
\end{align*}
$$

## E. 4 Proof of Lemma 36

Proof of Lemma 36 By (Pennec, 2020, Equation 3.8), we know that $\left\langle\log _{*, X}(V), Y\right\rangle=$ $\left\langle\log _{*, X}(Y), V\right\rangle$. Thus, by linearity, we have that

$$
\begin{equation*}
\forall V \in T_{X} S_{d}^{++}(\mathbb{R}), P_{*, X}^{A}(V)=\left\langle A, \log _{*, X}(V)\right\rangle_{F}=\left\langle\log _{*, X}(A), V\right\rangle_{F} \tag{218}
\end{equation*}
$$

Then, applying Lemma 12, we have the result.

## Appendix F. Busemann Function on SPDs endowed with Affine-Invariant Metric

Let $A \in S_{d}(\mathbb{R}), M \in S_{d}^{++}(\mathbb{R})$, we recall from Section 4.3.1 that the Busemann function can be computed as

$$
\begin{align*}
B^{A}(M) & =\lim _{t \rightarrow \infty} d_{A I}(\exp (t A), M)-t  \tag{219}\\
& =-\left\langle A, \log \left(\pi_{A}(M)\right\rangle_{F}\right.
\end{align*}
$$

where $\pi_{A}$ is a projection on the spaces of matrices commuting with $\exp (A)$ which belongs to a group $G \subset G L_{d}(\mathbb{R})$ leaving the Busemann function invariant. In the next paragraph, we detail how we can proceed to obtain $\pi^{A}$.

When $A$ is diagonal with sorted values such that $A_{11}>\cdots>A_{d d}$, then the group leaving the Busemann function invariant is the set of upper triangular matrices with ones on the diagonal (Bridson and Haefliger, 2013, II. Proposition 10.66), i.e. for any such matrix $g$ in that group, $B^{A}(M)=B^{A}\left(g M g^{T}\right)$. If the points are sorted in increasing order, then the group is the set of lower triangular matrices. Let's note $G_{U}$ the set of upper triangular matrices with ones on the diagonal. For a general $A \in S_{d}(\mathbb{R})$, we can first find an appropriate diagonalization $A=P \tilde{A} P^{T}$, where $\tilde{A}$ is diagonal sorted, and apply the change of basis $\tilde{M}=P^{T} M P$ (Fletcher et al., 2009). We suppose that all the eigenvalues of $A$ have an order of multiplicity of one. By the affine-invariance property, the distances do not change, i.e. $d_{A I}(\exp (t A), M)=d_{A I}(\exp (t \tilde{A}), \tilde{M})$ and hence, using the definition of the Busemann function, we have that $B^{A}(M)=B^{\tilde{A}}(\tilde{M})$. Then, we need to project $\tilde{M}$ on the space of matrices commuting with $\exp (\tilde{A})$ which we denote $F(A)$. By Bridson and Haefliger (2013, II. Proposition 10.67), this space corresponds to the diagonal matrices. Moreover, by Bridson and Haefliger (2013, II. Proposition 10.69), there is a unique pair $(g, D) \in G_{U} \times F(A)$ such that $\tilde{M}=g D g^{T}$, and therefore, we can note $\pi_{A}(\tilde{M})=D$. This decomposition actually corresponds to a UDU decomposition. If the eigenvalues of $A$ are sorted in increasing order, this would correspond to a LDL decomposition.

## Appendix G. Additional Details on Experiments

Table 3: Dataset characteristics.

|  | BBCSport | Movies | Goodreads genre | Goodreads like |
| :---: | :---: | :---: | :---: | :---: |
| Doc | 737 | 2000 | 1003 | 1003 |
| Train | 517 | 1500 | 752 | 752 |
| Test | 220 | 500 | 251 | 251 |
| Classes | 5 | 2 | 8 | 2 |
| Mean words by doc | $116 \pm 54$ | $182 \pm 65$ | $1491 \pm 538$ | $1491 \pm 538$ |
| Median words by doc | 104 | 175 | 1518 | 1518 |
| Max words by doc | 469 | 577 | 3499 | 3499 |

We sum up the statistics of the different datasets in Table 3.

## References

Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for image classification. IEEE transactions on pattern analysis and machine intelligence, $38(7): 1425-1438,2015$. (Cited on p. 30)

David Alvarez-Melis and Nicolo Fusi. Geometric dataset distances via optimal transport. Advances in Neural Information Processing Systems, 33:21428-21439, 2020. (Cited on p. 30)

David Alvarez-Melis, Youssef Mroueh, and Tommi Jaakkola. Unsupervised hierarchy matching with optimal transport over hyperbolic spaces. In International Conference on Artificial Intelligence and Statistics, pages 1606-1617. PMLR, 2020. (Cited on p. 2)

Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient flows: in metric spaces and in the space of probability measures. Springer Science \& Business Media, 2005. (Cited on p. 24,31$)$

Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum mean discrepancy gradient flow. Advances in Neural Information Processing Systems, 32, 2019. (Cited on p. 32)

Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Fast and Simple Computations on Tensors with Log-Euclidean Metrics. PhD thesis, INRIA, 2005. (Cited on p. 20)

Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Log-euclidean metrics for fast and simple calculus on diffusion tensors. Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine, 56(2): 411-421, 2006. (Cited on p. 20)

Iskander Azangulov, Andrei Smolensky, Alexander Terenin, and Viacheslav Borovitskiy. Stationary kernels and gaussian processes on lie groups and their homogeneous spaces i: the compact case. arXiv preprint arXiv:208.14960, 2022. (Cited on p. 2)

Iskander Azangulov, Andrei Smolensky, Alexander Terenin, and Viacheslav Borovitskiy. Stationary kernels and gaussian processes on lie groups and their homogeneous spaces ii: non-compact symmetric spaces. arXiv preprint arXiv:2301.13088, 2023. (Cited on p. 2)

Werner Ballmann, Mikhael Gromov, and Viktor Schroeder. Manifolds of non positive curvature. In Arbeitstagung Bonn 1984: Proceedings of the meeting held by the Max-PlanckInstitut für Mathematik, Bonn June 15-22, 1984, pages 261-268. Springer, 2006. (Cited on p. 8, 39)

Aurélien Bellet, Amaury Habrard, and Marc Sebban. A survey on metric learning for feature vectors and structured data. arXiv preprint arXiv:1306.6709, 2013. (Cited on p. 15)

Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, $35(8): 1798-1828,2013$. (Cited on p. 1)

Espen Bernton, Pierre E Jacob, Mathieu Gerber, and Christian P Robert. Approximate bayesian computation with the wasserstein distance. Journal of the Royal Statistical Society Series B: Statistical Methodology, 81(2):235-269, 2019. (Cited on p. 37)

Jérôme Bertrand and Benoît Kloeckner. A geometric study of wasserstein spaces: Hadamard spaces. Journal of Topology and Analysis, 4(04):515-542, 2012. (Cited on p. 8)

Rajendra Bhatia. Positive definite matrices. In Positive Definite Matrices. Princeton university press, 2009. (Cited on p. 18)

Kingshook Biswas. The fourier transform on negatively curved harmonic manifolds. arXiv preprint arXiv:1802.07236, 2018. (Cited on p. 25)

Benjamin Blankertz, Ryota Tomioka, Steven Lemm, Motoaki Kawanabe, and Klaus-Robert Muller. Optimizing spatial filters for robust eeg single-trial analysis. IEEE Signal processing magazine, 25(1):41-56, 2007. (Cited on p. 18)

Vladimir Igorevich Bogachev and Maria Aparecida Soares Ruas. Measure theory, volume 1. Springer, 2007. (Cited on p. 55)

Emmanuel Boissard and Thibaut Le Gouic. On the mean speed of convergence of empirical and occupation measures in wasserstein distance. In Annales de l'IHP Probabilités et statistiques, volume 50, pages 539-563, 2014. (Cited on p. 3)

Jan Boman and Filip Lindskog. Support theorems for the radon transform and cramér-wold theorems. Journal of theoretical probability, 22(3):683-710, 2009. (Cited on p. 23, 52)

Clément Bonet, Nicolas Courty, François Septier, and Lucas Drumetz. Efficient Gradient Flows in Sliced-Wasserstein Space. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. (Cited on p. 32, 37)

Clément Bonet, Laetitia Chapel, Lucas Drumetz, and Nicolas Courty. Hyperbolic SlicedWasserstein via Geodesic and Horospherical Projections. In Timothy Doster, Tegan Emerson, Henry Kvinge, Nina Miolane, Mathilde Papillon, Bastian Rieck, and Sophia Sanborn, editors, Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML), volume 221 of Proceedings of Machine Learning Research, pages 334-370. PMLR, 28 Jul 2023a. (Cited on p. 2, 18, 28)

Clément Bonet, Benoît Malézieux, Alain Rakotomamonjy, Lucas Drumetz, Thomas Moreau, Matthieu Kowalski, and Nicolas Courty. Sliced-Wasserstein on Symmetric Positive Definite Matrices for M/EEG Signals. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40 th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 2777-2805. PMLR, 23-29 Jul 2023b. (Cited on p. 2, $21,26,28)$

Clément Bonet, Paul Berg, Nicolas Courty, François Septier, Lucas Drumetz, and MinhTan Pham. Spherical Sliced-Wasserstein. In The Eleventh International Conference on Learning Representations, 2023c. (Cited on p. 2, 13)

Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic Control, 58(9):2217-2229, 2013. (Cited on p. 6, 32)

Nicolas Bonneel, Julien Rabin, Gabriel Peyré, and Hanspeter Pfister. Sliced and radon wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 51: $22-45,2015$. (Cited on p. 23)

Nicolas Bonnotte. Unidimensional and evolution methods for optimal transportation. PhD thesis, Université Paris Sud-Paris XI; Scuola normale superiore (Pise, Italie), 2013. (Cited on p. $32,55,57,59)$

Valentin De Bortoli, Emile Mathieu, Michael John Hutchinson, James Thornton, Yee Whye Teh, and Arnaud Doucet. Riemannian score-based generative modelling. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. (Cited on p. 2)

Joey Bose, Ariella Smofsky, Renjie Liao, Prakash Panangaden, and Will Hamilton. Latent variable modelling with hyperbolic normalizing flows. In International Conference on Machine Learning, pages 1045-1055. PMLR, 2020. (Cited on p. 2)

Nicolas Boumal. An introduction to optimization on smooth manifolds. Cambridge University Press, 2023. (Cited on p. 6, 34)

Martin R Bridson and André Haefliger. Metric spaces of non-positive curvature, volume 319. Springer Science \& Business Media, 2013. (Cited on p. 8, 9, 10, 20, 23, 39, 42, 50, 62 )

Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, $34(4): 18-42,2017$. (Cited on p. 2)

Daniel Brooks, Olivier Schwander, Frederic Barbaresco, Jean-Yves Schneider, and Matthieu Cord. Riemannian batch normalization for spd neural networks. Advances in Neural Information Processing Systems, 32, 2019a. (Cited on p. 19)

Daniel A Brooks, Olivier Schwander, Frédéric Barbaresco, Jean-Yves Schneider, and Matthieu Cord. Exploring complex time-series representations for riemannian machine learning of radar data. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3672-3676. IEEE, 2019b. (Cited on p. 18)

Yann Cabanes. Apprentissage dans les disques de Poincaré et de Siegel de séries temporelles multidimensionnelles complexes suivant un modèle autorégressif gaussien stationnaire centré: application à la classification de données audio et de fouillis radar. PhD thesis, Bordeaux, 2022. (Cited on p. 22, 37)

Jules Candau-Tilh. Wasserstein and sliced-wasserstein distances. Master's thesis, Université Pierre et Marie Curie, 2020. (Cited on p. 37, 59)

James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry. Flavors of geometry, 31(59-115):2, 1997. (Cited on p. 18)

Mathieu Carriere, Marco Cuturi, and Steve Oudot. Sliced wasserstein kernel for persistence diagrams. In International conference on machine learning, pages 664-673. PMLR, 2017. (Cited on p. 26)

Ines Chami, Albert Gu, Dat P Nguyen, and Christopher Ré. Horopca: Hyperbolic dimensionality reduction via horospherical projections. In International Conference on Machine Learning, pages 1419-1429. PMLR, 2021. (Cited on p. 10, 11, 37)

Ricky TQ Chen and Yaron Lipman. Riemannian flow matching on general geometries. In International Conference on Machine Learning, 2023. (Cited on p. 2)

Ziheng Chen, Yue Song, Gaowen Liu, Ramana Rao Kompella, Xiaojun Wu, and Nicu Sebe. Riemannian multiclass logistics regression for spd neural networks. arXiv preprint arXiv:2305.11288, 2023a. (Cited on p. 20, 21, 33)

Ziheng Chen, Tianyang Xu, Zhiwu Huang, Yue Song, Xiao-Jun Wu, and Nicu Sebe. Adaptive riemannian metrics on spd manifolds. arXiv preprint arXiv:2303.15477, 2023b. (Cited on p. 14,20 )

Emmanuel Chevallier and Nicolas Guigui. Wrapped statistical models on manifolds: motivations, the case se (n), and generalization to symmetric spaces. In Workshop on Joint Structures and Common Foundations of Statistical Physics, Information Geometry and Inference for Learning, pages 96-106. Springer, 2020. (Cited on p. 7)

Emmanuel Chevallier, Emmanuel Kalunga, and Jesús Angulo. Kernel density estimation on spaces of gaussian distributions and symmetric positive definite matrices. SIAM Journal on Imaging Sciences, 10(1):191-215, 2017. (Cited on p. 18)

Emmanuel Chevallier, Didong Li, Yulong Lu, and David Dunson. Exponential-wrapped distributions on symmetric spaces. SIAM Journal on Mathematics of Data Science, 4(4): $1347-1368,2022$. (Cited on p. 7)

Sinho Chewi, Tyler Maunu, Philippe Rigollet, and Austin J Stromme. Gradient descent algorithms for bures-wasserstein barycenters. In Conference on Learning Theory, pages 1276-1304. PMLR, 2020. (Cited on p. 39)

Seunghyuk Cho, Juyong Lee, and Dongwoo Kim. Gm-vae: Representation learning with vae on gaussian manifold. arXiv preprint arXiv:2209.15217, 2022a. (Cited on p. 22)

Seunghyuk Cho, Juyong Lee, Jaesik Park, and Dongwoo Kim. A rotated hyperbolic wrapped normal distribution for hierarchical representation learning. Advances in Neural Information Processing Systems, 35:17831-17843, 2022b. (Cited on p. 7)

Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718, 2018. (Cited on p. 31)

Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 international joint conference on neural networks (IJCNN), pages 2921-2926. IEEE, 2017. (Cited on p. 31)

Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013. (Cited on p. 2)

Andrej Cvetkovski and Mark Crovella. Multidimensional scaling in the poincaré disk. arXiv preprint arXiv:1105.5332, 2011. (Cited on p. 30)

Biwei Dai and Uros Seljak. Sliced iterative normalizing flows. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021. (Cited on p. 13)

Haitz Sáez de Ocáriz Borde, Alvaro Arroyo, Ismael Morales, Ingmar Posner, and Xiaowen Dong. Neural latent geometry search: Product manifold inference via gromov-hausdorffinformed bayesian optimization, 2023a. (Cited on p. 22)

Haitz Sáez de Ocáriz Borde, Anees Kazi, Federico Barbero, and Pietro Lio. Latent graph inference using product manifolds. In The Eleventh International Conference on Learning Representations, 2023b. (Cited on p. 6, 22)

Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and Shanmukha Ramakrishna Vedantam. Hyperbolic image-text representations. In International Conference on Machine Learning, pages 7694-7731. PMLR, 2023. (Cited on p. 30)

Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David Forsyth, and Alexander G Schwing. Max-sliced wasserstein distance and its use for gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10648-10656, 2019. (Cited on p. 11)

Hanze Dong, Xi Wang, LIN Yong, and Tong Zhang. Particle-based variational inference with preconditioned functional gradient flow. In The Eleventh International Conference on Learning Representations, 2023. (Cited on p. 34)

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011. (Cited on p. 34)

Pengfei Fang, Mehrtash Harandi, and Lars Petersson. Kernel methods in hyperbolic spaces. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10665-10674, 2021. (Cited on p. 2, 32)

Kilian Fatras, Younes Zine, Rémi Flamary, Remi Gribonval, and Nicolas Courty. Learning with minibatch wasserstein : asymptotic and gradient properties. In Silvia Chiappa and

Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 2131-2141. PMLR, 26-28 Aug 2020. (Cited on p. 2)

Aasa Feragen, Francois Lauze, and Soren Hauberg. Geodesic exponential kernels: When curvature and linearity conflict. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3032-3042, 2015. (Cited on p. 1, 2)

Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z Alaya, Aurélie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, et al. Pot: Python optimal transport. The Journal of Machine Learning Research, 22(1): 3571-3578, 2021. (Cited on p. 29)

P Thomas Fletcher, Conglin Lu, Stephen M Pizer, and Sarang Joshi. Principal geodesic analysis for the study of nonlinear statistics of shape. IEEE transactions on medical imaging, 23(8):995-1005, 2004. (Cited on p. 1)

P Thomas Fletcher, John Moeller, Jeff M Phillips, and Suresh Venkatasubramanian. Computing hulls and centerpoints in positive definite space. arXiv preprint arXiv:0912.1580, 2009. (Cited on p. 20, 62)

P Thomas Fletcher, John Moeller, Jeff M Phillips, and Suresh Venkatasubramanian. Horoball hulls and extents in positive definite space. In Workshop on Algorithms and Data Structures, pages 386-398. Springer, 2011. (Cited on p. 20)

Fernando Galaz-Garcia, Marios Papamichalis, Kathryn Turnbull, Simon Lunagomez, and Edoardo Airoldi. Wrapped distributions on homogeneous riemannian manifolds. arXiv preprint arXiv:2204.09790, 2022. (Cited on p. 7)

Sylvestre Gallot, Dominique Hulin, and Jacques Lafontaine. Riemannian geometry, volume 2. Springer, 1990. (Cited on p. 3, 13)

Robert C Garrett, Trevor Harris, Bo Li, and Zhuo Wang. Validating climate models with spherical convolutional wasserstein distance. arXiv preprint arXiv:2401.14657, 2024. (Cited on p. 13)

Baptiste Genest, Nicolas Courty, and David Coeurjolly. Non-euclidean sliced optimal transport sampling. arXiv preprint arXiv:2402.16981, 2024. (Cited on p. 13)

Jacob Goldberger, Geoffrey E Hinton, Sam Roweis, and Russ R Salakhutdinov. Neighbourhood components analysis. Advances in neural information processing systems, 17, 2004. (Cited on p. 29)

Ziv Goldfeld, Kengo Kato, Gabriel Rioux, and Ritwik Sadhu. Statistical inference with regularized optimal transport. arXiv preprint arXiv:2205.04283, 2022. (Cited on p. 37)

Jumpei Goto and Hiroyuki Sato. Approximated logarithmic maps on riemannian manifolds and their applications. JSIAM Letters, 13:17-20, 2021. (Cited on p. 39)

Albert Gu, Frederic Sala, Beliz Gunel, and Christopher Ré. Learning mixed-curvature representations in product spaces. In International Conference on Learning Representations, 2019. (Cited on p. 8, 14, 22)

Mehrtash T Harandi, Mathieu Salzmann, and Richard Hartley. From manifold to manifold: Geometry-aware dimensionality reduction for spd matrices. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part II 13, pages 17-32. Springer, 2014. (Cited on p. 18)

Thomas Hofmann, Bernhard Schölkopf, and Alexander J. Smola. Kernel methods in machine learning. The Annals of Statistics, 36(3):1171 - 1220, 2008. (Cited on p. 26)

Andrés Hoyos-Idrobo. Aligning hyperbolic representations: an optimal transport-based approach. arXiv preprint arXiv:2012.01089, 2020. (Cited on p. 2)

Zihao Hu, Guanghui Wang, and Jacob Abernethy. On riemannian projection-free online learning, 2023. (Cited on p. 6)

Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville. Riemannian diffusion models. Advances in Neural Information Processing Systems, 35:2750-2761, 2022. (Cited on p. 2)

Gao Huang, Chuan Guo, Matt J Kusner, Yu Sun, Fei Sha, and Kilian Q Weinberger. Supervised word mover's distance. Advances in neural information processing systems, 29, 2016. (Cited on p. 15, 29)

Zhiwu Huang and Luc Van Gool. A riemannian network for spd matrix learning. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017. (Cited on p. 19)

Stephan Huckemann and Herbert Ziezold. Principal component analysis for riemannian manifolds, with an application to triangular shape spaces. Advances in Applied Probability, 38(2):299-319, 2006. (Cited on p. 1)

Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on pattern analysis and machine intelligence, 16(5):550-554, 1994. (Cited on p. 31)

Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, and Mehrtash Harandi. Kernel methods on riemannian manifolds with gaussian rbf kernels. IEEE transactions on pattern analysis and machine intelligence, 37(12):2464-2477, 2015. (Cited on p. 1, 26)

Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker-planck equation. SIAM journal on mathematical analysis, 29(1):1-17, 1998. (Cited on p. 32)

Ce Ju and Cuntai Guan. Deep optimal transport on spd manifolds for domain adaptation. arXiv preprint arXiv:2201.05745, 2022. (Cited on p. 19)

Isay Katsman, Eric M Chen, Sidhanth Holalkere, Anna C Asch, Aaron Lou, Ser-Nam Lim, and Christopher De Sa. Riemannian residual networks. In ICML2022 Workshop on Topology, Algebra and Geometry in Machine Learning, 2022. (Cited on p. 2)

Jun Kitagawa and Asuka Takatsu. Two new families of metrics via optimal transport and barycenter problems. arXiv preprint arXiv:2311.15874, 2023. (Cited on p. 37)

Soheil Kolouri, Yang Zou, and Gustavo K Rohde. Sliced wasserstein kernels for probability distributions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5258-5267, 2016. (Cited on p. 26)

Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized sliced wasserstein distances. Advances in neural information processing systems, 32, 2019. (Cited on p. 13)

Anna Korba, Pierre-Cyril Aubin-Frankowski, Szymon Majewski, and Pierre Ablin. Kernel stein discrepancy descent. In International Conference on Machine Learning, pages 57195730. PMLR, 2021. (Cited on p. 32)

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In International conference on machine learning, pages 957-966. PMLR, 2015. (Cited on p. 15, 28, 29)

Serge Lang. Fundamentals of differential geometry, volume 191. Springer Science \& Business Media, 2012. (Cited on p. 8)

Tam Le, Makoto Yamada, Kenji Fukumizu, and Marco Cuturi. Tree-sliced variants of wasserstein distances. Advances in neural information processing systems, 32, 2019. (Cited on p. 2)

Alice Le Brigant. Probability on the spaces of curves and the associated metric spaces via information geometry; radar applications. PhD thesis, Université de Bordeaux, 2017. (Cited on p. 22)

Alice Le Brigant and Stéphane Puechmorel. Approximation of densities on riemannian manifolds. Entropy, 21(1):43, 2019. (Cited on p. 1)

Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http: //yann.lecun.com/exdb/mnist/. (Cited on p. 31)

John M Lee. Riemannian manifolds: an introduction to curvature, volume 176. Springer Science \& Business Media, 2006. (Cited on p. 3, 6, 7, 8, 9, 15, 48)

John M Lee and John M Lee. Smooth manifolds. Springer, 2012. (Cited on p. 3)

Jere Lehtonen. The geodesic ray transform on two-dimensional cartan-hadamard manifolds. arXiv preprint arXiv:1612.04800, 2016. (Cited on p. 25)

Jere Lehtonen, Jesse Railo, and Mikko Salo. Tensor tomography on cartan-hadamard manifolds. Inverse Problems, 34(4):044004, 2018. (Cited on p. 25)

Wenjie Lei, Zhengming Ma, Shuyu Liu, and Yuanping Lin. Eeg mental recognition based on rkhs learning and source dictionary regularized rkhs subspace learning. IEEE Access, 9:150545-150559, 2021. (Cited on p. 19)

Rémi Leluc, François Portier, Johan Segers, and Aigerim Zhuman. Speeding up monte carlo integration: Control neighbors for optimal convergence. arXiv preprint arXiv:2305.06151, 2023. (Cited on p. 5)

Rémi Leluc, Aymeric Dieuleveut, François Portier, Johan Segers, and Aigerim Zhuman. Sliced-wasserstein estimation with spherical harmonics as control variates, 2024. (Cited on p. 5)

Tao Li, Cheng Meng, Jun Yu, and Hongteng Xu. Hilbert curve projection distance for distribution comparison. arXiv preprint arXiv:2205.15059, 2022. (Cited on p. 37)

Ya-Wei Eileen Lin, Ronald R. Coifman, Gal Mishne, and Ronen Talmon. Hyperbolic diffusion embedding and distance for hierarchical representation learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40 th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 21003-21025. PMLR, 23-29 Jul 2023. (Cited on p. 37)

Zhenhua Lin. Riemannian geometry of symmetric positive definite matrices via cholesky decomposition. SIAM Journal on Matrix Analysis and Applications, 40(4):1353-1370, 2019. (Cited on p. 20, 22, 49)

Xinran Liu, Yikun Bai, Yuzhe Lu, Andrea Soltoggio, and Soheil Kolouri. Wasserstein task embedding for measuring task similarities. arXiv preprint arXiv:2208.11726, 2022. (Cited on p. 30)

Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert Stöter. Sliced-wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In International Conference on Machine Learning, pages 4104-4113. PMLR, 2019. (Cited on p. 32, 33)

Federico López, Beatrice Pozzetti, Steve Trettel, Michael Strube, and Anna Wienhard. Symmetric spaces for graph embeddings: A finsler-riemannian approach. In International Conference on Machine Learning, pages 7090-7101. PMLR, 2021. (Cited on p. 37)

Aaron Lou, Derek Lim, Isay Katsman, Leo Huang, Qingxuan Jiang, Ser Nam Lim, and Christopher M De Sa. Neural manifold ordinary differential equations. Advances in Neural Information Processing Systems, 33:17548-17558, 2020. (Cited on p. 2)

Brice Loustau. Hyperbolic geometry. arXiv e-prints, pages arXiv-2003, 2020. (Cited on p. 18)

Suraj Maharjan, John Arevalo, Manuel Montes, Fabio A González, and Thamar Solorio. A multi-task approach to predict likability of books. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1217-1227, 2017. (Cited on p. 29)

Tudor Manole, Sivaraman Balakrishnan, Jonathan Niles-Weed, and Larry Wasserman. Plugin estimation of smooth optimal transport maps. arXiv preprint arXiv:2107.12364, 2021. (Cited on p. 27)

Tudor Manole, Sivaraman Balakrishnan, and Larry Wasserman. Minimax confidence intervals for the sliced wasserstein distance. Electronic Journal of Statistics, 16(1):2252-2345, 2022. (Cited on p. 27, 37)

Kanti V Mardia, Peter E Jupp, and KV Mardia. Directional statistics, volume 2. Wiley Online Library, 2000. (Cited on p. 1)

Emile Mathieu and Maximilian Nickel. Riemannian continuous normalizing flows. Advances in Neural Information Processing Systems, 33:2503-2515, 2020. (Cited on p. 2)

Robert J McCann. Polar factorization of maps on riemannian manifolds. Geometric $\mathcal{G}$ Functional Analysis GAFA, 11(3):589-608, 2001. (Cited on p. 2, 7)

Pascal Mettes, Mina Ghadimi Atigh, Martin Keller-Ressel, Jeffrey Gu, and Serena Yeung. Hyperbolic deep learning in computer vision: A survey. arXiv preprint arXiv:2305.06611, 2023. (Cited on p. 15)

Dimitri Meunier, Massimiliano Pontil, and Carlo Ciliberto. Distribution regression with sliced wasserstein kernels. In International Conference on Machine Learning, pages 15501-15523. PMLR, 2022. (Cited on p. 26, 27)

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013. (Cited on p. 29)

Gal Mishne, Zhengchao Wan, Yusu Wang, and Sheng Yang. The numerical stability of hyperbolic representation learning. In International Conference on Machine Learning, pages 24925-24949. PMLR, 2023. (Cited on p. 31)

Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. Pytorch metric learning. arXiv preprint arXiv:2008.09164, 2020. (Cited on p. 29)

Kimia Nadjahi, Alain Durmus, Umut Simsekli, and Roland Badeau. Asymptotic guarantees for learning generative models with the sliced-wasserstein distance. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. (Cited on p. 27)

Kimia Nadjahi, Alain Durmus, Lénaïc Chizat, Soheil Kolouri, Shahin Shahrampour, and Umut Simsekli. Statistical and topological properties of sliced probability divergences. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 20802-20812. Curran Associates, Inc., 2020. (Cited on p. 25, 27, 55, 56)

Yoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, and Masanori Koyama. A wrapped normal distribution on hyperbolic space for gradient-based learning. In International Conference on Machine Learning, pages 4693-4702. PMLR, 2019. (Cited on p. 7)

Khai Nguyen and Nhat Ho. Control variate sliced wasserstein estimators. arXiv preprint arXiv:2305.00402, 2023a. (Cited on p. 5)

Khai Nguyen and Nhat Ho. Energy-based sliced wasserstein distance. arXiv preprint arXiv:2304.13586, 2023b. (Cited on p. 12)

Khai Nguyen, Nhat Ho, Tung Pham, and Hung Bui. Distributional sliced-wasserstein and applications to generative modeling. In International Conference on Learning Representations, 2021a. (Cited on p. 12)

Khai Nguyen, Son Nguyen, Nhat Ho, Tung Pham, and Hung Bui. Improving relational regularized autoencoders with spherical sliced fused gromov wasserstein. In International Conference on Learning Representations, 2021b. (Cited on p. 12)

Khai Nguyen, Nicola Bariletto, and Nhat Ho. Quasi-monte carlo for 3d sliced wasserstein. arXiv preprint arXiv:2309.11713, 2023. (Cited on p. 5)

Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical representations. Advances in neural information processing systems, 30, 2017. (Cited on p. 1, 15,35 )

Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. In International Conference on Machine Learning, pages 37793788. PMLR, 2018. (Cited on p. 15)

Sloan Nietert, Ziv Goldfeld, Ritwik Sadhu, and Kengo Kato. Statistical, robustness, and computational guarantees for sliced wasserstein distances. Advances in Neural Information Processing Systems, 35:28179-28193, 2022. (Cited on p. 37)

Jonathan Niles-Weed and Philippe Rigollet. Estimation of wasserstein distances in the spiked transport model. Bernoulli, 28(4):2663-2688, 2022. (Cited on p. 27)

Vanni Noferini. A formula for the fréchet derivative of a generalized matrix function. SIAM Journal on Matrix Analysis and Applications, 38(2):434-457, 2017. (Cited on p. 21)

Ruben Ohana, Kimia Nadjahi, Alain Rakotomamonjy, and Liva Ralaivola. Shedding a PAC-Bayesian light on adaptive sliced-Wasserstein distances. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 26451-26473. PMLR, 23-29 Jul 2023. (Cited on p. 12)

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Thumbs up? sentiment classification using machine learning techniques. arXiv preprint cs/0205070, 2002. (Cited on p. 29)

Jiwoong Park, Junho Cho, Hyung Jin Chang, and Jin Young Choi. Unsupervised hyperbolic representation learning via message passing auto-encoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5516-5526, 2021. (Cited on p. 35)

Sangmin Park and Dejan Slepčev. Geometry and analytic properties of the sliced wasserstein space. arXiv preprint arXiv:2311.05134, 2023. (Cited on p. 37)

François-Pierre Paty and Marco Cuturi. Subspace robust wasserstein distances. In International Conference on Machine Learning, pages 5072-5081. PMLR, 2019. (Cited on p. 37,39 )

Ofir Pele and Michael Werman. Fast and robust earth mover's distances. In 2009 IEEE 12th international conference on computer vision, pages 460-467. IEEE, 2009. (Cited on p. 3)

Wei Peng, Tuomas Varanka, Abdelrahman Mostafa, Henglin Shi, and Guoying Zhao. Hyperbolic deep neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(12):10023-10044, 2021. (Cited on p. 15)

Xavier Pennec. Intrinsic statistics on riemannian manifolds: Basic tools for geometric measurements. Journal of Mathematical Imaging and Vision, 25:127-154, 2006. (Cited on p. 7)

Xavier Pennec. Manifold-valued image processing with spd matrices. In Riemannian geometric statistics in medical image analysis, pages 75-134. Elsevier, 2020. (Cited on p. $18,19,20,21,61$ )

Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A riemannian framework for tensor computing. International Journal of computer vision, 66(1):41-66, 2006. (Cited on p. 19)

Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends $\_$in Machine Learning, 11(5-6):355-607, 2019. (Cited on p. 3, 4, 25)

Aram-Alexandre Pooladian, Carles Domingo-Enrich, Ricky TQ Chen, and Brandon Amos. Neural optimal transport with lagrangian costs. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023. (Cited on p. 15)

Alison Pouplin, David Eklund, Carl Henrik Ek, and Søren Hauberg. Identifying latent distances with finslerian geometry. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. (Cited on p. 37)

Michael Quellmalz, Robert Beinert, and Gabriele Steidl. Sliced optimal transport on the sphere, 2023. (Cited on p. 2, 13)

Michael Quellmalz, Léo Buecher, and Gabriele Steidl. Parallelly sliced optimal transport on spheres and on the rotation group. arXiv preprint arXiv:2401.16896, 2024. (Cited on p. 13)

Julien Rabin, Gabriel Peyré, Julie Delon, and Marc Bernot. Wasserstein barycenter and its application to texture mixing. In Scale Space and Variational Methods in Computer Vision: Third International Conference, SSVM 2011, Ein-Gedi, Israel, May 29-June 2, 2011, Revised Selected Papers 3, pages 435-446. Springer, 2012. (Cited on p. 4)

Alain Rakotomamonjy, Mokhtar Z Alaya, Maxime Berar, and Gilles Gasso. Statistical and topological properties of gaussian smoothed sliced probability divergences. arXiv preprint arXiv:2110.10524, 2021. (Cited on p. 39)

Danilo J Rezende and Sébastien Racanière. Implicit riemannian concave potential maps. arXiv preprint arXiv:2110.01288, 2021. (Cited on p. 2)

Danilo Jimenez Rezende, George Papamakarios, Sébastien Racaniere, Michael Albergo, Gurtej Kanwar, Phiala Shanahan, and Kyle Cranmer. Normalizing flows on tori and spheres. In International Conference on Machine Learning, pages 8083-8092. PMLR, 2020. (Cited on p. 2)

Joel W Robbin and Dietmar A Salamon. Introduction to differential geometry. ETH, Lecture Notes, preliminary version, 18, 2011. (Cited on p. 8)

Noam Rozen, Aditya Grover, Maximilian Nickel, and Yaron Lipman. Moser flow: Divergence-based generative modeling on manifolds. Advances in Neural Information Processing Systems, 34:17669-17680, 2021. (Cited on p. 2)

Boris Rubin. Notes on radon transforms in integral geometry. Fractional Calculus and Applied Analysis, 6(1):25-72, 2003. (Cited on p. 25)

Raif M. Rustamov and Subhabrata Majumdar. Intrinsic sliced Wasserstein distances for comparing collections of probability distributions on manifolds and graphs. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 29388-29415. PMLR, 23-29 Jul 2023. (Cited on p. 2, 13)

David Sabbagh, Pierre Ablin, Gaël Varoquaux, Alexandre Gramfort, and Denis A Engemann. Manifold-regression to predict from meg/eeg brain signals without source modeling. Advances in Neural Information Processing Systems, 32, 2019. (Cited on p. 18)

Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, $N Y, 55$ (58-63):94, 2015. (Cited on p. 3, 24)

Filippo Santambrogio. \{Euclidean, metric, and Wasserstein\} gradient flows: an overview. Bulletin of Mathematical Sciences, 7:87-154, 2017. (Cited on p. 31)

Christopher Scarvelis and Justin Solomon. Riemannian metric learning via optimal transport. In The Eleventh International Conference on Learning Representations, 2023. (Cited on p. 15)

Meyer Scetbon and Marco Cuturi. Low-rank optimal transport: Approximation, statistics and debiasing. Advances in Neural Information Processing Systems, 35:6802-6814, 2022. (Cited on p. 2)

Zhongmin Shen. Lectures on Finsler geometry. World Scientific, 2001. (Cited on p. 37)

Ondrej Skopek, Octavian-Eugen Ganea, and Gary Bécigneul. Mixed-curvature variational autoencoders. In International Conference on Learning Representations, 2020. (Cited on p. 22)

Stefan Sommer, Tom Fletcher, and Xavier Pennec. Introduction to differential and riemannian geometry. In Riemannian Geometric Statistics in Medical Image Analysis, pages $3-37$. Elsevier, 2020. (Cited on p. 5)

Sho Sonoda, Isao Ishikawa, and Masahiro Ikeda. Fully-connected network on noncompact symmetric space and ridgelet transform based on helgason-fourier analysis. In International Conference on Machine Learning, pages 20405-20422. PMLR, 2022. (Cited on p. 25)

Thibault Séjourné, Clément Bonet, Kilian Fatras, Kimia Nadjahi, and Nicolas Courty. Unbalanced optimal transport meets sliced-wasserstein. arXiv preprint arXiv:2306.07176, 2023. (Cited on p. 37)

Yann Thanwerdas and Xavier Pennec. O (n)-invariant riemannian metrics on spd matrices. Linear Algebra and its Applications, 661:163-201, 2023. (Cited on p. 20, 21)

James Thornton, Michael Hutchinson, Emile Mathieu, Valentin De Bortoli, Yee Whye Teh, and Arnaud Doucet. Riemannian diffusion schrödinger bridge, 2022. (Cited on p. 2)

Huy Tran, Yikun Bai, Abihith Kothapalli, Ashkan Shahbazi, Xinran Liu, Rocio Diaz Martin, and Soheil Kolouri. Stereographic spherical sliced wasserstein distances, 2024. (Cited on p. 13)

Oncel Tuzel, Fatih Porikli, and Peter Meer. Region covariance: A fast descriptor for detection and classification. In Computer Vision-ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part II 9, pages 589-600. Springer, 2006. (Cited on p. 18)

Cédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009. (Cited on p. 2, 3, 7, 27)

Jörg A Walter. H-mds: a new approach for interactive visualization with multidimensional scaling in the hyperbolic space. Information systems, 29(4):273-292, 2004. (Cited on p. 30 )

Yifei Wang, Peng Chen, and Wuchen Li. Projected wasserstein gradient descent for highdimensional bayesian inference. SIAM/ASA Journal on Uncertainty Quantification, 10 (4):1513-1532, 2022. (Cited on p. 32)

Jiaqi Xi and Jonathan Niles-Weed. Distributional convergence of the sliced wasserstein process. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. (Cited on p. 37)

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. (Cited on p. 31)

Xianliang $\mathrm{Xu}$ and Zhongyi Huang. Central limit theorem for the sliced 1-wasserstein distance and the max-sliced 1-wasserstein distance. arXiv preprint arXiv:2205.14624, 2022. (Cited on p. 37)

Or Yair, Felix Dietrich, Ronen Talmon, and Ioannis G Kevrekidis. Domain adaptation with optimal transport on the manifold of spd matrices. arXiv preprint arXiv:1906.00616, 2019. (Cited on p. 18)

Ryoma Yataka and Masashi Shiraishi. Grassmann manifold flow. arXiv preprint arXiv:2211.02900, 2022. (Cited on p. 2)

Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian svrg: Fast stochastic optimization on riemannian manifolds. Advances in Neural Information Processing Systems, 29, 2016. (Cited on p. 6)

Rixin Zhuang, Zhengming Ma, Weijia Feng, and Yuanping Lin. Spd data dictionary learning based on kernel learning and riemannian metric. IEEE Access, 8:61956-61972, 2020. (Cited on p. 18)


[^0]:    1. Code available at https://github.com/clbonet/Sliced-Wasserstein_Distances_and_Flows_on_ Cartan-Hadamard_Manifolds
[^1]:    2. We expect it to be true in general as $\phi$ is an isometry, but we did not find in the literature a formal proof. In practice, this fact was verified for each tested case.
