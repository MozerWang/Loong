# Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents 

Zhuosheng Zhang ${ }^{\star, *}$, Yao Yao ${ }^{\star, *}$, Aston Zhang ${ }^{\star}$, Xiangru Tang ${ }^{\star}$, Xinbei Ma ${ }^{\star}$, Zhiwei He ${ }^{\star}$, Yiming Wang ${ }^{\star}$,<br>Mark Gerstein ${ }^{*}$, Rui Wang ${ }^{*}$, Gongshen Liu*, Hai Zhao ${ }^{*}$<br>\{zhangzs,yaoyao27,sjtumaxb,zwhe.cs,wangruil2,lgshen\}@ sjtu.edu.cn, az@astonzhang.com, \{xiangru.tang,mark.gerstein\}@yale.edu,<br>alsaceym@gmail.com, zhaohai@cs.sjtu.edu.cn<br>"Shanghai Jiao Tong University, "Amazon Web Services, " Yale University

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-01.jpg?height=782&width=1622&top_left_y=790&top_left_x=250)

Figure 1: An overview of language agent framework empowered with the chain-of-thought (CoT) mechanism in perception, memory, and reasoning.


#### Abstract

Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought $(\mathrm{CoT})$ reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. We hope to offer readers a comprehensive understanding of prevalent research areas such


[^0]as CoT reasoning and language agents and illuminate the interconnections weaving through these areas. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics. A repository for the related papers is available at https://github.com/Zoeyyao27/CoT-Igniting-Agent.

## 1 Introduction

Language intelligence pertains to the aptitude for comprehending and reasoning through concepts articulated in natural languages (Sternberg et al., 1982; Ryan \& Lopez, 2001, Ramsden et al., 2011; Luwel et al., 2013). Spurred by advancements in scale, large language models (LLMs) have achieved remarkable progress in pursuing human-level language intelligence, compellingly evidenced by the strong empirical benchmarking performance in complex reasoning tasks (Wei et al. 2022), as well as theoretical proofs for the emergent reasoning abilities (Prystawski \& Goodman. 2023; Wang \& Wang, 2023; Bi et al. 2023).

Reasoning, a pivotal research topic within the realm of language intelligence, is characterized as a multi-step process wherein inferences are drawn from discrete pieces of evidence, culminating in the formation of more abstract concepts that are instrumental in facilitating high-level predictions (Morency et al., 2022, Yu et al., 2023a; Huang \& Chang, 2023). Recent research revealed that remarkable enhancements in performance could be attained by prompting LLMs to engage in a step-by-step reasoning process, as opposed to generating answers in a direct manner (Nye et al., 2022; Wei et al. 2023b). The way to prompt LLMs to generate a series of intermediate reasoning steps for solving a problem is called chain-of-thought (CoT) prompting (Wei et al. 2023b).

Optimization of CoT prompting techniques has garnered escalating interest, catalyzing notable paradigm shifts within the CoT framework. These shifts encompass three key aspects: (i) prompting pattern: from manual design of in-context learning demonstrations to automatic prompt construction (Zhang et al., 2023c, Zhou et al., 2023f, Yang et al., 2023a); (ii) reasoning format: from unstructured natural language formats to structured ones (Chen et al. 2022; Ziqi \& Lu, 2023, Yao et al., 2023a c); (iii) application scenario: from singular language settings to multilingual environments (Shi et al. 2023), from a language modality to embracing multimodal approaches (Zhang et al., 2023d), and from complex reasoning tasks to general-purpose tasks (Wang et al., 2022, Li et al., 2022; Wang et al., 2023g, He et al., 2023).

CoT reasoning is a representative emergent ability of LLMs (Wei et al., 2022). It provides a proficient strategy for deconstructing intricate issues into smaller, manageable sub-problems, systematically enabling solutions through a step-by-step approach (Figure 2). Leveraging the reasoning capabilities developed during pre-training (Xie et al., 2022, Wang et al. 2023a), CoT prompting adeptly identifies atomic knowledge components essential for reasoning processes and seamlessly integrates their relationships, thereby constructing intermediate, coherent reasoning steps (Prystawski \& Goodman, 2023, Wang \& Wang, 2023). In addressing these sub-problems, the reasoning process can be further enhanced by employing knowledge retrieval and verification tools (Gou et al., 2023a, Qin et al. 2023b). By expanding CoT into a comprehensive framework for perception, memory, and reasoning, language agents, powered by LLMs, have been formulated to adeptly adhere to language instructions and execute actions in either real-world or simulated environments (Rawles et al. 2023, Zhang \& Zhang, 2023) (Figure 1). These language agents come in two flavors: (i) autonomous agents (Adept, 2022; Richards, 2023; Hong et al., 2023; Nakajima, 2023) and (ii) communicative agents (Park et al., 2023, Wang et al., 2023c, Zhu et al., 2023; Hong et al., 2023).

In this paper, we navigate through research topics that encompass: (i) unraveling the underlying mechanisms of CoT techniques, with a particular focus on discerning when and why CoT is effective; (ii) identifying and analyzing the paradigm shift occurring within CoT; and (iii) investigating the advent of language agents enabled by CoT techniques. The rest of this paper is structured for a coherent and sequential exploration. Initially, we immerse ourselves in the fundamental aspects of CoT reasoning, which encompasses its defining features and the merits arising from employing CoT techniques. Subsequently, we delve deeper into the inherent mechanisms of CoT, striving to elucidate the specific conditions and reasons that determine its functionality. In the ensuing section, we classify paradigm shifts, directing our attention towards various prompting techniques, reasoning formats, and application scenarios. Following that, we explore the emerging landscape of language agents, spotlighting those facilitated by CoT techniques. To conclude, we engage in a discussion about the challenges encountered and future opportunities looming on the horizon.

Various related papers have selectively concentrated on distinct facets of LLMs (Zhao et al. 2023c), CoT reasoning Lu et al. 2023; Qiao et al., 2023, Chu et al. 2023; Yu et al., 2023b), and autonomous agents (Wang et al., 2023b; Xi et al., 2023), each providing overviews and taxonomies tailored to their respective domains. In contrast, our paper transcends a mere summarization and aspires to furnish a thorough exploration of the fundamental mechanisms that underscore CoT reasoning, alongside a deep dive into the paradigm shifts enveloping this domain. Moreover, our paper chronicles the trajectory from CoT reasoning in LLMs to the most recent advancements in autonomous language agents, with a dedicated aim to illuminate the intricate interconnections weaving through these crucial areas of study. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and cutting-edge discussions on these topics.

Key Takeaways To the best of our knowledge, this constitutes the inaugural work to systematically explore the foundational mechanics of CoT techniques, the paradigm shift in CoT, and the complex interplay between CoT and agents. The key takeaways are:

- CoT demonstrates efficacy under two overarching conditions: first, when an LLM with preferably at least 20 billion parameters is employed, and second, when the parametric knowledge within the LLM encompasses knowledge pieces that are (i) pertinent to the task at hand and (ii) maintain strong mutual interconnections (Section 3.1).
- CoT functions by assisting in the identification of atomic knowledge pieces pivotal for reasoning and seamlessly connecting these components via the formation of intermediate reasoning steps (Section 3.2.
- CoT techniques have experienced substantial paradigm shifts, embracing alterations in prompting patterns, reasoning formats, and application scenarios (Section 4 ).
- CoT has acted as a catalyst in the evolution of LLM-empowered agents capable of understanding language instructions and executing actions in both real-world and simulated environments, specifically augmenting agent capabilities in perception, memory, and reasoning (Section 5 ).
- Despite the swift advancement of LLMs, CoT reasoning, and language agents, numerous challenges persist, such as generalization to unseen domains, efficiency amidst redundant interactions, customization of language agents, scaling up of language agents, and ensuring the safety of language agents (Section6).


## 2 Preliminaries of CoT

In this section, we immerse ourselves in the fundamental elements of CoT reasoning. Firstly, we carve out a distinct contrast between $\mathrm{CoT}$ reasoning and the traditional approach of direct reasoning. Subsequently, we proffer definitions for the key components within CoT. Finally, we delineate the advantages of adopting CoT.

### 2.1 Definition

The concept of chain-of-thought refers to a series of intermediate reasoning steps that are generated to solve a problem or arrive at an answer (Wei et al. 2023b), in the form of <input $\rightarrow$ reasoning chain (rationale) $\rightarrow$ output $>$ mappings. This approach is often more effective than traditional direct reasoning, which attempts to tackle the entire problem all at once. For example, standard classification, multiple choice, and question answering problems often leverage direct reasoning in the form of $<$ input $\rightarrow$ output $>$ mappings.

To elucidate CoT, we establish standard definitions for its key components as illustrated in Figure 2 Formally, assuming that the reasoning dataset distribution is $\mathcal{D}$, we denote $s=(x, y) \sim \mathcal{D}$ as a sampling on $\mathcal{D}$, where $x$ and $y$ denote the question (input) and the answer (output), respectively, and they are both in the form of text sequences. We use $|x|$ to denote the length of the sequence $x$, and $p_{\theta}$ to denote the pre-trained language model parameterized with $\theta$. Details of definitions are as follows:

Instruction. Instructions are usually short sentences used to prompt an LLM to generate answers in the desired format. They guide the LLM to think step by step in the reasoning process. We notate the instruction as $p$, which is set to different text sequences depending on the task requirements.

Rationale. We uniformly refer to the intermediate processes of CoT reasoning as "rationales". Rationales can encompass solutions, intermediate reasoning steps, or any relevant external knowledge pertaining to a question. We

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-04.jpg?height=808&width=851&top_left_y=276&top_left_x=255)

| Exemplars | Instruction |
| :---: | :---: |
| Direct Reasoning for Agent <br> Input <br> You are in the middle of a room. <br> Looking quickly around you, you <br> see a drawer 2, a shelf 6 , a shelf <br> 9 , a cabinet 2, a sofa 1, a safe 1, <br> a garbage can 1, and a armchair <br> 1. <br> Your task is to: put the pillow on <br> sofa. -------- <br> Output <br> $>$ go to drawer 2 <br> You arrive at loc 4 . The drawer <br> 2 is closed. <br> > take pillow 1 from drawer <br> 2 <br> Nothing happens <br> > go to sofa 1 <br> Nothing happens <br> > put pillow 1 in/on sofa 1 <br> Nothing happens | Chain-of-Thought for Agent <br> Input <br> You are in the middle of a room. Looking <br> quickly around you, you see a drawer 2, a <br> shelf 6, a shelf 9, a cabinet 2, a sofa 1, a <br> safe 1, a garbage can 1, and a armchair 1 . <br> Your task is to: put the pillow on sofa. <br> Output <br> First I need to find the pillow which is <br> more likely to be in the drawer 2 . <br> > go to drawer 2 <br> You arrive at loc 4 . The drawer 2 is closed. <br> The drawer is closed. I should open it first <br> > Open the drawer <br> You open the drawer 2 . The drawer 2 is <br> open. In it, you see a pillow 1 . <br> > take pillow 1 from drawer 2 <br> You pick up the pillow 1 from the drawer <br> 2 . <br> Now I found the pillow. I should put it on <br> the sofa. <br> > go to sofa 1 <br> You arrive at loc 3 . <br> > put pillow 1 in/on sofa 1 <br> You won! |

Figure 2: Comparison between CoT reasoning and direct reasoning. CoT refers to a series of intermediate reasoning steps that are generated to solve a problem or arrive at an answer (Wei et al. 2023b). This approach is often more effective than direct reasoning, which attempts to tackle the entire problem all at once.

define rationale as $r$. If $r$ is generated by the LLM, instruction $\mathrm{p}$ can be used to obtain $r \sim p_{\theta}(x, \mathrm{p})$. If $r$ is written by a human, instruction $\mathrm{p}$ can be exempted, and $r=f(x)$, where $f(\cdot)$ indicates the handwriting operation.

Exemplars. Exemplars are typically presented as desired input-output pairs in few-shot prompting approaches, each of which contains questions, a rationale, and an answer. Exemplars serve as in-context demonstrations of input-output relationships before generating predictions for test-time examples. Exemplars are usually concatenated before input questions. Specifically, assuming the exemplar size of $n$, exemplars $E$ can be formulated as:

$$
\begin{equation*}
E=\left[\left(x_{1}, r_{1}, y_{1}\right) \circ \cdots \circ\left(x_{n-1}, r_{n-1}, y_{n-1}\right) \circ\left(x_{n}, r_{n}, y_{n}\right)\right] \tag{1}
\end{equation*}
$$

where $\circ$ represents concatenation, $\left(x_{i}, y_{i}\right) \sim \mathcal{D}$ and $r_{i}=f\left(x_{i}\right)$.

Zero-Shot-CoT. Zero-Shot-CoT does not require users to provide exemplars. Instead, it typically relies on instructions to facilitate the LLM in conducting step-by-step reasoning, thereby generating answers. For example, Kojima et al. (2023) first elicited the LLM to generate the rationale $r$ using the instruction $\mathrm{p}_{1}$ such as "Let's think step by step", and then use the instruction $\mathrm{p}_{2}$ such as "The answer is" to obtain the final answer following the question and rationale. Formally, the output $y$ can be computed as follows:

$$
\begin{equation*}
r \sim \prod_{i=1}^{|r|} p_{\theta}\left(r_{i} \mid x, \mathrm{p}_{1}, r_{<i}\right), \quad y \sim \prod_{i=1}^{|y|} p_{\theta}\left(y_{i} \mid x, \mathrm{p}_{1}, r, \mathrm{p}_{2}, y_{<i}\right) \tag{2}
\end{equation*}
$$

Few-Shot-CoT. Few-Shot-CoT involves providing a set of exemplars with associated rationales. These exemplars are concatenated with the question to prompt the LLM to generate the rationale and answer. In this setting, the output $y$ is obtained in an end-to-end mode, which can be formulated as:

$$
\begin{equation*}
y \sim \prod_{i=1}^{|y|} p_{\theta}\left(y_{i} \mid E, x, y_{<i}\right) \tag{3}
\end{equation*}
$$

### 2.2 Benefits of CoT

CoT techniques have shown various kinds of benefits, including improved reasoning performance, interpretability, controllability, and flexibility. We summarize them in detail below.

Improved Reasoning Performance CoT facilitates a step-by-step progression in the reasoning process for LLMs. By breaking down complex, multi-step problems into intermediate stages, CoT minimizes the risk of overlooking crucial details. Moreover, it ensures the efficient allocation of additional computational resources to problems demanding a higher degree of reasoning steps. Numerous studies have conclusively demonstrated the efficacy of CoT across a wide range of domains, encompassing arithmetic reasoning, commonsense reasoning, and symbolic reasoning (Wei et al. 2023b, Kojima et al., 2023; Wang et al., 2023f).

Improved Interpretability CoT offers an interpretable glimpse into the decision-making process of LLMs. Breaking down complex reasoning tasks into a chain of interconnected thoughts makes it easier to understand the underlying logic and reasoning behind a decision or conclusion made by LLM. It sheds light on how the model may have reached a specific answer, offering valuable insights for debugging and pinpointing where the reasoning process may have deviated from the correct path. However, it is important to note that fully characterizing the model's computations supporting an answer still presents an open challenge (Wei et al. 2023b).

Improved Controllability By prompting LLMs to output a chain of interconnected thoughts, users can exert greater influence over the cognitive processes of LLM. Many studies (Yao et al. 2023a; Ling et al. 2023) were dedicated to the identification and rectification of specific thought units where the reasoning path may have gone off track or where additional information is required. This increased controllability allows for more deliberate and accurate answers.

Improved Flexibility The utilization of CoT reasoning can be easily prompted in adequately large, off-the-shelf LLMs by simply adding instruction at the end of the input question for Zero-Shot-CoT or incorporating CoT exemplars used for Few-Shot-CoT (Wei et al. 2023b). The flexibility of CoT extends beyond the realm of reasoning tasks, making it applicable to a wide range of fields, including classic natural language processing (NLP), scientific applications, and agent-based systems.

## 3 Underlying Mechanism of CoT

This section explores the foundational mechanisms of CoT, encompassing the general conditions that determine when and why CoT is effective.

### 3.1 When CoT Works

Although CoT has shown promising benefits, it may not be suitable in any conditions (Kojima et al., 2023, Wei et al. 2023b; Zhang et al., 2023d). We will introduce when CoT works in engineering and theoretical perspectives. Then we summarize the general conditions to suggest the effective application scopes of CoT reasoning.

- From an engineering perspective, Wei et al. (2023b) thought that CoT reasoning is helpful under three conditions: (i) an LLM is used; (ii) the task is challenging and requires multi-step reasoning; (iii) the performance of direct prompting does not increase dramatically while scaling the model size. Notably, Tay et al. (2022) further provided evidence that LLMs with 20 billion parameters, pre-trained on a mixture of denoising functions, can also achieve effective CoT reasoning ${ }^{1}$ Otherwise, CoT techniques tend to struggle with smaller LLMs (Wei et al., 2022). It may lead to hallucination because of lacking supportive knowledge in LLMs (Zhang et al., 2023d) and inferior reasoning capabilities (Magister et al. 2022). CoT reasoning is also less effective in simple-step tasks such as matching, sequence labeling (Qin et al. 2023a), and single-choice question (Chen et al. 2023a).
- From a theoretical perspective, Prystawski \& Goodman (2023) proved that CoT reasoning is helpful when training data (possibly considered as the parametric knowledge in an LLM) consists of local clusters of variables that strongly[^1]influence each other. This finding implied that the LLM must have the knowledge related to the task to support CoT reasoning. We call such knowledge as atomic knowledge.

As CoT reasoning is often elicited by in-context learning (ICL), such as Zero-Shot-CoT and Few-Shot-CoT, another line of study tries to understand when CoT works from the perspective of ICL. Zhang et al. (2023c) showed that CoT reasoning works effectively when prompted with diverse exemplars. Wang et al. (2023a) found that rationales being relevant to the query and correctly ordering the reasoning steps are the keys to the effectiveness of CoT prompting.

Besides prompting, introducing reasoning materials and necessary knowledge for LLMs in the training corpus has also exhibited a profound improvement in CoT reasoning ability in LLMs (Yu et al. 2023b). Recent studies found that pre-training with code data (Chung et al., 2022) or fine-tuning (e.g., instruction tuning) with CoT-style data (Yue et al., 2023 ) is beneficial for effective CoT reasoning. That is, the CoT reasoning in the same LLMs can be improved or the CoT reasoning ability can be induced in smaller models.

Based on the discussion above, CoT demonstrates efficacy under two overarching conditions: first, when an LLM with preferably at least 20 billion parameters is employed, and second, when the parametric knowledge within the LLM encompasses knowledge pieces that are (i) pertinent to the task at hand and (ii) maintain strong mutual interconnections.

### 3.2 Why CoT Works

Recent studies have employed both empirical and theoretical approaches in an effort to comprehend the underlying reasons for the effectiveness of CoT.

- Empirically, Wei et al. (2023b) believed that the success of CoT reasoning constitutes a multifaceted phenomenon that likely involves various emergent abilities. Those abilities include semantic understanding, symbol mapping, topic coherence, arithmetic ability, and faithfulness. Interestingly, Zhang et al. (2023c) found that mistakes in exemplar rationales do not lead to significant performance drops. Wang et al. (2023a) reported a similar observation that LLMs can generate coherent reasoning steps and achieve over $80-90 \%$ of the performance, though prompted with invalid reasoning steps in the exemplars. Those findings imply that LLMs already have an innate ability to reason after pre-training (Zhang et al., 2023c, Wang \& Wang, 2023). CoT prompting specifies an output format that regularizes the model generation to generate step-by-step while being in order and relevant to the query (Wang et al., 2023a). In other words, CoT techniques help compel the model to conduct reasoning rather than teaching it how to accomplish reasoning (Zhang et al., 2023c).
- Theoretically, Bayesian inference is a popular way to investigate why CoT works from a theoretical perspective (Prystawski \& Goodman, 2023; Wang \& Wang, 2023). Prystawski \& Goodman (2023) proved that CoT is effective when the training data exhibits a localized structure with respect to dependencies between variables. In the context of LLMs, the proof can be interpreted that the parametric knowledge within the LLM comprises knowledge pieces that are related to the target problem, and those knowledge pieces exert strong mutual connections with each other. To verify the proof, Bi et al. (2023) conducted an empirical study on code data and found that the local structural properties of the data are crucial for improving CoT reasoning abilities. These findings in Prystawski \& Goodman (2023) and Bi et al. (2023) compellingly indicated that CoT may help identify the atomic pieces of knowledge used for reasoning and bridge the relationship between the atomic pieces of knowledge with intermediate reasoning steps. Similarly, Wang \& Wang (2023) used knowledge graphs for analysis and found that organizing the known facts as "chains", i.e., CoT, can significantly impact the effectiveness of reasoning. By doing so, LLMs are able to accurately deduce previously unseen facts from known ones to answer a given query without explicitly encoding reasoning rules.


## 4 Paradigm Shifts of CoT

After elucidating the general conditions determining when and why CoT is effective, we seek to achieve a more profound and intuitive understanding of the improvements in CoT's reasoning capabilities for LLMs. To this end, we compile and summarize the best performances of CoT across seven of the most emblematic reasoning tasks as of October 2023. We compare these performances with those achieved without CoT and present our findings in Figure 3 These seven reasoning tasks span across distinct categories, including: (i) Arithmetic Reasoning: GSM8K (Cobbe et al., 2021), AQuA (Ling et al. 2017), and SVAMP (Patel et al. 2021); (ii) Commonsense Reasoning: CSQA (Talmor et al.

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-07.jpg?height=710&width=1589&top_left_y=336&top_left_x=257)

Figure 3: Performance on seven reasoning tasks. "Direct Prompt" refers to the standard few-shot prompting approach, where exemplars are formatted as questions and answers, with the model providing direct answers. "Best CoT w/o SC", "Best CoT w/ SC" and "Best CoT*" represent the highest accuracy (\%) achieved as of October 2023 ("SC" stands for self-consistency (Wang et al., 2023f).). While the first two uses the text-davinci-002 as the LLM engine, the latter allows for the model employed to vary for each task (details in Table 1). For a fair comparison, the performances of "Direct Prompt", "Manual-CoT", "Best CoT w/o SC" and "Best CoT w/ SC" are all based on using the text-davinci-002 as the LLM engine.

2019) and Strategy QA (Geva et al., 2021); (iii) Symbolic Reasoning: Last Letter Concatenation (Wei et al., 2023b), and Coin Flip (Wei et al. 2023b).

Figure 3 clearly illustrates that the benchmark performance in complex reasoning tasks has advanced rapidly, with CoT exerting a significant influence on the reasoning abilities of LLMs across all seven tasks. Notably, apart from commonsense reasoning, the relatively straightforward CoT format Manual-CoT proposed by Wei et al. (2023b) substantially improves overall accuracy compared to the direct prompt in both arithmetic and symbolic reasoning.

The deficiency in CoT's performance regarding commonsense reasoning tasks has been observed both in Manual-CoT (Wei et al. 2023b) and Zero-Shot-CoT Kojima et al. (2023). However, when CoT is integrated with a significantly larger $\mathrm{PaLM}$ (540B) model, it consistently enhances commonsense reasoning. Notably, Zero-Shot-CoT also notes that the rationales generated through CoT often exhibit logical correctness or only contain human-understandable errors. This suggests that CoT encourages improved commonsense reasoning, even when the task metrics do not explicitly measure it. While Manual-CoT fails to yield performance gains in commonsense reasoning, the optimization of reasoning techniques, such as self-consistency answer aggregation (Wang et al., 2023f) and automatic exemplar construction (Shum et al., 2023), reveals the potential of CoT to achieve remarkable results generally.

Moreover, for ease of reference and to provide a clear overview of how CoT can achieve top performance on the seven different datasets, we have included the latest models that have achieved the best performance and the specific LLM engine they utilized in Table 1

In conclusion, we see that compared with the vanilla prompting approach in Wei et al. (2023b), the latest CoT reasoning techniques have been strengthened throughout the full stack of the reasoning process, such as multimodal perception (Zhang et al., 2023d, Yao et al., 2023c, Huang et al., 2023b, Rose et al., 2023), automatic prompting (Zhang et al., 2023c; Diao et al. 2023), reasoning verification (Weng et al., 2022; Lightman et al., 2023, Ling et al., 2023), and consistency-based sampling (Wang et al., 2023f, 2022).

Table 1: Best CoT* on seven reasoning tasks (SC: self-consistency by Wang et al. (2023f)).

| Category | Dataset | Model | Best Ac | LLM |
| :---: | :---: | :---: | :---: | :---: |
| Arithmetic Reasoning | GSM8K | CSV (Zhou et al. 2023b) | 97.00 | GPT-4 Code Interpreter |
|  | AQuA | Natural Program Ling et al. 2023) | 70.34 | Chatgpt |
|  | SVAMP | PoT (Chen et al. 2022$)+\mathrm{SC}$ | 89.10 | Text-davinci-002 |
| Commonsense Reasoning | CSQA | Manual-CoT (Wei et al. $2023 b)+$ SC | 95.10 | PaLM 2 |
|  | Strategy QA | Manual-CoT (Wei et al. $2023 \mathrm{~b}$ ) + SC | 90.40 | PaLM 2 |
| Symbolic Reasoning | last letter concatenation | Natural Program (Ling et al. 2023) | 92.98 | Chatgpt |
|  | Coin Flip | Auto-CoT (Zhang et al. $2023 \mathrm{c}$ ) | 99.90 | Text-davinci-002 |

With the growing interest in CoT, researchers are continually striving to harness its full potential for enhancing LLMs reasoning capabilities. In this section, we will embark on a journey through the realm of CoT research, following the map of CoT overview as illustrated in Figure 4 , delving into the comprehensive discussions of advancements made in three key directions: (i) prompting pattern; (ii) reasoning format; and (iii) application scenario.

### 4.1 Prompting Pattern

The prompting pattern can be primarily divided into two components: instruction generation and exemplar generation. Instruction generation primarily focuses on finding the optimal instructions to prompt LLM, enabling them to engage in step-by-step reasoning instead of directly answering the question. This approach mainly aims to maximize LLM's zero-shot capability. Exemplar generation primarily focuses on finding the best set of input-output demonstration exemplar pairs for Few-Shot-CoT. These exemplars are used to prompt LLMs along with a test input, enabling the model to predict the corresponding output.

### 4.1.1 Instruction Generation

Instruction generation can be categorized into two distinct methods: manual instruction generation and automatic instruction generation, based on their respective generation processes.

Early efforts primarily involved manual construction of instruction prompts. The earliest and most traditional instruction generation method was Zero-Shot-CoT proposed by Kojima et al. (2023). Zero-Shot-CoT demonstrates that large language models (LLMs) can perform zero-shot reasoning by adding a simple prompt, "Let's think step by step", before each answer. Zero-Shot-CoT outperforms zero-shot LLM performances on various reasoning tasks without the need for hand-crafted few-shot examples, marking the inception of a new era in Zero-Shot-CoT.

Wang et al. (2023d) further proposed the Plan-and-Solve (PS) Prompting to address the missing-step errors in ZeroShot-CoT reasoning. It consists of devising a plan to divide the task into smaller subtasks and carrying out the subtasks according to the plan. PS prompting consists of two stages. In the first stage, the author prompts the LLM using the proposed prompting template "Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan and solve the problem step by step" to generate the reasoning process and the answer. The second stage extracts the answer using an answer prompt (e.g., "Therefore, the answer (arabic numerals) is").

However, manually designing instructions may not always yield the desired results, and users often need to experiment with various prompts to achieve the desired behavior. In response to this challenge, Zhou et al. (2023f) proposed the Automatic Prompt Engineer (APE), a method designed for the automated generation and selection of instructions for LLMs. APE treats instruction generation as a form of natural language program synthesis and optimizes this process by searching through a pool of instruction candidates proposed by an LLM. The primary goal is to maximize a chosen score function. To elaborate further, APE initiates the process by instructing the LLM to generate a set of candidate instructions using manually crafted templates. Subsequently, it utilizes the LLM to infer the most likely instructions with the highest score, based on input-output exemplars. By harnessing the capabilities of LLMs, APE streamlines the prompt engineering process, alleviating extensive human intervention and generating high-quality instructions.

Yang et al. (2023a) presented Optimization by PROmpting (OPRO), a straightforward yet highly effective approach that harnesses the power of Language Model (LLM) as optimizers. OPRO represents a groundbreaking method in optimization, utilizing LLMs to their full potential. OPRO initiates the optimization process by presenting a natural
![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-09.jpg?height=982&width=1636&top_left_y=278&top_left_x=236)

Figure 4: Overview of representative CoT approaches. We delve into the paradigm shifts of CoT techniques in three key directions: (i) prompting pattern (instruction generation and exemplar generation); (ii) reasoning format (CoT formulation, reasoning aggregation, and CoT verification); and (iii) application scenario (multilingualism, multimodality, and general-purpose tasks).

language description of both the optimization problem and the optimization trajectory. This trajectory includes prior solutions along with their associated optimization scores. Subsequently, updated solutions are devised and evaluated for their performance and quality. The prompt for the subsequent optimization step incorporates these solutions after thorough examination. As the iterative process unfolds, the solutions undergo progressive refinement, ultimately improving their quality.

Initially, OPRO is applied to address two classic optimization challenges: the linear regression problem and the traveling salesman problem. The study then proceeds to demonstrate that prompts optimized by OPRO surpass human-designed prompts in performance, particularly on tasks such as GSM8K and Big-Bench Hard tasks. OPRO showcases its efficiency in resolving common optimization challenges and enhancing prompts by presenting optimization tasks in natural language for LLMs, consistently generating and refining solutions.

### 4.1.2 Exemplar Generation

Similar to instruction generation, exemplar generation can also be classified into two categories based on the method of constructing exemplars: manual exemplar generation and automatic exemplar generation.

Few-Shot-CoT reasoning, formally explored by Wei et al. (2023b), represents a discrete prompt learning approach that uses multiple input-output pairs to prompt the LLM to output rationales and obtain the final answer. To provide a clearer distinction, we will refer to their work as Manual-CoT. Manual-CoT follows the traditional manual exemplar generation method. In contrast to the conventional in-context learning, where LLMs are prompted with a list of input-output demonstration pairs alongside a test input to enable the model to predict the output, Manual-CoT involves prompting the model's outputs with manually designed additional logical reasoning procedures in addition to the target output.

Diao et al. (2023) took Manual-CoT a step further by optimizing the selection of exemplars and introduced ActivePrompt, which uses task-specific example prompts annotated with human-designed rationales. Active-Prompt exists in a state that falls between manual exemplar generation and automatic exemplar generation. The method selects the most uncertain questions from a pool of task-specific queries using uncertainty-based active learning metrics. Active-Prompt first asks LLM to answer questions multiple times following the Manual-CoT (Wei et al. 2023b). The model then selects the most uncertain questions based on the uncertainty metric (e.g. disagreement, entropy, variance, self-confidence), manually annotates the rationales, and uses the questions and rationales as examples for inference.

To eliminate the need for manual efforts in hand-crafting task-specific demonstrations to generate reasoning chains one by one, Zhang et al. (2023c) proposed Auto-CoT which maintains the diversity of sampled questions and generates reasoning chains to automatically construct demonstrations. Specifically, Auto-CoT consists of two main stages: (i) Problem clustering: divide the given dataset of problems into several clusters; (ii) Demonstration sampling: select a representative problem from each cluster and use Zero-Shot-CoT to generate its reasoning chain.

Shum et al. (2023) proposed a strategy called Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought) that automates the process of augmenting and selecting rational chains for CoT prompting. The process consists of three steps: augmenting the language model to generate multiple pseudo-chains, pruning the pseudo-chains based on consistency with ground-truth answers, and selecting the most helpful chain-of-thought using a variance-reduced policy gradient strategy.

### 4.2 Reasoning Format

The enhancements in reasoning format primarily encompass three aspects: CoT formulation, reasoning aggregation, and CoT verification. CoT formulation focuses on transforming the sequential $\mathrm{CoT}$ into various cognitive structures, such as tree-like, graph-like, or table-like formats, thereby incorporating structural thinking cues. Reasoning aggregation primarily concerns the enhancement of LLM CoT reasoning accuracy through the aggregation of results sampled from the LLM. CoT verification primarily emphasizes the introduction of verification methods to verify and amend the CoT reasoning process. We will elaborate on these three aspects in the following sections.

### 4.2.1 CoT Formulation

We present five representative CoT formulations in Figure 5 We will progressively delve into the CoT formulation shifts based on this illustration.

Chen et al. (2022) introduced Program-of-Thoughts (PoT) for solving complex numerical reasoning tasks. PoT uses language models to generate both text and programming language statements, which can be executed on a program interpreter to decouple complex computation from reasoning and language understanding.

Ziqi \& Lu (2023) explored the structural reasoning ablilty for LLMs. They introduce the Tabular Chain of Thought (Tab-CoT), which adopts a table-filling approach to model CoT. In Tab-CoT, an instruction of "| step | subquestion | process | result |" is manually designed to prompt LLMs to generate a table while conducting the reasoning process. The answer is then extracted from the generated table at the end of the process. Tab-CoT showcases robust zero-shot and few-shot capabilities in performing reasoning across multiple dimensions, encompassing both rows and columns.

Yao et al. (2023a) proposed Tree-of-Thoughts (ToT) that breaks CoT into thought units and formulates them into tree structure. ToT allows LLMs to explore coherent thought units that serve as intermediate steps toward problem-solving, consider different options and evaluate their decisions. By incorporating different methods, ToT is able to look ahead to determine what to do next or trace-back to correct history decisions. Experiments have demonstrated that ToT significantly elevates the problem-solving capabilities of language models. This improvement is particularly noteworthy in the context of tasks that demand intricate non-trivial planning or search processes

Based on ToT, Besta et al. (2023) further extended the tree structure into graph structure and propose Graph-of-ThoughtsRationale (To distinguish from GoT proposed by Yao et al. (2023c), Besta et al. (2023) is dubbed as GoT-rationale). GoT-rationale models the thought generation process of language models as a graph. The system architecture of GoT comprises multiple interacting modules: (i) Prompter that prepares prompt for the LLM, which are then used to generate responses; (ii) Parser that extracts information from the LLM's responses, which is then used by other modules in the architecture; (iii) Scorer that verifies and scores the LLM's replies to determine their quality and relevance to the task at

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-11.jpg?height=992&width=1632&top_left_y=285&top_left_x=252)

Figure 5: Formulation Shifts of CoT. We illustrate five representative CoT formulations in chronological order: (i) Chain-of-Thoughts (CoT), (ii) Programm-of-Thoughts (PoT) (Chen et al. 2022), (iii) Table-of-Thoughts (Tab-CoT) (Ziqi \& Lu, 2023), (iv) Tree-of-Thoughts (ToT) (Yao et al., 2023a), (v) Graph-of-Thoughts-Rationale (GoT-Rationale) (Besta et al., 2023).

hand; (iv) Controller that process with two elements: the Graph of Operations (GoO) and the Graph Reasoning State (GRS). GoO is a static structure that specifies the graph decomposition of a given task, which means it prescribes the transformations to be applied to LLM thoughts, along with their order and dependencies. GRS is a dynamic structure that maintains the state of the ongoing LLM reasoning process, which includes the history of its thoughts and their states. Experimental results indicate that GoT outperforms state-of-the-art techniques in tasks such as sorting, set operations, keyword counting, and document merging.

Lee \& Kim (2023) proposed Recursion of Thought (RoT), which empowers language models to recursively generate multiple contexts for problem-solving. In RoT, LLMs are prompted to output special tokens such as GO, THINK, and STOP, which serve to initiate context-related operations. The THINK token indicates the model needs to solve a sub-problem, which triggers a recursive process to generate a new context for that sub-problem. This innovative approach enables the models to effectively handle problems whose solutions exceed the maximum context size by creating and managing multiple nested contexts.

Different from above works that focus on introducing structural information into CoT reasoning, Ning et al. (2023) proposed Skeleton-of-Thought to accelerate the CoT reasoning process. SoT consists of two stages: (i) Skeleton stage: SoT guides the LLM to output a concise skeleton of the answer through a manually designed a skeleton prompt template and extracts points from the skeleton response; (ii) Point-expanding stage: SoT prompts the LLM to expand on each point in parallel through a point expanding template and finally concatenates all the points to get the final answer.

### 4.2.2 Reasoning Aggregation

Wang et al. (2023f) introduced a novel decoding strategy called self-consistency to replace the greedy decoding strategy in CoT. Self-consistency CoT first prompts the language model following the Manual-CoT (Wei et al., 2023b) and then
samples a diverse set of reasoning paths from the language model's decoder. Finally, Self-consistency CoT finds the most consistent answer by taking a majority vote which was found to significantly improve the performance of the CoT.

Wang et al. (2022) further developed a unified framework for rationale-augmented ensembles which aims at aggregating over multiple rationales generated from the language model to mitigate the brittleness of the results. The author explores three distinct approaches of rationale-augmented ensembles,each differing in how randomness is introduced into the input or output space: (i) self-consistency (Wang et al., 2023f): the ensembling is based on sampling multiple language model outputs; (ii) prompt-order ensembling: the ensembling process is based on the order of the input exemplars; (iii) input-rationale ensembling: the ensembling is based on sampling multiple input exemplars rationales from LLMs. The author found that regardless of the variation in input or prompt, the best way to improve task performance is sampling rationale in the output space.

### 4.2.3 CoT Verification

CoT verification initially focused on self-verification through multiple rounds of questioning, enabling models to validate their own responses. Later works involve leveraging external tools for information validation, such as information retrieval, calculators, or program execution. This section explores various methods and strategies within CoT verification, contributing to the enhancement of model reliability and response accuracy.

Weng et al. (2022) first proposed and proved that LLMs have self-verification abilities by using the conclusion obtained through $\mathrm{Co} \mathrm{T}$ as a condition for verifying the original problem. Self-verification consists of two steps: (i) forward reasoning that samples multiple candidate reasoning paths; (ii) backward verification that calculates the verification scores for each candidate's answer by masking the original conditions and predicting their results in turn. The answer with the highest score is selected as the final answer.

Lightman et al. (2023) focused on training reward models and conducted a comparison between the outcome supervision reward model (ORM) and process supervision reward model (PRM) for LLM to solve problems from the MATH dataset (Hendrycks et al. 2021b), finding that process supervision significantly outperforms outcome supervision. Outcome supervision is provided without humans, as the MATH dataset has automatically checkable answers. Process supervision, on the other hand, requires human data-labelers to label the correctness of each step in model-generated solutions. The authors also released PRM800K, a complete dataset of 800,000 step-level human feedback labels used to train their best reward model.

Ling et al. (2023) proposed a verification process using a "Natural Program" format. Natural Program breaks down the reasoning process into individual steps which is accompanied by its corresponding minimal set of premises. Then, the author employed a 2-phase sequence generation, strategy Unanimity-Plurality Voting, to verify the deductive reasoning process. Unanimity-Plurality Voting first performs deductive validations on sampled reasoning chains and then conducts a majority-based voting among the verified candidate chains to obtain the final answer.

Based on Self-consistency CoT (Wang et al., 2023f), Zhao et al. (2023b) designed the Verify-and-Edit framework to improve the factuality and accuracy of reasoning chains generated by CoT. The framework first passes predictions with lower-than-average consistency to the next stages for further processing. The second step involves producing verifying questions using manually designed prompts to test the factual correctness of the predictions. The framework then retrieves external knowledge from reliable systems (e.g., Wikipedia, Google) and edits the generated rationales with the informed answers obtained from external knowledge. Finally, the framework produces new predictions based on the edited rationales.

Similarly, Gou et al. (2023a) introduced a framework called CRITIC that allows large language models (LLMs) to validate and amend their own outputs through tool-interactive critiquing. CRITIC formulates various external tools into text-to-text functions (e.g., search engines, code interpreters) to integrate external tools into LLMs. Through a manually designed prompt template, the framework starts with an initial output and interacts with appropriate external tools to evaluate certain aspects of the text, revising the output based on the feedback obtained during the validation process.

Zou et al. (2023) proposed AuRoRA, an augmented reasoning and refining system with task-adaptive CoT prompting. AuRoRA has the characteristics of task self-adaptation and process automation. It extracts relevant knowledge from multiple sources, reducing the issue of incorrect information. Knowledge from different sources (e.g., Wikipedia) is then combined, double-checked, and refined to enhance reliability. The system revises the initial CoT using high-quality extracted knowledge to enhance accuracy and logic.

Instead of using a single LLM to refine their outputs based on feedback on their previous outputs, multi-agent debate has been proposed to improve reasoning performance (Du et al., 2023). Liang et al. (2023) identified a degenerationof-thought (DoT) problem-the LLM fails to generate novel thoughts through reflection even if its initial stance is incorrect once the LLM has established confidence in its solutions. The DoT problem can be addressed by allowing divergent thinking using a Multi-Agent Debate (MAD) framework where multiple agents express their arguments and a judge manages the debate process to obtain a final solution. Du et al. (2023) also leveraged multiple instances of an LLM to debate their individual reasoning processes over multiple rounds to arrive at a consistent final answer. The approach has been shown to improve the factual validity of generated content and reduce fallacious answers and hallucinations.

- Can LLMs perform reliable CoT verification? Though CoT verification approaches above have been proposed as a remedy to improve reasoning performance and reliability, the role and efficacy of the verification are questioned. Recent work has tried to examine the self-verification capabilities of LLMs in reasoning tasks (Valmeekam et al. 2023, Huang et al. 2023a; Stechly et al., 2023). Huang et al. (2023a) identified that the enhancements observed in CoT verification studies were often facilitated by the utilization of oracles, which guided the self-correction process using ground-truth labels, external tools, or feedback from the environment to evaluate the correctness of the responses. However, it is crucial to note that obtaining high-quality external feedback is challenging in real-world applications. In the absence of oracles, LLMs encounter difficulties in rectifying their initial responses solely relying on their inherent capabilities-which we regard as imperfect verification. In the imperfect verification scenario, LLMs tend to nonexistent violations, and over-correct the reasoning process with false positives-walk right over the correct solution especially when there are mistakes in the verification process (Valmeekam et al. 2023). This phenomenon raises concerns about the inherent capability of the LLM to accurately assess the correctness of its reasoning process. It becomes evident that the key to achieving effective CoT verification lies in harnessing external, high-quality feedback for verification. For instance, integrating external tools such as search engines and calculators into the verification process has shown beneficial (Chen et al., 2022; 2023d; Olausson et al. 2023; Pan et al., 2023).


### 4.3 Application Scenarios

Inspired by the latest techniques proposed above to enhance the reasoning capabilities of LLMs, CoT techniques have shown greater impact with the shifts of its application scenarios. The application scenario shifts include the extension from single-language tasks to multilingual tasks, from single-language modality to multimodalities, and from complex reasoning tasks to general-purpose tasks.

### 4.3.1 From Single Language to Multilingual Scenarios

Shi et al. (2023) extended the CoT to encompass the realm of multilingualism and introduces the Multilingual Grade School Math (MGSM) benchmark, which evaluates the reasoning abilities of large language models in multilingual settings. This benchmark comprises 250 grade-school math problems that have been translated into ten linguistically diverse languages. Furthermore, the authors proposed a concept called "Multi-lingual CoT", which involves prompting LLMs with multilingual exemplars and incorporating English intermediate reasoning steps. This approach has been shown to yield competitive or even superior results. Multi-lingual CoT suggests that employing English chain-of-thought prompting as a baseline could be a valuable strategy for multilingual reasoning research.

### 4.3.2 From Text Modality to Multimodalities

Multimodalities in CoT can be classified into two categories: input multimodalities and output multimodalities, depending on where the multimodal elements are introduced. Figure 6 illustrates these types of multimodalities in CoT.

Zhang et al. (2023d) first explored input multimodalities CoT, which enables the CoT to transcend beyond textual information and proposes a multimodal CoT (MM-CoT). Instead of prompting LLMs, MM-CoT focuses on fine-tuning. MM-CoT incorporates language (text) and vision (images) modalities into a two-stage framework: rationale generation and answer inference. MM-CoT fine-tunes smaller LLMs and integrates language and visual modalities using a gated fusion mechanism. The results of this approach have demonstrated that incorporating visual information can enhance the LLM's ability to generate reasoning paths and mitigate the hallucination challenges, resulting in improved performance.
![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-14.jpg?height=892&width=1618&top_left_y=290&top_left_x=252)

Figure 6: Formulation of multimodalities CoT. We categorized multimodalities in CoT into two types: (i) Input Multimodalities: Various modalities such as text, image (Zhang et al. 2023d), caption (Huang et al. 2023b), and graph Yao et al. (2023c) are incorporated into the model's input; (ii) Output Multimodalities: Multimodalities, including text and image (Rose et al. 2023), are introduced into the model's output.

Based on Zhang et al. (2023d), Yao et al. (2023c) first introduced graph structures into input multimodalities CoT and proposes a two-stage pipeline, Graph-of-Thought-Input (GoT-Input). Different from GoT-Rationale (Besta et al., 2023) which models the thought generation process as a graph structure, GoT-Input, on the other hand, centers its attention on modeling thought graphs derived from CoT rationales to enhance the model's reasoning capabilities. In the first stage, the model generates the rationale given the input question and a thought graph built by leveraging open IE systems to extract the sub-verb-obj triplets from the input. In the second stage, the model generates the answer given the question and the generated rationales as inputs and a new thought graph based on the input text. GoT employs different encoders for text, graph, and image (optional) respectively and enhances the deductive reasoning capability through the usage of GNN. GoT then fuses the features using a gated fusion method to generate the final answer. By modeling the non-sequential nature of human thinking within LLMs, GoT proves to enhance the LLMs with deductive reasoning abilities

In Huang et al. (2023b), KOSMOS-1, a multimodal language model capable of processing various modalities, was introduced. The authors explored a multimodal chain-of-thought prompting approach using KOSMOS-1. In the initial stage, when presented with an image, the authors employed the prompt "Introduce this picture in detail:" to generate a detailed description of the image as the rationale. Subsequently, the model was provided with both the rationale and a task-specific prompt to generate the final results.

In contrast to the input multimodalities CoT mentioned above, Visual Chain of Thoughts (VCoT) (Rose et al., 2023) introduces multimodalities into the output space. VCoT initiates the process by generating captions for visual elements and identifying multipoint foveation to maintain input sequence consistency when producing multimodal infillings. Subsequently, it employs a recursive approach to generate multimodal infillings, encompassing both images and image captions. This is achieved through a combination of novelty-driven recursive infilling and consistency-driven visual augmentation. These strategies are employed to enhance interpretability for multi-step reasoning and bridge logical gaps, ultimately contributing to improved downstream task performance.

### 4.3.3 From Complex Reasoning Rasks to General-Purpose Tasks

The applicability of CoT has expanded from its initial utilization in mathematical, commonsense, and logical reasoning tasks to encompass a wide range of NLP tasks.

Wang et al. (2023g) introduced CoT into the realm of summarization and proposed the Summary Chain-of-Thought (SumCoT) technique with the aim of guiding Large Language Models (LLMs) to generate summaries in a step-by-step fashion. This approach enables the integration of more fine-grained details from source documents into the final summaries. SumCoT begins by instructing LLMs to extract core news elements from the source document using manually designed guiding question prompts. Subsequently, it involves integrating the extracted elements along with additional details from the source documents to produce comprehensive and informative summaries.

Li et al. (2023f) proposed Self-Prompting LLMs for Open-Domain QA (ODQA). Self-Prompting consists of two stages: In the first stage, the model tasks LLM with generating a pseudo ODQA dataset by prompting it to automatically construct QA pairs with context paragraphs and explanations. In the second stage, the model dynamically selects a few examples from a pool using a clustering-based retrieval method to serve as context demonstrations. These selected examples aid in understanding and answering specific questions.

He et al. (2023) explored the CoT technique in machine translation and introduced Multi-Aspect Prompting and Selection (MAPS). Drawing inspiration from strategies employed by human translators, MAPS breaks down the machine translation process into several steps. It requires the LLM to initially discern the topics and keywords of the sentence awaiting translation, and then to retrieve analogous example sentences. By integrating this extracted knowledge, the LLM produces more accurate translations.

In addition to the aforementioned classical NLP tasks, numerous studies have actively pursued the integration of CoT reasoning within the realm of science the development of automated intelligent agents.

Singhal et al. (2022) presented MultiMedQA, a benchmark that combines six existing open question answering datasets and HealthSearchQA, a new free-response dataset of medical questions searched online. Based on the benchmark, the author then proposed instruction prompt tuning to further align Flan-PaLM to the medical domain, producing Med-PaLM. Specifically, the author used the soft prompt as an initial prefix shared across multiple medical datasets, followed by the relevant task-specific manual exemplars or instructions along with the target question. Following the CoT reasoning format, Med-PaLM's answers to consumer medical questions compared favorably with cliniciangenerated answers, demonstrating the effectiveness of instruction prompt tuning. The research provides a glimpse into the opportunities and challenges of applying large language models to the medical domain.

Bran et al. (2023) incorporated CoT into the field of chemistry and proposed ChemCrow, a chemistry agent powered by LLM. Designed to tackle a wide spectrum of challenges spanning organic synthesis, drug discovery, and materials design, ChemCrow operates within the structured CoT reasoning format. Specifically, ChemCrow initially assembles a toolkit using various chemistry-related packages and software tools. The LLM in ChemCrow, guided by CoT reasoning principles, embarks on an automated and iterative chain-of-thought process. It begins by assessing the current state of the task, considering its alignment with the ultimate objective, planning the next steps and the choice of tools accordingly, and finally, solving the problem. Through the integration of 18 expert-designed tools, the LLM's performance in chemistry-related tasks is significantly improved. By integrating the CoT reasoning format, ChemCrow showcases its capacity to independently plan and execute a range of chemical syntheses, including an insect repellent, three organocatalysts, and even the discovery of a novel chromophore. This exemplifies its effectiveness in automating a diverse array of chemical tasks.

## 5 Towards Language Agents

With improved capabilities by the advanced techniques above, CoT reasoning has yielded a broader impact on the AI community, notably fueling the development of autonomous agents in real life. Building intelligent autonomous agents that are capable of learning and acting in a distinct environment is a long-standing goal of artificial intelligence (AI) (Searle, 1969; Wooldridge \& Jennings, 1995; Maes, 1995; Hendler, 1999, Wang et al., 2023b; Xi et al., 2023; Zhou et al. 2023d). In light of the swift advancements detailed previously, CoT reasoning approaches have been leveraged for perception, memory, and reasoning, language agents, thereby enabling interaction within increasingly complex

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-16.jpg?height=325&width=458&top_left_y=282&top_left_x=253)

Control: OS and Applications

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-16.jpg?height=287&width=445&top_left_y=694&top_left_x=257)

Control: Embodied Systems

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-16.jpg?height=347&width=477&top_left_y=279&top_left_x=781)

Research: Organic Synthesis

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-16.jpg?height=198&width=595&top_left_y=749&top_left_x=730)

Research: Scientific Experiments Design

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-16.jpg?height=327&width=444&top_left_y=281&top_left_x=1380)

Programming: Code Generation

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-16.jpg?height=282&width=509&top_left_y=688&top_left_x=1342)

Interaction: Multi-Agent Collaboration

Figure 7: Representative agents for autonomous control, research, programming, and interaction. The illustrations are adapted from Rawles et al. (2023), Jiang et al. (2022), Bran et al. (2023), Boiko et al. (2023), Bairi et al. (2023), and Park et al. (2023).

environments. These abilities serve as the foundation for developing autonomous agents that help solve complex tasks through human-agent and agent-agent collaboration.

As a result, LLM-based language agents, empowered by CoT techniques, have emerged in a wide range of research areas, such as engineering (Li et al., 2023a; Mehta et al., 2023, Qian et al., 2023), natural sciences (Bran et al., 2023; Kang \& Kim, 2023, Boiko et al., 2023), and social sciences (Aher et al., 2023; Akata et al., 2023, Ma et al., 2023; Dan et al. 2023). Those language agents are capable of following language instructions and executing actions in real-world or simulated environments. Figure 7illustrates the representative application scenarios of agents for autonomous control (Rawles et al. 2023, Jiang et al., 2022), research (Bran et al., 2023; Boiko et al., 2023), programming (Bairi et al., 2023), and interaction (Park et al. 2023). A detailed technical comparison of existing agents is presented in Table 2 . We will elaborate the technical philosophy in the following parts.

- What is new in language agents compared with RL agents? The pursuit of developing generally intelligent agents has been a long-standing goal of AI research. In the early stages, research on agents primarily RL techniques Wilkins, 2014, Mnih et al., 2015). RL agents are trained to make decisions through iterative interactions with an environment, receiving feedback in the form of rewards or penalties-correct moves are rewarded, while erroneous ones are penalized. This iterative process aims to minimize mistakes and maximize accurate decisions. RL agents possess a key trait: the ability to self-evolve through continuous interactions with their environments (Bai et al. 2023a). However, RL agents face limitations. They heavily rely on expert data and meticulously designed reward functions tailored for specific tasks. Consequently, their effectiveness is often confined to individual tasks, hampering their generalization capabilities to novel tasks or domains (Kim et al., 2023a). Furthermore, the inner workings of RL agents often lack transparency and interpretability (Lundberg \& Lee, 2017, Yang et al. 2018). In contrast, language agents distinguish themselves from RL agents by leveraging commonsense priors embedded in LLMs. These priors reduce dependence on human annotation and trial-and-error learning, enabling easy adaptation to new tasks or environments and allowing better interpretability with CoT (Yao et al. 2022, Shah et al. 2023). However, language agents face challenges in evolving their parameters in response to environmental changes, primarily because they are predominantly adapted to environments through prompts or the heavy costs of fine-tuning the LLMs. While recent studies on language agents, such as Retroformer (Yao et al. 2023b), have incorporated RL-like policies to enhance the capabilities of language agents, the focus remains largely limited to language reasoning tasks. It holds promise to see how to bridge the gap between RL agents and language agents to facilitate future architectures that can work generally with strong performance and high interpretability in complex environments. In consideration of the pros and cons of RL agents and Language agents, please refer to Table 3 for more details.

Table 2: A technical comparison of representative agents. Specifically, we classify the memory modules into two main types: short-term memory and long-term memory. As defined in Section 5.2.2, short-term memory is dynamic in nature and can be easily read and written via prompts. The most common form of short-term memory is chat history. Long-term memory, on the other hand, is static and is typically stored in a database, accessible through various retrieval methods, including tree search, text search, and vector retrieval. For the external tools module, we divide the tools into three types: Web search (Web), Code interpreter (Code), and other tools (Other). More details of tool use can be found in Section 5.1.3

| Agent <br> CAMEL Li et al., 2023a | Type <br> nmunicative | Memory |  |  | Methodology | Domain | Environment Interaction |  | {External Tools <br> Web Code Other} |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Operation |  | ng-term |  |  |  | Model |  |  |  |
|  |  | Prompt | $\checkmark$ | - | Prompting | AI Society \& Coding | Text | GPT-3.5-Turbo | - | - | - |
| Generative Agents (Park et al., 2023) | Communicative | Tree Search | $\checkmark$ | $\checkmark$ | Prompting | AI Society | Text | GPT-3.5-Turbo | - | - | - |
| Voyager Wang et al., 2023c, | Communicative | Tree Search | $\checkmark$ | $\checkmark$ | Prompting | MineCraft | Text | GPT-4 | - | - | - |
| GITM Zhu et al., 2023 | Communicative | Tree Search | $\checkmark$ | $\checkmark$ | Prompting | MineCraft | Text | GPT-3.5-Turbo | - | - | - |
| MetaGPT Hong et al., 2023) | Communicative | Text Retrieval | $\checkmark$ | $\checkmark$ | Prompting | AI Society \& Coding | Text | GPT-4 | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| ChatDev Qian et al., 2023) | Communicative | Text Summary | $\checkmark$ | $\checkmark$ | Prompting | Software Engineering | Text | GPT-3.5-Turbo | - | $\checkmark$ | - |
| MAD (Liang et al., 2023) | Communicative | Prompt | $\bar{\checkmark}$ | - | Prompting | Reasoning | Text | GPT-3.5-Turbo | - | $\overline{-} \quad$ | - |
| Multiagent Debate Du et al., 2023, | Communicative | Prompt | $\bar{\checkmark}$ | - | Prompting | Reasoning | Text | GPT-3.5-Turbo | - | - | - |
| ORD Xiong et al., 2023a | Comm | Prompt | $\checkmark$ | - | ing | easoning | Text | GPT-4 | - | - | - |
| 3 ) | Autol | Vecto $-2 x-1$ | $\checkmark$ | $\checkmark$ | ing | Man: | Text <br> Speech <br> Image | GPT-4 <br> DALL-e <br> ElevenLabs | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| BabyAGI Nakajima, 2023, | Autonomous | Vector Search | $\checkmark$ | $\checkmark$ | Prompting | Task Management | Text | GPT-4 | - | - | - |
| gentGPT Reworkd, 2023, | Autor | Vector | $\checkmark$ | $\checkmark$ | pting | Task Management | Text | GPT-4 | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| Auto-UI Zhang \& Zhang, 2023, | Autonomous | Prompt | $\checkmark$ | - | Finetuning | UI control | Text <br> Image | FLAN-Alpaca <br> BLIP-2 | - | - | - |
| AITW Rawles et al., 2023, | Autonomous | Prompt | $\checkmark$ | - | Prompting | UI control | Text | PaLM 2 | - | - | - |
| DCACQ Mehta et al., 2023, | Autonomous | Prompt | $\checkmark$ | - | Finetuning | Engineer | Text | BART | - | - | - |
| ChemCrow (Bran et al., 2023) | Autonomous | Prompt | $\bar{\checkmark}$ | - | Prompting | Chemistry | Text | $\overline{\text { GPT-4 }}$ | $\bar{\checkmark}$ | $\checkmark$ | $\bar{\checkmark}$ |
| Chatmof Kang \& Kim, 2023) | Autonomous | Text Search | - | $\checkmark$ | Prompting | Material Sciences | Text | GPT-4 | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| IASSE Boiko et al., 2023] | Autonomous | Vector Search | - | $\bar{\checkmark}$ | Prompting | Scientific Experiments | Text | GPT-4 | $\bar{\checkmark}$ | $\checkmark$ | $\bar{\checkmark}$ |
| $\overline{\mathrm{TE}}$ Aher et al., 2023, | Communicative | Prompt | $\bar{\checkmark}$ | - | Prompting | Social Science | Text | GPT-4 | - | - | - |
| CodePlan Bairi et al., 2023 | Autonomous | Tree Search | $\checkmark$ | $\bar{\checkmark}$ | Prompting | Coding | Text | GPT-4 | - | $\checkmark$ | - |
| VIMA Jiang et al., 2022, | Communicative | Prompt | $\checkmark$ | - | Finetuning | Robot Manipulation | Text <br> Image | $\mathrm{T5}$ <br> ViT | - | - | - |
| React Yao et al., 2022, | Communicative | Text Retrieval | $\checkmark$ | $\checkmark$ | Finetuning | Decision-Making | Text | PaLM-8B | $\checkmark$ | - | - |
| Reflexion Shinn et al., 2023, | Communicative | Text Retrieval | $\checkmark$ | $\checkmark$ | Prompting | Decision-Making | Text | GPT-4 | $\checkmark$ | - | - |
| ToRA Gou et al., 2023b] | Autonomous | Prompt | $\checkmark$ | - | Prompting | Mathematical reasoning | Text | GPT-4 | - | $\checkmark$ | - |
| Toolformer Schick et al., 2023, | Autonomous | Text Retrieval | $\checkmark$ | $\checkmark$ | Finetuning | Mathematical reasoning | Text | GPT-J | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| Fireact SChen et al., 2023a, | Autonomous | Prompt | $\bar{\checkmark}$ | - | Finetuning | Question Answering | Text | GPT-3.5 | $\checkmark$ | - | - |

Table 3: Comparison between RL agents and language agents.

| Aspect | RL Agents | Language Agents |
| :--- | :--- | :--- |
| Knowledge Aquisition | Primarily use RL techniques. | Leverage commonsense priors embedded in LLMs. |
| Training Process | Trained through iterative interactions with the environment, <br> receiving rewards or penalties. | Adaptation to new tasks or environments with reduced dependence on <br> human annotation, primarily through prompts. |
| Self-Evolution | Possess the ability to self-evolve through <br> continuous interactions with the environment. | Face challenges in evolving parameters in response to environmental changes; <br> Adaptation is mainly through prompts or costly fine-tuning of LLMs. |
| Limitations | Heavily relies on expert data and task-specific reward functions; <br> Effectiveness often confined to individual tasks. | Challenges in evolving parameters dynamically; <br> Focus on language reasoning tasks, may lack adaptability to broader tasks. |
| Transparency | Working mechanism often lacks transparency and interpretability. | Generally allow better interpretability with commonsense priors, <br> but have challenges in parameter evolution transparency. |
| Generalization | Limited generalization capabilities to novel tasks or domains. | Facilitates easy adaptation to new tasks or environments, <br> reducing dependence on task-specific training. <br> Primary focus remains on language tasks. |
| Future Goals | Aim to bridge the gap between RL agents and language agents, facilitating more versatile and adaptable architectures. |  |

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-18.jpg?height=664&width=1613&top_left_y=275&top_left_x=256)

Figure 8: General framework of language agents. Language agents are capable of following language instructions and executing actions in real-world or simulated environments.

The following part will introduce the basic concepts of language agents and show how CoT is utilized in those agents.

### 5.1 General Framework

The landscape of language agent frameworks within the existing literature is notably diverse. We outline representative architectures in recent studies and summarize a cohesive and overarching conceptual framework for language agents.

Wang et al. (2023b) designed a modulized agent framework with four modules: (i) a profiling module to identify the role of the agent, (ii) a memory module to recall past behaviors, (iii) a planning module to plan future action, and (iv) an action module to translate the agent's decisions into specific outputs.

Xi et al. (2023) proposed a conceptual agent framework with three components: (i) brain that undertakes basic tasks like memorizing, thinking, and decision-making, (ii) perception that perceives and processes multimodal information from the external environment, and (iii) action that carries out the execution using tools and influences the surroundings.

Zhou et al. (2023d) presented a featurized agent framework for language agents, which supports important features, including planning, memory, tool use, multi-agent communication, and fine-grained symbolic control.

Sumers et al. (2023) proposed another conceptual architecture for language agents called CoALA. CoALA organizes agents along three key dimensions: (i) information storage that is divided into working and long-term memories, (ii) action space that is divided into internal and external actions, and (iii) decision-making procedure that is structured as an interactive loop with planning and execution.

Though different architectures have been designed, recent technical research (Yao et al., 2022; 2023b; Park et al., 2023, Zhu et al., 2023) tends to follow the line of the conceptual framework by prompting LLMs to imitate the agent processes such as perception, memory, and reasoning. The basic assumption is that LLMs have already captured world knowledge to some extent (Gurnee \& Tegmark, 2023), which can be induced by CoT prompting step by step.

Therefore, we summarize a general conceptual framework of language agents in view of technical practice as shown in Figure 8 . Given a user instruction (also known as a goal), an agent needs to complete the task with multiple steps of interaction across the environment, possibly operating with tools. Without the loss of generality, we focus on a single agent when introducing the framework. It is worth noting that multiple agents can cooperate or compete with each other in a multi-agent environment. Before diving into the technical discussion, we first present the basic concepts of language agents, i.e., agent, environment, and tool use, as below.

### 5.1.1 Agent Backbone Model

A language agent can be built upon either a single-modality LLM or a multimodal LLM. Completing a task often comes with multiple steps of interaction. The entire process is called an episode, which is composed of a series of turns. To accomplish the task, the agent needs to plan ahead, make decisions, and execute actions at each turn of the episode. The process of planning, decision-making, and action execution may reflect the reasoning ability of LLMs as LLMs are exposed to real-world or virtual environments that do not exist during the pre-training of LLMs. In such environments, the LLM must perceive the world's knowledge and take action, in which cases we will show that CoT helps bridge the gap between the environment perception and the innate ability of LLMs.

Such agents expand the landscape of language models to compete in specific fields, including application operation, web searching, and web shopping. There are two popular types of language agents: autonomous agents and communicative agents. Typical examples of autonomous agents are AutoGPT (Richards, 2023), BabyAGI (Nakajima, 2023), and AgentGPT (Reworkd, 2023). In contrast, communicative agents are personalized and socialized agents with human behaviors that can communicate (Park et al., 2023, Wang et al., 2023c, Zhu et al., 2023), collaborate (Hong et al. 2023, Qian et al. 2023) and debate (Liang et al. 2023; Du et al., 2023; Xiong et al., 2023a) with each other. They are often deployed in immersive environments.

### 5.1.2 Environment Interaction

An intrinsic characteristic of language agents is communicating, interacting, and evolving with environments. Such environments include operation systems, third-party applications, webpages, and virtual environments. LLMs handle environments with two kinds of approaches, namely, environment parsing and multimodal perception, depending on whether the LLM has the ability to model the multimodal inputs. Environment parsing refers to those approaches that leverage external tools such as optical character recognition (OCR) and icon detectors Zhang et al., 2021; Sunkara et al. 2022) to parse the environment into textual elements (e.g., HTML layouts) as inputs to an LLM. In contrast, multimodal perception, also dubbed as first principles thinking (Zhang \& Zhang, 2023), refers to using a multimodal LLM to simultaneously process the inputs in different modalities. To build a multimodal LLM, a popular way is to use a simple projection matrix to integrate a pre-trained large vision model (e.g., CLIP (Radford et al., 2021) and BLIP-2 (Li et al. 2023c) into an LLM (Liu et al. 2023b, Zhang et al., 2023a). More recent studies have also explored modeling the inputs of different modalities into the same vector space, thus resulting in any-to-any representation learning Huang et al. 2023b, Wu et al., 2023, Moon et al. 2023) and interleaved multimodal representation learning (Li et al., 2023b; Zhao et al., 2023a).

### 5.1.3 Tool Use

Tool use can be seen as an expansion of a language model's ability boundary, compensating for parametric knowledge for reasoning and grounding the language model's capabilities to interact with environments (Qin et al. 2023b). Tools coming into play include knowledge bases, search engines, code interpreters, online models, applications, databases, and even bespoke tools specially created for specific tasks, overcoming the constraints of generic APIs (Li et al. 2023d, Schick et al., 2023; Cai et al. 2023; Zhou et al. 2023d, Team, 2023).

The purpose of tool use comes with three aspects:

- Action execution. The language model is not confined to merely predicting the next action; it has the capability to execute it in the real environment. This includes everything from executing codes or queries through a JavaScript element selection on a webpage (Zhou et al. 2023c), executing programs via code interpreters or compilers (Gur et al., 2023, $\mathrm{Ni}$ et al. 2023; Dídac et al., 2023, Ruan et al., 2023a, Gou et al., 2023b), to interacting with online expert models which serve as callable APIs (Shen et al. 2023; Patil et al., 2023; Ge et al., 2023). These steps can be dynamically adjusted with effective scaling of the tool set depending on task requirements and computational capacity (Yuan et al., 2023).
- External knowledge acquisition. Retrieval augmentation has been shown so effective that has been regarded as a standard solution to alleviate the factuality drawback (Trivedi et al., 2022, Yao et al., 2022). To empower the CoT process, up-to-date knowledge is accessible through search engines (Khattab et al., 2022; Nakano et al., 2021), while domain-specific through expert candidates (Bran et al. 2023; Ge et al. 2023). The purpose of tool use extends beyond augmenting the language model's scope; they enable language models to adapt to a complex environment or a vast
application ecosystem and ensure that the information language models have access to is up-to-date, thereby reducing the propensity to generate non-factual information (Wang et al., 2023b).
- Reasoning and verification. In the reasoning process, language models are sometimes prone to errors. Tools that provide accurate, real-time knowledge can help correct reasoning errors and formulate more accurate responses. Pieces of evidence from these tools are used to rewrite the initial output for self-correction (Gou et al., 2023a). Code LLMs can be further verified with execution results from program executors (Ni et al., 2023). Multi-tool and multi-step planning and retrieval strategies, involving depth-first or breadth-first approaches, can be deployed for a deep or diverse range of possible pathways (Liu et al. 2023e; Qin et al. 2023b).


### 5.2 CoT Facilitates Agent Abilities

Language agents are placed in interactive loops with the external environment (Sumers et al. 2023). The interface loops can be elicited in three ways (Figure 1), namely, perception, memory, and reasoning. CoT methods empower the agents from all three perspectives.

### 5.2.1 Perception as CoT

Prompting the agent to interpret the perception step by step, as a chain of perception, has been shown to improve the action success rate. It enhances the understanding of the environment or the context. Notably, Rawles et al. (2023) found that using the CoT template, "Answer: Let's think step by step. I see $<$ Screen Caption $>$, I need to ...", substantially improves the action prediction accuracy. As an example shown in Figure 1 , the prompt of perception as CoT can be "Let's think step by step. I see unrelated search results in the Google app". Furthermore, Zhang et al. (2023d) and Huang et al. (2023b) leveraged external tools to obtain the image captions as supplemental inputs to help improve the perception of the multimodal environments. The captions are placed in $<$ Screen Caption $>$ to organize the input prompt.

In addition to the one-way interpretation of perception, language agents can benefit significantly from integrating environmental feedback, especially in the context of multi-turn interactions where the environment is subject to alterations (Chen et al., 2023d, Olausson et al., 2023, Jignasu et al., 2023). Effectively integrating this feedback necessitates the implementation of a crucial method: self-correction with environment feedback Xu et al. 2023d, Zhou et al. 2023a, Yao et al., 2023b; Zhao et al. 2023d). Self-correction entails exposing the model to intricate sequences of operations, encompassing tasks such as executing codes, conducting operations, and controlling robots. These operations can lead to execution failures and generate error messages. In this context, the agent is not only required to comprehend these environmental cues but must also actively engage in iterative error correction processes until the desired outcome is achieved. Consequently, the agent's performance within these dynamic environments serves as a direct indicator of its self-correction proficiency. This proficiency, in turn, showcases the agent's ability to assimilate feedback from the environment effectively. The seamless incorporation of such feedback not only refines the interpretive capacities but also enhances its overall functionality, making it pivotal in the realm of advanced language agents.

- Is language-centered perception the future? Multimodal perception stands as one of the key steps toward achieving artificial general intelligence. Current trends, likely inspired by the impressive reasoning capacities of language models, predominantly adopt a language-centered perception approach (Figure 9 (a)). Typically, distinct encoders are utilized to process inputs from various modalities, such as images. The resulting encodings are then linked to an existing language model through cross-attention or supplementary adapters, facilitating the integration of multimodal inputs into the language model's embedding space (Alayrac et al., 2022; Liu et al., 2023a, Wu et al., 2023, Driess et al., 2023, Chen et al. 2023c, Bai et al. 2023b; Zhang et al., 2023a). In contrast to this prevailing language-centric modeling, Rust et al. (2023) has proposed an image-centered approach (Figure 9(b)) by rendering text as images, enabling the transfer of representations across languages based on orthographic similarity or the co-activation of pixels. To better align the inputs from different modalities and allow for convenient scaling up model parameters, recent research endeavors have explored a unified approach (Figure 9-(c)). For instance, in the context of vision-language modalities, instead of employing a separate image encoder, image patches are treated as tokens and linearly projected into the embedding layer of the transformer. These patches are then fused with the representations of language tokens, allowing for seamless integration (Huang et al. 2023b, Bavishi et al. 2023).

Though various kinds of perception approaches, including language-centered, image-centered, and unified methods, have been proposed in the realm of agent perception, determining the most suitable choice remains a formidable

![](https://cdn.mathpix.com/cropped/2024_06_04_902faaa8a83047c380bdg-21.jpg?height=983&width=1640&top_left_y=300&top_left_x=248)

Figure 9: Multimodal perception methods including (a) language-centered method; (b) image-centered method; (c) unified method.

challenge. This difficulty arises due to the involvement of more diverse and complex modalities such as auditory, tactile, and brain signals during interactions between agents and environments. Besides, these modalities often come with imbalanced data scales, complicating the perception process. Additionally, the diversity in types and formats of multimodal data poses challenges related to computation efficiency and the scalability of models. Exploring innovative methods to address these challenges will pave the way for the development of effective and efficient perception frameworks in the future.

### 5.2.2 Memory as CoT

A language agent is commonly equipped with both long-term memory and short-term memory (Sumers et al. 2023; Wang et al., 2023e).

Short-term memory. Short-term memory is formed as temporal information that may be flexible to change in different steps of episodes (also known as working memory in Sumers et al. (2023). Short-term memory is more temporal-specific, offering explicit, recent context that facilitates the agent. On the one hand, short-term memory shows direct support and closer relations with the exact current state. On the other hand, short-term memory yields a relatively moderate impact on the whole environment. For example, short-term memory can be modeled within an episode of a multi-step task, the chain of action history (Zhang \& Zhang, 2023), or the rationales or sub-question in the last several hops of multi-hop question answering (Yao et al. 2022; Khattab et al., 2022). Due to the significant temporal character, short-term memory raises little storage concern.

Long-term memory. Long-term memory provides the agent with the capability to retain and recall static information over episodes Weng, 2023). In contrast to short-term memory, long-term memory is more general to the task, as a macroscopic and abstract understanding of the whole world. This can include procedural memory that stores the production system itself, semantic memory that stores facts about the world, and episodic memory that stores sequences of the agent's past behavior (Sumers et al., 2023). For example, given a goal, upvote the latest post, in the varied
environment states, two chains of actions have been observed to accomplish the goal: (i) [opening Instagram, going to home feed, looking at the latest post, upvoting the latest post $]$ and (ii) $=[$ go to the HOME screen, opening Instagram, going to home feed, looking at a post, upvoting the latest post]. It can be found that atom actions [opening Instagram, going to home feed, looking at the latest post, upvoting the latest post] can serve as long-term memory for this goal, i.e., a chain of static memory.

Long-term memories can rely on both parametric and non-parametric knowledge storage. They can be from the trainable parameters of the language agents or maintained as external knowledge that can be leveraged through retrieval systems. For example, the earlier hops of former episodes are long-term memories from agent parameters, and the output action formulations are parametric long-term memories.

- Towards efficient memory operation. Modeling memory as linear natural language sequences becomes inefficient as sequences lengthen during the agent's interaction with environments. Besides, the context window of LLMs is predetermined to be limited in length. To pursue more efficient memory operations, recent studies have explored two types of approaches, i.e., leveraging (i) tree search and (ii) vector retrieval.

(i) Tree search. Memory can be stored with a tree structure and fetched by searching on the tree. Notably, MemWalker (Chen et al. 2023b) empowered agents to access textual memory information through iterative prompting. In this approach, the agent initially processes the lengthy context into a tree of summary nodes. Upon receiving a query, the agent navigates this tree to search for relevant information and responds after gathering sufficient information. Similarly, GITM (Zhu et al. 2023) proposed an LLM Decomposer that recursively decomposes goals into a sub-goal tree. The hierarchical tree structure helps the model to explicitly capture the relationships between goals and corresponding plans in the memory. Park et al. (2023) proposed the reflection tree to organise the memory of a communicative agent. When facing the trivial observations during the interaction with the environment, the agent periodically reflects on existing memories in a abstract manner, thus forming a reflection tree: "the leaf nodes of the tree represent the base observations, and the non-leaf nodes represent thoughts that become more abstract and higher-level the higher up the tree they are".

(ii) Vector retrieval. The other way to store memory is via vector storage (Hu et al, 2023, Zhou et al., 2023e). Vector database has become a key carrier for storing, managing, and retrieving high-dimensional data, such as the long-term memory of language agents. It can represent complex data types such as text, images, videos, and even structured data. Agentsims (Lin et al. 2023) employed a vector database to enable efficient storage and retrieval within long-term memory. Specifically, it stores daily memories as embeddings within this vector database. When the agent encounters new situations and necessitates the recall of past memories, the long-term memory system adeptly retrieves pertinent information, thereby ensuring the consistency of the agent's behavior.

### 5.2.3 Reasoning as CoT

Inspired by the success of eliciting LLMs' step-by-step reasoning abilities, CoT has also been applied in inducing the agents to reason via planning or decision-making. More importantly, CoT methods for language agents require careful design to handle the action execution and state observation.

The gap between reasoning and action is bridged by combining interleaving thought, action, and observation Yao et al. 2022; Khattab et al. 2022, Shinn et al. 2023). By exploring the use of LLMs to generate both CoT traces and task-specific actions in an interleaved manner, it has been found that reasoning and acting achieve mutual promotion. Reasoning traces help the model make action plans and handle exceptions, while actions allow the LLM to interface with external sources, such as knowledge bases or environments, to gather additional information for knowledge support. (Xu et al. 2023b) detached the reasoning process from external observations to reduce token consumption during multiple steps of CoT.

Similarly, AgentBench (Liu et al., 2023c) compelled language agents to complete tasks via "think" and "Act" steps. Further, Zhang \& Zhang (2023) proposed a chain-of-action technique-leveraging a series of intermediate previous action histories and future action plans-to help the agent decide what action to execute, which transforms the decision-making as a CoT reasoning problem.

- How to expand the capability of agents? Currently, the mainstream interest is to apply CoT prompting approaches to elicit LLMs' reasoning abilities during the interaction with the environments as discussed above. The basic hypothesis is that LLMs already have the prior knowledge to perform as the language agents for our concerned tasks and CoT prompting approaches are effective in invoking the knowledge. Those prompting techniques have the advantage of
flexibility and convenience because it is easy to design and adjust the prompts according to the task requirements and characteristics. However, LLM performance has shown to be sensitive to prompts and there is a lack of evidence that LLM can actually learn domain knowledge from the prompts. Therefore, purely prompting methods may not be adequate to make LLMs generalizable to new domains. To expand the capability boundary of language agents, there is a recent interest in fine-tuning LLMs on curated datasets to build effective agents. Chen et al. (2023a) called for a re-thinking of fine-tuning language models when the target tasks and data formats are known and enough data can be collected (e.g., possibly automatically with GPT-4). The results have revealed that fine-tuning can not only achieve strong generalization and robustness but also improve performance. Gou et al. (2023b) curated interleaved tool-use data composed of natural language CoT with tool-integrated programs. Then, a tool-integrated reasoning agent was trained on those high-quality annotations and achieved substantial performance gains on various mathematical reasoning tasks.


## 6 Challenges

Despite the swift advancements in the realms of LLMs, CoT reasoning, and language agents, numerous promising challenges still beckon for deeper exploration, particularly pertaining to generalization to unseen domains, enhancing efficiency amidst redundant interactions, developing customizable agents, scaling up language agents, ensuring the safety of language agents, and capacity evaluation.

### 6.1 Generalization to Unseen Domains

Language agents have found extensive applications in practical fields such as engineering (Li et al., 2023a; Mehta et al., 2023, Qian et al. 2023), natural sciences (Bran et al., 2023, Kang \& Kim, 2023; Boiko et al., 2023), and social sciences (Aher et al., 2023; Akata et al., 2023, Ma et al. 2023, Dan et al., 2023). Despite their widespread use, a significant challenge persists: adapting LLMs to specific, especially unseen domains. This challenge is twofold: firstly, determining an efficient method for acquiring domain-specific knowledge, such as employing CoT prompting techniques. The limitations arise from the finite scope of knowledge acquisition during pre-training on textual corpora, lacking substantial interaction with the physical world. Secondly, there is the challenge of effectively adapting LLMs to diverse, unseen domains. Given the substantial variation in action spaces across tasks (e.g., drone control versus web browsing), aligning the model's knowledge with the specific task requirements remains a formidable obstacle. These challenges underscore a critical gap in current research. The need to enhance LLMs' adaptability to novel domains and help LLMs learn from environments is paramount, requiring innovative solutions that address both knowledge acquisition and effective task alignment.

Prompting and fine-tuning are widely used techniques to adapt pre-trained LLMs to new domains. However, it remains an underexplored area of when and how to leverage prompting (e.g., prompting pattern and reasoning format) and fine-tuning (e.g., instruction tuning) techniques to help LLMs generalize to unseen domains. In doing so, researchers can pave the way for more versatile and impactful applications of language agents across a myriad of fields.

### 6.2 Efficiency against Redundant Interactions

Completing a task necessitates intricate, multi-step interactions with the environment. This process results in extensive and repetitive logs, which have been identified as pivotal for task completion (Zhang \& Zhang, 2023). However, due to computational constraints, most studies utilize only a limited number of log steps (Park et al. 2023). Although recent advancements have expanded the capacity of LLMs to handle extended contexts (Xiong et al., 2023b), conducting inference based on these logs is hampered by the inherently slow speed of autoregressive LLMs. This issue is exacerbated in multi-agent interaction environments, where numerous agents generate a substantial volume of interaction logs.

To tackle this challenge, one potential solution is to incorporate a memory mechanism for storing and retrieving knowledge from these logs. However, the key challenge lies in exploring effective methods to discern salient knowledge and distill relevant information from the logs. Addressing this challenge is crucial for enhancing the efficiency of inference processes in complex, multi-agent scenarios.

### 6.3 Customizable Language Agents

LLMs are usually supposed to acquire general language ability and common knowledge through pre-training on largescale corpora, and then cater to human preferences following instructions through further alignment tuning, including instruction tuning and reinforcement learning from human feedback. Whereas, users have specialized requirements and individual characteristics. Thus, building a customizable assistant from LLMs is of great importance.

Existing related studies mostly fall into three general methods: (i) customizable prompting, often with role or tool specifications. CAMEL (Li et al. 2023a) prompted LLM with formatted profiles of human-agent pairs to simulate the workflow of diverse groups of internet users or occupations. MetaAgents (Li et al. 2023e) prompted the language agent to play a specific role in some certain social context. ExpertPrompting (Xu et al., 2023a) proposed to prompt LLMs to solve a problem conditioned on an expert identity profile that is best suited for the problem. RoCo (Mandi et al. 2023) assigned robots with an LLM role to talk on their behalves, generating plans for practical tasks. Customizable ChatGPT has also been announced to comply with specified instructions, extra knowledge, and a combination of skills. ${ }^{2}$ (ii) customizable training. The gradient updates in the language model can further ensure the customizable alignment. Auto-UI (Zhang \& Zhang, 2023) was trained on the Android UI control domain, achieving stable performance as an autonomous agent. For the communicative agent, Character-LLM (Shao et al. 2023) trained the LLMs with profiles and detailed scenes, enabling LLMs to mimic well-known people, like Beethoven; (iii) customizable model editing. Besides training, editing is an alternative to changing stored knowledge in language agents, which improves the factuality and reliability of a customized assistant. ROME (Meng et al., 2022a) and MEMIT (Meng et al., 2022b) used the locate-and-edit method to correct wrong knowledge. Transformer-Patcher (Huang et al. 2023c) further alleviated the error recurrence by real-time sequential editing. Beyond factual knowledge correctness, PersonalityEdit (Mao et al., 2023) changed the model response to match the Big Five personality traits.

Despite the recent progress, the challenge of developing customizable agents still lies within three folds. Firstly, existing studies mainly focus on methods for practical applications in certain, separate domains. However, fewer considerations are oriented to the specific requirements of users. Secondly, language agent customization requires lightweight, efficient, and low-resource consumption, especially for user-level customization. Different from large-scale, general training, customization peruses effective methods that involve fewer data, partial parameters, or only elaborate designed prompts. Thirdly, the balance between customization and information security needs to be maintained. The user's properties and records (such as age, gender, and medical record) may be exposed to an agent, resulting in a risk of privacy leakage.

### 6.4 Scaling up Language Agents

Multi-agent systems have exhibited social phenomena (Park et al., 2023, Wang et al., 2023c, Zhu et al., 2023). Inspired by the observations, recent interest has considered scaling the number of language agents ( $\mathrm{Li}$ et al. 2023a) to form a large-scale language model society. However, computation overhead is still an obstacle when modeling multi-agent communications ( $\mathrm{Xi}$ et al., 2023). In the realm of future prospects, the exploration of scaling unveils intriguing possibilities across two pivotal domains. Firstly, there arises a profound curiosity concerning the potential emergence of novel capabilities within a singular agent amidst communication. Secondly, comprehending the implications of scaling, such as personality change and social phenomenon, becomes imperative in empowering language agents to address increasingly complex challenges. Furthermore, this comprehension serves as a linchpin in observing, detecting, and mitigating the risks entailed by potentially harmful behaviors, thereby ensuring the secure and beneficial evolution of these agents for the betterment of society.

### 6.5 Safety of Language Agents

Imagine a near future where intelligent agents are anticipated to seamlessly collaborate with humans and other agents, simplifying daily tasks and interacting with diverse environments. This convenience is accompanied by a significant challenge: ensuring the safety of these agents, especially during prolonged, multi-round interactions. For example, the popular user interface agents designed for web operation (Zhou et al., 2023c) and mobile device control (Zhang \& Zhang, 2023) may result in privacy leakage and permission abuse. Shaikh et al. (2022) called for attention to the bias and toxicity in Zero-Shot-CoT reasoning as it tends to significantly induce the model to produce harmful or undesirable output, which may also bring negative effects in language agents. Effectively addressing the safety challenge demands[^2]a multifaceted approach. Firstly, the exploration of more robust and controllable model architectures, coupled with an in-depth understanding of their underlying mechanisms, shows great promise. Delving into the intricacies of agent behavior and enhancing the reliability of their responses are pivotal in this endeavor. Secondly, the rapid evolution of attacks tailored for LLMs necessitates a reevaluation of traditional defense techniques.

Existing studies concerning LLM safety mainly focus on content safety, such as offensiveness, fairness, and bias of LLM-generated contents (Zhang et al. 2023b). As language agents are exposed to multi-turn interactions in distinct environments possibly with operating tools, new safety risks may emerge at a systematic level Xu et al., 2023c; Sato et al. 2023), ranging from instruction input, environment perception, reasoning process, as well as tool use. We summarize three key properties of agent safety risks, including (i) new attacking types, such as operation attacking by environment injection (Liu et al., 2023d), tool misuse (Fu et al., 2023), jailbreaking (Deng et al., 2023, Wei et al. 2023a), and privacy leakage (Kim et al. 2023b); (ii) new attacking surface during the interaction between agent-human, agent-agent, and agent-environment; (iii) complex types of environments, such as operation systems, third-party applications, webpages, and virtual environment.

However, the safety of language languages has been underexplored. The definition of language agent safety has not yet reached an agreement. Novel attack methods, specifically designed for language agents, present unique challenges. Consequently, innovative defense strategies must be developed to mitigate safety risks induced by these sophisticated attacks, particularly in complex environments. This dual focus on building benchmarking resources, enhancing internal safety measures, and fortifying defenses against external threats is paramount for ensuring the secure integration of intelligent agents into our daily lives.

### 6.6 Evaluation of Language Agents

Early studies in NLP mainly focus on assessing a specific ability of models, for example, machine translation, question answering, and summarization (Chang et al. 2023). The evaluation tends to be dataset-centered, which makes it hard to reflect the model's general ability. In the era of LLMs, more comprehensive benchmark datasets have been released, such as MMLU (Hendrycks et al., 2021a), BIG-Bench (Srivastava et al., 2022), and AGI Eval (Zhong et al., 2023). However, the major focus of those benchmark datasets is on the understanding and reasoning abilities of LLMs. Besides, they are mostly single-turn evaluations, which makes it hard to evaluate the planning and decision-making abilities of LLM in distinct environments.

There is an increasing interest in developing environment-centered evaluation approaches. As language agents are exposed to interactive environments, it remains challenging to evaluate those agents in a volatile environment. Specifically, the measurement of task success might be task-specific and ambiguous. For example, in a system control problem (Rawles et al. 2023), a user instruction can be completed by different trajectories, however, it is hard to annotate all possible ways as gold labels for evaluation. To address the challenge, simulation-based evaluation (Wang et al., 2023e; Yang et al., 2023b, Ruan et al. 2023b) has attracted increasing interest. Execution feedback or external judgment can be used to measure if the task is successful or not. However, execution feedback is not always accessible in every kind of environment, and using external judgment may also include model bias (Wang et al., 2023e).

Besides assessing task success rate, it is also critical to consider safety risks as discussed in Section 6.5. Furthermore, as language agents may evolve in the environments, especially in multi-agent communities, how to track and evaluate the agent properties is also a challenge.

## 7 Conclusion

In slightly over a year, CoT techniques have substantially enhanced the reasoning capabilities of LLMs. Going beyond the confines of reasoning tasks in NLP, CoT techniques have been expanded to facilitate the development of language agents. These agents have demonstrated the ability to comprehend language instructions and execute actions in diverse environments. This study meticulously examines the evolution from CoT reasoning to the automation of language agents, offering a comprehensive review and delving into key research topics. These topics include investigating the foundational mechanics underpinning CoT techniques, understanding the paradigm shift associated with CoT, and exploring the emergence of language agents facilitated by CoT techniques. Furthermore, this research delineates several promising avenues for future exploration, including aspects related to generalization, efficiency, customization, scaling, and safety.

## References

Adept. Act-1: Transformer for actions. https://www.adept.ai/act, 2022.

Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans and replicate human subject studies. In International Conference on Machine Learning, pp. 337-371. PMLR, 2023.

Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz. Playing repeated games with large language models. ArXiv preprint, abs/2305.16867, 2023. URL https://arxiv.org/abs/2305.16867.

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022.

Hui Bai, Ran Cheng, and Yaochu Jin. Evolutionary reinforcement learning: A survey. Intelligent Computing, 2:0025, 2023a.

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. ArXiv preprint, abs/2308.12966, 2023b. URL https://arxiv.org/abs/2308.12966.

Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, Shashank Shet, et al. Codeplan: Repository-level coding using llms and planning. ArXiv preprint, abs/2309.12499, 2023. URL https://arxiv.org/abs/2309.12499.

Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar. Introducing our multimodal models, 2023. URL https://www.adept.ai/blog/fuyu-8b

Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. ArXiv preprint, abs/2308.09687, 2023. URL https://arxiv.org/abs/2308. 09687 .

Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, and Huajun Chen. When do program-of-thoughts work for reasoning? ArXiv preprint, abs/2308.15452, 2023. URL https://arxiv.org/abs/2308.15452.

Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. ArXiv preprint, abs/2304.05332, 2023. URL https://arxiv.org/abs/2304.05332.

Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. ArXiv preprint, abs/2304.05376, 2023. URL https://arxiv.org/abs/2304.05376.

Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. ArXiv preprint, abs/2305.17126, 2023. URLhttps://arxiv.org/abs/2305.17126.

Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ArXiv preprint, abs/2307.03109, 2023. URL https://arxiv.org/abs/2307.03109.

Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. ArXiv preprint, abs/2310.05915, 2023a. URL https://arxiv.org/abs/2310.05915.

Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading. ArXiv preprint, abs/2310.05029, 2023b. URL https://arxiv.org/abs/ 2310.05029 .

Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. ArXiv preprint, abs/2211.12588, 2022. URL https: //arxiv.org/abs/2211.12588

Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language model. ArXiv preprint, abs/2305.18565, 2023c. URL https://arxiv.org/abs/2305.18565

Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. ArXiv preprint, abs/2304.05128, 2023d. URL https://arxiv.org/abs/2304.05128.

Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. A survey of chain of thought reasoning: Advances, frontiers and future. ArXiv preprint, abs/2309.15402, 2023. URL https://arxiv.org/abs/2309.15402.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. ArXiv preprint, abs/2210.11416, 2022. URLhttps://arxiv.org/abs/2210.11416.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021. URLhttps://arxiv.org/abs/2110.14168.

Yuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao Yin, Jiaju Lin, Linhao Ye, Zhiyan Tie, Yougen Zhou, Yilei Wang, et al. Educhat: A large-scale language model-based chatbot system for intelligent education. ArXiv preprint, abs/2308.02773, 2023. URLhttps://arxiv.org/abs/2308.02773.

Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. ArXiv preprint, abs/2307.08715, 2023. URL https://arxiv.org/abs/2307.08715.

Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. Active prompting with chain-of-thought for large language models. ArXiv preprint, abs/2302.12246, 2023. URL https://arxiv.org/abs/2302.12246

Surís Dídac, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. ArXiv preprint, abs/2303.08128, 2023. URLhttps://arxiv.org/abs/2303.08128.

Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. ArXiv preprint, abs/2303.03378, 2023. URLhttps://arxiv.org/abs/2303.03378.

Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. ArXiv preprint, abs/2305.14325, 2023. URL https://arxiv.org/ $\mathrm{abs} / 2305.14325$.

Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, and Earlence Fernandes. Misusing tools in large language models with visual adversarial examples. 2023.

Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang. Openagi: When llm meets domain experts. ArXiv preprint, abs/2304.04370, 2023. URL https://arxiv.org/abs/2304.04370.

Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021. doi: 10.1162/tacl_a_00370. URLhttps://aclanthology.org/2021.tacl-1.21

Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. ArXiv preprint, abs/2305.11738, 2023a. URL https://arxiv.org/abs/2305.11738.

Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving, 2023b.

Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A realworld webagent with planning, long context understanding, and program synthesis. ArXiv preprint, abs/2307.12856, 2023. URL https://arxiv.org/abs/2307.12856.

Wes Gurnee and Max Tegmark. Language models represent space and time, 2023.

Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. Exploring human-like translation strategy with large language models. ArXiv preprint, abs/2305.04118, 2023. URL https://arxiv.org/abs/2305.04118.

James Hendler. Is there an intelligent agent in your future? Nature, 11, 1999.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URLhttps://openreview.net/forum?id= d7KBjmI3GmQ.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. URL https://datasets-benchmarks-proceedings. neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html

Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework, 2023.

Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. ArXiv preprint, abs/2306.03901, 2023. URL https://arxiv.org/abs/2306.03901

Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1049-1065, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.67. URL https://aclanthology.org/2023. findings-acl.67.

Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. ArXiv preprint, abs/2310.01798, 2023a. URL https://arxiv.org/abs/2310.01798

Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv preprint, abs/2302.14045, 2023b. URL https://arxiv.org/abs/2302.14045.

Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. Transformer-patcher: One mistake worth one neuron. arXiv preprint arXiv:2301.09785, 2023c.

Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. ArXiv preprint, abs/2210.03094, 2022. URL https://arxiv.org/abs/2210.03094.

Anushrut Jignasu, Kelly Marshall, Baskar Ganapathysubramanian, Aditya Balu, Chinmay Hegde, and Adarsh Krishnamurthy. Towards foundational ai models for additive manufacturing: Language models for $g$-code debugging, manipulation, and comprehension. ArXiv preprint, abs/2309.02465, 2023. URL https://arxiv.org/abs/2309.02465

Yeonghun Kang and Jihan Kim. Chatmof: An autonomous ai system for predicting and generating metal-organic frameworks. ArXiv preprint, abs/2308.01423, 2023. URL https://arxiv.org/abs/2308.01423

Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. ArXiv preprint, abs/2212.14024, 2022. URL https://arxiv.org/abs/2212.14024.

Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. ArXiv preprint, abs/2303.17491, 2023a. URL https://arxiv.org/abs/2303.17491

Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. Propile: Probing privacy leakage in large language models. ArXiv preprint, abs/2307.01881, 2023b. URL https://arxiv.org/abs/2307. 01881 .

Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pp. 22199-22213, 2023.

Soochan Lee and Gunhee Kim. Recursion of thought: A divide-and-conquer approach to multi-context reasoning with language models. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 623-658, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.40. URL https://aclanthology.org/2023.findings-acl.40.

Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. ArXiv preprint, abs/2303.17760, 2023a. URLhttps://arxiv.org/abs/2303.17760.

Huayang Li, Siheng Li, Deng Cai, Longyue Wang, Lemao Liu, Taro Watanabe, Yujiu Yang, and Shuming Shi. Textbind: Multi-turn interleaved multimodal instruction-following. ArXiv preprint, abs/2309.08637, 2023b. URL https://arxiv.org/abs/2309.08637.

Junlong Li, Zhuosheng Zhang, and Hai Zhao. Self-prompting large language models for open-domain qa. ArXiv preprint, abs/2212.08635, 2022. URLhttps://arxiv.org/abs/2212.08635.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. The Fortieth International Conference on Machine Learning, 2023c.

Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A benchmark for tool-augmented llms. ArXiv preprint, abs/2304.08244, 2023d. URL https://arxiv.org/abs/2304.08244.

Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. ArXiv preprint, abs/2310.06500, 2023e. URL https://arxiv.org/abs/2310.06500.

Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu. Prompting large language models for zero-shot domain adaptation in speech recognition. ArXiv preprint, abs/2306.16007, 2023f. URL https://arxiv.org/abs/2306.16007.

Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. ArXiv preprint, abs/2305.19118, 2023. URL https://arxiv.org/abs/2305.19118.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. ArXiv preprint, abs/2305.20050, 2023. URL https://arxiv.org/abs/2305.20050.

Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. Agentsims: An open-source sandbox for large language model evaluation. ArXiv preprint, abs/2308.04026, 2023. URL https://arxiv.org/abs/2308. 04026 .

Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 158-167, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.

Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning. ArXiv preprint, abs/2306.03872, 2023. URL https://arxiv.org/abs/2306.03872.

Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. ArXiv preprint, abs/2310.03744, 2023a. URL https://arxiv.org/abs/2310.03744.

Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv preprint, abs/2304.08485, 2023b. URL https://arxiv.org/abs/2304.08485.

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. ArXiv preprint, abs/2308.03688, 2023c. URL https://arxiv.org/abs/2308.03688.

Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against llm-integrated applications. ArXiv preprint, abs/2306.05499, 2023d. URL https://arxiv.org/abs/2306.05499.

Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, and Wenhai Wang. Controlllm: Augment language models with tools by searching on graphs. ArXiv preprint, abs/2310.17796, 2023e. URL https://arxiv.org/abs/2310.17796.

Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14605-14631, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.817. URL https://aclanthology.org/2023.acl-long. 817.

Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 4765-4774, 2017. URL https://proceedings.neurips.cc/ paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html

Koen Luwel, Ageliki Foustana, Patrick Onghena, and Lieven Verschaffel. The role of verbal and performance intelligence in children's strategy selection and execution. Learning and Individual Differences, 24:134-138, 2013.

Zilin Ma, Yiyang Mei, and Zhaoyuan Su. Understanding the benefits and challenges of using large language modelbased conversational agents for mental well-being support. ArXiv preprint, abs/2307.15810, 2023. URL https: //arxiv.org/abs/2307.15810.

Pattie Maes. Agents that reduce work and information overload. In Readings in human-computer interaction, pp. 811-821. Elsevier, 1995.

Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching small language models to reason. ArXiv preprint, abs/2212.08410, 2022. URL https://arxiv.org/abs/2212.08410.

Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. ArXiv preprint, abs/2307.04738, 2023. URL https://arxiv.org/abs/2307.04738.

Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Editing personality for llms. arXiv preprint arXiv:2310.02168, 2023.

Nikhil Mehta, Milagro Teruel, Patricio Figueroa Sanz, Xin Deng, Ahmed Hassan Awadallah, and Julia Kiseleva. Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback. ArXiv preprint, abs/2304.10750, 2023. URLhttps://arxiv.org/abs/2304.10750.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 35, 2022a.

Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass editing memory in a transformer. arXiv preprint arXiv:2210.07229, 2022b.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529-533, 2015.

Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya Srinet, Babak Damavandi, and Anuj Kumar. Anymal: An efficient and scalable any-modality augmented language model, 2023.

Louis-Philippe Morency, Amir Zadeh, and Paul Liang. Advanced multimodal machine learning. https://cmu-multicomplab.github.io/adv-mmml-course/spring2022/schedule/11877_week5.pdf, 2022.

Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. ArXiv preprint, abs/2112.09332, 2021. URL https://arxiv.org/abs/2112.09332.

Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In Proceedings of the 40th International Conference on Machine Learning (ICML'23), 2023.

Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and Yu Wang. Skeleton-of-thought: Large language models can do parallel decoding. ArXiv preprint, abs/2307.15337, 2023. URLhttps://arxiv.org/abs/2307. 15337

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop, 2022.

Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. Demystifying gpt self-repair for code generation. ArXiv preprint, abs/2306.09896, 2023. URL https://arxiv.org/abs/2306.09896

Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. ArXiv preprint, abs/2308.03188, 2023. URLhttps://arxiv.org/abs/2308.03188.

Joon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. ArXiv preprint, abs/2304.03442, 2023. URL https: //arxiv.org/abs/2304.03442.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080-2094, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.naacl-main. 168.

Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. ArXiv preprint, abs/2305.15334, 2023. URL https://arxiv.org/abs/2305.15334.

Ben Prystawski and Noah D Goodman. Why think step-by-step? reasoning emerges from the locality of experience. ArXiv preprint, abs/2304.03843, 2023. URL https://arxiv.org/abs/2304.03843.

Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. ArXiv preprint, abs/2307.07924, 2023. URL https://arxiv.org/abs/ 2307.07924 .

Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. Reasoning with language model prompting: A survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5368-5393, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.294. URL https://aclanthology. org/2023.acl-long. 294

Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? ArXiv preprint, abs/2302.06476, 2023a. URL https: //arxiv.org/abs/2302.06476

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. ArXiv preprint, abs/2307.16789, 2023b. URL https://arxiv.org/abs/2307.16789.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 8748-8763. PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a. html.

Sue Ramsden, Fiona M Richardson, Goulven Josse, Michael SC Thomas, Caroline Ellis, Clare Shakeshaft, Mohamed L Seghier, and Cathy J Price. Verbal and non-verbal intelligence changes in the teenage brain. Nature, 479(7371): $113-116,2011$.

Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for android device control. ArXiv preprint, abs/2307.10088, 2023. URL https://arxiv.org/abs/2307. 10088 .

Reworkd. Agentgpt. https://github.com/reworkd/AgentGPT, 2023.

Toran Bruce Richards. Auto-gpt: An autonomous gpt-4 experiment. https://github.com/Significant-Gravitas/Auto-GPT, 2023 .

Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, and William Yang Wang. Visual chain of thought: Bridging logical gaps with multimodal infillings. ArXiv preprint, abs/2305.02317, 2023. URL https://arxiv.org/abs/2305.02317.

Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and tool usage of large language model-based ai agents. ArXiv preprint, abs/2308.03427, 2023a. URL https://arxiv.org/abs/2308.03427

Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, and Tatsunori Hashimoto. Identifying the risks of $1 \mathrm{~m}$ agents with an $1 \mathrm{~m}$-emulated sandbox. ArXiv preprint, abs/2309.15817, 2023b. URL https://arxiv.org/abs/2309.15817.

Phillip Rust, Jonas F Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, and Desmond Elliott. Language modelling with pixels. In The Eleventh International Conference on Learning Representations, 2023.

Joseph J Ryan and Shane J Lopez. Wechsler adult intelligence scale-iii. In Understanding psychological assessment, pp. 19-42. Springer, 2001.

Megan Kinniment Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Tao R Lin, Hjalmar Wijk, Joel Burget, Aaron Ho, et al. Evaluating language-model agents on realistic autonomous tasks. 2023.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. ArXiv preprint, abs/2302.04761, 2023. URLhttps://arxiv.org/abs/2302.04761.

John R Searle. Speech acts: An essay in the philosophy of language, volume 626. Cambridge university press, 1969.

Dhruv Shah, Michael Equi, Blazej Osinski, Fei Xia, Brian Ichter, and Sergey Levine. Navigation with large language models: Semantic guesswork as a heuristic for planning. ArXiv preprint, abs/2310.10103, 2023. URL https: //arxiv.org/abs/2310.10103

Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning. arXiv preprint arXiv:2212.08061, 2022.

Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: A trainable agent for role-playing. ArXiv preprint, abs/2310.10158, 2023. URL https://arxiv.org/abs/2310.10158.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. ArXiv preprint, abs/2303.17580, 2023. URL https://arxiv.org/abs/ 2303.17580 .

Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=fR3wGCk-IXp.

Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.

Kashun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with chain-of-thought from labeled data. ArXiv preprint, abs/2302.12822, 2023. URL https://arxiv.org/abs/2302.12822

Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Schärli, Aakanksha Chowdhery, Philip Andrew Mansfield, Blaise Agüera y Arcas, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. ArXiv preprint, abs/2212.13138, 2022. URLhttps://arxiv.org/abs/2212.13138

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv preprint, abs/2206.04615, 2022. URL https: //arxiv.org/abs/2206.04615

Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. ArXiv preprint, abs/2310.12397, 2023. URL https://arxiv.org/abs/2310. 12397 .

Robert J. Sternberg, Janet S. Powell, and Daniel B. Kaye. The nature of verbal comprehension. Poetics, 11(2):155-187, 1982. ISSN 0304-422X. doi: https://doi.org/10.1016/0304-422X(82)90031-6. URL https://www.sciencedirect. com/science/article/pii/0304422X82900316

Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths. Cognitive architectures for language agents. ArXiv preprint, abs/2309.02427, 2023. URL https://arxiv.org/abs/2309.02427.

Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles Baechler, Yu-Chung Hsiao, Jindong Chen, Abhanshu Sharma, and James W. W. Stout. Towards better semantic understanding of mobile interfaces. In Proceedings of the 29th International Conference on Computational Linguistics, pp. 5636-5650, Gyeongju, Republic of Korea, 2022. International Committee on Computational Linguistics. URL https://aclanthology.org/2022.coling-1.497.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149-4158, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421.

Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. U12: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations, 2022.

XAgent Team. Xagent: An autonomous agent for complex task solving, 2023.

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chainof-thought reasoning for knowledge-intensive multi-step questions. ArXiv preprint, abs/2212.10509, 2022. URL https://arxiv.org/abs/2212.10509.

Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Can large language models really improve by selfcritiquing their own plans? ArXiv preprint, abs/2310.08118, 2023. URL https://arxiv.org/abs/2310.08118.

Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2717-2739, Toronto, Canada, 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.153. URL https://aclanthology. org/2023.acl-long. 153

Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al. Survey on factuality in large language models: Knowledge, retrieval and domain-specificity. ArXiv preprint, abs/2310.07521, 2023b. URL https://arxiv.org/abs/2310.07521

Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. ArXiv preprint, abs/2305.16291, 2023c. URL https://arxiv.org/abs/2305.16291.

Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-andsolve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 26092634. Association for Computational Linguistics, 2023d. doi: 10.18653/v1/2023.acl-long.147. URL https: //doi.org/10.18653/v1/2023.acl-long.147.

Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, and Ji-Rong Wen. When large language model based agent meets user behavior analysis: A novel user simulation paradigm, 2023e.

Xinyi Wang and William Yang Wang. Reasoning ability emerges in large language models as aggregation of reasoning paths: A case study with knowledge graphs. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented ensembles in language models. ArXiv preprint, abs/2207.00747, 2022. URL https://arxiv.org/abs/2207.00747.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023f. URL https://openreview.net/pdf?id=1PL1NIMMrw

Yiming Wang, Zhuosheng Zhang, and Rui Wang. Element-aware summarization with large language models: Expertaligned evaluation and chain-of-thought method. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8640-8665, Toronto, Canada, 2023g. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.482. URL https://aclanthology.org/2023. acl-long. 482 .

Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023a.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. Survey Certification.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2023b.

Lilian Weng. Llm-powered autonomous agents. lilianweng.github.io, 2023. URLhttps://lilianweng.github.io/ posts/2023-06-23-agent/.

Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners with selfverification. ArXiv preprint, abs/2212.09561, 2022. URLhttps://arxiv.org/abs/2212.09561

David E Wilkins. Practical planning: extending the classical AI planning paradigm. Elsevier, 2014.

Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge engineering review, $10(2): 115-152,1995$.

Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. ArXiv preprint, abs/2309.05519, 2023. URLhttps://arxiv.org/abs/2309.05519.

Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. ArXiv preprint, abs/2309.07864, 2023. URLhttps://arxiv.org/abs/2309.07864.

Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI

Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining the inter-consistency of large language models: An in-depth analysis via debate. arXiv e-prints, pp. arXiv-2305, 2023a.

Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. ArXiv preprint, abs/2309.16039, 2023b. URL https://arxiv.org/abs/2309.16039.

Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao. Expertprompting: Instructing large language models to be distinguished experts. ArXiv preprint, abs/2305.14688, 2023a. URL https://arxiv.org/abs/2305.14688.

Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. Rewoo: Decoupling reasoning from observations for efficient augmented language models. ArXiv preprint, abs/2305.18323, 2023b. URL https://arxiv.org/abs/2305.18323.

Liang Xu, Kangkang Zhao, Lei Zhu, and Hang Xue. Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese. ArXiv preprint, abs/2310.05818, 2023c. URL https: //arxiv.org/abs/2310.05818

Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al. Lemur: Harmonizing natural language and code for language agents. ArXiv preprint, abs/2310.06830, 2023d. URLhttps://arxiv.org/abs/2310.06830.

Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. ArXiv preprint, abs/2309.03409, 2023a. URLhttps://arxiv.org/abs/2309.03409

Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. ArXiv preprint, abs/2310.06114, 2023b. URL https://arxiv.org/abs/2310. 06114 .

Zhao Yang, Song Bai, Li Zhang, and Philip HS Torr. Learn to interpret atari agents. ArXiv preprint, abs/1812.11276, 2018. URLhttps://arxiv.org/abs/1812.11276.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2022.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv preprint, abs/2305.10601, 2023a. URL https://arxiv.org/abs/2305.10601.

Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large language agents with policy gradient optimization. ArXiv preprint, abs/2308.02151, 2023b. URLhttps://arxiv.org/abs/2308.02151.

Yao Yao, Zuchao Li, and Hai Zhao. Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. ArXiv preprint, abs/2305.16582, 2023c. URL https://arxiv.org/abs/2305.16582.

Fei Yu, Hongbo Zhang, and Benyou Wang. Nature language reasoning, a survey. ArXiv preprint, abs/2303.14725, 2023a. URL https://arxiv.org/abs/2303.14725.

Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen. Towards better chain-of-thought prompting strategies: A survey. ArXiv preprint, abs/2310.04959, 2023b. URL https://arxiv.org/abs/2310.04959.

Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Ren Fung, Hao Peng, and Heng Ji. Craft: Customizing llms by creating and retrieving from specialized toolsets. ArXiv preprint, abs/2309.17428, 2023. URL https://arxiv.org/abs/ 2309.17428 .

Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. ArXiv preprint, abs/2309.05653, 2023. URL https://arxiv.org/abs/2309.05653.

Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. ArXiv preprint, abs/2303.16199, 2023a. URL https://arxiv.org/abs/2303.16199.

Xiaoyi Zhang, Lilian de Greef, Amanda Swearngin, Samuel White, Kyle Murray, Lisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, et al. Screen recognition: Creating accessibility metadata for mobile applications from pixels. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-15, 2021.

Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. Safetybench: Evaluating the safety of large language models with multiple choice questions. ArXiv preprint, abs/2309.07045, 2023b. URL https://arxiv.org/abs/2309.07045.

Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. ArXiv preprint, abs/2309.11436, 2023. URL https://arxiv.org/abs/2309.11436.

Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations (ICLR 2023), 2023c.

Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. ArXiv preprint, abs/2302.00923, 2023d. URL https://arxiv.org/abs/2302. 00923 .

Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning. ArXiv preprint, abs/2309.07915, 2023a. URL https://arxiv.org/abs/2309.07915.

Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. Verify-and-edit: A knowledge-enhanced chain-of-thought framework. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 5823-5840. Association for Computational Linguistics, 2023b. doi: 10.18653/v1/2023.acl-long.320. URL https://doi.org/10.18653/v1/2023.acl-long. 320

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. ArXiv preprint, abs/2303.18223, 2023c. URL https://arxiv.org/abs/2303.18223.

Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. ArXiv preprint, abs/2305.14078, 2023d. URL https://arxiv.org/abs/2305.14078.

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. ArXiv preprint, abs/2304.06364, 2023. URL https://arxiv.org/abs/2304.06364.

Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. ArXiv preprint, abs/2310.04406, 2023a. URL https://arxiv.org/abs/2310.04406.

Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. ArXiv preprint, abs/2308.07921, 2023b. URL https://arxiv.org/abs/2308.07921.

Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. ArXiv preprint, abs/2307.13854, 2023c. URL https://arxiv.org/abs/2307.13854.

Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, et al. Agents: An open-source framework for autonomous language agents. ArXiv preprint, abs/2309.07870, 2023d. URL https://arxiv.org/abs/2309.07870.

Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. Llm as dba. ArXiv preprint, abs/2308.05481, 2023e. URL https: //arxiv.org/abs/2308.05481.

Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023f. URL https://openreview. net/pdf?id=92gvk82DE-

Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. ArXiv preprint, abs/2305.17144, 2023. URLhttps://arxiv.org/abs/ 2305.17144 .

Jin Ziqi and Wei Lu. Tab-cot: Zero-shot tabular chain of thought. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 10259-10277, 2023.

Anni Zou, Zhuosheng Zhang, and Hai Zhao. Aurora: Augmented reasoning and refining with task-adaptive chain-ofthought prompting, 2023. URL https://anni-zou.github.io/aurora-en.github.io/.


[^0]:    ${ }^{*}$ Equal contribution. We thank Diyi Yang for providing valuable feedback on the draft.

[^1]:    ${ }^{1}$ It should be noted that recent studies have explored fine-tuning smaller language models to perform CoT reasoning for specific tasks Magister et al. 2022. Yue et al. 2023. Here we only discuss general scenarios where LLMs can achieve effective CoT reasoning per se-better performance than direct reasoning-without additional task-specific fine-tuning on CoT-style training data.

[^2]:    2 https://openai.com/blog/introducing-gpts

