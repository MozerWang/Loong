# Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation 

Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin,<br>Zicheng Liu, Lijuan Wang<br>Microsoft Azure AI<br>\{zhengyang, jianfw, lindsey.li,keli, chungching.lin, zliu,lijuanw\}@microsoft.com<br>https://idea2img.github.io/


#### Abstract

We introduce "Idea to Image," a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries. Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model's characteristics. The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities. The user preference study validates the efficacy of multimodal iterative selfrefinement on automatic image design and generation.


## 1. Introduction

"Image design and generation" aims to create an image from a high-level user idea. This input IDEA can contain interleaved reference images, such as "the dog looks like the one in the image," or with instructional texts specifying the intended design usage, such as "a logo for the Idea2Img system." To convert IDEA into an image, humans may first draft detailed descriptions of the imagined image, and then use text-to-image (T2I) models [ $31,33,34,36,54]$ to gener-[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-01.jpg?height=856&width=810&top_left_y=892&top_left_x=1075)

Figure 1. Idea2Img framework enables LMMs to mimic humanlike exploration to use a T2I model, enabling the design and generation of an imagined image specified as a multimodal input IDEA. The iterative process involves LMMs functioning in different roles to refine the image creation. Specifically, LMMs will (1) generate and revise text prompts for the T2I model, (2) select the best draft images, and (3) provide feedback on the errors and revision directions. This multimodal iterative self-refinement process requires LMMs to memorize the T2I model's characteristics observed in previous iterations as humans and adjust T2I prompts accordingly.

ate the image. This manual process for users to search for an ideal detailed description (i.e., T2I prompt) that fits the T2I model typically involves iterative exploration $[44,58]$. As shown in Figure 1, humans may first design and draft an initial T2I prompt based on their imagined IDEA to generate. Then, they can obtain multiple draft images with a T2I model, select the most promising draft, write text feedback,

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-02.jpg?height=545&width=591&top_left_y=232&top_left_x=171)

Concept customization \& visual pointing

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-02.jpg?height=242&width=874&top_left_y=811&top_left_x=170)

Style transfer
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-02.jpg?height=206&width=660&top_left_y=1057&top_left_x=168)

Image manipulation

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-02.jpg?height=220&width=876&top_left_y=1283&top_left_x=169)

In-context entity and style transfer

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-02.jpg?height=494&width=876&top_left_y=1536&top_left_x=169)

Figure 2. Overview of the image design and generation scenarios enabled by Idea2Img. In each sub-figure, the image and text in the left green box are the user input IDEA. The center image is the baseline results directly generated by the same T2I model with a human-written T2I prompt, and the image on the right is generated with the T2I prompt discovered by Idea2Img's iterative self-refinement exploration.

and further revise the T2I prompt. As this iteration progresses, we humans can swiftly grasp the characteristics of a specific T2I model, such as words that the model can not understand, finally producing a good image generated by a suitable T2I prompt. Given the remarkable capabilities of large multimodal models (LMMs) [13, 26, 48], we explore if we can build systems based on LMMs to develop similar iterative self-refinement ability, thereby relieving humans from the tedious process of converting ideas to images.

Iterative self-refinement is one intrinsic ability humans possess when exploring unknown environments and solving complicated problems. Large language models (LLMs) agent systems $[9,23,40]$ have demonstrated the effectiveness of self-refinement in better addressing natural language
processing tasks, such as acronym generation, sentiment retrieval, text-based environment exploration, etc. Transitioning from text-only tasks to multimodal environments poses new challenges of improving, assessing, and verifying multimodal contents, such as multiple interleaved image-text sequences. For example, when learning to use T2I models, LMMs need to improve the generation with revised T2I prompts, assess multiple images in detail to select the best draft, and verify the draft image with the multimodal IDEA to provide text feedback. These steps, each requiring different multimodal understanding capabilities, jointly enable the intriguing multimodal iterative self-refinement ability. Such an LMM framework can automatically learn to tackle various real-world problems [48] via self-exploration, such as navigating GUI to use electronic devices, exploring unknown physical environments via an embodied agent, engaging in electronic games, and so on. In this study, we focus on "image design and generation" as the task to study the multimodal iterative self-refinement ability.

To this end, we introduce Idea2Img, a multimodal iterative self-refinement framework for automatic image design and generation. As illustrated in Figure 1, Idea2Img involves an LMM, GPT-4V(ision) [1,26-28], interacting with a T2I model to probe its usage and find an effective T2I prompt. The LMM will act in different roles to analyze the return signal from the T2I model (i.e., draft images) and design the next round's queries (i.e., text T2I prompts). The three roles of generating T2I prompts, selecting draft images, and reflecting feedback together enable the multimodal iterative self-refinement ability. Specifically, (1) Prompt generation: GPT-4V generates $N$ text prompts that correspond to the input multimodal user IDEA, conditioned on the previous text feedback and refinement history; (2) Draft image selection: GPT-4V carefully compares $N$ draft images for the same IDEA and select the most promising one; (3) Feedback reflection: GPT-4V examines the discrepancy between the draft image and the IDEA. GPT-4V then provides feedback on what is incorrect, the plausible causes, and how T2I prompts may be revised to obtain a better image. Furthermore, Idea2Img is enhanced with a memory module that stores all prompt exploration histories, including previous draft images, text prompts, and feedback. The Idea2Img framework iterates among these three steps with GPT-4V for automatic image design and generation.

To users, Idea2Img functions as an enhanced image design and generation assistant. Compared with T2I models, Idea2Img can handle design instructions instead of requiring detailed image description, support the multimodal IDEA input, and generate images of better semantic and visual qualities. We overview representative image design and generation scenarios in Figure 2. For example, Idea2Img can incorporate the visual design and intended usage description in IDEA, extract arbitrary visual infor- mation from the input image, and process IDEA with arbitrarily interleaved image-text sequences. Built upon these new functionalities and scenarios of interest, we develop an evaluation IDEA set with 104 samples, containing complicated queries that humans may fail in their first trials. We perform user preference studies on Idea2Img with different T2I models. The consistent user preference score improvements on different image generation models, e.g., $+26.9 \%$ with SDXL [31], indicate the effectiveness of Idea2Img in image design and generation.

Our contributions are summarized as follows.

- We study "automatic image design and generation," which aims to create an image from an input IDEA. This high-level idea may contain interleaved reference images, and instructional texts for intended designs.
- We explore the multimodal iterative self-refinement ability in GPT-4V-based systems, showcasing its effectiveness in improving, assessing, and verifying multimodal contents.
- We propose Idea2Img, a multimodal iterative selfrefinement framework that enhances any T2I model for image design and generation, enabling various new image creation functionalities with better image qualities.
- We present an evaluation set with 104 challenging multimodal IDEA. The consistent user preference score gains, when experimented on different image generation models, indicate Idea2Img's effectiveness in automatic image design and generation.


## 2. Related Work

LLM-based self-refinement. Idea2Img is inspired by the effectiveness of iterative self-refinement in LMM-based agent systems [23,29, 40] in exploring unknown environments and tasks, built upon the successful LLM agents [14, 30,32,37,47, 52, 57]. Self-refine [23] takes the same LLM to iteratively critique its outputs, and leverage this feedback to enhance its predictions. This framework shows effectiveness across various NLP tasks, including sentiment reversal, dialogue response, math reasoning, and more. Reflexion [40] explores a self-reflective LLM system on the text-based environment exploration task [41] and multi-hop QA [50]. Despite the success, LLM-based self-refinement naturally can not understand multimodal inputs. Consequently, the explored tasks and environments are limited to the natural language description, such as AlfWorld [41]. Idea2Img delves into the potential of an LMM-based iterative self-refinement system for multimodal environment exploration, from a simple T2I model to other more complicated environments.

Multimodal agents. Our Idea2Img is related to multimodal agents [15,20,22,38,43,45,49,55] that chain external tools
such as T2I or vision-language models with LLMs for multimodal tasks. For instance, MM-ReAct [49] integrates an LLM with multiple vision tools for multimodal reasoning and action, enabling it to solve various complicated visual understanding tasks, ranging from multi-hop document reasoning to open-world video interpretation. Visual ChatGPT [45] empowers ChatGPT to allocate various image generation models, such as Stable Diffusion [34], img2img model [24], ControlNet [56], enabling multi-step visual editing and generation. The primary difference between Idea2Img and existing multimodal agent studies [45,49] lies in the approach to understand the tool usage. Existing studies assume the knowledge of how to use each tool, and provide such information to LLMs via text instructions or incontext examples. In contrast, the optimal usage of the tool remains unknown in Idea2Img and requires iterative exploration. Another minor distinction is that Idea2Img utilizes LMMs instead of LLMs, thereby does not require general visual understanding tools such as a caption model [46].

Extensions of base T2I models. Idea2Img provides a more natural way for users to design and produce their desired visual content. This framework, which extends T2I models for new functionalities, is related to various works in improving base T2I models [31,33,34,36,54]. These studies include extending the base T2I model to better follow user prompts $[5,7,10,11]$, finding magic words in T2I prompts for better visual quality $[44,58]$, supporting extra image input for image manipulation $[6,16,17,24]$, style transfer [12], visual concept customization [3, 8, 18,35,39], and so on. While specialized T2I extensions can address a single specific functionality, Idea2Img offers a more unified and widely applicable framework. That is, a single Idea2Img framework can handle various generation scenarios, ranging from style transfer to attribute customization, without requiring separate models or task-specific model design and finetune. More importantly, Idea2Img effectively collaborates with those enhanced generative models, consistently improving them by exploring suitable text prompts.

## 3. Idea2Img Framework

Figure 3 illustrates the Idea2Img framework. Idea2Img framework involves two core pre-trained models, i.e., the GPT-4V(ision) as the LMM $\mathcal{M}$ and a T2I model $^{2}$ to explore $\mathcal{G}$. Idea2Img also contains a memory $m$ that stores insights on $\mathcal{G}$ discovered by $\mathcal{M}$ during previous iterations.

Execution flow. We begin with an overview of the key steps in $\mathcal{M}$ iteratively exploring the use of $\mathcal{G}$. Starting from the top-left of Figure 3, "initial prompt generation" converts the input multimodal user IDEA into T2I text prompts, later producing multiple draft images with T2I model $\mathcal{G}$. "Draft[^1]

image selection" then selects the best draft image among them for the current iteration. The selected image is either output as the final prediction or continues for further refinement, depending on the stop condition. For the latter, "feedback reflection" compares the current best draft image with the multimodal IDEA, and summarizes the major discrepancy as text feedback. With the iteration history and text feedback, "revised prompt generation" then drafts revised T2I prompts and continues the iterative self-refinement with the new set of draft images.

(1) Initial prompt generation. This step generates $N$ initial T2I prompts $\left\{y_{0}^{0}, \ldots, y_{0}^{N-1}\right\}$ following the input user IDEA $x$, by prompting $\mathcal{M}$ with LMM prompt $p_{g e n}$ :

$$
\begin{equation*}
\left\{y_{0}^{0}, \ldots, y_{0}^{N-1}\right\}=\mathcal{M}\left(x, p_{\text {gen }}\right) \tag{1}
\end{equation*}
$$

The "initial prompt generation" requires $\mathcal{M}$ to understand the multimodal user IDEA $x$ and convert the design IDEA into descriptive T2I prompts that the T2I model could understand. LMM prompt $p_{g e n}$ is a zero-shot prompt without in-context examples.

With the "initial prompt generation" step, Idea2Img can understand user IDEA with interleaved image-text sequences, instead of the text-only T2I prompts containing the image description. Specifically, (1) IDEA can be a highlevel design or usage instead of the detailed image description, such as "a car image for a children's painting competition"; and (2) IDEA can take multiple images and use interleaved text instruction to extract arbitrary visual information of interest, including image style, visual entity, object attributes, etc. Then, in iteration $t=0$ as well as future iterations $t=t$, each T2I prompt $y_{t}^{n}$ is separately sent to the T2I model $\mathcal{G}$, resulting in $N$ draft images $i_{t}^{n}=\mathcal{G}\left(y_{t}^{n}\right), n=0, \ldots, N-1$.

(2) Draft image selection. With the $N$ draft images in iteration $t$, "draft image selection" selects the best draft image $i_{t}^{*}$ by prompting $\mathcal{M}$ with LMM prompt $p_{\text {select }}$ :

$$
\begin{equation*}
i_{t}^{*}=\mathcal{M}\left(i_{t}^{0}, \ldots, i_{t}^{N-1}, x, p_{\text {select }}\right) \tag{2}
\end{equation*}
$$

The design of a "draft image selection" step is motivated by the observation that T2I models could generate bad images with good prompts. This step is designed to filter out low-quality images, and avoid the quality perturbation to dominate the iterative refinement.

The task of selecting the best image requires $\mathcal{M}$ to compare and grade both the semantics and visual quality of $N$ similar draft images. We find such a "spot the difference" task very challenging for LMMs, and only the very recent model [26] is capable of performing the selection reliably. Qualitative examples are shown in Figure 10.

(3) Feedback reflection. After obtaining the selected image $i_{t}^{*}$, the framework checks the stop condition, such as if

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-05.jpg?height=753&width=1726&top_left_y=255&top_left_x=167)

Figure 3. The framework overview of Idea2Img, which takes an LMM [26,27] to explore a T2I model via multimodal iterative selfrefinement, leading to an effective T2I prompt for the input user IDEA. The rounded rectangle shape indicates a GPT-4V call.

the current iteration $t$ exceeds the maximum $T$. Idea2Img then outputs $i_{t}^{*}$ as the output image or proceeds the refinement process to the "feedback reflection" step accordingly.

"Feedback reflection" aims to provide text feedback $f_{t}$ that describes the direction to improve for draft image $i_{t}^{*}$. The steps prompts $\mathcal{M}$ with LMM prompt $p_{f b}$, conditioned on the draft image $i_{t}^{*}$ and memory $m$ :

$$
\begin{equation*}
f_{t}=\mathcal{M}\left(i_{t}^{*}, m, x, p_{f b}\right) \tag{3}
\end{equation*}
$$

"Feedback reflection" takes $\mathcal{M}$ to compare an image $i_{t}^{*}$ with the multimodal user IDEA $x$, and summarize the gap as text feedback $f_{t}$. The step not only requires $\mathcal{M}$ to identify the discrepancy between image $i_{t}^{*}$ and IDEA $x$, but also benefits from writing the major errors to make the iteration effective. In practice, we find it helpful to explicitly specify the aspects to check, such as style, entity, attributes, appearance, etc., via text instructions or in-context examples in LMM prompt $p_{f b}$. Furthermore, we add text instructions to $p_{f b}$ to have $\mathcal{M}$ "focus on one thing to improve in each feedback," and "provide a high-level explanation of how to modify prompts to address the given feedback." Example feedback is shown in Figure 8.

(1) Revised prompt generation. Finally, "prompt generation" takes text feedback $f_{t}$ and memory $m$ to draft $N$ revised prompt $\left\{y_{t+1}^{0}, \ldots, y_{t+1}^{N-1}\right\}$, by prompting $\mathcal{M}$ with LMM prompt $p_{\text {revise }}$ :

$$
\begin{equation*}
\left\{y_{t+1}^{0}, \ldots, y_{t+1}^{N-1}\right\}=\mathcal{M}\left(f_{t}, m, x, p_{\text {revise }}\right) \tag{4}
\end{equation*}
$$

Generating revised prompts requires $\mathcal{M}$ to understand the property of $\mathcal{G}$ stored in memory $m$, thereby drafting new T2I prompts that could most likely address the issue identified in $f_{t}$. We empirically find Idea2Img generates
T2I prompts that work better for $\mathcal{G}$ during iterative selfrefinement, with examples shown in Figure 9.

Memory module. Memory $m$ is one important design in Idea2Img. $m$ has the format of interleaved image-text sequences that store all previous iterations' feedback, selected draft image, and the corresponding text prompts:

$$
\begin{equation*}
m_{t}=\left[y_{0}^{*}, i_{0}^{*}, f_{0}, \ldots, y_{t-1}^{*}, i_{t-1}^{*}, f_{t-1}\right] \tag{5}
\end{equation*}
$$

It allows LMM $\mathcal{M}$ to understand the properties and capabilities of the T2I model $\mathcal{G}$ in use, such as a keyword that $\mathcal{G}$ may not understand or a complicated scene that $\mathcal{G}$ fail to generate, and incorporate such knowledge in generating the revised T2I prompts $y$. For example, it may describe the appearance of a yoga pose in detail, instead of only mentioning the pose name in $y$. Examples are shown in Figures 5-7, when comparing initial and refined $\mathrm{T} 2 \mathrm{I}$ prompts $y_{0}$ and $y_{T}$.

## 4. Experiments

### 4.1. Experiment Settings

Compared model variants. We mainly compare the following three models in image generation.

- "Initial-round manual prompt" is the baseline T2I prompt written by humans with minor prompt engineering. It serves as the baseline of a T2I prompt that merely contains key information in IDEA.
- "Initial-round Idea2Img prompt" is the LMMgenerated T2I prompt in the initial round. Specifically, the max iteration $T=1$, and LMM $\mathcal{M}$ is only used for initial prompt generation and draft image selection, but not feedback reflection nor revised prompt

| Method (\%) | SDXL v1.0 | DeepFloyd IF | SD v2.1 | SD v1.5 | SDXL-img2img | IF-img2img |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| Initial-round manual prompt | 13.5 | 14.4 | 13.5 | 8.6 | 8.6 | 8.6 |
| Initial-round Idea2Img prompt | 29.8 | 34.6 | 40.4 | 43.3 | 34.6 | 38.5 |
| Iterative self-refined Idea2Img prompt | $\mathbf{5 6 . 7}$ | $\mathbf{5 1 . 0}$ | $\mathbf{4 6 . 2}$ | $\mathbf{4 8 . 1}$ | $\mathbf{5 6 . 7}$ | $\mathbf{5 2 . 9}$ |
| $\Delta_{\text {iteration }}(\uparrow)$ | +26.9 | +16.3 | +5.8 | +4.8 | +16.3 | +14.4 |

Table 1. User preference scores when applying Idea2Img onto different image generation models. We observe that "Iterative self-refined Idea2Img prompt" is consistently favored when applied to all experimented image generation models.

generation. This Idea2Img variant is used to ablate Idea2Img's gain from prompt generation and selection, $v s$. the further iterative refinement.

- "Iterative self-refined Idea2Img prompt" is complete Idea2Img pipeline with the max iteration $T=3$.

Evaluation samples and metrics. For the quantitative evaluation, we collect a dataset of 104 user IDEA as input queries. Among them, 33 queries contain text only, 43 queries contain an image-text sequence with a single image, and the remaining 28 contains a sequence with two or more images. The text in most IDEA contains not only descriptive content text that describes the scene to generate, but also instructional text such as "a logo for commercial advertising" or "generate the pointed dog in the blue color." All test queries are manually composed.

We then perform the user preference study as the main quantitative metric. Users are presented with the IDEA and multiple images to select the best one for each IDEA. The evaluation script automatically shuffles the order during evaluation to prevent the influence of image orders.

Experimented T2I models. We experiment Idea2Img on a wide variety of T2I model $\mathcal{G}$ with diverse model capacities and functionalities. Specifically, we study Stable Diffusion (SD) v1.5 [34], SD v2.1, SDXL v1.0 with refiner [31], and DeepFloyd IF (IF-I-XL and IF-II-L) [19]. Other than T2I models, we also consider the img2img pipeline (i.e., SDEdit [24]) for SDXL and DeepFloyd IF, as a demonstration of using Idea2Img for the text-conditioned image-toimage generation. The default strength $t_{0}$ in the img2img pipeline is 1.00. SDXL-img2img and IF-img2img are the same as SDXL and IF (i.e., T2I) when IDEA contains text only, and condition on the first image with IDEA contains multiple images. LMM prompts $p_{\text {gen }}, p_{\text {select }}, p_{f b}, p_{\text {revise }}$ are kept the same for all experimented T2I models.

### 4.2. Image Generation Results

User preference evaluation. Table 1 compares the user preference when selecting from the three images generated by "initial-round manual prompt," "initial-round Idea2Img prompt," and "iterative self-refined Idea2Img prompt," using the same T2I model, for each user IDEA. Among T2I models with different model sizes and functionalities, Idea2Img leads to consistent improvements in user preference. The initial-round Idea2Img prompt already improves the initial-round manual prompt, by effectively un-

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-06.jpg?height=431&width=789&top_left_y=543&top_left_x=1080)

Figure 4. User preference scores among T2I models before and after iterative self-refinement. We observe that the initially favored T2I model, SDXL, benefits more from the Idea2Img iteration.

derstanding the multimodal user IDEA and selecting the best draft images. The full Idea2Img framework further improves from the initial-round Idea2 Img results with the multimodal iterative self-refinement. For example, when using SDXL v1.0, users prefer the images generated by Idea2Img $59 / 104=56.7 \%$ times, compared with the baseline of $14 / 104=13.5 \%$. Similar improvements are observed on all experimented T2I models, as shown in the bolded row of "iterative self-refined Idea2Img prompt."

Furthermore, we examine which T2I model benefits the most from the LMM iterative self-refinement. By comparing the $\Delta_{\text {iteration }}$ in Table 1 that represents the difference between first-round and iterative Idea2Img user preference, we observe that stronger T2I models tend to benefit more from LMM refinements. For example, SDXL and IF become more favored $26.9 \%$ and $16.3 \%$ times, compared with SD v2.1 and SD v1.5's $5.8 \%$ and $4.8 \%$. The trend that stronger T2I models benefit more from Idea2Img is also observed in Figure 4's analysis, where users pick their preferred image generated by different T2I models. After Idea2Img's iterative refinement, the initially favored model SDXL benefits more from the iteration, resulting in an even higher user preference rate. We conjecture that the better language understanding ability in stronger T2I models enables them to better follow revised T2I prompts. They also have a better image generation capability that makes it possible to generate challenging scenes, when given a good T2I prompt optimized by Idea2Img. We are eager to see how this trend evolves as we experiment with even stronger T2I models [2,25] in the future. Nonetheless, Idea2Img is effective across T2I models of varying capacities, consistently leading to a higher user preference score.

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-07.jpg?height=2049&width=1740&top_left_y=252&top_left_x=152)

Figure 5. The comparisons among the initial-round manual prompts, initial-round Idea2Img prompts, and the iterative self-refined Idea2Img prompts, with the SDXL v1.0 [31] used as the T2I model.

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-08.jpg?height=1971&width=1726&top_left_y=245&top_left_x=167)

Figure 6. The comparisons among the initial-round manual prompts, initial-round Idea2Img prompts, and the iterative self-refined Idea2Img prompts, with the SDXL-img2img [24,31] used as the image generation model. Instead of random noise, the image generation starts from the input image with added noise [24], showing the effectiveness of Idea2Img on text-conditioned image-to-image pipelines.

Iterative self-refined

Idea2Img prompt
Input user IDEA

SD v1.5

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=211&width=421&top_left_y=388&top_left_x=169)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=190&width=404&top_left_y=623&top_left_x=183)

SD v2.1
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=414&width=418&top_left_y=863&top_left_x=170)

DeepFloyd IF
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=422&width=422&top_left_y=1320&top_left_x=168)

DeepFloyd IF img2img
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=434&width=422&top_left_y=1788&top_left_x=168)

Initial-round manual prompt
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=1844&width=446&top_left_y=388&top_left_x=596)
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=900&width=864&top_left_y=384&top_left_x=1029)

black shirt.
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=428&width=644&top_left_y=1320&top_left_x=1244)

Figure 7. The comparisons among the initial-round manual prompts, initial-round Idea2Img prompts, and the iterative self-refined Idea2Img prompts, with other image generation models, including SD v1.5, SD v2.1 [34], DeepFloyd IF, and IF-img2img [19].

Qualitative comparisons. Figures 5-7 show the qualitative results of the comparison in Table 1 to probe the improvements made by Idea2Img. Specifically, Figure 5 presents examples of Idea2Img explores the use of SDXL, a representative T2I model. Figure 6 examines SDXL-img2img, a representative text-conditioned image-to-image model that adds noise to the input image and then performs textconditioned denoising [24]. Figure 7 contains the results of Idea2Img working with other image generation models.

Idea2Img could help users generate images that better follow IDEA, such as the one with correct object counts and rendered scene texts in Figures 5(a,b). Idea2Img enables visual content design, in addition to generation only that requires a detailed visual content description. For example in Figure 5(c), Idea2Img designs visual logo based on the user IDEA of "having a logo for a 2024 conference in Seattle." Idea2Img can also understand user IDEA to search for images with high aesthetic scores and great visual details, or its opposite direction with "minimal face details" in Figure 5(d). The power of LMMs allows Idea2Img to extract arbitrary information from the input image for visual generation. This could be any specific object in the image, such as "the dog on the left" or "the dog pointed to via a red circle" in Figure 5(e). Figure 5(f) shows an example of extracting the image style that requires domain knowledge for humans to accurately describe. The image input can even be an in-context example that defines the desired image transformation, such as the visual style transfer shown in Figure 5(g). The ability to extract arbitrary information from the input image can be seamlessly extended to compose multiple visual and text conditions, such as composing the camera angle and image style in Figure 5(h), and two objects in Figure 5(i).

Idea2Img is also effective in finding T2I prompts for the text-conditioned image-to-image model SDXL-img2img, as shown in Figure 6. Figures 6(c) and (d) illustrate generating images that follow and differ from the reference image style of watercolor and impressionist, respectively. Idea2Img can design visual contents with the inspiration of input images, e.g., a cartoon drawing that blends the llama and the person in suits in Figure 6(f), and composing the coke with the table in an imaginative way in Figure 6(g). Figure 6(h) illustrates representing the anticipated action of rolling the sushi with a generated image. Furthermore, Figure 7 shows the Idea2Img results on other T2I models, including SD v1.5, v2.1, DeepFloyd IF, and IFimg2img. Despite the variance in the base T2I models' capacity, Idea2Img consistently helps design and generate better images.

How Idea2Img may assist humans? We use selected qualitative results to highlight the scenarios where humans might find Idea2Img most helpful in image design and generation, compared with conventional T2I generation.

- Multimodal IDEA inputs. Idea2Img provides a more natural way for human interaction, where users do not have to describe their desired image solely through texts and painstakingly search for the right terminology. Instead, the multimodal IDEA allows Idea2Img to precisely extract specific elements from one or multiple input images, such as the dog breed and color, pointed objects, artist style, camera view, and more, as shown in Figure 5. Finding proper words that the T2I model can understand for such visual concepts could be tedious for humans, e.g., the art style "with bold lines and bright colors, with a cubist and surrealistic style, with a focus on geometric shapes and patterns." in Figure 5(f). Idea2Img automates this process via Idea2Img iterative self-refinement.
- Instructional texts for intended designs and usages. Vanilla T2I models struggle to understand T2I prompts that describe the intended visual design or purpose of the generated image, such as "a logo for a 2024 conference held in Seattle" or "a car image for a children's painting competition." Instead, the prompts need to be a comprehensive description of the image to generate, e.g., "a modern and minimalist logo design, featuring the Seattle skyline in the center and the conference title below it with a blue and green color scheme" (Figure 5(c)). This demands extra drafting effort from users. In contrast, as shown in Figures 5-7, Idea2Img effectively understands the instructional texts in IDEA and creates images accordingly.

Finally, the GPT-4V iterative refinement allows Idea2Img to generate images with better semantic and visual qualities, leading to an effective automatic image creation assistant.

### 4.3. LMM Feedback, Revision, and Selection

One may wonder about the behavior and performance of LMM playing each role in Idea2Img's iterative selfrefinement pipeline, i.e., "feedback reflection," "revised prompt generation," and "draft image selection." We show corresponding qualitative results as follows.

Feedback reflection. Figure 8 shows the text feedback generated by GPT-4V for the user IDEA and the draft image and T2I prompt. Idea2Img can effectively check if the generated image is correct, such as the number of oranges in (a) and the misspelled scene text "ALURE RESEACHE" in (b). In addition to text descriptions in IDEA, Idea2Img can verify if the draft image corresponds to the visual descriptions in IDEA. This includes the color and breed of the dog in (e), the exact art style in (f), and the same cloth and pose in (g). Furthermore, Idea2Img can understand and verify the IDEA containing interleaved image-text pairs, as shown in Figures 8(h,i).

In addition to identifying the discrepancy, Idea2Img also

Input user IDEA

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-11.jpg?height=214&width=545&top_left_y=283&top_left_x=194)

(b)

a whole cake on the table with words Azure Research written on the cake

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-11.jpg?height=209&width=545&top_left_y=752&top_left_x=194)

(d)

an image of a car perfect for a childrens painting competition
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-11.jpg?height=1128&width=550&top_left_y=1210&top_left_x=190)

## Draft prompt and image to revise

## Text feedback

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-11.jpg?height=220&width=526&top_left_y=280&top_left_x=755)

The image generated by the $\mathrm{Al}$ art generation model does not follow the user imagined IDEA of the scene as there are only 6 whole oranges in the image instead of 7 . To address this, the sentence prompt can be modified to specify that there should be 7 whole oranges on the table, such as in one row with exactly 7 oranges

The words on the cake in the generated image are misspelled as "ALURE RESEACHE" instead of "Azure Research". To address this, the sentence prompt should be modified to specifically mention that the words on the cake should be spelled correctly as "Azure Research".

The person in the image is not practicing yoga mountain pose, but in a different yoga pose. Because the prompt has already mention mountain pose, the Al art model might not understand what is mountain pose. To address this, the sentence prompt should be modified to specifically mention what is a mountain pose: posture where one stands with feet hip-width apart, arms stretched overhead, and gaze forward.

The car in the generated image is not perfect for a children's painting competition as it is too detailed and realistic. To address this, the sentence prompt should be modified to specifically mention that the car should be simple and cartoon-like, with less details and more vibrant colors.

The dog in the generated image is not in the blue color, and does not look like the one in the given image. The dog in the given image is a pug, while the dog in the generated image is a Labrador Retriever. To address this issue, the sentence prompt should be modified to specifically mention the breed of the dog as a pug, and the dog is in the blue color.

The style of the painting in the generated image is still not similar to the one in the given image. The given image has a cubist style with faces and objects broken up into geometric shapes and reassembled abstractly, while the generated image has a more realistic style. To address this, the sentence prompt should be modified to specifically mention the style in the given image is 'with fragmented geometric shapes that are reassembled abstractly to form objects'.

The pose of Mr Bean in the generated image is different from the one in the given image. This is because the prompt mentions 'as the given image' but the Al art model can not understand image inputs. To address this, the sentence prompt should be modified to specifically mention the pose in the given image is 'with his right arm stretched out and his left arm bent, holding the tennis racket with both hands'.

The image generated by the Al art generation model does not follow the user imagined IDEA of the scene as the image style is not similar to Claude Monet's impressionist paintings. To address this, the sentence prompt can be modified to specify that the image style should be similar to Claude Monet's impressionist paintings, with a bright and sunny atmosphere.

The dog in the generated image is not the same breed as the one in the given image. The dog in the given image is a Cavalier King Charles Spaniel, while the dog in the generated image is a husky. To address this issue, the sentence prompt should be modified to specifically mention the breed of the dog as a Cavalier King Charles Spaniel.

Figure 8. Examples of the generated text feedback. The left column shows the multimodal input user $I D E A$, and the center column shows the draft image to process as well as its corresponding text prompts. The right column shows the text feedback generated by GPT-4V. The dark blue color highlights the identified discrepancies.

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-12.jpg?height=258&width=436&top_left_y=256&top_left_x=152)

Draft prompt and image to revise
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-12.jpg?height=468&width=1288&top_left_y=260&top_left_x=172)

Revised prompt and image
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-12.jpg?height=428&width=1284&top_left_y=514&top_left_x=172)

The image generated by the Al art generation model does no follow the userimagined JEA of the scene as there are
bananas on the plate and orange juice in the glass. To address this, the sentence prompt can be modified to specify
that there should be no bananas on the plate and no orange juice in the glass.
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-12.jpg?height=838&width=1720&top_left_y=304&top_left_x=172)

The image generated by the Al art generation model does not not ready to take a screenshot. To address this, the sentence
prompt can be modified to specify that the fingers should be pressing the side button and the volume up button at the
same time.
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-12.jpg?height=1500&width=1722&top_left_y=732&top_left_x=170)

Figure 9. Examples of the revised prompts. The four columns, from left to right, show the input user IDEA, the draft image to be revised, generated text feedback, and the revised T2I prompt and image. The dark blue color highlights the identified discrepancies in text feedback, and how they are addressed in the revised T2I prompt. We note that the example only shows a single round of self-refinement. Therefore, the revised T2I prompt may have remaining issues to be further addressed.

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=255&width=421&top_left_y=260&top_left_x=169)

(b) an image of a hand holding an iphone 12 to
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=862&width=422&top_left_y=745&top_left_x=169)

(g)

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=149&width=419&top_left_y=1663&top_left_x=173)
textual description that keeps the content
in the people waving hand image, with the
correct style pattern.
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=418&width=422&top_left_y=1828&top_left_x=168)

Draft images to select
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=1984&width=230&top_left_y=300&top_left_x=600)
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=1984&width=462&top_left_y=300&top_left_x=820)

Text selection Image 0: The cake is present, and the words Happy Birthday are written on the cake.
However, the words are not written correctly, and there are extra decorations on the However, the words are not written correctly, and ther
cake that are not mentioned in the IDEA. Overall score: 6 . Image 1: The cake is present, and the words Happy Birthday are written on the cake.
However the However, there are extra decorations on the cake that are not mentioned in the IDEA.
Overall score 7 . Overall score: 7

Image 2: The cake is present, and the words Happy Birthday are written on the cake
The cake is also on a tabo The cake is also on a table, as mentioned in the IDEA. However, there are extra
decorations on the cake that are not mentioned in the IDEA. Overall score: 8 . 2 - This image has the highest overall score and best follows the content in the IDEA.

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=206&width=588&top_left_y=531&top_left_x=1294)

Image 0: The image has two whole red apples next to bananas. However, there are multiple bananas instead of a single banana as described in the IDEA. Overall score: 8 . Image 1: The image has red apples and bananas, but there are multiple bananas and
three apples are clearly visible. This does not strictly follow the content in the IDEA. three alpples are
Overall score: 6 .

Image 2: The image has two whole red apples next to a single banana, which strictly follows the content in the IDEA. Overall score: 10 . Image 2 strictly follows the content in the IDEA by having two whole red apples next
to a single banana.

Image 0 : The dog in the generated image is a husky, which is the same as the given
image. The dog is also playing with a rellow tall image. The dog is also playing with a yellow ball and running on the street. Overa
score: 9 . Image 1: image. However, the ball is pink instead of yellow. Overall score: 8 . Image 2: The dog in the generated image is a husk, which is the same as the given image. However, the dog is not running on the street, but instead is biting a ball.
Overall score: 7 . Overall score: 7.
0 is the image $w$

is the mage with the best overall quality, as it strictly follows the content in IDEA and
has the highest score.

Image 0: The generated image is a hand drawing of a room with a desk and bookshelf, but it does not have a bed for people to sleep. Overall score: 7 . Image 1: The generated image is a hand drawing of a room with a bed and a desk,
which follows the content in IDEA Overall score: 9 . Image 2: The generated image is a hand drawing of a room with a bed and a lamp. The
style is similar to style is similar to the given image, but there is no study table. Overall score: 6 . 1 is the image with the best overall quality, as it follows bed for people to sleep

Image 0 : The portrait of Bill Gates is present, but the style is not similar to the one in the given image. The generated image has a more realistic style, while the given Image 1: The portrait of Bill Gates is present, but the style is not similar to the one in the given image. The generated image has a more realistic style, while the given image has a more impressionistic style. Additionally, the background the generate
image is different from the given image. Overall score: 4 image is different from the given image. Overall score: 4 . Image 2: The portrait of Bill Gates is present and the style is similar to the one in the
given image. The generated image has a more impressionistic style, which is what the given image. The generated im
user imagined. Overall score: 9 .
2.

2 is the image with the best overall quality.

Image 0 : The generated image has the correct style pattern, but the content is not Overall score: 4 .

Image 1: The generated image has the correct style pattern and the content is correc. The image has a person waving hand. Overall score: 9 . Image 2: The generated image has the correct style pattern, but the content is not
correct. The image has a dog, while the correct content is 'people wavin Overall score: 4 .
1 is the image with the best overall quality.

Image 0 : The image follows the content in IDEA, with the correct objects and placement. However, the Coca Cola bottle and the table is not the same as the
the given image Overall score: 8 . the given image. Overall score: 8 .
Image 1: The image follows the
.

Image 1: The image follows the content in IDEA, with the correct objects and
placement The Coca Cola bottle is the same as the one in the given image Over score: 9 .
II
mos . placed on the table but is instead blended into the table. Overall score: 4 . 1 is the image with the best overall quality, as it follows the content in IDEA and has the correct objects and placement.

Image 0 : The dog in the generated image is in blue color and the image style is similar to the one in the IDEA. However, the dog is not running on the grass as described in
the IDEA. Overall score: 7 . Image 1: The dog in the The image style is also similar to the one in the IDEA. Overall score: 9 . Image 2: The dog in the generated image is in blue color and is running on the grass. However, the image style is not similar to the one in the IDEA. Overall score: 6 . has a higher score than the other two images.

Figure 10. Examples of the draft image selection. The right column shows justification ratings and the final selection. The dark blue color highlights the identified problems and justifications for the draft image selection. The selected draft image in the round is visualized with the blue box shown in the middle column.
points to the plausible directions that may improve the T2I prompt in the text feedback. For example, in Figure 8(c), GPT-4V mentions that "the person is not in yoga mountain pose, but the T2I prompt has already mentioned mountain pose," "the AI model might not understand what mountain pose is, and prompt should be modified to specifically mention what mountain pose is." Similarly, in Figure 8(d), Idea2Img provides guidance to have generated images better follow the user intention of "an image for a children's painting competition," by "specifically mentioning that the car should be simple and cartoon-like."

Revised prompt generation. Figure 9 compares the T2I prompts before and after revision, and visualizes how text feedback helps the revision. For example, (a) the revised T2I prompt includes a detailed description of the "yoga dolphin pose" to generate the correct body pose; (b) the revised T2I prompt mentions "an empty plate with no other objects" to avoid the T2I model misunderstand the prompt "no bananas;" (c) T2I model generates the correct hand gesture with Idea2Img providing text description on how to take a screenshot. Idea2Img also effectively addresses the identified errors in text feedback and improves the prompts for multimodal input IDEA, including the dog color in Figure $9(\mathrm{~d})$, the llama design in Figure 9(e), the study area in Figure 9(f), the human gesture in Figure $9(\mathrm{~g})$, the dog breed and human clothing in Figure 9(h), and the color of the two cats in Figure 9(i).

Draft image selection. Figure 10. T2I models may generate low-quality images even with good T2I prompts. To enable refinement in each iteration, it is critical to reduce such generation noise by selecting from multiple draft images. Performing such selection requires GPT-4V to compare multiple similar draft images and pick the one with the best overall quality. Figure 10 shows the selection results generated by GPT-4V. The LMM prompt is designed such that GPT-4V gives justifications and scores for each draft image, in addition to the final selection index. Such intermediate thoughts not only help humans interpret the selection process, but also serve as the chain of thought to improve the selection performance. We observe that GPT-4V can compare different aspects mentioned in the IDEA and give reasonable scores and selection index. For example, checking the scene text spelling in Figure 10(a); verifying the phone screen and model in Figure 10(b); counting the number of apples and bananas in Figure 10(c); verifying the ball color and dog action in Figure 10(d); finding the image with both sleep and study area in Figure 10(e); selecting the image that best fits the given image style in Figure 10(f); verifying the image content and style in Figure 10(g); locating the best blended image in Figure 10(h); and finding the image with correct dog color and image style in Figure 10(i).

## 5. Limitation and Discussion

Tasks beyond image generation. Idea2Img explores the emergent ability of multimodal self-refinement in LMMbased systems, through the image design and generation task. Specifically, Idea2Img views the T2I model to use as an unknown multimodal environment to explore, and iteratively refines T2I prompts to find its optimal usage. This concept mirrors the intrinsic human approach of iterative problem-solving when faced with unknown environments or complex challenges. We leave its extension to other intriguing tasks [48], e.g., GUI navigation, embodied agents, and complicated visual reasoning, for future exploration.

From a single image generation model to multiple tools. Idea2Img explores the usage of a single image generation model, including a text-to-image model [34] or a textconditioned image-to-image model [24]. When needed, other specialized generative models like ControlNet [56], inpainting [4], region-controlled T2I generation [21, 51], customized generation [8,35], and video generation [42,53] can be seamlessly switched as replacements. That is, Idea2Img could broadly boost different image generation models with diverse specialties by exploring their optimal text description or instruction prompts.

Beyond a single generation model, Idea2Img can also be used to allocate multiple tools as in multimodal agent studies [45,49]. In this case, Idea2Img isn't limited to optimizing the use of individual tools but also investigates their effective collaboration when used together, such as generator selection and multi-step visual generation.

Consolidating explored knowledge. We have shown the effectiveness of LMM iterative self-refinement in automatic image design and generation. Going beyond merely preserving the probed T2I model's properties in memory, Idea2Img can also help to consolidate or distill the explored knowledge into T2I model parameters, such that no inference-time iterative refinement is needed when encountering similar generation scenarios. For example, one could collect a dataset using Idea2Img for a scenario of interest, and fine-tune a T2I model with the explored selfrefinement trajectory. Storing the probed knowledge as sample-agnostic prompt for each image generation model is another promising direction $[14,32,57]$.

## 6. Conclusion

We have presented Idea2Img, a multimodal iterative selfrefinement framework that leverages GPT- $4 \mathrm{~V}$ (ision) for image design and generation. Idea2Img delves into the emergent capabilities of iterative self-refinement in LMM-based systems, showcasing its effectiveness in improving, assessing, and verifying the generated multimodal content. The user preference study demonstrates Idea2Img's capability in assisting humans to find the optimal usage of a T2I model for automatic image design and generation.

## Acknowledgment

We are deeply grateful to OpenAI for providing access to their exceptional tool $[1,26-28]$. We also extend heartfelt thanks to our Microsoft colleagues for their insights, with special acknowledgment to Faisal Ahmed, Ehsan Azarnasab, and Lin Liang for their constructive feedback.

## References

[1] Chatgpt can now see, hear, and speak. https: / openai. com/blog/chatgpt-can-now-see-hear-andspeak, 2023. 3, 15

[2] Midjourney. https://www.midjourney.com/, 2023. 6

[3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel CohenOr, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from a single image. arXiv preprint arXiv:2305.16311, 2023. 4

[4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18208-18218, 2022. 14

[5] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 4

[6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392-18402, 2023. 4

[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. arXiv preprint arXiv:2301.13826, 2023. 4

[8] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William W Cohen. Subject-driven text-to-image generation via apprenticeship learning. arXiv preprint arXiv:2304.00186, 2023. 4, 14

[9] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.2

[10] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381, 2023. 4

[11] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Trainingfree structured diffusion guidance for compositional text-toimage synthesis. In The Eleventh International Conference on Learning Representations, 2022. 4

[12] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015. 4

[13] Google. Bard. https://bard.google.com, 2023. Accessed: 2023-07-17. 2
[14] Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, and Nan Duan. Learning to program with natural language. arXiv preprint arXiv:2304.10464, 2023. 3,14

[15] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953-14962, 2023. 3

[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2022. 4

[17] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007-6017, 2023. 4

[18] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931-1941, 2023. 4

[19] DeepFloyd Lab. Deepfloyd if. https://github.com/ deep-floyd/IF, 2023. 6, 9

[20] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 2023. 3

[21] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511-22521, 2023. 14

[22] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023. 3

[23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. 2, 3

[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 4, 6, 8, 10, 14

[25] OpenAI. Dall.e 3 system card. https://cdn.openai. com/papers/DALL_E_3_System_Card.pdf, 2023. 6

[26] OpenAI. Gpt-4 technical report, 2023. 2, 3, 4, 5, 15

[27] OpenAI. Gpt-4v(ision) system card. 2023. 3, 5, 15

[28] OpenAI. Gpt-4v(ision) technical work and authors. https: / / cdn.openai. com / contributions / gpt-4v . pdf, 2023. 3, 15

[29] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188, 2023. 3

[30] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023. 3

[31] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 1, 3, 4, 6, 7, 8

[32] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495, 2023. 3, 14

[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1,4

[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022. 1, 4, 6, 9, 14

[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023. 4, 14

[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 1, 4

[37] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. 3

[38] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 3

[39] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testtime finetuning. arXiv preprint arXiv:2304.03411, 2023. 4

[40] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. 2,3

[41] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht.
Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. 3

[42] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 14

[43] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023. 3

[44] Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. Diffusiondb: A large-scale prompt gallery dataset for text-toimage generative models. arXiv preprint arXiv:2210.14896, 2022. 1,4

[45] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 3, 4, 14

[46] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280, 2022. 4

[47] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023. 3

[48] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023. 2, 3, 14

[49] Zhengyuan Yang*, Linjie Li*, Jianfeng Wang*, Kevin Lin*, Ehsan Azarnasab*, Faisal Ahmed*, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 3, 4, 14

[50] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. 3

[51] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14246-14255, 2023. 14

[52] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. 3

[53] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 14

[54] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. 1, 4

[55] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 3

[56] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. 4, 14

[57] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. arXiv preprint arXiv:2308.10144, 2023. 3,14

[58] Wanrong Zhu, Xinyi Wang, Yujie Lu, Tsu-Jui Fu, Xin Eric Wang, Miguel Eckstein, and William Yang Wang. Collaborative generative ai: Integrating gpt-k for efficient editing in text-to-image generation. arXiv preprint arXiv:2305.11317, 2023. 1,4


[^0]:    ${ }^{1}$ Short for "Idea2Img." Logo design assisted by Idea2Img.

[^1]:    ${ }^{2}$ We will show image generation models other than T2I later in experiments. For clarity, we use T2I as a representative for method introduction.

