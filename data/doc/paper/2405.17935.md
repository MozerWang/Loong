# Tool Learning with Large Language Models: A Survey 

Changle Qu ${ }^{1}$, Sunhao Dai ${ }^{1}$, Xiaochi Wei ${ }^{2}$, Hengyi Cai ${ }^{3}$, Shuaiqiang Wang ${ }^{2}$,<br>Dawei Yin ${ }^{2}$, Jun Xu ${ }^{1}$, Ji-Rong Wen ${ }^{1}$<br>${ }^{1}$ Gaoling School of Artificial Intelligence, Renmin University of China<br>${ }^{2}$ Baidu Inc., ${ }^{3}$ Institute of Computing Technology, Chinese Academy of Sciences<br>\{changlequ, sunhaodai,junxu,jrwen\}@ruc.edu.cn, caihengyi@ict.ac.cn<br>\{weixiaochi,wangshuaiqiang\}@baidu.com, yindawei@acm.org


#### Abstract

Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs. We first explore the "why" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of "how", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area.


## 1 Introduction

"Sharp tools make good work."

—The Analects: Wei Ling Gong

Throughout history, humanity has continually sought innovation, utilizing increasingly sophisticated tools to boost efficiency and enhance capabilities (Washburn, 1960; Gibson et al., 1993). These tools, extending both our intellect and physicality, have been crucial in driving social and cultural evolution (Von Eckardt, 1995). From primitive stone tools to advanced machinery, this progression has expanded our potential beyond natural limits, enabling more complex and efficient task management (Shumaker et al., 2011).

Today, we are experiencing a new technological renaissance, driven by breakthroughs in artificial intelligence, especially through the development of large language models (LLMs). Pioneering models such as ChatGPT (Achiam et al., 2023) have demonstrated remarkable capabilities, marking significant progress in a range of natural language processing (NLP) tasks, including summarization (El-Kassas et al., 2021; Zhang et al., 2024c), machine translation (Zhang et al., 2023a; Feng et al., 2024), question answering (Yang et al., 2018; Kwiatkowski et al., 2019), etc. However, despite their impressive capabilities, LLMs often struggle with complex computations and delivering accurate, timely information due to their reliance on fixed and parametric knowledge (Mallen et al., 2022; Vu et al., 2023). This inherent limitation frequently results in responses that are plausible yet factually incorrect or outdated (often referred to as hallucination) (Ji et al., 2023; Zhang et al., 2023c), posing significant risks and misleading users.

With the continuous enhancement of LLMs capabilities, it is expected that LLMs will become proficient in using tools to solve complex problems as human (Qin et al., 2023), a concept known as tool learning with LLMs. Tool learning emerges as a promising solution to mitigate these limitations of LLMs by enabling dynamic interaction with external tools (Schick et al., 2024; Qin et al., 2024; Tang et al., 2023). This approach not only enhances problem-solving capabilities of LLMs but also broadens their functional scope (Yao et al., 2022a; Lazaridou et al., 2022a; Lu et al., 2023). For instance, LLMs can perform complex calculations using a calculator tool, access real-time weather updates through weather APIs, and execute programming code via interpreters (Pan et al., 2023;

![](https://cdn.mathpix.com/cropped/2024_06_04_5bcae938f6bbc9528ff4g-02.jpg?height=880&width=1602&top_left_y=231&top_left_x=227)

Figure 1: An illustration of the development trajectory of tool learning. We present the statistics of papers with the publication year and venue, with each venue uniquely represented by a distinct color. For each time period, we have selected a range of representative landmark studies that have significantly contributed to the field. (Note that we use the institution of the first author as the representing institution in the figure.)

Wang et al., 2024d). This integration significantly improves their response accuracy to user queries, facilitating more effective and reliable user interactions. As this field continues to evolve, toolaugmented LLMs are expected to play a pivotal role in the future of NLP (Parisi et al., 2022; Karpas et al., 2022), offering more versatile and adaptable solutions (Nakano et al., 2021; Surís et al., 2023).

As shown in Figure 1, the past year has witnessed a rapid surge in research efforts on tool learning concurrent with the rise of LLMs. Notably, in practical applications, GPT-4 (Achiam et al., 2023) addresses its knowledge limitations and augments its capabilities by calling on plugins, ultimately integrating the returned results of plugins with its internal knowledge to generate better responses for users. Within the research community, much effort has been made in exploring how to evaluate the tool learning capabilities of LLMs (Li et al., 2023b; Huang et al., 2024c; Chen et al., 2023b) and how to enhance it to strengthen the capabilities of LLMs (Qin et al., 2024; Xu et al., 2023; Gao et al., 2024a; Zhao et al., 2024d). Given the increasing attention and rapid development of tool learning with LLMs, it is essential to systematically review the most recent advancements and challenges, so as to benefit researchers and industrial developers in understanding the current progress and inspire more future work in this area.

In this survey, we conduct a systematic exploration of existing studies in two primary dimensions: (1) why tool learning is beneficial and (2) how tool learning is implemented. Specifically, the "why tool learning" dimension examines both the advantages of tool integration and the inherent benefits of the tool learning paradigm, while the "how tool learning" dimension details the four stages of the entire tool learning workflow: task planning, tool selection, tool calling, and response generation. These dimensions are foundational to understanding tool learning with LLMs. Moreover, we provide a systematic summary of existing benchmarks and evaluation methods, classifying them based on their focus across different stages. Finally, we discuss the current challenges and propose future directions, offering critical insights to facilitate the development of this promising and burgeoning research area. We also maintain a GitHub repository to continually keep track of the relevant papers and resources in this rising area at https://github.com/quchangle1/ LLM-Tool-Survey.

It is worth noting that while other surveys provide comprehensive overviews of techniques and

![](https://cdn.mathpix.com/cropped/2024_06_04_5bcae938f6bbc9528ff4g-03.jpg?height=1613&width=1600&top_left_y=233&top_left_x=228)

Figure 2: The overall structure of this paper.

methods used by LLMs (Zhao et al., 2023), applications in planning (Huang et al., 2024b), reasoning (Qiao et al., 2022; Sun et al., 2023b), agents (Wang et al., 2024c; Sumers et al., 2024; Xi et al., 2023), and retrieval-augmented generation (Gao et al., 2023d; Zhao et al., 2024c), they often mention tools or tool learning but do not extensively explore this aspect. Compared with them, our survey provides a focused and detailed analysis of tool learning with LLMs, especially elucidating the dual aspects of why tool learning is essential for LLMs and how tool learning can be systematically implemented. Through these two principle aspects, we offer an up-to-date and comprehensive review of tool learning with LLMs. Meanwhile, we also acknowledge the foundational contributions of earlier perspective papers like those by Mialon et al. (2023) and Qin et al. (2023), which initially highlighted the promising opportunities that tools present to enhance LLMs capabilities. Since the field has seen rapid growth with many new studies emerging, our survey provides a broader introduction to these latest developments. Additionally, a more recent survey (Wang et al., 2024f) discusses various tooling scenarios and approaches employed in language models, serving as an excellent supplement to our comprehensive review.

The remaining part of this paper (as illustrated
in Figure 2) is organized as follows: We begin by introducing the foundational concepts and terminology related to tool learning (\$2). Following this, we explore the significance of tool learning for LLMs from six specific aspects (§3). We then systematically review the recent advancements in tool learning, focusing on four distinct stages of the tool learning workflow (\$4). Subsequently, we provide a summary of the resources available for tool learning, including benchmarks and evaluation methods (\$5). Next, we discuss the current challenges in the field and outline open directions for future research (\$6). Lastly, we conclude the survey ( $\$ 7$ ).

## 2 Background

In this section, we present an overview of the concept and terminology associated with tool learning.

What is a Tool? The definition of a tool is notably broad within the context of augmented LLMs. Mialon et al. (2023) articulates a tool as "the external employment of an unattached or manipulable attached environmental object to alter more efficiently the form, position, or condition of another object." On the other hand, Wang et al. (2024f) define a tool as "An LM-used tool is a function interface to a computer program that runs externally to the LM, where the LM generates the function calls and input arguments in order to use the tool." Similarly, it is our contention that any method enhancing LLMs through external means qualifies as a tool. Notably, retrieval-augmented generation (RAG) represents a specific instance of tool learning, wherein the search engine is employed as a tool for LLMs. Meanwhile, the definition of "tool" often remains vague and inconsistent across different papers. For example, some studies distinctly define tools and APIs, positing that a tool comprises an aggregation of multiple APIs (Patil et al., 2023; Xu et al., 2023; Qin et al., 2024). Conversely, other studies treat each API as an independent tool (Anantha et al., 2023; Li et al., 2023b; Tang et al., 2023). In this survey, adhering to the definitions of tools established earlier in the text, we consider each API as an individual tool.

What is Tool Learning? Tool learning refers to the process that "aims to unleash the power of LLMs to effectively interact with various tools to accomplish complex tasks" (Qin et al., 2024). This paradigm significantly improves the ability of LLMs to solve complex problems. For example, when ChatGPT receives a user query, it evaluates the necessity of calling a specific tool. If a tool is required, ChatGPT will transparently outline the problem-solving process using the tool, explaining the rationale behind its responses, thereby ensuring the user receives a well-informed answer. Moreover, in instances where the initial solution fails, ChatGPT will reassess its tool selection and employ an alternative to generate a new response.

## 3 Why Tool Learning?

In this section, we will delineate the multifaceted importance of tool learning for LLMs from two principal perspectives: the benefits of tool integration and the benefits of the tool learning paradigm itself. On the one hand, tool integration into LLMs enhances capabilities across several domains, namely knowledge acquisition, expertise enhancement, automation and efficiency, and interaction enhancement. On the other hand, the adoption of the tool learning paradigm bolsters the robustness of responses and transparency of generation processes, thereby enhancing interpretability and user trust, as well as improving system robustness and adaptability. Subsequent subsections will elaborate on these six aspects in detail, outlining why tool learning is important for LLMs.

### 3.1 Knowledge Acquisition

Although LLMs have showcased their immense capabilities across various fields (Ouyang et al., 2022), their abilities are still bounded by the extent of knowledge learned during pre-training (Mallen et al., 2022). This embedded knowledge is finite and lacks the ability to acquire updated information. Additionally, the effectiveness of LLMs is further compromised by prompts from users, which may not always be meticulously crafted. Consequently, LLMs are prone to generating contents that seem superficially plausible but may contain factual inaccuracies, which is known as hallucination. A promising approach to mitigate these limitations involves augmenting LLMs with the capability to access external tools, which allows LLMs to acquire and integrate external knowledge dynamically. For example, the employment of search engine tool can enable LLMs to access contemporary information (Komeili et al., 2022; Nakano et al., 2021; Lazaridou et al., 2022b; Shi et al., 2023; Schick et al., 2024; Paranjape et al.,

2023; Gou et al., 2024a), while the integration of database tool allows LLMs to access structured databases to retrieve specific information or execute complex queries, thus expanding their knowledge base (Thoppilan et al., 2022; Patil et al., 2023; Hao et al., 2024; Zhuang et al., 2024b; Zhang et al., 2024b; Gu et al., 2024). Additionally, connections to weather tools allow for real-time updates on weather conditions, forecasts, and historical data (Xu et al., 2023; Tang et al., 2023; Huang et al., 2024c), and interfacing with mapping tools enables LLMs to get and provide geographical data, aiding in navigation and location-based queries (Qin et al., 2023). Through these enhancements, LLMs can surpass traditional limitations, offering more accurate and contextually relevant outputs.

### 3.2 Expertise Enhancement

Given the fact that LLMs are trained on datasets comprising general knowledge, they often exhibit deficiencies in specialized domains. While LLMs demonstrate robust problem-solving capabilities for basic mathematical problems, excelling in operations such as addition, subtraction, and exhibiting reasonable proficiency in multiplication tasks, their abilities significantly decline when confronted with division, exponentiation, logarithms, trigonometric functions, and other more complex composite functions (Dao and Le, 2023; Wei et al., 2023). This limitation extends to tasks involving code generation (Chen et al., 2021; Austin et al., 2021) and chemistry and physics problems (Inaba et al., 2023), etc., further underscoring the gap in their expertise in more specialized areas. Consequently, it is feasible to employ specific tools to augment the domainspecific expertise of LLMs (He-Yueya et al., 2023; Kadlčík et al., 2023; Jin et al., 2024b; M. Bran et al., 2024). For example, LLMs can use online calculators or mathematical tools to perform complex calculations, solve equations, or analyze statistical data (Cobbe et al., 2021; Karpas et al., 2022; Shao et al., 2022; Kadlčík et al., 2023; He-Yueya et al., 2023; Zhang et al., 2024a; Gou et al., 2024b; Das et al., 2024; Veerendranath et al., 2024). Additionally, the integration of external programming resources such as Python compilers and interpreters allows LLMs to receive code execution feedback, which is essential for refining code to align with user requirements and to optimize the code generation (Gao et al., 2023b; Chen et al., 2022; Pan et al., 2023; Lu et al., 2024; Wang et al., 2023b, 2024d;
Wu et al., 2024b). This approach not only mitigates the expertise gap in LLMs but also enhances their utility in specialized applications.

### 3.3 Automation and Efficiency

LLMs are fundamentally language processors that lack the capability to execute external actions independently, such as reserving conference rooms or booking flight tickets (Wang et al., 2024f). The integration of LLMs with external tools facilitates the execution of such tasks by simply populating tool interfaces with the necessary parameters. For example, LLMs can employ task automation tools to automate repetitive tasks such as scheduling (Schick et al., 2024), setting reminders (Zhuang et al., 2024b), and filtering emails (Qin et al., 2024), thereby enhancing their practicality for user assistance. Moreover, by interfacing with project management and workflow tools, LLMs can aid users in managing tasks, monitoring progress, and optimizing work processes (Qin et al., 2024). In addition, the integration with online shopping assistants not only simplifies the shopping process (Yao et al., 2022a) but also enhances processing efficiency and user experience. Furthermore, employing data table processing tools enables LLMs to perform data analysis and visualization directly (Qin et al., 2023), thereby simplifying the data manipulation process of users.

### 3.4 Interaction Enhancement

Due to the diverse and multifaceted nature of user queries in the real-world, which may encompass multiple languages and modalities, LLMs often face challenges in consistently understanding different types of input. This variability can lead to ambiguities in discerning the actual user intent (Wang et al., 2024b). The deployment of specialized tools can significantly enhance the perceptual capabilities of LLMs. For example, LLMs can utilize multi-modal tools, such as speech recognition and image analysis, to better understand and respond to a broader spectrum of user inputs (Surís et al., 2023; Yang et al., 2023; Liu et al., 2023a; Gao et al., 2023a, 2024c; Zhao et al., 2024b; Ma et al., 2024b; Wang et al., 2024b). Moreover, by interfacing with machine translator tools, LLMs have the capability to convert languages in which they are less proficient into languages they comprehend more effectively (Schick et al., 2024; Qin et al., 2023). Additionally, the integration of advanced natural language processing tools can augment the linguis-
tic understanding of LLMs, thereby optimizing dialogue management and intent recognition (Qin et al., 2024; Shen et al., 2024b; Lyu et al., 2023). Such advancements may include platforms that utilize contextual understanding models to elevate the performance of chatbot systems. Ultimately, improving perceptual input and sensory perception is crucial for the progression of LLMs capabilities in managing intricate user interactions.

### 3.5 Enhanced Interpretability and User Trust

A significant concern with current LLMs is their opaque, "black-box" nature, which does not reveal the decision-making process to users (Linardatos et al., 2020; Zhao et al., 2024a), thereby severely lacking in interpretability. This opacity often leads to skepticism about the reliability of the response provided by LLMs and makes it challenging to ascertain their correctness (Weidinger et al., 2021). Moreover, interpretability is particularly crucial in high-stakes domains such as aviation, healthcare and finance (Qin et al., 2023; Theuma and Shareghi, 2024), where accuracy is imperative. Therefore, understanding and explaining LLMs is crucial for elucidating their behaviors (Zhao et al., 2024a). Some studies have enhanced the accuracy and interpretability of LLMs by enabling them to generate text with citations (Gao et al., 2023c; Sun et al., 2023a). In contrast, through the utilization of tool learning, LLMs can exhibit each step of their decision-making process, thereby making their operations more transparent (Qin et al., 2023). Even in cases of erroneous outputs, such transparency allows users to quickly identify and understand the source of errors, which facilitates a better understanding and trust in the decisions of LLMs, thus enhancing effective human-machine collaboration.

### 3.6 Improved Robustness and Adaptability

Existing research indicates that LLMs are highly sensitive to user inputs within prompts (Wallace et al., 2019; Jin et al., 2020; Wu et al., 2024a). Merely minor modifications to these inputs can elicit substantial changes in the responses, highlighting a lack of robustness in LLMs. In the real world, different users have varying interests and ways of asking questions, leading to a diverse array of prompts. The integration of specialized tools has been proposed as a strategy to reduce reliance on the statistical patterns in the training data (Qin et al., 2023; Shen et al., 2024b; Schick et al., 2024; Qin et al., 2024; Hao et al., 2024). This enhancement increases the resistance of LLMs to input perturbations and their adaptability to new environments. Consequently, such integration not only stabilizes the models in uncertain conditions but also reduces the risks associated with input errors.

## 4 How Tool Learning?

In this section, we will first introduce the overall paradigm of tool learning, which includes four distinct stages and two typical paradigms. Following this framework, we provide a detailed review of each stage within the tool learning workflow, along with the latest advancements associated with each stage. It's important to note that many works involve multiple stages of tool learning, but we only discuss its core stages here. For each stage, we also present a real example utilizing GPT-4 for tool learning to address a specific problem, which are designed to help newcomers better understand what each stage involves and how it is implemented.

### 4.1 Overall Paradigm of Tool Learning

In this section, we will introduce the entire process of tool learning, including four stages and two paradigms involved in the utilization of toolaugmented LLMs.

Four Stages of Tool Learning. As illustrated in the left part of Figure 3, the typical process of tool learning comprises four stages: task planning, tool selection, tool calling, and response generation, which is adopted in numerous works related to tools (Song et al., 2023; Shen et al., 2024b; Ruan et al., 2023a; Shen et al., 2024a). This process outlines the user interaction pipeline with toolaugmented LLMs: given a user question, the preliminary stage involves the LLMs analyzing the requests of users to understand their intent and decompose it into potential solvable sub-questions. Subsequently, the appropriate tools are selected to tackle these sub-questions. This tool selection process is categorized into two types based on whether a retriever is used: retriever-based tool selection and LLM-based tool selection. Recently, there has been an increasing focus on initially using a retriever to filter out the top-k suitable tools (Qin et al., 2024; Gao et al., 2024a; Anantha et al., 2023). This necessity stems from the fact real-world systems usually have a vast number of tools, rendering it impractical to incorporate the descriptions of all tools as input for LLMs due to the constraints related to length and latency (Qu et al., 2024). Sub-
![](https://cdn.mathpix.com/cropped/2024_06_04_5bcae938f6bbc9528ff4g-07.jpg?height=1208&width=1562&top_left_y=247&top_left_x=246)

Figure 3: The overall workflow for tool learning with large language models. The left part illustrates the four stages of tool learning: task planning, tool selection, ttool calling, and response generation. The right part shows two paradigms of tool learning: Tool Learning with One-step Task Solving and Tool Learning with Iterative Task Solving.

sequently, the user query along with the selected tools are furnished to the LLMs, enabling it to select the optimal tool and configure the necessary parameters for tool calling. This necessitates that the LLMs possess a keen awareness of using tools and be able to correctly select the tools needed. Moreover, it is imperative for the LLMs to extract the correct tool parameters from the user query, a process that demands not only the accuracy of the parameter content but also adherence to the specific format requirements of the tool's parameters. Following the invocation of the tool, the LLMs utilizes the results returned by the tool to craft a superior response for the user.

Two Paradigms of Tool Learning. As illustrated in the right part of Figure 3, the paradigms for employing tool learning can be categorized into two types: tool learning with one-step task solving and tool learning with iterative task solving. These are also referred to as planning without feedback and planning with feedback in Wang et al. (2024c), and decomposition-first and interleaved decomposition in Huang et al. (2024b). In earlier studies on tool learning (Schick et al., 2024; Shen et al., 2024b; Lu et al., 2024), the primary paradigm is tool learning with one-step task solving: upon receiving a user question, LLMs would analyze the requests of user to understand the user intent and immediately plan all the sub-tasks needed to solve the problem. The LLMs would then directly generate a response based on the results returned by the selected tools without considering the possibility of errors during the process or altering the plan based on tool feedback. Subsequent researches introduce a new paradigm known as tool learning with iterative task solving (Li et al., 2023b; Song et al., 2023; Qin et al., 2024; Gao et al., 2024a). This approach does not commit to a complete task plan upfront. Instead, it allows for iterative interactions with the
tools, adjusting the sub-tasks progressively based on tool feedback. This enables LLMs to address the problem step-by-step, refining its plan continuously in response to the results returned by tools, thereby improving problem-solving capabilities.

### 4.2 Task Planning

Generally, in real-world scenarios, user queries often embody complex intent. Thus, it is essential to first engage in task planning to conduct a comprehensive analysis of the user intent. This stage involves the decomposition of a user question into multiple sub-questions as required to achieve the specified objective. Additionally, given the dependency relationships inherent among some tasks, the planner is also tasked with delineating the dependencies and execution sequence of the decomposed tasks, thereby facilitating the establishment of interconnections between the sub-questions. For example, a real example for task planning with GPT-4 is shown as follows:

An Example for Task Planning with GPT-4

Instruction Prompt: You are currently in the task planning stage. You are given a user query requiring multi-step actions and reasoning. You will break down the user's query into sub-questions, and you only need to output these sub-questions after the breakdown. Ensure that the original problem is comprehensively covered through the minimal number of sub-questions.

User Question: I would like to know the value of 5 ounces of gold plus 1 million AMZN stocks in CNY.

Output: 1. What is the current price of gold per ounce in USD? 2. What is the current stock price of Amazon (AMZN) per share in USD? 3. What is the current exchange rate between USD and CNY (Chinese Yuan)?

The tool learning with iterative task solving paradigm is not convenient for staged demonstrations, so this example uses the tool learning with one-step task solving paradigm. From this example, we can see that GPT-4 breaks down the user's question into three sub-questions. Since internal knowledge of GPT-4 does not contain the latest information, it is necessary to obtain the current prices of gold and Amazon (AMZN) stock, as well as the latest exchange rate between USD and CNY. By breaking down the query in this manner, GPT-4 can address complex questions step by step, demonstrating its logical analysis capabilities and ability to handle multi-step tasks. Next, we will introduce the latest developments in two categories: tuningfree methods and tuning-based methods.

### 4.2.1 Tuning-free Methods

Existing studies (Paranjape et al., 2023; Zhang, 2023; Li et al., 2024) demonstrate that the innate abilities of LLMs enable effective planning through methods such as few-shot or even zero-shot prompting. For example, some studies (Huang et al., 2022; Chern et al., 2023; Xu et al., 2024b) leverage prompts to decompose complex tasks into simpler sub-tasks, facilitating a structured plan of action. ART (Paranjape et al., 2023) constructs a task library, from which it retrieves examples as few-shot prompts when encountering real-world tasks. RestGPT (Song et al., 2023) introduces a Coarse-to-Fine Online Planning approach, an iterative task planning methodology that enables LLMs to progressively refine the process of task decomposition. HuggingGPT (Shen et al., 2024b) leverages a sophisticated prompt design framework, which integrates specification-based instructions with demonstration-based parsing methodologies. ToolChain* (Zhuang et al., 2024a) employs a planning mechanism by constructing the entire action space as a decision tree, where each node within the tree represents a potential API function call. TPTU (Ruan et al., 2023a) introduces a structured framework specifically designed for LLM-based AI agents, incorporating two distinct types of agents: the one-step agent and the sequential agent. Attention Buckets (Chen et al., 2023a) operates in parallel with unique RoPE angles, forming distinct waveforms that compensate for each other, reducing the risk of LLMs missing critical information. ControlLLM (Liu et al., 2023b) introduces a paradigm known as Thoughts-on-Graph (ToG), which leverages Depth-First Search (DFS) on a pre-constructed tool graph to identify solutions. PLUTO (Huang et al., 2024a) utilizes an autoregressive planning approach that iteratively enhances its performance by generating hypotheses, conducting cluster analysis, and selecting distinct sub-queries for refinement, until the initial query requirements are fully satisfied. ATC (Shi et al., 2024a) directly utilizes a chain of tools through programming and proposes a black-box probing method, enabling the LLM to function as a tool learner that can independently identify and record tool usages, thus teaching itself to effectively master new tools.

### 4.2.2 Tuning-based Methods

Though LLMs demonstrate impressive performance in zero-shot or few-shot settings, they remain less effective compared to models that have been fine-tuned (Erbacher et al., 2024). Toolformer (Schick et al., 2024) employs API calls that actually assist the model in predicting future tokens to fine-tune GPT-J, which enhances the awareness and capability of LLMs to utilize tools effectively. TaskMatrix.AI (Liang et al., 2024) leverages Reinforcement Learning from Human Feedback (RLHF) to utilize the knowledge and insights gained through human feedback, thereby enhancing the foundation model. Toolink (Qian et al., 2023b) innovates by decomposing the target task into a toolkit for problem-solving, then employing a model to utilize these tools to answer queries via a chain-of-solving (CoS) approach. TPTU-v2 (Kong et al., 2023) develops an LLM finetuner to fine-tune a base LLM using a meticulously curated dataset, so that the finetuned LLM can be more capable of task planning and API calls, especially for domainspecific tasks. $\alpha$-UMi (Shen et al., 2024a) presents a novel two-phase training paradigm where a foundational large language model is first extensively fine-tuned and then replicated as a planner for further fine-tuning on planning tasks. COA (Gao et al., 2024b) trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. DEER (Gui et al., 2024) stimulates decision-making awareness in LLMs across various scenarios by automatically generating tool usage examples with multiple decision branches, and enhances the generalization ability of LLMs towards unseen tools through proposing novel tool sampling strategies. SOAY (Wang et al., 2024e) first lets the LLM generate a feasible API calling plan, i.e. solution, based on complex user inputs, and then allows the LLM to generate executable API calling code based on the generated solution.

Remark. In summary, task planning, as the initial stage of tool learning, is crucial for solving the entire problem. Although there are many methods currently available to enhance the task planning capabilities of LLMs, generating a perfect plan directly when facing complex issues remains challenging. Furthermore, tool learning is a process involving interaction between LLMs and tools. How to better utilize feedback from tools to improve planning is still a question worthy of investigation.

### 4.3 Tool Selection

After the task planning phase, LLMs have already decomposed the user question into multiple subquestions. In order to better address these subquestions, it is necessary to select appropriate tools. The tool selection process involves choosing through a retriever or directly allowing LLMs to pick from a provided list of tools. When there are too many tools, a tool retriever is typically used to identify the top- $K$ relevant tools to offer to the LLMs, a process known as retriever-based tool selection. If the quantity of tools is limited or upon receiving the tools retrieved during the tool retrieval phase, the LLMs need to select the appropriate tools based on the tool descriptions and the sub-question, which is known as LLM-based tool selection. For example, an example for tool selection with GPT-4 is shown as follows:

## An Example for Tool Selection with GPT-4

Instruction Prompt: You are currently in the tool selection stage. You are given candidate tools that can be potentially used to solve the sub-question. Among candidate tools, select a list of relevant tools that would help solve the sub-question.

Sub-question 1: What is the current price of gold per ounce in USD?

Candidate Tools: 1.Metals Prices Rates API: The latest API endpoint will return real-time exchange rate data updated every 60 seconds. 2.Medium: Get official news from Medium. 3.Cryptocurrency Markets: Recently published cryptocurrencies videos.

Output: 1.Metals Prices Rates API: The latest API endpoint will return real-time exchange rate data updated every 60 seconds.

Sub-question 2: . .

Output: .

From this example, we can see that for the subquestion about obtaining the price of gold, GPT-4 can correctly select the necessary tools. Specifically, when faced with multiple candidate tools, GPT-4 can analyze the features of each tool and choose the one most suitable for answering the question. In this example, GPT-4 selects the Metals Prices Rates API because it provides real-time updated information on gold prices. This demonstrates accuracy and effectiveness of GPT-4 in tool selection. Next, we will introduce the latest de-
velopments in two categories: retriever-based tool selection and LLM-based tool selection.

### 4.3.1 Retriever-based Tool Selection

Real-world systems often incorporate a wide array of tools, making it impractical to input descriptions of all tools into LLMs due to length limitations and latency constraints. Therefore, to fully exploit the potential of tool-augmented LLMs, it is crucial to develop an efficient tool retrieval system. This system aims to bridge the gap between the broad capabilities of LLMs and the practical limitations of input size by efficiently selecting the top- $K$ most suitable tools for a given query from a vast tool set. State-of-the-art retrieval methods can be categorized into two types: term-based and semantic-based.

Term-based Methods. Term-based methods (i.e., sparse retrieval), such as TF-IDF (Sparck Jones, 1972) and BM25 (Robertson et al., 2009), represent both documents and queries as high-dimensional sparse vectors. These methods employ exact term matching to achieve efficient alignment between queries and documents. For example, Gorilla (Patil et al., 2023) employs BM25 and GPT-Index to construct a retriever for implementing tool retrieval.

Semantic-based Methods. Conversely, semanticbased methods (i.e., dense retrieval) utilize neural networks to learn the semantic relationship between queries and tool descriptions (Reimers and Gurevych, 2019; Xiong et al., 2021; Hofstätter et al., 2021; Gao and Callan, 2022; Izacard et al., 2021), and then calculate the semantic similarity using methods such as cosine similarity. Recently, there has been a burgeoning interest in the development and refinement of more efficient tool retrievers. Some studies (Kong et al., 2023; Qin et al., 2024; Gao et al., 2024a) train a SentenceBert model as the tool retriever, enabling the highefficiency retrieval of relevant tools. CRAFT (Yuan et al., 2024a) instructs LLMs to generate a fictitious tool description based on the given query, and then employs this fabricated tool to conduct a search. Anantha et al. (2023) propose ProTIP based on the concept of task decomposition. COLT (Qu et al., 2024) proposes a novel tool retrieval approach using Graph Neural Networks (GNNs), identifying that a critical dimension often overlooked in conventional tool retrieval methodologies is the necessity to ensure the completeness of the tools retrieved. In addition to the recall phase, Zheng et al.
(2024) also take into account the re-ranking stage of tool retrieval. They consider the differences between seen and unseen tools, as well as the hierarchical structure of the tool library. Building on these considerations, they propose an adaptive and hierarchy-aware Re-ranking method, ToolRerank. Meanwhile, we can also directly employ offthe-shelf embeddings (OpenAI, 2024; Team et al., 2024) to get the representations of user query and tool descriptions. In conclusion, constructing an efficient tool retriever is of paramount importance.

Remark. Although traditional information retrieval methods are suitable for tool retrieval scenarios, they still have issues such as focusing solely on semantic similarity and ignoring the hierarchical structure of the tools, etc. Future work should consider the unique needs and characteristics specific to tool retrieval scenarios in order to build a more effective tool retriever.

### 4.3.2 LLM-based Tool Selection

In instances where the quantity of tool libraries is limited or upon receiving the tools retrieved from the tool retrieval phase, it is feasible to incorporate the descriptions and parameter lists of these tools into the input context along with the user query provided to LLMs. Subsequently, LLMs are tasked with selecting the appropriate tools from the available tool list based on the user query. Given that the resolution of queries is occasionally sensitive to the order in which tools are invoked, there is a necessity for serial tool calling, where the output of one tool may serve as the input parameter for another. Consequently, this demands a high degree of reasoning capability from the LLMs. It must adeptly select the correct tools based on the information currently at its disposal and the information that needs to be acquired. Existing methods can be similarly categorized into tuning-free and tuning-based approaches.

Tuning-free Methods. Tuning-free methods capitalize on the in context learning ability of LLMs through strategic prompting (Song et al., 2023; Shen et al., 2024b). For instance, Wei et al. (2022) introduce the concept of chain of thought (COT), effectively incorporating the directive "let's think step by step" into the prompt structure. Further advancing this discourse, Yao et al. (2022b) propose ReACT, a framework that integrates reasoning with action, thus enabling LLMs to not only justify actions but also to refine their reasoning processes
based on feedback from environment (e.g., output of tools). This development marks a significant step forward in enhancing the adaptability and decisionmaking capabilities of LLMs by fostering a more dynamic interaction between reasoning and action. Building upon these insights, Qin et al. (2024) propose DFSDT method, which addresses the issue of error propagation by incorporating a depth-first search strategy to improve decision-making accuracy. ToolNet (Liu et al., 2024) organizes a multitude of tools into a directed graph to address the challenges LLMs face, allowing LLMs to start from an initial tool node and navigate through the graph, iteratively selecting the next tool until the task is resolved. GeckOpt (Fore et al., 2024) narrows down tool selection by adding intent-driven gating.

Tuning-based Methods. Tuning-based methods directly fine-tune the parameters of LLMs on the tool learning dataset to master tool usage. Toolbench (Xu et al., 2023) analyzes the challenges faced by open-source LLMs during the tool learning process, suggesting that fine-tuning, along with utilizing demonstration retrieval and system prompts, can significantly enhance the effectiveness of LLMs in tool learning. TRICE (Qiao et al., 2024) proposes a two-stage framework, which initially employs behavior cloning for instruct-tuning of the LLMs to imitate the behavior of tool usage, followed by further reinforcing the model through RLEF by utilizing the tool execution feedback. ToolLLaMA (Qin et al., 2024) employs the instruction-solution pairs derived from DFSDT method to fine-tune the LLaMA 7B model, significantly enhancing its tool usage capabilities. Confucius (Gao et al., 2024a) acknowledges the diversity in tool complexity and proposes a novel tool learning framework. ToolVerifier (Mekala et al., 2024) introduces a self-verification method which distinguishes between close candidates by self-asking contrastive questions during tool selection.

Remark. By comparing the aforementioned methods, we can find that the tuning-based method improves the capability of LLMs in tool selection by modifying model parameters. This approach can integrate extensive knowledge about tools, but it is only applicable to open-source LLMs and incurs substantial computational resource consumption. Conversely, the tuning-free method enhances the capability of LLMs in tool selection using precise prompting strategies or by modifying ex- isting mechanisms, and it is compatible with all LLMs. However, since the possibilities for designing prompts are limitless, finding the ideal way to create the perfect prompt is still a major challenge.

### 4.4 Tool Calling

In the tool calling stage, LLMs need to extract the required parameters from the user query in accordance with the specifications outlined in the tool description and request data from tool servers. This process mandates that the LLMs not only correctly extract the parameters' content and format but also adhere strictly to the prescribed output format to prevent the generation of superfluous sentences. For example, an example for tool calling with GPT4 is shown as follows:

## An Example for Tool Calling with GPT-4

Instruction Prompt: You are currently in the tool calling stage. You are given selected tools that can be potentially used to solve the sub-question. Your goal is to extract the required parameters needed to call the tool from the sub-question based on the tool descriptions. Output in the following format: \{parameter name: parameter, $\cdots$, parameter name: parameter\} Sub-question 1: What is the current price of gold per ounce in USD?

Selected Tools: Tool Name: \{Metals Prices Rates API\}. Tool description: \{The latest API endpoint will return real-time exchange rate data updated every 60 seconds.] Required params: [ [name: symbols, type: STRING, description: Enter a list of commaseparated currency codes or metal codes to limit output codes., name: base, type: STRING, description: Enter the three-letter currency code or metal code of your preferred base currency.] \}

Output: $\{$ symbols: "XAU", base: "USD"\}

Sub-question 2: $\cdots$

Output: $\cdots$

From this example, we can see that GPT-4 can extract the necessary parameters for calling a tool based on the provided user question and the selected tool's documentation. Specifically, GPT-4 can parse the critical information in the tool description and accurately identify which parameters need to be provided. Next, we will introduce the latest developments in the same way as the previous two stages, dividing them into tuning-free
methods and tuning-based methods.

### 4.4.1 Tuning-free Methods

Tuning-free methods predominantly leverage the few-shot approach to provide demonstrations for parameter extraction or rule-based methods, thereby enhancing the capability of LLMs to identify parameters (Hsieh et al., 2023; Song et al., 2023; Liu et al., 2023b, 2024). Reverse Chain (Zhang et al., 2023b) utilizes reverse thinking by first selecting a final tool for a task and then having the LLMs populate the necessary parameters; if any are missing, an additional tool is chosen based on the description to complete them and accomplish the task. EasyTool (Yuan et al., 2024b) enhances the comprehension of LLMs regarding tool functions and parameter requirements by prompting ChatGPT to rewrite tool descriptions, making them more concise and incorporating guidelines for tool functionality directly within the descriptions. ConAgents (Shi et al., 2024b) introduces a multi-agent collaborative framework, featuring a specialized execution agent tasked with parameter extraction and tool calling.

### 4.4.2 Tuning-based Methods

Some studies enhance the tool calling capabilities of LLMs using tuning-based methods (Patil et al., 2023; Qiao et al., 2024; Mekala et al., 2024). For example, GPT4Tools (Yang et al., 2024) enhances open-source LLMs by integrating tool usage capabilities through fine-tuning with LoRA optimization techniques, using a dataset of tool usage instructions generated by ChatGPT. Toolkengpt (Hao et al., 2024) uses special tokens called "toolkens" to seamlessly call tools, switching to a special mode upon predicting a toolken to generate required input parameters and integrate the output back into the generation process. Themis (Li et al., 2023a) enhances the interpretability and scoring reliability of RMs by integrating tool usage and reasoning processes in an auto-regressive manner, dynamically determining which tools to call, how to pass parameters, and effectively incorporating the results into the reasoning process. STE (Wang et al., 2024a) coordinates three key mechanisms in biological systems for the successful use of tools: trial and error, imagination, and memory, aiding LLMs in the accurate use of its trained tools.

Moreover, given the frequent occurrence of calling errors during the utilization of tools, such as incorrect formatting of input parameters, input pa- rameters exceeding acceptable ranges of the tool, and tool server errors, it is imperative to integrate error handling mechanisms. These mechanisms are designed to refine the action based on the error messages returned upon calling failure. This enables a more resilient and adaptive system, ensuring continuity and efficiency in tool learning even in the face of operational disruptions.

Remark. To sum up, although tuning-based methods can yield better results, they heavily rely on the tools seen in the training set and perform poorly with new tools not included in the dataset. Additionally, they face challenges such as catastrophic forgetting and a lack of robust generalization. In contrast, tuning-free methods offer significant flexibility and do not require the construction of a specific dataset. So both tuning-based methods and tuning-free methods are crucial for enhancing the tool calling capabilities of LLMs.

### 4.5 Response Generation

Given the diversity and complexity of tool outputs, which can include various formats like text, numbers, code, and images, it is often impractical to present these results directly to users. Upon receiving outputs from tools, LLMs must synthesize information relevant to user queries and integrate their own knowledge to construct comprehensive responses. For example, an example for response generation with GPT-4 is shown as follows:

## An Example for Response Generation with GPT-4

Instruction Prompt: You are currently in the response generation stage. You are given the output of selected tools that can be potentially used to solve the sub-task. Your goal is to generate a response to the user question based on the results from tools and your internal knowledge.

User Question: I would like to know the value of 5 ounces of gold plus 1 million AMZN stocks in CNY.

Return from Tool Calling: The stock price of AMZN is 130 USD, XAU USD is 1919.7605807522993, and the exchange rate between USD and YNC is 7.24.

Output: $\cdots$ Total value in $C N Y$ : Total value $=$ Value of gold in CNY + Value of stocks in $C N Y=69,495.87$ $+941,200,000=941,269,495.87$ CNY. Therefore, the combined value of 5 ounces of gold and 1 million Amazon stocks in Chinese Yuan is approximately $941,269,495.87$

From this example, we can see that GPT-4 can provide a better response by combining the results from tools and its internal knowledge. Specifically, GPT-4 uses real-time data provided by tools, such as the prices of gold and Amazon stock and the exchange rate, to calculate the final answer to the user's question. This demonstrates the ability of GPT-4 to integrate multiple information sources and perform complex calculations. We categorize the latest advancements in this stage into two types: direct insertion methods and information integration methods.

### 4.5.1 Direct Insertion Methods

The methods adopted in the early work involved directly inserting the output of tools into the generated response (Parisi et al., 2022; Schick et al., 2024; Hao et al., 2024; Wang et al., 2024f). For instance, if the user query is "How is the weather today?", LLMs produce a response like "It's Weather()" (as illustrated in Figure 3), which is subsequently replaced with the result returned by the tool (e.g., from "It's Weather()." to "It's rainy."). However, given the outputs of tools are unpredictable, this method could potentially affect the user experience.

### 4.5.2 Information Integration Methods

Most methodologies opt to incorporate the output of tools into the context as input to LLMs, thereby enabling the LLMs to craft a superior reply based on the information provided by the tool (Shen et al., 2024b; Wang et al., 2023a; Qian et al., 2023a). However, due to the limited context length of LLMs, some tool outputs cannot be directly fed into them. Consequently, various methods have emerged to address this issue. For example, RestGPT (Song et al., 2023) simplifies the lengthy results using the pre-created schema, which is a documentation that elaborates on the examples, format, and possible errors. ToolLLaMA (Qin et al., 2024) resorts to truncation, cutting the output to fit within the length constraints, which potentially loses the required information to solve the user query. Conversely, ReCOMP (Xu et al., 2024a) develops a compressor to condense lengthy information into a more succinct format, which keeps only the most useful information. ConAgents (Shi et al., 2024b) proposes a schema-free method, enabling the observing agent to dynamically generate a function adaptive to extracting the target output following the instruction. And some studies suggest that re- fining the response generated by LLMs using the tool feedback is more effective than generating the response after invoking the tool (Jacovi et al., 2023; Nathani et al., 2023; Gou et al., 2024a).

Remark. In conclusion, direct insertion methods embed tool outputs directly into the generated response. These approaches are straightforward but are only suitable for simple tool outputs. Conversely, information integration methods allow LLMs to process tool results to generate responses. These methods are more powerful and can provide better responses, enhancing user experience. However, future work should consider how to address issues related to overly lengthy tool outputs and the inclusion of multiple other modalities.

## 5 Benchmarks and Evaluation

In this section, we systematically summarize and categorize the benchmarks and evaluation methods that are tailored specifically to the various stages of tool learning. This provides a structured overview of the evaluation protocols used to validate the effectiveness and efficiency of tool learning methods.

### 5.1 Benchmarks

With the advancement of research in tool learning, a considerable number of benchmarks have been developed and made available. In our survey, we compile a selection of 26 popular benchmarks ${ }^{1}$, as shown in Table 1. Each benchmark evaluates distinct facets of tool learning, offering significant contributions to their respective fields. We categorize these benchmarks into two principal classes: general benchmarks and other benchmarks.

General Benchmarks. Given the current uncertainty regarding the capacity of LLMs to effectively utilize tools, a large number of benchmarks have been established to evaluate the tool learning proficiency of LLMs. As tool learning comprises four distinct stages, existing benchmarks focus on evaluating the capabilities of LLMs at different stages. For instance, MetaTool (Huang et al., 2024c) benchmark is designed to assess whether LLMs can recognize the necessity of using tools and appropriately select the most suitable tool to meet user needs. This assessment particularly focuses on the stages of task planning and tool selection. On[^0]

| Benchmark | Focus | \# Tools | \# Instances | Tool Source | Multi-tool? | Executable? | Time |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General Benchmarks |  |  |  |  |  |  |  |
| API-Bank (Li et al., 2023b) | (1), (2), (3), (4) | 73 | 314 | Manual Creation | $\checkmark$ | $\checkmark$ | 2023-04 |
| APIBench (Patil et al., 2023) | (2), (3) | 1,645 | 16,450 | Public Models | $x$ | $x$ | 2023-05 |
| ToolBench1 (Xu et al., 2023) | (2), (3) | 232 | 2,746 | Public APIs | $x$ | $\checkmark$ | 2023-05 |
| ToolAlpaca (Tang et al., 2023) | (2), (3), (4) | 426 | 3,938 | Public APIs | $x$ | $x$ | 2023-06 |
| RestBench (Song et al., 2023) | (1), (2), (3) | 94 | 157 | RESTful APIs | $\checkmark$ | $x$ | 2023-06 |
| ToolBench2 (Qin et al., 2024) | (1), (2), (3) | 16,464 | 126,486 | Rapid API | $\checkmark$ | $\checkmark$ | 2023-07 |
| MetaTool (Huang et al., 2024c) | (1), (2) | 199 | 21,127 | OpenAI Plugins | $\checkmark$ | $x$ | 2023-10 |
| TaskBench (Shen et al., 2023) | (1), (2), (3) | 103 | 28,271 | Public APIs | $\checkmark$ | $\checkmark$ | 2023-11 |
| T-Eval (Chen et al., 2023b) | (1), (2), (3) | 15 | 533 | Manual Creation | $\checkmark$ | $\checkmark$ | 2023-12 |
| ToolEyes (Ye et al., 2024a) | (1), (2), (3), (4) | 568 | 382 | Manual Creation | $\checkmark$ | $\checkmark$ | 2024-01 |
| UltraTool (Huang et al., 2024a) | (1), (2), (3) | 2,032 | 5,824 | Manual Creation | $\checkmark$ | $x$ | 2024-01 |
| API-BLEND (Basu et al., 2024) | (2), (3) | - | 189,040 | Exsiting Datasets | $\checkmark$ | $\checkmark$ | 2024-02 |
| Seal-Tools (Wu et al., 2024c) | (2), (3) | 4,076 | 14,076 | Manual Creation | $\checkmark$ | $x$ | 2024-05 |
| Other Benchmarks |  |  |  |  |  |  |  |
| ToolQA (Zhuang et al., 2024b) | $\mathrm{QA}$ | 13 | 1,530 | Manual Creation | $x$ | $\checkmark$ | 2023-06 |
| ToolEmu (Ruan et al., 2023b) | Safety | 311 | 144 | Manual Creation | $x$ | $\checkmark$ | 2023-09 |
| ToolTalk (Farn and Shin, 2023) | Conversation | 28 | 78 | Manual Creation | $x$ | $\checkmark$ | 2023-11 |
| VIoT (Zhong et al., 2023) | VIoT | 11 | 1,841 | Public Models | $x$ | $\checkmark$ | 2023-12 |
| RoTBench (Ye et al., 2024c) | Robustness | 568 | 105 | ToolEyes | $\checkmark$ | $\checkmark$ | 2024-01 |
| MLLM-Tool (Wang et al., 2024b) | Multi-modal | 932 | 11,642 | Public Models | $\checkmark$ | $\checkmark$ | 2024-01 |
| ToolSword (Ye et al., 2024b) | Safety | 100 | 440 | Manual Creation | $\checkmark$ | $\checkmark$ | 2024-02 |
| SciToolBench (Ma et al., 2024a) | Sci-Reasoning | 2,446 | 856 | Manual Creation | $\checkmark$ | $\checkmark$ | $2024-02$ |
| InjecAgent (Zhan et al., 2024) | Safety | 17 | 1,054 | Public APIs | $x$ | $\checkmark$ | $2024-02$ |
| StableToolBench (Guo et al., 2024) | Stable | 16,464 | 126,486 | ToolBench2 | $\checkmark$ | $\checkmark$ | 2024-03 |
| m\&m's (Ma et al., 2024b) | Multi-modal | 33 | 4,427 | Public Models | $\checkmark$ | $\checkmark$ | 2024-03 |
| GeoLLM-QA (Singh et al., 2024) | Remote Sensing | 117 | 1,000 | Public Models | $\checkmark$ | $\checkmark$ | 2024-04 |
| ToolLens (Qu et al., 2024) | Tool Retrieval | 464 | 18,770 | ToolBench2 | $\checkmark$ | $\checkmark$ | 2024-05 |

Table 1: A detailed list of different benchmarks and their specific configurations. Symbols (1), (2), (3), and (4) represent the four stages in tool learning-task planning, tool selection, tool calling, and response generation, respectively.

the other hand, APIBench (Patil et al., 2023), ToolBench1 (Xu et al., 2023), API-BLEND (Basu et al., 2024), and Seal-Tools (Wu et al., 2024c) concentrate on the abilities of LLMs to accurately choose the right tool and configure the correct parameters for its invocation, which correspond to the tool selection and tool calling stages respectively. Additionally, RestBench (Song et al., 2023), ToolBench2 (Qin et al., 2024), TaskBench (Shen et al., 2023), T-Eval (Chen et al., 2023b), and UltraTool (Huang et al., 2024a) extend their focus to include task planning, tool selection, and tool calling, covering three of the four stages. Notably, ToolBench2 has constructed the existing largest tool learning dataset, comprising 16,464 tools and 126,486 instances. Subsequent studies such as APIBank (Li et al., 2023b) and ToolEyes (Ye et al., 2024a) have provided a more comprehensive evaluation of the tool usage capabilities of LLMs, spanning all four stages of tool learning.

Other Benchmarks. In addition to general benchmarks, there are also benchmarks specifically designed for particular tasks. For example, ToolQA (Zhuang et al., 2024b) focuses on enhancing the question-answering capabilities of LLMs through the use of external tools, which has developed a dataset comprising questions that LLMs can only answer with the assistance of these external tools. ToolTalk (Farn and Shin, 2023) concentrates on the ability of LLMs to utilize tools within multiturn dialogues. VIoT (Zhong et al., 2023) focuses on the capability of using Viot tools with LLMs. RoTBench (Ye et al., 2024c), ToolSword (Ye et al., 2024b), and ToolEmu (Ruan et al., 2023b) are
benchmarks that emphasize the robustness and safety issues in tool learning. These benchmarks highlight the critical necessity for enhancing the robustness and safety of LLMs in tool learning applications. MLLM-Tool (Wang et al., 2024b) and m\&m's (Ma et al., 2024b) extend tool learning into the multi-modal domain, assessing tool usage capabilities of LLMs in multi-modal contexts. Meanwhile, StableToolBench (Guo et al., 2024) advocates for the creation of a large-scale and stable benchmark for tool learning. SCITOOLBENCH (Ma et al., 2024a) introduces a novel task named tool-augmented scientific reasoning, expanding the frontier of tool learning with LLMs applications. GeoLLM-QA (Singh et al., 2024) is designed to capture complex remote sensing workflows where LLMs handle complex data structures, nuanced reasoning, and interactions with dynamic user interfaces. Finally, ToolLens (Qu et al., 2024), acknowledging that user queries in the real world are often concise yet have ambiguous and complex intent, has created a benchmark focused on the tool retrieval stage.

### 5.2 Evaluation

In this section, we will introduce the evaluation methods corresponding to the four stages of tool learning.

Task Planning. The task planning capabilities of LLMs can be evaluated in several ways. Firstly, it is crucial to assess whether LLMs correctly identify if a given query requires a external tool, measuring the accuracy of tool usage awareness (Huang et al., 2024c, a). Next, the effectiveness of the proposed task planning in addressing the query should be evaluated, using metrics like the pass rate provided by ChatGPT (Qin et al., 2024) or human evaluations (Song et al., 2023). Furthermore, the precision of the plan generated by LLMs can be quantitatively analyzed by comparing it to the gold solution, ensuring its alignment and accuracy (Song et al., 2023; Chen et al., 2023b; Qin et al., 2024).

Tool Selection. Existing works employ several metrics to evaluate the effectiveness of tool selection from different perspectives, including Recall, NDCG, and COMP.

Recall@K (Zhu, 2004) is measured by calculating the proportion of selected top- $K$ tools that are present in the set of ground-truth tools:

$$
\text { Recall@ } K=\frac{1}{|\mathcal{Q}|} \sum_{q=1}^{|\mathcal{Q}|} \frac{\left|T_{q}^{K} \cap T_{q}^{*}\right|}{\left|T_{q}^{*}\right|}
$$

where $\mathcal{Q}$ is the set of queries, $T_{q}^{*}$ is the set of relevant tools for the query $q$, and $T_{q}^{K}$ is the top- $K$ tools for the query $q$ selected by the model.

NDCG@K (Järvelin and Kekäläinen, 2002) metric not only considers the proportion of positive tools but also takes into account their positions within the list:

$$
\begin{gathered}
\mathrm{DCG}_{q} @ K=\sum_{i=1}^{K} \frac{2^{g_{i}}-1}{\log _{2}(i+1)} \\
\mathrm{NDCG} @ K=\frac{1}{|\mathcal{Q}|} \sum_{q=1}^{|\mathcal{Q}|} \frac{\mathrm{DCG}_{q} @ K}{\operatorname{IDCG}_{q} @ K}
\end{gathered}
$$

where $g_{i}$ is the graded relevance sore for the $i$ th selected tool, and $\mathrm{IDCG}_{q} @ K$ denote ideal discounted cumulative gain at the rank position $k$.

COMP@K (Qu et al., 2024) is designed to measure whether the top- $K$ selected tools form a complete set with respect to the ground-truth set:

$$
\operatorname{COMP} @ K=\frac{1}{|\mathcal{Q}|} \sum_{q=1}^{|\mathcal{Q}|} \mathbb{I}\left(\Phi_{q} \subseteq \Psi_{q}^{K}\right)
$$

where $\Phi_{q}$ denotes the set of ground-truth tools for query $q, \Psi_{q}^{K}$ represents the top- $K$ tools retrieved for query $q$, and $\mathbb{I}(\cdot)$ is an indicator function that returns 1 if the retrieval results include all groundtruth tools within the top- $K$ results for query $q$, and 0 otherwise.

Tool Calling. In the stage of tool calling, LLMs are required to generate requests for tool calling in a specified format. The effectiveness of LLMs in executing tool calling functions can be assessed by evaluating whether the parameters input by LLMs are consistent with the stipulations delineated in the tool documentation (Chen et al., 2023b; Ye et al., 2024a; Huang et al., 2024a). This assessment entails verifying whether the parameters provided match those required by the specific tool, including confirming if all required parameters are included and whether the output parameters meet the required range and format.

Response Generation. The ultimate goal of tool learning is to enhance the capability of LLMs to effectively address downstream tasks. Consequently,
the effectiveness of tool utilization is often evaluated based on the performance in solving these downstream tasks (Tang et al., 2023; Ye et al., 2024a). This necessitates that the LLMs consolidate information gathered throughout the entire process, providing a direct response to the user query. The quality of the final response can be assessed using metrics such as ROUGE-L (Lin, 2004), exact match (Blackwell et al., 2009), F1 (Basu et al., 2024), and other relevant indicators.

## 6 Challenges and Future Directions

In this section, we will identify current challenges in tool learning with LLMs and propose some promising directions for future research.

### 6.1 High Latency in Tool Learning

In the reasoning process, LLMs often struggle with high latency and low throughput (Miao et al., 2023), challenges that become more pronounced when integrating tool learning. For example, even simple queries using ChatGPT with plugins can take 5 seconds to resolve, significantly diminishing the user experience compared to faster search engines. It is essential to explore ways to reduce latency, such as enhancing the awareness of LLMs in tool utilization, enabling them to better assess when the use of tools is genuinely necessary. Additionally, maintaining the simplicity and responsiveness of tools is crucial. Overloading a single tool with too many features should be avoided to maintain efficiency and effectiveness.

### 6.2 Rigorous and Comprehensive Evaluation

Although current research demonstrates considerable advancements in tool learning with LLMs, evidenced by empirical studies across various applications, there remains a notable gap in establishing solid quantitative metrics to evaluate and understand how effectively LLMs utilize tools. Additionally, while numerous strategies have been suggested to enhance the tool learning capabilities of LLMs, a thorough comparative evaluation of these approaches is still missing. For instance, while human evaluation is capable of accurately reflecting human preferences, it is associated with significant costs and exhibits issues with repeatability, lacking in universal applicability. While the automated evaluation method, ToolEval (Qin et al., 2024), has enhanced the efficiency and reproducibility of assessments, it does not necessarily reflect the genuine preference of users. There is a need for a rigorous and comprehensive evaluation framework that considers efficiency, precision, cost, and practicality holistically. Specifically, this framework should provide independent assessments and attribution analysis for improvements at different stages, clearly delineating their specific contributions to the final response. This could involve defining new evaluation metrics and constructing assessment environments that simulate the complexity of the real world.

### 6.3 Comprehensive and Accessible Tools

While existing efforts have predominantly focused on leveraging tools to enhance the capabilities of LLMs, the quality of these tools critically impacts the performance of tool learning (Wang et al., 2024f). The majority of current tools are aggregated from existing datasets or public APIs, which imposes limitations on their accessibility and comprehensiveness. Moreover, existing datasets only contains a limited set of tools, which is unable to cover a diverse range of user queries (Lyu et al., 2023). Such constraints curtail the practical applicability and depth of tool learning. Additionally, current work acquires tools from different sources such as Public APIs (Tang et al., 2023), RESTful APIs (Song et al., 2023), Rapid APIs (Qin et al., 2024), Hugging Face (Patil et al., 2023; Shen et al., 2024b, 2023) or the OpenAI plugin list (Huang et al., 2024c). The diverse origins of these tools result in a variance in the format of descriptions, which hampers the development of a unified framework for tool learning. There is a pressing need to develop and compile a more comprehensive and easily accessible tool set. Given the substantial overhead associated with manually creating tools, a viable approach is to employ LLMs for the mass automatic construction of tool set (Cai et al., 2024; Wang et al., 2024g). Furthermore, the tool set should encompass a wider range of fields, offering a diverse array of functionalities to meet the specific needs of various domains. We posit that a comprehensive and accessible tool set will significantly accelerate the advancement of tool learning.

### 6.4 Safe and Robust Tool Learning

Current research predominantly emphasizes the capabilities of LLMs in utilizing tools within wellstructured environments, yet it overlooks the inevitable presence of noise and emerging safety considerations relevant to real-world applications.

Upon deploying tool learning with LLMs into practical scenarios, safety issues and noise become unavoidable, necessitating a thoughtful approach to defend against these potential attacks. Ye et al. (2024c) introduce five levels of noise (Clean, Slight, Medium, Heavy, and Union) to evaluate the robustness of LLMs in tool learning. Their findings indicate a significant degradation in performance, revealing that even the most advanced model, GPT-4, exhibits poor resistance to interference. Furthermore, Ye et al. (2024b) devise six safety scenarios to evaluate the safety of LLMs in tool learning, uncovering a pronounced deficiency in safety awareness among LLMs, rendering them incapable of identifying safety issues within tool learning contexts. With the extensive deployment of tool learning systems across various industries, the imperative for robust security measures has significantly intensified. This necessitates a profound investigation and the introduction of innovative methodologies to fortify the security and resilience of these systems. Concurrently, in anticipation of emergent attack vectors, it is essential to perpetually refine and update security strategies to align with the rapidly evolving technological landscape.

### 6.5 Unified Tool Learning Framework

As discussed in $\S 4$, the process of tool learning can be categorized into four distinct stages. However, prevailing research predominantly concentrates on only one of these stages for specific problems, leading to a fragmented approach and a lack of standardization. This poses significant challenges to scalability and generality in practical scenarios. It is imperative to explore and develop a comprehensive solution that encompasses task planning, tool selection, tool invocation, and response generation within a singular, unified tool learning framework.

### 6.6 Real-Word Benchmark for Tool Learning

Despite the substantial volume of work already conducted in the field of tool learning, the majority of queries in existing benchmarks are generated by LLMs rather than originating from real-world user queries. These synthesized queries may not accurately reflect genuine human interests and the manner in which users conduct searches. To date, there has been no publication of a tool learning dataset that encompasses authentic interactions between users and tool-augmented LLMs. The release of such a dataset, along with the establishment of a corresponding benchmark, is believed to significantly advance the development of tool learning.

### 6.7 Tool Learning with Multi-Modal

While numerous studies have focused on bridging the LLMs with external tools to broaden the application scenarios, the majority of existing work on LLMs in tool learning has been confined to textbased queries. This limitation potentially leads to ambiguous interpretations of the true user intent. LLMs are poised to enhance understanding of user intent through the integration of visual and auditory information. The increasing use of multimodal data, such as images, audio, 3D, and video, opens up significant opportunities for further development. This encompasses exploring the capabilities of multi-modal LLMs in tool use and the combination of multi-modal tools to generate superior responses. Several pioneering research projects have explored this area. For example, Wang et al. (2024b) propose the MLLM-Tool, a system that incorporates open-source LLMs and multi-modal encoders, enabling the learned LLMs to be aware of multi-modal input instructions and subsequently select the correctly matched tool. Despite these initial efforts, the exploration of tool learning with multi-modal inputs has not been extensively studied. A comprehensive understanding of the capabilities of multi-modal LLMs in tool use is crucial for advancing the field.

## 7 Conclusion

In this paper, with reviewing about 100 papers, we present a comprehensive survey of tool learning with LLMs. We begin the survey with a brief introduction to the concepts of 'tool' and 'tool learning,' providing beginners with a foundational overview and essential background knowledge. Then we elucidate the benefits of tool integration and tool learning paradigm, detailing six specific aspects to underscore why tool learning is crucial for LLMs. Moreover, to provide a more detailed introduction to how to conduct tool learning, we break down the tool learning process into four distinct phases: task planning, tool selection, tool calling, and response generation. Each phase is discussed in depth, integrating the latest research advancements to provide a thorough understanding of each step. Additionally, we summarize and categorize existing benchmarks and evaluation methods specific to these stages of tool learning, offering a structured
overview of evaluation protocols. Finally, we highlight some potential challenges and identify future directions for research within this evolving field. We hope this survey can provide a comprehensive and invaluable resource for researchers and developers keen on navigating the burgeoning domain of tool learning with LLMs, thereby paving the way for future research endeavors.

## Acknowledgements

The authors would like to extend their sincere gratitude to Yankai Lin for his constructive feedback throughout the development of this work. His insights and detailed suggestions significantly enhanced the quality and clarity of our research.

## References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report arXiv preprint arXiv:2303.08774.

Raviteja Anantha, Bortik Bandyopadhyay, Anirudh Kashi, Sayantan Mahinder, Andrew W Hill, and Srinivas Chappidi. 2023. Protip: Progressive tool retrieval improves planning. arXiv preprint arXiv:2312.10332.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021 Program synthesis with large language models. arXiv preprint arXiv:2108.07732.

Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, and Luis A Lastras. 2024. Api-blend: A comprehensive corpora for training and benchmarking api llms. arXiv preprint arXiv:2402.15491.

Matthew Blackwell, Stefano Iacus, Gary King, and Giuseppe Porro. 2009. cem: Coarsened exact matching in stata. The Stata Journal, 9(4):524-546.

Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2024. Large language models as tool makers. In Proceedings of the 12th International Conference on Learning Representations (ICLR).

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.

Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, and Rui Yan. 2023a. Fortify the shortest stave in attention: Enhancing context awareness of large language models for effective tool use. arXiv preprint arXiv:2312.04455.

Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, et al. 2023b. T-eval: Evaluating the tool utilization capability step by step. arXiv preprint arXiv:2312.14033.

I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. Factool: Factuality detection in generative ai-a tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

Xuan-Quy Dao and Ngoc-Bich Le. 2023. Investigating the effectiveness of chatgpt in mathematical reasoning and problem solving: Evidence from the vietnamese national high school graduation examination. arXiv preprint arXiv:2306.06331.

Debrup Das, Debopriyo Banerjee, Somak Aditya, and Ashish Kulkarni. 2024. Mathsensei: A toolaugmented large language model for mathematical reasoning. arXiv preprint arXiv:2402.17231.

Yu Du, Fangyun Wei, and Hongyang Zhang. 2024. Anytool: Self-reflective, hierarchical agents for largescale api calls. arXiv preprint arXiv:2402.04253.

Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed. 2021. Automatic text summarization: A comprehensive survey. Expert systems with applications, 165:113679.

Pierre Erbacher, Louis Falissar, Vincent Guigue, and Laure Soulier. 2024. Navigating uncertainty: Optimizing api dependency for hallucination reduction in closed-book question answering. arXiv preprint arXiv:2401.01780.

Nicholas Farn and Richard Shin. 2023. Tooltalk: Evaluating tool-usage in a conversational setting. arXiv preprint arXiv:2311.10775.

Zhaopeng Feng, Yan Zhang, Hao Li, Wenqiang Liu, Jun Lang, Yang Feng, Jian Wu, and Zuozhu Liu. 2024. Improving llm-based machine translation with systematic self-correction. arXiv preprint arXiv:2402.16379.

Michael Fore, Simranjit Singh, and Dimitrios Stamoulis. 2024. Geckopt: Llm system efficiency via intent-based tool selection. arXiv preprint arXiv:2404.15804.

Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. 2023a. Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640.

Luyu Gao and Jamie Callan. 2022. Unsupervised corpus aware language model pre-training for dense passage retrieval. In Proceedings of the 60st Annual Meeting of the Association for Computational Linguistics (ACL).

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023b. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764-10799. PMLR.

Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie Ren, Zhumin Chen, Jun Ma, and Zhaochun Ren. 2024a. Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum. In In Proceedings of 38th Conference on Artificial Intelligence (AAAI).

Silin Gao, Jane Dwivedi-Yu, Ping Yu, Xiaoqing Ellen Tan, Ramakanth Pasunuru, Olga Golovneva, Koustuv Sinha, Asli Celikyilmaz, Antoine Bosselut, and Tianlu Wang. 2024b. Efficient tool use with chain-of-abstraction reasoning. arXiv preprint arXiv:2401.17464.

Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023c. Enabling large language models to generate text with citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6465-6488, Singapore. Association for Computational Linguistics.

Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023d. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997.

Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing Li. 2024c. Clova: A closed-loop visual assistant with tool usage and update. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang, et al. 2024. Openagi: When llm meets domain experts. Advances in Neural Information Processing Systems, 36.

Kathleen R Gibson, Kathleen Rita Gibson, and Tim Ingold. 1993. Tools, language and cognition in human evolution. Cambridge University Press.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024a. Critic: Large language models can self-correct with tool-interactive critiquing. In Proceedings of the 12th International Conference on Learning Representations (ICLR).

Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. 2024b. Tora: A tool-integrated reasoning agent for mathematical problem solving. In Proceedings of the 12th International Conference on Learning Representations (ICLR).

Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, and Yu Su. 2024. Middleware for llms: Tools are instrumental for language agents in complex environments. arXiv preprint arXiv:2402.14672.

Anchun Gui, Jian Li, Yong Dai, Nan Du, and Han Xiao. 2024. Look before you leap: Towards decision-aware and generalizable tool-usage for large language models. arXiv preprint arXiv:2402.16696.

Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714.

Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2024. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Advances in neural information processing systems, 36

Joy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. 2023. Solving math word problems by combining language models with symbolic solvers. In Proceedings of the 2023 Annual Conference on Neural Information Processing Systems (NeurIPS).

Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 113-122.

Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. 2023. Tool documentation enables zero-shot tool-usage with large language models. arXiv preprint arXiv:2308.00675.

Tenghao Huang, Dongwon Jung, and Muhao Chen. 2024a. Planning and editing what you retrieve for enhanced tool learning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).

Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR.

Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024b. Understanding the planning of llm agents: A survey. arXiv preprint arXiv:2402.02716.

Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, et al. 2024c. Metatool benchmark for large language models: Deciding whether to use tools and which to use. In Proceedings of 12th International Conference on Learning Representations (ICLR).

Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao Kurohashi. 2023. MultiTool-CoT: GPT-3 can use multiple external tools with chain of thought prompting. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1522-1532, Toronto, Canada. Association for Computational Linguistics.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118.

Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd Bohnet, and Mor Geva. 2023. A comprehensive evaluation of tool-assisted generation strategies. Findings of the Association for Computational Linguistics: EMNLP 2023.

Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems (TOIS), $20(4): 422-446$.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.

Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In Proceedings of the AAAI conference on artificial intelligence, pages 8018-8025.

Qiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu, Donald Wright, Thomas Huang, W John Wilbur, Zhe He, Andrew Taylor, Qingyu Chen, et al. 2024a. Agentmd: Empowering language agents for risk prediction with large-scale clinical tool learning. arXiv preprint arXiv:2402.13225.
Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. 2024b. Genegpt: Augmenting large language models with domain tools for improved access to biomedical information. Bioinformatics, 40(2):btae075.

Marek Kadlčík, Michal Štefánik, Ondrej Sotolar, and Vlastimil Martinek. 2023. Calc-x and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems. In The 2023 Conference on Empirical Methods in Natural Language Processing.

Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. 2022. Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. arXiv preprint arXiv:2205.00445.

Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. Internet-augmented dialogue generation. In Proceedings of the 60st Annual Meeting of the Association for Computational Linguistics (ACL).

Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. 2023. Tptu-v2: Boosting task planning and tool usage of large language model-based agents in realworld systems.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466.

Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022a. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115.

Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022b. Internetaugmented language models through few-shot prompting for open-domain question answering.

Chuanhao Li, Runhan Yang, Tiankai Li, Milad Bafarassat, Kourosh Sharifi, Dirk Bergemann, and Zhuoran Yang. 2024. Stride: A tool-assisted llm agent framework for strategic and interactive decision-making. arXiv preprint arXiv:2405.16376.

Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, and Hua Wu. 2023a. Tool-augmented reward modeling. arXiv preprint arXiv:2310.01045.

Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023b. Api-bank: A comprehensive benchmark for tool-augmented llms. In The 2023 Conference on Empirical Methods in Natural Language Processing.

Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. 2024. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. Advances in Neural Information Processing Systems.

Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.

Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. 2020. Explainable ai: A review of machine learning interpretability methods. Entropy, 23(1):18.

Xukun Liu, Zhiyuan Peng, Xiaoyuan Yi, Xing Xie, Lirong Xiang, Yuchen Liu, and Dongkuan Xu. 2024. Toolnet: Connecting large language models with massive tools via tool graph. arXiv preprint arXiv:2403.00839.

Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, et al. 2023a. Internchat: Solving vision-centric tasks by interacting with chatbots beyond language. arXiv preprint arXiv:2305.05662.

Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, et al. 2023b. Controlllm: Augment language models with tools by searching on graphs. arXiv preprint arXiv:2310.17796.

Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2024. Chameleon: Plug-and-play compositional reasoning with large language models. Advances in Neural Information Processing Systems, 36 .

Yining Lu, Haoping Yu, and Daniel Khashabi. 2023. Gear: Augmenting language models with generalizable and efficient tool resolution. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL).

Bohan Lyu, Xin Cong, Heyang Yu, Pan Yang, Yujia Qin, Yining Ye, Yaxi Lu, Zhong Zhang, Yukun Yan, Yankai Lin, et al. 2023. Gitagent: Facilitating autonomous agent with github by tool extension. arXiv preprint arXiv:2312.17294.

Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. 2024. Augmenting large language models with chemistry tools. Nature Machine Intelligence, pages 1-11.

Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, and Aixin Sun. 2024a. Sciagent: Toolaugmented language models for scientific reasoning. arXiv preprint arXiv:2402.11451.
Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, and Ranjay Krishna. 2024b. m\&m's: A benchmark to evaluate tool-use for multi-step multi-modal tasks. arXiv preprint arXiv:2403.11085.

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511.

Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang, and Jane Dwivedi-Yu. 2024. Toolverifier: Generalization to new tools via self-verification. arXiv preprint arXiv:2402.14158.

Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented language models: a survey. arXiv preprint arXiv:2302.07842.

Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, and Zhihao Jia. 2023. Towards efficient generative large language model serving: A survey from algorithms to systems. arXiv preprint arXiv:2312.15234.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.

Deepak Nathani, David Wang, Liangming Pan, and William Yang Wang. 2023. Maf: Multi-aspect feedback for improving reasoning in large language models. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.

OpenAI. 2024. New embeddings models and api updates.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744.

Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023. Fact-checking complex claims with program-guided reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6981-7004, Toronto, Canada. Association for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia,

Pennsylvania, USA. Association for Computational Linguistics.

Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. 2023. Art: Automatic multistep reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014.

Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255.

Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334.

Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023a. Creator: Tool creation for disentangling abstract and concrete reasoning of large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6922-6939.

Cheng Qian, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2023b. Toolink: Linking toolkit creation and using through chain-of-solving on opensource model. arXiv preprint arXiv:2310.05155.

Shuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu Zhang. 2024. Making language models better tool learners with execution feedback. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).

Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022. Reasoning with language model prompting: A survey. arXiv preprint arXiv:2212.09597.

Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023. Tool learning with foundation models. arXiv preprint arXiv:2304.08354.

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2024. Toolllm: Facilitating large language models to master $16000+$ real-world apis. In Proceedings of the 12th International Conference on Learning Representations (ICLR).

Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. 2024. Colt: Towards completeness-oriented tool retrieval for large language models. arXiv preprint arXiv:2405.16089.

Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: $\mathrm{Bm} 25$ and beyond. Foundations and Trends $\circledR^{\circledR}$ in Information Retrieval, 3(4):333-389.

Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. 2023a. Tptu: Task planning and tool usage of large language modelbased ai agents. arXiv preprint arXiv:2308.03427.

Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, and Tatsunori Hashimoto. 2023b. Identifying the risks of $\mathrm{lm}$ agents with an $1 \mathrm{~m}-$ emulated sandbox. arXiv preprint arXiv:2309.15817.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36.

Zhihong Shao, Fei Huang, and Minlie Huang. 2022. Chaining simultaneous thoughts for numerical reasoning. arXiv preprint arXiv:2211.16482.

Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. 2024a. Small llms are weak tool learners: A multi-llm agent. arXiv preprint arXiv:2401.07324.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2024b. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36.

Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, and Yueting Zhuang. 2023. Taskbench: Benchmarking large language models for task automation. arXiv preprint arXiv:2311.18760.

Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrievalaugmented black-box language models. arXiv preprint arXiv:2301.12652.

Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Suzan Verberne, and Zhaochun Ren. 2024a. Chain of tools: Large language model is an automatic multitool learner. arXiv preprint arXiv:2405.16533

Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, and Zhaochun Ren. 2024b. Learning to use tools via cooperative and interactive agents. arXiv preprint arXiv:2403.03031.

Robert W Shumaker, Kristina R Walkup, and Benjamin B Beck. 2011. Animal tool behavior: the use and manufacture of tools by animals. JHU Press.

Simranjit Singh, Michael Fore, and Dimitrios Stamoulis 2024. Evaluating tool-augmented agents in remote sensing platforms. arXiv preprint arXiv:2405.00709.

Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. 2023. Restgpt: Connecting large language models with realworld applications via restful apis. arXiv preprint arXiv:2306.06624.

Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1):11-21.

Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths. 2024. Cognitive architectures for language agents. Transactions on Machine Learning Research.

Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xiaochi Wei, Shuaiqiang Wang, Yan Zhang, and Dawei Yin. 2023a. Towards verifiable text generation with evolving memory and self-reflection. arXiv preprint arXiv:2312.09075.

Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. 2023b. A survey of reasoning with foundation models. arXiv preprint arXiv:2312.11562.

Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1188811898 .

Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301.

Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, et al. 2024. Gemini: A family of highly capable multimodal models.

Adrian Theuma and Ehsan Shareghi. 2024. Equipping language models with tool use capability for tabular data analysis in finance. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL).

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.

Vishruth Veerendranath, Vishwa Shah, and Kshitish Ghate. 2024. Calc-cmu at semeval-2024 task 7: Precalc - learning to use the calculator improves numeracy in language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).
Barbara Von Eckardt. 1995. What is cognitive science? MIT press.

Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. 2023. Freshllms: Refreshing large language models with search engine augmentation. arXiv preprint arXiv:2310.03214.

Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial triggers for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2153-2162, Hong Kong, China. Association for Computational Linguistics.

Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su. 2024a. Llms in the imaginarium: Tool learning through simulated trial and error. arXiv preprint arXiv:2403.04746.

Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Zhengxin Li, Lin Ma, Shenghua Gao, et al. 2024b. Tool-lmm: A large multi-modal model for tool agent learning. arXiv preprint arXiv:2401.10727.

Hongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui Wang, Boyang Xue, Hongyuan Lu, Fei Mi, and Kam-Fai Wong. 2023a. Tpe: Towards better compositional reasoning over conceptual tools with multi-persona collaboration. arXiv preprint arXiv:2309.16090.

Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024c. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):1-26.

Xingyao Wang, Hao Peng, Reyhaneh Jabbarvand, and Heng Ji. 2023b. Leti: Learning to generate from textual interactions. arXiv preprint arXiv:2305.10314.

Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2024d. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. In Proceedings of 12th International Conference on Learning Representations (ICLR).

Yuanchun Wang, Jifan Yu, Zijun Yao, Jing Zhang, Yuyang Xie, Shangqing Tu, Yiyang Fu, Youhe Feng, Jinkai Zhang, Jingyao Zhang, et al. 2024e. A solution-based llm api-using methodology for academic information seeking. arXiv preprint arXiv:2405.15165.

Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig. 2024f. What are tools anyway? a survey from the language model perspective.

Zhiruo Wang, Daniel Fried, and Graham Neubig. 2024g. Trove: Inducing verifiable and efficient toolboxes for solving programmatic tasks. arXiv preprint arXiv:2401.12869.

Sherwood L Washburn. 1960. Tools and human evolution. Scientific American, 203(3):62-75.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.

Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. 2023. Cmath: can your language model pass chinese elementary school math test? arXiv preprint arXiv:2306.16636.

Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.

Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, and Chaowei Xiao. 2024a. A new era in llm security: Exploring security concerns in real-world llm-based systems. arXiv preprint arXiv:2402.18649.

Jiayi Wu, Renyu Zhu, Nuo Chen, Qiushi Sun, Xiang Li, and Ming Gao. 2024b. Structure-aware fine-tuning for code pre-trained models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).

Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. 2024c. Sealtools: Self-instruct tool learning dataset for agent tuning and detailed benchmark. arXiv preprint arXiv:2405.08355.

Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864.

Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In Proceedings of 9th International Conference on Learning Representations (ICLR).

Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024a. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. In Proceedings of 12th International Conference on Learning Representations (ICLR).

Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023. On the tool manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504.
Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-seng Chua. 2024b. Search-in-the-chain: Towards the accurate, credible and traceable content generation for complex knowledge-intensive tasks. In Proceedings of the ACM Web Conference 2024.

Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2024. Gpt4tools: Teaching large language model to use tools via self-instruction. Advances in Neural Information Processing Systems, 36 .

Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mmreact: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600.

Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022a. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744-20757.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.

Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, et al. 2024a. Tooleyes: Finegrained evaluation for tool learning capabilities of large language models in real-world scenarios. arXiv preprint arXiv:2401.00741.

Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024b. Toolsword: Unveiling safety issues of large language models in tool learning across three stages. arXiv preprint arXiv:2402.10753.

Junjie Ye, Yilong Wu, Songyang Gao, Sixian Li, Guanyu Li, Xiaoran Fan, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024c. Rotbench: A multi-level benchmark for evaluating the robustness of large language models in tool learning. arXiv preprint arXiv:2401.08326.

Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R Fung, Hao Peng, and Heng Ji. 2024a. Craft: Customizing llms by creating and retrieving from specialized toolsets. In Proceedings of 12th International Conference on Learning Representations (ICLR).

Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and Deqing Yang. 2024b. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv preprint arXiv:2401.06201.

Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. 2024. Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents. arXiv preprint arXiv:2403.02691.

Beichen Zhang, Kun Zhou, Xilin Wei, Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. 2024a. Evaluating and improving tool-augmented computationintensive math reasoning. Advances in Neural Information Processing Systems, 36.

Biao Zhang, Barry Haddow, and Alexandra Birch. 2023a. Prompting large language model for machine translation: A case study. In International Conference on Machine Learning, pages 41092-41110. PMLR.

Jiawei Zhang. 2023. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. arXiv preprint arXiv:2304.11116.

Kexun Zhang, Hongqiao Chen, Lei Li, and William Wang. 2024b. Syntax error-free and generalizable tool use for llms via finite-state decoding. Advances in Neural Information Processing Systems.

Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2024c. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics, 12:39-57.

Yinger Zhang, Hui Cai, Yicheng Chen, Rui Sun, and Jing Zheng. 2023b. Reverse chain: A generic-rule for llms to master multi-api planning. arXiv preprint arXiv:2310.04474.

Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023c. Siren's song in the ai ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219.

Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. 2024a. Explainability for large language models: A survey. ACM Transactions on Intelligent Systems and Technology, 15(2):1-38.

Lirui Zhao, Yue Yang, Kaipeng Zhang, Wenqi Shao, Yuxin Zhang, Yu Qiao, Ping Luo, and Rongrong Ji. 2024b. Diffagent: Fast and accurate text-to-image api selection with large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).

Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024c. Retrievalaugmented generation for ai-generated content: A survey. arXiv preprint arXiv:2402.19473.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.

Yuyue Zhao, Jiancan Wu, Xiang Wang, Wei Tang, Dingxian Wang, and Maarten De Rijke. 2024d. Let me do it for you: Towards llm empowered recommendation via tool learning. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval.

Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, and Bin Wang. 2024. Toolrerank: Adaptive and hierarchy-aware reranking for tool retrieval. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).

Yaoyao Zhong, Mengshi Qi, Rui Wang, Yuhan Qiu, Yang Zhang, and Huadong Ma. 2023. Viotgpt: Learning to schedule vision tools towards intelligent video internet of things. arXiv preprint arXiv:2312.00401.

Mu Zhu. 2004. Recall, precision and average precision. Department of Statistics and Actuarial Science, University of Waterloo, Waterloo, 2(30):6.

Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel, and Chao Zhang. 2024a. Toolchain*: Efficient action space navigation in large language models with $\mathrm{a}^{*}$ search. In Proceedings of the 12th International Conference on Learning Representations (ICLR).

Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2024b. Toolqa: A dataset for llm question answering with external tools. Advances in Neural Information Processing Systems, 36.


[^0]:    ${ }^{1}$ Given the growing interest in tool learning, this survey may not encompass all benchmarks. We welcome suggestions to expand this list.

