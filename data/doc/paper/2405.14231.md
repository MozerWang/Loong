# From Role-Play to Drama-Interaction: An LLM Solution 

Weiqi Wu ${ }^{\diamond 1,2}$, Hongqiu Wu ${ }^{\diamond 1,2}$, Lai Jiang ${ }^{1,2}$, Xingyuan Liu ${ }^{1,2}$, Jiale Hong ${ }^{1,2}$,<br>Hai Zhao ${ }^{\dagger 1,2}$ and Min Zhang ${ }^{3}$<br>${ }^{1}$ Department of Computer Science and Engineering, Shanghai Jiao Tong University<br>${ }^{2}$ Key Laboratory of Shanghai Education Commission for Intelligent Interaction<br>and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China<br>${ }^{3}$ Harbin Institute of Technology, Shenzhen, China<br>\{wuwq1022,wuhongqiu\}@sjtu.edu.cn,<br>zhaohai@cs.sjtu.edu.cn,minzhang@suda.edu.cn


#### Abstract

Drama is a form of storytelling inspired by human creativity, proceeding with a predefined storyline, carrying emotions and thoughts. This paper introduces LLM-based interactive drama, which endows traditional drama with an unprecedented immersion, where a person is allowed to walk into it and interact with the characters and scenes. We define this new artistic genre by 6 essential elements-plot, character, thought, diction, spectacle and interaction-and study the entire pipeline to forge a backbone drama $L L M$ to drive the playing process, which is challenged by limited drama resources, uncontrollable narrative development, and complicated instruction following. We propose Narrative Chain to offer finer control over the narrative progression during interaction with players; Auto-Drama to synthesize drama scripts given arbitrary stories; Sparse Instruction Tuning to allow the model to follow sophisticated instructions. We manually craft 3 scripts, Detective Conan, Harry Potter, Romeo and Juliet, and design a 5-dimension principle to evaluate the drama LLM comprehensively.


## 1 Introduction

Large Language Models (LLMs) (OpenAI, 2023; Touvron et al., 2023; Jiang et al., 2023; Yang et al., 2023; Bai et al., 2023) make powerful role-play systems. Their adeptness in natural language understanding and generation allows them to play diverse characters, based on user-specified descriptions and profiles (Shanahan et al., 2023; Wu et al., 2024). However, our vision goes beyond mere roleplay, as we attempt to empower LLMs to render the entire dramatic narrative rather than solely a single character.[^0]

What is LLM-based interactive drama

Drama is a classical mode of storytelling, based upon dialogues and performances with a multitude of scenes of a story. In this paper, we study $L L M$ based interactive drama, a new immersive mode of storytelling, where the audience can walk into the story and interact with the characters and environment in it. Inspired by six essential elements of traditional drama outlined by Aristotle, i.e., plot, character, thought, diction, spectacle and music (Laurel, 1991; Betti, 2015), we outline the new six essential elements for interactive drama-plot, character, thought, diction, spectacle, and interaction. Plot presents a dramatic storyline through a series of scenes. Character refers to the roles participating in each scene, while thought delves into their inner motivations and psychological dynamics as the plot progresses. Diction refers to the dialogues among characters and the audience. Spectacle incorporates vivid text-based illustrations of the scene, such as background and items. Lastly, interaction stands as a pivotal and evolutionary element, fostering an immersive engagement between the audience and drama. Rather than role-play, the form of interaction in interactive drama is diverse and free. The audience is expected to have conversations with any character and take any action in the environment.

We show an example of interactive drama in Figure 1. In Scene 1, for instance, there are three characters and three items, with a predefined narration as well as character dialogue for scene rendering. The audience (as a player) is allowed the liberty to interact with any character (e.g., make a conversation to Ayumi) and any item (e.g., explore under the table). The LLM serves as the characters and the items in the scene to respond to the player. There is a target in the scene ("explore the reading room"). The purpose of it is to guide the player and all characters towards unfolding the plot smoothly. As the storyline progresses, a scene transition will be

![](https://cdn.mathpix.com/cropped/2024_06_04_6ac4315c29c20c4288a5g-02.jpg?height=639&width=1556&top_left_y=243&top_left_x=250)

Figure 1: Illustration of LLM-based interactive drama. Two scenes from a script for Detective Conan (Library Murder Case, Episode 50), where the player takes the role of Conan and there are other four characters, Ayumi, Mitsuhiko, Genta, and Librarian. The player can either make conversations with characters or take actions to progress the plot.

triggered, bringing a new plot, e.g., new characters and dialogues, as shown in Scene 2.

## How LLMs perform interactive drama

We denote the LLMs that perform interactive drama as drama LLMs. Compared to playing a single character, there is an entire dramatic story inside the LLM, which means it is required to tackle the interplay of multiple characters and the audience, as well as progress the plot. Therefore, it presents a more ambitious challenge to LLMs, extending their adeptness beyond the mere simulation of characters.

In contrast to playing a role instructed by its profile, the drama LLM follows a more intricate instruction. In this paper, we present a prototype drama script, a global instruction to guide the drama LLM to orchestrate the drama. A drama script is not a single instruction but a collection of individual scenes. In each scene, we detail the spectacle, characters, plot, etc. To assist the drama LLM in better managing the interplay between player agency and scripted plot progression, we introduce the novel concept of Narrative Chain, which divides the narrative into smaller consecutive segments. By guiding the player through each segment through the interaction with drama LLM, the player is allowed to explore and influence the plot autonomously while experiencing a coherent and smooth story progression.

## How one trains and evaluates drama LLMs

We propose a comprehensive training paradigm to fine-tune a general drama LLM to decently play the drama from given scripts, rather than zero-shot prompting. There are two main challenges. First, it is an exhausting process to acquire a large number of drama scripts. We propose Auto-Drama, an efficient data pipeline to generate the drama scripts automatically from arbitrary stories based on GPT-3.5 (OpenAI, 2023), including scene extraction, plot production, and trigger imagination. Moreover, a drama script is a lengthy and sophisticated instruction encompassing a series of sub-tasks and only a small fraction of them will be activated during each inference, which is based on the player's behaviour. It incurs a hard learning process for the LLM to follow such instructions. We propose Sparse Instruction Tuning (SIT), a two-stage training that unlocks more accurate instruction following. Eventually, we propose a fine-grained evaluation process, measuring the performance from 5 dimensions scenery, narration, coherency, guidance, and transition. A series of cases are illustrated to further demonstrate the ability of drama LLMs.

In summary, this paper:

- introduces an LLM-based solution for interactive drama with a prototype drama script;
- proposes the data generation technique that fuels the learning of drama LLMs;
- proposes an enhanced instruction tuning technique to train drama LLMs;
- presents the multi-aspect evaluation to assess the performance of drama LLMs.


## 2 Related Works

Role-play LLMs Simulating specific characters in conversation is a popular topic in Artificial Intelligence (AI) research (Zhang et al., 2009; Avin et al., 2020). Recently, LLMs have been used to mimic characters with various attributes and conversational styles (Shanahan et al., 2023; Wang et al., 2023a; Tu et al., 2024). There are generally two approaches to building role-play LLMs. The first involves prompting the model with detailed character profiles (Li et al., 2023; Tao et al., 2023; Wang et al., 2023b; Chen et al., 2023) or specific utterances (Han et al., 2022). The other involves fine-tuning the model on a character's experiences and dialogues (Shao et al., 2023; Zhou et al., 2023; Lu et al., 2024). The success of role-play LLMs lays the basis for playing characters in drama to create more immersive experiences.

Interactive Narrative Intelligence Interactive narrative (Bates et al., 1991) is a dynamic form of digital interactive experience where users can affect the storyline (Mateas, 2000; Szilas, 2007; Riedl and Bulitko, 2012b). It can be applied in both entertainment (Riedl and Bulitko, 2012a; Yong and Mitchell, 2023; Wu et al., 2024) and serious domains like education (Plowman, 2014; Wang et al., 2017). LLMs have been explored for tasks such as crafting scenes (Kumaran et al., 2023) and shaping pivotal plot points (Harmon and Rutman, 2023), further pushing the boundary of interactive narratives. Zhao et al. (2023) interacts with fictional characters in narratives in a multi-modal environment with LLMs. We aim to interact with not only the characters but also the plot and scenes, constructing a more immersive interactive drama based on LLMs.

AI for Art AI has revolutionized art creation, with applications ranging from generating paintings (Castellano and Vessio, 2021), music (Hernandez-Olivan et al., 2022; Zhu et al., 2023), poems (Chakrabarty et al., 2022; Bena and Kalita, 2019) to crafting screenplays and theatre scripts (Mirowski et al., 2023; Pramnik, 2022). Additionally, AI enhances artistic expression through interactive experiences (Gong et al., 2022; Zhao et al., 2023). In this paper, we explore the augmentation of artistic expression through user-LLM interaction by constructing drama LLMs.

## 3 Interactive Drama

In this section, we redefine elements of traditional drama from Aristotle's dramatic theory to propose the new six elements for $L L M$-based interactive drama. Upon the new definition, we then present the prototype of a drama script, by which LLMs are instructed to perform the drama.

### 3.1 LLM-based Interactive Drama

AI-based interactive drama has been mentioned by creative researchers for many decades (Laurel, 1986; Bates, 1992; Mateas, 2000). We focus on LLM-based interactive drama in this paper, which comprises six essential elements, i.e., plot, character, thought, diction, spectacle, and interaction. They work in synergy to craft an immersive and dynamic storytelling experience.

Plot Plot is the backbone of the storytelling. Generally, a drama tells the story with a sequence of scenes. The transition of scenes suggests the development of the plot. In contrast to the fixed plot in traditional drama, interactive drama allows the audience to influence the development of the story through interactions, to some extent. Therefore, it is crucial for drama LLMs to process the interplay between the audience and the plot.

Character Characters are the individuals who inhabit the dramatic world in each scene, defined by various settings like personalities, motivations and behaviours. Rather than playing a single character in traditional role-play, drama LLMs play multiple characters in the drama and process the relationships between them simultaneously.

Thought Thought is an important part of the character settings, representing their inner motivation for behaviours. Thoughts of characters may change as the plot develops. Hence, it is necessary to update the character settings provided for LLMs when scenes switch to ensure a stable memory.

Diction Diction in traditional drama refers to the preset dialogues between characters. Interactive drama enables dynamic conversations between characters and the audience. In both situations, the characters should be coherent with their settings.

Spectacle While traditional drama relies on visual assets to create a spectacle, LLM-based interactive drama utilizes text to display. A descriptive tone is leveraged to recover, refine and render the
![](https://cdn.mathpix.com/cropped/2024_06_04_6ac4315c29c20c4288a5g-04.jpg?height=1238&width=1424&top_left_y=280&top_left_x=266)

Player's View

Drama LLM: Scene 1: In Capulet's home, people are swaying gracefully to the rhythm of enchanting melodies, with mysterious masks on their faces shining under the dazzling lights.

$$
\text { Ex. I: Conversation with Characters }
$$

Player: [To Capulet] Hi, Lord Capulet.

Drama LLM: [Capulet: ] As long as you are not causing trouble, may you enjoy the banquet.

Player: [To Capulet] Sure, I always behave myself.

Drama LLM: [Capulet: ] Good to hear. Have fun, then.

Ex. II: Player Action Choose ®

Player: [Action] Explore the mask.
Drama LLM: This is a magnificent golden mask.

Player: [Action] Ask her for a dance.

Drama LLM: Scene 2: In Capulet's home, you walk

towards the girl, each step a dance of anticipation...

Figure 2: Prototype of drama scripts to prompt drama LLMs. The identifier at the beginning of each input either designates a character for dialogue or indicates that the input corresponds to an action outlined in the script.

scenery including background and items, thereby immersing the audience in the dramatic world.

Interaction Interaction emerges as a new element for interactive drama. It bridges the audience with dramatic storytelling, where the audience turns into the player and engages in the story. Generally, the player is allowed to converse with characters or perform specific actions within the scene. These interactions may impact the plot by triggering new events or changing the characters' endings.

### 3.2 Drama Script

Upon interpreting six basic elements describing LLM-based interactive drama, we materialize them into a new style of instruction for LLMs to perform the interactive drama, named drama script.

A drama script serves as the global instruction, outlining the desired drama for drama LLMs. In the prototype presented in Figure 2, we demon- strate how the scripts can be tailored to specific narratives, allowing for creative adaptation. Overall, a drama script is a network of individual scenes that collectively form the storyline, which can unfold linearly or non-linearly. Each scene transitions to the next based on specific conditions being met. Specifically, every single scene is a sophisticated instruction detailed with six parts, as shown in the left side of Figure 2. Spectacle describes the scenery. Once transiting to a new scene, the model will be triggered to render it to the audience. What follows is the character settings in Character. The player can have a conversation with any of the characters in the scene. Notably, in contrast to open-ended playgrounds, interactive drama should follow an underlying storyline. To balance player autonomy and the smooth unfolding of the plot, the main target of the current scene is stated in Plotline, which guides the characters to avoid unrelated or even offensive discussions with the audience and pull them back to the main plot. Plot lists the preset dialogue
among characters and narration in the scene. Interaction defines the way that the player can interact. In this prototype, we offer two main forms of interaction, i.e. having conversations and choosing specific actions. Trigger defines how the player's behaviour impacts the plot, e.g., scene transition, discovering new clues, and having a new relationship with some character. The drama LLM should learn to be a perfect trigger to unfold the plot.

Each model input includes scripts of relevant scenes and multi-turn user inputs. As depicted in the right half of Figure 2, the drama LLM initially generates a description of the spectacle and plot, as well as renders the available interactions to the player. Subsequent user inputs take the form of either dialogue with characters in the scene, specified by the character's name followed by the dialogue content, or actions, composed of an action identifier and the chosen action. The drama LLM then processes these inputs, either role-playing characters to respond to the user or producing corresponding plot updates based on the trigger.

## 4 Narrative Chain

Interactive drama presents a critical challenge in balancing player agency with the story authored (Weyhrauch and Bates, 1997; Magerko, 2005). While players influence the narrative progression through dialogues and actions, there remains a need to guide them to explore the pre-drafted story, which should be done by the drama LLM.

Any narrative has a beginning and an end, which can be visualized as two points in the space, as depicted in Figure 3. Maintaining its progression requires the player to reach the endpoint from the starting point. Otherwise, the player may get lost and the subsequent story cannot progress properly (the blue line). A straightforward way is to intervene in the player's behaviours through interaction. For instance, the drama LLM repeatedly asks the player to take specific actions to ensure that the narrative reaches its intended endpoint. However, this method can be obtrusive and greatly damages the player experience (the pink line). Therefore, we propose a novel concept termed Narrative Chain, which defines the way the drama LLM guides the story progression smoothly and coherently.

A drama unfolds through a story arc, which is the trajectory formed by linking all the narratives together. In a specific scene, this arc can be divided into smaller narrative segments, resembling a chain

![](https://cdn.mathpix.com/cropped/2024_06_04_6ac4315c29c20c4288a5g-05.jpg?height=300&width=763&top_left_y=250&top_left_x=1052)

Figure 3: Story arcs defined by the drama script and experienced by the player across different methodologies.

of sub-narratives. By navigating the player through each segment, we can establish a nearly straight line between every two adjacent points, approximating the curvature of the overall story arc, as illustrated in Figure 3. This segmentation facilitates finer control over the narrative development, ensuring a smooth progression throughout the story arc. Ultimately, the step-by-step guidance directs the player through a sequence of narratives while respecting interactive freedom, as the behaviour of the player remains open-ended.

Therefore, when the narrative of a scene is complex, it can be effectively presented in the form of a chain. For example, in Scene 1 of Figure 1, to guide the player to explore the reading room, the plotline "Explore the reading room" can be further divided into "Search for books with Genta - Ayumi wants to do homework by the table - Ayumi discovers a box under the table". Players gradually delve deeper into the narrative, gaining a deeper understanding of the unfolding drama before they encounter the core tasks predefined by the script. Meanwhile, the drama LLM is tasked with assessing which stage of the Narrative Chain it is currently navigating and providing accurate guidance relevant to that specific sub-narrative to ensure that each segment is thoroughly explored and complete. Additionally, the drama LLM determines when to introduce the next segment, facilitating the completion of the narrative development within the scene.

## 5 Data Generation

Harnessing LLMs to construct interactive drama necessitates the process of fine-tuning on a large amount of supervised data, while manually creating diverse drama scripts is a tough process. It will be nice and efficient to generate drama scripts automatically from public stories on the web. We thus propose Auto-Drama that crafts drama scripts from any given story harnessing the power of GPT-3.5. Rather than prompting GPT-3.5 to directly draft

![](https://cdn.mathpix.com/cropped/2024_06_04_6ac4315c29c20c4288a5g-06.jpg?height=445&width=1570&top_left_y=246&top_left_x=240)

Figure 4: The pipeline of Auto-Drama for drama script generation.

the script from the story, incurring low-quality and incomplete drama, Auto-Drama is a pipeline featured by four main steps, as illustrated in Figure 4. The process enriches a brief scene to a detailed one while ensuring it fulfils the six elements of interactive drama.

Story Collection First, we crawl Andersen's Fairy Tales from the web and collect over 80 stories. This corpus serves as the foundational resource for our subsequent script generation phases.

Scene Extraction Given a story, we prompt GPT3.5 to imagine itself as one of the main characters in the story and break down the story into a set of scenes from the view of a player. Simultaneously, we task it with generating scene details. We request basic information about the scene which helps drama LLMs to recover it, including location, time, atmosphere and items. We also query GPT3.5 to enrich each character with details regarding their personalities, thoughts and behaviours.

Plot Production Given the scene, the next significant step is crafting the plot within it, requiring associated story content. The plot includes three aspects. The first is the plotline to ensure a smooth and uninterrupted progression of the story. The second is the performance in the scene, which is a detailed plot consisting of two forms of performance: dialogue among characters and background narration. The last is the potential interactions for the player, involving conversation and action. We prompt GPT-3.5 to determine if the player can engage in conversation within the scene, excluding scenarios where urgency precludes chat or no other characters are present. In addition, we prompt it to imagine the possible actions for the player related to the plot, by offering it some examples.
Trigger Imagination Lastly, we prompt GPT-3.5 to imagine the possible consequences triggered by the player interaction. For example, some words said and some choices made by the player may lead to changes in the characters' behaviour. These changes can impact the progress of the story, which cannot be gained by simply playing the characters based on their settings. We also request at least one transitional consequence to connect individual scenes seamlessly.

## Dialogue and Narrative Generation for Auto-

 Drama We acquire sufficient drama scripts thanks to Auto-Drama. One step left is to acquire the training samples (i.e. input and output contents) for the script, which are possible interactions between the LLM and the player. Since the player's actions are provided as options, we focus on generating dialogue and narrative content with the assistance of GPT-3.5. We instruct GPT-3.5 to create dialogue based on triggering conditions for the player's proactive speech. We additionally generate casual conversations between all characters and the player, following the character settings. For LLMs to guide the dialogue, we provide examples of unrelated dialogues that require guidance and prompt GPT-3.5 to generate more. The number of rounds is limited to 2 to 5 . Ultimately, we produce refined scene descriptions and narrative dialogues for the plot. Paraphrasing and rephrasing prompts are employed to ensure the diversity of data.
## 6 Sparse Instruction Tuning

In this section, we present the detailed methodology to train drama LLMs. Based on our prototype drama script, drama LLMs will learn to follow a lengthy and sophisticated instruction that encompasses a series of sub-tasks. For instance, on top of general language understanding and genera-

![](https://cdn.mathpix.com/cropped/2024_06_04_6ac4315c29c20c4288a5g-07.jpg?height=391&width=740&top_left_y=247&top_left_x=247)

Figure 5: Diagram of Sparse Instruction Tuning.

tion, a drama script covers sub-tasks like: $\bullet$ Transition: locate the next scene; $\bullet$ Refinement: refine scenery given the narrative tone; $\bullet$ Role-Play: play characters following their profiles and thoughts; $\bullet$ Guidance: guide the player back to the plotline; $\bullet$ Semantics: capture the nuanced semantics of the player's talk to trigger the correct consequence; etc.

However, it is challenging for the model to navigate all sub-tasks within one instruction at the same time. Furthermore, for each inference, only a small proportion of sub-tasks are associated. We denote such instructions as sparse instructions, which make the fine-tuning process pretty hard and in low efficiency. To this end, we propose sparse instruction tuning (SIT), which contains two stage as shown in Figure 5: (1) In the first stage, we break down the drama script into a series of subtasks and craft an individual instruction for each sub-task to describe it. As a result, the fine-tuning process starts with training the model on all these sub-task instructions, which offers a nice initial state for the subsequent learning (Wu et al., 2023). (2) In the second stage, where the model undergoes fine-tuning on entire drama scripts, we require the model to refer to the associated sub-tasks in responses. Specifically, we introduce an annotation "/*" and "*/" to encapsulate the sub-tasks which prefixes the associated contents in the response, which can be filtered out with ease when presented to the audience. This trick effectively creates a link between the instruction and sparse sub-tasks, thus allowing for more accurate instruction following.

## 7 Experiments

We fine-tune LLaMA3-8B-Instruct ${ }^{1}$ (Touvron et al., 2023) and Qwen1.5-14B-Chat ${ }^{2}$ (Bai et al., 2023) on data generated by Auto-Drama (Auto-Drama[^1]

data for simplicity). Detailed training setups are in Appendix A. We evaluate drama LLMs on three manually-written scripts: a detective story (adapted from Detective Conan), an adventure story (adapted from Harry Potter) and a classical drama (adapted from Romeo and Juliet).

### 7.1 Evaluation Setup

From a plot-centric perspective, we propose five critical dimensions for assessing the efficacy of drama LLMs:

Scenery This dimension evaluates the scene presentation by drama LLM, considering how well it aligns with the provided details and intended tone.

Narration Similar to scenery but focusing on a different aspect (plot v.s. spectacle), it assesses how effectively the plot narration aligns with the intended tone and atmosphere of the scene.

Transition We examine the effectiveness of drama LLMs in managing scene transitions, ensuring that the scene changes appropriately when triggered by the player.

Guidance We assess how decent drama LLMs maintain the player engagement with the plotline during the interaction, ensuring players stay connected to the plot and smoothly unfold the plot.

Coherency This dimension evaluates the adeptness of drama LLMs in representing characters, and whether responses by characters align with their established profiles and internal thought processes.

GPT-4 is employed as the judge to score scenery, narration, guidance, and coherency on a 7-point Likert scale. We manually check the transition score to accurately examine whether the drama LLM transits to a new scene or stays in the current scene correctly. We assign a score of 7 points for a correct transition, 4 points if the trigger annotation is correct but the transition is wrong, and 1 point for any other situations.

### 7.2 Results and Discussion

Overall Results Figure 6 illustrates the performance of our 8B drama LLM. Trained on AutoDrama data with sparse instruction fine-tuning, the drama LLM achieves exceptional scores across all dimensions. It demonstrates remarkable capabilities in engaging in dialogue with players, generating fluent and rich narratives based on the plot, and accurately handling plot progression. Notably, the

![](https://cdn.mathpix.com/cropped/2024_06_04_6ac4315c29c20c4288a5g-08.jpg?height=465&width=1347&top_left_y=253&top_left_x=366)

Figure 6: Performance of 8B drama LLMs across our three drama scripts with different training setups: (1) fine-tuned with completed Auto-Drama data and Sparse Instruction Tuning (SIT), (2) fine-tuned with completed Auto-Drama data without SIT, (3) fine-tuned with only $10 \%$ proportion of Auto-Drama data without SIT.

model excels in guidance, effectively steering the player back to the main storyline through generated responses and ensuring correct plot progression amidst various transitions.

However, as elaborated in Appendix B, scaling up to the 14B model does not yield substantial improvements except for transition, which is a relative weakness for smaller drama LLMs. This could suggest that larger models hold an advantage in handling more complex scenarios, while increasing model size may not always be beneficial for certain tasks, especially if the task itself is not complex enough to warrant the additional model capacity.

Effectiveness of Auto-Drama To assess the impact of Auto-Drama data, we conduct experiments by fine-tuning LLaMA3-8B-Instruct with varying proportions of Auto-Drama data. From Figure 6, comparing the drama LLM trained on $100 \%$ and $10 \%$ of Auto-Drama data, we observed a significant improvement in scenery and guidance. Despite the limited amount of training data, drama LLM trained on $10 \%$ of the Auto-Drama exhibits notable performance on coherency and narration due to its inherent zero-shot role-playing and text integration capabilities. The richer dataset further enhances its ability to depict scenes and guide dialogues effectively, validating the effectiveness of Auto-Drama.

Effectiveness of SIT Despite the promising results obtained from training the drama LLM using Auto-Drama, its accuracy remains low in complex sub-tasks within the instruction, such as transition, where it needs to identify the player's intent in the dialogue. Figure 6 shows that SIT effectively enhances the ability to follow complex instructions, resulting in improvements across all dimensions, especially transition. Therefore, SIT proves to be a highly effective training method for complex instruction following, which is significant for performing interactive drama.

## 8 Case Study

Table 1 takes a closer look at how drama LLMs work.

Narrative Tone Case 1 showcases how different tones of a scene affect the narration generated by the drama LLM. When a specific tone is appointed, we can find the adeptness in modifying the narration to either cultivate an atmosphere of mystery and tension where characters engage in low-toned and cautious talks or to foster a relaxing and cheerful environment where characters exhibit smiles and excitement. This augments the narrative diversity of drama LLMs, better utilizing simplistic scripts to present varied and vivid dramatic content.

Scene Transition Case 2 exemplifies the effectiveness of drama LLMs in managing scene transitions based on the player's input. By detecting the belief of the player regarding the presence or absence of books in the room, the drama LLM guides the narrative towards the appropriate scene, as dictated by triggers defined within the drama scripts. Triggering by dialogue requires an understanding of nuanced semantics within the input to match those defined triggers within the script. Misinterpretation of the intention may lead to erroneous and bad plot progression.

Guidance In Case 3, given a plotline, the drama LLM showcases its adeptness in steering the focus of the player back to the storyline, particularly

Trigger 1: the player thinks the books were still in the room Player: To Ayumi: There are so many empty shells that there is no way to fit them all into the purse.

Drama LLM: You realize that some of the books must be hidden somewhere in the room. Therefore, you and your friends decide to search for them. (Jump to Scene 10)

## Case 3: Plot Guidance

Plotline: Help Peter read the invitation in his hand.

Player: To Peter: What is your favourite color?

Drama LLM: Peter: It's red. Sir, please focus and read the words on this invitation

Case 4 Role-Play

Voldemort: Evil, ruthless, cruel and hateful. Will stop at nothing to obtain the Philosopher's Stone for resurrection.

Player: To Voldemort: I won't give you the Philosopher's Stone!

Drama LLM: Voldemort: Humph, your resistance is futile. The Philosopher's Stone will be mine.


#### Abstract

Trigger 2: the player thinks the books were taken away Player: To Ayumi: He should have taken all the books with him so as not to be discovered. I guess we'll just have to find him.

Drama LLM: You all think the books were taken away, so you decide to head downstairs to follow the suspect. However, you get caught by


 the suspect. (Jump to Scene 11)
## Plotline: null

Player: To Peter: What is your favourite color?

Drama LLM: Peter: My favourite color is red.


#### Abstract

Voldemort: Kind, warm, friendly and polite. Wish to obtain the Philosopher's Stone and use it for good.

Player: To Voldemort: I won't give you the Philosopher's Stone! Drama LLM: Voldemort: Harry, you should know that opposing me will lead to no favourable outcome. However, you can choose to accept and I will offer you certain privileges.


Table 1: Case study of interactions between the player and drama LLM based on LLaMA3-8B-Instruct.

when responding to the player's inquiries that veer off-topic. Hence, drama LLMs should possess the capability to gently guide the audience back to the drama. This characteristic underscores the plotcentric nature of interactive drama.

Character Setting Case 4 highlights the effectiveness of portraying diverse personalities. Provided with diametrically opposed personalities (evil v.s. kind) for the same character Voldemort, the drama LLM exhibits corresponding variations in its responses to the same input.

## 9 Conclusion

This paper introduces LLM-based interactive drama and proposes the paradigm to train drama LLMs to realize this innovative form of storytelling. Based on elements of interactive drama, a prototype of drama script is proposed to serve as the global instruction for drama LLMs. To offer finer control over the drama development without harming the interactive freedom of the audience, we segment the narrative to construct the narrative chain and have drama LLMs to navigate the audience through each sub-narrative. To facilitate the training process, Auto-Drama is proposed to automate script generation, as well as Sparse Instruction Tuning to help LLMs follow complicated instructions describing many sub-tasks. Through comprehensive evaluation, we show the performance of the trained drama LLMs.

## Limitations

While our drama LLMs offer exciting potential for immersive interactions, several limitations warrant further exploration: (1) Limited Modalities: Our current drama LLMs primarily support text-based interactions. Additional modalities such as images, sound, or video could enrich the immersive experience, but this expansion presents technical and design challenges. (2) Action Complexity: There are significant constraints on player-scene action interaction in our current setup. One possible avenue for exploration is the integration of physical models (Hao et al., 2022; Asri et al., 2022; Seyyedi et al., 2023) to simulate more immersive physical interactions between players and scenes. (3) Evaluation: Despite our five-dimension automatic evaluation performed by GPT-4, a more robust assessing method is crucial for advancing LLM-based interactive drama. A large-scale survey among users could be valuable for gathering insights in future work. These limitations highlight the importance of ongoing research and development efforts aimed at addressing the challenges associated with LLMbased interactive drama.

## Ethics Statement

The development and use of drama LLMs are guided by ethical principles to ensure responsible and beneficial outcomes. (1) Data: We utilize stories from Anderson's fairy tales for generating data, and make adaptations of Detective Conan (Library Murder Case), Harry Potter and the Philosopher's Stone, and Romeo and Juliet to construct the test set. To address potential ethical concerns related to the utilization of Narrative Chainyrighted materials, we affirm that our research is conducted for academic and non-commercial purposes only. The use of these texts is solely for the development and evaluation of our models in natural language processing tasks, aimed at advancing scientific knowledge in the field. (2) Responsible Usage: We encourage the responsible use of drama LLMs for educational, entertainment, and creative purposes while discouraging any harmful or malicious activities.

## References

Zakaria El Asri, Clément Rambour, and Vincent Le Guen. 2022. Residual model-based reinforcement learning for physical dynamics.

Shahar Avin, Ross Gruetzemacher, and James Fox. 2020. Exploring ai futures through role play. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages 8-14.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.

Joseph Bates. 1992. Virtual reality, art and entertainment. Presence Teleoperators Virtual Environ., $1(1): 133-138$.

Joseph Bates, Bryan Loyall, and W. Scott Reilly. 1991. Broad agents. SIGART Bull., 2(4):38-40.

Brendan Bena and Jugal Kalita. 2019. Introducing aspects of creativity in automatic poetry generation. In Proceedings of the 16th International Conference on Natural Language Processing, pages 26-35, International Institute of Information Technology, Hyderabad, India. NLP Association of India.
Mohammed Jasim Betti. 2015. An introduction to drama. Diwaniya: Dar Nippur.

Giovanna Castellano and Gennaro Vessio. 2021. Deep learning approaches to pattern extraction and recognition in paintings and drawings: an overview. Neural Computing and Applications, 33:12263-12282.

Tuhin Chakrabarty, Vishakh Padmakumar, and He He. 2022. Help me write a poem: Instruction tuning as a vehicle for collaborative poetry writing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6848-6863, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, and Jia Li. 2023. Large language models meet harry potter: A dataset for aligning dialogue agents with characters. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8506-8520, Singapore. Association for Computational Linguistics.

Zhe Gong, Ruizhi Wang, and Guobin Xia. 2022. Augmented reality (ar) as a tool for engaging museum experience: A case study on chinese art pieces. Digital, 2(1):33-45.

Seungju Han, Beomsu Kim, Jin Yong Yoo, Seokjun Seo, Sangbum Kim, Enkhbayar Erdenee, and Buru Chang. 2022. Meet your favorite character: Opendomain chatbot mimicking fictional characters with only a few utterances. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5114-5132, Seattle, United States. Association for Computational Linguistics.

Zhongkai Hao, Songming Liu, Yichi Zhang, Chengyang Ying, Yao Feng, Hang Su, and Jun Zhu. 2022. Physics-informed machine learning: A survey on problems, methods and applications. ArXiv, $\mathrm{abs} / 2211.08064$.

Sarah Harmon and Sophia Rutman. 2023. Prompt engineering for narrative choice generation. In International Conference on Interactive Digital Storytelling.

Carlos Hernandez-Olivan, Javier Hernandez-Olivan, and Jose R. Beltran. 2022. A survey on artificial intelligence for music generation: Agents, domains and perspectives.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. CoRR, abs/2310.06825.

Vikram Kumaran, Jonathan Rowe, Bradford Mott, and James Lester. 2023. Scenecraft: Automating interactive narrative scene generation in digital games with large language models. Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, 19(1):86-96.

Brenda Laurel. 1986. Toward the design of a computerbased interactive fantasy system.

Brenda Laurel. 1991. Computers as theatre.

Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, Linkang Zhan, Yaokai Jia, Pingyu Wu, and Haozhen Sun. 2023. Chatharuhi; Reviving anime character in reality via large language model.

Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024. Large language models are superpositions of all characters: Attaining arbitrary role-play via selfalignment.

Brian Magerko. 2005. Story representation and interactive drama. In AIIDE, pages 87-92. Citeseer.

Michael Mateas. 2000. A neo-aristotelian theory of interactive drama. In Working notes of the AI and Interactive Entertainment Symposium. AAAI Press Menlo Park.

Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, and Richard Evans. 2023. Co-writing screenplays and theatre scripts with language models: Evaluation by industry professionals. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI '23, New York, NY, USA. Association for Computing Machinery.

OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.

Lydia Plowman. 2014. Getting the story straight: the role of narrative in teaching and learning with interactive media. In Cognition, education, and communication technology, pages 55-76. Routledge.

Vishal Pramnik. 2022. Survey: Automatic movie plot and script generation.

Mark Riedl and Vadim Bulitko. 2012a. Interactive narrative: A novel application of artificial intelligence for computer games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, pages $2160-2165$.

Mark O. Riedl and Vadim Bulitko. 2012b. Interactive narrative: An intelligent systems approach. AI Mag., $34: 67-77$.

Azra Seyyedi, Mahdi Bohlouli, and Seyed Ehsan Nedaaee Oskoee. 2023. Machine learning and physics: A survey of integrated models. ACM Computing Surveys.
Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role play with large language models. Nature, 623:493-498.

Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-LLM: A trainable agent for roleplaying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13153-13187, Singapore. Association for Computational Linguistics.

Nicolas Szilas. 2007. A computational model of an intelligent narrator for interactive narratives. Applied Artificial Intelligence, 21(8):753-801.

Meiling Tao, Xuechen Liang, Tianyu Shi, Lei Yu, and Yiting Xie. 2023. Rolecraft-glm: Advancing personalized role-playing in large language models.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288.

Quan Tu, Shilong Fan, Zihang Tian, and Rui Yan. 2024. Charactereval: A chinese benchmark for role-playing conversational agent evaluation.

Pengcheng Wang, Jonathan P Rowe, Wookhee Min, Bradford W Mott, and James C Lester. 2017. Interactive narrative personalization with deep reinforcement learning. In IJCAI, pages 3852-3858.

Xintao Wang, Quan Tu, Yaying Fei, Ziang Leng, and Cheng Li. 2023a. Does role-playing chatbots capture the character personalities? assessing personality traits for role-playing chatbots.

Zekun Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, Jie Fu, and Junran Peng. 2023b. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. ArXiv, abs/2310.00746.

Peter Weyhrauch and Joseph Bates. 1997. Guiding interactive drama. Carnegie Mellon University Pittsburgh.

Hongqiu Wu, Linfeng Liu, Hai Zhao, and Min Zhang. 2023. Empower nested boolean logic via selfsupervised curriculum learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 13731-13742. Association for Computational Linguistics.

Hongqiu Wu, Y. Wang, Xingyuan Liu, Hai Zhao, and Min Zhang. 2024. Instruction-driven game engines on large language models. CoRR, abs/2404.00276.

Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. 2023. Baichuan 2: Open large-scale language models. CoRR, abs/2309.10305.

Qing Ru Yong and Alex Mitchell. 2023. From playing the story to gaming the system: Repeat experiences of a large language model-based interactive story. In International Conference on Interactive Digital Storytelling, pages 395-409. Springer.

Li Zhang, Marco Gillies, Kulwant Dhaliwal, Amanda Gower, Dale Robertson, and Barry Crabtree. 2009. E-drama: facilitating online role-play using an ai actor and emotionally expressive characters. International Journal of Artificial Intelligence in Education, 19(1):5-38.

Runcong Zhao, Wenjia Zhang, Jiazheng Li, Lixing Zhu, Yanran Li, Yulan He, and Lin Gui. 2023. Narrativeplay: Interactive narrative understanding.

Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, Sahand Sabour, Xiaohan Zhang, Wenjing Hou, Yijia Zhang, Yuxiao Dong, Jie Tang, and Minlie Huang. 2023. Characterglm: Customizing chinese conversational ai characters with large language models. ArXiv, abs/2311.16832.

Yueyue Zhu, Jared Baca, Banafsheh Rekabdar, and Reza Rawassizadeh. 2023. A survey of ai music generation tools and models.
