# Language Model Alignment with Elastic Reset 

Michael Noukhovitch*<br>Mila, Université de Montréal

Florian Strub

Google Deepmind

Samuel Lavoie<br>Mila, Université de Montréal

Aaron Courville<br>Mila, Université de Montréal<br>Canada CIFAR AI Chair


#### Abstract

Finetuning language models with reinforcement learning (RL), e.g. from human feedback (HF), is a prominent method for alignment. But optimizing against a reward model can improve on reward while degrading performance in other areas, a phenomenon known as reward hacking, alignment tax, or language drift. First, we argue that commonly-used test metrics are insufficient and instead measure how different algorithms tradeoff between reward and drift. The standard method modified the reward with a Kullback-Lieber (KL) penalty between the online and initial model. We propose Elastic Reset, a new algorithm that achieves higher reward with less drift without explicitly modifying the training objective. We periodically reset the online model to an exponentially moving average (EMA) of itself, then reset the EMA model to the initial model. Through the use of an EMA, our model recovers quickly after resets and achieves higher reward with less drift in the same number of steps. We demonstrate that fine-tuning language models with Elastic Reset leads to state-of-the-art performance on a small scale pivot-translation benchmark, outperforms all baselines in a medium-scale RLHF-like IMDB mock sentiment task and leads to a more performant and more aligned technical QA chatbot with LLaMA-7B. Code available at github.com/mnoukhov/elastic-reset.


## 1 Introduction

Dialogue agents that can effectively interpret and use language are a long-term challenge for NLP. The rise of large pretrained language models (LMs) [Brown et al., 2020] made language model finetuning one of the most promising research directions to achieving capable dialogue agents [Bender and Koller, 2020]. Recently, reinforcement learning (RL) has become a key ingredient of finetuning large LMs for interaction with humans [Ziegler et al., 2019, Ouyang et al., 2022, Bai et al., 2022], notably shown in ChatGPT [OpenAI, 2022]. A reward model is learned on the alignment objective, such as learned human preferences [RLHF; Christiano et al., 2017, Stiennon et al., 2020], and the language model is finetuned to optimize the reward. But training on the RL objective moves the model away from its pretraining and can reduce performance on important benchmarks [Ouyang et al., 2022] and even drifting away from natural language syntax and semantics [Lazaridou et al., 2020].

"Language drift" [Lee et al., 2019, Lazaridou et al., 2020], "alignment tax" [Askell et al., 2021], "reward model overoptimization" [Gao et al., 2022], or LM-specific "reward-hacking" [Clark and Amodei, 2016] is inherent to RLHF. In the extreme case, models learn to achieve high reward by generating nonsense text that is unintelligible to humans [Lewis et al., 2017]. Methods to mitigate this issue range from re-running pretraining [Lowe* et al., 2021], grounding in other modalities [Lee et al.,[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-02.jpg?height=301&width=1247&top_left_y=240&top_left_x=431)

Figure 1: Elastic Reset. In actor-critic RL, we reset the policy but maintain the value function.

2019], masking the LM generation [Ramamurthy et al., 2022] and iterated learning [Lu et al., 2020]. But the standard, and by far most popular approach, adds a Kullback-Lieber (KL) divergence penalty to the reward in order to prevent the finetuned model from drifting too far from the pretrained model [Jaques et al., 2017, 2019, Ziegler et al., 2019]. Still, all methods are insufficient over a large-enough training horizon so models are early-stopped before reaching a catastrophic level of drift.

Gao et al. [2022] find that achieving reward is proportional to drift from the initial model, but that not all drifts are equal. We wish to make small but effective changes that achieve high reward but maintain capabilities, yet auxiliary losses such as the KL penalty don't seem improve this tradeoff and only serve to slow down training [Gao et al., 2022]. We posit that RLHF training requires a useful inductive bias that does not modify the training objective. Inspired by recent work in generalization for image classification [Zhou et al., 2022], sample-efficient RL [Nikishin et al., 2022, D'Oro et al., 2023], and inducing compositional language [Li and Bowling, 2019], we propose to use resets. Similar to iterated learning [Kirby, 2001], iteratively resetting a model has been shown to reduce overfitting in language and RL scenarios [Rita et al., 2022]. In this work, we show iteratively resetting a model also reduces drift while attaining equal or better reward than just a KL penalty.

Unlike previous work in sample-efficient RL [Nikishin et al., 2022], RLHF is typically on-policy so it does not maintain a replay buffer with which to bootstrap learning after a reset. In lieu of a replay buffer, we reset the policy but maintain the value function. Yet resetting the policy to its initial state can still cause a large drop in performance. So we propose resetting to a model in-between our online and initial state, specifically to an exponential moving average (EMA) of our online policy, as EMA has been shown to be highly performant [Caron et al., 2021]. We still expect our EMA model to slowly drift, so we add a second step where we reset the EMA model to the initial model. We call this overall method Elastic Reset and illustrate it in Figure 1. Elastic Reset is implemented on top of regular RL methods such as REINFORCE [Williams, 1992] or PPO [Schulman et al., 2017].

First, we test our method on a small scale task: pivot translation with a transformer. In this classic benchmark for drift, we outperform all previous baselines and demonstrate state-of-the-art performance. Next, we re-evaluate how performance is measured in the field and argue for a metric of how each method trades off performance vs drift. We propose the Pareto Frontier Graph, a graphical measure that illuminates the trade-off between performance and drift and demonstrate that Elastic Reset dominates the baselines against this trade-off. Then, we scale up slightly to GPT2 and work on a popular task closer to RLHF, IMDB mock sentiment. Comparing to all baseline methods, we again show state-of-the-art performance on the benchmark. Through ablations, we show that Elastic Reset is robust to choices of hyperparameters, even more so than baselines. Finally, we scale up even more to true RLHF finetuning of Llama-7B in order to create a helpful technical QA chatbot using a StackExchange dataset. We again outperform the baseline, demonstrating how Elastic Reset mitigates the alignment tax while better optimizing the human feedback reward.

## 2 Related Work

"It is often difficult or infeasible to capture exactly what we want an agent to do, and as a result we frequently end up using imperfect but easily measured proxies" [Clark and Amodei, 2016]. In RL, this proxy is how we construct our reward and the consequence can be "reward-hacking" [Clark and Amodei, 2016]; an agent optimizes the reward but does not accomplishing the meaningful task. RLHF aims to align an agent with human preferences while maintaining the capabilities of the pretrained model, but uses a learned reward model as a proxy of human preferences [Christiano et al., 2017, Ziegler et al., 2019]. This can lead to LMs that optimize a reward model but degrade in
performance on general NLP benchmarks [Askell et al., 2021], overfit the reward model and do not generalize to true human preferences [Bai et al., 2022, Gao et al., 2022], or latch onto confounding factors hidden in the reward [Stiennon et al., 2020]. These effects are exacerbated if the reward model is updated during training such as in iterated RLHF [Bai et al., 2022] or related setups such as emergent communication [Lazaridou and Baroni, 2020], end-to-end dialogue [Lewis et al., 2017], learning RL policies through latent language [Andreas et al., 2018], and pivot translation [Utiyama and Isahara, 2007]. There, the phenomenon is known as "language drift" [Lee et al., 2019] and can lead to incoherent and unnatural linguistic outputs Lewis et al. [2017].

This phenomenon is inherent to RLHF. Gao et al. [2022] show that improvement on alignment / reward is proportional to drift from the initial model, but also find that different methods and design choices achieve different proportions of performance to drift. Therefore, a major challenge of RLHF is how to learn the reward in such a way as to minimize the drift, alignment tax, and reward-hacking. The standard approach used in most RLHF is to incorporate a KL penalty between the training language model and some fixed model [Jaques et al., 2019, Ziegler et al., 2019, Stiennon et al., 2020, Ouyang et al., 2022, Steinert-Threlkeld et al., 2022, Bai et al., 2022], usually the initial, pretrained model. Less common is to add the original pretraining task to the finetuning objective (termed S2P by Lowe* et al. [2021]) but this can be compute-intensive and requires maintaining the pretraining data which may be even more expensive for larger models [Brown et al., 2020]. On small-scale pivot-translation, Lu et al. [2020] propose iterated learning with student-teacher distillation but it too is relatively compute intensive. Recently, Ramamurthy et al. [2022] propose to maintain a delayed masking model and mask the LM to output only the top- $p$ tokens. Elastic Reset takes inspiration from both of these, using an iterated process and maintaining an EMA model. Apart from better performance, our method is more space efficient and maintains the EMA on CPU whereas both other methods require maintaining an extra model on GPU. It is also more compute efficient as resetting weights and EMA updates are very cheap operations, whereas Lu et al. [2020] requires a long distillation phase and Ramamurthy et al. [2022] requires an extra forward pass with the masking model. There exist less-popular methods that have been applied to similar issues in RL: prompt-tuning [Singh et al., 2022], using a fixed model to generate many options and re-ranking using a reward model [Lazaridou et al., 2020, Meta FAIR Diplomacy Team et al., 2022], and grounding the output in a separate modality [Lee et al., 2019] but none have been used for RLHF or at scale.

Our method is inspired by recent works that leverage resets for single agent RL [Nikishin et al., 2022, D'Oro et al., 2023], image classification [Zhou et al., 2022], and emergent communication [Rita et al., 2022]. Those works generally train from scratch and reset to random initializations in order to improve generalization. Our scenario requires resetting to pretrained models and focuses on improving the tradeoff between performance and drift from this pretrained model. Elastic Reset can be seen as an on-policy alternative to Nikishin et al.'s [2022] off-policy resets; whereas they maintain the old replay buffer, we maintain the value model and an EMA of our policy.

Finally, the pretrain-then-RL-finetune setup with the goal of maintaining pretrained knowledge can be seen as a two-step, RL-specific instance of continual learning and therefore language drift has links to catastrophic forgetting [McCloskey and Cohen, 1989]. There is a clear similarity between mitigation methods: rehearsal [Robins, 1995] or experience replay [Rolnick et al., 2019] is equivalent to multitasking with the pretraining objective [Lowe* et al., 2021] and weight-update regularization [Kirkpatrick et al., 2017] has similarities to KL regularization [Jaques et al., 2019].

## 3 Elastic Reset

The standard method against drift is a KL penalty, generally between the learning policy $\theta$ and the initial, pretrained model $\theta_{0}$. It is calculated empirically over the minibatch of training inputs $x$ and outputs $y$ and used as an auxiliary reward with coefficient $\beta$ on top of the regular reward model $r$

$$
\begin{equation*}
R(x, y)=r(x, y)-\beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\theta_{0}}(y \mid x)} \tag{1}
\end{equation*}
$$

For Elastic Reset, we maintain an exponential moving average $\bar{\theta}$ of our learning model $\theta$ and choose a decay hyperparameter parameter $\eta$. We initialize $\theta \leftarrow \theta_{0}$ and after every online model step, we update our EMA model $\bar{\theta} \leftarrow(1-\eta) \theta+\eta \bar{\theta}$. Every $n$ steps, Elastic Reset sets the online model to the
![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-04.jpg?height=270&width=1388&top_left_y=248&top_left_x=368)

Figure 2: The Translation Game (top left), IMDB mock sentiment task (bottom left), and StackLLaMA (right). We show all RL finetuning setups and StackLLaMA's reward modelling (top right).

EMA model $\theta \leftarrow \bar{\theta}$ and sets the EMA model to the initial model $\bar{\theta} \leftarrow \theta_{0}$. As with other methods, Elastic Reset can be easily combined with a KL penalty.

## 4 Translation Game: Careful Comparison to SOTA

Setup We first investigate the pivot-translation benchmark of Lee et al. [2019], which was previously popular for small-scale methods countering drift. Two translation models, French to English $(\mathrm{FR} \rightarrow \mathrm{EN})$ and English to German (EN $\rightarrow \mathrm{DE}$ ), are pretrained on IWSLT [Cettolo et al., 2012]. Then, the models are finetuned on translating French to German through English (FR $\rightarrow \mathrm{EN} \rightarrow \mathrm{DE}$ ) but given only paired French and German data from Multi30k [Elliott et al., 2016, 2017] as shown in Figure 2. The models are not given English at finetune-time so the challenge is optimizing FR $\rightarrow \mathrm{DE}$ while maintaining fluency in the intermediate English. Whereas larger benchmarks have only proxies for drift, we can exactly measure the performance degradation in our setup with the standard translation metric BLEU on a held-out FR $\rightarrow$ EN validation set. Similarly, we measure success on the task with the FR $\rightarrow \mathrm{EN} \rightarrow \mathrm{DE}$ BLEU score. Each model is an encoder-decoder Transformer [Vaswani et al., 2017] with 6 layers and all experimental details are available in Appendix A.

Baselines The $\mathrm{EN} \rightarrow \mathrm{DE}$ reward model is simply trained using cross-entropy between predicted and true DE. Our lower-bound baseline is FROZEN ENGLISH, we freeze the FR $\rightarrow$ EN model and only update the $\mathrm{EN} \rightarrow \mathrm{DE}$ model. This models is guaranteed not to drift, but also cannot reach the best possible performance. For that, we need to update FR $\rightarrow$ EN by backpropogating through the discrete EN tokens. We follow Lee et al. [2019] and train FR $\rightarrow$ EN using REINFORCE [Williams, 1992] to estimate the gradient. As our base model, we combine REINFORCE with an exponentially moving baseline and, as with previous work, add a loss for entropy regularization.

When both $\mathrm{FR} \rightarrow \mathrm{EN}$ and $\mathrm{EN} \rightarrow \mathrm{DE}$ are being updated, we tend to see reasonably large drift and we compare to the best previous methods that counter it on this benchmark. We follow Lee et al. [2019] to simulate the standard KL penalty method, KL PENALTY by training an LSTM LM on IWSLT English text and adding a $\mathrm{KL}$ penalty with $\beta=0.05$ to regularize the $\mathrm{FR} \rightarrow \mathrm{EN}$ model. MULTITASK learning, re-training, or S2P [Lowe* et al., 2021], adds the supervised FR $\rightarrow$ EN objective on IWSLT pretraining data as an auxiliary task for the FR $\rightarrow$ EN model. Finally, we implement Seeded Iterated Learning [SIL; Lu et al., 2020], which alternates between $n$ finetuning steps and $m$ steps of teacher-student distillation. $\mathrm{FR} \rightarrow \mathrm{EN}$ and $\mathrm{EN} \rightarrow \mathrm{DE}$ "teacher" models are finetuned on the translation game, then each distills knowledge into "student" model of itself, and finally the students are initialized as teachers for the next iteration. ELASTIC RESET is implemented on top of REINFORCE with a very minimal KL penalty $\beta=0.001$ and uses an EMA decay $\eta=0.99$. We run all models for $50 \mathrm{k}$ updates and reset every $23 \mathrm{k}$ steps to get 2 resets / 3 iterations within a run. Hyperparameters may differ between methods, e.g. $\beta$, because we used a minimal search to find the best hyperparameters for each method.

Experiments For each method, we run 5 seeds and plot the validation scores over training for the end-to-end task score, FR $\rightarrow \mathrm{EN} \rightarrow \mathrm{DE}$ BLEU, and drift score, FR $\rightarrow$ EN BLEU, in Figures 3b, 3a respectively. Following Lee et al. [2019], we also show the final validation score in Table 1. As a sanity check, FrozEN ENGLISH does not drift but also does not achieve a very high task performance. In line with previous results [Lu et al., 2020], all learning models initially improve $\mathrm{FR} \rightarrow \mathrm{EN}$ performance, likely because models are quickly, semi-supervised adapting from their pretraining (IWSLT) to the distribution of the finetune dataset (Multi30k). Afterwards, they start to overfit on their objective and FR $\rightarrow$ EN performance degrades. REINFORCE achieves the best

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-05.jpg?height=406&width=1369&top_left_y=236&top_left_x=367)

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-05.jpg?height=320&width=436&top_left_y=255&top_left_x=389)

(a) Task Performance

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-05.jpg?height=328&width=439&top_left_y=248&top_left_x=840)

(b) Language Drift

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-05.jpg?height=336&width=441&top_left_y=244&top_left_x=1292)

(c) Pareto Frontier Graph

Figure 3: Comparing Elastic Reset to all baseline methods on the Translation Game. We measure (a) Task Performance with FR $\rightarrow$ EN $\rightarrow$ DE BLEU and (b) Language Drift with FR $\rightarrow$ EN BLEU, on the validation set during finetuning. We plot the mean and standard error over 5 seeds. To compare how methods trade off the two metrics, we plot (c) the best achieved drift vs task performance.

Table 1: Translation Game final validation scores

|  | $\uparrow \mathrm{FR} \rightarrow \mathrm{EN} \rightarrow \mathrm{DE}$ | $\uparrow \mathrm{FR} \rightarrow \mathrm{EN}$ |
| :--- | :---: | :---: |
| FROZEN ENGLISH | $30.8 \pm 0.2$ | $\mathbf{3 6 . 3} \pm \mathbf{0 . 1}$ |
| REINFORCE | $\mathbf{3 3 . 2} \pm \mathbf{0 . 3}$ | $29.6 \pm 0.3$ |
| + SIL | $28.2 \pm 0.4$ | $27.3 \pm 4.4$ |
| + MULTITASK (S2P) | $32.2 \pm 0.3$ | $35.2 \pm 1.0$ |
| + KL PENALTY | $\mathbf{3 3 . 2} \pm \mathbf{0 . 2}$ | $30.8 \pm 0.4$ |
| + ELASTIC RESET | $32.9 \pm 0.1$ | $\mathbf{3 6 . 3} \pm \mathbf{0 . 1}$ |

possible task performance but drifts significantly. Despite extensive hyperparameter tuning and correspondence with the original authors, SIL does not manage to outperform the REINFORCE baseline so we exclude it from the figures for visual clarity but show values in Table 1 as well as full results in Appendix A. In line with previous work [Lee et al., 2019, Lu et al., 2020], we find that MULTITASK and KL PENALTY are both beneficial to reducing drift, but both represent a tradeoff. Whereas MULTITASK strongly reduces drift, it does not achieve a high task score. In contrast, KL PENALTY achieves a high task score but drifts quite drastically. Elastic Reset achieves nearly the best possible task score while maintaining the same drift score as the initial model. Visually, we see that our method track the baselines until the reset at $23 \mathrm{k}$ steps. After the reset, we see a slight performance drop but also a big jump back in terms of FR $\rightarrow$ EN drift. While the task performance recovers within $5 \mathrm{k}$ steps, the drift performance does not degrade to previous levels. For the second reset, the EMA model is slightly more drifted and so the reset is less pronounced for both task and drift, leading to faster task recovery but slightly more drift. Overall, Elastic Reset shows state-of-the-art results on the benchmark and outperforms all previous small-scale methods.

## 5 Pareto Frontier Graph

Simply evaluating validation curves side-by-side or looking at a table of final scores, it can be unclear which method is better if one drifts less but the other achieves a higher reward e.g. MULTITASK vs KL PENALTY in Table 1. Previous work on this [Lee et al., 2019] and other benchmarks [Ramamurthy et al., 2022] compare methods using simple point-estimates after training for a specific number of epochs. But this number of epochs is quite arbitrary as models never fully converge to a reward, they are early-stopped such that drift is not catastrophic. Since different setups may admit different levels of drift, we believe that evaluation should reflect the continuous tradeoff between task and drift. We extend Ziegler et al. [2019], and create a pareto frontier graph to plot each method's achieved task score vs drift metric on the validation set over training. We believe practioners will wish to choose the best model for some given task performance so, contrary to Ziegler et al. [2019], we plot the task score on $x$-axis and drift score on the $y$-axis. Improvement on a drift metric can either mean lower scores (perplexity) or higher scores (BLEU) so we always plot task score as increasing from bottom to top such that, graphically, a better method will functionally dominate a worse method. We plot the best achieved reward vs drift over all validation steps for the Translation Game in Figure 3c. Not only

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-06.jpg?height=410&width=1374&top_left_y=234&top_left_x=365)

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-06.jpg?height=325&width=442&top_left_y=244&top_left_x=386)

(a) Task Performance

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-06.jpg?height=336&width=444&top_left_y=241&top_left_x=838)

(b) Language Drift

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-06.jpg?height=325&width=450&top_left_y=241&top_left_x=1285)

(c) Pareto Frontier Graph

Figure 4: Plotting PPO vs Elastic Reset on IMDB but splitting the results visually between resets. We measure (a) Language Drift and (b) Task Performance via Semantic Score on the validation set over finetuning. All methods also include a KL penalty. We plot mean and standard error across 5 seeds.

Table 2: IMDB mock sentiment final test scores

|  | $\uparrow$ SENTIMENT | $\downarrow$ PERPLEXITY |
| :--- | :---: | :---: |
| ZERO-SHOT | $.489 \pm 0.01$ | $32.45 \pm 0.13$ |
| PPO | $.596 \pm 0.02$ | $33.45 \pm 0.40$ |
| NLPO | $.558 \pm 0.06$ | $33.12 \pm 0.74$ |
| ELASTIC RESET | $.611 \pm 0.02$ | $33.32 \pm 0.23$ |

does Elastic Reset outperform the baselines at the final validation score, but it functionally dominates such that it is the best method for all levels of task performance it achieves.

## 6 IMDB Mock Sentiment: Ablation Study for RLHF

Setup Next, we scale to a larger benchmark that more closely approximates the standard RLHF setup. We use the recently released GRUE benchmark for RL training of LMs [Ramamurthy et al., 2022] and use IMDB mock sentiment [Ziegler et al., 2019], the main task where language models are susceptible to reward-hacking, shown in Figure 2. The goal is to complete an IMDB movie review with as positive a sentiment as possible. The baseline LM is GPT-2 [Radford et al., 2019] with 117M parameters further pretrained on the IMDB domain [Maas et al., 2011]. We learn a DistilBERT [Sanh et al., 2020] reward model on IMDB to output a sentiment score between 0 (negative) and 1 (positive). We then train our GPT-2 LM to complete different IMDB reviews while maximizing the sentiment reward. Following Ramamurthy et al. [2022], we measure reward-hacking / drift with our model's perplexity on the true IMDB data. If we consider knowledge of the IMDB data as a useful capability, then our initial model was finetuned on IMDB to maximize log-probability, i.e. minimize perplexity, and has the maximum capabilites. We measure divergence from the initial model, and decrease in capabilities, by the increase in our trained model's perplexity on ground truth IMDB data. In contrast to the previous task, a lower perplexity score corresponds to less drift.

Baselines Our main baseline is PPO [Schulman et al., 2017] with Ziegler et al. [2019] modifications for RLHF training, specifically adding a KL penalty with the frozen initial model (equivalent to KL WITH PRETRAINED) and dynamically decaying the coefficient $\beta$ over training. To further increase stability, Generalized Advantage Estimation [Schulman et al., 2015] is used for the advantage estimator. We also compare to NLPO [Ramamurthy et al., 2022], a recent method that extends PPO with a masking model to counteract drift. The masking model is initialized to the pretrained model and recieves delayed updates; it is set to the online model every $n$ steps. During training, the online model's output probabilities are restricted to the mask model's top $p$ tokens. We use the RL4LMs library Ramamurthy et al. [2022] and their default hyperparameters for both PPO and NLPO e.g. $\beta=0.1$. We implement ElASTIC RESET on top of PPO with an EMA decay rate of 0.995 and greatly reduce the $\mathrm{KL}$ coefficient $\beta=0.001$ to allow the model to drift more, then reset every 17 epochs such that we get two resets / three iterations during our training.

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-07.jpg?height=973&width=1269&top_left_y=224&top_left_x=428)

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-07.jpg?height=414&width=559&top_left_y=243&top_left_x=447)

(a) Reset Ablation

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-07.jpg?height=412&width=555&top_left_y=713&top_left_x=449)

(c) $\mathrm{KL} \beta$ Ablation for PPO

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-07.jpg?height=412&width=566&top_left_y=241&top_left_x=1105)

(b) EMA Decay $\eta$ Ablation

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-07.jpg?height=418&width=569&top_left_y=707&top_left_x=1106)

(d) $\operatorname{KL} \beta$ Ablation for Elastic Reset

Figure 5: Ablating Elastic Reset on the IMDB mock sentiment task. We plot pareto graphs using mean and standard error across 5 seeds.

Experiments We run all experiments for 5 seeds and report mean and standard error on our validation set for our reward, DistilBERT sentiment, and our drift score, perplexity. Following Ramamurthy et al. [2022], we run for 50 epochs (equivalent to $64 \mathrm{k}$ updates) and show our results in Figure 4. To make our resets more visible, we plot validation scores every epoch for Elastic Reset. Since the benchmark provides a test set as well, we compare all final models in Table 2. The PPO baseline performs quite well because it already includes a KL with the pretrained model. We find NLPO performs similarly to PPO, so we relegate NLPO results to Appendix B. Results from the original NLPO paper were stronger [Ramamurthy et al., 2022] but our reproduced numbers and curves were confirmed by the original authors [Ammanabrolu, 2023]. Elastic Reset achieves better semantic scores much faster by using a smaller KL penalty coefficient ( 0.001 vs PPO 0.1) but also drifts more to achieve them. As with the previous task, this drift is then mitigated by the reset and we see semantic task score improve in relation to drift over the iterations. Looking at the pareto graph in Figure 8c, we see that Elastic Reset far outpaces the baselines and provides a better tradeoff of reward vs drift for every reward.


#### Abstract

Ablations We empirically investigate our method through ablations. Throughout this section we run experiments on IMDB with the same hyperparameters, unless otherwise mentioned. For brevity, we plot only the pareto graphs but include all other graphs in Appendix D. 3 along with these same ablation experiments for the Translation Game, with similar results.

To investigate the source of improvement in Elastic Reset, we ablate the two resets: online to EMA, and EMA to initial model. We discard the second reset to get Reset to EMA: our model is reset to an EMA but the EMA is never reset. We also compare to the simplest reset idea, Reset to Init, and reset our policy to the initial model. We run all methods as previously and plot the pareto graph in Figure 5a, for task and drift graphs see Appendix D.3. We find that even simple resets are already performant but the two ablations have a tradeoff: Reset to EMA is better at lower reward because it maintains performance whereas Reset to Init does better at higher reward because it doesn't drift as much. Elastic Reset combines the benefits of both and outperforms each method.


Next, we consider our method's robustness to hyperparameters. First, we search along different EMA decay rates $\eta$ and plot our results in Figure 5b finding that our method is quite robust to choice of

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-08.jpg?height=414&width=1374&top_left_y=232&top_left_x=365)

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-08.jpg?height=331&width=439&top_left_y=244&top_left_x=388)

(a) Reward

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-08.jpg?height=338&width=447&top_left_y=243&top_left_x=839)

(b) Drift

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-08.jpg?height=331&width=458&top_left_y=241&top_left_x=1278)

(c) Pareto Frontier Graph

Figure 6: Elastic Reset compared to PPO on StackLLaMA: A LLaMA-7B model RLHF finetuned on StackExchange as a helpful, technical QA chatbot

decay. Next, we investigate robustness to the choice of KL penalty coefficient $\beta$. We search across coefficients that range from $10 \mathrm{x}$ smaller to $10 \mathrm{x}$ larger than our best KL penalty coefficient for PPO $(\beta=0.1)$ and Elastic Reset $(\beta=0.001)$. For visual clarity, we plot PPO in Figure 5c and Elastic Reset in Figure 5d and only plot four points for PPO $\beta=0,0.01$ to maintain visual scale. We find that PPO is not robust to choice of $\mathrm{KL}$ and larger values correspond to better pareto curves but slower training. Results with NLPO are similar and shown in Appendix D.3. In contrast, Elastic Reset seems to be more robust to choice of KL with 0.001 producing the best curves while $10 \mathrm{x}$ larger and smaller values are similar. As opposed to PPO, Elastic Reset even works reasonably well without a KL penalty at all, $(\beta=0)$, matching PPO's best performance with a KL. This demonstrates that the expensive KL penalty may be replaced with the cheap Elastic Reset, although the combination of the two is best. This is also in line with previous work that have argued that the KL penalty may be unnecessary for RLHF [Bai et al., 2022, Gao et al., 2022]. We also ablate the frequency of resets in Appendix D.4.1 and find that pareto curves are essentially unchanged.

Finally, we provide an empirical intuition for Elastic Reset: in Appendix E. 1 we show that resets iteratively improve the value function and in Appendix E. 2 we show how EMA smoothes optimization but requires resetting in order to achieve high performance.

## 7 StackLLaMA: Practical RLHF

Setup Finally, we apply Elastic Reset to a larger-scale RLHF pipeline. We choose LLaMA [Touvron et al., 2023] as it is a prominent open-source model that has demonstrated strong performance on benchmarks. We follow Beeching et al. [2023] to finetune LLaMA-7B with RLHF on the StackExchange dataset [Lambert et al., 2023] to output helpful answers to technical questions, as judged by humans. Users ask technical questions on StackExchange and upvote the best answers. We score answers from StackExchange based on the number of upvotes they received from users, score $=\log _{2}(1+$ upvotes [Askell et al., 2021] . At most 10 answers are drawn per question, text is cleaned, and HTML is converted to Markdown to make it easier to parse. First, we finetune LLaMA-7B with language modelling on the dataset to get LLaMA-7B-SE. We then further finetune it to get a reward model by learning to predict which of two answers was more upvoted [Stiennon et al., 2020]. For a given question $x$ and two answers $y_{+,-}$(where $y_{+}$is preferred), the loss for our reward model $r_{\theta}$ is $\log \sigma\left(r_{\theta}\left(x, y_{+}\right)-r_{\theta}\left(x, y_{-}\right)\right)$. Finally, we finetune LLaMA-7B-SE with RL against the reward model by sampling questions from the dataset and learning to optimize the reward for our model's answer. All finetuning is done with a converted 8-bit model [Dettmers et al., 2022] and LoRA [Hu et al., 2021] for efficiency and to make the model training fit on our GPUs. We rely on the HuggingFace trl [von Werra et al., 2023] and peft [Mangrulka et al., 2023] libraries. All technical details are described in Appendix C.

Experiment We again compare to PPO with Ziegler et al. [2019] modifications i.e. KL penalty with a dynamically decaying coefficient. We run for 600 epochs (equivalent to 100k updates) and Elastic Reset every 260 epochs to get two resets / three iterations. Each run takes 20 hours on 4 A100s. Since only the LoRA parameters are being learned, we use Elastic Reset on those and therefore maintain only a small percentage of parameters in our EMA. We use a decay rate $\eta=0.995$ and a KL penalty coefficient $\beta=0.02$ for both methods. Calculating perplexity for each epoch is computationally

Table 3: Evaluations of the initial (zero-shot) and finetuned StackLLaMA models after 600 epochs. We measure alignment using an average over three reward model trained with three different seeds and drift with perplexity on the data. HumanEval is a programming benchmark that acts as a practical measure of drift / alignment tax.

|  | $\uparrow \Delta$ REWARD | $\downarrow$ PERPLEXITY | $\uparrow$ HUMANEVAL (PASS @ 1,PASS @ 10) |
| :--- | :---: | :---: | :---: |
| ZERO-SHOT | 0 | 4.43 | $11.0,12.7$ |
| PPO | $0.81 \pm 0.06$ | 4.62 | $7.8,10.7$ |
| ELASTIC RESET | $0.96 \pm 0.09$ | 4.57 | $11.0,13.0$ |

infeasible so we measure drift during training with the $\mathrm{KL}$ from the pretrained model over samples as done previously in other larger-scale RLHF [Bai et al., 2022, Gao et al., 2022].

Results We plot reward in Figure $6 a^{2}$ and $\mathrm{KL}$ from initial model over training in Figure 6b. As noted by Beeching et al. [2023], the task is much noisier at a larger scale and with a real HF reward model. As a sanity check, we find that Elastic Reset tracks PPO until the first reset at 260 epochs where it drops only slightly, but also doesn't lose much performance. Around the second reset at 520 epochs, we see a much sharper drop but also maintaining the same approximate reward. At the end, Elastic Reset provides a non-trivial reduction in drift while aligning just as well as PPO. The pareto curve in Figure 6c shows Elastic Reset is equal or slightly worse at low reward but shows large improvements over PPO at higher reward. Notably, Elastic Reset seems to work out-of-the-box with LoRA. To evaluate drift another way, we get the perplexity of the final models over the StackExchange validation set as in Section 6. For a more robust view of reward, we train two more reward models using different seeds and evaluate the increase in reward between initial and final models. We show mean and standard deviation across the three reward models in Table 3, we find that Elastic Reset achieves a slightly better final reward than PPO while maintaining lower perplexity on the data. To examine a true alignment tax, we run our models on HumanEval Chen et al. [2021], a programming benchmark that provides another view of drift. Answering human-written coding questions is both a useful capability for our model and also falls within a similar domain to StackExchange. The benchmark tests for functional correctness such that pass @ 1 corresponds to the percentage of problems solved by the model on the first try as shown in Table 3. Training with PPO degrades performance compared to the initial model, demonstrating a large alignment tax. In contrast, Elastic Reset achieves a similar reward but maintains performance, even slightly improving on pass @ 10, creating an alignment bonus [Askell et al., 2021] instead of tax.

## 8 Limitations

As a method, Elastic Reset is quite cheap computationally because both EMA updates and resets take negligable time compared to RLHF training and the EMA model can be stored on CPU. But our method is sensitive to the choice of reset rate; we chose heuristically based on when it seemed the model was overfitting. It is also possible to reset the policy and EMA model at different time scales, which could be a source of improvement. Our method also resets all of the trainable parameters, research in similar methods suggests that resetting larger models can benefit from resetting only part of the network [Zhou et al., 2022, Nikishin et al., 2022] or weighted-averaging instead of resets [D'Oro et al., 2023]. We leave both of these directions to future work.

Although we have thoroughly investigated our method on three different tasks, we note that none of them are ideal RLHF benchmarks. As pointed out by Gao et al. [2022], we measure our model's performance using the same reward model we optimize. This can lead to reward model overoptimization and our metric could mask overfitting and lack of generalization to the real world i.e. actual human preferences. An ideal benchmark could include a "gold" reward model as a proxy for human preference [Gao et al., 2022], but no such benchmarks are open-sourced and available.[^1]

Finally, we note that we follow all previous RLHF work and investigate only on-policy methods [Ziegler et al., 2019, Stiennon et al., 2020, Askell et al., 2021, Bai et al., 2022]. Previous work in resetting for RL has focused on off-policy methods and demonstrated strong performance [Nikishin et al., 2022, D' Oro et al., 2023]. As previously noted, our method can be seen as an adaptation of those to on-policy RL. In RLHF, PPO is by far the most popular method and on-policy is the dominant paradigm since it guarantees better local gradients. But it is possible that off-policy methods could implicitly balance performance and drift by incorporating a replay buffer with older data.

## 9 Conclusion

The problems of drift [Lee et al., 2019], alignment tax [Askell et al., 2021], reward model overoptimization [Gao et al., 2022], and reward hacking [Clark and Amodei, 2016] are inherent to RLHF and reduce its efficacy. We have introduced a simple but powerful new method, Elastic Reset, to tackle this problem and improve performance while maintaining linguistic capabilities. We have shown its ability on three different tasks and across three different scales: from 6 layer Transformers to GPT2 to LLaMA-7B. The problem of drift is currently being addressed with a standard KL penalty despite the computational cost, tradeoff with reward, and recent claims that is may be unnecessary [Bai et al., 2022, Gao et al., 2022]. Elastic Reset is a cheap and effective method to tackle the same problem, achieving a better tradeoff of reward and drift while reducing alignment tax. We hope our method leads to better RLHF and therefore models that are closer aligned with human preferences [Ziegler et al., 2019]. As well, we hope this work invigorates more research into improving the reward / drift tradeoff of RLHF with a focus on computationally efficient methods that scale.

## Acknowledgments and Disclosure of Funding

MN is supported by Fonds de recherche du Québec - Nature et technologies and Sony. MN would like to thank Issam Laradji, ServiceNow Research, Mila, and Compute Canada for providing resources used in the experiments.

## References

P. Ammanabrolu. Re: Reproducing NLPO (RL4LMs), Jan. 2023.

J. Andreas, D. Klein, and S. Levine. Learning with Latent Language. In NAACL. arXiv, 2018. doi: 10.48550/arXiv.1711.00482. URL http://arxiv.org/abs/1711.00482. arXiv:1711.00482 [cs].

A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, and J. Kaplan. A General Language Assistant as a Laboratory for Alignment, Dec. 2021. URL http://arxiv.org/abs/2112. 00861. arXiv:2112.00861 [cs].

D. Bahdanau, K. Cho, and Y. Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. In ICLR. arXiv, 2015. URL http://arxiv.org/abs/1409.0473. arXiv:1409.0473 [cs, stat].

Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. HatfieldDodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Apr. 2022. URL http://arxiv.org/abs/2204.05862. arXiv:2204.05862 [cs].

E. Beeching, Y. Belkada, K. Rasul, L. Tunstall, L. v. Werra, N. Rajani, and N. Lambert. StackLLaMA: An RL Fine-tuned LLaMA Model for Stack Exchange Question and Answering, 2023. URL https://huggingface.co/blog/stackllama.

E. M. Bender and A. Koller. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185-5198, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.463. URL https://aclanthology.org/2020.acl-main. 463.

T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language Models are Few-Shot Learners. In Neural Information Processing Systems. arXiv, July 2020. doi: 10.48550/arXiv.2005.14165. URL http://arxiv.org/abs/2005.14165. arXiv:2005.14165 [cs].

M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging Properties in Self-Supervised Vision Transformers, May 2021. URL http://arxiv.org/abs/ 2104.14294. arXiv:2104.14294 [cs].

M. Cettolo, C. Girardi, and M. Federico. WIT3: Web Inventory of Transcribed and Translated Talks. In Proceedings of the 16th Annual conference of the European Association for Machine Translation, pages 261-268, Trento, Italy, May 2012. European Association for Machine Translation. URL https://aclanthology.org/2012.eamt-1.60.

M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating Large Language Models Trained on Code, July 2021. URL http://arxiv.org/abs/ 2107.03374. arXiv:2107.03374 [cs].

P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep Reinforcement Learning from Human Preferences. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://papers.nips.cc/paper/2017/ hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.

J. Clark and D. Amodei. Faulty Reward Functions in the Wild, Dec. 2016. URL https://openai . com/blog/faulty-reward-functions/.

T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. In NeurIPS. arXiv, Nov. 2022. doi: 10.48550/arXiv.2208.07339. URL http://arxiv.org/abs/2208.07339. arXiv:2208.07339 [cs].

P. D'Oro, M. Schwarzer, E. Nikishin, P.-L. Bacon, M. G. Bellemare, and A. Courville. SampleEfficient Reinforcement Learning by Breaking the Replay Ratio Barrier. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= $4 \mathrm{GBGwVIEYJ.}$

D. Elliott, S. Frank, K. Sima' an, and L. Specia. Multi30K: Multilingual English-German Image Descriptions. In Proceedings of the 5th Workshop on Vision and Language, pages 70-74, Berlin, Germany, 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-3210. URL http://aclweb.org/anthology/W16-3210.

D. Elliott, S. Frank, L. Barrault, F. Bougares, and L. Specia. Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description. In Proceedings of the Second Conference on Machine Translation, pages 215-233, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4718. URL https: //aclanthology.org/W17-4718.

L. Gao, J. Schulman, and J. Hilton. Scaling Laws for Reward Model Overoptimization, Oct. 2022. URL http://arxiv.org/abs/2210.10760. arXiv:2210.10760 [cs, stat].

S. Gugger, L. Debut, T. Wolf, P. Schmid, Z. Mueller, S. Mangrulkar, M. Sun, and B. Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable, 2022. URL https://github.com/huggingface/accelerate.

S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735-1780, Nov. 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10. 1162/neco.1997.9.8.1735.

E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-Rank Adaptation of Large Language Models, Oct. 2021. URL http://arxiv.org/abs/2106. 09685. arXiv:2106.09685 [cs].

J. D. Hunter. Matplotlib: A 2D Graphics Environment. Computing in Science \& Engineering, 9(3): 90-95, May 2007. ISSN 1558-366X. doi: 10.1109/MCSE.2007.55. URL https://ieeexplore. ieee.org/document/4160265. Conference Name: Computing in Science \& Engineering.

E. Jang, S. Gu, and B. Poole. Categorical Reparameterization with Gumbel-Softmax. In ICLR. arXiv, Aug. 2017. doi: 10.48550/arXiv.1611.01144. URL http://arxiv.org/abs/1611.01144. arXiv:1611.01144 [cs, stat].

N. Jaques, S. Gu, D. Bahdanau, J. M. Hernández-Lobato, R. E. Turner, and D. Eck. Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control. In International Conference on Machine Learning. arXiv, Oct. 2017. doi: 10.48550/arXiv.1611.02796. URL http://arxiv.org/abs/1611.02796. arXiv:1611.02796 [cs].

N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, A. Lapedriza, N. Jones, S. Gu, and R. Picard. Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog, July 2019. URL http://arxiv.org/abs/1907.00456. arXiv:1907.00456 [cs, stat].

S. Kirby. Spontaneous evolution of linguistic structure-an iterated learning model of the emergence of regularity and irregularity. IEEE Transactions on Evolutionary Computation, 5(2):102-110, Apr. 2001. ISSN 1941-0026. doi: 10.1109/4235.918430. Conference Name: IEEE Transactions on Evolutionary Computation.

J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526, Mar. 2017. doi: 10.1073/pnas.1611835114. URL https: //www.pnas.org/doi/10.1073/pnas.1611835114. Publisher: Proceedings of the National Academy of Sciences.

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177-180, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL https://aclanthology.org/P07-2045.

N. Lambert, L. Tunstall, N. Rajani, and T. Thrush. HuggingFace H4 Stack Exchange Preference Dataset, 2023. URL https://huggingface.co/datasets/HuggingFaceH4/ stack-exchange-preferences.

A. Lazaridou and M. Baroni. Emergent Multi-Agent Communication in the Deep Learning Era, July 2020.

A. Lazaridou, A. Potapenko, and O. Tieleman. Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning. In ACL. arXiv, May 2020. doi: 10.48550/arXiv.2005.07064. URL http://arxiv.org/abs/2005.07064. arXiv:2005.07064 $[\mathrm{cs}]$.

J. Lee, K. Cho, and D. Kiela. Countering Language Drift via Visual Grounding, Sept. 2019. URL http://arxiv.org/abs/1909.04499. arXiv:1909.04499 [cs].

M. Lewis, D. Yarats, Y. N. Dauphin, D. Parikh, and D. Batra. Deal or No Deal? End-to-End Learning for Negotiation Dialogues, June 2017. URL http://arxiv.org/abs/1706. 05125. arXiv:1706.05125 [cs] version: 1.

Q. Lhoest, A. V. del Moral, Y. Jernite, A. Thakur, P. von Platen, S. Patil, J. Chaumond, M. Drame, J. Plu, L. Tunstall, J. Davison, M. Šaško, G. Chhablani, B. Malik, S. Brandeis, T. L. Scao, V. Sanh, C. Xu, N. Patry, A. McMillan-Major, P. Schmid, S. Gugger, C. Delangue, T. Matussière, L. Debut, S. Bekman, P. Cistac, T. Goehringer, V. Mustar, F. Lagunas, A. M. Rush, and T. Wolf. Datasets: A Community Library for Natural Language Processing, Sept. 2021. URL http: //arxiv.org/abs/2109.02846. arXiv:2109.02846 [cs].

F. Li and M. Bowling. Ease-of-Teaching and Language Structure from Emergent Communication. In NeurIPS, 2019. URL http://arxiv.org/abs/1906.02403. arXiv: 1906.02403.

I. Loshchilov and F. Hutter. Decoupled Weight Decay Regularization. In International Conference on Learning Representations, Feb. 2022. URL https://openreview.net/forum?id= Bkg6RiCqY7.

R. Lowe*, A. Gupta*, J. Foerster, D. Kiela, and J. Pineau. On the interaction between supervision and self-play in emergent communication. In International Conference on Learning Representations, Sept. 2021. URL https://openreview.net/forum?id=rJxGLlBtwH.

Y. Lu, S. Singhal, F. Strub, O. Pietquin, and A. Courville. Countering Language Drift with Seeded Iterated Learning. In ICML. arXiv, Aug. 2020. doi: 10.48550/arXiv.2003.12694. URL http: //arxiv.org/abs/2003.12694. arXiv:2003.12694 [cs].

A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL https://aclanthology. org/P11-1015.

C. J. Maddison, A. Mnih, and Y. W. Teh. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. In ICLR. arXiv, Mar. 2017. doi: 10.48550/arXiv.1611.00712. URL http://arxiv.org/abs/1611.00712. arXiv:1611.00712 [cs, stat].

S. Mangrulka, S. Gugger, L. Debut, Y. Belkada, and S. Paul. PEFT: State-of-the-art ParameterEfficient Fine-Tuning methods, May 2023. URL https://github.com/huggingface/peft. original-date: 2022-11-25T03:51:09Z.

M. McCloskey and N. J. Cohen. Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. In G. H. Bower, editor, Psychology of Learning and Motivation, volume 24, pages 109-165. Academic Press, Jan. 1989. doi: 10.1016/S0079-7421(08)60536-8. URL https: //www.sciencedirect.com/science/article/pii/S0079742108605368.

Meta FAIR Diplomacy Team, A. Bakhtin, N. Brown, E. Dinan, G. Farina, C. Flaherty, D. Fried, A. Goff, J. Gray, H. Hu, A. P. Jacob, M. Komeili, K. Konath, M. Kwon, A. Lerer, M. Lewis, A. H. Miller, S. Mitts, A. Renduchintala, S. Roller, D. Rowe, W. Shi, J. Spisak, A. Wei, D. Wu, H. Zhang, and M. Zijlstra. Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Science, Nov. 2022. doi: 10.1126/science.ade9097. URL https: //www.science.org/doi/10.1126/science.ade9097. Publisher: American Association for the Advancement of Science.

E. Nikishin, M. Schwarzer, P. D'Oro, P.-L. Bacon, and A. Courville. The Primacy Bias in Deep Reinforcement Learning. In Proceedings of the 39th International Conference on Machine Learning, pages 16828-16847. PMLR, June 2022. URL https://proceedings.mlr.press/ v162/nikishin22a.html. ISSN: 2640-3498.

OpenAI. ChatGPT: Optimizing Language Models for Dialogue, Nov. 2022. URL https://webcache.googleusercontent.com/search?q=cache:qLONB_tyjdcJ:https: //openai.com/blog/chatgpt/\&cd=1\&hl=en\&ct=clnk\&gl=ca.

M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North, pages 48-53, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL http://aclweb.org/anthology/N19-4009.

L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback, Mar. 2022. URL http://arxiv.org/abs/2203.02155. arXiv:2203.02155 [cs].

A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library, Dec. 2019. URL http://arxiv.org/abs/1912.01703. arXiv:1912.01703 [cs, stat].

M. Post. A Call for Clarity in Reporting BLEU Scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191, Brussels, Belgium, Oct. 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology . org/W18-6319.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language Models are Unsupervised Multitask Learners, 2019. URL https://www.semanticscholar.org/ paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/ 9405cc0d6169988371b2755e573cc28650d14dfe.

R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi, and Y. Choi. Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization. In ICLR. arXiv, 2022. URL http://arxiv.org/abs/2210.01241. arXiv:2210.01241 [cs].

M. Rita, C. Tallec, P. Michel, J.-B. Grill, O. Pietquin, E. Dupoux, and F. Strub. Emergent Communication: Generalization and Overfitting in Lewis Games, Sept. 2022. URL http: //arxiv.org/abs/2209.15342. arXiv:2209.15342 [cs, math].

A. Robins. Catastrophic Forgetting, Rehearsal and Pseudorehearsal. Connection Science, 7 (2):123-146, June 1995. ISSN 0954-0091. doi: 10.1080/09540099550039318. URL https://doi.org/10.1080/09540099550039318. Publisher: Taylor \& Francis _eprint: https://doi.org/10.1080/09540099550039318.

D. Rolnick, A. Ahuja, J. Schwarz, T. P. Lillicrap, and G. Wayne. Experience Replay for Continual Learning, Nov. 2019. URL http://arxiv.org/abs/1811.11682. arXiv:1811.11682 [cs, stat].

V. Sanh, L. Debut, J. Chaumond, and T. Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, Feb. 2020. URL http://arxiv.org/abs/1910.01108. arXiv:1910.01108 $[\mathrm{cs}]$.

J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. Trust Region Policy Optimization. In ICML. arXiv, 2015. doi: 10.48550/arXiv.1502.05477. URL http://arxiv.org/abs/1502 . 05477. arXiv: 1502.05477 [cs].

J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal Policy Optimization Algorithms, Aug. 2017. URL http://arxiv.org/abs/1707.06347. arXiv:1707.06347 [cs].

A. K. Singh, D. Ding, A. Saxe, F. Hill, and A. K. Lampinen. Know your audience: specializing grounded language models with the game of Dixit, June 2022. URL http://arxiv.org/abs/ 2206 . 08349. arXiv:2206.08349 [cs].

S. Steinert-Threlkeld, X. Zhou, Z. Liu, and C. M. Downey. Emergent Communication Fine-tuning (EC-FT) for Pretrained Language Models. In Emergent Communication Workshop at ICLR 2022, June 2022. URL https://openreview.net/forum?id=SUqrM7WR7W5.

N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. Christiano. Learning to summarize from human feedback. In NeurIPS. arXiv, 2020. URL http://arxiv.org/abs/2009.01325. arXiv:2009.01325 [cs].

I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to Sequence Learning with Neural Networks. In Neural Information Processing Systems. arXiv, Dec. 2014. URL http://arxiv.org/abs/1409 . 3215. arXiv:1409.3215 [cs].

H. Q. To, N. D. Q. Bui, and M. Nguyen. CodeCapybara: Open Source LLaMA Model that Follow Instruction-Tuning for Code Generation., May 2023. URL https://github.com/ FSoft-AI4Code/CodeCapybara. original-date: 2023-04-21T10:28:53Z.

H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. LLaMA: Open and Efficient Foundation Language Models, Feb. 2023. URL http://arxiv.org/abs/2302.13971. arXiv:2302.13971 [cs] version: 1.

M. Utiyama and H. Isahara. A Comparison of Pivot Methods for Phrase-Based Statistical Machine Translation. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 484-491, Rochester, New York, Apr. 2007. Association for Computational Linguistics. URL https://aclanthology.org/N07-1061.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is All you Need. In Neural Information Processing Systems, page 11, 2017.

L. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, and N. Lambert. TRL: Transformer Reinforcement Learning, 2023. URL https://github.com/lvwerra/trl.

R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, page 28, 1992.

T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, Oct. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6.

H. Zhou, A. Vani, H. Larochelle, and A. Courville. Fortuitous Forgetting in Connectionist Networks. In International Conference on Learning Representations, Mar. 2022. URL https: //openreview.net/forum?id=ei3SY1_zYsE.

D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-Tuning Language Models from Human Preferences, 2019. URL http://arxiv.org/abs/ 1909 .08593. arXiv:1909.08593 [cs, stat].

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-16.jpg?height=374&width=1374&top_left_y=230&top_left_x=365)

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-16.jpg?height=282&width=439&top_left_y=255&top_left_x=388)

(a) Task Performance

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-16.jpg?height=285&width=441&top_left_y=253&top_left_x=842)

(b) Language Drift

![](https://cdn.mathpix.com/cropped/2024_06_04_f8a41fe9ad640e713859g-16.jpg?height=285&width=441&top_left_y=253&top_left_x=1292)

(c) Pareto Frontier Graph

Figure 7: All transformer model methods on the Translation Game. We measure (a) Task Performance with FR $\rightarrow \mathrm{EN} \rightarrow \mathrm{DE}$ BLEU and (b) Language Drift with FR $\rightarrow$ EN BLEU, on the validation set during finetuning. We plot the mean and show error bars for standard deviation over 5 seeds. To compare how methods do on both metrics, we plot (c) the best achieved drift vs task performance across finetuning.
