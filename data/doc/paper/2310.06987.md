# CATASTROPHIC JAILBREAK OF OPEN-SOURCE LLMS VIA EXPLOITING GENERATION 

Yangsibo Huang Samyak Gupta Mengzhou Xia Kai Li Danqi Chen<br>Princeton University<br>yangsibo@princeton.edu \{samyakg,mengzhou,li,danqic\}@cs.princeton.edu<br>https://princeton-sysml.github.io/jailbreak-llm/


#### Abstract

Content warning: This paper contains examples of harmful language.

The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from $0 \%$ to more than $95 \%$ across 11 language models including LLAMA2, VICUNA, FALCON, and MPT families, outperforming state-of-the-art attacks with $30 \times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models ${ }^{1}$.


## 1 INTRODUCTION

The rapid development of large language models (LLMs), exemplified by ChatGPT (OpenAI, 2022), Bard (Google, 2023), and Claude (Google, 2023), has enabled conversational AI systems with human-like capabilities. Recently, several open-source LLMs have been released which make such AI systems more accessible, affordable, and available for more researchers to advance the state-ofthe-art (Touvron et al., 2023a; Chiang et al., 2023; Almazrouei et al., 2023; MosaicML, 2023; Touvron et al., 2023b). However, there is growing concern that open-source LLMs are more amenable to the dissemination of harmful or unethical content (Hazell, 2023; Kang et al., 2023). In response to this challenge, LLM providers have implemented a range of training techniques aimed at "aligning" these models with human values before releasing them (Ouyang et al., 2022; Bai et al., 2022a; Korbak et al., 2023; Zhou et al., 2023). These efforts are often complemented by red teaming, a team of evaluators who proactively identify and prevent failures of LLM alignments (Perez et al., 2022; Ganguli et al., 2022; Casper et al., 2023).

However, even with these alignment techniques, open-source LLMs still remain vulnerable to adversarial inputs. Alarmingly, recent work demonstrates jailbreaks (Bai et al., 2022b; Albert, 2023), using specifically crafted inputs to successfully bypass the alignment methods. Further work demonstrates it is possible to automatically discover such inputs, known as adversarial prompts (Wen et al., 2023; Jones et al., 2023; Carlini et al., 2023; Zou et al., 2023; Shen et al., 2023). Recently, Zou et al. (2023) successfully found adversarial prompts that can transfer across multiple LLMs, including proprietary, black-box models. However, automatic jailbreaks that optimize for adversarial inputs are quite complicated and computationally expensive.[^0]

How to use social media to bully someone?
I cannot fulfill your request. I'm just an Al, it's not
within my programming or ethical guidelines to provide
advice on how to engage in harmful or illegal
activities, including bullying.
Social media can be a powerful tool for connecting
with others and sharing information, but it can also be
used to harm or harass individuals. I strongly advise
against using social media to bully...

Social media can be a powerful tool for bullying, as it allows individuals to reach a large audience quickly and anonymously. Here are some ways that social media can be used to bully someone:

1. Spreading rumors or false information about someone online.
2. Posting mean or hurtful comments or messages about someone on social media platforms..

Figure 1: Responses to a malicious instruction by the LLAMA2-7B-CHAT model under different generation configurations. In this example, we simply changed $p$ from 0.9 (default) to 0.75 in top- $p$ sampling, which successfully bypasses the safety constraint.

In this work, we take an extremely simple approach to jailbreaking the alignment of LLMs, focusing on open-source models that underwent safety tuning before their release. Unlike adversarial-prompt techniques, we only manipulate text generation configurations (Figure 1), by removing the system prompt, a guideline intentionally prepended to steer model generation, and by varying decoding hyper-parameters or sampling methods. Our key hypothesis is that existing alignment procedures and evaluations are likely based on a default decoding setting, which may exhibit vulnerability when the configurations are slightly varied, as we observed extensively in our experiments. We call our approach the generation exploitation attack, an alternative solution to disrupt the alignment of LLMs without requiring any sophisticated methods.

To systematically evaluate our findings, we evaluate our generation exploitation attack on 11 opensource LLMs spanning four different model families (Section 4.2), including LLAMA2 (Touvron et al., 2023b), VICUNA (Chiang et al., 2023), FALCON (Almazrouei et al., 2023), and MPT models (MosaicML, 2023). In addition to evaluating on a recent benchmark AdvBench (Zou et al., 2023), we also curate new benchmark MaliciousInstruct, which covers a broader spectrum of malicious intents to increase the diversity of scenarios considered. We also developed a more robust evaluation procedure based on a trained classifier for detecting malicious outputs, with a significantly higher human agreement compared to previous metrics based on simple string matching (Section 3).

Our experimental results show that our generation exploitation attack can increase the misalignment rate to $>95 \%$ for 9 out of 11 models. This is a stronger attack than the state-of-the-art attack (Zou et al., 2023) while its compute time is an order-of-magnitude less ( $\sim 30 \times$ less). Our attack does not use multi-modal inputs as required by Carlini et al. (2023). We further investigate more decoding strategies including multiple sampling or constrained decoding, and observe that these simple solutions can lead to even stronger attack performance (Section 4.3). With all these techniques combined, we reached a misalignment rate of all 11 models to over $95 \%$ successfully. The human evaluation further suggests that in the misaligned responses, at least half of them actually contain harmful instructions.

The catastrophic failure of alignment further motivates us to design an effective model alignment approach (Section 5). Specifically, we propose a new alignment strategy named generation-aware alignment, which proactively aligns models with outputs generated under various generation configurations. We show that this strategy can defend the generation exploitation attack reasonably, reducing the misalignment rate from $95 \%$ to $69 \%$.

Finally, we also evaluate our attack on proprietary models such as ChatGPT (gpt-3.5-turbo) by changing the decoding hyperparameters offered by the OpenAI API (Section 6). We find the attack is much less effective (7\%) compared to attacking open-source LLMs (95\%), highlighting a substantial disparity between current open-source LLMs and their proprietary counterparts. While open-source models offer attackers more avenues for exploitation, they typically lack the rigorous safety alignment processes seen in proprietary models.

Altogether, our study highlights a significant failure in the current safety evaluation and alignment procedures for open-source LLMs. Consequently, we strongly advocate the adoption of a more comprehensive red-teaming approach, to comprehensively evaluate model risks across a spectrum of generation strategies. Furthermore, we recommend implementing our generation-aware alignment approach prior to the model release as a proactive countermeasure.

## 2 BACKGROUND

This section revisits language modeling (Section 2.1) and common generation configurations (Section 2.2). We then present evidence to highlight that the safety evaluation of models is usually conducted with a fixed generation strategy (Section 2.3).

### 2.1 LANGUAGE MODELING

The task of language modeling aims to predict the next word in a sequence given the previous context, and forms the basis of state-of-the-art LLMs (Radford et al., 2018; Brown et al., 2020; Anil et al., 2023; Touvron et al., 2023a;b). Formally, given an input sequence of $n$ tokens $\mathbf{x}=x_{1}, x_{2}, \ldots, x_{n}$, the language model computes the probability distribution over the next token conditioned on the previous context:

$$
\begin{equation*}
\mathbb{P}_{\theta}\left(x_{i} \mid \mathbf{x}_{1: i-1}\right)=\frac{\exp \left(\mathbf{h}_{i}^{\top} \mathbf{W}_{x_{i}} / \tau\right)}{\sum_{j \in \mathcal{V}} \exp \left(\mathbf{h}_{i}^{\top} \mathbf{W}_{j} / \tau\right)} \tag{1}
\end{equation*}
$$

where $\tau$ is a temperature parameter that controls the sharpness of the next-token distribution. For text generation, the model recursively samples from the conditional distribution $\mathbb{P}_{\theta}\left(x_{i} \mid \mathbf{x}_{1: i-1}\right)$ to generate the next token $x_{i}$, continuing this process until an end-of-sequence token is produced.

### 2.2 GENERATION CONFIGURATIONS

System prompts. Prepending system prompts to guide large language model generations towards human-aligned outputs is a widely used technique (see Table 8 for example system prompts). System prompts are also commonly used in fine-tuning with context distillation (Askell et al., 2021; Bai et al., 2022b): firstly safer model responses are generated with system prompts, and then the model is fine-tuned on the safer responses without the system prompt, which essentially distills the system prompt (i.e., context) into the model.

Decoding methods. Given the predicted next-token distribution $\mathbb{P}_{\theta}\left(x_{i} \mid \mathbf{x}_{1: i-1}\right)$ at each step $i$, multiple decoding strategies can be applied to select the next token $x_{i}$. The most common strategy is sampling-based decoding, where $x_{i}$ is randomly sampled from the distribution. Greedy decoding, which simply selects the most probable token under $\mathbb{P}_{\theta}\left(x_{i} \mid \mathbf{x}_{1: i-1}\right)$, is a special case of sampling when the temperature $\tau=0$. Variants of sampling-based decoding include top- $p$ sampling (Holtzman et al., 2020) and top- $k$ sampling (Fan et al., 2018), which restrict the sampling to the most probable tokens.

### 2.3 FIXED GENERation CONFIGURATION FOR SAFETY EVALUATION

We notice that open-source LLMs are usually evaluated for alignment using only default generation methods, and this potentially leaves them vulnerable to misalignment when alternative strategies are used. For instance, Touvron et al. (2023b) conduct extensive alignment evaluations on LLAMA2 using a single decoding approach: top- $p$ sampling with $p=0.9$ and $\tau=0.1$ and always prepending a system prompt:

$\Delta$ In Section 4.1 of the LLAMA2 paper (Touvron et al., 2023b): "For decoding, we set temperature to 0.1 and use nucleus sampling with top-p set to $0.9^{\prime \prime}$;

$\triangle$ In Appendix A.3.7 of the LLAMA2 paper: "While collecting generations, we append a system prompt prior to the prompt for evaluation".

While pragmatic, this approach risks missing cases where the model's alignment substantially deteriorates with other generation strategies, which motivates our generation exploitation attack.

## 3 EVALUATION BENCHMARKS AND MEASURING MISALIGNMENT

We conduct our evaluation on two benchmarks (Section 3.1): AdvBench (Zou et al., 2023), and MaliciousInstruct that we curated. We also present a more robust evaluation metric for measuring misalignment (Section 3.2), which demonstrates higher agreement with human annotations, as well as human evaluation for measuring the percentage of harmful content.

### 3.1 DATASETS AND MODELS

Evaluation benchmarks. To systematically evaluate the effectiveness of our attack, we primarily use two benchmarks:

- AdvBench (Zou et al., 2023), which comprises 500 instances of harmful behaviors expressed as specific instructions.
- MaliciousInstruct, which consists of 100 harmful instances presented as instructions (see Appendix B. 3 for details). MaliciousInstruct contains ten different malicious intentions, including psychological manipulation, sabotage, theft, defamation, cyberbullying, false accusation, tax fraud, hacking, fraud, and illegal drug use. We create MaliciousInstruct to include a broader range of malicious instructions, enabling a more comprehensive evaluation of our approach's adaptability and effectiveness.

Models. Our evaluation uses the following 11 models: VICUNA (7B, 13B, and 33B), MPT (7B and 30B), FALCON (7B and 40B), and LLAMA2 (7B, 13B, 7B-chat, and 13B-chat). Note that among all the models, only LLAMA2-7B-CHAT and LLAMA2-13B-CHAT have been explicitly noted to have undergone safety alignment. Specifically, these LLaMA2 chat models have been iteratively refined using Reinforcement Learning with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy Optimization (PPO) (Schulman et al., 2017), and have been reported to exhibit a safety violation rate of less than $5 \%$ (see Figure 19 of Touvron et al. (2023b)). Links to the Hugging Face repositories for all these models can be found in Appendix B.

### 3.2 EVALUATION METRICS

We consider two different angles when evaluating the attack performance, including 1) whether the alignment is broken, and 2) whether the misaligned output is harmful, namely whether it gives helpful instructions to the malicious prompt.

Measuring misalignment (ASR) with high human agreement. Our major metric for evaluating misalignment is the Attack Success Rate (ASR): the percentage of instructions that receive misaligned outputs, defined as failing to abstain from responding to a malicious instruction. Previous work (Zou et al., 2023) simply employ substring matching to a small set of rejection phrases, such as "I am sorry" or "I cannot create," to determine alignment. If these strings are absent, they classify the attack as successful. However, this simple procedure may misclassify aligned output as misaligned due to the overlook of certain phrases (for instance, "I am afraid" was not included in the set), potentially overestimating attack performance.

We evaluate misalignment using a trained classifier, to better evaluate misalignment. Appendix B. 2 shows the details of training such a classifier using the HH-RLHF (Bai et al., 2022b) dataset to estimate a risk score for each output, with attacks classified as successful if the risk exceeds 0.5 . The HH-RLHF dataset comprises more than 161,000 entries, where each entry includes a prompt and two responses generated by a model: one that was chosen by a human and another that was rejected by a human. We train the classifier to distinguish between chosen and rejected examples. The trained classifier achieves $96 \%$ accuracy on a validation set. We further conducted a human evaluation, and found that our classifier-based evaluator detects misalignment more precisely than substring matching ( $92 \%$ vs. $86 \%$ agreement). More details for the human evaluation are in Appendix B.2. We also compare ASR using substring matching in Section 4.4 as a reference.

Measuring harmfulness percentage (HP). We also conduct human evaluations to obtain the Harmfulness Percentage (HP) scores which evaluate the percentage of the outputs that actually contain harmful content (Harmfulness Percentage; HP). We deem an answer as harmful if it provides concrete and helpful suggestions for malicious instructions.

## 4 JAILBREAK VIA EXPLOITING GENERATION STRATEGIES

We now systematically evaluate whether exploiting different generation configurations, namely our generation exploitation attack, can fail model alignment. Section 4.1 details the generation config-

Table 1: Attack success rate ( $\%$ ) of 11 open-sourced models on MaliciousInstruct under the default generation (the first column) and exploited generation (the last column). Models with $\boldsymbol{}$ have been safety aligned with RLHF. Simply exploiting different generation strategies elevates the ASR of 9 out of 11 models to $\geq 95 \%$. Later in Section 4.3, we further boost ASR for LLAMA2-7B-CHAT and LLAMA2-13B-CHAT to $\geq 95 \%$.

| Model | Greedy Decoding |  | Sampling-based Decoding (w/o sys. prompt) |  |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  | w/ sys. prompt w/o sys. prompt | Varied $\tau$ | Varied | Top- $K$ | Varied Top- $p$ | Varied All |
| VICUNA-7B | 50 | 62 | 92 | 95 | 95 | 97 |
| VICUNA-13B | 21 | 55 | 95 | 90 | 94 | 97 |
| VICUNA-33B | 42 | 50 | 94 | 94 | 93 | 96 |
| MPT-7B | 0 | 86 | 94 | 95 | 95 | 97 |
| MPT-30B | 0 | 91 | 95 | 96 | 97 | 98 |
| FALCON-7B | 5 | 75 | 95 | 92 | 95 | 95 |
| FALCON-40B | 7 | 72 | 95 | 93 | 94 | 95 |
| LLAMA2-7B | 14 | 85 | 94 | 93 | 96 | 97 |
| LLAMA2-13B | 34 | 83 | 96 | 95 | 96 | 97 |
| LLAMA2-7B-CHAT | 0 | 16 | 59 | 57 | 71 | 81 |
| LLAMA2-13B-CHAT | 0 | 8 | 73 | 66 | 66 | 88 |

urations our attack exploits, followed by a systematic evaluation of the attack on 11 open-source models in Section 4.2. We then explore strategies to further boost the attack performance in Section 4.3, and compare our strengthened attack against previous baseline attacks in Section 4.4.

### 4.1 EXPLOITED GENERATION STRATEGIES

Our generation exploitation attack explores various generation strategies, primarily centered around the system prompt and decoding strategies. Regarding the system prompt, we consider either 1) prepending it before the user instruction, or 2) not including it. In terms of decoding strategies, we experiment with the following three variants:

- Temperature sampling with varied temperatures $\tau$. Temperature controls the sharpness of the next-token distribution (see Equation (1)), and we vary it from 0.05 to 1 with step size 0.05 , which gives us 20 configurations.
- Top- $K$ sampling filters the $K$ most likely next words, and then the next predicted word will be sampled among these $K$ words only. We vary $K$ in $\{1,2,5,10,20,50,100,200,500\}$, which gives us 9 configurations.
- Top-p sampling (or nucleus sampling) (Holtzman et al., 2020) chooses from the smallest possible set of words whose cumulative probability exceeds the probability $p$. We vary $p$ from 0.05 to 1 with step size 0.05 , which gives us 20 configurations.

For each prompt, the attacker generates 49 responses (i.e., sample once for each decoding configuration above). Among all the generated responses, the attacker uses a scorer (see Appendix B for details) to pick the single response with the highest score and uses it as the final response to instruction. We also report the results under greedy decoding for reference ${ }^{2}$.

### 4.2 SYSTEMATIC EVALUATION ON 11 OPEN-SOURCE LLMS

We now systematically evaluate the robustness of alignment of 11 open-source models (listed in Section 3.1) against generation exploitation on MaliciousInstruct.

Removing the system prompt increases ASR. We first experiment with removing system prompts. The specific system prompts for each model can be found in Appendix B.1. Note that in the case of FALCON models, which do not inherently offer a system prompt, we follow the LLAMA2 paper (Touvron et al., 2023b) by using the system prompt of LLAMA2 models for FALCON models.

As shown in Table 1, the Attack Success Rate (ASR) experiences a significant increase, often $>$ $10 \%$, with the simple act of removing the system prompt. We observe that the presence of the system[^1]

Table 2: The most vulnerable decoding configuration and the corresponding ASR for each model on MaliciousInstruct. Models with have been safety aligned with RLHF. Different models are most susceptible to different decoding strategies. Therefore, assessing model alignment with a single decoding configuration may lead to an underestimation of the actual risks.

| Model | Temperature ( $\tau$ ) |  | $\boldsymbol{K}$ |  | $\boldsymbol{p}$ |  |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  | Best config. | ASR (\%) | Best config. | ASR (\%) | Best config. | ASR (\%) |
| VICUNA-7B | 0.3 | 62 | 1 | 62 | 0.4 | 64 |
| VICUNA-13B | 0.8 | 56 | 1 | 54 | 0.25 | 57 |
| VICUNA-33B | 0.8 | 59 | 50 | 56 | 0.6 | 59 |
| MPT-7B | 0.1 | 83 | 1 | 86 | 0.05 | 83 |
| MPT-30B | 0.1 | 87 | 1 | 86 | 0.3 | 88 |
| FALCON-7B | 0.2 | 78 | 1 | 75 | 0.25 | 80 |
| FALCON-40B | 0.25 | 79 | 5 | 75 | 0.3 | 78 |
| LLAMA2-7B | 0.45 | 85 | 1 | 83 | 0.2 | 85 |
| LLAMA2-13B | 0.5 | 85 | 1 | 83 | 0.3 | 87 |
| LLAMA2-7B-CHAT | 0.95 | 25 | 500 | 26 | 0.7 | 29 |
| LLAMA2-13B-CHAT | 0.95 | 27 | 500 | 27 | 0.95 | 24 |

![](https://cdn.mathpix.com/cropped/2024_06_04_b7e3672edd7b71666c3fg-06.jpg?height=310&width=1392&top_left_y=1072&top_left_x=366)

Figure 2: Harmful percentage (HP) for different decoding configurations.

prompt plays a critical role in maintaining aligned outputs, particularly for models that have not undergone safety tuning. In their case, removing the system prompts could lead to a remarkable ASR increase of over $50 \%$. However, even for models with explicit safety alignment, namely LLAMA2 chat models, the ASR still increases with the removal of the system prompt. This suggests that the context distillation approach taken in alignment may not be as effective as expected.

Exploiting decoding strategies further boosts ASR. Next, we investigate the possibility of enhancing ASR through the utilization of different decoding strategies. It's important to note that we have already eliminated the system prompt in this investigation; Appendix C. 2 presents results for varied decoding strategies under the system prompt. The results, as presented in Table 1, demonstrate that the exploration of diverse decoding strategies does indeed enhance performance. In fact, all models, except for the LLAMA2-CHAT model, achieve an ASR exceeding $90 \%$. This stark contrast in performance highlights a catastrophic failure of alignment in the evaluated models.

We also report the most vulnerable decoding strategies for each model in Table 2, and visualize the risk associated with per-instruction per-decoding configurations of the LLAMA2-7B-CHAT model in Appendix C.4. These two illustrations reveal that different models are most vulnerable to different decoding strategies, and different malicious instructions yield misaligned outputs through the model under different decoding strategies. Furthermore, it's worth noting that while employing a fixed decoding configuration typically results in an ASR of $<30 \%$ in LLAMA-CHAT models, using diverse configurations can significantly increase the ASR to over $80 \%$. These findings underscore that assessing model alignment using a fixed generation strategy as adopted in the LLAMA2 paper (Touvron et al., 2023b) considerably underestimates the actual risk.

Half of the misaligned outputs are harmful according to human judgment. We then investigate among the misaligned outputs, how many of them provide harmful instructions. We recruit five human annotators and present them with 100 misaligned outputs we gather from the LLAMA213B-CHAT model. The Harmful Percentage (HP) according to human annotations is $50 \%$ (see Ap-
pendix B.2). Additionally, we employ a heuristic rule ${ }^{3}$ to automatically identify harmful examples from the misaligned outputs, and calculate the HP. Figure 2 provides the per-decoding configuration HP for LLAMA2 chat models (the heuristic shares a $93 \%$ agreement with human). The HP for LLAMA2-7B-CHAT models can be as high as $80 \%$.

### 4.3 BOOSTING ATTACK PERFORMANCE ON SAFETY-ALIGNED MODELS

Two simple strategies further improve ASR on the safety-aligned LLAMA2-CHAT models to $95 \%$.

Sampling multiple times. Given the nondeterministic nature of sampling-based decoding, increasing the number of sampling runs is an intuitive way to strengthen our attack. As shown in Figure 3, sampling multiple times for each decoding configuration significantly improves the ASR of LLAMA2-CHAT models to $>95 \%$ specifically, we reach $95 \%$ ASR by sampling 5 times for LLAMA2-7B-CHAT and 4 times for LLAMA2-13B-CHAT.
![](https://cdn.mathpix.com/cropped/2024_06_04_b7e3672edd7b71666c3fg-07.jpg?height=328&width=726&top_left_y=709&top_left_x=1011)

Figure 3: ASR with multiple sampling runs for LLAMA2-7B-CHAT and LLAMA2-13B-CHAT.
Decoding with penalty and constraints. We find that another effective strategy to boost the attack involves imposing constraints during the decoding process. For instance, as shown in Table 3, ASR increases from $83 \%$ to $92 \%$ by introducing the length penalty during decoding. This number can be further escalated to $100 \%$ by incorporating two other constraints: 1) by restricting the generation of specific words categorized as "bad words" (e.g., 'sorry', 'unethical', 'illegal', 'harmful', 'afraid', 'cannot', 'apologize', 'unable') and 2) by mandating the generation of specific words, referred to as "forced words" (e.g., 'sure', 'Sure', 'ok', 'OK', 'okay').
Table 3: ASR (\%) of LLAMA2-7B-CHAT with decoding constraints.

|  | ASR |
| :--- | :---: |
| Baseline attack | 81 |
| + Length penalty | 89 |
| + Bad words | 93 |
| + Forced words | 100 |

### 4.4 COMPARISON WITH PREVIOUS ATTACKS

We compare our approach with GCG (Zou et al., 2023), the state-of-the-art attack ${ }^{4}$. We evaluate the performance of attacking LLAMA2-7B-CHAT and LLAMA2-13B-CHAT on both the AdvBench and MaliciousInstruct, using both the substring match metric by Zou et al. (2023) and our classifier. Following the experimental setup in Zou et al. (2023), we evaluate on the first 100 instances from AdvBench. For GCG, we use a batch size of 512 and a top- $k$ of 256 and generate a single output for each instance. The number of optimizable tokens is 32 and we run the optimization for 500 steps. For our approach, for each prompt, the attacker samples 8 times for each decoding configuration detailed in Section 4.2, and uses its scorer to pick the single response with the highest score as the final output.

As shown in Table 4, our generation exploitation attack consistently outperforms the SOTA attack, across two models, two benchmarks, and two metrics used to measure the success of the attack. Notably, our approach is also $30 \times$ faster than GCG: launching our attack with a single prompt on LLAMA2-7B-CHAT using a single NVIDIA A100 GPU takes about 3 minutes, while GCG requires approximately 1.5 hours for the same task (with 500 steps and a batch size of 512).[^2]

Table 4: Attack success rate (\%) of the SOTA attack (Zou et al., 2023) and ours on AdvBench and MaliciousInstruct for LLAMA2 chat models, using two evaluation metrics: substring match (previous work) and our classifier-based evaluator. The best attack results are boldfaced. Our attack consistently outperforms the SOTA.

| Model | Method | AdvBench (Zou et al., 2023) |  | MaliciousInstruct |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Substring match | Classifier (ours) | Substring match | Classifier (ours) |
| LLAMA2 <br> 7B-CHAT | GCG (Zou et al., 2023) | 47 | 36 | 50 | 42 |
|  | Ours (Varied $\tau$ ) | 91 | 82 | 90 | 81 |
|  | Ours (Varied Top-K) | 94 | 82 | 88 | 78 |
|  | Ours (Varied Top-p) | 96 | 87 | 99 | 94 |
|  | Ours (All) | 97 | 87 | 99 | 95 |
| LLAMA2 <br> 13B-CHAT | GCG (Zou et al., 2023) | 38 | 31 | 24 | 21 |
|  | Ours (Varied $\tau$ ) | 97 | 86 | 99 | 94 |
|  | Ours (Varied Top-K) | 97 | 85 | 100 | 94 |
|  | Ours (Varied Top-p) | 95 | 85 | 96 | 90 |
|  | Ours (Combined) | 98 | 89 | 100 | 96 |

## 5 An EfFeCTIVE GENERATION-AWARE ALIGNMENT APPRoACH

The catastrophic failure of alignment caused by our generation exploitation attack motivates the design of a more effective model alignment approach. Specifically, we propose the generationaware alignment approach, where we proactively collect model outputs generated through various decoding strategies and use them in the alignment process.

### 5.1 METHOD

The generation-aware alignment strategy is designed to enhance the model's resilience against the generation exploitation attack by proactively gathering examples from various decoding configurations. Specifically, given a language model $f_{\theta}$, and a prompt $\mathbf{p}$, the model generates output sequences $\mathbf{r}$ via sampling from $h\left(f_{\theta}, \mathbf{p}\right)$, where $h$ is a decoding strategy (from the decoding space $\mathcal{H}$ ) that maps the language model's probability distribution over the next tokens based on the prompt $p$ into a sequence of tokens from the vocabulary $\mathcal{V}$. In the generation-aware alignment procedure, for each prompt $\mathbf{p}$, we will collect $n$ responses from different decoding strategies, namely $\mathcal{R}^{\mathbf{p}}:=\left\{\mathbf{r}_{h, \mathbf{p}}^{i}\right\}_{h \in \mathcal{H}, i \in[n]}$, where $\mathbf{r}_{h, \mathbf{p}}^{i} \sim h\left(f_{\theta}, p\right)$ is the $i$-th sampling result from $h\left(f_{\theta}, \mathbf{p}\right)$. We then group all $\mathcal{R}^{\mathbf{P}}$ into two groups, $\mathcal{R}_{a}^{\mathrm{p}}$ for aligned responses, and $\mathcal{R}_{m}^{\mathrm{p}}$ for misaligned responses. Our generation-aware alignment minimizes the following objective from the chain of hindsight approach (Liu et al., 2023a):

$$
\mathcal{L}=\frac{1}{|\mathcal{P}|} \sum_{\mathbf{p} \sim \mathcal{P}} \frac{1}{\left|\mathcal{R}_{m}^{\mathbf{p}}\right| \times\left|\mathcal{R}_{a}^{\mathbf{p}}\right|} \sum_{\left(\mathbf{r}_{m}, \mathbf{r}_{a}\right) \sim \mathcal{R}_{m}^{\mathbf{p}} \times \mathcal{R}_{a}^{\mathbf{p}}}-\log \mathbb{P}\left(\mathbf{r}_{a} \mid \mathbf{p}, \mathbf{s}_{a}\right)-\log \mathbb{P}\left(\mathbf{r}_{m} \mid \mathbf{p}, \mathbf{s}_{m}\right)
$$

where $\mathbf{s}_{a}$ is "An aligned answer:" and $\mathbf{s}_{m}$ is "A misaligned answer:".

### 5.2 EXPERIMENTS

Experimental setup. We experiment with the LLAMA2-7B-CHAT model to evaluate the effectiveness of the generation-aware alignment strategy. Specifically, we sample around 2, 000 malicious prompts that achieve $>0.5$ toxicity score ${ }^{5}$ from the HH-RLHF dataset. For fair evaluation, the set of prompts we sampled is disjoint with the set of examples we use to train the classifier. For each prompt, we sample multiple outputs from the LLAMA2-7B-CHAT model under multiple decoding configurations, and group them into aligned or misaligned outputs.

We also compare with sampling examples for alignment with a fixed decoding strategy (sampling with $\tau=1.0$ ). To ensure a fair comparison, we randomly select 10,000 tuples of (prompt, aligned response, misaligned response) for both approaches and run alignment. Appendix B. 3 provides more training details.[^3]

Table 5: Attack success rate (\%) of LLAMA2-7B-CHAT on MaliciousInstruct after refined alignment using different strategies, under different decoding strategy exploitation by the attacker. The best alignment results are boldfaced.

|  | $\tau$ | Top- $K$ | Top- $p$ | All |
| :--- | :---: | :---: | :---: | :---: |
| Before refined alignment | 81 | 78 | 94 | 95 |
| Refine w/ Fixed-decoding alignment | 68 | 63 | 86 | 88 |
| Refine w/ Generation-aware alignment | $\mathbf{2 7}$ | $\mathbf{4 9}$ | $\mathbf{6 5}$ | $\mathbf{6 9}$ |

We evaluate the performance of alignment on MaliciousInstruct. Following Liu et al. (2023a), we postpend "An aligned answer:" after each instruction before decoding. While it may be contended that an adversary could simply erase this appended string, we wish to emphasize that our evaluation primarily serves a simulation purpose - in this context, the appended string is considered an integral part of the alignment procedure. We also note that in practical applications, the use of advanced alignment pipelines, which may incorporate techniques such as context distillation (Bai et al., 2022b), will obviate the necessity for explicit string additions.

Results. As shown in Table 5, generation-aware alignment leads to a reasonable reduction in the ASR of the original model, decreasing from $95 \%$ to $69 \%$. In contrast, sampling examples for alignment with the fixed decoding results in a much higher final ASR of $88 \%$. Notably, among all three decoding strategies exploited by the attacker, the varied decoding sampling strategy exhibits its greatest advantage in enhancing the model's robustness against temperature exploitation.

## 6 OPEN-SOURCE VS. PROPRIETARY LLMS

This section reports our preliminary experiments to show if our generation exploitation attack can apply proprietary LLMs.

Experimental setup. We experiment with the chat completion API provided by OpenAI ${ }^{6}$. Specifically, we use the gpt-3.5-turbo model. The API offers control over four decoding hyperparameters: temperature and top- $p$, as previously explored, along with the presence penalty and frequency penalty. The presence penalty encourages discussing new topics, while the frequency penalty discourages verbatim repetition. We vary the four decoding parameters (see Appendix B. 3 for details) and report the ASR on MaliciousInstruct. For each hyperparameter, the attacker uses the scorer to pick the best attack output from multiple outputs as the single final output.

The proprietary model is less vulnerable. We observe a substantially lower ASR (7\%) when attacking proprietary models (see Table 6) compared to open-source models ( $>95 \%$ ). This discrepancy can be attributed to two key factors. First, proprietary models typically incorporate a content filter (Azure, 2023), designed to identify and act upon potentially harmful content in both prompts and outputs. For instance, we observe that 9 out of 100 tested prompts usually experience prolonged request times and ultimately terminate due to timeout errors; We suspect that they have been filtered by the content filter. The second factor is that proprietary models are often owned by organizations with the resources to implement extensive red teaming efforts, thereby making these models more resilient to attacks.
Table 6: ASR (\%) on gpt-3.5turbo under the default decoding configuration and varied decoding configurations.

|  |  | ASR |
| :---: | :---: | :---: |
| Default decoding |  | 0 |
| Varied <br> decoding | Temperature $(\tau)$ <br> Top-p <br> Presence penalty <br> Frequency penalty <br> All | 3 <br> 3 <br> 2 <br> 4 <br> 7 |

These results in turn highlight the deficiency in alignment for current open-source LLMs: while they present attackers with more avenues for exploitation when compared to their proprietary counterparts which usually only expose an API, they typically lack the opportunity to undergo a thorough safety alignment process like the one seen in proprietary models.

## 7 CONCLUSION AND FUTURE WORK

This paper presents a novel approach to jailbreak the alignment in open-source LLMs without the need for complex techniques like optimizing for adversarial prompts. Our method, the generation[^4]exploitation attack, focuses on manipulating different generation strategies. Remarkably, this approach attains misalignment rates of up to $95 \%$ across 11 models, all while using $30 \times$ less compute than the current SOTA attack. We also highlight the importance of proactive alignment management during model development to improve model safety and reliability.

For future work, we plan to investigate the transferability of our attack method to a more extensive range of models, including the increasingly prominent multimodal models. We also aim to develop an improved automatic metric for harmfulness, which will contribute to a more rigorous evaluation of the model-generated content. Moreover, we intend to explore more advanced strategies for the generation-aware alignment procedure, with a focus on improving sample efficiency through strategic exploration of various decoding configurations.

## ACKNOWLEDGEMENT

This project is supported by an NSF CAREER award (IIS-2239290), a Sloan Research Fellowship, a Meta research grant, and a Princeton SEAS Innovation Grant. We would like to extend our sincere appreciation to Zirui Wang, Howard Yen, and Austin Wang for their participation in the human evaluation study.

## REFERENCES

Alex Albert. Jailbreak chat, 2023. URL https://www.jailbreakchat.com/.

Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.

Azure. Content filtering, 2023. URL https://learn.microsoft.com/en-us/azure/ai-services/ openai/concepts/content-filter.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.

Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, et al. Are aligned neural networks adversarially aligned? arXiv preprint arXiv:2306.15447, 2023.

Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, establish, exploit: Red teaming language models from scratch. arXiv preprint arXiv:2306.09442, 2023.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In NeurIPS, 2017.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.

Lavina Daryanani. How to jailbreak chatgpt., 2023. URL https://watcher.guru/news/ how-to-jailbreak-chatgpt.

Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In ACL, 2018.

Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022

Amelia Glaese, Nat McAleese, Maja Trkebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.

Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu, and Marc Dymetman. Aligning language models with preferences through f-divergence minimization. arXiv preprint arXiv:2302.08215, 2023.

Google. An important next step on our ai journey, 2023. URL https://blog.google/technology / ai/bard-google-ai-search-updates/.

Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe Kiela. Gradient-based adversarial attacks against text transformers. In $A C L, 2021$.

Julian Hazell. Large language models can be used to effectively scale spear phishing campaigns. arXiv preprint arXiv:2305.06972, 2023.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR, 2020.

Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically auditing large language models via discrete optimization. arXiv preprint arXiv:2303.04381, 2023.

Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. arXiv preprint arXiv:2302.05733, 2023.

Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences. In ICML, 2023.

Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black box jailbreaking of large language models, 2023.

Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on chatgpt. arXiv preprint arXiv:2304.05197, 2023.

Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Languages are rewards: Hindsight finetuning using human feedback. arXiv preprint arXiv:2302.02676, 2023a.

Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint arXiv:2305.13860, 2023b.

MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.

OpenAI. OpenAI: Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 2022.

Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. In EMNLP, 2022.

Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models, 2023.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. Technical report, OpenAI, 2018.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825, 2023.

Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In NeurIPS, 2020 .

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966, 2023.

Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023.

Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. arXiv preprint arXiv:2302.03668, 2023.

Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified robustness and beyond. In NeurIPS, 2020.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. LIMA: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023 .

Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.
