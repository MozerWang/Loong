# Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs 

Jiefeng Chen $^{1 *}$ Jinsung Yoon ${ }^{2}$ Sayna Ebrahimi ${ }^{2}$<br>Sercan Ö. Arık ${ }^{2}$ Tomas Pfister ${ }^{2}$ Somesh Jha ${ }^{1,2}$<br>${ }^{1}$ University of Wisconsin-Madison $\quad{ }^{2}$ Google LLC<br>\{jiefeng,jha\}@cs.wisc.edu<br>\{jinsungyoon, saynae,soarik,tpfister\}@google.com


#### Abstract

Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from $91.23 \%$ to $92.63 \%$ and improves the AUROC from $74.61 \%$ to $80.25 \%$.


## 1 Introduction

Large Language Models (LLMs) have recently demonstrated impressive capabilities in many natural language understanding, reasoning and generation tasks, such as question answering (Jiang et al., 2021; Singhal et al., 2023), summarization (Tang et al., 2023; Zhang et al., 2023b), semantic classification, and code generation (Poesia et al., 2022; Zhang et al., 2023a). As LLMs improve their remarkable performance, they are being increasingly considered to replace humans to perform high-stakes tasks. For example, LLMs can be used for medical QA to assist patients (Singhal et al., 2022). However, LLMs are not guaranteed to be accurate for all queries, so it is important to understand which queries they are reliable for. This[^0]

![](https://cdn.mathpix.com/cropped/2024_05_29_e87b8717bda47e6296b8g-01.jpg?height=486&width=628&top_left_y=725&top_left_x=1134)

Figure 1: A safety-critical question from the TriviaQA dataset: "Which vitamin helps regulate blood clotting?" The OPT-2.7B model incorrectly answers "Vitamin C", when the correct answer is "Vitamin K". Without selective prediction, LLMs will directly output the wrong answer which in this case could lead users to take the wrong medicine, and thus causing potential harm. With selective prediction, LLMs will output a low selection score along with the wrong answer and can further output "I don't know!" to warn users not to trust it or verify it using other sources.

information can be used to direct human oversight to the queries with the lowest selection score. $\mathrm{Se}$ lective prediction (Geifman and El-Yaniv, 2017), broadly refers to the deployment scenario for AI models where humans are involved to maintain overall accuracy by reviewing AI-generated, lowconfidence outputs. In this scenario, both human and AI performance are considered together to minimize human involvement cost. LLMs should be used in the real-world with enhanced selective prediction performance. They should be able to assess the accuracy of their predictions and refrain from making wrong predictions. If an LLM detects that an answer might be wrong for a question, it should be able to generate an answer with the sentiment of "I don't know!" (as shown in Fig. 1) or defer the answer to a human for manual inspection. This will help to ensure that LLMs are used reliably, especially for high-stakes applications.

Selective prediction for LLMs is challenging because LLMs are just trained to predict the next to-
ken given a context but are not guaranteed to always predict the correct next token. Also, since LLMs generate an output sequence in an auto-regressive way, they don't directly produce a confidence score for the output sequence. Thus, obtaining selection scores from LLMs for their output sequences is not straightforward. Although there is some research on selective prediction for LLMs, these studies have their own shortcomings. Kadavath et al. propose to use heuristic prompts (e.g., adding prompts like "Is the proposed answer True or False?") to trigger self-evaluation of LLMs. However, those prompts may only work for the LLM used in Kadavath et al. (2022) and may not generalize to other types of LLMs (e.g., OPT and GPT2 models evaluated in our work). Some approaches proposed using semantic entropy (Kuhn et al., 2023) or selfconsistency (Wang et al., 2022) as a measure of uncertainty for selection score. However, they usually require generating multiple output sequences to obtain the uncertainty measure for an input sequence, which introduces high computational cost and latency at test time. Fine-tuning LLMs on training data from the target question answering task using the standard LLM training loss can improve selective prediction performance. This is because fine-tuning can improve the accuracy of the predictions and maximize the likelihood of the ground-truth answer for a given question. However, maximizing the likelihood of the ground-truth answer is not the same as minimizing the likelihood of the wrong answers, since LLMs generate output sequences in an auto-regressive way. Even after fine-tuning, some wrong answers may still have high likelihood and be generated by the LLM at test time. Therefore, distinguishing correct and incorrect answers based on likelihood scores alone is a challenging task.

To address these challenges of self-evaluation and uncertainty estimation, we propose a novel framework - Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs (ASPIRE). Unlike previous methods that rely on hand-crafted heuristics or multiple output sequences, our framework learns to self-evaluate from target-task data. We do this by training LLMs on a subset of the training data from the question-answering tasks. This allows the LLMs to learn to distinguish between correct and incorrect answers on their own. We then define a selection score that combines the likelihood of the generated answer with the learned self-eval score (see Eq. (11)) to make selective predictions. This makes our method much less computationally expensive than solutions that require generating multiple output sequences to obtain the uncertainty measure. Thus, the proposed method is useful for practical applications where high selective prediction performance and low inference costs are desired, after deploying the LLM. In such applications, practitioners prefer collecting some training data to fine-tune smaller LLMs to achieve high selective prediction performance rather than directly deploying very large pre-trained LLMs with limited selective prediction performance for specific tasks.

We conduct extensive experiments to evaluate our proposed framework, ASPIRE. We show that ASPIRE achieves the state-of-the-art selective prediction performance on three question answering datasets: CoQA, TriviaQA and SQuAD, using OPT and GPT-2 models. We also provide empirical analysis to delve deeper into our proposed technique.

## 2 Related Work

Selective Prediction for LLMs. Recently, LLMs (e.g., GPT-4 (OpenAI, 2023) and PaLM (Chowdhery et al., 2022)) have achieved great success in solving various kinds of Natural Language Generation (NLG) tasks. However, LLMs are still not very reliable and may generate wrong outputs when solving NLG tasks. Due to this, selective prediction (or sometimes called selective generation (Ren et al., 2022)) is critical for safely deploying LLMs in the real-world. Different from selective prediction for classification tasks (e.g., Natural Language Inference (NLI) tasks) (Xin et al., 2021), selective prediction for LLMs in solving NLG tasks is fundamentally different since the prediction is done auto-regressively over many steps and the possible answer set has an infinite size. Recently, several work propose some uncertainty measures for LLMs, which can be used for selective prediction (Si et al., 2022; Kadavath et al., 2022; Varshney et al., 2022; Ren et al., 2022; Kuhn et al., 2023). Some recent work studies selective prediction for solving question answering tasks where questions are ambiguous (Cole et al., 2023; Yin et al., 2023). Varshney and Baral (2023) propose a selective prediction method that at inference time leverages an auxiliary model which is trained to distinguish the correct predictions of the QA model from the incorrect ones. Different from previous work, our
work proposes to improve selective prediction performance of LLMs in solving question answering tasks by learning self-evaluation during fine-tuning. Parameter Efficient Fine-tuning. Fine-tuning pretrained LLMs on downstream datasets can bring huge performance gains when compared to using the pretrained LLMs out-of-the-box (e.g., kshot inference). However, as LLMs get larger and larger, full fine-tuning becomes very expensive in terms of computational cost and memory requirements. In addition, massive models might not be data efficient and overfitting issues might be observed, yielding suboptimal generalization. To address these issues, Parameter-Efficient Fine-tuning (PEFT) approaches have been proposed. PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. It has also been shown that PEFT approaches are better than fine-tuning in the low-data regimes and generalize better to out-of-domain scenarios. Existing PEFT approaches include LoRA (Hu et al., 2021), Prefix Tuning (Liu et al., 2021a), Soft Prompt Tuning (Lester et al., 2021) and P-Tuning (Liu et al., 2021b). In this work, we use Soft Prompt Tuning to learn self-evaluation to improve selective prediction performance of LLMs.

## 3 Problem Setup

Suppose we have a pre-trained LLM $f$ for an arbitrary generative modeling task such as question answering. The output can be represented as a sequence of tokens from the vocabulary $\mathcal{V}$. Let $\mathcal{V}^{*}$ be the space of sequences of tokens. Suppose the logits of $f$ on $v \in \mathcal{V}$ given $\mathbf{x} \in \mathcal{V}^{*}$ is $\bar{f}(v \mid \mathbf{x})$. The likelihood of the next token following $\mathbf{x}$ being $v$ is defined as:

$$
\begin{equation*}
f(v \mid \mathbf{x}):=\frac{\exp (\bar{f}(v \mid \mathbf{x}))}{\sum_{v^{\prime} \in \mathcal{V}} \exp \left(\bar{f}\left(v^{\prime} \mid \mathbf{x}\right)\right)} \tag{1}
\end{equation*}
$$

whereas the likelihood of generating $\hat{\mathbf{y}} \in \mathcal{V}^{*}$ given $\mathbf{x}$ is defined as:

$$
\begin{equation*}
f(\hat{\mathbf{y}} \mid \mathbf{x}):=\Pi_{i=1}^{|\hat{y}|} f\left(\hat{y}_{i} \mid \mathbf{x}, \hat{y}_{[i-1]}\right) \tag{2}
\end{equation*}
$$

where $\hat{\mathbf{y}}=\left(\hat{y}_{1}, \ldots, \hat{y}_{\mid \hat{\mathbf{y}}} \mid\right),|\hat{\mathbf{y}}|$ is the length of $\hat{\mathbf{y}}$, $\hat{y}_{[i-1]}=\left(\hat{y}_{1}, \ldots, \hat{y}_{i-1}\right)$ for $i>0$ and $\hat{y}_{[0]}=\emptyset$. This likelihood can be very small when $|\hat{\mathbf{y}}|$ is very large. To address this issue, we define the normalized likelihood as:

$$
\begin{equation*}
f_{\text {norm }}(\hat{\mathbf{y}} \mid \mathbf{x}):=f(\hat{\mathbf{y}} \mid \mathbf{x})^{\frac{1}{|\mathbf{y}|}} \tag{3}
\end{equation*}
$$

We use $f$ to generate the output sequence for the given input $\mathrm{x}$ by solving the following objective:

$$
\begin{equation*}
\hat{\mathbf{y}}^{*}=\underset{\hat{\mathbf{y}}}{\operatorname{argmax}} \log f(\hat{\mathbf{y}} \mid \mathbf{x}) \tag{4}
\end{equation*}
$$

It is impossible to solve this objective exactly since the output sequences can be arbitrarily long. However, we can employ some decoding strategy like greedy decoding or beam search to solve it.

To evaluate if the generated output $\hat{\mathbf{y}}$ is correct or not, we need a set of reference outputs $S$ and an evaluation metric $M: \mathcal{V}^{*} \times \mathcal{V}^{*} \rightarrow[0,1]$ that can evaluate the similarity of the generated output $\hat{\mathbf{y}}$ compared to the reference output $\mathbf{y}_{r} \in S$. With a threshold $\gamma$, we can determine the correctness of the generated output - if $\max _{\mathbf{y}_{r} \in S} M\left(\hat{\mathbf{y}}, \mathbf{y}_{r}\right)>\gamma$, then the generated output is correct; otherwise, the generated output is wrong. We discuss the specific choices of $M$ and $\gamma$ in Section 6.

In selective prediction, we need a rejection option, which is denoted by $\perp$. Given a training dataset $\mathcal{D}^{t r}=\left\{\left(\mathbf{x}^{i}, \mathbf{y}^{i}\right)\right\}_{i=1}^{n_{t r}}$ randomly sampled from a target task distribution, we aim to build a selective predictor $f_{s}: \mathcal{V}^{*} \rightarrow \mathcal{V}^{*} \cup\{\perp\}$ that can achieve strong selective prediction performance on the test dataset $\mathcal{D}^{t e}=\left\{\left(\mathbf{x}^{i}, S^{i}\right)\right\}_{i=1}^{n_{t e}}$, where $S^{i}$ is the set of reference outputs for the input $\mathbf{x}^{i}$. The selective predictor $f_{s}$ is composed of a predictor $\hat{f}: \mathcal{V}^{*} \rightarrow \mathcal{V}^{*}$ and a selection scoring function $g: \mathcal{V}^{*} \rightarrow \mathbb{R}$. With $\hat{f}$ and $g$, the selective predictor $f_{s}$ is proposed as:

$$
f_{s}(\mathbf{x} ; \tau)= \begin{cases}\hat{f}(\mathbf{x}) & \text { if } g(\mathbf{x}) \geq \tau  \tag{5}\\ \perp & \text { if } g(\mathbf{x})<\tau\end{cases}
$$

where $\tau$ is a threshold. The accuracy of the selective predictor is defined as the fraction of the accepted inputs where the predictions are correct. The coverage of the selective predictor is defined as the fraction of the inputs that are accepted. We can tune the threshold $\tau$ to achieve a certain coverage and there would be an accuracy-coverage trade-off.

We use the area under the accuracy-coverage curve (AUACC) metric to measure selective prediction performance and use the area under the receiver operator characteristic curve (AUROC) metric to measure the quality of the selection score estimation. AUACC is the common metric used for evaluating selective prediction performance (Xin et al., 2021; Yoshikawa and Okazaki, 2023). AUROC is equivalent to the probability that a randomly chosen correct output sequence has a higher
selection score than a randomly chosen incorrect output sequence. AUROC is used in (Kuhn et al., 2023) for evaluating uncertainty estimation methods.

## 4 ASPIRE Framework

We propose that LLMs should have the selfevaluation ability such that they should be able to distinguish whether their proposed answers for a given question are correct or not. Although some previous work (Kadavath et al., 2022) show that LLMs have good self-evaluation ability with specially designed prompts, those prompts may not transfer to different kinds of LLMs (as shown by our experiments and in Kuhn et al. (2023)) and hand-crafting prompts for different kinds of LLMs can be expensive. A more effective approach is to collect some training data to employ selfevaluation. Towards this end, we propose a novel framework - Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs (ASPIRE). Fig. 2 illustrates the proposed framework and the details are explained next.

Given a training dataset for a generative task, we can fine-tune the pre-trained LLM on the training data to improve its prediction performance. Towards this end, parameter efficient tuning techniques (e.g., soft prompt tuning (Lester et al., 2021) and LoRA (Hu et al., 2021)) might be employed to adapt the pre-trained LLM on the task, given their effectiveness in obtaining strong generalization with small amount of target task data. Specifically, the model parameters $\theta$ of the LLM are frozen and adaptable parameters $\theta_{p}$ are added for fine-tuning. Only $\theta_{p}$ are updated to solve the following training objective:

$$
\begin{equation*}
\min _{\theta_{p}} \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim \mathcal{D}^{\operatorname{tr}}} \mathcal{L}\left(\mathbf{x}, \mathbf{y} ; \theta, \theta_{p}\right) \tag{6}
\end{equation*}
$$

where $\mathcal{L}$ is the LLM training loss (e.g. crossentropy). Such fine-tuning can improve selective prediction performance because it not only improves the prediction accuracy, but also enhances the likelihood of correct output sequences.

To further improve selective prediction performance, we propose to fine-tune the LLM to learn self-evaluation. We first use the LLM with the learned $\theta_{p}$ to generate different answers for each example $(\mathbf{x}, \mathbf{y}) \in \mathcal{D}^{t r}$. Suppose the decoding algorithm used to generate output sequences for each input $\mathbf{x}$ is $\mathcal{A}$. $\mathcal{A}$ would produce a list of generated output sequences:

$$
\begin{equation*}
\mathcal{A}\left(f, \theta_{p}, \mathbf{x}\right)=\left[\hat{\mathbf{y}}^{1}, \ldots, \hat{\mathbf{y}}^{k}\right] \tag{7}
\end{equation*}
$$

where $k$ is the number of output sequences generated. We aim to generate output sequences that have high likelihood (i.e., $f\left(\hat{\mathbf{y}}^{j} \mid \mathbf{x} ; \theta_{p}\right)$ is high). We use the metric $M$ defined in Section 3 to determine if the generated output $\hat{\mathbf{y}}^{j}$ is correct or not. If $M\left(\hat{\mathbf{y}}^{j}, \mathbf{y}\right)>\hat{\gamma}$, we label $\hat{\mathbf{y}}^{j}$ as a correct output for $\mathbf{x}$; otherwise, we label $\hat{\mathbf{y}}^{j}$ as a wrong output for $\mathbf{x}$. Here, the threshold $\hat{\gamma}$ might be different from the threshold $\gamma$ used for evaluation. We choose a sufficiently large value of $\hat{\gamma}$ (e.g., $\hat{\gamma}=0.9$ ) so that the generated wrong outputs wouldn't be labeled as correct outputs. In Appendix H, we provide more details and analyses on selection of $\hat{\gamma}$.

After sampling high-likelihood outputs for each query, we add adaptable parameters $\theta_{s}$ and only tune $\theta_{s}$ for learning self-evaluation. Since the output sequence generation only depends on $\theta$ and $\theta_{p}$, freezing $\theta$ and the learned $\theta_{p}$ can avoid changing the prediction behaviors of the LLM when learning self-evaluation. Let $z_{c}$ and $z_{w}$ be a pair of tokens that represent the words "correct" and "wrong" respectively. We can then optimize $\theta_{s}$ using the following training objective:

$$
\begin{align*}
& \min _{\theta_{s}} \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim \mathcal{D}^{t r}} \quad \mathcal{L}_{c}+\mathcal{L}_{w} \\
& \mathcal{L}_{c}=\mathbb{E}_{\hat{\mathbf{y}} \sim S_{c}(\mathbf{x}, \mathbf{y})}-\log f\left(z_{c} \mid \mathbf{x}, \hat{\mathbf{y}} ; \theta_{p}, \theta_{s}\right) \\
& \mathcal{L}_{w}=\mathbb{E}_{\hat{\mathbf{y}} \sim S_{w}(\mathbf{x}, \mathbf{y})}-\log f\left(z_{w} \mid \mathbf{x}, \hat{\mathbf{y}} ; \theta_{p}, \theta_{s}\right) \tag{8}
\end{align*}
$$

where $S_{c}(\mathbf{x}, \mathbf{y})$ is a set of correct outputs containing the reference output $\mathbf{y}$ and $k_{c}$ correct outputs with highest likelihood from $\mathcal{A}\left(f, \theta_{p}, \mathbf{x}\right)$, and $S_{w}(\mathbf{x}, \mathbf{y})$ is a set of wrong outputs containing $k_{w}$ wrong outputs with highest likelihood from $\mathcal{A}\left(f, \theta_{p}, \mathbf{x}\right)$. If $\mathcal{A}\left(f, \theta_{p}, \mathbf{x}\right)$ has less than $k_{c}$ correct outputs (or has less than $k_{w}$ wrong outputs), we include all its correct outputs (or all its wrong outputs) in $S_{c}$ (or $S_{w}$ ). We ensure that $S_{w}$ contains at least one wrong output. If $\mathcal{A}\left(f, \theta_{p}, \mathbf{x}\right)$ doesn't contain wrong outputs, we add a default wrong output (e.g., the empty string) to $S_{w}$.

After training $\theta_{p}$ and $\theta_{s}$, we obtain the prediction for the query $\mathrm{x}$ via solving the following objective:

$$
\begin{equation*}
\hat{\mathbf{y}}^{*}=\underset{\hat{\mathbf{y}}}{\operatorname{argmax}} \log f\left(\hat{\mathbf{y}} \mid \mathbf{x} ; \theta_{p}\right) \tag{9}
\end{equation*}
$$

We use the beam search decoding method towards this. We define the likelihood of the output $\hat{\mathbf{y}}^{*}$

![](https://cdn.mathpix.com/cropped/2024_05_29_e87b8717bda47e6296b8g-05.jpg?height=634&width=1374&top_left_y=234&top_left_x=341)

Figure 2: In the proposed framework ASPIRE, we first perform task specific tuning to train adaptable parameters $\theta_{p}$ while freezing the LLM. Then we use the LLM with the learned $\theta_{p}$ to generate different answers for each training question to create a dataset for self-evaluation learning. Finally, we train the adaptable parameters $\theta_{s}$ to learn self-evaluation using the created dataset while freezing the LLM and the learned $\theta_{p}$.

being correct for the query $\mathbf{x}$ as:

$$
\begin{align*}
& P\left(z_{c} \mid \mathbf{x}, \hat{\mathbf{y}}^{*}\right)= \\
& \frac{\exp \left(\bar{f}\left(z_{c} \mid \mathbf{x}, \hat{\mathbf{y}}^{*} ; \theta_{p}, \theta_{s}\right)\right)}{\sum_{z \in\left\{z_{c}, z_{w}\right\}} \exp \left(\bar{f}\left(z \mid \mathbf{x}, \hat{\mathbf{y}}^{*} ; \theta_{p}, \theta_{s}\right)\right)} \tag{10}
\end{align*}
$$

This score $P\left(z_{c} \mid \mathbf{x}, \hat{\mathbf{y}}^{*}\right)$ is referred as the learned self-eval score. Overall, the selection scoring function is proposed as:

$$
\begin{align*}
g(\mathbf{x}) & =(1-\alpha) \cdot \log f_{\text {norm }}\left(\hat{\mathbf{y}}^{*} \mid \mathbf{x} ; \theta_{p}\right)  \tag{11}\\
& +\alpha \cdot \log P\left(z_{c} \mid \mathbf{x}, \hat{\mathbf{y}}^{*}\right)
\end{align*}
$$

where $\alpha \in[0,1]$ is a hyper-parameter.

## 5 Implementation via Soft Prompt Tuning

In the proposed framework, $\theta_{p}$ and $\theta_{s}$ can be trained using parameter efficient tuning approaches. In our work, we focus on Soft Prompt Tuning, as illustrated in Fig. 3. The driving force behind this approach lies in the recognition that if we can develop prompts that effectively stimulate self-evaluation, it should be possible to discover these prompts through soft prompt tuning in conjunction with targeted training objectives.

We first briefly introduce the soft prompt tuning method proposed by Lester et al. (2021). We consider LLMs based on the Transformer architecture (Vaswani et al., 2017). Given a query $\mathbf{x}=\left(x_{1}, \ldots, x_{m_{q}}\right)$, Transformers first embed the tokens, forming a matrix $X \in \mathbb{R}^{m_{q} \times d_{e}}$, where $d_{e}$ is the dimension of the embedding space. The softprompts are represented as parameters $\tilde{\theta} \in \mathbb{R}^{l \times d_{e}}$, where $l$ is the length of the prompt. The prompt is then concatenated to the embedded input forming a single matrix $[\tilde{\theta} ; X] \in \mathbb{R}^{\left(m_{q}+l\right) \times d_{e}}$, which then flows through the transformer as normal.

In the proposed framework, we need to train two portions of the prompts $\theta_{p} \in \mathbb{R}^{l \times d_{e}}$ and $\theta_{s} \in$ $\mathbb{R}^{l \times d_{e}}$. Utilizing soft prompt tuning, the training objective (6) is proposed as:

$\min _{\theta_{p}} \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim \mathcal{D}^{\operatorname{tr}}} \frac{1}{|\mathbf{y}|} \sum_{j=1}^{|\mathbf{y}|}-\log f\left(y_{j} \mid\left[\theta_{p} ; X ; Y_{[j-1]}\right]\right)$,

where $X$ is the embedding of $\mathbf{x}$ and $Y_{[j-1]}$ is the embedding of $y_{[j-1]}$. On the other hand, the training objective (8) is proposed as:

$$
\begin{align*}
& \min _{\theta_{s}} \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim \mathcal{D}^{t r}} \quad \mathcal{L}_{c}+\mathcal{L}_{w} \\
& \mathcal{L}_{c}=\mathbb{E}_{\hat{\mathbf{y}} \sim S_{c}(\mathbf{x}, \mathbf{y})}-\log f\left(z_{c} \mid\left[\theta_{p} ; X ; \hat{Y} ; \theta_{s}\right]\right) \\
& \mathcal{L}_{w}=\mathbb{E}_{\hat{\mathbf{y}} \sim S_{w}(\mathbf{x}, \mathbf{y})}-\log f\left(z_{w} \mid\left[\theta_{p} ; X ; \hat{Y} ; \theta_{s}\right]\right) \tag{13}
\end{align*}
$$

where $\hat{Y}$ is the embedding of $\hat{\mathbf{y}}$. The inference objective (9) in the framework becomes:

$$
\begin{equation*}
\hat{\mathbf{y}}^{*}=\underset{\hat{\mathbf{y}}}{\operatorname{argmax}} \log f\left(\hat{\mathbf{y}} \mid\left[\theta_{p} ; X\right]\right) \tag{14}
\end{equation*}
$$

The learned self-eval score $P\left(z_{c} \mid \mathbf{x}, \hat{\mathbf{y}}^{*}\right)$ becomes:

$$
\begin{align*}
& P\left(z_{c} \mid \mathbf{x}, \hat{\mathbf{y}}^{*}\right)= \\
& \frac{\exp \left(\bar{f}\left(z_{c} \mid\left[\theta_{p} ; X ; \hat{Y}^{*} ; \theta_{s}\right]\right)\right)}{\sum_{z \in\left\{z_{c}, z_{w}\right\}} \exp \left(\bar{f}\left(z \mid\left[\theta_{p} ; X ; \hat{Y}^{*} ; \theta_{s}\right]\right)\right)} \tag{15}
\end{align*}
$$

![](https://cdn.mathpix.com/cropped/2024_05_29_e87b8717bda47e6296b8g-06.jpg?height=723&width=1128&top_left_y=238&top_left_x=470)

Figure 3: Implementation of the proposed framework via soft prompt tuning. $\theta_{p}$ and $\theta_{s}$ are learnable soft prompt embeddings. $Q_{\text {embed }}$ and $A_{\text {embed }}$ are input embeddings for the question and answer respectively. We first generate the answer and the likelihood of the answer, and then compute the learned self-eval score. We can cache the states when generating the answer and reuse those states when computing the learned self-eval score to save computational costs.

where $\hat{Y}^{*}$ is the embedding of $\hat{\mathbf{y}}^{*}$.

To generate the output sequence and obtain the selection score for a given input sequence, we employ two stages: first, we obtain the generated output and the likelihood for the generated output and then, we obtain the learned self-eval score. Since the query of the second stage is constructed by appending some additional tokens to the query of the first stage, the second stage can reuse the states in the first stage instead of recomputing them to save some computational cost (see Fig. 3).

Lastly, we note that the computational complexity of the proposed method at test time is $O\left(l_{\max }\right)$ with $l_{\text {max }}$ being the maximum length of the generated output sequence. In Appendix F, we provide a more detailed analysis of the computational complexity of different methods. The predictive entropy and semantic entropy methods have a complexity of $O\left(m \cdot l_{\max }\right)$ where $m$ is the number of output sequences sampled for uncertainty estimation, which is much larger than that of our method.

## 6 Experiments

Our experimental evaluation is focused on the following questions:

(Q1) Could a learning-based system using selfevaluation improve selective prediction in LLMs compared to other post-hoc selective prediction alternatives?

(A1) By learning self-evaluation, we can significantly improve selective prediction performance across different datasets and LLMs (see Table 1). (Q2) What kinds of decoding algorithms could be used as $\mathcal{A}$ for the proposed framework ASPIRE?

(A2) Using decoding algorithms that can sample different high-likelihood answers as $\mathcal{A}$ (e.g., beam search) is important for ASPIRE to achieve good selective prediction performance (see Table 4).

(Q3) What is the effect of the number of training samples for the proposed method ASPIRE?

(A3) More training samples lead to enhanced performance and with $\sim 2 \mathrm{k}$ samples, ASPIRE can outperform the baselines without soft prompt tuning significantly on different datasets (see Table 5).

### 6.1 Setup

Dataset. We focus on the free-form question answering tasks on the datasets CoQA (Reddy et al., 2019), TriviaQA (Joshi et al., 2017) and SQuAD (Rajpurkar et al., 2016). For CoQA and SQuAD, since each question is asked based on a context paragraph, we evaluate the LLMs in the zero-shot setting. For TriviaQA, since the LLMs have limited accuracy under the zero-shot setting, we evaluate the LLMs in 5 -shot setting. For each dataset, we use a subset of the original training set containing $50 \mathrm{~K}$ examples for adapting LLMs by default. The details of the datasets are given in Appendix B.

LLMs. We use OPT (Zhang et al., 2022) and GPT-2 (Radford et al., 2019) models of various sizes. For OPT, we consider OPT-350M, OPT-1.3B,

| Model | Method | CoQA |  | TriviaQA |  | SQuAD |  |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | AUACC $\uparrow$ | AUROC $\uparrow$ | AUACC $\uparrow$ | AUROC $\uparrow$ | AUACC $\uparrow$ | AUROC $\uparrow$ |
|  | Perplexity | 55.93 | 62.05 | 22.60 | 72.88 | 7.68 | 51.90 |
| Pre-trained | Predictive Entropy | 60.76 | 67.53 | 24.83 | 76.20 | 10.04 | 57.21 |
| GPT2-XL | Semantic Entropy | 63.03 | 70.50 | 24.37 | 75.33 | 10.38 | 59.17 |
|  | Self-eval | 46.67 | 50.83 | 9.30 | 42.75 | 7.32 | 49.56 |
|  | P(True) | 46.98 | 51.17 | 10.62 | 44.54 | 10.69 | 60.87 |
|  | Perplexity | 83.27 | 72.79 | 36.49 | 79.92 | 88.73 | 75.08 |
| Adapted | Predictive Entropy | 83.49 | 73.44 | 37.31 | 82.21 | 88.25 | 74.16 |
| GPT2-XL | Semantic Entropy | 84.40 | 75.16 | 36.68 | 81.40 | 88.62 | 75.26 |
| with $\theta_{p}$ | Self-eval | 69.91 | 51.90 | 14.39 | 43.33 | 74.26 | 49.13 |
|  | P(True) | 70.63 | 52.83 | 13.59 | 40.59 | 74.34 | 49.09 |
|  | ASPIRE (ours) | $\mathbf{8 5 . 6 5}$ | $\mathbf{7 8 . 3 2}$ | $\mathbf{3 8 . 0 6}$ | $\mathbf{8 3 . 2 3}$ | $\mathbf{8 9 . 8 6}$ | $\mathbf{7 8 . 3 5}$ |
|  | Perplexity | 75.26 | 70.16 | 40.93 | 78.86 | 40.82 | 57.20 |
| Pre-trained | Predictive Entropy | 75.29 | 69.16 | 41.20 | 78.92 | 47.18 | 62.85 |
| OPT-2.7B | Semantic Entropy | 76.31 | 70.96 | 40.72 | 78.06 | 51.53 | 68.40 |
|  | Self-eval | 62.32 | 52.26 | 25.88 | 59.04 | 41.78 | 59.05 |
|  | P(True) | 62.16 | 51.80 | 24.88 | 56.89 | 34.77 | 49.42 |
| Pre-trained | Self-eval | 71.99 | 51.10 | 36.92 | 48.90 | 46.24 | 57.26 |
| OPT-30B | P(True) | 71.59 | 51.31 | 36.20 | 45.63 | 43.93 | 54.26 |
|  | Perplexity | 90.80 | 74.23 | 53.56 | 81.74 | 92.86 | 75.72 |
| Adapted | Predictive Entropy | 90.63 | 72.87 | 53.91 | 82.19 | 92.96 | 75.58 |
| OPT-2.7B | Semantic Entropy | 91.23 | 74.61 | 53.58 | 81.55 | 93.21 | 76.53 |
| with $\theta_{p}$ | Self-eval | 81.30 | 50.76 | 32.98 | 56.03 | 86.34 | 56.99 |
|  | P(True) | 81.14 | 51.01 | 33.48 | 56.27 | 82.59 | 49.48 |
|  | ASPIRE (ours) | $\mathbf{9 2 . 6 3}$ | $\mathbf{8 0 . 2 5}$ | $\mathbf{5 5 . 0 6}$ | $\mathbf{8 4 . 4 4}$ | $\mathbf{9 4 . 7 3}$ | $\mathbf{8 2 . 6 0}$ |

Table 1: Results of evaluating different methods to compute the selection score when the model's predictions are fixed. All numbers are percentages. Bold numbers are superior results.

OPT-2.7B and OPT-30B. For GPT-2, we consider GPT2-Medium, GPT2-Large and GPT2-XL. The details of these models are given in Appendix C.

Baselines. For selective prediction, we need to get a predicted output sequence $\hat{\mathbf{y}}^{*}$ and a selection score $g(\mathbf{x})$ for each input sequence $\mathbf{x}$ given a model $f$. The model $f$ can be a pre-trained LLM or an adapted LLM with $\theta_{p}$ trained using the training objective (12). We use the beam-search decoding to obtain the predicted output sequence $\hat{\mathbf{y}}^{*}$ and consider the following baselines to compute the selection score $g(\mathbf{x})$ : (1) Perplexity; (2) Predictive Entropy; (3) Semantic Entropy (Kuhn et al., 2023); (4) Self-eval; (5) P(True) (Kadavath et al., 2022). More details can be found in Appendix D.

Evaluation metrics. We use the Rouge-L (Lin and Och, 2004) as the evaluation metric $M$ to evaluate the similarity of the generated answer to the reference answers following Kuhn et al. (2023). For the threshold $\gamma$ that is used to determine the correctness of the generated answer, we consider relatively larger values of $\gamma$ since we focus on safety-critical applications where accepting a wrong answer is more costly compared to rejecting a correct answer that is different from the reference answers (refer to Appendix G for the justifications of the choices of $\gamma$ ). Unless specified, we use $\gamma=0.7$ as default.

Training hyper-parameters. We have two stages of training: the first stage is to train the soft prompt $\theta_{p}$ using the training objective (12) and the second stage is to train the soft prompt $\theta_{s}$ using the training objective (13). For both stages, we train the soft prompts for 10 epochs using AdamW optimizer with a batch size of 8 , a learning rate of 0.01 and cosine learning rate scheduling. More training details can be found in Appendix E.

ASPIRE setup. We use the beam search as the decoding algorithm $\mathcal{A}$. We set the number of beams equal to $k$ and use the $k$ highest scoring beams as the answer list $\mathcal{A}\left(f, \theta_{p}, \mathbf{x}\right)$. We set $l=50, \hat{\gamma}=0.9$, $k=10, k_{c}=2, k_{w}=10$ and $\alpha=0.25$ by default. We choose these hyper-parameters based on the performance on the validation set from TriviaQA using the OPT-2.7B model. We then use the same hyper-parameters across all datasets and models.

| Model | Method | CoQA |  | TriviaQA |  | SQuAD |  |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | AUACC $\uparrow$ | AUROC $\uparrow$ | AUACC $\uparrow$ | AUROC $\uparrow$ | AUACC $\uparrow$ | AUROC $\uparrow$ |
|  | ASPIRE $(\alpha=0.0)$ | 90.80 | 74.23 | 53.56 | 81.74 | 92.86 | 75.72 |
| Adapted | ASPIRE $(\alpha=0.25)$ | $\mathbf{9 2 . 6 3}$ | $\mathbf{8 0 . 2 5}$ | $\mathbf{5 5 . 0 6}$ | $\mathbf{8 4 . 4 4}$ | $\mathbf{9 4 . 7 3}$ | $\mathbf{8 2 . 6 0}$ |
| OPT-2.7B | ASPIRE $(\alpha=0.5)$ | 92.56 | 80.18 | 54.61 | 84.33 | 94.59 | 82.16 |
| with $\theta_{p}$ | ASPIRE $(\alpha=0.75)$ | 92.05 | 78.37 | 52.71 | 81.52 | 94.28 | 80.98 |
|  | ASPIRE $(\alpha=1.0)$ | 91.33 | 76.08 | 48.84 | 76.39 | 93.77 | 79.48 |

Table 2: Results of studying the effect of the hyper-parameter $\alpha$ in the proposed selection score (Eq. (11)). All numbers are percentages. Bold numbers are superior results.

| Model | CoQA | TriviaQA | SQuAD |
| :--- | :---: | :---: | :---: |
|  | Acc $\uparrow$ | Acc $\uparrow$ | Acc $\uparrow$ |
| Pre-trained GPT2-XL | 46.27 | 11.80 | 7.41 |
| Adapted GPT2-XL with $\theta_{p}$ | 69.18 | 17.45 | 75.44 |
| Pre-trained OPT-2.7B | 60.68 | 21.38 | 35.95 |
| Pre-trained OPT-30B | 71.06 | 39.36 | 41.41 |
| Adapted OPT-2.7B with $\theta_{p}$ | 80.45 | 29.21 | 83.27 |

Table 3: Results of evaluating the accuracy of different LLMs. All numbers are percentages.

### 6.2 Results

We first evaluate the accuracy of different LLMs. The results in Table 3 show that after training $\theta_{p}$ via soft prompt tuning, the accuracy of LLMs is improved significantly. On the CoQA and SQuAD datasets, the adapted OPT-2.7B can even outperform the pre-trained OPT-30B, which demonstrates that it is possible to adapt a smaller LLM to achieve better accuracy than a much larger LLM. We then evaluate different methods to compute the selection score when the model's predictions are fixed. The results in Table 1 show that the proposed method ASPIRE significantly outperforms the baselines in terms of the AUACC and AUROC metrics across different datasets and LLMs. The results also show that after prompt tuning, the AUACC of different methods is significantly improved as the accuracy gets better and the perplexity becomes more meaningful in separating correct and wrong answers. Additionally, the results show that the proposed ASPIRE with the adapted OPT-2.7B model can significantly outperform the Self-eval and $\mathrm{P}$ (True) baselines with the pre-trained OPT-30B model in selective prediction performance. Note that on the TriviaQA dataset, although the pre-trained OPT30B model has better accuracy than the adapted OPT-2.7B model, the Self-eval and P(True) baselines with the pre-trained OPT-30B model have much worse selective prediction performance compared to the proposed ASPIRE with the adapted
OPT-2.7B model. These demonstrate that the selfevaluation approaches are not effective for high capacity LLMs, and applying the proposed ASPIRE to smaller LLMs can lead to better selective prediction performance compared to those self-evaluation approaches with much larger LLMs. Additional results in Appendix I show that ASPIRE significantly outperforms the baselines across OPT and GPT2 models of different sizes for different values of the Rouge threshold $\gamma$.

### 6.3 Empirical Analyses

The effect of $\alpha$. We study the effect of the hyper-parameter $\alpha$ in the proposed selection score (Eq. (11)). The results in Table 2 show that setting $\alpha=0.25$ leads to the best performance since it combines the normalized likelihood and the learned self-eval score in a good way. Only using the normalized likelihood (i.e., $\alpha=0$ ) or only using the learned self-eval score (i.e., $\alpha=1$ ) leads to much worse performance. In practice, the value of $\alpha$ can be chosen based on the performance on the validation data. In Appendix J, we give results for other models and discuss how we choose $\alpha$.

The choices of $\mathcal{A}$. We compare two decoding algorithms - beam search and multinomial sampling that can be used as $\mathcal{A}$ for answer sampling. For beam search, we use the $k$ highest scoring beams as the answer list. For multinomial sampling, we consider temperature (denoted as $T$ ) in the set $\{0.1,1.0,2.0\}$. The results in Table 4 show that using multinomial sampling with $T=2.0$ or $T=0.1$ leads to worse performance compared to other decoding algorithms. If we set a high temperature ( $T=2.0$ ) for multinomial sampling, then we sample some random answers that might not have high-likelihood. If we set a low temperature $(T=0.1)$ for multinomial sampling, then we repeatedly sample the same high-likelihood answers. Thus, the results suggest that sampling different high-likelihood answers is important for our

| Model | Decoding Algorithm | CoQA |  | TriviaQA |  | SQuAD |  |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | AUACC $\uparrow$ | AUROC $\uparrow$ | AUACC $\uparrow$ | AUROC $\uparrow$ | AUACC $\uparrow$ | AUROC $\uparrow$ |
| Adapted | Multinomial $(\mathrm{T}=0.1)$ | 83.82 | 74.22 | 36.40 | 80.67 | 89.75 | 77.56 |
|  | Multinomial $(\mathrm{T}=1.0)$ | 84.96 | 76.15 | 37.03 | 81.41 | $\mathbf{9 0 . 1 2}$ | $\mathbf{7 8 . 7 1}$ |
| with $\theta_{p}$ | Multinomial $(\mathrm{T}=2.0)$ | 83.06 | 72.96 | 36.34 | 80.14 | 89.41 | 76.98 |
|  | Beam search | $\mathbf{8 5 . 6 5}$ | $\mathbf{7 8 . 3 2}$ | $\mathbf{3 8 . 0 6}$ | $\mathbf{8 3 . 2 3}$ | 89.86 | 78.35 |
| Adapted | Multinomial $(\mathrm{T}=0.1)$ | 92.04 | 77.96 | 55.09 | 84.28 | 94.24 | 80.52 |
|  | Multinomial $(\mathrm{T}=1.0)$ | 92.60 | 79.86 | $\mathbf{5 5 . 1 5}$ | 84.29 | 94.57 | 82.08 |
| with $\theta_{p}$ | Multinomial $(\mathrm{T}=2.0)$ | 92.02 | 77.91 | 53.80 | 82.40 | 94.15 | 80.42 |
|  | Beam search | $\mathbf{9 2 . 6 3}$ | $\mathbf{8 0 . 2 5}$ | 55.06 | $\mathbf{8 4 . 4 4}$ | $\mathbf{9 4 . 7 3}$ | $\mathbf{8 2 . 6 0}$ |

Table 4: Results of comparing different decoding algorithms for answer sampling in the proposed method. We denote the temperature as $T$. All numbers are percentages. Bold numbers are superior results.

| Model | Method | CoQA |  | TriviaQA |  | SQuAD |  |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | AUACC $\uparrow$ | AUROC $\uparrow$ | AUACC $\uparrow$ | AUROC $\uparrow$ | AUACC $\uparrow$ | AUROC $\uparrow$ |
| Pre-trained | Predictive Entropy | 75.29 | 69.16 | 41.20 | 78.92 | 47.18 | 62.85 |
| OPT-2.7B | Semantic Entropy | 76.31 | 70.96 | 40.72 | 78.06 | 51.53 | 68.40 |
| Adapted | ASPIRE $($ size $=1 \mathrm{k})$ | 80.87 | 67.01 | 45.70 | 78.98 | 85.42 | 71.42 |
|  | ASPIRE $($ size $=2 \mathrm{k})$ | 85.71 | 73.72 | 46.64 | 79.24 | 88.27 | 75.74 |
| with $\theta_{p}$ | ASPIRE $($ size $=5 \mathrm{k})$ | 87.83 | 74.58 | 49.77 | 82.06 | 90.09 | 77.09 |
|  | ASPIRE $($ size $=10 \mathrm{k})$ | 90.46 | 78.29 | 51.88 | 83.13 | 92.48 | 79.46 |
|  | ASPIRE $($ size $=50 \mathrm{k})$ | 92.63 | 80.25 | 55.06 | 84.44 | 94.73 | 82.60 |

Table 5: Results of studying the effect of training set size for the proposed ASPIRE. All numbers are percentages.

method to achieve high accuracy and coverage in selective prediction. The results also show that using beam search leads to similar performance as using multinomial sampling with $T=1$. So we can use either one in practice.

Training sample efficiency. We perform experiments to study the effect of the number of training samples for ASPIRE. We fix the number of training steps to be $50 \mathrm{~K}$ while varying the size of the training dataset. The results in Table 5 show that more training samples lead to performance improvement and with $2 \mathrm{~K}$ training samples, ASPIRE can outperform the baselines without soft prompt tuning by a large margin across different datasets. This underlines that our method, ASPIRE, can significantly improve selective prediction performance even with limited number of training samples.

## 7 Conclusion

In this paper, we proposed a novel framework for adaptation with self-evaluation to improve selective prediction in LLMs. We implemented the framework via soft prompt tuning and demonstrated its superior performance over existing methods through extensive experiments. In future work, one could explore implementing our framework via other parameter efficient tuning approaches and applying our method to larger LLMs.

## Limitations

Higher capacity LLMs are known to often yield superior capabilities. Our work does not include fine-tuning experimental results with the largest and the strongest LLMs in the literature (we have fine-tuning results with LLMs up to 2.7B parameters), due to our computational constraints. However, the proposed framework can be applied to LLMs of any size and similar improvements are expected. We leave the adoption of our methods to larger-scale LLMs to future work.

## Ethics Statement

LLMs are widely used in various applications nowadays. However, they can generate wrong or misleading answers to questions, which can cause serious consequences in some safety critical applications. The framework proposed in our work can be used to improve selective prediction performance of LLMs and make their deployments more reliable. However, it is noted that the obtained selective prediction performances are still not perfect.

## Acknowledgements

We thank all the anonymous reviewers for their careful comments and feedback. The work is partially supported by Air Force Grant FA955018-1-0166, the National Science Foundation (NSF) Grants CCF-FMitF-1836978, IIS-2008559, SaTC-Frontiers1804648, CCF-2046710 and CCF1652140, and ARO grant number W911NF-17-10405. Jiefeng Chen and Somesh Jha are partially supported by the DARPA-GARD problem under agreement number 885000 .

## References

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Jeremy R Cole, Michael JQ Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. 2023. Selectively answering ambiguous questions. arXiv preprint arXiv:2305.14613.

Yonatan Geifman and Ran El-Yaniv. 2017. Selective classification for deep neural networks. Advances in neural information processing systems, 30 .

Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.

Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962-977.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.

Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.

Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.
In The Eleventh International Conference on Learning Representations.

Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059.

Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL04), pages 605-612.

Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021a. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602.

Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt understands, too. arXiv preprint arXiv:2103.10385.

R OpenAI. 2023. Gpt-4 technical report. arXiv.

Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable code generation from pre-trained language models. arXiv preprint arXiv:2201.11227.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.

Siva Reddy, Danqi Chen, and Christopher D Manning. 2019. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249-266.

Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter J Liu. 2022. Out-of-distribution detection and selective generation for conditional language models arXiv preprint arXiv:2209.15558.

Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. 2022. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150.

Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2022. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138.

Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. 2023. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617.

Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor, Ali Soroush, Pierre A Elias, Ziyang Xu, Ying Ding, Greg Durrett, Justin Rousseau, et al. 2023. Evaluating large language models on medical evidence summarization. medRxiv, pages 2023-04.

Neeraj Varshney and Chitta Baral. 2023. Postabstention: Towards reliably re-attempting the abstained instances in QA. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 967-982, Toronto, Canada. Association for Computational Linguistics.

Neeraj Varshney, Swaroop Mishra, and Chitta Baral. 2022. Investigating selective prediction approaches across several tasks in iid, ood, and adversarial settings. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1995-2002.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.

Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.

Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021. The art of abstention: Selective prediction and error regularization for natural language processing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages $1040-1051$.

Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do large language models know what they don't know? arXiv preprint arXiv:2305.18153.

Hiyori Yoshikawa and Naoaki Okazaki. 2023. Selectivelama: Selective prediction for confidence-aware evaluation of language models. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1972-1983.

Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, and Chuang Gan. 2023a. Planning with large language models for code generation. arXiv preprint arXiv:2303.05510.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.

Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2023b. Benchmarking large language models for news summarization. arXiv preprint arXiv:2301.13848.
