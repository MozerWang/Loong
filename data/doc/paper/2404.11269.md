# DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series 

Zahra Zamanzadeh Darban ${ }^{1 *}$, Geoffrey I. Webb ${ }^{1}$, Mahsa Salehi ${ }^{1}$<br>${ }^{1}$ Monash University, Melbourne<br>\{zahra.zamanzadeh, geoff.webb, mahsa.salehi\}@ monash.edu


#### Abstract

Time series anomaly detection (TAD) faces a significant challenge due to the scarcity of labelled data, which hinders the development of accurate detection models. Unsupervised domain adaptation (UDA) addresses this challenge by leveraging a labelled dataset from a related domain to detect anomalies in a target dataset. Existing domain adaptation techniques assume that the number of anomalous classes does not change between the source and target domains. In this paper, we propose a novel Domain Adaptation Contrastive learning for Anomaly Detection in multivariate time series (DACAD) model to address this issue by combining UDA and contrastive representation learning. DACAD's approach includes an anomaly injection mechanism that introduces various types of synthetic anomalies, enhancing the model's ability to generalise across unseen anomalous classes in different domains. This method significantly broadens the model's adaptability and robustness. Additionally, we propose a supervised contrastive loss for the source domain and a self-supervised contrastive triplet loss for the target domain, improving comprehensive feature representation learning and extraction of domaininvariant features. Finally, an effective Centrebased Entropy Classifier (CEC) is proposed specifically for anomaly detection, facilitating accurate learning of normal boundaries in the source domain. Our extensive evaluation across multiple real-world datasets against leading models in time series anomaly detection and UDA underscores DACAD's effectiveness. The results validate DACAD's superiority in transferring knowledge across domains and its potential to mitigate the challenge of limited labelled data in time series anomaly detection.


[^0]
## 1 Introduction

Unsupervised Domain Adaptation (UDA) is a technique used to transfer knowledge from a labelled source domain to an unlabeled target domain, particularly useful when labelled data in the target domain are scarce or unavailable. Deep learning methods have become the predominant approach in UDA, offering advanced capabilities and significantly improved performance compared to traditional techniques. UDA's approach is crucial in situations where deep models' performance drops significantly due to the discrepancy between the data distributions in the source and target domains, a phenomenon known as domain shift [Liu et al., 2022].

Applications of UDA are diverse, ranging from image and video analysis to natural language processing and time series data analysis. In time series data analysis and anomaly detection, UDA can be particularly challenging, given that often i) the nature of time series data across different domains is complex and varied [Wilson and Cook, 2020; Liu et al., 2022], and ii) the number of anomalous classes changes between the source and target domains.

For time series data specifically, UDA methods often employ neural architectures as feature extractors. These models are designed to handle domain adaptation, primarily target time series regression and classification problems [Purushotham et al., 2016; Cai et al., 2021; Ozyurt et al., 2023; Wilson et al., 2020], focusing on aligning the major distributions of two domains. This approach may lead to negative transfer effects [Zhang et al., 2022] on minority distributions, which is a critical concern in time series anomaly detection (TAD). The minority distributions, often representing rare events or anomalies, may be overshadowed or incorrectly adapted due to the model's focus on aligning dominant data distributions, potentially leading to a higher rate of false negatives in anomaly detection. Since transferring knowledge of anomalies requires aligning minority distributions and the anomaly label spaces often have limited similarity across domains, existing methods face limitations in addressing anomaly detection in time series data. This highlights a significant gap in current UDA methodologies, pointing towards the need for novel approaches that can effectively address the unique requirements of anomaly detection in time series data.

Furthermore, in the realm of time series anomaly detection, in the most recent model called ContextDA [Lai et al., 2023],
the discriminator aligns source/target domain windows without leveraging label information in the source domain, which makes the alignment ineffective. Our model leverages source labels and anomaly injection for better feature extraction, enhancing the alignment of normal samples. This is particularly vital due to the challenges in aligning anomalies across domains with different anomalous classes.

In our study, we introduce Domain Adaptation Contrastive learning model for Anomaly Detection in time series (DACAD), a unique framework for UDA in multivariate time series anomaly detection, leveraging contrastive learning (CL). DACAD focuses on contextual representations. It forms positive pairs based on proximity and negative pairs using anomaly injection, subsequently learning their embeddings with both supervised CL in the source domain and selfsupervised CL in the target domain. To sum up, we make the following contributions:

- We introduce DACAD, a pioneering CL framework for multivariate time series anomaly detection with UDA. It is a novel deep UDA model based on contrastive representation utilising a labelled source dataset and synthetic anomaly injection to overcome the lack of labelled data in TAD, distinguishing between normal and anomalous patterns effectively.
- Extending on the deep one-class classifier (DeepSVDD) [Ruff et al., 2018], our proposed one-class classifier, the CEC, operates on the principle of spatial separation in the feature space by leveraging the existing label information in the source domain, aiming to bring "normal" sample representations closer to the centre and distancing anomalous ones. This spatial adjustment is measured using a distance metric, establishing the basis for anomaly detection.
- Our comprehensive evaluation with real-world datasets highlights DACAD's efficiency. In comparison with the recent TAD deep models and UDA models for time series classification and UDA for anomaly detection, DACAD demonstrates superior performance, emphasising the considerable importance of our study.


## 2 Related Works

In this section, we provide a brief survey of the existing literature that intersects with our research. We concentrate on two areas: UDA for time series and deep learning for TAD.

UDA for time series: In the realm of UDA for time series analysis, the combination of domain adaptation techniques with the unique properties of time series data presents both challenges and opportunities. Traditional approaches such as Maximum Mean Discrepancy (MMD) [Long et al., 2018] and adversarial learning frameworks [Tzeng et al., 2017] are geared towards minimizing domain discrepancies by developing domain-invariant features. These methods are vital in applications spanning medical, industrial, and speech data, with significant advancements in areas like sleep classification [Zhao et al., 2021], arrhythmia classification [Wang et al., 2021], and various forms of anomaly detection [Lai et al., 2023], fault diagnosis [Lu et al., 2021], and lifetime prediction [Ragab et al., 2020].
Time series specific UDA approaches like variational recurrent adversarial deep domain adaptation (VRADA) by Purushotham et al., pioneered UDA for multivariate time series, utilising adversarial learning with an LSTM networks [Hochreiter and Schmidhuber, 1997] and variational RNN [Chung et al., 2015] feature extractor. Convolutional deep domain adaptation for time series (CoDATS) by Wilson et al., built on VRADA's adversarial training but employed a convolutional neural network as the feature extractor. A metric-based method, time series sparse associative structure alignment (TS-SASA) [Cai et al., 2021] aligns intra and inter-variable attention mechanisms between domains using MMD. Adding to these, the CL for UDA of time series (CLUDA) [Ozyurt et al., 2023] model offers a novel approach, enhancing domain adaptation capabilities in time series. All these methods share a common objective of aligning features across domains, each contributing unique strategies to the complex challenge of domain adaptation in time series classification data. However, they are ineffective when applied to TAD tasks. Additionally, ContextDA, introduced by Lai et al., is a TAD model that applies deep reinforcement learning to optimise domain adaptation, framing context sampling as a Markov decision process. However, it is ineffective when the anomaly classes change between source and target.

Deep Learning for TAD: The field of Time Series Anomaly Detection (TAD) has advanced significantly, embracing a variety of methods ranging from basic statistical approaches to sophisticated deep learning techniques [Schmidl et al., 2022; Audibert et al., 2022]. Notably, deep learning has emerged as a promising approach due to its autonomous feature extraction capabilities [Darban et al., 2022]. TAD primarily focuses on unsupervised [Yue et al., 2022; Hundman et al., 2018; Audibert et al., 2020; Xu et al., 2022] and semi-supervised methods [Niu et al., 2020; Park et al., 2018] to tackle the challenge of limited labelled data availability. Unsupervised methods like OmniAnomaly [Su et al., 2019], GDN [Deng et al., 2021], and BeatGAN [Zhou et al., 2019] are especially valuable in scenarios with sparse anomaly labels, whereas semi-supervised methods leverage available labels effectively.

Advanced models such as LSTM-NDT [Hundman et al., 2018] and THOC [Shen et al., 2020] excel in minimizing forecasting errors and capturing temporal dependencies. However, models like USAD [Audibert et al., 2020] face challenges with long time series data due to error accumulation in decoding. Additionally, unsupervised representation learning, exemplified by SPIRAL [Lei et al., 2019] and TST [Zerveas et al., 2021], shows impressive performance, albeit with scalability issues in long time series. Newer models like TNC [Tonekaboni et al., 2021] and TLoss [Franceschi et al., 2019] aim to overcome these challenges using methods such as time-based negative sampling.

Furthermore, contrastive representation learning, crucial in TAD for pattern recognition, groups similar samples together while distancing dissimilar ones. It has been effectively employed in TS2Vec [Yue et al., 2022] for multi-level semantic representation and in DCdetector [Yang et al., 2023], which uses a dual attention asymmetric design for permutation invariant representations.

## 3 DACAD

Problem formulation: Given an unlabeled time series dataset $T$ (target), the problem is to detect anomalies in $T$ using a labelled time series dataset $S$ (source) from a related domain.

In this section, we present DACAD, which ingeniously uses temporal correlations in time series data and adapts to differences between source and target domains. It starts with a labelled dataset in the source domain, featuring both normal and anomalous instances. Both real and synthetic (injected) anomalies aid in domain adaptation, ensuring training and improving generalisability on a wide range of anomaly classes.

DACAD's core is CL as inspired by a recent UDA for time series work [Ozyurt et al., 2023], which strengthens its ability to handle changes between domains by improving the feature representation learning in source and target domains. As described in subsection 3.3, in the target domain, we use a self-supervised contrastive loss [Schroff et al., 2015] by forming triplets to minimise the distance between similar samples and maximise the distance between different samples. Additionally, in the source domain, we leverage label information of both anomaly and normal classes and propose an effective supervised contrastive loss [Khosla et al., 2020] named supervised mean-margin contrastive loss. A Temporal Convolutional Network (TCN) in DACAD captures temporal dependencies, generating domain-invariant features. A discriminator ensures these features are domain-agnostic, leading to consistent anomaly detection across domains. DACAD's strength lies in its dual ability to adapt to new data distributions and to distinguish between normal and anomalous patterns effectively. In DACAD, time series data is split into overlapping windows of size $W S$ with a stride of 1 , forming detailed source $(S)$ and target $(T)$ datasets. Windows in the source domain are classified based on anomaly presence: those with anomalies are marked as anomalous windows ( $S_{\text {anom }}$ ), and others as normal ( $S_{\text {norm }}$ ). Figure 1 shows the DACAD architecture, and Algorithm 1 details its steps. The following subsections explore DACAD's components and their functionalities.

### 3.1 Anomaly Injection

In the anomaly injection phase, we augment the original time series windows through a process of negative augmentation, thereby generating synthetic anomalous time series windows. This step is applied to all windows in the target domain $(T)$ and all normal windows in the source domain ( $S_{\text {norm }}$ ). We employ the anomaly injection method outlined in [Darban et al., 2023], which encompasses five distinct types of anomalies: Global, Seasonal, Trend, Shapelet, and Contextual. This procedure results in the creation of two new sets of time series windows. The first set, $S_{\mathrm{inj}}$, consists of anomaly-injected windows derived from the normal samples in the source domain. The second set, $T_{\mathrm{inj}}$, comprises anomaly-injected windows originating from the target domain data.

### 3.2 Pair Selection

In DACAD's pair selection step, appropriate triplets from source and target domains are created for CL. In the source

```
Algorithm 1 DACAD $(S, T, \alpha, \beta, \gamma, \lambda)$
Input: Source time series windows $S=\left\{w_{1}^{s}, w_{2}^{s}, \ldots, w_{|S|}^{s}\right\}$, Target
    time series windows $T=\left\{w_{1}^{t}, w_{2}^{t}, \ldots, w_{|T|}^{t}\right.$, Loss coefficients
    $\alpha, \beta \gamma, \lambda$
Output: Representation function $\phi^{R}$, classifier $\phi^{C L}$, centre $c$
    Initialise $\phi^{R}, \phi^{D}, \phi^{C L}, c$
    Split $S$ to $S_{\text {norm }}$ and $S_{\text {anom }}$
    Create $S_{\text {inj }}, T_{\text {inj }} \quad \triangleright$ Anomaly Injection (3.1)
    Form $S_{\text {triplets }}$ and $T_{\text {triplets }} \quad \triangleright$ Pair Selection (3.2)
    for each training iteration do
        Compute $\phi^{R}(S), \phi^{R}(T), \phi^{R}\left(S_{\text {Triplets }}\right), \phi^{R}\left(T_{\text {Triplets }}\right) \quad \triangleright(3.3)$
        Compute $\mathcal{L}_{\text {SupCont }}$ using Eq. (1) and $\phi^{R}\left(S_{\text {Triplets }}\right)$
        Compute $\mathcal{L}_{\text {SelfCont }}$ using Eq. (2) and $\phi^{R}\left(T_{\text {Triplets }}\right)$
        Compute $\mathcal{L}_{\text {Disc }}$ using Eq. (3), $\phi^{R}(S)$ and $\phi^{R}(T) \quad \triangleright(3.4)$
        Compute $\mathcal{L}_{\text {Cls }}$ using Eq. (4) and $\phi^{R}(S) \quad \triangleright(3.5)$
        $\mathcal{L}_{\text {DACAD }} \leftarrow \alpha \cdot \mathcal{L}_{\text {SupCont }}+\beta \cdot \mathcal{L}_{\text {SelfCont }}+\gamma \cdot \mathcal{L}_{\text {Disc }}+\lambda \cdot \mathcal{L}_{\text {Cls }}$
        Update model parameters to minimise $\mathcal{L}_{\text {DACAD }}$
    end for
    Return $\phi^{R}, \phi^{C L}, c$
```

domain, we use labels to form distinct lists of normal samples ( $S_{\text {norm }}$ ), anomalous samples ( $S_{\text {anom }}$ ), and anomaly-injected samples ( $S_{\mathrm{inj}}$ ). This allows for a supervised CL approach, enhancing differentiation between normal and anomalous samples. Here, triplets ( $S_{\text {triplets }}$ ) consist of an anchor (normal window from $S_{\text {norm }}$ ), a positive (different normal window from $S_{\text {norm }}$ ), and a negative (randomly selected from either an anomalous window from $S_{\text {anom }}$ or an anomaly-injected anchor from $S_{\text {inj }}$ ).

In the target domain, lacking ground truth labels, a selfsupervised approach shapes the triplets ( $T_{\text {triplets }}$ ). Each target triplet includes an anchor (original window from $T$ ), a positive (temporally close window to anchor, from $T$, likely sharing similar characteristics), and a negative (anomaly-injected anchor from $T_{\text {inj }}$ ).

### 3.3 Representation Layer ( $\phi^{R}$ )

In our model, we employ a TCN [Lea et al., 2016] for the representation layer, which is adept at handling both multivariate time series windows. This choice is motivated by the TCN's ability to capture temporal dependencies effectively, a critical aspect in time series analysis. The inputs to the TCN are the datasets and triplets from both domains, specifically $S$, $T, S_{\text {Triplets }}$, and $T_{\text {Triplets. }}$. The outputs of the TCN, representing the transformed feature space, are

- $\phi^{R}(S)$ : The representation of source windows.
- $\phi^{R}(T)$ : The representation of target windows.
- $\phi^{R}\left(S_{\text {Triplets }}\right)$ : The representation of source triplets.
- $\phi^{R}\left(T_{\text {Triplets }}\right)$ : The representation of target triplets.

Utilising the outputs from the representation layer $\phi^{R}$, we compute two distinct loss functions. These are the supervised mean margin contrastive loss for source domain data $\left(\mathcal{L}_{\text {SupCont }}\right)$ and the self-supervised contrastive triplet loss for target domain $\left(\mathcal{L}_{\text {SelfCont }}\right)$. These loss functions are critical for training our model to differentiate between normal and anomalous patterns in both source and target domains.

![](https://cdn.mathpix.com/cropped/2024_06_04_22d3ce01a9980333ad6fg-04.jpg?height=545&width=1778&top_left_y=194&top_left_x=171)

Figure 1: DACAD Model Overview: Involves source $(S)$ and target $(T)$ domains. Source domain uses normal ( $S_{\text {norm }}$ ) and anomalous ( $S_{\text {anom }}$ ) samples, plus synthetic anomalies ( $S_{\mathrm{inj}}$ ) for source triplets ( $S_{\text {Triplets }}$ ) and contrastive loss. Target domain similarly uses proximity-based pair selection and anomaly injection ( $T_{\text {inj }}$ ) to create target triplets ( $T_{\text {Triplets }}$ ). $\operatorname{TCN}\left(\phi^{R}\right)$ is used for feature extraction. Features from both domains are fed into discriminator ( $\phi^{D}$ ) for domain-invariant learning. Source features are classified by classifier $\phi^{C L}$.

## Supervised Mean Margin Contrastive Loss for Source Domain $\left(\mathcal{L}_{\text {SupCont }}\right)$

This loss aims to embed time series windows into a feature space where normal sequences are distinctively separated from anomalous ones. It utilises a triplet loss framework, comparing a base (anchor) window with both normal (positive) and anomalous (negative) windows [Khosla et al., 2020]. Our method diverges from traditional triplet loss by focusing on the average effect of all negatives within a batch. The formula is given by equation (1):

$$
\begin{align*}
& \mathcal{L}_{\text {SupCont }}=\frac{1}{|B|} \sum_{i=1}^{|B|} \max \\
& \left(\frac{1}{|N|} \sum_{j=1}^{|N|}\left(\left\|\phi^{R}\left(a_{i}^{s}\right)-\phi^{R}\left(p_{i}^{s}\right)\right\|_{2}^{2}-\left\|\phi^{R}\left(a_{i}^{s}\right)-\phi^{R}\left(n_{j}^{s}\right)\right\|_{2}^{2}+m\right), 0\right) \tag{1}
\end{align*}
$$

Here, $|B|$ is the batch size, and $|N|$ is the number of negative samples in the batch. The anchor $a_{i}^{s}$ is a normal time series window, and the positive pair $p_{i}^{s}$ is another randomly selected normal window. Negative pairs $n_{j}^{s}$ are either true anomalous windows or synthetically created anomalies through the anomaly injector module applied on the anchor $a_{i}^{s}$. This loss function uses true anomaly labels to enhance the separation between normal and anomalous behaviours by at least the margin $m$. It includes both genuine and injected anomalies as negatives, balancing ground truth and potential anomaly variations. The supervised mean margin contrastive loss offers several advantages for TAD: supervised learning (uses labels for better anomaly distinction), comprehensive margin (applies a margin over average distance to negatives), and flexible negative sampling (incorporates a mix of real and injected anomalies, enhancing robustness against diverse anomalous patterns).

## Self-supervised Contrastive Triplet Loss for Target Domain $\left(\mathcal{L}_{\text {SelfCont }}\right)$

For the target domain, we employ a self-supervised contrastive approach using triplet loss [Schroff et al., 2015], designed to ensure that the anchor window is closer to a positive window than to a negative window by a specified margin. The self-supervised triplet loss formula is shown in equation (2):

$$
\begin{align*}
& \mathcal{L}_{\text {Selff Cont }}= \\
& \frac{1}{|B|} \sum_{i=1}^{|B|} \max \left(\left\|\phi^{R}\left(a_{i}^{t}\right)-\phi^{R}\left(p_{i}^{t}\right)\right\|_{2}^{2}-\left\|\phi^{R}\left(a_{i}^{t}\right)-\phi^{R}\left(n_{i}^{t}\right)\right\|_{2}^{2}+m, 0\right) \tag{2}
\end{align*}
$$

In this setup, the anchor window $a_{i}^{t}$ is compared to a positive window $p_{i}^{t}$ (a randomly selected window in temporal proximity to the anchor from $T$ ) and a negative window $n_{i}^{t}$ (the anomaly-injected version of the anchor from $T_{\mathrm{inj}}$ ), ensuring the anchor is closer to the positive than the negative by at least the margin $m$.

### 3.4 Discriminator Component ( $\phi^{D}$ )

The model incorporates a discriminator component within an adversarial framework. This component is crucial for ensuring that the learned features are not only relevant for the anomaly detection task but also general enough to be applicable across different domains. The discriminator is specifically trained to distinguish between representations from the source and target domains.

Designed to differentiate between features extracted from the source and target domains, the discriminator employs adversarial training techniques similar to those found in Generative Adversarial Networks (GANs) [Creswell et al., 2018]. Discriminator $\phi^{D}$ is trained to differentiate between source and target domain features, while $\phi^{R}$ is conditioned to produce domain-invariant features.

A crucial element is the Gradient Reversal Layer [Ganin and Lempitsky, 2015], which functions normally during forward passes but reverses gradient signs during backpropagation. This setup enhances the training of $\phi^{D}$ and simultaneously adjusts $\phi^{R}$ to produce features that challenge $\phi^{D}$.

The discriminator's training involves balancing its loss against other model losses. Effective domain adaptation occurs when $\phi^{R}$ yields discriminator accuracy close to random guessing. $\phi^{D}$, taking $\phi^{R}(S)$ and $\phi^{R}(T)$ as inputs, classifies them as belonging to the source or target domain. Its loss,
a binary classification problem, minimizes classification error for source and target representations. The loss function for the discriminator $\phi^{D}$ is defined using the Binary CrossEntropy (BCE) loss, as shown in equation (3):

$$
\begin{align*}
& \left.\left.\mathcal{L}_{\text {Disc }}=-\frac{1}{|S|+|T|}\left(\sum_{i=1}^{|S|} \log \left(f\left(w_{i}^{s}\right)\right)\right)+\sum_{j=1}^{|T|} \log \left(1-f\left(w_{j}^{t}\right)\right)\right)\right)  \tag{3}\\
& \text { where } f(w)=\phi^{D}\left(\phi^{R}(w)\right)
\end{align*}
$$

Here, $|S|$ and $|T|$ are the source and target window counts. $w_{i}^{s}$ and $w_{j}^{t}$ represent the $i^{t h}$ and $j^{t h}$ windows from $S$ and $T$. The function $\phi^{D}\left(\phi^{R}(\cdot)\right)$ computes the likelihood of a window being from the source domain.

### 3.5 Centre-based Entropy Classifier ( $\phi^{C L}$ )

Extending the DeepSVDD [Ruff et al., 2018] designed for anomaly detection, the $\mathrm{CEC}\left(\phi^{C L}\right.$ ) in DACAD is proposed as an effective anomaly detection classifier in the source domain. It assigns anomaly scores to time series windows, using labelled data from the source domain $S$ for training and applying the classifier to target domain data $T$ during inference. It is centred around a Multi-Layer Perceptron (MLP) with a unique "centre" parameter crucial for classification.

The classifier operates by spatially separating transformed time series window representations $\left(\phi^{R}\left(w_{i}^{s}\right)\right)$ in the feature space relative to a predefined "centre" $c$. The MLP's aim is to draw normal sample representations closer to $c$ and push anomalous ones further away. This spatial reconfiguration is quantified using a distance metric, forming the basis for anomaly scoring. A sample closer to $c$ is considered more normal, and vice versa. The effectiveness of this spatial adjustment is measured using a loss function based on $\mathrm{BCE}$, expressed in equation (4):

$\mathcal{L}_{\mathrm{Cls}}=-\frac{1}{|S|} \sum_{i=1}^{|S|}\left[y_{i} \cdot \log \left(\left\|g\left(w_{i}^{s}\right)-c\right\|_{2}^{2}\right)+\left(1-y_{i}\right) \cdot \log \left(1-\left\|g\left(w_{i}^{s}\right)-c\right\|_{2}^{2}\right)\right]$ where $g(w)=\phi^{C L}\left(\phi^{R}(w)\right)$

In this equation, $|S|$ is the number of samples in $S, w_{i}^{s}$ is the $i^{\text {th }}$ window in $S$, and $y_{i}$ is its ground truth label, with 1 for normal and 0 for anomalous samples.

The loss function is designed to minimise the distance between normal samples and $c$ while maximising it for anomalous samples. These distances are directly used as anomaly scores, offering a clear method for anomaly detection.

### 3.6 Overall Loss in DACAD

The overall loss function in the DACAD model is the amalgamation of four distinct loss components, each contributing to the model's learning process in different aspects. These components are the Supervised Contrastive Loss ( $\mathcal{L}_{\text {SupCont }}$ ), the Self-Supervised Contrastive Loss ( $\mathcal{L}_{\text {SelfCont }}$ ), the Discriminator Loss $\left(\mathcal{L}_{\text {Disc }}\right)$, and the Classifier Loss $\left(\mathcal{L}_{\mathrm{Cls}}\right)$. The overall loss function for DACAD denoted as $\mathcal{L}_{\text {DACAD }}$, is formulated as a weighted sum of these components (with a specific weight: $\alpha$ for $\mathcal{L}_{\text {SupCont }}, \beta$ for $\mathcal{L}_{\text {SelfCont }}, \gamma$ for $\mathcal{L}_{\text {Disc }}$, and $\lambda$ for $\mathcal{L}_{\mathrm{Cls}}$ ), as shown in equation (5):

$$
\begin{equation*}
\mathcal{L}_{\text {DACAD }}=\alpha \cdot \mathcal{L}_{\text {SupCont }}+\beta \cdot \mathcal{L}_{\text {SelfCont }}+\gamma \cdot \mathcal{L}_{\text {Disc }}+\lambda \cdot \mathcal{L}_{\mathrm{Cls}} \tag{5}
\end{equation*}
$$

Table 1: Statistics of the benchmark datasets used.

| Benchmark | \# datasets | \# dims | Train size | Test size | Anomaly ratio |
| :---: | :---: | :---: | :---: | :---: | :---: |
| MSL [Hundman et al., 2018] | 27 | 55 | 58,317 | 73,729 | $10.72 \%$ |
| SMAP [Hundman et al., 2018] | 55 | 25 | 140,825 | 444,035 | $13.13 \%$ |
| SMD [Su et al., 2019] | 28 | 38 | 708,405 | 708,420 | $4.16 \%$ |
| Boiler [Cai et al., 2021] | 3 | 274 | 277,152 | 277,152 | $15.00 \%$ |

The overall loss function $\mathcal{L}_{\text {DACAD }}$ is what the model seeks to optimise during the training process. By fine-tuning these weights ( $\alpha, \beta, \gamma$, and $\lambda$ ), the model can effectively balance the significance of each loss component. This balance is crucial as it allows the model to cater to specific task requirements.

### 3.7 DACAD's Inference

In the inference phase of the DACAD model, the primary objective is to identify anomalies in the target domain $T$. This is accomplished for each window $w^{t}$ in $T$. The anomaly detection is based on the concept of spatial separation in the feature space, as established during the training phase. The anomaly score for each window is derived from the spatial distance between the classifier's output and a predefined centre $c$ in the feature space. This score is calculated as the squared Euclidean distance, as shown in Equation 6:

$$
\begin{equation*}
\text { Anomaly Score }\left(w^{t}\right)=\left\|\phi^{C L}\left(\phi^{R}\left(w^{t}\right)\right)-c\right\|_{2}^{2} \tag{6}
\end{equation*}
$$

Where $\phi^{C L}\left(\phi^{R}\left(w^{t}\right)\right)$ denotes the feature representation of the window $w^{t}$ after processing by the representation layer $\phi^{R}$ and the classifier layer $\phi^{C L}$. The distance measured here quantifies how much each window in the target domain deviates from the "normal" pattern, as defined by the centre $c$.

The anomaly score is a crucial metric in this process. A higher score suggests a significant deviation from the normative pattern, indicating a higher likelihood of the window being an anomaly. Conversely, a lower score implies that the window's representation is closer to the centre, suggesting it is more likely to be normal.

In practical terms, the anomaly score can be thresholded to classify windows as either normal or anomalous. By effectively utilising these anomaly scores, the DACAD model provides a robust mechanism for identifying anomalous patterns in unlabelled target domain data.

## 4 Experiments

The aim of this section is to provide a comprehensive evaluation of DACAD, covering the experimental setups (Section 4.2) and results (Section 4.3 and 4.4), to clearly understand its performance and capabilities in different contexts.

### 4.1 Datasets

We evaluate the performance of the proposed model and make comparisons of the results across the four datasets, including the three most commonly used real benchmark datasets for TAD and the dataset used for time series domain adaptation. The datasets are summarised in Table 1.

### 4.2 Experimental Setup

In our study, we evaluate SOTA TAD models including OmniAnomaly [Su et al., 2019], TS2Vec [Yue et al., 2022], THOC [Shen et al., 2020], and DCdetector [Yang et al., 2023]
and SOTA UDA models that support multivariate time series classification including VRADA [Purushotham et al., 2016] and CLUDA [Ozyurt et al., 2023] on benchmark datasets previously mentioned in section 1 using their source code and best hyper-parameters as they stated to ensure a fair evaluation. The hyper-parameters used in our implementation are as follows: DACAD consists of a 3-layer TCN architecture with three different channel sizes [128, 256, 512] to capture temporal dependencies. The dimension of the representation is 1024. We use the same hyper-parameters across all datasets to evaluate DACAD: window size $(W S)=100$, margin $m=$ 1 , and we run our model for 20 epochs.

### 4.3 Baselines Comparison

Table 2 provides a comprehensive comparison of various models' performance on multivariate time series benchmark datasets, using F1 score, AUPR (Area Under the PrecisionRecall Curve), and AUROC (Area Under the Receiver Operating Characteristic Curve). Despite Point Adjustment (PA) popularity in recent years, we do not use PA when calculating these metrics due to Kim et al.'s findings that its application leads to an overestimation of a TAD model's capability and can bias results in favour of methods that produce extreme anomaly scores. Instead, we use conventional performance metrics for anomaly detection.

Benchmark datasets like SMD contain multiple time series that cannot be merged due to missing timestamps, making them unsuitable for averaging their F1 scores. The F1 score is a non-additive metric combining precision and recall. To address this, we compute individual confusion matrices for each time series. These matrices are then aggregated into a collective confusion matrix for the entire dataset. From this aggregated matrix, we calculate the overall precision, recall, and F1 score, ensuring an accurate and undistorted representation of the dataset's F1 score. For datasets with multiple time series, we present the mean and standard deviation of AUPR and AUROC for each series.

DACAD emerges as the top performer in Table 2, consistently achieving the best results across all scenarios and metrics, as highlighted in bold. This suggests its robustness and adaptability in handling normal and anomalous representations of time series for anomaly detection. VRADA and CLUDA models often rank as the second best, with their results underlined in several instances. Other models like OmniAnomaly, THOC, TS2Vec, and DCdetector demonstrate more uniform performance across various metrics but generally do not reach the top performance levels seen in DACAD, VRADA, or CLUDA. Their consistent but lower performance could make them suitable for applications where top-tier accuracy is less critical.

To assess our model against ContextDA [Lai et al., 2023] -the only exiting UDA for TAD model-, we use the same datasets and metrics reported in ContextDA's main paper, as its code is unavailable. On the SMD dataset, ContextDA achieves an average macro F1 of 0.63 and AUROC of 0.75, whereas our model achieves a higher average macro F1 of 0.81 and AUROC of 0.86 . Similarly, on the Boiler dataset, ContextDA's average macro F1 is 0.50 and AUROC 0.65, compared to our model's superior performance with an av- erage macro F1 of 0.63 and AUROC of 0.71. Details of these results are provided in the appendix, due to page limitations.

### 4.4 Ablation Study

Our ablation study focused on the following aspects: (1) Effect of the loss components, (2) Effect of CEC, and (3) Effect of anomaly injection.

Effect of the loss components: Table 3 offers several key insights into the impact of different loss components on the MSL dataset using F-5 as a source. Removing the target self-supervised CL (w/o $\mathcal{L}_{\text {SelfCont }}$ ) leads to lower metrics (F1: 0.481, AUPR: 0.495, AUROC: 0.697). Moreover, excluding source supervised CL (w/o $\mathcal{L}_{\text {SupCont }}$ ) reduces effectiveness (F1: 0.463, AUPR: 0.427, AUROC: 0.639), highlighting its role in capturing source-specific features which are crucial for the model's overall accuracy. Similarly, omitting the discriminator component results in performance reduction (F1: 0.471, AUPR: 0.484, AUROC: 0.699). However, the most significant decline occurs without the classifier (F1: 0.299, AUPR: 0.170, AUROC: 0.503), underscoring its crucial role in effectively distinguishing between nor$\mathrm{mal} /$ anomaly classes. Overall, the best results (F1: 0.595, AUPR: 0.554, AUROC: 0.787) are achieved with all components, highlighting the effectiveness of an integrated approach.

Overall, each component within the model plays a crucial role in enhancing its overall performance. The highest performance across all metrics (F1: 0.595, AUPR: 0.554, AUROC: 0.787 ) is achieved when all components are included.

To elucidate the effectiveness of UDA within DACAD, we examine the feature representations from the MSL dataset, as illustrated in Figure 2. It presents the t-SNE 2D embeddings of DACAD feature representations $\phi^{C L}\left(\phi^{R}(w)\right)$ for MSL dataset. Each point represents a time series window, which can be normal, anomalous or anomaly-injected. These representations highlight the domain discrepancies between source and target entities and demonstrate how DACAD aligns the time series window features effectively. The comparison of feature representations with and without UDA reveals a significant domain shift when UDA is not employed, between source (entity F-5) and target (entity T-5) within MSL dataset.

Effect of CEC: Table 4 compares the performance of our CEC classifier with two BCE and DeepSVDD - on the MSL dataset using F-5 as a source. Our proposed CEC shows superior performance compared to BCE and DeepSVDD across three metrics on the MSL dataset. With the highest F1 score, it demonstrates a better balance of precision and recall. Its leading performance in AUPR indicates greater effectiveness in identifying positive classes in imbalanced datasets. Additionally, CEC's higher AUROC suggests it is more capable of distinguishing between classes.

Effect of anomaly injection: We study the impact of anomaly injection in Table 5, on MSL dataset when using F-5 as a source. It shows that anomaly injection significantly improves all metrics (F1: 0.595, AUPR: 0.554, AUROC: 0.787), enhancing the model's ability to differentiate between normal and anomalous patterns, thereby improving DACAD's overall accuracy. Without anomaly injection, there's a notable decline in performance, emphasizing its role in precision. The

Table 2: F1, AUPR, and AUROC results for various models on multivariate time series benchmark datasets (SMD, MSL, SMAP). The most optimal evaluation results are displayed in bold, while the second best ones are indicated by underline.

| $\underset{2}{2}$ | $\overline{\mathrm{g}}$ <br> $\bar{v}$ | src | SMD |  |  |  | MSL |  |  |  | SMAP |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  | $1-1$ | $2-3$ | $3-7$ | $1-5$ | F-5 | P-10 | D-14 | C-1 | A-7 | P-2 | E-8 | D-7 |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_22d3ce01a9980333ad6fg-07.jpg?height=198&width=35&top_left_y=360&top_left_x=191) | $\frac{\widehat{\hat{}}}{2}$ | F1 | $\underline{0.511}$ | 0.435 | 0.268 | 0.291 | 0.302 | 0.321 | 0.310 | 0.285 | 0.244 | 0.282 | 0.349 | 0.226 |
|  |  | AUPR | $\underline{0.523 \pm 0.223}$ | $0.433 \pm 0.172$ | $0.316 \pm 0.208$ | $0.292 \pm 0.147$ | $0.201 \pm 0.133$ | $0.199 \pm 0.159$ | $\underline{0.351 \pm 0.244}$ | $0.167 \pm 0.132$ | $0.133 \pm 0.196$ | $\underline{0.265 \pm 0.263}$ | $0.261 \pm 0.247$ | $0.120 \pm 0.149$ |
|  |  | AUROC | $302 \pm 0.130$ | $0.731 \pm 0.121$ | $0.597 \pm 0.146$ | $0.651 \pm 0.088$ | $0.526 \pm 0.054$ | $0.516 \pm 0.097$ | $0.615 \pm 0.207$ | $0.506 \pm 0.024$ | $0.474 \pm 0.176$ | $\underline{0.573 \pm 0.162}$ | $0.612 \pm 0.120$ | $0.443 \pm 0.170$ |
|  | $\hat{E}$ | FI | 0.435 | $\underline{0.487}$ | 0.320 | 0.314 | $\underline{0.395}$ | $\underline{0.368}$ | $\underline{0.324}$ | 0.312 | 0.292 | 0.278 | $\underline{0.384}$ | 0.293 |
|  |  | AUPR | $0.423 \pm 0.223$ | $\underline{0.494 \pm 0.174}$ | $\underline{0.400 \pm 0.201}$ | $\underline{0.328 \pm 0.140}$ | $\underline{0.325 \pm 0.263}$ | $0.239 \pm 1617$ | $0.318 \pm 0.276$ | $0.193 \pm 1640$ | $\underline{0.250 \pm 0.290}$ | $0.242 \pm 0.217$ | $\underline{0.332 \pm 0.263}$ | $\underline{0.193 \pm 0.220}$ |
|  |  | AUROC | $0.788 \pm 0.124$ | $\underline{0.794 \pm 0.121}$ | $\underline{0.725 \pm 0.155}$ | $\underline{0.693 \pm 0.119}$ | $\underline{0.579 \pm 0.204}$ | $0.569 \pm 0.132$ | $0.558 \pm 0.222$ | $0.479 \pm 0.164$ | $\underline{0.560 \pm 0.223}$ | $0.500 \pm 0.209$ | $\underline{0.692 \pm 0.171}$ | $0.480 \pm 0.222$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_22d3ce01a9980333ad6fg-07.jpg?height=506&width=43&top_left_y=559&top_left_x=190) | $\bar{E}$ | F | 0.397 | 0.413 | $\underline{0.419}$ | $\underline{0.421}$ | 0.254 | 0.266 | 0.246 | 0.242 | 0.320 | 0.326 | 0.331 | 0.318 |
|  |  | AUPR | $0.296 \pm 0.155$ | $0.309 \pm 0.165$ | $0.314 \pm 0.171$ | $0.318 \pm 0.182$ | $0.154 \pm 0.184$ | $0.154 \pm 0.184$ | $0.152 \pm 0.185$ | $0.148 \pm 0.186$ | $0.111 \pm 0.129$ | $0.114 \pm 0.130$ | $0.116 \pm 0.131$ | $0.112 \pm 0.131$ |
|  |  | AUROC | $0.619 \pm 0.153$ | $0.628 \pm 0.164$ | $0.639 \pm 0.156$ | $0.620 \pm 0.155$ | $0.392 \pm 0.173$ | $0.392 \pm 0.173$ | $0.398 \pm 0.165$ | $0.386 \pm 0.175$ | $0.421 \pm 0.189$ | $0.406 \pm 0.188$ | $0.406 \pm 0.190$ | $0.414 \pm 0.181$ |
|  | O | F1 | 0.156 | 0.167 | 0.168 | 0.168 | 0.307 | 0.317 | 0.309 | 0.310 | 0.321 | 0.328 | 0.335 | 0.320 |
|  |  | AUPR | $0.090 \pm 0.095$ | $0.106 \pm 0.129$ | $0.109 \pm 0.128$ | $0.109 \pm 0.128$ | $0.241 \pm 0.278$ | $\underline{0.247 \pm 0.275}$ | $0.244 \pm 0.277$ | $\underline{0.242 \pm 2778}$ | $0.191 \pm 0.263$ | $0.197 \pm 0.264$ | $0.200 \pm 0.266$ | $0.191 \pm 0.264$ |
|  |  | AUROC | $0.646 \pm 1.550$ | $0.646 \pm 0.154$ | $0.657 \pm 0.161$ | $0.648 \pm 0.158$ | $0.631 \pm 0.177$ | $\underline{0.637 \pm 0.182}$ | $\underline{0.637 \pm 0.182}$ | $\underline{0.640 \pm 0.183}$ | $0.535 \pm 0.211$ | $0.539 \pm 0.211$ | $0.543 \pm 0.211$ | $\underline{0.539 \pm 0.213}$ |
|  | $\stackrel{0}{\Delta}$ <br> $\stackrel{2}{c}$ | F1 | 0.171 | 0.173 | 0.173 | 0.173 | 0.320 | 0.316 | 0.317 | $\underline{0.313}$ | $\underline{0.362}$ | $\underline{0.368}$ | 0.374 | $\underline{0.362}$ |
|  |  | AUPR | $0.112 \pm 0.076$ | $0.112 \pm 0.076$ | $0.116 \pm 0.075$ | $0.116 \pm 0.075$ | $0.137 \pm 0.137$ | $0.138 \pm 0.136$ | $0.135 \pm 0.138$ | $0.133 \pm 0.138$ | $0.145 \pm 0.167$ | $0.147 \pm 0.167$ | $0.149 \pm 0.168$ | $0.143 \pm 0.165$ |
|  |  | AUROC | $0.492 \pm 0.046$ | $0.493 \pm 0.045$ | $0.489 \pm 0.044$ | $0.487 \pm 0.040$ | $0.511 \pm 0.096$ | $0.514 \pm 0.096$ | $0.514 \pm 0.096$ | $0.513 \pm 0.096$ | $0.504 \pm 0.088$ | $0.506 \pm 0.088$ | $0.507 \pm 0.089$ | $0.507 \pm 0.089$ |
|  | ![](https://cdn.mathpix.com/cropped/2024_06_04_22d3ce01a9980333ad6fg-07.jpg?height=108&width=38&top_left_y=859&top_left_x=239) | $\mathrm{Fl}$ | 0.079 | 0.085 | 0.085 | 0.083 | 0.208 | 0.215 | 0.203 | 0.201 | 0.280 | 0.283 | 0.287 | 0.278 |
|  |  | AUPR | $0.041 \pm 0.036$ | $0.044 \pm 0.037$ | $0.044 \pm 0.037$ | $0.044 \pm 0.037$ | $0.124 \pm 0.139$ | $0.125 \pm 0.138$ | $0.122 \pm 0.140$ | $0.122 \pm 0.140$ | $0.128 \pm 0.158$ | $0.129 \pm 0.159$ | $0.132 \pm 0.159$ | $0.127 \pm 0.157$ |
|  |  | AUROC | $0.496 \pm 0.008$ | $0.495 \pm 0.008$ | $0.495 \pm 0.008$ | $0.495 \pm 0.008$ | $0.501 \pm 0.011$ | $0.501 \pm 0.011$ | $0.501 \pm 0.011$ | $0.501 \pm 0.011$ | $0.516 \pm 0.082$ | $0.513 \pm 0.079$ | $0.516 \pm 0.082$ | $0.517 \pm 0.082$ |
|  | $\hat{\imath}$ <br> Ãš <br> 0 | F1 | 0.600 | 0.633 | 0.598 | 0.572 | 0.595 | 0.528 | 0.522 | 0.528 | 0.532 | 0.464 | 0.651 | 0.463 |
|  |  | AUPR | $0.528 \pm 0.245$ | $0.605 \pm 0.196$ | $0.565 \pm 0.211$ | $0.535 \pm 0.219$ | $0.554 \pm 0.268$ | $0.448 \pm 0.280$ | $0.475 \pm 0.304$ | $0.514 \pm 0.273$ | $\mathbf{0 . 4 8 3} \pm 0.288$ | $0.447 \pm 0.270$ | $0.550 \pm 0.270$ | $0.388 \pm 0.270$ |
|  |  | AUROC | $0.856 \pm 0.094$ | $0.858 \pm 0.083$ | $\mathbf{0 . 8 2 2} \pm 0.093$ | $0.813 \pm 0.098$ | $0.787 \pm 0.268$ | $\mathbf{0 . 7 4 8} \pm 0.142$ | $0.719 \pm 0.191$ | $0.769 \pm 0.151$ | $\mathbf{0 . 7 6 0} \pm 0.151$ | $0.687 \pm 0.202$ | $\mathbf{0 . 7 9 0} \pm \mathbf{0 . 1 0 5}$ | $0.619 \pm 0.234$ |

Table 3: Effect of loss components on MSL dataset (source: F-5).

| Component | F1 | AUPR | AUROC |
| :---: | :---: | :---: | :---: |
| DACAD | $\mathbf{0 . 5 9 5}$ | $\mathbf{0 . 5 5 4} \pm \mathbf{0 . 2 6 8}$ | $\mathbf{0 . 7 8 7} \pm \mathbf{0 . 2 6 8}$ |
| w/o $\mathcal{L}_{\text {SelfCont }}$ | 0.481 | $0.495 \pm 0.291$ | $0.697 \pm 0.203$ |
| w/o $\mathcal{L}_{\text {SupCont }}$ | 0.463 | $0.427 \pm 0.274$ | $0.639 \pm 0.198$ |
| w/o $\mathcal{L}_{\text {Disc }}$ | 0.471 | $0.484 \pm 0.315$ | $0.699 \pm 0.229$ |
| w/o $\mathcal{L}_{\text {Cls }}$ | 0.299 | $0.170 \pm 0.127$ | $0.503 \pm 0.018$ |

Table 4: Effect of CEC classifier on MSL dataset (source: F-5).

| Classifier | F1 | AUPR | AUROC |
| :---: | :---: | :---: | :---: |
| Our proposed CEC | $\mathbf{0 . 5 9 5}$ | $\mathbf{0 . 5 5 4} \pm \mathbf{0 . 2 6 8}$ | $\mathbf{0 . 7 8 7} \pm \mathbf{0 . 2 6 8}$ |
| BCE-based | 0.467 | $0.504 \pm 0.304$ | $0.670 \pm 0.241$ |
| DeepSVDD | 0.428 | $0.400 \pm 0.250$ | $0.595 \pm 0.185$ |

Table 5: Effect of anomaly injection on MSL dataset (source: F-5).

| Approach | F1 | AUPR | AUROC |
| :---: | :---: | :---: | :---: |
| with injection | $\mathbf{0 . 5 9 5}$ | $\mathbf{0 . 5 5 4} \pm \mathbf{0 . 2 6 8}$ | $\mathbf{0 . 7 8 7} \pm \mathbf{0 . 2 6 8}$ |
| w/o injection | 0.489 | $0.461 \pm 0.276$ | $0.682 \pm 0.666$ |

higher standard deviation in AUROC scores without injection suggests more variability and less stability in the model's performance. The study underscores the vital role of anomaly injection in improving anomaly detection models. It reveals that incorporating anomaly injection not only boosts detection accuracy but also enhances the model's overall stability and reliability.

## 5 Conclusion

The DACAD model stands as a notable innovation in the field of TAD, particularly effective in environments with limited labelled data. By melding domain adaptation and contrastive learning, it applies labelled data from one domain to detect anomalies in another. Its anomaly injection mechanism, introducing a spectrum of synthetic anomalies, significantly bolsters the model's adaptability and robustness across var-

![](https://cdn.mathpix.com/cropped/2024_06_04_22d3ce01a9980333ad6fg-07.jpg?height=731&width=637&top_left_y=1109&top_left_x=1205)

![](https://cdn.mathpix.com/cropped/2024_06_04_22d3ce01a9980333ad6fg-07.jpg?height=287&width=295&top_left_y=1128&top_left_x=1224)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_22d3ce01a9980333ad6fg-07.jpg?height=306&width=290&top_left_y=1473&top_left_x=1210)

(c)

![](https://cdn.mathpix.com/cropped/2024_06_04_22d3ce01a9980333ad6fg-07.jpg?height=292&width=309&top_left_y=1128&top_left_x=1512)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_22d3ce01a9980333ad6fg-07.jpg?height=303&width=306&top_left_y=1472&top_left_x=1517)

(d)
Figure 2: Impact of UDA on DACAD feature representations. It contrasts the embeddings of (a) source entity F-5 with UDA, (b) target entity T-5 with UDA, (c) source entity F-5 without UDA, and (d) target entity T-5 without UDA.

ious domains. Our evaluations on diverse real-world datasets establish DACAD's superiority in handling domain shifts and outperforming existing models. Its capability to generalise and accurately detect anomalies, regardless of the scarcity of labelled data in the target domain, represents a significant contribution to TAD. In future work, we aim to refine the anomaly injection process further, enhancing the model's ability to simulate a broader range of anomalous patterns. Additionally, we plan to evolve the model to encompass univariate time series analysis, broadening its scope and utility.

## A Datasets' Descriptions

Mars Science Laboratory (MSL) and Soil Moisture Active Passive (SMAP) ${ }^{1}$ datasets [Hundman et al., 2018] are real-world datasets collected from NASA spacecraft. These datasets contain anomaly information derived from reports of incident anomalies for a spacecraft monitoring system. MSL and SMAP comprises of 27 and 55 datasets respectively and each equipped with a predefined train/test split, where, unlike other datasets, their training set are unlabeled.

Server Machine Dataset (SMD) ${ }^{2}$ [Su et al., 2019] is gathered from 28 servers, incorporating 38 sensors, over a span of 10 days. During this period, normal data was observed within the initial 5 days, while anomalies were sporadically injected during the subsequent 5 days. The dataset is also equipped with a predefined train/test split, where the training data is unlabeled.

Boiler Fault Detection Dataset. ${ }^{3}$ [Cai et al., 2021] The Boiler dataset includes sensor information from three separate boilers, with each boiler representing an individual domain. The objective of the learning process is to identify the malfunctioning blowdown valve in each boiler. Obtaining samples of faults is challenging due to their scarcity in the mechanical system. Therefore, it's crucial to make use of both labelled source data and unlabeled target data to enhance the model's ability to generalise.

## B Baselines

Below, we will provide an enhanced description of the UDA models for time series classification and anomaly detection.

Additionally, we provide a description of the four prominent and state-of-the-art TAD models that were used for comparison with DACAD. We have selected the models from different categories of TAD, namely, unsupervised reconstruction-based (OmniAnomaly [Su et al., 2019] model, unsupervised forecasting-based (THOC [Shen et al., 2020]), and self-supervised contrastive learning (TS2Vec [Yue et al., 2022] and DCdetector [Yang et al., 2023]) TAD models.

VRADA ${ }^{4}$ combines deep domain confusion [?] with variational recurrent adversarial deep domain adaptation [Purushotham et al., 2016], which simultaneously optimises source domain label prediction, MMD and domain discrimination with the latent representations generated by the LSTM encoder. Meanwhile, the reconstruction objective of AELSTM is performed to detect anomalies.

CLUDA $^{5}$ is a novel framework developed for UDA in time series data and evaluated on time series classification. This framework utilises a contrastive learning approach to learn domain-invariant semantics within multivariate time series data, aiming to preserve label information relevant to prediction tasks. CLUDA is distinctive as it is the first UDA frame-[^1]

work designed to learn contextual representations of time series data, maintaining the integrity of label information. Its effectiveness is demonstrated through evaluations of various time series classification datasets.

ContextDA ${ }^{6}$ is a sophisticated approach for detecting anomalies in time series data. It employs context sampling formulated as a Markov decision process and uses deep reinforcement learning to optimise the domain adaptation process. This model is designed to generate domain-invariant features for better anomaly detection across various domains. It has shown promise in transferring knowledge between similar or entirely different domains.

OmniAnomaly ${ }^{7}$ is a model operating on an unsupervised basis, employing a Variational Autoencoder (VAE) to handle multivariate time series data. It identifies anomalies by evaluating the reconstruction likelihood of specific data windows.

THOC ${ }^{8}$ utilises a multi-layer dilated recurrent neural network (RNN) alongside skip connections in order to handle contextual information effectively. It adopts a temporal hierarchical one-class network approach for detecting anomalies.

TS2 $\mathbf{V e c}^{9}$ is an unsupervised model that is capable of learning multiple contextual representations of MTS and UTS semantically at various levels. This model employs contrastive learning in a hierarchical way, which provides a contextual representation. A method within TS2Vec has been proposed for application in TAD.

DCdetector. ${ }^{10}$ is distinctive for its use of a dual attention asymmetric design combined with contrastive learning. Unlike traditional models, DCdetector does not rely on reconstruction loss for training. Instead, it utilises pure contrastive loss to guide the learning process. This approach enables the model to learn a permutation invariant representation of time series anomalies, offering superior discrimination abilities compared to other methods.

## C Extended Experiments

## C. 1 UDA Comparison

Table 6, adapted from [Lai et al., 2023], now includes the results of our model in its final column. This table provides a comprehensive comparison of the performances of various models on the SMD and Boiler datasets, as measured by Macro F1 and AUROC scores.

In this comparative analysis, the DACAD model distinctly outperforms others, consistently achieving the highest scores in both Macro F1 and AUROC across the majority of test cases in both datasets. Its performance is not only superior but also remarkably stable across different scenarios within each dataset, showcasing its robustness and adaptability to diverse data challenges.

Following DACAD, the ContexTDA model frequently ranks as the runner-up, particularly in the SMD dataset, where[^2]

Table 6: Macro F1/AUROC results on SMD and Boiler dataset. The most optimal evaluation results are displayed in bold, while the second best ones are indicated by underline.

| æ…ˆ | $\mathbf{s r c} \mapsto \operatorname{trg}$ | Models (Macro F1/AUROC) |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | AE-MLP | AE_LSTM | RDC | VRADA | SASA | ContexTDA | DACAD |
| $\sum_{k}^{1}$ | $1-1 \mapsto 1-2$ | $0.72 / 0.83$ | $0.74 / 0.90$ | $0.74 / 0.89$ | $0.74 / 0.91$ | $0.59 / 0.63$ | $\underline{0.75} / 0.91$ | $\mathbf{0 . 8 0} 0.84$ |
|  | $1-1 \mapsto 1-3$ | $0.57 / 0.70$ | $0.49 / 0.41$ | $0.57 / 0.72$ | $0.49 / 0.41$ | $\underline{0.61} / \mathbf{0 . 9 0}$ | $0.57 / 0.75$ | $0.71 / \underline{0.84}$ |
|  | $1-1 \mapsto 1-4$ | $0.55 / 0.74$ | $0.54 / 0.41$ | $0.54 / 0.75$ | $0.54 / 0.41$ | $0.55 / 0.75$ | $\underline{0.59} / \underline{0.76}$ | $0.75 / \overline{0.83}$ |
|  | $1-1 \mapsto 1-5$ | $0.54 / 0.79$ | $0.55 / 0.84$ | $0.56 / 0.85$ | $0.55 / 0.79$ | $0.65 / 0.87$ | $\underline{0.66} / \underline{0.87}$ | $0.82 / 0.96$ |
|  | $1-1 \mapsto 1-6$ | $0.71 / 0.88$ | $0.71 / 0.91$ | $0.71 / 0.87$ | $0.71 / \underline{0.91}$ | $0.44 / 0.84$ | $\underline{0.73} / \overline{0.84}$ | $0.89 / 0.92$ |
|  | $1-1 \mapsto 1-7$ | $0.48 / 0.55$ | $0.48 / 0.50$ | $0.49 / 0.54$ | $0.48 / \overline{0.50}$ | $0.31 / \underline{0.57}$ | $\underline{0.51} / 0.53$ | $0.88 / 0.91$ |
|  | $1-1 \mapsto 1-8$ | $0.55 / 0.57$ | $0.53 / \underline{0.70}$ | $0.55 / 0.58$ | $0.54 / 0.56$ | $0.52 / 0.56$ | $\underline{0.58} / 0.58$ | $0.79 / 0.72$ |
| çŽ„ | $1 \mapsto 2$ | $0.44 / 0.64$ | $0.43 / 0.48$ | $0.43 / 0.54$ | $0.43 / 0.48$ | $\underline{0.53 / 0.88}$ | $0.50 / 0.59$ | $0.61 / 0.66$ |
|  | $1 \mapsto 3$ | $0.40 / 0.28$ | $0.40 / 0.11$ | $0.43 / 0.49$ | $0.42 / 0.18$ | $\overline{0.41} / 0.48$ | $\underline{0.50} / \underline{0.67}$ | $0.74 / \overline{0.81}$ |
|  | $2 \mapsto 1$ | $0.39 / 0.18$ | $0.40 / 0.21$ | $0.40 / 0.36$ | $0.40 / 0.15$ | $\underline{0.53} / \mathbf{0 . 9 0}$ | $\overline{0.51} / \overline{0.66}$ | $0.58 / \underline{0.65}$ |
|  | $2 \mapsto 3$ | $0.40 / 0.38$ | $0.40 / 0.20$ | $0.45 / 0.39$ | $0.42 / 0.21$ | $\overline{0.41} / 0.49$ | $0.50 / \mathbf{0 . 6 9}$ | $\mathbf{0 . 5 9} / \overline{0.63}$ |
|  | $3 \mapsto 1$ | $0.39 / 0.20$ | $0.40 / 0.16$ | $0.39 / 0.31$ | $0.40 / 0.15$ | $0.48 / 0.67$ | $\underline{0.51} / \underline{0.67}$ | $0.72 / \overline{0.84}$ |
|  | $3 \mapsto 2$ | $0.48 / 0.54$ | $0.49 / 0.48$ | $0.49 / 0.55$ | $0.49 / 0.48$ | $0.46 / 0.31$ | $\underline{0.50} / \overline{0.57}$ | $0.58 / 0.68$ |

it often secures the second-highest scores in either or both the Macro F1 and AUROC metrics. Interestingly, certain models exhibit a degree of specialization. For example, SASA, which underperforms in the SMD dataset, demonstrates notably better results in specific scenarios within the Boiler dataset, particularly in terms of the Macro F1 score. This suggests that some models may be more suited to specific types of data or scenarios.

In conclusion, while the DACAD model emerges as the most effective across most scenarios, the varying performances of different models across scenarios and datasets emphasize the need to carefully consider the unique characteristics and capabilities of each model when selecting the most appropriate one for a specific task. This nuanced approach is crucial in leveraging the strengths of each model to achieve optimal results.

## References

[Audibert et al., 2020] Julien Audibert, Pietro Michiardi, FrÃ©dÃ©ric Guyard, SÃ©bastien Marti, and Maria A Zuluaga. Usad: Unsupervised anomaly detection on multivariate time series. In SIGKDD, pages 3395-3404, 2020.

[Audibert et al., 2022] Julien Audibert, Pietro Michiardi, FrÃ©dÃ©ric Guyard, SÃ©bastien Marti, and Maria A Zuluaga. Do deep neural networks contribute to multivariate time series anomaly detection? Pattern Recognition, 132:108945, 2022.

[Cai et al., 2021] Ruichu Cai, Jiawei Chen, Zijian Li, Wei Chen, Keli Zhang, Junjian Ye, Zhuozhang Li, Xiaoyan Yang, and Zhenjie Zhang. Time series domain adaptation via sparse associative structure alignment. In $A A A I$, volume 35, pages 6859-6867, 2021.

[Chung et al., 2015] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. Advances in neural information processing systems, 28, 2015.

[Creswell et al., 2018] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and
Anil A Bharath. Generative adversarial networks: An overview. IEEE signal processing magazine, 35(1):53-65, 2018.

[Darban et al., 2022] Zahra Zamanzadeh Darban, Geoffrey I Webb, Shirui Pan, Charu C Aggarwal, and Mahsa Salehi. Deep learning for time series anomaly detection: A survey. arXiv preprint arXiv:2211.05244, 2022.

[Darban et al., 2023] Zahra Zamanzadeh Darban, Geoffrey I Webb, Shirui Pan, Charu C. Aggarwal, and Mahsa Salehi. Carla: Self-supervised contrastive representation learning for time series anomaly detection. arXiv preprint arXiv:2308.09296, 2023.

[Deng et al., 2021] Hao Deng, Yifan Sun, Ming Qiu, Chao Zhou, and Zhiqiang Chen. Graph neural network-based anomaly detection in multivariate time series data. In COMPSAC'2021, pages 1128-1133. IEEE, 2021.

[Franceschi et al., 2019] J.-Y. Franceschi, A. Dieuleveut, and M. Jaggi. Unsupervised scalable representation learning for multivariate time series. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.

[Ganin and Lempitsky, 2015] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, pages 1180-1189. PMLR, 2015.

[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation, 9:1735-1780, 1997.

[Hundman et al., 2018] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Soderstrom. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. In SIGKDD, pages 387-395, 2018.

[Khosla et al., 2020] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:18661-18673, 2020.

[Kim et al., 2022] Siwon Kim, Kukjin Choi, Hyun-Soo Choi, Byunghan Lee, and Sungroh Yoon. Towards a rigorous evaluation of time-series anomaly detection. In $A A A I$, volume 36, pages 7194-7201, 2022.

[Lai et al., 2023] Kwei-Herng Lai, Lan Wang, Huiyuan Chen, Kaixiong Zhou, Fei Wang, Hao Yang, and Xia Hu. Context-aware domain adaptation for time series anomaly detection. In Proceedings of the SDM'2023, pages 676684. SIAM, 2023.

[Lea et al., 2016] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional networks: A unified approach to action segmentation. In Computer Vision-ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14, pages 47-54. Springer, 2016.

[Lei et al., 2019] Q. Lei, J. Yi, R. Vaculin, L. Wu, and I. S. Dhillon. Similarity preserving representation learning for time series clustering. In IJCAI, volume 19, pages 2845$2851,2019$.

[Liu et al., 2022] Xiaofeng Liu, Chaehwa Yoo, Fangxu Xing, Hyejin Oh, Georges El Fakhri, Je-Won Kang, Jonghye Woo, et al. Deep unsupervised domain adaptation: A review of recent advances and perspectives. $A P$ SIPA Transactions on Signal and Information Processing, $11,2022$.

[Long et al., 2018] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. Advances in neural information processing systems, 31, 2018.

[Lu et al., 2021] Nannan Lu, Hanhan Xiao, Yanjing Sun, Min Han, and Yanfen Wang. A new method for intelligent fault diagnosis of machines based on unsupervised domain adaptation. Neurocomputing, 427:96-109, 2021.

[Niu et al., 2020] Zijian Niu, Ke Yu, and Xiaofei Wu. Lstmbased vae-gan for time-series anomaly detection. Sensors, 20:3738, 2020.

[Ozyurt et al., 2023] Yilmazcan Ozyurt, Stefan Feuerriegel, and $\mathrm{Ce}$ Zhang. Contrastive learning for unsupervised domain adaptation of time series. ICLR, 2023.

[Park et al., 2018] Daehyung Park, Yuuna Hoshi, and Charles C Kemp. A multimodal anomaly detector for robot-assisted feeding using an lstm-based variational autoencoder. IEEE Robotics and Automation Letters, 3:1544-1551, 2018.

[Purushotham et al., 2016] Sanjay Purushotham, Wilka Carvalho, Tanachat Nilanon, and Yan Liu. Variational recurrent adversarial deep domain adaptation. In ICLR, 2016.

[Ragab et al., 2020] Mohamed Ragab, Zhenghua Chen, Min Wu, Chuan Sheng Foo, Chee Keong Kwoh, Ruqiang Yan, and Xiaoli Li. Contrastive adversarial domain adaptation for machine remaining useful life prediction. IEEE Transactions on Industrial Informatics, 17:5239-5249, 2020.

[Ruff et al., 2018] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel MÃ¼ller, and Marius Kloft. Deep one-class classification. In ICML, pages 4393-4402. PMLR, 2018.

[Schmidl et al., 2022] Sebastian Schmidl, Phillip Wenig, and Thorsten Papenbrock. Anomaly detection in time series: a comprehensive evaluation. Proceedings of the $V L D B, 15: 1779-1797,2022$.

[Schroff et al., 2015] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815-823, 2015.

[Shen et al., 2020] Lifeng Shen, Zhuocong Li, and James Kwok. Timeseries anomaly detection using temporal hierarchical one-class network. Advances in Neural Information Processing Systems, 33:13016-13026, 2020.

[Su et al., 2019] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In SIGKDD, pages 2828-2837, 2019.

[Tonekaboni et al., 2021] S. Tonekaboni, D. Eytan, and A. Goldenberg. Unsupervised representation learning for time series with temporal neighborhood coding. In ICLR, 2021.

[Tzeng et al., 2017] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $7167-7176,2017$.

[Wang et al., 2021] Guijin Wang, Ming Chen, Zijian Ding, Jiawei Li, Huazhong Yang, and Ping Zhang. Inter-patient ecg arrhythmia heartbeat classification based on unsupervised domain adaptation. Neurocomputing, 454:339-349, 2021.

[Wilson and Cook, 2020] Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11:1-46, 2020.

[Wilson et al., 2020] Garrett Wilson, Janardhan Rao Doppa, and Diane J Cook. Multi-source deep domain adaptation with weak supervision for time-series sensor data. In SIGKDD, pages 1768-1778, 2020.

[Xu et al., 2022] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series anomaly detection with association discrepancy. In ICLR, 2022.

[Yang et al., 2023] Yiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. Dcdetector: Dual attention contrastive representation learning for time series anomaly detection. In SIGKDD, 2023.

[Yue et al., 2022] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In AAAI, volume 36, pages 8980-8987, 2022.

[Zerveas et al., 2021] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff. A transformerbased framework for multivariate time series representation learning. In SIGKDD, pages 2114-2124. Association for Computing Machinery, 2021.

[Zhang et al., 2022] Wen Zhang, Lingfei Deng, Lei Zhang, and Dongrui Wu. A survey on negative transfer. IEEE/CAA Journal of Automatica Sinica, 10(2):305-329, 2022.

[Zhao et al., 2021] Ranqi Zhao, Yi Xia, and Yongliang Zhang. Unsupervised sleep staging system based on domain adaptation. Biomedical Signal Processing and Control, 69:102937, 2021.

[Zhou et al., 2019] Bin Zhou, Shenghua Liu, Bryan Hooi, Xueqi Cheng, and Jing Ye. Beatgan: Anomalous rhythm detection using adversarially generated time series. In $I J$ CAI, volume 2019, pages 4433-4439, 2019.


[^0]:    ${ }^{*}$ Corresponding Author

[^1]:    ${ }^{1}$ https://www.kaggle.com/datasets/patrickfleith/ nasa-anomaly-detection-dataset-smap-msl

    ${ }^{2}$ https://github.com/NetManAIOps/OmniAnomaly/tree/master/ ServerMachineDataset

    ${ }^{3}$ https://github.com/DMIRLAB-Group/SASA-pytorch/tree/ main/datasets/Boiler

    ${ }^{4}$ https://github.com/floft/vrada

    ${ }^{5}$ https://github.com/oezyurty/CLUDA

[^2]:    ${ }^{6}$ There is no implementation available for this model, so we rely on the results claimed in the paper.

    ${ }^{7}$ https://github.com/smallcowbaby/OmniAnomaly

    ${ }^{8}$ We utilised the authors' shared implementation, as it is not publicly available.

    ${ }^{9}$ https://github.com/yuezhihan/ts2vec

    ${ }^{10}$ https://github.com/DAMO-DI-ML/KDD2023-DCdetector

