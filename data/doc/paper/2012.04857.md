# Accurate and Fast Federated Learning via IID and Communication-Aware Grouping 

Jin-woo Lee ${ }^{1}$ Jaehoon Oh ${ }^{1}$ Yooju Shin ${ }^{1}$ Jae-Gil Lee ${ }^{1}$ Se-Young Yoon ${ }^{1}$


#### Abstract

Federated learning has emerged as a new paradigm of collaborative machine learning; however, it has also faced several challenges such as non-independent and identically distributed (IID) data and high communication cost. To this end, we propose a novel framework of IID and communication-aware group federated learning that simultaneously maximizes both accuracy and communication speed by grouping nodes based on data distributions and physical locations of the nodes. Furthermore, we provide a formal convergence analysis and an efficient optimization algorithm called FedAvg-IC. Experimental results show that, compared with the state-of-the-art algorithms, FedAvg-IC improved the test accuracy by up to $22.2 \%$ and simultaneously reduced the communication time to as small as $12 \%$.


## 1. Introduction

Federated learning (Konecn√Ω et al., 2016a; McMahan et al., 2017) enables mobile devices to collaboratively learn a shared model while keeping all training data on the devices, thus avoiding transferring data to the cloud or central server. In this framework, a local model is updated using the data on each device, and all local updates are periodically aggregated to the global model; then, each local model is synchronized with the global model. Federated learning is attracting more attention, as indicated by the recent release of TensorFlow Federated (TFF) in March 2019 (Google, 2019). One of the main reasons for this recent boom in federated learning is that it does not compromise user privacy. However, there are several challenges despite federated learning's growing popularity. McMa-[^0]

han et al. (2017) pointed out that federated learning has three unique properties: non-independent and identically distributed (IID), unbalanced, and massively-distributed. In this study, we tackle the challenges for the non-IID and massively-distributed properties as follows:

- Non-IID Challenge: Because each mobile device typically stores the data generated by a particular user, each local data distribution does not represent the global population distribution. This non-IID property definitely hinders the convergence of federated learning and degrades prediction accuracy.
- Limited Communication Challenge: Because several thousands of devices typically participate in federated learning, the training process is massively distributed, thus causing a huge burden on the backbone (wireless) network (Park et al., 2018).

To the best of our knowledge, no existing work has addressed both of the above challenges simultaneously. However, there have been active studies on each challenge. Notably, Lin et al. (2018) proposed a group-based learning algorithm, where the nodes (i.e., devices) are grouped into node groups; the local models are first aggregated to a group model, and the group models are then aggregated to the global model. While the group-based learning relieves the non-IID issue, it may cause high communication overheads especially if far-away nodes belong to the same node group. Wang et al. (2019) proposed a resourceconstrained optimization algorithm to optimize the number of communication rounds but did not address the non-IID issue. In contrast, Zhao et al. (2018) proposed a data sharing strategy that distributes a small subset of global data to all nodes for resolving the non-IIDness, but the additional global communication cost is not seriously considered and it somewhat violates the philosophy of federated learning.

In this paper, we propose a novel framework of IID and communication-aware group federated learning to address both challenges. Here, nodes are grouped by the IID and communication-aware grouping principle to make the data distribution of each group closer to the global IID data distribution and to reduce node-to-group communication simultaneously. Fig. 1, where the data distributions are distinguished by their different shapes, illustrates the

![](https://cdn.mathpix.com/cropped/2024_06_04_a1f96dfb5f64cdfb04b6g-02.jpg?height=572&width=708&top_left_y=213&top_left_x=237)

Figure 1: Concept of IID and communication-aware group federated learning.

proposed framework as well as two simple alternatives. Communication-aware grouping concentrates on the limited communication challenge at the cost of accuracy, and IID grouping concentrates on the non-IID challenge at the cost of efficiency. On the other hand, our proposed framework aims at presenting a hybrid of the two extreme cases. Overall, the key contributions are summarized as follows:

- Problem Formulation (Section 3): We formulate the problem as a bi-objective optimization that determines node groups by considering the difference in data distribution for the local-to-group and group-to-global levels as well as the communication delay based on the physical locations of nodes.
- Convergence Analysis (Section 4): We formally derive the convergence bound of group federated learning. As per our analysis, the optimal node grouping is achieved when the difference in data distribution for the groupto-global level and the group communication delay are simultaneously minimized.
- Optimization Algorithm (Section 5): We design a control algorithm, called FedAvg-IC, to find the near-optimal node grouping that minimizes both IID and communication costs.
- High Performance (Section 6): We empirically compared FedAvg-IC with three federated learning algorithms on four benchmark datasets. FedAvg-IC reached a higher accuracy by up to $22.2 \%$ and simultaneously reduced communication time to as small as $12 \%$, compared with the three selected algorithms.


## 2. Preliminaries and Related Work

In this section, we first briefly describe federated learning and then survey relevant studies that handle either nonIIDness or limited communication.

### 2.1. Basics of Federated Learning

The objective of federated learning is to find an approximate solution of Eq. (1) (McMahan et al., 2017). Here, $F(\mathbf{w})$ is the loss of predictions with a model $\mathbf{w}$ over the set of all data examples $\mathcal{D} \triangleq \cup_{i \in \mathcal{N}} \mathcal{D}_{i}$ across all nodes, where $\mathcal{N}$ is the set of node indices, $F_{i}(\mathbf{w}) \triangleq \sum_{j \in \mathcal{D}_{i}} \frac{1}{\left|\mathcal{D}_{i}\right|} F_{i j}(\mathbf{w})$ is the loss of predictions with $\mathbf{w}$ over the set of data examples $\mathcal{D}_{i}$ on the $i$-th node, and $F_{i j}(\mathbf{w}) \triangleq l\left(\mathbf{w}, \mathbf{x}_{i j}, y_{i j}\right)$ is the loss of a prediction with $\mathbf{w}$ on the $j$-th data example $\left(\mathbf{x}_{i j}, y_{i j}\right)$ on the $i$-th node.

$$
\begin{equation*}
\mathbf{w}^{*} \triangleq \underset{\mathbf{w} \in \mathbb{R}^{d}}{\arg \min } F(\mathbf{w}) \text { where } F(\mathbf{w}) \triangleq \sum_{i \in \mathcal{N}} \frac{\left|\mathcal{D}_{i}\right|}{|\mathcal{D}|} F_{i}(\mathbf{w}) \tag{1}
\end{equation*}
$$

Federated Averaging (FedAvg) (McMahan et al., 2017), which is the canonical algorithm for federated learning in Eq. (1), involves local update, which learns a local model $\mathbf{w}_{i}$ at the $i$-th node by performing gradient descent steps, and global aggregation, which learns the global model $\mathbf{w}$ by aggregating all $\mathbf{w}_{i}$ and synchronizes $\mathbf{w}_{i}$ with $\mathbf{w}$ every $\tau$ steps, as shown in Eq. (2).

$$
\begin{align*}
& \mathbf{w}_{i}(t) \triangleq \begin{cases}\mathbf{w}_{i}(t-1)-\eta \nabla F_{i}\left(\mathbf{w}_{i}(t-1)\right) & \text { if } t \bmod \tau \neq 0 \\
\mathbf{w}(t) & \text { if } t \bmod \tau=0\end{cases}  \tag{2}\\
& \quad \text { where } \mathbf{w}(t) \triangleq \sum_{i \in N} \frac{\left|\mathcal{D}_{i}\right|}{|\mathcal{D}|}\left[\mathbf{w}_{i}(t-1)-\eta \nabla F_{i}\left(\mathbf{w}_{i}(t-1)\right)\right]
\end{align*}
$$

### 2.2. Related Work on the Non-IID Challenge

### 2.2.1. GROUP FEDERATED LEARNING

To reduce the learning divergence between $\mathbf{w}_{i}$ and $\mathbf{w}$ in Eq. (2), Lin et al. (2018) proposed a group-based architecture of allowing multiple intermediate aggregations before a global aggregation. Formally speaking, the set of all node indices $\mathcal{N}$ is partitioned into sets of node indices for $|\mathcal{K}|$ node groups $\left\{\mathcal{N}^{1}, \mathcal{N}^{2}, \cdots, \mathcal{N}^{|\mathcal{K}|}\right\}$, i.e., $\cup_{k \in \mathcal{K}} \mathcal{N}^{k}=\mathcal{N}$ and $\forall k \neq l, \mathcal{N}^{k} \cap \mathcal{N}^{l}=\emptyset$. Additionally, let $\mathcal{D}^{k}$ be the set of data examples on the $k$-th node group and $\mathcal{D}_{i}^{k}$ be its subset of $\mathcal{D}^{k}$ on the $i$-th node. Then, the loss function of Eq. (1) is extended to that of Eq. (3) by considering the node groups.

$$
\begin{equation*}
F(\mathbf{w}) \triangleq \sum_{k \in \mathcal{K}} \frac{\left|\mathcal{D}^{k}\right|}{|\mathcal{D}|} F^{k}(\mathbf{w}), F^{k}(\mathbf{w}) \triangleq \sum_{i \in \mathcal{N}^{k}} \frac{\left|\mathcal{D}_{i}^{k}\right|}{\left|\mathcal{D}^{k}\right|} F_{i}^{k}(\mathbf{w}) \tag{3}
\end{equation*}
$$

Group federated learning was implemented as hierarchical local SGD (Lin et al., 2018), and it learns the group model
$\mathbf{w}^{k}$ by aggregating all $\mathbf{w}_{i}^{k}$ and synchronizes $\mathbf{w}_{i}^{k}$ with $\mathbf{w}^{k}$ every $\tau_{1}$ steps, which can be expressed as Eq. (4).

$$
\begin{align*}
& \mathbf{w}_{i}^{k}(t) \triangleq\left\{\begin{array}{lr}
\mathbf{w}_{i}^{k}(t-1)-\eta \nabla F_{i}^{k}\left(\mathbf{w}_{i}^{k}(t-1)\right) & \text { if } t \bmod \tau_{1} \neq 0 \\
\mathbf{w}^{k}(t) & \text { if } t \bmod \tau_{1}=0 \\
& t \bmod \tau_{1} \tau_{2} \neq 0 \\
\mathbf{w}(t) & \text { if } t \bmod \tau_{1} \tau_{2}=0
\end{array}\right. \\
& \text { where } \mathbf{w}^{k}(t) \triangleq \sum_{i \in \mathcal{N}^{k}} \frac{\left|\mathcal{D}_{i}^{k}\right|}{\left|\mathcal{D}^{k}\right|}\left[\mathbf{w}_{i}^{k}(t-1)-\eta \nabla F_{i}^{k}\left(\mathbf{w}_{i}^{k}(t-1)\right)\right] \\
& \text { and } \mathbf{w}(t) \triangleq \sum_{k \in \mathcal{K}} \sum_{i \in \mathcal{N}^{k}} \frac{\left|\mathcal{D}_{i}^{k}\right|}{|\mathcal{D}|}\left[\mathbf{w}_{i}^{k}(t-1)-\eta \nabla F_{i}^{k}\left(\mathbf{w}_{i}^{k}(t-1)\right)\right] \tag{4}
\end{align*}
$$

### 2.2.2. GLOBAL-INFORMATION SHARING

Sharing global information is effective in mitigating the non-IIDness of a local node. The most common approach is to share a subset of global IID data samples to make the local data distribution closer to the population data distribution (Zhao et al., 2018; Yoshida et al., 2019). FSVRG (Konecn√Ω et al., 2016a) shares a subset of global data features to scale up the feature-related parameters of a local optimizer. FAug (Jeong et al., 2018) shares a generative model that can produce an augmented IID dataset.

### 2.3. Related Work on the Communication Challenge

### 2.3.1. COMMUNICATION-AWARE LEARNING

AdaptiveFL(Wang et al., 2019) extends FedAvg to adaptively optimize the number of global aggregations by considering resource consumption such as communication. FedCS (Nishio \& Yonetani, 2019) minimizes the overall communication delay for a set of sampled learners by considering a round-trip time constraint. HierFAVG (Liu et al., 2019), which is the state-of-the-art approach for group federated learning, groups nodes by network edges to facilitate communication between the nodes in proximity. Similarly, we define a novel optimization problem that considers both IID and communication costs for maximizing accuracy and efficiency of federated learning, as shown in Section 3.

### 2.3.2. COMMUNICATION OVERHEAD REDUCTION

Reducing communication overheads in federated learning usually leads to saving both communication and computation resources. The overheads include the number of participating nodes and the amount of communication data. The participating nodes can be sampled by following a certain probability distribution (McMahan et al., 2017; Li et al., 2019; Sahu et al., 2018), but this approach is beyond the scope of this paper. Meanwhile, communication data size can be reduced by using a quantization or compression technique (Konecn√Ω et al., 2016b; Sattler et al., 2019) or by placing intermediate parameter servers in a network topology (Bonawitz et al., 2019). We also attempt to reduce communication data size in Section 5.

Table 1: Summary of the notation.

| Notation | Description |
| :---: | :--- |
| $\mathbf{w}_{i}^{k}$ | Local model of $i$-th node in $k$-th group |
| $\mathbf{w}^{k}$ | Group model of $k$-th group |
| $\mathbf{w}(T)$ | Global model after $T$ steps |
| $\tau_{1}$ | \# of local updates per group aggregation |
| $\tau_{2}$ | \# of group aggregations per global aggregation |
| $[r]$ | Group interval, i.e., $\left[(r-1) \tau_{1}, r \tau_{1}\right]$ |
| $[l]$ | Global interval, i.e., $\left[(l-1) \tau_{1} \tau_{2}, l \tau_{1} \tau_{2}\right]$ |
| $\delta$ | Local-to-group divergence |
| $\Delta$ | Group-to-global divergence |

## 3. IID and Communication-Aware Group Federated Learning

Our primary goal is to train a global model that simultaneously minimizes the global loss in Eq. (3) and the total communication delay by considering the aforementioned challenges, which can be formulated as Eq. (5).

$$
\begin{equation*}
\min _{\tau_{1}, \tau_{2},|\mathcal{K}|, \mathbf{z}}\left\{F(\mathbf{w}(T)),\left(d_{\text {group }}\left(\tau_{2}-1\right)+d_{\text {global }}\right)\right\} \tag{5}
\end{equation*}
$$

- The IID objective is defined as the minimization of global loss after $T$ steps, and the communication objective is defined as the minimization of total communication delay, where $d_{\text {group }}$ and $d_{\text {global }}$ represent the communication delay (e.g., in seconds) spent for a single iteration of group and global aggregations, respectively. $d_{\text {group }}$ and $d_{\text {global }}$ can be easily estimated from a given network topology (e.g., by using hop counts (Vahdat \& Becker, 2000)). $\tau_{2}-1$ implies that a global aggregation takes over a group aggregation every $\tau_{2}$ steps.
- The optimization parameters are the learning steps $\tau_{1}$ and $\tau_{2}$, the number of node groups $|\mathcal{K}|$, and the group membership $\mathbf{z} \triangleq\left(z_{i} \mid[\forall i \in \mathcal{N}, \exists k \in \mathcal{K}]\left(z_{i}=k\right)\right)$.


## 4. Theoretical Analysis

In this section, we provide a theoretical analysis of the IID and communication-aware group federated learning. Based on an assumption and definitions in Section 4.1, we analyze the convergence of group federated learning in Section 4.2 and draw notable remarks for the main problem in Section 4.3. Table 1 summarizes the notation used in this paper.

### 4.1. Assumption and Definitions

We make the following assumption for the loss function $F_{i}^{k}$, as in many other relevant studies (Liu et al., 2019; Wang et al., 2019). For every $i$ and $k$, (1) $F_{i}^{k}$ is con-

![](https://cdn.mathpix.com/cropped/2024_06_04_a1f96dfb5f64cdfb04b6g-04.jpg?height=407&width=814&top_left_y=222&top_left_x=192)

Figure 2: Illustration of loss divergence and synchronization between $\mathbf{w}^{k}$ and $\mathbf{v}_{[r]}^{k}$ and between $\mathbf{w}$ and $\mathbf{v}_{[l]}$.

vex $^{1}$; (2) $F_{i}^{k}$ is $\rho$-Lipschitz, i.e., $\left\|F_{i}^{k}(\mathbf{w})-F_{i}^{k}\left(\mathbf{w}^{\prime}\right)\right\| \leq$ $\rho\left\|\mathbf{w}-\mathbf{w}^{\prime}\right\|$ for any $\mathbf{w}$ and $\mathbf{w}^{\prime}$; and (3) $F_{i}^{k}$ is $\beta$-smooth, i.e., $\left\|\nabla F_{i}^{k}(\mathbf{w})-\nabla F_{i}^{k}\left(\mathbf{w}^{\prime}\right)\right\| \leq \beta\left\|\mathbf{w}-\mathbf{w}^{\prime}\right\|$ for any $\mathbf{w}$ and $\mathbf{w}^{\prime}$.

Under this assumption, Lemma 1 holds for the group and global loss functions.

Lemma 1. $F$ and $F^{k}$ are convex, $\rho$-Lipschitz, and $\beta$ smooth.

Proof. It is straightforward from the aforementioned assumption and the definitions of $F$ and $F^{k}$ in Eq. (3).

We introduce two types of intervals depending on the learning level: a group interval, $[r] \triangleq\left[(r-1) \tau_{1}, r \tau_{1}\right]$, indicates an interval between two successive group aggregations, and a global interval, $[l] \triangleq\left[(l-1) \tau_{1} \tau_{2}, l \tau_{1} \tau_{2}\right]$, indicates an interval between two successive global aggregations.

Next, we introduce the notion of group-based virtual learning in Definition 1, where training data is assumed to exist on a virtual central repository for each model.

Definition 1 (Group-Based Virtual Learning). Given a certain group membership $\mathbf{z}$, for any $k,[r]$, and $[l]$, the virtual group model $\mathbf{v}_{[r]}^{k}$ and virtual global model $\mathbf{v}_{[l]}$ are updated by performing gradient descent steps on the centralized data examples for $\mathcal{N}^{k}$ and $\mathcal{N}$, respectively, and synchronized with the federated group model $\mathbf{w}^{k}$ and the global model $\mathbf{w}$ at the beginning of each interval, as in Eq. (6).

$$
\begin{align*}
\mathbf{v}_{[r]}^{k}(t) \triangleq \begin{cases}\mathbf{w}^{k}(t) & \text { if } t=(m-1) \tau_{1} \\
\mathbf{v}_{[r]}^{k}(t-1)-\eta \nabla F^{k}\left(\mathbf{v}_{[r]}^{k}(t-1)\right) & \text { otherwise }\end{cases} \\
\mathbf{v}_{[l]}(t) \triangleq \begin{cases}\mathbf{w}(t) & \text { if } t=(n-1) \tau_{1} \tau_{2} \\
\mathbf{v}_{[l]}(t-1)-\eta \nabla F\left(\mathbf{v}_{[l]}(t-1)\right) & \text { otherwise }\end{cases} \tag{6}
\end{align*}
$$

To facilitate the interpretation, Fig. 2 shows how a virtual model $\mathbf{v}$ is updated, following Definition 1. For example, $\mathbf{v}_{[l]}$ starts diverging from $\mathbf{w}$ after $(l-1) \tau_{1} \tau_{2}$ and becomes synchronized with $\mathbf{w}$ at $l \tau_{1} \tau_{2}$.[^1]

Then, we formalize group-based gradient divergence in Definition 2 that models the impact of the difference in data distributions across nodes on federated learning.

Definition 2 (Group-Based Gradient Divergence). Given a certain group membership $\mathbf{z}$, for any $i, k$, and $\mathbf{w}, \delta_{i}^{k}$ is defined as the gradient difference between the $i$-th local loss and the $k$-th group loss; $\Delta^{k}$ is defined as the gradient difference between the $k$-th group loss and the global loss, which can be expressed as Eq. (7).

$$
\begin{array}{r}
\delta_{i}^{k} \triangleq\left\|\nabla F_{i}^{k}(\mathbf{w})-\nabla F^{k}(\mathbf{w})\right\| \\
\Delta^{k} \triangleq\left\|\nabla F^{k}(\mathbf{w})-\nabla F(\mathbf{w})\right\| \tag{7}
\end{array}
$$

Then, the local-to-group divergence $\delta$ and the group-toglobal divergence $\Delta$ are formulated as Eq. (8).

$$
\begin{equation*}
\delta \triangleq \sum_{k \in \mathcal{K}} \sum_{i \in \mathcal{N}^{k}} \frac{\left|\mathcal{D}_{i}^{k}\right|}{|\mathcal{D}|} \delta_{i}^{k}, \Delta \triangleq \sum_{k \in \mathcal{K}} \frac{\left|\mathcal{D}^{k}\right|}{|\mathcal{D}|} \Delta^{k} \tag{8}
\end{equation*}
$$

### 4.2. Convergence of the Group Federated Learning

We provide a proof sketch for the convergence of the global loss $F(\mathbf{w}(T))$ in Appendix A.1, which derives Theorem 1.

Theorem 1. Let $\omega \triangleq \min _{q} \frac{1}{\left\|\mathbf{v}_{[l]}\left((l-1) \tau_{1} \tau_{2}\right)-\mathbf{w}^{*}\right\|^{2}}$. When $\eta \leq \frac{1}{\beta}$, the convergence upper bound of group federated learning after $T$ steps can be expressed as Eq. (9).

$$
\begin{align*}
& F(\mathbf{w}(T))-F\left(\mathbf{w}^{*}\right) \leq \frac{1}{2 \tau_{1} \tau_{2} \eta \omega} \\
& +\rho\left(\frac{\delta}{\beta}\left((\eta \beta+1)^{\tau_{1}}-1\right)+\frac{\Delta}{\beta}\left((\eta \beta+1)^{\tau_{1} \tau_{2}}-1\right)\right) \tag{9}
\end{align*}
$$

Proof. Please refer to Appendix A. 3 for details.

From Eq. (9), it is straightforward to see that the optimality gap is dominantly affected by $\tau_{1}, \tau_{2}, \delta$, and $\Delta$. Therefore, given small values, the convergence is guaranteed.

### 4.3. Theoretical Analysis for the Main Problem

Based on the convergence analysis, we interpret the IID and communication-aware group federated learning as follows.

Remark 1 (Dominance of $\Delta$ ). The IID objective $(\min F(\mathbf{w}(T)))$ in Eq. (5) is the same as minimizing $F(\mathbf{w}(T))-F\left(\mathbf{w}^{*}\right)$ in Eq. (9) because $F\left(\mathbf{w}^{*}\right)$ is a constant. Thus, given $\tau_{1}, \tau_{2}$, and $|\mathcal{K}|$, because $\Delta$ is the most dominant factor in Eq. (9), it is important for the IID objective to reduce $\Delta$ by changing $\mathbf{z}$.

Remark 2 (Dominance of $d_{\text {group }}$ ). For the communication objective $\left(\min d_{\text {group }}\left(\tau_{2}-1\right)+d_{\text {global }}\right)$, because $d_{\text {global }}$ is not affected by a certain node grouping from the definition of global aggregation in Eq. (4), given $\tau_{1}$, $\tau_{2}$, and $|\mathcal{K}|$, it is important for the communication objective to reduce $d_{\text {group }}$ by changing $\mathbf{z}$.

In conclusion, we establish the IID and communicationaware grouping principle: a group federated learning algorithm should group nodes by simultaneously minimizing $\Delta$ and $d_{\text {group }}$ to maximize both accuracy and efficiency.

## 5. Optimization Algorithm: FedAvg-IC

To solve Eq. (5), an efficient heuristic algorithm is essential because the grouping problem itself is NP-Hard with the complexity of $O\left(|\mathcal{N}|^{|\mathcal{K}|}\right)$. In this regard, we propose a novel algorithm called FedAvg-IC (Federated Averaging with IID and Communication-Aware Grouping).

### 5.1. Algorithm Description

FedAvg-IC aims at quickly finding an accurate global model based on the near-optimal node grouping that follows the IID and communication-aware grouping principle, for which we adopt the $k$-medoids algorithm (Park \& Jun, 2009). The node grouping involves assigning each node to the closest medoid node and updating a representative medoid node for each group. Here, the distance is measured by the cost functions defined as follows.

Assign Cost: To evaluate the cost of assigning the $i$-th node to the $k$-th group, we model the IID cost $\left(\operatorname{CosT}_{A, i i d}\right)$ and communication cost $\left(\operatorname{CosT}_{A, \text { comm }}\right)$ using $\Delta^{k}$ in Eq. (7) and the hop distance between the $i$-th node and $i_{k}$-th medoid node, respectively, as shown in Eq. (10).

$$
\begin{align*}
& \operatorname{Cost}_{A, i i d}(i, k) \triangleq \Delta^{k} \text { where } i \in \mathcal{N}^{k}  \tag{10}\\
& \operatorname{Cost}_{A, \text { comm }}(i, k) \triangleq \operatorname{HopDISTANCE}\left(i, i_{k}\right)
\end{align*}
$$

Update Cost: To evaluate the cost of selecting the $i$-th node in the $k$-th group as a new medoid for the group, we model the IID cost $\left(\operatorname{CosT}_{U, i i d}\right)$ and the communication $\operatorname{cost}\left(\operatorname{CosT}_{U, c o m m}\right)$ by the local-to-global divergence of the $i$-th node and the sum of hop distances to all other nodes in the group, respectively, as shown in Eq. (11).

$$
\begin{align*}
& \operatorname{Cost}_{U, i i d}(i, k) \triangleq \frac{\left|\mathcal{D}_{i}^{k}\right|}{|\mathcal{D}|}\left\|\nabla F_{i}^{k}(\mathbf{w})-\nabla F(\mathbf{w})\right\| \\
& \operatorname{Cost}_{U, c o m m}(i, k) \triangleq \sum_{j \in \mathcal{N}^{k}} \operatorname{HopDistancE}(i, j) \tag{11}
\end{align*}
$$

Combined Cost: Given $X \in\{A, U\}, \operatorname{Cost}_{X, i i d}$ (IID cost) and $\operatorname{CosT}_{X, \text { comm }}$ (communication cost) are combined into a single cost, as shown in Eq. (12).

$$
\begin{equation*}
\operatorname{CosT}_{X} \triangleq \alpha_{i i d} \frac{\operatorname{CosT}_{X, i i d}}{C_{X, i i d}}+\alpha_{c o m m} \frac{\operatorname{CosT}_{X, c o m m}}{C_{X, c o m m}} \tag{12}
\end{equation*}
$$

$\alpha$ is the weight, and $C$ is the normalizing constant ${ }^{2}$.

Algorithm 1 shows the overall procedure of FedAvg-IC. It takes the set of node indices $\mathcal{N}$, the final time $T$, the learning steps $\tau_{1}^{0}$ and $\tau_{2}^{0}$, and the number of node groups $|\mathcal{K}|$[^2]

```
Algorithm 1 FedAvg-IC
INPUT: $\quad \mathcal{N}, T, \tau_{1}^{0}, \tau_{2}^{0},|\mathcal{K}|$
OUTPUT: $\mathbf{w}(T)$
    Initialize $\mathbf{w}(0)$ and $\mathbf{z}$ randomly, $\tau_{1} \leftarrow 1, \tau_{2} \leftarrow 1$
    $\left[\mathbf{w}_{i}^{k}(0)\right]_{i \in \mathcal{N}} \leftarrow \mathbf{w}(0) / *$ Initial global broadcast ${ }^{3 *} /$
    for $t \leftarrow 1,2, \cdots, T$ do
        for each $i \in \mathcal{N}$ in parallel do /* Local */
            $\mathbf{w}_{i}^{k}(t) \leftarrow \mathbf{w}_{i}^{k}(t-1)-\eta \nabla F_{i}^{k}\left(\mathbf{w}_{i}^{k}(t-1)\right)$
        if $(t-1) \bmod \tau_{1} \tau_{2} \neq 0$ then /* Group */
            for each $k \in \mathcal{K}$ in parallel do
                $\left[\mathbf{w}^{k}\right]_{i_{k}} \leftarrow \sum_{i \in \mathcal{N}^{k}} \frac{\left|\mathcal{D}_{i}^{k}\right|}{\left|\mathcal{D}^{k}\right|}\left[\mathbf{w}_{i}^{k}(t)\right]_{i}$
                $\left[\mathbf{w}_{i}^{k}(t)\right]_{i \in \mathcal{N}^{k}} \leftarrow\left[\mathbf{w}^{k}\right]_{i_{k}}$
        if $(t-1) \bmod \tau_{1} \tau_{2}=0$ then /* Global */
            $\mathbf{w} \leftarrow \sum_{i \in \mathcal{N}} \frac{\left|\mathcal{D}_{i}^{k}\right|}{|\mathcal{D}|}\left[\mathbf{w}_{i}^{k}(t)\right]_{i}$
            $\left[\mathbf{w}_{i}^{k}(t)\right]_{i \in \mathcal{N}} \leftarrow \mathbf{w}$
            if $\mathcal{N}$ is not grouped then
                $\mathbf{z} \leftarrow$ NODE_GROUPING $(\mathbf{z})$
                $\left(\tau_{1}, \tau_{2}\right) \leftarrow\left(\tau_{1}^{0}, \tau_{2}^{0}\right)$
    function NODE_GROUPING( $\mathbf{z})$
        Select random medoid nodes $\mathcal{N}_{m}$
        $\mathbf{z} \leftarrow\left(\arg \min _{k \in \mathcal{K}} \operatorname{Cost}_{A}(i, k) \mid \forall i \in \mathcal{N}\right)$
        until the last $\operatorname{CosT}_{A}$ is steady do
            $\mathcal{N}_{m} \leftarrow\left(\arg \min _{i \in \mathcal{N}^{k}} \operatorname{CosT}_{U}(i, k) \mid \forall k \in \mathcal{K}\right)$
            $\mathbf{z} \leftarrow\left(\arg \min _{k \in \mathcal{K}} \operatorname{Cost}_{A}(i, k) \mid \forall i \in \mathcal{N}\right)$
        return $\mathrm{z}$
```

as the input and returns the final global model $\mathbf{w}(T)$ as the output. It begins by initializing the global model and group membership randomly (Line 1). Then, the global model is broadcast to all nodes (Line 2). Then, the local update is performed at each node (Lines 4-5); each group model is learned by aggregating all local models in the group and then broadcast back to all nodes (Lines 6-9); the global model is learned by aggregating all local models and then broadcast back to all nodes (Lines 10-12). After the first global aggregation, the group membership $\mathbf{z}$ is updated (Line 14). Overall, Lines 3-15 repeat for $T$ steps.

The NODE_GRoupING function attempts to find a group membership $\mathbf{z}$ that reduces the combined cost in Eq. (12) to the extent possible. For this purpose, it begins by selecting random medoid nodes $\mathcal{N}_{m}$ of size $|\mathcal{K}|$. Then, it iteratively updates $\mathbf{z}$ by minimizing $\operatorname{CosT}_{A}$ in Eq. (10) for all nodes and $\operatorname{CosT}_{U}$ in Eq. (11) for all groups until the cost is steady (Lines 19-21).

## 6. Evaluation

### 6.1. Experimental Setting

Configuration: We developed a federated learning simulator to extensively evaluate the performance of various algorithms, models, datasets, and networks based on TensorFlow 1.14.0. Please refer to Appendix C. 1 for details.

Algorithms: We compared the following three algorithms.

- FedAvg (McMahan et al., 2017), which is used as a baseline, does not consider node grouping at all.
- HierFAVG (Liu et al., 2019) groups nodes by network edges to facilitate communication between nodes.
- FedAvg-IC groups nodes by minimizing both IID and communication costs. We also considered FedAvg-IC that only minimizes either IID or communication cost as FedAvg-I or FedAvg-C, respectively.

Datasets: We used four datasets, (1) MNIST-O (LeCun et al., 1998), (2) MNIST-F (Xiao et al., 2017), (3) FEMNIST (Caldas et al., 2018), and (4) CelebA(Liu et al., 2015), which consist of $70,000,70,000,78,353$, and 10,014 examples, respectively. The ratio of train/validation/test examples was 3:1:1, as suggested by Caldas et al. (2018).

Table 2: Class diversity across nodes and edges. An entry is the number of classes per node or edge.

|  | Dtt | Dtq | Dth | Dqq | Dqh | Dhh |
| ---: | ---: | ---: | ---: | ---: | ---: | ---: |
| Node | tenth | tenth | tenth | quarter | quarter | half |
| Edge | tenth | quarter | half | quarter | half | half |

Data Distribution: To simulate a wide range of nonIIDness, we designed six cases of class diversity on each node and edge, as shown in Table 2. For example, in the Dtq setting, only a tenth of the classes can exist per node, and a quarter of the classes can exist per edge.

Models: We used three training models, (1) the softmax regression(SR), (2) the 2 layered perceptron neural network ( $2 \mathrm{NN}$ ), and (3) the convolutional neural network (CNN). Please refer to Appendix C. 1 for details.

Methodology: Deterministic gradient descent (DGD) was used for the SR to solve convex problems, and stochastic gradient descent (SGD) was used for the 2NN and the CNN to solve non-convex problems. We evaluate each algorithm five times and report the average with standard deviation.

### 6.2. Accuracy Results

Fig. 3 and Fig. 4a show the test accuracy of three federated learning algorithms on a non-IID (Dtt) dataset according to the elapsed time and epoch, respectively. Overall, FedAvgIC outperformed FedAvg by up to $17.4 \%$ (Fig. 3d) and HierFAVG by up to $22.2 \%$ (Fig. 3b). In Fig. 4a, the algo- rithms that considered communication such as FedAvg-IC and FedAvg-C outperformed the others. The higher accuracy of FedAvg-IC is attributed to a decreased $\Delta$ in the IID cost in Eq. (12). Please refer to Appendix C. 2 for details.

### 6.3. Efficiency Results

Table 3: Elapsed time (in seconds) of the algorithms on the non-IID and IID datasets at the final test accuracy of the baseline FedAvg within a given time, where the final accuracy is specified in parenthesis next to each model name.

|  | SR(84\%) | Non-IID(Dtt) <br> 2NN(73\%) | CNN(83\%) |
| :---: | :---: | :---: | :---: |
| FedAvg | 50 | 300 | 300 |
| HierFAVG | $29(1.7 \mathrm{x})$ | - | - |
| FedAvg-IC | $6(8.3 \mathrm{x})$ | $47(6.4 \mathrm{x})$ | $149(2.0 \mathrm{x})$ |
|  |  | IID (Dhh) |  |
|  | SR(86\%) | 2NN(90\%) | CNN(96\%) |
| FedAvg | 100 | 600 | 600 |
| HierFAVG | $29(3.4 \mathrm{x})$ | $468(1.3 \mathrm{x})$ | - |
| FedAvg-IC | $18(5.6 \mathrm{x})$ | $291(2.1 \mathrm{x})$ | $543(1.1 \mathrm{x})$ |

Table 3 shows the elapsed time and speedup on the most non-IID (Dtt) and IID (Dhh) datasets. In terms of the elapsed time, FedAvg-IC outperformed FedAvg and HierFAVG by up to 8.3 times and 4.8 times, respectively. Even though HierFAVG is in favor of communication efficiency, because the edge-based learning of HierFAVG degrades the accuracy in non-IID settings, it did not reach the target accuracy for the $2 \mathrm{NN}$ and the CNN. The faster convergence speed of FedAvg-IC is attributed to a decreased $d_{\text {group }}$ in Eq. (12) as well as a decreased communication data size by the combined aggregation. Please refer to Appendix C.2.

## 7. Conclusion

In this paper, we proposed a novel framework of IID and communication-aware group federated learning to address both the non-IID and limited communication challenges simultaneously. Our formal convergence analysis led to the IID and communication-aware grouping principle that is incorporated into our optimization algorithm FedAvg-IC. Extensive experiments were performed using our own federated learning simulator, and the results demonstrated that FedAvg-IC outperformed HierFAVG by up to $22.2 \%$ in terms of test accuracy and FedAvg by up to 8.3 times in terms of convergence speed. Overall, we believe that our framework has made important steps towards accurate and fast federated learning.

![](https://cdn.mathpix.com/cropped/2024_06_04_a1f96dfb5f64cdfb04b6g-07.jpg?height=469&width=1675&top_left_y=215&top_left_x=190)

FedAvg HierFAVG $\quad$ FedAvg-I _ FedAvg-C _ FedAvg-IC $(\tau=5)-\left(\tau_{1}=1, \tau_{2}=5\right) \quad\left(\tau_{1}=1, \tau_{2}=5\right) \quad\left(\tau_{1}=1, \tau_{2}=5\right) \quad\left(\tau_{1}=1, \tau_{2}=5\right)$

![](https://cdn.mathpix.com/cropped/2024_06_04_a1f96dfb5f64cdfb04b6g-07.jpg?height=303&width=314&top_left_y=331&top_left_x=206)

(a) MNIST-O.

![](https://cdn.mathpix.com/cropped/2024_06_04_a1f96dfb5f64cdfb04b6g-07.jpg?height=311&width=290&top_left_y=327&top_left_x=576)

(b) MNIST-F.

![](https://cdn.mathpix.com/cropped/2024_06_04_a1f96dfb5f64cdfb04b6g-07.jpg?height=311&width=312&top_left_y=327&top_left_x=885)

(c) FEMNIST.

![](https://cdn.mathpix.com/cropped/2024_06_04_a1f96dfb5f64cdfb04b6g-07.jpg?height=312&width=309&top_left_y=332&top_left_x=1231)

(d) CelebA.

![](https://cdn.mathpix.com/cropped/2024_06_04_a1f96dfb5f64cdfb04b6g-07.jpg?height=301&width=315&top_left_y=332&top_left_x=1531)

(a) MNIST-O.

Figure 3: Test accuracy of the CNN on four datasets with Dtt according to elapsed time.

Figure 4: Test accuracy according to epochs.

## References

Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V., Kiddon, C., Konecn√Ω, J., Mazzocchi, S., McMahan, H. B., et al. Towards federated learning at scale: System design. In Proc. 2nd Conf. on Systems and Machine Learning, 2019.

Caldas, S., Duddu, S. M. K., Wu, P., Li, T., Konecn√Ω, J., McMahan, H. B., Smith, V., and Talwalkar, A. Leaf: A benchmark for federated settings. arXiv:1812.01097, 2018.

Cisco. Cisco annual internet report, 2018-2023, 2020.

Dean, J. and Ghemawat, S. MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1):107-113, 2008 .

Google. TensorFlow Federated. https://www. tensorflow.org/federated/federated_ learning, 2019. Accessed: 2020-06-08.

Grodzevich, O. and Romanko, O. Normalization and other topics in multi-objective optimization. In Proc. Fields-MITACS Industrial Problems Workshop, 2006.

Isard, M., Budiu, M., Yu, Y., Birrell, A., and Fetterly, D. Dryad: distributed data-parallel programs from sequential building blocks. ACM SIGOPS Operating Systems Review, 41(3):59-72, 2007.

Jeong, E., Oh, S., Kim, H., Park, J., Bennis, M., and Kim, S.-L. Communication-efficient on-device machine learning: Federated distillation and augmentation under noniid private data. arXiv:1811.11479, 2018.

Konecn√Ω, J., McMahan, H. B., Ramage, D., and Richt√°rik, P. Federated optimization: Distributed machine learning for on-device intelligence. arXiv:1610.02527, 2016a.

Konecn√Ω, J., McMahan, H. B., Yu, F. X., Richt√°rik, P., Suresh, A. T., and Bacon, D. Federated learning: Strategies for improving communication efficiency. In Proc.
NIPS 2016 Workshop on Private Multi-Party Machine Learning, 2016 .

LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., et al. Gradient-based learning applied to document recognition. Proc. IEEE, 86(11):2278-2324, 1998.

Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On the convergence of fedavg on non-iid data. arXiv:1907.02189, 2019.

Lin, T., Stich, S. U., Patel, K. K., and Jaggi, M. Don't use large mini-batches, use local sgd. arXiv:1808.07217, 2018.

Liu, L., Zhang, J., Song, S., and Letaief, K. B. Edgeassisted hierarchical federated learning with non-iid data. arXiv:1905.06641, 2019.

Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proc. 2015 Int'l Conf. on Computer Vision(ICCV), December 2015.

McMahan, H. B., Moore, E., Ramage, D., Hampson, S., et al. Communication-efficient learning of deep networks from decentralized data. In Proc. 20th Int'l Conf. Artificial Intelligence and Statistics (AISTATS), pp. 1273-1282, 2017.

Nishio, T. and Yonetani, R. Client selection for federated learning with heterogeneous resources in mobile edge. In Proc. IEEE Int'l Conf. on Communications, pp. 1-7, 2019.

Park, H.-S. and Jun, C.-H. A simple and fast algorithm for k-medoids clustering. Expert Systems with Applications, 36(2):3336-3341, 2009.

Park, J., Samarakoon, S., Bennis, M., and Debbah, M. Wireless network intelligence at the edge. arXiv:1812.02858, 2018.

Sahu, A. K., Li, T., Sanjabi, M., Zaheer, M., Talwalkar, A., and Smith, V. On the convergence of federated optimization in heterogeneous networks. arXiv:1812.06127, 2018 .

Sattler, F., Wiedemann, S., M√ºller, K.-R., and Samek, W. Robust and communication-efficient federated learning from non-iid data. arXiv:1903.02891, 2019.

Singla, A., Hong, C.-Y., Popa, L., and Godfrey, P. B. Jellyfish: Networking data centers randomly. In 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI), pp. 225-238, 2012.

Vahdat, A. and Becker, D. Epidemic routing for partiallyconnected ad hoc networks. Technical report, Duke University, 2000.

Wang, S., Tuor, T., Salonidis, T., Leung, K. K., Makaya, C., He, T., and Chan, K. Adaptive federated learning in resource constrained edge computing systems. IEEE Journal on Selected Areas in Communications, 37(6):1205$1221,2019$.

Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv:1708.07747, 2017.

Yoshida, N., Nishio, T., Morikura, M., Yamamoto, K., and Yonetani, R. Hybrid-FL: Cooperative learning mechanism using non-iid data in wireless networks. arXiv:1905.07210, 2019.

Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V. Federated learning with non-iid data. arXiv:1806.00582, 2018.
