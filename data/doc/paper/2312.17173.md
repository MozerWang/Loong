# Non-Vacuous Generalization Bounds for Large Language Models 

Sanae Lotfi*<br>Tim G. J. Rudner<br>Marc Finzi ${ }^{*}$<br>Micah Goldblum<br>New York University

Yilun Kuang*

Andrew Gordon Wilson


#### Abstract

Modern language models can contain billions of parameters, raising the question of whether they can generalize beyond the training data or simply regurgitate their training corpora. We provide the first non-vacuous generalization bounds for pretrained large language models (LLMs), indicating that language models are capable of discovering regularities that generalize to unseen data. In particular, we derive a compression bound that is valid for the unbounded log-likelihood loss using prediction smoothing, and we extend the bound to handle subsampling, making bound computation 900 times faster on massive datasets. To achieve the extreme level of compression required for non-vacuous bounds, we devise SubLoRA, a simple low-dimensional nonlinear parameterization that leads to non-vacuous generalization bounds for very large models with up to 849 million parameters. Finally, we use our bounds to understand LLM generalization and find that larger models have better generalization bounds and are more compressible than smaller models.


## 1 Introduction

Do large language models (LLMs) merely memorize the training data, and if so, are they able to meaningfully generalize beyond their training set? This question is central to understanding LLMs as they continue to grow in capacity and are capable of memorizing and regurgitating training examples verbatim (Brown et al., 2020; Chowdhery et al., 2022; Carlini et al., 2020, 2023).

In this work, we address the question of generalization in LLMs by computing the first non-vacuous generalization bounds for language model pretraining on next token prediction, thereby providing a mathematical guarantee that LLMs are able to generalize beyond their training data.

Although significant progress has been made in constructing non-vacuous generalization bounds for image classification models using the PAC-Bayes framework (Catoni, 2007) in conjunction with extreme levels of model compression (Zhou et al., 2019; Lotfi et al., 2022), non-vacuous generalization bounds for large language models remain elusive.

Compared to image classification models, constructing non-trivial bounds for language models presents additional challenges: (i) LLMs are trained on autoregressive token prediction, and thus token predictions are not independent; (ii) the relevant negative log-likelihood (NLL) metric (bits per dimension) is a continuous and unbounded random variable for which previously used non-vacuous PAC-Bayes bounds are invalid; and (iii) LLMs have orders of magnitude more parameters than image classification models. To address these challenges, we derive new generalization bounds that can be applied to the unbounded bits per dimension[^0]![](https://cdn.mathpix.com/cropped/2024_06_04_55bb7bc7a129f3cb95c4g-02.jpg?height=448&width=1370&top_left_y=242&top_left_x=366)

Figure 1: Finding solutions that simultaneously achieve low training error and low complexity with SubLoRA. (Left): The Pareto frontier of model complexity (the 2nd term in Equation 1) and the empirical risk (bits per dimension (BPD) and Top-1 Error) of language models using LoRA and subspace compression for next token prediction pretraining. The generalization bound is formed from the sum of the two axes (lower is better), with the shaded region showing where bounds are vacuous. Combining both LoRA and subspace compression in the form of SubLoRA yields the best bounds, while using LoRA alone yields vacuous bounds for top-1 error. (Right): SubLoRA enables a smooth tradeoff over the extent of model compression for a fixed model, finding the degree of compression that is optimal for the situation in constructing the generalization bounds. We plot the contributions of the empirical risk and the complexity term to the bound as a function of this degree of compression.

objective. We also introduce an extension of these bounds which can be computed using only a subset of the training data, making bound computation 900 times faster on the OpenWebText dataset, which contains more than 9 billion tokens.

Achieving the extreme level of compression required to obtain non-vacuous generalization bounds for LLMs is another challenge. To this end, we devise SubLoRA (Subspace-Enhanced Low-Rank Adaptation): simple nonlinear parameterization for LLMs that makes it possible to smoothly vary the level of compression while maintaining expressivity. SubLoRA combines low-rank adaptation (LoRA) (Hu et al., 2021), originally proposed for efficient fine-tuning, with subspace training (Li et al., 2018; Lotfi et al., 2022) to pretrain highly compressible LLMs from scratch.

Combining the above-described theoretical and practical contributions, we achieve the first non-vacuous bounds for large language models. To highlight the efficiency of our new compression technique, we compare SubLoRA to LoRA and subspace training in Figure 1 (left). We compute two metrics that we define as follows: Top-1 Error, which is the 0-1 error in predicting the next token averaged over a given document; and the bits per dimension metric, which corresponds to the average negative log-likelihood per document. The shaded region highlights where bounds become vacuous, with SubLoRA achieving non-vacuous bounds for both bits per dimension and Top-1 Error. In contrast, we see that only using LoRA achieves vacuous bounds for Top-1 Error and only using subspace achieves a high value of empirical BPD. Despite the simplicity of SubLoRA, it has an improved ability to trade-off model complexity with training error. In Figure 1 (right), we highlight the trade-off between model complexity and empirical risk in the generalization bounds as we vary the level of compression.

We summarize our contributions as follows:

- Novel bounds for the unbounded negative log-likelihood objective: we introduce novel bounds specifically tailored to account for the unbounded continuous bits-per-dimension loss, commonly used to evaluate LLMs for next-token prediction.
- Subsampling bounds for practical bound evaluation: To make the evaluation of the bounds practical on LLMs with massive datasets, we derive subsampling-based bounds that allow for efficient evaluation. In practice, the evaluation of the bound takes 45 minutes on a single GPU instead of 3 days on 8 GPUs in parallel for the OpenWebText dataset.
- A simple yet powerful nonlinear subspace compression for LLMs: as we show in Figure 1, using LoRA alone to compress the discrepancy between the random initialization and a learned model leads to vacuous bounds for the top-1 error. At the same time, linear subspace training alone does not unlock the full compression potential of LLMs compared to a nonlinear compression scheme. We show that a combination of these two approaches, while simple, yields a strong nonlinear compression of the model, which leads to the best generalization bounds for LLMs.
- Non-vacuous generalization bounds for models with nearly a billion parameters: our work does not only introduce the first non-vacuous generalization bounds for LLMs, but it also extends these bounds to models with over 800 million parameters, demonstrating the scalability of our compression technique.
- Improved understanding of generalization in LLMs: as we increase the size of models, we find that they are more compressible and achieve better bounds, therefore disproving the claim that larger LLMs are simply better at regurgitating their training data.

The significance of these contributions lies in the ability to offer mathematical proof that large language model are, in fact, powerful knowledge compressors and are capable of generalization beyond their training samples, especially as their scale increases. To the best of our knowledge, our work is the first to show that generalization bounds improve with more parameters on models of practical sizes, in line with the empirical benefits of large models.

## 2 Related Work

Generalization bounds. Neural networks have seen widespread adoption because of their strong performance on new unseen test samples, known as generalization. Early generalization theory literature bounded the difference in training and test error, called the generalization gap, using complexity measures like VC-dimension (Vapnik, 1991) and Rademacher complexity (Bartlett and Mendelson, 2002). These generalization bounds were vacuous for neural networks, which are often flexible enough to fit randomly labeled training data (Zhang et al., 2021). The flexibility of neural networks and its negative impact on these classical bounds calls into question why they generalize. Neural networks are so flexible that they have parameter vectors where they fit their training data and simultaneously assign incorrect labels to testing data, and they also have parameter vectors where they fit their training data and instead assign correct labels to the testing data. Why do such flexible models actually make correct test predictions in practice?

PAC-Bayes generalization theory bridges this gap by leveraging the fact that while neural networks are highly flexible and can fit random labels, they encode a preference for the correct ones (Catoni, 2007; Dziugaite and Roy, 2017; Arora et al., 2018). Unlike earlier generalization bounds which measured complexity merely as a function of the hypothesis class, PAC-Bayes generalization bounds reward models which have a strong prior that places its mass on parameter vectors that align with observed data. This formulation allows one to draw a parallel between generalization and compressibility (Zhou et al., 2019; Lotfi
et al., 2022). By placing disproportionate prior mass on compressible parameter vectors, achieving a tight bound simply requires finding a family of models (posterior) that well fit the training data. Such compression bounds achieve the tightest guarantees to date on modern convolutional architectures and large-scale datasets, showcasing the strong inductive bias of neural networks and indicating that they can significantly compress their training sets (Lotfi et al., 2022). While PAC-Bayes has proven a very fruitful framework for devising such bounds, the insight on using a prior to bound the complexity of a given model does not require a posterior and can actually be incorporated into simpler finite hypothesis bounds.

Recent generalization theory literature has expanded analysis to several relevant modelsautoregressive time-series models and simple n-gram language models (McDonald et al., 2011; Bharadwaj and Hasegawa-Johnson, 2014; Vankadara et al., 2022). In contrast, we construct bounds for autoregressive transformer-based language models.

Existing bounds for unbounded objectives. A number of works have explored techniques for generating generalization bounds on unbounded objective functions more generally, but these approaches are not practical for application to LLMs. A well established strategy relevant for e.g. linear regression with Gaussian errors is to bound the tails of the objective as subgaussian random variables, and then generalization bounds can be constructed for subgaussians more generally (Alquier et al., 2016; Germain et al., 2016). Other kinds of known tail behavior have also been exploited (Holland, 2019; Kuzborskij and Szepesvári, 2019). For the NLL of a language model, there is no clear analogous tail behavior, so we must take a different approach.

Haddouche et al. (2021) devise an approach for general unbounded objectives by constructing a hypothesis dependent bound on the objective, even if the objective is unbounded more generally. If the risk can be bounded $\sup _{x} R(h, x) \leq Q(h)$ for a function $Q(h)$, then PACBayes bounds can be constructed using $Q(h)$ even if $\sup _{h} Q(h)=\infty$. However, even though $Q(h)$ is finite for LLMs as there are only a finite number of inputs, $Q$ grows exponentially for NLL with the number of layers in the network and is closely related with the Lipschitz constant. For large models like LLMs, this value is far too large to be useful in constructing bounds.

Language models and compression. Large language models are parameterized with as many as billions of parameters and, as a result, have a significant memory footprint, which makes pretraining, finetuning, and even evaluation challenging without access to large-scale computing infrastructure. To reduce the memory footprint of large language models, a wide array of compression schemes has been proposed to enable evaluation, fine-tuning, and pretraining with limited computational resources. Low-Rank Adaptation (Hu et al., 2021, LoRA) freezes the pre-trained model weights and inserts trainable rank decomposition matrices into each attention layer of the transformer architecture used in large language models. Doing so allows for significantly reducing the number of trainable parameters for fine-tuning on downstream tasks. For example, LoRA can reduce the number of trainable parameters in GPT-3 175B fine-tuned with Adam by a factor of 10,000 and the GPU memory requirement by a factor of 3. Building on LoRA, Q-LoRA (Dettmers et al., 2023a) quantizes a pretrained model to 4-bits, adds a small set of learnable weights parameterized using LoRA, and then tunes these weights by backpropagating gradients through the quantized model. Other compression methods for large language models use distillation (Liu et al., 2023), sub-4-bit integer quantization (Kim et al., 2023; Park et al., 2022), sparse quantized representations that identify and isolate outlier weights (Dettmers et al., 2023b), weight quantization based on approximate second-order information (Frantal et al., 2022), or tensor-train decompositions (Xu et al., 2023).

Achieving a good generalization bound has distinct requirements from the existing compression literature. Unlike existing compression schemes for language models, which aim to accelerate inference and training or to reduce the memory footprint, we focus on specifying
the trained model parameters in only few bits, even if doing so decreases neither latency nor memory requirements.

## 3 Background

Subspace training. Lotfi et al. (2022) train a compressible model by parameterizing a carefully constructed low-dimensional random subspace. The weights $\theta \in \mathbb{R}^{D}$ are then defined as the sum of a random initialization $\theta_{0}$ and a projection $P \in \mathbb{R}^{D \times d}$ from a lowerdimensional subspace $w \in \mathbb{R}^{d}: \theta=\theta_{0}+P w . P$ is constructed as the Kronecker product of random Gaussian matrices $P=\left(Q_{1} \otimes Q_{2}\right) / \sqrt{D}$ for $Q_{1}, Q_{2} \sim \mathcal{N}(0,1)^{\sqrt{D} \times \sqrt{d}}$, normalized so that $P^{\top} P \approx I$. The weights $w$ can then be optimized over by backpropagating through the transformation. With a learned quantization strategy-optimizing over quantized weights and the quantization levels-Lotfi et al. (2022) use arithmetic coding to encode the weights using the empirical probabilities over quantization bins.

Low Rank Adaptation (LoRA). Similarly inspired by evidence that overparametrized models have low intrinsic dimensionality (Li et al., 2018; Aghajanyan et al., 2020), Hu et al. (2021) propose LoRA as a parameter-efficient finetuning method. Given a pretrained weight matrix $W_{\text {pretrained }} \in \mathbb{R}^{a \times b}$, LoRA decomposes its total update $\Delta W$ accumulated throughout finetuning as a product of two trainable low-rank matrices $U \in \mathbb{R}^{a \times r}, V \in \mathbb{R}^{r \times b}$ for $r \ll \min (a, b)$ while freezing $W_{\text {pretrained }}$. Thus $W_{\text {finetuned }}=W_{\text {pretrained }}+\Delta W=W_{\text {pretrained }}+U V$. In this work, we use LoRA for pretraining instead. In particular, we take randomly initialized neural network weights $W_{0} \in \mathbb{R}^{a \times b}$ and represent their update during pretraining as $U V$, yielding $W_{\text {pretrained }}=W_{0}+\Delta W=W_{0}+U V$. We decrease the dimensionality further by applying subspace projection to the LoRA matrices, which we describe in detail in Section 5.

## 4 Methodology

In constructing non-vacuous generalization bounds for LLMs, we expand and improve upon existing techniques in three ways: (1) we construct a simple and effective nonlinear parameterization which is more effective and scalable than purely linear subspaces; (2) we construct new bounds that can handle the continuous and unbounded nature of the negative log-likelihood; (3) we make these bounds more practical to compute with LLMs by deriving a new bound which holds even when the empirical risk is evaluated only on a small subsample of the full training dataset.

### 4.1 Finite Hypothesis Compression Based Generalization Bounds

Given a bounded risk $R(h, x) \in[a, a+\Delta]$ and a finite hypothesis space $h \in \mathcal{H}$ for which we have a prior $P(h)$, it is straightforward to derive a generalization bound relating the empirical risk $\hat{R}(h)=\frac{1}{m} \sum_{i=1}^{m} R\left(h, X_{i}\right)$ to the expected risk $R(h)=\mathbb{E}[\hat{R}(h)]$ so long as $\left\{X_{i}\right\}_{i=1}^{m}$ are sampled independently. With probability at least $1-\delta$, we have

$$
\begin{equation*}
R(h) \leq \hat{R}(h)+\Delta \sqrt{\frac{\log 1 / P(h)+\log 1 / \delta}{2 m}} \tag{1}
\end{equation*}
$$

We provide an elementary proof in Appendix A.1.

If the prior likelihood $P(h)$ of the found model $h$ can be increased (either by choosing a better prior, or by finding more likely hypotheses), then the generalization bound improves. Following Lotfi et al. (2022), we adopt the powerful but general Solomonoff prior $P(h) \leq$ $2^{-K(h \mid A)}$ (Solomonoff, 1964) where $K$ is the prefix Kolmogorov complexity of $h$, with the model architecture $A$ provided as input. While $K$ is not computable, it is possible to compute the upper bound

$$
\log 1 / P(h) \leq K(h \mid A) \log 2 \leq C(h) \log 2+2 \log C(h)
$$

where $C(h)$ is the compressed size of $h$ given any particular strategy for compressing $h$ and we may make use of the prior knowledge describing the architecture. Therefore, if we can find hypotheses $h$ that both have a low empirical risk and a small compressed size, then we can construct strong generalization bounds.

### 4.2 Enabling the Independence Assumption for Generalization Bounds on Text Data

Using Equation 1 requires that $X_{i}$ in the sum $\hat{R}(h)=\frac{1}{m} \sum_{i=1}^{m} R\left(h, X_{i}\right)$ are drawn independently. Thus, we must be careful in the construction and interpretation of our bounds so that this constraint is satisfied. Instead of considering bounds at the level of tokens, which are correlated, we instead define $X_{i}$ to be an entire document sampled uniformly from the data generating process from which the corpus was sampled. In other words, we break our dataset into its constituent documents and sample uniformly documents uniformly from it, where each $X_{i}$ represents an entire document. We define the risk on a given document as the negative log-likelihood of the entire document divided by its length, according to the autoregressive model.

It is also possible to choose $X_{i}$ to be a context chunk, i.e., a sequence of length equal to the context length, as is commonly used in the training of models since a document may be larger than the maximum transformer context length. In such cases, the sequences are no longer independent samples from the data generating process. It is possible to construct valid bounds on these sequences which respect the independence assumption. However, in doing so we must shift the interpretation of the bounds from being over the randomness in sampling from the data generating process to the randomness in sampling sequences that can be constructed from a fixed and finite dataset formed by concatenating the documents together. We explore these alternate sequence-level bounds in Appendix B. However, we believe that the document-level bounds provide a more meaningful and significant statement about generalization.

### 4.3 Accommodating the Unbounded Negative Log-Likelihood Objective Using Prediction Smoothing

The primary metric for pretraining of large language models, as for other autoregressive models, is the negative log-likelihood (NLL), or bits per dimension (BPD), of the generative model. Unlike classification error which is a $\{0,1\}$ valued random variable, the log-likelihood is an unbounded quantity that does not have an obvious sub-Gaussian, or other, well-understood tail behavior.

To overcome this challenge, we construct generalization bounds for BPD not of the original model but instead on a smoothed version of it that limits the worst case behavior. We define this smoothed model as a token-level mixture of the original LLM token predictions and a uniform distribution over the vocabulary of size $V$ :

$$
\begin{equation*}
p_{h}\left(x_{i} \mid x_{<i}\right)=(1-\alpha) p_{\theta}\left(x_{i} \mid x_{<i}\right)+\alpha / V \tag{2}
\end{equation*}
$$

where $p_{\theta}\left(x_{i} \mid x_{<i}\right)$ is the base model of token probabilities, $\alpha \in(0,1)$ is the mixing parameter, and $p_{h}\left(x_{i} \mid x_{<i}\right)$ is the smoothed predictor.

The model on an entire document $X=\left\{x_{i}\right\}_{i=1}^{L}$ composed of $L$ tokens is defined autoregressively in terms of this mixture model $p_{h}(X):=\Pi_{i}^{L} p_{h}\left(x_{i} \mid x_{<i}\right)$, and we find this to be a more effective way of constructing the bounds than constructing the mixture at the document level. In analogy to label smoothing where the labels of the training objective are mixed with the uniform distribution, we term this operation as prediction smoothing.
![](https://cdn.mathpix.com/cropped/2024_06_04_55bb7bc7a129f3cb95c4g-07.jpg?height=466&width=1286&top_left_y=241&top_left_x=408)

Figure 2: Varying Parameters of the Compression Bounds. (Left): A plot of the generalization bound as a function of the projection dimension $d$ with LoRA. The subspace dimension gives us a way to explicitly trade off the degree of compression with the empirical risk, and we optimize $d$ to produce the best bounds. (Right): A plot of the worst case range of BPD values $\Delta$, empirical risk, and the resulting generalization bounds as a function of the prediction smoothing parameter $\alpha$. For each model, a different alpha can be chosen after the models have already been trained.

As we show in Appendix A.2, the NLL of the prediction smoothed model on a document $\operatorname{BPD}(h, X):=-\log _{2} p_{h}(X) / L$ can be bounded as follows:

$$
\log _{2}(V / \alpha)-\Delta \leq \operatorname{BPD}(h, X) \leq \log _{2}(V / \alpha)
$$

for $\Delta=\log _{2}(1+(1-\alpha) V / \alpha)$. With prediction smoothing, the risk $R(h, X)=\operatorname{BPD}(h, X)$ on a given document is bounded in an interval of size $\Delta$, and therefore we can use Equation (1) to generate bounds for negative log-likelihood of this model. We refer to $\Delta$ as the worst-case interval size.

We explore the trade-off over different values of $\alpha$ in Figure 2 (right). As $\alpha$ gets larger, the interval size $\Delta$ representing the worst-case behavior goes down, whereas the empirical risk goes up, leading to a sweet spot in the middle. By defining the hypothesis $h=(\theta, d, r, \alpha)$ to include the model parameters, LoRA space hyperparameters $d, r$, and the mixture weight $\alpha$, we can view $\alpha$ as merely one additional model parameter accounted in $\log 1 / P(h)$. By doing so, we are free to optimize over $\alpha$ in the computation of the bound, and we can do so without retraining the model.

### 4.4 Using Subsampling in Bound Computation

The empirical risk requires evaluating the model on the full training dataset of $m$ data points: $\hat{R}(h)=\frac{1}{m} \sum_{i=1} \hat{R}_{i}(h)$. As large language models are typically trained for only 1 epoch or less, doing so is prohibitively expensive. Instead, we propose to modify our generalization bounds to account for evaluating only a subsample of size $n \ll m$ of the training dataset when computing the empirical risk.

Denoting $\hat{\hat{R}}(h)=\sum_{i=1}^{n} \hat{R}_{\sigma(i)}(h)$ where $\sigma(i)$ is a random sample (with replacement) from $1, \ldots, m$. In Appendix A. 3 we derive a new bound both over the randomness in $\sigma(i)$ and the randomness in $X$ which holds with probability $\geq 1-\delta$ :

$$
\begin{equation*}
R(h) \leq \hat{\hat{R}}(h)+\Delta \sqrt{\frac{\log \frac{1}{P(h)}+\log \frac{1}{s \delta}}{2 m}}+\Delta \sqrt{\frac{\log \frac{1}{(1-s) \delta}}{2 n}} \tag{3}
\end{equation*}
$$

where $s=n /(n+m)$. Using this subsampling bound, we can accelerate bound computation. For dataset sizes in the 10's of millions, we can get away with evaluating only 10,000 data
points after the model has been trained, with a negligible penalty in the bounds. In fact, we need not even train on the entirety of the training data in order to produce valid bounds as long we indeed sample uniformly.

## 5 SubLoRA: A Simple and Efficient Nonlinear Parameterization of the Hypothesis Space

To find compressible solutions $h$ that simultaneously are expressive enough to achieve low training error, we search over a carefully designed manifold of possible parameters that live within the parameter space.

In contrast to Lotfi et al. (2022), we consider a nonlinear parameterization of the model weights $\theta=f\left(\theta_{0}, w\right)$ given by the composition of LoRA (Hu et al., 2021) (a nonlinear parameterization) and the subspace compression matrices. Given a vector of model parameters $\theta$, we break down its constituent components into the different weight matrices $W_{i}$ and associated biases $b_{i}$ : unflatten $(\theta)=\left\{\left(W_{i}, b_{i}\right)\right\}_{i \in I}$. We define a nonlinear parameterization of the hypothesis space,

$$
\begin{equation*}
\theta=\theta_{0}+\operatorname{LoRA}(P w) \tag{4}
\end{equation*}
$$

where LoRA is defined by the implementation of the low-rank products for the weight matrices, leaving the biases unchanged. As $P w$ and $\theta$ are the flattened parameter vectors, LoRA $(\cdot)$ is defined as the operation that unflattens the vector, applies the low-rank product, and then flattens the result. Here, $\theta_{0}$ is merely a random initialization of the model parameters, and $P \in \mathbb{R}^{D \times d}$ is a Kronecker product projector $P=Q_{1} \otimes Q_{2}$ for $Q_{1}, Q_{2}$ constructed by orthogonalizing Gaussian random matrices by $\mathrm{QR}$ factorization: $P_{1}, P_{2} \sim \mathcal{N}(0,1 / \sqrt{D})^{\sqrt{D} \times \sqrt{d}}$ with $Q_{1} R_{1}=P_{1}$ and similarly for $Q_{2}$. We apply LoRA only over the self-attention layer and the last linear layer weight matrices, meaning that other model parameters do not differ from their initialized values. While LoRA was developed for finetuning LLMs, we find that even when pretraining using LoRA, we can achieve non-trivial performance. In order to compress the model, we need only to represent the vector $w$ since $\theta_{0}$ and $P$ are chosen ahead of time and specified in the architecture via random initialization.

In Figure 1 (left), we show the Pareto frontier of empirical risk and the complexity penalty in the relevant generalization bound with LoRA, subspace training, and SubLoRA. Rather than being competing methods for compression, LoRA and subspace training are complementary and exploit different structures in the parameter space to provide a family of models in the original hypothesis space that are both expressive and compressible. SubLoRA achieves a strict improvement over LoRA and subspace training, often being the deciding factor whether the bounds are vacuous or non-vacuous. In Figure 2 (left), we explore how the compressed size of the model and the empirical risk vary as a function of the subspace dimension $d$.

## 6 Non-Vacuous Generalization Bounds for LLMs

We outline the pretraining and bound computation pipeline and present our empirical results.

### 6.1 End-to-end Pipeline

Assembling the components described in Section 4, we train variants of a GPT-style architecture through the nonlinear compressed parameterization in Equation (4). We use several values for the subspace dimension $d$ and two values for the rank of the LoRA matrices $r$. Nearing the end of training, we train for additional steps using quantization-aware training with a small number of quantization levels (with additional details listed in Appendix D). We express $w$ in this quantization and encode it using arithmetic coding to determine the
compressed size of the model. Added to the size of the model are the bits needed to encode the choice of $d, r, \alpha$, the learning rate, and the quantization levels.

We evaluate the empirical log probabilities and token predictions for each token in the sequence on a small subset of the training data $n=10000$. With these predictions, we can compute the generalization bound in Equation (3) as a function of $\alpha$, and we optimize over this parameter for each model. Finally, we can tune the extent of compression through the different choices of $d$ and choose the subspace dimension that produces the best bound.

### 6.2 Non-Vacuous Bounds for GPT-2 Small

We consider the GPT-2 small architecture with $124 \mathrm{M}$ parameters and compute our next token prediction document-level bounds by pretraining these models on the OpenWebText dataset using SubLoRA. We report the results in Table 1. We consider the token level error averaged over a document as the empirical risk. For instance, the Top-1 Error Bound refers to the upper bound on the expected Top-1 error per token averaged over the document $R\left(h, X_{k}\right)=\frac{1}{L} \sum_{i=1}^{L} \mathbf{1}\left[\operatorname{argmax} p\left(x_{i} \mid x_{<i}=x_{<i}^{k}\right)=x_{i}^{k}\right]$, where the upper index $k$ denotes the document index and the lower index denotes the position within the document. Random guess performance is $\log _{2} V$ for BPD and $1-k / V$ for Top-k Error.

The best bounds are indeed obtained using our simple compression technique, which combines the strengths of both low-rank adaptation and subspace training. When we solely apply quantization and arithmetic coding without implementing LoRA or linear subspace compression during the training phase, we obtain vacuous bounds.

Note (Significance of our bounds with I.I.D sampling). The bits-per-dimension for a given document can be computed as the average error for each token in the sequence given previous tokens withing the same document, where the token error here refers to the negative log-likelihood $\operatorname{BPD}(h, X):=-\log _{2} p_{h}(X) / L=-\sum_{i}^{L} \log _{2} p_{h}\left(x_{i} \mid x_{<i}\right) / L$. Therefore, an upper bound on the expected BPD error reflects a guarantee on the average performance of the model at the token level, conditioned on previous tokens within the same document, and is a quantity of interest in language modeling.

Table 1: Our best document-level generalization bounds achieved for the GPT-2 architecture for BPD and Top-k token prediction error, all of which are non-vacuous.

| Metric | SubLoRA | LoRA Only | Subspace Only | Original Model | Random Guess |
| :--- | :---: | :---: | :---: | :---: | :---: |
| Top-1 Error (\%) | $\mathbf{9 6 . 4 1}$ | 100 | 96.52 | 100 | 99.99 |
| Top-10 Error (\%) | $\mathbf{7 7 . 9 0}$ | 84.37 | 79.36 | 100 | 99.98 |
| Top-100 Error (\%) | $\mathbf{5 8 . 3 4}$ | 67.26 | 75.95 | 100 | 99.80 |
| Bits per Dimension | $\mathbf{1 2 . 1 2}$ | 13.09 | 14.59 | 70.76 | 15.62 |

### 6.3 Extending Our Bounds to Larger Models

We use SubLoRA to obtain generalization bounds for much larger variants of GPT-2 of sizes 354M (GPT-2 medium), 458M, 773M (GPT-2 large), and 849M parameters. Table 2 shows that our simple compression approach yields non-vacuous bounds for models with nearly a billion parameters. Moreover, we see that the smallest model, where we previously performed experiments and tuned our hyperparameters, actually achieves the worst bound on bits per dimension as we scale the models up. In conclusion, our approach extends naturally to much larger language models and proves that it is possible to achieve tighter bounds as we increase the size of the model.

Note (Limitations). Note that due to computational constraints, we pre-train the larger GPT-2 variants with SubLoRA only for a limited number of hyperparameter settings in

Table 2: Non-vacuous bounds achieved for GPT-2 architectures with different sizes, ranging from 124 to 849 million parameters. We report below the bounds on the bits-per-dimension (BPD), Top-1 Error, Top-10 Error, and Top-100 Error. All of the BPD bounds are nonvacuous and tighter than the GPT-2 small bounds.

| Model Size | BPD | Top-1 Error | Top-10 Error | Top-100 Error |
| :--- | :---: | :---: | :---: | :---: |
| 124M (GPT-2 small) | 12.12 | 96.41 | 77.90 | 58.34 |
| 354M (GPT-2 medium) | 11.96 | 95.99 | 78.36 | 58.4 |
| 458M (GPT-2 large) | 11.95 | 96.69 | 78.03 | 58.49 |
| 773M (G8.10 | 12.10 | 96.17 | 78.57 | 59.25 |
| 849M | 12.01 | 96.51 | 78.53 | 58.89 |

contrast to the $124 M$ model for which we did a thorough hyperparameter sweep. It is likely that the tightest empirically achievable bounds are much stronger for the new large models than what we report in Table 2.

## $7 \quad$ Understanding the Generalization of LLMs

As language models grow in size, it is clear that they gain an increasing capacity to fit their training data. On the one hand, this increasing capacity might mean that, as LLMs become capable of learning increasingly complex functions, they become increasingly likely to merely memorize their training samples and not perform any meaningful generalization beyond their training corpora. After all, they have many more parameters to use in fitting the data. On the other hand, large language models have proven to be surprisingly capable of generalizing, often extending to tasks that seem quite different from the training objective.

We investigate the tension between these two narratives along several fronts: We assess how generalization bounds change with the size of the model, whether language models can form a compression of the training data even when accounting for their large size, and how structure in the training data affects the generalization of the learned model. In Appendix C, we use our bounds to quantify of the benefits of pre-training in LLMs.

### 7.1 Larger Models Are More Compressible and Generalize Better

Empirically, it has been found that LLMs generalize better as the number of parameters is increased, with a fixed size of dataset (Kaplan et al., 2020; Brown et al., 2020), and this fact is of great importance leading to the creation of ever larger and more powerful models. From a generalization theory perspective, this trend is counterintuitive because of the growing hypothesis class, and a naive analysis would suggest that larger models should generalize worse. To date, we are not aware of any convincing demonstration that generalization bounds improve with more parameters on models of practical sizes.

We evaluate our bounds on a collection of LLMs with different numbers of parameters, choosing the appropriate scaling for the width, depth, number of attention heads, etc. Surprisingly, we find that our generalization bounds in fact improve with model size, even as the training dataset is held fixed. With our SubLoRA compression, larger models are more compressible given a fixed training error. These results are shown in Figure 3. While some explanations for why larger models should generalize better have been put forward in the literature (Nakkiran et al., 2021; Gunasekar et al., 2017), the mechanism by which larger models become more compressible is not clear, and we believe this result is noteworthy and requires further investigation.

In addition to constructing generalization bounds, we can use our compressed models to form a compression of the training dataset itself. In Figure 3, we count the number of bits

![](https://cdn.mathpix.com/cropped/2024_06_04_55bb7bc7a129f3cb95c4g-11.jpg?height=499&width=1350&top_left_y=206&top_left_x=385)

![](https://cdn.mathpix.com/cropped/2024_06_04_55bb7bc7a129f3cb95c4g-11.jpg?height=444&width=358&top_left_y=220&top_left_x=386)

![](https://cdn.mathpix.com/cropped/2024_06_04_55bb7bc7a129f3cb95c4g-11.jpg?height=463&width=559&top_left_y=210&top_left_x=1165)

Figure 3: Larger models achieve stronger generalization bounds. As we scale up the size of the model via the model parameters (holding the training set fixed), we find that our generalization bounds get better rather than worse. Dots show models trained with differing degrees of compression, indicated by their color. On the right we show the number of bits required to express the training dataset using the model and including the model weights in the compression. Classification error bounds consistently favor smaller models, while data compression favors much larger models, and BPD bounds are in between.

needed to encode the model $C(h)$ and the number of bits to encode the data using the model $C\left(\{X\}_{i=1}^{m} \mid h\right)$, which is the negative log-likelihood of the entire dataset according to the model. Adding these two up, we have a compression of the training dataset using the model, and one which is closely related to our generalization bounds.

### 7.2 How Does Generalization of LLMs Depend on Structure in Text?

Neural networks that fit a training dataset of random noise will not be able to generalize, and the ability of overparametrized networks to fit noise implies that uniform convergence is impossible across the general hypothesis class (Nagarajan and Kolter, 2019). This fact is a clear demonstration that the structure of the dataset influences the generalization properties of the model. However, the impact of more subtle structures on generalization is less understood theoretically. Here, we use our bounds to investigate how the temporal order structure relates to generalization.

![](https://cdn.mathpix.com/cropped/2024_06_04_55bb7bc7a129f3cb95c4g-11.jpg?height=353&width=699&top_left_y=1428&top_left_x=1057)

Figure 4: Breaking text structure with permutations. We compute bounds for LLMs that were trained with the order of the tokens shuffled within each sequence.

We train models that explicitly break the temporal structure of the text data by applying random permutations to each sequence during training. Consequently, the model can only make use of the input information as if it were a bag of words. We find that this broken order structure indeed leads to less favorable generalization bounds. Figure 4 shows the best error bounds when the original and perturbed data are used to train the model and evaluate the bounds for the bits per dimension, top-1 error, and top-100 error losses. While the top-1 error bound becomes vacuous as we break the text structure, the top-100 error and bits per dimensions bounds remain non-vacuous. This might be due to the fact that as we perturb the sequence, predicting the next token accurately becomes an extremely difficult task for LLMs, while predicting a token that fits generally into the context, without necessarily being the correct token, is an easier task.

## 8 Discussion

In this work, we have demonstrated that-despite containing a very large number of parameters-large language models are highly compressible. Using highly compressed LLMs, we were able to compute the first non-vacuous generalization bounds for LLM pretraining. Our findings suggest that the development of tighter compression bounds presents a fruitful avenue for understanding how and why language models generalize. We close with a discussion of the limitations of this work, along with their implications for future generalization theory of language models:

Non I.I.D. token level bounds. In our work, we split up the training data into i.i.d. chunks that form the basis of our bounds. However, the loss for each of these chunks also decomposes as a (non i.i.d.) sum, and it is likely that this additional structure could also be exploited in the bound construction to significantly increase the effective number of training samples.

Efficient bound computation on pretrained models. Our procedure for computing generalization bounds requires training LLMs from scratch through our SubLoRA parametrization. It may be possible to devise a fast method of computing bounds on a model that has already been trained, but still constraining its generalization error. Additionally we may hope to bridge the gap between the compressed model and the uncompressed model, which may behave differently in some regards.

Nonlinear parameterizations. Unlike previous state-of-the-art bounds from Lotfi et al. (2022), we employ a non-linear parameterization via LoRA, significantly improving the bounds. This observation opens up an avenue for rich non-linear parameterizations that simultaneously reduce the number of parameters while also including diverse functions which are likely to fit the training data.

Text generation. The SubLoRA technique is by no means a substitute recipe for stateof-the-art language model pretraining. In Table A. 3 and Table A.4, we show samples of generated text using both a GPT-2 style model pretrained in the standard fashion and a GPT-2 style model pretrained using SubLoRA. While the vanilla GPT-2 style model produces reasonable sentences, the SubLoRA pretrained model outputs ungrammatical text which seem to overly favor tokens with high frequencies of appearances in the training dataset.

Alternative approaches to learning with LLMs. Modern language models make possible new inference techniques such as in-context learning and prompt-tuning. These modes are already seeing widespread deployment and warrant analogous theories of generalization.

Generalization beyond the training distribution. Recent work showed that language models prefer low-complexity numerical sequences on which they were not trained, even at random initialization (Goldblum et al., 2023), and generalization theory may be useful for explaining why LLMs can generalize far outside of their training distribution, and even outside of the text modality, for example to tabular data (Hegselmann et al., 2023) or images (Delétang et al., 2023).

## References

Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020.

Pierre Alquier, James Ridgway, and Nicolas Chopin. On the properties of variational approximations of gibbs posteriors. The Journal of Machine Learning Research, 17(1): 8374-8414, 2016 .

Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In International Conference on Machine Learning, pages 254-263. PMLR, 2018.

Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.

Sujeeth Bharadwaj and Mark Hasegawa-Johnson. A PAC-Bayesian approach to minimum perplexity language modeling. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 130-140, Dublin, Ireland, 2014.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. arXiv preprint arXiv:2012.07805, 2020.

Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. Proceedings of the 37th International Conference on Learning Representations (ICLR 2023), 2023.

Olivier Catoni. Pac-bayesian supervised classification: the thermodynamics of statistical learning. arXiv preprint arXiv:0712.0248, 2007.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.

Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. arXiv preprint arXiv:2309.10668, 2023.

Tim Dettmers, Sage Shmitchell, Adam Roberts, Katherine Lee, Tom B. Brown, Dawn Song, and Colin Raffel. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023a.

Tim Dettmers, Sage Shmitchell, Adam Roberts, Katherine Lee, Tom B. Brown, Dawn Song, and Colin Raffel. Spqr: A sparse-quantized representation for near-lossless $11 \mathrm{~m}$ weight compression. arXiv preprint arXiv:2308.07234, 2023b.

Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.

Zdenek Frantal, Audrius Gruslys, and Dusan Kiela. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.

Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016 .

Micah Goldblum, Marc Finzi, Keefer Rowan, and Andrew Gordon Wilson. The no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning. arXiv preprint arXiv:2304.05366, 2023.

Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. Advances in neural information processing systems, 30, 2017.

Maxime Haddouche, Benjamin Guedj, Omar Rivasplata, and John Shawe-Taylor. Pac-bayes unleashed: Generalisation bounds with unbounded losses. Entropy, 23(10):1330, 2021.

Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models. In International Conference on Artificial Intelligence and Statistics, pages 5549-5581. PMLR, 2023.

Wassily Hoeffding. Probability inequalities for sums of bounded random variables. The collected works of Wassily Hoeffding, pages 409-426, 1994.

Matthew Holland. Pac-bayes under potentially heavy tails. Advances in Neural Information Processing Systems, 32, 2019.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization. arXiv preprint arXiv:2305.14152, 2023.

Ilja Kuzborskij and Csaba Szepesvári. Efron-stein pac-bayesian inequalities. arXiv preprint arXiv:1909.01931, 2019.

Glen G Langdon. An introduction to arithmetic coding. IBM Journal of Research and Development, 28(2):135-149, 1984.

Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. arXiv preprint arXiv:1804.08838, 2018.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.

Yuxuan Liu, Qi Xu, Wei Xu, and Juncheng Zhu. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

Sanae Lotfi, Marc Finzi, Sanyam Kapoor, Andres Potapczynski, Micah Goldblum, and Andrew G Wilson. Pac-bayes compression bounds so tight that they can explain generalization. Advances in Neural Information Processing Systems, 35:31459-31473, 2022.

Daniel J McDonald, Cosma Rohilla Shalizi, and Mark Schervish. Generalization error bounds for stationary autoregressive models. arXiv preprint arXiv:1103.0942, 2011.

Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning. Advances in Neural Information Processing Systems, 32, 2019.

Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124003, 2021.

Gunho Park, Jihye Kim, Jaeyoung Kim, Eunho Choi, Sungroh Kim, Seungjoo Kim, Minsu Lee, Hyeonwoo Shin, and Juho Lee. Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language model. arXiv preprint arXiv:2206.09557, 2022.

Ray J Solomonoff. A formal theory of inductive inference. part i. Information and control, 7 (1):1-22, 1964 .

Leena Chennuru Vankadara, Philipp Michael Faller, Michaela Hardt, Lenon Minorics, Debarghya Ghoshdastidar, and Dominik Janzing. Causal forecasting: generalization bounds for autoregressive models. In Uncertainty in Artificial Intelligence, pages 2002-2012. PMLR, 2022.

Vladimir Vapnik. Principles of risk minimization for learning theory. Advances in neural information processing systems, 4, 1991.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019 .

Qi Xu, Wei Xu, and Juncheng Zhu. Tensorgpt: Efficient compression of the embedding layer in llms based on the tensor-train decomposition. arXiv preprint arXiv:2307.00526, 2023.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107-115, 2021.

Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Nonvacuous generalization bounds at the imagenet scale: a pac-bayesian compression approach. In International Conference on Learning Representations, 2019.
