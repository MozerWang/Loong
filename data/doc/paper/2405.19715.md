# SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths 

Kaixuan Huang<br>Princeton University<br>kaixuanh@princeton.edu

Xudong Guo<br>Tsinghua University<br>gxd20@mails.tsinghua.edu.cn

Mengdi Wang<br>Princeton University<br>mengdiw@princeton.edu


#### Abstract

Speculative decoding reduces the inference latency of a target large language model via utilizing a smaller and faster draft model. Its performance depends on a hyperparameter $K$ - the candidate length, i.e., the number of candidate tokens for the target model to verify in each round. However, previous methods often use simple heuristics to choose $K$, which may result in sub-optimal performance. We study the choice of the candidate length $K$ and formulate it as a Markov Decision Process. We theoretically show that the optimal policy of this Markov decision process takes the form of a threshold policy, i.e., the current speculation should stop and be verified when the probability of getting a rejection exceeds a threshold value. Motivated by this theory, we propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly. We augment the draft model with a trained acceptance prediction head to predict the conditional acceptance probability of the candidate tokens. SpecDec++ will stop the current speculation when the predicted probability that at least one token gets rejected exceeds a threshold. We implement SpecDec++ and apply it to the llama-2-chat 7B \& 70B model pair. Our adaptive method achieves a 2.04x speedup on the Alpaca dataset (an additional $7.2 \%$ improvement over the baseline speculative decoding). On the GSM8K and HumanEval datasets, our method achieves a $2.26 \mathrm{x}$ speedup ( $9.4 \%$ improvement) and $2.23 \mathrm{x}$ speedup ( $11.1 \%$ improvement), respectively.


## 1 Introduction

Current state-of-the-art Large Language Models (LLMs) have demonstrated extraordinary capabilities in various language tasks and have shown early signs of artificial general intelligence [1, 3, 40-42]. As the top-performing LLMs often have more than a hundred billion parameters, there is an increasing demand for serving such huge models efficiently. To decrease the inference latency, motivated by speculative execution techniques in processors, speculative decoding [8, 24] incorporates a draft model, which is smaller and faster, as the speculator for the target model, which is the large language model we want to accelerate. Given the current prefix, the draft model first auto-regressively generates $K$ tokens, taking substantially less time than it would take the target model. The target model computes their log probabilities in parallel and then sequentially determines whether each token is accepted or not. Following the first rejected token (if any), the algorithm discards the remaining tokens and corrects the rejected token with a fresh sample from a modified distribution. If all tokens are accepted, a new token is sampled from the next-token probability given by the target

![](https://cdn.mathpix.com/cropped/2024_06_04_14e5db9689ddc0750527g-02.jpg?height=363&width=1330&top_left_y=236&top_left_x=384)

Figure 1: The performance of SpecDec++. Compared with the baseline speculative decoding (SpecDec) with fixed candidate lengths, by adaptively determining the candidate lengths via a trained acceptance prediction head, SpecDec++ achieves a relative $\mathbf{7 . 2 \% , 1 1 . 1 \%}$, and $\mathbf{9 . 4 \%}$ improvement over the baseline methods on the Alpaca, HumanEval, and GSM8K dataset, respectively. The experiments are conducted with llama-2-chat 7B \& 70B model pair on 2 NVIDIA A100-80G GPUs.

model and appended to the sequence of accepted tokens, and then the process moves forward. Such draft-verify-correct loops continue until the desired output is fully generated.

The speedup effect of speculative decoding depends on two crucial aspects: (1) how well the draft model aligns with the target model, and (2) how fast the draft model gets compared to the target model. The two aspects influence the choice of the hyperparameter $K$ : the number of candidate tokens generated by the draft model in each loop. When the draft model aligns well and/or runs fast, we can choose a larger $K$, which potentially allows more tokens to be accepted in each loop. However, a larger $K$ also increases the chances of rejection so that more tokens get discarded.

Leviathan et al. [24] studied the problem of choosing the hyperparameter $K$ under the assumption that the acceptance rates of all the candidate tokens are constant. The authors showed that there exists one constant $K$ that can maximize the speedup. However, such an assumption is unrealistic and does not approximate real-world cases well. Whether the draft model and the target model align well depends on the hardness of predicting the next token. Intuitively, when the next token is unambiguous from the prefix, the draft model and the target model align well, which means the acceptance probability of the current candidate token is large compared to other cases.

In this work, we aim to boost the performance of speculative decoding by adaptively choosing the candidate length $K$ for each round. We formalize the adaptive decision-making of $K$ for speculative decoding as a Markov Decision Process (MDP). The decision to make at each timestep is whether or not to stop the current speculation round and submit the candidate tokens to the target model for verification and correction. The objective is to minimize the total inference time taken to generate a full response. Theoretically, we show that the optimal policy takes the form of a threshold policy, i.e., it is optimal to stop the speculation round whenever the probability of existing at least one rejected token in the candidates exceeds a threshold.

Inspired by the theory, we propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly. First, we train an acceptance prediction head on top of the draft model to predict the acceptance probability of the candidate token. Training such an acceptance prediction head has two challenges: (1) there will be a severe class imbalance problem, e.g., most tokens generated by the draft model will have a high probability of acceptance, depending on how well the two models align; (2) the input sequence to the model contains mostly tokens from the target model and only a fraction of tokens generated by the draft model, so the training signal is sparse. To overcome the two challenges, we adopt a weighted Binary Cross-Entropy loss to address the class imbalance problem, and we adapt the random masking idea from BERT [14] to randomly mix tokens from the target model and the draft model to increase training efficiency.

At inference time, we opt to stop the current speculation round when the predicted probability of the existence of a rejected token exceeds a constant stopping threshold. The procedure is illustrated in Figure 2. We implement SpecDec++ and apply it to llama-2-chat 7B \& 70B model pair. Our adaptive method achieves a $2.04 \mathrm{x}$ speedup compared with the $1.90 \mathrm{x}$ speedup of the baseline speculative decoding method on the Alpaca dataset (an additional $7.2 \%$ improvement). On the easier GSM8K and HumanEval datasets, our method improves the baseline from 2.07x to 2.26x speedup $(9.4 \%$ improvement) and from $2.00 x$ to $2.23 x$ speedup ( $11.1 \%$ improvement), respectively.

We summarize the contributions below.

- We formalize the dynamic choice of candidate length in speculative decoding as a Markov Decision Process (MDP). We theoretically show that when the probability that at least one token gets rejected exceeds a threshold, the optimal action is to stop the speculation and submit it for verification.
- We propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly. We adopt a weighted loss and a token-mixing method to efficiently train the prediction head and use it for dynamic decision-making in the decoding process.
- Empirically, our method achieves an additional $7.2 \%, 9.4 \%$, and $11.1 \%$ improvement over the baseline speculative decoding on the Alpaca, HumanEval, and GSM8K datasets, respectively.


## 2 Background

Rejection Sampling. If we want to sample from a target discrete distribution $p(x)$, we first sample $x$ from a draft distribution $q(x)$. We accept the sample $x$ with probability $\min \left(1, \frac{p(x)}{q(x)}\right)$; otherwise we replace it with a sample from the modified distribution $\operatorname{Norm}\left[(p-q)_{+}\right]$, where $z_{+}=\max (z, 0)$ is the positive part of $z$ and $\operatorname{Norm}[f]=\frac{f(\cdot)}{\sum_{x} f(x)}$ normalizes a function $f$ to make it a proper probability distribution. The proof of the unbiasedness of rejection sampling can be found in [8].

Speculative Decoding. Speculative decoding extends to the auto-regressive generation scenarios by chaining $K$ rejection sampling procedures together. To auto-regressively generate a sequence from $p\left(\cdot \mid x_{\text {prefix }}\right)$, we first generates $K$ candidate tokens $\left(y_{1}, y_{2}, \ldots, y_{K}\right)$ from $q\left(\cdot \mid x_{\text {prefix }}\right)$

$$
y_{i} \sim q\left(Y_{i} \mid x_{\text {prefix }}, y_{1}, \ldots, y_{i-1}\right), \quad i=1,2, \ldots, K
$$

Next, we sequentially check if each $y_{i}$ is accepted or not. If there is any rejection, we replace the first rejected token with a fresh sample from the corresponding modified probability distribution and discard the subsequent tokens.

The key practical consideration is that the probabilities of the candidate tokens $p\left(y_{i}\right.$ | $\left.x_{\text {prefix }}, y_{1}, \ldots, y_{i-1}\right)$ can be calculated in parallel by the target model with no additional overhead, as the forward time is bottlenecked by the memory operations [32]. For completeness, the speculative decoding algorithm is stated in Algorithm 1.

```
Algorithm 1 Speculative Decoding [8, 24]
Require: draft model $q$, target model $p$, prefix $x_{\text {prefix }}$, number of candidate tokens $K$.
    for $i=1$ to $K$ do
        Compute $q_{i}=q\left(\cdot \mid x_{\text {prefix }}, y_{1}, \ldots, y_{i-1}\right)$.
        Sample $y_{i} \sim q_{i}$.
    end for
    Compute in parallel $p_{i}=p\left(\cdot \mid x_{\text {prefix }}, y_{1}, \ldots, y_{i-1}\right)$ for $i=1, \ldots, K+1$.
    Sample $r_{1}, \ldots, r_{K}$ with $r_{i} \sim \operatorname{Unif}[0,1], i=1, \ldots, K$.
    Compute the number of accepted tokens $n=\min \left(\left\{i-1 \mid r_{i} \geq p_{i}\left(y_{i}\right) / q_{i}\left(y_{i}\right)\right\} \cup K\right)$.
    if $n<K$ then
        Sample $y^{\prime}$ from the modified distribution $\operatorname{Norm}\left[\left(p_{n+1}-q_{n+1}\right)_{+}\right]$
    else
        Sample $y^{\prime}$ from $p_{K+1}$
    end if
    Return $x_{\text {prefix }}, y_{1}, \ldots, y_{n}, y^{\prime}$
```


## Inference Time of Speculative Decoding.

Our objective is to minimize the total inference time, which consists of

$$
\begin{equation*}
T_{\text {total }}=t_{\text {draft }} N_{\text {draft }}+t_{\text {target }} N_{\text {target }} \tag{2.1}
\end{equation*}
$$

where $t_{\text {draft }}$ and $t_{\text {target }}$ are the time needed for one forward pass and $N_{\text {draft }}$ and $N_{\text {target }}$ are the total number of forward passes of the draft model and the target model, respectively. Equation (2.1) holds under the implicit assumption that the forward passes of each of the models take constant time, which
is true when we have enough computational resources to support the increased concurrency when the length of the input sequence grows [24]. We empirically verify that Equation (2.1) holds in our setting; see Section 4.2.

Let $N$ be the number of the final generated tokens. $N$ is a random variable inherent to the target model and the initial prompt, independent of the draft model and the number of candidate tokens $K$ of each round we choose. Let $N_{\text {discarded }}$ be the number of total discarded tokens. Then we have the following identity for Algorithm 1

$$
N_{\text {draft }}+N_{\text {target }}=N+N_{\text {discarded }}
$$

Therefore, $T_{\text {total }}$ can be written as

$$
\begin{equation*}
T_{\text {total }}=T_{0}+t_{\text {draft }} N_{\text {discarded }}+\left(t_{\text {target }}-t_{\text {draft }}\right) N_{\text {target }} \tag{2.2}
\end{equation*}
$$

where $T_{0}=t_{\text {draft }} N$ is the oracle inference time.

To minimize the total inference time, we are required to trade-off between two objectives: minimizing the number of the discarded tokens $N_{\text {discarded }}$ and minimizing the number of forward passes of the target model $N_{\text {target }}$. The two objectives conflict with each other, as a larger $K$ will incur more discarded tokens but less number of forward passes of the target model. Equation (2.2) states that the total cost is the weighted sum of the two and the weights are given by $t_{\text {draft }}$ and $\left(t_{\text {target }}-t_{\text {draft }}\right)$.

Metrics. To measure the benefit of a speculative decoding pipeline, we divide Equation (2.2) by $N$ and get

$$
\begin{equation*}
\text { latency }=T_{\text {total }} / N=t_{\text {draft }}+t_{\text {draft }} \cdot N_{\text {discarded }} / N+\left(t_{\text {target }}-t_{\text {draft }}\right) \cdot N_{\text {target }} / N \tag{2.3}
\end{equation*}
$$

We focus on two metrics: (1) discard rate $N_{\text {discarded }} / N$, which measures the average number of discarded tokens per one generated token, and (1) verification rate $N_{\text {target }} / N$, which measures the average number of the forward calls of the target model per one generated token.

### 2.1 A Motivating Example: Oracle Performances of Greedy Speculative Decoding

Let us focus on a simplified deterministic setting of speculative decoding, where we use greedy decoding for the draft model and the target model. In this setting, the draft model deterministically generates a series of greedy tokens $\left(Y_{1}, \ldots, Y_{K}\right)$, and the speculative decoding algorithm reduces to sequentially checking whether $Y_{i}$ is also the greedy token of the target model. The first rejected token is replaced by the greedy token of the target model. If all the tokens are accepted, an additional token is generated by the target model directly.

For a given prompt $x_{\text {prompt }}$, let $\left(X_{1}, X_{2}, \ldots, X_{N}\right)$ be the greedy tokens generated by the target model. We ask the following question:

What is the oracle performance of the speculative decoding algorithm we can obtain by varying the number of candidate tokens, if we have the knowledge of $\left(X_{1}, X_{2}, \ldots, X_{N}\right)$ in hindsight?

Let us consider the first speculation round. The draft model generates $\left(Y_{1}, Y_{2}, \ldots\right)$ greedily. Let $Y_{i}$ be the first token such that $Y_{i} \neq X_{i}$. The optimal strategy is to stop the speculation at time $(i-1)$, so the last candidate token $Y_{i-1}$ is accepted, and $Y_{i}$ will be generated directly by the target model, because (1) if we stop the speculation earlier, then the shorter candidate tokens will still be accepted, but this induces at least one unnecessary forward pass of the target model; (2) if we stop the speculation later, then we waste at least one candidate token $Y_{i}$. By repeating the argument, we have the following.

Lemma 2.1. In the greedy decoding setting, for a given prompt $x_{\text {prompt }}$, let $\left(X_{1}, X_{2}, \ldots, X_{N}\right)$ be the greedy tokens generated by the target model. We define $Y_{i}=\operatorname{argmax} q($. $\left.x_{\text {prompt }}, X_{1}, X_{2}, \ldots, X_{i-1}\right)$ to be the greedy token of the draft model $q$ conditioned on the partial generation of the target model. Let $S$ be the set of disagreement between the draft model and the target model: $S=\left\{1 \leq i \leq N \mid Y_{i} \neq X_{i}\right\}$. Then, by optimally stopping at time $(i-1)$ for every $i \in S$, we obtain the oracle performance with $N_{\text {discarded }}=0$ and $N_{\text {target }}=|S|+1$.

To empirically verify this, we perform a preliminary experiment with the same setting in Section 4, where we use all the prompts in the Alpaca dataset and calculate the set of disagreement $S$ for each prompt with the llama-2-chat-7B/llama-2-chat-70B model pair. The results show that the average $N_{\text {target }} / N=0.164 \pm 0.078$ and the corresponding oracle throughput is $27.06 \pm 4.13$ tokens/second (2.92x speedup) according to Equation (2.3) with the empirical value of $\left(t_{\text {target }}, t_{\text {draft }}\right)$ reported in

![](https://cdn.mathpix.com/cropped/2024_06_04_14e5db9689ddc0750527g-05.jpg?height=673&width=1391&top_left_y=249&top_left_x=367)

Figure 2: SpecDec++ uses a trained acceptance prediction head to predict the conditional acceptance probability of the candidate tokens. When the predicted probability of the existence of at least one rejected token exceeds the stopping threshold $h$, the current speculation round ends and the candidate tokens go through the target model for verification and correction.

Section 4.2. In comparison, the average throughput for the target model without speculative decoding is 9.26 tokens/second, while speculative decoding with the best fixed $K$ gives 17.58 tokens/second (1.90x speedup) (Section 4). We see that there is a huge potential in adaptively tuning the candidate lengths.

## 3 SpecDec++: Theory and Algorithm

### 3.1 Speculative Decoding as Markov Decision Processes

We formulate speculative decoding into the following Markov Decision Process (MDP) framework.

States. We define the tuple $s=\left(x_{\text {prefix }},\left(Y_{1}, \ldots, Y_{k}\right)\right)$ as the current state of the MDP. Specifically, $x_{\text {prefix }}$ is the concatenation of the prompt and the partial response containing all the accepted tokens. $\left(Y_{1}, \ldots, Y_{k}\right)$ is the current candidate tokens, which are auto-regressively sampled from the draft distribution $q$ :

$$
Y_{i} \sim q\left(\cdot \mid x_{\text {prefix }}, Y_{1}, \ldots, Y_{i-1}\right), \quad i=1,2, \ldots
$$

The initial state of the MDP is $\left(x_{\text {prompt }}, \varnothing\right)$.

Actions. Given the current state $\left(x_{\text {prefix }},\left(Y_{1}, \ldots, Y_{k}\right)\right)$, the decision to make is whether or not to end the current speculation round and submit the candidate tokens to the target model for verification. We denote the current action by $a \in\{$ stop, continue $\}$ as the choice of stopping or continuing the current speculation round.

We note that in an extended MDP setting, we can include the draft probability $q_{k+1}$ for the token $Y_{k+1}$ as a part of the current action. Finetuning the draft model to align better with the target model can be viewed as an offline policy optimization algorithm that will likely improve the performance. And it has been studied in previous work, e.g. DistillSpec [54] and Medusa [7]. In the paper, we consider the draft probability $q_{k+1}$ as given by the draft model and do not optimize $q_{k+1}$.

Transitions. First, we draw a random sample $Y_{k+1} \sim q_{k+1}$ and append $Y_{k+1}$ to the current list of the candidate tokens.

- When $a=$ continue, the next state $s^{\prime}$ is simply $\left(x_{\text {prefix }},\left(Y_{1}, \ldots, Y_{k}, Y_{k+1}\right)\right)$.[^0]
- When $a=$ stop, the candidate tokens $\left(Y_{1}, \ldots, Y_{k+1}\right)$ are verified via speculative decoding (Algorithm 1). Let $n$ be the number of the accepted tokens. Let $y^{\prime}$ be the replaced token when $n<k+1$ or the fresh token from the next-token distribution given by the target model when $n=k+1$. The next state $s^{\prime}=\left(x_{\text {prefix }}^{\prime}, \varnothing\right)$ with the new prefix $x_{\text {prefix }}^{\prime}=\left(x_{\text {prefix }}, y_{1}, \ldots, y_{n}, y^{\prime}\right)$ being the concatenation of the previous prefix and the newly generated tokens.

Immediate Costs. According to Equation (2.2), let $c_{1}=t_{\text {draft }}$ and $c_{2}=\left(t_{\text {target }}-t_{\text {draft }}\right)$. We can define the immediate cost as the following

$$
\begin{aligned}
& c\left(s, \text { continue, } s^{\prime}\right)=\mathbb{I}\left(\exists 1 \leq i \leq k+1, Y_{i} \text { is rejected }\right) \cdot c_{1} \\
& c\left(s, \text { stop }, s^{\prime}\right)=\mathbb{I}\left(\exists 1 \leq i \leq k+1, Y_{i} \text { is rejected }\right) \cdot c_{1}+c_{2}
\end{aligned}
$$

For both cases, we suffer a loss $c_{1}$ if the current candidate token $Y_{k+1}$ is discarded, which happens if there exists any candidate token $Y_{i}(1 \leq i \leq k+1)$ that is rejected. If we choose to stop at the current step, we suffer an additional cost $c_{2}$ corresponding to the extra inference time of the target model.

Note that different from the traditional MDP setting when the reward/cost is immediately available to the learner, our setting is more related to the delayed feedback setting $[18,23,50,10]$, where the cost is only available after the candidate tokens are submitted to the target model for verification.

Theorem 3.1. For any time-homogeneous policy $\pi$ that has an upper bound for the number of candidate tokens, at the current state $s=\left(x_{\text {prefix }},\left(Y_{1}, \ldots, Y_{k}\right)\right)$, when

$$
\mathbb{P}\left(\exists 1 \leq i \leq k, Y_{i} \text { is rejected } \mid x_{\text {prefix }}\right) \geq \frac{c_{2}+\Delta}{c_{1}+c_{2}+\Delta}
$$

the expected total cost of stop is smaller than the expected total cost of continue, where $\Delta=$ $\Delta\left(\pi, x_{\text {prompt }}, p, q, c_{1}, c_{2}\right)$ is a problem-specific constant.

We defer the proof of Theorem 3.1 to Appendix D.

### 3.2 SpecDec++

Theorem 3.1 provides a sufficient condition for us to stop the current round of speculation and call the target model to verify the candidate tokens. Motivated by Theorem 3.1, we propose SpecDec++, an adaptive speculative decoding algorithm that utilizes an additional prediction head to determine whether or not to stop the current speculation round.

SpecDec++ incorporates an additional prediction head $f_{\theta}$ on top of the draft model that predicts the conditional probability

$$
\mathbb{P}\left(Y_{i} \text { is accepted } \mid Y_{1}, \ldots, Y_{i-1} \text { are accepted }, x_{\text {prefix }}\right)=\min \left(1, \frac{p\left(Y_{i} \mid x_{\text {prefix }}, Y_{1}, \ldots, Y_{i-1}\right)}{q\left(Y_{i} \mid x_{\text {prefix }}, Y_{1}, \ldots, Y_{i-1}\right)}\right)
$$

We opt to implement a small prediction head such that the computational overhead is negligible compared to a forward pass of the draft model. During inference time, we feed the input ( $\left.x_{\text {prefix }}, Y_{1}, \ldots, Y_{i}\right)$ to the draft model and obtain the final embedding $\boldsymbol{e}_{i}$ of the last token $Y_{i}$. The predicted acceptance probability is given by

$$
\begin{equation*}
\widehat{\mathbb{P}}\left(Y_{i} \text { is accepted } \mid Y_{1}, \ldots, Y_{i-1} \text { are accepted }, x_{\text {prefix }}\right)=\operatorname{sigmoid}\left(f_{\theta}\left(\boldsymbol{e}_{i}\right)\right) \tag{3.1}
\end{equation*}
$$

Given a threshold $h$, we end the current round of speculation when the predicted probability that there exists one rejected token exceeds $h$

$$
\pi\left(s_{k}\right)=\text { stop } \Leftrightarrow \widehat{\mathbb{P}}\left(\exists 1 \leq i \leq k, \text { such that } Y_{i} \text { is rejected } \mid x_{\text {prefix }}\right)>h
$$

which can be computed by chain rule

$$
\begin{aligned}
& \widehat{\mathbb{P}}\left(\exists 1 \leq i \leq k, \text { such that } Y_{i} \text { is rejected } \mid x_{\text {prefix }}\right) \\
= & 1-\prod_{i=1}^{k} \widehat{\mathbb{P}}\left(Y_{i} \text { is accepted } \mid Y_{1}, \ldots, Y_{i-1} \text { are accepted , } x_{\text {prefix }}\right)
\end{aligned}
$$

We summarize the proposed algorithm in Algorithm 2 and illustrate it in Figure 2.

```
Algorithm 2 SpecDec++
Require: draft model $q$, target model $p$, prefix $x_{\text {prefix }}$, acceptance prediction head $f_{\theta}$, threshold
    $h$.
Initialize the cumulative acceptance probability $\widehat{p}=1$
for $i=1$ do
    if $i>1$ then
        Compute the final hidden embedding $\boldsymbol{e}_{i-1}$ of the token $y_{i-1}$.
    end if
    Compute $q_{i}=q\left(\cdot \mid x_{\text {prefix }}, y_{1}, \ldots, y_{i-1}\right)$.
    Sample $y_{i} \sim q_{i}$.
    Update $\widehat{p} \leftarrow \widehat{p} \cdot \operatorname{sigmoid}\left(f_{\theta}\left(\boldsymbol{e}_{i-1}\right)\right)$.
    if $1-\widehat{p}>h$ then
        Break
    end if
end for
```

Let $K$ be the number of candidate tokens in the previous for-loop.
Compute in parallel $p_{i}=p\left(\cdot \mid x_{\text {prefix }}, y_{1}, \ldots, y_{i-1}\right)$ for $i=1, \ldots, K+1$.
Sample $r_{1}, \ldots, r_{K}$ with $r_{i} \sim \operatorname{Unif}[0,1], i=1, \ldots, K$.
Compute the number of accepted tokens $n=\min \left(\left\{i-1 \mid r_{i} \geq p_{i}\left(y_{i}\right) / q_{i}\left(y_{i}\right)\right\} \cup K\right)$.
if $n<K$ then
Sample $y^{\prime}$ from the modified distribution $\operatorname{Norm}\left[\left(p_{n+1}-q_{n+1}\right)_{+}\right]$
else
Sample $y^{\prime}$ from $p_{K+1}$
end if
Return $x_{\text {prefix }}, y_{1}, \ldots, y_{n}, y^{\prime}$

### 3.3 Training Dataset and Objective

Let $\mathcal{D}_{\text {prompt }}$ be the prompt distribution. For each $x_{\text {prompt }}$ in $\mathcal{D}_{\text {prompt }}$, we generate a response $\left(X_{1}, \ldots, X_{N}\right)$ using the target model. Next, we feed the prompt and the response into the draft model to get $q\left(\cdot \mid x_{\text {prompt }}, X_{1}, \ldots, X_{i-1}\right)$ for every $i$. We sample a $Y_{i}$ from the distribution and calculate the conditional acceptance probability $\mathbb{P}_{i}=\min \left(1, \frac{p\left(Y_{i} \mid x_{\text {prompt }}, X_{1}, \ldots, X_{i-1}\right)}{q\left(Y_{i} \mid x_{\text {prompt }}, X_{1}, \ldots, X_{i-1}\right)}\right)$ for each token, which will be the training target.

We construct the response sequence $\left(Z_{1}, \ldots, Z_{N}\right)$ by randomly taking $r \%$ tokens from $\left(X_{1}, \ldots, X_{N}\right)$ and the remaining tokens from $\left(Y_{1}, \ldots, Y_{N}\right)$, borrowing the random masking idea from BERT [14]. We only compute losses for the tokens from $\left(Y_{1}, \ldots, Y_{N}\right)$.

We note that there will be distribution shifts between $\left(x_{\text {prefix }}, Y_{1}, \ldots, Y_{k}\right)$, the sequence encountered during the inference process, and $\left(x_{\text {prefix }}, Z_{1}, \ldots, Z_{k}\right)$, the sequence encountered during training process. The distribution shift may cause certain biases in the prediction head, e.g., over-confident about the acceptance. Furthermore, as in the typical setting of speculative decoding where the draft model and the target model align reasonably well, there will be class imbalance issues in the training dataset, where most of the training examples will have $\mathbb{P}_{i}$ close to 1 .

To accommodate the issues above, we train the prediction head using a weighted binary cross-entropy (BCE) loss, taken over the tokens $Z_{i}$ 's stemming from $Y_{i}$ 's:

![](https://cdn.mathpix.com/cropped/2024_06_04_14e5db9689ddc0750527g-07.jpg?height=132&width=1092&top_left_y=2262&top_left_x=514)

where $w_{\text {acc }}$ and $w_{\text {rej }}$ are the weights and $\widehat{\mathbb{P}}_{i}=\operatorname{sigmoid}\left(f_{\theta}\left(\boldsymbol{e}_{i}\left(x_{\text {prompt }}, Z_{1}, \ldots, Z_{i-1}, Y_{i}\right)\right)\right)$.

## 4 Experiments

### 4.1 Experimental Setups

Datasets and Model Pairs. We adopt three datasets in our experiments: Alpaca [39], HumanEval [9], GSM8K [13]. We only use prompts of the datasets and do not use responses. In the experiments, we use llama-2-chat models [42]. We choose to use llama-2-chat 7B as the draft model and llama-2-chat 70B as the target model. To reduce memory consumption, we use the bfloat16 format for the models.

Network Architecture, Weighted BCE Loss, and Stopping Criteria for SpecDec++. We build a $(D+1)$-layer ResNet with SiLU activation as the acceptance prediction head, and we sweep $D$ from 0 (linear layer) to 4 in the experiments. We adopt the weighted BCE loss where set $w_{\text {acc }}=1$ and choose $w_{\text {rej }}$ from $\{1,3,6,12\}$. We tune the stopping threshold $h$ in $\{0.1,0.3,0.5,0.7,0.9\}$. To ensure the robustness of SpecDec++, we manually stop each speculation round when the number of candidate tokens exceeds 20 .

Baseline Method. We compare SpecDec++ with the naive speculative decoding algorithm where the number of the candidate tokens $K$ is fixed as a hyperparameter. We tune $K$ in $\{2,4,6,8,10,12,14\}$.

Due to space limits, additional experimental setup is deferred to Appendix C.1.

### 4.2 Forward Time Analysis

First, we verify the correctness of Equation (2.1) and determine the forward time of the draft model $t_{\text {draft }}$ and the target model $t_{\text {target }}$ under our specific setting. We collect all the $\left(N_{\text {draft }}, N_{\text {target }}, T_{\text {total }}\right)$ tuples from generations using speculative decoding (either the baseline version or SpecDec++) and perform a linear regression to determine the coefficients. We also determine the standalone inference time when using only the draft model or the target model with linear regression. The linear regressions fit well with all $R^{2} \geq 0.98$ and the results are summarized in Table 2. Additionally, we visualize $t_{\text {draft }}$ and $t_{\text {target }}$ across the three settings in Figure 3.
![](https://cdn.mathpix.com/cropped/2024_06_04_14e5db9689ddc0750527g-08.jpg?height=262&width=1266&top_left_y=1370&top_left_x=424)

Figure 3: The forward time of the draft model (llama-2-chat-7B) and the target model (llama-2-chat70B) under different settings. For each setting, we perform linear regression to calculate the forward times and then average them across different datasets. The additional cost of the acceptance prediction head is negligible compared to the systematic error and the random noise of the environment. Full results are deferred to Table 2 .

The additional cost of the acceptance prediction head is negligible, as we find that the average $t_{\text {draft }}$ in SpecDec++ setting is smaller than the average $t_{\text {draft }}$ in baseline SpecDec setting by $0.0004 s$, which is likely caused by random noise of the environment, as the standard deviation between difference datasets around $0.0006 \mathrm{~s}$. Therefore, for both the baseline speculative decoding setting and SpecDec++ setting, we choose $\left(t_{\text {draft }}, t_{\text {target }}\right)=(0.0234,0.112)$, which is the average between the two cases. We use Equation (2.3) to calculate the theoretical throughputs (tokens per second), which match the noisier empirical throughputs well with relative error $\leq 6.2 \%$ for all prompts.

In the standalone setting where only the draft model or the target model is used, we see significant decreases in both $t_{\text {draft }}$ and $t_{\text {target }}$, which indicates that speculative decoding induces minor additional communication overhead. We use $\left(t_{\text {draft }}, t_{\text {target }}\right)=(0.0207,0.108)$ for the stand-alone setting. The average throughput for the target model is 9.26 tokens/second.

### 4.3 Performances

We test the performances of the baseline speculative decoding with different $K$ and SpecDec++ with the different acceptance prediction heads and different thresholds $h$. We calculate the discard rates
![](https://cdn.mathpix.com/cropped/2024_06_04_14e5db9689ddc0750527g-09.jpg?height=332&width=1328&top_left_y=232&top_left_x=388)

Figure 4: The average verification rate $N_{\text {target }} / N$ and the average discard rate $N_{\text {discarded }} / N$ for the baseline SpecDec under different candidate lengths and SpecDec++ with different acceptance prediction heads and stopping thresholds. SpecDec++ has better Pareto frontiers than SpecDec on both the in-distribution dataset Alpaca and the two out-of-distribution datasets HumanEval and GSM8K.

$N_{\text {discarded }} / N$ and the verification rates $N_{\text {target }} / N$ (Equation (2.3)). The results are plotted in Figure 4 . We see that SpecDec++ has strictly better Pareto frontiers than the baseline SpecDec on both the in-distribution test set Alpaca and the two out-of-distribution datasets HumanEval and GSM8K. Our method with adaptive candidate lengths improves upon the baseline method of fixed candidate lengths by reducing both the discard rate and the verification rate. The two metrics are independent of the actual forward times ( $t_{\text {draft }}$ and $\left.t_{\text {target }}\right)$ and hence reusable for other hardware configurations, which indicates that SpecDec ++ will still outperform the baseline under different sets of $t_{\text {draft }}$ and $t_{\text {target }}$. Finally, we plug in the actual values of $\left(t_{\text {draft }}, t_{\text {target }}\right)=(0.0234,0.112)$ as in Section 4.2. We summarize the throughputs in Table 1 and visualize the improvements in Figure 1.

Table 1: The best throughputs achieved by SpecDec++ compared to the best throughputs achieved by the speculative decoding baseline on Alpaca, HumanEval, and GSM8K datasets.

| Dataset | Alpaca | HumanEval | GSM8K |
| :--- | :--- | :---: | :---: |
| SpecDec (baseline) | 18.88 (tokens/s) | 20.61 (tokens/s) | 20.95 (tokens/s) |
| SpecDec++ | 17.62 (tokens/s) | 18.55 (tokens/s) | 19.14 (tokens/s) |

Discussions. As the distribution shift of the OOD datasets will influence the accuracies and the calibrations of the acceptance prediction heads, a natural question to ask is whether the optimal performances for different datasets are achieved with different acceptance prediction heads and stopping thresholds. Empirically, we confirm that this is indeed the case. Nevertheless, we find that using the acceptance prediction trained with $w_{\text {rej }}=6$ and network depth $D=3$ and the stopping threshold $h=0.7$ achieves over $\mathbf{9 9 . 3 \%}$ of the best tokens per second across the three datasets ( $2.03 \mathrm{x}$ for Alpaca, 2.21x for HumanEval, and 2.26x for GSM8K). Additional ablation studies on how the hyperparameters ( $w_{\text {rej }}, D, h$ ) influence the final tokens per second can be found in Appendix C.3.

## 5 Related Work

Speculative decoding. Since the proposal of speculative decoding, people have been improving the algorithm from different perspectives. Our work is complementary to the works that improve speculative decoding by (1) making the draft model align better with the target model [54, 2, 29], (2) building smaller draft models or merging draft models into the target model (e.g. early-exiting) [30, 28, 49, 5, 51, 31, 12], and (3) building a heirachical system of speculative decoding [34, 37]. Our work is not directly appliable to the methods that do not have the concept of an auto-regressive draft model [35, 27, 6, 7] and the retrieval-based methods [17, 52, 47, 16]. See Appendix B for a detailed discussion on related work about speculative decoding, token trees, and diffusion language models.

Candidate length selection. Leviathan et al. [24] make the i.i.d. assumption on the acceptance probabilities of the candidate tokens and theoretically derive the optimal choice of $K$. Besides, Liu et al. [28] and Kim et al. [20] adopt a simple heuristic that ends the speculation if the confidence of the current draft token distribution falls below a threshold. $\mathrm{Xu}$ et al. [46] uses the cumulative product of the confidences and extends to the token tree version. In comparison, our work systematically studies the candidate length selection within the MDP framework and uses the cumulative product of our trained prediction head to determine the end of the speculation.

## 6 Conclusion

In this work, we study the determination of the candidate lengths for speculative decoding. We formulate the problem as a Markov Decision Process and provide a theorem that gives a sufficient condition to stop the current speculation. Motivated by the theoretical result, we propose SpecDec++ to adaptively select the candidate length with a trained acceptance prediction head. We demonstrate significant speedups over baselines and our method can be seamlessly integrated with other improvements.

## Acknowledgments

We would like to thank Tianle Cai, Kaifeng Lyu, Zhuoming Chen, and Beidi Chen for the helpful feedback on this paper.

## References

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[2] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In The Twelfth International Conference on Learning Representations, 2024.

[3] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

[4] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981-17993, 2021.

[5] Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages $5910-5924,2023$.

[6] Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, and Mahyar Najibi. Speculative streaming: Fast llm inference without auxiliary models. arXiv preprint arXiv:2402.11131, 2024.

[7] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv: $2401.10774,2024$.

[8] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv: $2302.01318,2023$.

[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

[10] Minshuo Chen, Yu Bai, H Vincent Poor, and Mengdi Wang. Efficient rl with impaired observability: Learning to act with delayed and missing state observations. Advances in Neural Information Processing Systems, 36, 2024.

[11] Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. Sequoia: Scalable, robust, and hardware-aware speculative decoding. arXiv preprint arXiv:2402.12374, 2024.

[12] Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, and Kevin ChenChuan Chang. Cascade speculative drafting for even faster llm inference. arXiv preprint arXiv:2312.11462, 2023.

[13] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology . org/N19-1423.

[15] Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang Nie, Zhaopeng Tu, et al. Glide with a cape: A low-hassle method to accelerate speculative decoding. arXiv preprint arXiv:2402.02082, 2024.

[16] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of $11 \mathrm{~m}$ inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024.

[17] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based speculative decoding. arXiv preprint arXiv:2311.08252, 2023.

[18] Benjamin Howson, Ciara Pike-Burke, and Sarah Filippi. Delayed feedback in generalised linear bandits revisited. In International Conference on Artificial Intelligence and Statistics, pages 6095-6119. PMLR, 2023.

[19] Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, and Christopher Lott. Recursive speculative decoding: Accelerating llm inference via sampling without replacement. arXiv preprint arXiv:2402.14160, 2024.

[20] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. Advances in Neural Information Processing Systems, 36, 2024.

[21] Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models. arXiv preprint arXiv:2403.00835, 2024.

[22] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611-626, 2023.

[23] Jonathan Lee, Alekh Agarwal, Christoph Dann, and Tong Zhang. Learning in pomdps is sampleefficient with hindsight observability. In International Conference on Machine Learning, pages 18733-18773. PMLR, 2023.

[24] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 19274-19286. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/ v202/leviathan23a.html.

[25] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328-4343, 2022.

[26] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024.

[27] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024.

[28] Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, and Yunhe Wang. Kangaroo: Lossless self-speculative decoding via double early exiting. arXiv preprint arXiv:2404.18911, 2024.

[29] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding. arXiv preprint arXiv:2310.07177, 2023.

[30] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification, 2023.

[31] Giovanni Monea, Armand Joulin, and Edouard Grave. Pass: Parallel speculative sampling. arXiv preprint arXiv:2311.13581, 2023.

[32] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.

[33] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. Accelerating transformer inference for translation via parallel decoding. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12336-12355, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.689. URL https://aclanthology.org/2023.acl-long.689.

[34] Benjamin Frederick Spector and Christopher Re. Accelerating LLM inference with staged speculative decoding. In Workshop on Efficient Systems for Foundation Models @ ICML2023, 2023. URL https://openreview.net/forum?id=RKHF3VYjLK.

[35] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018.

[36] Qidong Su, Christina Giannoula, and Gennady Pekhimenko. The synergy of speculative decoding and batching in serving large language models. arXiv preprint arXiv:2310.18813, 2023 .

[37] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint arXiv:2404.11912, 2024.

[38] Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. Advances in Neural Information Processing Systems, 36, 2024.

[39] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model https://github.com/tatsu-lab/stanford_alpaca, 2023.

[40] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[44] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long. 754 .

[45] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024.

[46] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and Xuanzhe Liu. Llmcad: Fast and scalable on-device large language model inference. arXiv preprint arXiv:2309.04255, 2023.

[47] Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. Inference with reference: Lossless acceleration of large language models. arXiv preprint arXiv:2304.04487, 2023.

[48] Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. Multi-candidate speculative decoding. arXiv preprint arXiv:2401.06706, 2024.

[49] Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris Papailiopoulos, and Kangwook Lee. Predictive pipelined decoding: A compute-latency trade-off for exact llm decoding. arXiv preprint arXiv:2307.05908, 2023.

[50] Yunchang Yang, Han Zhong, Tianhao Wu, Bin Liu, Liwei Wang, and Simon S Du. A reductionbased framework for sequential decision making with delayed feedback. Advances in Neural Information Processing Systems, 36, 2024.

[51] Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, and Yunfei Cheng. Recurrent drafter for fast speculative decoding in large language models. arXiv preprint arXiv:2403.09919, 2024.

[52] Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun. Ouroboros: Speculative decoding with large model enhanced drafting. arXiv preprint arXiv:2402.13720, 2024.

[53] Shuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, and Ru Huang. Propd: Dynamic token tree pruning and generation for llm parallel decoding. arXiv preprint arXiv:2402.13485, 2024.

[54] Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=rsY6J3ZaTF.
