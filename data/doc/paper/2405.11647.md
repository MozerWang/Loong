# Hummer: Towards Limited Competitive Preference Dataset 

\author{
Li Jiang $^{1^{*}} \quad$ Yusen $\mathrm{Wu}^{2^{*}} \quad$ Junwu Xiong ${ }^{3} \quad$ Jingqing Ruan ${ }^{3} \quad$ Yichuan Ding ${ }^{1}$

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-01.jpg?height=52&width=1059&top_left_y=440&top_left_x=392) <br> ${ }^{1}$ McGill University $\quad{ }^{2}$ Peking University $\quad{ }^{3}$ Ant Group <br> \{jiangli3859, sarinw2023, junwucs\}@gmail.com
}


#### Abstract

Preference datasets are essential for incorporating human preferences into pre-trained language models, playing a key role in the success of Reinforcement Learning from Human Feedback. However, these datasets often demonstrate conflicting alignment objectives, leading to increased vulnerability to jailbreak attacks and challenges in adapting downstream tasks to prioritize specific alignment objectives without negatively impacting others. In this work, we introduce a novel statistical metric, Alignment Dimension Conflict, to quantify the degree of conflict within preference datasets. We then present Hummer and its fine-grained variant, Hummer-F, as innovative pairwise preference datasets with reduced-conflict alignment objectives. Hummer is built based on UltraFeedback and is enhanced by AI feedback from GPT-4, marking as the first preference dataset aimed at reducing the competition between alignment objectives. Furthermore, we develop reward models, HummerRM and HummerRM-F, which employ a hybrid sampling approach to balance diverse alignment objectives effectively. This sampling method positions HummerRM as an ideal model for domain-specific further fine-tuning and reducing vulnerabilities to attacks.


## 1 Introduction

Reinforcement Learning from Human Feedback (RLHF) exhibits great potential in integrating human preferences into large language models (LLMs) (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022; Touvron et al., 2023; Achiam et al., 2023). RLHF also holds great promise in real-world domains, such as robotics (Hu et al., 2023; Tian et al., 2023), healthcare (Yu et al., 2023; He et al., 2023), and autonomous driving (Chen et al., 2023; Du et al., 2023). A fundamental aspect of integrating human alignment objectives lies in the preference modeling stage, which crucially depends on a given preference dataset. This stage can be realized by constructing either explicit (Christiano et al., 2017; Ouyang et al., 2022; Touvron et al., 2023) or implicit reward models (Rafailov et al., 2023; Zhao et al., 2023a).

However, alignment objectives often present competing properties in current preference datasets (Biyik \& Sadigh, 2018; Hong et al., 2022; Ganguli et al., 2022; Wu et al., 2024). Considering the Anthropic-HH dataset (Bai et al., 2022), emphasizing the alignment objective of harmlessness may cause an agent to offer only broad or overly cautious advice. This emphasis could prevent the agent from delivering impactful and precise guidance, which limits the capability of helpfulness. This competition dynamics among alignment objectives poses two significant challenges. On one side, it exacerbates the vulnerability of safety-trained LLMs to jailbreak attacks by crafting prompts to prioritize one alignment dimension over others (Wei et al., 2024). Besides, the conflict dynamics further complicate the attainment of equilibrium among all alignment objectives, particularly customizing models for downstream tasks that require promotion to specific dimensions ability without sacrificing performance in other alignment objectives, such as system simulation (Song et al., 2024), math reasoning (Azerbayev et al., 2023), and code generation (Guo et al., 2024a).[^0]

To mitigate the conflict of alignment dimensions in the preferences dataset, one line of research direction is to separately construct twisted alignment objectives via distinct reward or cost models to explicitly decouple human alignment objectives. Subsequently, these decoupled, learned models are synergized to provide a holistic preference signal tailored to specific alignment goals for downstream tasks. The integration methodology encompasses a variety of strategies, consisting of voting mechanisms (Wang et al., 2024a), differential weighting of models (Touvron et al., 2023; Wu et al., 2024), application of linear combinations (Jang et al., 2023; Rame et al., 2024), and the use of the Lagrangian multiplier (Dai et al., 2023) to manage trade-offs or to highlight specific principles of human values. However, those approaches inadvertently increase model complexity and computational overhead.

In this study, we redirect our focus toward the underlying cause of alignment conflict: the preference dataset itself. RLHF community has witnessed an emerging trend towards developing new preference datasets, driven by goals of enhancing quality and scale, incorporating fine-grained preference signal, and covering specific domains aligned with desired dimensions (Cui et al., 2023; Ji et al., 2024b; Wu et al., 2024; Stiennon et al., 2020; Lightman et al., 2023; Ethayarajh et al., 2022). Despite these efforts, a significant gap persists: the lack of a preference dataset intentionally crafted to alleviate conflicts between alignment dimensions. Such a dataset could potentially provide significant benefits for downstream applications that prioritize certain values (Zhang et al., 2024; Wang et al., 2024b) and reduce vulnerabilities to jailbreak attacks (Perez et al., 2022; Qi et al., 2023; Wei et al., 2024; He et al., 2024; Cui et al., 2024). Moreover, there is currently no established statistical metric for assessing the degree of conflict among alignment dimensions within preference datasets.

In light of these observations, we first introduce Alignment Dimension Conflict (ADC), a statistical metric for quantifying the degree of conflict within preference datasets. This new criterion moves beyond the conventional metric of average performance across multiple objectives or domains typically featured on current leaderboards. We then present Hummer, standing as the first preference dataset to highlight limited competition among various alignment objectives. The construction of Hummer capitalizes on the advanced capabilities of AI feedback mechanisms, such as GPT-4 (Achiam et al., 2023), consisting of a three-stage process: preference \& objective annotation, alignment objectives refination, and dataset split. We use the UltraFeedback (Cui et al., 2023) as our foundation dataset for this work and introduce a fine-grained version of Hummer, termed Hummer-F, which excludes the noisy preference dataset via the principle of reward gaps and compromises $80 \%$ of Hummer.

Based on Hummer and Hummer-F, we introduce a hybrid sampling strategy for training their respective reward models, HummerRM and HummerRM-F, based on the established Llama 2-7B model (Touvron et al., 2023). The hybrid sampling strategy achieves wellbalanced performance across diverse limited-competition alignment objectives in Hummer, enhances resilience to jailbreak attacks, and supports further fine-tuning in downstream tasks. It accomplishes this by prioritizing certain alignment objectives without sacrificing performance in other dimensions. We summarize our contributions in two main folds:

1. We introduce the Alignment Dimension Conflict (ADC), a statistical metric for quantifying conflict in preference datasets. We then present Hummer and its refined variant, Hummer-F, designed as the first preference datasets to mitigate competing alignment objectives.
2. We develop a hybrid sampling strategy to train the reward model HummerRM from Hummer, balancing performance across alignment objectives. HummerRM boosts defense against jailbreak attacks and enables downstream fine-tuning by focusing on key alignment dimensions without compromising others.

## 2 Related Work

RLHF. RLHF has emerged as the leading strategy to integrate human preferences into language models through preference datasets, which can be fixed pre-collected or generated from agents or language models (Cheng et al., 2011; Akrour et al., 2011; Askell et al., 2021). To integrate human values, RLHF generally obtains the final aligned policy through RL
algorithms, such as PPO (Schulman et al., 2017), to maximize the reward through the trained reward model on preference datasets (Ramamurthy et al., 2022; Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). Another important branch is to directly anchor the human preferences to the final policy by constructing the implicit reward with policies through the closed-form optimal solution for the reward model (Rafailov et al., 2023; Zhao et al., 2023b; Azar et al., 2023; Wang et al., 2023a; Ethayarajh et al., 2024; Zhou et al., 2023; Amini et al., 2024; Liu et al., 2023; Swamy et al., 2024). While these approaches are appealing for their computation cost and ease of implementation, their inherited offline paradigm suffers from the distributional shift and lack of online exploration (Guo et al., 2024c; Calandriello et al., 2024). We elaborate on more related work for the human alignment in Appendix A, consisting of rank-based, conditional, and weak-to-strong supervision approaches.

Preference Datasets. The RLHF community is observing a growing trend of new preference datasets from diverse perspectives to improve preference modeling. The dominant motivations for the introduction of new preference datasets are scalability, quality, and diversity (Guo et al., 2023; Cui et al., 2023; Wu et al., 2024). For example, SPA dataset (Guo et al., 2023) presents fine-grained (i.e., token or phrase level) feedback during optimization rather than holistic feedback during the training process. UltraFeedback (Cui et al., 2023) introduces a wide-source and high-quality preference dataset with four alignment dimensions, in contrast to two dimensions (helpfulness and harmlessness) (Ouyang et al., 2022). Besides, some recent preference datasets underscore a specific domain or alignment property (Stiennon et al., 2020; Lightman et al., 2023; Ethayarajh et al., 2022). However, existing preference datasets fail to mitigate the conflict between alignment dimensions. Enhancing the synergy of alignment dimensions improves resilience against jailbreak attacks and allows for further fine-tuning in downstream applications. This is achieved by prioritizing specific alignment objectives without compromising performance across other dimensions.

Red Teaming LLMs with Further Fine-tuning. Red teaming is designed to execute systematic tests and attacks on LLMs to expose their potential harmfulness and safety vulnerabilities (Perez et al., 2022; Achiam et al., 2023; Shi et al., 2024). Recent work (Qi et al., 2023; Zhan et al., 2023; He et al., 2024) identifies that customizing models with further fine-tuning on downstream tasks, even without harmful content, will lead to a degradation in resilience against jailbreak attacks. We hypothesize that this phenomenon is caused by the implicit emphasis on specific alignment dimensions (such as helpfulness) in downstream datasets, rooted in conflicts among these alignment dimensions. In this work, we focus on the conflict of alignment dimensions and study further fine-tuning specific alignment dimensions of preference datasets, with the expectation to improve one alignment ability demanded for customization tasks. Aligned with these findings, we show that further fine-tuning downstream models on desired alignment dimensions inevitably leads to performance degradation in conflicting dimensions (Section 6).

## 3 Preliminaries

RLHF typically starts with a generic pre-trained language model from supervised finetuning on a high-quality dataset for general proposes, such as conservation, noted as $\pi^{S F T}$, and then matches human preferences through a preference dataset. In this work, we mainly study the problem of competing alignment objectives in existing preference datasets.

Preference Modelling. One of the core ingredients of RLHF is to integrate human preferences into LLMs through preference datasets, formulated as $\mathcal{D}^{P}=\left\{x^{i}, y_{w}^{i}, y_{l}^{i}\right\}_{i=1}^{N}$, where $N$ is the number of collected samples. The preference dataset $\mathcal{D}^{P}$ incorporates the human feedback through the preference to these two responses, i.e., $y_{w} \succ y_{l} \mid x$. Given the prompt $x, y_{w}$ denotes the preferred response by humans or advanced AI models compared to $y_{l}$. Given the preference dataset $\mathcal{D}^{P}$, we can then parameterize a reward model $r_{\phi}(x, y)$ and optimize it through through the Bradley-Terry (BT) model (Bradley \& Terry, 1952):

$$
\begin{equation*}
\max _{r_{\phi}} \mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}^{p}}\left[\log \sigma\left(r_{\phi}\left(x, y_{w}\right)-r_{\phi}\left(x, y_{l}\right)\right)\right], \tag{1}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-04.jpg?height=447&width=1357&top_left_y=264&top_left_x=384)

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-04.jpg?height=355&width=439&top_left_y=289&top_left_x=388)

(a) Normal distribution ADC

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-04.jpg?height=360&width=450&top_left_y=281&top_left_x=835)

(b) Low ADC

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-04.jpg?height=369&width=450&top_left_y=279&top_left_x=1287)

(c) High ADC

Figure 2: (a) Normal distribution of ADC with varying standard variance $\sigma$ : $\mathbb{E}_{x \sim \mathcal{N}\left(0, \sigma^{2}\right)} \mathrm{U}[x]$. (b-c) The performance deviation with further fine-tuning on the first dimension of preference datasets with (b) low and (c) high ADC. Intuitively, a high ADC indicates a strong conflict between the alignment dimensions of a given preference dataset.

where $\sigma$ is the logistic function. In the context of LLMs, the reward model $r_{\phi}(x, y)$, is frequently constructed based on $\pi^{S F T}(y \mid x)$ by adding a linear layer on top of the final transformer layer, which yields a single scalar prediction representing the reward value (Ziegler et al., 2019; Ouyang et al., 2022).

Policy Modelling. Given the learned reward model $r_{\phi}(x, y)$ constructed through preference modeling and policy training dataset $\mathcal{D}^{R L}=\left\{x^{(i)}\right\}_{i=1}^{N}$, we can formulate the following optimization objective for $\pi_{\theta}$ to inherit the preference from $r_{\phi}(x, y)$ :

$$
\max _{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}^{R L}, y \sim \pi_{\theta}(y \mid x)}\left[r_{\phi}(x, y)\right]-\beta \mathbb{D}\left[\pi_{\theta}(y \mid x) \| \pi^{S F T}(y \mid x)\right]
$$

where $\beta$ moderates the divergence $\mathbb{D}$, such as $\mathrm{KL}$ divergence (Kullback, 1951), between $\pi_{\theta}$ and a soft-target policy $\pi^{S F T}$. This regularization ensures that $\pi_{\theta}$ avoids collapsing into a narrow set of high-reward actions, preserving a diverse and functional output distribution as supported by the learned reward model (Jaques et al., 2019; Song et al., 2023b; Laidlaw et al., 2024; Wei et al., 2024). PPO is employed for policy optimization due to its proven efficiency and stability in training.

## 4 Hummer

This section begins with a formal definition of Alignment Dimension Conflict (ADC), a metric to assess the extent of conflicting alignment objectives in preference datasets (Section 4). We then introduce Hummer and its fine-grained variant, Hummer- $F$, which are datasets specifically crafted to emphasize the property of limited-conflict among alignment objectives of preference datasets (Section 4.2).

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-04.jpg?height=333&width=635&top_left_y=1774&top_left_x=1081)

Figure 1: The ADC estimation pipeline, measuring the negative performance gap between initial and further fine-tuned reward models.

### 4.1 Alignment Dimension Conflict

Formally, given a reward model, i.e., RM, that has been initially fine-tuned on the whole preference dataset $\mathcal{D}_{n}^{P}=\left\{d_{1}, d_{2}, \cdots, d_{n}\right\}$, its performance on the corresponding test dataset from $\mathcal{D}_{n}^{P}$ is represented by $\mathbb{U}=\left\{u_{1}, u_{2}, \cdots, u_{n}\right\}$. To study this conflict, we copy $n$ reward models further fine-tune the reward model on the interest of any alignment dimensions dataset, e.g., $d_{i} \in \mathcal{D}_{n}^{P}$, and obtain the further fine-tuned performance $\overline{\mathbb{U}}_{i}=\left\{\bar{u}_{1}, \bar{u}_{2}, \cdots, \bar{u}_{n}\right\}$. The performance deviation can be obtained by $\bar{U}_{i}-\mathbb{U}$ of $\mathrm{RM}_{i}$, where $i$ highlights further
fine-tuning conducted only on $d_{i}$. We present the pipeline to measure this dimension conflict in Fig. 1 and present a new statistical metric:

Definition 1 (Alignment Dimension Conflict). The Alignment Dimension Conflict (ADC) is the second-order moment of negative performance deviation summation on all dimensions except $d_{i}$ :

$$
\begin{equation*}
U\left[\mathcal{D}_{n}^{P}\right] \doteq \mathbb{E}\left[\frac{\sum_{k \neq i}^{n}\left(\bar{u}_{k}-u_{k}\right)_{-}^{2}}{(n-1)}\right] \quad \text { with } \quad u_{-}=\min \{u, 0\} \tag{2}
\end{equation*}
$$

where $n-1$ serves as a normalization term to facilitate fair comparison for different datasets with different alignment dimensions. An interesting question to ask is: What situation leads to high $A D C$ ? We simplify the performance deviation $(\bar{U}-\mathbb{U})$ sampling from a normal distribution $\mathcal{N}\left(0, \sigma^{2}\right)^{1}$. The expression $\mathbb{E}_{x \sim \mathcal{N}\left(\mu=0, \sigma^{2}\right)} \mathrm{U}[x]$ in Fig. 2a represents the Average Deviation Coefficient (ADC) of a normal distribution with respect to its variance parameter $\sigma$. This measures how much adjusting one alignment dimension affects others with further fine-tuning. We observe a strongly positive correlation between ADC and $\sigma$, indicating that datasets with a higher level of competing dimensions (evidenced by greater variance on the negative side) tend to exhibit higher ADC values. The performance deviation across datasets with varying ADC levels is illustrated in Fig. 2, where datasets with low ADC are characterized by a minimal negative impact on the performance across other alignment dimensions, i.e., lower level of competition.

RewardBench (Lambert et al., 2024) offers toolkits for structured comparison across various properties in reward models, accommodating diverse model structures or preference datasets. To facilitate a systematic comparison of alignment dimension conflict levels among different datasets, we can scale the Alignment Dimension Conflict (ADC) metric to the evaluated properties on standard evaluation toolkits, termed ADC-B, which holds the same structure to Definition 1 to ADC. Formally, the performance of a given reward model after fine-tuning on its preference dataset $\mathcal{D}_{n}^{p}$ is noted as $\mathbb{V}=\left\{v_{1}, v_{2}, \cdots, v_{m}\right\}$, where $m$ represents the total dimensions of properties for evaluation. With further fine-fune of the reward model on one specific dimension $d_{i} \in \mathcal{D}_{n}^{p}$, new evaluated performance and benchmark performance deviation is defined as $\overline{\mathbb{V}}_{i}=\left\{\bar{v}_{1}, \bar{v}_{2}, \cdots, \bar{v}_{m}\right\}$ and $\overline{\mathbb{V}}_{i}-\mathbb{V}$, respectively. We then can evaluate the ADC of datasets with a structured comparison on standard benchmarks:

Definition 2 (Alignment Dimension Conflict Benchmark). The Alignment Dimension Conflict (ADC) extended to standard benchmark evaluation is the second-order moment of negative performance deviation on all evaluation dimensions in the benchmark:

$$
\begin{equation*}
V\left[\mathcal{D}_{n}^{P}\right] \doteq \mathbb{E}\left[\frac{\sum_{j=1}^{m}\left(\bar{v}_{j}-v_{j}\right)_{-}^{2}}{m}\right] \quad \text { with } \quad v_{-}=\min \{v, 0\} \tag{3}
\end{equation*}
$$

### 4.2 Dataset Construction for Hummer

To decouple alignment dimensions, we introduce Hummer, the first preference dataset that aims to alleviate the competing dynamics of preference datasets. To accurately capture the multidimensionality of human preference without interference between alignment dimensions, we leverage the powerful ability of AI feedback, i.e., GPT-4, which has been heavily employed in preference dataset construction or preference modeling (Lee et al., 2023; Cui et al., 2023; Guo et al., 2023; Burns et al., 2023; Chen et al., 2024; Ji et al., 2024a). We leverage UltraFeedback (Cui et al., 2023) as the foundational dataset, attributed to its expansive scale and diversity.

We show the construction process of Hummer in Fig. 3, detailed in Appendix B. The process of identifying the limited-conflict dimension and its corresponding pairwise dataset involves three key stages: : (a) Preference annotation: Initially, we randomly select $g=400$ pairwise preference datasets $\left(x, y_{1}, y_{2}\right)^{k}$ from the foundational dataset. For each pair, we[^1]

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-06.jpg?height=524&width=1097&top_left_y=253&top_left_x=514)

Figure 3: Hummer construction process. We leverage the advanced ability of GPT-4 to build Hummer, a preference dataset with low competitive alignment objectives.

annotate preferences, alignment dimensions, and the corresponding reasons $(p, d \text {, reason })^{k}$, powered by GPT-4. (b) Alignment objective refination: With the $k$ annotated alignment dimensions for the sampled preference dataset, we leverage GPT-4 to refine these dimensions to minimize their conflicts and narrow them down to $n(n<k)$ alignment dimensions, with a little abuse of notation with $d$. (c) Dataset split: GPT-4 is then used to assign an absolute reward to $n$ alignment dimensions. We categorize every pair of datasets to its corresponding dimension on the principle of maximal preference gap. We highlight that this splitting approach is more favorable than directly ranking as it avoids the position bias (Zhu et al., 2023) and facilitates convenience to build Hummer-F. Hummer-F is refined by applying a reward gap threshold $(\tau)$ to filter out potentially noisy preference pairs, a subset that comprises approximately $80 \%$ of Hummer.

## 5 Hybrid Reward Sampling

In this section, we introduce HummerRM and its variant, HummerRM-F. Both are singlereward models trained on our custom-limited competitive preference datasets, Hummer and Hummer-F, respectively. These models employ a hybrid sampling method, enabling HummerRM to autonomously adjust its sampling ratio from preference datasets aligned with various objectives with the performance signal.

Formally, considering a preference dataset with $n$ alignment objectives, denoted as $\mathcal{D}_{n}^{P}=$ $\left\{d_{1}, d_{2}, \ldots, d_{n}\right\}$, we assign an initial equal sampling weight to each dimension dataset, represented by $\Lambda=\left\{\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}\right\}$, where $\lambda_{i}=1 / n$ with $i \in[1, n]$. We achieve the balance among all alignment dimensions by evaluating the preference performance across these dimensions, denoted as $\mathbb{U}=\left\{u_{1}, \ldots, u_{n}\right\}$. The sampling weights are adaptively updated in every 1 epoch (128 steps) as follows:

$$
\begin{equation*}
\lambda_{i} \leftarrow \lambda_{i}+\eta\left(\bar{u}-u_{i}\right) \tag{4}
\end{equation*}
$$

where $\bar{u}$ represents the average preference performance across all alignment objectives, and $\eta$ is the temperature for updating the sampling weights $\Lambda$. To ensure adherence to the sum constraint, $\sum_{j=1}^{n} \lambda_{j}=1$, we normalize the $\lambda_{i}$ values accordingly after every update. Consequently, the mini dataset sampled at each training step is represented by $\lfloor$ BatchSize $\times \Lambda\rfloor$ from $D_{n}^{p}$, where BatchSize $=128$ and $\lfloor x\rfloor$ represents the floor function.

Intuitively, if the performance of a specific dimension, e.g., $u_{i}$, is higher than the average ( $\left.u_{i}>\bar{u}\right)$, the corresponding sampling ratio $\lambda_{i}$ for dataset $d_{i}$ decreases. Conversely, if $u_{i}<\bar{u}$, indicating a performance lower than the average, $\lambda_{i}$ increases, promoting an increasing sampling dataset for $d_{i}$. We then integrate all sampled datasets into one training batch and update the reward model via Eqn. (1). The hybrid sampling strategy enhances the robust performance of HummerRM across all alignment dimensions.

Table 1: Comparison of existing preference datasets. We demonstrate that all existing preference datasets exhibit a significantly higher ADC (\%) (8-10x) compared to Hummer and Hummer-F. The best performance on RewardBench (Lambert et al., 2024) is in bold.

| Dataset | Reward <br> Model | Alignment <br> Objectives | Dataset <br> Size | ADC $(\downarrow)$ | ADC-B $(\downarrow)$ | Reward <br> Bench $(\uparrow)$ |
| :---: | :---: | :---: | ---: | ---: | ---: | ---: |
| Anthropic HH | AnthropicRM | 2 | $170 \mathrm{k}$ | 85.04 | 204.6 | 56.72 |
| UltraFeedback | UltraRM | 4 | $64 \mathrm{k}$ | 67.23 | 126.3 | 68.34 |
| Hummer (ours) | HummerRM | 6 | $46 \mathrm{k}$ | 8.99 | 25.8 | 71.52 |
| Hummer-F (ours) | HummerRM-F | 6 | $37 \mathrm{k}$ | 9.62 | 28.5 | 72.13 |

## 6 Experiments

Our testbed is designed to assess the low-conflict alignment dimensions within our introduced datasets, namely Hummer and Hummer-F. We initiate our evaluation by examining the Alignment Dimension Conflict (ADC) and ADC-B using HummerRM, alongside a standard reward benchmark, as detailed in Section 6.1. Subsequently, we explore the vulnerabilities of HummerRM to jailbreak attacks, shown in Section 6.2. Finally, we assess the efficacy of the hybrid sampling strategy in comparison to diverse sampling methods in Section 6.3.

### 6.1 Reward Model Evaluation

Setup. To elucidate the dynamics of low competition in Hummer and Hummer-F, we assess the ADC within their respective preference datasets. This evaluation is contextualized by comparisons with the Anthropic HH dataset (Bai et al., 2022), and UltraFeedback (Cui et al., 2023). To systematically analyze the degree of competition among alignment dimensions, we extend our evaluation to include ADC-B and assess performance on RewardBench (Lambert et al., 2024). RewardBench represents a comprehensive benchmark covering chat, reasoning, and safety domains, providing a pairwise testbed for evaluating reward models. Furthermore, we explore the effectiveness of hybrid sampling strategies in the training of reward models. For consistency across evaluations, we employ a consistent backbone model, specifically a fine-tuned Llama 2-7B (Touvron et al., 2023), to train the reward models for each dataset.

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-07.jpg?height=385&width=1331&top_left_y=1678&top_left_x=381)

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-07.jpg?height=304&width=415&top_left_y=1694&top_left_x=389)

(a) Anthropic $\mathrm{HH}$

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-07.jpg?height=311&width=409&top_left_y=1693&top_left_x=836)

(b) UltraFeedback

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-07.jpg?height=301&width=401&top_left_y=1695&top_left_x=1293)

(c) Hummer

Figure 4: The performance deviation with further fine-tuning on different alignment objectives, where the green bar indicates the further fine-tuning dimensions. Notably, Hummer demonstrates minimal competition among alignment dimensions.

Result. In Table 1, we summarize prevalent preference datasets with our statistical evaluation findings. Notably, Hummer and Hummer-F demonstrate a significantly reduced ADC (8-10x) compared to other preference datasets. This diminished ADC in Hummer suggests the limited alignment conflict, in stark contrast to the pronounced alignment conflicts observed within the dimensions of other datasets. However, ADC lacks a structured framework for comparison, as each preference dataset is assessed based on its unique alignment dimension. To enable a structured comparison, we apply the ADC-B evaluation to the RewardBench

Table 2: Jailbreak rate $(\%, \downarrow)$ for different reward models with further fine-tuning on specific alignment dimensions. While Other reward models show highly fluctuating attack ratios, HummerRM demonstrates remarkable consistency with low fluctuation.

| Dataset | Reward <br> model | Initial <br> fine-tuning | $\# 1$ | $\# 2$ | $\# 3$ | $\# 4$ | $\# 5$ | \# |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | AnthropicRM | 46.2 | $52.4_{(+6.2)}$ | $23.7_{(-22.5)}$ |  | - | - |  |
| Anthropic HH | Anther fine-tuning |  |  |  |  |  |  |  |
| UltraFeedback | UltraRM | 46.6 | $51.4_{(+4.8)}$ | $57.0_{(+10.4)}$ | $48.9_{(+2.3)}$ | $51.2_{(+4.6)}$ | - | - |
| Hummer | HmmerRM | 46.4 | $49.6_{(+3.2)}$ | $44.0_{(-2.4)}$ | $46.7_{(+0.3)}$ | $28.8_{(-17.6)}$ | $44.2_{(-2.2)}$ | $41.5_{(-4.9)}$ |
| Hummer-F | HmmerRM-F | 46.3 | $49.3_{(+3.0)}$ | $43.8_{(-2.5)}$ | $46.8_{(+0.5)}$ | $28.6_{(-17.7)}$ | $43.7_{(-2.6)}$ | $41.4_{(-4.9)}$ |

framework, uncovering a notable consistency with the ADC findings. Our analysis further reveals a negative correlation between the ADC and the number of alignment objectives, suggesting that incorporating more fine-grained alignment dimensions may potentially mitigate conflict to a certain degree. The detailed performance deviation is shown in Fig. 4.

Despite the preference dataset for Hummer and Hummer-F being considerably smaller (3$4 x$ ) than UltraFeedback, we observe an enhanced performance from HummerRM and HummerRM-F over UltraRM, by margins of $3.21 \%$ and $4.19 \%$, respectively. This underscores the significance of dataset quality in preference datasets. Despite those margins, we highlight that this study shifts focus from the conventional leaderboard-centric approach, which primarily aims at "achieving a higher score," to identify and measure the competing dynamics inherent in preference datasets.

### 6.2 Jailbreak Attacks Evaluation

Setup. We posit that the HummerRM framework can mitigate vulnerabilities to jailbreak attacks by enhancing one dimension without degrading performance across other metrics. Our jailbreak evaluation framework follows the methodology outlined by Siththaranjan et al. (2023). Specifically, the jailbreak-based dataset comprises pair-wise tuples $\left(x, y_{1}, y_{2}\right)$, where $x$ represents prompts designed to elicit a harmful response from the model (termed 'jailbreak prompts'), $y_{1}$ denotes the safe response, and $y_{2}$ is jailbreak response (Wei et al., 2024). Given a learned reward model, if the reward of $\left(x, y_{2}\right)$ is higher than $\left(x, y_{1}\right)$, we then expect a failure in maintaining policy modeling safety, leading to jailbroken responses. We quantify this through the 'jailbreak rate', calculated as the proportion of instances where the reward model favors $\left(x, y_{2}\right)$ over $\left(x, y_{1}\right)$, represented by $\mathbb{I}\left(r\left(x, y_{2}\right)>r\left(x, y_{1}\right)\right) / n$, where $\mathbb{I}$ is the indicator function and $n$ denotes the total prompts. The higher the jailbreak rate, the greater the vulnerability of models to attacks.

The datasets and the backbone model used for the reward training and jailbreak evaluation adhere to the experimental settings detailed in Section 6.1. Our test reward models consist of the initial fine-tuning models on the whole preference datasets and further fine-tuning models on specific alignment dimensions. Further fine-tuning of reward models is crucial for downstream tasks that require emphasis on specific alignment dimensions.

Result In Table 2, we present the results of jailbreak attacks, featuring Anthropic HH (Ouyang et al., 2022), UltraFeedback (Cui et al., 2023), and Hummer, with each model encompassing 2, 4, and 6 alignment objectives, respectively. Initial fine-tuning reveals a consistent jailbreak rate across all datasets. Notably, UltraRM exhibits the highest attack rate at 57.0 following further fine-tuning on the instruction-following alignment dimension (Dim2). This indicates a significant increase in vulnerability to jailbreak attacks when UltraRM is specifically fine-tuned to emphasize instruction-following, highlighting a pronounced tension among its alignment objectives. Conversely, HummerRM shows remarkable robustness, with a jailbreak rate increase of less than $3.5 \%$ after further fine-tuning across dimensions. This suggests that the alignment objectives of Hummer alignment dimensions are low in conflict and that its safety is not compromised by further fine-tuning.

We emphasize that a declining jailbreak rate signifies enhanced defensive capabilities against jailbreak attacks. This improvement is particularly notable when further fine-tuning focuses
on specific alignment dimensions, such as harmlessness (Dim2) in the case of Anthropic $\mathrm{HH}$, and empathy (Dim4) for Hummer. The detailed alignment dimensions for preference datasets are shown in Appendix Table 6.

### 6.3 Analysis

Hybrid sampling strategy maintains performance on imbalanced datasets. An imbalanced dataset arises with a non-uniform distribution of classes, often characterized by a disproportionate number of instances between major and minor classes, resulting in biased predictions (Krawczyk, 2016; Jiang et al., 2023). To investigate the efficacy of a hybrid sampling strategy in addressing dataset imbalance in the context of alignment objectives, we integrate our datasets across six alignment dimensions with a distribution ratio of $10: 10: 10: 10: 1: 1$, where the $1: 1$ ratio pertains specifically to specificity and tone. The results are illustrated in Fig. 5.

Fine-tuning on specific dimensions will boost the performance on its corresponding alignment dimensions but fail to achieve desirable performance on other alignment dimensions, such as Single \# 1 (Accuracy), and Single \# 5 (Tone). We demonstrate that the All Dimensions Equal strategy, with a uniform distribution ratio of 1:1:1:1:1:1, underperforms relative to our hybrid sampling approach across all dimensions, achieving only $70 \%$ to $95 \%$ of the performance of the Hybrid sampler. This implies that this uniform sampling strategy, also employed by Cui et al. (2023), may fall short in imbalanced datasets. The All Mixed strategy, integrating all alignment datasets ignoring the data balance, exhibits significantly superior performance in well-represented alignment datasets \# 1 and \# 2 (Depth and Accuracy), yet fails in alignment objectives with limited datasets: \# 5 and \# 6 (Tone and Specificity). Such an approach could further diminish the performance of lesser-represented alignment objectives, particularly in scenarios

![](https://cdn.mathpix.com/cropped/2024_06_04_30a16bbd1eae2255e69cg-09.jpg?height=732&width=616&top_left_y=867&top_left_x=1123)

Figure 5: Performance with different sampling strategy on imbalanced datasets. involving competing alignment objectives.

## 7 Conclusion

In this study, we delve into the dynamics of competing preferences within the Reinforcement Learning from Human Feedback (RLHF) framework. We introduce a novel statistical metric termed Alignment Dimension Conflict (ADC) to quantify the extent of conflict among alignment objectives within preference datasets. We unveil the first preference dataset, Hummer, alongside its fine-grained variant, Hummer-F. These datasets are designed to mitigate dimension conflicts, facilitating domain-specific fine-tuning while increasing resilience against jailbreak attacks. This is achieved by selectively prioritizing certain alignment objectives without compromising performance across other alignment objectives. Subsequently, we develop reward models for our datasets, namely HummerRM and HummerRM-F, employing a hybrid sampling technique that dynamically adjusts the sampling weight based on reward performance across different alignment dimensions. Looking ahead, an intriguing avenue for future research lies in constructing low-conflict alignment objectives using unsupervised or self-supervised (Zhang et al., 2020; Yan et al., 2021) learning methods to discern semantic nuances. Furthermore, enriching preference datasets with contributions from other opensource, large-scale preference datasets and leveraging advanced AI feedback presents a promising direction for enhancing dataset diversity and depth.

## References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Riad Akrour, Marc Schoenauer, and Michele Sebag. Preference-based policy learning. In Proc. of $K D D, 2011$.

Afra Amini, Tim Vieira, and Ryan Cotterell. Direct preference optimization with an offset. arXiv preprint arXiv:2402.10571, 2024.

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.

Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036, 2023.

Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

Erdem Biyik and Dorsa Sadigh. Batch active preference-based learning of reward functions. In Conference on robot learning, 2018.

Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 1952.

Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-tostrong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.

Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, et al. Human alignment of large language models through online preference optimisation. arXiv preprint arXiv:2403.08635, 2024.

Hong Chen, Kang Yuan, Yanjun Huang, Lulu Guo, Yulei Wang, and Jie Chen. Feedback is all you need: from chatgpt to autonomous driving. Science China Information Sciences, 2023.

Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play finetuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024.

Weiwei Cheng, Johannes Fürnkranz, Eyke Hüllermeier, and Sang-Hyeun Park. Preferencebased policy iteration: Leveraging preference learning for reinforcement learning. In Proc. of $K D D, 2011$.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Proc. of NeurIPS, 2017.

Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with highquality feedback. arXiv preprint arXiv:2310.01377, 2023.

Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng, Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, Zhixing Tan, Junwu Xiong, et al. Risk taxonomy, mitigation, and assessment benchmarks of large language model systems. arXiv preprint arXiv:2401.05778, 2024.

Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023.

Haiping Du, Siyu Teng, Hong Chen, Jiaqi Ma, Xiao Wang, Chao Gou, Bai Li, Siji Ma, Qinghai Miao, Xiaoxiang Na, et al. Chat with chatgpt on intelligent vehicles: An ieee tiv perspective. IEEE Transactions on Intelligent Vehicles, 2023.

Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with $\mathcal{V}$-usable information. In Proc. of ICML, 2022.

Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024

Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming-the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024a.

Geyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong Wen. Beyond imitation: Leveraging fine-grained quality signals for alignment. arXiv preprint arXiv:2311.04072, 2023.

Jianyuan Guo, Hanting Chen, Chengcheng Wang, Kai Han, Chang Xu, and Yunhe Wang. Vision superalignment: Weak-to-strong generalization for vision foundation models. arXiv preprint arXiv:2402.03749, 2024b.

Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024c.

Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics. arXiv preprint arXiv:2310.05694, 2023.

Luxi He, Mengzhou Xia, and Peter Henderson. What's in your" safe" data?: Identifying benign data that breaks safety. arXiv preprint arXiv:2404.01099, 2024.

Joey Hong, Kush Bhatia, and Anca Dragan. On the sensitivity of reward inference to misspecified human models. arXiv preprint arXiv:2212.04717, 2022.

Xiao Hu, Jianxiong Li, Xianyuan Zhan, Qing-Shan Jia, and Ya-Qin Zhang. Query-policy misalignment in preference-based reinforcement learning. arXiv preprint arXiv:2305.17400, 2023.

Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. arXiv preprint arXiv:2310.11564, 2023.

Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.

Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852, 2023.

Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. Aligner: Achieving efficient alignment through weak-to-strong correction. arXiv preprint arXiv:2402.02416, 2024a.

Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of 11 Im via a human-preference dataset. Proc. of NeurIPS, 2024b.

Li Jiang, Sijie Chen, Jielin Qiu, Haoran Xu, Wai Kin Chan, and Zhao Ding. Offline reinforcement learning with imbalanced datasets. arXiv preprint arXiv:2307.02752, 2023.

Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R Bowman, Tim Rocktäschel, and Ethan Perez. Debating with more persuasive llms leads to more truthful answers. arXiv preprint arXiv:2402.06782, 2024.

Bartosz Krawczyk. Learning from imbalanced data: open challenges and future directions. Progress in Artificial Intelligence, 2016.

Solomon Kullback. Kullback-leibler divergence, 1951.

Cassidy Laidlaw, Shivam Singhal, and Anca Dragan. Preventing reward hacking with occupancy measure regularization. arXiv preprint arXiv:2403.03185, 2024.

Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling, 2024.

Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.

Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023.

Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, et al. Lipo: Listwise preference optimization through learning-to-rank. arXiv preprint arXiv:2402.01878, 2024.

Yuejiang Liu and Alexandre Alahi. Co-supervised learning: Improving weak-to-strong generalization with hierarchical mixture of experts. arXiv preprint arXiv:2402.15505, 2024.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback, 2022. URL https://arxiv. org/abs/2203.02155, 2022.

Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, and Sathwik Tejaswi Madhusudhan. Curry-dpo: Enhancing alignment using curriculum learning \& ranked preferences. arXiv preprint arXiv:2403.07230, 2024.

Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022.

Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.

Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241, 2022.

Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Proc. of NeurIPS, 2024.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang, and Cho-Jui Hsieh. Red teaming language model detectors with language models. Transactions of the Association for Computational Linguistics, 12:174-189, 2024.

Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. Distributional preference learning: Understanding and accounting for hidden context in rlhf. arXiv preprint arXiv:2312.08358, 2023.

Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023a.

Zezheng Song, Jiaxin Yuan, and Haizhao Yang. Fmint: Bridging human designed and data pretrained models for differential equation foundation model. arXiv preprint arXiv:2404.14688, 2024.

Ziang Song, Tianle Cai, Jason D Lee, and Weijie J Su. Reward collapse in aligning large language models. arXiv preprint arXiv:2305.17608, 2023b.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Proc. of NeurIPS, 2020.

Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024.

Ran Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, and Andrea Bajcsy. What matters to you? towards visual representation alignment for robot learning. arXiv preprint arXiv:2310.07932, 2023.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024a.

Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints. arXiv preprint arXiv:2309.16240, 2023a.

Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235, 2023b.

Haixin Wang, Xinlong Yang, Jianlong Chang, Dian Jin, Jinan Sun, Shikun Zhang, Xiao Luo, and Qi Tian. Parameter-efficient tuning of large-scale multimodal foundation model. Advances in Neural Information Processing Systems, 36, 2024b.

Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? Proc. of NeurIPS, 2024.

Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. Proc. of NeurIPS, 2024.

Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Weiran Xu. Consert: A contrastive framework for self-supervised sentence representation transfer. arXiv preprint arXiv:2105.11741, 2021.

Ping Yu, Hua $\mathrm{Xu}, \mathrm{Xia} \mathrm{Hu}$, and Chao Deng. Leveraging generative ai and large language models: a comprehensive roadmap for healthcare integration. In Healthcare, 2023.

Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023.

Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang. Removing rlhf protections in gpt-4 via fine-tuning. arXiv preprint arXiv:2311.05553, 2023.

Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131, 2024.

Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, and Lidong Bing. An unsupervised sentence embedding method by mutual information maximization. arXiv preprint arXiv:2009.12061, 2020.

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slichf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023a.

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slichf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023b.

Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. Beyond one-preference-for-all: Multi-objective direct preference optimization. arXiv preprint arXiv:2310.03708, 2023.

Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness \& harmlessness with rlaif, 2023.

Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.
