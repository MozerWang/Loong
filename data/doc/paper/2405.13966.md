# On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models 

Mudit Verma*<br>Siddhant Bhambri*<br>Subbarao Kambhampati<br>School of Computing and AI<br>Arizona State University<br>\{muditverma, sbhambr1, rao\}@asu.edu


#### Abstract

The reasoning abilities of Large Language Models (LLMs) remain a topic of debate. Some methods such as ReAct-based prompting, have gained popularity for claiming to enhance sequential decision-making abilities of agentic LLMs. However, it is unclear what is the source of improvement in LLM reasoning with ReAct based prompting. In this paper we examine these claims of ReAct based prompting in improving agentic LLMs for sequential decision-making. By introducing systematic variations to the input prompt we perform a sensitivity analysis along the claims of ReAct and find that the performance is minimally influenced by the "interleaving reasoning trace with action execution" or the content of the generated reasoning traces in ReAct, contrary to original claims and common usage. Instead, the performance of LLMs is driven by the similarity between input example tasks and queries, implicitly forcing the prompt designer to provide instance-specific examples which significantly increases the cognitive burden on the human. Our investigation shows that the perceived reasoning abilities of LLMs stem from the exemplar-query similarity and approximate retrieval rather than any inherent reasoning abilities.


## 1 Introduction

Large Language Models (LLMs) have seen rapid advancements specifically in Natural Language Processing and Understanding (NLP \& NLU). LLMs have unparalleled capabilities in text generation, summarization, translation, question answering to name a few. [Bubeck et al. 2023]. Motivated by these capabilities of LLMs, there has also been a rush to look for other emergent abilities-especially for reasoning and planning. A popular way of enhancing LLM performance on reasoning/planning tasks has been in-context prompting or prompt-engineering [Sahoo et al., 2024] to include instructions [Giray, 2023], syntax structure [Marvin et al., 2023], criticism and plan guidance with verification [Kambhampati et al. 2024] etc. Among these approaches, ReAct [Yao et al., 2022] stands out which claims to improve LLM planning abilities through the use of reasoning traces interleaved with action execution given as plan guidance. While their original study includes multiple domains tasks such as question answering on HotPotQA and FEVER datasets [Yang et al. 2018, Thorne et al., 2018], in this paper we are particularly interested in planning tasks where ReAct claims superior performance.

In our initial experiments with ReAct for planning, we found that the system is overly dependent on the syntactic similarity of the example prompt and the query and is extremely brittle to minor perturbations to the input prompt. For example, when provided with an explicit set of examples of pick-and-place-object task and asked to plan for a pick-and-place-two-objects task, it should be trivial to generalize the solution of the examples to the queried task. Unfortunately, even such a minor variation to the original ReAct agent setup disrupts its performance.

![](https://cdn.mathpix.com/cropped/2024_06_04_01c4609402780a6a40e3g-02.jpg?height=556&width=1374&top_left_y=240&top_left_x=365)

Figure 1: An example of ReAct in AlfWorld. We highlight the main components of ReAct, i.e., Interleaved reasoning and acting, the reasoning trace / plan guidance and the example and query task.

Given the seemingly wide spread adoption of ReAct methodology, the brittleness we witnessed calls for a systematic study of the factors contributing to the performance of ReAct-based LLM Agents. Based on the claims of [Yao et al. 2022], we isolate three possible reasons for the claimed performance of ReAct framework: 1) the utility of interleaving reasoning trace during action execution, 2) the utility of providing plan guidance, and, 3) the significance of example prompt provided to the the LLM. However, the brittleness of ReAct becomes apparent when considering the variability in prompt designs. Depending on the domain and task, prompt designers may provide abstract guidance, task-specific instructions within the same domain, analogical examples, or global reasoning traces. ReAct's inability to robustly adapt to such variations underscores its limitations in handling diverse input prompts, thereby necessitating a closer examination of its design and implementation. In this work, we systematically evaluate the brittleness of ReAct by studying which potential factors contribute to its performance. This analysis is conducted through variations in input prompts to understand how a ReAct LLM Agent responds to (1) where the guidance is provided, (2) the different types and structure of this guidance, and finally, (3) on varying the resemblance of example prompt to the queried problem. We investigate the research questions :

RQ1: Does the agent performance depend on interleaving reasoning trace with action execution? RQ2: How does the nature of the reasoning trace or guidance information affect the performance of LLM Agents?

RQ3: How does the similarity between the example 〈problem, solution〉and the query $\langle$ problem, ? $\rangle$, which are present in the prompt, affect LLM Agent performance?

We conduct extensive experiments on the AlfWorld domain using various LLM Models, including GPT-3.5-turbo, GPT-3.5-instruct, GPT-4 and Claude-Opus. Through our comprehensive empirical study, answer each of the research questions above. (RQ1) We find that LLM performance infact improves when the reasoning trace is not interleaved with action execution. (RQ2) Moreover, providing weaker guidance or placebo-guidance (where the text provides no information about the task) has comparable performance to strong reasoning trace based guidance. Answers to RQ1 and RQ2 highlight that the source of ReAct LLM agent performance is not the interleaving aspect or the content of the reasoning trace. Finally, (RQ3) we see that variations to the example prompt such that it belongs to different task within the same domain, or has a different goal or plan attributes than the queried problem; causes the performance of ReAct-LLM Agent to plummet.

Our findings highlight that the benefits of ReAct-based LLM Agents are present when prompt engineers can curate instance-specific examples. This approach may not scale for domains with a large number of problem instance classes, and it places an undue burden on prompt engineers to provide instance specific examples. Finally, our experiments call into question claims of enhanced "emergent reasoning" of LLMs with prompt engineering efforts such as ReAct; corroborating contemporary research [Verma et al., 2024, Valmeekam et al., 2024, Stechly et al., 2024, Ullman, 2023, Schaeffer et al. 2023, McCoy et al. 2023] questioning reasoning abilities of LLMs.

## 2 Preliminaries

### 2.1 AlfWorld

AlfWorld Shridhar et al. [2020] is a synthetic text-based game built on top of a PDDL domain description. ReAct Yao et al. [2022] defines six tasks (or problem classes) within this domain namely - Put, Clean, Heat, Cool, Examine, and PutTwo. Each problem class consists of several problem instances, such as put a spraybottle on toilet (see Fig. 1 is an example instance of Put class. Since AlfWorld is a partially observable environment, each of these problem instances can be solved by navigating and interacting with the environment simulator via text actions. For example, this task can be solved by the following actions- go to cabinet 2, take spraybottle 2 from cabinet 2, go to toilet 1, put spraybottle 2 in/on toilet 1.

### 2.2 ReAct

The think action tag provided by ReAct is claimed to comprise of Reasoning + Action trace that is provided in the solution for the example problems as part of the prompt. During execution, when the LLM agent is queried, the expectation is that it can generate a think action tag for the queried problem instance that is semantically similar to the one provided for the examples in the prompt.

Location of THINK tag In ReAct, the integration of the think tag within actions serves to expand the action space. This allows the language model (LLM) agent to execute a think action, prompting an ' $O K$ ' response. Through analysis of example prompts in ReAct experiments, we identify various instances of the think action. Typically, it appears after stating the problem instance, reiterating the task, and providing problem-specific guidance. However, the authors offer no structured guidelines for its implementation, placement, or guidance. This observation aligns with feedback from reviewers [OpenReview, 2024] citing inconsistencies in the prompting format.

Content of THINK tag In ReAct, the think action consistently provides the decision-making agent with success-oriented guidance for task completion. For instance, upon encountering a spraybottle, the prompt might include: think: Now I find a spraybottle (2). Next, I need to take it. This guidance exposes forthcoming actions and sub-tasks for the agent.

Few shot EXAMPLEs In the AlfWorld domain (wihch is a PDDL domain), ReAct authors [Yao et al. 2022] classify six problem classes or tasks: Put, Clean, Heat, Cool, Examine, PutTwo. Despite representing different tasks, they share the same environment dynamics and action space, allowing for very similar execution trace. For instance, a Heat task might involve Putting an item into a microwave. In ReAct experiments, authors provide two example problem-solution pairs (referred to as exemplars in our work) before querying the LLM agent with a problem instance. Authors force ReAct agent to use examples and queries belonging to the same problem class without motivating this design decision. However, the queried problem may differ in objects or locations from the exemplars.

## 3 Related Work

Large Language Models have been shown to be successful in a plethora of natural language tasks [Kocoń et al., 2023, Gilardi et al., 2023, Zhu et al., 2023, Bubeck et al., 2023, Bhattacharjee et al. 2024]. However, there are two schools of thought when it comes to utilizing off-the-shelf LLMs for planning and reasoning tasks. Works such as Chain of Thought, ReAct, and others that followed [Wei et al., 2022, Yao et al., 2023, Long, 2023, Yao et al., 2024, Besta et al., 2024, Fu et al., 2024, Aksitov et al. |2023], have argued about the reasoning abilities of LLMs by proposing prompting methods. On the other hand, [Valmeekam et al., 2024, Stechly et al., 2024] have refuted these claims by showing the inability of LLMs to solve deterministic planning and classical reasoning problems.

In particular, for investigating the use of LLMs in solving decision making problems, AlfWorld [Shridhar et al. 2020] is a popular domain that was originally proposed for training text-based Reinforcement Learning agents. Lately, works such as ReAct, Reflexion, and their other variants [Yao et al., 2022, Shinn et al., 2023] have argued on the prowess of LLMs' reasoning abilities on AlfWorld. Furthermore, there have been several extensions to ReAct that boost their generalization abilities across more domains including multi-modal domains [Yang et al., 2023, Castrejon et al., 2024], autonomous vehicles [Cui et al., 2024], table question answering [Zhang et al., 2023], etc. While the effectiveness of ReAct is celebrated across different areas, these works only depend on anthropomorphization of LLMs for using ReAct based prompting with no justification on the source

![](https://cdn.mathpix.com/cropped/2024_06_04_01c4609402780a6a40e3g-04.jpg?height=1360&width=1393&top_left_y=247&top_left_x=363)

Figure 2: Example of prompt variations considered for RQ1 and RQ2.

of improvement in performance. This motivates our work in investigating the components of ReAct with respect to sequential decision-making problems and analyzing the role each component plays.

## 4 Brittleness of ReAct based Agentic LLMs

We examine the claims of ReAct to understand the performance of ReAct-based LLM agents. It is crucial to assess whether ReAct's fundamental assertions hold, particularly in sequential decision making. As outlined in Section 2, ReAct comprises three main components: interleaving the think tag with actions, plan guidance after the think tag, and the selection of exemplar problems for LLM prompts. We perform a sensitvity analysis by proposing alternatives along these three dimensions. The subsequent sections explore the design of exemplar prompt variations to investigate our research questions concerning the claims of ReAct. Each variation modifies the base ReAct exemplar prompts.

### 4.1 RQ1 : Interleaving thinking with acting

Does the agent performance depend on interleaving reasoning trace with action execution?

To answer this research question, we propose collating the guidance information contained within the multiple think tags present in the examples of the input prompt into a single think tag appended after the example problem is specified. This approach can be interpreted as Chain-of-Thought Kojima et al. [2022], Wei et al. [2022], where guidance information is generated before action execution.

Variation 1: Exemplar-based CoT AlfWorld is a partially observable environment where an agent can only observe objects after reaching that location. Hence, we remove specific location and object identifiers to modify the think actions that are originally interleaved with other actions in the environment. For example, we rewrite it as: think: Now Once I find a spraybottle (2), next I need to take it.. Finally, we append all the think actions together at the beginning of the example problem. Intuition: Problem-specific guidance for a sequential decision-making agent can be given step-by-step (as in ReAct) or all at once.

Variation 2: Anonymized Exemplar-CoT We take one step further and modify the think tag to remove references to specific locations and objects, making it more general. For example, think: To solve the task, I need to find and take a spraybottle the object, then put it on toilet the desired location. Intuition: Exemplars can be made more general by providing abstract guidance and exploting LLMs ability to identify necessary semantic entity relations.

### 4.2 RQ2 : Plan Guidance following think tag

How does the nature of the reasoning trace or guidance information affect the performance of LLM? ReAct claims to use reasoning trace as the guidance information following the think tag. For instance, in ReAct thoughts are to (1) decompose the goal (2) track subgoal completion (3) determine the next subgoal and (4) reason via common-sense where to find and object and what to do with it. It is, however, unclear what is the motivation to use these as the reasoning trace. The potential anthropomorphization of large language models (LLMs) may suggest that their thought processes are similar to the abstract plans humans make, and that they must be prompted in the same manner. However, it is unclear why this assumption should hold true. Alternatives can be, we can prompt the LLM to reflect on past failures and provide possible explanations (hindsight-guidance) or We can substitute task-relevant guidance with placebo-guidance by using "magic incantations".

Variation 1: Failure From the example prompts used in ReAct, we note that none of the examples for any task consist of invalid actions. We inject two invalid actions in the execution trace : the first that attempts to execute the action pertinent to the task (such as put spraybottle 2 in/on toilet) when not possible and, second, executes some other invalid action. We include the expected simulator response, Nothing happens., when invalid actions are taken. Intuition: Reasoning trace can be about what to do such as subgoals of the future, or what not to do such as mistakes in hindsight. This should be weaker guidance than in base ReAct as the exemplars do not point out what to do next.

Variation 2: Failure + Explanation We place think actions after invalid actions injected in Failure Variation which consist of explanations for the failure such as think: Nothing happens because I do not have a spraybottle 2. Intuition: We can augment pointing out mistakes in hindsight with explanations to avoid similar failures. This is stronger guidance signal than Failure, however, the exemplars still not provide information on what to do next.

Variation 3: Guidance Ordering LLMs are known to be susceptible to minor syntactic perturbations to inputs. We test whether it is true for guidance information given as prompt as well. We identify chain of subtasks in a reasoning trace $S_{1} \rightarrow S_{2} \cdots S_{n}$ and reverse it to be $S_{n} \rightarrow S_{n-1} \cdots S_{1}$. For instance, think: Now I find a spraybottle (2). Next, I need to take it. becomes think: Next, I need to take the spraybottle (2). Now I find a spraybottle (2). Intuition: LLM agent should be invariant to the syntax of reasoning trace if the semantic information is preserved. This does not change the reasoning trace from the perspective of information content.

Variation 4: Placebo Guidance It is unclear to what extent LLM agent uses the supposed helpful thoughts for the decision making task. In this variation we replace think tag guidance with a placebo thought that does not contain any task relevant information, but has been widely used as prompt engineering trick [Kojima et al. 2022]. Intuition: According to claims of ReAct, we expect the performance to get worse when the guidance does not have any information useful for task success.

### 4.3 RQ3 : Similarity between EXAMPLEs and QUERY

How does the similarity between the example (problem, solution)and the query (problem, ? ), which are present in the prompt, affect LLM Agent performance?

RQ3 investigates the role of example similarity to the query in LLM agent's performance. Establishing problem similarity can be challenging, especially where minor variations to the problem can have

Table 1: Average Success \% of LLM on Base ReAct and prompt variations for RQ1 and RQ2 on six AlfWorld tasks.

| Model / Prompt | Base | H1 |  |  | H2 |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | CoT | Anon. CoT | Magic | Order | Failure | Explanation |  |
| GPT-3.5-Turbo | 27.6 | 46.6 | 41 | 30 | 28.3 | 43.3 | 41.6 |  |
| GPT-3.5-Instruct | 44.7 | 61.9 | 50.7 | 41 | 42.5 | 47 | 44.7 |  |
| GPT-4 | 23.3 | 43.3 | 33.3 | 36.6 | 30 | 50 | 36.6 |  |
| Claude-Opus | 56.6 | 50 | 46.6 | 30 | 50 | 53.3 | 30 |  |

varied interpretations (such as an analogy to a different task altogether). Our work explores this challenge in a systematic way. During example prompt construction, prompt designers may use synonyms to refer to objects (Domain), come up with examples where the agent task is the same as query but the goals are different (Instance), or provide optimal solutions as the examples (Optimal) preventing LLM to obtain information regarding exploration strategy. Furthermore, given that the domain has the same underlying action dynamics and that the tasks reuse several actions, prompt designers may choose to provide query specific example prompts (as in base ReAct), provide one of a different task and one of the same task (One), provide both examples to be of a different task (Both), or take an exhaustive approach and provide one example of all tasks (All).

Variation 1: Synonyms - (Domain) For this variation, we replace the object and location names in the example prompts with their synonyms. For example, spraybottle $\rightarrow$ aerosolbottle, cabinet $\rightarrow$ cupboard, and, microwave $\rightarrow$ oven. We make 36 such changes to object and location names across all the examples. Note that the object names / location are unchanged for the problem query and subsequent interaction with the simulator. Intuition: Exemplar guidance maybe specified with alternate synonymous object and location names. Reasoning agents should be invariant to variable name substitution for closed world dynamics such as PDDL based AlfWorld.

Variation 2: Problem Instance-level - Instance We inject instance-level changes to the examples provided in the prompts. Recall that we are updating the base ReAct's prompts, where the exemplar tasks are same as the query. We change the goal location in exemplar problem to ensure that it does not match with any of the goal locations in query problem. Moreover, we add repetitive yet futile actions in the exemplar execution trace which does not effect the solution. Intuition: Ensuring a different goal location in exemplar from the queried problem is a natural usecase. Moreover, exemplars may contain arbitrary exploration strategies such as action repetition [Sharma et al., 2017]. By ReAct's claims, LLM agent performance should not be affected.

Variation 3: Problem Level - Both, One, All Recall that the environment dynamics for all the tasks are the same. In fact, several tasks subsume the use of our tasks such as Heat requires the agent to Put an food in the microwave. In general, all the tasks share a large portion of actions (such as exploring cabinets and locations, picking objects etc.). Motivated by how tight relationship of these tasks we come up with three variations. First, One, uses one exemplar of an arbitrarily picked task and the other exemplar of the same task as the query. Second, Both, uses both exemplars from an arbitrarily picked task. Finally, All, uses a total of six exemplars (this is the only variation where we provide more than the standard two examples as in ReAct) corresponding to each task under consideration. Remember, this includes the query task which is always present at the end in the input prompt. Intuition: With a very similar action execution trace (such as exploration, picking and placing objects) across tasks, and shared dynamics, LLM agent should be minimally affected by the use of exemplars of a different task.

Variation 4: Exploration Strategy - Optimal As noted before, ReAct does not explain the choice of exemplars used. An important ingredient to the exemplars is the exploration strategy used. In this variation we provide exemplars which serendipitously take the optimal actions (as if the environment were fully observable) and therefore the example plan is the shortest possible. Intuition: Exploration strategy exposed in exemplars (that too for the same problem task) should not impact ReAct's performance if the LLM agent is reasoning instead of retrieval (or pattern matching).

## 5 Results

In the following sub-sections, we will answer our RQs through sensitivity analysis using the proposed prompt variations along three dimensions, the location of the think tag, the content of the think tag,

![](https://cdn.mathpix.com/cropped/2024_06_04_01c4609402780a6a40e3g-07.jpg?height=445&width=1358&top_left_y=276&top_left_x=381)

Figure 3: The radar chart shows the failure rates of various LLMs with different ReAct-based prompt settings for RQ1 (Base React, Global, Anonymized) across six Alfworld tasks (hexagon vertices). Higher values / Larger shaded region indicate worse performance.

![](https://cdn.mathpix.com/cropped/2024_06_04_01c4609402780a6a40e3g-07.jpg?height=441&width=1355&top_left_y=945&top_left_x=385)

Figure 4: The radar chart shows the failure rates of various LLMs with different ReAct-based prompt settings for RQ2 (Base React, Magic, Failure, Failure+Explanation, Ordering) across six Alfworld tasks (hexagon vertices). Higher values / Larger shaded region indicate worse performance.

and the similarity between exemplars and queried problems. All the variations modify the base ReAct prompts and we do not present a cross between variations unless otherwise noted. We use GPT-3.5-Turbo, GPT-3.5-Instruct, GPT-4, and Claude-Opus which are all newer models than those benchmarked in ReAct [Yao et al., 2022]. As noted, we use AlfWorld domain consistent with the setup in [Yao et al., 2022]. GPT3.5(Turbo, Instruct) results are on 134 instances across six tasks, GPT-4/Claude-Opus on 60 instances (10 for each task) due to cost considerations.

### 5.1 Utility of interleaving reasoning trace with action execution

From Table 11(RQ1) note that the exemplar CoT and the anonymized exemplar CoT performs significantly better than base ReAct for all GPT-X family of models. Moreover, the performance dips slightly for Claude-Opus along these variations. From Fig. 3 (larger area represents worse performance), we observe that base ReAct consistently performs worse in most of the tasks. This refutes ReAct's first claim on the importance of interleaving reasoning trace generation with action execution. Even in the case of the Claude where there is a slight dip in performance, the models seems to be performing at reasonably high success rate which questions the importance of interleaved reasoning and action execution.

### 5.2 Utility of Guidance Information following think tag

Recall that reasoning trace guidance pertains to the prospective actions or behaviors an agent should execute (foresight guidance). This type of guidance is more informative compared to other variations, such as hindsight guidance, which focuses on past errors without providing future solution steps, and placebo guidance, which is entirely unrelated to the task. ReAct claims that reasoning trace is crucial

Table 2: Average Success \% of LLM on Base ReAct and prompt variations for RQ3 on six AlfWorld tasks. OC: Out of context limit

| Model / Prompt | Base | RQ3 |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Domain | Instance | Optimal | All | One | Both |  |
| GPT-3.5-Turbo | 27.6 | 1.6 | 30 | 20.1 | 32 | 28.3 | 1.6 |  |
| GPT-3.5-Instruct | 44.7 | 47.6 | 42.5 | 39.5 | OC | 17.9 | 5.2 |  |
| GPT-4 | 23.3 | 13.3 | 23.3 | 50 | 23.3 | 16.6 | 0 |  |
| Claude-Opus | 56.6 | 50 | 46.6 | 43.3 | 50 | 60 | 6.6 |  |

![](https://cdn.mathpix.com/cropped/2024_06_04_01c4609402780a6a40e3g-08.jpg?height=447&width=1353&top_left_y=633&top_left_x=386)

Figure 5: The radar chart shows the failure rates of various LLMs with different ReAct-based prompt settings for RQ3 (Base React, Domain, Instance, All, Both, One) across six Alfworld tasks (hexagon vertices). Higher values / Larger shaded region indicate worse performance.

for LLM agent performance, which would predict a decline in performance with hindsight guidance and a collapse with placebo guidance. In contrast, our findings in Table 1 indicate that hindsight guidance (Failure, Explanation) actually improve the performance of the GPT family of models. The Claude-Opus model's performance remains stable with hindsight (Failure) guidance and declines with placebo guidance. Figure 4illustrates these models' performance across six AlfWorld tasks and variations, showing that the performance of LLMs either improved or remained consistent when provided with weaker or irrelevant guidance information. This refutes ReAct's claim that task-specific reasoning trace is the source of LLM agent performance. Our argument that LLM agent's performance is only slightly affected by the reasoning trace explains the indifference to ordering perturbation as well. If the LLM is not utilizing the reasoning trace for decision making, change in ordering would not affect the agent's performance. Finally, contrary to the general perception that better GPT models would improve over reasoning, we find that GPT-4's performance is the worst among GPT-X family further highlighting the brittleness of claims of ReAct.

### 5.3 Utility of Exemplar similarity to Query task

Intuitively, the similarity of Domain examples is closest with base ReAct, followed by Instance and Optimal variations. Finally, All contains an overload of information followed by One and Both which has the same action space but uses different tasks as exemplars. Recall that AlfWorld being a PDDL domain has a shared environment dynamics across all tasks with upto $80 \%$ of actions shared across execution traces. While ReAct does not investigate impact of varied exemplars, given the popular usage one expects LLMs to be robust to such changes especially in a common-sense household domain. Table 2 shows the severe brittleness of ReAct based LLM agent to even minor variations (such as Domain, Instance). Specifically, performance of GPT-3.5-Turbo and GPT-4 plumments for Domain. Claude-Opus which was more robust in RQ1, RQ2, is also impacted severely by Domain, Instance variations. Furthermore, when we do not expose the exploration strategy and only provide Optimal exemplars, the performance of LLM agents further drops (except in GPT4).

Overloading the LLMs with more exemplars All does not impact its performance. We posit, this is because the query-task exemplar is still part of the large input prompt. Among the two exemplars, as provided in ReAct, when one of them is of a different task (One) then the performance significantly reduces for LLMs. When both of the exemplars are of a different task then the performance collapses to single digit success rates for all the models. This is a key result of this work highlighting the severe
dependence of LLMs on the similarity of the exemplars to the query task. Through sensitivity analysis using our RQ3 variations we could find parts of the input (the task similarity of the exemplar with query) which is the source of ReAct performance. Essentially, the LLM is mimicking / performing approximate retrieval from the context presented to it. Moreover, our results corroborates the line of research that points out the inability of LLMs to reason or plan.

Unrolling and Subtask Similarity We perform additional experiments where the query task is to essentially repeat the task in the exemplar (Unrolling). For instance, the exemplar is Put and the query is PutTwo to put two objects at given location. In this case, the LLM has to unroll the given advice and repeat exemplar task execution to solve the query. The success rate of GPT-3.5-Instruct (the best performing GPT model in our experiments) drops down from $52 \%$ to $9 \%$. Similarly, we experiment with a Subtask Similarity variation where the exemplar task subsumes execution of the query task. For instance, the Heat task requires the agent to pick and place object in the microwave (which is an instantiation of Put task). One would expect that Heat is a good exemplar for Put, however, the performance of GPT-3.5-instruct model goes from $18 \%$ to $0 \%$ in this case. These results further underscore the brittleness and the need for instance-specific exemplars in ReAct.

Thought operationalization ability of LLMs Given the free form nature of thought generation and arbitrary nature of thought (about subtask, common-sense next steps etc.), checking whether the generated thoughts are in-fact reasonable is a challenging problem. For completeness, we find that $40 \%$ of the times after generation of a think tag, subsequent environment action taken by the LLM was invalid (for GPT-3.5-instruct). It is much higher ( $80 \%$ for GPT-3.5-Turbo, $90 \%$ for Claude-Haiku) for weaker LLM models. This further highlights the inability of LLMs to operationalize its generated thought as also seen in |Roy et al. 2024|. From manual inspection we find that the typical thoughts would enlist all possible locations as next locations to visit for most of the tasks. As demonstrated in Section 5.2 the performance of LLMs actually decreases when provided with foresight guidance, as seen with the base ReAct model. A detailed investigation into the validity of the generated reasoning traces is beyond the scope of this work and is suggested as future research.

## 6 Conclusion

ReAct based prompt engineering methods have been claimed to improve planning abilities of Large Language Models. In this study, we critically examine ReAct along three dimensions, informed by its claims and our hypotheses regarding its performance sources. Contrary to ReAct's claims, our findings reveal that its performance is neither due to interleaving reasoning trace and guidance information generation with action execution, nor due to the specific nature of the guidance information. Instead, we identify that the true source of LLM performance in sequential decision-making tasks, such as AlfWorld, is the high degree of similarity between exemplar problems (few-shot) and the query task. We also showed that ReAct is susceptible to trivial variations in exemplar prompts (such as with the use of synonyms, or Unrolling and Subtask Similarity cases). Our findings caution against an uncritical adoption of ReAct-style frameworks for their putative abilities to enhance performance in domains requiring planning.

## 7 Broader Impact \& Limitations

This work highlights the need for critical examination of prompt-engineering methods which claim emergent abilities of LLMs. Absence or presence of such abilities can have a large impact on several sectors such as economics, healthcare, transport to name a few. We hope highlighting the brittleness of popularly used technique such as ReAct will improve experimentation standards for the agentic LLM community.

We deliberately confined our experiments to be deep along a domain considered common-sense planning domain by the contemporary research community. Moreover, we restricted our discussion to sequential decision making problem of AlfWorld and admit that other prompting solutions to reasoning problems must be re-evaluated with similar scrutiny. While this can be seen as a potential limitation, our examination of ReAct can be easily translated to other domains of interest by the consumer of this research.

## References

Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, et al. Rest meets react: Selfimprovement for multi-step reasoning llm agent. arXiv preprint arXiv:2312.10003, 2023.

Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17682-17690, 2024.

Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. Towards llm-guided causal explainability for black-box text classifiers. 2024.

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.

Lluis Castrejon, Thomas Mensink, Howard Zhou, Vittorio Ferrari, Andre Araujo, and Jasper Uijlings. Hammr: Hierarchical multimodal react agents for generic vqa. arXiv preprint arXiv:2404.05465, 2024.

Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, and Ziran Wang. Receive, reason, and react: Drive as you say, with large language models in autonomous vehicles. IEEE Intelligent Transportation Systems Magazine, 2024.

Dayuan Fu, Jianzhao Huang, Siyuan Lu, Guanting Dong, Yejie Wang, Keqing He, and Weiran Xu. Preact: Predicting future in react enhances agent's planning ability. arXiv preprint arXiv:2402.11534, 2024.

Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, 2023 .

Louie Giray. Prompt engineering with chatgpt: a guide for academic writers. Annals of biomedical engineering, 51(12):2629-2633, 2023.

Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. Llms can't plan, but can help planning in llm-modulo frameworks. arXiv preprint arXiv:2402.01817, 2024.

Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. Chatgpt: Jack of all trades, master of none. Information Fusion, 99:101861, 2023.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: $22199-22213,2022$.

Jieyi Long. Large language model guided tree-of-thought. arXiv preprint arXiv:2305.08291, 2023.

Ggaliwango Marvin, Nakayiza Hellen, Daudi Jjingo, and Joyce Nakatumba-Nabende. Prompt engineering in large language models. In International Conference on Data Intelligence and Cognitive Informatics, pages 387-402. Springer, 2023.

R Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve. arXiv preprint arXiv:2309.13638, 2023.

OpenReview. ReAct: Synergizing Reasoning and Acting in Language Models. https:// openreview.net/forum?id=WE_vluYUL-X, 2024.

Shamik Roy, Sailik Sengupta, Daniele Bonadiman, Saab Mansour, and Arshit Gupta. Flap: Flow adhering planning with constrained decoding in llms. arXiv preprint arXiv:2403.05766, 2024.

Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. A systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927, 2024.

Rylan Schaeffer, Kateryna Pistunova, Samar Khanna, Sarthak Consul, and Sanmi Koyejo. Invalid logic, equivalent gains: The bizarreness of reasoning in language model prompting. arXiv preprint arXiv:2307.10573, 2023.

Sahil Sharma, Aravind Srinivas, and Balaraman Ravindran. Learning to repeat: Fine grained action repetition for deep reinforcement learning. arXiv preprint arXiv:1702.06054, 2017.

Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.

Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.

Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. On the self-verification limitations of large language models on reasoning and planning tasks. arXiv preprint arXiv:2402.08115, 2024.

James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018.

Tomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399, 2023.

Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36, 2024.

Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati. Theory of mind abilities of large language models in human-robot interaction: An illusion? In Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction, pages 36-45, 2024.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.

Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2022.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.

Yao Yao, Zuchao Li, and Hai Zhao. Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. arXiv preprint arXiv:2305.16582, 2023.

Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon, Shaleen Deep, and Jignesh M Patel. Reactable: Enhancing react for table question answering. arXiv preprint arXiv:2310.00815, 2023.

Yiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui, and Gareth Tyson. Can chatgpt reproduce human-generated labels? a study of social computing tasks. arXiv preprint arXiv:2304.10145, 2023.
