# Cross-ModAL ConTEXTUALIZED DifFUSION MODELS FOR TEXT-GUIDED VISUAL GENERATION AND EDITING 

Ling Yang ${ }^{1 * \dagger} \quad$ Zhilong $\mathbf{Z h a n g}^{1 *} \quad$ Zhaochen Yu ${ }^{1 *}$ Jingwei Liu ${ }^{1} \quad$ Minkai Xu $^{2}$<br>Stefano Ermon ${ }^{2} \quad$ Bin Cui $^{1 \dagger}$<br>${ }^{1}$ Peking University ${ }^{2}$ Stanford University<br>\{yangling0818, bityzcedu, jingweiliu1996\}@163.com<br>\{minkai, ermon\}@cs.stanford.edu,\{zzl2018math, bin.cui\}@pku.edu.cn


#### Abstract

Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (CONTEXTDIFF) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing. In each task, our CONTEXTDIFF achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at https://github.com/YangLing0818/ContextDiff


## 1 INTRODUCTION

Diffusion models (Yang et al., 2023b) have made remarkable progress in visual generation and editing. They are first introduced by Sohl-Dickstein et al. (2015) and then improved by Song \& Ermon (2019) and Ho et al. (2020), and can now generate samples with unprecedented quality and diversity (Rombach et al., 2022; Yang et al., 2023a; Podell et al., 2023; Yang et al., 2024b;a). As a powerful representation space for multi-modal data, CLIP latent space (Radford et al., 2021) is widely used by diffusion models to semantically modify images/videos by moving in the direction of any encoded text condition for controllable text-guided visual synthesis (Ramesh et al., 2022; Saharia et al., 2022b; Ho et al., 2022; Molad et al., 2023; Wu et al., 2022; Khachatryan et al., 2023).

Generally, text-guided visual diffusion models gradually disrupt visual input by adding noise through a fixed forward process, and learn its reverse process to generate samples from noise in a denoising way by incorporating clip text embedding. For example, text-to-image diffusion models usually estimate the similarity between text and noisy data to guide pretrained unconditional DDPMs (Dhariwal \& Nichol, 2021; Nichol et al., 2022a), or directly train a conditional DDPM from scratch by incorporating text into the function approximator of the reverse process (Rombach et al., 2022; Ramesh et al., 2022). Text-to-video diffusion models mainly build upon pretrained DDPMs, and extend them with designed temporal modules (e.g., spatio-temporal attention) and DDIM Song et al. (2020) inversion for both temporal and structural consistency (Wu et al., 2022; Qi et al., 2023).[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-02.jpg?height=173&width=585&top_left_y=255&top_left_x=412)

Reverse Process

![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-02.jpg?height=358&width=632&top_left_y=244&top_left_x=1061)

(b) Contextualized forward and reverse diffusion processes (a) Conventional forward and reverse diffusion processes diffusion models with (a) conver for Figure 1: A simplified illustration of text-guided visual diffusion models with (a) conventional forward and reverse diffusion processes, (b) our contextualized forward and reverse diffusion processes. $\tilde{x}_{0}$ denotes the estimation of visual sample by the denoising network at each timestep.

Despite all this progress, there are common limitations in the majority of existing text-guided visual diffusion models. They typically employ an unconditional forward process but rely on a textconditional reverse process for denoising and sample generation. This inconsistency in the utilization of text condition between forward and reverse processes would constrain the potential of conditional diffusion models. Furthermore, they usually neglect the cross-modal context, which encompasses the interaction and alignment between textual and visual modalities in the diffusion process, which may limit the precise expression of textual semantics in visual synthesis results.

To address these limitations, we propose a novel and general cross-modal contextualized diffusion model (CONTEXTDIFF) that harnesses cross-modal context to facilitate the learning capacity of cross-modal diffusion models. As illustrated in Figure 1, we compare our contextualized diffusion models with conventional text-guided diffusion models. We incorporate the cross-modal interactions between text condition and image/video sample into the forward process, serving as a context-aware adapter to optimize diffusion trajectories. Furthermore, to facilitate the conditional modeling in the reverse process and align it with the adapted forward process, we also use the context-aware adapter to adapt the sampling trajectories. In contrast to traditional textual guidance employed for visual sampling process (Rombach et al., 2022; Saharia et al., 2022b), our CONTEXTDIFF offers a distinct approach by providing enhanced and contextually informed guidance for visual sampling. We generalize our contextualized diffusion to both DDPMs and DDIMs for benefiting both cross-modal generation and editing tasks, and provide detailed theoretical derivations. We demonstrate the effectiveness of our CONTEXTDIFF in two challenging text-guided visual synthesis tasks: text-to-video generation and text-to-video editing. Empirical results reveal that our contextualized diffusion models can consistently improve the semantic alignment between text conditions and synthesis results over existing diffusion models in both tasks.

To summarize, we have made the following contributions: (i) To the best of our knowledge, We for the first time propose CONTEXTDIFF to consider cross-modal interactions as context-aware trajectory adapter to contextualize both forward and sampling processes in text-guided visual diffusion models. (ii) We generalize our contextualized diffusion to DDPMs and DDIMs with thereotical derivations for benefiting both cross-modal visual generation and editing tasks. (iii) Our ConTEXTDIFF achieves new state-of-the-art performance on text-to-image generation and text-tovideo editing tasks (in Figure 2), consistently demonstrating the superiority of our CONTEXTDIFF over existing diffusion models with both quantitative and qualitative comparisons.

## 2 RELATED WORK

Text-Guided Visual Diffusion Models Text-to-image diffusion models (Yang et al., 2023a; Podell et al., 2023; Yang et al., 2024b; Zhang et al., 2024) mainly incorporate the text semantics that extracted by pretrained language models into the image sampling process (Nichol et al., 2022a). For example, Latent Diffusion Models (LDMs) (Rombach et al., 2022) apply diffusion models on the latent space of powerful pretrained autoencoders for high-resolution synthesis. RPG (Yang et al., 2024b) proposes a LLM-grounded text-to-image diffusion and utilizes the multimodal chain-ofthought reasoning ability of MLLMs to enable complex/compositional image generation. Regarding text-to-video diffusion models, recent methods mainly leverage the pretrained text-to-image diffusion models in zero-shot (Qi et al., 2023; Wang et al., 2023) and one-shot (Wu et al., 2022; Liu
![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-03.jpg?height=1526&width=1392&top_left_y=274&top_left_x=366)

A man is walking on the mountaintop

A red rose. covered with dry grass.

Figure 2: Qualitative comparison in text-to-video editing, edited text prompt is denoted in color. Our CONTEXTDIFF achieves best semantic alignment, image fidelity, and editing quality.

et al., 2023) methodologies for text-to-video editing. For example, Tune-A-Video (Wu et al., 2022) employs DDIM (Song et al., 2020) inversion to provide structural guidance for sampling, and proposes efficient attention tuning for improving temporal consistency. FateZero (Qi et al., 2023) fuses the attention maps in the inversion process and generation process to preserve the motion and structure consistency during editing. In this work, we for the first time improve both text-to-image and text-to-video diffusion models with a general context-aware trajectory adapter.

Diffusion Trajectory Optimization Our work focuses on optimizing the diffusion trajectories that denotes the distribution of the entire diffusion process. Some methods modify the forward process with a carefully-designed transition kernel or a new data-dependent initialization distribution (Liu et al., 2022; Hoogeboom \& Salimans, 2022; Dockhorn et al., 2021; Lee et al., 2021; Karras et al., 2022). For example, Rectified Flow (Liu et al., 2022) learns a straight path connecting the

![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-04.jpg?height=601&width=1371&top_left_y=190&top_left_x=385)

Figure 3: Illustration of our CONTEXTDIFF.

data distribution and prior distribution. Grad-TTS (Popov et al., 2021) and PriorGrad (Lee et al., 2021) introduce conditional forward process with data-dependent priors for audio diffusion models. Other methods mainly parameterize the forward process with additional neural networks (Zhang \& Chen, 2021; Kim et al., 2022; Kingma et al., 2021). VDM (Kingma et al., 2021) parameterizes the noise schedule with a monotonic neural network, which is jointly trained with the denoising network. However, these methods only utilize unimodal information in forward process, and thus are inadequate for handling complex multimodal synthesis tasks. In contrast, our CONTEXTDIFF for the first time incorporates cross-modal context into the diffusion process for improving text-guided visual synthesis, which is more informative and contextual guidance compared to text guidance.

## 3 METHOD

### 3.1 Cross-Modal ConteXTUALIZED DIFFUSION

We aim to incorporate cross-modal context of each text-image(video) pair $\left(\boldsymbol{c}, \boldsymbol{x}_{0}\right)$ into the diffusion process as in Figure 3. We use clip encoders to extract the embeddings of each pair, and adopt an relational network (e.g., cross attention) to model the interactions and alignments between the two modalities as cross-modal context. This context is then propagated to all timesteps of the diffusion process as a bias term (we highlight the critical parts of our CONTEXTDIFF in brown):

$$
\begin{equation*}
q_{\phi}\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{0}, \boldsymbol{c}\right)=\mathcal{N}\left(\boldsymbol{x}_{t}, \sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}+k_{t} \boldsymbol{r}_{\phi}\left(\boldsymbol{x}_{0}, \boldsymbol{c}, t\right),\left(1-\bar{\alpha}_{t}\right) \boldsymbol{I}\right) \tag{1}
\end{equation*}
$$

where scalar $k_{t}$ control the magnitude of the bias term, and we set the $k_{t}$ to $\sqrt{\bar{\alpha}_{t}} \cdot\left(1-\sqrt{\bar{\alpha}_{t}}\right)$. $\boldsymbol{r}_{\phi}(\cdot)$ is the relational network with trainable parameters $\phi$, it takes the visual sample $\boldsymbol{x}_{0}$ and text condition $\boldsymbol{c}$ as inputs and produces the bias with the same dimension as $\boldsymbol{x}_{0}$.

Concretely, the forward process is defined as $q_{\phi}\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{T} \mid \boldsymbol{x}_{0}, \boldsymbol{c}\right)=\prod_{t=1}^{T} q_{\phi}\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_{0}, \boldsymbol{c}\right)$. Given cross-modal context $\boldsymbol{r}_{\phi}\left(\boldsymbol{x}_{0}, \boldsymbol{c}, t\right)$, the forward transition kernel depends on $\boldsymbol{x}_{t-1}, \boldsymbol{x}_{0}$, and $\boldsymbol{c}$ :

$$
\begin{equation*}
q_{\phi}\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_{0}, \boldsymbol{c}\right)=\mathcal{N}\left(\sqrt{\alpha_{t}} \boldsymbol{x}_{t-1}+k_{t} \boldsymbol{r}_{\phi}\left(\boldsymbol{x}_{0}, \boldsymbol{c}, t\right)-\sqrt{\alpha_{t}} k_{t-1} \boldsymbol{r}_{\phi}\left(\boldsymbol{x}_{0}, \boldsymbol{c}, t-1\right), \beta_{t} \boldsymbol{I}\right) \tag{2}
\end{equation*}
$$

where $\beta_{t}=1-\alpha_{t}$. This transition kernel gives marginal distribution as Equation (1) (proof in Appendix A.1). At each timestep $t$, we add a noise that explicitly biased by the cross-modal context. With Equation (1) and Equation (2), we can derive the posterior distribution of the forward process for $t>1$ (proof in Appendix A.1):

$q_{\phi}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}_{0}, \boldsymbol{c}\right)=\mathcal{N}\left(\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \boldsymbol{x}_{0}+\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}}\left(\boldsymbol{x}_{t}-b_{t}\left(x_{0}, c\right)\right)+b_{t-1}\left(x_{0}, c\right), \frac{\left(1-\bar{\alpha}_{t-1}\right) \beta_{t}}{1-\bar{\alpha}_{t}} \boldsymbol{I}\right)$,

where $\boldsymbol{b}_{t}\left(\boldsymbol{x}_{0}, \boldsymbol{c}\right)$ is an abbreviation form of $k_{t} \boldsymbol{r}_{\phi}\left(\boldsymbol{x}_{0}, \boldsymbol{c}, t\right)$, and we use it for simplicity. With Equation (3), we can simplify the training objective which will be described latter. In this way, we contextualize the entire diffusion process with a context-aware trajectory adapter. In CONTEXTDIFF, we also utilize our context-aware context to adapt the reverse process of diffusion models, which encourages to align with the adapted forward process, and facilitates the precise expression of textual semantics in visual sampling process.

### 3.2 ADAPTING REVERSE PROCESS

We aim to learn a contextualized reverse process $\left\{p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{c}\right)\right\}_{t=1}^{T}$, which minimizes a variational upper bound of the negative log likelihood. $p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{c}\right)$ is gaussian kernel with learnable mean and pre-defined variance. Allowing the forward transition kernel to depend on $\boldsymbol{x}_{0}$ and $\boldsymbol{c}$, the objective function $\mathcal{L}_{\theta, \phi}$ of our CONTEXTDIFF can be formulated as (proof in Appendix A.2):

$$
\begin{align*}
\mathcal{L}_{\theta, \phi}=\mathbb{E}_{q_{\phi}\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_{0}, \boldsymbol{c}\right)} & {\left[D_{\mathrm{KL}}\left(q_{\phi}\left(\boldsymbol{x}_{T} \mid \boldsymbol{x}_{0}, \boldsymbol{c}\right) \| p\left(\boldsymbol{x}_{T} \mid \boldsymbol{c}\right)\right)-\log p_{\theta}\left(\boldsymbol{x}_{0} \mid \boldsymbol{x}_{1}, \boldsymbol{c}\right)\right.} \\
& \left.+\sum_{t>1} D_{\mathrm{KL}}\left(q_{\phi}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}_{0}, \boldsymbol{c}\right) \| p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{c}\right)\right)\right] \tag{4}
\end{align*}
$$

where $\theta$ denotes the learnable parameters of denoising network in reverse process. Equation (4) uses KL divergence to directly compare $p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{c}\right)$ against the adapted forward process posteriors, which are tractable when conditioned on $\boldsymbol{x}_{0}$ and $\boldsymbol{c}$. If $\boldsymbol{r}_{\phi}$ is identically zero, the objective can be viewed as the original DDPMs. Thus ContEXTDIFF is theoretically capable of achieving better likelihood compared to original DDPMs.

Kindly note that optimizing $\mathcal{L}_{t}=E_{q_{\phi}} D_{\mathrm{KL}}\left(q_{\phi}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}_{0}, \boldsymbol{c}\right) \| p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{c}\right)\right)$ is equivalent to matching the means for $q_{\phi}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}_{0}, \boldsymbol{c}\right)$ and $p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{c}\right)$, as they are gaussian distributions with the same variance. According to Equation (3), directly matching the means requires to parameterize a neural network $\boldsymbol{\mu}_{\theta}$ that not only predicting $\boldsymbol{x}_{0}$, but also matching the complex cross-modal context information in the forward process, i.e.,

$$
\begin{equation*}
\mathcal{L}_{\theta, \phi, t}=\left\|\boldsymbol{\mu}_{\theta}\left(\boldsymbol{x}_{t}, c, t\right)-\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \boldsymbol{x}_{0}-\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}}\left(\boldsymbol{x}_{t}-b_{t}\left(x_{0}, c\right)\right)-b_{t-1}\left(x_{0}, c\right)\right\|_{2}^{2} \tag{5}
\end{equation*}
$$

Simplified Training Objective Directly optimizing this objective is inefficient in practice because it needs to compute the bias twice at each timestep. To simplify the training process, we employ a denoising network $\boldsymbol{f}_{\theta}\left(\boldsymbol{x}_{t}, \boldsymbol{c}, t\right)$ to directly predict $\boldsymbol{x}_{0}$ from $\boldsymbol{x}_{t}$ at each time step $\mathrm{t}$, and insert the predicted $\hat{\boldsymbol{x}}_{0}$ in Equation (3), i.e., $p_{\theta, \phi}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{c}\right)=q_{\phi}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \hat{\boldsymbol{x}}_{0}, \boldsymbol{c}\right)$. Under mild condition, we can derive that the reconstruction objective $\mathbb{E}\left\|f_{\theta}-\boldsymbol{x}_{0}\right\|_{2}^{2}$ is an upper bound of $\mathcal{L}_{t}$, and thus an upper bound of negative log likehood (proof in Appendix A.2). Our simplified training objective is:

$$
\begin{equation*}
\mathcal{L}_{\theta, \phi}=\sum_{t=1}^{T} \lambda_{t} \mathbb{E}_{\boldsymbol{x}_{0}, \boldsymbol{x}_{t}}\left\|\boldsymbol{f}_{\theta}\left(\boldsymbol{x}_{t, \phi}, \boldsymbol{c}, t\right)-\boldsymbol{x}_{0}\right\|_{2}^{2} \tag{6}
\end{equation*}
$$

where $\lambda_{t}$ is a weighting scalar. We set $k_{T}=0$ and there is no learnable parameters in $D_{\mathrm{KL}}\left(q\left(\boldsymbol{x}_{T} \mid \boldsymbol{x}_{0}, \boldsymbol{c}\right) \| p\left(\boldsymbol{x}_{T} \mid \boldsymbol{c}\right)\right)$, which can be ignored. To adapt the reverse process at each timestep, we can efficiently sample a noisy sample $\boldsymbol{x}_{t}$ according to Equation (1) using re-parameterization trick, which has included parameterized cross-modal context $k_{t} \boldsymbol{r}_{\phi}\left(\boldsymbol{x}_{0}, \boldsymbol{c}, t\right)$, and then passes $\boldsymbol{x}_{t}$ into the denoising network. The gradients will be propagated to $\boldsymbol{r}_{\phi}$ from the denoising network, and our context-aware adapter and denoising network are jointly optimized in training.

Context-Aware Sampling During sampling, we use the denoising network to predict $\hat{\boldsymbol{x}}_{0}$, and the predicted context-aware adaptation $\boldsymbol{r}_{\phi}\left(\hat{\boldsymbol{x}}_{0}, \boldsymbol{c}, t\right)$ is then used to contextualize the sampling trajectory. Hence the gaussian kernel $p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{c}\right)$ has mean:

$$
\begin{equation*}
\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \hat{\boldsymbol{x}}_{0}+\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}}\left(\boldsymbol{x}_{t}-b_{t}\left(\hat{x}_{0}, c\right)\right)+b_{t-1}\left(\hat{x}_{0}, c\right) \tag{7}
\end{equation*}
$$

where $\boldsymbol{b}_{t}\left(\hat{\boldsymbol{x}}_{0}, \boldsymbol{c}\right)=k_{t} \boldsymbol{r}_{\phi}\left(\hat{\boldsymbol{x}}_{0}, c, t\right)$, and variance $\frac{\left(1-\bar{\alpha}_{t-1}\right) \beta_{t}}{1-\bar{\alpha}_{t}} \boldsymbol{I}$. In this way, our CONTEXTDIFF can effectively adapt sampling process with cross-modal context, which is more informative and contextual guided compared to traditional text guidance (Rombach et al., 2022; Saharia et al., 2022b). Next, we will introduce how to generalize our contextualized diffusion to DDIMs for fast sampling.

### 3.3 GENERALIZING CONTEXTUALIZEd DIFFUSION TO DDIMS

DDIMs (Song et al., 2020) accelerate the reverse process of pretrained DDPMs, which are also faced with the inconsistency problem that exists in DDPMs. Therefore, we address this problem by

“A bustling city street during a rainy day with reflections on wet pavement."
![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-06.jpg?height=308&width=944&top_left_y=214&top_left_x=732)

" There is an old typewriter and a sheet of paper with four letters 'LOVE' on the table."
![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-06.jpg?height=306&width=944&top_left_y=519&top_left_x=734)

“A film camera , with a roll of film partially exposed."
![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-06.jpg?height=308&width=944&top_left_y=820&top_left_x=732)

“ A cup of coffee is put on a table in a coffee shop with an arched ceiling, vintage furniture."
![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-06.jpg?height=310&width=942&top_left_y=1132&top_left_x=736)

"A market with textiles, aromatic spices under the sun."
![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-06.jpg?height=312&width=946&top_left_y=1443&top_left_x=732)

LDM

Imagen

Ours

Figure 4: Synthesis examples demonstrating text-to-image capabilities of for various text prompts with LDM, Imagen, and ConteXTDifF (Ours). Our model can better express the semantics of the texts marked in blue. We use red boxes to highlight critical fine-grained parts where LDM and Imagen fail to align with texts. For example, in second row, only our method successfully generates the four letters spelling "LOVE". In third row, we generate the specific detail of a film roll, while other methods lose this detail.

generalizing our contextualized diffusion to DDIMs. Specifically, we define a posterior distribution $q_{\phi}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}_{0}, \boldsymbol{c}\right)$ for each timestep, thus the forward diffusion process has the desired distribution:

$$
\begin{equation*}
q_{\phi}\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{0}, \boldsymbol{c}\right)=\mathcal{N}\left(\boldsymbol{x}_{t}, \sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}+b_{t}\left(\boldsymbol{x}_{0}, \boldsymbol{c}\right),\left(1-\bar{\alpha}_{t}\right) \boldsymbol{I}\right) \tag{8}
\end{equation*}
$$

If the posterior distribution is defined as:

$$
\begin{equation*}
q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}_{0}, \boldsymbol{c}\right)=\mathcal{N}\left(\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_{0}+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_{t}^{2}} * \frac{\boldsymbol{x}_{t}-\sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}}{\sqrt{1-\bar{\alpha}_{t}}}, \sigma_{t}^{2} \boldsymbol{I}\right) \tag{9}
\end{equation*}
$$

then the mean of $q_{\phi}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{0}, \boldsymbol{c}\right)$ is (proof in Appendix A.1):

$$
\begin{equation*}
\sqrt{\bar{\alpha}_{t-1}} x_{0}+b_{t}\left(x_{0}, c\right) * \frac{\sqrt{1-\bar{\alpha}_{t-1}-\sigma_{t}^{2}}}{\sqrt{1-\bar{\alpha}_{t}}} \tag{10}
\end{equation*}
$$

To match the forward diffusion, we need to replace the adaptation $k_{t} \boldsymbol{r}_{\phi}\left(\boldsymbol{x}_{0}, \boldsymbol{c}, t\right) * \frac{\sqrt{1-\bar{\alpha}_{t-1}-\sigma_{t}^{2}}}{\sqrt{1-\bar{\alpha}_{t}}}$ with $k_{t-1} \boldsymbol{r}_{\phi}\left(\boldsymbol{x}_{0}, \boldsymbol{c}, t-1\right)$. Given $\sigma_{t}^{2}=0$, the sampling process becomes deterministic:

$$
\begin{align*}
& \tilde{\boldsymbol{x}}_{t-1}=\sqrt{\bar{\alpha}_{t-1}} \hat{\boldsymbol{x}}_{0}+\sqrt{1-\bar{\alpha}_{t-1}} * \frac{\boldsymbol{x}_{t}-\sqrt{\bar{\alpha}_{t}} \hat{\boldsymbol{x}}_{0}}{\sqrt{1-\bar{\alpha}_{t}}}  \tag{11}\\
& \boldsymbol{x}_{t-1}=\tilde{\boldsymbol{x}}_{t-1}-b_{t}\left(\hat{x}_{0}, c\right) * \frac{\sqrt{1-\bar{\alpha}_{t-1}}}{\sqrt{1-\bar{\alpha}_{t}}}+b_{t-1}\left(\hat{x}_{0}, c\right)
\end{align*}
$$

In this way, DDIMs can better convey textual semantics in generated samples when accelerating the sampling of pretrained DDPMs, which will be evaluated in later text-to-video editing task.

## 4 EXPERIMENTS

We conduct experiments on two text-guided visual synthesis tasks: text-to-image generation, and text-to-video editing. For the text-to-image generation task, we aim to generate images given textual descriptions. For the text-to-video editing task, we aim to edit videos given textual descriptions. We evaluate the semantic alignment between text conditions and synthesis results in the two tasks with qualitative and quantitative comparisons, and evaluate temporal consistency on text-to-video editing task. We further evaluate the generalization ability and model convergence of our CONTEXTDIFF.

### 4.1 TEXT-TO-IMAGE GENERATION

Datasets and Metrics. Following Rombach et al. (2022); Saharia et al. (2022b), we use public LAION-400M (Schuhmann et al., 2021), a dataset with CLIP-filtered 400 million image-text pairs for training CONTEXTDIFF. We conduct evaluations with FID and CLIP score (Hessel et al., 2021; Radford et al., 2021), which aim to assess the generation quality and resulting image-text alignment.

Implementation Details. For our context-aware adapter, we use text CLIP and image CLIP (Radford et al., 2021) (ViT-B/32) to encode text and image inputs, and adopt multi-head cross attention (Vaswani et al., 2017) to model cross-modal interactions with 8 parallel attention layers. For the diffusion backbone, we mainly follow Imagen (Saharia et al., 2022b) using a $64 \times 64$ base diffusion model (Nichol \& Dhariwal, 2021; Saharia et al., 2022a) and a super-resolution diffusion models to upsample a $64 \times 64$ generated image into a $256 \times 256$ image. For $64 \times 64 \rightarrow 256 \times 256$ super-resolution, we use the efficient U-Net model in Imagen for improving memory efficiency. We condition on the entire sequence of text embeddings (Raffel et al., 2020) by adding cross attention (Ramesh et al., 2022) over the text embeddings at multiple resolutions. More details about the hyper-parameters can be found in Appendix C.

Quantitative and Qualitative Results Following previous works (Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022b), we make quantitative evaluations CONTEXTDIFF on the MSCOCO dataset using zero-shot FID score, which measures the quality and diversity of generated images. Similar to Rombach et al. (2022); Ramesh et al. (2022); Saharia et al. (2022b), 30,000 images are randomly selected from the validation set for evaluation. As demonstrated in Tab. 1, our CONTEXTDIFF achieves a new state-of-the-art performance on text-to-image generation task with 6.48 zero-shot FID score, outperforming previous dominant diffusion models such as Stable Diffusion (Rombach et al., 2022), DALL-E 2 (Ramesh et al., 2022), and Imagen (Saharia et al., 2022b). We also make qualitative comparisons in Figure 4, and find that our CONTEXTDIFF can achieve more precise semantic alignment between text prompt and generated image than previous methods, demonstrating the effectiveness of incorporating cross-modal context into diffusion models.

### 4.2 TEXT-TO-VIDEO EDITING

Datasets and Metrics To demonstrate the strength of our CONTEXTDifF for text-to-video edting, we use 42 representative videos taken from DAVIS dataset (Pont-Tuset et al., 2017) and other in-thewild videos following previous works (Wu et al., 2022; Qi et al., 2023; Bar-Tal et al., 2022; Esser et al., 2023). These videos cover a range of categories including animals, vehicles, and humans. To obtain video footage, we use BLIP-2 (Li et al., 2023) for automated captions. We also use their

Table 1: Quantitative results in text-to-image generation with FID score on MS-COCO dataset for $256 \times 256$ image resolution.

| Approach | Model Type | FID-30K | Zero-shot <br> FID-30K |
| :--- | :---: | :---: | :---: |
| DF-GAN (Tao et al., 2022) | GAN | 21.42 | - |
| DM-GAN + CL (Ye et al., 2021) | GAN | 20.79 | - |
| LAFITE (Zhou et al., 2022) | GAN | 8.12 | - |
| Make-A-Scene (Gafni et al., 2022) | Autoregressive | 7.55 | - |
| DALL-E (Ramesh et al., 2021) | Autoregressive | - | 17.89 |
| Stable Diffusion (Rombach et al., 2022) | Continuous Diffusion | - | 12.63 |
| GLIDE (Nichol et al., 2022) | Continuous Diffusion | - | 12.24 |
| DALL-E 2 (Ramesh et al., 2022) | Continuous Diffusion | - | 10.39 |
| Improved VQ-Diffusion (Tang et al., 2022) | Discrete Diffusion | - | 8.44 |
| Simple Diffusion (Hoogeboom et al., 2023) | Continuous Diffusion | - | 8.32 |
| Imagen (Saharia et al., 2022b) | Continuous Diffusion | - | 7.27 |
| Parti (Yu et al., 2022) | Autoregressive | - | 7.23 |
| Muse (Chang et al., 2023) | Non-Autoregressive | - | 7.88 |
| eDiff-I (Balaji et al., 2022) | Continuous Diffusion | - | 6.95 |
| ERNIE-ViLG 2.0 (Feng et al., 2023) | Continuous Diffusion | - | 6.75 |
| RAPHAEL (Xue et al., 2023) | Continuous Diffusion | - | 6.61 |
| ConTEXTDIFF | Continuous Diffusion | - | $\mathbf{6 . 4 8}$ |

Table 2: Quantitative results in text-to-video editing. Text. and Temp. denote CLIP-text and CLIP-temp, respectively. User study shows the preference rate of CONTEXTDIFF against baselines via human evaluation.

| Method | Metric |  | User Study |  |
| :---: | :---: | :---: | :---: | :---: |
|  | Text. $\uparrow$ | Temp. $\uparrow$ | Text. $(\%) \uparrow$ | Temp.(\%)个 |
| Tune-A-Video (Wu et al., 2022) | 0.260 | 0.934 | 91 | 84 |
| FateZero (Qi et al., 2023) | 0.252 | 0.954 | 84 | 75 |
| ControlVideo (Zhao et al., 2023) | 0.258 | 0.961 | 81 | 73 |
| CONTEXTDIFF | 0.274 | 0.970 | - | - |

designed prompts for each video, including object editing, background changes, and style transfers. To measure textual alignment, we compute average CLIP score between all frames of output videos and corresponding edited prompts. For temporal consistency, we compute CLIP (Radford et al., 2021) image embeddings on all frames of output videos and report the average cosine similarity between all pairs of video frames. Moreover, We perform user study to quantify text alignment, and temporal consistency by pairwise comparisons between the baselines and our CONTEXTDIFF. A total of 10 subjects participated in this user study. Taking text alignment as an example, given a source video, the participants are instructed to select which edited video is more aligned with the text prompt in the pairwise comparisons between the baselines and CONTEXTDIFF.

Implementation Details In order to reproduce the baselines of Tune-A-Video (Wu et al., 2022), FateZero (Qi et al., 2023), and ControlVideo (Zhao et al., 2023), we use their official repositories for one-shot video tuning. Following FateZero, we use the trained Stable Diffusion v1.4 (Rombach et al., 2022) as the base text-to-image diffusion model, and fuse the attention maps in DDIM inversion (Song et al., 2020) and sampling processes for retaining both structural and motion information. We fuse the attentions in the interval of $t \in[0.5 \times T, T]$ of the DDIM step with total timestep $\mathrm{T}=20$. For context-aware adapter, we use the same encoders and cross attention as in text-to-image generation. We additionally incorporate spatio-temporal attention, which includes spatial self-attention and temporal causal attention, into our context-aware adapter for capturing spatio-temporal consistency. For each source video, we tune our adapter using source text prompt for learning both context-aware structural and motion information, and use the learned adapter to conduct video editing with edited text prompt. Details about the hyper-parameters are in Appendix C.

Quantitative and Qualitative Results We report our quantitative and qualitative results in Tab. 2 and Figure 2. Extensive results demonstrate that CONTEXTDIFF substantially outperforms all these
![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-09.jpg?height=426&width=1396&top_left_y=290&top_left_x=364)

[Tuna-A-Video+Context-Aware Adapter]

[Tuna-A-Video+Context-Aware Adapter]

![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-09.jpg?height=208&width=1391&top_left_y=747&top_left_x=367)

Figure 5: Generalizing our context-aware adapter to Tune-A-Video (Wu et al., 2022).

baselines in both textual alignment and temporal consistency. Notably, in the textual alignment in user study, we outperform the baseline by a significant margin (over $80 \%$ ), demonstrating the superior cross-modal understanding of our contextualized diffusion. In qualitative comparisons, we observe that CONTEXTDifF not only achieves better semantic alignment, but also preserves the structure information in source video. Besides, the context-aware adapter in our contextualized diffusion can be generalized to previous methods, which substantially improves the generation quality as in Figure 5. More results demonstrating our generalization ability can be found in Appendix D.

### 4.3 GENERALIZING TO CLASS-TO-IMAGE AND LAYOUT-TO-IMAGE GENERATION

We generalize our context-aware adapter into class and layout conditional generation tasks. We replace the text encoder in original adapter with ResNet blocks for embedding classes or layouts, and keep the original image encoder and cross-attention module for obtaining cross-modal context information. We put both quantitative and qualitative results in Tabs. 3 and 4 and Figures 6 and 7. From the results, we conclude that our context-aware adapter can benefit the conditional diffusion models with different condition modalities and enable more realistic and precise generation consistent with input conditions, demonstrating the satisfying generalization ability of our method.

Table 3: Performance comparison in class-to-image generation on ImageNet $256 \times 256$.

| Method | FID $\downarrow$ | IS $\uparrow$ | Precision $\uparrow$ | Recall $\uparrow$ |
| :--- | :---: | :---: | :---: | :---: |
| BigGAN (Brock et al., 2018) | 6.95 | 203.63 | 0.87 | 0.28 |
| ADM-G (Dhariwal \& Nichol, 2021) | 4.59 | 186.70 | 0.82 | 0.52 |
| LDM (Rombach et al., 2022) | 3.60 | 247.67 | 0.87 | 0.48 |
| LDM+Context-Aware Adapter | $\mathbf{2 . 9 7}$ | $\mathbf{2 7 3 . 0 4}$ | $\mathbf{0 . 8 9}$ | $\mathbf{0 . 5 5}$ |

Table 4: FID performance comparison in layout-to-image generation on MS-COCO $256 \times 256$.

| Method | FID $\downarrow$ |
| :--- | :---: |
| VQGAN+T (Jahn et al., 2021) | 56.58 |
| Frido (Fan et al., 2023) | 37.14 |
| LDM (Rombach et al., 2022) | 40.91 |
| LDM+Context-Aware Adapter | $\mathbf{3 4 . 5 8}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-10.jpg?height=848&width=1371&top_left_y=232&top_left_x=388)

Figure 6: Qualitative results in class-to-image generation on ImageNet $256 \times 256$. Our context-aware adapter improves the generation quality of LDM.

![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-10.jpg?height=850&width=1262&top_left_y=1260&top_left_x=450)

Layout-to-image generation
![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-10.jpg?height=818&width=1242&top_left_y=1287&top_left_x=448)

Figure 7: Qualitative results in layout-to-image generation on MS-COCO $256 \times 256$. Our contextaware adapter improves both fidelity and precision of the generation results of LDM. We use red boxes to highlight critical fine-grained parts where original LDM fails to align with conditional layout. Our method substantially improves both quality and precision of the generation results.

### 4.4 MODEL ANALYSIS

Visual Analysis on Context Awareness of Our Model We conduct visual analysis to investigate how our context-aware adapter works in text-guided visual synthesis. As illustrated in Figure 8 and

Figure 9 , we visualize the heatmaps of text-image cross-attention module in the sampling process of each frame image. We find that our context-aware adapter can enable the model to better focus on the fine-grained semantics in text prompt and sufficiently convey them in final generation results. Because incorporating textual information into diffusion process of the image benefits the crossmodal understanding for image diffusion models.

![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-11.jpg?height=843&width=1355&top_left_y=500&top_left_x=385)

Figure 8: Our context-aware adapter improves the shape awareness of diffusion models in textguided video editing.

![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-11.jpg?height=865&width=1371&top_left_y=1492&top_left_x=385)

Figure 9: Our context-aware adapter can improve the background awareness of diffusion models in text-guided video editing.

Guidance Scale $\boldsymbol{v s .}$. FID Given the significance of classifier-free guidance weight in controlling image quality and text alignment, in Figure 10, we conduct ablation study on the trade-off between CLIP and FID scores across a range of guidance weights, specifically 1.5, 3.0, 4.5, 6.0, 7.5, and 9.0. The results indicate that our context-aware adapter contribute effectively. At the same guidance weight, our context-aware adapter considerably and consistently reduces the FID, resulting in a significant improvement in image quality.

Training Convergence We evaluate CONTEXTDIFF regarding our contribution to the model convergence. The comparison in Figure 11 demonstrates that our context-aware adapter can significantly accelerate the training convergence and improve the semantic alignment between text and generated video. This observation also reveals the generalization ability of our contextualized diffusion.

![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-12.jpg?height=388&width=507&top_left_y=809&top_left_x=370)

Figure 10: The trade-off between FID and CLIP scores for LDM and LDM with our context-aware adapter.

![](https://cdn.mathpix.com/cropped/2024_06_04_10c9d5292e29a23ad0c4g-12.jpg?height=387&width=851&top_left_y=804&top_left_x=924)

Figure 11: The comparison of model convergence between Tune-A-Video and Tune-AVideo + our context-aware adapter.

## 5 CONCLUSION

In this paper, we propose a novel and general conditional diffusion model (CONTEXTDIFF) by propagating cross-modal context to all timesteps in both diffusion and reverse processes, and adapt their trajectories for facilitating the model capacity of cross-modal synthesis. We generalize our contextualized trajectory adapter to DDPMs and DDIMs with theoretical derivation, and consistently achieve state-of-the-art performance in two challenging tasks: text-to-image generation, and textto-video editing. Extensive quantitative and qualitative results on the two tasks demonstrate the effectiveness and superiority of our proposed cross-modal contextualized diffusion models.

## REFERENCES

Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.

Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Textdriven layered image and video editing. In European Conference on Computer Vision, pp. 707723. Springer, 2022.

Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2018.

Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.

Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, volume 34, pp. 8780-8794, 2021.

Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with criticallydamped langevin diffusion. In International Conference on Learning Representations, 2021.

Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011, 2023.

Wan-Cyuan Fan, Yen-Chun Chen, DongDong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank Wang. Frido: Feature pyramid diffusion for complex scene image synthesis. In The AAAI Conference on Artificial Intelligence, volume 37, pp. 579-587, 2023.

Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 10135-10145, 2023.

Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131, 2022.

Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7514-7528, 2021.

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pp. 6840-6851, 2020.

Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.

Emiel Hoogeboom and Tim Salimans. Blurring diffusion models. In The Eleventh International Conference on Learning Representations, 2022.

Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. arXiv preprint arXiv:2301.11093, 2023.

Manuel Jahn, Robin Rombach, and Björn Ommer. High-resolution complex scene synthesis with transformers. arXiv preprint arXiv:2105.06458, 2021.

Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in Neural Information Processing Systems, 35:26565-26577, 2022.

Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023.

Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, and Il-chul Moon. Maximum likelihood training of implicit nonlinear diffusion model. Advances in Neural Information Processing Systems, 35:32270-32284, 2022.

Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In Advances in Neural Information Processing Systems, volume 34, pp. 21696-21707, 2021.

Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, and Tie-Yan Liu. Priorgrad: Improving conditional denoising diffusion models with data-dependent adaptive prior. In International Conference on Learning Representations, 2021.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, 2023.

Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. arXiv preprint arXiv:2303.04761, 2023.

Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.

Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023.

Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pp. 8162-8171, 2021.

Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, pp. 16784-16804. PMLR, 2022a.

Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, pp. $16784-16804,2022 b$.

Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.

Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017.

Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Gradtts: A diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, pp. 8599-8608. PMLR, 2021.

Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In IEEE International Conference on Computer Vision, 2023.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763, 2021.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.

Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8821-8831. PMLR, 18-24 Jul 2021.

Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022.

Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pp. 1-10, 2022a.

Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479-36494, 2022b.

Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.

Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256-2265, 2015.

Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020.

Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, volume 32, 2019.

Zhicong Tang, Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen. Improved vector quantized diffusion models. arXiv preprint arXiv:2205.16007, 2022.

Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A simple and effective baseline for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16515-16525, 2022.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zeroshot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2023.

Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022.

Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. arXiv preprint arXiv:2305.18295, 2023.

Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, and CUI Bin. Improving diffusion-based image synthesis with context prediction. In Advances in Neural Information Processing Systems, 2023a.

Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1-39, $2023 \mathrm{~b}$.

Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, and Bin Cui. Structure-guided adversarial training of diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2024a.

Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering textto-image diffusion: Recaptioning, planning, and generating with multimodal llms. arXiv preprint arXiv:2401.11708, 2024b.

Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving text-toimage synthesis using contrastive learning. arXiv preprint arXiv:2107.02423, 2021.

Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.

Qinsheng Zhang and Yongxin Chen. Diffusion normalizing flow. In Advances in Neural Information Processing Systems, volume 34, pp. 16280-16291, 2021.

Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, and Bin Cui. Realcompo: Dynamic equilibrium between realism and compositionality improves text-to-image diffusion models. arXiv preprint arXiv:2402.12908, 2024.

Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and Jun Zhu. Controlvideo: Adding conditional control for one shot text-to-video editing. arXiv preprint arXiv:2305.17098, 2023.

Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1790717917, 2022.
