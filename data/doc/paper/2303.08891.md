# VITO: VISION TRANSFORMER-OPERATOR 

Oded Ovadia, Eli Turkel<br>Department of Applied Mathematics<br>Tel Aviv University<br>Tel Aviv<br>odedovadia@mail.tau.ac.il, eliturkel@gmail.com<br>Adar Kahana<br>Division of Applied Mathematics<br>Brown University<br>Providence, RI<br>adar_kahana@brown.edu<br>George Em Karniadakis<br>Division of Applied Mathematics<br>Brown University<br>Providence, RI<br>and<br>Advanced Computing, Mathematics and Data Division<br>Pacific Northwest National Laboratory<br>Richland, WA<br>george_karniadakis@brown.edu<br>Panos Stinis<br>Advanced Computing, Mathematics and Data Division<br>Pacific Northwest National Laboratory<br>Richland, WA<br>panagiotis.stinis@pnnl.gov


#### Abstract

We combine vision transformers with operator learning to solve diverse inverse problems described by partial differential equations (PDEs). Our approach, named ViTO, combines a U-Net based architecture with a vision transformer. We apply ViTO to solve inverse PDE problems of increasing complexity, namely for the wave equation, the Navier-Stokes equations and the Darcy equation. We focus on the more challenging case of super-resolution, where the input dataset for the inverse problem is at a significantly coarser resolution than the output. The results we obtain are comparable or exceed the leading operator network benchmarks in terms of accuracy. Furthermore, ViTO's architecture has a small number of trainable parameters (less than $10 \%$ of the leading competitor), resulting in a performance speed-up of over $5 \mathrm{x}$ when averaged over the various test cases.


Keywords Deep learning $\cdot$ Vision transformers $\cdot$ Scientific machine learning $\cdot$ Inverse problems $\cdot$ Super-resolution

## 1 Introduction

Operator learning refers to training neural networks to represent mappings between families of functions. For example, if we want to infer the acoustic wave pressure in the ocean for each and every initial source, we can define the operator as the mapping from the initial source (a function, initial pressure in every point of the domain) to the pressures at a later time (also a function, future pressures in every point of the domain). The main advantage of operator learning is that, after the operator has been learned, no further training is needed and the solution, e.g., for a partial differential equation (PDE), which can be very expensive using classic methods, can be simply inferred (estimated) by the network with negligible computational cost in real-time. The first operator learning method was introduced by Lu et al., named the Deep Operator Network (DeepONet) [1]. DeepONet is composed of a branch and a trunk; the branch learns the input function space of the operator, while the trunk learns the space of functions onto which the output is projected. By multiplying the branch with the trunk, the projection provides a representation of the output function space. Another popular invention is the Fourier Neural Operator [2, 3], which is based on replacing the kernel integral operator with a convolution operator defined in Fourier space by employing a fast Fourier transform on the input space. It uses a ResNet but does not have a trunk net.

When modeling a physical system/experiment we usually find one of two scenarios. The first scenario is the forward problem, when given a set of conditions, one attempts to simulate the physical process. For example, given an initial

ViTO: Vision Transformer-Operator

source, find the acoustic wave amplitude in the ocean after some time. The second scenario is the opposite one, called the inverse problem. Given the state of the physical experiment, find the causal condition that led to that state. For example, given measurements of the acoustic pressures in the ocean at some time instant, find the source that started emitting the acoustic sounds. The inverse problem is often considered more challenging since one has access to limited data (for example, recordings at a small set of sensors only), the data may be noisy and of low resolution, some recordings may be missing, etc. Inverse problems are often ill-posed, meaning they do not necessarily have a solution and if so, it may not necessarily be unique. In this work, we focus on inverse problems given relatively sparse data sets.

A recent innovation in the field of deep learning is the so-called transformers [4], which refer to deep neural networks that have an attention mechanism. The attention module attempts to understand context from the given input. To learn the context, the attention mechanism operates on a discrete embedding of the data that is composed of tokens. An immediate example is Natural Language Processing (NLP) using transformers, where a sentence is embedded using tokens according to a specific vocabulary. A challenge that arises when using transformers is considering non-discrete data. For example, instead of sentences (sequence of words), we would like to embed a continuous function or signal. The literature offers methods to achieve that, as introduced in the following section. One prominent method, explored in the current work, is called Vision Transformers (ViT) [5]. The vision transformers receive an image as input, for example the initial condition of a system (in the forward problem), or a future state of a system (in the inverse problem). Then, the image is split into small regions, often referred to as patches, and each region acts as a token. The vision transformer extracts the context from the tokens (regions of the image), and thus is able to utilize the attention mechanism for the continuous signal and make accurate predictions. In the original ViT paper [5], the authors demonstrated how ViT outperforms the state-of-the-art (SOTA) methods for image classification, including ImageNet [6], CIFAR [7], Oxford pets and flowers [8], and VTAB [9]. In addition, the benchmark ViT model is up to four times more efficient than the SOTA methods used as reference. In [10], Okolo et. al. used a ViT for X-ray image classification, while in [11] it was used for unsupervised volumetric medical image registration. ViTs are currently being adopted in various areas of research.

In the current work, we are interested in using transformers for operator learning. There have been some recent attempts to use various types of transformers for operator learning [12, 13, 14, 15, 16]. For example, in [12], Li et. al. use transformers to approximate forward solutions of PDEs with operator learning. They propose an innovative way for choosing the collocation points and iterate through time to find the solution at those points. The attention mechanism is split in such a way that the query, key and value are split into different parts of the forward pass and is implemented as a multi-layer perceptron (MLP). The latent encoding, which is the outcome of the combination of the three components, is the embedding of the coordinates used for the spatio-temporal input of the PDE. In [15], Cao presents a method to combine FNO with attention to improve the performance for PDE solutions, by replacing the softmax (often used in transformers especially in classification) with a linear variant of it that does not involve normalization.

Here, we introduce a novel way to perform operator learning using vision transformers combined with a U-net [17, 18] based architecture to design the Vision Transformer-Operator or ViTO. We apply ViTO to solve inverse problems of increasing complexity, obtaining the solution at high resolution using only sparse and low resolution data. Compared to SOTA results for operator learning, our current results exceed the leading operator network benchmarks in terms of accuracy, and they are also obtained at a significant speedup.

The paper is organized as follows. Section 2 presents the proposed methodology. Section 3 presents numerical results for a collection of inverse problems of increasing complexity. Section 4 offers a discussion of the results and directions for future work.

## 2 Methodology

### 2.1 Mathematical formulation of operator learning

We first present the general problem formulation of operator learning for PDEs before focusing on the particular case of inverse problems. We follow the DeepONet theory and notation, as given by [1, 19].

### 2.1.1 PDE operators

Typically, when tackling a forward PDE problem, our objective is to determine the solution to the PDE. Thus, we aim to approximate the PDE solution by utilizing a set of parameters that describe the PDE problem setup. Such parameters include initial and boundary conditions, forcing terms, and other physical characteristics that may vary between different PDEs. Hence, forward PDE problems can be formulated as a mapping between an input function that corresponds to these parameters to an output function representing the solution.

Mathematically, let $v$ denote the input function defined on some physical domain $D \in \mathbb{R}^{d}$ and $u$ denote the corresponding output function defined on the physical domain $D^{\prime} \in \mathbb{R}^{d^{\prime}}$ :

$$
\begin{gathered}
v: D \ni x \longmapsto v(x) \in \mathbb{R} \\
u: D^{\prime} \ni \xi \longmapsto u(\xi) \in \mathbb{R}
\end{gathered}
$$

Let $\mathcal{V}$ and $\mathcal{U}$ be the spaces of the functions $v$ and $u$, respectively. Then, the mapping from $v$ to $u$ is defined by an operator $\mathcal{G}$ :

$$
\mathcal{G}: \mathcal{V} \ni v \longmapsto u \in \mathcal{U}
$$

This operator describes the forward problem. For example, in many applications $v$ is the initial condition of the PDE and $u$ is its solution at some final time. However, in this work we are interested in the inverse problem. So, the operator corresponding to the inverse problem is of the form:

$$
\begin{equation*}
\tilde{\mathcal{G}}: \mathcal{U} \ni u \longmapsto v \in \mathcal{V} \tag{1}
\end{equation*}
$$

Continuing the previous example, the relevant inverse problem would be to retrieve the initial condition of the PDE given a snapshot of its solution.

We note that in many cases, the inverse operator relates to an ill-posed problem [20]. This type of problem is generally considered more challenging, particularly when dealing with incomplete or noisy data.

### 2.1.2 Super-resolution

For most applications, it is impossible to get the full analytical solution of a PDE. In some cases, it is even hard to get a discrete approximation of the solution on a fine mesh due to computational, physical, or experimental difficulties. This is especially common in the domain of inverse problems, where the input function is often derived from sensor measurements of physical phenomena. These considerations often lead to a low-resolution mesh for the discrete approximation.

Low-resolution data is challenging to use. The main goal of Super-Resolution (SR) methods is to produce highresolution accurate results given low-resolution input data. In this work, we do not treat the SR aspect as a separate problem. Instead, we combine it with the inverse operator defined in the previous section 2.1.1 to form a unified inverse-SR framework. Using the same notation as before, instead of getting functions $u \in \mathcal{U}, v \in \mathcal{V}$, we get discrete approximations of these functions on meshes. However, $u \in \mathcal{U}$ is discretized using a much coarser mesh in comparison to $v \in \mathcal{V}$. For example, the input might be a low-resolution snapshot of a PDE solution, while the desired output would be a high resolution discretization of the initial condition.

### 2.2 Data driven formulation of operator learning

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-03.jpg?height=57&width=1645&top_left_y=1798&top_left_x=240)
using a ViT-based neural network. We define a dataset, where each sample is composed of pairs of discretized functions: $\mathcal{T}=\left\{\left(\mathrm{u}^{(1)}, \mathrm{v}^{(1)}\right),\left(\mathrm{u}^{(2)}, \mathrm{v}^{(2)}\right), \ldots,\left(\mathrm{u}^{(N)}, \mathrm{v}^{(N)}\right)\right\}$ such that $\forall n, \mathrm{u}^{(n)}$ and $\mathrm{v}^{(n)}$ are the projections of functions $u^{(n)} \in \mathcal{U}$ and $v^{(n)} \in \mathcal{V}$ onto discrete meshes $\mathcal{M}_{u}$ and $\mathcal{M}_{v}$, respectively. We let $\mathcal{M}_{u}$ and $\mathcal{M}_{v}$ remain constant for all samples, and assume that the discretization is equispaced. For ease of notation and without loss of generality, we assume that the domain coordinates are positive. Then, in the two-dimensional case, which is used for all of the numerical experiments, these meshes are written as:

$$
\begin{aligned}
& \mathcal{M}_{u}=\left\{\left(i \Delta_{x, u}, \quad j \Delta_{y, u}\right) \mid i, j \in \mathbb{N}, i \leq N_{x, u}, j \leq N_{y, u}\right\} \\
& \mathcal{M}_{v}=\left\{\left(i \Delta_{x, v}, j \Delta_{y, v}\right) \mid i, j \in \mathbb{N}, i \leq N_{x, v}, j \leq N_{y, v}\right\}
\end{aligned}
$$

where $\Delta_{x, .,} \Delta_{y, .}, N_{x, \cdot}, N_{y,}$. determine the resolution of the discretization. We use $\# \mathcal{M}_{u}=N_{x, u} \cdot N_{y, u}$ to note the number of points in the set $\mathcal{M}_{u}$.

If $\mathcal{M}_{u}$ and $\mathcal{M}_{v}$ have the same number of points ( $\# \mathcal{M}_{u}=\# \mathcal{M}_{v}$.), SR is not performed, and instead, we are describing a standard inverse problem. However, as highlighted in the previous section 2.1.2, the focus is on scenarios where the input grid is significantly coarser than the output grid, i.e., $\# \mathcal{M}_{u} \ll \# \mathcal{M}_{v}$. To achieve this, we can choose

ViTO: Vision Transformer-Operator

a super-resolution factor $s>1$ that specifies the relationship between the discretization. We choose $N_{\cdot, v}$ such that $N_{x, v}=s N_{x, u}$ and $N_{y, v}=s N_{y, u}$ to attain a SR factor of $s$. This results in the output grid size becoming $\# \mathcal{M}_{v}=s^{2} N_{x, u} N_{y, u}$. For large values of $s$, this renders the problem considerably more challenging.

### 2.3 Network architecture

A modified version of the TransUNet [18] architecture is employed in this study, wherein a U-Net [17] backbone is integrated with a ViT (see Figure 1). The U-Net model comprises an encoder-decoder structure with interconnecting skip connections. The U-Net architecture has emerged as a powerful technique in the computer vision field, particularly in the realm of segmentation problems. Given the image-to-image nature of the data-driven problem outlined in 2.2 . utilizing segmentation tools is a natural choice.

The network has two inputs: the observed solution of the PDE $u$, and its corresponding numerical grid $\mathcal{M}_{u}$. In the two-dimensional case, $u$ is represented by a two-dimensional matrix, where all elements are values of the $u$ at points on the grid $\mathcal{M}_{u}$. We want the network to be exposed to the grid itself, so it can learn some relation between the $(x, y)$ values of a grid point and their corresponding solution value $u(x, y)$. We achieve this by a simple encoding of the grid using two matrices representing discretizations in the $x$ and $y$ directions, as seen in Figure 2. The $i$-th row of the $x$-matrix is defined as a vector consisting of $x_{i}=i \Delta_{x, u}$ repeated $N_{y, u}$ times. Similarly, the $j$-th column of the $y$-matrix is defined as a column vector consisting of $y_{j}=j \Delta_{y, u}$ repeated $N_{x, u}$ times. We also use bilinear interpolation to modify the sizes of the inputs to the desired output shape. We make sure that the results of the interpolation operation is divisible by 16 , so it could be compatible with the downsampling of the U-Net.

The U-Net architecture we employ is composed of three convolutional blocks for both the encoder and decoder (see Figure 1). Each block is comprised of three convolutional layers, equipped with a residual skip connection [21]. All convolutions are followed by a batch normalization [22] layer and a GELU activation function [23]. The final layer of each block performs either a downsampling operation for the encoder or an upsampling operation for the decoder.

In our approach, we utilize a ViT within the latent space of the U-Net by taking the encoded values as input. As the encoding is significantly smaller than the original inputs due to the U-Net's downsampling nature, we can use a patch size of $1 \times 1$ without any computational difficulties.

The original ViT utilizes absolute positional embedding via a linear projection layer. However, this can be problematic in PDE applications as it requires all inputs to have the same shape. In operator learning, we are often interested in models that can handle inputs of various sizes. To address this issue, we employ a form of relative conditional embedding [24, 25], where we use convolutions to learn the relationships between tokens instead of linearly projecting their absolute positions within the representation in the latent space. Specifically, we use a separable convolutional layer [26] followed by a standard convolutional layer, similar to the approach proposed in [27].

### 2.4 Training loss function

The loss function is defined as the mean relative $L^{2}$ error:

$$
\begin{equation*}
\mathcal{L}=\frac{1}{N} \sum_{j=1}^{N} \frac{\left\|\hat{\mathrm{v}}^{(j)}-\mathrm{v}^{(j)}\right\|_{2}}{\varepsilon+\left\|\mathrm{v}^{(j)}\right\|_{2}} \tag{2}
\end{equation*}
$$

where $N$ is the size of training data, $\mathrm{v}^{(j)}$ is the $j$-th ground-truth sample of the training data, $\hat{\mathrm{v}}^{(j)}$ is the $j$-th sample prediction, and $\varepsilon$ is a small number to prevent a zero denominator and stabilize the loss. Note that the inputs and outputs of the model are two-dimensional, so they are flattened inside the loss function.

## 3 Numerical results

We apply the ViTO method on various ill-posed two-dimensional inverse problems. The conducted tests include three PDEs: the acoustic wave equation, time-dependent incompressible Navier-Stokes, and a steady-state Darcy flow equation. In all cases we compare the results of the ViTO to three other popular methods in the scientific machine learning literature: 1) DeepONet [1], 2) FNO [2], and 3) a standard ResNet [21]. For each method we compute the relative $L^{2}$ error compared to the ground truth data. We also measure relevant information regarding the training time and the number of parameters of used by each method. Details regarding the data generation process can be found in each of the following sections.

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-05.jpg?height=621&width=1420&top_left_y=394&top_left_x=336)

Figure 1: The architecture of the ViTO deep neural network. The inputs are the discretized function $u(x, y)$ and the grid points, concatenated and inserted into U-net convolutional blocks. In the lowest level of the U-net the ViT is employed.

An important consideration in the domain of inverse problems is robustness to noise. Since inverse problems are often ill-posed, even a small amount of noise in the observed data can greatly amplify the numerical error [28]. To test how well ViTO can handle noise, we run all experiments twice: with noise and without noise. In both cases we train the model with the relevant amount of noise. We used zero-mean Gaussian additive noise, which is a common choice. Since different PDEs can behave quite differently, we make sure that the variance of the Gaussian noise is dependent on the input data. The operation of adding noise is given by: $\mathrm{D} \ni x^{(n)} \longmapsto x^{(n)}+\gamma \mathcal{N}\left(0, \sigma_{\mathrm{D}}^{2}\right)$ where $x^{(n)}$ is an input sample in the dataset $\mathrm{D}, \sigma_{\mathrm{D}}^{2}$ is the variance of the entire dataset, and $\gamma$ is the desired noise level, e.g. $\gamma=0.1$ is equivalent to $10 \%$ noise.

For all our evaluations, we utilized a super-resolution scale factor of 8 , which means, for example, that an image of size $16 \times 16$ would be mapped to an image of size $128 \times 128$. It should be noted that a magnification of $\times 8$ is considerably high. Although lower SR factors are utilized in several benchmarking datasets, the choice of $\times 8$ is still prevalent for certain datasets such as Urban100 [29] and the Berkeley Segmentation Dataset [30].

For the FNO we used the same architecture as described in [2] for the FNO-2D network: four Fourier layers with width of 32 and 12 modes. Each layer was followed by a GeLU [23] activation function. For the ResNet we used 3 residual blocks with 16, 32, and 64 filters for each convolution within each block of depth 3. In the DeepONet case we used the ResNet described before as the branch network, 4 hidden fully connected layers for the trunk network, and 256 neurons for the latent dimension.

Following standard machine learning practice, we split all datasets into train, test, and validation sets. During training, we monitor the relative $L^{2}$ losses and save the model with the lowest validation loss. Unless stated otherwise, all models are trained with a batch size of 100 for 500 epochs, subject to an early stopping criterion of 50 consecutive epochs with no validation loss improvement. The optimizer of choice is the Adam/AdamW optimizer [31, 32] with an initial learning rate $10^{-3}$ and weight decay $10^{-4}$. The learning rate is updated throughout the training process using cosine annealing [33]. The main code was implemented in PyTorch [34], and the DeepONet was implemented using DeepXDE [35] with a PyTorch backend. All computations were conducted using a single RTX-4090 GPU.

In Table 1 we present a computational comparison of the four models mentioned above. These results are given for a Darcy problem (see 3.3) experiment with a grid size of $128 \times 128$ and batch size 100. Memory was calculated as the peak GPU memory usage from the beginning of the training process to its end. The iterations per second metric was calculated by measuring the time it took the model to train for a single batch, averaged over 200 batches to increase consistency. Note that we do not report these two metrics for the DeepONet, since the training process was quite different from the other three models, so any direct comparison would have been misleading.
![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-06.jpg?height=550&width=942&top_left_y=424&top_left_x=602)

Figure 2: An example of discrete $X, Y$ grid encoded inputs for a problem defined in $[0,1]^{2}$.

| Method | \# of parameters (M) | Memory (GB) | Iterations per second |
| :---: | :---: | :---: | :---: |
| FNO | 2.376 | 12.74 | 3.45 |
| DeepONet | 0.297 | - | - |
| ResNet | $\mathbf{0 . 1 4 8}$ | 4.66 | 9.25 |
| ViTO | 0.150 | $\mathbf{0 . 8 5}$ | $\mathbf{5 9 . 9 8}$ |

Table 1: Computational performance of the different models.

ViTO was the most efficient model by a substantial margin, both in terms of memory consumption and training time. ViTO was able to produce results on-par with SOTA methods, using a surprisingly small number of trainable parameters. The full details of the ViT-related weights in ViTO are shown in Table 2 Despite having a similar number of parameters to the ResNet, ViTO employs downsampling due to its U-Net architecture. Consequently, many convolutional operations occur on smaller feature maps compared to the ResNet which explains the lower memory usage and running time for ViTO.

| Problem | Transformer blocks | Attention heads | Embedding dimension | ViT MLP size |
| :---: | :---: | :---: | :---: | :---: |
| Wave equation | 2 | 2 | 16 | 128 |
| Navier-Stokes | 4 | 8 | 16 | 64 |
| Darcy Flow | 2 | 2 | 16 | 128 |

Table 2: ViT parameters for each scenario.

### 3.1 Wave equation

The formulation of the acoustic wave equation in two dimensions is given by [36, 37]:

$$
\begin{cases}\ddot{u}(x, y, t)=c^{2}(x, y)\left(u_{x x}(x, y, t)+u_{y y}(x, y, t)\right)+f(x, y, t) & (x, y) \in(0, L)^{2} ; 0 \leq t \leq T  \tag{3}\\ u(x, y, 0)=u_{0}(x, y) & (x, y) \in(0, L)^{2} \\ \dot{u}(x, y, 0)=v_{0}(x, y) & (x, y) \in(0, L)^{2} \\ u(0, y, t)=u(L, y, t)=0 & y \in(0, L), 0 \leq t \leq T \\ u(x, 0, t)=u(x, L, t)=0 & x \in(0, L), 0 \leq t \leq T\end{cases}
$$

ViTO: Vision Transformer-Operator

where $u(x, y, t)$ is the wave amplitude or acoustic pressure, $c(x, y)$ is the wave propagation speed, $f(x, y, t)$ is the source term, $T$ is the final propagation time, $L$ is the size of the physical domain, and $u_{0}(x, y), v_{0}(x, y)$ are the initial pressures and velocities, respectively. The boundary condition is a homogeneous Dirichlet boundary (fully-reflective). The inverse problem is to learn the following mapping:

$$
u(x, y, T) \longmapsto u_{0}(x, y)
$$

We chose a physical domain with $L=\pi$ and propagation time $T=0.001$. We set the initial pressure and velocity to be 0 , and randomly created Gaussian-shaped sources at different locations. For each sample, we created two such Gaussian sources with random amplitudes and locations. The locations are selected using a discrete random uniform distribution on the indices of the grid. The amplitudes are sampled uniformly using $\mathcal{U}(-1,1)$ for each source in each initial condition. The wave velocity was taken as $c(x, y)=c_{0} \sin (x) \sin (y)$, where $c_{0}$ was randomly sampled for each initial condition using a uniform distribution set between 1,300 and 1,600, which is centred around the average acoustic wave propagation speed of 1,484 in the Mediterranean sea.

The dataset was generated using a standard explicit second-order finite-difference scheme [38, 39]. We generated 20,000 samples, of which 16,000 were used as the training set, while the remaining 4,000 were evenly split to form the testing and validation sets.

The results are shown in Table 3 and Figure 3 . ViTO obtains the lowest error compared to the other methods, both with and without noise. It is worth noting that ViTO is able to reconstruct the initial condition even in difficult scenarios where there is a large difference between the amplitudes of the different sources (such as the first row in Figure 3 .

| Method | $0 \%$ noise | $10 \%$ noise |
| :---: | :---: | :---: |
| FNO | 0.3260 | 0.4383 |
| DeepONet | 0.7128 | 0.7131 |
| ResNet | 0.7892 | 0.8154 |
| ViTO | $\mathbf{0 . 2 6 7 8}$ | $\mathbf{0 . 2 9 4 2}$ |

Table 3: Test relative $L^{2}$ errors for the wave problem.

### 3.2 Navier-Stokes equations

The time-dependent two-dimensional Navier-Stokes equation for the viscous, incompressible fluid in vorticity form is given by:

$$
\begin{cases}\partial_{t} \omega(x, y, t)+u(x, y, t) \cdot \nabla \omega(x, y, t)=\nu \Delta \omega(x, y, t)+f(x, y), & x, y \in(0,1)^{2}, t \in(0, T]  \tag{4}\\ \nabla \cdot u(x, y, t)=0, & (x, y) \in(0,1)^{2}, t \in(0, T] \\ \omega(x, y, 0)=\omega_{0}, & (x, y) \in(0,1)^{2}\end{cases}
$$

where $\omega$ is the vorticity, $u$ is the velocity field, $\nu=10^{-3}$ is the viscosity, and $\Delta$ is the two-dimensional Laplacian. We consider periodic boundary conditions. The source term $f$ is set as: $f(x, y)=0.1(\sin (2 \pi(x+y))+\cos (2 \pi(x+y)))$, and the initial condition $\omega_{0}(x)$ is sampled from a Gaussian random field according to the following distribution: $\mathcal{N}\left(0,7^{3 / 2}(-\Delta+49 I)^{-5 / 2}\right)$. The inverse problem is to learn the following mapping:

$$
\omega(x, y, T) \longmapsto \omega_{0}(x, y)
$$

We used the publicly available Python solver given in [2] to create two separate datasets with different final simulation times $T=1$ and $T=5$. Each dataset was composed of 10,000 samples, which we then split into train, test, and validation. The error analysis for $T=1,5$ are shown in Table 4 ViTO and FNO obtain very similar accuracy in both cases, ViTO is slightly more accurate without noise, while FNO has a minor advantage with noise.

A visualization of the results is shown in Figure 4 and Figure 5 In the case $T=1$ (Figure 4), the vorticities for different initial conditions are still very different from one another, and so the reconstructions are able to capture fine details. However, for $T=5$ (Figure 5), the behavior of the vorticity becomes very similar, regardless of the choice of initial condition. In that case, some fine details are lost in all reconstructions, which explains the larger error compared to the $T=1$ case.

Data

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-08.jpg?height=851&width=483&top_left_y=342&top_left_x=428)

Predictions

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-08.jpg?height=851&width=639&top_left_y=339&top_left_x=1052)

Figure 3: Predictions for the wave problem 3.1 for 5 random samples. In the section labeled "Data" is the propagated wave at the final time using fine and coarse discretizations, alongside a high-resolution image of the ground truth sources. In the section labeled "Predictions" are the initial condition reconstructions by the various methods.

Table 4: Test relative $L^{2}$ errors for the errors for the Navier-Stokes problem with different final simulation times $(T)$ and noise levels $(\gamma)$.

|  | $T=1$ |  |  | $T=5$ |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\gamma=0$ | $\gamma=0.1$ |  | $\gamma=0$ | $\gamma=0.1$ |
| FNO | 0.06449 | $\mathbf{0 . 1 5 8 7}$ |  | 0.1881 | $\mathbf{0 . 3 5 8 2}$ |
| DeepONet | 0.09424 | 0.1684 |  | 0.2007 | 0.4528 |
| ResNet | 0.1271 | 0.4471 |  | 0.4520 | 0.5745 |
| ViTO | $\mathbf{0 . 0 6 3 4 8}$ | 0.1635 |  | $\mathbf{0 . 1 7 5 7}$ | 0.3757 |

### 3.3 Darcy equation

The steady-state two-dimensional Darcy flow for a porous medium is given by the following equation:

$$
\begin{cases}-\nabla \cdot(K(x, y) \nabla h(x, y))=f(x, y), & (x, y) \in(0,1)^{2}  \tag{5}\\ h(x, y)=0, & (x, y) \in \partial(0,1)^{2}\end{cases}
$$

where $\partial(0,1)^{2}$ is the domain boundary, $K(x, y)$ is the permeability coefficient field, $h(x, y)$ is the pressure, and $f$ is a forcing function. The boundary condition used here is a homogeneous Dirichlet boundary (fully-reflective). The inverse problem is to learn the following mapping:

$$
h(x, y) \longmapsto K(x, y)
$$

We used the publicly available finite difference solver (written in MATLAB [40], given in [2]) to create data with piecewise smooth coefficients $K$, with a constant forcing function $f \equiv 1$. The coefficient was selected using a Gaussian

Data

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-09.jpg?height=848&width=483&top_left_y=346&top_left_x=428)

Predictions

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-09.jpg?height=851&width=637&top_left_y=339&top_left_x=1053)

Figure 4: Predictions for the Navier-Stokes problem 3.2 with $T=1$ for 5 random samples. In the section labeled "Data" is the vorticity at the final time using fine and coarse discretizations, alongside a high-resolution image of the initial vorticity. In the section labeled "Predictions" are the initial condition reconstructions by the various methods.

random field according to the following distribution: $\mathcal{N}\left(0,(-\Delta+9 I)^{-2}\right)$. This is followed by a binarization operation that mapped positive values to 12 and negative values to 3 . We created 3 such datasets for the following resolutions: $n=128,256,512$ with a SR scale factor of 8 . Hence, the super-resolution mappings were of the following dimensions: $16 \times 16 \longmapsto 128 \times 128,32 \times 32 \longmapsto 256 \times 256$, and $64 \times 64 \longmapsto 512 \times 512$. Each dataset contained 1,000 samples which were split into $800,100,100$ training, validation, and testing samples, respectively. Despite being a binary problem, we still used the $L^{2}$ loss function (2), and not a binary loss function like negative log likelihood, since $K(x, y)$ does not have to be binary in many applications. For the two datasets with finer grids we had to use a smaller batch size of 10 to fit the models into memory. We note that ViTO was the only model we were able to run with a batch size of 100 , but we kept it at 10 to make the comparison more accurate.

The full error analysis is shown in Table 5. In all 6 cases ViTO obtained the best accuracy compared to the benchmark methods. Note that the results for the Darcy problem and the wave problem 3.1 are more decisive in comparison to the Navier-Stokes problem 3.2. This could potentially be explained by the shift from smooth functions to functions consisting of irregular interfaces and sharp features. Recall that FNOs rely on Fourier transforms, which can be very accurate for smooth functions, but face severe difficulties with discontinuities. Furthermore, as we refined the grid in the Darcy case we saw a noticeable improvement in the ViTO results, which was not observed in the FNO case. This can also be explained by Fourier analysis, since FNOs are learning a global base of functions, which renders them grid-invariant.

Visualizations of the results for $n=128$ and $n=512$ are shown in Figure 6, respectively. Note that ViTO was able to capture sharp features of the coefficient $K(x, y)$. This is especially noticeable in cases where there are very small discontinuities (cavities) in the data; while ViTO was mostly able to capture them, FNO tended to smooth them.

### 3.4 Varying input size

Finally, we assessed the ability of ViTO to handle inputs of various sizes without requiring retraining. Typically, transformers are capable of handling such inputs, which are prevalent in NLP contexts. As mentioned in 2.3, we

Data

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-10.jpg?height=848&width=483&top_left_y=346&top_left_x=428)

Predictions

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-10.jpg?height=849&width=637&top_left_y=340&top_left_x=1053)

Figure 5: Predictions for the Navier-Stokes problem 3.2 with $T=5$ for 5 random samples. In the section labeled "Data" is the vorticity at the final time using fine and coarse discretizations, alongside a high-resolution image of the initial vorticity. In the section labeled "Predictions" are the initial condition reconstructions by the various methods.

Table 5: Test relative $L^{2}$ errors for the Darcy flow problem with different grid sizes $(n)$ and noise levels $(\gamma)$.

|  | $n=128$ |  | $n=256$ |  | $n=512$ |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  | $\gamma=0$ | $\gamma=0.1$ | $\gamma=0$ | $\gamma=0.1$ | $\gamma=0$ | $\gamma=0.1$ |
| FNO | 0.1422 | 0.4502 | 0.1272 | 0.1915 | 0.1235 | 0.1683 |
| DeepONet | 0.1463 | 0.4502 | 0.1422 | 0.2090 | 0.1608 | 0.2174 |
| ResNet | 0.1603 | 0.2760 | 0.1287 | 0.3078 | 0.1416 | 0.3702 |
| ViTO | 0.1184 | 0.1943 | 0.08216 | 0.1799 | 0.05197 | 0.1623 |

employed relative positional encoding, which was shown to be effective for computer vision problems of this sort. Additionally, the U-Net architecture is fully convolutional [41], allowing it to handle such inputs.

To evaluate this capability, we used the Darcy example 3.3 with $n=512$. We followed the same steps as in all other experiments, with one addition to the training process. During each training batch, we randomly selected a subsampling parameter $r \in\{1,2,3, \ldots, 9\}$ and applied it to the input image (rounding the number of grid points to the nearest integer). For instance, taking $r=4$, an input of size $512 \times 512$ was downsampled to size $128 \times 128$. We used this process to allow ViTO to generalize better for new discretizations. This procedure can be considered a type of data augmentation. We also dropped the super-resolution part of the inverse problem (i.e. $s=1$ in 2.2) to enable us to run tests for large grids.

Finally, we tested the model twice. First, we evaluated it on samples from the test set with grid sizes it had encountered during training, which were: $\left\{\frac{512}{r}: r=1, \ldots, 9\right\}$, rounded to the nearest integer. Next, we created a zero-shot scenario, where the model was presented with samples having random discretizations that it had not seen during training. We created these discretizations by resizing the original samples accordingly. The results are presented in Figure 7. The results show that ViTO is capable of handling different grids without retraining, even in a zero-shot scenario. The

Data

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-11.jpg?height=875&width=494&top_left_y=343&top_left_x=398)

Predictions

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-11.jpg?height=873&width=656&top_left_y=344&top_left_x=1038)

(a) Results for $n=128$.

Data

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-11.jpg?height=878&width=496&top_left_y=1483&top_left_x=397)

Predictions

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-11.jpg?height=880&width=658&top_left_y=1479&top_left_x=1037)

(b) Results for $n=512$.

Figure 6: Predictions for the Darcy problem 3.3 for 5 random samples using for $n=128$ and $n=512$. In the section labeled "Data" is the PDE solution $u(x, y)$ using fine and 1 coarse discretizations, alongside a high-resolution image of the permeability coefficient field $K(x, y)$. In the section labeled "Predictions" are the permeability coefficient reconstructions by the various methods.
error maps show us that larger grids generally yield better results, and that the error is mostly concentrated around the discontinuities (due to the binarization of the permeability field).

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-12.jpg?height=721&width=1309&top_left_y=439&top_left_x=408)

(a) Previously seen discretizations.

![](https://cdn.mathpix.com/cropped/2024_06_04_77dfdafce9e536b2ae63g-12.jpg?height=717&width=1301&top_left_y=1352&top_left_x=420)

(b) Zero-shot discretizations.

Figure 7: ViTO predictions with varying input sizes. The results are presented in columns corresponding to different discretizations. The first two rows show the ground-truth values of $u(x, y)$ and $K(x, y)$ as given by 5 . The third row presents the predictions of ViTO. The last row shows the point-wise relative $L^{2}$ error between $K(x, y)$ and the ViTO prediction.

ViTO: Vision Transformer-Operator

## 4 Discussion and future work

We have introduced a novel approach to inverse problems and super-resolution, which incorporates vision transformers with operator learning. Our approach, named ViTO, combines a U-Net based architecture with a vision transformer. We have obtained comparable or superior results compared to the leading operator network benchmarks in terms of accuracy, accompanied by substantial efficiency gains.

The impressive performance of ViTO on inverse problems of considerable complexity requires thorough investigation to uncover the mathematical and algorithmic reasons behind it. In particular, we should understand the learning mechanism in the latent space and provide some theoretical background.

Extending ViTO to solve problems with three spatial dimensions is an avenue worth exploring. Moreover, we need to determine ViTO's ability to adapt to forward problems, especially those that are time-dependent.

## 5 Acknowledgements

This work was supported by the Vannevar Bush Faculty Fellowship award (GEK) from ONR (N00014-22-1-2795). The work of PS and GEK is supported by the U.S. Department of Energy, Advanced Scientific Computing Research program, under the Scalable, Efficient and Accelerated Causal Reasoning Operators, Graphs and Spikes for Earth and Embedded Systems (SEA-CROGS) project, DE-SC0023191. Pacific Northwest National Laboratory (PNNL) is a multi-program national laboratory operated for the U.S. Department of Energy (DOE) by Battelle Memorial Institute under Contract No. DE-AC05-76RL01830.

## References

[1] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature Machine Intelligence, $3(3): 218-229,2021$.

[2] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020 .

[3] Nikola B. Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew M. Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. CoRR, abs/2108.08481, 2021.

[4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017.

[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs/2010.11929, 2020.

[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. IEEE, 2009.

[7] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.

[8] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.

[9] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, André Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. The visual task adaptation benchmark. CoRR, abs/1910.04867, 2019.

[10] Gabriel Iluebe Okolo, Stamos Katsigiannis, and Naeem Ramzan. Ievit: An enhanced vision transformer architecture for chest x-ray image classification. Computer Methods and Programs in Biomedicine, 226:107141, 2022.

[11] Junyu Chen, Yufan He, Eric C Frey, Ye Li, and Yong Du. Vit-v-net: Vision transformer for unsupervised volumetric medical image registration. arXiv preprint arXiv:2104.06468, 2021.

[12] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations' operator learning. arXiv preprint arXiv:2205.13671, 2022.

ViTO: Vision Transformer-Operator

[13] Xinliang Liu, Bo Xu, and Lei Zhang. Ht-net: Hierarchical transformer based operator learning model for multiscale pdes. arXiv preprint arXiv:2210.10890, 2022.

[14] Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu, Ze Cheng, Jun Zhu, and Jian Song. Gnot: A general neural operator transformer for operator learning. arXiv preprint arXiv:2302.14376, 2023.

[15] Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in neural information processing systems, $34: 24924-24940,2021$.

[16] Ruchi Guo, Shuhao Cao, and Long Chen. Transformer meets boundary value inverse problems. arXiv preprint arXiv:2209.14977, 2022.

[17] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234-241. Springer, 2015.

[18] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.

[19] Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. Computer Methods in Applied Mechanics and Engineering, 393:114778, 2022.

[20] Jacques Hadamard. Sur les problèmes aux dérivées partielles et leur signification physique. Princeton university bulletin, pages 49-52, 1902 .

[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.

[22] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456. pmlr, 2015.

[23] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.

[24] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.

[25] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. Rethinking and improving relative position encoding for vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10033-10041, 2021.

[26] François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1251-1258, 2017.

[27] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021.

[28] Andrei Nikolaevich Tikhonov, AV Goncharsky, Vyacheslav Vasil'evich Stepanov, and Anatoly G Yagola. $\mathrm{Nu}$ merical methods for the solution of ill-posed problems, volume 328. Springer Science \& Business Media, 1995.

[29] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed selfexemplars. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.

[30] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Int'l Conf. Computer Vision, volume 2, pages 416-423, July 2001.

[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

[32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

[33] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.

[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.

ViTO: Vision Transformer-Operator

[35] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. DeepXDE: A deep learning library for solving differential equations. SIAM Review, 63(1):208-228, 2021.

[36] Lawrence C Evans. Partial differential equations, volume 19. American Mathematical Society, 2022.

[37] Jürgen Jost. Partial differential equations, volume 214. Springer Science \& Business Media, 2012.

[38] R. Abgrall and C.W. Shu. Handbook of Numerical Methods for Hyperbolic Problems: Basic and Fundamental Issues. Elsevier, North Holland, 1 edition, 2016.

[39] J. Oliger B. Gustafsson, H.O. Kreiss. Time-Dependent Problems and Difference Methods. Pure and Applied Mathematics, Wiley, 2 edition, 2016.

[40] The MathWorks Inc. Matlab version: 9.13.0 (r2022b), 2022.

[41] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431-3440, 2015.

