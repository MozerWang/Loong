# Neural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD with Near-perfect Representation Learning 

Chendi Wang*广 Yuqing Zhu ${ }^{* \ddagger}$ Weijie J. Su ${ }^{8}$ Yu-Xiang Wang $\mathbb{I}$

May 17, 2024


#### Abstract

A recent study by De et al. (2022) has reported that large-scale representation learning through pre-training on a public dataset significantly enhances differentially private (DP) learning in downstream tasks, despite the high dimensionality of the feature space. To theoretically explain this phenomenon, we consider the setting of a layer-peeled model in representation learning, which results in interesting phenomena related to learned features in deep learning and transfer learning, known as Neural Collapse (NC).

Within the framework of $\mathrm{NC}$, we establish an error bound indicating that the misclassification error is independent of dimension when the distance between actual features and the ideal ones is smaller than a threshold. Additionally, the quality of the features in the last layer is empirically evaluated under different pre-trained models within the framework of $\mathrm{NC}$, showing that a more powerful transformer leads to a better feature representation. Furthermore, we reveal that DP fine-tuning is less robust compared to fine-tuning without DP, particularly in the presence of perturbations. These observations are supported by both theoretical analyses and experimental evaluation. Moreover, to enhance the robustness of DP fine-tuning, we suggest several strategies, such as feature normalization or employing dimension reduction methods like Principal Component Analysis (PCA). Empirically, we demonstrate a significant improvement in testing accuracy by conducting PCA on the last-layer features.


## 1 Introduction

Recently, privately fine-tuning a publicly pre-trained model with differential privacy (DP) has become the workhorse of private deep learning. For example, De et al. (2022) demonstrates that fine-tuning the last-layer of an ImageNet pre-trained Wide-ResNet achieves an accuracy of $95.4 \%$ on CIFAR-10 with $\left(\epsilon=2.0, \delta=10^{-5}\right)$-DP, surpassing the $67.0 \%$ accuracy from private training from scratch with a three-layer convolutional neural network (Abadi et al., 2016). Additionally, Li et al. (2021); Yu et al. (2021) show that pre-trained BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2018) models achieve near no-privacy utility trade-off when fine-tuned for sentence classification and generation tasks.

However, the empirical success of privately fine-tuning pre-trained large models appears to defy the worst-case dimensionality dependence in private learning problems - noisy stochastic[^0]gradient descent (NoisySGD) requires adding noise scaled to $\sqrt{p}$ to each coordinate of the gradient in a model with $p$ parameters, rendering it infeasible for large models with millions of parameters. This suggests that the benefits of pre-training may help mitigate the dimension dependency in NoisySGD. A recent work (Li et al., 2022) makes a first attempt on this problem - they show that if gradient magnitudes projected onto subspaces decay rapidly, the empirical loss of NoisySGD becomes independent to the model dimension. However, the exact behaviors of gradients remain intractable to analyze theoretically, and it remains uncertain whether the "dimension independence" property is robust across different fine-tuning applications.

In this work, we explore private fine-tuning behaviors from an alternative direction - we employ an representation of pre-trained models using the Neural Collapse (NC) theory (Papyan et al., 2020) and study the dimension dependence in a specific private fine-tuning setup - fine-tuning only the last layer of the pre-trained model, a benchmark method in private fine-tuning.
![](https://cdn.mathpix.com/cropped/2024_05_26_b1ed682555cfdb8567e6g-02.jpg?height=282&width=1288&top_left_y=862&top_left_x=406)

Figure 1: The figure depicts the evolution of the feature layer outputs of a VGG-13 neural network when trained on CIFAR-10 with three randomly selected classes. Each class is represented by a distinct color in the small blue sphere. As the training progresses, the last-layer feature means collapse onto their corresponding classes. Credit to Papyan et al. (2020).

To elaborate, Neural Collapse (Papyan et al., 2020; Fang et al., 2021; He and Su, 2023) is a phenomenon observed in the late stage of training deep classifier neural networks. At a high level, NC suggests that when a deep neural network is trained on a $K$-class classification task, the last-layer feature converges to $K$ distinct points (see Figure 1 for $K=3$ ). This insight provides a novel characterization of deep model behaviors, sparking numerous subsequent studies that delve into this phenomenon (Masarczyk et al., 2023; Li et al., 2023a). Notably, Masarczyk et al. (2023) shows that when pre-training a model on CIFAR-10 and fine-tuning it on a subset of 10 classes from CIFAR-100, the collapse phenomenon has been observed.

To summarize, we aim to answer the following questions about private learning under Neural Collapse.

1. When fine-tuning the last layer, what specific property of the features of that layer is important to make private learning dimension-independent?
2. How robust is this dimension-independence property against perturbations?

### 1.1 Contributions of this paper

In essence, our contribution lies in correlating the performance and robustness of privately fine-tuning with the recently proposed Neural Collapse theory.

Theoretically, we identify a key structure of the last-layer feature that leads to a dimensionindependent misclassification error (using 0-1 loss) in noisy gradient descent (NoisyGD). We formalize
this structure through the feature shift parameter $\beta$, which measures the $\ell_{\infty}$-distance between the actual last-layer features obtained from the pre-trained model on a private set, and the ideal maximally separable features posited by Neural Collapse theory. A smaller value of $\beta$ indicates a better representation of the last-layer feature. We show that if the feature shift parameter $\beta$ remains below a certain threshold related to the model dimension $p$, the sample complexity bound of achieving a $\gamma$ misclassification error is dimension-independent.

Empirically, we evaluate the feature shift vector $\beta$ when fine-tuning different transformers on CIFAR-10. Figure 2 plots the distribution of per-class $\beta$ when the pre-trained transformer is either the Vision Transformer (ViT) or ResNet-50. Blue and green scatter plots represent the $\beta$ values for each sample from the two models, while purple and yellow scatter plots denote the median $\beta$ within each class. The median $\beta$ is centered around 0.10 for ViT and around 0.20 for ResNet-50. The pre-trained ViT model is known to have better feature representations than the ResNet-50 model. Our results show that: (1) $\beta$ is bounded for the two pre-trained models, with very few outliers, and (2) the better the pre-trained model, the smaller the $\beta$. For example, ViT (with $\beta \approx 0.1$ ) outperforms ResNet-50 (with $\beta \approx 0.2$ ) due to its smaller shift parameter. We postpone the details of our experiments to Section 5.1.

![](https://cdn.mathpix.com/cropped/2024_05_26_b1ed682555cfdb8567e6g-03.jpg?height=775&width=1114&top_left_y=1103&top_left_x=533)

Figure 2: CIFAR-10. A figure depicting the feature shift parameter $\beta$ when fine-tuning different pre-trained models on CIFAR-10. As observed, ViT performs better than ResNet-50, as the shift parameter is much smaller. The feature shift vectors are quite stochastic.

Moreover, we study the robustness of the "dimension-independence" property against various types of perturbations, including stochastic, adversarial, and offset perturbations. Each perturbation type will alter the feature presentations, increasing $\beta$ and potentially making the sample complexity bound dimension-dependent. Notably, we find that the adversarial perturbations impose a stricter limitation on $\beta$, lowering the acceptable upper-bound threshold for $\beta$ from $p^{-\frac{1}{2}}$ to $p^{-1}$, meaning that adversarial perturbations are more fragile compared to other types of perturbations.

Moving forward, to mitigate the "non-robustness of dimension-independency" issue under perturbations, we propose solutions to enhance the robustness of NoisyGD. We find out that releasing the mean of feature embeddings effectively neutralizes offset perturbation (detailed in Section 4.2), and dimension reduction techniques like PCA (detailed in Section 4.1) reduce the constraint on the feature shift parameter $\beta$, thus improve NoisyGD's robustness. Our theoretical results shed light on the recent success of private fine-tuning. Specifically, our analysis on PCA provides a plausible explanation for the effectiveness of employing DP-PCA in the first private deep learning work (Abadi et al., 2016).

We summarize our contributions below:

- We present a direct analysis of the misclassification error (on population) under 0-1 loss. Existing analyses of the excess risk on population (Bassily et al., 2014, 2019, 2020; Feldman et al., 2020) mainly focus on the excess risk under convex surrogate loss, leading to sample complexity bounds (inverse) polynomial in the required excess risk. We show that Neural Collapse theory allows us to directly bound 0-1 loss, which results in a logarithmic sample complexity bound in the misclassification error $\gamma$.
- We introduce a feature shift parameter $\beta$ to measure the discrepancy between actual and ideal last-layer features. Our theoretical findings show that this feature structure is crucial to guarantee NoisyGD's resistance to dimension dependence. In particular, when $\beta \leq p^{-\frac{1}{2}}$, the sample complexity for NoisyGD becomes $O(\log (1 / \gamma))$, which is dimension-independent. We also provide an empirical evaluation of this feature shift parameter $\beta$ on CIFAR-10 using ImageNet pre-trained vision transformer and ResNet-50, which shows that the vision transformer outperforms ResNet-50 as it leads to a smaller $\beta$.
- We theoretically analyze the misclassification error and robustness of NoisyGD against several types of perturbations and class-imbalanced scenarios, with sample complexity bounds summarized in Table 1.
- We empirically validate our theoretical findings on private fine-tuning vision models. We show that privately fine-tuning an ImageNet pre-trained vision transformer is not affected by the last-layer dimension on CIFAR-10. However, we observe a degradation in the utility-dimension trade-off when a minor perturbation is introduced, aligning with our theoretical results.
- We propose two solutions to address the non-robustness challenges in DP fine-tuning with theoretical insights. In particular, we empirically apply PCA on the perturbed last-layer feature before private fine-tuning, demonstrating that the utility of NoisySGD remains unaffected by the original feature dimension. We believe these results will provide new insights into enhancing the robustness of private fine-tuning.


## 2 Preliminaries and Problem Setup

In this section, we review the private fine-tuning literature, introduce the Neural Collapse phenomenon, and formally describe the private fine-tuning problem under (approximate) Neural Collapse.

| Setting | Assumption | Sample complexity |
| :---: | :---: | :---: |
| Non-private <br> learning <br> with GD | Perfect NC or $\beta$-approximate NC with $\beta^{2} p \leq 1$ <br> $\beta$-approximate NC <br> $\beta$-approximate NC (separability) | No. of classes <br> $p \beta^{2} \log \frac{1}{\gamma}$ <br> $p \beta^{4} \log \frac{1}{\gamma}$ |
| Private <br> learning <br> $(\rho$-zCDP $)$ <br> with <br> NoisyGD | Perfect NC <br> $\beta$-approximate NC <br> $\beta$-approximate NC (separability) <br> $\widetilde{\beta}$-stochastic perturbation (test) <br> $\widetilde{\beta}$-adversarial perturbation (test) <br> $\widetilde{\beta}$-offset perturbation (training) <br> $\widetilde{\beta}$-offset perturbation $+\alpha$-Class imbalance | ![](https://cdn.mathpix.com/cropped/2024_05_26_b1ed682555cfdb8567e6g-05.jpg?height=536&width=511&top_left_y=459&top_left_x=1333) |

Table 1: Summary of the sample complexity of achieving a misclassification error $\gamma$ of private learning under $\rho$-zCDP. We consider perfect features, actual features (GD and NoisyGD), and perturbed features (stochastic, adversarial, offset perturbations). For the actual features, we assume that the feature shift vectors of traing and testing features are from the same distribution. The separability assumption here is that all $p$ components of the feature shift vectors are independent. We have also considered the effects of $\alpha$-class imbalance.

Symbols and notations. Let the data space be $\mathcal{Z}$. A dataset $\mathcal{D}$ is a set of individual data points $\left\{z_{1}, z_{2}, \ldots\right\}$ where $z_{i} \in \mathcal{Z}$. Unless otherwise specified, the size of the data $|\mathcal{D}|:=n . \mathcal{Z}=\mathcal{X} \times \mathcal{Y}$ where $\mathcal{X}$ is the feature space and $\mathcal{Y}$ the label space with $\mathcal{X} \subset \mathbb{R}^{p}$ and $\mathcal{Y}=\{1,2, \ldots, K\}$. A data point $z_{i}$ is a feature-label pair $\left(x_{i}, y_{i}\right)$. Occasionally, we overload $y_{i}$ to also denote the one-hot representation of the label. We use standard probability notations, e.g., $\operatorname{Pr}[\cdot]$ and $\mathbb{E}[\cdot]$ for probabilities and expectations. Other notations will be introduced when they first appear.

Differentially private learning. The general setting of interest is differentially private learning where the goal is to train a classifier while satisfying a mathematical definition of privacy known as differential privacy (Dwork, 2006). DP ensures that any individual training data point cannot be identified using the trained model and any additional side information.

More formally, we adopt the popular modern variant called zero-centered Concentrated Differential Privacy ( $z C D P)$, as defined below.

Definition 2.1 (Zero-Concentrated Differential Privacy, zCDP, Bun and Steinke (2016)). Two datasets $\mathcal{D}_{0}, \mathcal{D}_{1}$ are neighbors if they can be constructed from each other by adding or removing one data point. A randomized mechanism $\mathcal{A}$ satisfies $\rho$-zero-concentrated differentially private $\left(\rho\right.$-zCDP) if, for all neighboring datasets $\mathcal{D}_{0}$ and $\mathcal{D}_{1}$, we have $R_{\alpha}\left(\mathcal{A}\left(\mathcal{D}_{0}\right) \| \mathcal{A}\left(\mathcal{D}_{1}\right)\right) \leq \rho \alpha$, where $R_{\alpha}(P \| Q)=\frac{1}{\alpha-1} \log \int\left(\frac{p(x)}{q(x)}\right)^{\alpha} q(x) d x$ is the Rényi divergence between two distributions $P$ and $Q$.

In the above definition, $\rho \geq 0$ is the privacy loss parameter that measures the strength of the protection. $\rho=0$ indicates perfect privacy, $\rho=\infty$ means no protection at all. The privacy
protection is considered sufficiently strong in practice if $\rho$ is a small constant, e.g., 1,2,4,8. For readers familiar with the standard approximate DP but not zCDP, $\rho$-zCDP implies $(\epsilon, \delta)$-DP for all $\delta>0$ with $\epsilon=\rho+2 \sqrt{\rho \log (1 / \delta)}$.

The goal of differentially private learning is to come up with a differentially private ( $\rho$-zCDP) algorithms that outputs a classifier $f: \mathcal{X} \rightarrow \mathcal{Y}$ such that the misclassification error

$$
\operatorname{Err}(f)=\mathbb{E}_{(x, y) \sim \mathcal{P}}[\mathbf{1}(f(x) \neq y)]
$$

is minimized (in expectation or with high probability), where $\mathcal{P}$ is the data distribution under which the training data is sampled from i.i.d.

For reasons that will become clear soon, we will focus on linear classifiers parameterized by $W \in \mathbb{R}^{K \times p}$ of the form

$$
f_{W}(x)=\underset{y \in[K]}{\operatorname{argmax}}[W x]_{y} .
$$

Noisy gradient descent. Noisy Gradient Descent or its stochastic version Noisy Stochastic Gradient Descent (Song et al., 2013; Abadi et al., 2016) is a fundamental algorithm in DP deep learning. To minimize the loss function $\mathcal{L}(\theta):=\sum_{i=1}^{n} \ell\left(\theta, z_{i}\right)$, the NoisyGD algorithm updates the model parameter $\theta_{t}$ by combining the gradient with an isotropic Gaussian.

$$
\begin{equation*}
\theta_{t+1}=\theta_{t}-\eta_{t}\left(\sum_{i=1}^{n} \nabla \ell\left(\theta_{t}, z_{i}\right)+\mathcal{N}\left(0, \frac{G^{2}}{2 \rho} I_{p}\right)\right) \tag{2.1}
\end{equation*}
$$

Here $G$ is the $\ell_{2}$-sensitivity of the gradient, and the algorithm runs for $T$ iterations that satisfy $T \rho-\mathrm{zCDP}$.

However, the excess risk on population of NoisySGD must grow as $\sqrt{p} / \epsilon$ (Bassily et al., 2014, 2019), which limits private deep learning benefit from model scales. To overcome this, DP fine-tuning (De et al., 2022; Li et al., 2021; Bu et al., 2022) is emerging as a promising approach to train large models with privacy guarantee.

Private fine-tuning. In DP fine-tuning, we begin by pre-training a model on a public dataset and then privately fine-tuning the pre-trained model on the private dataset. Our focus is on fine-tuning the last-layer of pre-trained models using the NoisySGD/NoisyGD algorithm, which has consistently achieved state-of-the-art results across both vision and language classification tasks (De et al., 2022; Tramer and Boneh, 2020; Bao et al., 2023). However, we acknowledge that in some scenarios, fine-tuning all layers under DP can result in better performance, as demonstrated in the CIFAR-10 task by De et al. (2022). The comprehensive analysis of dimension-dependence in other private fine-tuning benchmarks remains an area for future investigation.

Theoretical setup for private fine-tuning. For a $K$-class classification task, we rewrite each data point $z$ as $z=(x, y)$ with $x \in \mathbb{R}^{p}$ being the feature and $y=\left(y_{1}, \cdots, y_{K}\right) \in\{0,1\}^{K}$ being the corresponding label generated by the one-hot encoding, that is $y$ belongs to the $k$-th class if $y_{k}=1$ and $y_{j}=0$ for $j \neq k$.

When applying NoisyGD for fine-tuning the last-layer parameters, the model is in a linear form. Thus, we consider the linear model $f_{W}(x)=W x$ with $W \in \mathbb{R}^{K \times p}$ being the last-layer parameter to be trained and $x$ is the last layer feature of a data point. The parameter $\theta_{t}$ in NoisyGD is the vectorization of $W$. Let $\ell: \mathbb{R}^{K} \times \mathbb{R}^{K} \rightarrow \mathbb{R}$ be a loss function that maps $f_{W}(x) \in \mathbb{R}^{K}$ and the label $y$ to $\ell\left(f_{W}(x), y\right)$. For example, for the cross-entropy loss, we have $\ell\left(f_{W}(x), y\right)=-\sum_{i=1}^{K} y_{i} \log \left[\left(f_{W}(x)\right)_{i}\right]$.

Misclassification error. For an output $\widehat{W}$ and a testing data point $(x, y)$, the misclassification error we considered is defined as $\operatorname{Pr}\left[y \neq f_{\widehat{W}}(x)\right]$, where the probability is taken with respect to the randomness of $\widehat{W}$ and $(x, y) \sim \mathcal{P}$.

Beyond the distribution-free theory. Distribution-free learning with differential privacy is however known to be statistically intractable even for linear classification in 1-dimension (Chaudhuri and Hsu, 2011; Bun et al., 2015; Wang et al., 2016). Existing work resorts to either proving results about (convex and Lipschitz) surrogate losses (Bassily et al., 2014) or making assumptions on the data distribution (Chaudhuri and Hsu, 2011; Bun et al., 2020). For example, Chaudhuri and Hsu (2011) assumes bounded density, and Bun et al. (2020) shows that linear classifiers are privately learnable if the distribution satisfies a large-margin condition. Our setting, as detailed in Section 3.1.1, can be viewed as a new family of distributional assumptions motivated by the recent discovery of the Neural Collapse phenomenon. As we will see, these assumptions not only make private learning statistically and computationally tractable (using NoisyGD), but also produce sample complexity bounds that are dimension-free and exponentially faster than existing results that are applicable to our setting.

Definition 2.2 (Sample complexity for private $\mathfrak{D}$-learnability). For a set of distributions $\mathfrak{D}$, the sample complexity of private $\mathfrak{D}$-learning a hypothesis class $\mathcal{H}$ under $\rho$-zCDP is defined to be

$$
n_{\mathcal{H}, \mathcal{Q}, \rho}(\gamma)=\min \left\{n \in \mathbb{N} \mid \inf _{\mathcal{A} \text { satisfies } \rho-\mathrm{zCDP}} \sup _{\mathcal{P} \in \mathcal{P}} \mathbb{E}_{\mathcal{A}, \text { data } \mathcal{P}^{n}}\left[\operatorname{Err}_{\mathcal{P}}(\mathcal{A}(\text { data }))-\inf _{h \in \mathcal{H}} \operatorname{Err}_{\mathcal{P}}(h)\right] \leq \gamma\right\}
$$

i.e., the smallest integer such that the minimax excess risk is smaller than $\gamma$.

We say $\mathfrak{D}$ is learnable under $\rho$-zCDP if $n_{\mathcal{H}, \mathfrak{Q}, \rho}$ is finite, meaning there exists an algorithm $\mathcal{A}$ such that the expected excess risk is smaller than $\gamma$ with finitely many training data. It is obvious that the misclassification error defined above can be written as $\mathbb{E}_{\mathcal{A}, \text { data } \mathcal{P}^{n}}\left[\operatorname{Err}_{\mathcal{P}}(\mathcal{A}(\right.$ data $\left.))\right]$ as the randomness of $\widehat{W}$ comes from the randomized algorithm $\mathcal{A}$ and the training data. Since the second term $\inf _{h \in \mathcal{H}} \operatorname{Err}_{\mathcal{P}}(h)$, which is the misclassification error of the optimal classifier, is non-negative, the misclassification error is always larger than the excess risk. Thus, in Section 3, we investigate the convergence of the misclassification error, which implies an upper bound on the excess risk as well. We will focus on linear classifiers, due to fine-tuning the last layer, and study distribution families $\mathfrak{D}$ inspired by Neural Collapse.

Neural Collapse. Neural Collapse (Papyan et al., 2020; Fang et al., 2021; He and Su, 2023) describes a phenomenon about the last-layer feature structure obtained when a deep classifier neural network converges. It demonstrates that the last-layer feature converges to the column of an equiangular tight frame (ETF). Mathematically, an ETF is a matrix

$$
\begin{equation*}
M=\sqrt{\frac{K}{K-1}} P\left(I_{K}-\frac{1}{K} \mathbf{1}_{K} \mathbf{1}_{K}^{T}\right) \in \mathbb{R}^{p \times K} \tag{2.2}
\end{equation*}
$$

where $P=\left[P_{1}, \cdots, P_{K}\right] \in \mathbb{R}^{p \times K}$ is a partial orthogonal matrix such that $P^{T} P=I_{K}$. For a given dimension $d=p$ or $K$, we denote $I_{d} \in \mathbb{R}^{d}$ the identity matrix and denote $\mathbf{1}_{d}=[1, \cdots, 1]^{T} \in \mathbb{R}^{d}$. Rewrite $M=\left[M_{1}, \cdots, M_{K}\right]$ with $M_{k}$ being the $k$-th column of $M$, that is, the ideal feature of the data belonging to class $k$.

We adopt the Neural Collapse theory to describe an ideal feature representation of applying the pre-trained model on the private set. However, achieving perfect collapse on the private set is an ambitious assumption, as in practice, the private feature of a given class is distributed around $M_{k}$. Therefore, we introduce a feature shift parameter $\beta$ to measure the discrepancy between the actual feature and the perfect feature $M_{k}$.

Definition 2.3 (Feature shift parameter $\beta$ ). For any $1 \leq k \leq K$, given a feature $x$ belonging to the $k$-th class and the perfect feature $M_{k}$, we define $\beta=\left\|x-M_{k}\right\|_{\infty}$ as the feature shift parameter of $x$ that measures the $\ell_{\infty}$ distance between $x$ and $M_{k}$.

Here, we use the $\ell_{\infty}$ norm since it is related to adversarial attacks, which are important in our study of the robustness of NoisyGD. Our numerical results in Figure 2 show that $\beta$ is bounded on CIFAR-10 if the pretrained model is the vision transformer or ResNet-50.

## 3 Bounds on misclassification errors and robustness in private fine-tuning

In this section, we establish bounds on the misclassification error for both GD and the NoisyGD .

Section 3.1 aims to delineate the connection between the feature shift parameter $\beta$ and the misclassification error. Additionally, we derive a threshold for $\beta$ below which the misclassification error is dimension-independent.

In Section 3.2, our focus is the robustness of private fine-tuning. Specifically, we elucidate how various perturbations impact both $\beta$ and the misclassification error.

### 3.1 Bounds on misclassification errors

We consider a binary classification problem with a training set $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}$, where $x_{i}$ represents features and $y_{i} \in\{ \pm 1\}$ are the labels. For the broader multi-class scenarios, we state our theory for the perfect case in Section D.1. The rest theory can be extended to the multi-class case similarly.

For binary classification problems, the ETF $M=\left[M_{1}, M_{2}\right]$ satisfies $M_{1}=-M_{2}$, which is equivalent to $M_{1}=e_{1}$ and $M_{2}=-e_{1}$ up to some rotation map. In fact, for this binary classification problem, we have $M_{1}=\frac{P_{1}-P_{2}}{\sqrt{2}}$, according to the definition of an ETF in Eq. 2.2. Similarly, it holds that $M_{2}=\frac{P_{2}-P_{1}}{\sqrt{2}}$. Thus, we have $M_{1}=-M_{2}$ and $\left\|M_{1}\right\|_{2}=\left\|M_{2}\right\|_{2}=1$. Since our theory depends only on the norm of the feature means $M_{1}$ and $M_{2}$, and the angle between $M_{1}$ and $M_{2}$, while the rotation matrix $P$ will not change the misclassification error, without loss of generality, we assume that $M_{1}=e_{1}$ and $M_{2}=-e_{1}$. For a data point $(x, y)$ with $y=1$, recall the feature shift parameter $\beta=\left\|x-e_{1}\right\|_{\infty}$, which is the infinity norm of $v=x-e_{1}$. We call $v$ a feature shift vector since it is the difference between an actual feature and the perfect one. Similarly, if $y=-1$, the feature shift vector is $v=x+e_{1}$. For a training set $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}$, let $\left\{v_{i}=x_{i}-\text { perfect feature }\right\}_{i=1}^{n}$ be a sequence of feature shift vectors.

We first consider the scenario where the shift vectors $v_{i}$ 's are i.i.d. copies of a symmetric centered random vector $v$ with $\|v\|_{\infty} \leq \beta$ in Section 3.1.1. This setting is quite practical as can be seen from Figure 1. Notably, the law of large numbers ensures that $\frac{1}{n} \sum_{i=1}^{n} v_{i}$ converges to zero. Consequently, the mean of the features approaches the ideal feature, in line with the principles of the Neural Collapse theory. Moreover, in our theory for the softmargin case with $\beta^{2} p>1$, we need further
assumption to make the model linearly separable. We assume that all $p$ components of $v$ are independent of each other.

Furthermore, acknowledging that at times, the feature may be influenced by a fixed vector, such as an offset shift vector which will not change the margin and angles between features, we also investigated the case where $v_{i}$ is deterministic in Section 3.1.2.

### 3.1.1 Stochastic shift vectors

For conciseness, we mainly focus on the results for 1-iteration NoisyGD, which is sufficient to ensure the convergence. As presented in Theorem 3.4, our theory can be extended to multi-iteration projected NoisyGD. However, the dimension dependency can not be mitigated using multiple iterations. The proofs of all results in this section are given in Appendix A.

For 1-iteration GD without DP guarantee, the output is $\widehat{\theta}_{\mathrm{GD}}=\eta \sum_{i=1}^{n} y_{i} x_{i}$. Moreover, the 1-iteration NoisyGD outputs $\widehat{\theta}_{\text {NoisyGD }}=\eta \sum_{i=1}^{n} y_{i} x_{i}+\mathcal{N}\left(0, \sigma^{2}\right)$. For the private learning problem, the sensitivity of the gradient is $G=\sup _{x_{i}}\left\|x_{i}\right\|_{2}=\sqrt{1+\beta^{2} p}$, which is dimension dependent. If we still want $G$ to be dimension independent, then every data point needs to be shrunk to $\left(e_{1}+v\right) / \sqrt{1+\|v\|_{2}^{2}}$. In both cases, the error bounds remain the same. In the testing procedure, we consider a testing point $(x, y) \sim \mathcal{P}$.

For an estimate $\widehat{\theta}$ whose randomness is from a training dataset drawn independently from $\mathcal{P}$ and a randomized algorithm $\mathcal{A}$, the misclassification error $\mathbb{E}_{\mathcal{A}, \text { data } \mathcal{P}^{n}}\left[\operatorname{Err}_{\mathcal{P}}(\mathcal{A}\right.$ (data) $\left.)\right]$ in Definition 2.2 can be rewritten as

$$
\mathbb{E}_{\mathcal{A}, \text { data } \sim \mathcal{P}^{n}}\left[\operatorname{Err}_{\mathcal{P}}(\mathcal{A}(\text { data }))\right]=\mathbb{E}_{\mathbb{\theta}^{-}} \mathbb{E}_{(x, y) \sim \mathcal{P}}\left[\mathbf{1}\left(y \neq \operatorname{sign}\left(\widehat{\theta}^{T} x\right)\right)\right]=\operatorname{Pr}\left[y \widehat{\theta}^{T} x<0\right]
$$

where the probability is taken with respect to the randomness of both $(x, y) \sim \mathcal{P}$ and $\widehat{\theta}$.

Theorem 3.1 (misclassification error for GD). Let $\widehat{\theta}_{\mathrm{GD}}$ be a predictor trained by GD under the cross entropy loss with zero initialization. Then, we have the following error bound on the misclassification error.

- If we assume that $\beta^{2} p \leq 1$, then it holds $\operatorname{Pr}\left[y \widehat{\theta}_{\mathrm{GD}}^{T} x<0\right]=0$ for $n$ greater than the number of classes. As a result, to achieve a misclassification error $\gamma$, the sample complexity is constant.
- In general, it holds

$$
\operatorname{Pr}\left[y \widehat{\theta}_{\mathrm{GD}}^{T} x<0\right] \leq \exp \left(-\frac{n}{2\left(\beta^{4} p^{2}+\frac{1}{3} \beta^{2} p\right)}\right)
$$

Therefore, to achieve a misclassification error $\gamma$, the sample complexity is $O\left(p \beta^{2} \log (1 / \gamma)\right)$. If we further assume that all $p$ components of $v$ are independent of each other, then, it holds

$$
\operatorname{Pr}\left[y \widehat{\theta}_{\mathrm{GD}}^{T} x<0\right] \leq \exp \left(-\frac{n}{2\left(\beta^{4} p+\frac{1}{3} \beta^{2}\right)}\right)
$$

Thus, to achieve a misclassification error $\gamma$, the sample complexity is $O\left(p \beta^{4} \log (1 / \gamma)\right)$.

Theorem 3.2 (misclassification error for NoisyGD). Let $\widehat{\theta}_{\text {NoisyGD }}$ be a predictor trained by NoisyGD under the cross entropy loss with zero initialization. Then, we have the following error bound on the misclassification error.

$$
\begin{equation*}
\operatorname{Pr}\left[y \widehat{\theta}_{\text {NoisyGD }}^{T} x<0\right] \leq \exp \left(-\frac{n^{2} \rho}{2\left(1+\beta^{2} p\right)^{2}}\right)+\exp \left(-\frac{n}{8\left(\beta^{4} p^{2}+\frac{1}{3} \beta^{2} p\right)}\right) \tag{3.1}
\end{equation*}
$$

As a result, to achieve a misclassification error $\gamma$, the sample complexity is $O\left(\frac{\left(1+\beta^{2} p\right)^{2} \sqrt{\log _{\frac{1}{\gamma}}^{\gamma}}}{2 \rho}+p \beta^{2} \log (1 / \gamma)\right)$. If we further assume that all $p$ components of $v$ are independent of each other, then, it holds

$$
\operatorname{Pr}\left[y \widehat{\theta}_{\text {NoisyGD }}^{T} x<0\right] \leq \exp \left(-\frac{n^{2} \rho}{2\left(1+\beta^{2} p\right)^{2}}\right)+\exp \left(-\frac{n}{8\left(\beta^{4} p+\frac{1}{3} \beta^{2}\right)}\right)
$$

To achieve a misclassification error $\gamma$, the sample complexity is $O\left(\frac{\left(1+\beta^{2} p\right)^{2} \sqrt{\log \frac{1}{\gamma}}}{2 \rho}+4 p \beta^{4} \log \frac{1}{\gamma}\right)$.

Remark. Note that with further assumptions on feature separability, the second term in Equation 3.1 (which aligns with GD in Theorem 3.1) can be improved from $\beta^{2} p$ to $\beta^{4} p$. However, the first term, caused by DP, remains unchanged by this assumption. Thus, NoisyGD has a stricter requirement on feature quality due to the added random noise. Theorems 3.1 and 3.2 indicate that the error bound is exponentially close to 0 under the following conditions: $\beta \leq p^{-1 / 2}$ for both NoisyGD and GD and $\beta \leq p^{-1 / 4}$ for GD under stronger assumptions. This result is dimension-independent when $\beta$ satisfies the above conditions. Moreover, GD has robustness against larger shift vectors compared to NoisyGD. This aligns with the observations from our experiments detailed in Section 5, where we note a significant decrease in accuracy with increasing dimensionality. In addition, when $\beta \leq p^{-1 / 2}$, the misclassification error for GD is always 0 while that for NoisyGD is $\exp \left(-\frac{n \rho}{1+\beta^{2} p}\right)$.

Promising properties for perfect collapse. In the special case $\beta=0$, all features are equivalent to the perfect feature. For this perfect scenario, numerous promising properties are outlined as follows. The details are discussed in Section D.1.

1. The error bound is exponentially close to 0 if $\rho \gg G^{2} / n^{2}$ - very strong privacy and very strong utility at the same time.
2. The result is dimension independent - it doesn't depend on the dimension $p$.
3. The result is robust to class imbalance for binary classification tasks.
4. The result is independent of the shape of the loss functions. Logistic loss works, while square losses also works.
5. The result does not require careful choice of learning rate. Any learning rate works equally well.

Neural collapse in domain adaptation: In many private fine-tuning scenarios, the model is initially pre-trained on an extensive dataset with thousands of classes (e.g., ImageNet), and is subsequently fine-tuned for a downstream task with a smaller number of classes. Our theory for the perfect case can be extended to the domain adaptation context, as detailed in Appendix D.4.

### 3.1.2 Deterministic shift vectors

In this section, we consider the case where each $v_{i}$ is a fixed vector with $\left\|v_{i}\right\|_{\infty} \leq \beta$. Recall that the 1 -iteration NoisyGD outputs $\widehat{\theta}_{\text {NoisyGD }}=\eta\left(n e_{1}+\sum_{i=1}^{n} v_{i}\right)+\mathcal{N}\left(0, \sigma^{2}\right)$. As discussed, when the feature is deterministic without assumptions on the distribution, the dataset is linearly separable only when $\beta^{2} p$ is less than 1 .

Theorem 3.3 (misclassification error for NoisyGD). Let $\widehat{\theta}_{\text {NoisyGD }}$ be a predictor trained by NoisyGD under the cross entropy loss with zero initialization. Then, for $\beta$ such that $1-\beta^{2} p>0$, we have the following error bound on the misclassification error.

$$
\operatorname{Pr}\left[y \widehat{\theta}_{\text {NoisyGD }}^{T} x<0\right] \leq \exp \left(-\frac{n^{2}\left(1-\beta^{2} p\right)^{2}}{\left(1+\beta^{2} p\right) \sigma^{2}}\right)
$$

As a result, to make the misclassification error less than $\gamma$, the sample complexity for $n$ is $O\left(\frac{\left(1+\beta^{2} p\right) \log \frac{1}{\gamma}}{2 \rho\left(1-\beta^{2} p\right)^{2}}\right)$.

Remark. This misclassification error also corresponds to $\beta^{2} p$, which is similar to the stochastic case. When $\beta^{2} p<1$, the misclassification error decays exponentially and the misclassification error is dimension-independent.

Multiple iterations. For the multi-iteration case, we consider the projected NoisyGD to bound the parameters. Precisely, the output is defined iteratively as

$$
\begin{equation*}
\theta_{k+1}=\mathcal{P}_{B_{2}^{p}(0, R)}\left(\theta_{k}-\eta\left(g_{n}\left(\theta_{k}\right)+\xi_{k}\right)\right) \tag{3.2}
\end{equation*}
$$

where $B_{2}^{p}(0, R) \subset \mathbb{R}^{p}$ is an $\ell_{2}$-norm ball with radius $R, \xi_{k} \sim \mathcal{N}\left(0, \sigma^{2} I_{p}\right)$, and $\mathcal{P}_{A}$ is the projection onto a convex set $A$ w.r.t. the Euclidean inner product. Here we take $\sigma^{2}=\left(1+\beta^{2} p\right) / 2 \rho$ and the overall privacy budget is $k \rho$.

Theorem 3.4 (Multiple iterations). Let $\theta_{k+1}$ be the output of projected NoisyGD defined in equation 3.2. For any $t>0$, if we take $\eta=\frac{R}{n\left(1+\beta^{2} p\right)+(p+\sqrt{p t+t)}}$, then, the misclassification error is

$$
\operatorname{Pr}\left[\theta_{k+1}^{T}\left(e_{1}+v\right)<0\right] \leq \exp \left(-\frac{n^{2}}{C_{p, k}^{2} \sigma^{2}\left(1+\beta^{2} p\right)}\right)+k e^{-t}
$$

where $C_{p, k}=\frac{1+2^{-k}}{1-2^{-k}} \cdot \frac{1-1 / 2}{1+1 / 2} \cdot \frac{\left(1+e^{R\left(1+\beta^{2} p\right)}\right)^{2}}{\left(1-\beta^{2} p\right)^{2}}$ and $\sigma^{2}=\left(1+\beta^{2} p\right) / 2 \rho$. Specifically, for $t=n^{2}+\log 1 / k$, we have $\operatorname{Pr}\left[\theta_{k+1}^{T}\left(e_{1}+v\right)<0\right] \leq O\left(e^{-\frac{\rho n^{2}}{\left(1+\beta^{2} p\right)^{2}}}\right)$.

Remark. To make the projected NoisyGD converge exponentially, we still require $\beta^{2}<\frac{1}{p}$.

### 3.2 Robustness of NoisyGD under perturbations

In this section, we explore the robustness of NoisyGD against various perturbation types. For the sake of brevity, we focus on perturbations of the perfect feature $(\beta=0)$ by different attackers. The theoretical framework can easily be extended to include perturbations of actual features, following the same proof structure as outlined in Theorem 3.1 and Theorem 3.2. Our findings indicate that each mentioned perturbation type affects the feature shift parameter $\beta$, potentially increasing NoisyGD's dimension dependency.

### 3.2.1 Stochastic attackers

Non-robustness to perturbations in the training time. If the training feature is perturbed by some stochastic perturbation (while the testing feature is perfect), then, the misclassification error for GD is $\exp \left(-\frac{n^{2}}{\widetilde{\beta}^{2}}\right)$, which is dimension-independent for any $\widetilde{\beta}>0$. However, the sample complexity for NoisyGD is $O\left(\sqrt{\frac{\max \left\{\tilde{\beta}^{2} p, 1\right\} \log (1 / \gamma)}{\rho}}\right)$. Thus, the NoisyGD is non-robust even when we only perturb the training feature with attackers that make $\widetilde{\beta}>p^{-1 / 2}$. We postpone the details to Appendix B. 2

Non-robustness to perturbations in the testing time. If we only perturb the testing feature, then we still require $O\left(\frac{\max \{\sqrt{\bar{\beta}} \widetilde{\sim}, 1\} \sqrt{\log (1 / \gamma)}}{\sqrt{2 \rho}}\right)$ samples to achieve a misclassification error $\gamma$, which is still non-robust when $\widetilde{\beta}>p^{-1 / 2}$. The technical detail is similar to the proof of Theorem 3.2 .

### 3.2.2 Deterministic attackers

Non robustness to offset perturbations in the training time. Even if we just shift the training feature vectors away by a constant offset (while keeping the same margin and angle between features), it makes DP learning a lot harder. Precisely, for some vector $v \in \mathbb{R}^{p}$, we consider $v_{i}=v$ for $y_{i}=1$ and $v_{i}=-v$ for $y_{i}=-1$. Moreover, this makes absolutely no difference to the gradient, when we start from 0 because

$$
\nabla \mathcal{L}(\theta)=\frac{n}{2} \cdot 0.5 \cdot-\left(-e_{1}+v\right)+\frac{n}{2} \cdot 0.5 \cdot\left(e_{1}+v\right)=\frac{n}{2} e_{1}
$$

Thus, for GD, the misclassification error is always 0 . If all we know is that $\|v\|_{\infty} \leq \widetilde{\beta}$, the sample complexity for making the classification error less than $\gamma$ will be $O\left(\frac{\max \left\{p \tilde{\beta}^{2}, 1\right\} \sqrt{\log (1 / \gamma)}}{\sqrt{\bar{\rho}}}\right)$. The details are given in Appendix B.1.

Non-robustness to class imbalance. Note that in the above case, it is quite a coincidence that $v$ gets cancelled out in the non-private gradient. When the class is not balanced, the offset $v$ will be part of the gradient that overwhelms the signal. Consider the case where we have $\alpha n$ data points with label -1 and $(1-\alpha) n$ data points with label 1 for $\alpha \neq 0.5$, and we start at 0 , then $\nabla \mathcal{L}(\theta)=\frac{n}{2} e_{1}+\frac{(1-2 \alpha) n}{2} v$. If we allow the perturbation $v$ to be adversarially chosen, then there exists
$v$ satisfying $\|v\|_{\infty} \leq \widetilde{\beta}$ such that the sample complexity bound to achieve a misclassification error $\gamma$ is $O\left(\frac{\max \{\sqrt{\bar{\beta}} \widetilde{\beta}, 1\} \sqrt{\log \frac{1}{\gamma}}}{\sqrt{(1-\widetilde{\beta}+2 \widetilde{\beta} \alpha)^{2} \cdot \rho}}\right)$.

Non-robustness to adversarial perturbations in the testing time. When $v_{i}=0$ and $\|v\|_{\infty} \leq \beta$, that is, we only consider perturbations in the testing time, if we allow the perturbation $v$ to be adversarially chosen, then there exists $v$ satisfying $\|v\|_{\infty} \leq \widetilde{\beta}$ such that the sample complexity bound to achieve misclassification rate $\gamma$ is $O\left(\frac{G \max \{p \widetilde{\beta}, 1\} \sqrt{\log (1 / \gamma)}}{\sqrt{2 \rho}}\right)$. One may refer to Appendix C. 2 for the detail.

## 4 Solutions for non-robustness issues

In this section, we will explore various solutions for enhancing the robustness of NoisyGD. To deal with random perturbations, we suggest performing dimension reduction to reduce the feature shift parameter $\beta$, as detailed in Section 4.1. For offset perturbations, we will consider feature normalization to cancel out the perturbation, as discussed in Section 4.2. The proof of this section can be found in Appendix E.

### 4.1 Mitigating random perturbation: dimension reduction

In Abadi et al. (2016), dimension reduction methods, such as DP-PCA, were employed to enhance the performance of deep models. In this section, we demonstrate that applying PCA to the private features effectively improves robustness against random perturbations. Since we have a public dataset for pre-training a model, we consider performing dimension reduction with this public dataset.

To perform dimension reduction, our goal is to generate a projection matrix $\widehat{P}=\left[\widehat{P}_{1}, \ldots, \widehat{P}_{K-1}\right] \in$ $\mathbb{R}^{p \times(K-1)}$ and train with the dataset $\left(\widetilde{x}_{i}=\widehat{P} x_{i}, y_{i}\right)_{i=1}^{n}$. It is obvious that the "best projection" is one where the space spanned by $\widehat{P}$ matches the space spanned by $\left\{M_{i}\right\}_{i=1}^{K}$, with $M_{i}$ being the perfect feature of the $i$-th class (the $i$-th column of an ETF).

In practice, it is not possible to obtain $\left\{M_{i}\right\}_{i=1}^{K}$ directly, and $\widehat{P}$ needs to be generated using another dataset $\left\{\left(\widehat{x}_{i}, \widehat{y}_{i}\right)\right\}_{i=1}^{m}$.

Recalling the binary classification problem with a training set $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}$ and $v_{i}$ as the feature shift vector of $x_{i}$, as discussed in Section 3.2, even in the case of class balance, the accuracy is not robust when $\beta^{2} \geq 1 / p$. Consider a projection vector $\widehat{P}=e_{1}+\Delta$ with $\Delta$ satisfying $\|\Delta\|_{\infty} \leq \beta_{0}$. The following theorem shows that for $\beta \beta_{0}<\frac{1}{p}$, the misclassification error decays exponentially and is dimension-independent.

Theorem 4.1. For the NoisyGD trained with $\left\{\left(\widetilde{x}_{i}, y_{i}\right)\right\}_{i=1}^{n}$, the sample complexity to achieve a misclassification error $\gamma$ is $n=O\left(\sqrt{\frac{G_{\beta, \beta_{0}, p}^{2} \log _{\gamma}^{\gamma}}{M_{\beta, \beta_{0}, p}}}\right)$ with $G_{\beta, \beta_{0}, p}=1+\beta\left(1+\beta_{0}+p \beta_{0}\right)$ and $M_{\beta, \beta_{0}, p}=\left(1-\beta_{0}\right)^{2}-p \beta \beta_{0}-\left(1+\beta_{0}\right)\left(\beta+\beta_{0} p\right)-\left(\beta+\beta_{0} p\right)\left(1+\beta+\beta_{0}+\beta \beta_{0} p\right)$.

Remark. Theorem 4.1 suggests that dimension reduction can relax the requirement from $\beta^{2} p \leq 1$ to $\beta \beta_{0} p \leq 1$. Thus, dimension reduction can enhance robustness whenever $\beta_{0}<\beta$. Typically, $\beta_{0}$ is relatively small and tends to 0 as $m$ increases.

The next question is how to construct the projection matrix $\widehat{P}=\left[\widehat{P}_{1}, \cdots, \widehat{P}_{K-1}\right] \in \mathbb{R}^{p \times(K-1)}$. We introduce the follwoing two methods.

Principle component analysis. Let $\left\{\widehat{P}_{j}\right\}_{j=1}^{K-1}$ be the the eigenvectors corresponding to $K-1$ largest eigenvalues of $\widehat{\Sigma}=\frac{1}{m} \sum_{i=1}^{m} \widehat{x}_{i} \widehat{x}_{i}^{T}$. For the binary case $(K=2)$, we have $\widehat{\Sigma}$ converges to $\Sigma=e_{1} e_{1}^{T}+\widehat{\beta}^{2} I_{p}$ for some constant $\widehat{\beta}$. Note that the eigenvector corresponding to the largest eigenvalue of $\Sigma$ is the perfect feature $e_{1}$. As $\beta_{0}$ is the infinity norm of $\Delta$, we use a bound on the infinity norm of eigenvectors (Fan et al., 2017). We state the results for $K=2$ that can be extended to $K>2$. Precisely, for $K=2$, let $\widehat{P}$ be the eigenvector of $\frac{1}{m} \sum_{i=1}^{m} \widehat{x}_{i} \widehat{x}_{i}^{T}$ that corresponds to the largest eigenvalue. Then, it holds $\beta_{0}=\left\|\widehat{P}-e_{1}\right\|_{\infty} \leq O\left(\frac{1}{\sqrt{m}}\right)$ with probability $O\left(p e^{-m^{2}}\right)$.

Releasing the mean of features. Let $X_{k}=\left\{\widehat{x}_{i}: \widehat{y}_{i}\right.$ belongs to the $k$-th class $\}$. Let $\widehat{P}_{k}=$ $\frac{1}{m_{k}} \sum_{\widehat{x}_{i} \in X_{k}} \widehat{x}_{i}$ with $m_{k}$ being the size of $X_{k}$. Then, we have $\Delta=\widehat{P}_{k}-M_{k}=\frac{1}{m_{k}} \sum_{\widehat{x}_{i} \in X_{k}} \widehat{x}_{i}$. By the concentration inequality, we have $\beta_{0}=\|\Delta\|_{\infty} \leq O\left(\frac{\widetilde{\beta}}{\sqrt{m_{k}}}\right)$ with probability $p e^{-m_{k}^{2}}$.

### 4.2 Addressing offset perturbations: normalization

Recall the shift perturbation where, for $x_{i} \in X_{k}$, we have $x_{i}=\widetilde{M}_{k}:=M_{k}+v$ with some fixed vector $v$.

To deal with the offset perturbation $v$, we pre-process the feature as $\widetilde{x}_{i}=x_{i}-\frac{1}{n} \sum_{j=1}^{n} x_{j}$. Then, if the class is balanced, it holds $\widetilde{x}_{i}=\widetilde{M}_{k}-\frac{1}{K} \sum_{j=1}^{K} \widetilde{M}_{j}=M_{k}$ for $x_{i} \in X_{k}$. That is, the perturbations canceled out.

We still need to bound the sensitivity of the gradient when training with $\left\{\left(\widetilde{x}_{i}, y_{i}\right)\right\}_{i=1}^{n}$. If we delete arbitrary $\left(x_{j}, y_{j}\right)$ from the dataset, then for the case $K=2$ with data balance, the sensitivity of the gradient is $G=\frac{n}{n-1} \leq 2$, which is upper bounded by a dimension-independent constant. The sample complexity to achieve the misclassification error $\gamma$ is $O\left(\frac{\sqrt{\log (1 / \gamma)}}{\sqrt{\rho}}\right)$, which is dimension independent.

Note that this normalization method is not robust to class imbalance. In fact, if we consider the class imbalanced case with which we have $\alpha$ data points with label +1 and the rest $(1-\alpha) n$ data points with label -1 for some $\alpha>0$, then we have $\widetilde{x}_{i}=2(1-\alpha) e_{1}$ for $y_{i}=1$ and $\widetilde{x}_{i}=-2 \alpha e_{1}$ for $y_{i}=-1$. In this class-imbalance case, one can recover the feature embedding $e_{1}$ and $-e_{1}$ by considering $\frac{\widetilde{x}_{i}}{\left\|\tilde{x}_{i}\right\|_{2}}$. However, in this case, the sensitivity remains a constant $G_{\alpha}$ which, although independent of the dimension $p$, still relies on $\alpha$.

## 5 Experiments

In this section, we will conduct four sets of experiments to validate our theoretical analysis in Section 3 and Section 4. We first discuss the details of Figure 2, which empirically evaluates the neural collapse level using the feature shift parameter with different pre-trained models in Section

5.1. The second experiment, outlined in Section 5.2, focuses on the synthetic perfect neural collapse scenario. In the third experiment, detailed in Section 5.3, we empirically investigate the behavior of NoisySGD (with fine-tuning the last layer) under different robustness settings and demonstrate that NoisySGD is "almost" dimension-independent, but this "dimension-independency" is not robust to minor perturbations in the last-layer feature. In the last experiment, discussed in Section 5.4, we demonstrate that dimension reduction methods such as PCA effectively enhance the robustness of NoisySGD.

### 5.1 Evaluate the feature shift vector

In this section, we empirically investigate the feature shift parameter $\beta$ if the pre-trained transformer is the Vision Transformer (ViT) or ResNet-50 and the fine-tuned dataset is CIFAR-10. The results are displayed in Figure 2 in the Introduction section.

Recall the feature shift parameter $\beta=\left\|x-M_{k}\right\|_{\infty}$ that is the $\ell_{\infty}$ distance between the feature $x$ and the perfect feature feature mean $M_{k}$ of the class $k$. The perfect feature $M_{k}$ is unknown, thus, we cannot compute the feature shift parameter $\beta=\left\|x-M_{k}\right\|_{\infty}$ exactly. However, we can approximate $M_{k}$ by the empirical feature mean $\widehat{M}_{k}$, which is the average of all features in the $k$-th class. We use the following steps to evaluate $\beta$ numerically.

- If $\cos \left(\widehat{M}_{i}, \widehat{M}_{j}\right) \approx \frac{1}{K-1}$ for all $1 \leq i, j \leq K$, then we may claim that NC happens and $\widehat{M}_{k}$ can be regarded as the perfect feature (as implied by the maximal-equiangularity in equation 2.2).
- Suppose we have $n$ data points with features $\left\{x_{i}\right\}_{i=1}^{n}$. For $x_{i}$ in the $k$-th class, we empirically calculate $\left\|x_{i}-\widehat{M}_{k}\right\|_{\infty}$ and plot their distribution.

To instantiate various $\beta$ distributions, we consider two ImageNet pre-trained models (ResNet50 and the vision transformer (ViT) model). We apply two models to extract training features, applying standard feature normalizations $\left(\|x\|_{2}=1\right)$ and evaluate their empirical $\beta$ across all training samples.

Specifically, we first calculate the training feature mean per class $\widehat{M}_{i}$ for $i \in[K]$ and evaluate the cosine matrix of $\cos \left(\widehat{M}_{i}, \widehat{M}_{j}\right)$ for $1 \leq i, j \leq K$. We found that all entries of the cosine matrix are close to $1 /(K-1)$ (roughly all entries between $[-0.2,0.2]$ and the cosine median is 0.082 for the ViT model and 0.112 for the ResNet-50 model). Therefore, we next evaluate $\left\|x-\widehat{M}_{k}\right\|_{\infty}$ for all training samples.

### 5.2 Fine-tune NoisyGD with synthetic neural collapse feature

We first generate a synthetic data matrix $X \in \mathcal{R}^{n \times d}$ with feature dimension $d$ under perfect neural collapse. The number of classes $K$ is 10 and the sample size is $n=10^{4}$. In the default setting, we assume each class draws $n / K$ data from a column of $K$-ETF, the training starts from a zero weight $\theta$ and the testing data are drawn from the same distribution as $X$. The Gaussian noise is selected such that the NoisyGD is $\left(1,10^{-4}\right)$-DP.

In Figure 3(a), we observe that an imbalanced class alone does not affect the utility. However, NoisyGD becomes non-robust to class imbalance when combined with a private feature offset with $\|\nu\|_{\infty}=0.1$. Additionally, it is non-robust to perturbed test data with $\|\nu\|_{\infty}=0.1$.

### 5.3 Fine-tune NoisySGD with real datasets

In this section, we empirically investigate the non-robustness of neural collapse using real datatsets.

Precisely, we fine-tune NoisySGD with the ImageNet pre-trained vision transformer (Dosovitskiy et al., 2020) on CIFAR-10 for 10 epochs. Test features in the perturb setting are subjected to Gaussian noise with a variance of 0.1 . The vision transformer produces a 768 -dimensional feature for each image. To simulate different feature dimensions, we randomly sample a subset of coordinates or make copies of the entire feature space.

In Figure 3(b), we observe that while perturbing the testing features degrades the utility of both Linear SGD and NoisySGD, Linear SGD is generally unaffected by the increasing dimension. On the other hand, the accuracy of NoisySGD deteriorates significantly as the dimension increases.

![](https://cdn.mathpix.com/cropped/2024_05_26_b1ed682555cfdb8567e6g-16.jpg?height=778&width=1656&top_left_y=787&top_left_x=229)

![](https://cdn.mathpix.com/cropped/2024_05_26_b1ed682555cfdb8567e6g-16.jpg?height=499&width=751&top_left_y=802&top_left_x=248)

(a) Synthetic perfect NC feature $X \in \mathcal{R}^{n \times d}$ with $K=10, n=10^{4}$ and $\left(1.0,10^{-4}\right)$-DP. In the default setting, we assume each class draws $|n / K|$ data from a column of $K$-ETF, the training starts from a zero weight $w$ and the testing data are drawn from the same distribution as $X$

![](https://cdn.mathpix.com/cropped/2024_05_26_b1ed682555cfdb8567e6g-16.jpg?height=504&width=745&top_left_y=800&top_left_x=1118)

(b) CIFAR-10: Test features in the perturb setting are subjected to Gaussian noise with a variance of 0.1 . The vision transformer produces a 768 -dimensional feature for each image. To simulate different feature dimensions, we randomly sample a subset of coordinates or make copies of the entire feature space.

Figure 3: Empirical behaviors of NoisyGD under various robustness setting.

### 5.4 Enhance NoisySGD's robustness with PCA

In this experiment, we replicate the set up from Exp 5.3, simulating different feature dimensions and injecting Gaussian noise with a variance of 0.1 to perturb all dimensions of both training and testing features. For simplicity, we apply PCA to the covariance matrix of private feature instead of a DP-PCA, followed by principal component projections on both private and testing features prior to feed them into the neural network. As discussed in Section 4.1, choosing $K-1$ largest eigenvalues is sufficient to improve the robustness of NoisyGD. Therefore, we consider projecting features onto the top $k \in\{10,50,100\}$ components. Figure 4 shows that the best utility of NoisyGD is achieved when $k=10$, aligning with our theoretical findings. Moreover, a larger $k=100$ fails to improve robustness, likely because the additional 90 principal vectors contribute minimal information and introduce further randomness to the training.

![](https://cdn.mathpix.com/cropped/2024_05_26_b1ed682555cfdb8567e6g-17.jpg?height=518&width=762&top_left_y=237&top_left_x=733)

Figure 4: CIFAR-10. Apply PCA on both training and testing features before NoisySGD: setting $K-1$ principal components improves NoisySGD's robustness.

## 6 Discussion and future work

Most existing theory of DP-learning focuses on suboptimality in surrogate loss of testing data. Our paper studies 0-1 loss directly and observed very different behaviors under perfect and near-perfect neural collapse. In particular, we have $\log (1 /$ error) sample complexity rather than $1 /$ error sample complexity. Our theoretical findings shed on light that privacy theorists should look into structures of data and how one can adapt to them. Additionally, our result suggests a number of practical mitigations to make DP-learning more robust in nearly neural collapse settings. It will be useful to investigate whether the same tricks are useful for private learning in general even without neural collapse. Moreover, our results suggest that under neural collapse, choice of loss functions (square loss vs CE loss) do not matter very much for private learning. Square loss has the advantage of having a fixed Hessian independent to the parameter, thus making it easier to adapt to strong convexity parameters like in AdaSSP (Wang, 2018). This is worth exploring.

NoisyGD and NoisySGD theory suggests that one needs $\Omega\left(n^{2}\right)$ time complexity to achieve optimal privacy-utility-trade off in DP-ERM (faster algorithms exist but more complex and they handle only some of the settings). Our results on the other hand, suggest that when there are structures in the data, e.g., near-perfect neural collapse, the choice of number of iterations is no longer important, thus making computation easier.

Another perspective in our future study is to investigate when NC will occur in transfer learning. The presence of low-rank structures such as $\mathrm{NC}$ in representation learning depends on the pre-trained dataset and the downstream task. Based on our experiments, if the downstream task is CIFAR-10, then ViT performs better than ResNet-50. According to other references in our paper, for example, Masarczyk et al. (2023) show that the low-rank structure is observed if we pre-train on CIFAR-10 and fine-tune on any 10 classes of CIFAR-100. As NC may not consistently occur, we speculate that the collapse level is more significant when the classes of the downstream task closely resemble those of the pre-trained dataset. For instance, NC may manifest when a model is pre-trained on a broad category such as all animals, and the downstream task involves classifying more specific sub-classes (e.g., different breeds of dogs). This intuition needs further empirical investigation with ample computational resources.

There is another important future topic to consider: what if NC does not occur? When NC cannot be observed, we conjecture that fine-tuning all layers under DP may lead to some low-rank
structure of the last layer (potentially inducing some minor collapse phenomenon due to the random noise introduced by DP). Whether NC can be observed after fine-tuning all layers under DP will be validated in our future study.

It is obvious that our theory for NoisyGD can be further generalized using Gaussian differential privacy (Dong et al., 2022), which leads to better utility analysis of our theory. However, extending our theory to NoisySGD is not straightforward, as the privacy accounting becomes complicated when considering sub-sampling due to mini-batches (Zhu and Wang, 2019; Wang et al., 2020; Balle et al., 2018). If we further consider multiple iterations, the joint effects of sub-sampling and composition on the privacy budget is another challenge (Zhu et al., 2022). Besides using composition and the central limit theorem for Gaussian DP (Bu et al., 2020; Wang et al., 2022), incorporating recently developed privacy analyses of last-iteration output of Noisy(S)GD in terms of the training dynamic (Ye and Shokri, 2022; Altschuler and Talwar, 2022; Bok et al., 2024) to obtain refined privacy analyses is another potential future topic. Existing privacy analyses of the last-iteration output are applied to strongly convex loss functions or convex functions with bounded domain, which is applicable to our setting when fine-tuning the last layer. If we further consider the privacy of NoisySGD under random initialization, the privacy analysis becomes much more complicated, as studied by (Ye et al., 2023; Wang et al., 2023). Moreover, in Appendix F, we discussed some extensions of our utility theory to the random initialization case, which shows that the utility theory is much more sophisticated than in the 0 -initialization case.

In addition to DP-ERM, there has been notable recent interest in using public data to enhance the accuracy of DP synthetic data (Ghalebikesabi et al., 2023; Liu et al., 2021). The incorporation of public data, either for traditional query-based synthetic data methods (McKenna et al., 2021; Li et al., 2023b) or more recent techniques such as DP generative adversarial networks or diffusion models (Goodfellow et al., 2020; Ho et al., 2020), has shown promise. The possibility of extending our perspective from Neural Collapse on DP-ERM to DP synthetic data in future research is intriguing.

## References

Abadi, M., Chu, A., Goodfellow, I. J., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. (2016). Deep learning with differential privacy. In Weippl, E. R., Katzenbeisser, S., Kruegel, C., Myers, A. C., and Halevi, S., editors, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna, Austria, October 24-28, 2016, pages 308-318. ACM.

Altschuler, J. M. and Talwar, K. (2022). Privacy of noisy stochastic gradient descent: More iterations without more privacy loss. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.

Balle, B., Barthe, G., and Gaboardi, M. (2018). Privacy amplification by subsampling: Tight analyses via couplings and divergences. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages $6280-6290$.

Bao, W., Pittaluga, F., b g, V. K., and Bindschaedler, V. (2023). DP-mix: Mixup-based data augmentation for differentially private learning. In Thirty-seventh Conference on Neural Information Processing Systems.

Bassily, R., Feldman, V., Guzmán, C., and Talwar, K. (2020). Stability of stochastic gradient descent on nonsmooth convex losses. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Bassily, R., Feldman, V., Talwar, K., and Thakurta, A. G. (2019). Private stochastic convex optimization with optimal rates. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E. B., and Garnett, R., editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages $11279-11288$.

Bassily, R., Smith, A. D., and Thakurta, A. (2014). Private empirical risk minimization: Efficient algorithms and tight error bounds. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014, pages 464-473. IEEE Computer Society.

Bok, J., Su, W., and Altschuler, J. M. (2024). Shifted interpolation for differential privacy. arXiv preprint arXiv:2403.00278.

Bu, Z., Dong, J., Long, Q., and Su, W. (2020). Deep Learning With Gaussian Differential Privacy. Harvard Data Science Review, 2(3). https://hdsr.mitpress.mit.edu/pub/u24wj42y.

Bu, Z., Wang, Y., Zha, S., and Karypis, G. (2022). Differentially private bias-term only fine-tuning of foundation models. arXiv preprint arXiv:2210.00036.

Bun, M., Livni, R., and Moran, S. (2020). An equivalence between private classification and online prediction. In Irani, S., editor, 61st IEEE Annual Symposium on Foundations of Computer Science, FOCS 2020, Durham, NC, USA, November 16-19, 2020, pages 389-402. IEEE.

Bun, M., Nissim, K., Stemmer, U., and Vadhan, S. (2015). Differentially private release and learning of threshold functions. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages $634-649$. IEEE.

Bun, M. and Steinke, T. (2016). Concentrated differential privacy: Simplifications, extensions, and lower bounds. In Hirt, M. and Smith, A. D., editors, Theory of Cryptography - 14th International Conference, TCC 2016-B, Beijing, China, October 31 - November 3, 2016, Proceedings, Part I, volume 9985 of Lecture Notes in Computer Science, pages $635-658$.

Chaudhuri, K. and Hsu, D. (2011). Sample complexity bounds for differentially private learning. In Proceedings of the 24th Annual Conference on Learning Theory, pages 155-186. JMLR Workshop and Conference Proceedings.

De, S., Berrada, L., Hayes, J., Smith, S. L., and Balle, B. (2022). Unlocking high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Dong, J., Roth, A., and Su, W. J. (2022). Gaussian differential privacy. J. R. Stat. Soc. Ser. B. Stat. Methodol., 84(1):3-54. With discussions and a reply by the authors.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth $16 \times 16$ words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.

Dwork, C. (2006). Differential privacy. In Bugliesi, M., Preneel, B., Sassone, V., and Wegener, I., editors, Automata, Languages and Programming, 33rd International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II, volume 4052 of Lecture Notes in Computer Science, pages 1-12. Springer.

Fan, J., Wang, W., and Zhong, Y. (2017). An $\$ \backslash$ ell_\{\infty $\}$ eigenvector perturbation bound and its application. J. Mach. Learn. Res., 18:207:1-207:42.

Fang, C., He, H., Long, Q., and Su, W. J. (2021). Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences, 118(43):e2103091118.

Feldman, V., Koren, T., and Talwar, K. (2020). Private stochastic convex optimization: optimal rates in linear time. In Makarychev, K., Makarychev, Y., Tulsiani, M., Kamath, G., and Chuzhoy, J., editors, Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, Chicago, IL, USA, June 22-26, 2020, pages 439-449. ACM.

Ghalebikesabi, S., Berrada, L., Gowal, S., Ktena, I., Stanforth, R., Hayes, J., De, S., Smith, S. L., Wiles, O., and Balle, B. (2023). Differentially private diffusion models generate useful synthetic images. arXiv preprint arXiv:2302.13861.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2020). Generative adversarial networks. Commun. ACM, 63(11):139-144.

He, H. and Su, W. J. (2023). A law of data separation in deep learning. Proceedings of the National Academy of Sciences, 120(36):e2221704120.

Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Laurent, B. and Massart, P. (2000). Adaptive estimation of a quadratic functional by model selection. Ann. Statist., 28(5):1302-1338.

Li, X., Liu, D., Hashimoto, T. B., Inan, H. A., Kulkarni, J., Lee, Y.-T., and Guha Thakurta, A. (2022). When does differentially private learning not suffer in high dimensions? Advances in Neural Information Processing Systems, 35:28616-28630.

Li, X., Liu, S., Zhou, J., Fernandez-Granda, C., Zhu, Z., and Qu, Q. (2023a). What deep representations should we learn? - a neural collapse perspective. https://openreview.net/forum?id=ZKEhS93FjhR.

Li, X., Tramer, F., Liang, P., and Hashimoto, T. (2021). Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679.

Li, X., Wang, C., and Cheng, G. (2023b). Statistical theory of differentially private marginal-based data synthesis algorithms. In The Eleventh International Conference on Learning Representations.

Liu, T., Vietri, G., Steinke, T., Ullman, J. R., and Wu, Z. S. (2021). Leveraging public data for practical private query release. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 6968-6977. PMLR.

Masarczyk, W., Ostaszewski, M., Imani, E., Pascanu, R., Miłoś, P., and Trzciński, T. (2023). The tunnel effect: Building data representations in deep neural networks. NeurIPS.

McKenna, R., Miklau, G., and Sheldon, D. (2021). Winning the NIST contest: A scalable and general approach to differentially private synthetic data. J. Priv. Confidentiality, 11(3).

Papyan, V., Han, X. Y., and Donoho, D. L. (2020). Prevalence of neural collapse during the terminal phase of deep learning training. Proc. Natl. Acad. Sci. USA, 117(40):24652-24663.

Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. (2018). Improving language understanding by generative pre-training. OpenAI.

Song, S., Chaudhuri, K., and Sarwate, A. D. (2013). Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245-248. IEEE.

Sutskever, I., Martens, J., Dahl, G. E., and Hinton, G. E. (2013). On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of JMLR Workshop and Conference Proceedings, pages 1139-1147. JMLR.org.

Tramer, F. and Boneh, D. (2020). Differentially private learning needs better features (or much more data). arXiv preprint arXiv:2011.11660.

Wang, C., Su, B., Ye, J., Shokri, R., and Su, W. J. (2023). Unified enhancement of privacy bounds for mixture mechanisms via $f$-differential privacy. In NeurIPS.

Wang, H., Gao, S., Zhang, H., Shen, M., and Su, W. J. (2022). Analytical composition of differential privacy via the edgeworth accountant.

Wang, Y. (2018). Revisiting differentially private linear regression: optimal and adaptive prediction \& estimation in unbounded domain. In Globerson, A. and Silva, R., editors, Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018, pages 93-103. AUAI Press.

Wang, Y., Balle, B., and Kasiviswanathan, S. P. (2020). Subsampled Rényi differential privacy and analytical moments accountant. J. Priv. Confidentiality, 10(2).

Wang, Y., Lei, J., and Fienberg, S. E. (2016). Learning with differential privacy: stability, learnability and the sufficiency and necessity of ERM principle. J. Mach. Learn. Res., 17:Paper No. 183, 40.

Ye, J. and Shokri, R. (2022). Differentially private learning needs hidden state (or much faster convergence). In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.

Ye, J., Zhu, Z., Liu, F., Shokri, R., and Cevher, V. (2023). Initialization matters: Privacy-utility analysis of overparameterized neural networks. In NeurIPS.

Yu, D., Naik, S., Backurs, A., Gopi, S., Inan, H. A., Kamath, G., Kulkarni, J., Lee, Y. T., Manoel, A., Wutschitz, L., et al. (2021). Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500.

Zhu, Y., Dong, J., and Wang, Y. (2022). Optimal accounting of differential privacy via characteristic function. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I., editors, International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event, volume 151 of Proceedings of Machine Learning Research, pages 4782-4817. PMLR.

Zhu, Y. and Wang, Y. (2019). Poission subsampled Rényi differential privacy. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 7634-7642. PMLR.
